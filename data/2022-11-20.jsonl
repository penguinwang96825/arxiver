{"title": "UnifiedABSA: A Unified ABSA Framework Based on Multi-task Instruction\n  Tuning", "abstract": "Aspect-Based Sentiment Analysis (ABSA) aims to provide fine-grained\naspect-level sentiment information. There are many ABSA tasks, and the current\ndominant paradigm is to train task-specific models for each task. However,\napplication scenarios of ABSA tasks are often diverse. This solution usually\nrequires a large amount of labeled data from each task to perform excellently.\nThese dedicated models are separately trained and separately predicted,\nignoring the relationship between tasks. To tackle these issues, we present\nUnifiedABSA, a general-purpose ABSA framework based on multi-task instruction\ntuning, which can uniformly model various tasks and capture the inter-task\ndependency with multi-task learning. Extensive experiments on two benchmark\ndatasets show that UnifiedABSA can significantly outperform dedicated models on\n11 ABSA tasks and show its superiority in terms of data efficiency.", "published": "2022-11-20 14:21:09", "link": "http://arxiv.org/abs/2211.10986v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Fine-grained Information via Knowledge-aware Hierarchical Graph\n  for Zero-shot Entity Retrieval", "abstract": "Zero-shot entity retrieval, aiming to link mentions to candidate entities\nunder the zero-shot setting, is vital for many tasks in Natural Language\nProcessing. Most existing methods represent mentions/entities via the sentence\nembeddings of corresponding context from the Pre-trained Language Model.\nHowever, we argue that such coarse-grained sentence embeddings can not fully\nmodel the mentions/entities, especially when the attention scores towards\nmentions/entities are relatively low. In this work, we propose GER, a\n\\textbf{G}raph enhanced \\textbf{E}ntity \\textbf{R}etrieval framework, to\ncapture more fine-grained information as complementary to sentence embeddings.\nWe extract the knowledge units from the corresponding context and then\nconstruct a mention/entity centralized graph. Hence, we can learn the\nfine-grained information about mention/entity by aggregating information from\nthese knowledge units. To avoid the graph information bottleneck for the\ncentral mention/entity node, we construct a hierarchical graph and design a\nnovel Hierarchical Graph Attention Network~(HGAN). Experimental results on\npopular benchmarks demonstrate that our proposed GER framework performs better\nthan previous state-of-the-art models. The code has been available at\nhttps://github.com/wutaiqiang/GER-WSDM2023.", "published": "2022-11-20 14:37:53", "link": "http://arxiv.org/abs/2211.10991v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Embracing Ambiguity: Improving Similarity-oriented Tasks with Contextual\n  Synonym Knowledge", "abstract": "Contextual synonym knowledge is crucial for those similarity-oriented tasks\nwhose core challenge lies in capturing semantic similarity between entities in\ntheir contexts, such as entity linking and entity matching. However, most\nPre-trained Language Models (PLMs) lack synonym knowledge due to inherent\nlimitations of their pre-training objectives such as masked language modeling\n(MLM). Existing works which inject synonym knowledge into PLMs often suffer\nfrom two severe problems: (i) Neglecting the ambiguity of synonyms, and (ii)\nUndermining semantic understanding of original PLMs, which is caused by\ninconsistency between the exact semantic similarity of the synonyms and the\nbroad conceptual relevance learned from the original corpus. To address these\nissues, we propose PICSO, a flexible framework that supports the injection of\ncontextual synonym knowledge from multiple domains into PLMs via a novel\nentity-aware Adapter which focuses on the semantics of the entities (synonyms)\nin the contexts. Meanwhile, PICSO stores the synonym knowledge in additional\nparameters of the Adapter structure, which prevents it from corrupting the\nsemantic understanding of the original PLM. Extensive experiments demonstrate\nthat PICSO can dramatically outperform the original PLMs and the other\nknowledge and synonym injection models on four different similarity-oriented\ntasks. In addition, experiments on GLUE prove that PICSO also benefits general\nnatural language understanding tasks. Codes and data will be public.", "published": "2022-11-20 15:25:19", "link": "http://arxiv.org/abs/2211.10997v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SeDR: Segment Representation Learning for Long Documents Dense Retrieval", "abstract": "Recently, Dense Retrieval (DR) has become a promising solution to document\nretrieval, where document representations are used to perform effective and\nefficient semantic search. However, DR remains challenging on long documents,\ndue to the quadratic complexity of its Transformer-based encoder and the finite\ncapacity of a low-dimension embedding. Current DR models use suboptimal\nstrategies such as truncating or splitting-and-pooling to long documents\nleading to poor utilization of whole document information. In this work, to\ntackle this problem, we propose Segment representation learning for long\ndocuments Dense Retrieval (SeDR). In SeDR, Segment-Interaction Transformer is\nproposed to encode long documents into document-aware and segment-sensitive\nrepresentations, while it holds the complexity of splitting-and-pooling and\noutperforms other segment-interaction patterns on DR. Since GPU memory\nrequirements for long document encoding causes insufficient negatives for DR\ntraining, Late-Cache Negative is further proposed to provide additional cache\nnegatives for optimizing representation learning. Experiments on MS MARCO and\nTREC-DL datasets show that SeDR achieves superior performance among DR models,\nand confirm the effectiveness of SeDR on long document retrieval.", "published": "2022-11-20 01:28:44", "link": "http://arxiv.org/abs/2211.10841v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Mulco: Recognizing Chinese Nested Named Entities Through Multiple Scopes", "abstract": "Nested Named Entity Recognition (NNER) has been a long-term challenge to\nresearchers as an important sub-area of Named Entity Recognition. NNER is where\none entity may be part of a longer entity, and this may happen on multiple\nlevels, as the term nested suggests. These nested structures make traditional\nsequence labeling methods unable to properly recognize all entities. While\nrecent researches focus on designing better recognition methods for NNER in a\nvariety of languages, the Chinese NNER (CNNER) still lacks attention, where a\nfree-for-access, CNNER-specialized benchmark is absent. In this paper, we aim\nto solve CNNER problems by providing a Chinese dataset and a learning-based\nmodel to tackle the issue. To facilitate the research on this task, we release\nChiNesE, a CNNER dataset with 20,000 sentences sampled from online passages of\nmultiple domains, containing 117,284 entities failing in 10 categories, where\n43.8 percent of those entities are nested. Based on ChiNesE, we propose Mulco,\na novel method that can recognize named entities in nested structures through\nmultiple scopes. Each scope use a designed scope-based sequence labeling\nmethod, which predicts an anchor and the length of a named entity to recognize\nit. Experiment results show that Mulco has outperformed several baseline\nmethods with the different recognizing schemes on ChiNesE. We also conduct\nextensive experiments on ACE2005 Chinese corpus, where Mulco has achieved the\nbest performance compared with the baseline methods.", "published": "2022-11-20 02:53:05", "link": "http://arxiv.org/abs/2211.10854v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How to Describe Images in a More Funny Way? Towards a Modular Approach\n  to Cross-Modal Sarcasm Generation", "abstract": "Sarcasm generation has been investigated in previous studies by considering\nit as a text-to-text generation problem, i.e., generating a sarcastic sentence\nfor an input sentence. In this paper, we study a new problem of cross-modal\nsarcasm generation (CMSG), i.e., generating a sarcastic description for a given\nimage. CMSG is challenging as models need to satisfy the characteristics of\nsarcasm, as well as the correlation between different modalities. In addition,\nthere should be some inconsistency between the two modalities, which requires\nimagination. Moreover, high-quality training data is insufficient. To address\nthese problems, we take a step toward generating sarcastic descriptions from\nimages without paired training data and propose an\nExtraction-Generation-Ranking based Modular method (EGRM) for cross-model\nsarcasm generation. Specifically, EGRM first extracts diverse information from\nan image at different levels and uses the obtained image tags, sentimental\ndescriptive caption, and commonsense-based consequence to generate candidate\nsarcastic texts. Then, a comprehensive ranking algorithm, which considers\nimage-text relation, sarcasticness, and grammaticality, is proposed to select a\nfinal text from the candidate texts. Human evaluation at five criteria on a\ntotal of 1200 generated image-text pairs from eight systems and auxiliary\nautomatic evaluation show the superiority of our method.", "published": "2022-11-20 14:38:24", "link": "http://arxiv.org/abs/2211.10992v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Understanding and Improving Knowledge Distillation for\n  Quantization-Aware Training of Large Transformer Encoders", "abstract": "Knowledge distillation (KD) has been a ubiquitous method for model\ncompression to strengthen the capability of a lightweight model with the\ntransferred knowledge from the teacher. In particular, KD has been employed in\nquantization-aware training (QAT) of Transformer encoders like BERT to improve\nthe accuracy of the student model with the reduced-precision weight parameters.\nHowever, little is understood about which of the various KD approaches best\nfits the QAT of Transformers. In this work, we provide an in-depth analysis of\nthe mechanism of KD on attention recovery of quantized large Transformers. In\nparticular, we reveal that the previously adopted MSE loss on the attention\nscore is insufficient for recovering the self-attention information. Therefore,\nwe propose two KD methods; attention-map and attention-output losses.\nFurthermore, we explore the unification of both losses to address\ntask-dependent preference between attention-map and output losses. The\nexperimental results on various Transformer encoder models demonstrate that the\nproposed KD methods achieve state-of-the-art accuracy for QAT with sub-2-bit\nweight quantization.", "published": "2022-11-20 16:23:23", "link": "http://arxiv.org/abs/2211.11014v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Explaining (Sarcastic) Utterances to Enhance Affect Understanding in\n  Multimodal Dialogues", "abstract": "Conversations emerge as the primary media for exchanging ideas and\nconceptions. From the listener's perspective, identifying various affective\nqualities, such as sarcasm, humour, and emotions, is paramount for\ncomprehending the true connotation of the emitted utterance. However, one of\nthe major hurdles faced in learning these affect dimensions is the presence of\nfigurative language, viz. irony, metaphor, or sarcasm. We hypothesize that any\ndetection system constituting the exhaustive and explicit presentation of the\nemitted utterance would improve the overall comprehension of the dialogue. To\nthis end, we explore the task of Sarcasm Explanation in Dialogues, which aims\nto unfold the hidden irony behind sarcastic utterances. We propose MOSES, a\ndeep neural network, which takes a multimodal (sarcastic) dialogue instance as\nan input and generates a natural language sentence as its explanation.\nSubsequently, we leverage the generated explanation for various natural\nlanguage understanding tasks in a conversational dialogue setup, such as\nsarcasm detection, humour identification, and emotion recognition. Our\nevaluation shows that MOSES outperforms the state-of-the-art system for SED by\nan average of ~2% on different evaluation metrics, such as ROUGE, BLEU, and\nMETEOR. Further, we observe that leveraging the generated explanation advances\nthree downstream tasks for affect classification - an average improvement of\n~14% F1-score in the sarcasm detection task and ~2% in the humour\nidentification and emotion recognition task. We also perform extensive analyses\nto assess the quality of the results.", "published": "2022-11-20 18:05:43", "link": "http://arxiv.org/abs/2211.11049v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semantic Similarity-Based Clustering of Findings From Security Testing\n  Tools", "abstract": "Over the last years, software development in domains with high security\ndemands transitioned from traditional methodologies to uniting modern\napproaches from software development and operations (DevOps). Key principles of\nDevOps gained more importance and are now applied to security aspects of\nsoftware development, resulting in the automation of security-enhancing\nactivities. In particular, it is common practice to use automated security\ntesting tools that generate reports after inspecting a software artifact from\nmultiple perspectives. However, this raises the challenge of generating\nduplicate security findings. To identify these duplicate findings manually, a\nsecurity expert has to invest resources like time, effort, and knowledge. A\npartial automation of this process could reduce the analysis effort, encourage\nDevOps principles, and diminish the chance of human error. In this study, we\ninvestigated the potential of applying Natural Language Processing for\nclustering semantically similar security findings to support the identification\nof problem-specific duplicate findings. Towards this goal, we developed a web\napplication for annotating and assessing security testing tool reports and\npublished a human-annotated corpus of clustered security findings. In addition,\nwe performed a comparison of different semantic similarity techniques for\nautomatically grouping security findings. Finally, we assess the resulting\nclusters using both quantitative and qualitative evaluation methods.", "published": "2022-11-20 19:03:19", "link": "http://arxiv.org/abs/2211.11057v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "A Theory of Unsupervised Translation Motivated by Understanding Animal\n  Communication", "abstract": "Neural networks are capable of translating between languages -- in some cases\neven between two languages where there is little or no access to parallel\ntranslations, in what is known as Unsupervised Machine Translation (UMT). Given\nthis progress, it is intriguing to ask whether machine learning tools can\nultimately enable understanding animal communication, particularly that of\nhighly intelligent animals. We propose a theoretical framework for analyzing\nUMT when no parallel translations are available and when it cannot be assumed\nthat the source and target corpora address related subject domains or posses\nsimilar linguistic structure. We exemplify this theory with two stylized models\nof language, for which our framework provides bounds on necessary sample\ncomplexity; the bounds are formally proven and experimentally verified on\nsynthetic data. These bounds show that the error rates are inversely related to\nthe language complexity and amount of common ground. This suggests that\nunsupervised translation of animal communication may be feasible if the\ncommunication system is sufficiently complex.", "published": "2022-11-20 20:55:38", "link": "http://arxiv.org/abs/2211.11081v2", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Conceptor-Aided Debiasing of Large Language Models", "abstract": "Pre-trained large language models (LLMs) reflect the inherent social biases\nof their training corpus. Many methods have been proposed to mitigate this\nissue, but they often fail to debias or they sacrifice model accuracy. We use\nconceptors--a soft projection method--to identify and remove the bias subspace\nin LLMs such as BERT and GPT. We propose two methods of applying conceptors (1)\nbias subspace projection by post-processing by the conceptor NOT operation; and\n(2) a new architecture, conceptor-intervened BERT (CI-BERT), which explicitly\nincorporates the conceptor projection into all layers during training. We find\nthat conceptor post-processing achieves state-of-the-art (SoTA) debiasing\nresults while maintaining LLMs' performance on the GLUE benchmark. Further, it\nis robust in various scenarios and can mitigate intersectional bias efficiently\nby its AND operation on the existing bias subspaces. Although CI-BERT's\ntraining takes all layers' bias into account and can beat its post-processing\ncounterpart in bias mitigation, CI-BERT reduces the language model accuracy. We\nalso show the importance of carefully constructing the bias subspace. The best\nresults are obtained by removing outliers from the list of biased words,\ncombining them (via the OR operation), and computing their embeddings using the\nsentences from a cleaner corpus.", "published": "2022-11-20 21:24:48", "link": "http://arxiv.org/abs/2211.11087v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VER: Unifying Verbalizing Entities and Relations", "abstract": "Entities and relationships between entities are vital in the real world.\nEssentially, we understand the world by understanding entities and relations.\nFor instance, to understand a field, e.g., computer science, we need to\nunderstand the relevant concepts, e.g., machine learning, and the relationships\nbetween concepts, e.g., machine learning and artificial intelligence. To\nunderstand a person, we should first know who he/she is and how he/she is\nrelated to others. To understand entities and relations, humans may refer to\nnatural language descriptions. For instance, when learning a new scientific\nterm, people usually start by reading its definition in dictionaries or\nencyclopedias. To know the relationship between two entities, humans tend to\ncreate a sentence to connect them. In this paper, we propose VER: a unified\nmodel for Verbalizing Entities and Relations. Specifically, we attempt to build\na system that takes any entity or entity set as input and generates a sentence\nto represent entities and relations. Extensive experiments demonstrate that our\nmodel can generate high-quality sentences describing entities and entity\nrelationships and facilitate various tasks on entities and relations, including\ndefinition modeling, relation modeling, and generative commonsense reasoning.", "published": "2022-11-20 21:50:33", "link": "http://arxiv.org/abs/2211.11093v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Stack: 3 TB of permissively licensed source code", "abstract": "Large Language Models (LLMs) play an ever-increasing role in the field of\nArtificial Intelligence (AI)--not only for natural language processing but also\nfor code understanding and generation. To stimulate open and responsible\nresearch on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting\nof permissively licensed source code in 30 programming languages. We describe\nhow we collect the full dataset, construct a permissively licensed subset,\npresent a data governance plan, discuss limitations, and show promising results\non text2code benchmarks by training 350M-parameter decoders on different Python\nsubsets. We find that (1) near-deduplicating the data significantly boosts\nperformance across all experiments, and (2) it is possible to match previously\nreported HumanEval and MBPP performance using only permissively licensed data.\nWe make the dataset available at https://hf.co/BigCode, provide a tool called\n\"Am I in The Stack\" (https://hf.co/spaces/bigcode/in-the-stack) for developers\nto search The Stack for copies of their code, and provide a process for code to\nbe removed from the dataset by following the instructions at\nhttps://www.bigcode-project.org/docs/about/the-stack/.", "published": "2022-11-20 18:15:30", "link": "http://arxiv.org/abs/2211.15533v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Artificial Interrogation for Attributing Language Models", "abstract": "This paper presents solutions to the Machine Learning Model Attribution\nchallenge (MLMAC) collectively organized by MITRE, Microsoft, Schmidt-Futures,\nRobust-Intelligence, Lincoln-Network, and Huggingface community. The challenge\nprovides twelve open-sourced base versions of popular language models developed\nby well-known organizations and twelve fine-tuned language models for text\ngeneration. The names and architecture details of fine-tuned models were kept\nhidden, and participants can access these models only through the rest APIs\ndeveloped by the organizers. Given these constraints, the goal of the contest\nis to identify which fine-tuned models originated from which base model. To\nsolve this challenge, we have assumed that fine-tuned models and their\ncorresponding base versions must share a similar vocabulary set with a matching\nsyntactical writing style that resonates in their generated outputs. Our\nstrategy is to develop a set of queries to interrogate base and fine-tuned\nmodels. And then perform one-to-many pairing between them based on similarities\nin their generated responses, where more than one fine-tuned model can pair\nwith a base model but not vice-versa. We have employed four distinct approaches\nfor measuring the resemblance between the responses generated from the models\nof both sets. The first approach uses evaluation metrics of the machine\ntranslation, and the second uses a vector space model. The third approach uses\nstate-of-the-art multi-class text classification, Transformer models. Lastly,\nthe fourth approach uses a set of Transformer based binary text classifiers,\none for each provided base model, to perform multi-class text classification in\na one-vs-all fashion. This paper reports implementation details, comparison,\nand experimental studies, of these approaches along with the final obtained\nresults.", "published": "2022-11-20 05:46:29", "link": "http://arxiv.org/abs/2211.10877v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pragmatic Constraint on Distributional Semantics", "abstract": "This paper studies the limits of language models' statistical learning in the\ncontext of Zipf's law. First, we demonstrate that Zipf-law token distribution\nemerges irrespective of the chosen tokenization. Second, we show that Zipf\ndistribution is characterized by two distinct groups of tokens that differ both\nin terms of their frequency and their semantics. Namely, the tokens that have a\none-to-one correspondence with one semantic concept have different statistical\nproperties than those with semantic ambiguity. Finally, we demonstrate how\nthese properties interfere with statistical learning procedures motivated by\ndistributional semantics.", "published": "2022-11-20 17:51:06", "link": "http://arxiv.org/abs/2211.11041v1", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT", "E.4; H.1.1; I.2.7"], "primary_category": "cs.CL"}
{"title": "Convexifying Transformers: Improving optimization and understanding of\n  transformer networks", "abstract": "Understanding the fundamental mechanism behind the success of transformer\nnetworks is still an open problem in the deep learning literature. Although\ntheir remarkable performance has been mostly attributed to the self-attention\nmechanism, the literature still lacks a solid analysis of these networks and\ninterpretation of the functions learned by them. To this end, we study the\ntraining problem of attention/transformer networks and introduce a novel convex\nanalytic approach to improve the understanding and optimization of these\nnetworks. Particularly, we first introduce a convex alternative to the\nself-attention mechanism and reformulate the regularized training problem of\ntransformer networks with our alternative convex attention. Then, we cast the\nreformulation as a convex optimization problem that is interpretable and easier\nto optimize. Moreover, as a byproduct of our convex analysis, we reveal an\nimplicit regularization mechanism, which promotes sparsity across tokens.\nTherefore, we not only improve the optimization of attention/transformer\nnetworks but also provide a solid theoretical understanding of the functions\nlearned by them. We also demonstrate the effectiveness of our theory through\nseveral numerical experiments.", "published": "2022-11-20 18:17:47", "link": "http://arxiv.org/abs/2211.11052v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Deep Learning on a Healthy Data Diet: Finding Important Examples for\n  Fairness", "abstract": "Data-driven predictive solutions predominant in commercial applications tend\nto suffer from biases and stereotypes, which raises equity concerns. Prediction\nmodels may discover, use, or amplify spurious correlations based on gender or\nother protected personal characteristics, thus discriminating against\nmarginalized groups. Mitigating gender bias has become an important research\nfocus in natural language processing (NLP) and is an area where annotated\ncorpora are available. Data augmentation reduces gender bias by adding\ncounterfactual examples to the training dataset. In this work, we show that\nsome of the examples in the augmented dataset can be not important or even\nharmful for fairness. We hence propose a general method for pruning both the\nfactual and counterfactual examples to maximize the model's fairness as\nmeasured by the demographic parity, equality of opportunity, and equality of\nodds. The fairness achieved by our method surpasses that of data augmentation\non three text classification datasets, using no more than half of the examples\nin the augmented dataset. Our experiments are conducted using models of varying\nsizes and pre-training settings.", "published": "2022-11-20 22:42:30", "link": "http://arxiv.org/abs/2211.11109v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Detecting Conspiracy Theory Against COVID-19 Vaccines", "abstract": "Since the beginning of the vaccination trial, social media has been flooded\nwith anti-vaccination comments and conspiracy beliefs. As the day passes, the\nnumber of COVID- 19 cases increases, and online platforms and a few news\nportals entertain sharing different conspiracy theories. The most popular\nconspiracy belief was the link between the 5G network spreading COVID-19 and\nthe Chinese government spreading the virus as a bioweapon, which initially\ncreated racial hatred. Although some disbelief has less impact on society,\nothers create massive destruction. For example, the 5G conspiracy led to the\nburn of the 5G Tower, and belief in the Chinese bioweapon story promoted an\nattack on the Asian-Americans. Another popular conspiracy belief was that Bill\nGates spread this Coronavirus disease (COVID-19) by launching a mass\nvaccination program to track everyone. This Conspiracy belief creates distrust\nissues among laypeople and creates vaccine hesitancy. This study aims to\ndiscover the conspiracy theory against the vaccine on social platforms. We\nperformed a sentiment analysis on the 598 unique sample comments related to\nCOVID-19 vaccines. We used two different models, BERT and Perspective API, to\nfind out the sentiment and toxicity of the sentence toward the COVID-19\nvaccine.", "published": "2022-11-20 04:59:33", "link": "http://arxiv.org/abs/2211.13003v1", "categories": ["cs.CY", "cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Contrastive Regularization for Multimodal Emotion Recognition Using\n  Audio and Text", "abstract": "Speech emotion recognition is a challenge and an important step towards more\nnatural human-computer interaction (HCI). The popular approach is multimodal\nemotion recognition based on model-level fusion, which means that the\nmultimodal signals can be encoded to acquire embeddings, and then the\nembeddings are concatenated together for the final classification. However, due\nto the influence of noise or other factors, each modality does not always tend\nto the same emotional category, which affects the generalization of a model. In\nthis paper, we propose a novel regularization method via contrastive learning\nfor multimodal emotion recognition using audio and text. By introducing a\ndiscriminator to distinguish the difference between the same and different\nemotional pairs, we explicitly restrict the latent code of each modality to\ncontain the same emotional information, so as to reduce the noise interference\nand get more discriminative representation. Experiments are performed on the\nstandard IEMOCAP dataset for 4-class emotion recognition. The results show a\nsignificant improvement of 1.44\\% and 1.53\\% in terms of weighted accuracy (WA)\nand unweighted accuracy (UA) compared to the baseline system.", "published": "2022-11-20 06:56:26", "link": "http://arxiv.org/abs/2211.10885v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LA-VocE: Low-SNR Audio-visual Speech Enhancement using Neural Vocoders", "abstract": "Audio-visual speech enhancement aims to extract clean speech from a noisy\nenvironment by leveraging not only the audio itself but also the target\nspeaker's lip movements. This approach has been shown to yield improvements\nover audio-only speech enhancement, particularly for the removal of interfering\nspeech. Despite recent advances in speech synthesis, most audio-visual\napproaches continue to use spectral mapping/masking to reproduce the clean\naudio, often resulting in visual backbones added to existing speech enhancement\narchitectures. In this work, we propose LA-VocE, a new two-stage approach that\npredicts mel-spectrograms from noisy audio-visual speech via a\ntransformer-based architecture, and then converts them into waveform audio\nusing a neural vocoder (HiFi-GAN). We train and evaluate our framework on\nthousands of speakers and 11+ different languages, and study our model's\nability to adapt to different levels of background noise and speech\ninterference. Our experiments show that LA-VocE outperforms existing methods\naccording to multiple metrics, particularly under very noisy scenarios.", "published": "2022-11-20 15:27:55", "link": "http://arxiv.org/abs/2211.10999v2", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Simultaneously Learning Robust Audio Embeddings and balanced Hash codes\n  for Query-by-Example", "abstract": "Audio fingerprinting systems must efficiently and robustly identify query\nsnippets in an extensive database. To this end, state-of-the-art systems use\ndeep learning to generate compact audio fingerprints. These systems deploy\nindexing methods, which quantize fingerprints to hash codes in an unsupervised\nmanner to expedite the search. However, these methods generate imbalanced hash\ncodes, leading to their suboptimal performance. Therefore, we propose a\nself-supervised learning framework to compute fingerprints and balanced hash\ncodes in an end-to-end manner to achieve both fast and accurate retrieval\nperformance. We model hash codes as a balanced clustering process, which we\nregard as an instance of the optimal transport problem. Experimental results\nindicate that the proposed approach improves retrieval efficiency while\npreserving high accuracy, particularly at high distortion levels, compared to\nthe competing methods. Moreover, our system is efficient and scalable in\ncomputational load and memory storage.", "published": "2022-11-20 19:22:44", "link": "http://arxiv.org/abs/2211.11060v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
