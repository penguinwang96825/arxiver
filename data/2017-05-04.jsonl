{"title": "Probabilistic Typology: Deep Generative Models of Vowel Inventories", "abstract": "Linguistic typology studies the range of structures present in human\nlanguage. The main goal of the field is to discover which sets of possible\nphenomena are universal, and which are merely frequent. For example, all\nlanguages have vowels, while most---but not all---languages have an /u/ sound.\nIn this paper we present the first probabilistic treatment of a basic question\nin phonological typology: What makes a natural vowel inventory? We introduce a\nseries of deep stochastic point processes, and contrast them with previous\ncomputational, simulation-based approaches. We provide a comprehensive suite of\nexperiments on over 200 distinct languages.", "published": "2017-05-04 03:13:03", "link": "http://arxiv.org/abs/1705.01684v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Finite State and Rule-based Akshara to Prosodeme (A2P) Converter in\n  Hindi", "abstract": "This article describes a software module called Akshara to Prosodeme (A2P)\nconverter in Hindi. It converts an input grapheme into prosedeme (sequence of\nphonemes with the specification of syllable boundaries and prosodic labels).\nThe software is based on two proposed finite state machines\\textemdash one for\nthe syllabification and another for the syllable labeling. In addition to that,\nit also uses a set of nonlinear phonological rules proposed for foot formation\nin Hindi, which encompass solutions to schwa-deletion in simple, compound,\nderived and inflected words. The nonlinear phonological rules are based on\nmetrical phonology with the provision of recursive foot structure. A software\nmodule is implemented in Python. The testing of the software for\nsyllabification, syllable labeling, schwa deletion and prosodic labeling yield\nan accuracy of more than 99% on a lexicon of size 28664 words.", "published": "2017-05-04 13:33:00", "link": "http://arxiv.org/abs/1705.01833v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sharp Models on Dull Hardware: Fast and Accurate Neural Machine\n  Translation Decoding on the CPU", "abstract": "Attentional sequence-to-sequence models have become the new standard for\nmachine translation, but one challenge of such models is a significant increase\nin training and decoding cost compared to phrase-based systems. Here, we focus\non efficient decoding, with a goal of achieving accuracy close the\nstate-of-the-art in neural machine translation (NMT), while achieving CPU\ndecoding speed/throughput close to that of a phrasal decoder.\n  We approach this problem from two angles: First, we describe several\ntechniques for speeding up an NMT beam search decoder, which obtain a 4.4x\nspeedup over a very efficient baseline decoder without changing the decoder\noutput. Second, we propose a simple but powerful network architecture which\nuses an RNN (GRU/LSTM) layer at bottom, followed by a series of stacked\nfully-connected layers applied at every timestep. This architecture achieves\nsimilar accuracy to a deep recurrent model, at a small fraction of the training\nand decoding cost. By combining these techniques, our best system achieves a\nvery competitive accuracy of 38.3 BLEU on WMT English-French NewsTest2014,\nwhile decoding at 100 words/sec on single-threaded CPU. We believe this is the\nbest published accuracy/speed trade-off of an NMT system.", "published": "2017-05-04 19:50:35", "link": "http://arxiv.org/abs/1705.01991v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine Comprehension by Text-to-Text Neural Question Generation", "abstract": "We propose a recurrent neural model that generates natural-language questions\nfrom documents, conditioned on answers. We show how to train the model using a\ncombination of supervised and reinforcement learning. After teacher forcing for\nstandard maximum likelihood training, we fine-tune the model using policy\ngradient techniques to maximize several rewards that measure question quality.\nMost notably, one of these rewards is the performance of a question-answering\nsystem. We motivate question generation as a means to improve the performance\nof question answering systems. Our model is trained and evaluated on the recent\nquestion-answering dataset SQuAD.", "published": "2017-05-04 20:58:06", "link": "http://arxiv.org/abs/1705.02012v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Senti17 at SemEval-2017 Task 4: Ten Convolutional Neural Network Voters\n  for Tweet Polarity Classification", "abstract": "This paper presents Senti17 system which uses ten convolutional neural\nnetworks (ConvNet) to assign a sentiment label to a tweet. The network consists\nof a convolutional layer followed by a fully-connected layer and a Softmax on\ntop. Ten instances of this network are initialized with the same word\nembeddings as inputs but with different initializations for the network\nweights. We combine the results of all instances by selecting the sentiment\nlabel given by the majority of the ten voters. This system is ranked fourth in\nSemEval-2017 Task4 over 38 systems with 67.4%", "published": "2017-05-04 21:13:24", "link": "http://arxiv.org/abs/1705.02023v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
