{"title": "Unit Dependency Graph and its Application to Arithmetic Word Problem\n  Solving", "abstract": "Math word problems provide a natural abstraction to a range of natural\nlanguage understanding problems that involve reasoning about quantities, such\nas interpreting election results, news about casualties, and the financial\nsection of a newspaper. Units associated with the quantities often provide\ninformation that is essential to support this reasoning. This paper proposes a\nprincipled way to capture and reason about units and shows how it can benefit\nan arithmetic word problem solver. This paper presents the concept of Unit\nDependency Graphs (UDGs), which provides a compact representation of the\ndependencies between units of numbers mentioned in a given problem. Inducing\nthe UDG alleviates the brittleness of the unit extraction system and allows for\na natural way to leverage domain knowledge about unit compatibility, for word\nproblem solving. We introduce a decomposed model for inducing UDGs with minimal\nadditional annotations, and use it to augment the expressions used in the\narithmetic word problem solver of (Roy and Roth 2015) via a constrained\ninference framework. We show that introduction of UDGs reduces the error of the\nsolver by over 10 %, surpassing all existing systems for solving arithmetic\nword problems. In addition, it also makes the system more robust to adaptation\nto new vocabulary and equation forms .", "published": "2016-12-03 14:14:11", "link": "http://arxiv.org/abs/1612.00969v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Joint Learning of Natural Language Understanding and Dialogue\n  Manager", "abstract": "Natural language understanding and dialogue policy learning are both\nessential in conversational systems that predict the next system actions in\nresponse to a current user utterance. Conventional approaches aggregate\nseparate models of natural language understanding (NLU) and system action\nprediction (SAP) as a pipeline that is sensitive to noisy outputs of\nerror-prone NLU. To address the issues, we propose an end-to-end deep recurrent\nneural network with limited contextual dialogue memory by jointly training NLU\nand SAP on DSTC4 multi-domain human-human dialogues. Experiments show that our\nproposed model significantly outperforms the state-of-the-art pipeline models\nfor both NLU and SAP, which indicates that our joint model is capable of\nmitigating the affects of noisy NLU outputs, and NLU model can be refined by\nerror flows backpropagating from the extra supervised signals of system\nactions.", "published": "2016-12-03 02:13:18", "link": "http://arxiv.org/abs/1612.00913v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Using Discourse Signals for Robust Instructor Intervention Prediction", "abstract": "We tackle the prediction of instructor intervention in student posts from\ndiscussion forums in Massive Open Online Courses (MOOCs). Our key finding is\nthat using automatically obtained discourse relations improves the prediction\nof when instructors intervene in student discussions, when compared with a\nstate-of-the-art, feature-rich baseline. Our supervised classifier makes use of\nan automatic discourse parser which outputs Penn Discourse Treebank (PDTB) tags\nthat represent in-post discourse features. We show PDTB relation-based features\nincrease the robustness of the classifier and complement baseline features in\nrecalling more diverse instructor intervention patterns. In comprehensive\nexperiments over 14 MOOC offerings from several disciplines, the PDTB discourse\nfeatures improve performance on average. The resultant models are less\ndependent on domain-specific vocabulary, allowing them to better generalize to\nnew courses.", "published": "2016-12-03 09:08:51", "link": "http://arxiv.org/abs/1612.00944v1", "categories": ["cs.AI", "cs.CL", "cs.CY", "I.2.7; K.3.1"], "primary_category": "cs.AI"}
