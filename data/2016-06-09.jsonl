{"title": "Neural Network-Based Abstract Generation for Opinions and Arguments", "abstract": "We study the problem of generating abstractive summaries for opinionated\ntext. We propose an attention-based neural network model that is able to absorb\ninformation from multiple text units to construct informative, concise, and\nfluent summaries. An importance-based sampling method is designed to allow the\nencoder to integrate information from an important subset of input. Automatic\nevaluation indicates that our system outperforms state-of-the-art abstractive\nand extractive summarization systems on two newly collected datasets of movie\nreviews and arguments. Our system summaries are also rated as more informative\nand grammatical in human evaluation.", "published": "2016-06-09 00:15:23", "link": "http://arxiv.org/abs/1606.02785v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inducing Domain-Specific Sentiment Lexicons from Unlabeled Corpora", "abstract": "A word's sentiment depends on the domain in which it is used. Computational\nsocial science research thus requires sentiment lexicons that are specific to\nthe domains being studied. We combine domain-specific word embeddings with a\nlabel propagation framework to induce accurate domain-specific sentiment\nlexicons using small sets of seed words, achieving state-of-the-art performance\ncompetitive with approaches that rely on hand-curated resources. Using our\nframework we perform two large-scale empirical studies to quantify the extent\nto which sentiment varies across time and between communities. We induce and\nrelease historical sentiment lexicons for 150 years of English and\ncommunity-specific sentiment lexicons for 250 online communities from the\nsocial media forum Reddit. The historical lexicons show that more than 5% of\nsentiment-bearing (non-neutral) English words completely switched polarity\nduring the last 150 years, and the community-specific lexicons highlight how\nsentiment varies drastically between different communities.", "published": "2016-06-09 04:28:10", "link": "http://arxiv.org/abs/1606.02820v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cultural Shift or Linguistic Drift? Comparing Two Computational Measures\n  of Semantic Change", "abstract": "Words shift in meaning for many reasons, including cultural factors like new\ntechnologies and regular linguistic processes like subjectification.\nUnderstanding the evolution of language and culture requires disentangling\nthese underlying causes. Here we show how two different distributional measures\ncan be used to detect two different types of semantic change. The first\nmeasure, which has been used in many previous works, analyzes global shifts in\na word's distributional semantics, it is sensitive to changes due to regular\nprocesses of linguistic drift, such as the semantic generalization of promise\n(\"I promise.\" -> \"It promised to be exciting.\"). The second measure, which we\ndevelop here, focuses on local changes to a word's nearest semantic neighbors;\nit is more sensitive to cultural shifts, such as the change in the meaning of\ncell (\"prison cell\" -> \"cell phone\"). Comparing measurements made by these two\nmethods allows researchers to determine whether changes are more cultural or\nlinguistic in nature, a distinction that is essential for work in the digital\nhumanities and historical linguistics.", "published": "2016-06-09 04:42:12", "link": "http://arxiv.org/abs/1606.02821v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Edinburgh Neural Machine Translation Systems for WMT 16", "abstract": "We participated in the WMT 2016 shared news translation task by building\nneural translation systems for four language pairs, each trained in both\ndirections: English<->Czech, English<->German, English<->Romanian and\nEnglish<->Russian. Our systems are based on an attentional encoder-decoder,\nusing BPE subword segmentation for open-vocabulary translation with a fixed\nvocabulary. We experimented with using automatic back-translations of the\nmonolingual News corpus as additional training data, pervasive dropout, and\ntarget-bidirectional models. All reported methods give substantial\nimprovements, and we see improvements of 4.3--11.2 BLEU over our baseline\nsystems. In the human evaluation, our systems were the (tied) best constrained\nsystem for 7 out of 8 translation directions in which we participated.", "published": "2016-06-09 10:06:28", "link": "http://arxiv.org/abs/1606.02891v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistic Input Features Improve Neural Machine Translation", "abstract": "Neural machine translation has recently achieved impressive results, while\nusing little in the way of external linguistic information. In this paper we\nshow that the strong learning capability of neural MT models does not make\nlinguistic features redundant; they can be easily incorporated to provide\nfurther improvements in performance. We generalize the embedding layer of the\nencoder in the attentional encoder--decoder architecture to support the\ninclusion of arbitrary features, in addition to the baseline word feature. We\nadd morphological features, part-of-speech tags, and syntactic dependency\nlabels as input features to English<->German, and English->Romanian neural\nmachine translation systems. In experiments on WMT16 training and test sets, we\nfind that linguistic input features improve model quality according to three\nmetrics: perplexity, BLEU and CHRF3. An open-source implementation of our\nneural MT system is available, as are sample files and configurations.", "published": "2016-06-09 10:12:36", "link": "http://arxiv.org/abs/1606.02892v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Key-Value Memory Networks for Directly Reading Documents", "abstract": "Directly reading documents and being able to answer questions from them is an\nunsolved challenge. To avoid its inherent difficulty, question answering (QA)\nhas been directed towards using Knowledge Bases (KBs) instead, which has proven\neffective. Unfortunately KBs often suffer from being too restrictive, as the\nschema cannot support certain types of answers, and too sparse, e.g. Wikipedia\ncontains much more information than Freebase. In this work we introduce a new\nmethod, Key-Value Memory Networks, that makes reading documents more viable by\nutilizing different encodings in the addressing and output stages of the memory\nread operation. To compare using KBs, information extraction or Wikipedia\ndocuments directly in a single framework we construct an analysis tool,\nWikiMovies, a QA dataset that contains raw text alongside a preprocessed KB, in\nthe domain of movies. Our method reduces the gap between all three settings. It\nalso achieves state-of-the-art results on the existing WikiQA benchmark.", "published": "2016-06-09 21:33:55", "link": "http://arxiv.org/abs/1606.03126v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PerSum: Novel Systems for Document Summarization in Persian", "abstract": "In this paper we explore the problem of document summarization in Persian\nlanguage from two distinct angles. In our first approach, we modify a popular\nand widely cited Persian document summarization framework to see how it works\non a realistic corpus of news articles. Human evaluation on generated summaries\nshows that graph-based methods perform better than the modified systems. We\ncarry this intuition forward in our second approach, and probe deeper into the\nnature of graph-based systems by designing several summarizers based on\ncentrality measures. Ad hoc evaluation using ROUGE score on these summarizers\nsuggests that there is a small class of centrality measures that perform better\nthan three strong unsupervised baselines.", "published": "2016-06-09 23:32:41", "link": "http://arxiv.org/abs/1606.03143v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task", "abstract": "Enabling a computer to understand a document so that it can answer\ncomprehension questions is a central, yet unsolved goal of NLP. A key factor\nimpeding its solution by machine learned systems is the limited availability of\nhuman-annotated data. Hermann et al. (2015) seek to solve this problem by\ncreating over a million training examples by pairing CNN and Daily Mail news\narticles with their summarized bullet points, and show that a neural network\ncan then be trained to give good performance on this task. In this paper, we\nconduct a thorough examination of this new reading comprehension task. Our\nprimary aim is to understand what depth of language understanding is required\nto do well on this task. We approach this from one side by doing a careful\nhand-analysis of a small subset of the problems and from the other by showing\nthat simple, carefully designed systems can obtain accuracies of 73.6% and\n76.6% on these two datasets, exceeding current state-of-the-art results by\n7-10% and approaching what we believe is the ceiling for performance on this\ntask.", "published": "2016-06-09 08:19:16", "link": "http://arxiv.org/abs/1606.02858v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large scale biomedical texts classification: a kNN and an ESA-based\n  approaches", "abstract": "With the large and increasing volume of textual data, automated methods for\nidentifying significant topics to classify textual documents have received a\ngrowing interest. While many efforts have been made in this direction, it still\nremains a real challenge. Moreover, the issue is even more complex as full\ntexts are not always freely available. Then, using only partial information to\nannotate these documents is promising but remains a very ambitious issue.\nMethodsWe propose two classification methods: a k-nearest neighbours\n(kNN)-based approach and an explicit semantic analysis (ESA)-based approach.\nAlthough the kNN-based approach is widely used in text classification, it needs\nto be improved to perform well in this specific classification problem which\ndeals with partial information. Compared to existing kNN-based methods, our\nmethod uses classical Machine Learning (ML) algorithms for ranking the labels.\nAdditional features are also investigated in order to improve the classifiers'\nperformance. In addition, the combination of several learning algorithms with\nvarious techniques for fixing the number of relevant topics is performed. On\nthe other hand, ESA seems promising for this classification task as it yielded\ninteresting results in related issues, such as semantic relatedness computation\nbetween texts and text classification. Unlike existing works, which use ESA for\nenriching the bag-of-words approach with additional knowledge-based features,\nour ESA-based method builds a standalone classifier. Furthermore, we\ninvestigate if the results of this method could be useful as a complementary\nfeature of our kNN-based approach.ResultsExperimental evaluations performed on\nlarge standard annotated datasets, provided by the BioASQ organizers, show that\nthe kNN-based method with the Random Forest learning algorithm achieves good\nperformances compared with the current state-of-the-art methods, reaching a\ncompetitive f-measure of 0.55% while the ESA-based approach surprisingly\nyielded reserved results.ConclusionsWe have proposed simple classification\nmethods suitable to annotate textual documents using only partial information.\nThey are therefore adequate for large multi-label classification and\nparticularly in the biomedical domain. Thus, our work contributes to the\nextraction of relevant information from unstructured documents in order to\nfacilitate their automated processing. Consequently, it could be used for\nvarious purposes, including document indexing, information retrieval, etc.", "published": "2016-06-09 14:32:50", "link": "http://arxiv.org/abs/1606.02976v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Sequence-to-Sequence Learning as Beam-Search Optimization", "abstract": "Sequence-to-Sequence (seq2seq) modeling has rapidly become an important\ngeneral-purpose NLP tool that has proven effective for many text-generation and\nsequence-labeling tasks. Seq2seq builds on deep neural language modeling and\ninherits its remarkable accuracy in estimating local, next-word distributions.\nIn this work, we introduce a model and beam-search training scheme, based on\nthe work of Daume III and Marcu (2005), that extends seq2seq to learn global\nsequence scores. This structured approach avoids classical biases associated\nwith local training and unifies the training loss with the test-time usage,\nwhile preserving the proven model architecture of seq2seq and its efficient\ntraining approach. We show that our system outperforms a highly-optimized\nattention-based seq2seq system and other baselines on three different sequence\nto sequence tasks: word ordering, parsing, and machine translation.", "published": "2016-06-09 13:29:34", "link": "http://arxiv.org/abs/1606.02960v2", "categories": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
{"title": "MuFuRU: The Multi-Function Recurrent Unit", "abstract": "Recurrent neural networks such as the GRU and LSTM found wide adoption in\nnatural language processing and achieve state-of-the-art results for many\ntasks. These models are characterized by a memory state that can be written to\nand read from by applying gated composition operations to the current input and\nthe previous state. However, they only cover a small subset of potentially\nuseful compositions. We propose Multi-Function Recurrent Units (MuFuRUs) that\nallow for arbitrary differentiable functions as composition operations.\nFurthermore, MuFuRUs allow for an input- and state-dependent choice of these\ncomposition operations that is learned. Our experiments demonstrate that the\nadditional functionality helps in different sequence modeling tasks, including\nthe evaluation of propositional logic formulae, language modeling and sentiment\nanalysis.", "published": "2016-06-09 15:41:17", "link": "http://arxiv.org/abs/1606.03002v1", "categories": ["cs.NE", "cs.AI", "cs.CL"], "primary_category": "cs.NE"}
{"title": "Sentence Similarity Measures for Fine-Grained Estimation of Topical\n  Relevance in Learner Essays", "abstract": "We investigate the task of assessing sentence-level prompt relevance in\nlearner essays. Various systems using word overlap, neural embeddings and\nneural compositional models are evaluated on two datasets of learner writing.\nWe propose a new method for sentence-level similarity calculation, which learns\nto adjust the weights of pre-trained word embeddings for a specific task,\nachieving substantially higher accuracy compared to other relevant baselines.", "published": "2016-06-09 23:42:45", "link": "http://arxiv.org/abs/1606.03144v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Generative Topic Embedding: a Continuous Representation of Documents\n  (Extended Version with Proofs)", "abstract": "Word embedding maps words into a low-dimensional continuous embedding space\nby exploiting the local word collocation patterns in a small context window. On\nthe other hand, topic modeling maps documents onto a low-dimensional topic\nspace, by utilizing the global word collocation patterns in the same document.\nThese two types of patterns are complementary. In this paper, we propose a\ngenerative topic embedding model to combine the two types of patterns. In our\nmodel, topics are represented by embedding vectors, and are shared across\ndocuments. The probability of each word is influenced by both its local context\nand its topic. A variational inference method yields the topic embeddings as\nwell as the topic mixing proportions for each document. Jointly they represent\nthe document in a low-dimensional continuous space. In two document\nclassification tasks, our method performs better than eight existing methods,\nwith fewer features. In addition, we illustrate with an example that our method\ncan generate coherent topics even based on only one document.", "published": "2016-06-09 14:45:39", "link": "http://arxiv.org/abs/1606.02979v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
