{"title": "Trusted Source Alignment in Large Language Models", "abstract": "Large language models (LLMs) are trained on web-scale corpora that inevitably\ninclude contradictory factual information from sources of varying reliability.\nIn this paper, we propose measuring an LLM property called trusted source\nalignment (TSA): the model's propensity to align with content produced by\ntrusted publishers in the face of uncertainty or controversy. We present\nFactCheckQA, a TSA evaluation dataset based on a corpus of fact checking\narticles. We describe a simple protocol for evaluating TSA and offer a detailed\nanalysis of design considerations including response extraction, claim\ncontextualization, and bias in prompt formulation. Applying the protocol to\nPaLM-2, we find that as we scale up the model size, the model performance on\nFactCheckQA improves from near-random to up to 80% balanced accuracy in\naligning with trusted sources.", "published": "2023-11-12 00:25:25", "link": "http://arxiv.org/abs/2311.06697v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comprehending Lexical and Affective Ontologies in the Demographically\n  Diverse Spatial Social Media Discourse", "abstract": "This study aims to comprehend linguistic and socio-demographic features,\nencompassing English language styles, conveyed sentiments, and lexical\ndiversity within spatial online social media review data. To this end, we\nundertake a case study that scrutinizes reviews composed by two distinct and\ndemographically diverse groups. Our analysis entails the extraction and\nexamination of various statistical, grammatical, and sentimental features from\nthese two groups. Subsequently, we leverage these features with machine\nlearning (ML) classifiers to discern their potential in effectively\ndifferentiating between the groups. Our investigation unveils substantial\ndisparities in certain linguistic attributes between the two groups. When\nintegrated into ML classifiers, these attributes exhibit a marked efficacy in\ndistinguishing the groups, yielding a macro F1 score of approximately 0.85.\nFurthermore, we conduct a comparative evaluation of these linguistic features\nwith word n-gram-based lexical features in discerning demographically diverse\nreview data. As expected, the n-gram lexical features, coupled with fine-tuned\ntransformer-based models, show superior performance, attaining accuracies\nsurpassing 95\\% and macro F1 scores exceeding 0.96. Our meticulous analysis and\ncomprehensive evaluations substantiate the efficacy of linguistic and\nsentimental features in effectively discerning demographically diverse review\ndata. The findings of this study provide valuable guidelines for future\nresearch endeavors concerning the analysis of demographic patterns in textual\ncontent across various social media platforms.", "published": "2023-11-12 04:23:33", "link": "http://arxiv.org/abs/2311.06729v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are LLMs Rigorous Logical Reasoner? Empowering Natural Language Proof\n  Generation with Contrastive Stepwise Decoding", "abstract": "Logical reasoning remains a pivotal component within the realm of artificial\nintelligence. The recent evolution of large language models (LLMs) has marked\nsignificant progress in this domain. The adoption of strategies like\nchain-of-thought (CoT) has enhanced the performance of LLMs across diverse\nreasoning tasks. Nonetheless, logical reasoning that involves proof planning,\nspecifically those that necessitate the validation of explanation accuracy,\ncontinues to present stumbling blocks. In this study, we first evaluate the\nefficacy of LLMs with advanced CoT strategies concerning such tasks. Our\nanalysis reveals that LLMs still struggle to navigate complex reasoning chains,\nwhich demand the meticulous linkage of premises to derive a cogent conclusion.\nTo address this issue, we finetune a smaller-scale language model, equipping it\nto decompose proof objectives into more manageable subgoals. We also introduce\ncontrastive decoding to stepwise proof generation, making use of negative\nreasoning paths to strengthen the model's capacity for logical deduction.\nExperiments on EntailmentBank underscore the success of our method in\naugmenting the proof planning abilities of language models.", "published": "2023-11-12 05:12:49", "link": "http://arxiv.org/abs/2311.06736v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BeautifulPrompt: Towards Automatic Prompt Engineering for Text-to-Image\n  Synthesis", "abstract": "Recently, diffusion-based deep generative models (e.g., Stable Diffusion)\nhave shown impressive results in text-to-image synthesis. However, current\ntext-to-image models often require multiple passes of prompt engineering by\nhumans in order to produce satisfactory results for real-world applications. We\npropose BeautifulPrompt, a deep generative model to produce high-quality\nprompts from very simple raw descriptions, which enables diffusion-based models\nto generate more beautiful images. In our work, we first fine-tuned the\nBeautifulPrompt model over low-quality and high-quality collecting prompt\npairs. Then, to ensure that our generated prompts can generate more beautiful\nimages, we further propose a Reinforcement Learning with Visual AI Feedback\ntechnique to fine-tune our model to maximize the reward values of the generated\nprompts, where the reward values are calculated based on the PickScore and the\nAesthetic Scores. Our results demonstrate that learning from visual AI feedback\npromises the potential to improve the quality of generated prompts and images\nsignificantly. We further showcase the integration of BeautifulPrompt to a\ncloud-native AI platform to provide better text-to-image generation service in\nthe cloud.", "published": "2023-11-12 06:39:00", "link": "http://arxiv.org/abs/2311.06752v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Complex to Simple: Unraveling the Cognitive Tree for Reasoning with\n  Small Language Models", "abstract": "Reasoning is a distinctive human capacity, enabling us to address complex\nproblems by breaking them down into a series of manageable cognitive steps.\nYet, complex logical reasoning is still cumbersome for language models. Based\non the dual process theory in cognitive science, we are the first to unravel\nthe cognitive reasoning abilities of language models. Our framework employs an\niterative methodology to construct a Cognitive Tree (CogTree). The root node of\nthis tree represents the initial query, while the leaf nodes consist of\nstraightforward questions that can be answered directly. This construction\ninvolves two main components: the implicit extraction module (referred to as\nthe intuitive system) and the explicit reasoning module (referred to as the\nreflective system). The intuitive system rapidly generates multiple responses\nby utilizing in-context examples, while the reflective system scores these\nresponses using comparative learning. The scores guide the intuitive system in\nits subsequent generation step. Our experimental results on two popular and\nchallenging reasoning tasks indicate that it is possible to achieve a\nperformance level comparable to that of GPT-3.5 (with 175B parameters), using a\nsignificantly smaller language model that contains fewer parameters (<=7B) than\n5% of GPT-3.5.", "published": "2023-11-12 06:56:21", "link": "http://arxiv.org/abs/2311.06754v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sharing, Teaching and Aligning: Knowledgeable Transfer Learning for\n  Cross-Lingual Machine Reading Comprehension", "abstract": "In cross-lingual language understanding, machine translation is often\nutilized to enhance the transferability of models across languages, either by\ntranslating the training data from the source language to the target, or from\nthe target to the source to aid inference. However, in cross-lingual machine\nreading comprehension (MRC), it is difficult to perform a deep level of\nassistance to enhance cross-lingual transfer because of the variation of answer\nspan positions in different languages. In this paper, we propose X-STA, a new\napproach for cross-lingual MRC. Specifically, we leverage an attentive teacher\nto subtly transfer the answer spans of the source language to the answer output\nspace of the target. A Gradient-Disentangled Knowledge Sharing technique is\nproposed as an improved cross-attention block. In addition, we force the model\nto learn semantic alignments from multiple granularities and calibrate the\nmodel outputs with teacher guidance to enhance cross-lingual transferability.\nExperiments on three multi-lingual MRC datasets show the effectiveness of our\nmethod, outperforming state-of-the-art approaches.", "published": "2023-11-12 07:20:37", "link": "http://arxiv.org/abs/2311.06758v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Knowledge-Enhanced Contextual Language Representations for\n  Domain Natural Language Understanding", "abstract": "Knowledge-Enhanced Pre-trained Language Models (KEPLMs) improve the\nperformance of various downstream NLP tasks by injecting knowledge facts from\nlarge-scale Knowledge Graphs (KGs). However, existing methods for pre-training\nKEPLMs with relational triples are difficult to be adapted to close domains due\nto the lack of sufficient domain graph semantics. In this paper, we propose a\nKnowledge-enhanced lANGuAge Representation learning framework for various\nclOsed dOmains (KANGAROO) via capturing the implicit graph structure among the\nentities. Specifically, since the entity coverage rates of closed-domain KGs\ncan be relatively low and may exhibit the global sparsity phenomenon for\nknowledge injection, we consider not only the shallow relational\nrepresentations of triples but also the hyperbolic embeddings of deep\nhierarchical entity-class structures for effective knowledge fusion.Moreover,\nas two closed-domain entities under the same entity-class often have locally\ndense neighbor subgraphs counted by max point biconnected component, we further\npropose a data augmentation strategy based on contrastive learning over\nsubgraphs to construct hard negative samples of higher quality. It makes the\nunderlying KELPMs better distinguish the semantics of these neighboring\nentities to further complement the global semantic sparsity. In the\nexperiments, we evaluate KANGAROO over various knowledge-aware and general NLP\ntasks in both full and few-shot learning settings, outperforming various KEPLM\ntraining paradigms performance in closed-domains significantly.", "published": "2023-11-12 07:37:24", "link": "http://arxiv.org/abs/2311.06761v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tunable Soft Prompts are Messengers in Federated Learning", "abstract": "Federated learning (FL) enables multiple participants to collaboratively\ntrain machine learning models using decentralized data sources, alleviating\nprivacy concerns that arise from directly sharing local data. However, the lack\nof model privacy protection in FL becomes an unneglectable challenge,\nespecially when people want to federally finetune models based on a proprietary\nlarge language model. In this study, we propose a novel FL training approach\nthat accomplishes information exchange among participants via tunable soft\nprompts. These soft prompts, updated and transmitted between the server and\nclients, assume the role of the global model parameters and serve as messengers\nto deliver useful knowledge from the local data and global model. As the global\nmodel itself is not required to be shared and the local training is conducted\nbased on an auxiliary model with fewer parameters than the global model, the\nproposed approach provides protection for the global model while reducing\ncommunication and computation costs in FL. Extensive experiments show the\neffectiveness of the proposed approach compared to several baselines. We have\nreleased the source code at\n\\url{https://github.com/alibaba/FederatedScope/tree/fedsp/federatedscope/nlp/fedsp}.", "published": "2023-11-12 11:01:10", "link": "http://arxiv.org/abs/2311.06805v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Robustness of Question Rewriting Systems to Questions of Varying\n  Hardness", "abstract": "In conversational question answering (CQA), the task of question\nrewriting~(QR) in context aims to rewrite a context-dependent question into an\nequivalent self-contained question that gives the same answer. In this paper,\nwe are interested in the robustness of a QR system to questions varying in\nrewriting hardness or difficulty. Since there is a lack of questions classified\nbased on their rewriting hardness, we first propose a heuristic method to\nautomatically classify questions into subsets of varying hardness, by measuring\nthe discrepancy between a question and its rewrite. To find out what makes\nquestions hard or easy for rewriting, we then conduct a human evaluation to\nannotate the rewriting hardness of questions. Finally, to enhance the\nrobustness of QR systems to questions of varying hardness, we propose a novel\nlearning framework for QR that first trains a QR model independently on each\nsubset of questions of a certain level of hardness, then combines these QR\nmodels as one joint model for inference. Experimental results on two datasets\nshow that our framework improves the overall performance compared to the\nbaselines.", "published": "2023-11-12 11:09:30", "link": "http://arxiv.org/abs/2311.06807v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of GPT-4 for chest X-ray impression generation: A reader\n  study on performance and perception", "abstract": "The remarkable generative capabilities of multimodal foundation models are\ncurrently being explored for a variety of applications. Generating radiological\nimpressions is a challenging task that could significantly reduce the workload\nof radiologists. In our study we explored and analyzed the generative abilities\nof GPT-4 for Chest X-ray impression generation. To generate and evaluate\nimpressions of chest X-rays based on different input modalities (image, text,\ntext and image), a blinded radiological report was written for 25-cases of the\npublicly available NIH-dataset. GPT-4 was given image, finding section or both\nsequentially to generate an input dependent impression. In a blind randomized\nreading, 4-radiologists rated the impressions and were asked to classify the\nimpression origin (Human, AI), providing justification for their decision.\nLastly text model evaluation metrics and their correlation with the\nradiological score (summation of the 4 dimensions) was assessed. According to\nthe radiological score, the human-written impression was rated highest,\nalthough not significantly different to text-based impressions. The automated\nevaluation metrics showed moderate to substantial correlations to the\nradiological score for the image impressions, however individual scores were\nhighly divergent among inputs, indicating insufficient representation of\nradiological quality. Detection of AI-generated impressions varied by input and\nwas 61% for text-based impressions. Impressions classified as AI-generated had\nsignificantly worse radiological scores even when written by a radiologist,\nindicating potential bias. Our study revealed significant discrepancies between\na radiological assessment and common automatic evaluation metrics depending on\nthe model input. The detection of AI-generated findings is subject to bias that\nhighly rated impressions are perceived as human-written.", "published": "2023-11-12 11:40:57", "link": "http://arxiv.org/abs/2311.06815v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GIELLM: Japanese General Information Extraction Large Language Model\n  Utilizing Mutual Reinforcement Effect", "abstract": "Information Extraction (IE) stands as a cornerstone in natural language\nprocessing, traditionally segmented into distinct sub-tasks. The advent of\nLarge Language Models (LLMs) heralds a paradigm shift, suggesting the\nfeasibility of a singular model addressing multiple IE subtasks. In this vein,\nwe introduce the General Information Extraction Large Language Model (GIELLM),\nwhich integrates text Classification, Sentiment Analysis, Named Entity\nRecognition, Relation Extraction, and Event Extraction using a uniform\ninput-output schema. This innovation marks the first instance of a model\nsimultaneously handling such a diverse array of IE subtasks. Notably, the\nGIELLM leverages the Mutual Reinforcement Effect (MRE), enhancing performance\nin integrated tasks compared to their isolated counterparts. Our experiments\ndemonstrate State-of-the-Art (SOTA) results in five out of six Japanese mixed\ndatasets, significantly surpassing GPT-3.5-Turbo. Further, an independent\nevaluation using the novel Text Classification Relation and Event\nExtraction(TCREE) dataset corroborates the synergistic advantages of MRE in\ntext and word classification. This breakthrough paves the way for most IE\nsubtasks to be subsumed under a singular LLM framework. Specialized fine-tune\ntask-specific models are no longer needed.", "published": "2023-11-12 13:30:38", "link": "http://arxiv.org/abs/2311.06838v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Textual Normalization for Hate Speech Detection", "abstract": "Social media data is a valuable resource for research, yet it contains a wide\nrange of non-standard words (NSW). These irregularities hinder the effective\noperation of NLP tools. Current state-of-the-art methods for the Vietnamese\nlanguage address this issue as a problem of lexical normalization, involving\nthe creation of manual rules or the implementation of multi-staged deep\nlearning frameworks, which necessitate extensive efforts to craft intricate\nrules. In contrast, our approach is straightforward, employing solely a\nsequence-to-sequence (Seq2Seq) model. In this research, we provide a dataset\nfor textual normalization, comprising 2,181 human-annotated comments with an\ninter-annotator agreement of 0.9014. By leveraging the Seq2Seq model for\ntextual normalization, our results reveal that the accuracy achieved falls\nslightly short of 70%. Nevertheless, textual normalization enhances the\naccuracy of the Hate Speech Detection (HSD) task by approximately 2%,\ndemonstrating its potential to improve the performance of complex NLP tasks.\nOur dataset is accessible for research purposes.", "published": "2023-11-12 14:01:38", "link": "http://arxiv.org/abs/2311.06851v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models are In-context Teachers for Knowledge Reasoning", "abstract": "In this work, we study in-context teaching (ICT), where a teacher provides\nin-context example rationales to teach a student to reason over unseen cases.\nHuman teachers are usually required to craft in-context demonstrations, which\nare costly and have high variance. We ask whether a large language model (LLM)\ncan serve as a more effective in-context teacher for itself or other LLMs,\ncompared to humans. Inspired by the Encoding Specificity Hypothesis from human\nepisodic memory, we hypothesize that in-context exemplars crafted by the\nteacher should match the training data of the student. This hypothesis\nmotivates us to propose Self-Explain where an LLM's self-elicited explanations\nare used as in-context demonstrations for prompting it as they are generalized\nfrom the model's training examples. Self-Explain is shown to significantly\noutperform using human-crafted exemplars and other baselines.\n  Furthermore, we reveal that for ICT, rationales from different teacher LLMs\nor human experts that more resemble the student LLM's self-explanations are\nbetter in-context demonstrations. This supports our encoding specificity\nhypothesis. We then propose Teach-Back that aligns a teacher LLM with the\nstudent to enhance the ICT performance. For example, Teach-Back enables a 7B\nmodel to teach the much larger GPT-3.5 in context, surpassing human teachers by\naround 5% in test accuracy on medical question answering.", "published": "2023-11-12 23:14:43", "link": "http://arxiv.org/abs/2311.06985v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simple and Effective Input Reformulations for Translation", "abstract": "Foundation language models learn from their finetuning input context in\ndifferent ways. In this paper, we reformulate inputs during finetuning for\nchallenging translation tasks, leveraging model strengths from pretraining in\nnovel ways to improve downstream performance. These reformulations are simple\ndata level modifications, require no additional collection of training data or\nmodification of data at inference time. They can be applied either on single\nlanguage pair translation tasks or massively multilingual translation tasks.\nExperiments with these techniques demonstrate significant performance\nimprovements up to $\\textbf{3.5 chrF++ on the Flores200 translation\nbenchmark}$. We hope our research accessibly improves finetuning data\nefficiency, enabling more effective training to scalably improve\nstate-of-the-art performance. Our code is released\n$\\href{https://github.com/bri25yu/LanguageModelExperimentation}{here}.$", "published": "2023-11-12 00:23:37", "link": "http://arxiv.org/abs/2311.06696v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What factors influence the popularity of user-generated text in the\n  creative domain? A case study of book reviews", "abstract": "This study investigates a range of psychological, lexical, semantic, and\nreadability features of book reviews to elucidate the factors underlying their\nperceived popularity. To this end, we conduct statistical analyses of various\nfeatures, including the types and frequency of opinion and emotion-conveying\nterms, connectives, character mentions, word uniqueness, commonness, and\nsentence structure, among others. Additionally, we utilize two readability\ntests to explore whether reading ease is positively associated with review\npopularity. Finally, we employ traditional machine learning classifiers and\ntransformer-based fine-tuned language models with n-gram features to\nautomatically determine review popularity. Our findings indicate that, with the\nexception of a few features (e.g., review length, emotions, and word\nuniqueness), most attributes do not exhibit significant differences between\npopular and non-popular review groups. Furthermore, the poor performance of\nmachine learning classifiers using the word n-gram feature highlights the\nchallenges associated with determining popularity in creative domains. Overall,\nour study provides insights into the factors underlying review popularity and\nhighlights the need for further research in this area, particularly in the\ncreative realm.", "published": "2023-11-12 02:54:11", "link": "http://arxiv.org/abs/2311.06714v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small\n  Scorer", "abstract": "Large language models (LLMs) such as T0, FLAN, and OPT-IML, excel in\nmulti-tasking under a unified instruction-following paradigm, where they also\nexhibit remarkable generalization abilities to unseen tasks. Despite their\nimpressive performance, these LLMs, with sizes ranging from several billion to\nhundreds of billions of parameters, demand substantial computational resources,\nmaking their training and inference expensive and inefficient. Furthermore,\nadapting these models to downstream applications, particularly complex tasks,\nis often unfeasible due to the extensive hardware requirements for finetuning,\neven when utilizing parameter-efficient approaches such as prompt tuning.\nAdditionally, the most powerful multi-task LLMs, such as OPT-IML-175B and\nFLAN-PaLM-540B, are not publicly accessible, severely limiting their\ncustomization potential. To address these challenges, we introduce a pretrained\nsmall scorer, Cappy, designed to enhance the performance and efficiency of\nmulti-task LLMs. With merely 360 million parameters, Cappy functions either\nindependently on classification tasks or serve as an auxiliary component for\nLLMs, boosting their performance. Moreover, Cappy enables efficiently\nintegrating downstream supervision without requiring LLM finetuning nor the\naccess to their parameters. Our experiments demonstrate that, when working\nindependently on 11 language understanding tasks from PromptSource, Cappy\noutperforms LLMs that are several orders of magnitude larger. Besides, on 45\ncomplex tasks from BIG-Bench, Cappy boosts the performance of the advanced\nmulti-task LLM, FLAN-T5, by a large margin. Furthermore, Cappy is flexible to\ncooperate with other LLM adaptations, including finetuning and in-context\nlearning, offering additional performance enhancement.", "published": "2023-11-12 03:25:34", "link": "http://arxiv.org/abs/2311.06720v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Controllable Topic-Focused Abstractive Summarization", "abstract": "Controlled abstractive summarization focuses on producing condensed versions\nof a source article to cover specific aspects by shifting the distribution of\ngenerated text towards a desired style, e.g., a set of topics. Subsequently,\nthe resulting summaries may be tailored to user-defined requirements. This\npaper presents a new Transformer-based architecture capable of producing\ntopic-focused summaries. The architecture modifies the cross-attention\nmechanism of the Transformer to bring topic-focus control to the generation\nprocess while not adding any further parameters to the model. We show that our\nmodel sets a new state of the art on the NEWTS dataset in terms of\ntopic-focused abstractive summarization as well as a topic-prevalence score.\nMoreover, we show via extensive experiments that our proposed topical\ncross-attention mechanism can be plugged into various Transformer models, such\nas BART and T5, improving their performance on the CNN/Dailymail and XSum\nbenchmark datasets for abstractive summarization. This is achieved via\nfine-tuning, without requiring training from scratch. Finally, we show through\nhuman evaluation that our model generates more faithful summaries outperforming\nthe state-of-the-art Frost model.", "published": "2023-11-12 03:51:38", "link": "http://arxiv.org/abs/2311.06724v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Detecting and Correcting Hate Speech in Multimodal Memes with Large\n  Visual Language Model", "abstract": "Recently, large language models (LLMs) have taken the spotlight in natural\nlanguage processing. Further, integrating LLMs with vision enables the users to\nexplore more emergent abilities in multimodality. Visual language models\n(VLMs), such as LLaVA, Flamingo, or GPT-4, have demonstrated impressive\nperformance on various visio-linguistic tasks. Consequently, there are enormous\napplications of large models that could be potentially used on social media\nplatforms. Despite that, there is a lack of related work on detecting or\ncorrecting hateful memes with VLMs. In this work, we study the ability of VLMs\non hateful meme detection and hateful meme correction tasks with zero-shot\nprompting. From our empirical experiments, we show the effectiveness of the\npretrained LLaVA model and discuss its strengths and weaknesses in these tasks.", "published": "2023-11-12 05:20:20", "link": "http://arxiv.org/abs/2311.06737v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AudioChatLlama: Towards General-Purpose Speech Abilities for LLMs", "abstract": "In this work, we extend the instruction-tuned Llama-2 model with end-to-end\ngeneral-purpose speech processing and reasoning abilities while maintaining the\nwide range of original LLM capabilities, without using any carefully curated\npaired data. The resulting end-to-end model, named AudioChatLlama, can utilize\naudio prompts as a replacement for text and sustain a conversation. Such a\nmodel also has extended cross-modal capabilities such as being able to perform\nspoken question answering (QA), speech translation, and audio summarization\namongst many other closed and open-domain tasks. This is unlike prior\napproaches in speech, in which LLMs are extended to handle audio for a limited\nnumber of pre-designated tasks. On both synthesized and recorded speech QA test\nsets, evaluations show that our end-to-end approach is on par with or\noutperforms cascaded systems (speech recognizer + LLM) in terms of modeling the\nresponse to a prompt. Furthermore, unlike cascades, our approach can\ninterchange text and audio modalities and intrinsically utilize prior context\nin a conversation to provide better results.", "published": "2023-11-12 06:56:14", "link": "http://arxiv.org/abs/2311.06753v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Globally Optimized Language Structure via Adversarial Training", "abstract": "Recent work has explored integrating autoregressive language models with\nenergy-based models (EBMs) to enhance text generation capabilities. However,\nlearning effective EBMs for text is challenged by the discrete nature of\nlanguage. This work proposes an adversarial training strategy to address\nlimitations in prior efforts. Specifically, an iterative adversarial attack\nalgorithm is presented to generate negative samples for training the EBM by\nperturbing text from the autoregressive model. This aims to enable the EBM to\nsuppress spurious modes outside the support of the data distribution.\nExperiments on an arithmetic sequence generation task demonstrate that the\nproposed adversarial training approach can substantially enhance the quality of\ngenerated sequences compared to prior methods. The results highlight the\npromise of adversarial techniques to improve discrete EBM training. Key\ncontributions include: (1) an adversarial attack strategy tailored to text to\ngenerate negative samples, circumventing MCMC limitations; (2) an adversarial\ntraining algorithm for EBMs leveraging these attacks; (3) empirical validation\nof performance improvements on a sequence generation task.", "published": "2023-11-12 08:21:43", "link": "http://arxiv.org/abs/2311.06771v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cricket Player Profiling: Unraveling Strengths and Weaknesses Using Text\n  Commentary Data", "abstract": "Devising player-specific strategies in cricket necessitates a meticulous\nunderstanding of each player's unique strengths and weaknesses. Nevertheless,\nthe absence of a definitive computational approach to extract such insights\nfrom cricket players poses a significant challenge. This paper seeks to address\nthis gap by establishing computational models designed to extract the rules\ngoverning player strengths and weaknesses, thereby facilitating the development\nof tailored strategies for individual players. The complexity of this endeavor\nlies in several key areas: the selection of a suitable dataset, the precise\ndefinition of strength and weakness rules, the identification of an appropriate\nlearning algorithm, and the validation of the derived rules. To tackle these\nchallenges, we propose the utilization of unstructured data, specifically\ncricket text commentary, as a valuable resource for constructing comprehensive\nstrength and weakness rules for cricket players. We also introduce\ncomputationally feasible definitions for the construction of these rules, and\npresent a dimensionality reduction technique for the rule-building process. In\norder to showcase the practicality of this approach, we conduct an in-depth\nanalysis of cricket player strengths and weaknesses using a vast corpus of more\nthan one million text commentaries. Furthermore, we validate the constructed\nrules through two distinct methodologies: intrinsic and extrinsic. The outcomes\nof this research are made openly accessible, including the collected data,\nsource code, and results for over 250 cricket players, which can be accessed at\nhttps://bit.ly/2PKuzx8.", "published": "2023-11-12 11:51:05", "link": "http://arxiv.org/abs/2311.06818v1", "categories": ["cs.LG", "cs.CL", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Can Large Language Models Augment a Biomedical Ontology with missing\n  Concepts and Relations?", "abstract": "Ontologies play a crucial role in organizing and representing knowledge.\nHowever, even current ontologies do not encompass all relevant concepts and\nrelationships. Here, we explore the potential of large language models (LLM) to\nexpand an existing ontology in a semi-automated fashion. We demonstrate our\napproach on the biomedical ontology SNOMED-CT utilizing semantic relation types\nfrom the widely used UMLS semantic network. We propose a method that uses\nconversational interactions with an LLM to analyze clinical practice guidelines\n(CPGs) and detect the relationships among the new medical concepts that are not\npresent in SNOMED-CT. Our initial experimentation with the conversational\nprompts yielded promising preliminary results given a manually generated gold\nstandard, directing our future potential improvements.", "published": "2023-11-12 14:20:55", "link": "http://arxiv.org/abs/2311.06858v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Retrieval and Generative Approaches for a Pregnancy Chatbot in Nepali\n  with Stemmed and Non-Stemmed Data : A Comparative Study", "abstract": "The field of Natural Language Processing which involves the use of artificial\nintelligence to support human languages has seen tremendous growth due to its\nhigh-quality features. Its applications such as language translation, chatbots,\nvirtual assistants, search autocomplete, and autocorrect are widely used in\nvarious domains including healthcare, advertising, customer service, and target\nadvertising. To provide pregnancy-related information a health domain chatbot\nhas been proposed and this work explores two different NLP-based approaches for\ndeveloping the chatbot. The first approach is a multiclass classification-based\nretrieval approach using BERTbased multilingual BERT and multilingual\nDistilBERT while the other approach employs a transformer-based generative\nchatbot for pregnancy-related information. The performance of both stemmed and\nnon-stemmed datasets in Nepali language has been analyzed for each approach.\nThe experimented results indicate that BERT-based pre-trained models perform\nwell on non-stemmed data whereas scratch transformer models have better\nperformance on stemmed data. Among the models tested the DistilBERT model\nachieved the highest training and validation accuracy and testing accuracy of\n0.9165 on the retrieval-based model architecture implementation on the\nnon-stemmed dataset. Similarly, in the generative approach architecture\nimplementation with transformer 1 gram BLEU and 2 gram BLEU scores of 0.3570\nand 0.1413 respectively were achieved.", "published": "2023-11-12 17:16:46", "link": "http://arxiv.org/abs/2311.06898v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Flames: Benchmarking Value Alignment of LLMs in Chinese", "abstract": "The widespread adoption of large language models (LLMs) across various\nregions underscores the urgent need to evaluate their alignment with human\nvalues. Current benchmarks, however, fall short of effectively uncovering\nsafety vulnerabilities in LLMs. Despite numerous models achieving high scores\nand 'topping the chart' in these evaluations, there is still a significant gap\nin LLMs' deeper alignment with human values and achieving genuine harmlessness.\nTo this end, this paper proposes a value alignment benchmark named Flames,\nwhich encompasses both common harmlessness principles and a unique morality\ndimension that integrates specific Chinese values such as harmony. Accordingly,\nwe carefully design adversarial prompts that incorporate complex scenarios and\njailbreaking methods, mostly with implicit malice. By prompting 17 mainstream\nLLMs, we obtain model responses and rigorously annotate them for detailed\nevaluation. Our findings indicate that all the evaluated LLMs demonstrate\nrelatively poor performance on Flames, particularly in the safety and fairness\ndimensions. We also develop a lightweight specified scorer capable of scoring\nLLMs across multiple dimensions to efficiently evaluate new models on the\nbenchmark. The complexity of Flames has far exceeded existing benchmarks,\nsetting a new challenge for contemporary LLMs and highlighting the need for\nfurther alignment of LLMs. Our benchmark is publicly available at\nhttps://github.com/AIFlames/Flames.", "published": "2023-11-12 17:18:21", "link": "http://arxiv.org/abs/2311.06899v6", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CLAMP: A Contrastive Language And Molecule Pre-training Network", "abstract": "This paper highlights a shift in how to approach material generation. Instead\nof material-to-material, we propose a language-to-material generation\narchitecture that utilizes millions of untapped data points. Using a web\nscraper to collect crystal text pairs from open-source research papers, a\ncontrastive model can be trained using a convolutional graph neural network\nencoder and a language encoder. This would allow unsupervised zero-shot\nclassification which can be trained by taking advantage of linguistic\nstructure. Without any specific training data, an ~82\\% accuracy was achieved\nand ~75\\% accuracy for photocatalyst prediction with an extremely small\ndataset. This novel network could ideally be cross-applied to any reaction that\ncan be described via text, opening completely new methods to think about 3D\nchemical framework generation. In the full experiment diffusion models would\nlikely be incorporated to fully exploit the latent space.", "published": "2023-11-12 07:45:35", "link": "http://arxiv.org/abs/2311.07617v1", "categories": ["cs.CL", "cs.IR", "8.2.D.2.5"], "primary_category": "cs.CL"}
{"title": "Which One? Leveraging Context Between Objects and Multiple Views for\n  Language Grounding", "abstract": "When connecting objects and their language referents in an embodied 3D\nenvironment, it is important to note that: (1) an object can be better\ncharacterized by leveraging comparative information between itself and other\nobjects, and (2) an object's appearance can vary with camera position. As such,\nwe present the Multi-view Approach to Grounding in Context (MAGiC), which\nselects an object referent based on language that distinguishes between two\nsimilar objects. By pragmatically reasoning over both objects and across\nmultiple views of those objects, MAGiC improves over the state-of-the-art model\non the SNARE object reference task with a relative error reduction of 12.9\\%\n(representing an absolute improvement of 2.7\\%). Ablation studies show that\nreasoning jointly over object referent candidates and multiple views of each\nobject both contribute to improved accuracy. Code:\nhttps://github.com/rcorona/magic_snare/", "published": "2023-11-12 00:21:58", "link": "http://arxiv.org/abs/2311.06694v3", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.RO"], "primary_category": "cs.CL"}
{"title": "DialMAT: Dialogue-Enabled Transformer with Moment-Based Adversarial\n  Training", "abstract": "This paper focuses on the DialFRED task, which is the task of embodied\ninstruction following in a setting where an agent can actively ask questions\nabout the task. To address this task, we propose DialMAT. DialMAT introduces\nMoment-based Adversarial Training, which incorporates adversarial perturbations\ninto the latent space of language, image, and action. Additionally, it\nintroduces a crossmodal parallel feature extraction mechanism that applies\nfoundation models to both language and image. We evaluated our model using a\ndataset constructed from the DialFRED dataset and demonstrated superior\nperformance compared to the baseline method in terms of success rate and path\nweighted success rate. The model secured the top position in the DialFRED\nChallenge, which took place at the CVPR 2023 Embodied AI workshop.", "published": "2023-11-12 14:12:19", "link": "http://arxiv.org/abs/2311.06855v1", "categories": ["cs.CV", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Large Language Models' Understanding of Math: Source Criticism and\n  Extrapolation", "abstract": "It has been suggested that large language models such as GPT-4 have acquired\nsome form of understanding beyond the correlations among the words in text\nincluding some understanding of mathematics as well. Here, we perform a\ncritical inquiry into this claim by evaluating the mathematical understanding\nof the GPT-4 model. Considering that GPT-4's training set is a secret, it is\nnot straightforward to evaluate whether the model's correct answers are based\non a mathematical understanding or based on replication of proofs that the\nmodel has seen before. We specifically craft mathematical questions which their\nformal proofs are not readily available on the web, proofs that are more likely\nnot seen by the GPT-4. We see that GPT-4 is unable to solve those problems\ndespite their simplicity. It is hard to find scientific evidence suggesting\nthat GPT-4 has acquired an understanding of even basic mathematical concepts. A\nstraightforward way to find failure modes of GPT-4 in theorem proving is to\ncraft questions where their formal proofs are not available on the web. Our\nfinding suggests that GPT-4's ability is to reproduce, rephrase, and polish the\nmathematical proofs that it has seen before, and not in grasping mathematical\nconcepts. We also see that GPT-4's ability to prove mathematical theorems is\ncontinuously expanding over time despite the claim that it is a fixed model. We\nsuggest that the task of proving mathematical theorems in formal language is\ncomparable to the methods used in search engines such as Google while\npredicting the next word in a sentence may be a misguided approach, a recipe\nthat often leads to excessive extrapolation and eventual failures. Prompting\nthe GPT-4 over and over may benefit the GPT-4 and the OpenAI, but we question\nwhether it is valuable for machine learning or for theorem proving.", "published": "2023-11-12 07:52:32", "link": "http://arxiv.org/abs/2311.07618v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.HO"], "primary_category": "cs.LG"}
{"title": "Transfer Learning to Detect COVID-19 Coughs with Incremental Addition of\n  Patient Coughs to Healthy People's Cough Detection Models", "abstract": "Millions of people have died worldwide from COVID-19. In addition to its high\ndeath toll, COVID-19 has led to unbearable suffering for individuals and a huge\nglobal burden to the healthcare sector. Therefore, researchers have been trying\nto develop tools to detect symptoms of this human-transmissible disease\nremotely to control its rapid spread. Coughing is one of the common symptoms\nthat researchers have been trying to detect objectively from smartphone\nmicrophone-sensing. While most of the approaches to detect and track cough\nsymptoms rely on machine learning models developed from a large amount of\npatient data, this is not possible at the early stage of an outbreak. In this\nwork, we present an incremental transfer learning approach that leverages the\nrelationship between healthy peoples' coughs and COVID-19 patients' coughs to\ndetect COVID-19 coughs with reasonable accuracy using a pre-trained healthy\ncough detection model and a relatively small set of patient coughs, reducing\nthe need for large patient dataset to train the model. This type of model can\nbe a game changer in detecting the onset of a novel respiratory virus.", "published": "2023-11-12 02:01:24", "link": "http://arxiv.org/abs/2311.06707v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
