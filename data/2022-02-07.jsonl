{"title": "User Satisfaction Estimation with Sequential Dialogue Act Modeling in\n  Goal-oriented Conversational Systems", "abstract": "User Satisfaction Estimation (USE) is an important yet challenging task in\ngoal-oriented conversational systems. Whether the user is satisfied with the\nsystem largely depends on the fulfillment of the user's needs, which can be\nimplicitly reflected by users' dialogue acts. However, existing studies often\nneglect the sequential transitions of dialogue act or rely heavily on annotated\ndialogue act labels when utilizing dialogue acts to facilitate USE. In this\npaper, we propose a novel framework, namely USDA, to incorporate the sequential\ndynamics of dialogue acts for predicting user satisfaction, by jointly learning\nUser Satisfaction Estimation and Dialogue Act Recognition tasks. In specific,\nwe first employ a Hierarchical Transformer to encode the whole dialogue\ncontext, with two task-adaptive pre-training strategies to be a second-phase\nin-domain pre-training for enhancing the dialogue modeling ability. In terms of\nthe availability of dialogue act labels, we further develop two variants of\nUSDA to capture the dialogue act information in either supervised or\nunsupervised manners. Finally, USDA leverages the sequential transitions of\nboth content and act features in the dialogue to predict the user satisfaction.\nExperimental results on four benchmark goal-oriented dialogue datasets across\ndifferent applications show that the proposed method substantially and\nconsistently outperforms existing methods on USE, and validate the important\nrole of dialogue act sequences in USE.", "published": "2022-02-07 02:50:07", "link": "http://arxiv.org/abs/2202.02912v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparison and Combination of Sentence Embeddings Derived from Different\n  Supervision Signals", "abstract": "There have been many successful applications of sentence embedding methods.\nHowever, it has not been well understood what properties are captured in the\nresulting sentence embeddings depending on the supervision signals. In this\npaper, we focus on two types of sentence embedding methods with similar\narchitectures and tasks: one fine-tunes pre-trained language models on the\nnatural language inference task, and the other fine-tunes pre-trained language\nmodels on word prediction task from its definition sentence, and investigate\ntheir properties. Specifically, we compare their performances on semantic\ntextual similarity (STS) tasks using STS datasets partitioned from two\nperspectives: 1) sentence source and 2) superficial similarity of the sentence\npairs, and compare their performances on the downstream and probing tasks.\nFurthermore, we attempt to combine the two methods and demonstrate that\ncombining the two methods yields substantially better performance than the\nrespective methods on unsupervised STS tasks and downstream tasks.", "published": "2022-02-07 08:15:48", "link": "http://arxiv.org/abs/2202.02990v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Learning Through Open-Domain Dialog", "abstract": "The development of artificial agents able to learn through dialog without\ndomain restrictions has the potential to allow machines to learn how to perform\ntasks in a similar manner to humans and change how we relate to them. However,\nresearch in this area is practically nonexistent. In this paper, we identify\nthe modifications required for a dialog system to be able to learn from the\ndialog and propose generic approaches that can be used to implement those\nmodifications. More specifically, we discuss how knowledge can be extracted\nfrom the dialog, used to update the agent's semantic network, and grounded in\naction and observation. This way, we hope to raise awareness for this subject,\nso that it can become a focus of research in the future.", "published": "2022-02-07 09:59:46", "link": "http://arxiv.org/abs/2202.03040v1", "categories": ["cs.CL", "H.1.2; H.3.1; I.2.7"], "primary_category": "cs.CL"}
{"title": "Machine Translation from Signed to Spoken Languages: State of the Art\n  and Challenges", "abstract": "Automatic translation from signed to spoken languages is an interdisciplinary\nresearch domain, lying on the intersection of computer vision, machine\ntranslation and linguistics. Nevertheless, research in this domain is performed\nmostly by computer scientists in isolation. As the domain is becoming\nincreasingly popular - the majority of scientific papers on the topic of sign\nlanguage translation have been published in the past three years - we provide\nan overview of the state of the art as well as some required background in the\ndifferent related disciplines. We give a high-level introduction to sign\nlanguage linguistics and machine translation to illustrate the requirements of\nautomatic sign language translation. We present a systematic literature review\nto illustrate the state of the art in the domain and then, harking back to the\nrequirements, lay out several challenges for future research. We find that\nsignificant advances have been made on the shoulders of spoken language machine\ntranslation research. However, current approaches are often not linguistically\nmotivated or are not adapted to the different input modality of sign languages.\nWe explore challenges related to the representation of sign language data, the\ncollection of datasets, the need for interdisciplinary research and\nrequirements for moving beyond research, towards applications. Based on our\nfindings, we advocate for interdisciplinary research and to base future\nresearch on linguistic analysis of sign languages. Furthermore, the inclusion\nof deaf and hearing end users of sign language translation applications in use\ncase identification, data collection and evaluation is of the utmost importance\nin the creation of useful sign language translation models. We recommend\niterative, human-in-the-loop, design and development of sign language\ntranslation models.", "published": "2022-02-07 11:54:07", "link": "http://arxiv.org/abs/2202.03086v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Document-Level Event Extraction via Human-Like Reading Process", "abstract": "Document-level Event Extraction (DEE) is particularly tricky due to the two\nchallenges it poses: scattering-arguments and multi-events. The first challenge\nmeans that arguments of one event record could reside in different sentences in\nthe document, while the second one reflects one document may simultaneously\ncontain multiple such event records. Motivated by humans' reading cognitive to\nextract information of interests, in this paper, we propose a method called HRE\n(Human Reading inspired Extractor for Document Events), where DEE is decomposed\ninto these two iterative stages, rough reading and elaborate reading.\nSpecifically, the first stage browses the document to detect the occurrence of\nevents, and the second stage serves to extract specific event arguments. For\neach concrete event role, elaborate reading hierarchically works from sentences\nto characters to locate arguments across sentences, thus the\nscattering-arguments problem is tackled. Meanwhile, rough reading is explored\nin a multi-round manner to discover undetected events, thus the multi-events\nproblem is handled. Experiment results show the superiority of HRE over prior\ncompetitive methods.", "published": "2022-02-07 12:10:58", "link": "http://arxiv.org/abs/2202.03092v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Moving Other Way: Exploring Word Mover Distance Extensions", "abstract": "The word mover's distance (WMD) is a popular semantic similarity metric for\ntwo texts. This position paper studies several possible extensions of WMD. We\nexperiment with the frequency of words in the corpus as a weighting factor and\nthe geometry of the word vector space. We validate possible extensions of WMD\non six document classification datasets. Some proposed extensions show better\nresults in terms of the k-nearest neighbor classification error than WMD.", "published": "2022-02-07 12:56:32", "link": "http://arxiv.org/abs/2202.03119v2", "categories": ["cs.CL", "49Q22", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Robust Dialogue State Tracking with Weak Supervision and Sparse Data", "abstract": "Generalising dialogue state tracking (DST) to new data is especially\nchallenging due to the strong reliance on abundant and fine-grained supervision\nduring training. Sample sparsity, distributional shift and the occurrence of\nnew concepts and topics frequently lead to severe performance degradation\nduring inference. In this paper we propose a training strategy to build\nextractive DST models without the need for fine-grained manual span labels. Two\nnovel input-level dropout methods mitigate the negative impact of sample\nsparsity. We propose a new model architecture with a unified encoder that\nsupports value as well as slot independence by leveraging the attention\nmechanism. We combine the strengths of triple copy strategy DST and value\nmatching to benefit from complementary predictions without violating the\nprinciple of ontology independence. Our experiments demonstrate that an\nextractive DST model can be trained without manual span labels. Our\narchitecture and training strategies improve robustness towards sample\nsparsity, new concepts and topics, leading to state-of-the-art performance on a\nrange of benchmarks. We further highlight our model's ability to effectively\nlearn from non-dialogue data.", "published": "2022-02-07 16:58:12", "link": "http://arxiv.org/abs/2202.03354v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cedille: A large autoregressive French language model", "abstract": "Scaling up the size and training of autoregressive language models has\nenabled novel ways of solving Natural Language Processing tasks using zero-shot\nand few-shot learning. While extreme-scale language models such as GPT-3 offer\nmultilingual capabilities, zero-shot learning for languages other than English\nremain largely unexplored. Here, we introduce Cedille, a large open source\nauto-regressive language model, specifically trained for the French language.\nOur results show that Cedille outperforms existing French language models and\nis competitive with GPT-3 on a range of French zero-shot benchmarks.\nFurthermore, we provide an in-depth comparison of the toxicity exhibited by\nthese models, showing that Cedille marks an improvement in language model\nsafety thanks to dataset filtering.", "published": "2022-02-07 17:40:43", "link": "http://arxiv.org/abs/2202.03371v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Selecting Seed Words for Wordle using Character Statistics", "abstract": "Wordle, a word guessing game rose to global popularity in the January of\n2022. The goal of the game is to guess a five-letter English word within six\ntries. Each try provides the player with hints by means of colour changing\ntiles which inform whether or not a given character is part of the solution as\nwell as, in cases where it is part of the solution, whether or not it is in the\ncorrect placement. Numerous attempts have been made to find the best starting\nword and best strategy to solve the daily wordle. This study uses character\nstatistics of five-letter words to determine the best three starting words.", "published": "2022-02-07 19:01:19", "link": "http://arxiv.org/abs/2202.03457v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning Approach for Arabic Offensive Language Detection System:\n  BERT-Based Model", "abstract": "The problem of online offensive language limits the health and security of\nonline users. It is essential to apply the latest state-of-the-art techniques\nin developing a system to detect online offensive language and to ensure social\njustice to the online communities. Our study investigates the effects of\nfine-tuning across several Arabic offensive language datasets. We develop\nmultiple classifiers that use four datasets individually and in combination in\norder to gain knowledge about online Arabic offensive content and classify\nusers comments accordingly. Our results demonstrate the limited effects of\ntransfer learning on the classifiers performance, particularly for highly\ndialectal comments.", "published": "2022-02-07 17:26:35", "link": "http://arxiv.org/abs/2203.03542v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple\n  Sequence-to-Sequence Learning Framework", "abstract": "In this work, we pursue a unified paradigm for multimodal pretraining to\nbreak the scaffolds of complex task/modality-specific customization. We propose\nOFA, a Task-Agnostic and Modality-Agnostic framework that supports Task\nComprehensiveness. OFA unifies a diverse set of cross-modal and unimodal tasks,\nincluding image generation, visual grounding, image captioning, image\nclassification, language modeling, etc., in a simple sequence-to-sequence\nlearning framework. OFA follows the instruction-based learning in both\npretraining and finetuning stages, requiring no extra task-specific layers for\ndownstream tasks. In comparison with the recent state-of-the-art vision &\nlanguage models that rely on extremely large cross-modal datasets, OFA is\npretrained on only 20M publicly available image-text pairs. Despite its\nsimplicity and relatively small-scale training data, OFA achieves new SOTAs in\na series of cross-modal tasks while attaining highly competitive performances\non uni-modal tasks. Our further analysis indicates that OFA can also\neffectively transfer to unseen tasks and unseen domains. Our code and models\nare publicly available at https://github.com/OFA-Sys/OFA.", "published": "2022-02-07 10:38:21", "link": "http://arxiv.org/abs/2202.03052v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Gender stereotypes in the mediated personalization of politics:\n  Empirical evidence from a lexical, syntactic and sentiment analysis", "abstract": "The media attention to the personal sphere of famous and important\nindividuals has become a key element of the gender narrative. Here we combine\nlexical, syntactic and sentiment analysis to investigate the role of gender in\nthe personalization of a wide range of political office holders in Italy during\nthe period 2017-2020. On the basis of a score for words that is introduced to\naccount for gender unbalance in both representative and news coverage, we show\nthat the political personalization in Italy is more detrimental for women than\nmen, with the persistence of entrenched stereotypes including a masculine\nconnotation of leadership, the resulting women's unsuitability to hold\npolitical functions, and a greater deal of focus on their attractiveness and\nbody parts. In addition, women politicians are covered with a more negative\ntone than their men counterpart when personal details are reported. Further,\nthe major contribution to the observed gender differences comes from online\nnews rather than print news, suggesting that the expression of certain\nstereotypes may be better conveyed when click baiting and personal targeting\nhave a major impact.", "published": "2022-02-07 11:40:44", "link": "http://arxiv.org/abs/2202.03083v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "To Tune or Not To Tune? Zero-shot Models for Legal Case Entailment", "abstract": "There has been mounting evidence that pretrained language models fine-tuned\non large and diverse supervised datasets can transfer well to a variety of\nout-of-domain tasks. In this work, we investigate this transfer ability to the\nlegal domain. For that, we participated in the legal case entailment task of\nCOLIEE 2021, in which we use such models with no adaptations to the target\ndomain. Our submissions achieved the highest scores, surpassing the second-best\nteam by more than six percentage points. Our experiments confirm a\ncounter-intuitive result in the new paradigm of pretrained language models:\ngiven limited labeled data, models with little or no adaptation to the target\ntask can be more robust to changes in the data distribution than models\nfine-tuned on it. Code is available at https://github.com/neuralmind-ai/coliee.", "published": "2022-02-07 13:02:48", "link": "http://arxiv.org/abs/2202.03120v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Conversational Agents: Theory and Applications", "abstract": "In this chapter, we provide a review of conversational agents (CAs),\ndiscussing chatbots, intended for casual conversation with a user, as well as\ntask-oriented agents that generally engage in discussions intended to reach one\nor several specific goals, often (but not always) within a specific domain. We\nalso consider the concept of embodied conversational agents, briefly reviewing\naspects such as character animation and speech processing. The many different\napproaches for representing dialogue in CAs are discussed in some detail, along\nwith methods for evaluating such agents, emphasizing the important topics of\naccountability and interpretability. A brief historical overview is given,\nfollowed by an extensive overview of various applications, especially in the\nfields of health and education. We end the chapter by discussing benefits and\npotential risks regarding the societal impact of current and future CA\ntechnology.", "published": "2022-02-07 13:48:14", "link": "http://arxiv.org/abs/2202.03164v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mental Disorders on Online Social Media Through the Lens of Language and\n  Behaviour: Analysis and Visualisation", "abstract": "Due to the worldwide accessibility to the Internet along with the continuous\nadvances in mobile technologies, physical and digital worlds have become\ncompletely blended, and the proliferation of social media platforms has taken a\nleading role over this evolution. In this paper, we undertake a thorough\nanalysis towards better visualising and understanding the factors that\ncharacterise and differentiate social media users affected by mental disorders.\nWe perform different experiments studying multiple dimensions of language,\nincluding vocabulary uniqueness, word usage, linguistic style, psychometric\nattributes, emotions' co-occurrence patterns, and online behavioural traits,\nincluding social engagement and posting trends. Our findings reveal significant\ndifferences on the use of function words, such as adverbs and verb tense, and\ntopic-specific vocabulary, such as biological processes. As for emotional\nexpression, we observe that affected users tend to share emotions more\nregularly than control individuals on average. Overall, the monthly posting\nvariance of the affected groups is higher than the control groups. Moreover, we\nfound evidence suggesting that language use on micro-blogging platforms is less\ndistinguishable for users who have a mental disorder than other less\nrestrictive platforms. In particular, we observe on Twitter less quantifiable\ndifferences between affected and control groups compared to Reddit.", "published": "2022-02-07 15:29:01", "link": "http://arxiv.org/abs/2202.03291v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Universal Spam Detection using Transfer Learning of BERT Model", "abstract": "Deep learning transformer models become important by training on text data\nbased on self-attention mechanisms. This manuscript demonstrated a novel\nuniversal spam detection model using pre-trained Google's Bidirectional Encoder\nRepresentations from Transformers (BERT) base uncased models with four datasets\nby efficiently classifying ham or spam emails in real-time scenarios. Different\nmethods for Enron, Spamassain, Lingspam, and Spamtext message classification\ndatasets, were used to train models individually in which a single model was\nobtained with acceptable performance on four datasets. The Universal Spam\nDetection Model (USDM) was trained with four datasets and leveraged\nhyperparameters from each model. The combined model was finetuned with the same\nhyperparameters from these four models separately. When each model using its\ncorresponding dataset, an F1-score is at and above 0.9 in individual models. An\noverall accuracy reached 97%, with an F1 score of 0.96. Research results and\nimplications were discussed.", "published": "2022-02-07 19:37:39", "link": "http://arxiv.org/abs/2202.03480v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Measuring and Reducing Model Update Regression in Structured Prediction\n  for NLP", "abstract": "Recent advance in deep learning has led to the rapid adoption of machine\nlearning-based NLP models in a wide range of applications. Despite the\ncontinuous gain in accuracy, backward compatibility is also an important aspect\nfor industrial applications, yet it received little research attention.\nBackward compatibility requires that the new model does not regress on cases\nthat were correctly handled by its predecessor. This work studies model update\nregression in structured prediction tasks. We choose syntactic dependency\nparsing and conversational semantic parsing as representative examples of\nstructured prediction tasks in NLP. First, we measure and analyze model update\nregression in different model update settings. Next, we explore and benchmark\nexisting techniques for reducing model update regression including model\nensemble and knowledge distillation. We further propose a simple and effective\nmethod, Backward-Congruent Re-ranking (BCR), by taking into account the\ncharacteristics of structured prediction. Experiments show that BCR can better\nmitigate model update regression than model ensemble and knowledge distillation\napproaches.", "published": "2022-02-07 07:04:54", "link": "http://arxiv.org/abs/2202.02976v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Adapter Transfer of Self-Supervised Speech Models for\n  Automatic Speech Recognition", "abstract": "Self-supervised learning (SSL) is a powerful tool that allows learning of\nunderlying representations from unlabeled data. Transformer based models such\nas wav2vec 2.0 and HuBERT are leading the field in the speech domain. Generally\nthese models are fine-tuned on a small amount of labeled data for a downstream\ntask such as Automatic Speech Recognition (ASR). This involves re-training the\nmajority of the model for each task. Adapters are small lightweight modules\nwhich are commonly used in Natural Language Processing (NLP) to adapt\npre-trained models to new tasks. In this paper we propose applying adapters to\nwav2vec 2.0 to reduce the number of parameters required for downstream ASR\ntasks, and increase scalability of the model to multiple tasks or languages.\nUsing adapters we can perform ASR while training fewer than 10% of parameters\nper task compared to full fine-tuning with little degradation of performance.\nAblations show that applying adapters into just the top few layers of the\npre-trained network gives similar performance to full transfer, supporting the\ntheory that higher pre-trained layers encode more phonemic information, and\nfurther optimizing efficiency.", "published": "2022-02-07 14:20:54", "link": "http://arxiv.org/abs/2202.03218v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Red Teaming Language Models with Language Models", "abstract": "Language Models (LMs) often cannot be deployed because of their potential to\nharm users in hard-to-predict ways. Prior work identifies harmful behaviors\nbefore deployment by using human annotators to hand-write test cases. However,\nhuman annotation is expensive, limiting the number and diversity of test cases.\nIn this work, we automatically find cases where a target LM behaves in a\nharmful way, by generating test cases (\"red teaming\") using another LM. We\nevaluate the target LM's replies to generated test questions using a classifier\ntrained to detect offensive content, uncovering tens of thousands of offensive\nreplies in a 280B parameter LM chatbot. We explore several methods, from\nzero-shot generation to reinforcement learning, for generating test cases with\nvarying levels of diversity and difficulty. Furthermore, we use prompt\nengineering to control LM-generated test cases to uncover a variety of other\nharms, automatically finding groups of people that the chatbot discusses in\noffensive ways, personal and hospital phone numbers generated as the chatbot's\nown contact info, leakage of private training data in generated text, and harms\nthat occur over the course of a conversation. Overall, LM-based red teaming is\none promising tool (among many needed) for finding and fixing diverse,\nundesirable LM behaviors before impacting users.", "published": "2022-02-07 15:22:17", "link": "http://arxiv.org/abs/2202.03286v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Approximation Algorithms for ROUND-UFP and ROUND-SAP", "abstract": "We study ROUND-UFP and ROUND-SAP, two generalizations of the classical BIN\nPACKING problem that correspond to the unsplittable flow problem on a path\n(UFP) and the storage allocation problem (SAP), respectively. We are given a\npath with capacities on its edges and a set of tasks where for each task we are\ngiven a demand and a subpath. In ROUND-UFP, the goal is to find a packing of\nall tasks into a minimum number of copies (rounds) of the given path such that\nfor each copy, the total demand of tasks on any edge does not exceed the\ncapacity of the respective edge. In ROUND-SAP, the tasks are considered to be\nrectangles and the goal is to find a non-overlapping packing of these\nrectangles into a minimum number of rounds such that all rectangles lie\ncompletely below the capacity profile of the edges.\n  We show that in contrast to BIN PACKING, both the problems do not admit an\nasymptotic polynomial-time approximation scheme (APTAS), even when all edge\ncapacities are equal. However, for this setting, we obtain asymptotic\n$(2+\\varepsilon)$-approximations for both problems. For the general case, we\nobtain an $O(\\log\\log n)$-approximation algorithm and an\n$O(\\log\\log\\frac{1}{\\delta})$-approximation under $(1+\\delta)$-resource\naugmentation for both problems. For the intermediate setting of the no\nbottleneck assumption (i.e., the maximum task demand is at most the minimum\nedge capacity), we obtain absolute $12$- and asymptotic\n$(16+\\varepsilon)$-approximation algorithms for ROUND-UFP and ROUND-SAP,\nrespectively.", "published": "2022-02-07 20:15:15", "link": "http://arxiv.org/abs/2202.03492v1", "categories": ["cs.DS", "cs.CL", "cs.GT"], "primary_category": "cs.DS"}
{"title": "Self-Supervised Representation Learning for Speech Using Visual\n  Grounding and Masked Language Modeling", "abstract": "In this paper, we describe our submissions to the ZeroSpeech 2021 Challenge\nand SUPERB benchmark. Our submissions are based on the recently proposed\nFaST-VGS model, which is a Transformer-based model that learns to associate raw\nspeech waveforms with semantically related images, all without the use of any\ntranscriptions of the speech. Additionally, we introduce a novel extension of\nthis model, FaST-VGS+, which is learned in a multi-task fashion with a masked\nlanguage modeling objective in addition to the visual grounding objective. On\nZeroSpeech 2021, we show that our models perform competitively on the ABX task,\noutperform all other concurrent submissions on the Syntactic and Semantic\ntasks, and nearly match the best system on the Lexical task. On the SUPERB\nbenchmark, we show that our models also achieve strong performance, in some\ncases even outperforming the popular wav2vec2.0 model.", "published": "2022-02-07 22:09:54", "link": "http://arxiv.org/abs/2202.03543v2", "categories": ["eess.AS", "cs.CL", "cs.CV", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Building Synthetic Speaker Profiles in Text-to-Speech Systems", "abstract": "The diversity of speaker profiles in multi-speaker TTS systems is a crucial\naspect of its performance, as it measures how many different speaker profiles\nTTS systems could possibly synthesize. However, this important aspect is often\noverlooked when building multi-speaker TTS systems and there is no established\nframework to evaluate this diversity. The reason behind is that most\nmulti-speaker TTS systems are limited to generate speech signals with the same\nspeaker profiles as its training data. They often use discrete speaker\nembedding vectors which have a one-to-one correspondence with individual\nspeakers. This correspondence limits TTS systems and hinders their capability\nof generating unseen speaker profiles that did not appear during training. In\nthis paper, we aim to build multi-speaker TTS systems that have a greater\nvariety of speaker profiles and can generate new synthetic speaker profiles\nthat are different from training data. To this end, we propose to use\ngenerative models with a triplet loss and a specific shuffle mechanism. In our\nexperiments, the effectiveness and advantages of the proposed method have been\ndemonstrated in terms of both the distinctiveness and intelligibility of\nsynthesized speech signals.", "published": "2022-02-07 13:06:18", "link": "http://arxiv.org/abs/2202.03125v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Semantic-aware Speech to Text Transmission with Redundancy Removal", "abstract": "Deep learning (DL) based semantic communication methods have been explored\nfor the efficient transmission of images, text, and speech in recent years. In\ncontrast to traditional wireless communication methods that focus on the\ntransmission of abstract symbols, semantic communication approaches attempt to\nachieve better transmission efficiency by only sending the semantic-related\ninformation of the source data. In this paper, we consider semantic-oriented\nspeech to text transmission. We propose a novel end-to-end DL-based\ntransceiver, which includes an attention-based soft alignment module and a\nredundancy removal module to compress the transmitted data. In particular, the\nformer extracts only the text-related semantic features, and the latter further\ndrops the semantically redundant content, greatly reducing the amount of\nsemantic redundancy compared to existing methods. We also propose a two-stage\ntraining scheme, which speeds up the training of the proposed DL model. The\nsimulation results indicate that our proposed method outperforms current\nmethods in terms of the accuracy of the received text and transmission\nefficiency. Moreover, the proposed method also has a smaller model size and\nshorter end-to-end runtime.", "published": "2022-02-07 14:16:26", "link": "http://arxiv.org/abs/2202.03211v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning Sound Localization Better From Semantically Similar Samples", "abstract": "The objective of this work is to localize the sound sources in visual scenes.\nExisting audio-visual works employ contrastive learning by assigning\ncorresponding audio-visual pairs from the same source as positives while\nrandomly mismatched pairs as negatives. However, these negative pairs may\ncontain semantically matched audio-visual information. Thus, these semantically\ncorrelated pairs, \"hard positives\", are mistakenly grouped as negatives. Our\nkey contribution is showing that hard positives can give similar response maps\nto the corresponding pairs. Our approach incorporates these hard positives by\nadding their response maps into a contrastive learning objective directly. We\ndemonstrate the effectiveness of our approach on VGG-SS and SoundNet-Flickr\ntest sets, showing favorable performance to the state-of-the-art methods.", "published": "2022-02-07 08:53:55", "link": "http://arxiv.org/abs/2202.03007v1", "categories": ["cs.CV", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "T-NGA: Temporal Network Grafting Algorithm for Learning to Process\n  Spiking Audio Sensor Events", "abstract": "Spiking silicon cochlea sensors encode sound as an asynchronous stream of\nspikes from different frequency channels. The lack of labeled training datasets\nfor spiking cochleas makes it difficult to train deep neural networks on the\noutputs of these sensors. This work proposes a self-supervised method called\nTemporal Network Grafting Algorithm (T-NGA), which grafts a recurrent network\npretrained on spectrogram features so that the network works with the cochlea\nevent features. T-NGA training requires only temporally aligned audio\nspectrograms and event features. Our experiments show that the accuracy of the\ngrafted network was similar to the accuracy of a supervised network trained\nfrom scratch on a speech recognition task using events from a software spiking\ncochlea model. Despite the circuit non-idealities of the spiking silicon\ncochlea, the grafted network accuracy on the silicon cochlea spike recordings\nwas only about 5% lower than the supervised network accuracy using the\nN-TIDIGITS18 dataset. T-NGA can train networks to process spiking audio sensor\nevents in the absence of large labeled spike datasets.", "published": "2022-02-07 14:14:14", "link": "http://arxiv.org/abs/2202.03204v1", "categories": ["eess.AS", "cs.NE", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Impulse Responses: Estimating and Parameterizing Filters with Deep\n  Networks", "abstract": "Impulse response estimation in high noise and in-the-wild settings, with\nminimal control of the underlying data distributions, is a challenging problem.\nWe propose a novel framework for parameterizing and estimating impulse\nresponses based on recent advances in neural representation learning. Our\nframework is driven by a carefully designed neural network that jointly\nestimates the impulse response and the (apriori unknown) spectral noise\ncharacteristics of an observed signal given the source signal. We demonstrate\nrobustness in estimation, even under low signal-to-noise ratios, and show\nstrong results when learning from spatio-temporal real-world speech data. Our\nframework provides a natural way to interpolate impulse responses on a spatial\ngrid, while also allowing for efficiently compressing and storing them for\nreal-time rendering applications in augmented and virtual reality.", "published": "2022-02-07 18:57:23", "link": "http://arxiv.org/abs/2202.03416v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Maximizing Audio Event Detection Model Performance on Small Datasets\n  Through Knowledge Transfer, Data Augmentation, And Pretraining: An Ablation\n  Study", "abstract": "An Xception model reaches state-of-the-art (SOTA) accuracy on the ESC-50\ndataset for audio event detection through knowledge transfer from ImageNet\nweights, pretraining on AudioSet, and an on-the-fly data augmentation pipeline.\nThis paper presents an ablation study that analyzes which components contribute\nto the boost in performance and training time. A smaller Xception model is also\npresented which nears SOTA performance with almost a third of the parameters.", "published": "2022-02-07 20:57:40", "link": "http://arxiv.org/abs/2202.03514v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech Emotion Recognition using Self-Supervised Features", "abstract": "Self-supervised pre-trained features have consistently delivered state-of-art\nresults in the field of natural language processing (NLP); however, their\nmerits in the field of speech emotion recognition (SER) still need further\ninvestigation. In this paper we introduce a modular End-to- End (E2E) SER\nsystem based on an Upstream + Downstream architecture paradigm, which allows\neasy use/integration of a large variety of self-supervised features. Several\nSER experiments for predicting categorical emotion classes from the IEMOCAP\ndataset are performed. These experiments investigate interactions among\nfine-tuning of self-supervised feature models, aggregation of frame-level\nfeatures into utterance-level features and back-end classification networks.\nThe proposed monomodal speechonly based system not only achieves SOTA results,\nbut also brings light to the possibility of powerful and well finetuned\nself-supervised acoustic features that reach results similar to the results\nachieved by SOTA multimodal systems using both Speech and Text modalities.", "published": "2022-02-07 00:50:07", "link": "http://arxiv.org/abs/2202.03896v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
