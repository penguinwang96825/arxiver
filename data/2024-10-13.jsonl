{"title": "Can GANs Learn the Stylized Facts of Financial Time Series?", "abstract": "In the financial sector, a sophisticated financial time series simulator is\nessential for evaluating financial products and investment strategies.\nTraditional back-testing methods have mainly relied on historical data-driven\napproaches or mathematical model-driven approaches, such as various stochastic\nprocesses. However, in the current era of AI, data-driven approaches, where\nmodels learn the intrinsic characteristics of data directly, have emerged as\npromising techniques. Generative Adversarial Networks (GANs) have surfaced as\npromising generative models, capturing data distributions through adversarial\nlearning. Financial time series, characterized 'stylized facts' such as random\nwalks, mean-reverting patterns, unexpected jumps, and time-varying volatility,\npresent significant challenges for deep neural networks to learn their\nintrinsic characteristics. This study examines the ability of GANs to learn\ndiverse and complex temporal patterns (i.e., stylized facts) of both univariate\nand multivariate financial time series. Our extensive experiments revealed that\nGANs can capture various stylized facts of financial time series, but their\nperformance varies significantly depending on the choice of generator\narchitecture. This suggests that naively applying GANs might not effectively\ncapture the intricate characteristics inherent in financial time series,\nhighlighting the importance of carefully considering and validating the\nmodeling choices.", "published": "2024-10-13 14:20:56", "link": "http://arxiv.org/abs/2410.09850v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "Backtesting Framework for Concentrated Liquidity Market Makers on Uniswap V3 Decentralized Exchange", "abstract": "Decentralized finance (DeFi) has revolutionized the financial landscape, with\nprotocols like Uniswap offering innovative automated market-making mechanisms.\nThis article explores the development of a backtesting framework specifically\ntailored for concentrated liquidity market makers (CLMM). The focus is on\nleveraging the liquidity distribution approximated using a parametric model, to\nestimate the rewards within liquidity pools.\n  The article details the design, implementation, and insights derived from\nthis novel approach to backtesting within the context of Uniswap V3. The\ndeveloped backtester was successfully utilized to assess reward levels across\nseveral pools using historical data from 2023 (pools Uniswap v3 for pairs of\naltcoins, stablecoins and USDC/ETH with different fee levels). Moreover, the\nerror in modeling the level of rewards for the period under review for each\npool was less than 1\\%. This demonstrated the effectiveness of the backtester\nin quantifying liquidity pool rewards and its potential in estimating LP's\nrevenues as part of the pool rewards, as focus of our next research.\n  The backtester serves as a tool to simulate trading strategies and liquidity\nprovision scenarios, providing a quantitative assessment of potential returns\nfor liquidity providers (LP). By incorporating statistical tools to mirror CLMM\npool liquidity dynamics, this framework can be further leveraged for strategy\nenhancement and risk evaluation for LPs operating within decentralized\nexchanges.", "published": "2024-10-13 19:56:11", "link": "http://arxiv.org/abs/2410.09983v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "No arbitrage and the existence of ACLMMs in general diffusion models", "abstract": "In a seminal paper, F. Delbaen and W. Schachermayer proved that the classical\nNA (\"no arbitrage\") condition implies the existence of an \"absolutely\ncontinuous local martingale measure\" (ACLMM). It is known that in general the\nexistence of an ACLMM alone is not sufficient for NA. In this paper we\ninvestigate how close these notions are for single asset general diffusion\nmarket models. We show that NA is equivalent to the existence of an ACLMM plus\na mild regularity condition on the scale function and the absence of reflecting\nboundaries. For infinite time horizon scenarios, the regularity assumption and\nthe requirement on the boundaries can be dropped, showing equivalence between\nNA and the existence of an ACLMM. By means of counterexamples, we show that our\ncharacterization of NA for finite time horizons is sharp in the sense that\nneither the regularity condition on the scale function nor the absence of\nreflecting boundaries can be dropped.", "published": "2024-10-13 10:08:52", "link": "http://arxiv.org/abs/2410.09789v1", "categories": ["q-fin.MF", "math.PR", "60G44, 60H10, 60J60, 91B70, 91G15"], "primary_category": "q-fin.MF"}
{"title": "Achilles, Neural Network to Predict the Gold Vs US Dollar Integration with Trading Bot for Automatic Trading", "abstract": "Predicting the stock market is a big challenge for the machine learning\nworld. It is known how difficult it is to have accurate and consistent\npredictions with ML models. Some architectures are able to capture the movement\nof stocks but almost never are able to be launched to the production world. We\npresent Achilles, with a classical architecture of LSTM(Long Short Term Memory)\nneural network this model is able to predict the Gold vs USD commodity. With\nthe predictions minute-per-minute of this model we implemented a trading bot to\nrun during 23 days of testing excluding weekends. At the end of the testing\nperiod we generated $1623.52 in profit with the methodology used. The results\nof our method demonstrate Machine Learning can successfully be implemented to\npredict the Gold vs USD commodity.", "published": "2024-10-13 00:45:30", "link": "http://arxiv.org/abs/2410.21291v3", "categories": ["q-fin.ST", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "Taming Overconfidence in LLMs: Reward Calibration in RLHF", "abstract": "Language model calibration refers to the alignment between the confidence of\nthe model and the actual performance of its responses. While previous studies\npoint out the overconfidence phenomenon in Large Language Models (LLMs) and\nshow that LLMs trained with Reinforcement Learning from Human Feedback (RLHF)\nare overconfident with a more sharpened output probability, in this study, we\nreveal that RLHF tends to lead models to express verbalized overconfidence in\ntheir own responses. We investigate the underlying cause of this overconfidence\nand demonstrate that reward models used for Proximal Policy Optimization (PPO)\nexhibit inherent biases towards high-confidence scores regardless of the actual\nquality of responses. Building upon this insight, we propose two PPO variants:\nPPO-M: PPO with Calibrated Reward Modeling and PPO-C: PPO with Calibrated\nReward Calculation. PPO-M integrates explicit confidence scores in reward model\ntraining, which calibrates reward models to better capture the alignment\nbetween response quality and verbalized confidence. PPO-C adjusts the reward\nscore during PPO based on the difference between the current reward and the\nexponential average of past rewards. Both PPO-M and PPO-C can be seamlessly\nintegrated into the current PPO pipeline and do not require additional golden\nlabels. We evaluate our methods on both Llama3-8B and Mistral-7B across six\ndiverse datasets including multiple-choice and open-ended generation.\nExperimental results demonstrate that both of our methods can reduce\ncalibration error and maintain performance comparable to standard PPO. We\nfurther show that they could preserve model capabilities in open-ended\nconversational settings.", "published": "2024-10-13 04:48:40", "link": "http://arxiv.org/abs/2410.09724v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empirical Study of Mutual Reinforcement Effect and Application in\n  Few-shot Text Classification Tasks via Prompt", "abstract": "The Mutual Reinforcement Effect (MRE) investigates the synergistic\nrelationship between word-level and text-level classifications in text\nclassification tasks. It posits that the performance of both classification\nlevels can be mutually enhanced. However, this mechanism has not been\nadequately demonstrated or explained in prior research. To address this gap, we\nemploy empirical experiment to observe and substantiate the MRE theory. Our\nexperiments on 21 MRE mix datasets revealed the presence of MRE in the model\nand its impact. Specifically, we conducted compare experiments use fine-tune.\nThe results of findings from comparison experiments corroborates the existence\nof MRE. Furthermore, we extended the application of MRE to prompt learning,\nutilizing word-level information as a verbalizer to bolster the model's\nprediction of text-level classification labels. In our final experiment, the\nF1-score significantly surpassed the baseline in 18 out of 21 MRE Mix datasets,\nfurther validating the notion that word-level information enhances the language\nmodel's comprehension of the text as a whole.", "published": "2024-10-13 06:43:54", "link": "http://arxiv.org/abs/2410.09745v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Mixed-Language Multi-Document News Summarization Dataset and a\n  Graphs-Based Extract-Generate Model", "abstract": "Existing research on news summarization primarily focuses on single-language\nsingle-document (SLSD), single-language multi-document (SLMD) or cross-language\nsingle-document (CLSD). However, in real-world scenarios, news about a\ninternational event often involves multiple documents in different languages,\ni.e., mixed-language multi-document (MLMD). Therefore, summarizing MLMD news is\nof great significance. However, the lack of datasets for MLMD news\nsummarization has constrained the development of research in this area. To fill\nthis gap, we construct a mixed-language multi-document news summarization\ndataset (MLMD-news), which contains four different languages and 10,992 source\ndocument cluster and target summary pairs. Additionally, we propose a\ngraph-based extract-generate model and benchmark various methods on the\nMLMD-news dataset and publicly release our dataset and\ncode\\footnote[1]{https://github.com/Southnf9/MLMD-news}, aiming to advance\nresearch in summarization within MLMD scenarios.", "published": "2024-10-13 08:15:33", "link": "http://arxiv.org/abs/2410.09773v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reverse Modeling in Large Language Models", "abstract": "Humans are accustomed to reading and writing in a forward manner, and this\nnatural bias extends to text understanding in auto-regressive large language\nmodels (LLMs). This paper investigates whether LLMs, like humans, struggle with\nreverse modeling, specifically with reversed text inputs. We found that\npublicly available pre-trained LLMs cannot understand such inputs. However,\nLLMs trained from scratch with both forward and reverse texts can understand\nthem equally well during inference across multiple languages. Our case study\nshows that different-content texts result in different losses if input (to\nLLMs) in different directions -- some get lower losses for forward while some\nfor reverse. This leads us to a simple and nice solution for data selection\nbased on the loss differences between forward and reverse directions. Using our\nselected data in continued pretraining can boost LLMs' performance by a large\nmargin across different language understanding benchmarks.", "published": "2024-10-13 12:24:03", "link": "http://arxiv.org/abs/2410.09817v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-Based Multi-Agent Systems are Scalable Graph Generative Models", "abstract": "The structural properties of naturally arising social graphs are extensively\nstudied to understand their evolution. Prior approaches for modeling network\ndynamics typically rely on rule-based models, which lack realism and\ngeneralizability, or deep learning-based models, which require large-scale\ntraining datasets. Social graphs, as abstract graph representations of\nentity-wise interactions, present an opportunity to explore network evolution\nmechanisms through realistic simulations of human-item interactions. Leveraging\nthe pre-trained social consensus knowledge embedded in large language models\n(LLMs), we present GraphAgent-Generator (GAG), a novel simulation-based\nframework for dynamic, text-attributed social graph generation. GAG simulates\nthe temporal node and edge generation processes for zero-shot social graph\ngeneration. The resulting graphs exhibit adherence to seven key macroscopic\nnetwork properties, achieving an 11% improvement in microscopic graph structure\nmetrics. Through the node classification benchmarking task, we validate GAG\neffectively captures the intricate text-structure correlations in graph\ngeneration. Furthermore, GAG supports generating graphs with up to nearly\n100,000 nodes or 10 million edges through large-scale LLM-based agent\nsimulation with parallel acceleration, achieving a minimum speed-up of 90.4%.\nThe source code is available at https://github.com/Ji-Cather/GraphAgent.", "published": "2024-10-13 12:57:08", "link": "http://arxiv.org/abs/2410.09824v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RMB: Comprehensively Benchmarking Reward Models in LLM Alignment", "abstract": "Reward models (RMs) guide the alignment of large language models (LLMs),\nsteering them toward behaviors preferred by humans. Evaluating RMs is the key\nto better aligning LLMs. However, the current evaluation of RMs may not\ndirectly correspond to their alignment performance due to the limited\ndistribution of evaluation data and evaluation methods that are not closely\nrelated to alignment objectives. To address these limitations, we propose RMB,\na comprehensive RM benchmark that covers over 49 real-world scenarios and\nincludes both pairwise and Best-of-N (BoN) evaluations to better reflect the\neffectiveness of RMs in guiding alignment optimization. We demonstrate a\npositive correlation between our benchmark and the downstream alignment task\nperformance. Based on our benchmark, we conduct extensive analysis on the\nstate-of-the-art RMs, revealing their generalization defects that were not\ndiscovered by previous benchmarks, and highlighting the potential of generative\nRMs. Furthermore, we delve into open questions in reward models, specifically\nexamining the effectiveness of majority voting for the evaluation of reward\nmodels and analyzing the impact factors of generative RMs, including the\ninfluence of evaluation criteria and instructing methods. Our evaluation code\nand datasets are available at\nhttps://github.com/Zhou-Zoey/RMB-Reward-Model-Benchmark.", "published": "2024-10-13 16:06:54", "link": "http://arxiv.org/abs/2410.09893v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reddit is all you need: Authorship profiling for Romanian", "abstract": "Authorship profiling is the process of identifying an author's\ncharacteristics based on their writings. This centuries old problem has become\nmore intriguing especially with recent developments in Natural Language\nProcessing (NLP). In this paper, we introduce a corpus of short texts in the\nRomanian language, annotated with certain author characteristic keywords; to\nour knowledge, the first of its kind. In order to do this, we exploit a social\nmedia platform called Reddit. We leverage its thematic community-based\nstructure (subreddits structure), which offers information about the author's\nbackground. We infer an user's demographic and some broad personal traits, such\nas age category, employment status, interests, and social orientation based on\nthe subreddit and other cues. We thus obtain a 23k+ samples corpus, extracted\nfrom 100+ Romanian subreddits. We analyse our dataset, and finally, we\nfine-tune and evaluate Large Language Models (LLMs) to prove baselines\ncapabilities for authorship profiling using the corpus, indicating the need for\nfurther research in the field. We publicly release all our resources.", "published": "2024-10-13 16:27:31", "link": "http://arxiv.org/abs/2410.09907v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MisinfoEval: Generative AI in the Era of \"Alternative Facts\"", "abstract": "The spread of misinformation on social media platforms threatens democratic\nprocesses, contributes to massive economic losses, and endangers public health.\nMany efforts to address misinformation focus on a knowledge deficit model and\npropose interventions for improving users' critical thinking through access to\nfacts. Such efforts are often hampered by challenges with scalability, and by\nplatform users' personal biases. The emergence of generative AI presents\npromising opportunities for countering misinformation at scale across\nideological barriers.\n  In this paper, we introduce a framework (MisinfoEval) for generating and\ncomprehensively evaluating large language model (LLM) based misinformation\ninterventions. We present (1) an experiment with a simulated social media\nenvironment to measure effectiveness of misinformation interventions, and (2) a\nsecond experiment with personalized explanations tailored to the demographics\nand beliefs of users with the goal of countering misinformation by appealing to\ntheir pre-existing values. Our findings confirm that LLM-based interventions\nare highly effective at correcting user behavior (improving overall user\naccuracy at reliability labeling by up to 41.72%). Furthermore, we find that\nusers favor more personalized interventions when making decisions about news\nreliability and users shown personalized interventions have significantly\nhigher accuracy at identifying misinformation.", "published": "2024-10-13 18:16:50", "link": "http://arxiv.org/abs/2410.09949v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MARS: Multilingual Aspect-centric Review Summarisation", "abstract": "Summarizing customer feedback to provide actionable insights for\nproducts/services at scale is an important problem for businesses across\nindustries. Lately, the review volumes are increasing across regions and\nlanguages, therefore the challenge of aggregating and understanding customer\nsentiment across multiple languages becomes increasingly vital. In this paper,\nwe propose a novel framework involving a two-step paradigm\n\\textit{Extract-then-Summarise}, namely MARS to revolutionise traditions and\naddress the domain agnostic aspect-level multilingual review summarisation.\nExtensive automatic and human evaluation shows that our approach brings\nsubstantial improvements over abstractive baselines and efficiency to real-time\nsystems.", "published": "2024-10-13 20:16:39", "link": "http://arxiv.org/abs/2410.09991v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Gender Bias of LLMs in Making Morality Judgements", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in a\nmultitude of Natural Language Processing (NLP) tasks. However, these models are\nstill not immune to limitations such as social biases, especially gender bias.\nThis work investigates whether current closed and open-source LLMs possess\ngender bias, especially when asked to give moral opinions. To evaluate these\nmodels, we curate and introduce a new dataset GenMO (Gender-bias in Morality\nOpinions) comprising parallel short stories featuring male and female\ncharacters respectively. Specifically, we test models from the GPT family\n(GPT-3.5-turbo, GPT-3.5-turbo-instruct, GPT-4-turbo), Llama 3 and 3.1 families\n(8B/70B), Mistral-7B and Claude 3 families (Sonnet and Opus). Surprisingly,\ndespite employing safety checks, all production-standard models we tested\ndisplay significant gender bias with GPT-3.5-turbo giving biased opinions in\n24% of the samples. Additionally, all models consistently favour female\ncharacters, with GPT showing bias in 68-85% of cases and Llama 3 in around\n81-85% instances. Additionally, our study investigates the impact of model\nparameters on gender bias and explores real-world situations where LLMs reveal\nbiases in moral decision-making.", "published": "2024-10-13 20:19:11", "link": "http://arxiv.org/abs/2410.09992v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LoRE: Logit-Ranked Retriever Ensemble for Enhancing Open-Domain Question\n  Answering", "abstract": "Retrieval-based question answering systems often suffer from positional bias,\nleading to suboptimal answer generation. We propose LoRE (Logit-Ranked\nRetriever Ensemble), a novel approach that improves answer accuracy and\nrelevance by mitigating positional bias. LoRE employs an ensemble of diverse\nretrievers, such as BM25 and sentence transformers with FAISS indexing. A key\ninnovation is a logit-based answer ranking algorithm that combines the logit\nscores from a large language model (LLM), with the retrieval ranks of the\npassages. Experimental results on NarrativeQA, SQuAD demonstrate that LoRE\nsignificantly outperforms existing retrieval-based methods in terms of exact\nmatch and F1 scores. On SQuAD, LoRE achieves 14.5\\%, 22.83\\%, and 14.95\\%\nimprovements over the baselines for ROUGE-L, EM, and F1, respectively.\nQualitatively, LoRE generates more relevant and accurate answers, especially\nfor complex queries.", "published": "2024-10-13 23:06:08", "link": "http://arxiv.org/abs/2410.10042v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Honest AI: Fine-Tuning \"Small\" Language Models to Say \"I Don't Know\",\n  and Reducing Hallucination in RAG", "abstract": "Hallucination is a key roadblock for applications of Large Language Models\n(LLMs), particularly for enterprise applications that are sensitive to\ninformation accuracy. To address this issue, two general approaches have been\nexplored: Retrieval-Augmented Generation (RAG) to supply LLMs with updated\ninformation as context, and fine-tuning the LLMs with new information and\ndesired output styles. In this paper, we propose Honest AI: a novel strategy to\nfine-tune \"small\" language models to say \"I don't know\" to reduce\nhallucination, along with several alternative RAG approaches. The solution\nranked 1st in Task 2 for the false premise question. The alternative approaches\ninclude using RAG with search engine and knowledge graph results, fine-tuning\nbase LLMs with new information and combinations of both approaches. Although\nall approaches improve the performance of the LLMs, RAG alone does not\nsignificantly improve the performance and fine-tuning is needed for better\nresults. Finally, the hybrid approach achieved the highest score in the CRAG\nbenchmark. In addition, our approach emphasizes the use of relatively small\nmodels with fewer than 10 billion parameters, promoting resource efficiency.", "published": "2024-10-13 02:34:47", "link": "http://arxiv.org/abs/2410.09699v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BiDoRA: Bi-level Optimization-Based Weight-Decomposed Low-Rank\n  Adaptation", "abstract": "Parameter-efficient fine-tuning (PEFT) of large language models (LLMs) has\ngained considerable attention as a flexible and efficient way of adapting LLMs\nto downstream tasks. Among these methods, weighted decomposed low-rank\nadaptation (DoRA) has emerged as a promising approach. DoRA bridges the gap\nbetween low-rank adaptation (LoRA) and full fine-tuning (FT) by decomposing the\nweight matrices into magnitude and direction components, thereby maintaining\nlearning behavior similar to FT. Although DoRA shows encouraging performance,\nit introduces additional parameters compared to LoRA, which potentially\nincreases the risk of overfitting. Moreover, optimizing magnitude and direction\nsimultaneously leads to a coupled gradient updating pattern for both\ncomponents, limiting its learning capacity. To overcome these limitations, we\npropose BiDoRA, a bi-level optimization-based PEFT method. In BiDoRA, the\ndirection and magnitude components are optimized on two distinct datasets at\ndifferent optimization levels, mitigating the risk of overfitting.\nAdditionally, the asynchronous optimization of the two components promotes\ntheir decoupling, allowing for more flexible gradient updates suitable for\nvarious downstream tasks. Evaluation of BiDoRA on fourteen datasets spanning\nnatural language understanding, natural language generation, and token\nclassification reveals that it significantly outperforms DoRA and other PEFT\nmethods. The superior performance of BiDoRA underscores its effectiveness. The\ncode for BiDoRA is available at https://anonymous.4open.science/r/BiDoRA-5D31.", "published": "2024-10-13 07:28:26", "link": "http://arxiv.org/abs/2410.09758v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "EasyJudge: an Easy-to-use Tool for Comprehensive Response Evaluation of\n  LLMs", "abstract": "Recently, there has been a growing trend of employing large language models\n(LLMs) to judge the quality of other LLMs. Many studies have adopted\nclosed-source models, mainly using GPT-4 as the evaluator. However, due to the\nclosed-source nature of the GPT-4 model, employing it as an evaluator has\nresulted in issues including transparency, controllability, and\ncost-effectiveness. Some researchers have turned to using fine-tuned\nopen-source LLMs as evaluators. However, existing open-source evaluation LLMs\ngenerally lack a user-friendly visualization tool, and they have not been\noptimized for accelerated model inference, which causes inconvenience for\nresearchers with limited resources and those working across different fields.\nThis paper presents EasyJudge, a model developed to evaluate significant\nlanguage model responses. It is lightweight, precise, efficient, and\nuser-friendly, featuring an intuitive visualization interface for ease of\ndeployment and use. EasyJudge uses detailed datasets and refined prompts for\nmodel optimization, achieving strong consistency with human and proprietary\nmodel evaluations. The model optimized with quantitative methods enables\nEasyJudge to run efficiently on consumer-grade GPUs or even CPUs. We also\nprovide detailed analysis and case studies to further reveal the potential of\nour method.", "published": "2024-10-13 08:24:12", "link": "http://arxiv.org/abs/2410.09775v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "ECIS-VQG: Generation of Entity-centric Information-seeking Questions\n  from Videos", "abstract": "Previous studies on question generation from videos have mostly focused on\ngenerating questions about common objects and attributes and hence are not\nentity-centric. In this work, we focus on the generation of entity-centric\ninformation-seeking questions from videos. Such a system could be useful for\nvideo-based learning, recommending ``People Also Ask'' questions, video-based\nchatbots, and fact-checking. Our work addresses three key challenges:\nidentifying question-worthy information, linking it to entities, and\neffectively utilizing multimodal signals. Further, to the best of our\nknowledge, there does not exist a large-scale dataset for this task. Most video\nquestion generation datasets are on TV shows, movies, or human activities or\nlack entity-centric information-seeking questions. Hence, we contribute a\ndiverse dataset of YouTube videos, VideoQuestions, consisting of 411 videos\nwith 2265 manually annotated questions. We further propose a model architecture\ncombining Transformers, rich context signals (titles, transcripts, captions,\nembeddings), and a combination of cross-entropy and contrastive loss function\nto encourage entity-centric question generation. Our best method yields BLEU,\nROUGE, CIDEr, and METEOR scores of 71.3, 78.6, 7.31, and 81.9, respectively,\ndemonstrating practical usability. We make the code and dataset publicly\navailable. https://github.com/thePhukan/ECIS-VQG", "published": "2024-10-13 08:33:16", "link": "http://arxiv.org/abs/2410.09776v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Expanding Search Space with Diverse Prompting Agents: An Efficient\n  Sampling Approach for LLM Mathematical Reasoning", "abstract": "Large Language Models (LLMs) have exhibited remarkable capabilities in many\ncomplex tasks including mathematical reasoning. However, traditional approaches\nheavily rely on ensuring self-consistency within single prompting method, which\nlimits the exploration of diverse problem-solving strategies. This study\naddresses these limitations by performing an experimental analysis of distinct\nprompting methods within the domain of mathematical reasoning. Our findings\ndemonstrate that each method explores a distinct search space, and this\ndifferentiation becomes more evident with increasing problem complexity. To\nleverage this phenomenon, we applied efficient sampling process that uniformly\ncombines samples from these diverse methods, which not only expands the maximum\nsearch space but achieves higher performance with fewer runs compared to single\nmethods. Especially, within the subset of difficult questions of MATH dataset\nnamed MATH-hard, The maximum search space was achieved while utilizing\napproximately 43% fewer runs than single methods on average. These findings\nhighlight the importance of integrating diverse problem-solving strategies to\nenhance the reasoning abilities of LLMs.", "published": "2024-10-13 08:49:22", "link": "http://arxiv.org/abs/2410.09780v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Single Ground Truth Is Not Enough: Adding Flexibility to Aspect-Based\n  Sentiment Analysis Evaluation", "abstract": "Aspect-based sentiment analysis (ABSA) is a challenging task of extracting\nsentiments along with their corresponding aspects and opinion terms from the\ntext. The inherent subjectivity of span annotation makes variability in the\nsurface forms of extracted terms, complicating the evaluation process.\nTraditional evaluation methods often constrain ground truths (GT) to a single\nterm, potentially misrepresenting the accuracy of semantically valid\npredictions that differ in surface form. To address this limitation, we propose\na novel and fully automated pipeline that expands existing evaluation sets by\nadding alternative valid terms for aspect and opinion. Our approach facilitates\nan equitable assessment of language models by accommodating multiple-answer\ncandidates, resulting in enhanced human agreement compared to single-answer\ntest sets (achieving up to a 10\\%p improvement in Kendall's Tau score).\nExperimental results demonstrate that our expanded evaluation set helps uncover\nthe capabilities of large language models (LLMs) in ABSA tasks, which is\nconcealed by the single-answer GT sets. Consequently, our work contributes to\nthe development of a flexible evaluation framework for ABSA by embracing\ndiverse surface forms to span extraction tasks in a cost-effective and\nreproducible manner. Our code and dataset is open at\nhttps://github.com/dudrrm/zoom-in-n-out-absa.", "published": "2024-10-13 11:48:09", "link": "http://arxiv.org/abs/2410.09807v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Simultaneous Computation and Memory Efficient Zeroth-Order Optimizer for\n  Fine-Tuning Large Language Models", "abstract": "Fine-tuning is powerful for adapting large language models to downstream\ntasks, but it often results in huge memory usages. A promising approach to\nmitigate this is using Zeroth-Order (ZO) optimization, which estimates\ngradients to replace First-Order (FO) gradient calculations, albeit with longer\ntraining time due to its stochastic nature. By revisiting the Memory-efficient\nZO (MeZO) optimizer, we discover that the full-parameter perturbation and\nupdating processes consume over 50% of its overall fine-tuning time cost. Based\non these observations, we introduce a novel layer-wise sparse computation and\nmemory efficient ZO optimizer, named LeZO. LeZO treats layers as fundamental\nunits for sparsification and dynamically perturbs different parameter subsets\nin each step to achieve full-parameter fine-tuning. LeZO incorporates\nlayer-wise parameter sparsity in the process of simultaneous perturbation\nstochastic approximation (SPSA) and ZO stochastic gradient descent (ZO-SGD). It\nachieves accelerated computation during perturbation and updating processes\nwithout additional memory overhead. We conduct extensive experiments with the\nOPT model family on the SuperGLUE benchmark and two generative tasks. The\nexperiments show that LeZO accelerates training without compromising the\nperformance of ZO optimization. Specifically, it achieves over 3x speedup\ncompared to MeZO on the SST-2, BoolQ, and Copa tasks.", "published": "2024-10-13 12:47:37", "link": "http://arxiv.org/abs/2410.09823v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ChroKnowledge: Unveiling Chronological Knowledge of Language Models in\n  Multiple Domains", "abstract": "Large language models (LLMs) have brought significant changes to many aspects\nof our lives. However, assessing and ensuring their chronological knowledge\nremains challenging. Existing approaches fall short in addressing the temporal\nadaptability of knowledge, often relying on a fixed time-point view. To\novercome this, we introduce ChroKnowBench, a benchmark dataset designed to\nevaluate chronologically accumulated knowledge across three key aspects:\nmultiple domains, time dependency, temporal state. Our benchmark distinguishes\nbetween knowledge that evolves (e.g., personal history, scientific discoveries,\namended laws) and knowledge that remain constant (e.g., mathematical truths,\ncommonsense facts). Building on this benchmark, we present ChroKnowledge\n(Chronological Categorization of Knowledge), a novel sampling-based framework\nfor evaluating LLMs' non-parametric chronological knowledge. Our evaluation led\nto the following observations: (1) The ability of eliciting temporal knowledge\nvaries depending on the data format that model was trained on. (2) LLMs\npartially recall knowledge or show a cut-off at temporal boundaries rather than\nrecalling all aspects of knowledge correctly. Thus, we apply our\nChroKnowPrompt, an in-depth prompting to elicit chronological knowledge by\ntraversing step-by-step through the surrounding time spans. We observe that it\nsuccessfully recalls objects across both open-source and proprietary LLMs,\ndemonstrating versatility, though it faces challenges with dynamic datasets and\nunstructured formats.", "published": "2024-10-13 15:08:49", "link": "http://arxiv.org/abs/2410.09870v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Rank for Multiple Retrieval-Augmented Models through\n  Iterative Utility Maximization", "abstract": "This paper investigates the design of a unified search engine to serve\nmultiple retrieval-augmented generation (RAG) agents, each with a distinct\ntask, backbone large language model (LLM), and retrieval-augmentation strategy.\nWe introduce an iterative approach where the search engine generates retrieval\nresults for these RAG agents and gathers feedback on the quality of the\nretrieved documents during an offline phase. This feedback is then used to\niteratively optimize the search engine using a novel expectation-maximization\nalgorithm, with the goal of maximizing each agent's utility function.\nAdditionally, we adapt this approach to an online setting, allowing the search\nengine to refine its behavior based on real-time individual agents feedback to\nbetter serve the results for each of them. Experiments on diverse datasets from\nthe Knowledge-Intensive Language Tasks (KILT) benchmark demonstrates that our\napproach significantly on average outperforms competitive baselines across 18\nRAG models. We also demonstrate that our method effectively ``personalizes''\nthe retrieval process for each RAG agent based on the collected feedback.\nFinally, we provide a comprehensive ablation study to explore various aspects\nof our method.", "published": "2024-10-13 17:53:50", "link": "http://arxiv.org/abs/2410.09942v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "State of NLP in Kenya: A Survey", "abstract": "Kenya, known for its linguistic diversity, faces unique challenges and\npromising opportunities in advancing Natural Language Processing (NLP)\ntechnologies, particularly for its underrepresented indigenous languages. This\nsurvey provides a detailed assessment of the current state of NLP in Kenya,\nemphasizing ongoing efforts in dataset creation, machine translation, sentiment\nanalysis, and speech recognition for local dialects such as Kiswahili, Dholuo,\nKikuyu, and Luhya. Despite these advancements, the development of NLP in Kenya\nremains constrained by limited resources and tools, resulting in the\nunderrepresentation of most indigenous languages in digital spaces. This paper\nuncovers significant gaps by critically evaluating the available datasets and\nexisting NLP models, most notably the need for large-scale language models and\nthe insufficient digital representation of Indigenous languages. We also\nanalyze key NLP applications: machine translation, information retrieval, and\nsentiment analysis-examining how they are tailored to address local linguistic\nneeds. Furthermore, the paper explores the governance, policies, and\nregulations shaping the future of AI and NLP in Kenya and proposes a strategic\nroadmap to guide future research and development efforts. Our goal is to\nprovide a foundation for accelerating the growth of NLP technologies that meet\nKenya's diverse linguistic demands.", "published": "2024-10-13 18:08:24", "link": "http://arxiv.org/abs/2410.09948v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "When Neutral Summaries are not that Neutral: Quantifying Political\n  Neutrality in LLM-Generated News Summaries", "abstract": "In an era where societal narratives are increasingly shaped by algorithmic\ncuration, investigating the political neutrality of LLMs is an important\nresearch question. This study presents a fresh perspective on quantifying the\npolitical neutrality of LLMs through the lens of abstractive text summarization\nof polarizing news articles. We consider five pressing issues in current US\npolitics: abortion, gun control/rights, healthcare, immigration, and LGBTQ+\nrights. Via a substantial corpus of 20,344 news articles, our study reveals a\nconsistent trend towards pro-Democratic biases in several well-known LLMs, with\ngun control and healthcare exhibiting the most pronounced biases (max\npolarization differences of -9.49% and -6.14%, respectively). Further analysis\nuncovers a strong convergence in the vocabulary of the LLM outputs for these\ndivisive topics (55% overlap for Democrat-leaning representations, 52% for\nRepublican). Being months away from a US election of consequence, we consider\nour findings important.", "published": "2024-10-13 19:44:39", "link": "http://arxiv.org/abs/2410.09978v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Self-Data Distillation for Recovering Quality in Pruned Large Language\n  Models", "abstract": "Large language models have driven significant progress in natural language\nprocessing, but their deployment requires substantial compute and memory\nresources. As models scale, compression techniques become essential for\nbalancing model quality with computational efficiency. Structured pruning,\nwhich removes less critical components of the model, is a promising strategy\nfor reducing complexity. However, one-shot pruning often results in significant\nquality degradation, particularly in tasks requiring multi-step reasoning. To\nrecover lost quality, supervised fine-tuning (SFT) is commonly applied, but it\ncan lead to catastrophic forgetting by shifting the model's learned data\ndistribution. Therefore, addressing the degradation from both pruning and SFT\nis essential to preserve the original model's quality. In this work, we utilize\nself-data distilled fine-tuning to address these challenges. Our approach\nleverages the original, unpruned model to generate a distilled dataset that\npreserves semantic richness and mitigates catastrophic forgetting by\nmaintaining alignment with the base model's knowledge. Empirically, we\ndemonstrate that self-data distillation consistently outperforms standard SFT,\nimproving average accuracy by up to 8% on the HuggingFace OpenLLM Leaderboard\nv1. Specifically, when pruning six decoder blocks on Llama3.1-8B Instruct\n(i.e., 32 to 26 layers, reducing the model size from 8.03B to 6.72B\nparameters), our method retains 91.2% of the original model's accuracy compared\nto 81.7% with SFT, while reducing real-world FLOPs by 16.3%. Furthermore,\ncombining self-data distilled models through model merging yields enhanced\nquality retention. Additionally, leveraging these pruned models in speculative\ndecoding increases token acceptance rates, thereby improving inference\nefficiency in applied settings.", "published": "2024-10-13 19:53:40", "link": "http://arxiv.org/abs/2410.09982v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Safety-Aware Fine-Tuning of Large Language Models", "abstract": "Fine-tuning Large Language Models (LLMs) has emerged as a common practice for\ntailoring models to individual needs and preferences. The choice of datasets\nfor fine-tuning can be diverse, introducing safety concerns regarding the\npotential inclusion of harmful data samples. Manually filtering or avoiding\nsuch samples, however, can be labor-intensive and subjective. To address these\ndifficulties, we propose a novel Safety-Aware Fine-Tuning (SAFT) framework\ndesigned to automatically detect and remove potentially harmful data, by\nleveraging a scoring function that exploits the subspace information of harmful\nand benign samples. Experimental results demonstrate the efficacy of SAFT\nacross different LLMs and varying contamination rates, achieving reductions in\nharmfulness of up to 27.8%. Going beyond, we delve into the mechanism of our\napproach and validate its versatility in addressing practical challenges in\nreal-world scenarios.", "published": "2024-10-13 21:24:25", "link": "http://arxiv.org/abs/2410.10014v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Step Towards Mixture of Grader: Statistical Analysis of Existing\n  Automatic Evaluation Metrics", "abstract": "The explosion of open-sourced models and Question-Answering (QA) datasets\nemphasizes the importance of automated QA evaluation. We studied the statistics\nof the existing evaluation metrics for a better understanding of their\nlimitations. By measuring the correlation coefficients of each evaluation\nmetric concerning human-like evaluation score, we observed the following: (1)\nexisting metrics have a high correlation among them concerning the question\ntype (e.g., single word, single phrase, etc.), (2) no single metric can\nadequately estimate the human-like evaluation. As a potential solution, we\ndiscuss how a Mixture Of Grader could potentially improve the auto QA evaluator\nquality.", "published": "2024-10-13 22:10:42", "link": "http://arxiv.org/abs/2410.10030v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Investigating Implicit Bias in Large Language Models: A Large-Scale\n  Study of Over 50 LLMs", "abstract": "Large Language Models (LLMs) are being adopted across a wide range of tasks,\nincluding decision-making processes in industries where bias in AI systems is a\nsignificant concern. Recent research indicates that LLMs can harbor implicit\nbiases even when they pass explicit bias evaluations. Building upon the\nframeworks of the LLM Implicit Association Test (IAT) Bias and LLM Decision\nBias, this study highlights that newer or larger language models do not\nautomatically exhibit reduced bias; in some cases, they displayed higher bias\nscores than their predecessors, such as in Meta's Llama series and OpenAI's GPT\nmodels. This suggests that increasing model complexity without deliberate bias\nmitigation strategies can unintentionally amplify existing biases. The\nvariability in bias scores within and across providers underscores the need for\nstandardized evaluation metrics and benchmarks for bias assessment. The lack of\nconsistency indicates that bias mitigation is not yet a universally prioritized\ngoal in model development, which can lead to unfair or discriminatory outcomes.\nBy broadening the detection of implicit bias, this research provides a more\ncomprehensive understanding of the biases present in advanced models and\nunderscores the critical importance of addressing these issues to ensure the\ndevelopment of fair and responsible AI systems.", "published": "2024-10-13 03:43:18", "link": "http://arxiv.org/abs/2410.12864v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Empowering Dysarthric Speech: Leveraging Advanced LLMs for Accurate\n  Speech Correction and Multimodal Emotion Analysis", "abstract": "Dysarthria is a motor speech disorder caused by neurological damage that\naffects the muscles used for speech production, leading to slurred, slow, or\ndifficult-to-understand speech. It affects millions of individuals worldwide,\nincluding those with conditions such as stroke, traumatic brain injury,\ncerebral palsy, Parkinsons disease, and multiple sclerosis. Dysarthria presents\na major communication barrier, impacting quality of life and social\ninteraction. This paper introduces a novel approach to recognizing and\ntranslating dysarthric speech, empowering individuals with this condition to\ncommunicate more effectively. We leverage advanced large language models for\naccurate speech correction and multimodal emotion analysis. Dysarthric speech\nis first converted to text using OpenAI Whisper model, followed by sentence\nprediction using fine-tuned open-source models and benchmark models like\nGPT-4.o, LLaMA 3.1 70B and Mistral 8x7B on Groq AI accelerators. The dataset\nused combines the TORGO dataset with Google speech data, manually labeled for\nemotional context. Our framework identifies emotions such as happiness,\nsadness, neutrality, surprise, anger, and fear, while reconstructing intended\nsentences from distorted speech with high accuracy. This approach demonstrates\nsignificant advancements in the recognition and interpretation of dysarthric\nspeech.", "published": "2024-10-13 20:54:44", "link": "http://arxiv.org/abs/2410.12867v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Author Unknown: Evaluating Performance of Author Extraction Libraries on\n  Global Online News Articles", "abstract": "Analysis of large corpora of online news content requires robust validation\nof underlying metadata extraction methodologies. Identifying the author of a\ngiven web-based news article is one example that enables various types of\nresearch questions. While numerous solutions for off-the-shelf author\nextraction exist, there is little work comparing performance (especially in\nmultilingual settings). In this paper we present a manually coded cross-lingual\ndataset of authors of online news articles and use it to evaluate the\nperformance of five existing software packages and one customized model. Our\nevaluation shows evidence for Go-readability and Trafilatura as the most\nconsistent solutions for author extraction, but we find all packages produce\nhighly variable results across languages. These findings are relevant for\nresearchers wishing to utilize author data in their analysis pipelines,\nprimarily indicating that further validation for specific languages and\ngeographies is required to rely on results.", "published": "2024-10-13 20:19:15", "link": "http://arxiv.org/abs/2410.19771v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "MoIN: Mixture of Introvert Experts to Upcycle an LLM", "abstract": "The goal of this paper is to improve (upcycle) an existing large language\nmodel without the prohibitive requirements of continued pre-training of the\nfull-model. The idea is to split the pre-training data into semantically\nrelevant groups and train an expert on each subset. An expert takes the form of\na lightweight adapter added on the top of a frozen base model. During\ninference, an incoming query is first routed to the most relevant expert which\nis then loaded onto the base model for the forward pass. Unlike typical Mixture\nof Experts (MoE) models, the experts in our method do not work with other\nexperts for a single query. Hence, we dub them \"introvert\" experts. Freezing\nthe base model and keeping the experts as lightweight adapters allows extreme\nparallelism during training and inference. Training of all experts can be done\nin parallel without any communication channels between them. Similarly, the\ninference can also be heavily parallelized by distributing experts on different\nGPUs and routing each request to the GPU containing its relevant expert. We\nimplement a proof-of-concept version of this method and show the validity of\nour approach.", "published": "2024-10-13 01:11:04", "link": "http://arxiv.org/abs/2410.09687v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "'Quis custodiet ipsos custodes?' Who will watch the watchmen? On\n  Detecting AI-generated peer-reviews", "abstract": "The integrity of the peer-review process is vital for maintaining scientific\nrigor and trust within the academic community. With the steady increase in the\nusage of large language models (LLMs) like ChatGPT in academic writing, there\nis a growing concern that AI-generated texts could compromise scientific\npublishing, including peer-reviews. Previous works have focused on generic\nAI-generated text detection or have presented an approach for estimating the\nfraction of peer-reviews that can be AI-generated. Our focus here is to solve a\nreal-world problem by assisting the editor or chair in determining whether a\nreview is written by ChatGPT or not. To address this, we introduce the Term\nFrequency (TF) model, which posits that AI often repeats tokens, and the Review\nRegeneration (RR) model, which is based on the idea that ChatGPT generates\nsimilar outputs upon re-prompting. We stress test these detectors against token\nattack and paraphrasing. Finally, we propose an effective defensive strategy to\nreduce the effect of paraphrasing on our models. Our findings suggest both our\nproposed methods perform better than the other AI text detectors. Our RR model\nis more robust, although our TF model performs better than the RR model without\nany attacks. We make our code, dataset, and model public.", "published": "2024-10-13 08:06:08", "link": "http://arxiv.org/abs/2410.09770v1", "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generating Driving Simulations via Conversation", "abstract": "Cyber-physical systems like autonomous vehicles are tested in simulation\nbefore deployment, using domain-specific programs for scenario specification.\nTo aid the testing of autonomous vehicles in simulation, we design a natural\nlanguage interface, using an instruction-following large language model, to\nassist a non-coding domain expert in synthesising the desired scenarios and\nvehicle behaviours. We show that using it to convert utterances to the symbolic\nprogram is feasible, despite the very small training dataset. Human experiments\nshow that dialogue is critical to successful simulation generation, leading to\na 4.5 times higher success rate than a generation without engaging in extended\nconversation.", "published": "2024-10-13 13:07:31", "link": "http://arxiv.org/abs/2410.09829v1", "categories": ["cs.CL", "cs.IR", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Retrieval Instead of Fine-tuning: A Retrieval-based Parameter Ensemble\n  for Zero-shot Learning", "abstract": "Foundation models have become a cornerstone in deep learning, with techniques\nlike Low-Rank Adaptation (LoRA) offering efficient fine-tuning of large models.\nSimilarly, methods such as Retrieval-Augmented Generation (RAG), which leverage\nvectorized databases, have further improved model performance by grounding\noutputs in external information. While these approaches have demonstrated\nnotable success, they often require extensive training or labeled data, which\ncan limit their adaptability in resource-constrained environments. To address\nthese challenges, we introduce Retrieval-based Parameter Ensemble (RPE), a new\nmethod that creates a vectorized database of LoRAs, enabling efficient\nretrieval and application of model adaptations to new tasks. RPE minimizes the\nneed for extensive training and eliminates the requirement for labeled data,\nmaking it particularly effective for zero-shot learning. Additionally, RPE is\nwell-suited for privacy-sensitive domains like healthcare, as it modifies model\nparameters without accessing raw data. When applied to tasks such as medical\nreport generation and image segmentation, RPE not only proved effective but\nalso surpassed supervised fine-tuning methods in certain cases, highlighting\nits potential to enhance both computational efficiency and privacy in deep\nlearning applications.", "published": "2024-10-13 16:28:38", "link": "http://arxiv.org/abs/2410.09908v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Collu-Bench: A Benchmark for Predicting Language Model Hallucinations in\n  Code", "abstract": "Despite their success, large language models (LLMs) face the critical\nchallenge of hallucinations, generating plausible but incorrect content. While\nmuch research has focused on hallucinations in multiple modalities including\nimages and natural language text, less attention has been given to\nhallucinations in source code, which leads to incorrect and vulnerable code\nthat causes significant financial loss. To pave the way for research in LLMs'\nhallucinations in code, we introduce Collu-Bench, a benchmark for predicting\ncode hallucinations of LLMs across code generation (CG) and automated program\nrepair (APR) tasks. Collu-Bench includes 13,234 code hallucination instances\ncollected from five datasets and 11 diverse LLMs, ranging from open-source\nmodels to commercial ones. To better understand and predict code\nhallucinations, Collu-Bench provides detailed features such as the per-step log\nprobabilities of LLMs' output, token types, and the execution feedback of LLMs'\ngenerated code for in-depth analysis. In addition, we conduct experiments to\npredict hallucination on Collu-Bench, using both traditional machine learning\ntechniques and neural networks, which achieves 22.03 -- 33.15% accuracy. Our\nexperiments draw insightful findings of code hallucination patterns, reveal the\nchallenge of accurately localizing LLMs' hallucinations, and highlight the need\nfor more sophisticated techniques.", "published": "2024-10-13 20:41:47", "link": "http://arxiv.org/abs/2410.09997v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Leveraging Customer Feedback for Multi-modal Insight Extraction", "abstract": "Businesses can benefit from customer feedback in different modalities, such\nas text and images, to enhance their products and services. However, it is\ndifficult to extract actionable and relevant pairs of text segments and images\nfrom customer feedback in a single pass. In this paper, we propose a novel\nmulti-modal method that fuses image and text information in a latent space and\ndecodes it to extract the relevant feedback segments using an image-text\ngrounded text decoder. We also introduce a weakly-supervised data generation\ntechnique that produces training data for this task. We evaluate our model on\nunseen data and demonstrate that it can effectively mine actionable insights\nfrom multi-modal customer feedback, outperforming the existing baselines by\n$14$ points in F1 score.", "published": "2024-10-13 20:45:00", "link": "http://arxiv.org/abs/2410.09999v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR"], "primary_category": "cs.CL"}
{"title": "3DS: Decomposed Difficulty Data Selection's Case Study on LLM Medical\n  Domain Adaptation", "abstract": "Large Language Models(LLMs) excel in general tasks but struggle in\nspecialized domains like healthcare due to limited domain-specific\nknowledge.Supervised Fine-Tuning(SFT) data construction for domain adaptation\noften relies on heuristic methods, such as GPT-4 annotation or manual data\nselection, with a data-centric focus on presumed diverse, high-quality\ndatasets. However, these methods overlook the model's inherent knowledge\ndistribution, introducing noise, redundancy, and irrelevant data, leading to a\nmismatch between the selected data and the model's learning task, resulting in\nsuboptimal performance. To address this, we propose a two-stage model-centric\ndata selection framework, Decomposed Difficulty Data Selection (3DS), which\naligns data with the model's knowledge distribution for optimized adaptation.\nIn Stage1, we apply Prompt-Driven Data Selection via Explicit Alignment, where\nthe the model filters irrelevant or redundant data based on its internal\nknowledge. In Stage2, we perform Decomposed Difficulty Data Selection, where\ndata selection is guided by our defined difficulty decomposition, using three\nmetrics: Instruction Understanding, Response Confidence, and Response\nCorrectness. Additionally, an attention-based importance weighting mechanism\ncaptures token importance for more accurate difficulty calibration. This\ntwo-stage approach ensures the selected data is not only aligned with the\nmodel's knowledge and preferences but also appropriately challenging for the\nmodel to learn, leading to more effective and targeted domain adaptation. In\nthe case study of the medical domain, our extensive experiments on real-world\nhealthcare datasets demonstrate the superiority of 3DS over exisiting methods\nin accuracy by over 5.29%. Our dataset and code will be open-sourced at\nhttps://anonymous.4open.science/r/3DS-E67F.", "published": "2024-10-13 02:29:00", "link": "http://arxiv.org/abs/2410.10901v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ELF-Gym: Evaluating Large Language Models Generated Features for Tabular\n  Prediction", "abstract": "Crafting effective features is a crucial yet labor-intensive and\ndomain-specific task within machine learning pipelines. Fortunately, recent\nadvancements in Large Language Models (LLMs) have shown promise in automating\nvarious data science tasks, including feature engineering. But despite this\npotential, evaluations thus far are primarily based on the end performance of a\ncomplete ML pipeline, providing limited insight into precisely how LLMs behave\nrelative to human experts in feature engineering. To address this gap, we\npropose ELF-Gym, a framework for Evaluating LLM-generated Features. We curated\na new dataset from historical Kaggle competitions, including 251 \"golden\"\nfeatures used by top-performing teams. ELF-Gym then quantitatively evaluates\nLLM-generated features by measuring their impact on downstream model\nperformance as well as their alignment with expert-crafted features through\nsemantic and functional similarity assessments. This approach provides a more\ncomprehensive evaluation of disparities between LLMs and human experts, while\noffering valuable insights into specific areas where LLMs may have room for\nimprovement. For example, using ELF-Gym we empirically demonstrate that, in the\nbest-case scenario, LLMs can semantically capture approximately 56% of the\ngolden features, but at the more demanding implementation level this overlap\ndrops to 13%. Moreover, in other cases LLMs may fail completely, particularly\non datasets that require complex features, indicating broad potential pathways\nfor improvement.", "published": "2024-10-13 13:59:33", "link": "http://arxiv.org/abs/2410.12865v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "IMAS: A Comprehensive Agentic Approach to Rural Healthcare Delivery", "abstract": "Since the onset of COVID-19, rural communities worldwide have faced\nsignificant challenges in accessing healthcare due to the migration of\nexperienced medical professionals to urban centers. Semi-trained caregivers,\nsuch as Community Health Workers (CHWs) and Registered Medical Practitioners\n(RMPs), have stepped in to fill this gap, but often lack formal training. This\npaper proposes an advanced agentic medical assistant system designed to improve\nhealthcare delivery in rural areas by utilizing Large Language Models (LLMs)\nand agentic approaches. The system is composed of five crucial components:\ntranslation, medical complexity assessment, expert network integration, final\nmedical advice generation, and response simplification. Our innovative\nframework ensures context-sensitive, adaptive, and reliable medical assistance,\ncapable of clinical triaging, diagnostics, and identifying cases requiring\nspecialist intervention. The system is designed to handle cultural nuances and\nvarying literacy levels, providing clear and actionable medical advice in local\nlanguages. Evaluation results using the MedQA, PubMedQA, and JAMA datasets\ndemonstrate that this integrated approach significantly enhances the\neffectiveness of rural healthcare workers, making healthcare more accessible\nand understandable for underserved populations. All code and supplemental\nmaterials associated with the paper and IMAS are available at\nhttps://github.com/uheal/imas.", "published": "2024-10-13 23:07:11", "link": "http://arxiv.org/abs/2410.12868v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "BlackDAN: A Black-Box Multi-Objective Approach for Effective and\n  Contextual Jailbreaking of Large Language Models", "abstract": "While large language models (LLMs) exhibit remarkable capabilities across\nvarious tasks, they encounter potential security risks such as jailbreak\nattacks, which exploit vulnerabilities to bypass security measures and generate\nharmful outputs. Existing jailbreak strategies mainly focus on maximizing\nattack success rate (ASR), frequently neglecting other critical factors,\nincluding the relevance of the jailbreak response to the query and the level of\nstealthiness. This narrow focus on single objectives can result in ineffective\nattacks that either lack contextual relevance or are easily recognizable. In\nthis work, we introduce BlackDAN, an innovative black-box attack framework with\nmulti-objective optimization, aiming to generate high-quality prompts that\neffectively facilitate jailbreaking while maintaining contextual relevance and\nminimizing detectability. BlackDAN leverages Multiobjective Evolutionary\nAlgorithms (MOEAs), specifically the NSGA-II algorithm, to optimize jailbreaks\nacross multiple objectives including ASR, stealthiness, and semantic relevance.\nBy integrating mechanisms like mutation, crossover, and Pareto-dominance,\nBlackDAN provides a transparent and interpretable process for generating\njailbreaks. Furthermore, the framework allows customization based on user\npreferences, enabling the selection of prompts that balance harmfulness,\nrelevance, and other factors. Experimental results demonstrate that BlackDAN\noutperforms traditional single-objective methods, yielding higher success rates\nand improved robustness across various LLMs and multimodal LLMs, while ensuring\njailbreak responses are both relevant and less detectable.", "published": "2024-10-13 11:15:38", "link": "http://arxiv.org/abs/2410.09804v3", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CR"}
{"title": "Towards Homogeneous Lexical Tone Decoding from Heterogeneous\n  Intracranial Recordings", "abstract": "Recent advancements in brain-computer interfaces (BCIs) have enabled the\ndecoding of lexical tones from intracranial recordings, offering the potential\nto restore the communication abilities of speech-impaired tonal language\nspeakers. However, data heterogeneity induced by both physiological and\ninstrumental factors poses a significant challenge for unified invasive brain\ntone decoding. Traditional subject-specific models, which operate under a\nheterogeneous decoding paradigm, fail to capture generalized neural\nrepresentations and cannot effectively leverage data across subjects. To\naddress these limitations, we introduce Homogeneity-Heterogeneity Disentangled\nLearning for neural Representations (H2DiLR), a novel framework that\ndisentangles and learns both the homogeneity and heterogeneity from\nintracranial recordings across multiple subjects. To evaluate H2DiLR, we\ncollected stereoelectroencephalography (sEEG) data from multiple participants\nreading Mandarin materials comprising 407 syllables, representing nearly all\nMandarin characters. Extensive experiments demonstrate that H2DiLR, as a\nunified decoding paradigm, significantly outperforms the conventional\nheterogeneous decoding approach. Furthermore, we empirically confirm that\nH2DiLR effectively captures both homogeneity and heterogeneity during neural\nrepresentation learning.", "published": "2024-10-13 18:09:12", "link": "http://arxiv.org/abs/2410.12866v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "LEAD Dataset: How Can Labels for Sound Event Detection Vary Depending on\n  Annotators?", "abstract": "In this paper, we introduce a LargE-scale Annotator's labels for sound event\nDetection (LEAD) dataset, which is the dataset used to gain a better\nunderstanding of the variation in strong labels in sound event detection (SED).\nIn SED, it is very time-consuming to collect large-scale strong labels, and in\nmost cases, multiple workers divide up the annotations to create a single\ndataset. In general, strong labels created by multiple annotators have large\nvariations in the type of sound events and temporal onset/offset. Through the\nannotations of multiple workers, uniquely determining the strong label is quite\ndifficult because the dataset contains sounds that can be mistaken for similar\nclasses and sounds whose temporal onset/offset is difficult to distinguish. If\nthe strong labels of SED vary greatly depending on the annotator, the SED model\ntrained on a dataset created by multiple annotators will be biased. Moreover,\nif annotators differ between training and evaluation data, there is a risk that\nthe model cannot be evaluated correctly. To investigate the variation in strong\nlabels, we release the LEAD dataset, which provides distinct strong labels for\neach clip annotated by 20 different annotators. The LEAD dataset allows us to\ninvestigate how strong labels vary from annotator to annotator and consider SED\nmodels that are robust to the variation of strong labels. The LEAD dataset\nconsists of strong labels assigned to sound clips from TUT Sound Events\n2016/2017, TUT Acoustic Scenes 2016, and URBAN-SED. We also analyze variations\nin the strong labels in the LEAD dataset and provide insights into the\nvariations.", "published": "2024-10-13 08:40:52", "link": "http://arxiv.org/abs/2410.09778v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Novel Numerical Method for Relaxing the Minimal Configurations of\n  TOA-Based Joint Sensors and Sources Localization", "abstract": "This work introduces a novel numerical method that relaxes the minimal\nconfiguration requirements for joint sensors and sources localization (JSSL) in\n3D space using time of arrival (TOA) measurements. Traditionally, the principle\nrequires that the number of valid equations (TOA measurements) must be equal to\nor greater than the number of unknown variables (sensor and source locations).\nState-of-the-art literature suggests that the minimum numbers of sensors and\nsources needed for localization are four to six and six to four, respectively.\nHowever, these stringent configurations limit the application of JSSL in\nscenarios with an insufficient number of sensors and sources. To overcome this\nlimitation, we propose a numerical method that reduces the required number of\nsensors and sources, enabling more flexible JSSL configurations. First, we\nformulate the JSSL task as a series of triangles and apply the law of cosines\nto determine four unknown distances associated with one pair of sensors and\nthree pairs of sources. Next, by utilizing triangle inequalities, we establish\nthe lower and upper boundaries for these unknowns based on the known TOA\nmeasurements. The numerical method then searches within these boundaries to\nfind the global optimal solutions, demonstrating that JSSL in 3D space is\nachievable with only four sensors and four sources, thus significantly relaxing\nthe minimal configuration requirements. Theoretical proofs and simulation\nresults confirm the feasibility and effectiveness of the proposed method.", "published": "2024-10-13 20:38:01", "link": "http://arxiv.org/abs/2410.19772v1", "categories": ["eess.SP", "eess.AS"], "primary_category": "eess.SP"}
{"title": "M2M-Gen: A Multimodal Framework for Automated Background Music\n  Generation in Japanese Manga Using Large Language Models", "abstract": "This paper introduces M2M Gen, a multi modal framework for generating\nbackground music tailored to Japanese manga. The key challenges in this task\nare the lack of an available dataset or a baseline. To address these\nchallenges, we propose an automated music generation pipeline that produces\nbackground music for an input manga book. Initially, we use the dialogues in a\nmanga to detect scene boundaries and perform emotion classification using the\ncharacters faces within a scene. Then, we use GPT4o to translate this low level\nscene information into a high level music directive. Conditioned on the scene\ninformation and the music directive, another instance of GPT 4o generates page\nlevel music captions to guide a text to music model. This produces music that\nis aligned with the mangas evolving narrative. The effectiveness of M2M Gen is\nconfirmed through extensive subjective evaluations, showcasing its capability\nto generate higher quality, more relevant and consistent music that complements\nspecific scenes when compared to our baselines.", "published": "2024-10-13 17:15:59", "link": "http://arxiv.org/abs/2410.09928v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Prompt Tuning for Audio Deepfake Detection: Computationally Efficient\n  Test-time Domain Adaptation with Limited Target Dataset", "abstract": "We study test-time domain adaptation for audio deepfake detection (ADD),\naddressing three challenges: (i) source-target domain gaps, (ii) limited target\ndataset size, and (iii) high computational costs. We propose an ADD method\nusing prompt tuning in a plug-in style. It bridges domain gaps by integrating\nit seamlessly with state-of-the-art transformer models and/or with other\nfine-tuning methods, boosting their performance on target data (challenge (i)).\nIn addition, our method can fit small target datasets because it does not\nrequire a large number of extra parameters (challenge (ii)). This feature also\ncontributes to computational efficiency, countering the high computational\ncosts typically associated with large-scale pre-trained models in ADD\n(challenge (iii)). We conclude that prompt tuning for ADD under domain gaps\npresents a promising avenue for enhancing accuracy with minimal target data and\nnegligible extra computational burden.", "published": "2024-10-13 15:07:35", "link": "http://arxiv.org/abs/2410.09869v1", "categories": ["cs.SD", "cs.AI", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
