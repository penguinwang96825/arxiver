{"title": "End-to-End Knowledge-Routed Relational Dialogue System for Automatic\n  Diagnosis", "abstract": "Beyond current conversational chatbots or task-oriented dialogue systems that\nhave attracted increasing attention, we move forward to develop a dialogue\nsystem for automatic medical diagnosis that converses with patients to collect\nadditional symptoms beyond their self-reports and automatically makes a\ndiagnosis. Besides the challenges for conversational dialogue systems (e.g.\ntopic transition coherency and question understanding), automatic medical\ndiagnosis further poses more critical requirements for the dialogue rationality\nin the context of medical knowledge and symptom-disease relations. Existing\ndialogue systems (Madotto, Wu, and Fung 2018; Wei et al. 2018; Li et al. 2017)\nmostly rely on data-driven learning and cannot be able to encode extra expert\nknowledge graph. In this work, we propose an End-to-End Knowledge-routed\nRelational Dialogue System (KR-DS) that seamlessly incorporates rich medical\nknowledge graph into the topic transition in dialogue management, and makes it\ncooperative with natural language understanding and natural language\ngeneration. A novel Knowledge-routed Deep Q-network (KR-DQN) is introduced to\nmanage topic transitions, which integrates a relational refinement branch for\nencoding relations among different symptoms and symptom-disease pairs, and a\nknowledge-routed graph branch for topic decision-making. Extensive experiments\non a public medical dialogue dataset show our KR-DS significantly beats\nstate-of-the-art methods (by more than 8% in diagnosis accuracy). We further\nshow the superiority of our KR-DS on a newly collected medical dialogue system\ndataset, which is more challenging retaining original self-reports and\nconversational data between patients and doctors.", "published": "2019-01-30 00:34:05", "link": "http://arxiv.org/abs/1901.10623v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effective weakly supervised semantic frame induction using expression\n  sharing in hierarchical hidden Markov models", "abstract": "We present a framework for the induction of semantic frames from utterances\nin the context of an adaptive command-and-control interface. The system is\ntrained on an individual user's utterances and the corresponding semantic\nframes representing controls. During training, no prior information on the\nalignment between utterance segments and frame slots and values is available.\nIn addition, semantic frames in the training data can contain information that\nis not expressed in the utterances. To tackle this weakly supervised\nclassification task, we propose a framework based on Hidden Markov Models\n(HMMs). Structural modifications, resulting in a hierarchical HMM, and an\nextension called expression sharing are introduced to minimize the amount of\ntraining time and effort required for the user.\n  The dataset used for the present study is PATCOR, which contains commands\nuttered in the context of a vocally guided card game, Patience. Experiments\nwere carried out on orthographic and phonetic transcriptions of commands,\nsegmented on different levels of n-gram granularity. The experimental results\nshow positive effects of all the studied system extensions, with some effect\ndifferences between the different input representations. Moreover, evaluation\nexperiments on held-out data with the optimal system configuration show that\nthe extended system is able to achieve high accuracies with relatively small\namounts of training data.", "published": "2019-01-30 05:49:17", "link": "http://arxiv.org/abs/1901.10680v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reference-less Quality Estimation of Text Simplification Systems", "abstract": "The evaluation of text simplification (TS) systems remains an open challenge.\nAs the task has common points with machine translation (MT), TS is often\nevaluated using MT metrics such as BLEU. However, such metrics require high\nquality reference data, which is rarely available for TS. TS has the advantage\nover MT of being a monolingual task, which allows for direct comparisons to be\nmade between the simplified text and its original version. In this paper, we\ncompare multiple approaches to reference-less quality estimation of\nsentence-level text simplification systems, based on the dataset used for the\nQATS 2016 shared task. We distinguish three different dimensions:\ngram-maticality, meaning preservation and simplicity. We show that n-gram-based\nMT metrics such as BLEU and METEOR correlate the most with human judgment of\ngrammaticality and meaning preservation, whereas simplicity is best evaluated\nby basic length-based metrics.", "published": "2019-01-30 10:21:04", "link": "http://arxiv.org/abs/1901.10746v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Span Model for Open Information Extraction on Accurate Corpus", "abstract": "Open information extraction (Open IE) is a challenging task especially due to\nits brittle data basis. Most of Open IE systems have to be trained on\nautomatically built corpus and evaluated on inaccurate test set. In this work,\nwe first alleviate this difficulty from both sides of training and test sets.\nFor the former, we propose an improved model design to more sufficiently\nexploit training dataset. For the latter, we present our accurately\nre-annotated benchmark test set (Re-OIE6) according to a series of linguistic\nobservation and analysis. Then, we introduce a span model instead of previous\nadopted sequence labeling formulization for n-ary Open IE. Our newly introduced\nmodel achieves new state-of-the-art performance on both benchmark evaluation\ndatasets.", "published": "2019-01-30 15:04:16", "link": "http://arxiv.org/abs/1901.10879v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Twitter Job/Employment Corpus: A Dataset of Job-Related Discourse Built\n  with Humans in the Loop", "abstract": "We present the Twitter Job/Employment Corpus, a collection of tweets\nannotated by a humans-in-the-loop supervised learning framework that integrates\ncrowdsourcing contributions and expertise on the local community and employment\nenvironment. Previous computational studies of job-related phenomena have used\ncorpora collected from workplace social media that are hosted internally by the\nemployers, and so lacks independence from latent job-related coercion and the\nbroader context that an open domain, general-purpose medium such as Twitter\nprovides. Our new corpus promises to be a benchmark for the extraction of\njob-related topics and advanced analysis and modeling, and can potentially\nbenefit a wide range of research communities in the future.", "published": "2019-01-30 00:01:14", "link": "http://arxiv.org/abs/1901.10619v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Tensorized Embedding Layers for Efficient Model Compression", "abstract": "The embedding layers transforming input words into real vectors are the key\ncomponents of deep neural networks used in natural language processing.\nHowever, when the vocabulary is large, the corresponding weight matrices can be\nenormous, which precludes their deployment in a limited resource setting. We\nintroduce a novel way of parametrizing embedding layers based on the Tensor\nTrain (TT) decomposition, which allows compressing the model significantly at\nthe cost of a negligible drop or even a slight gain in performance. We evaluate\nour method on a wide range of benchmarks in natural language processing and\nanalyze the trade-off between performance and compression ratios for a wide\nrange of architectures, from MLPs to LSTMs and Transformers.", "published": "2019-01-30 12:43:50", "link": "http://arxiv.org/abs/1901.10787v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Compositionality for Recursive Neural Networks", "abstract": "Modelling compositionality has been a longstanding area of research in the\nfield of vector space semantics. The categorical approach to compositionality\nmaps grammar onto vector spaces in a principled way, but comes under fire for\nrequiring the formation of very high-dimensional matrices and tensors, and\ntherefore being computationally infeasible. In this paper I show how a linear\nsimplification of recursive neural tensor network models can be mapped directly\nonto the categorical approach, giving a way of computing the required matrices\nand tensors. This mapping suggests a number of lines of research for both\ncategorical compositional vector space models of meaning and for recursive\nneural network models of compositionality.", "published": "2019-01-30 09:32:51", "link": "http://arxiv.org/abs/1901.10723v1", "categories": ["cs.CL", "cs.AI", "cs.NE", "math.CT"], "primary_category": "cs.CL"}
{"title": "The Evolved Transformer", "abstract": "Recent works have highlighted the strength of the Transformer architecture on\nsequence tasks while, at the same time, neural architecture search (NAS) has\nbegun to outperform human-designed models. Our goal is to apply NAS to search\nfor a better alternative to the Transformer. We first construct a large search\nspace inspired by the recent advances in feed-forward sequence models and then\nrun evolutionary architecture search with warm starting by seeding our initial\npopulation with the Transformer. To directly search on the computationally\nexpensive WMT 2014 English-German translation task, we develop the Progressive\nDynamic Hurdles method, which allows us to dynamically allocate more resources\nto more promising candidate models. The architecture found in our experiments\n-- the Evolved Transformer -- demonstrates consistent improvement over the\nTransformer on four well-established language tasks: WMT 2014 English-German,\nWMT 2014 English-French, WMT 2014 English-Czech and LM1B. At a big model size,\nthe Evolved Transformer establishes a new state-of-the-art BLEU score of 29.8\non WMT'14 English-German; at smaller sizes, it achieves the same quality as the\noriginal \"big\" Transformer with 37.6% less parameters and outperforms the\nTransformer by 0.7 BLEU at a mobile-friendly model size of 7M parameters.", "published": "2019-01-30 22:03:01", "link": "http://arxiv.org/abs/1901.11117v4", "categories": ["cs.LG", "cs.CL", "cs.NE", "stat.ML"], "primary_category": "cs.LG"}
