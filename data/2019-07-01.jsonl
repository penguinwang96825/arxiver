{"title": "Few-Shot Representation Learning for Out-Of-Vocabulary Words", "abstract": "Existing approaches for learning word embeddings often assume there are\nsufficient occurrences for each word in the corpus, such that the\nrepresentation of words can be accurately estimated from their contexts.\nHowever, in real-world scenarios, out-of-vocabulary (a.k.a. OOV) words that do\nnot appear in training corpus emerge frequently. It is challenging to learn\naccurate representations of these words with only a few observations. In this\npaper, we formulate the learning of OOV embeddings as a few-shot regression\nproblem, and address it by training a representation function to predict the\noracle embedding vector (defined as embedding trained with abundant\nobservations) based on limited observations. Specifically, we propose a novel\nhierarchical attention-based architecture to serve as the neural regression\nfunction, with which the context information of a word is encoded and\naggregated from K observations. Furthermore, our approach can leverage\nModel-Agnostic Meta-Learning (MAML) for adapting the learned model to the new\ncorpus fast and robustly. Experiments show that the proposed approach\nsignificantly outperforms existing methods in constructing accurate embeddings\nfor OOV words, and improves downstream tasks where these embeddings are\nutilized.", "published": "2019-07-01 00:43:45", "link": "http://arxiv.org/abs/1907.00505v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Database Rule for Weak Supervised Text-to-SQL Generation", "abstract": "We present a simple way to do the task of text-to-SQL problem with weak\nsupervision. We call it Rule-SQL. Given the question and the answer from the\ndatabase table without the SQL logic form, Rule-SQL use the rules based on\ntable column names and question string for the SQL exploration first and then\nuse the explored SQL for supervised training. We design several rules for\nreducing the exploration search space. For the deep model, we leverage BERT for\nthe representation layer and separate the model to SELECT, AGG and WHERE parts.\nThe experiment result on WikiSQL outperforms the strong baseline of full\nsupervision and is comparable to the start-of-the-art weak supervised mothods.", "published": "2019-07-01 09:14:45", "link": "http://arxiv.org/abs/1907.00620v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modernizing Historical Documents: a User Study", "abstract": "Accessibility to historical documents is mostly limited to scholars. This is\ndue to the language barrier inherent in human language and the linguistic\nproperties of these documents. Given a historical document, modernization aims\nto generate a new version of it, written in the modern version of the\ndocument's language. Its goal is to tackle the language barrier, decreasing the\ncomprehension difficulty and making historical documents accessible to a\nbroader audience. In this work, we proposed a new neural machine translation\napproach that profits from modern documents to enrich its systems. We tested\nthis approach with both automatic and human evaluation, and conducted a user\nstudy. Results showed that modernization is successfully reaching its goal,\nalthough it still has room for improvement.", "published": "2019-07-01 11:13:19", "link": "http://arxiv.org/abs/1907.00659v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual, Multi-scale and Multi-layer Visualization of Intermediate\n  Representations", "abstract": "The main alternatives nowadays to deal with sequences are Recurrent Neural\nNetworks (RNN), Convolutional Neural Networks (CNN) architectures and the\nTransformer. In this context, RNN's, CNN's and Transformer have most commonly\nbeen used as an encoder-decoder architecture with multiple layers in each\nmodule. Far beyond this, these architectures are the basis for the contextual\nword embeddings which are revolutionizing most natural language downstream\napplications. However, intermediate layer representations in sequence-based\narchitectures can be difficult to interpret. To make each layer representation\nwithin these architectures more accessible and meaningful, we introduce a\nweb-based tool that visualizes them both at the sentence and token level. We\npresent three use cases. The first analyses gender issues in contextual word\nembeddings. The second and third are showing multilingual intermediate\nrepresentations for sentences and tokens and the evolution of these\nintermediate representations along the multiple layers of the decoder and in\nthe context of multilingual machine translation.", "published": "2019-07-01 14:15:16", "link": "http://arxiv.org/abs/1907.00810v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Post-editese: an Exacerbated Translationese", "abstract": "Post-editing (PE) machine translation (MT) is widely used for dissemination\nbecause it leads to higher productivity than human translation from scratch\n(HT). In addition, PE translations are found to be of equal or better quality\nthan HTs. However, most such studies measure quality solely as the number of\nerrors. We conduct a set of computational analyses in which we compare PE\nagainst HT on three different datasets that cover five translation directions\nwith measures that address different translation universals and laws of\ntranslation: simplification, normalisation and interference. We find out that\nPEs are simpler and more normalised and have a higher degree of interference\nfrom the source language than HTs.", "published": "2019-07-01 16:16:39", "link": "http://arxiv.org/abs/1907.00900v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Claim Extraction in Biomedical Publications using Deep Discourse Model\n  and Transfer Learning", "abstract": "Claims are a fundamental unit of scientific discourse. The exponential growth\nin the number of scientific publications makes automatic claim extraction an\nimportant problem for researchers who are overwhelmed by this information\noverload. Such an automated claim extraction system is useful for both manual\nand programmatic exploration of scientific knowledge. In this paper, we\nintroduce a new dataset of 1,500 scientific abstracts from the biomedical\ndomain with expert annotations for each sentence indicating whether the\nsentence presents a scientific claim. We introduce a new model for claim\nextraction and compare it to several baseline models including rule-based and\ndeep learning techniques. Moreover, we show that using a transfer learning\napproach with a fine-tuning step allows us to improve performance from a large\ndiscourse-annotated dataset. Our final model increases F1-score by over 14\npercent points compared to a baseline model without transfer learning. We\nrelease a publicly accessible tool for discourse and claims prediction along\nwith an annotation tool. We discuss further applications beyond biomedical\nliterature.", "published": "2019-07-01 17:56:41", "link": "http://arxiv.org/abs/1907.00962v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Weak Supervision Enhanced Generative Network for Question Generation", "abstract": "Automatic question generation according to an answer within the given passage\nis useful for many applications, such as question answering system, dialogue\nsystem, etc. Current neural-based methods mostly take two steps which extract\nseveral important sentences based on the candidate answer through manual rules\nor supervised neural networks and then use an encoder-decoder framework to\ngenerate questions about these sentences. These approaches neglect the semantic\nrelations between the answer and the context of the whole passage which is\nsometimes necessary for answering the question. To address this problem, we\npropose the Weak Supervision Enhanced Generative Network (WeGen) which\nautomatically discovers relevant features of the passage given the answer span\nin a weakly supervised manner to improve the quality of generated questions.\nMore specifically, we devise a discriminator, Relation Guider, to capture the\nrelations between the whole passage and the associated answer and then the\nMulti-Interaction mechanism is deployed to transfer the knowledge dynamically\nfor our question generation system. Experiments show the effectiveness of our\nmethod in both automatic evaluations and human evaluations.", "published": "2019-07-01 08:44:30", "link": "http://arxiv.org/abs/1907.00607v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EGG: a toolkit for research on Emergence of lanGuage in Games", "abstract": "There is renewed interest in simulating language emergence among deep neural\nagents that communicate to jointly solve a task, spurred by the practical aim\nto develop language-enabled interactive AIs, as well as by theoretical\nquestions about the evolution of human language. However, optimizing deep\narchitectures connected by a discrete communication channel (such as that in\nwhich language emerges) is technically challenging. We introduce EGG, a toolkit\nthat greatly simplifies the implementation of emergent-language communication\ngames. EGG's modular design provides a set of building blocks that the user can\ncombine to create new games, easily navigating the optimization and\narchitecture space. We hope that the tool will lower the technical barrier, and\nencourage researchers from various backgrounds to do original work in this\nexciting area.", "published": "2019-07-01 15:20:01", "link": "http://arxiv.org/abs/1907.00852v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HyST: A Hybrid Approach for Flexible and Accurate Dialogue State\n  Tracking", "abstract": "Recent works on end-to-end trainable neural network based approaches have\ndemonstrated state-of-the-art results on dialogue state tracking. The best\nperforming approaches estimate a probability distribution over all possible\nslot values. However, these approaches do not scale for large value sets\ncommonly present in real-life applications and are not ideal for tracking slot\nvalues that were not observed in the training set. To tackle these issues,\ncandidate-generation-based approaches have been proposed. These approaches\nestimate a set of values that are possible at each turn based on the\nconversation history and/or language understanding outputs, and hence enable\nstate tracking over unseen values and large value sets however, they fall short\nin terms of performance in comparison to the first group. In this work, we\nanalyze the performance of these two alternative dialogue state tracking\nmethods, and present a hybrid approach (HyST) which learns the appropriate\nmethod for each slot type. To demonstrate the effectiveness of HyST on a\nrich-set of slot types, we experiment with the recently released MultiWOZ-2.0\nmulti-domain, task-oriented dialogue-dataset. Our experiments show that HyST\nscales to multi-domain applications. Our best performing model results in a\nrelative improvement of 24% and 10% over the previous SOTA and our best\nbaseline respectively.", "published": "2019-07-01 15:55:36", "link": "http://arxiv.org/abs/1907.00883v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semantic Product Search", "abstract": "We study the problem of semantic matching in product search, that is, given a\ncustomer query, retrieve all semantically related products from the catalog.\nPure lexical matching via an inverted index falls short in this respect due to\nseveral factors: a) lack of understanding of hypernyms, synonyms, and antonyms,\nb) fragility to morphological variants (e.g. \"woman\" vs. \"women\"), and c)\nsensitivity to spelling errors. To address these issues, we train a deep\nlearning model for semantic matching using customer behavior data. Much of the\nrecent work on large-scale semantic search using deep learning focuses on\nranking for web search. In contrast, semantic matching for product search\npresents several novel challenges, which we elucidate in this paper. We address\nthese challenges by a) developing a new loss function that has an inbuilt\nthreshold to differentiate between random negative examples, impressed but not\npurchased examples, and positive examples (purchased items), b) using average\npooling in conjunction with n-grams to capture short-range linguistic patterns,\nc) using hashing to handle out of vocabulary tokens, and d) using a model\nparallel training architecture to scale across 8 GPUs. We present compelling\noffline results that demonstrate at least 4.7% improvement in Recall@100 and\n14.5% improvement in mean average precision (MAP) over baseline\nstate-of-the-art semantic search methods using the same tokenization method.\nMoreover, we present results and discuss learnings from online A/B tests which\ndemonstrate the efficacy of our method.", "published": "2019-07-01 17:20:02", "link": "http://arxiv.org/abs/1907.00937v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Natural Language Understanding with the Quora Question Pairs Dataset", "abstract": "This paper explores the task Natural Language Understanding (NLU) by looking\nat duplicate question detection in the Quora dataset. We conducted extensive\nexploration of the dataset and used various machine learning models, including\nlinear and tree-based models. Our final finding was that a simple Continuous\nBag of Words neural network model had the best performance, outdoing more\ncomplicated recurrent and attention based models. We also conducted error\nanalysis and found some subjectivity in the labeling of the dataset.", "published": "2019-07-01 19:48:34", "link": "http://arxiv.org/abs/1907.01041v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Is artificial data useful for biomedical Natural Language Processing\n  algorithms?", "abstract": "A major obstacle to the development of Natural Language Processing (NLP)\nmethods in the biomedical domain is data accessibility. This problem can be\naddressed by generating medical data artificially. Most previous studies have\nfocused on the generation of short clinical text, and evaluation of the data\nutility has been limited. We propose a generic methodology to guide the\ngeneration of clinical text with key phrases. We use the artificial data as\nadditional training data in two key biomedical NLP tasks: text classification\nand temporal relation extraction. We show that artificially generated training\ndata used in conjunction with real training data can lead to performance boosts\nfor data-greedy neural network algorithms. We also demonstrate the usefulness\nof the generated data for NLP setups where it fully replaces real training\ndata.", "published": "2019-07-01 20:17:59", "link": "http://arxiv.org/abs/1907.01055v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hidden in Plain Sight For Too Long: Using Text Mining Techniques to\n  Shine a Light on Workplace Sexism and Sexual Harassment", "abstract": "Objective: The goal of this study is to understand how people experience\nsexism and sexual harassment in the workplace by discovering themes in 2,362\nexperiences posted on the Everyday Sexism Project's website everydaysexism.com.\nMethod: This study used both quantitative and qualitative methods. The\nquantitative method was a computational framework to collect and analyze a\nlarge number of workplace sexual harassment experiences. The qualitative method\nwas the analysis of the topics generated by a text mining method. Results:\nTwenty-three topics were coded and then grouped into three overarching themes\nfrom the sex discrimination and sexual harassment literature. The Sex\nDiscrimination theme included experiences in which women were treated\nunfavorably due to their sex, such as being passed over for promotion, denied\nopportunities, paid less than men, and ignored or talked over in meetings. The\nSex Discrimination and Gender harassment theme included stories about sex\ndiscrimination and gender harassment, such as sexist hostility behaviors\nranging from insults and jokes invoking misogynistic stereotypes to bullying\nbehavior. The last theme, Unwanted Sexual Attention, contained stories\ndescribing sexual comments and behaviors used to degrade women. Unwanted\ntouching was the highest weighted topic, indicating how common it was for\nwebsite users to endure being touched, hugged or kissed, groped, and grabbed.\nConclusions: This study illustrates how researchers can use automatic processes\nto go beyond the limits of traditional research methods and investigate\nnaturally occurring large scale datasets on the internet to achieve a better\nunderstanding of everyday workplace sexism experiences.", "published": "2019-07-01 01:48:49", "link": "http://arxiv.org/abs/1907.00510v1", "categories": ["cs.CY", "cs.CL", "stat.AP"], "primary_category": "cs.CY"}
{"title": "Do Transformer Attention Heads Provide Transparency in Abstractive\n  Summarization?", "abstract": "Learning algorithms become more powerful, often at the cost of increased\ncomplexity. In response, the demand for algorithms to be transparent is\ngrowing. In NLP tasks, attention distributions learned by attention-based deep\nlearning models are used to gain insights in the models' behavior. To which\nextent is this perspective valid for all NLP tasks? We investigate whether\ndistributions calculated by different attention heads in a transformer\narchitecture can be used to improve transparency in the task of abstractive\nsummarization. To this end, we present both a qualitative and quantitative\nanalysis to investigate the behavior of the attention heads. We show that some\nattention heads indeed specialize towards syntactically and semantically\ndistinct input. We propose an approach to evaluate to which extent the\nTransformer model relies on specifically learned attention distributions. We\nalso discuss what this implies for using attention distributions as a means of\ntransparency.", "published": "2019-07-01 06:46:43", "link": "http://arxiv.org/abs/1907.00570v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Ultrasound tongue imaging for diarization and alignment of child speech\n  therapy sessions", "abstract": "We investigate the automatic processing of child speech therapy sessions\nusing ultrasound visual biofeedback, with a specific focus on complementing\nacoustic features with ultrasound images of the tongue for the tasks of speaker\ndiarization and time-alignment of target words. For speaker diarization, we\npropose an ultrasound-based time-domain signal which we call estimated tongue\nactivity. For word-alignment, we augment an acoustic model with low-dimensional\nrepresentations of ultrasound images of the tongue, learned by a convolutional\nneural network. We conduct our experiments using the Ultrasuite repository of\nultrasound and speech recordings for child speech therapy sessions. For both\ntasks, we observe that systems augmented with ultrasound data outperform\ncorresponding systems using only the audio signal.", "published": "2019-07-01 14:25:51", "link": "http://arxiv.org/abs/1907.00818v2", "categories": ["eess.AS", "cs.CL", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Understanding Memory Modules on Learning Simple Algorithms", "abstract": "Recent work has shown that memory modules are crucial for the generalization\nability of neural networks on learning simple algorithms. However, we still\nhave little understanding of the working mechanism of memory modules. To\nalleviate this problem, we apply a two-step analysis pipeline consisting of\nfirst inferring hypothesis about what strategy the model has learned according\nto visualization and then verify it by a novel proposed qualitative analysis\nmethod based on dimension reduction. Using this method, we have analyzed two\npopular memory-augmented neural networks, neural Turing machine and\nstack-augmented neural network on two simple algorithm tasks including\nreversing a random sequence and evaluation of arithmetic expressions. Results\nhave shown that on the former task both models can learn to generalize and on\nthe latter task only the stack-augmented model can do so. We show that\ndifferent strategies are learned by the models, in which specific categories of\ninput are monitored and different policies are made based on that to change the\nmemory.", "published": "2019-07-01 14:27:03", "link": "http://arxiv.org/abs/1907.00820v1", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Katecheo: A Portable and Modular System for Multi-Topic Question\n  Answering", "abstract": "We introduce a modular system that can be deployed on any Kubernetes cluster\nfor question answering via REST API. This system, called Katecheo, includes\nthree configurable modules that collectively enable identification of\nquestions, classification of those questions into topics, document search, and\nreading comprehension. We demonstrate the system using publicly available\nknowledge base articles extracted from Stack Exchange sites. However, users can\nextend the system to any number of topics, or domains, without the need to\nmodify any of the model serving code or train their own models. All components\nof the system are open source and available under a permissive Apache 2\nLicense.", "published": "2019-07-01 15:20:10", "link": "http://arxiv.org/abs/1907.00854v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Learning Representations from Imperfect Time Series Data via Tensor Rank\n  Regularization", "abstract": "There has been an increased interest in multimodal language processing\nincluding multimodal dialog, question answering, sentiment analysis, and speech\nrecognition. However, naturally occurring multimodal data is often imperfect as\na result of imperfect modalities, missing entries or noise corruption. To\naddress these concerns, we present a regularization method based on tensor rank\nminimization. Our method is based on the observation that high-dimensional\nmultimodal time series data often exhibit correlations across time and\nmodalities which leads to low-rank tensor representations. However, the\npresence of noise or incomplete values breaks these correlations and results in\ntensor representations of higher rank. We design a model to learn such tensor\nrepresentations and effectively regularize their rank. Experiments on\nmultimodal language data show that our model achieves good results across\nvarious levels of imperfection.", "published": "2019-07-01 18:40:52", "link": "http://arxiv.org/abs/1907.01011v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Representation, Exploration and Recommendation of Music Playlists", "abstract": "Playlists have become a significant part of our listening experience because\nof the digital cloud-based services such as Spotify, Pandora, Apple Music.\nOwing to the meteoric rise in the usage of playlists, recommending playlists is\ncrucial to music services today. Although there has been a lot of work done in\nplaylist prediction, the area of playlist representation hasn't received that\nlevel of attention. Over the last few years, sequence-to-sequence models,\nespecially in the field of natural language processing, have shown the\neffectiveness of learned embeddings in capturing the semantic characteristics\nof sequences. We can apply similar concepts to music to learn fixed length\nrepresentations for playlists and use those representations for downstream\ntasks such as playlist discovery, browsing, and recommendation. In this work,\nwe formulate the problem of learning a fixed-length playlist representation in\nan unsupervised manner, using Sequence-to-sequence (Seq2seq) models,\ninterpreting playlists as sentences and songs as words. We compare our model\nwith two other encoding architectures for baseline comparison. We evaluate our\nwork using the suite of tasks commonly used for assessing sentence embeddings,\nalong with a few additional tasks pertaining to music, and a recommendation\ntask to study the traits captured by the playlist embeddings and their\neffectiveness for the purpose of music recommendation.", "published": "2019-07-01 23:20:45", "link": "http://arxiv.org/abs/1907.01098v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Learning to Rank Broad and Narrow Queries in E-Commerce", "abstract": "Search is a prominent channel for discovering products on an e-commerce\nplatform. Ranking products retrieved from search becomes crucial to address\ncustomer's need and optimize for business metrics. While learning to Rank\n(LETOR) models have been extensively studied and have demonstrated efficacy in\nthe context of web search; it is a relatively new research area to be explored\nin the e-commerce. In this paper, we present a framework for building LETOR\nmodel for an e-commerce platform. We analyze user queries and propose a\nmechanism to segment queries between broad and narrow based on user's intent.\nWe discuss different types of features - query, product and query-product and\ndiscuss challenges in using them. We show that sparsity in product features can\nbe tackled through a denoising auto-encoder while skip-gram based word\nembeddings help solve the query-product sparsity issues. We also present\nvarious target metrics that can be employed for evaluating search results and\ncompare their robustness. Further, we build and compare performances of both\npointwise and pairwise LETOR models on fashion category data set. We also build\nand compare distinct models for broad and narrow queries, analyze feature\nimportance across these and show that these specialized models perform better\nthan a combined model in the fashion world.", "published": "2019-07-01 18:30:38", "link": "http://arxiv.org/abs/1907.01549v2", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Pentagon at MEDIQA 2019: Multi-task Learning for Filtering and\n  Re-ranking Answers using Language Inference and Question Entailment", "abstract": "Parallel deep learning architectures like fine-tuned BERT and MT-DNN, have\nquickly become the state of the art, bypassing previous deep and shallow\nlearning methods by a large margin. More recently, pre-trained models from\nlarge related datasets have been able to perform well on many downstream tasks\nby just fine-tuning on domain-specific datasets . However, using powerful\nmodels on non-trivial tasks, such as ranking and large document classification,\nstill remains a challenge due to input size limitations of parallel\narchitecture and extremely small datasets (insufficient for fine-tuning). In\nthis work, we introduce an end-to-end system, trained in a multi-task setting,\nto filter and re-rank answers in the medical domain. We use task-specific\npre-trained models as deep feature extractors. Our model achieves the highest\nSpearman's Rho and Mean Reciprocal Rank of 0.338 and 0.9622 respectively, on\nthe ACL-BioNLP workshop MediQA Question Answering shared-task.", "published": "2019-07-01 17:48:40", "link": "http://arxiv.org/abs/1907.01643v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Patent Claim Generation by Fine-Tuning OpenAI GPT-2", "abstract": "In this work, we focus on fine-tuning an OpenAI GPT-2 pre-trained model for\ngenerating patent claims. GPT-2 has demonstrated impressive efficacy of\npre-trained language models on various tasks, particularly coherent text\ngeneration. Patent claim language itself has rarely been explored in the past\nand poses a unique challenge. We are motivated to generate coherent patent\nclaims automatically so that augmented inventing might be viable someday. In\nour implementation, we identified a unique language structure in patent claims\nand leveraged its implicit human annotations. We investigated the fine-tuning\nprocess by probing the first 100 steps and observing the generated text at each\nstep. Based on both conditional and unconditional random sampling, we analyze\nthe overall quality of generated patent claims. Our contributions include: (1)\nbeing the first to generate patent claims by machines and being the first to\napply GPT-2 to patent claim generation, (2) providing various experiment\nresults for qualitative analysis and future research, (3) proposing a new\nsampling approach for text generation, and (4) building an e-mail bot for\nfuture researchers to explore the fine-tuned GPT-2 model further.", "published": "2019-07-01 00:02:59", "link": "http://arxiv.org/abs/1907.02052v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "UltraSuite: A Repository of Ultrasound and Acoustic Data from Child\n  Speech Therapy Sessions", "abstract": "We introduce UltraSuite, a curated repository of ultrasound and acoustic\ndata, collected from recordings of child speech therapy sessions. This release\nincludes three data collections, one from typically developing children and two\nfrom children with speech sound disorders. In addition, it includes a set of\nannotations, some manual and some automatically produced, and software tools to\nprocess, transform and visualise the data.", "published": "2019-07-01 14:54:31", "link": "http://arxiv.org/abs/1907.00835v1", "categories": ["cs.CL", "cs.CV", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CL"}
{"title": "Comparison of Lattice-Free and Lattice-Based Sequence Discriminative\n  Training Criteria for LVCSR", "abstract": "Sequence discriminative training criteria have long been a standard tool in\nautomatic speech recognition for improving the performance of acoustic models\nover their maximum likelihood / cross entropy trained counterparts. While\npreviously a lattice approximation of the search space has been necessary to\nreduce computational complexity, recently proposed methods use other\napproximations to dispense of the need for the computationally expensive step\nof separate lattice creation.\n  In this work we present a memory efficient implementation of the\nforward-backward computation that allows us to use uni-gram word-level language\nmodels in the denominator calculation while still doing a full summation on\nGPU. This allows for a direct comparison of lattice-based and lattice-free\nsequence discriminative training criteria such as MMI and sMBR, both using the\nsame language model during training.\n  We compared performance, speed of convergence, and stability on large\nvocabulary continuous speech recognition tasks like Switchboard and Quaero. We\nfound that silence modeling seriously impacts the performance in the\nlattice-free case and needs special treatment. In our experiments lattice-free\nMMI comes on par with its lattice-based counterpart. Lattice-based sMBR still\noutperforms all lattice-free training criteria.", "published": "2019-07-01 15:16:04", "link": "http://arxiv.org/abs/1907.01409v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Synchronising audio and ultrasound by learning cross-modal embeddings", "abstract": "Audiovisual synchronisation is the task of determining the time offset\nbetween speech audio and a video recording of the articulators. In child speech\ntherapy, audio and ultrasound videos of the tongue are captured using\ninstruments which rely on hardware to synchronise the two modalities at\nrecording time. Hardware synchronisation can fail in practice, and no mechanism\nexists to synchronise the signals post hoc. To address this problem, we employ\na two-stream neural network which exploits the correlation between the two\nmodalities to find the offset. We train our model on recordings from 69\nspeakers, and show that it correctly synchronises 82.9% of test utterances from\nunseen therapy sessions and unseen speakers, thus considerably reducing the\nnumber of utterances to be manually synchronised. An analysis of model\nperformance on the test utterances shows that directed phone articulations are\nmore difficult to automatically synchronise compared to utterances containing\nnatural variation in speech such as words, sentences, or conversations.", "published": "2019-07-01 13:22:48", "link": "http://arxiv.org/abs/1907.00758v2", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CL"}
{"title": "Speaker-independent classification of phonetic segments from raw\n  ultrasound in child speech", "abstract": "Ultrasound tongue imaging (UTI) provides a convenient way to visualize the\nvocal tract during speech production. UTI is increasingly being used for speech\ntherapy, making it important to develop automatic methods to assist various\ntime-consuming manual tasks currently performed by speech therapists. A key\nchallenge is to generalize the automatic processing of ultrasound tongue images\nto previously unseen speakers. In this work, we investigate the classification\nof phonetic segments (tongue shapes) from raw ultrasound recordings under\nseveral training scenarios: speaker-dependent, multi-speaker,\nspeaker-independent, and speaker-adapted. We observe that models underperform\nwhen applied to data from speakers not seen at training time. However, when\nprovided with minimal additional speaker information, such as the mean\nultrasound frame, the models generalize better to unseen speakers.", "published": "2019-07-01 12:04:13", "link": "http://arxiv.org/abs/1907.01413v1", "categories": ["eess.AS", "cs.CL", "cs.CV", "cs.LG", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Quasi-Periodic WaveNet Vocoder: A Pitch Dependent Dilated Convolution\n  Model for Parametric Speech Generation", "abstract": "In this paper, we propose a quasi-periodic neural network (QPNet) vocoder\nwith a novel network architecture named pitch-dependent dilated convolution\n(PDCNN) to improve the pitch controllability of WaveNet (WN) vocoder. The\neffectiveness of the WN vocoder to generate high-fidelity speech samples from\ngiven acoustic features has been proved recently. However, because of the fixed\ndilated convolution and generic network architecture, the WN vocoder hardly\ngenerates speech with given F0 values which are outside the range observed in\ntraining data. Consequently, the WN vocoder lacks the pitch controllability\nwhich is one of the essential capabilities of conventional vocoders. To address\nthis limitation, we propose the PDCNN component which has the time-variant\nadaptive dilation size related to the given F0 values and a cascade network\nstructure of the QPNet vocoder to generate quasi-periodic signals such as\nspeech. Both objective and subjective tests are conducted, and the experimental\nresults demonstrate the better pitch controllability of the QPNet vocoder\ncompared to the same and double sized WN vocoders while attaining comparable\nspeech qualities. Index Terms: WaveNet, vocoder, quasi-periodic signal,\npitch-dependent dilated convolution, pitch controllability", "published": "2019-07-01 14:04:45", "link": "http://arxiv.org/abs/1907.00797v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cosine similarity-based adversarial process", "abstract": "An adversarial process between two deep neural networks is a promising\napproach to train a robust model. In this paper, we propose an adversarial\nprocess using cosine similarity, whereas conventional adversarial processes are\nbased on inverted categorical cross entropy (CCE). When used for training an\nidentification model, the adversarial process induces the competition of two\ndiscriminative models; one for a primary task such as speaker identification or\nimage recognition, the other one for a subsidiary task such as channel\nidentification or domain identification. In particular, the adversarial process\ndegrades the performance of the subsidiary model by eliminating the subsidiary\ninformation in the input which, in assumption, may degrade the performance of\nthe primary model. The conventional adversarial processes maximize the CCE of\nthe subsidiary model to degrade the performance. We have studied a framework\nfor training robust discriminative models by eliminating channel or domain\ninformation (subsidiary information) by applying such an adversarial process.\nHowever, we found through experiments that using the process of maximizing the\nCCE does not guarantee the performance degradation of the subsidiary model. In\nthe proposed adversarial process using cosine similarity, on the contrary, the\nperformance of the subsidiary model can be degraded more efficiently by\nsearching feature space orthogonal to the subsidiary model. The experiments on\nspeaker identification and image recognition show that we found features that\nmake the outputs of the subsidiary models independent of the input, and the\nperformances of the primary models are improved.", "published": "2019-07-01 04:44:35", "link": "http://arxiv.org/abs/1907.00542v1", "categories": ["cs.LG", "eess.AS", "eess.IV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Analysis by Adversarial Synthesis -- A Novel Approach for Speech\n  Vocoding", "abstract": "Classical parametric speech coding techniques provide a compact\nrepresentation for speech signals. This affords a very low transmission rate\nbut with a reduced perceptual quality of the reconstructed signals. Recently,\nautoregressive deep generative models such as WaveNet and SampleRNN have been\nused as speech vocoders to scale up the perceptual quality of the reconstructed\nsignals without increasing the coding rate. However, such models suffer from a\nvery slow signal generation mechanism due to their sample-by-sample modelling\napproach. In this work, we introduce a new methodology for neural speech\nvocoding based on generative adversarial networks (GANs). A fake speech signal\nis generated from a very compressed representation of the glottal excitation\nusing conditional GANs as a deep generative model. This fake speech is then\nrefined using the LPC parameters of the original speech signal to obtain a\nnatural reconstruction. The reconstructed speech waveforms based on this\napproach show a higher perceptual quality than the classical vocoder\ncounterparts according to subjective and objective evaluation scores for a\ndataset of 30 male and female speakers. Moreover, the usage of GANs enables to\ngenerate signals in one-shot compared to autoregressive generative models. This\nmakes GANs promising for exploration to implement high-quality neural vocoders.", "published": "2019-07-01 13:46:54", "link": "http://arxiv.org/abs/1907.00772v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Designing Deep Reinforcement Learning for Human Parameter Exploration", "abstract": "Software tools for generating digital sound often present users with\nhigh-dimensional, parametric interfaces, that may not facilitate exploration of\ndiverse sound designs. In this paper, we propose to investigate artificial\nagents using deep reinforcement learning to explore parameter spaces in\npartnership with users for sound design. We describe a series of user-centred\nstudies to probe the creative benefits of these agents and adapting their\ndesign to exploration. Preliminary studies observing users' exploration\nstrategies with parametric interfaces and testing different agent exploration\nbehaviours led to the design of a fully-functioning prototype, called\nCo-Explorer, that we evaluated in a workshop with professional sound designers.\nWe found that the Co-Explorer enables a novel creative workflow centred on\nhuman-machine partnership, which has been positively received by practitioners.\nWe also highlight varied user exploration behaviors throughout partnering with\nour system. Finally, we frame design guidelines for enabling such\nco-exploration workflow in creative digital applications.", "published": "2019-07-01 14:32:47", "link": "http://arxiv.org/abs/1907.00824v2", "categories": ["cs.HC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Compression of Acoustic Event Detection Models With Quantized\n  Distillation", "abstract": "Acoustic Event Detection (AED), aiming at detecting categories of events\nbased on audio signals, has found application in many intelligent systems.\nRecently deep neural network significantly advances this field and reduces\ndetection errors to a large scale. However how to efficiently execute deep\nmodels in AED has received much less attention. Meanwhile state-of-the-art AED\nmodels are based on large deep models, which are computational demanding and\nchallenging to deploy on devices with constrained computational resources. In\nthis paper, we present a simple yet effective compression approach which\njointly leverages knowledge distillation and quantization to compress larger\nnetwork (teacher model) into compact network (student model). Experimental\nresults show proposed technique not only lowers error rate of original compact\nnetwork by 15% through distillation but also further reduces its model size to\na large extent (2% of teacher, 12% of full-precision student) through\nquantization.", "published": "2019-07-01 15:37:52", "link": "http://arxiv.org/abs/1907.00873v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "LSTM Language Models for LVCSR in First-Pass Decoding and\n  Lattice-Rescoring", "abstract": "LSTM based language models are an important part of modern LVCSR systems as\nthey significantly improve performance over traditional backoff language\nmodels. Incorporating them efficiently into decoding has been notoriously\ndifficult. In this paper we present an approach based on a combination of\none-pass decoding and lattice rescoring. We perform decoding with the LSTM-LM\nin the first pass but recombine hypothesis that share the last two words,\nafterwards we rescore the resulting lattice. We run our systems on GPGPU\nequipped machines and are able to produce competitive results on the Hub5'00\nand Librispeech evaluation corpora with a runtime better than real-time. In\naddition we shortly investigate the possibility to carry out the full sum over\nall state-sequences belonging to a given word-hypothesis during decoding\nwithout recombination.", "published": "2019-07-01 19:29:04", "link": "http://arxiv.org/abs/1907.01030v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Improving Performance of End-to-End ASR on Numeric Sequences", "abstract": "Recognizing written domain numeric utterances (e.g. I need $1.25.) can be\nchallenging for ASR systems, particularly when numeric sequences are not seen\nduring training. This out-of-vocabulary (OOV) issue is addressed in\nconventional ASR systems by training part of the model on spoken domain\nutterances (e.g. I need one dollar and twenty five cents.), for which numeric\nsequences are composed of in-vocabulary numbers, and then using an FST\nverbalizer to denormalize the result. Unfortunately, conventional ASR models\nare not suitable for the low memory setting of on-device speech recognition.\nE2E models such as RNN-T are attractive for on-device ASR, as they fold the AM,\nPM and LM of a conventional model into one neural network. However, in the\non-device setting the large memory footprint of an FST denormer makes spoken\ndomain training more difficult. In this paper, we investigate techniques to\nimprove E2E model performance on numeric data. We find that using a\ntext-to-speech system to generate additional numeric training data, as well as\nusing a small-footprint neural network to perform spoken-to-written domain\ndenorming, yields improvement in several numeric classes. In the case of the\nlongest numeric sequences, we see reduction of WER by up to a factor of 8.", "published": "2019-07-01 14:21:09", "link": "http://arxiv.org/abs/1907.01372v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Universal audio synthesizer control with normalizing flows", "abstract": "The ubiquity of sound synthesizers has reshaped music production and even\nentirely defined new music genres. However, the increasing complexity and\nnumber of parameters in modern synthesizers make them harder to master. Hence,\nthe development of methods allowing to easily create and explore with\nsynthesizers is a crucial need. Here, we introduce a novel formulation of audio\nsynthesizer control. We formalize it as finding an organized latent audio space\nthat represents the capabilities of a synthesizer, while constructing an\ninvertible mapping to the space of its parameters. By using this formulation,\nwe show that we can address simultaneously automatic parameter inference,\nmacro-control learning and audio-based preset exploration within a single\nmodel. To solve this new formulation, we rely on Variational Auto-Encoders\n(VAE) and Normalizing Flows (NF) to organize and map the respective auditory\nand parameter spaces. We introduce the disentangling flows, which allow to\nperform the invertible mapping between separate latent spaces, while steering\nthe organization of some latent dimensions to match target variation factors by\nsplitting the objective as partial density evaluation. We evaluate our proposal\nagainst a large set of baseline models and show its superiority in both\nparameter inference and audio reconstruction. We also show that the model\ndisentangles the major factors of audio variations as latent dimensions, that\ncan be directly used as macro-parameters. We also show that our model is able\nto learn semantic controls of a synthesizer by smoothly mapping to its\nparameters. Finally, we discuss the use of our model in creative applications\nand its real-time implementation in Ableton Live", "published": "2019-07-01 14:49:07", "link": "http://arxiv.org/abs/1907.00971v1", "categories": ["cs.LG", "cs.HC", "cs.MM", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
