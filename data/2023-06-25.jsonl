{"title": "SciMRC: Multi-perspective Scientific Machine Reading Comprehension", "abstract": "Scientific machine reading comprehension (SMRC) aims to understand scientific\ntexts through interactions with humans by given questions. As far as we know,\nthere is only one dataset focused on exploring full-text scientific machine\nreading comprehension. However, the dataset has ignored the fact that different\nreaders may have different levels of understanding of the text, and only\nincludes single-perspective question-answer pairs, leading to a lack of\nconsideration of different perspectives. To tackle the above problem, we\npropose a novel multi-perspective SMRC dataset, called SciMRC, which includes\nperspectives from beginners, students and experts. Our proposed SciMRC is\nconstructed from 741 scientific papers and 6,057 question-answer pairs. Each\nperspective of beginners, students and experts contains 3,306, 1,800 and 951 QA\npairs, respectively. The extensive experiments on SciMRC by utilizing\npre-trained models suggest the importance of considering perspectives of SMRC,\nand demonstrate its challenging nature for machine comprehension.", "published": "2023-06-25 07:25:14", "link": "http://arxiv.org/abs/2306.14149v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low-Rank Prune-And-Factorize for Language Model Compression", "abstract": "The components underpinning PLMs -- large weight matrices -- were shown to\nbear considerable redundancy. Matrix factorization, a well-established\ntechnique from matrix theory, has been utilized to reduce the number of\nparameters in PLM. However, it fails to retain satisfactory performance under\nmoderate to high compression rate. In this paper, we identify the\n\\textit{full-rankness} of fine-tuned PLM as the fundamental bottleneck for the\nfailure of matrix factorization and explore the use of network pruning to\nextract low-rank sparsity pattern desirable to matrix factorization. We find\nsuch low-rank sparsity pattern exclusively exists in models generated by\nfirst-order pruning, which motivates us to unite the two approaches and achieve\nmore effective model compression. We further propose two techniques:\nsparsity-aware SVD and mixed-rank fine-tuning, which improve the initialization\nand training of the compression procedure, respectively. Experiments on GLUE\nand question-answering tasks show that the proposed method has superior\ncompression-performance trade-off compared to existing approaches.", "published": "2023-06-25 07:38:43", "link": "http://arxiv.org/abs/2306.14152v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence-level Event Detection without Triggers via Prompt Learning and\n  Machine Reading Comprehension", "abstract": "The traditional way of sentence-level event detection involves two important\nsubtasks: trigger identification and trigger classifications, where the\nidentified event trigger words are used to classify event types from sentences.\nHowever, trigger classification highly depends on abundant annotated trigger\nwords and the accuracy of trigger identification. In a real scenario,\nannotating trigger words is time-consuming and laborious. For this reason, we\npropose a trigger-free event detection model, which transforms event detection\ninto a two-tower model based on machine reading comprehension and prompt\nlearning. Compared to existing trigger-based and trigger-free methods,\nexperimental studies on two event detection benchmark datasets (ACE2005 and\nMAVEN) have shown that the proposed approach can achieve competitive\nperformance.", "published": "2023-06-25 09:03:56", "link": "http://arxiv.org/abs/2306.14176v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stance Prediction and Analysis of Twitter data : A case study of Ghana\n  2020 Presidential Elections", "abstract": "On December 7, 2020, Ghanaians participated in the polls to determine their\npresident for the next four years. To gain insights from this presidential\nelection, we conducted stance analysis (which is not always equivalent to\nsentiment analysis) to understand how Twitter, a popular social media platform,\nreflected the opinions of its users regarding the two main presidential\ncandidates. We collected a total of 99,356 tweets using the Twitter API\n(Tweepy) and manually annotated 3,090 tweets into three classes: Against,\nNeutral, and Support. We then performed preprocessing on the tweets. The\nresulting dataset was evaluated using two lexicon-based approaches, VADER and\nTextBlob, as well as five supervised machine learning-based approaches: Support\nVector Machine (SVM), Logistic Regression (LR), Multinomial Na\\\"ive Bayes\n(MNB), Stochastic Gradient Descent (SGD), and Random Forest (RF), based on\nmetrics such as accuracy, precision, recall, and F1-score. The best performance\nwas achieved by Logistic Regression with an accuracy of 71.13%. We utilized\nLogistic Regression to classify all the extracted tweets and subsequently\nconducted an analysis and discussion of the results. For access to our data and\ncode, please visit:\nhttps://github.com/ShesterG/Stance-Detection-Ghana-2020-Elections.git", "published": "2023-06-25 10:54:24", "link": "http://arxiv.org/abs/2306.14203v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chinese Fine-Grained Financial Sentiment Analysis with Large Language\n  Models", "abstract": "Entity-level fine-grained sentiment analysis in the financial domain is a\ncrucial subtask of sentiment analysis and currently faces numerous challenges.\nThe primary challenge stems from the lack of high-quality and large-scale\nannotated corpora specifically designed for financial text sentiment analysis,\nwhich in turn limits the availability of data necessary for developing\neffective text processing techniques. Recent advancements in large language\nmodels (LLMs) have yielded remarkable performance in natural language\nprocessing tasks, primarily centered around language pattern matching. In this\npaper, we propose a novel and extensive Chinese fine-grained financial\nsentiment analysis dataset, FinChina SA, for enterprise early warning. We\nthoroughly evaluate and experiment with well-known existing open-source LLMs\nusing our dataset. We firmly believe that our dataset will serve as a valuable\nresource to advance the exploration of real-world financial sentiment analysis\ntasks, which should be the focus of future research. The FinChina SA dataset is\npublicly available at https://github.com/YerayL/FinChina-SA", "published": "2023-06-25 02:24:30", "link": "http://arxiv.org/abs/2306.14096v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Chain-of-Thought Prompt Distillation for Multimodal Named Entity\n  Recognition and Multimodal Relation Extraction", "abstract": "Multimodal Named Entity Recognition (MNER) and Multimodal Relation Extraction\n(MRE) necessitate the fundamental reasoning capacity for intricate linguistic\nand multimodal comprehension. In this study, we explore distilling the\nreasoning ability of large language models (LLMs) into a more compact student\nmodel by generating a \\textit{chain of thought} (CoT) -- a sequence of\nintermediate reasoning steps. Specifically, we commence by exemplifying the\nelicitation of such reasoning ability from LLMs through CoT prompts covering\nmulti-grain (noun, sentence, multimodality) and data-augmentation (style,\nentity, image) dimensions. Subsequently, we present a novel conditional prompt\ndistillation method to assimilate the commonsense reasoning ability from LLMs,\nthereby enhancing the utility of the student model in addressing text-only\ninputs without the requisite addition of image and CoT knowledge. Extensive\nexperiments reveal that our approach attains state-of-the-art accuracy and\nmanifests a plethora of advantages concerning interpretability, data\nefficiency, and cross-domain generalization on MNER and MRE datasets.", "published": "2023-06-25 04:33:56", "link": "http://arxiv.org/abs/2306.14122v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Interpretable Neural Embeddings with Sparse Self-Representation", "abstract": "Interpretability benefits the theoretical understanding of representations.\nExisting word embeddings are generally dense representations. Hence, the\nmeaning of latent dimensions is difficult to interpret. This makes word\nembeddings like a black-box and prevents them from being human-readable and\nfurther manipulation. Many methods employ sparse representation to learn\ninterpretable word embeddings for better interpretability. However, they also\nsuffer from the unstable issue of grouped selection in $\\ell1$ and online\ndictionary learning. Therefore, they tend to yield different results each time.\nTo alleviate this challenge, we propose a novel method to associate data\nself-representation with a shallow neural network to learn expressive,\ninterpretable word embeddings. In experiments, we report that the resulting\nword embeddings achieve comparable and even slightly better interpretability\nthan baseline embeddings. Besides, we also evaluate that our approach performs\ncompetitively well on all downstream tasks and outperforms benchmark embeddings\non a majority of them.", "published": "2023-06-25 05:57:01", "link": "http://arxiv.org/abs/2306.14135v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "$\u03b1$-$\u03b2$-Factorization and the Binary Case of Simon's Congruence", "abstract": "In 1991 H\\'ebrard introduced a factorization of words that turned out to be a\npowerful tool for the investigation of a word's scattered factors (also known\nas (scattered) subwords or subsequences). Based on this, first Karandikar and\nSchnoebelen introduced the notion of $k$-richness and later on Barker et al.\nthe notion of $k$-universality. In 2022 Fleischmann et al. presented a\ngeneralization of the arch factorization by intersecting the arch factorization\nof a word and its reverse. While the authors merely used this factorization for\nthe investigation of shortest absent scattered factors, in this work we\ninvestigate this new $\\alpha$-$\\beta$-factorization as such. We characterize\nthe famous Simon congruence of $k$-universal words in terms of $1$-universal\nwords. Moreover, we apply these results to binary words. In this special case,\nwe obtain a full characterization of the classes and calculate the index of the\ncongruence. Lastly, we start investigating the ternary case, present a full\nlist of possibilities for $\\alpha\\beta\\alpha$-factors, and characterize their\ncongruence.", "published": "2023-06-25 10:16:49", "link": "http://arxiv.org/abs/2306.14192v3", "categories": ["math.CO", "cs.CL"], "primary_category": "math.CO"}
{"title": "Let's Do a Thought Experiment: Using Counterfactuals to Improve Moral\n  Reasoning", "abstract": "Language models still struggle on moral reasoning, despite their impressive\nperformance in many other tasks. In particular, the Moral Scenarios task in\nMMLU (Multi-task Language Understanding) is among the worst performing tasks\nfor many language models, including GPT-3. In this work, we propose a new\nprompting framework, Thought Experiments, to teach language models to do better\nmoral reasoning using counterfactuals. Experiment results show that our\nframework elicits counterfactual questions and answers from the model, which in\nturn helps improve the accuracy on Moral Scenarios task by 9-16% compared to\nother zero-shot baselines. Interestingly, unlike math reasoning tasks,\nzero-shot Chain-of-Thought (CoT) reasoning doesn't work out of the box, and\neven reduces accuracy by around 4% compared to direct zero-shot. We further\nobserved that with minimal human supervision in the form of 5 few-shot\nexamples, the accuracy of the task can be improved to as much as 80%.", "published": "2023-06-25 18:40:43", "link": "http://arxiv.org/abs/2306.14308v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated\n  Adversarial Perturbations", "abstract": "Despite significant progress having been made in question answering on\ntabular data (Table QA), it's unclear whether, and to what extent existing\nTable QA models are robust to task-specific perturbations, e.g., replacing key\nquestion entities or shuffling table columns. To systematically study the\nrobustness of Table QA models, we propose a benchmark called RobuT, which\nbuilds upon existing Table QA datasets (WTQ, WikiSQL-Weak, and SQA) and\nincludes human-annotated adversarial perturbations in terms of table header,\ntable content, and question. Our results indicate that both state-of-the-art\nTable QA models and large language models (e.g., GPT-3) with few-shot learning\nfalter in these adversarial sets. We propose to address this problem by using\nlarge language models to generate adversarial examples to enhance training,\nwhich significantly improves the robustness of Table QA models. Our data and\ncode is publicly available at https://github.com/yilunzhao/RobuT.", "published": "2023-06-25 19:23:21", "link": "http://arxiv.org/abs/2306.14321v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DSE-TTS: Dual Speaker Embedding for Cross-Lingual Text-to-Speech", "abstract": "Although high-fidelity speech can be obtained for intralingual speech\nsynthesis, cross-lingual text-to-speech (CTTS) is still far from satisfactory\nas it is difficult to accurately retain the speaker timbres(i.e. speaker\nsimilarity) and eliminate the accents from their first language(i.e.\nnativeness). In this paper, we demonstrated that vector-quantized(VQ) acoustic\nfeature contains less speaker information than mel-spectrogram. Based on this\nfinding, we propose a novel dual speaker embedding TTS (DSE-TTS) framework for\nCTTS with authentic speaking style. Here, one embedding is fed to the acoustic\nmodel to learn the linguistic speaking style, while the other one is integrated\ninto the vocoder to mimic the target speaker's timbre. Experiments show that by\ncombining both embeddings, DSE-TTS significantly outperforms the\nstate-of-the-art SANE-TTS in cross-lingual synthesis, especially in terms of\nnativeness.", "published": "2023-06-25 06:46:36", "link": "http://arxiv.org/abs/2306.14145v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unveiling the Potential of Sentiment: Can Large Language Models Predict\n  Chinese Stock Price Movements?", "abstract": "The rapid advancement of Large Language Models (LLMs) has spurred discussions\nabout their potential to enhance quantitative trading strategies. LLMs excel in\nanalyzing sentiments about listed companies from financial news, providing\ncritical insights for trading decisions. However, the performance of LLMs in\nthis task varies substantially due to their inherent characteristics. This\npaper introduces a standardized experimental procedure for comprehensive\nevaluations. We detail the methodology using three distinct LLMs, each\nembodying a unique approach to performance enhancement, applied specifically to\nthe task of sentiment factor extraction from large volumes of Chinese news\nsummaries. Subsequently, we develop quantitative trading strategies using these\nsentiment factors and conduct back-tests in realistic scenarios. Our results\nwill offer perspectives about the performances of Large Language Models applied\nto extracting sentiments from Chinese news texts.", "published": "2023-06-25 12:08:44", "link": "http://arxiv.org/abs/2306.14222v2", "categories": ["cs.CL", "cs.AI", "q-fin.ST"], "primary_category": "cs.CL"}
{"title": "Visual Question Answering in Remote Sensing with Cross-Attention and\n  Multimodal Information Bottleneck", "abstract": "In this research, we deal with the problem of visual question answering (VQA)\nin remote sensing. While remotely sensed images contain information significant\nfor the task of identification and object detection, they pose a great\nchallenge in their processing because of high dimensionality, volume and\nredundancy. Furthermore, processing image information jointly with language\nfeatures adds additional constraints, such as mapping the corresponding image\nand language features. To handle this problem, we propose a cross attention\nbased approach combined with information maximization. The CNN-LSTM based\ncross-attention highlights the information in the image and language modalities\nand establishes a connection between the two, while information maximization\nlearns a low dimensional bottleneck layer, that has all the relevant\ninformation required to carry out the VQA task. We evaluate our method on two\nVQA remote sensing datasets of different resolutions. For the high resolution\ndataset, we achieve an overall accuracy of 79.11% and 73.87% for the two test\nsets while for the low resolution dataset, we achieve an overall accuracy of\n85.98%.", "published": "2023-06-25 15:09:21", "link": "http://arxiv.org/abs/2306.14264v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Addressing Cold Start Problem for End-to-end Automatic Speech Scoring", "abstract": "Integrating automatic speech scoring/assessment systems has become a critical\naspect of second-language speaking education. With self-supervised learning\nadvancements, end-to-end speech scoring approaches have exhibited promising\nresults. However, this study highlights the significant decrease in the\nperformance of speech scoring systems in new question contexts, thereby\nidentifying this as a cold start problem in terms of items. With the finding of\ncold-start phenomena, this paper seeks to alleviate the problem by following\nmethods: 1) prompt embeddings, 2) question context embeddings using BERT or\nCLIP models, and 3) choice of the pretrained acoustic model. Experiments are\nconducted on TOEIC speaking test datasets collected from\nEnglish-as-a-second-language (ESL) learners rated by professional TOEIC\nspeaking evaluators. The results demonstrate that the proposed framework not\nonly exhibits robustness in a cold-start environment but also outperforms the\nbaselines for known content.", "published": "2023-06-25 18:48:21", "link": "http://arxiv.org/abs/2306.14310v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards Trustworthy Explanation: On Causal Rationalization", "abstract": "With recent advances in natural language processing, rationalization becomes\nan essential self-explaining diagram to disentangle the black box by selecting\na subset of input texts to account for the major variation in prediction. Yet,\nexisting association-based approaches on rationalization cannot identify true\nrationales when two or more snippets are highly inter-correlated and thus\nprovide a similar contribution to prediction accuracy, so-called spuriousness.\nTo address this limitation, we novelly leverage two causal desiderata,\nnon-spuriousness and efficiency, into rationalization from the causal inference\nperspective. We formally define a series of probabilities of causation based on\na newly proposed structural causal model of rationalization, with its\ntheoretical identification established as the main component of learning\nnecessary and sufficient rationales. The superior performance of the proposed\ncausal rationalization is demonstrated on real-world review and medical\ndatasets with extensive experiments compared to state-of-the-art methods.", "published": "2023-06-25 03:34:06", "link": "http://arxiv.org/abs/2306.14115v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ME", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Aeroacoustic Source Localization", "abstract": "The deconvolutional DAMAS algorithm can effectively eliminate the\nmisconceptions in the usually-used beamforming localization algorithm, allowing\nfor more accurate calculation of the source location as well as the intensity.\nWhen solving a linear system of equations, the DAMAS algorithm takes into\naccount the mutual influence of different locations, reducing or even\neliminating sidelobes and producing more accurate results.\n  This work first introduces the principles of the DAMAS algorithm. Then it\napplies both the beamforming algorithm and the DAMAS algorithm to simulate the\nlocalization of a single-frequency source from a 1.5 MW wind turbine, a complex\nline source with the text \"UCAS\" and a line source downstream of an airfoil\ntrailing edge. Finally, the work presents experimental localization results of\nthe source of a 1.5 MW wind turbine using both the beamforming algorithm and\nthe DAMAS algorithm.", "published": "2023-06-25 15:58:00", "link": "http://arxiv.org/abs/2306.14276v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Deep Learning Architecture with Spatio-Temporal Focusing for Detecting\n  Respiratory Anomalies", "abstract": "This paper presents a deep learning system applied for detecting anomalies\nfrom respiratory sound recordings. Our system initially performs audio feature\nextraction using Continuous Wavelet transformation. This transformation\nconverts the respiratory sound input into a two-dimensional spectrogram where\nboth spectral and temporal features are presented. Then, our proposed deep\nlearning architecture inspired by the Inception-residual-based backbone\nperforms the spatial-temporal focusing and multi-head attention mechanism to\nclassify respiratory anomalies. In this work, we evaluate our proposed models\non the benchmark SPRSound (The Open-Source SJTU Paediatric Respiratory Sound)\ndatabase proposed by the IEEE BioCAS 2023 challenge. As regards the Score\ncomputed by an average between the average score and harmonic score, our robust\nsystem has achieved Top-1 performance with Scores of 0.810, 0.667, 0.744, and\n0.608 in Tasks 1-1, 1-2, 2-1, and 2-2, respectively.", "published": "2023-06-25 12:24:53", "link": "http://arxiv.org/abs/2306.14929v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AV-SepFormer: Cross-Attention SepFormer for Audio-Visual Target Speaker\n  Extraction", "abstract": "Visual information can serve as an effective cue for target speaker\nextraction (TSE) and is vital to improving extraction performance. In this\npaper, we propose AV-SepFormer, a SepFormer-based attention dual-scale model\nthat utilizes cross- and self-attention to fuse and model features from audio\nand visual. AV-SepFormer splits the audio feature into a number of chunks,\nequivalent to the length of the visual feature. Then self- and cross-attention\nare employed to model and fuse the multi-modal features. Furthermore, we use a\nnovel 2D positional encoding, that introduces the positional information\nbetween and within chunks and provides significant gains over the traditional\npositional encoding. Our model has two key advantages: the time granularity of\naudio chunked feature is synchronized to the visual feature, which alleviates\nthe harm caused by the inconsistency of audio and video sampling rate; by\ncombining self- and cross-attention, feature fusion and speech extraction\nprocesses are unified within an attention paradigm. The experimental results\nshow that AV-SepFormer significantly outperforms other existing methods.", "published": "2023-06-25 08:31:12", "link": "http://arxiv.org/abs/2306.14170v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "PrimaDNN': A Characteristics-aware DNN Customization for Singing\n  Technique Detection", "abstract": "Professional vocalists modulate their voice timbre or pitch to make their\nvocal performance more expressive. Such fluctuations are called singing\ntechniques. Automatic detection of singing techniques from audio tracks can be\nbeneficial to understand how each singer expresses the performance, yet it can\nalso be difficult due to the wide variety of the singing techniques. A deep\nneural network (DNN) model can handle such variety; however, there might be a\npossibility that considering the characteristics of the data improves the\nperformance of singing technique detection. In this paper, we propose PrimaDNN,\na CRNN model with a characteristics-oriented improvement. The features of the\nmodel are: 1) input feature representation based on auxiliary pitch information\nand multi-resolution mel spectrograms, 2) Convolution module based on the\nSqueeze-and-excitation (SENet) and the Instance normalization. In the results\nof J-POP singing technique detection, PrimaDNN achieved the best results of\n44.9% at the overall macro-F measure, compared to conventional works. We also\nfound that the contribution of each component varies depending on the type of\nsinging technique.", "published": "2023-06-25 10:15:18", "link": "http://arxiv.org/abs/2306.14191v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
