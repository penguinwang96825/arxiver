{"title": "Multi-class Multilingual Classification of Wikipedia Articles Using\n  Extended Named Entity Tag Set", "abstract": "Wikipedia is a great source of general world knowledge which can guide NLP\nmodels better understand their motivation to make predictions. Structuring\nWikipedia is the initial step towards this goal which can facilitate fine-grain\nclassification of articles. In this work, we introduce the Shinra 5-Language\nCategorization Dataset (SHINRA-5LDS), a large multi-lingual and multi-labeled\nset of annotated Wikipedia articles in Japanese, English, French, German, and\nFarsi using Extended Named Entity (ENE) tag set. We evaluate the dataset using\nthe best models provided for ENE label set classification and show that the\ncurrently available classification models struggle with large datasets using\nfine-grained tag sets.", "published": "2019-09-14 01:47:09", "link": "http://arxiv.org/abs/1909.06502v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Universal Parent Model for Low-Resource Neural Machine Translation\n  Transfer", "abstract": "Transfer learning from a high-resource language pair `parent' has been proven\nto be an effective way to improve neural machine translation quality for\nlow-resource language pairs `children.' However, previous approaches build a\ncustom parent model or at least update an existing parent model's vocabulary\nfor each child language pair they wish to train, in an effort to align parent\nand child vocabularies. This is not a practical solution. It is wasteful to\ndevote the majority of training time for new language pairs to optimizing\nparameters on an unrelated data set. Further, this overhead reduces the utility\nof neural machine translation for deployment in humanitarian assistance\nscenarios, where extra time to deploy a new language pair can mean the\ndifference between life and death. In this work, we present a `universal'\npre-trained neural parent model with constant vocabulary that can be used as a\nstarting point for training practically any new low-resource language to a\nfixed target language. We demonstrate that our approach, which leverages\northography unification and a broad-coverage approach to subword\nidentification, generalizes well to several languages from a variety of\nfamilies, and that translation systems built with our approach can be built\nmore quickly than competing methods and with better quality as well.", "published": "2019-09-14 03:11:52", "link": "http://arxiv.org/abs/1909.06516v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ALTER: Auxiliary Text Rewriting Tool for Natural Language Generation", "abstract": "In this paper, we describe ALTER, an auxiliary text rewriting tool that\nfacilitates the rewriting process for natural language generation tasks, such\nas paraphrasing, text simplification, fairness-aware text rewriting, and text\nstyle transfer. Our tool is characterized by two features, i) recording of\nword-level revision histories and ii) flexible auxiliary edit support and\nfeedback to annotators. The text rewriting assist and traceable rewriting\nhistory are potentially beneficial to the future research of natural language\ngeneration.", "published": "2019-09-14 09:18:44", "link": "http://arxiv.org/abs/1909.06564v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Current Challenges in Spoken Dialogue Systems and Why They Are Critical\n  for Those Living with Dementia", "abstract": "Dialogue technologies such as Amazon's Alexa have the potential to transform\nthe healthcare industry. However, current systems are not yet naturally\ninteractive: they are often turn-based, have naive end-of-turn detection and\ncompletely ignore many types of verbal and visual feedback - such as\nbackchannels, hesitation markers, filled pauses, gaze, brow furrows and\ndisfluencies - that are crucial in guiding and managing the conversational\nprocess. This is especially important in the healthcare industry as target\nusers of Spoken Dialogue Systems (SDSs) are likely to be frail, older,\ndistracted or suffer from cognitive decline which impacts their ability to make\neffective use of current systems. In this paper, we outline some of the\nchallenges that are in urgent need of further research, including Incremental\nSpeech Recognition and a systematic study of the interactional patterns in\nconversation that are potentially diagnostic of dementia, and how these might\ninform research on and the design of the next generation of SDSs.", "published": "2019-09-14 18:03:57", "link": "http://arxiv.org/abs/1909.06644v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond BLEU: Training Neural Machine Translation with Semantic\n  Similarity", "abstract": "While most neural machine translation (NMT) systems are still trained using\nmaximum likelihood estimation, recent work has demonstrated that optimizing\nsystems to directly improve evaluation metrics such as BLEU can substantially\nimprove final translation accuracy. However, training with BLEU has some\nlimitations: it doesn't assign partial credit, it has a limited range of output\nvalues, and it can penalize semantically correct hypotheses if they differ\nlexically from the reference. In this paper, we introduce an alternative reward\nfunction for optimizing NMT systems that is based on recent work in semantic\nsimilarity. We evaluate on four disparate languages translated to English, and\nfind that training with our proposed metric results in better translations as\nevaluated by BLEU, semantic similarity, and human evaluation, and also that the\noptimization procedure converges faster. Analysis suggests that this is because\nthe proposed metric is more conducive to optimization, assigning partial credit\nand providing more diversity in scores than BLEU.", "published": "2019-09-14 23:15:20", "link": "http://arxiv.org/abs/1909.06694v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tree Transformer: Integrating Tree Structures into Self-Attention", "abstract": "Pre-training Transformer from large-scale raw texts and fine-tuning on the\ndesired task have achieved state-of-the-art results on diverse NLP tasks.\nHowever, it is unclear what the learned attention captures. The attention\ncomputed by attention heads seems not to match human intuitions about\nhierarchical structures. This paper proposes Tree Transformer, which adds an\nextra constraint to attention heads of the bidirectional Transformer encoder in\norder to encourage the attention heads to follow tree structures. The tree\nstructures can be automatically induced from raw texts by our proposed\n\"Constituent Attention\" module, which is simply implemented by self-attention\nbetween two adjacent words. With the same training procedure identical to BERT,\nthe experiments demonstrate the effectiveness of Tree Transformer in terms of\ninducing tree structures, better language modeling, and further learning more\nexplainable attention scores.", "published": "2019-09-14 17:49:37", "link": "http://arxiv.org/abs/1909.06639v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Delivering Cognitive Behavioral Therapy Using A Conversational\n  SocialRobot", "abstract": "Social robots are becoming an integrated part of our daily life due to their\nability to provide companionship and entertainment. A subfield of robotics,\nSocially Assistive Robotics (SAR), is particularly suitable for expanding these\nbenefits into the healthcare setting because of its unique ability to provide\ncognitive, social, and emotional support. This paper presents our recent\nresearch on developing SAR by evaluating the ability of a life-like\nconversational social robot, called Ryan, to administer internet-delivered\ncognitive behavioral therapy (iCBT) to older adults with depression. For Ryan\nto administer the therapy, we developed a dialogue-management system, called\nProgram-R. Using an accredited CBT manual for the treatment of depression, we\ncreated seven hour-long iCBT dialogues and integrated them into Program-R using\nArtificial Intelligence Markup Language (AIML). To assess the effectiveness of\nRobot-based iCBT and users' likability of our approach, we conducted an HRI\nstudy with a cohort of elderly people with mild-to-moderate depression over a\nperiod of four weeks. Quantitative analyses of participant's spoken responses\n(e.g. word count and sentiment analysis), face-scale mood scores, and exit\nsurveys, strongly support the notion robot-based iCBT is a viable alternative\nto traditional human-delivered therapy.", "published": "2019-09-14 20:31:34", "link": "http://arxiv.org/abs/1909.06670v1", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Harnessing Indirect Training Data for End-to-End Automatic Speech\n  Translation: Tricks of the Trade", "abstract": "For automatic speech translation (AST), end-to-end approaches are\noutperformed by cascaded models that transcribe with automatic speech\nrecognition (ASR), then translate with machine translation (MT). A major cause\nof the performance gap is that, while existing AST corpora are small, massive\ndatasets exist for both the ASR and MT subsystems. In this work, we evaluate\nseveral data augmentation and pretraining approaches for AST, by comparing all\non the same datasets. Simple data augmentation by translating ASR transcripts\nproves most effective on the English--French augmented LibriSpeech dataset,\nclosing the performance gap from 8.2 to 1.4 BLEU, compared to a very strong\ncascade that could directly utilize copious ASR and MT data. The same\nend-to-end approach plus fine-tuning closes the gap on the English--Romanian\nMuST-C dataset from 6.7 to 3.7 BLEU. In addition to these results, we present\npractical recommendations for augmentation and pretraining approaches. Finally,\nwe decrease the performance gap to 0.01 BLEU using a Transformer-based\narchitecture.", "published": "2019-09-14 03:05:30", "link": "http://arxiv.org/abs/1909.06515v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multilingual Graphemic Hybrid ASR with Massive Data Augmentation", "abstract": "Towards developing high-performing ASR for low-resource languages, approaches\nto address the lack of resources are to make use of data from multiple\nlanguages, and to augment the training data by creating acoustic variations. In\nthis work we present a single grapheme-based ASR model learned on 7\ngeographically proximal languages, using standard hybrid BLSTM-HMM acoustic\nmodels with lattice-free MMI objective. We build the single ASR grapheme set\nvia taking the union over each language-specific grapheme set, and we find such\nmultilingual graphemic hybrid ASR model can perform language-independent\nrecognition on all 7 languages, and substantially outperform each monolingual\nASR model. Secondly, we evaluate the efficacy of multiple data augmentation\nalternatives within language, as well as their complementarity with\nmultilingual modeling. Overall, we show that the proposed multilingual\ngraphemic hybrid ASR with various data augmentation can not only recognize any\nwithin training set languages, but also provide large ASR performance\nimprovements.", "published": "2019-09-14 03:46:49", "link": "http://arxiv.org/abs/1909.06522v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Bootstrapping non-parallel voice conversion from speaker-adaptive\n  text-to-speech", "abstract": "Voice conversion (VC) and text-to-speech (TTS) are two tasks that share a\nsimilar objective, generating speech with a target voice. However, they are\nusually developed independently under vastly different frameworks. In this\npaper, we propose a methodology to bootstrap a VC system from a pretrained\nspeaker-adaptive TTS model and unify the techniques as well as the\ninterpretations of these two tasks. Moreover by offloading the heavy data\ndemand to the training stage of the TTS model, our VC system can be built using\na small amount of target speaker speech data. It also opens up the possibility\nof using speech in a foreign unseen language to build the system. Our\nsubjective evaluations show that the proposed framework is able to not only\nachieve competitive performance in the standard intra-language scenario but\nalso adapt and convert using speech utterances in an unseen language.", "published": "2019-09-14 04:43:32", "link": "http://arxiv.org/abs/1909.06532v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-view and Multi-source Transfers in Neural Topic Modeling with\n  Pretrained Topic and Word Embeddings", "abstract": "Though word embeddings and topics are complementary representations, several\npast works have only used pre-trained word embeddings in (neural) topic\nmodeling to address data sparsity problem in short text or small collection of\ndocuments. However, no prior work has employed (pre-trained latent) topics in\ntransfer learning paradigm. In this paper, we propose an approach to (1)\nperform knowledge transfer using latent topics obtained from a large source\ncorpus, and (2) jointly transfer knowledge via the two representations (or\nviews) in neural topic modeling to improve topic quality, better deal with\npolysemy and data sparsity issues in a target corpus. In doing so, we first\naccumulate topics and word representations from one or many source corpora to\nbuild a pool of topics and word vectors. Then, we identify one or multiple\nrelevant source domain(s) and take advantage of corresponding topics and word\nfeatures via the respective pools to guide meaningful learning in the sparse\ntarget domain. We quantify the quality of topic and document representations\nvia generalization (perplexity), interpretability (topic coherence) and\ninformation retrieval (IR) using short-text, long-text, small and large\ndocument collections from news and medical domains. We have demonstrated the\nstate-of-the-art results on topic modeling with the proposed framework.", "published": "2019-09-14 09:16:05", "link": "http://arxiv.org/abs/1909.06563v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Integrating Source-channel and Attention-based Sequence-to-sequence\n  Models for Speech Recognition", "abstract": "This paper proposes a novel automatic speech recognition (ASR) framework\ncalled Integrated Source-Channel and Attention (ISCA) that combines the\nadvantages of traditional systems based on the noisy source-channel model (SC)\nand end-to-end style systems using attention-based sequence-to-sequence models.\nThe traditional SC system framework includes hidden Markov models and\nconnectionist temporal classification (CTC) based acoustic models, language\nmodels (LMs), and a decoding procedure based on a lexicon, whereas the\nend-to-end style attention-based system jointly models the whole process with a\nsingle model. By rescoring the hypotheses produced by traditional systems using\nend-to-end style systems based on an extended noisy source-channel model, ISCA\nallows structured knowledge to be easily incorporated via the SC-based model\nwhile exploiting the complementarity of the attention-based model. Experiments\non the AMI meeting corpus show that ISCA is able to give a relative word error\nrate reduction up to 21% over an individual system, and by 13% over an\nalternative method which also involves combining CTC and attention-based\nmodels.", "published": "2019-09-14 15:40:27", "link": "http://arxiv.org/abs/1909.06614v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Efficiency Metrics for Data-Driven Models: A Text Summarization Case\n  Study", "abstract": "Using data-driven models for solving text summarization or similar tasks has\nbecome very common in the last years. Yet most of the studies report basic\naccuracy scores only, and nothing is known about the ability of the proposed\nmodels to improve when trained on more data. In this paper, we define and\npropose three data efficiency metrics: data score efficiency, data time\ndeficiency and overall data efficiency. We also propose a simple scheme that\nuses those metrics and apply it for a more comprehensive evaluation of popular\nmethods on text summarization and title generation tasks. For the latter task,\nwe process and release a huge collection of 35 million abstract-title pairs\nfrom scientific articles. Our results reveal that among the tested models, the\nTransformer is the most efficient on both tasks.", "published": "2019-09-14 16:03:49", "link": "http://arxiv.org/abs/1909.06618v1", "categories": ["cs.CL", "cs.LG", "cs.PF"], "primary_category": "cs.CL"}
{"title": "Joint Wasserstein Autoencoders for Aligning Multimodal Embeddings", "abstract": "One of the key challenges in learning joint embeddings of multiple\nmodalities, e.g. of images and text, is to ensure coherent cross-modal\nsemantics that generalize across datasets. We propose to address this through\njoint Gaussian regularization of the latent representations. Building on\nWasserstein autoencoders (WAEs) to encode the input in each domain, we enforce\nthe latent embeddings to be similar to a Gaussian prior that is shared across\nthe two domains, ensuring compatible continuity of the encoded semantic\nrepresentations of images and texts. Semantic alignment is achieved through\nsupervision from matching image-text pairs. To show the benefits of our\nsemi-supervised representation, we apply it to cross-modal retrieval and phrase\nlocalization. We not only achieve state-of-the-art accuracy, but significantly\nbetter generalization across datasets, owing to the semantic continuity of the\nlatent space.", "published": "2019-09-14 17:25:03", "link": "http://arxiv.org/abs/1909.06635v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "musicnn: Pre-trained convolutional neural networks for music audio\n  tagging", "abstract": "Pronounced as \"musician\", the musicnn library contains a set of pre-trained\nmusically motivated convolutional neural networks for music audio tagging:\nhttps://github.com/jordipons/musicnn. This repository also includes some\npre-trained vgg-like baselines. These models can be used as out-of-the-box\nmusic audio taggers, as music feature extractors, or as pre-trained models for\ntransfer learning.\n  We also provide the code to train the aforementioned models:\nhttps://github.com/jordipons/musicnn-training. This framework also allows\nimplementing novel models. For example, a musically motivated convolutional\nneural network with an attention-based output layer (instead of the temporal\npooling layer) can achieve state-of-the-art results for music audio tagging:\n90.77 ROC-AUC / 38.61 PR-AUC on the MagnaTagATune dataset --- and 88.81 ROC-AUC\n/ 31.51 PR-AUC on the Million Song Dataset.", "published": "2019-09-14 18:52:47", "link": "http://arxiv.org/abs/1909.06654v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Ouroboros: On Accelerating Training of Transformer-Based Language Models", "abstract": "Language models are essential for natural language processing (NLP) tasks,\nsuch as machine translation and text summarization. Remarkable performance has\nbeen demonstrated recently across many NLP domains via a Transformer-based\nlanguage model with over a billion parameters, verifying the benefits of model\nsize. Model parallelism is required if a model is too large to fit in a single\ncomputing device. Current methods for model parallelism either suffer from\nbackward locking in backpropagation or are not applicable to language models.\nWe propose the first model-parallel algorithm that speeds the training of\nTransformer-based language models. We also prove that our proposed algorithm is\nguaranteed to converge to critical points for non-convex problems. Extensive\nexperiments on Transformer and Transformer-XL language models demonstrate that\nthe proposed algorithm obtains a much faster speedup beyond data parallelism,\nwith comparable or better accuracy. Code to reproduce experiments is to be\nfound at \\url{https://github.com/LaraQianYang/Ouroboros}.", "published": "2019-09-14 23:21:56", "link": "http://arxiv.org/abs/1909.06695v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "NeMo: a toolkit for building AI applications using Neural Modules", "abstract": "NeMo (Neural Modules) is a Python framework-agnostic toolkit for creating AI\napplications through re-usability, abstraction, and composition. NeMo is built\naround neural modules, conceptual blocks of neural networks that take typed\ninputs and produce typed outputs. Such modules typically represent data layers,\nencoders, decoders, language models, loss functions, or methods of combining\nactivations. NeMo makes it easy to combine and re-use these building blocks\nwhile providing a level of semantic correctness checking via its neural type\nsystem. The toolkit comes with extendable collections of pre-built modules for\nautomatic speech recognition and natural language processing. Furthermore, NeMo\nprovides built-in support for distributed training and mixed precision on\nlatest NVIDIA GPUs. NeMo is open-source https://github.com/NVIDIA/NeMo", "published": "2019-09-14 03:51:46", "link": "http://arxiv.org/abs/1909.09577v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "An Investigation Into On-device Personalization of End-to-end Automatic\n  Speech Recognition Models", "abstract": "Speaker-independent speech recognition systems trained with data from many\nusers are generally robust against speaker variability and work well for a\nlarge population of speakers. However, these systems do not always generalize\nwell for users with very different speech characteristics. This issue can be\naddressed by building personalized systems that are designed to work well for\neach specific user. In this paper, we investigate the idea of securely training\npersonalized end-to-end speech recognition models on mobile devices so that\nuser data and models never leave the device and are never stored on a server.\nWe study how the mobile training environment impacts performance by simulating\non-device data consumption. We conduct experiments using data collected from\nspeech impaired users for personalization. Our results show that\npersonalization achieved 63.7\\% relative word error rate reduction when trained\nin a server environment and 58.1% in a mobile environment. Moving to on-device\npersonalization resulted in 18.7% performance degradation, in exchange for\nimproved scalability and data privacy. To train the model on device, we split\nthe gradient computation into two and achieved 45% memory reduction at the\nexpense of 42% increase in training time.", "published": "2019-09-14 21:12:38", "link": "http://arxiv.org/abs/1909.06678v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
