{"title": "Nakdan: Professional Hebrew Diacritizer", "abstract": "We present a system for automatic diacritization of Hebrew text. The system\ncombines modern neural models with carefully curated declarative linguistic\nknowledge and comprehensive manually constructed tables and dictionaries.\nBesides providing state of the art diacritization accuracy, the system also\nsupports an interface for manual editing and correction of the automatic\noutput, and has several features which make it particularly useful for\npreparation of scientific editions of Hebrew texts. The system supports Modern\nHebrew, Rabbinic Hebrew and Poetic Hebrew. The system is freely accessible for\nall use at http://nakdanpro.dicta.org.il.", "published": "2020-05-07 08:15:55", "link": "http://arxiv.org/abs/2005.03312v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JASS: Japanese-specific Sequence to Sequence Pre-training for Neural\n  Machine Translation", "abstract": "Neural machine translation (NMT) needs large parallel corpora for\nstate-of-the-art translation quality. Low-resource NMT is typically addressed\nby transfer learning which leverages large monolingual or parallel corpora for\npre-training. Monolingual pre-training approaches such as MASS (MAsked Sequence\nto Sequence) are extremely effective in boosting NMT quality for languages with\nsmall parallel corpora. However, they do not account for linguistic information\nobtained using syntactic analyzers which is known to be invaluable for several\nNatural Language Processing (NLP) tasks. To this end, we propose JASS,\nJapanese-specific Sequence to Sequence, as a novel pre-training alternative to\nMASS for NMT involving Japanese as the source or target language. JASS is joint\nBMASS (Bunsetsu MASS) and BRSS (Bunsetsu Reordering Sequence to Sequence)\npre-training which focuses on Japanese linguistic units called bunsetsus. In\nour experiments on ASPEC Japanese--English and News Commentary\nJapanese--Russian translation we show that JASS can give results that are\ncompetitive with if not better than those given by MASS. Furthermore, we show\nfor the first time that joint MASS and JASS pre-training gives results that\nsignificantly surpass the individual methods indicating their complementary\nnature. We will release our code, pre-trained models and bunsetsu annotated\ndata as resources for researchers to use in their own NLP tasks.", "published": "2020-05-07 09:53:25", "link": "http://arxiv.org/abs/2005.03361v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "2kenize: Tying Subword Sequences for Chinese Script Conversion", "abstract": "Simplified Chinese to Traditional Chinese character conversion is a common\npreprocessing step in Chinese NLP. Despite this, current approaches have poor\nperformance because they do not take into account that a simplified Chinese\ncharacter can correspond to multiple traditional characters. Here, we propose a\nmodel that can disambiguate between mappings and convert between the two\nscripts. The model is based on subword segmentation, two language models, as\nwell as a method for mapping between subword sequences. We further construct\nbenchmark datasets for topic classification and script conversion. Our proposed\nmethod outperforms previous Chinese Character conversion approaches by 6 points\nin accuracy. These results are further confirmed in a downstream application,\nwhere 2kenize is used to convert pretraining dataset for topic classification.\nAn error analysis reveals that our method's particular strengths are in dealing\nwith code-mixing and named entities.", "published": "2020-05-07 10:53:05", "link": "http://arxiv.org/abs/2005.03375v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does Multi-Encoder Help? A Case Study on Context-Aware Neural Machine\n  Translation", "abstract": "In encoder-decoder neural models, multiple encoders are in general used to\nrepresent the contextual information in addition to the individual sentence. In\nthis paper, we investigate multi-encoder approaches in documentlevel neural\nmachine translation (NMT). Surprisingly, we find that the context encoder does\nnot only encode the surrounding sentences but also behaves as a noise\ngenerator. This makes us rethink the real benefits of multi-encoder in\ncontext-aware translation - some of the improvements come from robust training.\nWe compare several methods that introduce noise and/or well-tuned dropout setup\ninto the training of these encoders. Experimental results show that noisy\ntraining plays an important role in multi-encoder-based NMT, especially when\nthe training data is small. Also, we establish a new state-of-the-art on IWSLT\nFr-En task by careful use of noise generation and dropout methods.", "published": "2020-05-07 11:39:11", "link": "http://arxiv.org/abs/2005.03393v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Grained Analysis of Cross-Linguistic Syntactic Divergences", "abstract": "The patterns in which the syntax of different languages converges and\ndiverges are often used to inform work on cross-lingual transfer. Nevertheless,\nlittle empirical work has been done on quantifying the prevalence of different\nsyntactic divergences across language pairs. We propose a framework for\nextracting divergence patterns for any language pair from a parallel corpus,\nbuilding on Universal Dependencies. We show that our framework provides a\ndetailed picture of cross-language divergences, generalizes previous\napproaches, and lends itself to full automation. We further present a novel\ndataset, a manually word-aligned subset of the Parallel UD corpus in five\nlanguages, and use it to perform a detailed corpus study. We demonstrate the\nusefulness of the resulting analysis by showing that it can help account for\nperformance patterns of a cross-lingual parser.", "published": "2020-05-07 13:05:03", "link": "http://arxiv.org/abs/2005.03436v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Danish Gigaword Project", "abstract": "Danish language technology has been hindered by a lack of broad-coverage\ncorpora at the scale modern NLP prefers. This paper describes the Danish\nGigaword Corpus, the result of a focused effort to provide a diverse and\nfreely-available one billion word corpus of Danish text. The Danish Gigaword\ncorpus covers a wide array of time periods, domains, speakers' socio-economic\nstatus, and Danish dialects.", "published": "2020-05-07 14:40:56", "link": "http://arxiv.org/abs/2005.03521v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Tale of Two Perplexities: Sensitivity of Neural Language Models to\n  Lexical Retrieval Deficits in Dementia of the Alzheimer's Type", "abstract": "In recent years there has been a burgeoning interest in the use of\ncomputational methods to distinguish between elicited speech samples produced\nby patients with dementia, and those from healthy controls. The difference\nbetween perplexity estimates from two neural language models (LMs) - one\ntrained on transcripts of speech produced by healthy participants and the other\ntrained on transcripts from patients with dementia - as a single feature for\ndiagnostic classification of unseen transcripts has been shown to produce\nstate-of-the-art performance. However, little is known about why this approach\nis effective, and on account of the lack of case/control matching in the most\nwidely-used evaluation set of transcripts (DementiaBank), it is unclear if\nthese approaches are truly diagnostic, or are sensitive to other variables. In\nthis paper, we interrogate neural LMs trained on participants with and without\ndementia using synthetic narratives previously developed to simulate\nprogressive semantic dementia by manipulating lexical frequency. We find that\nperplexity of neural LMs is strongly and differentially associated with lexical\nfrequency, and that a mixture model resulting from interpolating control and\ndementia LMs improves upon the current state-of-the-art for models trained on\ntranscript text exclusively.", "published": "2020-05-07 16:22:48", "link": "http://arxiv.org/abs/2005.03593v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Exposure Bias, Hallucination and Domain Shift in Neural Machine\n  Translation", "abstract": "The standard training algorithm in neural machine translation (NMT) suffers\nfrom exposure bias, and alternative algorithms have been proposed to mitigate\nthis. However, the practical impact of exposure bias is under debate. In this\npaper, we link exposure bias to another well-known problem in NMT, namely the\ntendency to generate hallucinations under domain shift. In experiments on three\ndatasets with multiple test domains, we show that exposure bias is partially to\nblame for hallucinations, and that training with Minimum Risk Training, which\navoids exposure bias, can mitigate this. Our analysis explains why exposure\nbias is more problematic under domain shift, and also links exposure bias to\nthe beam search problem, i.e. performance deterioration with increasing beam\nsize. Our results provide a new justification for methods that reduce exposure\nbias: even if they do not increase performance on in-domain test sets, they can\nincrease model robustness to domain shift.", "published": "2020-05-07 17:46:02", "link": "http://arxiv.org/abs/2005.03642v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Systematic Assessment of Syntactic Generalization in Neural Language\n  Models", "abstract": "While state-of-the-art neural network models continue to achieve lower\nperplexity scores on language modeling benchmarks, it remains unknown whether\noptimizing for broad-coverage predictive performance leads to human-like\nsyntactic knowledge. Furthermore, existing work has not provided a clear\npicture about the model properties required to produce proper syntactic\ngeneralizations. We present a systematic evaluation of the syntactic knowledge\nof neural language models, testing 20 combinations of model types and data\nsizes on a set of 34 English-language syntactic test suites. We find\nsubstantial differences in syntactic generalization performance by model\narchitecture, with sequential models underperforming other architectures.\nFactorially manipulating model architecture and training dataset size (1M--40M\nwords), we find that variability in syntactic generalization performance is\nsubstantially greater by architecture than by dataset size for the corpora\ntested in our experiments. Our results also reveal a dissociation between\nperplexity and syntactic generalization performance.", "published": "2020-05-07 18:35:25", "link": "http://arxiv.org/abs/2005.03692v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LIIR at SemEval-2020 Task 12: A Cross-Lingual Augmentation Approach for\n  Multilingual Offensive Language Identification", "abstract": "This paper presents our system entitled `LIIR' for SemEval-2020 Task 12 on\nMultilingual Offensive Language Identification in Social Media (OffensEval 2).\nWe have participated in sub-task A for English, Danish, Greek, Arabic, and\nTurkish languages. We adapt and fine-tune the BERT and Multilingual Bert models\nmade available by Google AI for English and non-English languages respectively.\nFor the English language, we use a combination of two fine-tuned BERT models.\nFor other languages we propose a cross-lingual augmentation approach in order\nto enrich training data and we use Multilingual BERT to obtain sentence\nrepresentations. LIIR achieved rank 14/38, 18/47, 24/86, 24/54, and 25/40 in\nGreek, Turkish, English, Arabic, and Danish languages, respectively.", "published": "2020-05-07 18:45:48", "link": "http://arxiv.org/abs/2005.03695v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FEQA: A Question Answering Evaluation Framework for Faithfulness\n  Assessment in Abstractive Summarization", "abstract": "Neural abstractive summarization models are prone to generate content\ninconsistent with the source document, i.e. unfaithful. Existing automatic\nmetrics do not capture such mistakes effectively. We tackle the problem of\nevaluating faithfulness of a generated summary given its source document. We\nfirst collected human annotations of faithfulness for outputs from numerous\nmodels on two datasets. We find that current models exhibit a trade-off between\nabstractiveness and faithfulness: outputs with less word overlap with the\nsource document are more likely to be unfaithful. Next, we propose an automatic\nquestion answering (QA) based metric for faithfulness, FEQA, which leverages\nrecent advances in reading comprehension. Given question-answer pairs generated\nfrom the summary, a QA model extracts answers from the document; non-matched\nanswers indicate unfaithful information in the summary. Among metrics based on\nword overlap, embedding similarity, and learned language understanding models,\nour QA-based metric has significantly higher correlation with human\nfaithfulness scores, especially on highly abstractive summaries.", "published": "2020-05-07 21:00:08", "link": "http://arxiv.org/abs/2005.03754v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Phonotactic Complexity and its Trade-offs", "abstract": "We present methods for calculating a measure of phonotactic complexity---bits\nper phoneme---that permits a straightforward cross-linguistic comparison. When\ngiven a word, represented as a sequence of phonemic segments such as symbols in\nthe international phonetic alphabet, and a statistical model trained on a\nsample of word types from the language, we can approximately measure bits per\nphoneme using the negative log-probability of that word under the model. This\nsimple measure allows us to compare the entropy across languages, giving\ninsight into how complex a language's phonotactics are. Using a collection of\n1016 basic concept words across 106 languages, we demonstrate a very strong\nnegative correlation of -0.74 between bits per phoneme and the average length\nof words.", "published": "2020-05-07 21:36:59", "link": "http://arxiv.org/abs/2005.03774v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RNN-T Models Fail to Generalize to Out-of-Domain Audio: Causes and\n  Solutions", "abstract": "In recent years, all-neural end-to-end approaches have obtained\nstate-of-the-art results on several challenging automatic speech recognition\n(ASR) tasks. However, most existing works focus on building ASR models where\ntrain and test data are drawn from the same domain. This results in poor\ngeneralization characteristics on mismatched-domains: e.g., end-to-end models\ntrained on short segments perform poorly when evaluated on longer utterances.\nIn this work, we analyze the generalization properties of streaming and\nnon-streaming recurrent neural network transducer (RNN-T) based end-to-end\nmodels in order to identify model components that negatively affect\ngeneralization performance. We propose two solutions: combining multiple\nregularization techniques during training, and using dynamic overlapping\ninference. On a long-form YouTube test set, when the nonstreaming RNN-T model\nis trained with shorter segments of data, the proposed combination improves\nword error rate (WER) from 22.3% to 14.8%; when the streaming RNN-T model\ntrained on short Search queries, the proposed techniques improve WER on the\nYouTube set from 67.0% to 25.3%. Finally, when trained on Librispeech, we find\nthat dynamic overlapping inference improves WER on YouTube from 99.8% to 33.0%.", "published": "2020-05-07 06:24:47", "link": "http://arxiv.org/abs/2005.03271v3", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Adaptive Dialog Policy Learning with Hindsight and User Modeling", "abstract": "Reinforcement learning methods have been used to compute dialog policies from\nlanguage-based interaction experiences. Efficiency is of particular importance\nin dialog policy learning, because of the considerable cost of interacting with\npeople, and the very poor user experience from low-quality conversations.\nAiming at improving the efficiency of dialog policy learning, we develop\nalgorithm LHUA (Learning with Hindsight, User modeling, and Adaptation) that,\nfor the first time, enables dialog agents to adaptively learn with hindsight\nfrom both simulated and real users. Simulation and hindsight provide the dialog\nagent with more experience and more (positive) reinforcements respectively.\nExperimental results suggest that, in success rate and policy quality, LHUA\noutperforms competitive baselines from the literature, including its\nno-simulation, no-adaptation, and no-hindsight counterparts.", "published": "2020-05-07 07:43:43", "link": "http://arxiv.org/abs/2005.03299v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "MISA: Modality-Invariant and -Specific Representations for Multimodal\n  Sentiment Analysis", "abstract": "Multimodal Sentiment Analysis is an active area of research that leverages\nmultimodal signals for affective understanding of user-generated videos. The\npredominant approach, addressing this task, has been to develop sophisticated\nfusion techniques. However, the heterogeneous nature of the signals creates\ndistributional modality gaps that pose significant challenges. In this paper,\nwe aim to learn effective modality representations to aid the process of\nfusion. We propose a novel framework, MISA, which projects each modality to two\ndistinct subspaces. The first subspace is modality-invariant, where the\nrepresentations across modalities learn their commonalities and reduce the\nmodality gap. The second subspace is modality-specific, which is private to\neach modality and captures their characteristic features. These representations\nprovide a holistic view of the multimodal data, which is used for fusion that\nleads to task predictions. Our experiments on popular sentiment analysis\nbenchmarks, MOSI and MOSEI, demonstrate significant gains over state-of-the-art\nmodels. We also consider the task of Multimodal Humor Detection and experiment\non the recently proposed UR_FUNNY dataset. Here too, our model fares better\nthan strong baselines, establishing MISA as a useful multimodal framework.", "published": "2020-05-07 15:13:23", "link": "http://arxiv.org/abs/2005.03545v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Implicit Text Generation via Feature Matching", "abstract": "Generative feature matching network (GFMN) is an approach for training\nimplicit generative models for images by performing moment matching on features\nfrom pre-trained neural networks. In this paper, we present new GFMN\nformulations that are effective for sequential data. Our experimental results\nshow the effectiveness of the proposed method, SeqGFMN, for three distinct\ngeneration tasks in English: unconditional text generation, class-conditional\ntext generation, and unsupervised text style transfer. SeqGFMN is stable to\ntrain and outperforms various adversarial approaches for text generation and\ntext style transfer.", "published": "2020-05-07 16:16:24", "link": "http://arxiv.org/abs/2005.03588v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Where is Linked Data in Question Answering over Linked Data?", "abstract": "We argue that \"Question Answering with Knowledge Base\" and \"Question\nAnswering over Linked Data\" are currently two instances of the same problem,\ndespite one explicitly declares to deal with Linked Data. We point out the lack\nof existing methods to evaluate question answering on datasets which exploit\nexternal links to the rest of the cloud or share common schema. To this end, we\npropose the creation of new evaluation settings to leverage the advantages of\nthe Semantic Web to achieve AI-complete question answering.", "published": "2020-05-07 17:42:13", "link": "http://arxiv.org/abs/2005.03640v1", "categories": ["cs.CL", "cs.DB", "68T99", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Learning to Segment Actions from Observation and Narration", "abstract": "We apply a generative segmental model of task structure, guided by narration,\nto action segmentation in video. We focus on unsupervised and weakly-supervised\nsettings where no action labels are known during training. Despite its\nsimplicity, our model performs competitively with previous work on a dataset of\nnaturalistic instructional videos. Our model allows us to vary the sources of\nsupervision used in training, and we find that both task structure and\nnarrative language provide large benefits in segmentation quality.", "published": "2020-05-07 18:03:57", "link": "http://arxiv.org/abs/2005.03684v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for\n  Multi-Document Summarization", "abstract": "We study unsupervised multi-document summarization evaluation metrics, which\nrequire neither human-written reference summaries nor human annotations (e.g.\npreferences, ratings, etc.). We propose SUPERT, which rates the quality of a\nsummary by measuring its semantic similarity with a pseudo reference summary,\ni.e. selected salient sentences from the source documents, using contextualized\nembeddings and soft token alignment techniques. Compared to the\nstate-of-the-art unsupervised evaluation metrics, SUPERT correlates better with\nhuman ratings by 18-39%. Furthermore, we use SUPERT as rewards to guide a\nneural-based reinforcement learning summarizer, yielding favorable performance\ncompared to the state-of-the-art unsupervised summarizers. All source code is\navailable at https://github.com/yg211/acl20-ref-free-eval.", "published": "2020-05-07 19:54:24", "link": "http://arxiv.org/abs/2005.03724v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Mapping Natural Language Instructions to Mobile UI Action Sequences", "abstract": "We present a new problem: grounding natural language instructions to mobile\nuser interface actions, and create three new datasets for it. For full task\nevaluation, we create PIXELHELP, a corpus that pairs English instructions with\nactions performed by people on a mobile UI emulator. To scale training, we\ndecouple the language and action data by (a) annotating action phrase spans in\nHowTo instructions and (b) synthesizing grounded descriptions of actions for\nmobile user interfaces. We use a Transformer to extract action phrase tuples\nfrom long-range natural language instructions. A grounding Transformer then\ncontextually represents UI objects using both their content and screen position\nand connects them to object descriptions. Given a starting screen and\ninstruction, our model achieves 70.59% accuracy on predicting complete\nground-truth action sequences in PIXELHELP.", "published": "2020-05-07 21:41:40", "link": "http://arxiv.org/abs/2005.03776v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ContextNet: Improving Convolutional Neural Networks for Automatic Speech\n  Recognition with Global Context", "abstract": "Convolutional neural networks (CNN) have shown promising results for\nend-to-end speech recognition, albeit still behind other state-of-the-art\nmethods in performance. In this paper, we study how to bridge this gap and go\nbeyond with a novel CNN-RNN-transducer architecture, which we call ContextNet.\nContextNet features a fully convolutional encoder that incorporates global\ncontext information into convolution layers by adding squeeze-and-excitation\nmodules. In addition, we propose a simple scaling method that scales the widths\nof ContextNet that achieves good trade-off between computation and accuracy. We\ndemonstrate that on the widely used LibriSpeech benchmark, ContextNet achieves\na word error rate (WER) of 2.1%/4.6% without external language model (LM),\n1.9%/4.1% with LM and 2.9%/7.0% with only 10M parameters on the clean/noisy\nLibriSpeech test sets. This compares to the previous best published system of\n2.0%/4.6% with LM and 3.9%/11.3% with 20M parameters. The superiority of the\nproposed ContextNet model is also verified on a much larger internal dataset.", "published": "2020-05-07 01:03:18", "link": "http://arxiv.org/abs/2005.03191v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Quda: Natural Language Queries for Visual Data Analytics", "abstract": "The identification of analytic tasks from free text is critical for\nvisualization-oriented natural language interfaces (V-NLIs) to suggest\neffective visualizations. However, it is challenging due to the ambiguity and\ncomplexity nature of human language. To address this challenge, we present a\nnew dataset, called Quda, that aims to help V-NLIs recognize analytic tasks\nfrom free-form natural language by training and evaluating cutting-edge\nmulti-label classification models. Our dataset contains $14,035$ diverse user\nqueries, and each is annotated with one or multiple analytic tasks. We achieve\nthis goal by first gathering seed queries with data analysts and then employing\nextensive crowd force for paraphrase generation and validation. We demonstrate\nthe usefulness of Quda through three applications. This work is the first\nattempt to construct a large-scale corpus for recognizing analytic tasks. With\nthe release of Quda, we hope it will boost the research and development of\nV-NLIs in data analysis and visualization.", "published": "2020-05-07 05:35:16", "link": "http://arxiv.org/abs/2005.03257v5", "categories": ["cs.CL", "cs.HC", "cs.IR"], "primary_category": "cs.CL"}
{"title": "DramaQA: Character-Centered Video Story Understanding with Hierarchical\n  QA", "abstract": "Despite recent progress on computer vision and natural language processing,\ndeveloping a machine that can understand video story is still hard to achieve\ndue to the intrinsic difficulty of video story. Moreover, researches on how to\nevaluate the degree of video understanding based on human cognitive process\nhave not progressed as yet. In this paper, we propose a novel video question\nanswering (Video QA) task, DramaQA, for a comprehensive understanding of the\nvideo story. The DramaQA focuses on two perspectives: 1) Hierarchical QAs as an\nevaluation metric based on the cognitive developmental stages of human\nintelligence. 2) Character-centered video annotations to model local coherence\nof the story. Our dataset is built upon the TV drama \"Another Miss Oh\" and it\ncontains 17,983 QA pairs from 23,928 various length video clips, with each QA\npair belonging to one of four difficulty levels. We provide 217,308 annotated\nimages with rich character-centered annotations, including visual bounding\nboxes, behaviors and emotions of main characters, and coreference resolved\nscripts. Additionally, we suggest Multi-level Context Matching model which\nhierarchically understands character-centered representations of video to\nanswer questions. We release our dataset and model publicly for research\npurposes, and we expect our work to provide a new perspective on video story\nunderstanding research.", "published": "2020-05-07 09:44:58", "link": "http://arxiv.org/abs/2005.03356v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "The Perceptimatic English Benchmark for Speech Perception Models", "abstract": "We present the Perceptimatic English Benchmark, an open experimental\nbenchmark for evaluating quantitative models of speech perception in English.\nThe benchmark consists of ABX stimuli along with the responses of 91 American\nEnglish-speaking listeners. The stimuli test discrimination of a large number\nof English and French phonemic contrasts. They are extracted directly from\ncorpora of read speech, making them appropriate for evaluating statistical\nacoustic models (such as those used in automatic speech recognition) trained on\ntypical speech data sets. We show that phone discrimination is correlated with\nseveral types of models, and give recommendations for researchers seeking\neasily calculated norms of acoustic distance on experimental stimuli. We show\nthat DeepSpeech, a standard English speech recognizer, is more specialized on\nEnglish phoneme discrimination than English listeners, and is poorly correlated\nwith their behaviour, even though it yields a low error on the decision task\ngiven to humans.", "published": "2020-05-07 12:35:44", "link": "http://arxiv.org/abs/2005.03418v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Learning Robust Models for e-Commerce Product Search", "abstract": "Showing items that do not match search query intent degrades customer\nexperience in e-commerce. These mismatches result from counterfactual biases of\nthe ranking algorithms toward noisy behavioral signals such as clicks and\npurchases in the search logs. Mitigating the problem requires a large labeled\ndataset, which is expensive and time-consuming to obtain. In this paper, we\ndevelop a deep, end-to-end model that learns to effectively classify mismatches\nand to generate hard mismatched examples to improve the classifier. We train\nthe model end-to-end by introducing a latent variable into the cross-entropy\nloss that alternates between using the real and generated samples. This not\nonly makes the classifier more robust but also boosts the overall ranking\nperformance. Our model achieves a relative gain compared to baselines by over\n26% in F-score, and over 17% in Area Under PR curve. On live search traffic,\nour model gains significant improvement in multiple countries.", "published": "2020-05-07 17:22:21", "link": "http://arxiv.org/abs/2005.03624v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "YANG2UML: Bijective Transformation and Simplification of YANG to UML", "abstract": "Software Defined Networking is currently revolutionizing computer networking\nby decoupling the network control (control plane) from the forwarding functions\n(data plane) enabling the network control to become directly programmable and\nthe underlying infrastructure to be abstracted for applications and network\nservices. Next to the well-known OpenFlow protocol, the XML-based NETCONF\nprotocol is also an important means for exchanging configuration information\nfrom a management platform and is nowadays even part of OpenFlow. In\ncombination with NETCONF, YANG is the corresponding protocol that defines the\nassociated data structures supporting virtually all network configuration\nprotocols. YANG itself is a semantically rich language, which -- in order to\nfacilitate familiarization with the relevant subject -- is often visualized to\ninvolve other experts or developers and to support them by their daily work\n(writing applications which make use of YANG). In order to support this\nprocess, this paper presents an novel approach to optimize and simplify YANG\ndata models to assist further discussions with the management and\nimplementations (especially of interfaces) to reduce complexity. Therefore, we\nhave defined a bidirectional mapping of YANG to UML and developed a tool that\nrenders the created UML diagrams. This combines the benefits to use the formal\nlanguage YANG with automatically maintained UML diagrams to involve other\nexperts or developers, closing the gap between technically improved data models\nand their human readability.", "published": "2020-05-07 07:29:49", "link": "http://arxiv.org/abs/2005.03292v1", "categories": ["cs.SE", "cs.CL", "cs.DC", "cs.NI", "cs.PL", "cs.SY", "eess.SY"], "primary_category": "cs.SE"}
{"title": "Scyclone: High-Quality and Parallel-Data-Free Voice Conversion Using\n  Spectrogram and Cycle-Consistent Adversarial Networks", "abstract": "This paper proposes Scyclone, a high-quality voice conversion (VC) technique\nwithout parallel data training. Scyclone improves speech naturalness and\nspeaker similarity of the converted speech by introducing CycleGAN-based\nspectrogram conversion with a simplified WaveRNN-based vocoder. In Scyclone, a\nlinear spectrogram is used as the conversion features instead of vocoder\nparameters, which avoids quality degradation due to extraction errors in\nfundamental frequency and voiced/unvoiced parameters. The spectrogram of source\nand target speakers are modeled by modified CycleGAN networks, and the waveform\nis reconstructed using the simplified WaveRNN with a single Gaussian\nprobability density function. The subjective experiments with completely\nunpaired training data show that Scyclone is significantly better than\nCycleGAN-VC2, one of the existing state-of-the-art parallel-data-free VC\ntechniques.", "published": "2020-05-07 09:05:34", "link": "http://arxiv.org/abs/2005.03334v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "AutoSpeech: Neural Architecture Search for Speaker Recognition", "abstract": "Speaker recognition systems based on Convolutional Neural Networks (CNNs) are\noften built with off-the-shelf backbones such as VGG-Net or ResNet. However,\nthese backbones were originally proposed for image classification, and\ntherefore may not be naturally fit for speaker recognition. Due to the\nprohibitive complexity of manually exploring the design space, we propose the\nfirst neural architecture search approach approach for the speaker recognition\ntasks, named as AutoSpeech. Our algorithm first identifies the optimal\noperation combination in a neural cell and then derives a CNN model by stacking\nthe neural cell for multiple times. The final speaker recognition model can be\nobtained by training the derived CNN model through the standard scheme. To\nevaluate the proposed approach, we conduct experiments on both speaker\nidentification and speaker verification tasks using the VoxCeleb1 dataset.\nResults demonstrate that the derived CNN architectures from the proposed\napproach significantly outperform current speaker recognition systems based on\nVGG-M, ResNet-18, and ResNet-34 back-bones, while enjoying lower model\ncomplexity.", "published": "2020-05-07 02:53:47", "link": "http://arxiv.org/abs/2005.03215v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Domain Aware Training for Far-field Small-footprint Keyword Spotting", "abstract": "In this paper, we focus on the task of small-footprint keyword spotting under\nthe far-field scenario. Far-field environments are commonly encountered in\nreal-life speech applications, causing severe degradation of performance due to\nroom reverberation and various kinds of noises. Our baseline system is built on\nthe convolutional neural network trained with pooled data of both far-field and\nclose-talking speech. To cope with the distortions, we develop three domain\naware training systems, including the domain embedding system, the deep CORAL\nsystem, and the multi-task learning system. These methods incorporate domain\nknowledge into network training and improve the performance of the keyword\nclassifier on far-field conditions. Experimental results show that our proposed\nmethods manage to maintain the performance on the close-talking speech and\nachieve significant improvement on the far-field test set.", "published": "2020-05-07 17:38:39", "link": "http://arxiv.org/abs/2005.03633v3", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Cotatron: Transcription-Guided Speech Encoder for Any-to-Many Voice\n  Conversion without Parallel Data", "abstract": "We propose Cotatron, a transcription-guided speech encoder for\nspeaker-independent linguistic representation. Cotatron is based on the\nmultispeaker TTS architecture and can be trained with conventional TTS\ndatasets. We train a voice conversion system to reconstruct speech with\nCotatron features, which is similar to the previous methods based on Phonetic\nPosteriorgram (PPG). By training and evaluating our system with 108 speakers\nfrom the VCTK dataset, we outperform the previous method in terms of both\nnaturalness and speaker similarity. Our system can also convert speech from\nspeakers that are unseen during training, and utilize ASR to automate the\ntranscription with minimal reduction of the performance. Audio samples are\navailable at https://mindslab-ai.github.io/cotatron, and the code with a\npre-trained model will be made available soon.", "published": "2020-05-07 07:37:31", "link": "http://arxiv.org/abs/2005.03295v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Segment Aggregation for short utterances speaker verification using raw\n  waveforms", "abstract": "Most studies on speaker verification systems focus on long-duration\nutterances, which are composed of sufficient phonetic information. However, the\nperformances of these systems are known to degrade when short-duration\nutterances are inputted due to the lack of phonetic information as compared to\nthe long utterances. In this paper, we propose a method that compensates for\nthe performance degradation of speaker verification for short utterances,\nreferred to as \"segment aggregation\". The proposed method adopts an\nensemble-based design to improve the stability and accuracy of speaker\nverification systems. The proposed method segments an input utterance into\nseveral short utterances and then aggregates the segment embeddings extracted\nfrom the segmented inputs to compose a speaker embedding. Then, this method\nsimultaneously trains the segment embeddings and the aggregated speaker\nembedding. In addition, we also modified the teacher-student learning method\nfor the proposed method. Experimental results on different input duration using\nthe VoxCeleb1 test set demonstrate that the proposed technique improves speaker\nverification performance by about 45.37% relatively compared to the baseline\nsystem with 1-second test utterance condition.", "published": "2020-05-07 08:57:22", "link": "http://arxiv.org/abs/2005.03329v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
