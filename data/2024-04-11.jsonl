{"title": "Scalable Language Model with Generalized Continual Learning", "abstract": "Continual learning has gained increasing importance as it facilitates the\nacquisition and refinement of scalable knowledge and skills in language models.\nHowever, existing methods typically encounter strict limitations and challenges\nin real-world scenarios, such as reliance on experience replay, optimization\nconstraints, and inference task-ID. In this study, we introduce the Scalable\nLanguage Model (SLM) to overcome these limitations within a more challenging\nand generalized setting, representing a significant advancement toward\npractical applications for continual learning. Specifically, we propose the\nJoint Adaptive Re-Parameterization (JARe), integrated with Dynamic Task-related\nKnowledge Retrieval (DTKR), to enable adaptive adjustment of language models\nbased on specific downstream tasks. This approach leverages the task\ndistribution within the vector space, aiming to achieve a smooth and effortless\ncontinual learning process. Our method demonstrates state-of-the-art\nperformance on diverse backbones and benchmarks, achieving effective continual\nlearning in both full-set and few-shot scenarios with minimal forgetting.\nMoreover, while prior research primarily focused on a single task type such as\nclassification, our study goes beyond, with the large language model, i.e.,\nLLaMA-2, to explore the effects across diverse domains and task types, such\nthat a single language model can be decently scaled to broader applications.", "published": "2024-04-11 04:22:15", "link": "http://arxiv.org/abs/2404.07470v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Data Augmentation for Process Information Extraction", "abstract": "Business Process Modeling projects often require formal process models as a\ncentral component. High costs associated with the creation of such formal\nprocess models motivated many different fields of research aimed at automated\ngeneration of process models from readily available data. These include process\nmining on event logs, and generating business process models from natural\nlanguage texts. Research in the latter field is regularly faced with the\nproblem of limited data availability, hindering both evaluation and development\nof new techniques, especially learning-based ones.\n  To overcome this data scarcity issue, in this paper we investigate the\napplication of data augmentation for natural language text data. Data\naugmentation methods are well established in machine learning for creating new,\nsynthetic data without human assistance. We find that many of these methods are\napplicable to the task of business process information extraction, improving\nthe accuracy of extraction. Our study shows, that data augmentation is an\nimportant component in enabling machine learning methods for the task of\nbusiness process model generation from natural language text, where currently\nmostly rule-based systems are still state of the art. Simple data augmentation\ntechniques improved the $F_1$ score of mention extraction by 2.9 percentage\npoints, and the $F_1$ of relation extraction by $4.5$. To better understand how\ndata augmentation alters human annotated texts, we analyze the resulting text,\nvisualizing and discussing the properties of augmented textual data.\n  We make all code and experiments results publicly available.", "published": "2024-04-11 06:32:03", "link": "http://arxiv.org/abs/2404.07501v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Best Practices and Lessons Learned on Synthetic Data", "abstract": "The success of AI models relies on the availability of large, diverse, and\nhigh-quality datasets, which can be challenging to obtain due to data scarcity,\nprivacy concerns, and high costs. Synthetic data has emerged as a promising\nsolution by generating artificial data that mimics real-world patterns. This\npaper provides an overview of synthetic data research, discussing its\napplications, challenges, and future directions. We present empirical evidence\nfrom prior art to demonstrate its effectiveness and highlight the importance of\nensuring its factuality, fidelity, and unbiasedness. We emphasize the need for\nresponsible use of synthetic data to build more powerful, inclusive, and\ntrustworthy language models.", "published": "2024-04-11 06:34:17", "link": "http://arxiv.org/abs/2404.07503v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does In-Context Learning Really Learn? Rethinking How Large Language\n  Models Respond and Solve Tasks via In-Context Learning", "abstract": "In-context Learning (ICL) has emerged as a powerful capability alongside the\ndevelopment of scaled-up large language models (LLMs). By instructing LLMs\nusing few-shot demonstrative examples, ICL enables them to perform a wide range\nof tasks without updating millions of parameters. However, the precise\ncontributions of demonstrations towards improving end-task performance have not\nbeen thoroughly investigated in recent analytical studies. In this paper, we\nempirically decompose the overall performance of ICL into three dimensions,\nlabel space, format, and discrimination, and we evaluate four general-purpose\nLLMs across a diverse range of tasks. Counter-intuitively, we find that the\ndemonstrations have a marginal impact on provoking discriminative knowledge of\nlanguage models. However, ICL exhibits significant efficacy in regulating the\nlabel space and format, which helps LLMs respond to desired label words. We\nthen demonstrate that this ability functions similar to detailed instructions\nfor LLMs to follow. We additionally provide an in-depth analysis of the\nmechanism of retrieval helping with ICL. Our findings demonstrate that\nretrieving the semantically similar examples notably boosts the model's\ndiscriminative capability. However, we also observe a trade-off in selecting\ngood in-context examples regarding label diversity.", "published": "2024-04-11 08:20:10", "link": "http://arxiv.org/abs/2404.07546v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comments as Natural Logic Pivots: Improve Code Generation via Comment\n  Perspective", "abstract": "Code generation aims to understand the problem description and generate\ncorresponding code snippets, where existing works generally decompose such\ncomplex tasks into intermediate steps by prompting strategies, such as\nChain-of-Thought and its variants. While these studies have achieved some\nsuccess, their effectiveness is highly dependent on the capabilities of\nadvanced Large Language Models (LLMs) such as GPT-4, particularly in terms of\nAPI calls, which significantly limits their practical applicability.\nConsequently, how to enhance the code generation capabilities of small and\nmedium-scale code LLMs without significantly increasing training costs is an\nappealing challenge. In this paper, we suggest that code comments are the\nnatural logic pivot between natural language and code language and propose\nusing comments to boost the code generation ability of code LLMs. Concretely,\nwe propose MANGO (comMents As Natural loGic pivOts), including a comment\ncontrastive training strategy and a corresponding logical comment decoding\nstrategy. Experiments are performed on HumanEval and MBPP, utilizing StarCoder\nand WizardCoder as backbone models, and encompassing model parameter sizes\nbetween 3B and 7B. The results indicate that MANGO significantly improves the\ncode pass rate based on the strong baselines. Meanwhile, the robustness of the\nlogical comment decoding strategy is notably higher than the Chain-of-thoughts\nprompting. The code is publicly available at\n\\url{https://github.com/pppa2019/Mango}.", "published": "2024-04-11 08:30:46", "link": "http://arxiv.org/abs/2404.07549v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UltraEval: A Lightweight Platform for Flexible and Comprehensive\n  Evaluation for LLMs", "abstract": "Evaluation is pivotal for refining Large Language Models (LLMs), pinpointing\ntheir capabilities, and guiding enhancements. The rapid development of LLMs\ncalls for a lightweight and easy-to-use framework for swift evaluation\ndeployment. However, considering various implementation details, developing a\ncomprehensive evaluation platform is never easy. Existing platforms are often\ncomplex and poorly modularized, hindering seamless incorporation into research\nworkflows. This paper introduces UltraEval, a user-friendly evaluation\nframework characterized by its lightweight nature, comprehensiveness,\nmodularity, and efficiency. We identify and reimplement three core components\nof model evaluation (models, data, and metrics). The resulting composability\nallows for the free combination of different models, tasks, prompts,\nbenchmarks, and metrics within a unified evaluation workflow. Additionally,\nUltraEval supports diverse models owing to a unified HTTP service and provides\nsufficient inference acceleration. UltraEval is now available for researchers\npublicly.", "published": "2024-04-11 09:17:12", "link": "http://arxiv.org/abs/2404.07584v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why do small language models underperform? Studying Language Model\n  Saturation via the Softmax Bottleneck", "abstract": "Recent advances in language modeling consist in pretraining highly\nparameterized neural networks on extremely large web-mined text corpora.\nTraining and inference with such models can be costly in practice, which\nincentivizes the use of smaller counterparts. However, it has been observed\nthat smaller models can suffer from saturation, characterized as a drop in\nperformance at some advanced point in training followed by a plateau. In this\npaper, we find that such saturation can be explained by a mismatch between the\nhidden dimension of smaller models and the high rank of the target contextual\nprobability distribution. This mismatch affects the performance of the linear\nprediction head used in such models through the well-known softmax bottleneck\nphenomenon. We measure the effect of the softmax bottleneck in various settings\nand find that models based on less than 1000 hidden dimensions tend to adopt\ndegenerate latent representations in late pretraining, which leads to reduced\nevaluation performance.", "published": "2024-04-11 11:10:36", "link": "http://arxiv.org/abs/2404.07647v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "rollama: An R package for using generative large language models through\n  Ollama", "abstract": "rollama is an R package that wraps the Ollama API, which allows you to run\ndifferent Generative Large Language Models (GLLM) locally. The package and\nlearning material focus on making it easy to use Ollama for annotating textual\nor imagine data with open-source models as well as use these models for\ndocument embedding. But users can use or extend rollama to do essentially\nanything else that is possible through OpenAI's API, yet more private,\nreproducible and for free.", "published": "2024-04-11 11:37:18", "link": "http://arxiv.org/abs/2404.07654v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Generation and Evaluation of Reading Comprehension Test Items\n  with Large Language Models", "abstract": "Reading comprehension tests are used in a variety of applications, reaching\nfrom education to assessing the comprehensibility of simplified texts. However,\ncreating such tests manually and ensuring their quality is difficult and\ntime-consuming. In this paper, we explore how large language models (LLMs) can\nbe used to generate and evaluate multiple-choice reading comprehension items.\nTo this end, we compiled a dataset of German reading comprehension items and\ndeveloped a new protocol for human and automatic evaluation, including a metric\nwe call text informativity, which is based on guessability and answerability.\nWe then used this protocol and the dataset to evaluate the quality of items\ngenerated by Llama 2 and GPT-4. Our results suggest that both models are\ncapable of generating items of acceptable quality in a zero-shot setting, but\nGPT-4 clearly outperforms Llama 2. We also show that LLMs can be used for\nautomatic evaluation by eliciting item reponses from them. In this scenario,\nevaluation results with GPT-4 were the most similar to human annotators.\nOverall, zero-shot generation with LLMs is a promising approach for generating\nand evaluating reading comprehension test items, in particular for languages\nwithout large amounts of available data.", "published": "2024-04-11 13:11:21", "link": "http://arxiv.org/abs/2404.07720v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Letter Positional Probabilities to Assess Word Complexity", "abstract": "Word complexity is defined in a number of different ways. Psycholinguistic,\nmorphological and lexical proxies are often used. Human ratings are also used.\nThe problem here is that these proxies do not measure complexity directly, and\nhuman ratings are susceptible to subjective bias. In this study we contend that\nsome form of 'latent complexity' can be approximated by using samples of simple\nand complex words. We use a sample of 'simple' words from primary school\npicture books and a sample of 'complex' words from high school and academic\nsettings. In order to analyse the differences between these classes, we look at\nthe letter positional probabilities (LPPs). We find strong statistical\nassociations between several LPPs and complexity. For example, simple words are\nsignificantly (p<.001) more likely to start with w, b, s, h, g, k, j, t, y or\nf, while complex words are significantly (p<.001) more likely to start with i,\na, e, r, v, u or d. We find similar strong associations for subsequent letter\npositions, with 84 letter-position variables in the first 6 positions being\nsignificant at the p<.001 level. We then use LPPs as variables in creating a\nclassifier which can classify the two classes with an 83% accuracy. We test\nthese findings using a second data set, with 66 LPPs significant (p<.001) in\nthe first 6 positions common to both datasets. We use these 66 variables to\ncreate a classifier that is able to classify a third dataset with an accuracy\nof 70%. Finally, we create a fourth sample by combining the extreme high and\nlow scoring words generated by three classifiers built on the first three\nseparate datasets and use this sample to build a classifier which has an\naccuracy of 97%. We use this to score the four levels of English word groups\nfrom an ESL program.", "published": "2024-04-11 14:06:39", "link": "http://arxiv.org/abs/2404.07768v4", "categories": ["cs.CL", "I.2.7; J.4"], "primary_category": "cs.CL"}
{"title": "Lexical Complexity Prediction and Lexical Simplification for Catalan and\n  Spanish: Resource Creation, Quality Assessment, and Ethical Considerations", "abstract": "Automatic lexical simplification is a task to substitute lexical items that\nmay be unfamiliar and difficult to understand with easier and more common\nwords. This paper presents the description and analysis of two novel datasets\nfor lexical simplification in Spanish and Catalan. This dataset represents the\nfirst of its kind in Catalan and a substantial addition to the sparse data on\nautomatic lexical simplification which is available for Spanish. Specifically,\nit is the first dataset for Spanish which includes scalar ratings of the\nunderstanding difficulty of lexical items. In addition, we present a detailed\nanalysis aiming at assessing the appropriateness and ethical dimensions of the\ndata for the lexical simplification task.", "published": "2024-04-11 14:57:19", "link": "http://arxiv.org/abs/2404.07814v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question Generation in Knowledge-Driven Dialog: Explainability and\n  Evaluation", "abstract": "We explore question generation in the context of knowledge-grounded dialogs\nfocusing on explainability and evaluation. Inspired by previous work on\nplanning-based summarisation, we present a model which instead of directly\ngenerating a question, sequentially predicts first a fact then a question. We\nevaluate our approach on 37k test dialogs adapted from the KGConv dataset and\nwe show that, although more demanding in terms of inference, our approach\nperforms on par with a standard model which solely generates a question while\nallowing for a detailed referenceless evaluation of the model behaviour in\nterms of relevance, factuality and pronominalisation.", "published": "2024-04-11 15:24:50", "link": "http://arxiv.org/abs/2404.07836v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HGRN2: Gated Linear RNNs with State Expansion", "abstract": "Hierarchically gated linear RNN (HGRN, \\citealt{HGRN}) has demonstrated\ncompetitive training speed and performance in language modeling while offering\nefficient inference. However, the recurrent state size of HGRN remains\nrelatively small, limiting its expressiveness. To address this issue, we\nintroduce a simple outer product-based state expansion mechanism, which\nsignificantly enlarges the recurrent state size without introducing any\nadditional parameters. This enhancement also provides a linear attention\ninterpretation for HGRN2, enabling hardware-efficient training. Our extensive\nexperiments verify the advantage of HGRN2 over HGRN consistently across\ndifferent settings and competitive with other recurrent models.", "published": "2024-04-11 16:43:03", "link": "http://arxiv.org/abs/2404.07904v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AmpleGCG: Learning a Universal and Transferable Generative Model of\n  Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs", "abstract": "As large language models (LLMs) become increasingly prevalent and integrated\ninto autonomous systems, ensuring their safety is imperative. Despite\nsignificant strides toward safety alignment, recent work\nGCG~\\citep{zou2023universal} proposes a discrete token optimization algorithm\nand selects the single suffix with the lowest loss to successfully jailbreak\naligned LLMs. In this work, we first discuss the drawbacks of solely picking\nthe suffix with the lowest loss during GCG optimization for jailbreaking and\nuncover the missed successful suffixes during the intermediate steps. Moreover,\nwe utilize those successful suffixes as training data to learn a generative\nmodel, named AmpleGCG, which captures the distribution of adversarial suffixes\ngiven a harmful query and enables the rapid generation of hundreds of suffixes\nfor any harmful queries in seconds. AmpleGCG achieves near 100\\% attack success\nrate (ASR) on two aligned LLMs (Llama-2-7B-chat and Vicuna-7B), surpassing two\nstrongest attack baselines. More interestingly, AmpleGCG also transfers\nseamlessly to attack different models, including closed-source LLMs, achieving\na 99\\% ASR on the latest GPT-3.5. To summarize, our work amplifies the impact\nof GCG by training a generative model of adversarial suffixes that is universal\nto any harmful queries and transferable from attacking open-source LLMs to\nclosed-source LLMs. In addition, it can generate 200 adversarial suffixes for\none harmful query in only 4 seconds, rendering it more challenging to defend.", "published": "2024-04-11 17:05:50", "link": "http://arxiv.org/abs/2404.07921v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MSciNLI: A Diverse Benchmark for Scientific Natural Language Inference", "abstract": "The task of scientific Natural Language Inference (NLI) involves predicting\nthe semantic relation between two sentences extracted from research articles.\nThis task was recently proposed along with a new dataset called SciNLI derived\nfrom papers published in the computational linguistics domain. In this paper,\nwe aim to introduce diversity in the scientific NLI task and present MSciNLI, a\ndataset containing 132,320 sentence pairs extracted from five new scientific\ndomains. The availability of multiple domains makes it possible to study domain\nshift for scientific NLI. We establish strong baselines on MSciNLI by\nfine-tuning Pre-trained Language Models (PLMs) and prompting Large Language\nModels (LLMs). The highest Macro F1 scores of PLM and LLM baselines are 77.21%\nand 51.77%, respectively, illustrating that MSciNLI is challenging for both\ntypes of models. Furthermore, we show that domain shift degrades the\nperformance of scientific NLI models which demonstrates the diverse\ncharacteristics of different domains in our dataset. Finally, we use both\nscientific NLI datasets in an intermediate task transfer learning setting and\nshow that they can improve the performance of downstream tasks in the\nscientific domain. We make our dataset and code available on Github.", "published": "2024-04-11 18:12:12", "link": "http://arxiv.org/abs/2404.08066v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distilling Algorithmic Reasoning from LLMs via Explaining Solution\n  Programs", "abstract": "Distilling explicit chain-of-thought reasoning paths has emerged as an\neffective method for improving the reasoning abilities of large language models\n(LLMs) across various tasks. However, when tackling complex tasks that pose\nsignificant challenges for state-of-the-art models, this technique often\nstruggles to produce effective chains of thought that lead to correct answers.\nIn this work, we propose a novel approach to distill reasoning abilities from\nLLMs by leveraging their capacity to explain solutions. We apply our method to\nsolving competitive-level programming challenges. More specifically, we employ\nan LLM to generate explanations for a set of <problem, solution-program> pairs,\nthen use <problem, explanation> pairs to fine-tune a smaller language model,\nwhich we refer to as the Reasoner, to learn algorithmic reasoning that can\ngenerate \"how-to-solve\" hints for unseen problems. Our experiments demonstrate\nthat learning from explanations enables the Reasoner to more effectively guide\nprogram implementation by a Coder, resulting in higher solve rates than strong\nchain-of-thought baselines on competitive-level programming problems. It also\noutperforms models that learn directly from <problem, solution-program> pairs.\nWe curated an additional test set in the CodeContests format, which includes\n246 more recent problems posted after the models' knowledge cutoff.", "published": "2024-04-11 22:19:50", "link": "http://arxiv.org/abs/2404.08148v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph Integrated Language Transformers for Next Action Prediction in\n  Complex Phone Calls", "abstract": "Current Conversational AI systems employ different machine learning\npipelines, as well as external knowledge sources and business logic to predict\nthe next action. Maintaining various components in dialogue managers' pipeline\nadds complexity in expansion and updates, increases processing time, and causes\nadditive noise through the pipeline that can lead to incorrect next action\nprediction. This paper investigates graph integration into language\ntransformers to improve understanding the relationships between humans'\nutterances, previous, and next actions without the dependency on external\nsources or components. Experimental analyses on real calls indicate that the\nproposed Graph Integrated Language Transformer models can achieve higher\nperformance compared to other production level conversational AI systems in\ndriving interactive calls with human users in real-world settings.", "published": "2024-04-11 22:47:50", "link": "http://arxiv.org/abs/2404.08155v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Contextual Dialogue Breakdown Detection for Conversational AI\n  Models", "abstract": "Detecting dialogue breakdown in real time is critical for conversational AI\nsystems, because it enables taking corrective action to successfully complete a\ntask. In spoken dialog systems, this breakdown can be caused by a variety of\nunexpected situations including high levels of background noise, causing STT\nmistranscriptions, or unexpected user flows. In particular, industry settings\nlike healthcare, require high precision and high flexibility to navigate\ndifferently based on the conversation history and dialogue states. This makes\nit both more challenging and more critical to accurately detect dialog\nbreakdown. To accurately detect breakdown, we found it requires processing\naudio inputs along with downstream NLP model inferences on transcribed text in\nreal time. In this paper, we introduce a Multimodal Contextual Dialogue\nBreakdown (MultConDB) model. This model significantly outperforms other known\nbest models by achieving an F1 of 69.27.", "published": "2024-04-11 23:09:18", "link": "http://arxiv.org/abs/2404.08156v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars", "abstract": "Large Language Models (LLMs) have achieved remarkable results, but their\nincreasing resource demand has become a major obstacle to the development of\npowerful and accessible super-human intelligence. This report introduces\nJetMoE-8B, a new LLM trained with less than $0.1 million, using 1.25T tokens\nfrom carefully mixed open-source corpora and 30,000 H100 GPU hours. Despite its\nlow cost, the JetMoE-8B demonstrates impressive performance, with JetMoE-8B\noutperforming the Llama2-7B model and JetMoE-8B-Chat surpassing the\nLlama2-13B-Chat model. These results suggest that LLM training can be much more\ncost-effective than generally thought. JetMoE-8B is based on an efficient\nSparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention\nand feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B\nto have 8B parameters while only activating 2B for each input token, reducing\ninference computation by about 70% compared to Llama2-7B. Moreover, JetMoE-8B\nis highly open and academia-friendly, using only public datasets and training\ncode. All training parameters and data mixtures have been detailed in this\nreport to facilitate future efforts in the development of open foundation\nmodels. This transparency aims to encourage collaboration and further\nadvancements in the field of accessible and efficient LLMs. The model weights\nare publicly available at https://github.com/myshell-ai/JetMoE.", "published": "2024-04-11 00:52:39", "link": "http://arxiv.org/abs/2404.07413v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Behavior Trees Enable Structured Programming of Language Model Agents", "abstract": "Language models trained on internet-scale data sets have shown an impressive\nability to solve problems in Natural Language Processing and Computer Vision.\nHowever, experience is showing that these models are frequently brittle in\nunexpected ways, and require significant scaffolding to ensure that they\noperate correctly in the larger systems that comprise \"language-model agents.\"\nIn this paper, we argue that behavior trees provide a unifying framework for\ncombining language models with classical AI and traditional programming. We\nintroduce Dendron, a Python library for programming language model agents using\nbehavior trees. We demonstrate the approach embodied by Dendron in three case\nstudies: building a chat agent, a camera-based infrastructure inspection agent\nfor use on a mobile robot or vehicle, and an agent that has been built to\nsatisfy safety constraints that it did not receive through instruction tuning\nor RLHF.", "published": "2024-04-11 02:44:13", "link": "http://arxiv.org/abs/2404.07439v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "An Audit on the Perspectives and Challenges of Hallucinations in NLP", "abstract": "We audit how hallucination in large language models (LLMs) is characterized\nin peer-reviewed literature, using a critical examination of 103 publications\nacross NLP research. Through the examination of the literature, we identify a\nlack of agreement with the term `hallucination' in the field of NLP.\nAdditionally, to compliment our audit, we conduct a survey with 171\npractitioners from the field of NLP and AI to capture varying perspectives on\nhallucination. Our analysis calls for the necessity of explicit definitions and\nframeworks outlining hallucination within NLP, highlighting potential\nchallenges, and our survey inputs provide a thematic understanding of the\ninfluence and ramifications of hallucination in society.", "published": "2024-04-11 03:51:29", "link": "http://arxiv.org/abs/2404.07461v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PromptSync: Bridging Domain Gaps in Vision-Language Models through\n  Class-Aware Prototype Alignment and Discrimination", "abstract": "The potential for zero-shot generalization in vision-language (V-L) models\nsuch as CLIP has spurred their widespread adoption in addressing numerous\ndownstream tasks. Previous methods have employed test-time prompt tuning to\nadapt the model to unseen domains, but they overlooked the issue of imbalanced\nclass distributions. In this study, we explicitly address this problem by\nemploying class-aware prototype alignment weighted by mean class probabilities\nobtained for the test sample and filtered augmented views. Additionally, we\nensure that the class probabilities are as accurate as possible by performing\nprototype discrimination using contrastive learning. The combination of\nalignment and discriminative loss serves as a geometric regularizer, preventing\nthe prompt representation from collapsing onto a single class and effectively\nbridging the distribution gap between the source and test domains. Our method,\nnamed PromptSync, synchronizes the prompts for each test sample on both the\ntext and vision branches of the V-L model. In empirical evaluations on the\ndomain generalization benchmark, our method outperforms previous best methods\nby 2.33% in overall performance, by 1% in base-to-novel generalization, and by\n2.84% in cross-dataset transfer tasks.", "published": "2024-04-11 07:26:00", "link": "http://arxiv.org/abs/2404.07520v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "From Words to Numbers: Your Large Language Model Is Secretly A Capable\n  Regressor When Given In-Context Examples", "abstract": "We analyze how well pre-trained large language models (e.g., Llama2, GPT-4,\nClaude 3, etc) can do linear and non-linear regression when given in-context\nexamples, without any additional training or gradient updates. Our findings\nreveal that several large language models (e.g., GPT-4, Claude 3) are able to\nperform regression tasks with a performance rivaling (or even outperforming)\nthat of traditional supervised methods such as Random Forest, Bagging, or\nGradient Boosting. For example, on the challenging Friedman #2 regression\ndataset, Claude 3 outperforms many supervised methods such as AdaBoost, SVM,\nRandom Forest, KNN, or Gradient Boosting. We then investigate how well the\nperformance of large language models scales with the number of in-context\nexemplars. We borrow from the notion of regret from online learning and\nempirically show that LLMs are capable of obtaining a sub-linear regret.", "published": "2024-04-11 08:12:43", "link": "http://arxiv.org/abs/2404.07544v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NoticIA: A Clickbait Article Summarization Dataset in Spanish", "abstract": "We present NoticIA, a dataset consisting of 850 Spanish news articles\nfeaturing prominent clickbait headlines, each paired with high-quality,\nsingle-sentence generative summarizations written by humans. This task demands\nadvanced text understanding and summarization abilities, challenging the\nmodels' capacity to infer and connect diverse pieces of information to meet the\nuser's informational needs generated by the clickbait headline. We evaluate the\nSpanish text comprehension capabilities of a wide range of state-of-the-art\nlarge language models. Additionally, we use the dataset to train\nClickbaitFighter, a task-specific model that achieves near-human performance in\nthis task.", "published": "2024-04-11 09:59:01", "link": "http://arxiv.org/abs/2404.07611v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Models Meet Anomaly Detection for Better Interpretability and\n  Generalizability", "abstract": "This research explores the integration of language models and unsupervised\nanomaly detection in medical imaging, addressing two key questions: (1) Can\nlanguage models enhance the interpretability of anomaly detection maps? and (2)\nCan anomaly maps improve the generalizability of language models in open-set\nanomaly detection tasks? To investigate these questions, we introduce a new\ndataset for multi-image visual question-answering on brain magnetic resonance\nimages encompassing multiple conditions. We propose KQ-Former (Knowledge\nQuerying Transformer), which is designed to optimally align visual and textual\ninformation in limited-sample contexts. Our model achieves a 60.81% accuracy on\nclosed questions, covering disease classification and severity across 15\ndifferent classes. For open questions, KQ-Former demonstrates a 70% improvement\nover the baseline with a BLEU-4 score of 0.41, and achieves the highest\nentailment ratios (up to 71.9%) and lowest contradiction ratios (down to 10.0%)\namong various natural language inference models. Furthermore, integrating\nanomaly maps results in an 18% accuracy increase in detecting open-set\nanomalies, thereby enhancing the language model's generalizability to\npreviously unseen medical conditions. The code and dataset are available at\nhttps://github.com/compai-lab/miccai-2024-junli?tab=readme-ov-file", "published": "2024-04-11 10:16:44", "link": "http://arxiv.org/abs/2404.07622v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Curated Datasets and Neural Models for Machine Translation of Informal\n  Registers between Mayan and Spanish Vernaculars", "abstract": "The Mayan languages comprise a language family with an ancient history,\nmillions of speakers, and immense cultural value, that, nevertheless, remains\nseverely underrepresented in terms of resources and global exposure. In this\npaper we develop, curate, and publicly release a set of corpora in several\nMayan languages spoken in Guatemala and Southern Mexico, which we call MayanV.\nThe datasets are parallel with Spanish, the dominant language of the region,\nand are taken from official native sources focused on representing informal,\nday-to-day, and non-domain-specific language. As such, and according to our\ndialectometric analysis, they differ in register from most other available\nresources. Additionally, we present neural machine translation models, trained\non as many resources and Mayan languages as possible, and evaluated exclusively\non our datasets. We observe lexical divergences between the dialects of Spanish\nin our resources and the more widespread written standard of Spanish, and that\nresources other than the ones we present do not seem to improve translation\nperformance, indicating that many such resources may not accurately capture\ncommon, real-life language usage. The MayanV dataset is available at\nhttps://github.com/transducens/mayanv.", "published": "2024-04-11 12:09:47", "link": "http://arxiv.org/abs/2404.07673v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs", "abstract": "The integration of Large Language Models (LLMs) and knowledge graphs (KGs)\nhas achieved remarkable success in various natural language processing tasks.\nHowever, existing methodologies that integrate LLMs and KGs often navigate the\ntask-solving process solely based on the LLM's analysis of the question,\noverlooking the rich cognitive potential inherent in the vast knowledge\nencapsulated in KGs. To address this, we introduce Observation-Driven Agent\n(ODA), a novel AI agent framework tailored for tasks involving KGs. ODA\nincorporates KG reasoning abilities via global observation, which enhances\nreasoning capabilities through a cyclical paradigm of observation, action, and\nreflection. Confronting the exponential explosion of knowledge during\nobservation, we innovatively design a recursive observation mechanism.\nSubsequently, we integrate the observed knowledge into the action and\nreflection modules. Through extensive experiments, ODA demonstrates\nstate-of-the-art performance on several datasets, notably achieving accuracy\nimprovements of 12.87% and 8.9%.", "published": "2024-04-11 12:16:16", "link": "http://arxiv.org/abs/2404.07677v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Nostra Domina at EvaLatin 2024: Improving Latin Polarity Detection\n  through Data Augmentation", "abstract": "This paper describes submissions from the team Nostra Domina to the EvaLatin\n2024 shared task of emotion polarity detection. Given the low-resource\nenvironment of Latin and the complexity of sentiment in rhetorical genres like\npoetry, we augmented the available data through automatic polarity annotation.\nWe present two methods for doing so on the basis of the $k$-means algorithm,\nand we employ a variety of Latin large language models (LLMs) in a neural\narchitecture to better capture the underlying contextual sentiment\nrepresentations. Our best approach achieved the second highest macro-averaged\nMacro-$F_1$ score on the shared task's test set.", "published": "2024-04-11 14:35:23", "link": "http://arxiv.org/abs/2404.07792v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Heron-Bench: A Benchmark for Evaluating Vision Language Models in\n  Japanese", "abstract": "Vision Language Models (VLMs) have undergone a rapid evolution, giving rise\nto significant advancements in the realm of multimodal understanding tasks.\nHowever, the majority of these models are trained and evaluated on\nEnglish-centric datasets, leaving a gap in the development and evaluation of\nVLMs for other languages, such as Japanese. This gap can be attributed to the\nlack of methodologies for constructing VLMs and the absence of benchmarks to\naccurately measure their performance. To address this issue, we introduce a\nnovel benchmark, Japanese Heron-Bench, for evaluating Japanese capabilities of\nVLMs. The Japanese Heron-Bench consists of a variety of imagequestion answer\npairs tailored to the Japanese context. Additionally, we present a baseline\nJapanese VLM that has been trained with Japanese visual instruction tuning\ndatasets. Our Heron-Bench reveals the strengths and limitations of the proposed\nVLM across various ability dimensions. Furthermore, we clarify the capability\ngap between strong closed models like GPT-4V and the baseline model, providing\nvaluable insights for future research in this domain. We release the benchmark\ndataset and training code to facilitate further developments in Japanese VLM\nresearch.", "published": "2024-04-11 15:09:22", "link": "http://arxiv.org/abs/2404.07824v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "On Training Data Influence of GPT Models", "abstract": "Amidst the rapid advancements in generative language models, the\ninvestigation of how training data shapes the performance of GPT models is\nstill emerging. This paper presents GPTfluence, a novel approach that leverages\na featurized simulation to assess the impact of training examples on the\ntraining dynamics of GPT models. Our approach not only traces the influence of\nindividual training instances on performance trajectories, such as loss and\nother key metrics, on targeted test points but also enables a comprehensive\ncomparison with existing methods across various training scenarios in GPT\nmodels, ranging from 14 million to 2.8 billion parameters, across a range of\ndownstream tasks. Contrary to earlier methods that struggle with generalization\nto new data, GPTfluence introduces a parameterized simulation of training\ndynamics, demonstrating robust generalization capabilities to unseen training\ndata. This adaptability is evident across both fine-tuning and\ninstruction-tuning scenarios, spanning tasks in natural language understanding\nand generation. We make our code and data publicly available at\nhttps://github.com/ernie-research/gptfluence.", "published": "2024-04-11 15:27:56", "link": "http://arxiv.org/abs/2404.07840v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Guiding Large Language Models to Post-Edit Machine Translation with\n  Error Annotations", "abstract": "Machine Translation (MT) remains one of the last NLP tasks where large\nlanguage models (LLMs) have not yet replaced dedicated supervised systems. This\nwork exploits the complementary strengths of LLMs and supervised MT by guiding\nLLMs to automatically post-edit MT with external feedback on its quality,\nderived from Multidimensional Quality Metric (MQM) annotations. Working with\nLLaMA-2 models, we consider prompting strategies varying the nature of feedback\nprovided and then fine-tune the LLM to improve its ability to exploit the\nprovided guidance. Through experiments on Chinese-English, English-German, and\nEnglish-Russian MQM data, we demonstrate that prompting LLMs to post-edit MT\nimproves TER, BLEU and COMET scores, although the benefits of fine-grained\nfeedback are not clear. Fine-tuning helps integrate fine-grained feedback more\neffectively and further improves translation quality based on both automatic\nand human evaluation.", "published": "2024-04-11 15:47:10", "link": "http://arxiv.org/abs/2404.07851v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "High-Dimension Human Value Representation in Large Language Models", "abstract": "The widespread application of LLMs across various tasks and fields has\nnecessitated the alignment of these models with human values and preferences.\nGiven various approaches of human value alignment, there is an urgent need to\nunderstand the scope and nature of human values injected into these LLMs before\ntheir deployment and adoption. We propose UniVaR, a high-dimensional neural\nrepresentation of symbolic human value distributions in LLMs, orthogonal to\nmodel architecture and training data. This is a continuous and scalable\nrepresentation, self-supervised from the value-relevant output of 8 LLMs and\nevaluated on 15 open-source and commercial LLMs. Through UniVaR, we visualize\nand explore how LLMs prioritize different values in 25 languages and cultures,\nshedding light on complex interplay between human values and language modeling.", "published": "2024-04-11 16:39:00", "link": "http://arxiv.org/abs/2404.07900v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DesignQA: A Multimodal Benchmark for Evaluating Large Language Models'\n  Understanding of Engineering Documentation", "abstract": "This research introduces DesignQA, a novel benchmark aimed at evaluating the\nproficiency of multimodal large language models (MLLMs) in comprehending and\napplying engineering requirements in technical documentation. Developed with a\nfocus on real-world engineering challenges, DesignQA uniquely combines\nmultimodal data-including textual design requirements, CAD images, and\nengineering drawings-derived from the Formula SAE student competition.\nDifferent from many existing MLLM benchmarks, DesignQA contains\ndocument-grounded visual questions where the input image and input document\ncome from different sources. The benchmark features automatic evaluation\nmetrics and is divided into segments-Rule Comprehension, Rule Compliance, and\nRule Extraction-based on tasks that engineers perform when designing according\nto requirements. We evaluate state-of-the-art models (at the time of writing)\nlike GPT-4o, GPT-4, Claude-Opus, Gemini-1.0, and LLaVA-1.5 against the\nbenchmark, and our study uncovers the existing gaps in MLLMs' abilities to\ninterpret complex engineering documentation. The MLLMs tested, while promising,\nstruggle to reliably retrieve relevant rules from the Formula SAE\ndocumentation, face challenges in recognizing technical components in CAD\nimages, and encounter difficulty in analyzing engineering drawings. These\nfindings underscore the need for multimodal models that can better handle the\nmultifaceted questions characteristic of design according to technical\ndocumentation. This benchmark sets a foundation for future advancements in\nAI-supported engineering design processes. DesignQA is publicly available at:\nhttps://github.com/anniedoris/design_qa/.", "published": "2024-04-11 16:59:54", "link": "http://arxiv.org/abs/2404.07917v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Rho-1: Not All Tokens Are What You Need", "abstract": "Previous language model pre-training methods have uniformly applied a\nnext-token prediction loss to all training tokens. Challenging this norm, we\nposit that \"9l training\". Our initial analysis examines token-level training\ndynamics of language model, revealing distinct loss patterns for different\ntokens. Leveraging these insights, we introduce a new language model called\nRho-1. Unlike traditional LMs that learn to predict every next token in a\ncorpus, Rho-1 employs Selective Language Modeling (SLM), which selectively\ntrains on useful tokens that aligned with the desired distribution. This\napproach involves scoring pretraining tokens using a reference model, and then\ntraining the language model with a focused loss on tokens with higher scores.\nWhen continual pretraining on 15B OpenWebMath corpus, Rho-1 yields an absolute\nimprovement in few-shot accuracy of up to 30% in 9 math tasks. After\nfine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40.6% and\n51.8% on MATH dataset, respectively - matching DeepSeekMath with only 3% of the\npretraining tokens. Furthermore, when continual pretraining on 80B general\ntokens, Rho-1 achieves 6.8% average enhancement across 15 diverse tasks,\nincreasing both efficiency and performance of the language model pre-training.", "published": "2024-04-11 17:52:01", "link": "http://arxiv.org/abs/2404.07965v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real\n  Computer Environments", "abstract": "Autonomous agents that accomplish complex computer tasks with minimal human\ninterventions have the potential to transform human-computer interaction,\nsignificantly enhancing accessibility and productivity. However, existing\nbenchmarks either lack an interactive environment or are limited to\nenvironments specific to certain applications or domains, failing to reflect\nthe diverse and complex nature of real-world computer use, thereby limiting the\nscope of tasks and agent scalability. To address this issue, we introduce\nOSWorld, the first-of-its-kind scalable, real computer environment for\nmultimodal agents, supporting task setup, execution-based evaluation, and\ninteractive learning across various operating systems such as Ubuntu, Windows,\nand macOS. OSWorld can serve as a unified, integrated computer environment for\nassessing open-ended computer tasks that involve arbitrary applications.\nBuilding upon OSWorld, we create a benchmark of 369 computer tasks involving\nreal web and desktop apps in open domains, OS file I/O, and workflows spanning\nmultiple applications. Each task example is derived from real-world computer\nuse cases and includes a detailed initial state setup configuration and a\ncustom execution-based evaluation script for reliable, reproducible evaluation.\nExtensive evaluation of state-of-the-art LLM/VLM-based agents on OSWorld\nreveals significant deficiencies in their ability to serve as computer\nassistants. While humans can accomplish over 72.36% of the tasks, the best\nmodel achieves only 12.24% success, primarily struggling with GUI grounding and\noperational knowledge. Comprehensive analysis using OSWorld provides valuable\ninsights for developing multimodal generalist agents that were not possible\nwith previous benchmarks. Our code, environment, baseline models, and data are\npublicly available at https://os-world.github.io.", "published": "2024-04-11 17:56:05", "link": "http://arxiv.org/abs/2404.07972v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "The Role of Language Imbalance in Cross-lingual Generalisation: Insights\n  from Cloned Language Experiments", "abstract": "Multilinguality is crucial for extending recent advancements in language\nmodelling to diverse linguistic communities. To maintain high performance while\nrepresenting multiple languages, multilingual models ideally align\nrepresentations, allowing what is learned in one language to generalise to\nothers. Prior research has emphasised the importance of parallel data and\nshared vocabulary elements as key factors for such alignment. In this study, we\ninvestigate an unintuitive novel driver of cross-lingual generalisation:\nlanguage imbalance. In controlled experiments on perfectly equivalent cloned\nlanguages, we observe that the existence of a predominant language during\ntraining boosts the performance of less frequent languages and leads to\nstronger alignment of model representations across languages. Furthermore, we\nfind that this trend is amplified with scale: with large enough models or long\nenough training, we observe that bilingual training data with a 90/10 language\nsplit yields better performance on both languages than a balanced 50/50 split.\nBuilding on these insights, we design training schemes that can improve\nperformance in all cloned languages, even without altering the training data.\nAs we extend our analysis to real languages, we find that infrequent languages\nstill benefit from frequent ones, yet whether language imbalance causes\ncross-lingual generalisation there is not conclusive.", "published": "2024-04-11 17:58:05", "link": "http://arxiv.org/abs/2404.07982v4", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Data-Augmentation-Based Dialectal Adaptation for LLMs", "abstract": "This report presents GMUNLP's participation to the Dialect-Copa shared task\nat VarDial 2024, which focuses on evaluating the commonsense reasoning\ncapabilities of large language models (LLMs) on South Slavic micro-dialects.\nThe task aims to assess how well LLMs can handle non-standard dialectal\nvarieties, as their performance on standard languages is already\nwell-established. We propose an approach that combines the strengths of\ndifferent types of language models and leverages data augmentation techniques\nto improve task performance on three South Slavic dialects: Chakavian,\nCherkano, and Torlak. We conduct experiments using a language-family-focused\nencoder-based model (BERTi\\'c) and a domain-agnostic multilingual model\n(AYA-101). Our results demonstrate that the proposed data augmentation\ntechniques lead to substantial performance gains across all three test datasets\nin the open-source model category. This work highlights the practical utility\nof data augmentation and the potential of LLMs in handling non-standard\ndialectal varieties, contributing to the broader goal of advancing natural\nlanguage understanding in low-resource and dialectal settings.\nCode:https://github.com/ffaisal93/dialect_copa", "published": "2024-04-11 19:15:32", "link": "http://arxiv.org/abs/2404.08092v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HLTCOE at TREC 2023 NeuCLIR Track", "abstract": "The HLTCOE team applied PLAID, an mT5 reranker, and document translation to\nthe TREC 2023 NeuCLIR track. For PLAID we included a variety of models and\ntraining techniques -- the English model released with ColBERT v2,\ntranslate-train~(TT), Translate Distill~(TD) and multilingual\ntranslate-train~(MTT). TT trains a ColBERT model with English queries and\npassages automatically translated into the document language from the MS-MARCO\nv1 collection. This results in three cross-language models for the track, one\nper language. MTT creates a single model for all three document languages by\ncombining the translations of MS-MARCO passages in all three languages into\nmixed-language batches. Thus the model learns about matching queries to\npassages simultaneously in all languages. Distillation uses scores from the mT5\nmodel over non-English translated document pairs to learn how to score\nquery-document pairs. The team submitted runs to all NeuCLIR tasks: the CLIR\nand MLIR news task as well as the technical documents task.", "published": "2024-04-11 20:46:18", "link": "http://arxiv.org/abs/2404.08118v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Extending Translate-Train for ColBERT-X to African Language CLIR", "abstract": "This paper describes the submission runs from the HLTCOE team at the CIRAL\nCLIR tasks for African languages at FIRE 2023. Our submissions use machine\ntranslation models to translate the documents and the training passages, and\nColBERT-X as the retrieval model. Additionally, we present a set of unofficial\nruns that use an alternative training procedure with a similar training\nsetting.", "published": "2024-04-11 21:31:02", "link": "http://arxiv.org/abs/2404.08134v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "MM-PhyQA: Multimodal Physics Question-Answering With Multi-Image CoT\n  Prompting", "abstract": "While Large Language Models (LLMs) can achieve human-level performance in\nvarious tasks, they continue to face challenges when it comes to effectively\ntackling multi-step physics reasoning tasks. To identify the shortcomings of\nexisting models and facilitate further research in this area, we curated a\nnovel dataset, MM-PhyQA, which comprises well-constructed, high schoollevel\nmultimodal physics problems. By evaluating the performance of contemporary LLMs\nthat are publicly available, both with and without the incorporation of\nmultimodal elements in these problems, we aim to shed light on their\ncapabilities. For generating answers for questions consisting of multimodal\ninput (in this case, images and text) we employed Zero-shot prediction using\nGPT-4 and utilized LLaVA (LLaVA and LLaVA-1.5), the latter of which were\nfine-tuned on our dataset. For evaluating the performance of LLMs consisting\nsolely of textual input, we tested the performance of the base and fine-tuned\nversions of the Mistral-7B and LLaMA2-7b models. We also showcased the\nperformance of the novel Multi-Image Chain-of-Thought (MI-CoT) Prompting\ntechnique, which when used to train LLaVA-1.5 13b yielded the best results when\ntested on our dataset, with superior scores in most metrics and the highest\naccuracy of 71.65% on the test set.", "published": "2024-04-11 07:11:47", "link": "http://arxiv.org/abs/2404.08704v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rumour Evaluation with Very Large Language Models", "abstract": "Conversational prompt-engineering-based large language models (LLMs) have\nenabled targeted control over the output creation, enhancing versatility,\nadaptability and adhoc retrieval. From another perspective, digital\nmisinformation has reached alarming levels. The anonymity, availability and\nreach of social media offer fertile ground for rumours to propagate. This work\nproposes to leverage the advancement of prompting-dependent LLMs to combat\nmisinformation by extending the research efforts of the RumourEval task on its\nTwitter dataset. To the end, we employ two prompting-based LLM variants\n(GPT-3.5-turbo and GPT-4) to extend the two RumourEval subtasks: (1) veracity\nprediction, and (2) stance classification. For veracity prediction, three\nclassifications schemes are experimented per GPT variant. Each scheme is tested\nin zero-, one- and few-shot settings. Our best results outperform the precedent\nones by a substantial margin. For stance classification,\nprompting-based-approaches show comparable performance to prior results, with\nno improvement over finetuning methods. Rumour stance subtask is also extended\nbeyond the original setting to allow multiclass classification. All of the\ngenerated predictions for both subtasks are equipped with confidence scores\ndetermining their trustworthiness degree according to the LLM, and post-hoc\njustifications for explainability and interpretability purposes. Our primary\naim is AI for social good.", "published": "2024-04-11 19:38:22", "link": "http://arxiv.org/abs/2404.16859v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Transferable and Principled Efficiency for Open-Vocabulary Segmentation", "abstract": "Recent success of pre-trained foundation vision-language models makes\nOpen-Vocabulary Segmentation (OVS) possible. Despite the promising performance,\nthis approach introduces heavy computational overheads for two challenges: 1)\nlarge model sizes of the backbone; 2) expensive costs during the fine-tuning.\nThese challenges hinder this OVS strategy from being widely applicable and\naffordable in real-world scenarios. Although traditional methods such as model\ncompression and efficient fine-tuning can address these challenges, they often\nrely on heuristics. This means that their solutions cannot be easily\ntransferred and necessitate re-training on different models, which comes at a\ncost. In the context of efficient OVS, we target achieving performance that is\ncomparable to or even better than prior OVS works based on large\nvision-language foundation models, by utilizing smaller models that incur lower\ntraining costs. The core strategy is to make our efficiency principled and thus\nseamlessly transferable from one OVS framework to others without further\ncustomization. Comprehensive experiments on diverse OVS benchmarks demonstrate\nour superior trade-off between segmentation accuracy and computation costs over\nprevious works. Our code is available on https://github.com/Xujxyang/OpenTrans", "published": "2024-04-11 03:08:53", "link": "http://arxiv.org/abs/2404.07448v3", "categories": ["cs.CV", "cs.CL", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Structure-aware Fine-tuning for Code Pre-trained Models", "abstract": "Over the past few years, we have witnessed remarkable advancements in Code\nPre-trained Models (CodePTMs). These models achieved excellent representation\ncapabilities by designing structure-based pre-training tasks for code. However,\nhow to enhance the absorption of structural knowledge when fine-tuning CodePTMs\nstill remains a significant challenge. To fill this gap, in this paper, we\npresent Structure-aware Fine-tuning (SAT), a novel structure-enhanced and\nplug-and-play fine-tuning method for CodePTMs. We first propose a structure\nloss to quantify the difference between the information learned by CodePTMs and\nthe knowledge extracted from code structure. Specifically, we use the attention\nscores extracted from Transformer layer as the learned structural information,\nand the shortest path length between leaves in abstract syntax trees as the\nstructural knowledge. Subsequently, multi-task learning is introduced to\nimprove the performance of fine-tuning. Experiments conducted on four\npre-trained models and two generation tasks demonstrate the effectiveness of\nour proposed method as a plug-and-play solution. Furthermore, we observed that\nSAT can benefit CodePTMs more with limited training data.", "published": "2024-04-11 04:24:48", "link": "http://arxiv.org/abs/2404.07471v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Laissez-Faire Harms: Algorithmic Biases in Generative Language Models", "abstract": "The rapid deployment of generative language models (LMs) has raised concerns\nabout social biases affecting the well-being of diverse consumers. The extant\nliterature on generative LMs has primarily examined bias via explicit identity\nprompting. However, prior research on bias in earlier language-based technology\nplatforms, including search engines, has shown that discrimination can occur\neven when identity terms are not specified explicitly. Studies of bias in LM\nresponses to open-ended prompts (where identity classifications are left\nunspecified) are lacking and have not yet been grounded in end-consumer harms.\nHere, we advance studies of generative LM bias by considering a broader set of\nnatural use cases via open-ended prompting. In this \"laissez-faire\" setting, we\nfind that synthetically generated texts from five of the most pervasive LMs\n(ChatGPT3.5, ChatGPT4, Claude2.0, Llama2, and PaLM2) perpetuate harms of\nomission, subordination, and stereotyping for minoritized individuals with\nintersectional race, gender, and/or sexual orientation identities (AI/AN,\nAsian, Black, Latine, MENA, NH/PI, Female, Non-binary, Queer). We find\nwidespread evidence of bias to an extent that such individuals are hundreds to\nthousands of times more likely to encounter LM-generated outputs that portray\ntheir identities in a subordinated manner compared to representative or\nempowering portrayals. We also document a prevalence of stereotypes (e.g.\nperpetual foreigner) in LM-generated outputs that are known to trigger\npsychological harms that disproportionately affect minoritized individuals.\nThese include stereotype threat, which leads to impaired cognitive performance\nand increased negative self-perception. Our findings highlight the urgent need\nto protect consumers from discriminatory harms caused by language models and\ninvest in critical AI education programs tailored towards empowering diverse\nconsumers.", "published": "2024-04-11 05:09:03", "link": "http://arxiv.org/abs/2404.07475v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interactive Prompt Debugging with Sequence Salience", "abstract": "We present Sequence Salience, a visual tool for interactive prompt debugging\nwith input salience methods. Sequence Salience builds on widely used salience\nmethods for text classification and single-token prediction, and extends this\nto a system tailored for debugging complex LLM prompts. Our system is\nwell-suited for long texts, and expands on previous work by 1) providing\ncontrollable aggregation of token-level salience to the word, sentence, or\nparagraph level, making salience over long inputs tractable; and 2) supporting\nrapid iteration where practitioners can act on salience results, refine\nprompts, and run salience on the new output. We include case studies showing\nhow Sequence Salience can help practitioners work with several complex\nprompting strategies, including few-shot, chain-of-thought, and constitutional\nprinciples. Sequence Salience is built on the Learning Interpretability Tool,\nan open-source platform for ML model visualizations, and code, notebooks, and\ntutorials are available at http://goo.gle/sequence-salience.", "published": "2024-04-11 06:22:56", "link": "http://arxiv.org/abs/2404.07498v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Medical mT5: An Open-Source Multilingual Text-to-Text LLM for The\n  Medical Domain", "abstract": "Research on language technology for the development of medical applications\nis currently a hot topic in Natural Language Understanding and Generation.\nThus, a number of large language models (LLMs) have recently been adapted to\nthe medical domain, so that they can be used as a tool for mediating in\nhuman-AI interaction. While these LLMs display competitive performance on\nautomated medical texts benchmarks, they have been pre-trained and evaluated\nwith a focus on a single language (English mostly). This is particularly true\nof text-to-text models, which typically require large amounts of\ndomain-specific pre-training data, often not easily accessible for many\nlanguages. In this paper, we address these shortcomings by compiling, to the\nbest of our knowledge, the largest multilingual corpus for the medical domain\nin four languages, namely English, French, Italian and Spanish. This new corpus\nhas been used to train Medical mT5, the first open-source text-to-text\nmultilingual model for the medical domain. Additionally, we present two new\nevaluation benchmarks for all four languages with the aim of facilitating\nmultilingual research in this domain. A comprehensive evaluation shows that\nMedical mT5 outperforms both encoders and similarly sized text-to-text models\nfor the Spanish, French, and Italian benchmarks, while being competitive with\ncurrent state-of-the-art LLMs in English.", "published": "2024-04-11 10:01:32", "link": "http://arxiv.org/abs/2404.07613v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Audio Dialogues: Dialogues dataset for audio and music understanding", "abstract": "Existing datasets for audio understanding primarily focus on single-turn\ninteractions (i.e. audio captioning, audio question answering) for describing\naudio in natural language, thus limiting understanding audio via interactive\ndialogue. To address this gap, we introduce Audio Dialogues: a multi-turn\ndialogue dataset containing 163.8k samples for general audio sounds and music.\nIn addition to dialogues, Audio Dialogues also has question-answer pairs to\nunderstand and compare multiple input audios together. Audio Dialogues\nleverages a prompting-based approach and caption annotations from existing\ndatasets to generate multi-turn dialogues using a Large Language Model (LLM).\nWe evaluate existing audio-augmented large language models on our proposed\ndataset to demonstrate the complexity and applicability of Audio Dialogues. Our\ncode for generating the dataset will be made publicly available. Detailed\nprompts and generated dialogues can be found on the demo website\nhttps://audiodialogues.github.io/.", "published": "2024-04-11 10:08:34", "link": "http://arxiv.org/abs/2404.07616v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ResearchAgent: Iterative Research Idea Generation over Scientific\n  Literature with Large Language Models", "abstract": "The pace of scientific research, vital for improving human life, is complex,\nslow, and needs specialized expertise. Meanwhile, novel, impactful research\noften stems from both a deep understanding of prior work, and a\ncross-pollination of ideas across domains and fields. To enhance the\nproductivity of researchers, we propose ResearchAgent, which leverages the\nencyclopedic knowledge and linguistic reasoning capabilities of Large Language\nModels (LLMs) to assist them in their work. This system automatically defines\nnovel problems, proposes methods and designs experiments, while iteratively\nrefining them based on the feedback from collaborative LLM-powered reviewing\nagents. Specifically, starting with a core scientific paper, ResearchAgent is\naugmented not only with relevant publications by connecting information over an\nacademic graph but also entities retrieved from a knowledge store derived from\nshared underlying concepts mined across numerous papers. Then, mimicking a\nscientific approach to improving ideas with peer discussions, we leverage\nmultiple LLM-based ReviewingAgents that provide reviews and feedback via\niterative revision processes. These reviewing agents are instantiated with\nhuman preference-aligned LLMs whose criteria for evaluation are elicited from\nactual human judgments via LLM prompting. We experimentally validate our\nResearchAgent on scientific publications across multiple disciplines, showing\nits effectiveness in generating novel, clear, and valid ideas based on both\nhuman and model-based evaluation results. Our initial foray into AI-mediated\nscientific research has important implications for the development of future\nsystems aimed at supporting researchers in their ideation and\noperationalization of novel work.", "published": "2024-04-11 13:36:29", "link": "http://arxiv.org/abs/2404.07738v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AnnoCTR: A Dataset for Detecting and Linking Entities, Tactics, and\n  Techniques in Cyber Threat Reports", "abstract": "Monitoring the threat landscape to be aware of actual or potential attacks is\nof utmost importance to cybersecurity professionals. Information about cyber\nthreats is typically distributed using natural language reports. Natural\nlanguage processing can help with managing this large amount of unstructured\ninformation, yet to date, the topic has received little attention. With this\npaper, we present AnnoCTR, a new CC-BY-SA-licensed dataset of cyber threat\nreports. The reports have been annotated by a domain expert with named\nentities, temporal expressions, and cybersecurity-specific concepts including\nimplicitly mentioned techniques and tactics. Entities and concepts are linked\nto Wikipedia and the MITRE ATT&CK knowledge base, the most widely-used taxonomy\nfor classifying types of attacks. Prior datasets linking to MITRE ATT&CK either\nprovide a single label per document or annotate sentences out-of-context; our\ndataset annotates entire documents in a much finer-grained way. In an\nexperimental study, we model the annotations of our dataset using\nstate-of-the-art neural models. In our few-shot scenario, we find that for\nidentifying the MITRE ATT&CK concepts that are mentioned explicitly or\nimplicitly in a text, concept descriptions from MITRE ATT&CK are an effective\nsource for training data augmentation.", "published": "2024-04-11 14:04:36", "link": "http://arxiv.org/abs/2404.07765v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Discourse-Aware In-Context Learning for Temporal Expression\n  Normalization", "abstract": "Temporal expression (TE) normalization is a well-studied problem. However,\nthe predominately used rule-based systems are highly restricted to specific\nsettings, and upcoming machine learning approaches suffer from a lack of\nlabeled data. In this work, we explore the feasibility of proprietary and\nopen-source large language models (LLMs) for TE normalization using in-context\nlearning to inject task, document, and example information into the model. We\nexplore various sample selection strategies to retrieve the most relevant set\nof examples. By using a window-based prompt design approach, we can perform TE\nnormalization across sentences, while leveraging the LLM knowledge without\ntraining the model. Our experiments show competitive results to models designed\nfor this task. In particular, our method achieves large performance\nimprovements for non-standard settings by dynamically including relevant\nexamples during inference.", "published": "2024-04-11 14:13:44", "link": "http://arxiv.org/abs/2404.07775v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RecurrentGemma: Moving Past Transformers for Efficient Open Language\n  Models", "abstract": "We introduce RecurrentGemma, a family of open language models which uses\nGoogle's novel Griffin architecture. Griffin combines linear recurrences with\nlocal attention to achieve excellent performance on language. It has a\nfixed-sized state, which reduces memory use and enables efficient inference on\nlong sequences. We provide two sizes of models, containing 2B and 9B\nparameters, and provide pre-trained and instruction tuned variants for both.\nOur models achieve comparable performance to similarly-sized Gemma baselines\ndespite being trained on fewer tokens.", "published": "2024-04-11 15:27:22", "link": "http://arxiv.org/abs/2404.07839v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Analyzing Toxicity in Deep Conversations: A Reddit Case Study", "abstract": "Online social media has become increasingly popular in recent years due to\nits ease of access and ability to connect with others. One of social media's\nmain draws is its anonymity, allowing users to share their thoughts and\nopinions without fear of judgment or retribution. This anonymity has also made\nsocial media prone to harmful content, which requires moderation to ensure\nresponsible and productive use. Several methods using artificial intelligence\nhave been employed to detect harmful content. However, conversation and\ncontextual analysis of hate speech are still understudied. Most promising works\nonly analyze a single text at a time rather than the conversation supporting\nit. In this work, we employ a tree-based approach to understand how users\nbehave concerning toxicity in public conversation settings. To this end, we\ncollect both the posts and the comment sections of the top 100 posts from 8\nReddit communities that allow profanity, totaling over 1 million responses. We\nfind that toxic comments increase the likelihood of subsequent toxic comments\nbeing produced in online conversations. Our analysis also shows that immediate\ncontext plays a vital role in shaping a response rather than the original post.\nWe also study the effect of consensual profanity and observe overlapping\nsimilarities with non-consensual profanity in terms of user behavior and\npatterns.", "published": "2024-04-11 16:10:44", "link": "http://arxiv.org/abs/2404.07879v1", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "LaVy: Vietnamese Multimodal Large Language Model", "abstract": "Large Language Models (LLMs) and Multimodal Large language models (MLLMs)\nhave taken the world by storm with impressive abilities in complex reasoning\nand linguistic comprehension. Meanwhile there are plethora of works related to\nVietnamese Large Language Models, the lack of high-quality resources in\nmultimodality limits the progress of Vietnamese MLLMs. In this paper, we\npioneer in address this by introducing LaVy, a state-of-the-art Vietnamese\nMLLM, and we also introduce LaVy-Bench benchmark designated for evaluating\nMLLMs's understanding on Vietnamese visual language tasks. Our project is\npublic at https://github.com/baochi0212/LaVy", "published": "2024-04-11 17:09:28", "link": "http://arxiv.org/abs/2404.07922v6", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLoCO: Learning Long Contexts Offline", "abstract": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco.", "published": "2024-04-11 17:57:22", "link": "http://arxiv.org/abs/2404.07979v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Manipulating Large Language Models to Increase Product Visibility", "abstract": "Large language models (LLMs) are increasingly being integrated into search\nengines to provide natural language responses tailored to user queries.\nCustomers and end-users are also becoming more dependent on these models for\nquick and easy purchase decisions. In this work, we investigate whether\nrecommendations from LLMs can be manipulated to enhance a product's visibility.\nWe demonstrate that adding a strategic text sequence (STS) -- a carefully\ncrafted message -- to a product's information page can significantly increase\nits likelihood of being listed as the LLM's top recommendation. To understand\nthe impact of STS, we use a catalog of fictitious coffee machines and analyze\nits effect on two target products: one that seldom appears in the LLM's\nrecommendations and another that usually ranks second. We observe that the\nstrategic text sequence significantly enhances the visibility of both products\nby increasing their chances of appearing as the top recommendation. This\nability to manipulate LLM-generated search responses provides vendors with a\nconsiderable competitive advantage and has the potential to disrupt fair market\ncompetition. Just as search engine optimization (SEO) revolutionized how\nwebpages are customized to rank higher in search engine results, influencing\nLLM recommendations could profoundly impact content optimization for AI-driven\nsearch services. Code for our experiments is available at\nhttps://github.com/aounon/llm-rank-optimizer.", "published": "2024-04-11 17:57:32", "link": "http://arxiv.org/abs/2404.07981v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "SQBC: Active Learning using LLM-Generated Synthetic Data for Stance\n  Detection in Online Political Discussions", "abstract": "Stance detection is an important task for many applications that analyse or\nsupport online political discussions. Common approaches include fine-tuning\ntransformer based models. However, these models require a large amount of\nlabelled data, which might not be available. In this work, we present two\ndifferent ways to leverage LLM-generated synthetic data to train and improve\nstance detection agents for online political discussions: first, we show that\naugmenting a small fine-tuning dataset with synthetic data can improve the\nperformance of the stance detection model. Second, we propose a new active\nlearning method called SQBC based on the \"Query-by-Comittee\" approach. The key\nidea is to use LLM-generated synthetic data as an oracle to identify the most\ninformative unlabelled samples, that are selected for manual labelling.\nComprehensive experiments show that both ideas can improve the stance detection\nperformance. Curiously, we observed that fine-tuning on actively selected\nsamples can exceed the performance of using the full dataset.", "published": "2024-04-11 18:34:11", "link": "http://arxiv.org/abs/2404.08078v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Variance-reduced Zeroth-Order Methods for Fine-Tuning Language Models", "abstract": "Fine-tuning language models (LMs) has demonstrated success in a wide array of\ndownstream tasks. However, as LMs are scaled up, the memory requirements for\nbackpropagation become prohibitively high. Zeroth-order (ZO) optimization\nmethods can leverage memory-efficient forward passes to estimate gradients.\nMore recently, MeZO, an adaptation of ZO-SGD, has been shown to consistently\noutperform zero-shot and in-context learning when combined with suitable task\nprompts. In this work, we couple ZO methods with variance reduction techniques\nto enhance stability and convergence for inference-based LM fine-tuning. We\nintroduce Memory-Efficient Zeroth-Order Stochastic Variance-Reduced Gradient\n(MeZO-SVRG) and demonstrate its efficacy across multiple LM fine-tuning tasks,\neliminating the reliance on task-specific prompts. Evaluated across a range of\nboth masked and autoregressive LMs on benchmark GLUE tasks, MeZO-SVRG\noutperforms MeZO with up to 20% increase in test accuracies in both full- and\npartial-parameter fine-tuning settings. MeZO-SVRG benefits from reduced\ncomputation time as it often surpasses MeZO's peak test accuracy with a\n$2\\times$ reduction in GPU-hours. MeZO-SVRG significantly reduces the required\nmemory footprint compared to first-order SGD, i.e. by $2\\times$ for\nautoregressive models. Our experiments highlight that MeZO-SVRG's memory\nsavings progressively improve compared to SGD with larger batch sizes.", "published": "2024-04-11 18:35:49", "link": "http://arxiv.org/abs/2404.08080v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.OC"], "primary_category": "cs.LG"}
{"title": "S3Editor: A Sparse Semantic-Disentangled Self-Training Framework for\n  Face Video Editing", "abstract": "Face attribute editing plays a pivotal role in various applications. However,\nexisting methods encounter challenges in achieving high-quality results while\npreserving identity, editing faithfulness, and temporal consistency. These\nchallenges are rooted in issues related to the training pipeline, including\nlimited supervision, architecture design, and optimization strategy. In this\nwork, we introduce S3Editor, a Sparse Semantic-disentangled Self-training\nframework for face video editing. S3Editor is a generic solution that\ncomprehensively addresses these challenges with three key contributions.\nFirstly, S3Editor adopts a self-training paradigm to enhance the training\nprocess through semi-supervision. Secondly, we propose a semantic disentangled\narchitecture with a dynamic routing mechanism that accommodates diverse editing\nrequirements. Thirdly, we present a structured sparse optimization schema that\nidentifies and deactivates malicious neurons to further disentangle impacts\nfrom untarget attributes. S3Editor is model-agnostic and compatible with\nvarious editing approaches. Our extensive qualitative and quantitative results\naffirm that our approach significantly enhances identity preservation, editing\nfidelity, as well as temporal consistency.", "published": "2024-04-11 20:25:26", "link": "http://arxiv.org/abs/2404.08111v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Introducing L2M3, A Multilingual Medical Large Language Model to Advance\n  Health Equity in Low-Resource Regions", "abstract": "Addressing the imminent shortfall of 10 million health workers by 2030,\npredominantly in Low- and Middle-Income Countries (LMICs), this paper\nintroduces an innovative approach that harnesses the power of Large Language\nModels (LLMs) integrated with machine translation models. This solution is\nengineered to meet the unique needs of Community Health Workers (CHWs),\novercoming language barriers, cultural sensitivities, and the limited\navailability of medical dialog datasets. I have crafted a model that not only\nboasts superior translation capabilities but also undergoes rigorous\nfine-tuning on open-source datasets to ensure medical accuracy and is equipped\nwith comprehensive safety features to counteract the risks of misinformation.\n  Featuring a modular design, this approach is specifically structured for\nswift adaptation across various linguistic and cultural contexts, utilizing\nopen-source components to significantly reduce healthcare operational costs.\nThis strategic innovation markedly improves the accessibility and quality of\nhealthcare services by providing CHWs with contextually appropriate medical\nknowledge and diagnostic tools. This paper highlights the transformative impact\nof this context-aware LLM, underscoring its crucial role in addressing the\nglobal healthcare workforce deficit and propelling forward healthcare outcomes\nin LMICs.", "published": "2024-04-11 07:39:22", "link": "http://arxiv.org/abs/2404.08705v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CEM: A Data-Efficient Method for Large Language Models to Continue\n  Evolving From Mistakes", "abstract": "As world knowledge advances and new task schemas emerge, Continual Learning\n(CL) becomes essential for keeping Large Language Models (LLMs) current and\naddressing their shortcomings. This process typically involves continual\ninstruction tuning (CIT) and continual pre-training (CPT) to enable these\nmodels to adapt to novel tasks and acquire critical knowledge. However,\ncollecting sufficient CPT data and efficiently bridging knowledge gaps remain\nsignificant challenges. Inspired by the 'summarizing mistakes' strategy, we\npropose the Continue Evolving from Mistakes (CEM) method, a data-efficient\napproach aiming to collect CPT data and continually improve LLMs' performance\nthrough iterative evaluation and supplementation with mistake-relevant\nknowledge. To further optimize data usage and mitigate forgetting, we introduce\na novel training paradigm that combines CIT and CPT. Experiments show that CEM\nsubstantially enhances multiple models' performance on both in-domain and\nout-of-domain QA tasks, achieving gains of up to 29.63%. Code and datasets are\navailable on https://anonymous.4open.science/r/cem-BB25.", "published": "2024-04-11 17:44:56", "link": "http://arxiv.org/abs/2404.08707v7", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Human Latency Conversational Turns for Spoken Avatar Systems", "abstract": "A problem with many current Large Language Model (LLM) driven spoken\ndialogues is the response time. Some efforts such as Groq address this issue by\nlightning fast processing of the LLM, but we know from the cognitive psychology\nliterature that in human-to-human dialogue often responses occur prior to the\nspeaker completing their utterance. No amount of delay for LLM processing is\nacceptable if we wish to maintain human dialogue latencies. In this paper, we\ndiscuss methods for understanding an utterance in close to real time and\ngenerating a response so that the system can comply with human-level\nconversational turn delays. This means that the information content of the\nfinal part of the speaker's utterance is lost to the LLM. Using the Google\nNaturalQuestions (NQ) database, our results show GPT-4 can effectively fill in\nmissing context from a dropped word at the end of a question over 60% of the\ntime. We also provide some examples of utterances and the impacts of this\ninformation loss on the quality of LLM response in the context of an avatar\nthat is currently under development. These results indicate that a simple\nclassifier could be used to determine whether a question is semantically\ncomplete, or requires a filler phrase to allow a response to be generated\nwithin human dialogue time constraints.", "published": "2024-04-11 20:20:48", "link": "http://arxiv.org/abs/2404.16053v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Augmenting Knowledge Graph Hierarchies Using Neural Transformers", "abstract": "Knowledge graphs are useful tools to organize, recommend and sort data.\nHierarchies in knowledge graphs provide significant benefit in improving\nunderstanding and compartmentalization of the data within a knowledge graph.\nThis work leverages large language models to generate and augment hierarchies\nin an existing knowledge graph. For small (<100,000 node) domain-specific KGs,\nwe find that a combination of few-shot prompting with one-shot generation works\nwell, while larger KG may require cyclical generation. We present techniques\nfor augmenting hierarchies, which led to coverage increase by 98% for intents\nand 99% for colors in our knowledge graph.", "published": "2024-04-11 05:53:38", "link": "http://arxiv.org/abs/2404.08020v1", "categories": ["cs.AI", "cs.CL", "cs.DL", "cs.IR", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Any2Point: Empowering Any-modality Large Models for Efficient 3D\n  Understanding", "abstract": "Large foundation models have recently emerged as a prominent focus of\ninterest, attaining superior performance in widespread scenarios. Due to the\nscarcity of 3D data, many efforts have been made to adapt pre-trained\ntransformers from vision to 3D domains. However, such 2D-to-3D approaches are\nstill limited, due to the potential loss of spatial geometries and high\ncomputation cost. More importantly, their frameworks are mainly designed for 2D\nmodels, lacking a general any-to-3D paradigm. In this paper, we introduce\nAny2Point, a parameter-efficient method to empower any-modality large models\n(vision, language, audio) for 3D understanding. Given a frozen transformer from\nany source modality, we propose a 3D-to-any (1D or 2D) virtual projection\nstrategy that correlates the input 3D points to the original 1D or 2D positions\nwithin the source modality. This mechanism enables us to assign each 3D token\nwith a positional encoding paired with the pre-trained model, which avoids 3D\ngeometry loss caused by the true projection and better motivates the\ntransformer for 3D learning with 1D/2D positional priors. Then, within each\ntransformer block, we insert an any-to-3D guided adapter module for\nparameter-efficient fine-tuning. The adapter incorporates prior spatial\nknowledge from the source modality to guide the local feature aggregation of 3D\ntokens, compelling the semantic adaption of any-modality transformers. We\nconduct extensive experiments to showcase the effectiveness and efficiency of\nour method. Code and models are released at\nhttps://github.com/Ivan-Tang-3D/Any2Point.", "published": "2024-04-11 17:59:45", "link": "http://arxiv.org/abs/2404.07989v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "A lightweight dual-stage framework for personalized speech enhancement\n  based on DeepFilterNet2", "abstract": "Isolating the desired speaker's voice amidst multiplespeakers in a noisy\nacoustic context is a challenging task. Per-sonalized speech enhancement (PSE)\nendeavours to achievethis by leveraging prior knowledge of the speaker's\nvoice.Recent research efforts have yielded promising PSE mod-els, albeit often\naccompanied by computationally intensivearchitectures, unsuitable for\nresource-constrained embeddeddevices. In this paper, we introduce a novel\nmethod to per-sonalize a lightweight dual-stage Speech Enhancement (SE)model\nand implement it within DeepFilterNet2, a SE modelrenowned for its\nstate-of-the-art performance. We seek anoptimal integration of speaker\ninformation within the model,exploring different positions for the integration\nof the speakerembeddings within the dual-stage enhancement architec-ture. We\nalso investigate a tailored training strategy whenadapting DeepFilterNet2 to a\nPSE task. We show that ourpersonalization method greatly improves the\nperformancesof DeepFilterNet2 while preserving minimal computationaloverhead.", "published": "2024-04-11 08:09:57", "link": "http://arxiv.org/abs/2404.08022v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Effective Automated Speaking Assessment Approach to Mitigating Data\n  Scarcity and Imbalanced Distribution", "abstract": "Automated speaking assessment (ASA) typically involves automatic speech\nrecognition (ASR) and hand-crafted feature extraction from the ASR transcript\nof a learner's speech. Recently, self-supervised learning (SSL) has shown\nstellar performance compared to traditional methods. However, SSL-based ASA\nsystems are faced with at least three data-related challenges: limited\nannotated data, uneven distribution of learner proficiency levels and\nnon-uniform score intervals between different CEFR proficiency levels. To\naddress these challenges, we explore the use of two novel modeling strategies:\nmetric-based classification and loss reweighting, leveraging distinct SSL-based\nembedding features. Extensive experimental results on the ICNALE benchmark\ndataset suggest that our approach can outperform existing strong baselines by a\nsizable margin, achieving a significant improvement of more than 10% in CEFR\nprediction accuracy.", "published": "2024-04-11 09:06:49", "link": "http://arxiv.org/abs/2404.07575v4", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Differentiable All-pole Filters for Time-varying Audio Systems", "abstract": "Infinite impulse response filters are an essential building block of many\ntime-varying audio systems, such as audio effects and synthesisers. However,\ntheir recursive structure impedes end-to-end training of these systems using\nautomatic differentiation. Although non-recursive filter approximations like\nfrequency sampling and frame-based processing have been proposed and widely\nused in previous works, they cannot accurately reflect the gradient of the\noriginal system. We alleviate this difficulty by re-expressing a time-varying\nall-pole filter to backpropagate the gradients through itself, so the filter\nimplementation is not bound to the technical limitations of automatic\ndifferentiation frameworks. This implementation can be employed within audio\nsystems containing filters with poles for efficient gradient evaluation. We\ndemonstrate its training efficiency and expressive capabilities for modelling\nreal-world dynamic audio systems on a phaser, time-varying subtractive\nsynthesiser, and compressor. We make our code and audio samples available and\nprovide the trained audio effect and synth models in a VST plugin at\nhttps://diffapf.github.io/web/.", "published": "2024-04-11 17:55:05", "link": "http://arxiv.org/abs/2404.07970v4", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Impact of Speech Anonymization on Pathology and Its Limits", "abstract": "Integration of speech into healthcare has intensified privacy concerns due to\nits potential as a non-invasive biomarker containing individual biometric\ninformation. In response, speaker anonymization aims to conceal personally\nidentifiable information while retaining crucial linguistic content. However,\nthe application of anonymization techniques to pathological speech, a critical\narea where privacy is especially vital, has not been extensively examined. This\nstudy investigates anonymization's impact on pathological speech across over\n2,700 speakers from multiple German institutions, focusing on privacy,\npathological utility, and demographic fairness. We explore both\ndeep-learning-based and signal processing-based anonymization methods. We\ndocument substantial privacy improvements across disorders-evidenced by equal\nerror rate increases up to 1933%, with minimal overall impact on utility.\nSpecific disorders such as Dysarthria, Dysphonia, and Cleft Lip and Palate\nexperience minimal utility changes, while Dysglossia shows slight improvements.\nOur findings underscore that the impact of anonymization varies substantially\nacross different disorders. This necessitates disorder-specific anonymization\nstrategies to optimally balance privacy with diagnostic utility. Additionally,\nour fairness analysis reveals consistent anonymization effects across most of\nthe demographics. This study demonstrates the effectiveness of anonymization in\npathological speech for enhancing privacy, while also highlighting the\nimportance of customized and disorder-specific approaches to account for\ninversion attacks.", "published": "2024-04-11 18:06:35", "link": "http://arxiv.org/abs/2404.08064v4", "categories": ["eess.AS", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "eess.AS"}
