{"title": "A Walk-based Model on Entity Graphs for Relation Extraction", "abstract": "We present a novel graph-based neural network model for relation extraction.\nOur model treats multiple pairs in a sentence simultaneously and considers\ninteractions among them. All the entities in a sentence are placed as nodes in\na fully-connected graph structure. The edges are represented with\nposition-aware contexts around the entity pairs. In order to consider different\nrelation paths between two entities, we construct up to l-length walks between\neach pair. The resulting walks are merged and iteratively used to update the\nedge representations into longer walks representations. We show that the model\nachieves performance comparable to the state-of-the-art systems on the ACE 2005\ndataset without using any external tools.", "published": "2019-02-19 12:34:40", "link": "http://arxiv.org/abs/1902.07023v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A novel repetition normalized adversarial reward for headline generation", "abstract": "While reinforcement learning can effectively improve language generation\nmodels, it often suffers from generating incoherent and repetitive phrases\n\\cite{paulus2017deep}. In this paper, we propose a novel repetition normalized\nadversarial reward to mitigate these problems. Our repetition penalized reward\ncan greatly reduce the repetition rate and adversarial training mitigates\ngenerating incoherent phrases. Our model significantly outperforms the baseline\nmodel on ROUGE-1\\,(+3.24), ROUGE-L\\,(+2.25), and a decreased repetition-rate\n(-4.98\\%).", "published": "2019-02-19 16:00:38", "link": "http://arxiv.org/abs/1902.07110v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Neural Machine Translation using AMR", "abstract": "It is intuitive that semantic representations can be useful for machine\ntranslation, mainly because they can help in enforcing meaning preservation and\nhandling data sparsity (many sentences correspond to one meaning) of machine\ntranslation models. On the other hand, little work has been done on leveraging\nsemantics for neural machine translation (NMT). In this work, we study the\nusefulness of AMR (short for abstract meaning representation) on NMT.\nExperiments on a standard English-to-German dataset show that incorporating AMR\nas additional knowledge can significantly improve a strong attention-based\nsequence-to-sequence neural translation model.", "published": "2019-02-19 21:03:35", "link": "http://arxiv.org/abs/1902.07282v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fusing Visual, Textual and Connectivity Clues for Studying Mental Health", "abstract": "With ubiquity of social media platforms, millions of people are sharing their\nonline persona by expressing their thoughts, moods, emotions, feelings, and\neven their daily struggles with mental health issues voluntarily and publicly\non social media. Unlike the most existing efforts which study depression by\nanalyzing textual content, we examine and exploit multimodal big data to\ndiscern depressive behavior using a wide variety of features including\nindividual-level demographics. By developing a multimodal framework and\nemploying statistical techniques for fusing heterogeneous sets of features\nobtained by processing visual, textual and user interaction data, we\nsignificantly enhance the current state-of-the-art approaches for identifying\ndepressed individuals on Twitter (improving the average F1-Score by 5 percent)\nas well as facilitate demographic inference from social media for broader\napplications. Besides providing insights into the relationship between\ndemographics and mental health, our research assists in the design of a new\nbreed of demographic-aware health interventions.", "published": "2019-02-19 00:10:08", "link": "http://arxiv.org/abs/1902.06843v1", "categories": ["cs.CY", "cs.CL", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Winning an Election: On Emergent Strategic Communication in Multi-Agent\n  Networks", "abstract": "Humans use language to collectively execute abstract strategies besides using\nit as a referential tool for identifying physical entities. Recently, multiple\nattempts at replicating the process of emergence of language in artificial\nagents have been made. While existing approaches study emergent languages as\nreferential tools, in this paper, we study their role in discovering and\nimplementing strategies. We formulate the problem using a voting game where two\ncandidate agents contest in an election with the goal of convincing population\nmembers (other agents), that are connected to each other via an underlying\nnetwork, to vote for them. To achieve this goal, agents are only allowed to\nexchange messages in the form of sequences of discrete symbols to spread their\npropaganda. We use neural networks with Gumbel-Softmax relaxation for sampling\ncategorical random variables to parameterize the policies followed by all\nagents. Using our proposed framework, we provide concrete answers to the\nfollowing questions: (i) Do the agents learn to communicate in a meaningful way\nand does the emergent communication play a role in deciding the winner? (ii)\nDoes the system evolve as expected under various reward structures? (iii) How\nis the emergent language affected by the community structure in the network? To\nthe best of our knowledge, we are the first to explore emergence of\ncommunication for discovering and implementing strategies in a setting where\nagents communicate over a network.", "published": "2019-02-19 05:14:14", "link": "http://arxiv.org/abs/1902.06897v2", "categories": ["cs.MA", "cs.AI", "cs.CL"], "primary_category": "cs.MA"}
{"title": "Measuring Compositionality in Representation Learning", "abstract": "Many machine learning algorithms represent input data with vector embeddings\nor discrete codes. When inputs exhibit compositional structure (e.g. objects\nbuilt from parts or procedures from subroutines), it is natural to ask whether\nthis compositional structure is reflected in the the inputs' learned\nrepresentations. While the assessment of compositionality in languages has\nreceived significant attention in linguistics and adjacent fields, the machine\nlearning literature lacks general-purpose tools for producing graded\nmeasurements of compositional structure in more general (e.g. vector-valued)\nrepresentation spaces. We describe a procedure for evaluating compositionality\nby measuring how well the true representation-producing model can be\napproximated by a model that explicitly composes a collection of inferred\nrepresentational primitives. We use the procedure to provide formal and\nempirical characterizations of compositional structure in a variety of\nsettings, exploring the relationship between compositionality and learning\ndynamics, human judgments, representational similarity, and generalization.", "published": "2019-02-19 18:27:12", "link": "http://arxiv.org/abs/1902.07181v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Learning to Generalize from Sparse and Underspecified Rewards", "abstract": "We consider the problem of learning from sparse and underspecified rewards,\nwhere an agent receives a complex input, such as a natural language\ninstruction, and needs to generate a complex response, such as an action\nsequence, while only receiving binary success-failure feedback. Such\nsuccess-failure rewards are often underspecified: they do not distinguish\nbetween purposeful and accidental success. Generalization from underspecified\nrewards hinges on discounting spurious trajectories that attain accidental\nsuccess, while learning from sparse feedback requires effective exploration. We\naddress exploration by using a mode covering direction of KL divergence to\ncollect a diverse set of successful trajectories, followed by a mode seeking KL\ndivergence to train a robust policy. We propose Meta Reward Learning (MeRL) to\nconstruct an auxiliary reward function that provides more refined feedback for\nlearning. The parameters of the auxiliary reward function are optimized with\nrespect to the validation performance of a trained policy. The MeRL approach\noutperforms our alternative reward learning technique based on Bayesian\nOptimization, and achieves the state-of-the-art on weakly-supervised semantic\nparsing. It improves previous work by 1.2% and 2.4% on WikiTableQuestions and\nWikiSQL datasets respectively.", "published": "2019-02-19 18:51:10", "link": "http://arxiv.org/abs/1902.07198v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A spelling correction model for end-to-end speech recognition", "abstract": "Attention-based sequence-to-sequence models for speech recognition jointly\ntrain an acoustic model, language model (LM), and alignment mechanism using a\nsingle neural network and require only parallel audio-text pairs. Thus, the\nlanguage model component of the end-to-end model is only trained on transcribed\naudio-text pairs, which leads to performance degradation especially on rare\nwords. While there have been a variety of work that look at incorporating an\nexternal LM trained on text-only data into the end-to-end framework, none of\nthem have taken into account the characteristic error distribution made by the\nmodel. In this paper, we propose a novel approach to utilizing text-only data,\nby training a spelling correction (SC) model to explicitly correct those\nerrors. On the LibriSpeech dataset, we demonstrate that the proposed model\nresults in an 18.6% relative improvement in WER over the baseline model when\ndirectly correcting top ASR hypothesis, and a 29.0% relative improvement when\nfurther rescoring an expanded n-best list using an external LM.", "published": "2019-02-19 18:18:59", "link": "http://arxiv.org/abs/1902.07178v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "P-Reverb: Perceptual Characterization of Early and Late Reflections for\n  Auditory Displays", "abstract": "We introduce a novel, perceptually derived metric (P-Reverb) that relates the\njust-noticeable difference (JND) of the early sound field(also called early\nreflections) to the late sound field (known as late reflections or\nreverberation). Early and late reflections are crucial components of the sound\nfield and provide multiple perceptual cues for auditory displays. We conduct\ntwo extensive user evaluations that relate the JNDs of early reflections and\nlate reverberation in terms of the mean-free path of the environment and\npresent a novel P-Reverb metric. Our metric is used to estimate dynamic\nreverberation characteristics efficiently in terms of important parameters like\nreverberation time (RT60). We show the numerical accuracy of our P-Reverb\nmetric in estimating RT60. Finally, we use our metric to design an interactive\nsound propagation algorithm and demonstrate its effectiveness on various\nbenchmarks.", "published": "2019-02-19 03:53:30", "link": "http://arxiv.org/abs/1902.06880v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Low-Latency Deep Clustering For Speech Separation", "abstract": "This paper proposes a low algorithmic latency adaptation of the deep\nclustering approach to speaker-independent speech separation. It consists of\nthree parts: a) the usage of long-short-term-memory (LSTM) networks instead of\ntheir bidirectional variant used in the original work, b) using a short\nsynthesis window (here 8 ms) required for low-latency operation, and, c) using\na buffer in the beginning of audio mixture to estimate cluster centres\ncorresponding to constituent speakers which are then utilized to separate\nspeakers within the rest of the signal. The buffer duration would serve as an\ninitialization phase after which the system is capable of operating with 8 ms\nalgorithmic latency. We evaluate our proposed approach on two-speaker mixtures\nfrom the Wall Street Journal (WSJ0) corpus. We observe that the use of LSTM\nyields around one dB lower SDR as compared to the baseline bidirectional LSTM\nin terms of source to distortion ratio (SDR). Moreover, using an 8 ms synthesis\nwindow instead of 32 ms degrades the separation performance by around 2.1 dB as\ncompared to the baseline. Finally, we also report separation performance with\ndifferent buffer durations noting that separation can be achieved even for\nbuffer duration as low as 300ms.", "published": "2019-02-19 13:00:18", "link": "http://arxiv.org/abs/1902.07033v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Data Efficient Voice Cloning for Neural Singing Synthesis", "abstract": "There are many use cases in singing synthesis where creating voices from\nsmall amounts of data is desirable. In text-to-speech there have been several\npromising results that apply voice cloning techniques to modern deep learning\nbased models. In this work, we adapt one such technique to the case of singing\nsynthesis. By leveraging data from many speakers to first create a multispeaker\nmodel, small amounts of target data can then efficiently adapt the model to new\nunseen voices. We evaluate the system using listening tests across a number of\ndifferent use cases, languages and kinds of data.", "published": "2019-02-19 21:31:50", "link": "http://arxiv.org/abs/1902.07292v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Shapes from Echoes: Uniqueness from Point-to-Plane Distance Matrices", "abstract": "We study the problem of localizing a configuration of points and planes from\nthe collection of point-to-plane distances. This problem models simultaneous\nlocalization and mapping from acoustic echoes as well as the notable \"structure\nfrom sound\" approach to microphone localization with unknown sources. In our\nearlier work we proposed computational methods for localization from\npoint-to-plane distances and noted that such localization suffers from various\nambiguities beyond the usual rigid body motions; in this paper we provide a\ncomplete characterization of uniqueness. We enumerate equivalence classes of\nconfigurations which lead to the same distance measurements as a function of\nthe number of planes and points, and algebraically characterize the related\ntransformations in both 2D and 3D. Here we only discuss uniqueness;\ncomputational tools and heuristics for practical localization from\npoint-to-plane distances using sound will be addressed in a companion paper.", "published": "2019-02-19 15:58:41", "link": "http://arxiv.org/abs/1902.09959v1", "categories": ["cs.CG", "cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.CG"}
