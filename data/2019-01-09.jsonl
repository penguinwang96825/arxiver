{"title": "Sequential Attention-based Network for Noetic End-to-End Response\n  Selection", "abstract": "The noetic end-to-end response selection challenge as one track in Dialog\nSystem Technology Challenges 7 (DSTC7) aims to push the state of the art of\nutterance classification for real world goal-oriented dialog systems, for which\nparticipants need to select the correct next utterances from a set of\ncandidates for the multi-turn context. This paper describes our systems that\nare ranked the top on both datasets under this challenge, one focused and small\n(Advising) and the other more diverse and large (Ubuntu). Previous\nstate-of-the-art models use hierarchy-based (utterance-level and token-level)\nneural networks to explicitly model the interactions among different turns'\nutterances for context modeling. In this paper, we investigate a sequential\nmatching model based only on chain sequence for multi-turn response selection.\nOur results demonstrate that the potentials of sequential matching approaches\nhave not yet been fully exploited in the past for multi-turn response\nselection. In addition to ranking the top in the challenge, the proposed model\noutperforms all previous models, including state-of-the-art hierarchy-based\nmodels, and achieves new state-of-the-art performances on two large-scale\npublic multi-turn response selection benchmark datasets.", "published": "2019-01-09 05:45:41", "link": "http://arxiv.org/abs/1901.02609v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What do Language Representations Really Represent?", "abstract": "A neural language model trained on a text corpus can be used to induce\ndistributed representations of words, such that similar words end up with\nsimilar representations. If the corpus is multilingual, the same model can be\nused to learn distributed representations of languages, such that similar\nlanguages end up with similar representations. We show that this holds even\nwhen the multilingual corpus has been translated into English, by picking up\nthe faint signal left by the source languages. However, just like it is a\nthorny problem to separate semantic from syntactic similarity in word\nrepresentations, it is not obvious what type of similarity is captured by\nlanguage representations. We investigate correlations and causal relationships\nbetween language representations learned from translations on one hand, and\ngenetic, geographical, and several levels of structural similarity between\nlanguages on the other. Of these, structural similarity is found to correlate\nmost strongly with language representation similarity, while genetic\nrelationships---a convenient benchmark used for evaluation in previous\nwork---appears to be a confounding factor. Apart from implications about\ntranslation effects, we see this more generally as a case where NLP and\nlinguistic typology can interact and benefit one another.", "published": "2019-01-09 09:19:28", "link": "http://arxiv.org/abs/1901.02646v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is it Time to Swish? Comparing Deep Learning Activation Functions Across\n  NLP tasks", "abstract": "Activation functions play a crucial role in neural networks because they are\nthe nonlinearities which have been attributed to the success story of deep\nlearning. One of the currently most popular activation functions is ReLU, but\nseveral competitors have recently been proposed or 'discovered', including\nLReLU functions and swish. While most works compare newly proposed activation\nfunctions on few tasks (usually from image classification) and against few\ncompetitors (usually ReLU), we perform the first large-scale comparison of 21\nactivation functions across eight different NLP tasks. We find that a largely\nunknown activation function performs most stably across all tasks, the\nso-called penalized tanh function. We also show that it can successfully\nreplace the sigmoid and tanh gates in LSTM cells, leading to a 2 percentage\npoint (pp) improvement over the standard choices on a challenging NLP task.", "published": "2019-01-09 10:45:20", "link": "http://arxiv.org/abs/1901.02671v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis of Czech Texts: An Algorithmic Survey", "abstract": "In the area of online communication, commerce and transactions, analyzing\nsentiment polarity of texts written in various natural languages has become\ncrucial. While there have been a lot of contributions in resources and studies\nfor the English language, \"smaller\" languages like Czech have not received much\nattention. In this survey, we explore the effectiveness of many existing\nmachine learning algorithms for sentiment analysis of Czech Facebook posts and\nproduct reviews. We report the sets of optimal parameter values for each\nalgorithm and the scores in both datasets. We finally observe that support\nvector machines are the best classifier and efforts to increase performance\neven more with bagging, boosting or voting ensemble schemes fail to do so.", "published": "2019-01-09 15:30:39", "link": "http://arxiv.org/abs/1901.02780v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "abstract": "Transformers have a potential of learning longer-term dependency, but are\nlimited by a fixed-length context in the setting of language modeling. We\npropose a novel neural architecture Transformer-XL that enables learning\ndependency beyond a fixed length without disrupting temporal coherence. It\nconsists of a segment-level recurrence mechanism and a novel positional\nencoding scheme. Our method not only enables capturing longer-term dependency,\nbut also resolves the context fragmentation problem. As a result,\nTransformer-XL learns dependency that is 80% longer than RNNs and 450% longer\nthan vanilla Transformers, achieves better performance on both short and long\nsequences, and is up to 1,800+ times faster than vanilla Transformers during\nevaluation. Notably, we improve the state-of-the-art results of bpc/perplexity\nto 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion\nWord, and 54.5 on Penn Treebank (without finetuning). When trained only on\nWikiText-103, Transformer-XL manages to generate reasonably coherent, novel\ntext articles with thousands of tokens. Our code, pretrained models, and\nhyperparameters are available in both Tensorflow and PyTorch.", "published": "2019-01-09 18:28:19", "link": "http://arxiv.org/abs/1901.02860v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
