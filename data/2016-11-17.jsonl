{"title": "What Do Recurrent Neural Network Grammars Learn About Syntax?", "abstract": "Recurrent neural network grammars (RNNG) are a recently proposed\nprobabilistic generative modeling family for natural language. They show\nstate-of-the-art language modeling and parsing performance. We investigate what\ninformation they learn, from a linguistic perspective, through various\nablations to the model and the data, and by augmenting the model with an\nattention mechanism (GA-RNNG) to enable closer inspection. We find that\nexplicit modeling of composition is crucial for achieving the best performance.\nThrough the attention mechanism, we find that headedness plays a central role\nin phrasal representation (with the model's latent attention largely agreeing\nwith predictions made by hand-crafted head rules, albeit with some important\ndifferences). By training grammars without nonterminal labels, we find that\nphrasal representations depend minimally on nonterminals, providing support for\nthe endocentricity hypothesis.", "published": "2016-11-17 16:41:41", "link": "http://arxiv.org/abs/1611.05774v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Node Selection for Deep Neural Networks using Group Lasso\n  Regularization", "abstract": "We examine the effect of the Group Lasso (gLasso) regularizer in selecting\nthe salient nodes of Deep Neural Network (DNN) hidden layers by applying a\nDNN-HMM hybrid speech recognizer to TED Talks speech data. We test two types of\ngLasso regularization, one for outgoing weight vectors and another for incoming\nweight vectors, as well as two sizes of DNNs: 2048 hidden layer nodes and 4096\nnodes. Furthermore, we compare gLasso and L2 regularizers. Our experiment\nresults demonstrate that our DNN training, in which the gLasso regularizer was\nembedded, successfully selected the hidden layer nodes that are necessary and\nsufficient for achieving high classification power.", "published": "2016-11-17 01:43:01", "link": "http://arxiv.org/abs/1611.05527v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Visual Question Answering", "abstract": "Part of the appeal of Visual Question Answering (VQA) is its promise to\nanswer new questions about previously unseen images. Most current methods\ndemand training questions that illustrate every possible concept, and will\ntherefore never achieve this capability, since the volume of required training\ndata would be prohibitive. Answering general questions about images requires\nmethods capable of Zero-Shot VQA, that is, methods able to answer questions\nbeyond the scope of the training questions. We propose a new evaluation\nprotocol for VQA methods which measures their ability to perform Zero-Shot VQA,\nand in doing so highlights significant practical deficiencies of current\napproaches, some of which are masked by the biases in current datasets. We\npropose and evaluate several strategies for achieving Zero-Shot VQA, including\nmethods based on pretrained word embeddings, object classifiers with semantic\nembeddings, and test-time retrieval of example images. Our extensive\nexperiments are intended to serve as baselines for Zero-Shot VQA, and they also\nachieve state-of-the-art performance in the standard VQA evaluation setting.", "published": "2016-11-17 03:21:00", "link": "http://arxiv.org/abs/1611.05546v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
