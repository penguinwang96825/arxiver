{"title": "Towards Effective Ancient Chinese Translation: Dataset, Model, and\n  Evaluation", "abstract": "Interpreting ancient Chinese has been the key to comprehending vast Chinese\nliterature, tradition, and civilization. In this paper, we propose Erya for\nancient Chinese translation. From a dataset perspective, we collect, clean, and\nclassify ancient Chinese materials from various sources, forming the most\nextensive ancient Chinese resource to date. From a model perspective, we devise\nErya training method oriented towards ancient Chinese. We design two\njointly-working tasks: disyllabic aligned substitution (DAS) and dual masked\nlanguage model (DMLM). From an evaluation perspective, we build a benchmark to\njudge ancient Chinese translation quality in different scenarios and evaluate\nthe ancient Chinese translation capacities of various existing models. Our\nmodel exhibits remarkable zero-shot performance across five domains, with over\n+12.0 BLEU against GPT-3.5 models and better human evaluation results than\nERNIE Bot. Subsequent fine-tuning further shows the superior transfer\ncapability of Erya model with +6.2 BLEU gain. We release all the\nabove-mentioned resources at https://github.com/RUCAIBox/Erya.", "published": "2023-08-01 02:43:27", "link": "http://arxiv.org/abs/2308.00240v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Skills-in-Context Prompting: Unlocking Compositionality in Large\n  Language Models", "abstract": "We investigate how to elicit compositional generalization capabilities in\nlarge language models (LLMs). Compositional generalization empowers LLMs to\nsolve complex problems by combining foundational skills, a critical reasoning\nability akin to human intelligence. However, even the most advanced LLMs\ncurrently struggle with this form of reasoning. We examine this problem within\nthe framework of in-context learning and find that demonstrating both\nfoundational skills and compositional examples grounded in these skills within\nthe same prompt context is crucial. We refer to this prompt structure as\nskills-in-context (SKiC). With as few as two exemplars, this in-context\nlearning structure enables LLMs to tackle more challenging problems requiring\ninnovative skill combinations, achieving near-perfect systematic generalization\nacross a broad range of tasks. Intriguingly, SKiC also unlocks the latent\npotential of LLMs, allowing them to more actively utilize pre-existing internal\nskills acquired during earlier pretraining stages to solve complex reasoning\nproblems. The SKiC structure is robust across different skill constructions and\nexemplar choices and demonstrates strong transferability to new tasks. Finally,\ninspired by our in-context learning study, we show that fine-tuning LLMs with\nSKiC-style data can elicit zero-shot weak-to-strong generalization, enabling\nthe models to solve much harder problems directly with standard prompting.", "published": "2023-08-01 05:54:12", "link": "http://arxiv.org/abs/2308.00304v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LimeAttack: Local Explainable Method for Textual Hard-Label Adversarial\n  Attack", "abstract": "Natural language processing models are vulnerable to adversarial examples.\nPrevious textual adversarial attacks adopt gradients or confidence scores to\ncalculate word importance ranking and generate adversarial examples. However,\nthis information is unavailable in the real world. Therefore, we focus on a\nmore realistic and challenging setting, named hard-label attack, in which the\nattacker can only query the model and obtain a discrete prediction label.\nExisting hard-label attack algorithms tend to initialize adversarial examples\nby random substitution and then utilize complex heuristic algorithms to\noptimize the adversarial perturbation. These methods require a lot of model\nqueries and the attack success rate is restricted by adversary initialization.\nIn this paper, we propose a novel hard-label attack algorithm named LimeAttack,\nwhich leverages a local explainable method to approximate word importance\nranking, and then adopts beam search to find the optimal solution. Extensive\nexperiments show that LimeAttack achieves the better attacking performance\ncompared with existing hard-label attack under the same query budget. In\naddition, we evaluate the effectiveness of LimeAttack on large language models,\nand results indicate that adversarial examples remain a significant threat to\nlarge language models. The adversarial examples crafted by LimeAttack are\nhighly transferable and effectively improve model robustness in adversarial\ntraining.", "published": "2023-08-01 06:30:37", "link": "http://arxiv.org/abs/2308.00319v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unimodal Intermediate Training for Multimodal Meme Sentiment\n  Classification", "abstract": "Internet Memes remain a challenging form of user-generated content for\nautomated sentiment classification. The availability of labelled memes is a\nbarrier to developing sentiment classifiers of multimodal memes. To address the\nshortage of labelled memes, we propose to supplement the training of a\nmultimodal meme classifier with unimodal (image-only and text-only) data. In\nthis work, we present a novel variant of supervised intermediate training that\nuses relatively abundant sentiment-labelled unimodal data. Our results show a\nstatistically significant performance improvement from the incorporation of\nunimodal text data. Furthermore, we show that the training set of labelled\nmemes can be reduced by 40% without reducing the performance of the downstream\nmodel.", "published": "2023-08-01 13:14:10", "link": "http://arxiv.org/abs/2308.00528v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GRDD: A Dataset for Greek Dialectal NLP", "abstract": "In this paper, we present a dataset for the computational study of a number\nof Modern Greek dialects. It consists of raw text data from four dialects of\nModern Greek, Cretan, Pontic, Northern Greek and Cypriot Greek. The dataset is\nof considerable size, albeit imbalanced, and presents the first attempt to\ncreate large scale dialectal resources of this type for Modern Greek dialects.\nWe then use the dataset to perform dialect idefntification. We experiment with\ntraditional ML algorithms, as well as simple DL architectures. The results show\nvery good performance on the task, potentially revealing that the dialects in\nquestion have distinct enough characteristics allowing even simple ML models to\nperform well on the task. Error analysis is performed for the top performing\nalgorithms showing that in a number of cases the errors are due to insufficient\ndataset cleaning.", "published": "2023-08-01 19:34:18", "link": "http://arxiv.org/abs/2308.00802v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DiactTOD: Learning Generalizable Latent Dialogue Acts for Controllable\n  Task-Oriented Dialogue Systems", "abstract": "Dialogue act annotations are important to improve response generation quality\nin task-oriented dialogue systems. However, it can be challenging to use\ndialogue acts to control response generation in a generalizable way because\ndifferent datasets and tasks may have incompatible annotations. While\nalternative methods that utilize latent action spaces or reinforcement learning\ndo not require explicit annotations, they may lack interpretability or face\ndifficulties defining task-specific rewards. In this work, we present a novel\nend-to-end latent dialogue act model (DiactTOD) that represents dialogue acts\nin a latent space. DiactTOD, when pre-trained on a large corpus, is able to\npredict and control dialogue acts to generate controllable responses using\nthese latent representations in a zero-shot fashion. Our approach demonstrates\nstate-of-the-art performance across a wide range of experimental settings on\nthe MultiWOZ dataset, including zero-shot, few-shot, and full data fine-tuning\nwith both end-to-end and policy optimization configurations.", "published": "2023-08-01 23:29:16", "link": "http://arxiv.org/abs/2308.00878v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoSMo: A constructor specification language for Abstract Wikipedia's\n  content selection process", "abstract": "Representing snippets of information abstractly is a task that needs to be\nperformed for various purposes, such as database view specification and the\nfirst stage in the natural language generation pipeline for generative AI from\nstructured input, i.e., the content selection stage to determine what needs to\nbe verbalised. For the Abstract Wikipedia project, requirements analysis\nrevealed that such an abstract representation requires multilingual modelling,\ncontent selection covering declarative content and functions, and both classes\nand instances. There is no modelling language that meets either of the three\nfeatures, let alone a combination. Following a rigorous language design process\ninclusive of broad stakeholder consultation, we created CoSMo, a novel {\\sc\nCo}ntent {\\sc S}election {\\sc Mo}deling language that meets these and other\nrequirements so that it may be useful both in Abstract Wikipedia as well as\nother contexts. We describe the design process, rationale and choices, the\nspecification, and preliminary evaluation of the language.", "published": "2023-08-01 13:57:23", "link": "http://arxiv.org/abs/2308.02539v1", "categories": ["cs.CL", "I.2.4; H.2.3"], "primary_category": "cs.CL"}
{"title": "Aspect based sentimental analysis for travellers' reviews", "abstract": "Airport service quality evaluation is commonly found on social media,\nincluding Google Maps. This valuable for airport management in order to enhance\nthe quality of services provided. However; prior studies either provide general\nreview for topics discussed by travellers or provide sentimental value to tag\nthe entire review without specifically mentioning the airport service that is\nbehind such value. Accordingly, this work proposes using aspect based\nsentimental analysis in order to provide more detailed analysis for travellers\nreviews. This works applied aspect based sentimental analysis on data collected\nfrom Google Map about Dubai and Doha airports. The results provide tangible\nreasons to use aspect based sentimental analysis in order to understand more\nthe travellers and spot airport services that are in need for improvement.", "published": "2023-08-01 21:23:02", "link": "http://arxiv.org/abs/2308.02548v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fountain -- an intelligent contextual assistant combining knowledge\n  representation and language models for manufacturing risk identification", "abstract": "Deviations from the approved design or processes during mass production can\nlead to unforeseen risks. However, these changes are sometimes necessary due to\nchanges in the product design characteristics or an adaptation in the\nmanufacturing process. A major challenge is to identify these risks early in\nthe workflow so that failures leading to warranty claims can be avoided. We\ndeveloped Fountain as a contextual assistant integrated in the deviation\nmanagement workflow that helps in identifying the risks based on the\ndescription of the existing design and process criteria and the proposed\ndeviation. In the manufacturing context, it is important that the assistant\nprovides recommendations that are explainable and consistent. We achieve this\nthrough a combination of the following two components 1) language models\nfinetuned for domain specific semantic similarity and, 2) knowledge\nrepresentation in the form of a property graph derived from the bill of\nmaterials, Failure Modes and Effect Analysis (FMEA) and prior failures reported\nby customers. Here, we present the nuances of selecting and adapting pretrained\nlanguage models for an engineering domain, continuous model updates based on\nuser interaction with the contextual assistant and creating the causal chain\nfor explainable recommendations based on the knowledge representation.\nAdditionally, we demonstrate that the model adaptation is feasible using\nmoderate computational infrastructure already available to most engineering\nteams in manufacturing organizations and inference can be performed on standard\nCPU only instances for integration with existing applications making these\nmethods easily deployable.", "published": "2023-08-01 08:12:43", "link": "http://arxiv.org/abs/2308.00364v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tackling Hallucinations in Neural Chart Summarization", "abstract": "Hallucinations in text generation occur when the system produces text that is\nnot grounded in the input. In this work, we tackle the problem of\nhallucinations in neural chart summarization. Our analysis shows that the\ntarget side of chart summarization training datasets often contains additional\ninformation, leading to hallucinations. We propose a natural language inference\n(NLI) based method to preprocess the training data and show through human\nevaluation that our method significantly reduces hallucinations. We also found\nthat shortening long-distance dependencies in the input sequence and adding\nchart-related information like title and legends improves the overall\nperformance.", "published": "2023-08-01 09:26:40", "link": "http://arxiv.org/abs/2308.00399v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ZRIGF: An Innovative Multimodal Framework for Zero-Resource\n  Image-Grounded Dialogue Generation", "abstract": "Image-grounded dialogue systems benefit greatly from integrating visual\ninformation, resulting in high-quality response generation. However, current\nmodels struggle to effectively utilize such information in zero-resource\nscenarios, mainly due to the disparity between image and text modalities. To\novercome this challenge, we propose an innovative multimodal framework, called\nZRIGF, which assimilates image-grounded information for dialogue generation in\nzero-resource situations. ZRIGF implements a two-stage learning strategy,\ncomprising contrastive pre-training and generative pre-training. Contrastive\npre-training includes a text-image matching module that maps images and texts\ninto a unified encoded vector space, along with a text-assisted masked image\nmodeling module that preserves pre-training visual features and fosters further\nmultimodal feature alignment. Generative pre-training employs a multimodal\nfusion module and an information transfer module to produce insightful\nresponses based on harmonized multimodal representations. Comprehensive\nexperiments conducted on both text-based and image-grounded dialogue datasets\ndemonstrate ZRIGF's efficacy in generating contextually pertinent and\ninformative responses. Furthermore, we adopt a fully zero-resource scenario in\nthe image-grounded dialogue dataset to demonstrate our framework's robust\ngeneralization capabilities in novel domains. The code is available at\nhttps://github.com/zhangbo-nlp/ZRIGF.", "published": "2023-08-01 09:28:36", "link": "http://arxiv.org/abs/2308.00400v2", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Discourse-Aware Text Simplification: From Complex Sentences to Linked\n  Propositions", "abstract": "Sentences that present a complex syntax act as a major stumbling block for\ndownstream Natural Language Processing applications whose predictive quality\ndeteriorates with sentence length and complexity. The task of Text\nSimplification (TS) may remedy this situation. It aims to modify sentences in\norder to make them easier to process, using a set of rewriting operations, such\nas reordering, deletion, or splitting. State-of-the-art syntactic TS approaches\nsuffer from two major drawbacks: first, they follow a very conservative\napproach in that they tend to retain the input rather than transforming it, and\nsecond, they ignore the cohesive nature of texts, where context spread across\nclauses or sentences is needed to infer the true meaning of a statement. To\naddress these problems, we present a discourse-aware TS approach that splits\nand rephrases complex English sentences within the semantic context in which\nthey occur. Based on a linguistically grounded transformation stage that uses\nclausal and phrasal disembedding mechanisms, complex sentences are transformed\ninto shorter utterances with a simple canonical structure that can be easily\nanalyzed by downstream applications. With sentence splitting, we thus address a\nTS task that has hardly been explored so far. Moreover, we introduce the notion\nof minimality in this context, as we aim to decompose source sentences into a\nset of self-contained minimal semantic units. To avoid breaking down the input\ninto a disjointed sequence of statements that is difficult to interpret because\nimportant contextual information is missing, we incorporate the semantic\ncontext between the split propositions in the form of hierarchical structures\nand semantic relationships. In that way, we generate a semantic hierarchy of\nminimal propositions that leads to a novel representation of complex assertions\nthat puts a semantic layer on top of the simplified sentences.", "published": "2023-08-01 10:10:59", "link": "http://arxiv.org/abs/2308.00425v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Structural Embeddings of Tools for Large Language Models", "abstract": "It is evident that the current state of Large Language Models (LLMs)\nnecessitates the incorporation of external tools. The lack of straightforward\nalgebraic and logical reasoning is well documented and prompted researchers to\ndevelop frameworks which allow LLMs to operate via external tools. The\nontological nature of tool utilization for a specific task can be well\nformulated with a Directed Acyclic Graph (DAG). The central aim of the paper is\nto highlight the importance of graph based approaches to LLM-tool interaction\nin near future. We propose an exemplary framework to guide the orchestration of\nexponentially increasing numbers of external tools with LLMs,where objectives\nand functionalities of tools are graph encoded hierarchically. Assuming that\ntextual segments of a Chain-of-Thought (CoT) can be imagined as a tool as\ndefined here, the graph based framework can pave new avenues in that particular\ndirection as well.", "published": "2023-08-01 10:46:09", "link": "http://arxiv.org/abs/2308.00447v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Retrieval Augmented Generation and Representative Vector Summarization\n  for large unstructured textual data in Medical Education", "abstract": "Large Language Models are increasingly being used for various tasks including\ncontent generation and as chatbots. Despite their impressive performances in\ngeneral tasks, LLMs need to be aligned when applying for domain specific tasks\nto mitigate the problems of hallucination and producing harmful answers.\nRetrieval Augmented Generation (RAG) allows to easily attach and manipulate a\nnon-parametric knowledgebases to LLMs. Applications of RAG in the field of\nmedical education are discussed in this paper. A combined extractive and\nabstractive summarization method for large unstructured textual data using\nrepresentative vectors is proposed.", "published": "2023-08-01 12:04:50", "link": "http://arxiv.org/abs/2308.00479v1", "categories": ["cs.CL", "cs.AI", "H.3.1; J.3"], "primary_category": "cs.CL"}
{"title": "JIANG: Chinese Open Foundation Language Model", "abstract": "With the advancements in large language model technology, it has showcased\ncapabilities that come close to those of human beings across various tasks.\nThis achievement has garnered significant interest from companies and\nscientific research institutions, leading to substantial investments in the\nresearch and development of these models. While numerous large models have\nemerged during this period, the majority of them have been trained primarily on\nEnglish data. Although they exhibit decent performance in other languages, such\nas Chinese, their potential remains limited due to factors like vocabulary\ndesign and training corpus. Consequently, their ability to fully express their\ncapabilities in Chinese falls short. To address this issue, we introduce the\nmodel named JIANG (Chinese pinyin of ginger) specifically designed for the\nChinese language. We have gathered a substantial amount of Chinese corpus to\ntrain the model and have also optimized its structure. The extensive\nexperimental results demonstrate the excellent performance of our model.", "published": "2023-08-01 15:51:41", "link": "http://arxiv.org/abs/2308.00624v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ALE: A Simulation-Based Active Learning Evaluation Framework for the\n  Parameter-Driven Comparison of Query Strategies for NLP", "abstract": "Supervised machine learning and deep learning require a large amount of\nlabeled data, which data scientists obtain in a manual, and time-consuming\nannotation process. To mitigate this challenge, Active Learning (AL) proposes\npromising data points to annotators they annotate next instead of a subsequent\nor random sample. This method is supposed to save annotation effort while\nmaintaining model performance. However, practitioners face many AL strategies\nfor different tasks and need an empirical basis to choose between them. Surveys\ncategorize AL strategies into taxonomies without performance indications.\nPresentations of novel AL strategies compare the performance to a small subset\nof strategies. Our contribution addresses the empirical basis by introducing a\nreproducible active learning evaluation (ALE) framework for the comparative\nevaluation of AL strategies in NLP. The framework allows the implementation of\nAL strategies with low effort and a fair data-driven comparison through\ndefining and tracking experiment parameters (e.g., initial dataset size, number\nof data points per query step, and the budget). ALE helps practitioners to make\nmore informed decisions, and researchers can focus on developing new, effective\nAL strategies and deriving best practices for specific use cases. With best\npractices, practitioners can lower their annotation costs. We present a case\nstudy to illustrate how to use the framework.", "published": "2023-08-01 10:42:11", "link": "http://arxiv.org/abs/2308.02537v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Proceedings Modalities in substructural logics: Applications at the\n  interfaces of logic, language and computation", "abstract": "By calling into question the implicit structural rules that are taken for\ngranted in classical logic, substructural logics have brought to the fore new\nforms of reasoning with applications in many interdisciplinary areas of\ninterest. Modalities, in the substructural setting, provide the tools to\ncontrol and finetune the logical resource management. The focus of the workshop\nis on applications in the areas of interest to the ESSLLI community, in\nparticular logical approaches to natural language syntax and semantics and the\ndynamics of reasoning. The workshop is held with the support of the Horizon\n2020 MSCA-Rise project MOSAIC .", "published": "2023-08-01 22:40:19", "link": "http://arxiv.org/abs/2308.03679v1", "categories": ["cs.LO", "cs.CL"], "primary_category": "cs.LO"}
{"title": "Covid-19 Public Sentiment Analysis for Indian Tweets Classification", "abstract": "When any extraordinary event takes place in the world wide area, it is the\nsocial media that acts as the fastest carrier of the news along with the\nconsequences dealt with that event. One can gather much information through\nsocial networks regarding the sentiments, behavior, and opinions of the people.\nIn this paper, we focus mainly on sentiment analysis of twitter data of India\nwhich comprises of COVID-19 tweets. We show how Twitter data has been extracted\nand then run sentimental analysis queries on it. This is helpful to analyze the\ninformation in the tweets where opinions are highly unstructured,\nheterogeneous, and are either positive or negative or neutral in some cases.", "published": "2023-08-01 09:29:55", "link": "http://arxiv.org/abs/2308.06241v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Advancing Beyond Identification: Multi-bit Watermark for Large Language\n  Models", "abstract": "We show the viability of tackling misuses of large language models beyond the\nidentification of machine-generated text. While existing zero-bit watermark\nmethods focus on detection only, some malicious misuses demand tracing the\nadversary user for counteracting them. To address this, we propose Multi-bit\nWatermark via Position Allocation, embedding traceable multi-bit information\nduring language model generation. Through allocating tokens onto different\nparts of the messages, we embed longer messages in high corruption settings\nwithout added latency. By independently embedding sub-units of messages, the\nproposed method outperforms the existing works in terms of robustness and\nlatency. Leveraging the benefits of zero-bit watermarking, our method enables\nrobust extraction of the watermark without any model access, embedding and\nextraction of long messages ($\\geq$ 32-bit) without finetuning, and maintaining\ntext quality, while allowing zero-bit detection all at the same time. Code is\nreleased here: https://github.com/bangawayoo/mb-lm-watermarking", "published": "2023-08-01 01:27:40", "link": "http://arxiv.org/abs/2308.00221v3", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Multimodal Multi-loss Fusion Network for Sentiment Analysis", "abstract": "This paper investigates the optimal selection and fusion of feature encoders\nacross multiple modalities and combines these in one neural network to improve\nsentiment detection. We compare different fusion methods and examine the impact\nof multi-loss training within the multi-modality fusion network, identifying\nsurprisingly important findings relating to subnet performance. We have also\nfound that integrating context significantly enhances model performance. Our\nbest model achieves state-of-the-art performance for three datasets (CMU-MOSI,\nCMU-MOSEI and CH-SIMS). These results suggest a roadmap toward an optimized\nfeature selection and fusion approach for enhancing sentiment detection in\nneural networks.", "published": "2023-08-01 03:54:27", "link": "http://arxiv.org/abs/2308.00264v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Making the V in Text-VQA Matter", "abstract": "Text-based VQA aims at answering questions by reading the text present in the\nimages. It requires a large amount of scene-text relationship understanding\ncompared to the VQA task. Recent studies have shown that the question-answer\npairs in the dataset are more focused on the text present in the image but less\nimportance is given to visual features and some questions do not require\nunderstanding the image. The models trained on this dataset predict biased\nanswers due to the lack of understanding of visual context. For example, in\nquestions like \"What is written on the signboard?\", the answer predicted by the\nmodel is always \"STOP\" which makes the model to ignore the image. To address\nthese issues, we propose a method to learn visual features (making V matter in\nTextVQA) along with the OCR features and question features using VQA dataset as\nexternal knowledge for Text-based VQA. Specifically, we combine the TextVQA\ndataset and VQA dataset and train the model on this combined dataset. Such a\nsimple, yet effective approach increases the understanding and correlation\nbetween the image features and text present in the image, which helps in the\nbetter answering of questions. We further test the model on different datasets\nand compare their qualitative and quantitative results.", "published": "2023-08-01 05:28:13", "link": "http://arxiv.org/abs/2308.00295v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step\n  Reasoning", "abstract": "The recent progress in large language models (LLMs), especially the invention\nof chain-of-thought prompting, has made it possible to automatically answer\nquestions by stepwise reasoning. However, when faced with more complicated\nproblems that require non-linear thinking, even the strongest LLMs make\nmistakes. To address this, we explore whether LLMs are able to recognize errors\nin their own step-by-step reasoning, without resorting to external resources.\nTo this end, we propose SelfCheck, a general-purpose zero-shot verification\nschema for recognizing such errors. We then use the results of these checks to\nimprove question-answering performance by conducting weighted voting on\nmultiple solutions to the question. We test SelfCheck on three datasets (GSM8K,\nMathQA, and MATH) and find that it successfully recognizes errors and, in turn,\nincreases final answer accuracies.", "published": "2023-08-01 10:31:36", "link": "http://arxiv.org/abs/2308.00436v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Tool Documentation Enables Zero-Shot Tool-Usage with Large Language\n  Models", "abstract": "Today, large language models (LLMs) are taught to use new tools by providing\na few demonstrations of the tool's usage. Unfortunately, demonstrations are\nhard to acquire, and can result in undesirable biased usage if the wrong\ndemonstration is chosen. Even in the rare scenario that demonstrations are\nreadily available, there is no principled selection protocol to determine how\nmany and which ones to provide. As tasks grow more complex, the selection\nsearch grows combinatorially and invariably becomes intractable. Our work\nprovides an alternative to demonstrations: tool documentation. We advocate the\nuse of tool documentation, descriptions for the individual tool usage, over\ndemonstrations. We substantiate our claim through three main empirical findings\non 6 tasks across both vision and language modalities. First, on existing\nbenchmarks, zero-shot prompts with only tool documentation are sufficient for\neliciting proper tool usage, achieving performance on par with few-shot\nprompts. Second, on a newly collected realistic tool-use dataset with hundreds\nof available tool APIs, we show that tool documentation is significantly more\nvaluable than demonstrations, with zero-shot documentation significantly\noutperforming few-shot without documentation. Third, we highlight the benefits\nof tool documentations by tackling image generation and video tracking using\njust-released unseen state-of-the-art models as tools. Finally, we highlight\nthe possibility of using tool documentation to automatically enable new\napplications: by using nothing more than the documentation of GroundingDino,\nStable Diffusion, XMem, and SAM, LLMs can re-invent the functionalities of the\njust-released Grounded-SAM and Track Anything models.", "published": "2023-08-01 17:21:38", "link": "http://arxiv.org/abs/2308.00675v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CodeBPE: Investigating Subtokenization Options for Large Language Model\n  Pretraining on Source Code", "abstract": "Recent works have widely adopted large language model pretraining for source\ncode, suggested source code-specific pretraining objectives and investigated\nthe applicability of various Transformer-based language model architectures for\nsource code. This work investigates another important aspect of such models,\nnamely the effect of different subtokenization options, and aims at identifying\nmost effective and length-efficient subtokenizations, taking into account code\nspecifics. We propose subtokenziation that reduces average length by 17%\nwithout downstream performance drop, and show that a carefully chosen\nsubtokenization may improve quality by 0.5-2%, possibly with some length\nincrease.", "published": "2023-08-01 17:40:48", "link": "http://arxiv.org/abs/2308.00683v1", "categories": ["cs.LG", "cs.CL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "The Bias Amplification Paradox in Text-to-Image Generation", "abstract": "Bias amplification is a phenomenon in which models exacerbate biases or\nstereotypes present in the training data. In this paper, we study bias\namplification in the text-to-image domain using Stable Diffusion by comparing\ngender ratios in training vs. generated images. We find that the model appears\nto amplify gender-occupation biases found in the training data (LAION)\nconsiderably. However, we discover that amplification can be largely attributed\nto discrepancies between training captions and model prompts. For example, an\ninherent difference is that captions from the training data often contain\nexplicit gender information while our prompts do not, which leads to a\ndistribution shift and consequently inflates bias measures. Once we account for\ndistributional differences between texts used for training and generation when\nevaluating amplification, we observe that amplification decreases drastically.\nOur findings illustrate the challenges of comparing biases in models and their\ntraining data, and highlight confounding factors that impact analyses.", "published": "2023-08-01 18:00:08", "link": "http://arxiv.org/abs/2308.00755v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Self-Supervised Contrastive BERT Fine-tuning for Fusion-based\n  Reviewed-Item Retrieval", "abstract": "As natural language interfaces enable users to express increasingly complex\nnatural language queries, there is a parallel explosion of user review content\nthat can allow users to better find items such as restaurants, books, or movies\nthat match these expressive queries. While Neural Information Retrieval (IR)\nmethods have provided state-of-the-art results for matching queries to\ndocuments, they have not been extended to the task of Reviewed-Item Retrieval\n(RIR), where query-review scores must be aggregated (or fused) into item-level\nscores for ranking. In the absence of labeled RIR datasets, we extend Neural IR\nmethodology to RIR by leveraging self-supervised methods for contrastive\nlearning of BERT embeddings for both queries and reviews. Specifically,\ncontrastive learning requires a choice of positive and negative samples, where\nthe unique two-level structure of our item-review data combined with meta-data\naffords us a rich structure for the selection of these samples. For contrastive\nlearning in a Late Fusion scenario, we investigate the use of positive review\nsamples from the same item and/or with the same rating, selection of hard\npositive samples by choosing the least similar reviews from the same anchor\nitem, and selection of hard negative samples by choosing the most similar\nreviews from different items. We also explore anchor sub-sampling and\naugmenting with meta-data. For a more end-to-end Early Fusion approach, we\nintroduce contrastive item embedding learning to fuse reviews into single item\nembeddings. Experimental results show that Late Fusion contrastive learning for\nNeural RIR outperforms all other contrastive IR configurations, Neural IR, and\nsparse retrieval baselines, thus demonstrating the power of exploiting the\ntwo-level structure in Neural RIR approaches as well as the importance of\npreserving the nuance of individual review content via Late Fusion methods.", "published": "2023-08-01 18:01:21", "link": "http://arxiv.org/abs/2308.00762v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "ChatMOF: An Autonomous AI System for Predicting and Generating\n  Metal-Organic Frameworks", "abstract": "ChatMOF is an autonomous Artificial Intelligence (AI) system that is built to\npredict and generate metal-organic frameworks (MOFs). By leveraging a\nlarge-scale language model (GPT-4 and GPT-3.5-turbo), ChatMOF extracts key\ndetails from textual inputs and delivers appropriate responses, thus\neliminating the necessity for rigid structured queries. The system is comprised\nof three core components (i.e. an agent, a toolkit, and an evaluator) and it\nforms a robust pipeline that manages a variety of tasks, including data\nretrieval, property prediction, and structure generations. The study further\nexplores the merits and constraints of using large language models (LLMs) AI\nsystem in material sciences using and showcases its transformative potential\nfor future advancements.", "published": "2023-08-01 02:08:13", "link": "http://arxiv.org/abs/2308.01423v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "physics.chem-ph"], "primary_category": "cs.CL"}
{"title": "Towards More Human-like AI Communication: A Review of Emergent\n  Communication Research", "abstract": "In the recent shift towards human-centric AI, the need for machines to\naccurately use natural language has become increasingly important. While a\ncommon approach to achieve this is to train large language models, this method\npresents a form of learning misalignment where the model may not capture the\nunderlying structure and reasoning humans employ in using natural language,\npotentially leading to unexpected or unreliable behavior. Emergent\ncommunication (Emecom) is a field of research that has seen a growing number of\npublications in recent years, aiming to develop artificial agents capable of\nusing natural language in a way that goes beyond simple discriminative tasks\nand can effectively communicate and learn new concepts. In this review, we\npresent Emecom under two aspects. Firstly, we delineate all the common\nproprieties we find across the literature and how they relate to human\ninteractions. Secondly, we identify two subcategories and highlight their\ncharacteristics and open challenges. We encourage researchers to work together\nby demonstrating that different methods can be viewed as diverse solutions to a\ncommon problem and emphasize the importance of including diverse perspectives\nand expertise in the field. We believe a deeper understanding of human\ncommunication is crucial to developing machines that can accurately use natural\nlanguage in human-machine interactions.", "published": "2023-08-01 14:43:10", "link": "http://arxiv.org/abs/2308.02541v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.MA", "I.2.6; I.2.7; I.2.11"], "primary_category": "cs.CL"}
{"title": "Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain\n  Adapted Least-To-Most Prompting", "abstract": "Cross-domain and cross-compositional generalization of Text-to-SQL semantic\nparsing is a challenging task. Existing Large Language Model (LLM) based\nsolutions rely on inference-time retrieval of few-shot exemplars from the\ntraining set to synthesize a run-time prompt for each Natural Language (NL)\ntest query. In contrast, we devise an algorithm which performs offline sampling\nof a minimal set-of few-shots from the training data, with complete coverage of\nSQL clauses, operators and functions, and maximal domain coverage within the\nallowed token length. This allows for synthesis of a fixed Generic Prompt (GP),\nwith a diverse set-of exemplars common across NL test queries, avoiding\nexpensive test time exemplar retrieval. We further auto-adapt the GP to the\ntarget database domain (DA-GP), to better handle cross-domain generalization;\nfollowed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle\ncross-compositional generalization. The synthesis of LTMP-DA-GP is an offline\ntask, to be performed one-time per new database with minimal human\nintervention. Our approach demonstrates superior performance on the KaggleDBQA\ndataset, designed to evaluate generalizability for the Text-to-SQL task. We\nfurther showcase consistent performance improvement of LTMP-DA-GP over GP,\nacross LLMs and databases of KaggleDBQA, highlighting the efficacy and model\nagnostic benefits of our prompt based adapt and decompose approach.", "published": "2023-08-01 05:31:36", "link": "http://arxiv.org/abs/2308.02582v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Circumvent spherical Bessel function nulls for open sphere microphone\n  arrays with physics informed neural network", "abstract": "Open sphere microphone arrays (OSMAs) are simple to design and do not\nintroduce scattering fields, and thus can be advantageous than other arrays for\nimplementing spatial acoustic algorithms under spherical model decomposition.\nHowever, an OSMA suffers from spherical Bessel function nulls which make it\nhard to obtain some sound field coefficients at certain frequencies. This paper\nproposes to assist an OSMA for sound field analysis with physics informed\nneural network (PINN). A PINN models the measurement of an OSMA and predicts\nthe sound field on another sphere whose radius is different from that of the\nOSMA. Thanks to the fact that spherical Bessel function nulls vary with radius,\nthe sound field coefficients which are hard to obtain based on the OSMA\nmeasurement directly can be obtained based on the prediction. Simulations\nconfirm the effectiveness of this approach and compare it with the rigid sphere\napproach.", "published": "2023-08-01 02:50:32", "link": "http://arxiv.org/abs/2308.00242v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Generative adversarial networks with physical sound field priors", "abstract": "This paper presents a deep learning-based approach for the spatio-temporal\nreconstruction of sound fields using Generative Adversarial Networks (GANs).\nThe method utilises a plane wave basis and learns the underlying statistical\ndistributions of pressure in rooms to accurately reconstruct sound fields from\na limited number of measurements. The performance of the method is evaluated\nusing two established datasets and compared to state-of-the-art methods. The\nresults show that the model is able to achieve an improved reconstruction\nperformance in terms of accuracy and energy retention, particularly in the\nhigh-frequency range and when extrapolating beyond the measurement region.\nFurthermore, the proposed method can handle a varying number of measurement\npositions and configurations without sacrificing performance. The results\nsuggest that this approach provides a promising approach to sound field\nreconstruction using generative models that allow for a physically informed\nprior to acoustics problems.", "published": "2023-08-01 10:11:23", "link": "http://arxiv.org/abs/2308.00426v1", "categories": ["eess.AS", "cs.AI", "65C60", "I.2.10; J.2.3; I.5.4"], "primary_category": "eess.AS"}
{"title": "Multi-goal Audio-visual Navigation using Sound Direction Map", "abstract": "Over the past few years, there has been a great deal of research on\nnavigation tasks in indoor environments using deep reinforcement learning\nagents. Most of these tasks use only visual information in the form of\nfirst-person images to navigate to a single goal. More recently, tasks that\nsimultaneously use visual and auditory information to navigate to the sound\nsource and even navigation tasks with multiple goals instead of one have been\nproposed. However, there has been no proposal for a generalized navigation task\ncombining these two types of tasks and using both visual and auditory\ninformation in a situation where multiple sound sources are goals. In this\npaper, we propose a new framework for this generalized task: multi-goal\naudio-visual navigation. We first define the task in detail, and then we\ninvestigate the difficulty of the multi-goal audio-visual navigation task\nrelative to the current navigation tasks by conducting experiments in various\nsituations. The research shows that multi-goal audio-visual navigation has the\ndifficulty of the implicit need to separate the sources of sound. Next, to\nmitigate the difficulties in this new task, we propose a method named sound\ndirection map (SDM), which dynamically localizes multiple sound sources in a\nlearning-based manner while making use of past memories. Experimental results\nshow that the use of SDM significantly improves the performance of multiple\nbaseline methods, regardless of the number of goals.", "published": "2023-08-01 01:26:55", "link": "http://arxiv.org/abs/2308.00219v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Choir Transformer: Generating Polyphonic Music with Relative Attention\n  on Transformer", "abstract": "Polyphonic music generation is still a challenge direction due to its correct\nbetween generating melody and harmony. Most of the previous studies used\nRNN-based models. However, the RNN-based models are hard to establish the\nrelationship between long-distance notes. In this paper, we propose a\npolyphonic music generation neural network named Choir Transformer[\nhttps://github.com/Zjy0401/choir-transformer], with relative positional\nattention to better model the structure of music. We also proposed a music\nrepresentation suitable for polyphonic music generation. The performance of\nChoir Transformer surpasses the previous state-of-the-art accuracy of 4.06%. We\nalso measures the harmony metrics of polyphonic music. Experiments show that\nthe harmony metrics are close to the music of Bach. In practical application,\nthe generated melody and rhythm can be adjusted according to the specified\ninput, with different styles of music like folk music or pop music and so on.", "published": "2023-08-01 06:44:15", "link": "http://arxiv.org/abs/2308.02531v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
