{"title": "Attend to Medical Ontologies: Content Selection for Clinical Abstractive\n  Summarization", "abstract": "Sequence-to-sequence (seq2seq) network is a well-established model for text\nsummarization task. It can learn to produce readable content; however, it falls\nshort in effectively identifying key regions of the source. In this paper, we\napproach the content selection problem for clinical abstractive summarization\nby augmenting salient ontological terms into the summarizer. Our experiments on\ntwo publicly available clinical data sets (107,372 reports of MIMIC-CXR, and\n3,366 reports of OpenI) show that our model statistically significantly boosts\nstate-of-the-art results in terms of Rouge metrics (with improvements: 2.9%\nRG-1, 2.5% RG-2, 1.9% RG-L), in the healthcare domain where any range of\nimprovement impacts patients' welfare.", "published": "2020-05-01 01:12:49", "link": "http://arxiv.org/abs/2005.00163v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recurrent Neural Network Language Models Always Learn English-Like\n  Relative Clause Attachment", "abstract": "A standard approach to evaluating language models analyzes how models assign\nprobabilities to valid versus invalid syntactic constructions (i.e. is a\ngrammatical sentence more probable than an ungrammatical sentence). Our work\nuses ambiguous relative clause attachment to extend such evaluations to cases\nof multiple simultaneous valid interpretations, where stark grammaticality\ndifferences are absent. We compare model performance in English and Spanish to\nshow that non-linguistic biases in RNN LMs advantageously overlap with\nsyntactic structure in English but not Spanish. Thus, English models may appear\nto acquire human-like syntactic preferences, while models trained on Spanish\nfail to acquire comparable human-like preferences. We conclude by relating\nthese results to broader concerns about the relationship between comprehension\n(i.e. typical language model use cases) and production (which generates the\ntraining data for language models), suggesting that necessary linguistic biases\nare not present in the training signal at all.", "published": "2020-05-01 01:21:47", "link": "http://arxiv.org/abs/2005.00165v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Information Seeking in the Spirit of Learning: a Dataset for\n  Conversational Curiosity", "abstract": "Open-ended human learning and information-seeking are increasingly mediated\nby digital assistants. However, such systems often ignore the user's\npre-existing knowledge. Assuming a correlation between engagement and user\nresponses such as \"liking\" messages or asking followup questions, we design a\nWizard-of-Oz dialog task that tests the hypothesis that engagement increases\nwhen users are presented with facts related to what they know. Through\ncrowd-sourcing of this experiment, we collect and release 14K dialogs (181K\nutterances) where users and assistants converse about geographic topics like\ngeopolitical entities and locations. This dataset is annotated with\npre-existing user knowledge, message-level dialog acts, grounding to Wikipedia,\nand user reactions to messages. Responses using a user's prior knowledge\nincrease engagement. We incorporate this knowledge into a multi-task model that\nreproduces human assistant policies and improves over a BERT content model by\n13 mean reciprocal rank points.", "published": "2020-05-01 01:55:09", "link": "http://arxiv.org/abs/2005.00172v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Selecting Informative Contexts Improves Language Model Finetuning", "abstract": "Language model fine-tuning is essential for modern natural language\nprocessing, but is computationally expensive and time-consuming. Further, the\neffectiveness of fine-tuning is limited by the inclusion of training examples\nthat negatively affect performance. Here we present a general fine-tuning\nmethod that we call information gain filtration for improving the overall\ntraining efficiency and final performance of language model fine-tuning. We\ndefine the information gain of an example as the improvement on a test metric\nafter training on that example. A secondary learner is then trained to\napproximate this quantity. During fine-tuning, this learner selects informative\nexamples and skips uninformative ones. We show that our method has consistent\nimprovement across datasets, fine-tuning tasks, and language model\narchitectures. For example, we achieve a median perplexity of 54.0 on a books\ndataset compared to 57.3 for standard fine-tuning. We present statistical\nevidence that offers insight into the improvements of our method over standard\nfine-tuning. The generality of our method leads us to propose a new paradigm\nfor language model fine-tuning -- we encourage researchers to release\npretrained secondary learners on common corpora to promote efficient and\neffective fine-tuning, thereby improving the performance and reducing the\noverall energy footprint of language model fine-tuning.", "published": "2020-05-01 02:01:18", "link": "http://arxiv.org/abs/2005.00175v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sparse, Dense, and Attentional Representations for Text Retrieval", "abstract": "Dual encoders perform retrieval by encoding documents and queries into dense\nlowdimensional vectors, scoring each document by its inner product with the\nquery. We investigate the capacity of this architecture relative to sparse\nbag-of-words models and attentional neural networks. Using both theoretical and\nempirical analysis, we establish connections between the encoding dimension,\nthe margin between gold and lower-ranked documents, and the document length,\nsuggesting limitations in the capacity of fixed-length encodings to support\nprecise retrieval of long documents. Building on these insights, we propose a\nsimple neural model that combines the efficiency of dual encoders with some of\nthe expressiveness of more costly attentional architectures, and explore\nsparse-dense hybrids to capitalize on the precision of sparse retrieval. These\nmodels outperform strong alternatives in large-scale retrieval.", "published": "2020-05-01 02:21:17", "link": "http://arxiv.org/abs/2005.00181v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Linguistic Syntactic Evaluation of Word Prediction Models", "abstract": "A range of studies have concluded that neural word prediction models can\ndistinguish grammatical from ungrammatical sentences with high accuracy.\nHowever, these studies are based primarily on monolingual evidence from\nEnglish. To investigate how these models' ability to learn syntax varies by\nlanguage, we introduce CLAMS (Cross-Linguistic Assessment of Models on Syntax),\na syntactic evaluation suite for monolingual and multilingual models. CLAMS\nincludes subject-verb agreement challenge sets for English, French, German,\nHebrew and Russian, generated from grammars we develop. We use CLAMS to\nevaluate LSTM language models as well as monolingual and multilingual BERT.\nAcross languages, monolingual LSTMs achieved high accuracy on dependencies\nwithout attractors, and generally poor accuracy on agreement across object\nrelative clauses. On other constructions, agreement accuracy was generally\nhigher in languages with richer morphology. Multilingual models generally\nunderperformed monolingual models. Multilingual BERT showed high syntactic\naccuracy on English, but noticeable deficiencies in other languages.", "published": "2020-05-01 02:51:20", "link": "http://arxiv.org/abs/2005.00187v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Neural Machine Comprehension Model Robustness to Noisy Inputs\n  and Adversarial Attacks", "abstract": "We evaluate machine comprehension models' robustness to noise and adversarial\nattacks by performing novel perturbations at the character, word, and sentence\nlevel. We experiment with different amounts of perturbations to examine model\nconfidence and misclassification rate, and contrast model performance in\nadversarial training with different embedding types on two benchmark datasets.\nWe demonstrate improving model performance with ensembling. Finally, we analyze\nfactors that effect model behavior under adversarial training and develop a\nmodel to predict model errors during adversarial attacks.", "published": "2020-05-01 03:05:43", "link": "http://arxiv.org/abs/2005.00190v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KPQA: A Metric for Generative Question Answering Using Keyphrase Weights", "abstract": "In the automatic evaluation of generative question answering (GenQA) systems,\nit is difficult to assess the correctness of generated answers due to the\nfree-form of the answer. Especially, widely used n-gram similarity metrics\noften fail to discriminate the incorrect answers since they equally consider\nall of the tokens. To alleviate this problem, we propose KPQA-metric, a new\nmetric for evaluating the correctness of GenQA. Specifically, our new metric\nassigns different weights to each token via keyphrase prediction, thereby\njudging whether a generated answer sentence captures the key meaning of the\nreference answer. To evaluate our metric, we create high-quality human\njudgments of correctness on two GenQA datasets. Using our human-evaluation\ndatasets, we show that our proposed metric has a significantly higher\ncorrelation with human judgments than existing metrics. The code is available\nat https://github.com/hwanheelee1993/KPQA.", "published": "2020-05-01 03:24:36", "link": "http://arxiv.org/abs/2005.00192v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions", "abstract": "A critical part of reading is being able to understand the temporal\nrelationships between events described in a passage of text, even when those\nrelationships are not explicitly stated. However, current machine reading\ncomprehension benchmarks have practically no questions that test temporal\nphenomena, so systems trained on these benchmarks have no capacity to answer\nquestions such as \"what happened before/after [some event]?\" We introduce\nTORQUE, a new English reading comprehension benchmark built on 3.2k news\nsnippets with 21k human-generated questions querying temporal relationships.\nResults show that RoBERTa-large achieves an exact-match score of 51% on the\ntest set of TORQUE, about 30% behind human performance.", "published": "2020-05-01 06:29:56", "link": "http://arxiv.org/abs/2005.00242v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning", "abstract": "Sequential fine-tuning and multi-task learning are methods aiming to\nincorporate knowledge from multiple tasks; however, they suffer from\ncatastrophic forgetting and difficulties in dataset balancing. To address these\nshortcomings, we propose AdapterFusion, a new two stage learning algorithm that\nleverages knowledge from multiple tasks. First, in the knowledge extraction\nstage we learn task specific parameters called adapters, that encapsulate the\ntask-specific information. We then combine the adapters in a separate knowledge\ncomposition step. We show that by separating the two stages, i.e., knowledge\nextraction and knowledge composition, the classifier can effectively exploit\nthe representations learned from multiple tasks in a non-destructive manner. We\nempirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it\neffectively combines various types of knowledge at different layers of the\nmodel. We show that our approach outperforms traditional strategies such as\nfull fine-tuning as well as multi-task learning. Our code and adapters are\navailable at AdapterHub.ml.", "published": "2020-05-01 07:03:42", "link": "http://arxiv.org/abs/2005.00247v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low Resource Multi-Task Sequence Tagging -- Revisiting Dynamic\n  Conditional Random Fields", "abstract": "We compare different models for low resource multi-task sequence tagging that\nleverage dependencies between label sequences for different tasks. Our analysis\nis aimed at datasets where each example has labels for multiple tasks. Current\napproaches use either a separate model for each task or standard multi-task\nlearning to learn shared feature representations. However, these approaches\nignore correlations between label sequences, which can provide important\ninformation in settings with small training datasets. To analyze which\nscenarios can profit from modeling dependencies between labels in different\ntasks, we revisit dynamic conditional random fields (CRFs) and combine them\nwith deep neural networks. We compare single-task, multi-task and dynamic CRF\nsetups for three diverse datasets at both sentence and document levels in\nEnglish and German low resource scenarios. We show that including silver labels\nfrom pretrained part-of-speech taggers as auxiliary tasks can improve\nperformance on downstream tasks. We find that especially in low-resource\nscenarios, the explicit modeling of inter-dependencies between task predictions\noutperforms single-task as well as standard multi-task models.", "published": "2020-05-01 07:11:34", "link": "http://arxiv.org/abs/2005.00250v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Controllable Biases in Language Generation", "abstract": "We present a general approach towards controllable societal biases in natural\nlanguage generation (NLG). Building upon the idea of adversarial triggers, we\ndevelop a method to induce societal biases in generated text when input prompts\ncontain mentions of specific demographic groups. We then analyze two scenarios:\n1) inducing negative biases for one demographic and positive biases for another\ndemographic, and 2) equalizing biases between demographics. The former scenario\nenables us to detect the types of biases present in the model. Specifically, we\nshow the effectiveness of our approach at facilitating bias analysis by finding\ntopics that correspond to demographic inequalities in generated text and\ncomparing the relative effectiveness of inducing biases for different\ndemographics. The second scenario is useful for mitigating biases in downstream\napplications such as dialogue generation. In our experiments, the mitigation\ntechnique proves to be effective at equalizing the amount of biases across\ndemographics while simultaneously generating less negatively biased text\noverall.", "published": "2020-05-01 08:25:11", "link": "http://arxiv.org/abs/2005.00268v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Facilitating Access to Multilingual COVID-19 Information via Neural\n  Machine Translation", "abstract": "Every day, more people are becoming infected and dying from exposure to\nCOVID-19. Some countries in Europe like Spain, France, the UK and Italy have\nsuffered particularly badly from the virus. Others such as Germany appear to\nhave coped extremely well. Both health professionals and the general public are\nkeen to receive up-to-date information on the effects of the virus, as well as\ntreatments that have proven to be effective. In cases where language is a\nbarrier to access of pertinent information, machine translation (MT) may help\npeople assimilate information published in different languages. Our MT systems\ntrained on COVID-19 data are freely available for anyone to use to help\ntranslate information published in German, French, Italian, Spanish into\nEnglish, as well as the reverse direction.", "published": "2020-05-01 09:31:38", "link": "http://arxiv.org/abs/2005.00283v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Selecting Backtranslated Data from Multiple Sources for Improved Neural\n  Machine Translation", "abstract": "Machine translation (MT) has benefited from using synthetic training data\noriginating from translating monolingual corpora, a technique known as\nbacktranslation. Combining backtranslated data from different sources has led\nto better results than when using such data in isolation. In this work we\nanalyse the impact that data translated with rule-based, phrase-based\nstatistical and neural MT systems has on new MT systems. We use a real-world\nlow-resource use-case (Basque-to-Spanish in the clinical domain) as well as a\nhigh-resource language pair (German-to-English) to test different scenarios\nwith backtranslation and employ data selection to optimise the synthetic\ncorpora. We exploit different data selection strategies in order to reduce the\namount of data used, while at the same time maintaining high-quality MT\nsystems. We further tune the data selection method by taking into account the\nquality of the MT systems used for backtranslation and lexical diversity of the\nresulting corpora. Our experiments show that incorporating backtranslated data\nfrom different sources can be beneficial, and that availing of data selection\ncan yield improved performance.", "published": "2020-05-01 10:50:53", "link": "http://arxiv.org/abs/2005.00308v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mind the Trade-off: Debiasing NLU Models without Degrading the\n  In-distribution Performance", "abstract": "Models for natural language understanding (NLU) tasks often rely on the\nidiosyncratic biases of the dataset, which make them brittle against test cases\noutside the training distribution. Recently, several proposed debiasing methods\nare shown to be very effective in improving out-of-distribution performance.\nHowever, their improvements come at the expense of performance drop when models\nare evaluated on the in-distribution data, which contain examples with higher\ndiversity. This seemingly inevitable trade-off may not tell us much about the\nchanges in the reasoning and understanding capabilities of the resulting models\non broader types of examples beyond the small subset represented in the\nout-of-distribution data. In this paper, we address this trade-off by\nintroducing a novel debiasing method, called confidence regularization, which\ndiscourage models from exploiting biases while enabling them to receive enough\nincentive to learn from all the training examples. We evaluate our method on\nthree NLU tasks and show that, in contrast to its predecessors, it improves the\nperformance on out-of-distribution datasets (e.g., 7pp gain on HANS dataset)\nwhile maintaining the original in-distribution accuracy.", "published": "2020-05-01 11:22:55", "link": "http://arxiv.org/abs/2005.00315v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning", "abstract": "In order to simulate human language capacity, natural language processing\nsystems must be able to reason about the dynamics of everyday situations,\nincluding their possible causes and effects. Moreover, they should be able to\ngeneralise the acquired world knowledge to new languages, modulo cultural\ndifferences. Advances in machine reasoning and cross-lingual transfer depend on\nthe availability of challenging evaluation benchmarks. Motivated by both\ndemands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a\ntypologically diverse multilingual dataset for causal commonsense reasoning in\n11 languages, which includes resource-poor languages like Eastern Apur\\'imac\nQuechua and Haitian Creole. We evaluate a range of state-of-the-art models on\nthis novel dataset, revealing that the performance of current methods based on\nmultilingual pretraining and zero-shot fine-tuning falls short compared to\ntranslation-based transfer. Finally, we propose strategies to adapt\nmultilingual models to out-of-sample resource-lean languages where only a small\ncorpus or a bilingual dictionary is available, and report substantial\nimprovements over the random baseline. The XCOPA dataset is freely available at\ngithub.com/cambridgeltl/xcopa.", "published": "2020-05-01 12:22:33", "link": "http://arxiv.org/abs/2005.00333v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Will-They-Won't-They: A Very Large Dataset for Stance Detection on\n  Twitter", "abstract": "We present a new challenging stance detection dataset, called\nWill-They-Won't-They (WT-WT), which contains 51,284 tweets in English, making\nit by far the largest available dataset of the type. All the annotations are\ncarried out by experts; therefore, the dataset constitutes a high-quality and\nreliable benchmark for future research in stance detection. Our experiments\nwith a wide range of recent state-of-the-art stance detection systems show that\nthe dataset poses a strong challenge to existing models in this domain.", "published": "2020-05-01 14:10:37", "link": "http://arxiv.org/abs/2005.00388v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Necessary Elements for BERT's Multilinguality", "abstract": "It has been shown that multilingual BERT (mBERT) yields high quality\nmultilingual representations and enables effective zero-shot transfer. This is\nsurprising given that mBERT does not use any crosslingual signal during\ntraining. While recent literature has studied this phenomenon, the reasons for\nthe multilinguality are still somewhat obscure. We aim to identify\narchitectural properties of BERT and linguistic properties of languages that\nare necessary for BERT to become multilingual. To allow for fast\nexperimentation we propose an efficient setup with small BERT models trained on\na mix of synthetic and natural data. Overall, we identify four architectural\nand two linguistic elements that influence multilinguality. Based on our\ninsights, we experiment with a multilingual pretraining setup that modifies the\nmasking strategy using VecMap, i.e., unsupervised embedding alignment.\nExperiments on XNLI with three languages indicate that our findings transfer\nfrom our small setup to larger scale settings.", "published": "2020-05-01 14:27:14", "link": "http://arxiv.org/abs/2005.00396v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topological Sort for Sentence Ordering", "abstract": "Sentence ordering is the task of arranging the sentences of a given text in\nthe correct order. Recent work using deep neural networks for this task has\nframed it as a sequence prediction problem. In this paper, we propose a new\nframing of this task as a constraint solving problem and introduce a new\ntechnique to solve it. Additionally, we propose a human evaluation for this\ntask. The results on both automatic and human metrics across four different\ndatasets show that this new technique is better at capturing coherence in\ndocuments.", "published": "2020-05-01 15:07:59", "link": "http://arxiv.org/abs/2005.00432v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Style Variation as a Vantage Point for Code-Switching", "abstract": "Code-Switching (CS) is a common phenomenon observed in several bilingual and\nmultilingual communities, thereby attaining prevalence in digital and social\nmedia platforms. This increasing prominence demands the need to model CS\nlanguages for critical downstream tasks. A major problem in this domain is the\ndearth of annotated data and a substantial corpora to train large scale neural\nmodels. Generating vast amounts of quality text assists several down stream\ntasks that heavily rely on language modeling such as speech recognition,\ntext-to-speech synthesis etc,. We present a novel vantage point of CS to be\nstyle variations between both the participating languages. Our approach does\nnot need any external annotations such as lexical language ids. It mainly\nrelies on easily obtainable monolingual corpora without any parallel alignment\nand a limited set of naturally CS sentences. We propose a two-stage generative\nadversarial training approach where the first stage generates competitive\nnegative examples for CS and the second stage generates more realistic CS\nsentences. We present our experiments on the following pairs of languages:\nSpanish-English, Mandarin-English, Hindi-English and Arabic-French. We show\nthat the trends in metrics for generated CS move closer to real CS data in each\nof the above language pairs through the dual stage training process. We believe\nthis viewpoint of CS as style variations opens new perspectives for modeling\nvarious tasks in CS text.", "published": "2020-05-01 15:53:16", "link": "http://arxiv.org/abs/2005.00458v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Broad-Coverage Medical Entity Linking with Semantic Type\n  Prediction and Large-Scale Datasets", "abstract": "Medical entity linking is the task of identifying and standardizing medical\nconcepts referred to in an unstructured text. Most of the existing methods\nadopt a three-step approach of (1) detecting mentions, (2) generating a list of\ncandidate concepts, and finally (3) picking the best concept among them. In\nthis paper, we probe into alleviating the problem of overgeneration of\ncandidate concepts in the candidate generation module, the most under-studied\ncomponent of medical entity linking. For this, we present MedType, a fully\nmodular system that prunes out irrelevant candidate concepts based on the\npredicted semantic type of an entity mention. We incorporate MedType into five\noff-the-shelf toolkits for medical entity linking and demonstrate that it\nconsistently improves entity linking performance across several benchmark\ndatasets. To address the dearth of annotated training data for medical entity\nlinking, we present WikiMed and PubMedDS, two large-scale medical entity\nlinking datasets, and demonstrate that pre-training MedType on these datasets\nfurther improves entity linking performance. We make our source code and\ndatasets publicly available for medical entity linking research.", "published": "2020-05-01 15:55:50", "link": "http://arxiv.org/abs/2005.00460v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification\n  Models with Multiple Rewriting Transformations", "abstract": "In order to simplify a sentence, human editors perform multiple rewriting\ntransformations: they split it into several shorter sentences, paraphrase words\n(i.e. replacing complex words or phrases by simpler synonyms), reorder\ncomponents, and/or delete information deemed unnecessary. Despite these varied\nrange of possible text alterations, current models for automatic sentence\nsimplification are evaluated using datasets that are focused on a single\ntransformation, such as lexical paraphrasing or splitting. This makes it\nimpossible to understand the ability of simplification models in more realistic\nsettings. To alleviate this limitation, this paper introduces ASSET, a new\ndataset for assessing sentence simplification in English. ASSET is a\ncrowdsourced multi-reference corpus where each simplification was produced by\nexecuting several rewriting transformations. Through quantitative and\nqualitative experiments, we show that simplifications in ASSET are better at\ncapturing characteristics of simplicity when compared to other standard\nevaluation datasets for the task. Furthermore, we motivate the need for\ndeveloping better methods for automatic evaluation using ASSET, since we show\nthat current popular metrics may not be suitable when multiple simplification\ntransformations are performed.", "published": "2020-05-01 16:44:54", "link": "http://arxiv.org/abs/2005.00481v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discourse-Aware Unsupervised Summarization of Long Scientific Documents", "abstract": "We propose an unsupervised graph-based ranking model for extractive\nsummarization of long scientific documents. Our method assumes a two-level\nhierarchical graph representation of the source document, and exploits\nasymmetrical positional cues to determine sentence importance. Results on the\nPubMed and arXiv datasets show that our approach outperforms strong\nunsupervised baselines by wide margins in automatic metrics and human\nevaluation. In addition, it achieves performance comparable to many\nstate-of-the-art supervised approaches which are trained on hundreds of\nthousands of examples. These results suggest that patterns in the discourse\nstructure are a strong signal for determining importance in scientific\narticles.", "published": "2020-05-01 17:31:11", "link": "http://arxiv.org/abs/2005.00513v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GoEmotions: A Dataset of Fine-Grained Emotions", "abstract": "Understanding emotion expressed in language has a wide range of applications,\nfrom building empathetic chatbots to detecting harmful online behavior.\nAdvancement in this area can be improved using large-scale datasets with a\nfine-grained typology, adaptable to multiple downstream tasks. We introduce\nGoEmotions, the largest manually annotated dataset of 58k English Reddit\ncomments, labeled for 27 emotion categories or Neutral. We demonstrate the high\nquality of the annotations via Principal Preserved Component Analysis. We\nconduct transfer learning experiments with existing emotion benchmarks to show\nthat our dataset generalizes well to other domains and different emotion\ntaxonomies. Our BERT-based model achieves an average F1-score of .46 across our\nproposed taxonomy, leaving much room for improvement.", "published": "2020-05-01 18:00:02", "link": "http://arxiv.org/abs/2005.00547v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset", "abstract": "Machine reading comprehension has made great progress in recent years owing\nto large-scale annotated datasets. In the clinical domain, however, creating\nsuch datasets is quite difficult due to the domain expertise required for\nannotation. Recently, Pampari et al. (EMNLP'18) tackled this issue by using\nexpert-annotated question templates and existing i2b2 annotations to create\nemrQA, the first large-scale dataset for question answering (QA) based on\nclinical notes. In this paper, we provide an in-depth analysis of this dataset\nand the clinical reading comprehension (CliniRC) task. From our qualitative\nanalysis, we find that (i) emrQA answers are often incomplete, and (ii) emrQA\nquestions are often answerable without using domain knowledge. From our\nquantitative experiments, surprising results include that (iii) using a small\nsampled subset (5%-20%), we can obtain roughly equal performance compared to\nthe model trained on the entire dataset, (iv) this performance is close to\nhuman expert's performance, and (v) BERT models do not beat the best performing\nbase model. Following our analysis of the emrQA, we further explore two desired\naspects of CliniRC systems: the ability to utilize clinical domain knowledge\nand to generalize to unseen questions and contexts. We argue that both should\nbe considered when creating future datasets.", "published": "2020-05-01 19:07:33", "link": "http://arxiv.org/abs/2005.00574v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Robustness to Input Perturbations for Neural Machine\n  Translation", "abstract": "Neural Machine Translation (NMT) models are sensitive to small perturbations\nin the input. Robustness to such perturbations is typically measured using\ntranslation quality metrics such as BLEU on the noisy input. This paper\nproposes additional metrics which measure the relative degradation and changes\nin translation when small perturbations are added to the input. We focus on a\nclass of models employing subword regularization to address robustness and\nperform extensive evaluations of these models using the robustness measures\nproposed. Results show that our proposed metrics reveal a clear trend of\nimproved robustness to perturbations when subword regularization methods are\nused.", "published": "2020-05-01 19:54:46", "link": "http://arxiv.org/abs/2005.00580v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Controllable Model of Grounded Response Generation", "abstract": "Current end-to-end neural conversation models inherently lack the flexibility\nto impose semantic control in the response generation process, often resulting\nin uninteresting responses. Attempts to boost informativeness alone come at the\nexpense of factual accuracy, as attested by pretrained language models'\npropensity to \"hallucinate\" facts. While this may be mitigated by access to\nbackground knowledge, there is scant guarantee of relevance and informativeness\nin generated responses. We propose a framework that we call controllable\ngrounded response generation (CGRG), in which lexical control phrases are\neither provided by a user or automatically extracted by a control phrase\npredictor from dialogue context and grounding knowledge. Quantitative and\nqualitative results show that, using this framework, a transformer based model\nwith a novel inductive attention mechanism, trained on a conversation-like\nReddit dataset, outperforms strong generation baselines.", "published": "2020-05-01 21:22:08", "link": "http://arxiv.org/abs/2005.00613v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Dimensional Gender Bias Classification", "abstract": "Machine learning models are trained to find patterns in data. NLP models can\ninadvertently learn socially undesirable patterns when training on gender\nbiased text. In this work, we propose a general framework that decomposes\ngender bias in text along several pragmatic and semantic dimensions: bias from\nthe gender of the person being spoken about, bias from the gender of the person\nbeing spoken to, and bias from the gender of the speaker. Using this\nfine-grained framework, we automatically annotate eight large scale datasets\nwith gender information. In addition, we collect a novel, crowdsourced\nevaluation benchmark of utterance-level gender rewrites. Distinguishing between\ngender bias along multiple dimensions is important, as it enables us to train\nfiner-grained gender bias classifiers. We show our classifiers prove valuable\nfor a variety of important applications, such as controlling for gender bias in\ngenerative models, detecting gender bias in arbitrary text, and shed light on\noffensive language in terms of genderedness.", "published": "2020-05-01 21:23:20", "link": "http://arxiv.org/abs/2005.00614v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Declension Class from Form and Meaning", "abstract": "The noun lexica of many natural languages are divided into several declension\nclasses with characteristic morphological properties. Class membership is far\nfrom deterministic, but the phonological form of a noun and/or its meaning can\noften provide imperfect clues. Here, we investigate the strength of those\nclues. More specifically, we operationalize this by measuring how much\ninformation, in bits, we can glean about declension class from knowing the form\nand/or meaning of nouns. We know that form and meaning are often also\nindicative of grammatical gender---which, as we quantitatively verify, can\nitself share information with declension class---so we also control for gender.\nWe find for two Indo-European languages (Czech and German) that form and\nmeaning respectively share significant amounts of information with class (and\ncontribute additional information above and beyond gender). The three-way\ninteraction between class, form, and meaning (given gender) is also\nsignificant. Our study is important for two reasons: First, we introduce a new\nmethod that provides additional quantitative support for a classic linguistic\nfinding that form and meaning are relevant for the classification of nouns into\ndeclensions. Secondly, we show not only that individual declensions classes\nvary in the strength of their clues within a language, but also that these\nvariations themselves vary across languages.", "published": "2020-05-01 21:48:48", "link": "http://arxiv.org/abs/2005.00626v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intermediate-Task Transfer Learning with Pretrained Models for Natural\n  Language Understanding: When and Why Does It Work?", "abstract": "While pretrained models such as BERT have shown large gains across natural\nlanguage understanding tasks, their performance can be improved by further\ntraining the model on a data-rich intermediate task, before fine-tuning it on a\ntarget task. However, it is still poorly understood when and why\nintermediate-task training is beneficial for a given target task. To\ninvestigate this, we perform a large-scale study on the pretrained RoBERTa\nmodel with 110 intermediate-target task combinations. We further evaluate all\ntrained models with 25 probing tasks meant to reveal the specific skills that\ndrive transfer. We observe that intermediate tasks requiring high-level\ninference and reasoning abilities tend to work best. We also observe that\ntarget task performance is strongly correlated with higher-level abilities such\nas coreference resolution. However, we fail to observe more granular\ncorrelations between probing and target task performance, highlighting the need\nfor further work on broad-coverage probing benchmarks. We also observe evidence\nthat the forgetting of knowledge learned during pretraining may limit our\nanalysis, highlighting the need for further work on transfer learning methods\nin these settings.", "published": "2020-05-01 21:49:34", "link": "http://arxiv.org/abs/2005.00628v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KLEJ: Comprehensive Benchmark for Polish Language Understanding", "abstract": "In recent years, a series of Transformer-based models unlocked major\nimprovements in general natural language understanding (NLU) tasks. Such a fast\npace of research would not be possible without general NLU benchmarks, which\nallow for a fair comparison of the proposed methods. However, such benchmarks\nare available only for a handful of languages. To alleviate this issue, we\nintroduce a comprehensive multi-task benchmark for the Polish language\nunderstanding, accompanied by an online leaderboard. It consists of a diverse\nset of tasks, adopted from existing datasets for named entity recognition,\nquestion-answering, textual entailment, and others. We also introduce a new\nsentiment analysis task for the e-commerce domain, named Allegro Reviews (AR).\nTo ensure a common evaluation scheme and promote models that generalize to\ndifferent NLU tasks, the benchmark includes datasets from varying domains and\napplications. Additionally, we release HerBERT, a Transformer-based model\ntrained specifically for the Polish language, which has the best average\nperformance and obtains the best results for three out of nine tasks. Finally,\nwe provide an extensive evaluation, including several standard baselines and\nrecently proposed, multilingual Transformer-based models.", "published": "2020-05-01 21:55:40", "link": "http://arxiv.org/abs/2005.00630v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Zero to Hero: On the Limitations of Zero-Shot Cross-Lingual\n  Transfer with Multilingual Transformers", "abstract": "Massively multilingual transformers pretrained with language modeling\nobjectives (e.g., mBERT, XLM-R) have become a de facto default transfer\nparadigm for zero-shot cross-lingual transfer in NLP, offering unmatched\ntransfer performance. Current downstream evaluations, however, verify their\nefficacy predominantly in transfer settings involving languages with sufficient\namounts of pretraining data, and with lexically and typologically close\nlanguages. In this work, we analyze their limitations and show that\ncross-lingual transfer via massively multilingual transformers, much like\ntransfer via cross-lingual word embeddings, is substantially less effective in\nresource-lean scenarios and for distant languages. Our experiments,\nencompassing three lower-level tasks (POS tagging, dependency parsing, NER), as\nwell as two high-level semantic tasks (NLI, QA), empirically correlate transfer\nperformance with linguistic similarity between the source and target languages,\nbut also with the size of pretraining corpora of target languages. We also\ndemonstrate a surprising effectiveness of inexpensive few-shot transfer (i.e.,\nfine-tuning on a few target-language instances after fine-tuning in the source)\nacross the board. This suggests that additional research efforts should be\ninvested to reach beyond the limiting zero-shot conditions.", "published": "2020-05-01 22:04:58", "link": "http://arxiv.org/abs/2005.00633v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Noisy Self-Reports to Predict Twitter User Demographics", "abstract": "Computational social science studies often contextualize content analysis\nwithin standard demographics. Since demographics are unavailable on many social\nmedia platforms (e.g. Twitter) numerous studies have inferred demographics\nautomatically. Despite many studies presenting proof of concept inference of\nrace and ethnicity, training of practical systems remains elusive since there\nare few annotated datasets. Existing datasets are small, inaccurate, or fail to\ncover the four most common racial and ethnic groups in the United States. We\npresent a method to identify self-reports of race and ethnicity from Twitter\nprofile descriptions. Despite errors inherent in automated supervision, we\nproduce models with good performance when measured on gold standard self-report\nsurvey data. The result is a reproducible method for creating large-scale\ntraining resources for race and ethnicity.", "published": "2020-05-01 22:10:35", "link": "http://arxiv.org/abs/2005.00635v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explainable Link Prediction for Emerging Entities in Knowledge Graphs", "abstract": "Despite their large-scale coverage, cross-domain knowledge graphs invariably\nsuffer from inherent incompleteness and sparsity. Link prediction can alleviate\nthis by inferring a target entity, given a source entity and a query relation.\nRecent embedding-based approaches operate in an uninterpretable latent semantic\nvector space of entities and relations, while path-based approaches operate in\nthe symbolic space, making the inference process explainable. However, these\napproaches typically consider static snapshots of the knowledge graphs,\nseverely restricting their applicability for evolving knowledge graphs with\nnewly emerging entities. To overcome this issue, we propose an inductive\nrepresentation learning framework that is able to learn representations of\npreviously unseen entities. Our method finds reasoning paths between source and\ntarget entities, thereby making the link prediction for unseen entities\ninterpretable and providing support evidence for the inferred link.", "published": "2020-05-01 22:17:37", "link": "http://arxiv.org/abs/2005.00637v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text and Causal Inference: A Review of Using Text to Remove Confounding\n  from Causal Estimates", "abstract": "Many applications of computational social science aim to infer causal\nconclusions from non-experimental data. Such observational data often contains\nconfounders, variables that influence both potential causes and potential\neffects. Unmeasured or latent confounders can bias causal estimates, and this\nhas motivated interest in measuring potential confounders from observed text.\nFor example, an individual's entire history of social media posts or the\ncontent of a news article could provide a rich measurement of multiple\nconfounders. Yet, methods and applications for this problem are scattered\nacross different communities and evaluation practices are inconsistent. This\nreview is the first to gather and categorize these examples and provide a guide\nto data-processing and evaluation decisions. Despite increased attention on\nadjusting for confounding using text, there are still many open problems, which\nwe highlight in this paper.", "published": "2020-05-01 23:20:49", "link": "http://arxiv.org/abs/2005.00649v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why and when should you pool? Analyzing Pooling in Recurrent\n  Architectures", "abstract": "Pooling-based recurrent neural architectures consistently outperform their\ncounterparts without pooling. However, the reasons for their enhanced\nperformance are largely unexamined. In this work, we examine three commonly\nused pooling techniques (mean-pooling, max-pooling, and attention), and propose\nmax-attention, a novel variant that effectively captures interactions among\npredictive tokens in a sentence. We find that pooling-based architectures\nsubstantially differ from their non-pooling equivalents in their learning\nability and positional biases--which elucidate their performance benefits. By\nanalyzing the gradient propagation, we discover that pooling facilitates better\ngradient flow compared to BiLSTMs. Further, we expose how BiLSTMs are\npositionally biased towards tokens in the beginning and the end of a sequence.\nPooling alleviates such biases. Consequently, we identify settings where\npooling offers large benefits: (i) in low resource scenarios, and (ii) when\nimportant words lie towards the middle of the sentence. Among the pooling\ntechniques studied, max-attention is the most effective, resulting in\nsignificant performance gains on several text classification tasks.", "published": "2020-05-01 00:47:37", "link": "http://arxiv.org/abs/2005.00159v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Recurrent Interaction Network for Jointly Extracting Entities and\n  Classifying Relations", "abstract": "The idea of using multi-task learning approaches to address the joint\nextraction of entity and relation is motivated by the relatedness between the\nentity recognition task and the relation classification task. Existing methods\nusing multi-task learning techniques to address the problem learn interactions\namong the two tasks through a shared network, where the shared information is\npassed into the task-specific networks for prediction. However, such an\napproach hinders the model from learning explicit interactions between the two\ntasks to improve the performance on the individual tasks. As a solution, we\ndesign a multi-task learning model which we refer to as recurrent interaction\nnetwork which allows the learning of interactions dynamically, to effectively\nmodel task-specific features for classification. Empirical studies on two\nreal-world datasets confirm the superiority of the proposed model.", "published": "2020-05-01 01:03:16", "link": "http://arxiv.org/abs/2005.00162v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Universal Adversarial Attacks with Natural Triggers for Text\n  Classification", "abstract": "Recent work has demonstrated the vulnerability of modern text classifiers to\nuniversal adversarial attacks, which are input-agnostic sequences of words\nadded to text processed by classifiers. Despite being successful, the word\nsequences produced in such attacks are often ungrammatical and can be easily\ndistinguished from natural text. We develop adversarial attacks that appear\ncloser to natural English phrases and yet confuse classification systems when\nadded to benign inputs. We leverage an adversarially regularized autoencoder\n(ARAE) to generate triggers and propose a gradient-based search that aims to\nmaximize the downstream classifier's prediction loss. Our attacks effectively\nreduce model accuracy on classification tasks while being less identifiable\nthan prior models as per automatic detection metrics and human-subject studies.\nOur aim is to demonstrate that adversarial attacks can be made harder to detect\nthan previously thought and to enable the development of appropriate defenses.", "published": "2020-05-01 01:58:24", "link": "http://arxiv.org/abs/2005.00174v2", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "TransOMCS: From Linguistic Graphs to Commonsense Knowledge", "abstract": "Commonsense knowledge acquisition is a key problem for artificial\nintelligence. Conventional methods of acquiring commonsense knowledge generally\nrequire laborious and costly human annotations, which are not feasible on a\nlarge scale. In this paper, we explore a practical way of mining commonsense\nknowledge from linguistic graphs, with the goal of transferring cheap knowledge\nobtained with linguistic patterns into expensive commonsense knowledge. The\nresult is a conversion of ASER [Zhang et al., 2020], a large-scale selectional\npreference knowledge resource, into TransOMCS, of the same representation as\nConceptNet [Liu and Singh, 2004] but two orders of magnitude larger.\nExperimental results demonstrate the transferability of linguistic knowledge to\ncommonsense knowledge and the effectiveness of the proposed approach in terms\nof quantity, novelty, and quality. TransOMCS is publicly available at:\nhttps://github.com/HKUST-KnowComp/TransOMCS.", "published": "2020-05-01 04:03:58", "link": "http://arxiv.org/abs/2005.00206v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Biomedical Entity Representations with Synonym Marginalization", "abstract": "Biomedical named entities often play important roles in many biomedical text\nmining tools. However, due to the incompleteness of provided synonyms and\nnumerous variations in their surface forms, normalization of biomedical\nentities is very challenging. In this paper, we focus on learning\nrepresentations of biomedical entities solely based on the synonyms of\nentities. To learn from the incomplete synonyms, we use a model-based candidate\nselection and maximize the marginal likelihood of the synonyms present in top\ncandidates. Our model-based candidates are iteratively updated to contain more\ndifficult negative samples as our model evolves. In this way, we avoid the\nexplicit pre-selection of negative samples from more than 400K candidates. On\nfour biomedical entity normalization datasets having three different entity\ntypes (disease, chemical, adverse reaction), our model BioSyn consistently\noutperforms previous state-of-the-art models almost reaching the upper bound on\neach dataset.", "published": "2020-05-01 06:20:36", "link": "http://arxiv.org/abs/2005.00239v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Transfer of Semantic Role Models from Verbal to Nominal\n  Domain", "abstract": "Semantic role labeling (SRL) is an NLP task involving the assignment of\npredicate arguments to types, called semantic roles. Though research on SRL has\nprimarily focused on verbal predicates and many resources available for SRL\nprovide annotations only for verbs, semantic relations are often triggered by\nother linguistic constructions, e.g., nominalizations. In this work, we\ninvestigate a transfer scenario where we assume role-annotated data for the\nsource verbal domain but only unlabeled data for the target nominal domain. Our\nkey assumption, enabling the transfer between the two domains, is that\nselectional preferences of a role (i.e., preferences or constraints on the\nadmissible arguments) do not strongly depend on whether the relation is\ntriggered by a verb or a noun. For example, the same set of arguments can fill\nthe Acquirer role for the verbal predicate `acquire' and its nominal form\n`acquisition'. We approach the transfer task from the variational autoencoding\nperspective. The labeler serves as an encoder (predicting role labels given a\nsentence), whereas selectional preferences are captured in the decoder\ncomponent (generating arguments for the predicting roles). Nominal roles are\nnot labeled in the training data, and the learning objective instead pushes the\nlabeler to assign roles predictive of the arguments. Sharing the decoder\nparameters across the domains encourages consistency between labels predicted\nfor both domains and facilitates the transfer. The method substantially\noutperforms baselines, such as unsupervised and `direct transfer' methods, on\nthe English CoNLL-2009 dataset.", "published": "2020-05-01 09:20:48", "link": "http://arxiv.org/abs/2005.00278v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hitachi at SemEval-2020 Task 12: Offensive Language Identification with\n  Noisy Labels using Statistical Sampling and Post-Processing", "abstract": "In this paper, we present our participation in SemEval-2020 Task-12 Subtask-A\n(English Language) which focuses on offensive language identification from\nnoisy labels. To this end, we developed a hybrid system with the BERT\nclassifier trained with tweets selected using Statistical Sampling Algorithm\n(SA) and Post-Processed (PP) using an offensive wordlist. Our developed system\nachieved 34 th position with Macro-averaged F1-score (Macro-F1) of 0.90913 over\nboth offensive and non-offensive classes. We further show comprehensive results\nand error analysis to assist future research in offensive language\nidentification with noisy labels.", "published": "2020-05-01 10:16:40", "link": "http://arxiv.org/abs/2005.00295v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language (Re)modelling: Towards Embodied Language Understanding", "abstract": "While natural language understanding (NLU) is advancing rapidly, today's\ntechnology differs from human-like language understanding in fundamental ways,\nnotably in its inferior efficiency, interpretability, and generalization. This\nwork proposes an approach to representation and learning based on the tenets of\nembodied cognitive linguistics (ECL). According to ECL, natural language is\ninherently executable (like programming languages), driven by mental simulation\nand metaphoric mappings over hierarchical compositions of structures and\nschemata learned through embodied interaction. This position paper argues that\nthe use of grounding by metaphoric inference and simulation will greatly\nbenefit NLU systems, and proposes a system architecture along with a roadmap\ntowards realizing this vision.", "published": "2020-05-01 10:57:02", "link": "http://arxiv.org/abs/2005.00311v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can Multilingual Language Models Transfer to an Unseen Dialect? A Case\n  Study on North African Arabizi", "abstract": "Building natural language processing systems for non standardized and low\nresource languages is a difficult challenge. The recent success of large-scale\nmultilingual pretrained language models provides new modeling tools to tackle\nthis. In this work, we study the ability of multilingual language models to\nprocess an unseen dialect. We take user generated North-African Arabic as our\ncase study, a resource-poor dialectal variety of Arabic with frequent\ncode-mixing with French and written in Arabizi, a non-standardized\ntransliteration of Arabic to Latin script. Focusing on two tasks,\npart-of-speech tagging and dependency parsing, we show in zero-shot and\nunsupervised adaptation scenarios that multilingual language models are able to\ntransfer to such an unseen dialect, specifically in two extreme cases: (i)\nacross scripts, using Modern Standard Arabic as a source language, and (ii)\nfrom a distantly related language, unseen during pretraining, namely Maltese.\nOur results constitute the first successful transfer experiments on this\ndialect, paving thus the way for the development of an NLP ecosystem for\nresource-scarce, non-standardized and highly variable vernacular languages.", "published": "2020-05-01 11:29:23", "link": "http://arxiv.org/abs/2005.00318v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MUSS: Multilingual Unsupervised Sentence Simplification by Mining\n  Paraphrases", "abstract": "Progress in sentence simplification has been hindered by a lack of labeled\nparallel simplification data, particularly in languages other than English. We\nintroduce MUSS, a Multilingual Unsupervised Sentence Simplification system that\ndoes not require labeled simplification data. MUSS uses a novel approach to\nsentence simplification that trains strong models using sentence-level\nparaphrase data instead of proper simplification data. These models leverage\nunsupervised pretraining and controllable generation mechanisms to flexibly\nadjust attributes such as length and lexical complexity at inference time. We\nfurther present a method to mine such paraphrase data in any language from\nCommon Crawl using semantic sentence embeddings, thus removing the need for\nlabeled data. We evaluate our approach on English, French, and Spanish\nsimplification benchmarks and closely match or outperform the previous best\nsupervised results, despite not using any labeled simplification data. We push\nthe state of the art further by incorporating labeled simplification data.", "published": "2020-05-01 12:54:30", "link": "http://arxiv.org/abs/2005.00352v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beneath the Tip of the Iceberg: Current Challenges and New Directions in\n  Sentiment Analysis Research", "abstract": "Sentiment analysis as a field has come a long way since it was first\nintroduced as a task nearly 20 years ago. It has widespread commercial\napplications in various domains like marketing, risk management, market\nresearch, and politics, to name a few. Given its saturation in specific\nsubtasks -- such as sentiment polarity classification -- and datasets, there is\nan underlying perception that this field has reached its maturity. In this\narticle, we discuss this perception by pointing out the shortcomings and\nunder-explored, yet key aspects of this field that are necessary to attain true\nsentiment understanding. We analyze the significant leaps responsible for its\ncurrent relevance. Further, we attempt to chart a possible course for this\nfield that covers many overlooked and unanswered questions.", "published": "2020-05-01 13:05:23", "link": "http://arxiv.org/abs/2005.00357v5", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Bipartite Flat-Graph Network for Nested Named Entity Recognition", "abstract": "In this paper, we propose a novel bipartite flat-graph network (BiFlaG) for\nnested named entity recognition (NER), which contains two subgraph modules: a\nflat NER module for outermost entities and a graph module for all the entities\nlocated in inner layers. Bidirectional LSTM (BiLSTM) and graph convolutional\nnetwork (GCN) are adopted to jointly learn flat entities and their inner\ndependencies. Different from previous models, which only consider the\nunidirectional delivery of information from innermost layers to outer ones (or\noutside-to-inside), our model effectively captures the bidirectional\ninteraction between them. We first use the entities recognized by the flat NER\nmodule to construct an entity graph, which is fed to the next graph module. The\nricher representation learned from graph module carries the dependencies of\ninner entities and can be exploited to improve outermost entity predictions.\nExperimental results on three standard nested NER datasets demonstrate that our\nBiFlaG outperforms previous state-of-the-art models.", "published": "2020-05-01 15:14:22", "link": "http://arxiv.org/abs/2005.00436v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Defense of Word-level Adversarial Attacks via Random Substitution\n  Encoding", "abstract": "The adversarial attacks against deep neural networks on computer vision tasks\nhave spawned many new technologies that help protect models from avoiding false\npredictions. Recently, word-level adversarial attacks on deep models of Natural\nLanguage Processing (NLP) tasks have also demonstrated strong power, e.g.,\nfooling a sentiment classification neural network to make wrong decisions.\nUnfortunately, few previous literatures have discussed the defense of such\nword-level synonym substitution based attacks since they are hard to be\nperceived and detected. In this paper, we shed light on this problem and\npropose a novel defense framework called Random Substitution Encoding (RSE),\nwhich introduces a random substitution encoder into the training process of\noriginal neural networks. Extensive experiments on text classification tasks\ndemonstrate the effectiveness of our framework on defense of word-level\nadversarial attacks, under various base and attack models.", "published": "2020-05-01 15:28:43", "link": "http://arxiv.org/abs/2005.00446v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "USR: An Unsupervised and Reference Free Evaluation Metric for Dialog\n  Generation", "abstract": "The lack of meaningful automatic evaluation metrics for dialog has impeded\nopen-domain dialog research. Standard language generation metrics have been\nshown to be ineffective for evaluating dialog models. To this end, this paper\npresents USR, an UnSupervised and Reference-free evaluation metric for dialog.\nUSR is a reference-free metric that trains unsupervised models to measure\nseveral desirable qualities of dialog. USR is shown to strongly correlate with\nhuman judgment on both Topical-Chat (turn-level: 0.42, system-level: 1.0) and\nPersonaChat (turn-level: 0.48 and system-level: 1.0). USR additionally produces\ninterpretable measures for several desirable properties of dialog.", "published": "2020-05-01 15:50:50", "link": "http://arxiv.org/abs/2005.00456v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic Discourse Segmentation: Review and Perspectives", "abstract": "Multilingual discourse parsing is a very prominent research topic. The first\nstage for discourse parsing is discourse segmentation. The study reported in\nthis article addresses a review of two on-line available discourse segmenters\n(for English and Portuguese). We evaluate the possibility of developing similar\ndiscourse segmenters for Spanish, French and African languages.", "published": "2020-05-01 16:03:37", "link": "http://arxiv.org/abs/2005.00468v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Regex Queries over Incomplete Knowledge Bases", "abstract": "We propose the novel task of answering regular expression queries (containing\ndisjunction ($\\vee$) and Kleene plus ($+$) operators) over incomplete KBs. The\nanswer set of these queries potentially has a large number of entities, hence\nprevious works for single-hop queries in KBC that model a query as a point in\nhigh-dimensional space are not as effective. In response, we develop RotatE-Box\n-- a novel combination of RotatE and box embeddings. It can model more\nrelational inference patterns compared to existing embedding based models.\nFurthermore, we define baseline approaches for embedding based KBC models to\nhandle regex operators. We demonstrate performance of RotatE-Box on two new\nregex-query datasets introduced in this paper, including one where the queries\nare harvested based on actual user query logs. We find that our final\nRotatE-Box model significantly outperforms models based on just RotatE and just\nbox embeddings.", "published": "2020-05-01 16:43:06", "link": "http://arxiv.org/abs/2005.00480v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Structured Tuning for Semantic Role Labeling", "abstract": "Recent neural network-driven semantic role labeling (SRL) systems have shown\nimpressive improvements in F1 scores. These improvements are due to expressive\ninput representations, which, at least at the surface, are orthogonal to\nknowledge-rich constrained decoding mechanisms that helped linear SRL models.\nIntroducing the benefits of structure to inform neural models presents a\nmethodological challenge. In this paper, we present a structured tuning\nframework to improve models using softened constraints only at training time.\nOur framework leverages the expressiveness of neural networks and provides\nsupervision with structured loss components. We start with a strong baseline\n(RoBERTa) to validate the impact of our approach, and show that our framework\noutperforms the baseline by learning to comply with declarative constraints.\nAdditionally, our experiments with smaller training sizes show that we can\nachieve consistent improvements under low-resource scenarios.", "published": "2020-05-01 17:12:20", "link": "http://arxiv.org/abs/2005.00496v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Why Overfitting Isn't Always Bad: Retrofitting Cross-Lingual Word\n  Embeddings to Dictionaries", "abstract": "Cross-lingual word embeddings (CLWE) are often evaluated on bilingual lexicon\ninduction (BLI). Recent CLWE methods use linear projections, which underfit the\ntraining dictionary, to generalize on BLI. However, underfitting can hinder\ngeneralization to other downstream tasks that rely on words from the training\ndictionary. We address this limitation by retrofitting CLWE to the training\ndictionary, which pulls training translation pairs closer in the embedding\nspace and overfits the training dictionary. This simple post-processing step\noften improves accuracy on two downstream tasks, despite lowering BLI test\naccuracy. We also retrofit to both the training dictionary and a synthetic\ndictionary induced from CLWE, which sometimes generalizes even better on\ndownstream tasks. Our results confirm the importance of fully exploiting\ntraining dictionary in downstream tasks and explains why BLI is a flawed CLWE\nevaluation.", "published": "2020-05-01 17:56:01", "link": "http://arxiv.org/abs/2005.00524v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "When BERT Plays the Lottery, All Tickets Are Winning", "abstract": "Large Transformer-based models were shown to be reducible to a smaller number\nof self-attention heads and layers. We consider this phenomenon from the\nperspective of the lottery ticket hypothesis, using both structured and\nmagnitude pruning. For fine-tuned BERT, we show that (a) it is possible to find\nsubnetworks achieving performance that is comparable with that of the full\nmodel, and (b) similarly-sized subnetworks sampled from the rest of the model\nperform worse. Strikingly, with structured pruning even the worst possible\nsubnetworks remain highly trainable, indicating that most pre-trained BERT\nweights are potentially useful. We also study the \"good\" subnetworks to see if\ntheir success can be attributed to superior linguistic knowledge, but find them\nunstable, and not explained by meaningful self-attention patterns.", "published": "2020-05-01 18:24:42", "link": "http://arxiv.org/abs/2005.00561v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Pre-training with Alignments for RNN Transducer based\n  End-to-End Speech Recognition", "abstract": "Recently, the recurrent neural network transducer (RNN-T) architecture has\nbecome an emerging trend in end-to-end automatic speech recognition research\ndue to its advantages of being capable for online streaming speech recognition.\nHowever, RNN-T training is made difficult by the huge memory requirements, and\ncomplicated neural structure. A common solution to ease the RNN-T training is\nto employ connectionist temporal classification (CTC) model along with RNN\nlanguage model (RNNLM) to initialize the RNN-T parameters. In this work, we\nconversely leverage external alignments to seed the RNN-T model. Two different\npre-training solutions are explored, referred to as encoder pre-training, and\nwhole-network pre-training respectively. Evaluated on Microsoft 65,000 hours\nanonymized production data with personally identifiable information removed,\nour proposed methods can obtain significant improvement. In particular, the\nencoder pre-training solution achieved a 10% and a 8% relative word error rate\nreduction when compared with random initialization and the widely used\nCTC+RNNLM initialization strategy, respectively. Our solutions also\nsignificantly reduce the RNN-T model latency from the baseline.", "published": "2020-05-01 19:00:57", "link": "http://arxiv.org/abs/2005.00572v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multi-scale Transformer Language Models", "abstract": "We investigate multi-scale transformer language models that learn\nrepresentations of text at multiple scales, and present three different\narchitectures that have an inductive bias to handle the hierarchical nature of\nlanguage. Experiments on large-scale language modeling benchmarks empirically\ndemonstrate favorable likelihood vs memory footprint trade-offs, e.g. we show\nthat it is possible to train a hierarchical variant with 30 layers that has 23%\nsmaller memory footprint and better perplexity, compared to a vanilla\ntransformer with less than half the number of layers, on the Toronto\nBookCorpus. We analyze the advantages of learned representations at multiple\nscales in terms of memory footprint, compute time, and perplexity, which are\nparticularly appealing given the quadratic scaling of transformers' run time\nand memory usage with respect to sequence length.", "published": "2020-05-01 19:58:56", "link": "http://arxiv.org/abs/2005.00581v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning an Unreferenced Metric for Online Dialogue Evaluation", "abstract": "Evaluating the quality of a dialogue interaction between two agents is a\ndifficult task, especially in open-domain chit-chat style dialogue. There have\nbeen recent efforts to develop automatic dialogue evaluation metrics, but most\nof them do not generalize to unseen datasets and/or need a human-generated\nreference response during inference, making it infeasible for online\nevaluation. Here, we propose an unreferenced automated evaluation metric that\nuses large pre-trained language models to extract latent representations of\nutterances, and leverages the temporal transitions that exist between them. We\nshow that our model achieves higher correlation with human annotations in an\nonline setting, while not requiring true responses for comparison during\ninference.", "published": "2020-05-01 20:01:39", "link": "http://arxiv.org/abs/2005.00583v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Probing Contextual Language Models for Common Ground with Visual\n  Representations", "abstract": "The success of large-scale contextual language models has attracted great\ninterest in probing what is encoded in their representations. In this work, we\nconsider a new question: to what extent contextual representations of concrete\nnouns are aligned with corresponding visual representations? We design a\nprobing model that evaluates how effective are text-only representations in\ndistinguishing between matching and non-matching visual representations. Our\nfindings show that language representations alone provide a strong signal for\nretrieving image patches from the correct object categories. Moreover, they are\neffective in retrieving specific instances of image patches; textual context\nplays an important role in this process. Visually grounded language models\nslightly outperform text-only language models in instance retrieval, but\ngreatly under-perform humans. We hope our analyses inspire future research in\nunderstanding and improving the visual capabilities of language models.", "published": "2020-05-01 21:28:28", "link": "http://arxiv.org/abs/2005.00619v5", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "We Need to Talk About Random Splits", "abstract": "Gorman and Bedrick (2019) argued for using random splits rather than standard\nsplits in NLP experiments. We argue that random splits, like standard splits,\nlead to overly optimistic performance estimates. We can also split data in\nbiased or adversarial ways, e.g., training on short sentences and evaluating on\nlong ones. Biased sampling has been used in domain adaptation to simulate\nreal-world drift; this is known as the covariate shift assumption. In NLP,\nhowever, even worst-case splits, maximizing bias, often under-estimate the\nerror observed on new samples of in-domain data, i.e., the data that models\nshould minimally generalize to at test time. This invalidates the covariate\nshift assumption. Instead of using multiple random splits, future benchmarks\nshould ideally include multiple, independent test sets instead; if infeasible,\nwe argue that multiple biased splits leads to more realistic performance\nestimates than multiple random splits.", "published": "2020-05-01 22:14:16", "link": "http://arxiv.org/abs/2005.00636v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Spatial Dependency Parsing for Semi-Structured Document Information\n  Extraction", "abstract": "Information Extraction (IE) for semi-structured document images is often\napproached as a sequence tagging problem by classifying each recognized input\ntoken into one of the IOB (Inside, Outside, and Beginning) categories. However,\nsuch problem setup has two inherent limitations that (1) it cannot easily\nhandle complex spatial relationships and (2) it is not suitable for highly\nstructured information, which are nevertheless frequently observed in\nreal-world document images. To tackle these issues, we first formulate the IE\ntask as spatial dependency parsing problem that focuses on the relationship\namong text tokens in the documents. Under this setup, we then propose SPADE\n(SPAtial DEpendency parser) that models highly complex spatial relationships\nand an arbitrary number of information layers in the documents in an end-to-end\nmanner. We evaluate it on various kinds of documents such as receipts, name\ncards, forms, and invoices, and show that it achieves a similar or better\nperformance compared to strong baselines including BERT-based IOB taggger.", "published": "2020-05-01 22:59:56", "link": "http://arxiv.org/abs/2005.00642v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Syntactic Question Abstraction and Retrieval for Data-Scarce Semantic\n  Parsing", "abstract": "Deep learning approaches to semantic parsing require a large amount of\nlabeled data, but annotating complex logical forms is costly. Here, we propose\nSyntactic Question Abstraction and Retrieval (SQAR), a method to build a neural\nsemantic parser that translates a natural language (NL) query to a SQL logical\nform (LF) with less than 1,000 annotated examples. SQAR first retrieves a\nlogical pattern from the train data by computing the similarity between NL\nqueries and then grounds a lexical information on the retrieved pattern in\norder to generate the final LF. We validate SQAR by training models using\nvarious small subsets of WikiSQL train data achieving up to 4.9% higher LF\naccuracy compared to the previous state-of-the-art models on WikiSQL test set.\nWe also show that by using query-similarity to retrieve logical pattern, SQAR\ncan leverage a paraphrasing dataset achieving up to 5.9% higher LF accuracy\ncompared to the case where SQAR is trained by using only WikiSQL data. In\ncontrast to a simple pattern classification approach, SQAR can generate unseen\nlogical patterns upon the addition of new examples without re-training the\nmodel. We also discuss an ideal way to create cost efficient and robust train\ndatasets when the data distribution can be approximated under a data-hungry\nsetting.", "published": "2020-05-01 23:05:55", "link": "http://arxiv.org/abs/2005.00644v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question\n  Answering", "abstract": "Existing work on augmenting question answering (QA) models with external\nknowledge (e.g., knowledge graphs) either struggle to model multi-hop relations\nefficiently, or lack transparency into the model's prediction rationale. In\nthis paper, we propose a novel knowledge-aware approach that equips pre-trained\nlanguage models (PTLMs) with a multi-hop relational reasoning module, named\nmulti-hop graph relation network (MHGRN). It performs multi-hop,\nmulti-relational reasoning over subgraphs extracted from external knowledge\ngraphs. The proposed reasoning module unifies path-based reasoning methods and\ngraph neural networks to achieve better interpretability and scalability. We\nalso empirically show its effectiveness and scalability on CommonsenseQA and\nOpenbookQA datasets, and interpret its behaviors with case studies.", "published": "2020-05-01 23:10:26", "link": "http://arxiv.org/abs/2005.00646v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Information Bottleneck Approach for Controlling Conciseness in\n  Rationale Extraction", "abstract": "Decisions of complex language understanding models can be rationalized by\nlimiting their inputs to a relevant subsequence of the original text. A\nrationale should be as concise as possible without significantly degrading task\nperformance, but this balance can be difficult to achieve in practice. In this\npaper, we show that it is possible to better manage this trade-off by\noptimizing a bound on the Information Bottleneck (IB) objective. Our fully\nunsupervised approach jointly learns an explainer that predicts sparse binary\nmasks over sentences, and an end-task predictor that considers only the\nextracted rationale. Using IB, we derive a learning objective that allows\ndirect control of mask sparsity levels through a tunable sparse prior.\nExperiments on ERASER benchmark tasks demonstrate significant gains over\nnorm-minimization techniques for both task performance and agreement with human\nrationales. Furthermore, we find that in the semi-supervised setting, a modest\namount of gold rationales (25% of training examples) closes the gap with a\nmodel that uses the full input.", "published": "2020-05-01 23:26:41", "link": "http://arxiv.org/abs/2005.00652v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Entity Summarization with Joint Encoding and Weak Supervision", "abstract": "In a large-scale knowledge graph (KG), an entity is often described by a\nlarge number of triple-structured facts. Many applications require abridged\nversions of entity descriptions, called entity summaries. Existing solutions to\nentity summarization are mainly unsupervised. In this paper, we present a\nsupervised approach NEST that is based on our novel neural model to jointly\nencode graph structure and text in KGs and generate high-quality diversified\nsummaries. Since it is costly to obtain manually labeled summaries for\ntraining, our supervision is weak as we train with programmatically labeled\ndata which may contain noise but is free of manual work. Evaluation results\nshow that our approach significantly outperforms the state of the art on two\npublic benchmarks.", "published": "2020-05-01 00:14:08", "link": "http://arxiv.org/abs/2005.00152v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Entity Alignment with Incidental Supervision", "abstract": "Much research effort has been put to multilingual knowledge graph (KG)\nembedding methods to address the entity alignment task, which seeks to match\nentities in different languagespecific KGs that refer to the same real-world\nobject. Such methods are often hindered by the insufficiency of seed alignment\nprovided between KGs. Therefore, we propose an incidentally supervised model,\nJEANS , which jointly represents multilingual KGs and text corpora in a shared\nembedding scheme, and seeks to improve entity alignment with incidental\nsupervision signals from text. JEANS first deploys an entity grounding process\nto combine each KG with the monolingual text corpus. Then, two learning\nprocesses are conducted: (i) an embedding learning process to encode the KG and\ntext of each language in one embedding space, and (ii) a selflearning based\nalignment learning process to iteratively induce the matching of entities and\nthat of lexemes between embeddings. Experiments on benchmark datasets show that\nJEANS leads to promising improvement on entity alignment with incidental\nsupervision, and significantly outperforms state-of-the-art methods that solely\nrely on internal information of KGs.", "published": "2020-05-01 01:53:56", "link": "http://arxiv.org/abs/2005.00171v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "HERO: Hierarchical Encoder for Video+Language Omni-representation\n  Pre-training", "abstract": "We present HERO, a novel framework for large-scale video+language\nomni-representation learning. HERO encodes multimodal inputs in a hierarchical\nstructure, where local context of a video frame is captured by a Cross-modal\nTransformer via multimodal fusion, and global video context is captured by a\nTemporal Transformer. In addition to standard Masked Language Modeling (MLM)\nand Masked Frame Modeling (MFM) objectives, we design two new pre-training\ntasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global\nand local temporal alignment; and (ii) Frame Order Modeling (FOM), where the\nmodel predicts the right order of shuffled video frames. HERO is jointly\ntrained on HowTo100M and large-scale TV datasets to gain deep understanding of\ncomplex social dynamics with multi-character interactions. Comprehensive\nexperiments demonstrate that HERO achieves new state of the art on multiple\nbenchmarks over Text-based Video/Video-moment Retrieval, Video Question\nAnswering (QA), Video-and-language Inference and Video Captioning tasks across\ndifferent domains. We also introduce two new challenging benchmarks How2QA and\nHow2R for Video QA and Retrieval, collected from diverse video content over\nmultimodalities.", "published": "2020-05-01 03:49:26", "link": "http://arxiv.org/abs/2005.00200v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Multi-head Monotonic Chunkwise Attention For Online Speech Recognition", "abstract": "The attention mechanism of the Listen, Attend and Spell (LAS) model requires\nthe whole input sequence to calculate the attention context and thus is not\nsuitable for online speech recognition. To deal with this problem, we propose\nmulti-head monotonic chunk-wise attention (MTH-MoChA), an improved version of\nMoChA. MTH-MoChA splits the input sequence into small chunks and computes\nmulti-head attentions over the chunks. We also explore useful training\nstrategies such as LSTM pooling, minimum world error rate training and\nSpecAugment to further improve the performance of MTH-MoChA. Experiments on\nAISHELL-1 data show that the proposed model, along with the training\nstrategies, improve the character error rate (CER) of MoChA from 8.96% to 7.68%\non test set. On another 18000 hours in-car speech data set, MTH-MoChA obtains\n7.28% CER, which is significantly better than a state-of-the-art hybrid system.", "published": "2020-05-01 04:00:51", "link": "http://arxiv.org/abs/2005.00205v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Cross-modal Language Generation using Pivot Stabilization for Web-scale\n  Language Coverage", "abstract": "Cross-modal language generation tasks such as image captioning are directly\nhurt in their ability to support non-English languages by the trend of\ndata-hungry models combined with the lack of non-English annotations. We\ninvestigate potential solutions for combining existing language-generation\nannotations in English with translation capabilities in order to create\nsolutions at web-scale in both domain and language coverage. We describe an\napproach called Pivot-Language Generation Stabilization (PLuGS), which\nleverages directly at training time both existing English annotations (gold\ndata) as well as their machine-translated versions (silver data); at run-time,\nit generates first an English caption and then a corresponding target-language\ncaption. We show that PLuGS models outperform other candidate solutions in\nevaluations performed over 5 different target languages, under a large-domain\ntestset using images from the Open Images dataset. Furthermore, we find an\ninteresting effect where the English captions generated by the PLuGS models are\nbetter than the captions generated by the original, monolingual English model.", "published": "2020-05-01 06:58:18", "link": "http://arxiv.org/abs/2005.00246v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-supervised Knowledge Triplet Learning for Zero-shot Question\n  Answering", "abstract": "The aim of all Question Answering (QA) systems is to be able to generalize to\nunseen questions. Current supervised methods are reliant on expensive data\nannotation. Moreover, such annotations can introduce unintended annotator bias\nwhich makes systems focus more on the bias than the actual task. In this work,\nwe propose Knowledge Triplet Learning (KTL), a self-supervised task over\nknowledge graphs. We propose heuristics to create synthetic graphs for\ncommonsense and scientific knowledge. We propose methods of how to use KTL to\nperform zero-shot QA and our experiments show considerable improvements over\nlarge pre-trained transformer models.", "published": "2020-05-01 11:24:18", "link": "http://arxiv.org/abs/2005.00316v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CDL: Curriculum Dual Learning for Emotion-Controllable Response\n  Generation", "abstract": "Emotion-controllable response generation is an attractive and valuable task\nthat aims to make open-domain conversations more empathetic and engaging.\nExisting methods mainly enhance the emotion expression by adding regularization\nterms to standard cross-entropy loss and thus influence the training process.\nHowever, due to the lack of further consideration of content consistency, the\ncommon problem of response generation tasks, safe response, is intensified.\nBesides, query emotions that can help model the relationship between query and\nresponse are simply ignored in previous models, which would further hurt the\ncoherence. To alleviate these problems, we propose a novel framework named\nCurriculum Dual Learning (CDL) which extends the emotion-controllable response\ngeneration to a dual task to generate emotional responses and emotional queries\nalternatively. CDL utilizes two rewards focusing on emotion and content to\nimprove the duality. Additionally, it applies curriculum learning to gradually\ngenerate high-quality responses based on the difficulties of expressing various\nemotions. Experimental results show that CDL significantly outperforms the\nbaselines in terms of coherence, diversity, and relation to emotion factors.", "published": "2020-05-01 12:16:44", "link": "http://arxiv.org/abs/2005.00329v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Visuo-Linguistic Question Answering (VLQA) Challenge", "abstract": "Understanding images and text together is an important aspect of cognition\nand building advanced Artificial Intelligence (AI) systems. As a community, we\nhave achieved good benchmarks over language and vision domains separately,\nhowever joint reasoning is still a challenge for state-of-the-art computer\nvision and natural language processing (NLP) systems. We propose a novel task\nto derive joint inference about a given image-text modality and compile the\nVisuo-Linguistic Question Answering (VLQA) challenge corpus in a question\nanswering setting. Each dataset item consists of an image and a reading\npassage, where questions are designed to combine both visual and textual\ninformation i.e., ignoring either modality would make the question\nunanswerable. We first explore the best existing vision-language architectures\nto solve VLQA subsets and show that they are unable to reason well. We then\ndevelop a modular method with slightly better baseline performance, but it is\nstill far behind human performance. We believe that VLQA will be a good\nbenchmark for reasoning over a visuo-linguistic context. The dataset, code and\nleaderboard is available at https://shailaja183.github.io/vlqa/.", "published": "2020-05-01 12:18:55", "link": "http://arxiv.org/abs/2005.00330v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "HLVU : A New Challenge to Test Deep Understanding of Movies the Way\n  Humans do", "abstract": "In this paper we propose a new evaluation challenge and direction in the area\nof High-level Video Understanding. The challenge we are proposing is designed\nto test automatic video analysis and understanding, and how accurately systems\ncan comprehend a movie in terms of actors, entities, events and their\nrelationship to each other. A pilot High-Level Video Understanding (HLVU)\ndataset of open source movies were collected for human assessors to build a\nknowledge graph representing each of them. A set of queries will be derived\nfrom the knowledge graph to test systems on retrieving relationships among\nactors, as well as reasoning and retrieving non-visual concepts. The objective\nis to benchmark if a computer system can \"understand\" non-explicit but obvious\nrelationships the same way humans do when they watch the same movies. This is\nlong-standing problem that is being addressed in the text domain and this\nproject moves similar research to the video domain. Work of this nature is\nfoundational to future video analytics and video understanding technologies.\nThis work can be of interest to streaming services and broadcasters hoping to\nprovide more intuitive ways for their customers to interact with and consume\nvideo content.", "published": "2020-05-01 15:58:13", "link": "http://arxiv.org/abs/2005.00463v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Partially-Typed NER Datasets Integration: Connecting Practice to Theory", "abstract": "While typical named entity recognition (NER) models require the training set\nto be annotated with all target types, each available datasets may only cover a\npart of them. Instead of relying on fully-typed NER datasets, many efforts have\nbeen made to leverage multiple partially-typed ones for training and allow the\nresulting model to cover a full type set. However, there is neither guarantee\non the quality of integrated datasets, nor guidance on the design of training\nalgorithms. Here, we conduct a systematic analysis and comparison between\npartially-typed NER datasets and fully-typed ones, in both theoretical and\nempirical manner. Firstly, we derive a bound to establish that models trained\nwith partially-typed annotations can reach a similar performance with the ones\ntrained with fully-typed annotations, which also provides guidance on the\nalgorithm design. Moreover, we conduct controlled experiments, which shows\npartially-typed datasets leads to similar performance with the model trained\nwith the same amount of fully-typed annotations", "published": "2020-05-01 17:16:18", "link": "http://arxiv.org/abs/2005.00502v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "SciREX: A Challenge Dataset for Document-Level Information Extraction", "abstract": "Extracting information from full documents is an important problem in many\ndomains, but most previous work focus on identifying relationships within a\nsentence or a paragraph. It is challenging to create a large-scale information\nextraction (IE) dataset at the document level since it requires an\nunderstanding of the whole document to annotate entities and their\ndocument-level relationships that usually span beyond sentences or even\nsections. In this paper, we introduce SciREX, a document level IE dataset that\nencompasses multiple IE tasks, including salient entity identification and\ndocument level $N$-ary relation identification from scientific articles. We\nannotate our dataset by integrating automatic and human annotations, leveraging\nexisting scientific knowledge resources. We develop a neural model as a strong\nbaseline that extends previous state-of-the-art IE models to document-level IE.\nAnalyzing the model performance shows a significant gap between human\nperformance and current baselines, inviting the community to use our dataset as\na challenge to develop document-level IE models. Our data and code are publicly\navailable at https://github.com/allenai/SciREX", "published": "2020-05-01 17:30:10", "link": "http://arxiv.org/abs/2005.00512v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Low-Dimensional Hyperbolic Knowledge Graph Embeddings", "abstract": "Knowledge graph (KG) embeddings learn low-dimensional representations of\nentities and relations to predict missing facts. KGs often exhibit hierarchical\nand logical patterns which must be preserved in the embedding space. For\nhierarchical data, hyperbolic embedding methods have shown promise for\nhigh-fidelity and parsimonious representations. However, existing hyperbolic\nembedding methods do not account for the rich logical patterns in KGs. In this\nwork, we introduce a class of hyperbolic KG embedding models that\nsimultaneously capture hierarchical and logical patterns. Our approach combines\nhyperbolic reflections and rotations with attention to model complex relational\npatterns. Experimental results on standard KG benchmarks show that our method\nimproves over previous Euclidean- and hyperbolic-based efforts by up to 6.1% in\nmean reciprocal rank (MRR) in low dimensions. Furthermore, we observe that\ndifferent geometric transformations capture different types of relations while\nattention-based transformations generalize to multiple relations. In high\ndimensions, our approach yields new state-of-the-art MRRs of 49.6% on WN18RR\nand 57.7% on YAGO3-10.", "published": "2020-05-01 18:00:02", "link": "http://arxiv.org/abs/2005.00545v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "POINTER: Constrained Progressive Text Generation via Insertion-based\n  Generative Pre-training", "abstract": "Large-scale pre-trained language models, such as BERT and GPT-2, have\nachieved excellent performance in language representation learning and\nfree-form text generation. However, these models cannot be directly employed to\ngenerate text under specified lexical constraints. To address this challenge,\nwe present POINTER (PrOgressive INsertion-based TransformER), a simple yet\nnovel insertion-based approach for hard-constrained text generation. The\nproposed method operates by progressively inserting new tokens between existing\ntokens in a parallel manner. This procedure is recursively applied until a\nsequence is completed. The resulting coarse-to-fine hierarchy makes the\ngeneration process intuitive and interpretable. We pre-train our model with the\nproposed progressive insertion-based objective on a 12GB Wikipedia dataset, and\nfine-tune it on downstream hard-constrained generation tasks.\nNon-autoregressive decoding yields an empirically logarithmic time complexity\nduring inference time. Experimental results on both News and Yelp datasets\ndemonstrate that POINTER achieves state-of-the-art performance on constrained\ntext generation. We released the pre-trained models and the source code to\nfacilitate future research (https://github.com/dreasysnail/POINTER).", "published": "2020-05-01 18:11:54", "link": "http://arxiv.org/abs/2005.00558v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Collaborative Agents with Rule Guidance for Knowledge Graph\n  Reasoning", "abstract": "Walk-based models have shown their advantages in knowledge graph (KG)\nreasoning by achieving decent performance while providing interpretable\ndecisions. However, the sparse reward signals offered by the KG during\ntraversal are often insufficient to guide a sophisticated walk-based\nreinforcement learning (RL) model. An alternate approach is to use traditional\nsymbolic methods (e.g., rule induction), which achieve good performance but can\nbe hard to generalize due to the limitation of symbolic representation. In this\npaper, we propose RuleGuider, which leverages high-quality rules generated by\nsymbolic-based methods to provide reward supervision for walk-based agents.\nExperiments on benchmark datasets show that RuleGuider improves the performance\nof walk-based models without losing interpretability.", "published": "2020-05-01 18:57:14", "link": "http://arxiv.org/abs/2005.00571v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Minimally Supervised Categorization of Text with Metadata", "abstract": "Document categorization, which aims to assign a topic label to each document,\nplays a fundamental role in a wide variety of applications. Despite the success\nof existing studies in conventional supervised document classification, they\nare less concerned with two real problems: (1) the presence of metadata: in\nmany domains, text is accompanied by various additional information such as\nauthors and tags. Such metadata serve as compelling topic indicators and should\nbe leveraged into the categorization framework; (2) label scarcity: labeled\ntraining samples are expensive to obtain in some cases, where categorization\nneeds to be performed using only a small set of annotated data. In recognition\nof these two challenges, we propose MetaCat, a minimally supervised framework\nto categorize text with metadata. Specifically, we develop a generative process\ndescribing the relationships between words, documents, labels, and metadata.\nGuided by the generative model, we embed text and metadata into the same\nsemantic space to encode heterogeneous signals. Then, based on the same\ngenerative process, we synthesize training samples to address the bottleneck of\nlabel scarcity. We conduct a thorough evaluation on a wide range of datasets.\nExperimental results prove the effectiveness of MetaCat over many competitive\nbaselines.", "published": "2020-05-01 21:42:32", "link": "http://arxiv.org/abs/2005.00624v3", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-episodic Perceived Quality of an Audio-on-Demand Service", "abstract": "QoE is traditionally evaluated by using short stimuli usually representing\nparts or single usage episodes. This opens the question on how the overall\nservice perception involving multiple} usage episodes can be evaluated---a\nquestion of high practical relevance to service operators. Despite initial\nresearch on this challenging aspect of multi-episodic perceived quality, the\nquestion of the underlying quality formation processes and its factors are\nstill to be discovered. We present a multi-episodic experiment of an Audio on\nDemand service over a usage period of 6~days with 93 participants. Our work\ndirectly extends prior work investigating the impact of time between usage\nepisodes. The results show similar effects---also the recency effect is not\nstatistically significant. In addition, we extend prediction of multi-episodic\njudgments by accounting for the observed saturation.", "published": "2020-05-01 14:28:27", "link": "http://arxiv.org/abs/2005.00400v1", "categories": ["cs.HC", "cs.NI", "cs.SD", "eess.AS", "H.5.1; H.5.5; C.2.m"], "primary_category": "cs.HC"}
