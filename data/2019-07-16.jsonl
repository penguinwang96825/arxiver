{"title": "Language comparison via network topology", "abstract": "Modeling relations between languages can offer understanding of language\ncharacteristics and uncover similarities and differences between languages.\nAutomated methods applied to large textual corpora can be seen as opportunities\nfor novel statistical studies of language development over time, as well as for\nimproving cross-lingual natural language processing techniques. In this work,\nwe first propose how to represent textual data as a directed, weighted network\nby the text2net algorithm. We next explore how various fast,\nnetwork-topological metrics, such as network community structure, can be used\nfor cross-lingual comparisons. In our experiments, we employ eight different\nnetwork topology metrics, and empirically showcase on a parallel corpus, how\nthe methods can be used for modeling the relations between nine selected\nlanguages. We demonstrate that the proposed method scales to large corpora\nconsisting of hundreds of thousands of aligned sentences on an of-the-shelf\nlaptop. We observe that on the one hand properties such as communities, capture\nsome of the known differences between the languages, while others can be seen\nas novel opportunities for linguistic studies.", "published": "2019-07-16 11:33:04", "link": "http://arxiv.org/abs/1907.06944v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RadioTalk: a large-scale corpus of talk radio transcripts", "abstract": "We introduce RadioTalk, a corpus of speech recognition transcripts sampled\nfrom talk radio broadcasts in the United States between October of 2018 and\nMarch of 2019. The corpus is intended for use by researchers in the fields of\nnatural language processing, conversational analysis, and the social sciences.\nThe corpus encompasses approximately 2.8 billion words of automatically\ntranscribed speech from 284,000 hours of radio, together with metadata about\nthe speech, such as geographical location, speaker turn boundaries, gender, and\nradio program information. In this paper we summarize why and how we prepared\nthe corpus, give some descriptive statistics on stations, shows and speakers,\nand carry out a few high-level analyses.", "published": "2019-07-16 15:34:58", "link": "http://arxiv.org/abs/1907.07073v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "You Write Like You Eat: Stylistic variation as a predictor of social\n  stratification", "abstract": "Inspired by Labov's seminal work on stylistic variation as a function of\nsocial stratification, we develop and compare neural models that predict a\nperson's presumed socio-economic status, obtained through distant\nsupervision,from their writing style on social media. The focus of our work is\non identifying the most important stylistic parameters to predict\nsocio-economic group. In particular, we show the effectiveness of\nmorpho-syntactic features as stylistic predictors of socio-economic group,in\ncontrast to lexical features, which are good predictors of topic.", "published": "2019-07-16 21:20:49", "link": "http://arxiv.org/abs/1907.07265v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Modeling competitive evolution of multiple languages", "abstract": "Increasing evidence demonstrates that in many places language coexistence has\nbecome ubiquitous and essential for supporting language and cultural diversity\nand associated with its financial and economic benefits. The competitive\nevolution among multiple languages determines the evolution outcome, either\ncoexistence, decline, or extinction. Here, we extend the Abrams-Strogatz model\nof language competition to multiple languages and then validate it by analyzing\nthe behavioral transitions of language usage over the recent several decades in\nSingapore and Hong Kong. In each case, we estimate from data the model\nparameters that measure each language utility for its speakers and the strength\nof two biases, the majority preference for their language, and the minority\naversion to it. The values of these two biases decide which language is the\nfastest growing in the competition and what would be the stable state of the\nsystem. We also study the system convergence time to stable states and discover\nthe existence of tipping points with multiple attractors. Moreover, the\ncritical slowdown of convergence to the stable fractions of language users\nappears near and peaks at the tipping points, signaling when the system\napproaches them. Our analysis furthers our understanding of multiple language\nevolution and the role of tipping points in behavioral transitions. These\ninsights may help to protect languages from extinction and retain the language\nand cultural diversity.", "published": "2019-07-16 05:31:35", "link": "http://arxiv.org/abs/1907.06848v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "A generic rule-based system for clinical trial patient selection", "abstract": "The n2c2 2018 Challenge task 1 aimed to identify patients who meet lists of\nheterogeneous inclusion/exclusion criteria for a hypothetical clinical trial.\nWe demonstrate a generic rule-based natural language pipeline can support this\ntask with decent performance (the average F1 score on the test set is 0.89,\nranked the 8th out of 45 teams ).", "published": "2019-07-16 06:43:34", "link": "http://arxiv.org/abs/1907.06860v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Abstract categorial grammars with island constraints and effective\n  decidability", "abstract": "A well-known approach to treating syntactic island constraints in the setting\nof Lambek grammars consists in adding specific bracket modalities to the logic.\nWe adapt this approach to abstract categorial grammars (ACG). Thus we define\nbracketed (implicational) linear logic, bracketed lambda-calculus, and,\neventually, bracketed ACG based on bracketed $\\lambda$-calculus. This allows us\nmodeling at least simplest island constraints, typically, in the context of\nrelativization. Next we identify specific safely bracketed ACG which, just like\nordinary (bracket-free) second order ACG generate effectively decidable\nlanguages, but are sufficiently flexible to model some higher order phenomena\nlike relativization and correctly deal with syntactic islands, at least in\nsimple toy examples.", "published": "2019-07-16 11:52:46", "link": "http://arxiv.org/abs/1907.06950v2", "categories": ["math.LO", "cs.CL"], "primary_category": "math.LO"}
{"title": "Neural Language Model Based Training Data Augmentation for Weakly\n  Supervised Early Rumor Detection", "abstract": "The scarcity and class imbalance of training data are known issues in current\nrumor detection tasks. We propose a straight-forward and general-purpose data\naugmentation technique which is beneficial to early rumor detection relying on\nevent propagation patterns. The key idea is to exploit massive unlabeled event\ndata sets on social media to augment limited labeled rumor source tweets. This\nwork is based on rumor spreading patterns revealed by recent rumor studies and\nsemantic relatedness between labeled and unlabeled data. A state-of-the-art\nneural language model (NLM) and large credibility-focused Twitter corpora are\nemployed to learn context-sensitive representations of rumor tweets. Six\ndifferent real-world events based on three publicly available rumor datasets\nare employed in our experiments to provide a comparative evaluation of the\neffectiveness of the method. The results show that our method can expand the\nsize of an existing rumor data set nearly by 200% and corresponding social\ncontext (i.e., conversational threads) by 100% with reasonable quality.\nPreliminary experiments with a state-of-the-art deep learning-based rumor\ndetection model show that augmented data can alleviate over-fitting and class\nimbalance caused by limited train data and can help to train complex neural\nnetworks (NNs). With augmented data, the performance of rumor detection can be\nimproved by 12.1% in terms of F-score. Our experiments also indicate that\naugmented training data can help to generalize rumor detection models on unseen\nrumors.", "published": "2019-07-16 14:32:33", "link": "http://arxiv.org/abs/1907.07033v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MedCATTrainer: A Biomedical Free Text Annotation Interface with Active\n  Learning and Research Use Case Specific Customisation", "abstract": "We present MedCATTrainer an interface for building, improving and customising\na given Named Entity Recognition and Linking (NER+L) model for biomedical\ndomain text. NER+L is often used as a first step in deriving value from\nclinical text. Collecting labelled data for training models is difficult due to\nthe need for specialist domain knowledge. MedCATTrainer offers an interactive\nweb-interface to inspect and improve recognised entities from an underlying\nNER+L model via active learning. Secondary use of data for clinical research\noften has task and context specific criteria. MedCATTrainer provides a further\ninterface to define and collect supervised learning training data for\nresearcher specific use cases. Initial results suggest our approach allows for\nefficient and accurate collection of research use case specific training data.", "published": "2019-07-16 15:32:04", "link": "http://arxiv.org/abs/1907.07322v1", "categories": ["cs.HC", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "STRASS: A Light and Effective Method for Extractive Summarization Based\n  on Sentence Embeddings", "abstract": "This paper introduces STRASS: Summarization by TRAnsformation Selection and\nScoring. It is an extractive text summarization method which leverages the\nsemantic information in existing sentence embedding spaces. Our method creates\nan extractive summary by selecting the sentences with the closest embeddings to\nthe document embedding. The model learns a transformation of the document\nembedding to minimize the similarity between the extractive summary and the\nground truth summary. As the transformation is only composed of a dense layer,\nthe training can be done on CPU, therefore, inexpensive. Moreover, inference\ntime is short and linear according to the number of sentences. As a second\ncontribution, we introduce the French CASS dataset, composed of judgments from\nthe French Court of cassation and their corresponding summaries. On this\ndataset, our results show that our method performs similarly to the state of\nthe art extractive methods with effective training and inferring time.", "published": "2019-07-16 16:14:09", "link": "http://arxiv.org/abs/1907.07323v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AP19-OLR Challenge: Three Tasks and Their Baselines", "abstract": "This paper introduces the fourth oriental language recognition (OLR)\nchallenge AP19-OLR, including the data profile, the tasks and the evaluation\nprinciples. The OLR challenge has been held successfully for three consecutive\nyears, along with APSIPA Annual Summit and Conference (APSIPA ASC). The\nchallenge this year still focuses on practical and challenging tasks, precisely\n(1) short-utterance LID, (2) cross-channel LID and (3) zero-resource LID. The\nevent this year includes more languages and more real-life data provided by\nSpeechOcean and the NSFC M2ASR project. All the data is free for participants.\nRecipes for x-vector system and back-end evaluation are also conducted as\nbaselines for the three tasks. The participants can refer to these\nonline-published recipes to deploy LID systems for convenience. We report the\nbaseline results on the three tasks and demonstrate that the three tasks are\nworth some efforts to achieve better performance.", "published": "2019-07-16 03:55:21", "link": "http://arxiv.org/abs/1907.07626v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Conversational Help for Task Completion and Feature Discovery in\n  Personal Assistants", "abstract": "Intelligent Personal Assistants (IPAs) have become widely popular in recent\ntimes. Most of the commercial IPAs today support a wide range of skills\nincluding Alarms, Reminders, Weather Updates, Music, News, Factual\nQuestioning-Answering, etc. The list grows every day, making it difficult to\nremember the command structures needed to execute various tasks. An IPA must\nhave the ability to communicate information about supported skills and direct\nusers towards the right commands needed to execute them. Users interact with\npersonal assistants in natural language. A query is defined to be a Help Query\nif it seeks information about a personal assistant's capabilities, or asks for\ninstructions to execute a task. In this paper, we propose an interactive system\nwhich identifies help queries and retrieves appropriate responses. Our system\ncomprises of a C-BiLSTM based classifier, which is a fusion of Convolutional\nNeural Networks (CNN) and Bidirectional LSTM (BiLSTM) architectures, to detect\nhelp queries and a semantic Approximate Nearest Neighbours (ANN) module to map\nthe query to an appropriate predefined response. Evaluation of our system on\nreal-world queries from a commercial IPA and a detailed comparison with popular\ntraditional machine learning and deep learning based models reveal that our\nsystem outperforms other approaches and returns relevant responses for help\nqueries.", "published": "2019-07-16 09:25:13", "link": "http://arxiv.org/abs/1907.07564v1", "categories": ["cs.HC", "cs.CL", "cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.HC"}
{"title": "Towards Adapting NMF Dictionaries Using Total Variability Modeling for\n  Noise-Robust Acoustic Features", "abstract": "We propose an algorithm to extract noise-robust acoustic features from noisy\nspeech. We use Total Variability Modeling in combination with Non-negative\nMatrix Factorization (NMF) to learn a total variability subspace and adapt NMF\ndictionaries for each utterance. Unlike several other approaches for extracting\nnoise-robust features, our algorithm does not require a training corpus of\nparallel clean and noisy speech. Furthermore, the proposed features are\nproduced by an utterance-specific transform, allowing the features to be robust\nto the noise occurring in each utterance. Preliminary results on the Aurora 4 +\nDEMAND noise corpus show that our proposed features perform comparably to\nbaseline acoustic features, including features calculated from a convolutive\nNMF (CNMF) model. Moreover, on unseen noises, our proposed features gives the\nmost similar word error rate to clean speech compared to the baseline features.", "published": "2019-07-16 06:30:29", "link": "http://arxiv.org/abs/1907.06859v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
