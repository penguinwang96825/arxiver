{"title": "Tapping the Potential of Coherence and Syntactic Features in Neural\n  Models for Automatic Essay Scoring", "abstract": "In the prompt-specific holistic score prediction task for Automatic Essay\nScoring, the general approaches include pre-trained neural model, coherence\nmodel, and hybrid model that incorporate syntactic features with neural model.\nIn this paper, we propose a novel approach to extract and represent essay\ncoherence features with prompt-learning NSP that shows to match the\nstate-of-the-art AES coherence model, and achieves the best performance for\nlong essays. We apply syntactic feature dense embedding to augment BERT-based\nmodel and achieve the best performance for hybrid methodology for AES. In\naddition, we explore various ideas to combine coherence, syntactic information\nand semantic embeddings, which no previous study has done before. Our combined\nmodel also performs better than the SOTA available for combined model, even\nthough it does not outperform our syntactic enhanced neural model. We further\noffer analyses that can be useful for future study.", "published": "2022-11-24 02:00:03", "link": "http://arxiv.org/abs/2211.13373v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InDEX: Indonesian Idiom and Expression Dataset for Cloze Test", "abstract": "We propose InDEX, an Indonesian Idiom and Expression dataset for cloze test.\nThe dataset contains 10438 unique sentences for 289 idioms and expressions for\nwhich we generate 15 different types of distractors, resulting in a large\ncloze-style corpus. Many baseline models of cloze test reading comprehension\napply BERT with random initialization to learn embedding representation. But\nidioms and fixed expressions are different such that the literal meaning of the\nphrases may or may not be consistent with their contextual meaning. Therefore,\nwe explore different ways to combine static and contextual representations for\na stronger baseline model. Experimentations show that combining definition and\nrandom initialization will better support cloze test model performance for\nidioms whether independently or mixed with fixed expressions. While for fixed\nexpressions with no special meaning, static embedding with random\ninitialization is sufficient for cloze test model.", "published": "2022-11-24 02:05:47", "link": "http://arxiv.org/abs/2211.13376v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Mahalanobis-Based Scores for Textual OOD Detection", "abstract": "Deep learning methods have boosted the adoption of NLP systems in real-life\napplications. However, they turn out to be vulnerable to distribution shifts\nover time which may cause severe dysfunctions in production systems, urging\npractitioners to develop tools to detect out-of-distribution (OOD) samples\nthrough the lens of the neural network. In this paper, we introduce TRUSTED, a\nnew OOD detector for classifiers based on Transformer architectures that meets\noperational requirements: it is unsupervised and fast to compute. The\nefficiency of TRUSTED relies on the fruitful idea that all hidden layers carry\nrelevant information to detect OOD examples. Based on this, for a given input,\nTRUSTED consists in (i) aggregating this information and (ii) computing a\nsimilarity score by exploiting the training distribution, leveraging the\npowerful concept of data depth. Our extensive numerical experiments involve 51k\nmodel configurations, including various checkpoints, seeds, and datasets, and\ndemonstrate that TRUSTED achieves state-of-the-art performances. In particular,\nit improves previous AUROC over 3 points.", "published": "2022-11-24 10:51:58", "link": "http://arxiv.org/abs/2211.13527v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How \"open\" are the conversations with open-domain chatbots? A proposal\n  for Speech Event based evaluation", "abstract": "Open-domain chatbots are supposed to converse freely with humans without\nbeing restricted to a topic, task or domain. However, the boundaries and/or\ncontents of open-domain conversations are not clear. To clarify the boundaries\nof \"openness\", we conduct two studies: First, we classify the types of \"speech\nevents\" encountered in a chatbot evaluation data set (i.e., Meena by Google)\nand find that these conversations mainly cover the \"small talk\" category and\nexclude the other speech event categories encountered in real life human-human\ncommunication. Second, we conduct a small-scale pilot study to generate online\nconversations covering a wider range of speech event categories between two\nhumans vs. a human and a state-of-the-art chatbot (i.e., Blender by Facebook).\nA human evaluation of these generated conversations indicates a preference for\nhuman-human conversations, since the human-chatbot conversations lack coherence\nin most speech event categories. Based on these results, we suggest (a) using\nthe term \"small talk\" instead of \"open-domain\" for the current chatbots which\nare not that \"open\" in terms of conversational abilities yet, and (b) revising\nthe evaluation methods to test the chatbot conversations against other speech\nevents.", "published": "2022-11-24 12:23:20", "link": "http://arxiv.org/abs/2211.13560v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotion-guided Cross-domain Fake News Detection using Adversarial Domain\n  Adaptation", "abstract": "Recent works on fake news detection have shown the efficacy of using emotions\nas a feature or emotions-based features for improved performance. However, the\nimpact of these emotion-guided features for fake news detection in cross-domain\nsettings, where we face the problem of domain shift, is still largely\nunexplored. In this work, we evaluate the impact of emotion-guided features for\ncross-domain fake news detection, and further propose an emotion-guided,\ndomain-adaptive approach using adversarial learning. We prove the efficacy of\nemotion-guided models in cross-domain settings for various combinations of\nsource and target datasets from FakeNewsAMT, Celeb, Politifact and Gossipcop\ndatasets.", "published": "2022-11-24 17:11:56", "link": "http://arxiv.org/abs/2211.13718v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "German Phoneme Recognition with Text-to-Phoneme Data Augmentation", "abstract": "In this study, we experimented to examine the effect of adding the most\nfrequent n phoneme bigrams to the basic vocabulary on the German phoneme\nrecognition model using the text-to-phoneme data augmentation strategy. As a\nresult, compared to the baseline model, the vowel30 model and the const20 model\nshowed an increased BLEU score of more than 1 point, and the total30 model\nshowed a significant decrease in the BLEU score of more than 20 points, showing\nthat the phoneme bigrams could have a positive or negative effect on the model\nperformance. In addition, we identified the types of errors that the models\nrepeatedly showed through error analysis.", "published": "2022-11-24 19:32:49", "link": "http://arxiv.org/abs/2211.13776v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question Answering and Question Generation for Finnish", "abstract": "Recent advances in the field of language modeling have improved the\nstate-of-the-art in question answering (QA) and question generation (QG).\nHowever, the development of modern neural models, their benchmarks, and\ndatasets for training them has mainly focused on English. Finnish, like many\nother languages, faces a shortage of large QA/QG model training resources,\nwhich has prevented experimenting with state-of-the-art QA/QG fine-tuning\nmethods. We present the first neural QA and QG models that work with Finnish.\nTo train the models, we automatically translate the SQuAD dataset and then use\nnormalization methods to reduce the amount of problematic data created during\nthe translation. Using the synthetic data, together with the Finnish partition\nof the TyDi-QA dataset, we fine-tune several transformer-based models to both\nQA and QG and evaluate their performance. To the best of our knowledge, the\nresulting dataset is the first large-scale QA/QG resource for Finnish. This\npaper also sets the initial benchmarks for Finnish-language QA and QG.", "published": "2022-11-24 20:40:00", "link": "http://arxiv.org/abs/2211.13794v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Selective Masking as a Bridge between Pre-training and Fine-tuning", "abstract": "Pre-training a language model and then fine-tuning it for downstream tasks\nhas demonstrated state-of-the-art results for various NLP tasks. Pre-training\nis usually independent of the downstream task, and previous works have shown\nthat this pre-training alone might not be sufficient to capture the\ntask-specific nuances. We propose a way to tailor a pre-trained BERT model for\nthe downstream task via task-specific masking before the standard supervised\nfine-tuning. For this, a word list is first collected specific to the task. For\nexample, if the task is sentiment classification, we collect a small sample of\nwords representing both positive and negative sentiments. Next, a word's\nimportance for the task, called the word's task score, is measured using the\nword list. Each word is then assigned a probability of masking based on its\ntask score. We experiment with different masking functions that assign the\nprobability of masking based on the word's task score. The BERT model is\nfurther trained on MLM objective, where masking is done using the above\nstrategy. Following this standard supervised fine-tuning is done for different\ndownstream tasks. Results on these tasks show that the selective masking\nstrategy outperforms random masking, indicating its effectiveness.", "published": "2022-11-24 22:25:27", "link": "http://arxiv.org/abs/2211.13815v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Entities in the Astrophysics Literature: A Comparison of\n  Word-based and Span-based Entity Recognition Methods", "abstract": "Information Extraction from scientific literature can be challenging due to\nthe highly specialised nature of such text. We describe our entity recognition\nmethods developed as part of the DEAL (Detecting Entities in the Astrophysics\nLiterature) shared task. The aim of the task is to build a system that can\nidentify Named Entities in a dataset composed by scholarly articles from\nastrophysics literature. We planned our participation such that it enables us\nto conduct an empirical comparison between word-based tagging and span-based\nclassification methods. When evaluated on two hidden test sets provided by the\norganizer, our best-performing submission achieved $F_1$ scores of 0.8307\n(validation phase) and 0.7990 (testing phase).", "published": "2022-11-24 23:07:48", "link": "http://arxiv.org/abs/2211.13819v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bidirectional Representations for Low Resource Spoken Language\n  Understanding", "abstract": "Most spoken language understanding systems use a pipeline approach composed\nof an automatic speech recognition interface and a natural language\nunderstanding module. This approach forces hard decisions when converting\ncontinuous inputs into discrete language symbols. Instead, we propose a\nrepresentation model to encode speech in rich bidirectional encodings that can\nbe used for downstream tasks such as intent prediction. The approach uses a\nmasked language modelling objective to learn the representations, and thus\nbenefits from both the left and right contexts. We show that the performance of\nthe resulting encodings before fine-tuning is better than comparable models on\nmultiple datasets, and that fine-tuning the top layers of the representation\nmodel improves the current state of the art on the Fluent Speech Command\ndataset, also in a low-data regime, when a limited amount of labelled data is\nused for training. Furthermore, we propose class attention as a spoken language\nunderstanding module, efficient both in terms of speed and number of\nparameters. Class attention can be used to visually explain the predictions of\nour model, which goes a long way in understanding how the model makes\npredictions. We perform experiments in English and in Dutch.", "published": "2022-11-24 17:05:16", "link": "http://arxiv.org/abs/2211.14320v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AI Knows Which Words Will Appear in Next Year's Korean CSAT", "abstract": "A text-mining-based word class categorization method and LSTM-based\nvocabulary pattern prediction method are introduced in this paper. A\npreprocessing method based on simple text appearance frequency analysis is\nfirst described. This method was developed as a data screening tool but showed\n4.35 ~ 6.21 times higher than previous works. An LSTM deep learning method is\nalso suggested for vocabulary appearance pattern prediction method. AI performs\na regression with various size of data window of previous exams to predict the\nprobabilities of word appearance in the next exam. Predicted values of AI over\nvarious data windows are processed into a single score as a weighted sum, which\nwe call an \"AI-Score\", which represents the probability of word appearance in\nnext year's exam. Suggested method showed 100% accuracy at the range 100-score\narea and showed only 1.7% error of prediction in the section where the scores\nwere over 60 points. All source codes are freely available at the authors' Git\nHub repository. (https://github.com/needleworm/bigdata_voca)", "published": "2022-11-24 12:44:26", "link": "http://arxiv.org/abs/2211.15426v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TSGP: Two-Stage Generative Prompting for Unsupervised Commonsense\n  Question Answering", "abstract": "Unsupervised commonsense question answering requires mining effective\ncommonsense knowledge without the rely on the labeled task data. Previous\nmethods typically retrieved from traditional knowledge bases or used\npre-trained language models (PrLMs) to generate fixed types of knowledge, which\nhave poor generalization ability. In this paper, we aim to address the above\nlimitation by leveraging the implicit knowledge stored in PrLMs and propose a\ntwo-stage prompt-based unsupervised commonsense question answering framework\n(TSGP). Specifically, we first use knowledge generation prompts to generate the\nknowledge required for questions with unlimited types and possible candidate\nanswers independent of specified choices. Then, we further utilize answer\ngeneration prompts to generate possible candidate answers independent of\nspecified choices. Experimental results and analysis on three different\ncommonsense reasoning tasks, CommonsenseQA, OpenBookQA, and SocialIQA,\ndemonstrate that TSGP significantly improves the reasoning ability of language\nmodels in unsupervised settings. Our code is available at:\nhttps://github.com/Yueqing-Sun/TSGP.", "published": "2022-11-24 10:19:24", "link": "http://arxiv.org/abs/2211.13515v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ham2Pose: Animating Sign Language Notation into Pose Sequences", "abstract": "Translating spoken languages into Sign languages is necessary for open\ncommunication between the hearing and hearing-impaired communities. To achieve\nthis goal, we propose the first method for animating a text written in\nHamNoSys, a lexical Sign language notation, into signed pose sequences. As\nHamNoSys is universal by design, our proposed method offers a generic solution\ninvariant to the target Sign language. Our method gradually generates pose\npredictions using transformer encoders that create meaningful representations\nof the text and poses while considering their spatial and temporal information.\nWe use weak supervision for the training process and show that our method\nsucceeds in learning from partial and inaccurate data. Additionally, we offer a\nnew distance measurement that considers missing keypoints, to measure the\ndistance between pose sequences using DTW-MJE. We validate its correctness\nusing AUTSL, a large-scale Sign language dataset, show that it measures the\ndistance between pose sequences more accurately than existing measurements, and\nuse it to assess the quality of our generated pose sequences. Code for the data\npre-processing, the model, and the distance measurement is publicly released\nfor future research.", "published": "2022-11-24 13:59:32", "link": "http://arxiv.org/abs/2211.13613v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Prototypical Fine-tuning: Towards Robust Performance Under Varying Data\n  Sizes", "abstract": "In this paper, we move towards combining large parametric models with\nnon-parametric prototypical networks. We propose prototypical fine-tuning, a\nnovel prototypical framework for fine-tuning pretrained language models (LM),\nwhich automatically learns a bias to improve predictive performance for varying\ndata sizes, especially low-resource settings. Our prototypical fine-tuning\napproach can automatically adjust the model capacity according to the number of\ndata points and the model's inherent attributes. Moreover, we propose four\nprinciples for effective prototype fine-tuning towards the optimal solution.\nExperimental results across various datasets show that our work achieves\nsignificant performance improvements under various low-resource settings, as\nwell as comparable and usually better performances in high-resource scenarios.", "published": "2022-11-24 14:38:08", "link": "http://arxiv.org/abs/2211.13638v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Undesirable Biases in NLP: Addressing Challenges of Measurement", "abstract": "As Large Language Models and Natural Language Processing (NLP) technology\nrapidly develop and spread into daily life, it becomes crucial to anticipate\nhow their use could harm people. One problem that has received a lot of\nattention in recent years is that this technology has displayed harmful biases,\nfrom generating derogatory stereotypes to producing disparate outcomes for\ndifferent social groups. Although a lot of effort has been invested in\nassessing and mitigating these biases, our methods of measuring the biases of\nNLP models have serious problems and it is often unclear what they actually\nmeasure. In this paper, we provide an interdisciplinary approach to discussing\nthe issue of NLP model bias by adopting the lens of psychometrics -- a field\nspecialized in the measurement of concepts like bias that are not directly\nobservable. In particular, we will explore two central notions from\npsychometrics, the construct validity and the reliability of measurement tools,\nand discuss how they can be applied in the context of measuring model bias. Our\ngoal is to provide NLP practitioners with methodological tools for designing\nbetter bias measures, and to inspire them more generally to explore tools from\npsychometrics when working on bias measurement tools.", "published": "2022-11-24 16:53:18", "link": "http://arxiv.org/abs/2211.13709v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Linguistic and Computational Requirements for Creating\n  Face-to-Face Multimodal Human-Machine Interaction", "abstract": "In this study, conversations between humans and avatars are linguistically,\norganizationally, and structurally analyzed, focusing on what is necessary for\ncreating face-to-face multimodal interfaces for machines. We videorecorded\nthirty-four human-avatar interactions, performed complete linguistic\nmicroanalysis on video excerpts, and marked all the occurrences of multimodal\nactions and events. Statistical inferences were applied to data, allowing us to\ncomprehend not only how often multimodal actions occur but also how multimodal\nevents are distributed between the speaker (emitter) and the listener\n(recipient). We also observed the distribution of multimodal occurrences for\neach modality. The data show evidence that double-loop feedback is established\nduring a face-to-face conversation. This led us to propose that knowledge from\nConversation Analysis (CA), cognitive science, and Theory of Mind (ToM), among\nothers, should be incorporated into the ones used for describing human-machine\nmultimodal interactions. Face-to-face interfaces require an additional control\nlayer to the multimodal fusion layer. This layer has to organize the flow of\nconversation, integrate the social context into the interaction, as well as\nmake plans concerning 'what' and 'how' to progress on the interaction. This\nhigher level is best understood if we incorporate insights from CA and ToM into\nthe interface system.", "published": "2022-11-24 21:17:36", "link": "http://arxiv.org/abs/2211.13804v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Multi-label Few-shot ICD Coding as Autoregressive Generation with Prompt", "abstract": "Automatic International Classification of Diseases (ICD) coding aims to\nassign multiple ICD codes to a medical note with an average of 3,000+ tokens.\nThis task is challenging due to the high-dimensional space of multi-label\nassignment (155,000+ ICD code candidates) and the long-tail challenge - Many\nICD codes are infrequently assigned yet infrequent ICD codes are important\nclinically. This study addresses the long-tail challenge by transforming this\nmulti-label classification task into an autoregressive generation task.\nSpecifically, we first introduce a novel pretraining objective to generate free\ntext diagnoses and procedure using the SOAP structure, the medical logic\nphysicians use for note documentation. Second, instead of directly predicting\nthe high dimensional space of ICD codes, our model generates the lower\ndimension of text descriptions, which then infer ICD codes. Third, we designed\na novel prompt template for multi-label classification. We evaluate our\nGeneration with Prompt model with the benchmark of all code assignment\n(MIMIC-III-full) and few shot ICD code assignment evaluation benchmark\n(MIMIC-III-few). Experiments on MIMIC-III-few show that our model performs with\na marco F1 30.2, which substantially outperforms the previous MIMIC-III-full\nSOTA model (marco F1 4.3) and the model specifically designed for few/zero shot\nsetting (marco F1 18.7). Finally, we design a novel ensemble learner, a cross\nattention reranker with prompts, to integrate previous SOTA and our best\nfew-shot coding predictions. Experiments on MIMIC-III-full show that our\nensemble learner substantially improves both macro and micro F1, from 10.4 to\n14.6 and from 58.2 to 59.1, respectively.", "published": "2022-11-24 22:10:50", "link": "http://arxiv.org/abs/2211.13813v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Seeing What You Miss: Vision-Language Pre-training with Semantic\n  Completion Learning", "abstract": "Cross-modal alignment is essential for vision-language pre-training (VLP)\nmodels to learn the correct corresponding information across different\nmodalities. For this purpose, inspired by the success of masked language\nmodeling (MLM) tasks in the NLP pre-training area, numerous masked modeling\ntasks have been proposed for VLP to further promote cross-modal interactions.\nThe core idea of previous masked modeling tasks is to focus on reconstructing\nthe masked tokens based on visible context for learning local-to-local\nalignment. However, most of them pay little attention to the global semantic\nfeatures generated for the masked data, resulting in a limited cross-modal\nalignment ability of global representations. Therefore, in this paper, we\npropose a novel Semantic Completion Learning (SCL) task, complementary to\nexisting masked modeling tasks, to facilitate global-to-local alignment.\nSpecifically, the SCL task complements the missing semantics of masked data by\ncapturing the corresponding information from the other modality, promoting\nlearning more representative global features which have a great impact on the\nperformance of downstream tasks. Moreover, we present a flexible vision\nencoder, which enables our model to perform image-text and video-text\nmultimodal tasks simultaneously. Experimental results show that our proposed\nmethod obtains state-of-the-art performance on various vision-language\nbenchmarks, such as visual question answering, image-text retrieval, and\nvideo-text retrieval.", "published": "2022-11-24 06:39:16", "link": "http://arxiv.org/abs/2211.13437v2", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Multitask Learning for Low Resource Spoken Language Understanding", "abstract": "We explore the benefits that multitask learning offer to speech processing as\nwe train models on dual objectives with automatic speech recognition and intent\nclassification or sentiment classification. Our models, although being of\nmodest size, show improvements over models trained end-to-end on intent\nclassification. We compare different settings to find the optimal disposition\nof each task module compared to one another. Finally, we study the performance\nof the models in low-resource scenario by training the models with as few as\none example per class. We show that multitask learning in these scenarios\ncompete with a baseline model trained on text features and performs\nconsiderably better than a pipeline model. On sentiment classification, we\nmatch the performance of an end-to-end model with ten times as many parameters.\nWe consider 4 tasks and 4 datasets in Dutch and English.", "published": "2022-11-24 16:38:17", "link": "http://arxiv.org/abs/2211.13703v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Question-type Identification for Academic Questions in Online Learning\n  Platform", "abstract": "Online learning platforms provide learning materials and answers to students'\nacademic questions by experts, peers, or systems. This paper explores\nquestion-type identification as a step in content understanding for an online\nlearning platform. The aim of the question-type identifier is to categorize\nquestion types based on their structure and complexity, using the question\ntext, subject, and structural features. We have defined twelve question-type\nclasses, including Multiple-Choice Question (MCQ), essay, and others. We have\ncompiled an internal dataset of students' questions and used a combination of\nweak-supervision techniques and manual annotation. We then trained a BERT-based\nensemble model on this dataset and evaluated this model on a separate\nhuman-labeled test set. Our experiments yielded an F1-score of 0.94 for MCQ\nbinary classification and promising results for 12-class multilabel\nclassification. We deployed the model in our online learning platform as a\ncrucial enabler for content understanding to enhance the student learning\nexperience.", "published": "2022-11-24 17:28:29", "link": "http://arxiv.org/abs/2211.13727v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PyTAIL: Interactive and Incremental Learning of NLP Models with Human in\n  the Loop for Online Data", "abstract": "Online data streams make training machine learning models hard because of\ndistribution shift and new patterns emerging over time. For natural language\nprocessing (NLP) tasks that utilize a collection of features based on lexicons\nand rules, it is important to adapt these features to the changing data. To\naddress this challenge we introduce PyTAIL, a python library, which allows a\nhuman in the loop approach to actively train NLP models. PyTAIL enhances\ngeneric active learning, which only suggests new instances to label by also\nsuggesting new features like rules and lexicons to label. Furthermore, PyTAIL\nis flexible enough for users to accept, reject, or update rules and lexicons as\nthe model is being trained. Finally, we simulate the performance of PyTAIL on\nexisting social media benchmark datasets for text classification. We compare\nvarious active learning strategies on these benchmarks. The model closes the\ngap with as few as 10% of the training data. Finally, we also highlight the\nimportance of tracking evaluation metric on remaining data (which is not yet\nmerged with active learning) alongside the test dataset. This highlights the\neffectiveness of the model in accurately annotating the remaining dataset,\nwhich is especially suitable for batch processing of large unlabelled corpora.\nPyTAIL will be available at https://github.com/socialmediaie/pytail.", "published": "2022-11-24 20:08:15", "link": "http://arxiv.org/abs/2211.13786v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "68T50, 68T07", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Using Natural Language Processing to Predict Costume Core Vocabulary of\n  Historical Artifacts", "abstract": "Historic dress artifacts are a valuable source for human studies. In\nparticular, they can provide important insights into the social aspects of\ntheir corresponding era. These insights are commonly drawn from garment\npictures as well as the accompanying descriptions and are usually stored in a\nstandardized and controlled vocabulary that accurately describes garments and\ncostume items, called the Costume Core Vocabulary. Building an accurate Costume\nCore from garment descriptions can be challenging because the historic garment\nitems are often donated, and the accompanying descriptions can be based on\nuntrained individuals and use a language common to the period of the items. In\nthis paper, we present an approach to use Natural Language Processing (NLP) to\nmap the free-form text descriptions of the historic items to that of the\ncontrolled vocabulary provided by the Costume Core. Despite the limited\ndataset, we were able to train an NLP model based on the Universal Sentence\nEncoder to perform this mapping with more than 90% test accuracy for a subset\nof the Costume Core vocabulary. We describe our methodology, design choices,\nand development of our approach, and show the feasibility of predicting the\nCostume Core for unseen descriptions. With more garment descriptions still\nbeing curated to be used for training, we expect to have higher accuracy for\nbetter generalizability.", "published": "2022-11-24 02:13:55", "link": "http://arxiv.org/abs/2212.07931v1", "categories": ["cs.CL", "cs.DL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A new Speech Feature Fusion method with cross gate parallel CNN for\n  Speaker Recognition", "abstract": "In this paper, a new speech feature fusion method is proposed for speaker\nrecognition on the basis of the cross gate parallel convolutional neural\nnetwork (CG-PCNN). The Mel filter bank features (MFBFs) of different frequency\nresolutions can be extracted from each speech frame of a speaker's speech by\nseveral Mel filter banks, where the numbers of the triangular filters in the\nMel filter banks are different. Due to the frequency resolutions of these MFBFs\nare different, there are some complementaries for these MFBFs. The CG-PCNN is\nutilized to extract the deep features from these MFBFs, which applies a cross\ngate mechanism to capture the complementaries for improving the performance of\nthe speaker recognition system. Then, the fusion feature can be obtained by\nconcatenating these deep features for speaker recognition. The experimental\nresults show that the speaker recognition system with the proposed speech\nfeature fusion method is effective, and marginally outperforms the existing\nstate-of-the-art systems.", "published": "2022-11-24 02:17:43", "link": "http://arxiv.org/abs/2211.13377v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "TESSP: Text-Enhanced Self-Supervised Speech Pre-training", "abstract": "Self-supervised speech pre-training empowers the model with the contextual\nstructure inherent in the speech signal while self-supervised text pre-training\nempowers the model with linguistic information. Both of them are beneficial for\ndownstream speech tasks such as ASR. However, the distinct pre-training\nobjectives make it challenging to jointly optimize the speech and text\nrepresentation in the same model. To solve this problem, we propose\nText-Enhanced Self-Supervised Speech Pre-training (TESSP), aiming to\nincorporate the linguistic information into speech pre-training. Our model\nconsists of three parts, i.e., a speech encoder, a text encoder and a shared\nencoder. The model takes unsupervised speech and text data as the input and\nleverages the common HuBERT and MLM losses respectively. We also propose\nphoneme up-sampling and representation swapping to enable joint modeling of the\nspeech and text information. Specifically, to fix the length mismatching\nproblem between speech and text data, we phonemize the text sequence and\nup-sample the phonemes with the alignment information extracted from a small\nset of supervised data. Moreover, to close the gap between the learned speech\nand text representations, we swap the text representation with the speech\nrepresentation extracted by the respective private encoders according to the\nalignment information. Experiments on the Librispeech dataset shows the\nproposed TESSP model achieves more than 10% improvement compared with WavLM on\nthe test-clean and test-other sets. We also evaluate our model on the SUPERB\nbenchmark, showing our model has better performance on Phoneme Recognition,\nAcoustic Speech Recognition and Speech Translation compared with WavLM.", "published": "2022-11-24 07:08:51", "link": "http://arxiv.org/abs/2211.13443v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Prosody-controllable spontaneous TTS with neural HMMs", "abstract": "Spontaneous speech has many affective and pragmatic functions that are\ninteresting and challenging to model in TTS. However, the presence of reduced\narticulation, fillers, repetitions, and other disfluencies in spontaneous\nspeech make the text and acoustics less aligned than in read speech, which is\nproblematic for attention-based TTS. We propose a TTS architecture that can\nrapidly learn to speak from small and irregular datasets, while also\nreproducing the diversity of expressive phenomena present in spontaneous\nspeech. Specifically, we add utterance-level prosody control to an existing\nneural HMM-based TTS system which is capable of stable, monotonic alignments\nfor spontaneous speech. We objectively evaluate control accuracy and perform\nperceptual tests that demonstrate that prosody control does not degrade\nsynthesis quality. To exemplify the power of combining prosody control and\necologically valid data for reproducing intricate spontaneous speech phenomena,\nwe evaluate the system's capability of synthesizing two types of creaky voice.\nAudio samples are available at\nhttps://www.speech.kth.se/tts-demos/prosodic-hmm/", "published": "2022-11-24 11:06:11", "link": "http://arxiv.org/abs/2211.13533v2", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.SD", "68T07", "I.2.7; I.2.6; G.3; H.5.5"], "primary_category": "eess.AS"}
