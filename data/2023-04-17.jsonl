{"title": "A Question-Answering Approach to Key Value Pair Extraction from\n  Form-like Document Images", "abstract": "In this paper, we present a new question-answering (QA) based key-value pair\nextraction approach, called KVPFormer, to robustly extracting key-value\nrelationships between entities from form-like document images. Specifically,\nKVPFormer first identifies key entities from all entities in an image with a\nTransformer encoder, then takes these key entities as \\textbf{questions} and\nfeeds them into a Transformer decoder to predict their corresponding\n\\textbf{answers} (i.e., value entities) in parallel. To achieve higher answer\nprediction accuracy, we propose a coarse-to-fine answer prediction approach\nfurther, which first extracts multiple answer candidates for each identified\nquestion in the coarse stage and then selects the most likely one among these\ncandidates in the fine stage. In this way, the learning difficulty of answer\nprediction can be effectively reduced so that the prediction accuracy can be\nimproved. Moreover, we introduce a spatial compatibility attention bias into\nthe self-attention/cross-attention mechanism for \\Ours{} to better model the\nspatial interactions between entities. With these new techniques, our proposed\n\\Ours{} achieves state-of-the-art results on FUNSD and XFUND datasets,\noutperforming the previous best-performing method by 7.2\\% and 13.2\\% in F1\nscore, respectively.", "published": "2023-04-17 02:55:31", "link": "http://arxiv.org/abs/2304.07957v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ERTIM@MC2: Diversified Argumentative Tweets Retrieval", "abstract": "In this paper, we present our participation to CLEF MC2 2018 edition for the\ntask 2 Mining opinion argumentation. It consists in detecting the most\nargumentative and diverse Tweets about some festivals in English and French\nfrom a massive multilingual collection. We measure argumentativity of a Tweet\ncomputing the amount of argumentation compounds it contains. We consider\nargumentation compounds as a combination between opinion expression and its\nsupport with facts and a particular structuration. Regarding diversity, we\nconsider the amount of festival aspects covered by Tweets. An initial step\nfilters the original dataset to fit the language and topic requirements of the\ntask. Then, we compute and integrate linguistic descriptors to detect claims\nand their respective justifications in Tweets. The final step extracts the most\ndiverse arguments by clustering Tweets according to their textual content and\nselecting the most argumentative ones from each cluster. We conclude the paper\ndescribing the different ways we combined the descriptors among the different\nruns we submitted and discussing their results.", "published": "2023-04-17 08:06:17", "link": "http://arxiv.org/abs/2304.08047v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comparative Study between Full-Parameter and LoRA-based Fine-Tuning on\n  Chinese Instruction Data for Instruction Following Large Language Model", "abstract": "Recently, the instruction-tuning of large language models is a crucial area\nof research in the field of natural language processing. Due to resource and\ncost limitations, several researchers have employed parameter-efficient tuning\ntechniques, such as LoRA, for instruction tuning, and have obtained encouraging\nresults In comparison to full-parameter fine-tuning, LoRA-based tuning\ndemonstrates salient benefits in terms of training costs. In this study, we\nundertook experimental comparisons between full-parameter fine-tuning and\nLoRA-based tuning methods, utilizing LLaMA as the base model. The experimental\nresults show that the selection of the foundational model, training dataset\nscale, learnable parameter quantity, and model training cost are all important\nfactors. We hope that the experimental conclusions of this paper can provide\ninspiration for training large language models, especially in the field of\nChinese, and help researchers find a better trade-off strategy between training\ncost and model performance. To facilitate the reproduction of the paper's\nresults, the dataset, model and code will be released.", "published": "2023-04-17 09:36:36", "link": "http://arxiv.org/abs/2304.08109v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of Multitask Learning to Improve Open Domain Dialogue\n  Systems", "abstract": "Autoregressive models used to generate responses in open-domain dialogue\nsystems often struggle to take long-term context into account and to maintain\nconsistency over a dialogue. Previous research in open-domain dialogue\ngeneration has shown that the use of \\emph{auxiliary tasks} can introduce\ninductive biases that encourage the model to improve these qualities. However,\nmost previous research has focused on encoder-only or encoder/decoder models,\nwhile the use of auxiliary tasks in \\emph{decoder-only} autoregressive models\nis under-explored. This paper describes an investigation where four different\nauxiliary tasks are added to small and medium-sized GPT-2 models fine-tuned on\nthe PersonaChat and DailyDialog datasets. The results show that the\nintroduction of the new auxiliary tasks leads to small but consistent\nimprovement in evaluations of the investigated models.", "published": "2023-04-17 09:44:56", "link": "http://arxiv.org/abs/2304.08115v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Political corpus creation through automatic speech recognition on EU\n  debates", "abstract": "In this paper, we present a transcribed corpus of the LIBE committee of the\nEU parliament, totalling 3.6 Million running words. The meetings of\nparliamentary committees of the EU are a potentially valuable source of\ninformation for political scientists but the data is not readily available\nbecause only disclosed as speech recordings together with limited metadata. The\nmeetings are in English, partly spoken by non-native speakers, and partly\nspoken by interpreters. We investigated the most appropriate Automatic Speech\nRecognition (ASR) model to create an accurate text transcription of the audio\nrecordings of the meetings in order to make their content available for\nresearch and analysis. We focused on the unsupervised domain adaptation of the\nASR pipeline. Building on the transformer-based Wav2vec2.0 model, we\nexperimented with multiple acoustic models, language models and the addition of\ndomain-specific terms. We found that a domain-specific acoustic model and a\ndomain-specific language model give substantial improvements to the ASR output,\nreducing the word error rate (WER) from 28.22 to 17.95. The use of\ndomain-specific terms in the decoding stage did not have a positive effect on\nthe quality of the ASR in terms of WER. Initial topic modelling results\nindicated that the corpus is useful for downstream analysis tasks. We release\nthe resulting corpus and our analysis pipeline for future research.", "published": "2023-04-17 10:41:59", "link": "http://arxiv.org/abs/2304.08137v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VECO 2.0: Cross-lingual Language Model Pre-training with\n  Multi-granularity Contrastive Learning", "abstract": "Recent studies have demonstrated the potential of cross-lingual\ntransferability by training a unified Transformer encoder for multiple\nlanguages. In addition to involving the masked language model objective,\nexisting cross-lingual pre-training works leverage sentence-level contrastive\nlearning or plugs in extra cross-attention module to complement the\ninsufficient capabilities of cross-lingual alignment. Nonetheless, synonym\npairs residing in bilingual corpus are not exploited and aligned, which is more\ncrucial than sentence interdependence establishment for token-level tasks. In\nthis work, we propose a cross-lingual pre-trained model VECO~2.0 based on\ncontrastive learning with multi-granularity alignments. Specifically, the\nsequence-to-sequence alignment is induced to maximize the similarity of the\nparallel pairs and minimize the non-parallel pairs. Then, token-to-token\nalignment is integrated to bridge the gap between synonymous tokens excavated\nvia the thesaurus dictionary from the other unpaired tokens in a bilingual\ninstance. Experiments show the effectiveness of the proposed strategy for\ncross-lingual model pre-training on the XTREME benchmark.", "published": "2023-04-17 12:23:41", "link": "http://arxiv.org/abs/2304.08205v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LED: A Dataset for Life Event Extraction from Dialogs", "abstract": "Lifelogging has gained more attention due to its wide applications, such as\npersonalized recommendations or memory assistance. The issues of collecting and\nextracting personal life events have emerged. People often share their life\nexperiences with others through conversations. However, extracting life events\nfrom conversations is rarely explored. In this paper, we present Life Event\nDialog, a dataset containing fine-grained life event annotations on\nconversational data. In addition, we initiate a novel conversational life event\nextraction task and differentiate the task from the public event extraction or\nthe life event extraction from other sources like microblogs. We explore three\ninformation extraction (IE) frameworks to address the conversational life event\nextraction task: OpenIE, relation extraction, and event extraction. A\ncomprehensive empirical analysis of the three baselines is established. The\nresults suggest that the current event extraction model still struggles with\nextracting life events from human daily conversations. Our proposed life event\ndialog dataset and in-depth analysis of IE frameworks will facilitate future\nresearch on life event extraction from conversations.", "published": "2023-04-17 14:46:59", "link": "http://arxiv.org/abs/2304.08327v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Compress Prompts with Gist Tokens", "abstract": "Prompting is the primary way to utilize the multitask capabilities of\nlanguage models (LMs), but prompts occupy valuable space in the input context\nwindow, and repeatedly encoding the same prompt is computationally inefficient.\nFinetuning and distillation methods allow for specialization of LMs without\nprompting, but require retraining the model for each task. To avoid this\ntrade-off entirely, we present gisting, which trains an LM to compress prompts\ninto smaller sets of \"gist\" tokens which can be cached and reused for compute\nefficiency. Gist models can be trained with no additional cost over standard\ninstruction finetuning by simply modifying Transformer attention masks to\nencourage prompt compression. On decoder (LLaMA-7B) and encoder-decoder\n(FLAN-T5-XXL) LMs, gisting enables up to 26x compression of prompts, resulting\nin up to 40% FLOPs reductions, 4.2% wall time speedups, and storage savings,\nall with minimal loss in output quality.", "published": "2023-04-17 17:47:37", "link": "http://arxiv.org/abs/2304.08467v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Researchers eye-view of sarcasm detection in social media textual\n  content", "abstract": "The enormous use of sarcastic text in all forms of communication in social\nmedia will have a physiological effect on target users. Each user has a\ndifferent approach to misusing and recognising sarcasm. Sarcasm detection is\ndifficult even for users, and this will depend on many things such as\nperspective, context, special symbols. So, that will be a challenging task for\nmachines to differentiate sarcastic sentences from non-sarcastic sentences.\nThere are no exact rules based on which model will accurately detect sarcasm\nfrom many text corpus in the current situation. So, one needs to focus on\noptimistic and forthcoming approaches in the sarcasm detection domain. This\npaper discusses various sarcasm detection techniques and concludes with some\napproaches, related datasets with optimal features, and the researcher's\nchallenges.", "published": "2023-04-17 19:45:10", "link": "http://arxiv.org/abs/2304.08582v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Testing the Reliability of ChatGPT for Text Annotation and\n  Classification: A Cautionary Remark", "abstract": "Recent studies have demonstrated promising potential of ChatGPT for various\ntext annotation and classification tasks. However, ChatGPT is non-deterministic\nwhich means that, as with human coders, identical input can lead to different\noutputs. Given this, it seems appropriate to test the reliability of ChatGPT.\nTherefore, this study investigates the consistency of ChatGPT's zero-shot\ncapabilities for text annotation and classification, focusing on different\nmodel parameters, prompt variations, and repetitions of identical inputs. Based\non the real-world classification task of differentiating website texts into\nnews and not news, results show that consistency in ChatGPT's classification\noutput can fall short of scientific thresholds for reliability. For example,\neven minor wording alterations in prompts or repeating the identical input can\nlead to varying outputs. Although pooling outputs from multiple repetitions can\nimprove reliability, this study advises caution when using ChatGPT for\nzero-shot text annotation and underscores the need for thorough validation,\nsuch as comparison against human-annotated data. The unsupervised application\nof ChatGPT for text annotation and classification is not recommended.", "published": "2023-04-17 00:41:19", "link": "http://arxiv.org/abs/2304.11085v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chinese Open Instruction Generalist: A Preliminary Release", "abstract": "Instruction tuning is widely recognized as a key technique for building\ngeneralist language models, which has attracted the attention of researchers\nand the public with the release of InstructGPT~\\citep{ouyang2022training} and\nChatGPT\\footnote{\\url{https://chat.openai.com/}}. Despite impressive progress\nin English-oriented large-scale language models (LLMs), it is still\nunder-explored whether English-based foundation LLMs can perform similarly on\nmultilingual tasks compared to English tasks with well-designed instruction\ntuning and how we can construct the corpora needed for the tuning. To remedy\nthis gap, we propose the project as an attempt to create a Chinese instruction\ndataset by various methods adapted to the intrinsic characteristics of 4\nsub-tasks. We collect around 200k Chinese instruction tuning samples, which\nhave been manually checked to guarantee high quality. We also summarize the\nexisting English and Chinese instruction corpora and briefly describe some\npotential applications of the newly constructed Chinese instruction corpora.\nThe resulting \\textbf{C}hinese \\textbf{O}pen \\textbf{I}nstruction\n\\textbf{G}eneralist (\\textbf{COIG}) corpora are available in\nHuggingface\\footnote{\\url{https://huggingface.co/datasets/BAAI/COIG}} and\nGithub\\footnote{\\url{https://github.com/BAAI-Zlab/COIG}}, and will be\ncontinuously updated.", "published": "2023-04-17 04:45:06", "link": "http://arxiv.org/abs/2304.07987v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Zero to Hero: Examining the Power of Symbolic Tasks in Instruction\n  Tuning", "abstract": "Fine-tuning language models on tasks with instructions has demonstrated\npotential in facilitating zero-shot generalization to unseen tasks. In this\npaper, we introduce a straightforward yet effective method for enhancing\ninstruction tuning by employing symbolic tasks. Compared to crowdsourced human\ntasks or model-generated tasks, symbolic tasks present a unique advantage as\nthey can be easily generated in vast quantities, theoretically providing an\ninfinite supply of high-quality training instances. To explore the potential of\nsymbolic tasks, we carry out an extensive case study on the representative\nsymbolic task of SQL execution. Empirical results on various benchmarks\nvalidate that the integration of SQL execution leads to significant\nimprovements in zero-shot scenarios, particularly in table reasoning. Notably,\nour 3B model surpasses both the 175B GPT-3 and ChatGPT in zero-shot table\nreasoning across four benchmarks. Furthermore, experimental results on BBH (27\ntasks) and MMLU (57 tasks) reveal that language models can be enhanced through\nsymbolic tasks without compromising their generality. We hope that our paper\nserves as a catalyst, inspiring increased efforts to incorporate symbolic tasks\nin instruction tuning.", "published": "2023-04-17 05:29:42", "link": "http://arxiv.org/abs/2304.07995v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "InstructUIE: Multi-task Instruction Tuning for Unified Information\n  Extraction", "abstract": "Large language models have unlocked strong multi-task capabilities from\nreading instructive prompts. However, recent studies have shown that existing\nlarge models still have difficulty with information extraction tasks. For\nexample, gpt-3.5-turbo achieved an F1 score of 18.22 on the Ontonotes dataset,\nwhich is significantly lower than the state-of-the-art performance. In this\npaper, we propose InstructUIE, a unified information extraction framework based\non instruction tuning, which can uniformly model various information extraction\ntasks and capture the inter-task dependency. To validate the proposed method,\nwe introduce IE INSTRUCTIONS, a benchmark of 32 diverse information extraction\ndatasets in a unified text-to-text format with expert-written instructions.\nExperimental results demonstrate that our method achieves comparable\nperformance to Bert in supervised settings and significantly outperforms the\nstate-of-the-art and gpt3.5 in zero-shot settings.", "published": "2023-04-17 09:00:50", "link": "http://arxiv.org/abs/2304.08085v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Low-code LLM: Graphical User Interface over Large Language Models", "abstract": "Utilizing Large Language Models (LLMs) for complex tasks is challenging,\noften involving a time-consuming and uncontrollable prompt engineering process.\nThis paper introduces a novel human-LLM interaction framework, Low-code LLM. It\nincorporates six types of simple low-code visual programming interactions to\nachieve more controllable and stable responses. Through visual interaction with\na graphical user interface, users can incorporate their ideas into the process\nwithout writing trivial prompts. The proposed Low-code LLM framework consists\nof a Planning LLM that designs a structured planning workflow for complex\ntasks, which can be correspondingly edited and confirmed by users through\nlow-code visual programming operations, and an Executing LLM that generates\nresponses following the user-confirmed workflow. We highlight three advantages\nof the low-code LLM: user-friendly interaction, controllable generation, and\nwide applicability. We demonstrate its benefits using four typical\napplications. By introducing this framework, we aim to bridge the gap between\nhumans and LLMs, enabling more effective and efficient utilization of LLMs for\ncomplex tasks. The code, prompts, and experimental details are available at\nhttps://github.com/moymix/TaskMatrix/tree/main/LowCodeLLM. A system\ndemonstration video can be found at\nhttps://www.youtube.com/watch?v=jb2C1vaeO3E.", "published": "2023-04-17 09:27:40", "link": "http://arxiv.org/abs/2304.08103v3", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Context-Dependent Embedding Utterance Representations for Emotion\n  Recognition in Conversations", "abstract": "Emotion Recognition in Conversations (ERC) has been gaining increasing\nimportance as conversational agents become more and more common. Recognizing\nemotions is key for effective communication, being a crucial component in the\ndevelopment of effective and empathetic conversational agents. Knowledge and\nunderstanding of the conversational context are extremely valuable for\nidentifying the emotions of the interlocutor. We thus approach Emotion\nRecognition in Conversations leveraging the conversational context, i.e.,\ntaking into attention previous conversational turns. The usual approach to\nmodel the conversational context has been to produce context-independent\nrepresentations of each utterance and subsequently perform contextual modeling\nof these. Here we propose context-dependent embedding representations of each\nutterance by leveraging the contextual representational power of pre-trained\ntransformer language models. In our approach, we feed the conversational\ncontext appended to the utterance to be classified as input to the RoBERTa\nencoder, to which we append a simple classification module, thus discarding the\nneed to deal with context after obtaining the embeddings since these constitute\nalready an efficient representation of such context. We also investigate how\nthe number of introduced conversational turns influences our model performance.\nThe effectiveness of our approach is validated on the open-domain DailyDialog\ndataset and on the task-oriented EmoWOZ dataset.", "published": "2023-04-17 12:37:57", "link": "http://arxiv.org/abs/2304.08216v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Thorny Roses: Investigating the Dual Use Dilemma in Natural Language\n  Processing", "abstract": "Dual use, the intentional, harmful reuse of technology and scientific\nartefacts, is a problem yet to be well-defined within the context of Natural\nLanguage Processing (NLP). However, as NLP technologies continue to advance and\nbecome increasingly widespread in society, their inner workings have become\nincreasingly opaque. Therefore, understanding dual use concerns and potential\nways of limiting them is critical to minimising the potential harms of research\nand development. In this paper, we conduct a survey of NLP researchers and\npractitioners to understand the depth and their perspective of the problem as\nwell as to assess existing available support. Based on the results of our\nsurvey, we offer a definition of dual use that is tailored to the needs of the\nNLP community. The survey revealed that a majority of researchers are concerned\nabout the potential dual use of their research but only take limited action\ntoward it. In light of the survey results, we discuss the current state and\npotential means for mitigating dual use in NLP and propose a checklist that can\nbe integrated into existing conference ethics-frameworks, e.g., the ACL ethics\nchecklist.", "published": "2023-04-17 14:37:43", "link": "http://arxiv.org/abs/2304.08315v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Use of social media and Natural Language Processing (NLP) in natural\n  hazard research", "abstract": "Twitter is a microblogging service for sending short, public text messages\n(tweets) that has recently received more attention in scientific comunity. In\nthe works of Sasaki et al. (2010) and Earle et al., (2011) the authors explored\nthe real-time interaction on Twitter for detecting natural hazards (e.g.,\nearthquakes, typhoons) baed on users' tweets. An inherent challenge for such an\napplication is the natural language processing (NLP), which basically consists\nin converting the words in number (vectors and tensors) in order to\n(mathematically/ computationally) make predictions and classifications.\nRecently advanced computational tools have been made available for dealing with\ntext computationally. In this report we implement a NLP machine learning with\nTensorFlow, an end-to-end open source plataform for machine learning\napplications, to process and classify evenct based on files containing only\ntext.", "published": "2023-04-17 15:03:05", "link": "http://arxiv.org/abs/2304.08341v1", "categories": ["cs.CL", "physics.geo-ph"], "primary_category": "cs.CL"}
{"title": "What Makes a Good Dataset for Symbol Description Reading?", "abstract": "The usage of mathematical formulas as concise representations of a document's\nkey ideas is common practice. Correctly interpreting these formulas, by\nidentifying mathematical symbols and extracting their descriptions, is an\nimportant task in document understanding. This paper makes the following\ncontributions to the mathematical identifier description reading (MIDR) task:\n  (i) introduces the Math Formula Question Answering Dataset (MFQuAD) with\n$7508$ annotated identifier occurrences;\n  (ii) describes novel variations of the noun phrase ranking approach for the\nMIDR task;\n  (iii) reports experimental results for the SOTA noun phrase ranking approach\nand our novel variations of the approach, providing problem insights and a\nperformance baseline;\n  (iv) provides a position on the features that make an effective dataset for\nthe MIDR task.", "published": "2023-04-17 15:14:27", "link": "http://arxiv.org/abs/2304.08352v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "New Product Development (NPD) through Social Media-based Analysis by\n  Comparing Word2Vec and BERT Word Embeddings", "abstract": "This study introduces novel methods for sentiment and opinion classification\nof tweets to support the New Product Development (NPD) process. Two popular\nword embedding techniques, Word2Vec and BERT, were evaluated as inputs for\nclassic Machine Learning and Deep Learning algorithms to identify the\nbest-performing approach in sentiment analysis and opinion detection with\nlimited data. The results revealed that BERT word embeddings combined with\nBalanced Random Forest yielded the most accurate single model for both\nsentiment analysis and opinion detection on a use case. Additionally, the paper\nprovides feedback for future product development performing word graph analysis\nof the tweets with same sentiment to highlight potential areas of improvement.", "published": "2023-04-17 15:32:11", "link": "http://arxiv.org/abs/2304.08369v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The MiniPile Challenge for Data-Efficient Language Models", "abstract": "The ever-growing diversity of pre-training text corpora has equipped language\nmodels with generalization capabilities across various downstream tasks.\nHowever, such diverse datasets are often too large for academic budgets; hence,\nmost research on Transformer architectures, training procedures, optimizers,\netc. gets conducted on smaller, homogeneous datasets. To this end, we present\nThe MiniPile Challenge, where one pre-trains a language model on a diverse text\ncorpus containing at most 1M documents. MiniPile is a 6GB subset of the\ndeduplicated 825GB The Pile corpus. To curate MiniPile, we perform a simple,\nthree-step data filtering process: we (1) infer embeddings for all documents of\nthe Pile, (2) cluster the embedding space using $k$-means, and (3) filter out\nlow-quality clusters. To verify MiniPile's suitability for language model\npre-training, we use it to pre-train a BERT and T5 model, yielding a\nperformance drop of only $1.9\\%$/$2.5\\%$ on the GLUE and SNI benchmarks\ncompared to the original pre-trained checkpoints trained on $2.6$x/$745$x the\namount of data. MiniPile is available at\nhttps://huggingface.co/datasets/JeanKaddour/minipile.", "published": "2023-04-17 17:03:56", "link": "http://arxiv.org/abs/2304.08442v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Iterative Optimizing Framework for Radiology Report Summarization\n  with ChatGPT", "abstract": "The 'Impression' section of a radiology report is a critical basis for\ncommunication between radiologists and other physicians, and it is typically\nwritten by radiologists based on the 'Findings' section. However, writing\nnumerous impressions can be laborious and error-prone for radiologists.\nAlthough recent studies have achieved promising results in automatic impression\ngeneration using large-scale medical text data for pre-training and fine-tuning\npre-trained language models, such models often require substantial amounts of\nmedical text data and have poor generalization performance. While large\nlanguage models (LLMs) like ChatGPT have shown strong generalization\ncapabilities and performance, their performance in specific domains, such as\nradiology, remains under-investigated and potentially limited. To address this\nlimitation, we propose ImpressionGPT, which leverages the in-context learning\ncapability of LLMs by constructing dynamic contexts using domain-specific,\nindividualized data. This dynamic prompt approach enables the model to learn\ncontextual knowledge from semantically similar examples from existing data.\nAdditionally, we design an iterative optimization algorithm that performs\nautomatic evaluation on the generated impression results and composes the\ncorresponding instruction prompts to further optimize the model. The proposed\nImpressionGPT model achieves state-of-the-art performance on both MIMIC-CXR and\nOpenI datasets without requiring additional training data or fine-tuning the\nLLMs. This work presents a paradigm for localizing LLMs that can be applied in\na wide range of similar application scenarios, bridging the gap between\ngeneral-purpose LLMs and the specific language processing needs of various\ndomains.", "published": "2023-04-17 17:13:42", "link": "http://arxiv.org/abs/2304.08448v3", "categories": ["cs.CL", "cs.AI", "68T50, 68T37, 68T20", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Improving Autoregressive NLP Tasks via Modular Linearized Attention", "abstract": "Various natural language processing (NLP) tasks necessitate models that are\nefficient and small based on their ultimate application at the edge or in other\nresource-constrained environments. While prior research has reduced the size of\nthese models, increasing computational efficiency without considerable\nperformance impacts remains difficult, especially for autoregressive tasks.\nThis paper proposes modular linearized attention (MLA), which combines multiple\nefficient attention mechanisms, including cosFormer, to maximize inference\nquality while achieving notable speedups. We validate this approach on several\nautoregressive NLP tasks, including speech-to-text neural machine translation\n(S2T NMT), speech-to-text simultaneous translation (SimulST), and\nautoregressive text-to-spectrogram, noting efficiency gains on TTS and\ncompetitive performance for NMT and SimulST during training and inference.", "published": "2023-04-17 17:25:48", "link": "http://arxiv.org/abs/2304.08453v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On Uncertainty Calibration and Selective Generation in Probabilistic\n  Neural Summarization: A Benchmark Study", "abstract": "Modern deep models for summarization attains impressive benchmark\nperformance, but they are prone to generating miscalibrated predictive\nuncertainty. This means that they assign high confidence to low-quality\npredictions, leading to compromised reliability and trustworthiness in\nreal-world applications. Probabilistic deep learning methods are common\nsolutions to the miscalibration problem. However, their relative effectiveness\nin complex autoregressive summarization tasks are not well-understood. In this\nwork, we thoroughly investigate different state-of-the-art probabilistic\nmethods' effectiveness in improving the uncertainty quality of the neural\nsummarization models, across three large-scale benchmarks with varying\ndifficulty. We show that the probabilistic methods consistently improve the\nmodel's generation and uncertainty quality, leading to improved selective\ngeneration performance (i.e., abstaining from low-quality summaries) in\npractice. We also reveal notable failure patterns of probabilistic methods\nwidely-adopted in NLP community (e.g., Deep Ensemble and Monte Carlo Dropout),\ncautioning the importance of choosing appropriate method for the data setting.", "published": "2023-04-17 23:06:28", "link": "http://arxiv.org/abs/2304.08653v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SkillGPT: a RESTful API service for skill extraction and standardization\n  using a Large Language Model", "abstract": "We present SkillGPT, a tool for skill extraction and standardization (SES)\nfrom free-style job descriptions and user profiles with an open-source Large\nLanguage Model (LLM) as backbone. Most previous methods for similar tasks\neither need supervision or rely on heavy data-preprocessing and feature\nengineering. Directly prompting the latest conversational LLM for standard\nskills, however, is slow, costly and inaccurate. In contrast, SkillGPT utilizes\na LLM to perform its tasks in steps via summarization and vector similarity\nsearch, to balance speed with precision. The backbone LLM of SkillGPT is based\non Llama, free for academic use and thus useful for exploratory research and\nprototype development. Hence, our cost-free SkillGPT gives users the\nconvenience of conversational SES, efficiently and reliably.", "published": "2023-04-17 08:43:20", "link": "http://arxiv.org/abs/2304.11060v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Effectiveness of Debiasing Techniques: An Indigenous Qualitative\n  Analysis", "abstract": "An indigenous perspective on the effectiveness of debiasing techniques for\npre-trained language models (PLMs) is presented in this paper. The current\ntechniques used to measure and debias PLMs are skewed towards the US racial\nbiases and rely on pre-defined bias attributes (e.g. \"black\" vs \"white\"). Some\nrequire large datasets and further pre-training. Such techniques are not\ndesigned to capture the underrepresented indigenous populations in other\ncountries, such as M\\=aori in New Zealand. Local knowledge and understanding\nmust be incorporated to ensure unbiased algorithms, especially when addressing\na resource-restricted society.", "published": "2023-04-17 22:25:25", "link": "http://arxiv.org/abs/2304.11094v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca", "abstract": "Large Language Models (LLMs), such as ChatGPT and GPT-4, have dramatically\ntransformed natural language processing research and shown promising strides\ntowards Artificial General Intelligence (AGI). Nonetheless, the high costs\nassociated with training and deploying LLMs present substantial obstacles to\ntransparent, accessible academic research. While several large language models,\nsuch as LLaMA, have been open-sourced by the community, these predominantly\nfocus on English corpora, limiting their usefulness for other languages. In\nthis paper, we propose a method to augment LLaMA with capabilities for\nunderstanding and generating Chinese text and its ability to follow\ninstructions. We achieve this by extending LLaMA's existing vocabulary with an\nadditional 20,000 Chinese tokens, thereby improving its encoding efficiency and\nsemantic understanding of Chinese. We further incorporate secondary\npre-training using Chinese data and fine-tune the model with Chinese\ninstruction datasets, significantly enhancing the model's ability to comprehend\nand execute instructions. Our experimental results indicate that the newly\nproposed model markedly enhances the original LLaMA's proficiency in\nunderstanding and generating Chinese content. Additionally, the results on the\nC-Eval dataset yield competitive performance among the models with several\ntimes the size of ours. We have made our pre-trained models, training scripts,\nand other resources available through GitHub, fostering open research for our\ncommunity. Chinese LLaMA series:\n\\url{https://github.com/ymcui/Chinese-LLaMA-Alpaca} and Chinese Llama-2 series:\n\\url{https://github.com/ymcui/Chinese-LLaMA-Alpaca-2}", "published": "2023-04-17 11:39:53", "link": "http://arxiv.org/abs/2304.08177v3", "categories": ["cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interactive and Explainable Region-guided Radiology Report Generation", "abstract": "The automatic generation of radiology reports has the potential to assist\nradiologists in the time-consuming task of report writing. Existing methods\ngenerate the full report from image-level features, failing to explicitly focus\non anatomical regions in the image. We propose a simple yet effective\nregion-guided report generation model that detects anatomical regions and then\ndescribes individual, salient regions to form the final report. While previous\nmethods generate reports without the possibility of human intervention and with\nlimited explainability, our method opens up novel clinical use cases through\nadditional interactive capabilities and introduces a high degree of\ntransparency and explainability. Comprehensive experiments demonstrate our\nmethod's effectiveness in report generation, outperforming previous\nstate-of-the-art models, and highlight its interactive capabilities. The code\nand checkpoints are available at https://github.com/ttanida/rgrg .", "published": "2023-04-17 14:12:09", "link": "http://arxiv.org/abs/2304.08295v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Tool Learning with Foundation Models", "abstract": "Humans possess an extraordinary ability to create and utilize tools, allowing\nthem to overcome physical limitations and explore new frontiers. With the\nadvent of foundation models, AI systems have the potential to be equally adept\nin tool use as humans. This paradigm, i.e., tool learning with foundation\nmodels, combines the strengths of specialized tools and foundation models to\nachieve enhanced accuracy, efficiency, and automation in problem-solving.\nDespite its immense potential, there is still a lack of a comprehensive\nunderstanding of key challenges, opportunities, and future endeavors in this\nfield. To this end, we present a systematic investigation of tool learning in\nthis paper. We first introduce the background of tool learning, including its\ncognitive origins, the paradigm shift of foundation models, and the\ncomplementary roles of tools and models. Then we recapitulate existing tool\nlearning research into tool-augmented and tool-oriented learning. We formulate\na general tool learning framework: starting from understanding the user\ninstruction, models should learn to decompose a complex task into several\nsubtasks, dynamically adjust their plan through reasoning, and effectively\nconquer each sub-task by selecting appropriate tools. We also discuss how to\ntrain models for improved tool-use capabilities and facilitate the\ngeneralization in tool learning. Considering the lack of a systematic tool\nlearning evaluation in prior works, we experiment with 18 representative tools\nand show the potential of current foundation models in skillfully utilizing\ntools. Finally, we discuss several open problems that require further\ninvestigation for tool learning. In general, we hope this paper could inspire\nfuture research in integrating tools with foundation models.", "published": "2023-04-17 15:16:10", "link": "http://arxiv.org/abs/2304.08354v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prak: An automatic phonetic alignment tool for Czech", "abstract": "Labeling speech down to the identity and time boundaries of phones is a\nlabor-intensive part of phonetic research. To simplify this work, we created a\nfree open-source tool generating phone sequences from Czech text and\ntime-aligning them with audio.\n  Low architecture complexity makes the design approachable for students of\nphonetics. Acoustic model ReLU NN with 56k weights was trained using PyTorch on\nsmall CommonVoice data. Alignment and variant selection decoder is implemented\nin Python with matrix library.\n  A Czech pronunciation generator is composed of simple rule-based blocks\ncapturing the logic of the language where possible, allowing modification of\ntranscription approach details.\n  Compared to tools used until now, data preparation efficiency improved, the\ntool is usable on Mac, Linux and Windows in Praat GUI or command line, achieves\nmostly correct pronunciation variant choice including glottal stop detection,\nalgorithmically captures most of Czech assimilation logic and is both didactic\nand practical.", "published": "2023-04-17 16:51:24", "link": "http://arxiv.org/abs/2304.08431v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "C.m"], "primary_category": "cs.CL"}
{"title": "LongForm: Effective Instruction Tuning with Reverse Instructions", "abstract": "Instruction tuning enables language models to more effectively generalize and\nbetter follow user intent. However, obtaining instruction data is costly and\nchallenging. Prior work employs methods such as expensive human annotation,\ncrowd-sourced datasets with alignment issues, and generating noisy examples via\nLLMs. We introduce the LongForm-C dataset, which is created by reverse\ninstructions. We generate instructions via LLMs for human-written corpus\nexamples using reverse instructions. First we select a diverse set of\nhuman-written documents from corpora such as C4 and Wikipedia; then we generate\ninstructions for these documents via LLMs. This approach provides a cheaper and\ncleaner instruction-tuning dataset with natural output and one suitable for\nlong text generation. Our models outperform 10x larger language models without\ninstruction tuning on tasks such as story/recipe generation and long-form\nquestion answering. Moreover, LongForm models outperform prior\ninstruction-tuned models such as FLAN-T5 and Alpaca by a large margin, and\nimprove language understanding capabilities further. We publicly release our\ndata and models: https://github.com/akoksal/LongForm.", "published": "2023-04-17 17:36:35", "link": "http://arxiv.org/abs/2304.08460v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Synthetic Data from Diffusion Models Improves ImageNet Classification", "abstract": "Deep generative models are becoming increasingly powerful, now generating\ndiverse high fidelity photo-realistic samples given text prompts. Have they\nreached the point where models of natural images can be used for generative\ndata augmentation, helping to improve challenging discriminative tasks? We show\nthat large-scale text-to image diffusion models can be fine-tuned to produce\nclass conditional models with SOTA FID (1.76 at 256x256 resolution) and\nInception Score (239 at 256x256). The model also yields a new SOTA in\nClassification Accuracy Scores (64.96 for 256x256 generative samples, improving\nto 69.24 for 1024x1024 samples). Augmenting the ImageNet training set with\nsamples from the resulting models yields significant improvements in ImageNet\nclassification accuracy over strong ResNet and Vision Transformer baselines.", "published": "2023-04-17 17:42:29", "link": "http://arxiv.org/abs/2304.08466v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "DisCo-CLIP: A Distributed Contrastive Loss for Memory Efficient CLIP\n  Training", "abstract": "We propose DisCo-CLIP, a distributed memory-efficient CLIP training approach,\nto reduce the memory consumption of contrastive loss when training contrastive\nlearning models. Our approach decomposes the contrastive loss and its gradient\ncomputation into two parts, one to calculate the intra-GPU gradients and the\nother to compute the inter-GPU gradients. According to our decomposition, only\nthe intra-GPU gradients are computed on the current GPU, while the inter-GPU\ngradients are collected via all_reduce from other GPUs instead of being\nrepeatedly computed on every GPU. In this way, we can reduce the GPU memory\nconsumption of contrastive loss computation from $\\bigO(B^2)$ to\n$\\bigO(\\frac{B^2}{N})$, where $B$ and $N$ are the batch size and the number of\nGPUs used for training. Such a distributed solution is mathematically\nequivalent to the original non-distributed contrastive loss computation,\nwithout sacrificing any computation accuracy. It is particularly efficient for\nlarge-batch CLIP training. For instance, DisCo-CLIP can enable contrastive\ntraining of a ViT-B/32 model with a batch size of 32K or 196K using 8 or 64\nA100 40GB GPUs, compared with the original CLIP solution which requires 128\nA100 40GB GPUs to train a ViT-B/32 model with a batch size of 32K. The code\nwill be released at https://github.com/IDEA-Research/DisCo-CLIP", "published": "2023-04-17 17:58:21", "link": "http://arxiv.org/abs/2304.08480v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Visual Instruction Tuning", "abstract": "Instruction tuning large language models (LLMs) using machine-generated\ninstruction-following data has improved zero-shot capabilities on new tasks,\nbut the idea is less explored in the multimodal field. In this paper, we\npresent the first attempt to use language-only GPT-4 to generate multimodal\nlanguage-image instruction-following data. By instruction tuning on such\ngenerated data, we introduce LLaVA: Large Language and Vision Assistant, an\nend-to-end trained large multimodal model that connects a vision encoder and\nLLM for general-purpose visual and language understanding.Our early experiments\nshow that LLaVA demonstrates impressive multimodel chat abilities, sometimes\nexhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and\nyields a 85.1% relative score compared with GPT-4 on a synthetic multimodal\ninstruction-following dataset. When fine-tuned on Science QA, the synergy of\nLLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make\nGPT-4 generated visual instruction tuning data, our model and code base\npublicly available.", "published": "2023-04-17 17:59:25", "link": "http://arxiv.org/abs/2304.08485v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Bridging Discrete and Backpropagation: Straight-Through and Beyond", "abstract": "Backpropagation, the cornerstone of deep learning, is limited to computing\ngradients for continuous variables. This limitation poses challenges for\nproblems involving discrete latent variables. To address this issue, we propose\na novel approach to approximate the gradient of parameters involved in\ngenerating discrete latent variables. First, we examine the widely used\nStraight-Through (ST) heuristic and demonstrate that it works as a first-order\napproximation of the gradient. Guided by our findings, we propose ReinMax,\nwhich achieves second-order accuracy by integrating Heun's method, a\nsecond-order numerical method for solving ODEs. ReinMax does not require\nHessian or other second-order derivatives, thus having negligible computation\noverheads. Extensive experimental results on various tasks demonstrate the\nsuperiority of ReinMax over the state of the art. Implementations are released\nat https://github.com/microsoft/ReinMax.", "published": "2023-04-17 20:59:49", "link": "http://arxiv.org/abs/2304.08612v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "An Evaluation on Large Language Model Outputs: Discourse and\n  Memorization", "abstract": "We present an empirical evaluation of various outputs generated by nine of\nthe most widely-available large language models (LLMs). Our analysis is done\nwith off-the-shelf, readily-available tools. We find a correlation between\npercentage of memorized text, percentage of unique text, and overall output\nquality, when measured with respect to output pathologies such as\ncounterfactual and logically-flawed statements, and general failures like not\nstaying on topic. Overall, 80.0% of the outputs evaluated contained memorized\ndata, but outputs containing the most memorized content were also more likely\nto be considered of high quality. We discuss and evaluate mitigation\nstrategies, showing that, in the models evaluated, the rate of memorized text\nbeing output is reduced. We conclude with a discussion on potential\nimplications around what it means to learn, to memorize, and to evaluate\nquality text.", "published": "2023-04-17 22:12:12", "link": "http://arxiv.org/abs/2304.08637v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Classification of US Supreme Court Cases using BERT-Based Techniques", "abstract": "Models based on bidirectional encoder representations from transformers\n(BERT) produce state of the art (SOTA) results on many natural language\nprocessing (NLP) tasks such as named entity recognition (NER), part-of-speech\n(POS) tagging etc. An interesting phenomenon occurs when classifying long\ndocuments such as those from the US supreme court where BERT-based models can\nbe considered difficult to use on a first-pass or out-of-the-box basis. In this\npaper, we experiment with several BERT-based classification techniques for US\nsupreme court decisions or supreme court database (SCDB) and compare them with\nthe previous SOTA results. We then compare our results specifically with SOTA\nmodels for long documents. We compare our results for two classification tasks:\n(1) a broad classification task with 15 categories and (2) a fine-grained\nclassification task with 279 categories. Our best result produces an accuracy\nof 80\\% on the 15 broad categories and 60\\% on the fine-grained 279 categories\nwhich marks an improvement of 8\\% and 28\\% respectively from previously\nreported SOTA results.", "published": "2023-04-17 22:53:54", "link": "http://arxiv.org/abs/2304.08649v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Supporting Qualitative Analysis with Large Language Models: Combining\n  Codebook with GPT-3 for Deductive Coding", "abstract": "Qualitative analysis of textual contents unpacks rich and valuable\ninformation by assigning labels to the data. However, this process is often\nlabor-intensive, particularly when working with large datasets. While recent\nAI-based tools demonstrate utility, researchers may not have readily available\nAI resources and expertise, let alone be challenged by the limited\ngeneralizability of those task-specific models. In this study, we explored the\nuse of large language models (LLMs) in supporting deductive coding, a major\ncategory of qualitative analysis where researchers use pre-determined codebooks\nto label the data into a fixed set of codes. Instead of training task-specific\nmodels, a pre-trained LLM could be used directly for various tasks without\nfine-tuning through prompt learning. Using a curiosity-driven questions coding\ntask as a case study, we found, by combining GPT-3 with expert-drafted\ncodebooks, our proposed approach achieved fair to substantial agreements with\nexpert-coded results. We lay out challenges and opportunities in using LLMs to\nsupport qualitative coding and beyond.", "published": "2023-04-17 04:52:43", "link": "http://arxiv.org/abs/2304.10548v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and\n  Dataset", "abstract": "In this paper, we propose a Vision-Audio-Language Omni-peRception pretraining\nmodel (VALOR) for multi-modal understanding and generation. Different from\nwidely-studied vision-language pretraining models, VALOR jointly models\nrelationships of vision, audio and language in an end-to-end manner. It\ncontains three separate encoders for single modality representations, and a\ndecoder for multimodal conditional text generation. We design two pretext tasks\nto pretrain VALOR model, including Multimodal Grouping Alignment (MGA) and\nMultimodal Grouping Captioning (MGC). MGA projects vision, language and audio\nto the same common space, building vision-language, audio-language and\naudiovisual-language alignment simultaneously. MGC learns how to generate text\ntokens in conditions of vision, audio or their both. To promote\nvision-audio-language pretraining research, we construct a large-scale\nhigh-quality tri-modality dataset named VALOR-1M, which contains 1M audiable\nvideos with human annotated audiovisual captions. Extensive experiments show\nthat VALOR can learn strong multimodal correlations and be generalized to\nvarious downstream tasks (e.g., retrieval, captioning and question answering),\nwith different input modalities (e.g., vision-language, audio-language and\naudiovisual-language). VALOR achieves new state-of-the-art performances on\nseries of public cross-modality benchmarks. Code and data are available at\nproject page https://casia-iva-group.github.io/projects/VALOR.", "published": "2023-04-17 15:08:15", "link": "http://arxiv.org/abs/2304.08345v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Audio coding with unified noise shaping and phase contrast control", "abstract": "Over the past decade, audio coding technology has seen standardization and\nthe development of many frameworks incorporated with linear predictive coding\n(LPC). As LPC reduces information in the frequency domain, LP-based\nfrequency-domain noise-shaping (FDNS) was previously proposed. To code\ntransient signals effectively, FDNS with temporal noise shaping (TNS) has\nemerged. However, these mainly operated in the modified discrete cosine\ntransform domain, which essentially accompanies time domain aliasing. In this\npaper, a unified noise-shaping (UNS) framework including FDNS and complex\nLPC-based TNS (CTNS) in the DFT domain is proposed to overcome the aliasing\nissues. Additionally, a modified polar quantizer with phase contrast control is\nproposed, which saves phase bits depending on the frequency envelope\ninformation. The core coding feasibility at low bit rates is verified through\nvarious objective metrics and subjective listening evaluations.", "published": "2023-04-17 08:41:16", "link": "http://arxiv.org/abs/2304.08076v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Neural TTS in French: Comparing Graphemic and Phonetic Inputs Using the\n  SynPaFlex-Corpus and Tacotron2", "abstract": "The SynPaFlex-Corpus is a publicly available TTS-oriented dataset, which\nprovides phonetic transcriptions automatically produced by the JTrans\ntranscriber, with a Phoneme Error Rate (PER) of 6.1%. In this paper, we analyze\ntwo mono-speaker Tacotron2 models trained on graphemic and phonetic inputs,\nprovided by the SynPaFlex-Corpus. Through three subjective listening tests, we\ncompare their pronunciation accuracy, sound quality and naturalness. Our\nresults show significantly better pronunciation accuracy and prosody\nnaturalness for the phoneme-based model, but no significant difference in terms\nof perceived sound quality. They demonstrate that a PER of 6.1% is sufficient\nto enhance pronunciation control by using phonetic transcripts instead of\ngraphemes with 83 hours of recorded French read speech. They suggest that the\nSynPaFlex-Corpus is suitable for pre-training a model in mono-speaker\nfine-tuning approaches.", "published": "2023-04-17 12:31:14", "link": "http://arxiv.org/abs/2304.08209v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Fast Random Approximation of Multi-channel Room Impulse Response", "abstract": "Modern neural-network-based speech processing systems are typically required\nto be robust against reverberation, and the training of such systems thus needs\na large amount of reverberant data. During the training of the systems,\non-the-fly simulation pipeline is nowadays preferred as it allows the model to\ntrain on infinite number of data samples without pre-generating and saving them\non harddisk. An RIR simulation method thus needs to not only generate more\nrealistic artificial room impulse response (RIR) filters, but also generate\nthem in a fast way to accelerate the training process. Existing RIR simulation\ntools have proven effective in a wide range of speech processing tasks and\nneural network architectures, but their usage in on-the-fly simulation pipeline\nremains questionable due to their computational complexity or the quality of\nthe generated RIR filters. In this paper, we propose FRAM-RIR, a fast random\napproximation method of the widely-used image-source method (ISM), to\nefficiently generate realistic multi-channel RIR filters. FRAM-RIR bypasses the\nexplicit calculation of sound propagation paths in ISM-based algorithms by\nrandomly sampling the location and number of reflections of each virtual sound\nsource based on several heuristic assumptions, while still maintains accurate\ndirection-of-arrival (DOA) information of all sound sources. Visualization of\noracle beampatterns and directional features shows that FRAM-RIR can generate\nmore realistic RIR filters than existing widely-used ISM-based tools, and\nexperiment results on multi-channel noisy speech separation and dereverberation\ntasks with a wide range of neural network architectures show that models\ntrained with FRAM-RIR can also achieve on par or better performance on real\nRIRs compared to other RIR simulation tools with a significantly accelerated\ntraining procedure. A Python implementation of FRAM-RIR is released.", "published": "2023-04-17 08:11:39", "link": "http://arxiv.org/abs/2304.08052v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "How Tiny Can Analog Filterbank Features Be Made for Ultra-low-power\n  On-device Keyword Spotting?", "abstract": "Analog feature extraction is a power-efficient and re-emerging signal\nprocessing paradigm for implementing the front-end feature extractor in on\ndevice keyword-spotting systems. Despite its power efficiency and re-emergence,\nthere is little consensus on what values the architectural parameters of its\ncritical block, the analog filterbank, should be set to, even though they\nstrongly influence power consumption. Towards building consensus and\napproaching fundamental power consumption limits, we find via simulation that\nthrough careful selection of its architectural parameters, the power of a\ntypical state-of-the-art analog filterbank could be reduced by 33.6x, while\nsacrificing only 1.8% in downstream 10-word keyword spotting accuracy through a\nback-end neural network.", "published": "2023-04-17 18:15:30", "link": "http://arxiv.org/abs/2304.08541v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Recursive Joint Attention for Audio-Visual Fusion in Regression based\n  Emotion Recognition", "abstract": "In video-based emotion recognition (ER), it is important to effectively\nleverage the complementary relationship among audio (A) and visual (V)\nmodalities, while retaining the intra-modal characteristics of individual\nmodalities. In this paper, a recursive joint attention model is proposed along\nwith long short-term memory (LSTM) modules for the fusion of vocal and facial\nexpressions in regression-based ER. Specifically, we investigated the\npossibility of exploiting the complementary nature of A and V modalities using\na joint cross-attention model in a recursive fashion with LSTMs to capture the\nintra-modal temporal dependencies within the same modalities as well as among\nthe A-V feature representations. By integrating LSTMs with recursive joint\ncross-attention, our model can efficiently leverage both intra- and inter-modal\nrelationships for the fusion of A and V modalities. The results of extensive\nexperiments performed on the challenging Affwild2 and Fatigue (private)\ndatasets indicate that the proposed A-V fusion model can significantly\noutperform state-of-art-methods.", "published": "2023-04-17 02:57:39", "link": "http://arxiv.org/abs/2304.07958v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "DAS-N2N: Machine learning Distributed Acoustic Sensing (DAS) signal\n  denoising without clean data", "abstract": "This article presents a weakly supervised machine learning method, which we\ncall DAS-N2N, for suppressing strong random noise in distributed acoustic\nsensing (DAS) recordings. DAS-N2N requires no manually produced labels (i.e.,\npre-determined examples of clean event signals or sections of noise) for\ntraining and aims to map random noise processes to a chosen summary statistic,\nsuch as the distribution mean, median or mode, whilst retaining the true\nunderlying signal. This is achieved by splicing (joining together) two fibres\nhosted within a single optical cable, recording two noisy copies of the same\nunderlying signal corrupted by different independent realizations of random\nobservational noise. A deep learning model can then be trained using only these\ntwo noisy copies of the data to produce a near fully-denoised copy. Once the\nmodel is trained, only noisy data from a single fibre is required. Using a\ndataset from a DAS array deployed on the surface of the Rutford Ice Stream in\nAntarctica, we demonstrate that DAS-N2N greatly suppresses incoherent noise and\nenhances the signal-to-noise ratios (SNR) of natural microseismic icequake\nevents. We further show that this approach is inherently more efficient and\neffective than standard stop/pass band and white noise (e.g., Wiener) filtering\nroutines, as well as a comparable self-supervised learning method based on\nmasking individual DAS channels. Our preferred model for this task is\nlightweight, processing 30 seconds of data recorded at a sampling frequency of\n1000 Hz over 985 channels (approx. 1 km of fiber) in $<$1 s. Due to the high\nnoise levels in DAS recordings, efficient data-driven denoising methods, such\nas DAS-N2N, will prove essential to time-critical DAS earthquake detection,\nparticularly in the case of microseismic monitoring.", "published": "2023-04-17 09:58:52", "link": "http://arxiv.org/abs/2304.08120v2", "categories": ["physics.geo-ph", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "physics.geo-ph"}
{"title": "Physics-inspired Neuroacoustic Computing Based on Tunable Nonlinear\n  Multiple-scattering", "abstract": "Waves, such as light and sound, inherently bounce and mix due to multiple\nscattering induced by the complex material objects that surround us. This\nscattering process severely scrambles the information carried by waves,\nchallenging conventional communication systems, sensing paradigms, and\nwave-based computing schemes. Here, we show that instead of being a hindrance,\nmultiple scattering can be beneficial to enable and enhance analog nonlinear\ninformation mapping, allowing for the direct physical implementation of\ncomputational paradigms such as reservoir computing and extreme learning\nmachines. We propose a physics-inspired version of such computational\narchitectures for speech and vowel recognition that operate directly in the\nnative domain of the input signal, namely on real-sounds, without any digital\npre-processing or encoding conversion and backpropagation training computation.\nWe first implement it in a proof-of-concept prototype, a nonlinear chaotic\nacoustic cavity containing multiple tunable and power-efficient nonlinear\nmeta-scatterers. We prove the efficiency of the acoustic-based computing system\nfor vowel recognition tasks with high testing classification accuracy (91.4%).\nFinally, we demonstrate the high performance of vowel recognition in the\nnatural environment of a reverberation room. Our results open the way for\nefficient acoustic learning machines that operate directly on the input sound,\nand leverage physics to enable Natural Language Processing (NLP).", "published": "2023-04-17 15:48:55", "link": "http://arxiv.org/abs/2304.08380v1", "categories": ["cs.SD", "eess.AS", "physics.app-ph"], "primary_category": "cs.SD"}
{"title": "Conditional Generation of Audio from Video via Foley Analogies", "abstract": "The sound effects that designers add to videos are designed to convey a\nparticular artistic effect and, thus, may be quite different from a scene's\ntrue sound. Inspired by the challenges of creating a soundtrack for a video\nthat differs from its true sound, but that nonetheless matches the actions\noccurring on screen, we propose the problem of conditional Foley. We present\nthe following contributions to address this problem. First, we propose a\npretext task for training our model to predict sound for an input video clip\nusing a conditional audio-visual clip sampled from another time within the same\nsource video. Second, we propose a model for generating a soundtrack for a\nsilent input video, given a user-supplied example that specifies what the video\nshould \"sound like\". We show through human studies and automated evaluation\nmetrics that our model successfully generates sound from video, while varying\nits output according to the content of a supplied example. Project site:\nhttps://xypb.github.io/CondFoleyGen/", "published": "2023-04-17 17:59:45", "link": "http://arxiv.org/abs/2304.08490v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
