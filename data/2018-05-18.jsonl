{"title": "Aspect Based Sentiment Analysis with Gated Convolutional Networks", "abstract": "Aspect based sentiment analysis (ABSA) can provide more detailed information\nthan general sentiment analysis, because it aims to predict the sentiment\npolarities of the given aspects or entities in text. We summarize previous\napproaches into two subtasks: aspect-category sentiment analysis (ACSA) and\naspect-term sentiment analysis (ATSA). Most previous approaches employ long\nshort-term memory and attention mechanisms to predict the sentiment polarity of\nthe concerned targets, which are often complicated and need more training time.\nWe propose a model based on convolutional neural networks and gating\nmechanisms, which is more accurate and efficient. First, the novel Gated\nTanh-ReLU Units can selectively output the sentiment features according to the\ngiven aspect or entity. The architecture is much simpler than attention layer\nused in the existing models. Second, the computations of our model could be\neasily parallelized during training, because convolutional layers do not have\ntime dependency as in LSTM layers, and gating units also work independently.\nThe experiments on SemEval datasets demonstrate the efficiency and\neffectiveness of our models.", "published": "2018-05-18 04:24:52", "link": "http://arxiv.org/abs/1805.07043v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SNU_IDS at SemEval-2018 Task 12: Sentence Encoder with Contextualized\n  Vectors for Argument Reasoning Comprehension", "abstract": "We present a novel neural architecture for the Argument Reasoning\nComprehension task of SemEval 2018. It is a simple neural network consisting of\nthree parts, collectively judging whether the logic built on a set of given\nsentences (a claim, reason, and warrant) is plausible or not. The model\nutilizes contextualized word vectors pre-trained on large machine translation\n(MT) datasets as a form of transfer learning, which can help to mitigate the\nlack of training data. Quantitative analysis shows that simply leveraging LSTMs\ntrained on MT datasets outperforms several baselines and non-transferred\nmodels, achieving accuracies of about 70% on the development set and about 60%\non the test set.", "published": "2018-05-18 05:19:49", "link": "http://arxiv.org/abs/1805.07049v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Advanced Methods in Japanese-Vietnamese Neural Machine\n  Translation", "abstract": "Neural machine translation (NMT) systems have recently obtained state-of-the\nart in many machine translation systems between popular language pairs because\nof the availability of data. For low-resourced language pairs, there are few\nresearches in this field due to the lack of bilingual data. In this paper, we\nattempt to build the first NMT systems for a low-resourced language\npairs:Japanese-Vietnamese. We have also shown significant improvements when\ncombining advanced methods to reduce the adverse impacts of data sparsity and\nimprove the quality of NMT systems. In addition, we proposed a variant of\nByte-Pair Encoding algorithm to perform effective word segmentation for\nVietnamese texts and alleviate the rare-word problem that persists in NMT\nsystems.", "published": "2018-05-18 10:36:37", "link": "http://arxiv.org/abs/1805.07133v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Style Obfuscation by Invariance", "abstract": "The task of obfuscating writing style using sequence models has previously\nbeen investigated under the framework of obfuscation-by-transfer, where the\ninput text is explicitly rewritten in another style. These approaches also\noften lead to major alterations to the semantic content of the input. In this\nwork, we propose obfuscation-by-invariance, and investigate to what extent\nmodels trained to be explicitly style-invariant preserve semantics. We evaluate\nour architectures on parallel and non-parallel corpora, and compare automatic\nand human evaluations on the obfuscated sentences. Our experiments show that\nstyle classifier performance can be reduced to chance level, whilst the\nautomatic evaluation of the output is seemingly equal to models applying\nstyle-transfer. However, based on human evaluation we demonstrate a trade-off\nbetween the level of obfuscation and the observed quality of the output in\nterms of meaning preservation and grammaticality.", "published": "2018-05-18 11:09:28", "link": "http://arxiv.org/abs/1805.07143v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Study on Dialog Act Recognition using Character-Level Tokenization", "abstract": "Dialog act recognition is an important step for dialog systems since it\nreveals the intention behind the uttered words. Most approaches on the task use\nword-level tokenization. In contrast, this paper explores the use of\ncharacter-level tokenization. This is relevant since there is information at\nthe sub-word level that is related to the function of the words and, thus,\ntheir intention. We also explore the use of different context windows around\neach token, which are able to capture important elements, such as affixes.\nFurthermore, we assess the importance of punctuation and capitalization. We\nperformed experiments on both the Switchboard Dialog Act Corpus and the DIHANA\nCorpus. In both cases, the experiments not only show that character-level\ntokenization leads to better performance than the typical word-level\napproaches, but also that both approaches are able to capture complementary\ninformation. Thus, the best results are achieved by combining tokenization at\nboth levels.", "published": "2018-05-18 14:17:07", "link": "http://arxiv.org/abs/1805.07231v2", "categories": ["cs.CL", "H.1.2; H.3.1; I.2.7"], "primary_category": "cs.CL"}
{"title": "Metric for Automatic Machine Translation Evaluation based on Universal\n  Sentence Representations", "abstract": "Sentence representations can capture a wide range of information that cannot\nbe captured by local features based on character or word N-grams. This paper\nexamines the usefulness of universal sentence representations for evaluating\nthe quality of machine translation. Although it is difficult to train sentence\nrepresentations using small-scale translation datasets with manual evaluation,\nsentence representations trained from large-scale data in other tasks can\nimprove the automatic evaluation of machine translation. Experimental results\nof the WMT-2016 dataset show that the proposed method achieves state-of-the-art\nperformance with sentence representation features only.", "published": "2018-05-18 23:05:18", "link": "http://arxiv.org/abs/1805.07469v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Handling of Polysemy via Sparse Representations", "abstract": "Words are polysemous and multi-faceted, with many shades of meanings. We\nsuggest that sparse distributed representations are more suitable than other,\ncommonly used, (dense) representations to express these multiple facets, and\npresent Category Builder, a working system that, as we show, makes use of\nsparse representations to support multi-faceted lexical representations. We\nargue that the set expansion task is well suited to study these meaning\ndistinctions since a word may belong to multiple sets with a different reason\nfor membership in each. We therefore exhibit the performance of Category\nBuilder on this task, while showing that our representation captures at the\nsame time analogy problems such as \"the Ganga of Egypt\" or \"the Voldemort of\nTolkien\". Category Builder is shown to be a more expressive lexical\nrepresentation and to outperform dense representations such as Word2Vec in some\nanalogy classes despite being shown only two of the three input terms.", "published": "2018-05-18 18:58:38", "link": "http://arxiv.org/abs/1805.07398v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A syllogistic system for propositions with intermediate quantifiers", "abstract": "This paper describes a formalism that subsumes Peterson's intermediate\nquantifier syllogistic system, and extends the ideas by van Eijck on\nAristotle's logic. Syllogisms are expressed in a concise form making use of and\nextending the Monotonicity Calculus. Contradictory and contrary relationships\nare added so that deduction can derive propositions expressing a form of\nnegation.", "published": "2018-05-18 19:33:04", "link": "http://arxiv.org/abs/1805.08707v1", "categories": ["cs.LO", "cs.CL"], "primary_category": "cs.LO"}
{"title": "Gated Recurrent Unit Based Acoustic Modeling with Future Context", "abstract": "The use of future contextual information is typically shown to be helpful for\nacoustic modeling. However, for the recurrent neural network (RNN), it's not so\neasy to model the future temporal context effectively, meanwhile keep lower\nmodel latency. In this paper, we attempt to design a RNN acoustic model that\nbeing capable of utilizing the future context effectively and directly, with\nthe model latency and computation cost as low as possible. The proposed model\nis based on the minimal gated recurrent unit (mGRU) with an input projection\nlayer inserted in it. Two context modules, temporal encoding and temporal\nconvolution, are specifically designed for this architecture to model the\nfuture context. Experimental results on the Switchboard task and an internal\nMandarin ASR task show that, the proposed model performs much better than long\nshort-term memory (LSTM) and mGRU models, whereas enables online decoding with\na maximum latency of 170 ms. This model even outperforms a very strong\nbaseline, TDNN-LSTM, with smaller model latency and almost half less\nparameters.", "published": "2018-05-18 02:33:53", "link": "http://arxiv.org/abs/1805.07024v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Improved Sentence Modeling using Suffix Bidirectional LSTM", "abstract": "Recurrent neural networks have become ubiquitous in computing representations\nof sequential data, especially textual data in natural language processing. In\nparticular, Bidirectional LSTMs are at the heart of several neural models\nachieving state-of-the-art performance in a wide variety of tasks in NLP.\nHowever, BiLSTMs are known to suffer from sequential bias - the contextual\nrepresentation of a token is heavily influenced by tokens close to it in a\nsentence. We propose a general and effective improvement to the BiLSTM model\nwhich encodes each suffix and prefix of a sequence of tokens in both forward\nand reverse directions. We call our model Suffix Bidirectional LSTM or\nSuBiLSTM. This introduces an alternate bias that favors long range\ndependencies. We apply SuBiLSTMs to several tasks that require sentence\nmodeling. We demonstrate that using SuBiLSTM instead of a BiLSTM in existing\nmodels leads to improvements in performance in learning general sentence\nrepresentations, text classification, textual entailment and paraphrase\ndetection. Using SuBiLSTM we achieve new state-of-the-art results for\nfine-grained sentiment classification and question classification.", "published": "2018-05-18 17:46:25", "link": "http://arxiv.org/abs/1805.07340v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Multi-view Sentence Representation Learning", "abstract": "Multi-view learning can provide self-supervision when different views are\navailable of the same data. The distributional hypothesis provides another form\nof useful self-supervision from adjacent sentences which are plentiful in large\nunlabelled corpora. Motivated by the asymmetry in the two hemispheres of the\nhuman brain as well as the observation that different learning architectures\ntend to emphasise different aspects of sentence meaning, we create a unified\nmulti-view sentence representation learning framework, in which, one view\nencodes the input sentence with a Recurrent Neural Network (RNN), and the other\nview encodes it with a simple linear model, and the training objective is to\nmaximise the agreement specified by the adjacent context information between\ntwo views. We show that, after training, the vectors produced from our\nmulti-view training provide improved representations over the single-view\ntraining, and the combination of different views gives further representational\nimprovement and demonstrates solid transferability on standard downstream\ntasks.", "published": "2018-05-18 21:04:08", "link": "http://arxiv.org/abs/1805.07443v1", "categories": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Unsupervised Cross-Modal Alignment of Speech and Text Embedding Spaces", "abstract": "Recent research has shown that word embedding spaces learned from text\ncorpora of different languages can be aligned without any parallel data\nsupervision. Inspired by the success in unsupervised cross-lingual word\nembeddings, in this paper we target learning a cross-modal alignment between\nthe embedding spaces of speech and text learned from corpora of their\nrespective modalities in an unsupervised fashion. The proposed framework learns\nthe individual speech and text embedding spaces, and attempts to align the two\nspaces via adversarial training, followed by a refinement procedure. We show\nhow our framework could be used to perform spoken word classification and\ntranslation, and the results on these two tasks demonstrate that the\nperformance of our unsupervised alignment approach is comparable to its\nsupervised counterpart. Our framework is especially useful for developing\nautomatic speech recognition (ASR) and speech-to-text translation systems for\nlow- or zero-resource languages, which have little parallel audio-text data for\ntraining modern supervised ASR and speech-to-text translation models, but\naccount for the majority of the languages spoken across the world.", "published": "2018-05-18 22:59:18", "link": "http://arxiv.org/abs/1805.07467v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Learning to Repair Software Vulnerabilities with Generative Adversarial\n  Networks", "abstract": "Motivated by the problem of automated repair of software vulnerabilities, we\npropose an adversarial learning approach that maps from one discrete source\ndomain to another target domain without requiring paired labeled examples or\nsource and target domains to be bijections. We demonstrate that the proposed\nadversarial learning approach is an effective technique for repairing software\nvulnerabilities, performing close to seq2seq approaches that require labeled\npairs. The proposed Generative Adversarial Network approach is\napplication-agnostic in that it can be applied to other problems similar to\ncode repair, such as grammar correction or sentiment translation.", "published": "2018-05-18 23:31:03", "link": "http://arxiv.org/abs/1805.07475v3", "categories": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
