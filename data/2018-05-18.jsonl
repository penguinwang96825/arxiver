{"title": "Hierarchical Reinforcement Learning with Deep Nested Agents", "abstract": "Deep hierarchical reinforcement learning has gained a lot of attention in recent years due to its ability to produce state-of-the-art results in challenging environments where non-hierarchical frameworks fail to learn useful policies. However, as problem domains become more complex, deep hierarchical reinforcement learning can become inefficient, leading to longer convergence times and poor performance. We introduce the Deep Nested Agent framework, which is a variant of deep hierarchical reinforcement learning where information from the main agent is propagated to the low level $nested$ agent by incorporating this information into the nested agent's state. We demonstrate the effectiveness and performance of the Deep Nested Agent framework by applying it to three scenarios in Minecraft with comparisons to a deep non-hierarchical single agent framework, as well as, a deep hierarchical framework.", "published": "2018-05-18 01:06:36", "link": "http://arxiv.org/abs/1805.07008v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Neural Architecture Search using Deep Neural Networks and Monte Carlo Tree Search", "abstract": "Neural Architecture Search (NAS) has shown great success in automating the design of neural networks, but the prohibitive amount of computations behind current NAS methods requires further investigations in improving the sample efficiency and the network evaluation cost to get better results in a shorter time. In this paper, we present a novel scalable Monte Carlo Tree Search (MCTS) based NAS agent, named AlphaX, to tackle these two aspects. AlphaX improves the search efficiency by adaptively balancing the exploration and exploitation at the state level, and by a Meta-Deep Neural Network (DNN) to predict network accuracies for biasing the search toward a promising region. To amortize the network evaluation cost, AlphaX accelerates MCTS rollouts with a distributed design and reduces the number of epochs in evaluating a network by transfer learning, which is guided with the tree structure in MCTS. In 12 GPU days and 1000 samples, AlphaX found an architecture that reaches 97.84\\% top-1 accuracy on CIFAR-10, and 75.5\\% top-1 accuracy on ImageNet, exceeding SOTA NAS methods in both the accuracy and sampling efficiency. Particularly, we also evaluate AlphaX on NASBench-101, a large scale NAS dataset; AlphaX is 3x and 2.8x more sample efficient than Random Search and Regularized Evolution in finding the global optimum. Finally, we show the searched architecture improves a variety of vision applications from Neural Style Transfer, to Image Captioning and Object Detection.", "published": "2018-05-18 20:57:41", "link": "http://arxiv.org/abs/1805.07440v5", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Positive and Unlabeled Learning through Negative Selection and Imbalance-aware Classification", "abstract": "Motivated by applications in protein function prediction, we consider a challenging supervised classification setting in which positive labels are scarce and there are no explicit negative labels. The learning algorithm must thus select which unlabeled examples to use as negative training points, possibly ending up with an unbalanced learning problem. We address these issues by proposing an algorithm that combines active learning (for selecting negative examples) with imbalance-aware learning (for mitigating the label imbalance). In our experiments we observe that these two techniques operate synergistically, outperforming state-of-the-art methods on standard protein function prediction benchmarks.", "published": "2018-05-18 17:14:23", "link": "http://arxiv.org/abs/1805.07331v2", "categories": ["cs.LG", "q-bio.QM", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Trusted Neural Networks for Safety-Constrained Autonomous Control", "abstract": "We propose Trusted Neural Network (TNN) models, which are deep neural network models that satisfy safety constraints critical to the application domain. We investigate different mechanisms for incorporating rule-based knowledge in the form of first-order logic constraints into a TNN model, where rules that encode safety are accompanied by weights indicating their relative importance. This framework allows the TNN model to learn from knowledge available in form of data as well as logical rules. We propose multiple approaches for solving this problem: (a) a multi-headed model structure that allows trade-off between satisfying logical constraints and fitting training data in a unified training framework, and (b) creating a constrained optimization problem and solving it in dual formulation by posing a new constrained loss function and using a proximal gradient descent algorithm. We demonstrate the efficacy of our TNN framework through experiments using the open-source TORCS~\\cite{BernhardCAA15} 3D simulator for self-driving cars. Experiments using our first approach of a multi-headed TNN model, on a dataset generated by a customized version of TORCS, show that (1) adding safety constraints to a neural network model results in increased performance and safety, and (2) the improvement increases with increasing importance of the safety constraints. Experiments were also performed using the second approach of proximal algorithm for constrained optimization --- they demonstrate how the proposed method ensures that (1) the overall TNN model satisfies the constraints even when the training data violates some of the constraints, and (2) the proximal gradient descent algorithm on the constrained objective converges faster than the unconstrained version.", "published": "2018-05-18 07:17:15", "link": "http://arxiv.org/abs/1805.07075v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Tropical Geometry of Deep Neural Networks", "abstract": "We establish, for the first time, connections between feedforward neural networks with ReLU activation and tropical geometry --- we show that the family of such neural networks is equivalent to the family of tropical rational maps. Among other things, we deduce that feedforward ReLU neural networks with one hidden layer can be characterized by zonotopes, which serve as building blocks for deeper networks; we relate decision boundaries of such neural networks to tropical hypersurfaces, a major object of study in tropical geometry; and we prove that linear regions of such neural networks correspond to vertices of polytopes associated with tropical rational functions. An insight from our tropical formulation is that a deeper network is exponentially more expressive than a shallow network.", "published": "2018-05-18 08:30:50", "link": "http://arxiv.org/abs/1805.07091v1", "categories": ["cs.LG", "math.AG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Tree Edit Distance Learning via Adaptive Symbol Embeddings: Supplementary Materials and Results", "abstract": "Metric learning has the aim to improve classification accuracy by learning a distance measure which brings data points from the same class closer together and pushes data points from different classes further apart. Recent research has demonstrated that metric learning approaches can also be applied to trees, such as molecular structures, abstract syntax trees of computer programs, or syntax trees of natural language, by learning the cost function of an edit distance, i.e. the costs of replacing, deleting, or inserting nodes in a tree. However, learning such costs directly may yield an edit distance which violates metric axioms, is challenging to interpret, and may not generalize well. In this contribution, we propose a novel metric learning approach for trees which learns an edit distance indirectly by embedding the tree nodes as vectors, such that the Euclidean distance between those vectors supports class discrimination. We learn such embeddings by reducing the distance to prototypical trees from the same class and increasing the distance to prototypical trees from different classes. In our experiments, we show that our proposed metric learning approach improves upon the state-of-the-art in metric learning for trees on six benchmark data sets, ranging from computer science over biomedical data to a natural-language processing data set containing over 300,000 nodes.", "published": "2018-05-18 10:09:41", "link": "http://arxiv.org/abs/1805.07123v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "No-arbitrage implies power-law market impact and rough volatility", "abstract": "Market impact is the link between the volume of a (large) order and the price move during and after the execution of this order. We show that under no-arbitrage assumption, the market impact function can only be of power-law type. Furthermore, we prove that this implies that the macroscopic price is diffusive with rough volatility, with a one-to-one correspondence between the exponent of the impact function and the Hurst parameter of the volatility. Hence we simply explain the universal rough behavior of the volatility as a consequence of the no-arbitrage property. From a mathematical viewpoint, our study relies in particular on new results about hyper-rough stochastic Volterra equations.", "published": "2018-05-18 10:46:35", "link": "http://arxiv.org/abs/1805.07134v1", "categories": ["q-fin.ST", "q-fin.MF", "q-fin.TR"], "primary_category": "q-fin.ST"}
{"title": "DVAE#: Discrete Variational Autoencoders with Relaxed Boltzmann Priors", "abstract": "Boltzmann machines are powerful distributions that have been shown to be an effective prior over binary latent variables in variational autoencoders (VAEs). However, previous methods for training discrete VAEs have used the evidence lower bound and not the tighter importance-weighted bound. We propose two approaches for relaxing Boltzmann machines to continuous distributions that permit training with importance-weighted bounds. These relaxations are based on generalized overlapping transformations and the Gaussian integral trick. Experiments on the MNIST and OMNIGLOT datasets show that these relaxations outperform previous discrete VAEs with Boltzmann priors. An implementation which reproduces these results is available at https://github.com/QuadrantAI/dvae .", "published": "2018-05-18 21:11:58", "link": "http://arxiv.org/abs/1805.07445v4", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Sequential Neural Likelihood: Fast Likelihood-free Inference with Autoregressive Flows", "abstract": "We present Sequential Neural Likelihood (SNL), a new method for Bayesian inference in simulator models, where the likelihood is intractable but simulating data from the model is possible. SNL trains an autoregressive flow on simulated data in order to learn a model of the likelihood in the region of high posterior density. A sequential training procedure guides simulations and reduces simulation cost by orders of magnitude. We show that SNL is more robust, more accurate and requires less tuning than related neural-based methods, and we discuss diagnostics for assessing calibration, convergence and goodness-of-fit.", "published": "2018-05-18 14:06:23", "link": "http://arxiv.org/abs/1805.07226v2", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Accurate Kernel Learning for Linear Gaussian Markov Processes using a Scalable Likelihood Computation", "abstract": "We report an exact likelihood computation for Linear Gaussian Markov processes that is more scalable than existing algorithms for complex models and sparsely sampled signals. Better scaling is achieved through elimination of repeated computations in the Kalman likelihood, and by using the diagonalized form of the state transition equation. Using this efficient computation, we study the accuracy of kernel learning using maximum likelihood and the posterior mean in a simulation experiment. The posterior mean with a reference prior is more accurate for complex models and sparse sampling. Because of its lower computation load, the maximum likelihood estimator is an attractive option for more densely sampled signals and lower order models. We confirm estimator behavior in experimental data through their application to speleothem data.", "published": "2018-05-18 17:56:58", "link": "http://arxiv.org/abs/1805.07346v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Knowledge Discovery from Layered Neural Networks based on Non-negative Task Decomposition", "abstract": "Interpretability has become an important issue in the machine learning field, along with the success of layered neural networks in various practical tasks. Since a trained layered neural network consists of a complex nonlinear relationship between large number of parameters, we failed to understand how they could achieve input-output mappings with a given data set. In this paper, we propose the non-negative task decomposition method, which applies non-negative matrix factorization to a trained layered neural network. This enables us to decompose the inference mechanism of a trained layered neural network into multiple principal tasks of input-output mapping, and reveal the roles of hidden units in terms of their contribution to each principal task.", "published": "2018-05-18 10:55:21", "link": "http://arxiv.org/abs/1805.07137v2", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
