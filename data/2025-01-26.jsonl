{"title": "A multi-factor model for improved commodity pricing: Calibration and an application to the oil market", "abstract": "We present a new model for commodity pricing that enhances accuracy by\nintegrating four distinct risk factors: spot price, stochastic volatility,\nconvenience yield, and stochastic interest rates. While the influence of these\nfour variables on commodity futures prices is well recognized, their combined\neffect has not been addressed in the existing literature. We fill this gap by\nproposing a model that effectively captures key stylized facts including a\ndynamic correlation structure and time-varying risk premiums. Using a Kalman\nfilter-based framework, we achieve simultaneous estimation of parameters while\nfiltering state variables through the joint term structure of futures prices\nand bond yields. We perform an empirical analysis focusing on crude oil\nfutures, where we benchmark our model against established approaches. The\nresults demonstrate that the proposed four-factor model effectively captures\nthe complexities of futures term structures and outperforms existing models.", "published": "2025-01-26 16:49:19", "link": "http://arxiv.org/abs/2501.15596v1", "categories": ["q-fin.ST"], "primary_category": "q-fin.ST"}
{"title": "Qwen2.5-1M Technical Report", "abstract": "We introduce Qwen2.5-1M, a series of models that extend the context length to\n1 million tokens. Compared to the previous 128K version, the Qwen2.5-1M series\nhave significantly enhanced long-context capabilities through long-context\npre-training and post-training. Key techniques such as long data synthesis,\nprogressive pre-training, and multi-stage supervised fine-tuning are employed\nto effectively enhance long-context performance while reducing training costs.\n  To promote the use of long-context models among a broader user base, we\npresent and open-source our inference framework. This framework includes a\nlength extrapolation method that can expand the model context lengths by at\nleast four times, or even more, without additional training. To reduce\ninference costs, we implement a sparse attention method along with chunked\nprefill optimization for deployment scenarios and a sparsity refinement method\nto improve precision. Additionally, we detail our optimizations in the\ninference engine, including kernel optimization, pipeline parallelism, and\nscheduling optimization, which significantly enhance overall inference\nperformance. By leveraging our inference framework, the Qwen2.5-1M models\nachieve a remarkable 3x to 7x prefill speedup in scenarios with 1 million\ntokens of context. This framework provides an efficient and powerful solution\nfor developing applications that require long-context processing using\nopen-source models.\n  The Qwen2.5-1M series currently includes the open-source models\nQwen2.5-7B-Instruct-1M and Qwen2.5-14B-Instruct-1M, as well as the API-accessed\nmodel Qwen2.5-Turbo. Evaluations show that Qwen2.5-1M models have been greatly\nimproved in long-context tasks without compromising performance in\nshort-context scenarios. Specifically, the Qwen2.5-14B-Instruct-1M model\nsignificantly outperforms GPT-4o-mini in long-context tasks and supports\ncontexts eight times longer.", "published": "2025-01-26 03:47:25", "link": "http://arxiv.org/abs/2501.15383v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Green are Neural Language Models? Analyzing Energy Consumption in\n  Text Summarization Fine-tuning", "abstract": "Artificial intelligence systems significantly impact the environment,\nparticularly in natural language processing (NLP) tasks. These tasks often\nrequire extensive computational resources to train deep neural networks,\nincluding large-scale language models containing billions of parameters. This\nstudy analyzes the trade-offs between energy consumption and performance across\nthree neural language models: two pre-trained models (T5-base and BART-base),\nand one large language model (LLaMA-3-8B). These models were fine-tuned for the\ntext summarization task, focusing on generating research paper highlights that\nencapsulate the core themes of each paper. The carbon footprint associated with\nfine-tuning each model was measured, offering a comprehensive assessment of\ntheir environmental impact. It is observed that LLaMA-3-8B produces the largest\ncarbon footprint among the three models. A wide range of evaluation metrics,\nincluding ROUGE, METEOR, MoverScore, BERTScore, and SciBERTScore, were employed\nto assess the performance of the models on the given task. This research\nunderscores the importance of incorporating environmental considerations into\nthe design and implementation of neural language models and calls for the\nadvancement of energy-efficient AI methodologies.", "published": "2025-01-26 04:37:27", "link": "http://arxiv.org/abs/2501.15398v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OpenCharacter: Training Customizable Role-Playing LLMs with Large-Scale\n  Synthetic Personas", "abstract": "Customizable role-playing in large language models (LLMs), also known as\ncharacter generalization, is gaining increasing attention for its versatility\nand cost-efficiency in developing and deploying role-playing dialogue agents.\nThis study explores a large-scale data synthesis approach to equip LLMs with\ncharacter generalization capabilities. We begin by synthesizing large-scale\ncharacter profiles using personas from Persona Hub and then explore two\nstrategies: response rewriting and response generation, to create\ncharacter-aligned instructional responses. To validate the effectiveness of our\nsynthetic instruction tuning data for character generalization, we perform\nsupervised fine-tuning (SFT) using the LLaMA-3 8B model. Our best-performing\nmodel strengthens the original LLaMA-3 8B Instruct model and achieves\nperformance comparable to GPT-4o models on role-playing dialogue. We release\nour synthetic characters and instruction-tuning dialogues to support public\nresearch.", "published": "2025-01-26 07:07:01", "link": "http://arxiv.org/abs/2501.15427v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Simple Debiasing Techniques in RoBERTa-based Hate Speech\n  Detection Models", "abstract": "The hate speech detection task is known to suffer from bias against African\nAmerican English (AAE) dialect text, due to the annotation bias present in the\nunderlying hate speech datasets used to train these models. This leads to a\ndisparity where normal AAE text is more likely to be misclassified as\nabusive/hateful compared to non-AAE text. Simple debiasing techniques have been\ndeveloped in the past to counter this sort of disparity, and in this work, we\napply and evaluate these techniques in the scope of RoBERTa-based encoders.\nExperimental results suggest that the success of these techniques depends\nheavily on the methods used for training dataset construction, but with proper\nconsideration of representation bias, they can reduce the disparity seen among\ndialect subgroups on the hate speech detection task.", "published": "2025-01-26 07:18:51", "link": "http://arxiv.org/abs/2501.15430v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "STATE ToxiCN: A Benchmark for Span-level Target-Aware Toxicity\n  Extraction in Chinese Hate Speech Detection", "abstract": "The proliferation of hate speech has caused significant harm to society. The\nintensity and directionality of hate are closely tied to the target and\nargument it is associated with. However, research on hate speech detection in\nChinese has lagged behind, and existing datasets lack span-level fine-grained\nannotations. Furthermore, the lack of research on Chinese hateful slang poses a\nsignificant challenge. In this paper, we provide a solution for fine-grained\ndetection of Chinese hate speech. First, we construct a dataset containing\nTarget-Argument-Hateful-Group quadruples (STATE ToxiCN), which is the first\nspan-level Chinese hate speech dataset. Secondly, we evaluate the span-level\nhate speech detection performance of existing models using STATE ToxiCN.\nFinally, we conduct the first study on Chinese hateful slang and evaluate the\nability of LLMs to detect such expressions. Our work contributes valuable\nresources and insights to advance span-level hate speech detection in Chinese.", "published": "2025-01-26 08:45:37", "link": "http://arxiv.org/abs/2501.15451v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data-adaptive Safety Rules for Training Reward Models", "abstract": "Reinforcement Learning from Human Feedback (RLHF) is commonly employed to\ntailor models to human preferences, especially to improve the safety of outputs\nfrom large language models (LLMs). Traditionally, this method depends on\nselecting preferred responses from pairs. However, due to the variability in\nhuman opinions and the challenges in directly comparing two responses, there is\nan increasing trend towards fine-grained annotation approaches that evaluate\nresponses using multiple targeted metrics or rules. The challenge lies in\nefficiently choosing and applying these rules to handle the diverse range of\npreference data. In this paper, we propose a dynamic method that adaptively\nselects the most important rules for each response pair. We introduce a\nmathematical framework that utilizes the maximum discrepancy across paired\nresponses and demonstrate theoretically that this approach maximizes the mutual\ninformation between the rule-based annotations and the underlying true\npreferences. We then train an 8B reward model using this adaptively labeled\npreference dataset and assess its efficacy using RewardBench. As of January 25,\n2025, our model achieved the highest safety performance on the leaderboard,\nsurpassing various larger models.", "published": "2025-01-26 08:49:46", "link": "http://arxiv.org/abs/2501.15453v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Query-based versus resource-based cache strategies in tag-based browsing\n  systems", "abstract": "Tag-based browsing is a popular interaction model for navigating digital\nlibraries. According to this model, users select descriptive tags to filter\nresources in the collections. Typical implementations of the model are based on\ninverted indexes. However, these implementations can require a considerable\namount of set operations to update the browsing state. To palliate this\ninconven-ience, it is possible to adopt suitable cache strategies. In this\npaper we describe and compare two of these strategies: (i) a query-based\nstrategy, according to which previously computed browsing states are indexed by\nsets of selected tags; and (ii) a resource-based strategy, according to which\nbrowsing states are in-dexed by sets of filtered resources. Our comparison\nfocused on runtime perfor-mance, and was carried out empirically, using a\nreal-world web-based collec-tion in the field of digital humanities. The\nresults obtained show that the re-source-based strategy clearly outperforms the\nquery-based one.", "published": "2025-01-26 11:01:10", "link": "http://arxiv.org/abs/2501.15481v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilevel Browsing of Folksonomy-Based Digital Collections", "abstract": "This paper describes how to extend the usual one-level tag selection\nnavigation paradigm in folksonomy-based digital collections to a multilevel\nbrowsing one, according to which it is possible to incrementally narrow down\nthe set of selected objects in a collection by sequentially adding more and\nmore filtering tags. For this purpose, we present a browsing strategy based on\nfinite automata. Also, we provide some experimental results concerning the\napplication of the approach in Clavy, a system for managing digital collections\nwith reconfigurable structures in digital humanities and educational settings.", "published": "2025-01-26 11:18:04", "link": "http://arxiv.org/abs/2501.15487v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer", "abstract": "As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\n\\href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside},\n\\href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}.", "published": "2025-01-26 15:56:56", "link": "http://arxiv.org/abs/2501.15570v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Cultural Fashion Design via Interactive Large Language Models and\n  Diffusion Models", "abstract": "Fashion content generation is an emerging area at the intersection of\nartificial intelligence and creative design, with applications ranging from\nvirtual try-on to culturally diverse design prototyping. Existing methods often\nstruggle with cultural bias, limited scalability, and alignment between textual\nprompts and generated visuals, particularly under weak supervision. In this\nwork, we propose a novel framework that integrates Large Language Models (LLMs)\nwith Latent Diffusion Models (LDMs) to address these challenges. Our method\nleverages LLMs for semantic refinement of textual prompts and introduces a weak\nsupervision filtering module to effectively utilize noisy or weakly labeled\ndata. By fine-tuning the LDM on an enhanced DeepFashion+ dataset enriched with\nglobal fashion styles, the proposed approach achieves state-of-the-art\nperformance. Experimental results demonstrate that our method significantly\noutperforms baselines, achieving lower Frechet Inception Distance (FID) and\nhigher Inception Scores (IS), while human evaluations confirm its ability to\ngenerate culturally diverse and semantically relevant fashion content. These\nresults highlight the potential of LLM-guided diffusion models in driving\nscalable and inclusive AI-driven fashion innovation.", "published": "2025-01-26 15:57:16", "link": "http://arxiv.org/abs/2501.15571v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Instruction Tuning for Story Understanding and Generation with Weak\n  Supervision", "abstract": "Story understanding and generation have long been a challenging task in\nnatural language processing (NLP), especially when dealing with various levels\nof instruction specificity. In this paper, we propose a novel approach called\n\"Weak to Strong Instruction Tuning\" for improving story generation by tuning\nmodels with instructions of varying clarity. We explore the potential of large\nlanguage models (LLMs) to adapt to different types of instructions, weak and\nstrong, and show that our method significantly enhances performance in story\ncomprehension and generation. By leveraging the strength of instruction tuning,\nwe train models to understand the nuances of story plots, characters, and\nthemes while generating coherent and engaging narratives. Through extensive\nexperiments on several benchmark datasets and comparison with state-of-the-art\nbaselines, we demonstrate that our method outperforms existing techniques,\nyielding substantial improvements in both automatic evaluation metrics and\nhuman evaluations. Our work shows that adaptive instruction tuning can be a\npowerful tool in refining generative models for complex narrative tasks.", "published": "2025-01-26 15:59:31", "link": "http://arxiv.org/abs/2501.15574v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Error Classification of Large Language Models on Math Word Problems: A\n  Dynamically Adaptive Framework", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains. Math Word Problems (MWPs) serve as a crucial benchmark for\nevaluating LLMs' reasoning abilities. While most research primarily focuses on\nimproving accuracy, it often neglects understanding and addressing the\nunderlying patterns of errors. Current error classification methods rely on\nstatic and predefined categories, which limit their ability to capture the full\nspectrum of error patterns in mathematical reasoning. To enable systematic\nerror analysis, we collect error samples from 15 different LLMs of varying\nsizes across four distinct MWP datasets using multiple sampling strategies.\nBased on this extensive collection, we introduce MWPES-300K, a comprehensive\ndataset containing 304,865 error samples that cover diverse error patterns and\nreasoning paths. To reduce human bias and enable fine-grained analysis of error\npatterns, we propose a novel framework for automated dynamic error\nclassification in mathematical reasoning. Experimental results demonstrate that\ndataset characteristics significantly shape error patterns, which evolve from\nbasic to complex manifestations as model capabilities increase. With deeper\ninsights into error patterns, we propose error-aware prompting that\nincorporates common error patterns as explicit guidance, leading to significant\nimprovements in mathematical reasoning performance.", "published": "2025-01-26 16:17:57", "link": "http://arxiv.org/abs/2501.15581v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Estonian Text Simplification through Pretrained Language\n  Models and Custom Datasets", "abstract": "This study introduces an approach to Estonian text simplification using two\nmodel architectures: a neural machine translation model and a fine-tuned large\nlanguage model (LLaMA). Given the limited resources for Estonian, we developed\na new dataset, the Estonian Simplification Dataset, combining translated data\nand GPT-4.0-generated simplifications. We benchmarked OpenNMT, a neural machine\ntranslation model that frames text simplification as a translation task, and\nfine-tuned the LLaMA model on our dataset to tailor it specifically for\nEstonian simplification. Manual evaluations on the test set show that the LLaMA\nmodel consistently outperforms OpenNMT in readability, grammaticality, and\nmeaning preservation. These findings underscore the potential of large language\nmodels for low-resource languages and provide a basis for further research in\nEstonian text simplification.", "published": "2025-01-26 18:10:20", "link": "http://arxiv.org/abs/2501.15624v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting Biomedical Abstracts into Plain language using Large Language\n  Models", "abstract": "A vast amount of medical knowledge is available for public use through online\nhealth forums, and question-answering platforms on social media. The majority\nof the population in the United States doesn't have the right amount of health\nliteracy to make the best use of that information. Health literacy means the\nability to obtain and comprehend the basic health information to make\nappropriate health decisions. To build the bridge between this gap,\norganizations advocate adapting this medical knowledge into plain language.\nBuilding robust systems to automate the adaptations helps both medical and\nnon-medical professionals best leverage the available information online. The\ngoal of the Plain Language Adaptation of Biomedical Abstracts (PLABA) track is\nto adapt the biomedical abstracts in English language extracted from PubMed\nbased on the questions asked in MedlinePlus for the general public using plain\nlanguage at the sentence level. As part of this track, we leveraged the best\nopen-source Large Language Models suitable and fine-tuned for dialog use cases.\nWe compare and present the results for all of our systems and our ranking among\nthe other participants' submissions. Our top performing GPT-4 based model\nranked first in the avg. simplicity measure and 3rd on the avg. accuracy\nmeasure.", "published": "2025-01-26 23:07:35", "link": "http://arxiv.org/abs/2501.15700v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models as Theory of Mind Aware Generative Agents with\n  Counterfactual Reflection", "abstract": "Recent studies have increasingly demonstrated that large language models\n(LLMs) possess significant theory of mind (ToM) capabilities, showing the\npotential for simulating the tracking of mental states in generative agents. In\nthis study, we propose a novel paradigm called ToM-agent, designed to empower\nLLMs-based generative agents to simulate ToM in open-domain conversational\ninteractions. ToM-agent disentangles the confidence from mental states,\nfacilitating the emulation of an agent's perception of its counterpart's mental\nstates, such as beliefs, desires, and intentions (BDIs). Using past\nconversation history and verbal reflections, ToM-Agent can dynamically adjust\ncounterparts' inferred BDIs, along with related confidence levels. We further\nput forth a counterfactual intervention method that reflects on the gap between\nthe predicted responses of counterparts and their real utterances, thereby\nenhancing the efficiency of reflection. Leveraging empathetic and persuasion\ndialogue datasets, we assess the advantages of implementing the ToM-agent with\ndownstream tasks, as well as its performance in both the first-order and the\n\\textit{second-order} ToM. Our findings indicate that the ToM-agent can grasp\nthe underlying reasons for their counterpart's behaviors beyond mere\nsemantic-emotional supporting or decision-making based on common sense,\nproviding new insights for studying large-scale LLMs-based simulation of human\nsocial behaviors.", "published": "2025-01-26 00:32:38", "link": "http://arxiv.org/abs/2501.15355v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Diffusion-based Hierarchical Negative Sampling for Multimodal Knowledge\n  Graph Completion", "abstract": "Multimodal Knowledge Graph Completion (MMKGC) aims to address the critical\nissue of missing knowledge in multimodal knowledge graphs (MMKGs) for their\nbetter applications. However, both the previous MMGKC and negative sampling\n(NS) approaches ignore the employment of multimodal information to generate\ndiverse and high-quality negative triples from various semantic levels and\nhardness levels, thereby limiting the effectiveness of training MMKGC models.\nThus, we propose a novel Diffusion-based Hierarchical Negative Sampling (DHNS)\nscheme tailored for MMKGC tasks, which tackles the challenge of generating\nhigh-quality negative triples by leveraging a Diffusion-based Hierarchical\nEmbedding Generation (DiffHEG) that progressively conditions on entities and\nrelations as well as multimodal semantics. Furthermore, we develop a Negative\nTriple-Adaptive Training (NTAT) strategy that dynamically adjusts training\nmargins associated with the hardness level of the synthesized negative triples,\nfacilitating a more robust and effective learning procedure to distinguish\nbetween positive and negative triples. Extensive experiments on three MMKGC\nbenchmark datasets demonstrate that our framework outperforms several\nstate-of-the-art MMKGC models and negative sampling techniques, illustrating\nthe effectiveness of our DHNS for training MMKGC models. The source codes and\ndatasets of this paper are available at https://github.com/ngl567/DHNS.", "published": "2025-01-26 04:20:34", "link": "http://arxiv.org/abs/2501.15393v1", "categories": ["cs.AI", "cs.CL", "I.2.7"], "primary_category": "cs.AI"}
{"title": "Semantic Layered Embedding Diffusion in Large Language Models for\n  Multi-Contextual Consistency", "abstract": "The Semantic Layered Embedding Diffusion (SLED) mechanism redefines the\nrepresentation of hierarchical semantics within transformer-based\narchitectures, enabling enhanced contextual consistency across a wide array of\nlinguistic tasks. By introducing a multi-layered diffusion process grounded in\nspectral analysis, it achieves a complex balance between global and local\nsemantic coherence. Experimental results demonstrate significant improvements\nin perplexity and BLEU scores, emphasizing the mechanism's ability to adapt\neffectively across diverse domains, including multilingual and cross-domain\ntext generation. A rigorous mathematical framework underpins the embedding\ndiffusion process, incorporating weighted adjacency matrices, kernel-based\nrefinements, and dynamic layer-wise normalization. Error distribution analysis\nreveals that SLED addresses challenges in semantic alignment and coherence,\noutperforming baseline approaches across varied benchmarks. Scalability studies\nillustrate that its performance gains are maintained consistently across\ndifferent model sizes, reflecting a practical balance between computational\nefficiency and linguistic precision. The implementation also achieves energy\nefficiency, reducing resource consumption during training and inference phases\nwithout compromising accuracy. Qualitative case studies further validate its\nadaptability to extended narratives and context-intensive scenarios,\nhighlighting the mechanism's potential for real-world applications. SLED offers\na different perspective on embedding design and its implications for advancing\nlanguage modeling.", "published": "2025-01-26 05:17:04", "link": "http://arxiv.org/abs/2501.15405v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Potential of Large Language Models in Supply Chain Management:\n  Advancing Decision-Making, Efficiency, and Innovation", "abstract": "The integration of large language models (LLMs) into supply chain management\n(SCM) is revolutionizing the industry by improving decision-making, predictive\nanalytics, and operational efficiency. This white paper explores the\ntransformative impact of LLMs on various SCM functions, including demand\nforecasting, inventory management, supplier relationship management, and\nlogistics optimization. By leveraging advanced data analytics and real-time\ninsights, LLMs enable organizations to optimize resources, reduce costs, and\nimprove responsiveness to market changes. Key findings highlight the benefits\nof integrating LLMs with emerging technologies such as IoT, blockchain, and\nrobotics, which together create smarter and more autonomous supply chains.\nEthical considerations, including bias mitigation and data protection, are\ntaken into account to ensure fair and transparent AI practices. In addition,\nthe paper discusses the need to educate the workforce on how to manage new\nAI-driven processes and the long-term strategic benefits of adopting LLMs.\nStrategic recommendations for SCM professionals include investing in\nhigh-quality data management, promoting cross-functional collaboration, and\naligning LLM initiatives with overall business goals. The findings highlight\nthe potential of LLMs to drive innovation, sustainability, and competitive\nadvantage in the ever-changing supply chain management landscape.", "published": "2025-01-26 05:41:50", "link": "http://arxiv.org/abs/2501.15411v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Token Democracy: The Architectural Limits of Alignment in\n  Transformer-Based Language Models", "abstract": "Modern language models paradoxically combine unprecedented capability with\npersistent vulnerability in that they can draft poetry yet cannot reliably\nrefuse harmful requests. We reveal this fragility stems not from inadequate\ntraining, but from a fundamental architectural limitation: transformers process\nall tokens as equals. Transformers operate as computational democracies,\ngranting equal voice to all tokens. This is a design tragically unsuited for\nAGI, where we cannot risk adversarial \"candidates\" hijacking the system.\nThrough formal analysis, we demonstrate that safety instructions fundamentally\nlack privileged status in transformer architectures, that they compete with\nadversarial inputs in the same computational arena, making robust alignment\nthrough prompting or fine-tuning inherently limited. This \"token democracy\"\nexplains why jailbreaks bypass even extensively safety-trained models and why\npositional shifts erode prompt effectiveness. Our work systematizes\npractitioners' tacit knowledge into an architectural critique, showing current\nalignment approaches create mere preferences, not constraints.", "published": "2025-01-26 08:26:06", "link": "http://arxiv.org/abs/2501.15446v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Commute Your Domains: Trajectory Optimality Criterion for Multi-Domain\n  Learning", "abstract": "In multi-domain learning, a single model is trained on diverse data domains\nto leverage shared knowledge and improve generalization. The order in which the\ndata from these domains is used for training can significantly affect the\nmodel's performance on each domain. However, this dependence is under-studied.\nIn this paper, we investigate the influence of training order (or data mixing)\nin multi-domain learning using the concept of Lie bracket of gradient vector\nfields. By analyzing the infinitesimal effects of changing the training order,\nwe identify regions in the parameter space where altering the order between two\ntraining domains can benefit the target loss. We validate the predictions of\nour theoretical framework on the influence of training order (or data mixing)\nboth on a toy example and bilingual LLM pre-training.", "published": "2025-01-26 15:12:06", "link": "http://arxiv.org/abs/2501.15556v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ConceptCLIP: Towards Trustworthy Medical AI via Concept-Enhanced\n  Contrastive Langauge-Image Pre-training", "abstract": "Trustworthiness is essential for the precise and interpretable application of\nartificial intelligence (AI) in medical imaging. Traditionally, precision and\ninterpretability have been addressed as separate tasks, namely medical image\nanalysis and explainable AI, each developing its own models independently. In\nthis study, for the first time, we investigate the development of a unified\nmedical vision-language pre-training model that can achieve both accurate\nanalysis and interpretable understanding of medical images across various\nmodalities. To build the model, we construct MedConcept-23M, a large-scale\ndataset comprising 23 million medical image-text pairs extracted from 6.2\nmillion scientific articles, enriched with concepts from the Unified Medical\nLanguage System (UMLS). Based on MedConcept-23M, we introduce ConceptCLIP, a\nmedical AI model utilizing concept-enhanced contrastive language-image\npre-training. The pre-training of ConceptCLIP involves two primary components:\nimage-text alignment learning (IT-Align) and patch-concept alignment learning\n(PC-Align). This dual alignment strategy enhances the model's capability to\nassociate specific image regions with relevant concepts, thereby improving both\nthe precision of analysis and the interpretability of the AI system. We\nconducted extensive experiments on 5 diverse types of medical image analysis\ntasks, spanning 51 subtasks across 10 image modalities, with the broadest range\nof downstream tasks. The results demonstrate the effectiveness of the proposed\nvision-language pre-training model. Further explainability analysis across 6\nmodalities reveals that ConceptCLIP achieves superior performance, underscoring\nits robust ability to advance explainable AI in medical imaging. These findings\nhighlight ConceptCLIP's capability in promoting trustworthy AI in the field of\nmedicine.", "published": "2025-01-26 16:07:11", "link": "http://arxiv.org/abs/2501.15579v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Rethinking External Slow-Thinking: From Snowball Errors to Probability\n  of Correct Reasoning", "abstract": "Test-time scaling, which is also often referred to as slow-thinking, has been\ndemonstrated to enhance multi-step reasoning in large language models (LLMs).\nHowever, despite its widespread utilization, the mechanisms underlying\nslow-thinking methods remain poorly understood. This paper explores the\nmechanisms of external slow-thinking from a theoretical standpoint. We begin by\nexamining the snowball error effect within the LLM reasoning process and\nconnect it to the likelihood of correct reasoning using information theory.\nBuilding on this, we show that external slow-thinking methods can be\ninterpreted as strategies to mitigate the error probability. We further provide\na comparative analysis of popular external slow-thinking approaches, ranging\nfrom simple to complex, highlighting their differences and interrelationships.\nOur findings suggest that the efficacy of these methods is not primarily\ndetermined by the specific framework employed, and that expanding the search\nscope or the model's internal reasoning capacity may yield more sustained\nimprovements in the long term. We open-source our code at\nhttps://github.com/ZyGan1999/Snowball-Errors-and-Probability.", "published": "2025-01-26 17:05:16", "link": "http://arxiv.org/abs/2501.15602v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Quantum-Enhanced Attention Mechanism in NLP: A Hybrid Classical-Quantum\n  Approach", "abstract": "Transformer-based models have achieved remarkable results in natural language\nprocessing (NLP) tasks such as text classification and machine translation.\nHowever, their computational complexity and resource demands pose challenges\nfor scalability and accessibility. This research proposes a hybrid\nquantum-classical transformer model that integrates a quantum-enhanced\nattention mechanism to address these limitations. By leveraging quantum kernel\nsimilarity and variational quantum circuits (VQC), the model captures intricate\ntoken dependencies while improving computational efficiency. Experimental\nresults on the IMDb dataset demonstrate that the quantum-enhanced model\noutperforms the classical baseline across all key metrics, achieving a 1.5%\nimprovement in accuracy (65.5% vs. 64%), precision, recall, and F1 score.\nStatistical significance tests validate these improvements, highlighting the\nrobustness of the quantum approach. These findings illustrate the\ntransformative potential of quantum-enhanced attention mechanisms in optimizing\nNLP architectures for real-world applications.", "published": "2025-01-26 18:29:06", "link": "http://arxiv.org/abs/2501.15630v1", "categories": ["cs.CL", "quant-ph"], "primary_category": "cs.CL"}
{"title": "People who frequently use ChatGPT for writing tasks are accurate and\n  robust detectors of AI-generated text", "abstract": "In this paper, we study how well humans can detect text generated by\ncommercial LLMs (GPT-4o, Claude, o1). We hire annotators to read 300\nnon-fiction English articles, label them as either human-written or\nAI-generated, and provide paragraph-length explanations for their decisions.\nOur experiments show that annotators who frequently use LLMs for writing tasks\nexcel at detecting AI-generated text, even without any specialized training or\nfeedback. In fact, the majority vote among five such \"expert\" annotators\nmisclassifies only 1 of 300 articles, significantly outperforming most\ncommercial and open-source detectors we evaluated even in the presence of\nevasion tactics like paraphrasing and humanization. Qualitative analysis of the\nexperts' free-form explanations shows that while they rely heavily on specific\nlexical clues ('AI vocabulary'), they also pick up on more complex phenomena\nwithin the text (e.g., formality, originality, clarity) that are challenging to\nassess for automatic detectors. We release our annotated dataset and code to\nspur future research into both human and automated detection of AI-generated\ntext.", "published": "2025-01-26 19:31:34", "link": "http://arxiv.org/abs/2501.15654v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TensorLLM: Tensorising Multi-Head Attention for Enhanced Reasoning and\n  Compression in LLMs", "abstract": "The reasoning abilities of Large Language Models (LLMs) can be improved by\nstructurally denoising their weights, yet existing techniques primarily focus\non denoising the feed-forward network (FFN) of the transformer block, and can\nnot efficiently utilise the Multi-head Attention (MHA) block, which is the core\nof transformer architectures. To address this issue, we propose a novel\nintuitive framework that, at its very core, performs MHA compression through a\nmulti-head tensorisation process and the Tucker decomposition. This enables\nboth higher-dimensional structured denoising and compression of the MHA\nweights, by enforcing a shared higher-dimensional subspace across the weights\nof the multiple attention heads. We demonstrate that this approach consistently\nenhances the reasoning capabilities of LLMs across multiple benchmark datasets,\nand for both encoder-only and decoder-only architectures, while achieving\ncompression rates of up to $\\sim 250$ times in the MHA weights, all without\nrequiring any additional data, training, or fine-tuning. Furthermore, we show\nthat the proposed method can be seamlessly combined with existing\nFFN-only-based denoising techniques to achieve further improvements in LLM\nreasoning performance.", "published": "2025-01-26 21:05:16", "link": "http://arxiv.org/abs/2501.15674v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Baichuan-Omni-1.5 Technical Report", "abstract": "We introduce Baichuan-Omni-1.5, an omni-modal model that not only has\nomni-modal understanding capabilities but also provides end-to-end audio\ngeneration capabilities. To achieve fluent and high-quality interaction across\nmodalities without compromising the capabilities of any modality, we\nprioritized optimizing three key aspects. First, we establish a comprehensive\ndata cleaning and synthesis pipeline for multimodal data, obtaining about 500B\nhigh-quality data (text, audio, and vision). Second, an audio-tokenizer\n(Baichuan-Audio-Tokenizer) has been designed to capture both semantic and\nacoustic information from audio, enabling seamless integration and enhanced\ncompatibility with MLLM. Lastly, we designed a multi-stage training strategy\nthat progressively integrates multimodal alignment and multitask fine-tuning,\nensuring effective synergy across all modalities. Baichuan-Omni-1.5 leads\ncontemporary models (including GPT4o-mini and MiniCPM-o 2.6) in terms of\ncomprehensive omni-modal capabilities. Notably, it achieves results comparable\nto leading models such as Qwen2-VL-72B across various multimodal medical\nbenchmarks.", "published": "2025-01-26 02:19:03", "link": "http://arxiv.org/abs/2501.15368v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Evaluating the Effectiveness of XAI Techniques for Encoder-Based\n  Language Models", "abstract": "The black-box nature of large language models (LLMs) necessitates the\ndevelopment of eXplainable AI (XAI) techniques for transparency and\ntrustworthiness. However, evaluating these techniques remains a challenge. This\nstudy presents a general evaluation framework using four key metrics:\nHuman-reasoning Agreement (HA), Robustness, Consistency, and Contrastivity. We\nassess the effectiveness of six explainability techniques from five different\nXAI categories model simplification (LIME), perturbation-based methods (SHAP),\ngradient-based approaches (InputXGradient, Grad-CAM), Layer-wise Relevance\nPropagation (LRP), and attention mechanisms-based explainability methods\n(Attention Mechanism Visualization, AMV) across five encoder-based language\nmodels: TinyBERT, BERTbase, BERTlarge, XLM-R large, and DeBERTa-xlarge, using\nthe IMDB Movie Reviews and Tweet Sentiment Extraction (TSE) datasets. Our\nfindings show that the model simplification-based XAI method (LIME)\nconsistently outperforms across multiple metrics and models, significantly\nexcelling in HA with a score of 0.9685 on DeBERTa-xlarge, robustness, and\nconsistency as the complexity of large language models increases. AMV\ndemonstrates the best Robustness, with scores as low as 0.0020. It also excels\nin Consistency, achieving near-perfect scores of 0.9999 across all models.\nRegarding Contrastivity, LRP performs the best, particularly on more complex\nmodels, with scores up to 0.9371.", "published": "2025-01-26 03:08:34", "link": "http://arxiv.org/abs/2501.15374v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mind the Value-Action Gap: Do LLMs Act in Alignment with Their Values?", "abstract": "Existing research primarily evaluates the values of LLMs by examining their\nstated inclinations towards specific values. However, the \"Value-Action Gap,\" a\nphenomenon rooted in environmental and social psychology, reveals discrepancies\nbetween individuals' stated values and their actions in real-world contexts. To\nwhat extent do LLMs exhibit a similar gap between their stated values and their\nactions informed by those values? This study introduces ValueActionLens, an\nevaluation framework to assess the alignment between LLMs' stated values and\ntheir value-informed actions. The framework encompasses the generation of a\ndataset comprising 14.8k value-informed actions across twelve cultures and\neleven social topics, and two tasks to evaluate how well LLMs' stated value\ninclinations and value-informed actions align across three different alignment\nmeasures. Extensive experiments reveal that the alignment between LLMs' stated\nvalues and actions is sub-optimal, varying significantly across scenarios and\nmodels. Analysis of misaligned results identifies potential harms from certain\nvalue-action gaps. To predict the value-action gaps, we also uncover that\nleveraging reasoned explanations improves performance. These findings\nunderscore the risks of relying solely on the LLMs' stated values to predict\ntheir behaviors and emphasize the importance of context-aware evaluations of\nLLM values and value-action gaps.", "published": "2025-01-26 09:33:51", "link": "http://arxiv.org/abs/2501.15463v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "SCP-116K: A High-Quality Problem-Solution Dataset and a Generalized\n  Pipeline for Automated Extraction in the Higher Education Science Domain", "abstract": "Recent breakthroughs in large language models (LLMs) exemplified by the\nimpressive mathematical and scientific reasoning capabilities of the o1 model\nhave spotlighted the critical importance of high-quality training data in\nadvancing LLM performance across STEM disciplines. While the mathematics\ncommunity has benefited from a growing body of curated datasets, the scientific\ndomain at the higher education level has long suffered from a scarcity of\ncomparable resources. To address this gap, we present SCP-116K, a new\nlarge-scale dataset of 116,756 high-quality problem-solution pairs,\nautomatically extracted from heterogeneous sources using a streamlined and\nhighly generalizable pipeline. Our approach involves stringent filtering to\nensure the scientific rigor and educational level of the extracted materials,\nwhile maintaining adaptability for future expansions or domain transfers. By\nopenly releasing both the dataset and the extraction pipeline, we seek to\nfoster research on scientific reasoning, enable comprehensive performance\nevaluations of new LLMs, and lower the barrier to replicating the successes of\nadvanced models like o1 in the broader science community. We believe SCP-116K\nwill serve as a critical resource, catalyzing progress in high-level scientific\nreasoning tasks and promoting further innovations in LLM development. The\ndataset and code are publicly available at\nhttps://github.com/AQA6666/SCP-116K-open.", "published": "2025-01-26 16:26:38", "link": "http://arxiv.org/abs/2501.15587v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Stepback: Enhanced Disentanglement for Voice Conversion via Multi-Task\n  Learning", "abstract": "Voice conversion (VC) modifies voice characteristics while preserving\nlinguistic content. This paper presents the Stepback network, a novel model for\nconverting speaker identity using non-parallel data. Unlike traditional VC\nmethods that rely on parallel data, our approach leverages deep learning\ntechniques to enhance disentanglement completion and linguistic content\npreservation. The Stepback network incorporates a dual flow of different domain\ndata inputs and uses constraints with self-destructive amendments to optimize\nthe content encoder. Extensive experiments show that our model significantly\nimproves VC performance, reducing training costs while achieving high-quality\nvoice conversion. The Stepback network's design offers a promising solution for\nadvanced voice conversion tasks.", "published": "2025-01-26 17:43:32", "link": "http://arxiv.org/abs/2501.15613v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Blissful (A)Ignorance: People form overly positive impressions of others\n  based on their written messages, despite wide-scale adoption of Generative AI", "abstract": "As the use of Generative AI (GenAI) tools becomes more prevalent in\ninterpersonal communication, understanding their impact on social perceptions\nis crucial. According to signaling theory, GenAI may undermine the credibility\nof social signals conveyed in writing, since it reduces the cost of writing and\nmakes it hard to verify the authenticity of messages. Using a pre-registered\nlarge-scale online experiment (N = 647; Prolific), featuring scenarios in a\nrange of communication contexts (personal vs. professional; close others vs.\nstrangers), we explored how senders' use of GenAI influenced recipients'\nimpressions of senders, both when GenAI use was known or uncertain. Consistent\nwith past work, we found strong negative effects on social impressions when\ndisclosing that a message was AI-generated, compared to when the same message\nwas human-written. However, under the more realistic condition when potential\nGenAI use was not explicitly highlighted, recipients did not exhibit any\nskepticism towards senders, and these \"uninformed\" impressions were virtually\nindistinguishable from those of fully human-written messages. Even when we\nhighlighted the potential (but uncertain) use of GenAI, recipients formed\noverly positive impressions. These results are especially striking given that\n46% of our sample admitted having used such tools for writing messages, just\nwithin the past two weeks. Our findings put past work in a new light: While\nsocial judgments can be substantially affected when GenAI use is explicitly\ndisclosed, this information may not be readily available in more realistic\ncommunication settings, making recipients blissfully ignorant about others'\npotential use of GenAI.", "published": "2025-01-26 21:38:12", "link": "http://arxiv.org/abs/2501.15678v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.CY"}
{"title": "Transformer-Based Multimodal Knowledge Graph Completion with Link-Aware\n  Contexts", "abstract": "Multimodal knowledge graph completion (MMKGC) aims to predict missing links\nin multimodal knowledge graphs (MMKGs) by leveraging information from various\nmodalities alongside structural data. Existing MMKGC approaches primarily\nextend traditional knowledge graph embedding (KGE) models, which often require\ncreating an embedding for every entity. This results in large model sizes and\ninefficiencies in integrating multimodal information, particularly for\nreal-world graphs. Meanwhile, Transformer-based models have demonstrated\ncompetitive performance in knowledge graph completion (KGC). However, their\nfocus on single-modal knowledge limits their capacity to utilize cross-modal\ninformation. Recently, Large vision-language models (VLMs) have shown potential\nin cross-modal tasks but are constrained by the high cost of training. In this\nwork, we propose a novel approach that integrates Transformer-based KGE models\nwith cross-modal context generated by pre-trained VLMs, thereby extending their\napplicability to MMKGC. Specifically, we employ a pre-trained VLM to transform\nrelevant visual information from entities and their neighbors into textual\nsequences. We then frame KGC as a sequence-to-sequence task, fine-tuning the\nmodel with the generated cross-modal context. This simple yet effective method\nsignificantly reduces model size compared to traditional KGE approaches while\nachieving competitive performance across multiple large-scale datasets with\nminimal hyperparameter tuning.", "published": "2025-01-26 22:23:14", "link": "http://arxiv.org/abs/2501.15688v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond Benchmarks: On The False Promise of AI Regulation", "abstract": "The rapid advancement of artificial intelligence (AI) systems in critical\ndomains like healthcare, justice, and social services has sparked numerous\nregulatory initiatives aimed at ensuring their safe deployment. Current\nregulatory frameworks, exemplified by recent US and EU efforts, primarily focus\non procedural guidelines while presuming that scientific benchmarking can\neffectively validate AI safety, similar to how crash tests verify vehicle\nsafety or clinical trials validate drug efficacy. However, this approach\nfundamentally misunderstands the unique technical challenges posed by modern AI\nsystems. Through systematic analysis of successful technology regulation case\nstudies, we demonstrate that effective scientific regulation requires a causal\ntheory linking observable test outcomes to future performance - for instance,\nhow a vehicle's crash resistance at one speed predicts its safety at lower\nspeeds. We show that deep learning models, which learn complex statistical\npatterns from training data without explicit causal mechanisms, preclude such\nguarantees. This limitation renders traditional regulatory approaches\ninadequate for ensuring AI safety. Moving forward, we call for regulators to\nreckon with this limitation, and propose a preliminary two-tiered regulatory\nframework that acknowledges these constraints: mandating human oversight for\nhigh-risk applications while developing appropriate risk communication\nstrategies for lower-risk uses. Our findings highlight the urgent need to\nreconsider fundamental assumptions in AI regulation and suggest a concrete path\nforward for policymakers and researchers.", "published": "2025-01-26 22:43:07", "link": "http://arxiv.org/abs/2501.15693v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Complete Chess Games Enable LLM Become A Chess Master", "abstract": "Large language models (LLM) have shown remarkable abilities in text\ngeneration, question answering, language translation, reasoning and many other\ntasks. It continues to advance rapidly and is becoming increasingly influential\nin various fields, from technology and business to education and entertainment.\nDespite LLM's success in multiple areas, its ability to play abstract games,\nsuch as chess, is underexplored. Chess-playing requires the language models to\noutput legal and reasonable moves from textual inputs. Here, we propose the\nLarge language model ChessLLM to play full chess games. We transform the game\ninto a textual format with the best move represented in the Forsyth-Edwards\nNotation. We show that by simply supervised fine-tuning, our model has achieved\na professional-level Elo rating of 1788 in matches against the standard\nElo-rated Stockfish when permitted to sample 10 times. We further show that\ndata quality is important. Long-round data supervision enjoys a 350 Elo rating\nimprovement over short-round data.", "published": "2025-01-26 09:43:39", "link": "http://arxiv.org/abs/2501.17186v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Visualizing Uncertainty in Translation Tasks: An Evaluation of LLM\n  Performance and Confidence Metrics", "abstract": "Large language models (LLMs) are increasingly utilized for machine\ntranslation, yet their predictions often exhibit uncertainties that hinder\ninterpretability and user trust. Effectively visualizing these uncertainties\ncan enhance the usability of LLM outputs, particularly in contexts where\ntranslation accuracy is critical. This paper addresses two primary objectives:\n(1) providing users with token-level insights into model confidence and (2)\ndeveloping a web-based visualization tool to quantify and represent translation\nuncertainties. To achieve these goals, we utilized the T5 model with the WMT19\ndataset for translation tasks and evaluated translation quality using\nestablished metrics such as BLEU, METEOR, and ROUGE. We introduced three novel\nuncertainty quantification (UQ) metrics: (1) the geometric mean of token\nprobabilities, (2) the arithmetic mean of token probabilities, and (3) the\narithmetic mean of the kurtosis of token distributions. These metrics provide a\nsimple yet effective framework for evaluating translation performance. Our\nanalysis revealed a linear relationship between the traditional evaluation\nmetrics and our UQ metrics, demonstrating the validity of our approach.\nAdditionally, we developed an interactive web-based visualization that uses a\ncolor gradient to represent token confidence. This tool offers users a clear\nand intuitive understanding of translation quality while providing valuable\ninsights into model performance. Overall, we show that our UQ metrics and\nvisualization are both robust and interpretable, offering practical tools for\nevaluating and accessing machine translation systems.", "published": "2025-01-26 17:14:51", "link": "http://arxiv.org/abs/2501.17187v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SEAL: Speech Embedding Alignment Learning for Speech Large Language\n  Model with Retrieval-Augmented Generation", "abstract": "Embedding-based retrieval models have made significant strides in\nretrieval-augmented generation (RAG) techniques for text and multimodal large\nlanguage models (LLMs) applications. However, when it comes to speech larage\nlanguage models (SLLMs), these methods are limited to a two-stage process,\nwhere automatic speech recognition (ASR) is combined with text-based retrieval.\nThis sequential architecture suffers from high latency and error propagation.\nTo address these limitations, we propose a unified embedding framework that\neliminates the need for intermediate text representations. Specifically, the\nframework includes separate speech and text encoders, followed by a shared\nscaling layer that maps both modalities into a common embedding space. Our\nmodel reduces pipeline latency by 50\\% while achieving higher retrieval\naccuracy compared to traditional two-stage methods. We also provide a\ntheoretical analysis of the challenges inherent in end-to-end speech retrieval\nand introduce architectural principles for effective speech-to-document\nmatching. Extensive experiments demonstrate the robustness of our approach\nacross diverse acoustic conditions and speaker variations, paving the way for a\nnew paradigm in multimodal SLLMs retrieval systems.", "published": "2025-01-26 15:04:02", "link": "http://arxiv.org/abs/2502.02603v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-End Target Speaker Speech Recognition Using Context-Aware\n  Attention Mechanisms for Challenging Enrollment Scenario", "abstract": "This paper presents a novel streaming end-to-end target-speaker speech\nrecognition that addresses two critical limitations in systems: the handling of\nnoisy enrollment utterances and specific enrollment phrase requirements. This\npaper proposes a robust Target-Speaker Recurrent Neural Network Transducer\n(TS-RNNT) with dual attention mechanisms for contextual biasing and overlapping\nenrollment processing. The model incorporates a text decoder and attention\nmechanism specifically designed to extract relevant speaker characteristics\nfrom noisy, overlapping enrollment audio. Experimental results on a synthesized\ndataset demonstrate the model's resilience, maintaining a Word Error Rate (WER)\nof 16.44% even with overlapping enrollment at 5dB Signal-to-Interference Ratio\n(SIR), compared to conventional approaches that degrade to WERs above 75% under\nsimilar conditions. This significant performance improvement, coupled with the\nmodel's semi-text-dependent enrollment capabilities, represents a substantial\nadvancement toward more practical and versatile voice-controlled devices.", "published": "2025-01-26 10:02:39", "link": "http://arxiv.org/abs/2501.15466v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Variational Bayesian Adaptive Learning of Deep Latent Variables for\n  Acoustic Knowledge Transfer", "abstract": "In this work, we propose a novel variational Bayesian adaptive learning\napproach for cross-domain knowledge transfer to address acoustic mismatches\nbetween training and testing conditions, such as recording devices and\nenvironmental noise. Different from the traditional Bayesian approaches that\nimpose uncertainties on model parameters risking the curse of dimensionality\ndue to the huge number of parameters, we focus on estimating a manageable\nnumber of latent variables in deep neural models. Knowledge learned from a\nsource domain is thus encoded in prior distributions of deep latent variables\nand optimally combined, in a Bayesian sense, with a small set of adaptation\ndata from a target domain to approximate the corresponding posterior\ndistributions. Two different strategies are proposed and investigated to\nestimate the posterior distributions: Gaussian mean-field variational\ninference, and empirical Bayes. These strategies address the presence or\nabsence of parallel data in the source and target domains. Furthermore,\nstructural relationship modeling is investigated to enhance the approximation.\nWe evaluated our proposed approaches on two acoustic adaptation tasks: 1)\ndevice adaptation for acoustic scene classification, and 2) noise adaptation\nfor spoken command recognition. Experimental results show that the proposed\nvariational Bayesian adaptive learning approach can obtain good improvements on\ntarget domain data, and consistently outperforms state-of-the-art knowledge\ntransfer methods.", "published": "2025-01-26 11:54:41", "link": "http://arxiv.org/abs/2501.15496v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AnyEnhance: A Unified Generative Model with Prompt-Guidance and\n  Self-Critic for Voice Enhancement", "abstract": "We introduce AnyEnhance, a unified generative model for voice enhancement\nthat processes both speech and singing voices. Based on a masked generative\nmodel, AnyEnhance is capable of handling both speech and singing voices,\nsupporting a wide range of enhancement tasks including denoising,\ndereverberation, declipping, super-resolution, and target speaker extraction,\nall simultaneously and without fine-tuning. AnyEnhance introduces a\nprompt-guidance mechanism for in-context learning, which allows the model to\nnatively accept a reference speaker's timbre. In this way, it could boost\nenhancement performance when a reference audio is available and enable the\ntarget speaker extraction task without altering the underlying architecture.\nMoreover, we also introduce a self-critic mechanism into the generative process\nfor masked generative models, yielding higher-quality outputs through iterative\nself-assessment and refinement. Extensive experiments on various enhancement\ntasks demonstrate AnyEnhance outperforms existing methods in terms of both\nobjective metrics and subjective listening tests. Demo audios are publicly\navailable at https://amphionspace.github.io/anyenhance/.", "published": "2025-01-26 06:40:30", "link": "http://arxiv.org/abs/2501.15417v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Overview of the Amphion Toolkit (v0.2)", "abstract": "Amphion is an open-source toolkit for Audio, Music, and Speech Generation,\ndesigned to lower the entry barrier for junior researchers and engineers in\nthese fields. It provides a versatile framework that supports a variety of\ngeneration tasks and models. In this report, we introduce Amphion v0.2, the\nsecond major release developed in 2024. This release features a 100K-hour\nopen-source multilingual dataset, a robust data preparation pipeline, and novel\nmodels for tasks such as text-to-speech, audio coding, and voice conversion.\nFurthermore, the report includes multiple tutorials that guide users through\nthe functionalities and usage of the newly released models.", "published": "2025-01-26 08:10:13", "link": "http://arxiv.org/abs/2501.15442v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
