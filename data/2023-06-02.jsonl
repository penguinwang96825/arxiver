{"title": "THiFLY Research at SemEval-2023 Task 7: A Multi-granularity System for\n  CTR-based Textual Entailment and Evidence Retrieval", "abstract": "The NLI4CT task aims to entail hypotheses based on Clinical Trial Reports\n(CTRs) and retrieve the corresponding evidence supporting the justification.\nThis task poses a significant challenge, as verifying hypotheses in the NLI4CT\ntask requires the integration of multiple pieces of evidence from one or two\nCTR(s) and the application of diverse levels of reasoning, including textual\nand numerical. To address these problems, we present a multi-granularity system\nfor CTR-based textual entailment and evidence retrieval in this paper.\nSpecifically, we construct a Multi-granularity Inference Network (MGNet) that\nexploits sentence-level and token-level encoding to handle both textual\nentailment and evidence retrieval tasks. Moreover, we enhance the numerical\ninference capability of the system by leveraging a T5-based model, SciFive,\nwhich is pre-trained on the medical corpus. Model ensembling and a joint\ninference method are further utilized in the system to increase the stability\nand consistency of inference. The system achieves f1-scores of 0.856 and 0.853\non textual entailment and evidence retrieval tasks, resulting in the best\nperformance on both subtasks. The experimental results corroborate the\neffectiveness of our proposed method. Our code is publicly available at\nhttps://github.com/THUMLP/NLI4CT.", "published": "2023-06-02 03:09:31", "link": "http://arxiv.org/abs/2306.01245v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Translation of Hate Speech to Non-hate Speech in Social Media\n  Texts", "abstract": "In this paper, we investigate the issue of hate speech by presenting a novel\ntask of translating hate speech into non-hate speech text while preserving its\nmeaning. As a case study, we use Spanish texts. We provide a dataset and\nseveral baselines as a starting point for further research in the task. We\nevaluated our baseline results using multiple metrics, including BLEU scores.\nThe aim of this study is to contribute to the development of more effective\nmethods for reducing the spread of hate speech in online communities.", "published": "2023-06-02 04:03:14", "link": "http://arxiv.org/abs/2306.01261v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VoteTRANS: Detecting Adversarial Text without Training by Voting on Hard\n  Labels of Transformations", "abstract": "Adversarial attacks reveal serious flaws in deep learning models. More\ndangerously, these attacks preserve the original meaning and escape human\nrecognition. Existing methods for detecting these attacks need to be trained\nusing original/adversarial data. In this paper, we propose detection without\ntraining by voting on hard labels from predictions of transformations, namely,\nVoteTRANS. Specifically, VoteTRANS detects adversarial text by comparing the\nhard labels of input text and its transformation. The evaluation demonstrates\nthat VoteTRANS effectively detects adversarial text across various\nstate-of-the-art attacks, models, and datasets.", "published": "2023-06-02 05:18:19", "link": "http://arxiv.org/abs/2306.01273v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MetaVL: Transferring In-Context Learning Ability From Language Models to\n  Vision-Language Models", "abstract": "Large-scale language models have shown the ability to adapt to a new task via\nconditioning on a few demonstrations (i.e., in-context learning). However, in\nthe vision-language domain, most large-scale pre-trained vision-language (VL)\nmodels do not possess the ability to conduct in-context learning. How can we\nenable in-context learning for VL models? In this paper, we study an\ninteresting hypothesis: can we transfer the in-context learning ability from\nthe language domain to VL domain? Specifically, we first meta-trains a language\nmodel to perform in-context learning on NLP tasks (as in MetaICL); then we\ntransfer this model to perform VL tasks by attaching a visual encoder. Our\nexperiments suggest that indeed in-context learning ability can be transferred\ncross modalities: our model considerably improves the in-context learning\ncapability on VL tasks and can even compensate for the size of the model\nsignificantly. On VQA, OK-VQA, and GQA, our method could outperform the\nbaseline model while having 20 times fewer parameters.", "published": "2023-06-02 07:21:03", "link": "http://arxiv.org/abs/2306.01311v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntax-aware Hybrid prompt model for Few-shot multi-modal sentiment\n  analysis", "abstract": "Multimodal Sentiment Analysis (MSA) has been a popular topic in natural\nlanguage processing nowadays, at both sentence and aspect level. However, the\nexisting approaches almost require large-size labeled datasets, which bring\nabout large consumption of time and resources. Therefore, it is practical to\nexplore the method for few-shot sentiment analysis in cross-modalities.\nPrevious works generally execute on textual modality, using the prompt-based\nmethods, mainly two types: hand-crafted prompts and learnable prompts. The\nexisting approach in few-shot multi-modality sentiment analysis task has\nutilized both methods, separately. We further design a hybrid pattern that can\ncombine one or more fixed hand-crafted prompts and learnable prompts and\nutilize the attention mechanisms to optimize the prompt encoder. The\nexperiments on both sentence-level and aspect-level datasets prove that we get\na significant outperformance.", "published": "2023-06-02 07:22:43", "link": "http://arxiv.org/abs/2306.01312v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Auxiliary Domain Parallel Data in Intermediate Task\n  Fine-tuning for Low-resource Translation", "abstract": "NMT systems trained on Pre-trained Multilingual Sequence-Sequence (PMSS)\nmodels flounder when sufficient amounts of parallel data is not available for\nfine-tuning. This specifically holds for languages missing/under-represented in\nthese models. The problem gets aggravated when the data comes from different\ndomains. In this paper, we show that intermediate-task fine-tuning (ITFT) of\nPMSS models is extremely beneficial for domain-specific NMT, especially when\ntarget domain data is limited/unavailable and the considered languages are\nmissing or under-represented in the PMSS model. We quantify the domain-specific\nresults variations using a domain-divergence test, and show that ITFT can\nmitigate the impact of domain divergence to some extent.", "published": "2023-06-02 09:05:18", "link": "http://arxiv.org/abs/2306.01382v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the Importance of Frequency versus Compositionality for\n  Subword-based Tokenization in NMT", "abstract": "Subword tokenization is the de facto standard for tokenization in neural\nlanguage models and machine translation systems. Three advantages are\nfrequently cited in favor of subwords: shorter encoding of frequent tokens,\ncompositionality of subwords, and ability to deal with unknown words. As their\nrelative importance is not entirely clear yet, we propose a tokenization\napproach that enables us to separate frequency (the first advantage) from\ncompositionality. The approach uses Huffman coding to tokenize words, by order\nof frequency, using a fixed amount of symbols. Experiments with CS-DE, EN-FR\nand EN-DE NMT show that frequency alone accounts for 90%-95% of the scores\nreached by BPE, hence compositionality has less importance than previously\nthought.", "published": "2023-06-02 09:39:36", "link": "http://arxiv.org/abs/2306.01393v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Extractive Summarization of Emotion Triggers", "abstract": "Understanding what leads to emotions during large-scale crises is important\nas it can provide groundings for expressed emotions and subsequently improve\nthe understanding of ongoing disasters. Recent approaches trained supervised\nmodels to both detect emotions and explain emotion triggers (events and\nappraisals) via abstractive summarization. However, obtaining timely and\nqualitative abstractive summaries is expensive and extremely time-consuming,\nrequiring highly-trained expert annotators. In time-sensitive, high-stake\ncontexts, this can block necessary responses. We instead pursue unsupervised\nsystems that extract triggers from text. First, we introduce CovidET-EXT,\naugmenting (Zhan et al. 2022)'s abstractive dataset (in the context of the\nCOVID-19 crisis) with extractive triggers. Second, we develop new unsupervised\nlearning models that can jointly detect emotions and summarize their triggers.\nOur best approach, entitled Emotion-Aware Pagerank, incorporates emotion\ninformation from external sources combined with a language understanding\nmodule, and outperforms strong baselines. We release our data and code at\nhttps://github.com/tsosea2/CovidET-EXT.", "published": "2023-06-02 11:07:13", "link": "http://arxiv.org/abs/2306.01444v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Light Coreference Resolution for Russian with Hierarchical Discourse\n  Features", "abstract": "Coreference resolution is the task of identifying and grouping mentions\nreferring to the same real-world entity. Previous neural models have mainly\nfocused on learning span representations and pairwise scores for coreference\ndecisions. However, current methods do not explicitly capture the referential\nchoice in the hierarchical discourse, an important factor in coreference\nresolution. In this study, we propose a new approach that incorporates\nrhetorical information into neural coreference resolution models. We collect\nrhetorical features from automated discourse parses and examine their impact.\nAs a base model, we implement an end-to-end span-based coreference resolver\nusing a partially fine-tuned multilingual entity-aware language model LUKE. We\nevaluate our method on the RuCoCo-23 Shared Task for coreference resolution in\nRussian. Our best model employing rhetorical distance between mentions has\nranked 1st on the development set (74.6% F1) and 2nd on the test set (73.3% F1)\nof the Shared Task. We hope that our work will inspire further research on\nincorporating discourse information in neural coreference resolution models.", "published": "2023-06-02 11:41:24", "link": "http://arxiv.org/abs/2306.01465v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GAIA Search: Hugging Face and Pyserini Interoperability for NLP Training\n  Data Exploration", "abstract": "Noticing the urgent need to provide tools for fast and user-friendly\nqualitative analysis of large-scale textual corpora of the modern NLP, we\npropose to turn to the mature and well-tested methods from the domain of\nInformation Retrieval (IR) - a research field with a long history of tackling\nTB-scale document collections. We discuss how Pyserini - a widely used toolkit\nfor reproducible IR research can be integrated with the Hugging Face ecosystem\nof open-source AI libraries and artifacts. We leverage the existing\nfunctionalities of both platforms while proposing novel features further\nfacilitating their integration. Our goal is to give NLP researchers tools that\nwill allow them to develop retrieval-based instrumentation for their data\nanalytics needs with ease and agility. We include a Jupyter Notebook-based walk\nthrough the core interoperability features, available on GitHub at\nhttps://github.com/huggingface/gaia. We then demonstrate how the ideas we\npresent can be operationalized to create a powerful tool for qualitative data\nanalysis in NLP. We present GAIA Search - a search engine built following\npreviously laid out principles, giving access to four popular large-scale text\ncollections. GAIA serves a dual purpose of illustrating the potential of\nmethodologies we discuss but also as a standalone qualitative analysis tool\nthat can be leveraged by NLP researchers aiming to understand datasets prior to\nusing them in training. GAIA is hosted live on Hugging Face Spaces -\nhttps://huggingface.co/spaces/spacerini/gaia.", "published": "2023-06-02 12:09:59", "link": "http://arxiv.org/abs/2306.01481v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data-Efficient French Language Modeling with CamemBERTa", "abstract": "Recent advances in NLP have significantly improved the performance of\nlanguage models on a variety of tasks. While these advances are largely driven\nby the availability of large amounts of data and computational power, they also\nbenefit from the development of better training methods and architectures. In\nthis paper, we introduce CamemBERTa, a French DeBERTa model that builds upon\nthe DeBERTaV3 architecture and training objective. We evaluate our model's\nperformance on a variety of French downstream tasks and datasets, including\nquestion answering, part-of-speech tagging, dependency parsing, named entity\nrecognition, and the FLUE benchmark, and compare against CamemBERT, the\nstate-of-the-art monolingual model for French. Our results show that, given the\nsame amount of training tokens, our model outperforms BERT-based models trained\nwith MLM on most tasks. Furthermore, our new model reaches similar or superior\nperformance on downstream tasks compared to CamemBERT, despite being trained on\nonly 30% of its total number of input tokens. In addition to our experimental\nresults, we also publicly release the weights and code implementation of\nCamemBERTa, making it the first publicly available DeBERTaV3 model outside of\nthe original paper and the first openly available implementation of a DeBERTaV3\ntraining objective. https://gitlab.inria.fr/almanach/CamemBERTa", "published": "2023-06-02 12:45:34", "link": "http://arxiv.org/abs/2306.01497v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparing a composite model versus chained models to locate a nearest\n  visual object", "abstract": "Extracting information from geographic images and text is crucial for\nautonomous vehicles to determine in advance the best cell stations to connect\nto along their future path. Multiple artificial neural network models can\naddress this challenge; however, there is no definitive guidance on the\nselection of an appropriate model for such use cases. Therefore, we\nexperimented two architectures to solve such a task: a first architecture with\nchained models where each model in the chain addresses a sub-task of the task;\nand a second architecture with a single model that addresses the whole task.\nOur results showed that these two architectures achieved the same level\nperformance with a root mean square error (RMSE) of 0.055 and 0.056; The\nfindings further revealed that when the task can be decomposed into sub-tasks,\nthe chain architecture exhibits a twelve-fold increase in training speed\ncompared to the composite model. Nevertheless, the composite model\nsignificantly alleviates the burden of data labeling.", "published": "2023-06-02 13:58:59", "link": "http://arxiv.org/abs/2306.01551v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EmoUS: Simulating User Emotions in Task-Oriented Dialogues", "abstract": "Existing user simulators (USs) for task-oriented dialogue systems only model\nuser behaviour on semantic and natural language levels without considering the\nuser persona and emotions. Optimising dialogue systems with generic user\npolicies, which cannot model diverse user behaviour driven by different\nemotional states, may result in a high drop-off rate when deployed in the real\nworld. Thus, we present EmoUS, a user simulator that learns to simulate user\nemotions alongside user behaviour. EmoUS generates user emotions, semantic\nactions, and natural language responses based on the user goal, the dialogue\nhistory, and the user persona. By analysing what kind of system behaviour\nelicits what kind of user emotions, we show that EmoUS can be used as a probe\nto evaluate a variety of dialogue systems and in particular their effect on the\nuser's emotional state. Developing such methods is important in the age of\nlarge language model chat-bots and rising ethical concerns.", "published": "2023-06-02 14:48:19", "link": "http://arxiv.org/abs/2306.01579v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning from Partially Annotated Data: Example-aware Creation of\n  Gap-filling Exercises for Language Learning", "abstract": "Since performing exercises (including, e.g., practice tests) forms a crucial\ncomponent of learning, and creating such exercises requires non-trivial effort\nfrom the teacher, there is a great value in automatic exercise generation in\ndigital tools in education. In this paper, we particularly focus on automatic\ncreation of gapfilling exercises for language learning, specifically grammar\nexercises. Since providing any annotation in this domain requires human expert\neffort, we aim to avoid it entirely and explore the task of converting existing\ntexts into new gap-filling exercises, purely based on an example exercise,\nwithout explicit instruction or detailed annotation of the intended grammar\ntopics. We contribute (i) a novel neural network architecture specifically\ndesigned for aforementioned gap-filling exercise generation task, and (ii) a\nreal-world benchmark dataset for French grammar. We show that our model for\nthis French grammar gap-filling exercise generation outperforms a competitive\nbaseline classifier by 8% in F1 percentage points, achieving an average F1\nscore of 82%. Our model implementation and the dataset are made publicly\navailable to foster future research, thus offering a standardized evaluation\nand baseline solution of the proposed partially annotated data prediction task\nin grammar exercise creation.", "published": "2023-06-02 14:54:16", "link": "http://arxiv.org/abs/2306.01584v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model\n  Training", "abstract": "Language models (LMs) often exhibit undesirable text generation behaviors,\nincluding generating false, toxic, or irrelevant outputs. Reinforcement\nlearning from human feedback (RLHF) - where human preference judgments on LM\noutputs are transformed into a learning signal - has recently shown promise in\naddressing these issues. However, such holistic feedback conveys limited\ninformation on long text outputs; it does not indicate which aspects of the\noutputs influenced user preference; e.g., which parts contain what type(s) of\nerrors. In this paper, we use fine-grained human feedback (e.g., which sentence\nis false, which sub-sentence is irrelevant) as an explicit training signal. We\nintroduce Fine-Grained RLHF, a framework that enables training and learning\nfrom reward functions that are fine-grained in two respects: (1) density,\nproviding a reward after every segment (e.g., a sentence) is generated; and (2)\nincorporating multiple reward models associated with different feedback types\n(e.g., factual incorrectness, irrelevance, and information incompleteness). We\nconduct experiments on detoxification and long-form question answering to\nillustrate how learning with such reward functions leads to improved\nperformance, supported by both automatic and human evaluation. Additionally, we\nshow that LM behaviors can be customized using different combinations of\nfine-grained reward models. We release all data, collected human feedback, and\ncodes at https://FineGrainedRLHF.github.io.", "published": "2023-06-02 17:11:37", "link": "http://arxiv.org/abs/2306.01693v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Multi-Step Reasoning by Solving Arithmetic Tasks", "abstract": "Mathematical reasoning is regarded as a necessary ability for Language Models\n(LMs). Recent works demonstrate large LMs' impressive performance in solving\nmath problems. The success is attributed to their Chain-of-Thought (CoT)\nreasoning abilities, i.e., the ability to decompose complex questions into\nstep-by-step reasoning chains, but such ability seems only to emerge from\nmodels with abundant parameters. This work investigates how to incorporate\nrelatively small LMs with the capabilities of multi-step reasoning. We propose\nto inject such abilities by continually pre-training LMs on a synthetic dataset\nMsAT which is composed of Multi-step Arithmetic Tasks. Our experiments on four\nmath word problem datasets show the effectiveness of the proposed method in\nenhancing LMs' math reasoning abilities.", "published": "2023-06-02 17:29:22", "link": "http://arxiv.org/abs/2306.01707v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distilling Efficient Language-Specific Models for Cross-Lingual Transfer", "abstract": "Massively multilingual Transformers (MMTs), such as mBERT and XLM-R, are\nwidely used for cross-lingual transfer learning. While these are pretrained to\nrepresent hundreds of languages, end users of NLP systems are often interested\nonly in individual languages. For such purposes, the MMTs' language coverage\nmakes them unnecessarily expensive to deploy in terms of model size, inference\ntime, energy, and hardware cost. We thus propose to extract compressed,\nlanguage-specific models from MMTs which retain the capacity of the original\nMMTs for cross-lingual transfer. This is achieved by distilling the MMT\nbilingually, i.e., using data from only the source and target language of\ninterest. Specifically, we use a two-phase distillation approach, termed\nBiStil: (i) the first phase distils a general bilingual model from the MMT,\nwhile (ii) the second, task-specific phase sparsely fine-tunes the bilingual\n\"student\" model using a task-tuned variant of the original MMT as its\n\"teacher\". We evaluate this distillation technique in zero-shot cross-lingual\ntransfer across a number of standard cross-lingual benchmarks. The key results\nindicate that the distilled models exhibit minimal degradation in target\nlanguage performance relative to the base MMT despite being significantly\nsmaller and faster. Furthermore, we find that they outperform multilingually\ndistilled models such as DistilmBERT and MiniLMv2 while having a very modest\ntraining budget in comparison, even on a per-language basis. We also show that\nbilingual models distilled from MMTs greatly outperform bilingual models\ntrained from scratch. Our code and models are available at\nhttps://github.com/AlanAnsell/bistil.", "published": "2023-06-02 17:31:52", "link": "http://arxiv.org/abs/2306.01709v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Binary and Ternary Natural Language Generation", "abstract": "Ternary and binary neural networks enable multiplication-free computation and\npromise multiple orders of magnitude efficiency gains over full-precision\nnetworks if implemented on specialized hardware. However, since both the\nparameter and the output space are highly discretized, such networks have\nproven very difficult to optimize. The difficulties are compounded for the\nclass of transformer text generation models due to the sensitivity of the\nattention operation to quantization and the noise-compounding effects of\nautoregressive decoding in the high-cardinality output space. We approach the\nproblem with a mix of statistics-based quantization for the weights and elastic\nquantization of the activations and demonstrate the first ternary and binary\ntransformer models on the downstream tasks of summarization and machine\ntranslation. Our ternary BART base achieves an R1 score of 41 on the\nCNN/DailyMail benchmark, which is merely 3.9 points behind the full model while\nbeing 16x more efficient. Our binary model, while less accurate, achieves a\nhighly non-trivial score of 35.6. For machine translation, we achieved BLEU\nscores of 21.7 and 17.6 on the WMT16 En-Ro benchmark, compared with a full\nprecision mBART model score of 26.8. We also compare our approach in the 8-bit\nactivation setting, where our ternary and even binary weight models can match\nor outperform the best existing 8-bit weight models in the literature. Our code\nand models are available at:\nhttps://github.com/facebookresearch/Ternary_Binary_Transformer", "published": "2023-06-02 18:01:02", "link": "http://arxiv.org/abs/2306.01841v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge of cultural moral norms in large language models", "abstract": "Moral norms vary across cultures. A recent line of work suggests that English\nlarge language models contain human-like moral biases, but these studies\ntypically do not examine moral variation in a diverse cultural setting. We\ninvestigate the extent to which monolingual English language models contain\nknowledge about moral norms in different countries. We consider two levels of\nanalysis: 1) whether language models capture fine-grained moral variation\nacross countries over a variety of topics such as ``homosexuality'' and\n``divorce''; 2) whether language models capture cultural diversity and shared\ntendencies in which topics people around the globe tend to diverge or agree on\nin their moral judgment. We perform our analyses with two public datasets from\nthe World Values Survey (across 55 countries) and PEW global surveys (across 40\ncountries) on morality. We find that pre-trained English language models\npredict empirical moral norms across countries worse than the English moral\nnorms reported previously. However, fine-tuning language models on the survey\ndata improves inference across countries at the expense of a less accurate\nestimate of the English moral norms. We discuss the relevance and challenges of\nincorporating cultural knowledge into the automated inference of moral norms.", "published": "2023-06-02 18:23:35", "link": "http://arxiv.org/abs/2306.01857v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Simple yet Effective Self-Debiasing Framework for Transformer Models", "abstract": "Current Transformer-based natural language understanding (NLU) models heavily\nrely on dataset biases, while failing to handle real-world out-of-distribution\n(OOD) instances. Many methods have been proposed to deal with this issue, but\nthey ignore the fact that the features learned in different layers of\nTransformer-based NLU models are different. In this paper, we first conduct\npreliminary studies to obtain two conclusions: 1) both low- and high-layer\nsentence representations encode common biased features during training; 2) the\nlow-layer sentence representations encode fewer unbiased features than the\nhighlayer ones. Based on these conclusions, we propose a simple yet effective\nself-debiasing framework for Transformer-based NLU models. Concretely, we first\nstack a classifier on a selected low layer. Then, we introduce a residual\nconnection that feeds the low-layer sentence representation to the top-layer\nclassifier. In this way, the top-layer sentence representation will be trained\nto ignore the common biased features encoded by the low-layer sentence\nrepresentation and focus on task-relevant unbiased features. During inference,\nwe remove the residual connection and directly use the top-layer sentence\nrepresentation to make predictions. Extensive experiments and indepth analyses\non NLU tasks show that our framework performs better than several competitive\nbaselines, achieving a new SOTA on all OOD test sets.", "published": "2023-06-02 20:31:58", "link": "http://arxiv.org/abs/2306.01907v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Responsible Task Automation: Empowering Large Language Models as\n  Responsible Task Automators", "abstract": "The recent success of Large Language Models (LLMs) signifies an impressive\nstride towards artificial general intelligence. They have shown a promising\nprospect in automatically completing tasks upon user instructions, functioning\nas brain-like coordinators. The associated risks will be revealed as we\ndelegate an increasing number of tasks to machines for automated completion. A\nbig question emerges: how can we make machines behave responsibly when helping\nhumans automate tasks as personal copilots? In this paper, we explore this\nquestion in depth from the perspectives of feasibility, completeness and\nsecurity. In specific, we present Responsible Task Automation (ResponsibleTA)\nas a fundamental framework to facilitate responsible collaboration between\nLLM-based coordinators and executors for task automation with three empowered\ncapabilities: 1) predicting the feasibility of the commands for executors; 2)\nverifying the completeness of executors; 3) enhancing the security (e.g., the\nprotection of users' privacy). We further propose and compare two paradigms for\nimplementing the first two capabilities. One is to leverage the generic\nknowledge of LLMs themselves via prompt engineering while the other is to adopt\ndomain-specific learnable models. Moreover, we introduce a local memory\nmechanism for achieving the third capability. We evaluate our proposed\nResponsibleTA on UI task automation and hope it could bring more attentions to\nensuring LLMs more responsible in diverse scenarios.", "published": "2023-06-02 02:42:58", "link": "http://arxiv.org/abs/2306.01242v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "KL-Divergence Guided Temperature Sampling", "abstract": "Temperature sampling is a conventional approach to diversify large language\nmodel predictions. As temperature increases, the prediction becomes diverse but\nalso vulnerable to hallucinations -- generating tokens that are sensible but\nnot factual. One common approach to mitigate hallucinations is to provide\nsource/grounding documents and the model is trained to produce predictions that\nbind to and are attributable to the provided source. It appears that there is a\ntrade-off between diversity and attribution. To mitigate any such trade-off, we\npropose to relax the constraint of having a fixed temperature over decoding\nsteps, and a mechanism to guide the dynamic temperature according to its\nrelevance to the source through KL-divergence. Our experiments justifies the\ntrade-off, and shows that our sampling algorithm outperforms the conventional\ntop-k and top-p algorithms in conversational question-answering and\nsummarization tasks.", "published": "2023-06-02 06:11:26", "link": "http://arxiv.org/abs/2306.01286v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improved Training for End-to-End Streaming Automatic Speech Recognition\n  Model with Punctuation", "abstract": "Punctuated text prediction is crucial for automatic speech recognition as it\nenhances readability and impacts downstream natural language processing tasks.\nIn streaming scenarios, the ability to predict punctuation in real-time is\nparticularly desirable but presents a difficult technical challenge. In this\nwork, we propose a method for predicting punctuated text from input speech\nusing a chunk-based Transformer encoder trained with Connectionist Temporal\nClassification (CTC) loss. The acoustic model trained with long sequences by\nconcatenating the input and target sequences can learn punctuation marks\nattached to the end of sentences more effectively. Additionally, by combining\nCTC losses on the chunks and utterances, we achieved both the improved F1 score\nof punctuation prediction and Word Error Rate (WER).", "published": "2023-06-02 06:46:14", "link": "http://arxiv.org/abs/2306.01296v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Text Style Transfer Back-Translation", "abstract": "Back Translation (BT) is widely used in the field of machine translation, as\nit has been proved effective for enhancing translation quality. However, BT\nmainly improves the translation of inputs that share a similar style (to be\nmore specific, translation-like inputs), since the source side of BT data is\nmachine-translated. For natural inputs, BT brings only slight improvements and\nsometimes even adverse effects. To address this issue, we propose Text Style\nTransfer Back Translation (TST BT), which uses a style transfer model to modify\nthe source side of BT data. By making the style of source-side text more\nnatural, we aim to improve the translation of natural inputs. Our experiments\non various language pairs, including both high-resource and low-resource ones,\ndemonstrate that TST BT significantly improves translation performance against\npopular BT benchmarks. In addition, TST BT is proved to be effective in domain\nadaptation so this strategy can be regarded as a general data augmentation\nmethod. Our training code and text style transfer model are open-sourced.", "published": "2023-06-02 07:33:47", "link": "http://arxiv.org/abs/2306.01318v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LyricSIM: A novel Dataset and Benchmark for Similarity Detection in\n  Spanish Song LyricS", "abstract": "In this paper, we present a new dataset and benchmark tailored to the task of\nsemantic similarity in song lyrics. Our dataset, originally consisting of 2775\npairs of Spanish songs, was annotated in a collective annotation experiment by\n63 native annotators. After collecting and refining the data to ensure a high\ndegree of consensus and data integrity, we obtained 676 high-quality annotated\npairs that were used to evaluate the performance of various state-of-the-art\nmonolingual and multilingual language models. Consequently, we established\nbaseline results that we hope will be useful to the community in all future\nacademic and industrial applications conducted in this context.", "published": "2023-06-02 07:48:20", "link": "http://arxiv.org/abs/2306.01325v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "MathChat: Converse to Tackle Challenging Math Problems with LLM Agents", "abstract": "Employing Large Language Models (LLMs) to address mathematical problems is an\nintriguing research endeavor, considering the abundance of math problems\nexpressed in natural language across numerous science and engineering fields.\nLLMs, with their generalized ability, are used as a foundation model to build\nAI agents for different tasks. In this paper, we study the effectiveness of\nutilizing LLM agents to solve math problems through conversations. We propose\nMathChat, a conversational problem-solving framework designed for math\nproblems. MathChat consists of an LLM agent and a user proxy agent which is\nresponsible for tool execution and additional guidance. This synergy\nfacilitates a collaborative problem-solving process, where the agents engage in\na dialogue to solve the problems. We perform evaluation on difficult high\nschool competition problems from the MATH dataset. Utilizing Python, we show\nthat MathChat can further improve previous tool-using prompting methods by 6%.", "published": "2023-06-02 08:02:15", "link": "http://arxiv.org/abs/2306.01337v3", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "ChatGPT for Zero-shot Dialogue State Tracking: A Solution or an\n  Opportunity?", "abstract": "Recent research on dialogue state tracking (DST) focuses on methods that\nallow few- and zero-shot transfer to new domains or schemas. However,\nperformance gains heavily depend on aggressive data augmentation and\nfine-tuning of ever larger language model based architectures. In contrast,\ngeneral purpose language models, trained on large amounts of diverse data, hold\nthe promise of solving any kind of task without task-specific training. We\npresent preliminary experimental results on the ChatGPT research preview,\nshowing that ChatGPT achieves state-of-the-art performance in zero-shot DST.\nDespite our findings, we argue that properties inherent to general purpose\nmodels limit their ability to replace specialized systems. We further theorize\nthat the in-context learning capabilities of such models will likely become\npowerful tools to support the development of dedicated and dynamic dialogue\nstate trackers.", "published": "2023-06-02 09:15:01", "link": "http://arxiv.org/abs/2306.01386v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Driving Context into Text-to-Text Privatization", "abstract": "\\textit{Metric Differential Privacy} enables text-to-text privatization by\nadding calibrated noise to the vector of a word derived from an embedding space\nand projecting this noisy vector back to a discrete vocabulary using a nearest\nneighbor search. Since words are substituted without context, this mechanism is\nexpected to fall short at finding substitutes for words with ambiguous\nmeanings, such as \\textit{'bank'}. To account for these ambiguous words, we\nleverage a sense embedding and incorporate a sense disambiguation step prior to\nnoise injection. We encompass our modification to the privatization mechanism\nwith an estimation of privacy and utility. For word sense disambiguation on the\n\\textit{Words in Context} dataset, we demonstrate a substantial increase in\nclassification accuracy by $6.05\\%$.", "published": "2023-06-02 11:33:06", "link": "http://arxiv.org/abs/2306.01457v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can LLMs like GPT-4 outperform traditional AI tools in dementia\n  diagnosis? Maybe, but not today", "abstract": "Recent investigations show that large language models (LLMs), specifically\nGPT-4, not only have remarkable capabilities in common Natural Language\nProcessing (NLP) tasks but also exhibit human-level performance on various\nprofessional and academic benchmarks. However, whether GPT-4 can be directly\nused in practical applications and replace traditional artificial intelligence\n(AI) tools in specialized domains requires further experimental validation. In\nthis paper, we explore the potential of LLMs such as GPT-4 to outperform\ntraditional AI tools in dementia diagnosis. Comprehensive comparisons between\nGPT-4 and traditional AI tools are conducted to examine their diagnostic\naccuracy in a clinical setting. Experimental results on two real clinical\ndatasets show that, although LLMs like GPT-4 demonstrate potential for future\nadvancements in dementia diagnosis, they currently do not surpass the\nperformance of traditional AI tools. The interpretability and faithfulness of\nGPT-4 are also evaluated by comparison with real doctors. We discuss the\nlimitations of GPT-4 in its current state and propose future research\ndirections to enhance GPT-4 in dementia diagnosis.", "published": "2023-06-02 12:47:45", "link": "http://arxiv.org/abs/2306.01499v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Machine Translation Quality with Conformal Predictive\n  Distributions", "abstract": "This paper presents a new approach for assessing uncertainty in machine\ntranslation by simultaneously evaluating translation quality and providing a\nreliable confidence score. Our approach utilizes conformal predictive\ndistributions to produce prediction intervals with guaranteed coverage, meaning\nthat for any given significance level $\\epsilon$, we can expect the true\nquality score of a translation to fall out of the interval at a rate of\n$1-\\epsilon$. In this paper, we demonstrate how our method outperforms a\nsimple, but effective baseline on six different language pairs in terms of\ncoverage and sharpness. Furthermore, we validate that our approach requires the\ndata exchangeability assumption to hold for optimal performance.", "published": "2023-06-02 13:56:30", "link": "http://arxiv.org/abs/2306.01549v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control\n  for Empathetic Response Generation", "abstract": "Empathy is a crucial factor in open-domain conversations, which naturally\nshows one's caring and understanding to others. Though several methods have\nbeen proposed to generate empathetic responses, existing works often lead to\nmonotonous empathy that refers to generic and safe expressions. In this paper,\nwe propose to use explicit control to guide the empathy expression and design a\nframework DiffusEmp based on conditional diffusion language model to unify the\nutilization of dialogue context and attribute-oriented control signals.\nSpecifically, communication mechanism, intent, and semantic frame are imported\nas multi-grained signals that control the empathy realization from coarse to\nfine levels. We then design a specific masking strategy to reflect the\nrelationship between multi-grained signals and response tokens, and integrate\nit into the diffusion model to influence the generative process. Experimental\nresults on a benchmark dataset EmpatheticDialogue show that our framework\noutperforms competitive baselines in terms of controllability, informativeness,\nand diversity without the loss of context-relatedness.", "published": "2023-06-02 16:26:21", "link": "http://arxiv.org/abs/2306.01657v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Generalization in Task-oriented Dialogues with Workflows and\n  Action Plans", "abstract": "Task-oriented dialogue is difficult in part because it involves understanding\nuser intent, collecting information from the user, executing API calls, and\ngenerating helpful and fluent responses. However, for complex tasks one must\nalso correctly do all of these things over multiple steps, and in a specific\norder. While large pre-trained language models can be fine-tuned end-to-end to\ncreate multi-step task-oriented dialogue agents that generate fluent text, our\nexperiments confirm that this approach alone cannot reliably perform new\nmulti-step tasks that are unseen during training. To address these limitations,\nwe augment the dialogue contexts given to \\textmd{text2text} transformers with\nknown \\textit{valid workflow names} and \\textit{action plans}. Action plans\nconsist of sequences of actions required to accomplish a task, and are encoded\nas simple sequences of keywords (e.g. verify-identity, pull-up-account,\nreset-password, etc.). We perform extensive experiments on the Action-Based\nConversations Dataset (ABCD) with T5-small, base and large models, and show\nthat such models: a) are able to more readily generalize to unseen workflows by\nfollowing the provided plan, and b) are able to generalize to executing unseen\nactions if they are provided in the plan. In contrast, models are unable to\nfully accomplish new multi-step tasks when they are not provided action plan\ninformation, even when given new valid workflow names.", "published": "2023-06-02 17:54:36", "link": "http://arxiv.org/abs/2306.01729v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Word Embeddings for Banking Industry", "abstract": "Applications of Natural Language Processing (NLP) are plentiful, from\nsentiment analysis to text classification. Practitioners rely on static word\nembeddings (e.g. Word2Vec or GloVe) or static word representation from\ncontextual models (e.g. BERT or ELMo) to perform many of these NLP tasks. These\nwidely available word embeddings are built from large amount of text, so they\nare likely to have captured most of the vocabulary in different context.\nHowever, how well would they capture domain-specific semantics and word\nrelatedness? This paper explores this idea by creating a bank-specific word\nembeddings and evaluates them against other sources of word embeddings such as\nGloVe and BERT. Not surprising that embeddings built from bank-specific corpora\ndoes a better job of capturing the bank-specific semantics and word\nrelatedness. This finding suggests that bank-specific word embeddings could be\na good stand-alone source or a complement to other widely available embeddings\nwhen performing NLP tasks specific to the banking industry.", "published": "2023-06-02 01:00:44", "link": "http://arxiv.org/abs/2306.01807v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Structural Similarities Between Language Models and Neural Response\n  Measurements", "abstract": "Large language models (LLMs) have complicated internal dynamics, but induce\nrepresentations of words and phrases whose geometry we can study. Human\nlanguage processing is also opaque, but neural response measurements can\nprovide (noisy) recordings of activation during listening or reading, from\nwhich we can extract similar representations of words and phrases. Here we\nstudy the extent to which the geometries induced by these representations,\nshare similarities in the context of brain decoding. We find that the larger\nneural language models get, the more their representations are structurally\nsimilar to neural response measurements from brain imaging. Code is available\nat \\url{https://github.com/coastalcph/brainlm}.", "published": "2023-06-02 22:09:46", "link": "http://arxiv.org/abs/2306.01930v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Simple Data Augmentation Techniques for Chinese Disease Normalization", "abstract": "Disease name normalization is an important task in the medical domain. It\nclassifies disease names written in various formats into standardized names,\nserving as a fundamental component in smart healthcare systems for various\ndisease-related functions. Nevertheless, the most significant obstacle to\nexisting disease name normalization systems is the severe shortage of training\ndata. Consequently, we present a novel data augmentation approach that includes\na series of data augmentation techniques and some supporting modules to help\nmitigate the problem. Our proposed methods rely on the Structural Invariance\nproperty of disease names and the Hierarchy property of the disease\nclassification system. The goal is to equip the models with extensive\nunderstanding of the disease names and the hierarchical structure of the\ndisease name classification system. Through extensive experimentation, we\nillustrate that our proposed approach exhibits significant performance\nimprovements across various baseline models and training objectives,\nparticularly in scenarios with limited training data.", "published": "2023-06-02 22:12:05", "link": "http://arxiv.org/abs/2306.01931v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Spoken Language Recognition via Multilabel Classification", "abstract": "Spoken language recognition (SLR) is the task of automatically identifying\nthe language present in a speech signal. Existing SLR models are either too\ncomputationally expensive or too large to run effectively on devices with\nlimited resources. For real-world deployment, a model should also gracefully\nhandle unseen languages outside of the target language set, yet prior work has\nfocused on closed-set classification where all input languages are known\na-priori. In this paper we address these two limitations: we explore efficient\nmodel architectures for SLR based on convolutional networks, and propose a\nmultilabel training strategy to handle non-target languages at inference time.\nUsing the VoxLingua107 dataset, we show that our models obtain competitive\nresults while being orders of magnitude smaller and faster than current\nstate-of-the-art methods, and that our multilabel strategy is more robust to\nunseen non-target languages compared to multiclass classification.", "published": "2023-06-02 23:04:19", "link": "http://arxiv.org/abs/2306.01945v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sampling and Ranking for Digital Ink Generation on a tight computational\n  budget", "abstract": "Digital ink (online handwriting) generation has a number of potential\napplications for creating user-visible content, such as handwriting\nautocompletion, spelling correction, and beautification. Writing is personal\nand usually the processing is done on-device. Ink generative models thus need\nto produce high quality content quickly, in a resource constrained environment.\n  In this work, we study ways to maximize the quality of the output of a\ntrained digital ink generative model, while staying within an inference time\nbudget. We use and compare the effect of multiple sampling and ranking\ntechniques, in the first ablation study of its kind in the digital ink domain.\n  We confirm our findings on multiple datasets - writing in English and\nVietnamese, as well as mathematical formulas - using two model types and two\ncommon ink data representations. In all combinations, we report a meaningful\nimprovement in the recognizability of the synthetic inks, in some cases more\nthan halving the character error rate metric, and describe a way to select the\noptimal combination of sampling and ranking techniques for any given\ncomputational budget.", "published": "2023-06-02 09:55:15", "link": "http://arxiv.org/abs/2306.03103v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Streaming Speech-to-Confusion Network Speech Recognition", "abstract": "In interactive automatic speech recognition (ASR) systems, low-latency\nrequirements limit the amount of search space that can be explored during\ndecoding, particularly in end-to-end neural ASR. In this paper, we present a\nnovel streaming ASR architecture that outputs a confusion network while\nmaintaining limited latency, as needed for interactive applications. We show\nthat 1-best results of our model are on par with a comparable RNN-T system,\nwhile the richer hypothesis set allows second-pass rescoring to achieve 10-20\\%\nlower word error rate on the LibriSpeech task. We also show that our model\noutperforms a strong RNN-T baseline on a far-field voice assistant task.", "published": "2023-06-02 20:28:14", "link": "http://arxiv.org/abs/2306.03778v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "How Ready are Pre-trained Abstractive Models and LLMs for Legal Case\n  Judgement Summarization?", "abstract": "Automatic summarization of legal case judgements has traditionally been\nattempted by using extractive summarization methods. However, in recent years,\nabstractive summarization models are gaining popularity since they can generate\nmore natural and coherent summaries. Legal domain-specific pre-trained\nabstractive summarization models are now available. Moreover, general-domain\npre-trained Large Language Models (LLMs), such as ChatGPT, are known to\ngenerate high-quality text and have the capacity for text summarization. Hence\nit is natural to ask if these models are ready for off-the-shelf application to\nautomatically generate abstractive summaries for case judgements. To explore\nthis question, we apply several state-of-the-art domain-specific abstractive\nsummarization models and general-domain LLMs on Indian court case judgements,\nand check the quality of the generated summaries. In addition to standard\nmetrics for summary quality, we check for inconsistencies and hallucinations in\nthe summaries. We see that abstractive summarization models generally achieve\nslightly higher scores than extractive models in terms of standard summary\nevaluation metrics such as ROUGE and BLEU. However, we often find inconsistent\nor hallucinated information in the generated abstractive summaries. Overall,\nour investigation indicates that the pre-trained abstractive summarization\nmodels and LLMs are not yet ready for fully automatic deployment for case\njudgement summarization; rather a human-in-the-loop approach including manual\nchecks for inconsistencies is more suitable at present.", "published": "2023-06-02 03:16:19", "link": "http://arxiv.org/abs/2306.01248v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DistilXLSR: A Light Weight Cross-Lingual Speech Representation Model", "abstract": "Multilingual self-supervised speech representation models have greatly\nenhanced the speech recognition performance for low-resource languages, and the\ncompression of these huge models has also become a crucial prerequisite for\ntheir industrial application. In this paper, we propose DistilXLSR, a distilled\ncross-lingual speech representation model. By randomly shuffling the phonemes\nof existing speech, we reduce the linguistic information and distill\ncross-lingual models using only English data. We also design a layer-jumping\ninitialization method to fully leverage the teacher's pre-trained weights.\nExperiments on 2 kinds of teacher models and 15 low-resource languages show\nthat our method can reduce the parameters by 50% while maintaining\ncross-lingual representation ability. Our method is proven to be generalizable\nto various languages/teacher models and has the potential to improve the\ncross-lingual performance of the English pre-trained models.", "published": "2023-06-02 07:03:06", "link": "http://arxiv.org/abs/2306.01303v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Speech Translation with Foundation Models and Optimal Transport: UPC at\n  IWSLT23", "abstract": "This paper describes the submission of the UPC Machine Translation group to\nthe IWSLT 2023 Offline Speech Translation task. Our Speech Translation systems\nutilize foundation models for speech (wav2vec 2.0) and text (mBART50). We\nincorporate a Siamese pretraining step of the speech and text encoders with CTC\nand Optimal Transport, to adapt the speech representations to the space of the\ntext model, thus maximizing transfer learning from MT. After this pretraining,\nwe fine-tune our system end-to-end on ST, with Cross Entropy and Knowledge\nDistillation. Apart from the available ST corpora, we create synthetic data\nwith SegAugment to better adapt our models to the custom segmentations of the\nIWSLT test sets. Our best single model obtains 31.2 BLEU points on MuST-C\ntst-COMMON, 29.8 points on IWLST.tst2020 and 33.4 points on the newly released\nIWSLT.ACLdev2023.", "published": "2023-06-02 07:48:37", "link": "http://arxiv.org/abs/2306.01327v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Task-Agnostic Structured Pruning of Speech Representation Models", "abstract": "Self-supervised pre-trained models such as Wav2vec2, Hubert, and WavLM have\nbeen shown to significantly improve many speech tasks. However, their large\nmemory and strong computational requirements hinder their industrial\napplicability. Structured pruning is a hardware-friendly model compression\ntechnique but usually results in a larger loss of accuracy. In this paper, we\npropose a fine-grained attention head pruning method to compensate for the\nperformance degradation. In addition, we also introduce the straight through\nestimator into the L0 regularization to further accelerate the pruned model.\nExperiments on the SUPERB benchmark show that our model can achieve comparable\nperformance to the dense model in multiple tasks and outperforms the Wav2vec\n2.0 base model on average, with 72% fewer parameters and 2 times faster\ninference speed.", "published": "2023-06-02 09:11:06", "link": "http://arxiv.org/abs/2306.01385v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Knowledge Graph Reasoning over Entities and Numerical Values", "abstract": "A complex logic query in a knowledge graph refers to a query expressed in\nlogic form that conveys a complex meaning, such as where did the Canadian\nTuring award winner graduate from? Knowledge graph reasoning-based\napplications, such as dialogue systems and interactive search engines, rely on\nthe ability to answer complex logic queries as a fundamental task. In most\nknowledge graphs, edges are typically used to either describe the relationships\nbetween entities or their associated attribute values. An attribute value can\nbe in categorical or numerical format, such as dates, years, sizes, etc.\nHowever, existing complex query answering (CQA) methods simply treat numerical\nvalues in the same way as they treat entities. This can lead to difficulties in\nanswering certain queries, such as which Australian Pulitzer award winner is\nborn before 1927, and which drug is a pain reliever and has fewer side effects\nthan Paracetamol. In this work, inspired by the recent advances in numerical\nencoding and knowledge graph reasoning, we propose numerical complex query\nanswering. In this task, we introduce new numerical variables and operations to\ndescribe queries involving numerical attribute values. To address the\ndifference between entities and numerical values, we also propose the framework\nof Number Reasoning Network (NRN) for alternatively encoding entities and\nnumerical values into separate encoding structures. During the numerical\nencoding process, NRN employs a parameterized density function to encode the\ndistribution of numerical values. During the entity encoding process, NRN uses\nestablished query encoding methods for the original CQA problem. Experimental\nresults show that NRN consistently improves various query encoding methods on\nthree different knowledge graphs and achieves state-of-the-art results.", "published": "2023-06-02 09:46:29", "link": "http://arxiv.org/abs/2306.01399v1", "categories": ["cs.AI", "cs.CL", "cs.LO"], "primary_category": "cs.AI"}
{"title": "Towards Robust FastSpeech 2 by Modelling Residual Multimodality", "abstract": "State-of-the-art non-autoregressive text-to-speech (TTS) models based on\nFastSpeech 2 can efficiently synthesise high-fidelity and natural speech. For\nexpressive speech datasets however, we observe characteristic audio\ndistortions. We demonstrate that such artefacts are introduced to the vocoder\nreconstruction by over-smooth mel-spectrogram predictions, which are induced by\nthe choice of mean-squared-error (MSE) loss for training the mel-spectrogram\ndecoder. With MSE loss FastSpeech 2 is limited to learn conditional averages of\nthe training distribution, which might not lie close to a natural sample if the\ndistribution still appears multimodal after all conditioning signals. To\nalleviate this problem, we introduce TVC-GMM, a mixture model of\nTrivariate-Chain Gaussian distributions, to model the residual multimodality.\nTVC-GMM reduces spectrogram smoothness and improves perceptual audio quality in\nparticular for expressive datasets as shown by both objective and subjective\nevaluation.", "published": "2023-06-02 11:03:26", "link": "http://arxiv.org/abs/2306.01442v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unsupervised Paraphrasing of Multiword Expressions", "abstract": "We propose an unsupervised approach to paraphrasing multiword expressions\n(MWEs) in context. Our model employs only monolingual corpus data and\npre-trained language models (without fine-tuning), and does not make use of any\nexternal resources such as dictionaries. We evaluate our method on the SemEval\n2022 idiomatic semantic text similarity task, and show that it outperforms all\nunsupervised systems and rivals supervised systems.", "published": "2023-06-02 11:06:48", "link": "http://arxiv.org/abs/2306.01443v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Guiding Text-to-Text Privatization by Syntax", "abstract": "Metric Differential Privacy is a generalization of differential privacy\ntailored to address the unique challenges of text-to-text privatization. By\nadding noise to the representation of words in the geometric space of\nembeddings, words are replaced with words located in the proximity of the noisy\nrepresentation. Since embeddings are trained based on word co-occurrences, this\nmechanism ensures that substitutions stem from a common semantic context.\nWithout considering the grammatical category of words, however, this mechanism\ncannot guarantee that substitutions play similar syntactic roles. We analyze\nthe capability of text-to-text privatization to preserve the grammatical\ncategory of words after substitution and find that surrogate texts consist\nalmost exclusively of nouns. Lacking the capability to produce surrogate texts\nthat correlate with the structure of the sensitive texts, we encompass our\nanalysis by transforming the privatization step into a candidate selection\nproblem in which substitutions are directed to words with matching grammatical\nproperties. We demonstrate a substantial improvement in the performance of\ndownstream tasks by up to $4.66\\%$ while retaining comparative privacy\nguarantees.", "published": "2023-06-02 11:52:21", "link": "http://arxiv.org/abs/2306.01471v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Supervised Adversarial Contrastive Learning for Emotion Recognition in\n  Conversations", "abstract": "Extracting generalized and robust representations is a major challenge in\nemotion recognition in conversations (ERC). To address this, we propose a\nsupervised adversarial contrastive learning (SACL) framework for learning\nclass-spread structured representations in a supervised manner. SACL applies\ncontrast-aware adversarial training to generate worst-case samples and uses\njoint class-spread contrastive learning to extract structured representations.\nIt can effectively utilize label-level feature consistency and retain\nfine-grained intra-class features. To avoid the negative impact of adversarial\nperturbations on context-dependent data, we design a contextual adversarial\ntraining (CAT) strategy to learn more diverse features from context and enhance\nthe model's context robustness. Under the framework with CAT, we develop a\nsequence-based SACL-LSTM to learn label-consistent and context-robust features\nfor ERC. Experiments on three datasets show that SACL-LSTM achieves\nstate-of-the-art performance on ERC. Extended experiments prove the\neffectiveness of SACL and CAT.", "published": "2023-06-02 12:52:38", "link": "http://arxiv.org/abs/2306.01505v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BabySLM: language-acquisition-friendly benchmark of self-supervised\n  spoken language models", "abstract": "Self-supervised techniques for learning speech representations have been\nshown to develop linguistic competence from exposure to speech without the need\nfor human labels. In order to fully realize the potential of these approaches\nand further our understanding of how infants learn language, simulations must\nclosely emulate real-life situations by training on developmentally plausible\ncorpora and benchmarking against appropriate test sets. To this end, we propose\na language-acquisition-friendly benchmark to probe spoken language models at\nthe lexical and syntactic levels, both of which are compatible with the\nvocabulary typical of children's language experiences. This paper introduces\nthe benchmark and summarizes a range of experiments showing its usefulness. In\naddition, we highlight two exciting challenges that need to be addressed for\nfurther progress: bridging the gap between text and speech and between clean\nspeech and in-the-wild speech.", "published": "2023-06-02 12:54:38", "link": "http://arxiv.org/abs/2306.01506v2", "categories": ["cs.CL", "eess.AS", "stat.ML"], "primary_category": "cs.CL"}
{"title": "PassGPT: Password Modeling and (Guided) Generation with Large Language\n  Models", "abstract": "Large language models (LLMs) successfully model natural language from vast\namounts of text without the need for explicit supervision. In this paper, we\ninvestigate the efficacy of LLMs in modeling passwords. We present PassGPT, a\nLLM trained on password leaks for password generation. PassGPT outperforms\nexisting methods based on generative adversarial networks (GAN) by guessing\ntwice as many previously unseen passwords. Furthermore, we introduce the\nconcept of guided password generation, where we leverage PassGPT sampling\nprocedure to generate passwords matching arbitrary constraints, a feat lacking\nin current GAN-based strategies. Lastly, we conduct an in-depth analysis of the\nentropy and probability distribution that PassGPT defines over passwords and\ndiscuss their use in enhancing existing password strength estimators.", "published": "2023-06-02 13:49:53", "link": "http://arxiv.org/abs/2306.01545v2", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "TIES-Merging: Resolving Interference When Merging Models", "abstract": "Transfer learning - i.e., further fine-tuning a pre-trained model on a\ndownstream task - can confer significant advantages, including improved\ndownstream performance, faster convergence, and better sample efficiency. These\nadvantages have led to a proliferation of task-specific fine-tuned models,\nwhich typically can only perform a single task and do not benefit from one\nanother. Recently, model merging techniques have emerged as a solution to\ncombine multiple task-specific models into a single multitask model without\nperforming additional training. However, existing merging methods often ignore\nthe interference between parameters of different models, resulting in large\nperformance drops when merging multiple models. In this paper, we demonstrate\nthat prior merging techniques inadvertently lose valuable information due to\ntwo major sources of interference: (a) interference due to redundant parameter\nvalues and (b) disagreement on the sign of a given parameter's values across\nmodels. To address this, we propose our method, TRIM, ELECT SIGN & MERGE\n(TIES-Merging), which introduces three novel steps when merging models: (1)\nresetting parameters that only changed a small amount during fine-tuning, (2)\nresolving sign conflicts, and (3) merging only the parameters that are in\nalignment with the final agreed-upon sign. We find that TIES-Merging\noutperforms several existing methods in diverse settings covering a range of\nmodalities, domains, number of tasks, model sizes, architectures, and\nfine-tuning settings. We further analyze the impact of different types of\ninterference on model parameters, and highlight the importance of resolving\nsign interference. Our code is available at\nhttps://github.com/prateeky2806/ties-merging", "published": "2023-06-02 17:31:32", "link": "http://arxiv.org/abs/2306.01708v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "DocFormerv2: Local Features for Document Understanding", "abstract": "We propose DocFormerv2, a multi-modal transformer for Visual Document\nUnderstanding (VDU). The VDU domain entails understanding documents (beyond\nmere OCR predictions) e.g., extracting information from a form, VQA for\ndocuments and other tasks. VDU is challenging as it needs a model to make sense\nof multiple modalities (visual, language and spatial) to make a prediction. Our\napproach, termed DocFormerv2 is an encoder-decoder transformer which takes as\ninput - vision, language and spatial features. DocFormerv2 is pre-trained with\nunsupervised tasks employed asymmetrically i.e., two novel document tasks on\nencoder and one on the auto-regressive decoder. The unsupervised tasks have\nbeen carefully designed to ensure that the pre-training encourages\nlocal-feature alignment between multiple modalities. DocFormerv2 when evaluated\non nine datasets shows state-of-the-art performance over strong baselines e.g.\nTabFact (4.3%), InfoVQA (1.4%), FUNSD (1%). Furthermore, to show generalization\ncapabilities, on three VQA tasks involving scene-text, Doc- Formerv2\noutperforms previous comparably-sized models and even does better than much\nlarger models (such as GIT2, PaLi and Flamingo) on some tasks. Extensive\nablations show that due to its pre-training, DocFormerv2 understands multiple\nmodalities better than prior-art in VDU.", "published": "2023-06-02 17:58:03", "link": "http://arxiv.org/abs/2306.01733v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Multilingual Conceptual Coverage in Text-to-Image Models", "abstract": "We propose \"Conceptual Coverage Across Languages\" (CoCo-CroLa), a technique\nfor benchmarking the degree to which any generative text-to-image system\nprovides multilingual parity to its training language in terms of tangible\nnouns. For each model we can assess \"conceptual coverage\" of a given target\nlanguage relative to a source language by comparing the population of images\ngenerated for a series of tangible nouns in the source language to the\npopulation of images generated for each noun under translation in the target\nlanguage. This technique allows us to estimate how well-suited a model is to a\ntarget language as well as identify model-specific weaknesses, spurious\ncorrelations, and biases without a-priori assumptions. We demonstrate how it\ncan be used to benchmark T2I models in terms of multilinguality, and how\ndespite its simplicity it is a good proxy for impressive generalization.", "published": "2023-06-02 17:59:09", "link": "http://arxiv.org/abs/2306.01735v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "eess.IV"], "primary_category": "cs.CL"}
{"title": "Beta Thalassemia Carriers detection empowered federated Learning", "abstract": "Thalassemia is a group of inherited blood disorders that happen when\nhemoglobin, the protein in red blood cells that carries oxygen, is not made\nenough. It is found all over the body and is needed for survival. If both\nparents have thalassemia, a child's chance of getting it increases. Genetic\ncounselling and early diagnosis are essential for treating thalassemia and\nstopping it from being passed on to future generations. It may be hard for\nhealthcare professionals to differentiate between people with thalassemia\ncarriers and those without. The current blood tests for beta thalassemia\ncarriers are too expensive, take too long, and require too much screening\nequipment. The World Health Organization says there is a high death rate for\npeople with thalassemia. Therefore, it is essential to find thalassemia\ncarriers to act quickly. High-performance liquid chromatography (HPLC), the\nstandard test method, has problems such as cost, time, and equipment needs. So,\nthere must be a quick and cheap way to find people carrying the thalassemia\ngene. Using federated learning (FL) techniques, this study shows a new way to\nfind people with the beta-thalassemia gene. FL allows data to be collected and\nprocessed on-site while following privacy rules, making it an excellent choice\nfor sensitive health data. Researchers used FL to train a model for\nbeta-thalassemia carriers by looking at the complete blood count results and\nred blood cell indices. The model was 92.38 % accurate at telling the\ndifference between beta-thalassemia carriers and people who did not have the\ndisease. The proposed FL model is better than other published methods in terms\nof how well it works, how reliable it is, and how private it is. This research\nshows a promising, quick, accurate, and low-cost way to find thalassemia\ncarriers and opens the door for screening them on a large scale.", "published": "2023-06-02 11:59:57", "link": "http://arxiv.org/abs/2306.01818v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "5IDER: Unified Query Rewriting for Steering, Intent Carryover,\n  Disfluencies, Entity Carryover and Repair", "abstract": "Providing voice assistants the ability to navigate multi-turn conversations\nis a challenging problem. Handling multi-turn interactions requires the system\nto understand various conversational use-cases, such as steering, intent\ncarryover, disfluencies, entity carryover, and repair. The complexity of this\nproblem is compounded by the fact that these use-cases mix with each other,\noften appearing simultaneously in natural language. This work proposes a\nnon-autoregressive query rewriting architecture that can handle not only the\nfive aforementioned tasks, but also complex compositions of these use-cases. We\nshow that our proposed model has competitive single task performance compared\nto the baseline approach, and even outperforms a fine-tuned T5 model in\nuse-case compositions, despite being 15 times smaller in parameters and 25\ntimes faster in latency.", "published": "2023-06-02 18:17:52", "link": "http://arxiv.org/abs/2306.01855v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Revisiting the Role of Language Priors in Vision-Language Models", "abstract": "Vision-language models (VLMs) are impactful in part because they can be\napplied to a variety of visual understanding tasks in a zero-shot fashion,\nwithout any fine-tuning. We study $\\textit{generative VLMs}$ that are trained\nfor next-word generation given an image. We explore their zero-shot performance\non the illustrative task of image-text retrieval across 8 popular\nvision-language benchmarks. Our first observation is that they can be\nrepurposed for discriminative tasks (such as image-text retrieval) by simply\ncomputing the match score of generating a particular text string given an\nimage. We call this probabilistic score the $\\textit{Visual Generative\nPre-Training Score}$ (VisualGPTScore). While the VisualGPTScore produces\nnear-perfect accuracy on some retrieval benchmarks, it yields poor accuracy on\nothers. We analyze this behavior through a probabilistic lens, pointing out\nthat some benchmarks inadvertently capture unnatural language distributions by\ncreating adversarial but unlikely text captions. In fact, we demonstrate that\neven a \"blind\" language model that ignores any image evidence can sometimes\noutperform all prior art, reminiscent of similar challenges faced by the\nvisual-question answering (VQA) community many years ago. We derive a\nprobabilistic post-processing scheme that controls for the amount of linguistic\nbias in generative VLMs at test time without having to retrain or fine-tune the\nmodel. We show that the VisualGPTScore, when appropriately debiased, is a\nstrong zero-shot baseline for vision-language understanding, oftentimes\nproducing state-of-the-art accuracy.", "published": "2023-06-02 19:19:43", "link": "http://arxiv.org/abs/2306.01879v4", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "LIC-GAN: Language Information Conditioned Graph Generative GAN Model", "abstract": "Deep generative models for Natural Language data offer a new angle on the\nproblem of graph synthesis: by optimizing differentiable models that directly\ngenerate graphs, it is possible to side-step expensive search procedures in the\ndiscrete and vast space of possible graphs. We introduce LIC-GAN, an implicit,\nlikelihood-free generative model for small graphs that circumvents the need for\nexpensive graph matching procedures. Our method takes as input a natural\nlanguage query and using a combination of language modelling and Generative\nAdversarial Networks (GANs) and returns a graph that closely matches the\ndescription of the query. We combine our approach with a reward network to\nfurther enhance the graph generation with desired properties. Our experiments,\nshow that LIC-GAN does well on metrics such as PropMatch and Closeness getting\nscores of 0.36 and 0.48. We also show that LIC-GAN performs as good as ChatGPT,\nwith ChatGPT getting scores of 0.40 and 0.42. We also conduct a few experiments\nto demonstrate the robustness of our method, while also highlighting a few\ninteresting caveats of the model.", "published": "2023-06-02 22:39:14", "link": "http://arxiv.org/abs/2306.01937v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Can Contextual Biasing Remain Effective with Whisper and GPT-2?", "abstract": "End-to-end automatic speech recognition (ASR) and large language models, such\nas Whisper and GPT-2, have recently been scaled to use vast amounts of training\ndata. Despite the large amount of training data, infrequent content words that\noccur in a particular task may still exhibit poor ASR performance, with\ncontextual biasing a possible remedy. This paper investigates the effectiveness\nof neural contextual biasing for Whisper combined with GPT-2. Specifically,\nthis paper proposes integrating an adapted tree-constrained pointer generator\n(TCPGen) component for Whisper and a dedicated training scheme to dynamically\nadjust the final output without modifying any Whisper model parameters.\nExperiments across three datasets show a considerable reduction in errors on\nbiasing words with a biasing list of 1000 words. Contextual biasing was more\neffective when applied to domain-specific data and can boost the performance of\nWhisper and GPT-2 without losing their generality.", "published": "2023-06-02 22:56:01", "link": "http://arxiv.org/abs/2306.01942v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "NLPositionality: Characterizing Design Biases of Datasets and Models", "abstract": "Design biases in NLP systems, such as performance differences for different\npopulations, often stem from their creator's positionality, i.e., views and\nlived experiences shaped by identity and background. Despite the prevalence and\nrisks of design biases, they are hard to quantify because researcher, system,\nand dataset positionality is often unobserved. We introduce NLPositionality, a\nframework for characterizing design biases and quantifying the positionality of\nNLP datasets and models. Our framework continuously collects annotations from a\ndiverse pool of volunteer participants on LabintheWild, and statistically\nquantifies alignment with dataset labels and model predictions. We apply\nNLPositionality to existing datasets and models for two tasks -- social\nacceptability and hate speech detection. To date, we have collected 16,299\nannotations in over a year for 600 instances from 1,096 annotators across 87\ncountries. We find that datasets and models align predominantly with Western,\nWhite, college-educated, and younger populations. Additionally, certain groups,\nsuch as non-binary people and non-native English speakers, are further\nmarginalized by datasets and models as they rank least in alignment across all\ntasks. Finally, we draw from prior literature to discuss how researchers can\nexamine their own positionality and that of their datasets and models, opening\nthe door for more inclusive NLP systems.", "published": "2023-06-02 23:02:09", "link": "http://arxiv.org/abs/2306.01943v1", "categories": ["cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "EdGCon: Auto-assigner of Iconicity Ratings Grounded by Lexical\n  Properties to Aid in Generation of Technical Gestures", "abstract": "Gestures that share similarities in their forms and are related in their\nmeanings, should be easier for learners to recognize and incorporate into their\nexisting lexicon. In that regard, to be more readily accepted as standard by\nthe Deaf and Hard of Hearing community, technical gestures in American Sign\nLanguage (ASL) will optimally share similar in forms with their lexical\nneighbors. We utilize a lexical database of ASL, ASL-LEX, to identify lexical\nrelations within a set of technical gestures. We use automated identification\nfor 3 unique sub-lexical properties in ASL- location, handshape and movement.\nEdGCon assigned an iconicity rating based on the lexical property similarities\nof the new gesture with an existing set of technical gestures and the\nrelatedness of the meaning of the new technical word to that of the existing\nset of technical words. We collected 30 ad hoc crowdsourced technical gestures\nfrom different internet websites and tested them against 31 gestures from the\nDeafTEC technical corpus. We found that EdGCon was able to correctly\nauto-assign the iconicity ratings 80.76% of the time.", "published": "2023-06-02 23:04:01", "link": "http://arxiv.org/abs/2306.01944v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "ChatGPT is a Remarkable Tool -- For Experts", "abstract": "This paper investigates the capabilities of ChatGPT as an automated assistant\nin diverse domains, including scientific writing, mathematics, education,\nprogramming, and healthcare. We explore the potential of ChatGPT to enhance\nproductivity, streamline problem-solving processes, and improve writing style.\nFurthermore, we highlight the potential risks associated with excessive\nreliance on ChatGPT in these fields. These limitations encompass factors like\nincorrect and fictitious responses, inaccuracies in code, limited logical\nreasoning abilities, overconfidence, and critical ethical concerns of\ncopyrights and privacy violation. We outline areas and objectives where ChatGPT\nproves beneficial, applications where it should be used judiciously, and\nscenarios where its reliability may be limited. In light of observed\nlimitations, and given that the tool's fundamental errors may pose a special\nchallenge for non-experts, ChatGPT should be used with a strategic methodology.\nBy drawing from comprehensive experimental studies, we offer methods and flow\ncharts for effectively using ChatGPT. Our recommendations emphasize iterative\ninteraction with ChatGPT and independent verification of its outputs.\nConsidering the importance of utilizing ChatGPT judiciously and with expertise,\nwe recommend its usage for experts who are well-versed in the respective\ndomains.", "published": "2023-06-02 06:28:21", "link": "http://arxiv.org/abs/2306.03102v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "Interpretable and Explainable Logical Policies via Neurally Guided\n  Symbolic Abstraction", "abstract": "The limited priors required by neural networks make them the dominating\nchoice to encode and learn policies using reinforcement learning (RL). However,\nthey are also black-boxes, making it hard to understand the agent's behaviour,\nespecially when working on the image level. Therefore, neuro-symbolic RL aims\nat creating policies that are interpretable in the first place. Unfortunately,\ninterpretability is not explainability. To achieve both, we introduce Neurally\ngUided Differentiable loGic policiEs (NUDGE). NUDGE exploits trained neural\nnetwork-based agents to guide the search of candidate-weighted logic rules,\nthen uses differentiable logic to train the logic agents. Our experimental\nevaluation demonstrates that NUDGE agents can induce interpretable and\nexplainable policies while outperforming purely neural ones and showing good\nflexibility to environments of different initial states and problem sizes.", "published": "2023-06-02 10:59:44", "link": "http://arxiv.org/abs/2306.01439v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.LO", "cs.SC"], "primary_category": "cs.LG"}
{"title": "Tensor decomposition for minimization of E2E SLU model toward on-device\n  processing", "abstract": "Spoken Language Understanding (SLU) is a critical speech recognition\napplication and is often deployed on edge devices. Consequently, on-device\nprocessing plays a significant role in the practical implementation of SLU.\nThis paper focuses on the end-to-end (E2E) SLU model due to its small latency\nproperty, unlike a cascade system, and aims to minimize the computational cost.\nWe reduce the model size by applying tensor decomposition to the Conformer and\nE-Branchformer architectures used in our E2E SLU models. We propose to apply\nsingular value decomposition to linear layers and the Tucker decomposition to\nconvolution layers, respectively. We also compare COMP/PARFAC decomposition and\nTensor-Train decomposition to the Tucker decomposition. Since the E2E model is\nrepresented by a single neural network, our tensor decomposition can flexibly\ncontrol the number of parameters without changing feature dimensions. On the\nSTOP dataset, we achieved 70.9% exact match accuracy under the tight constraint\nof only 15 million parameters.", "published": "2023-06-02 03:14:44", "link": "http://arxiv.org/abs/2306.01247v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "On Crowdsourcing-design with Comparison Category Rating for Evaluating\n  Speech Enhancement Algorithms", "abstract": "Speech enhancement techniques improve the quality or the intelligibility of\nan audio signal by removing unwanted noise. It is used as preprocessing in\nnumerous applications such as speech recognition, hearing aids, broadcasting\nand telephony. The evaluation of such algorithms often relies on\nreference-based objective metrics that are shown to correlate poorly with human\nperception. In order to evaluate audio quality as perceived by human observers\nit is thus fundamental to resort to subjective quality assessment. In this\npaper, a user evaluation based on crowdsourcing (subjective) and the Comparison\nCategory Rating (CCR) method is compared against the DNSMOS, ViSQOL and 3QUEST\n(objective) metrics. The overall quality scores of three speech enhancement\nalgorithms from real time communications (RTC) are used in the comparison using\nthe P.808 toolkit. Results indicate that while the CCR scale allows\nparticipants to identify differences between processed and unprocessed audio\nsamples, two groups of preferences emerge: some users rate positively by\nfocusing on noise suppression processing, while others rate negatively by\nfocusing mainly on speech quality. We further present results on the\nparameters, size considerations and speaker variations that are critical and\nshould be considered when designing the CCR-based crowdsourcing evaluation.", "published": "2023-06-02 13:41:39", "link": "http://arxiv.org/abs/2306.01538v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Non-uniform Speaker Disentanglement For Depression Detection From Raw\n  Speech Signals", "abstract": "While speech-based depression detection methods that use speaker-identity\nfeatures, such as speaker embeddings, are popular, they often compromise\npatient privacy. To address this issue, we propose a speaker disentanglement\nmethod that utilizes a non-uniform mechanism of adversarial SID loss\nmaximization. This is achieved by varying the adversarial weight between\ndifferent layers of a model during training. We find that a greater adversarial\nweight for the initial layers leads to performance improvement. Our approach\nusing the ECAPA-TDNN model achieves an F1-score of 0.7349 (a 3.7% improvement\nover audio-only SOTA) on the DAIC-WoZ dataset, while simultaneously reducing\nthe speaker-identification accuracy by 50%. Our findings suggest that\nidentifying depression through speech signals can be accomplished without\nplacing undue reliance on a speaker's identity, paving the way for\nprivacy-preserving approaches of depression detection.", "published": "2023-06-02 18:33:19", "link": "http://arxiv.org/abs/2306.01861v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Speaker-independent neural formant synthesis", "abstract": "We describe speaker-independent speech synthesis driven by a small set of\nphonetically meaningful speech parameters such as formant frequencies. The\nintention is to leverage deep-learning advances to provide a highly realistic\nsignal generator that includes control affordances required for stimulus\ncreation in the speech sciences. Our approach turns input speech parameters\ninto predicted mel-spectrograms, which are rendered into waveforms by a\npre-trained neural vocoder. Experiments with WaveNet and HiFi-GAN confirm that\nthe method achieves our goals of accurate control over speech parameters\ncombined with high perceptual audio quality. We also find that the small set of\nphonetically relevant speech parameters we use is sufficient to allow for\nspeaker-independent synthesis (a.k.a. universal vocoding).", "published": "2023-06-02 23:33:42", "link": "http://arxiv.org/abs/2306.01957v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "HD-DEMUCS: General Speech Restoration with Heterogeneous Decoders", "abstract": "This paper introduces an end-to-end neural speech restoration model,\nHD-DEMUCS, demonstrating efficacy across multiple distortion environments.\nUnlike conventional approaches that employ cascading frameworks to remove\nundesirable noise first and then restore missing signal components, our model\nperforms these tasks in parallel using two heterogeneous decoder networks.\nBased on the U-Net style encoder-decoder framework, we attach an additional\ndecoder so that each decoder network performs noise suppression or restoration\nseparately. We carefully design each decoder architecture to operate\nappropriately depending on its objectives. Additionally, we improve performance\nby leveraging a learnable weighting factor, aggregating the two decoder output\nwaveforms. Experimental results with objective metrics across various\nenvironments clearly demonstrate the effectiveness of our approach over a\nsingle decoder or multi-stage systems for general speech restoration task.", "published": "2023-06-02 10:03:09", "link": "http://arxiv.org/abs/2306.01411v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio-Visual Speech Enhancement with Score-Based Generative Models", "abstract": "This paper introduces an audio-visual speech enhancement system that\nleverages score-based generative models, also known as diffusion models,\nconditioned on visual information. In particular, we exploit audio-visual\nembeddings obtained from a self-super\\-vised learning model that has been\nfine-tuned on lipreading. The layer-wise features of its transformer-based\nencoder are aggregated, time-aligned, and incorporated into the noise\nconditional score network. Experimental evaluations show that the proposed\naudio-visual speech enhancement system yields improved speech quality and\nreduces generative artifacts such as phonetic confusions with respect to the\naudio-only equivalent. The latter is supported by the word error rate of a\ndownstream automatic speech recognition model, which decreases noticeably,\nespecially at low input signal-to-noise ratios.", "published": "2023-06-02 10:43:42", "link": "http://arxiv.org/abs/2306.01432v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Auditory Representation Effective for Estimating Vocal Tract Information", "abstract": "We can estimate the size of the speakers based on their speech sounds alone.\nWe had proposed an auditory computational theory of the Stabilised\nWavelet-Mellin Transform (SWMT), which segregates information about the size\nand shape of the vocal tract and glottal vibration, to explain this\nobservation. It has been shown that the auditory representation or excitation\npattern (EP) associated with a weighting function based on the SWMT, termed the\n``SSI weight,'' can account for the psychometric functions of size perception.\nIn this study, we investigated whether EP with SSI weight can accurately\nestimate vocal tract lengths (VTLs) which were measured by magnetic resonance\nimaging (MRI) in male and female subjects. It was found that the use of SSI\nweight significantly improved the VTL estimation. Furthermore, the estimation\nerrors in the EP with the SSI weight were significantly smaller than those in\nthe commonly used spectra derived from the Fourier transform, Mel filterbank,\nand WORLD vocoder. It was also shown that the SSI weight can be easily\nintroduced into these spectra to improve the performance.", "published": "2023-06-02 13:15:48", "link": "http://arxiv.org/abs/2306.01522v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enhance Temporal Relations in Audio Captioning with Sound Event\n  Detection", "abstract": "Automated audio captioning aims at generating natural language descriptions\nfor given audio clips, not only detecting and classifying sounds, but also\nsummarizing the relationships between audio events. Recent research advances in\naudio captioning have introduced additional guidance to improve the accuracy of\naudio events in generated sentences. However, temporal relations between audio\nevents have received little attention while revealing complex relations is a\nkey component in summarizing audio content. Therefore, this paper aims to\nbetter capture temporal relationships in caption generation with sound event\ndetection (SED), a task that locates events' timestamps. We investigate the\nbest approach to integrate temporal information in a captioning model and\npropose a temporal tag system to transform the timestamps into comprehensible\nrelations. Results evaluated by the proposed temporal metrics suggest that\ngreat improvement is achieved in terms of temporal relation generation.", "published": "2023-06-02 13:36:34", "link": "http://arxiv.org/abs/2306.01533v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Q&A: Query-Based Representation Learning for Multi-Track Symbolic Music\n  re-Arrangement", "abstract": "Music rearrangement is a common music practice of reconstructing and\nreconceptualizing a piece using new composition or instrumentation styles,\nwhich is also an important task of automatic music generation. Existing studies\ntypically model the mapping from a source piece to a target piece via\nsupervised learning. In this paper, we tackle rearrangement problems via\nself-supervised learning, in which the mapping styles can be regarded as\nconditions and controlled in a flexible way. Specifically, we are inspired by\nthe representation disentanglement idea and propose Q&A, a query-based\nalgorithm for multi-track music rearrangement under an encoder-decoder\nframework. Q&A learns both a content representation from the mixture and\nfunction (style) representations from each individual track, while the latter\nqueries the former in order to rearrange a new piece. Our current model focuses\non popular music and provides a controllable pathway to four scenarios: 1)\nre-instrumentation, 2) piano cover generation, 3) orchestration, and 4) voice\nseparation. Experiments show that our query system achieves high-quality\nrearrangement results with delicate multi-track structures, significantly\noutperforming the baselines.", "published": "2023-06-02 15:53:30", "link": "http://arxiv.org/abs/2306.01635v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-View Multi-Task Representation Learning for Mispronunciation\n  Detection", "abstract": "The disparity in phonology between learner's native (L1) and target (L2)\nlanguage poses a significant challenge for mispronunciation detection and\ndiagnosis (MDD) systems. This challenge is further intensified by lack of\nannotated L2 data. This paper proposes a novel MDD architecture that exploits\nmultiple `views' of the same input data assisted by auxiliary tasks to learn\nmore distinctive phonetic representation in a low-resource setting. Using the\nmono- and multilingual encoders, the model learn multiple views of the input,\nand capture the sound properties across diverse languages and accents. These\nencoded representations are further enriched by learning articulatory features\nin a multi-task setup. Our reported results using the L2-ARCTIC data\noutperformed the SOTA models, with a phoneme error rate reduction of 11.13% and\n8.60% and absolute F1 score increase of 5.89%, and 2.49% compared to the\nsingle-view mono- and multilingual systems, with a limited L2 dataset.", "published": "2023-06-02 18:04:38", "link": "http://arxiv.org/abs/2306.01845v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "JEPOO: Highly Accurate Joint Estimation of Pitch, Onset and Offset for\n  Music Information Retrieval", "abstract": "Melody extraction is a core task in music information retrieval, and the\nestimation of pitch, onset and offset are key sub-tasks in melody extraction.\nExisting methods have limited accuracy, and work for only one type of data,\neither single-pitch or multipitch. In this paper, we propose a highly accurate\nmethod for joint estimation of pitch, onset and offset, named JEPOO. We address\nthe challenges of joint learning optimization and handling both single-pitch\nand multi-pitch data through novel model design and a new optimization\ntechnique named Pareto modulated loss with loss weight regularization. This is\nthe first method that can accurately handle both single-pitch and multi-pitch\nmusic data, and even a mix of them. A comprehensive experimental study on a\nwide range of real datasets shows that JEPOO outperforms state-ofthe-art\nmethods by up to 10.6%, 8.3% and 10.3% for the prediction of Pitch, Onset and\nOffset, respectively, and JEPOO is robust for various types of data and\ninstruments. The ablation study shows the effectiveness of each component of\nJEPOO.", "published": "2023-06-02 07:04:33", "link": "http://arxiv.org/abs/2306.01304v2", "categories": ["cs.SD", "cs.IR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Differentiable Grey-box Modelling of Phaser Effects using Frame-based\n  Spectral Processing", "abstract": "Machine learning approaches to modelling analog audio effects have seen\nintensive investigation in recent years, particularly in the context of\nnon-linear time-invariant effects such as guitar amplifiers. For modulation\neffects such as phasers, however, new challenges emerge due to the presence of\nthe low-frequency oscillator which controls the slowly time-varying nature of\nthe effect. Existing approaches have either required foreknowledge of this\ncontrol signal, or have been non-causal in implementation. This work presents a\ndifferentiable digital signal processing approach to modelling phaser effects\nin which the underlying control signal and time-varying spectral response of\nthe effect are jointly learned. The proposed model processes audio in short\nframes to implement a time-varying filter in the frequency domain, with a\ntransfer function based on typical analog phaser circuit topology. We show that\nthe model can be trained to emulate an analog reference device, while retaining\ninterpretable and adjustable parameters. The frame duration is an important\nhyper-parameter of the proposed model, so an investigation was carried out into\nits effect on model accuracy. The optimal frame length depends on both the rate\nand transient decay-time of the target effect, but the frame length can be\naltered at inference time without a significant change in accuracy.", "published": "2023-06-02 07:53:41", "link": "http://arxiv.org/abs/2306.01332v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Active Noise Control in The New Century: The Role and Prospect of Signal\n  Processing", "abstract": "Since Paul Leug's 1933 patent application for a system for the active control\nof sound, the field of active noise control (ANC) has not flourished until the\nadvent of digital signal processors forty years ago. Early theoretical\nadvancements in digital signal processing and processors laid the groundwork\nfor the phenomenal growth of the field, particularly over the past\nquarter-century. The widespread commercial success of ANC in aircraft cabins,\nautomobile cabins, and headsets demonstrates the immeasurable public health and\neconomic benefits of ANC. This article continues where Elliott and Nelson's\n1993 Signal Processing Magazine article and Elliott's 1997 50th anniversary\ncommentary on ANC left off, tracing the technical developments and applications\nin ANC spurred by the seminal texts of Nelson and Elliott (1991), Kuo and\nMorgan (1996), Hansen and Snyder (1996), and Elliott (2001) since the turn of\nthe century. This article focuses on technical developments pertaining to\nreal-world implementations, such as improving algorithmic convergence, reducing\nsystem latency, and extending control to non-stationary and/or broadband noise,\nas well as the commercial transition challenges from analog to digital ANC\nsystems. Finally, open issues and the future of ANC in the era of artificial\nintelligence are discussed.", "published": "2023-06-02 10:31:23", "link": "http://arxiv.org/abs/2306.01425v2", "categories": ["eess.AS", "cs.SY", "eess.SP", "eess.SY"], "primary_category": "eess.AS"}
{"title": "Improved DeepFake Detection Using Whisper Features", "abstract": "With a recent influx of voice generation methods, the threat introduced by\naudio DeepFake (DF) is ever-increasing. Several different detection methods\nhave been presented as a countermeasure. Many methods are based on so-called\nfront-ends, which, by transforming the raw audio, emphasize features crucial\nfor assessing the genuineness of the audio sample. Our contribution contains\ninvestigating the influence of the state-of-the-art Whisper automatic speech\nrecognition model as a DF detection front-end. We compare various combinations\nof Whisper and well-established front-ends by training 3 detection models\n(LCNN, SpecRNet, and MesoNet) on a widely used ASVspoof 2021 DF dataset and\nlater evaluating them on the DF In-The-Wild dataset. We show that using\nWhisper-based features improves the detection for each model and outperforms\nrecent results on the In-The-Wild dataset by reducing Equal Error Rate by 21%.", "published": "2023-06-02 10:34:05", "link": "http://arxiv.org/abs/2306.01428v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Blind Audio Bandwidth Extension: A Diffusion-Based Zero-Shot Approach", "abstract": "Audio bandwidth extension involves the realistic reconstruction of\nhigh-frequency spectra from bandlimited observations. In cases where the\nlowpass degradation is unknown, such as in restoring historical audio\nrecordings, this becomes a blind problem. This paper introduces a novel method\ncalled BABE (Blind Audio Bandwidth Extension) that addresses the blind problem\nin a zero-shot setting, leveraging the generative priors of a pre-trained\nunconditional diffusion model. During the inference process, BABE utilizes a\ngeneralized version of diffusion posterior sampling, where the degradation\noperator is unknown but parametrized and inferred iteratively. The performance\nof the proposed method is evaluated using objective and subjective metrics, and\nthe results show that BABE surpasses state-of-the-art blind bandwidth extension\nbaselines and achieves competitive performance compared to informed methods\nwhen tested with synthetic data. Moreover, BABE exhibits robust generalization\ncapabilities when enhancing real historical recordings, effectively\nreconstructing the missing high-frequency content while maintaining coherence\nwith the original recording. Subjective preference tests confirm that BABE\nsignificantly improves the audio quality of historical music recordings.\nExamples of historical recordings restored with the proposed method are\navailable on the companion webpage:\n(http://research.spa.aalto.fi/publications/papers/ieee-taslp-babe/)", "published": "2023-06-02 10:47:15", "link": "http://arxiv.org/abs/2306.01433v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Discovering COVID-19 Coughing and Breathing Patterns from Unlabeled Data\n  Using Contrastive Learning with Varying Pre-Training Domains", "abstract": "Rapid discovery of new diseases, such as COVID-19 can enable a timely\nepidemic response, preventing the large-scale spread and protecting public\nhealth. However, limited research efforts have been taken on this problem. In\nthis paper, we propose a contrastive learning-based modeling approach for\nCOVID-19 coughing and breathing pattern discovery from non-COVID coughs. To\nvalidate our models, extensive experiments have been conducted using four large\naudio datasets and one image dataset. We further explore the effects of\ndifferent factors, such as domain relevance and augmentation order on the\npre-trained models. Our results show that the proposed model can effectively\ndistinguish COVID-19 coughing and breathing from unlabeled data and labeled\nnon-COVID coughs with an accuracy of up to 0.81 and 0.86, respectively.\nFindings from this work will guide future research to detect an outbreak of a\nnew disease early.", "published": "2023-06-02 18:41:39", "link": "http://arxiv.org/abs/2306.01864v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "In-the-wild Speech Emotion Conversion Using Disentangled Self-Supervised\n  Representations and Neural Vocoder-based Resynthesis", "abstract": "Speech emotion conversion aims to convert the expressed emotion of a spoken\nutterance to a target emotion while preserving the lexical information and the\nspeaker's identity. In this work, we specifically focus on in-the-wild emotion\nconversion where parallel data does not exist, and the problem of disentangling\nlexical, speaker, and emotion information arises. In this paper, we introduce a\nmethodology that uses self-supervised networks to disentangle the lexical,\nspeaker, and emotional content of the utterance, and subsequently uses a\nHiFiGAN vocoder to resynthesise the disentangled representations to a speech\nsignal of the targeted emotion. For better representation and to achieve\nemotion intensity control, we specifically focus on the aro\\-usal dimension of\ncontinuous representations, as opposed to performing emotion conversion on\ncategorical representations. We test our methodology on the large in-the-wild\nMSP-Podcast dataset. Results reveal that the proposed approach is aptly\nconditioned on the emotional content of input speech and is capable of\nsynthesising natural-sounding speech for a target emotion. Results further\nreveal that the methodology better synthesises speech for mid-scale arousal (2\nto 6) than for extreme arousal (1 and 7).", "published": "2023-06-02 21:02:51", "link": "http://arxiv.org/abs/2306.01916v1", "categories": ["eess.AS", "cs.HC", "cs.LG"], "primary_category": "eess.AS"}
