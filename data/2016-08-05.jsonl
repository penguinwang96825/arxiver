{"title": "Resolving Out-of-Vocabulary Words with Bilingual Embeddings in Machine\n  Translation", "abstract": "Out-of-vocabulary words account for a large proportion of errors in machine\ntranslation systems, especially when the system is used on a different domain\nthan the one where it was trained. In order to alleviate the problem, we\npropose to use a log-bilinear softmax-based model for vocabulary expansion,\nsuch that given an out-of-vocabulary source word, the model generates a\nprobabilistic list of possible translations in the target language. Our model\nuses only word embeddings trained on significantly large unlabelled monolingual\ncorpora and trains over a fairly small, word-to-word bilingual dictionary. We\ninput this probabilistic list into a standard phrase-based statistical machine\ntranslation system and obtain consistent improvements in translation quality on\nthe English-Spanish language pair. Especially, we get an improvement of 3.9\nBLEU points when tested over an out-of-domain test set.", "published": "2016-08-05 15:11:58", "link": "http://arxiv.org/abs/1608.01910v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Boundary-based MWE segmentation with text partitioning", "abstract": "This work presents a fine-grained, text-chunking algorithm designed for the\ntask of multiword expressions (MWEs) segmentation. As a lexical class, MWEs\ninclude a wide variety of idioms, whose automatic identification are a\nnecessity for the handling of colloquial language. This algorithm's core\nnovelty is its use of non-word tokens, i.e., boundaries, in a bottom-up\nstrategy. Leveraging boundaries refines token-level information, forging\nhigh-level performance from relatively basic data. The generality of this\nmodel's feature space allows for its application across languages and domains.\nExperiments spanning 19 different languages exhibit a broadly-applicable,\nstate-of-the-art model. Evaluation against recent shared-task data places text\npartitioning as the overall, best performing MWE segmentation algorithm,\ncovering all MWE classes and multiple English domains (including user-generated\ntext). This performance, coupled with a non-combinatorial, fast-running design,\nproduces an ideal combination for implementations at scale, which are\nfacilitated through the release of open-source software.", "published": "2016-08-05 21:27:00", "link": "http://arxiv.org/abs/1608.02025v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Winograd Schemas and Machine Translation", "abstract": "A Winograd schema is a pair of sentences that differ in a single word and\nthat contain an ambiguous pronoun whose referent is different in the two\nsentences and requires the use of commonsense knowledge or world knowledge to\ndisambiguate. This paper discusses how Winograd schemas and other sentence\npairs could be used as challenges for machine translation using distinctions\nbetween pronouns, such as gender, that appear in the target language but not in\nthe source.", "published": "2016-08-05 13:39:08", "link": "http://arxiv.org/abs/1608.01884v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "De-Conflated Semantic Representations", "abstract": "One major deficiency of most semantic representation techniques is that they\nusually model a word type as a single point in the semantic space, hence\nconflating all the meanings that the word can have. Addressing this issue by\nlearning distinct representations for individual meanings of words has been the\nsubject of several research studies in the past few years. However, the\ngenerated sense representations are either not linked to any sense inventory or\nare unreliable for infrequent word senses. We propose a technique that tackles\nthese problems by de-conflating the representations of words based on the deep\nknowledge it derives from a semantic network. Our approach provides multiple\nadvantages in comparison to the past work, including its high coverage and the\nability to generate accurate representations even for infrequent word senses.\nWe carry out evaluations on six datasets across two semantic similarity tasks\nand report state-of-the-art results on most of them.", "published": "2016-08-05 18:14:19", "link": "http://arxiv.org/abs/1608.01961v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bridging the Gap: Incorporating a Semantic Similarity Measure for\n  Effectively Mapping PubMed Queries to Documents", "abstract": "The main approach of traditional information retrieval (IR) is to examine how\nmany words from a query appear in a document. A drawback of this approach,\nhowever, is that it may fail to detect relevant documents where no or only few\nwords from a query are found. The semantic analysis methods such as LSA (latent\nsemantic analysis) and LDA (latent Dirichlet allocation) have been proposed to\naddress the issue, but their performance is not superior compared to common IR\napproaches. Here we present a query-document similarity measure motivated by\nthe Word Mover's Distance. Unlike other similarity measures, the proposed\nmethod relies on neural word embeddings to compute the distance between words.\nThis process helps identify related words when no direct matches are found\nbetween a query and a document. Our method is efficient and straightforward to\nimplement. The experimental results on TREC Genomics data show that our\napproach outperforms the BM25 ranking function by an average of 12% in mean\naverage precision. Furthermore, for a real-world dataset collected from the\nPubMed search logs, we combine the semantic measure with BM25 using a learning\nto rank method, which leads to improved ranking scores by up to 25%. This\nexperiment demonstrates that the proposed approach and BM25 nicely complement\neach other and together produce superior performance.", "published": "2016-08-05 18:53:42", "link": "http://arxiv.org/abs/1608.01972v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
