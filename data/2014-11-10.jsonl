{"title": "Modeling Word Relatedness in Latent Dirichlet Allocation", "abstract": "Standard LDA model suffers the problem that the topic assignment of each word\nis independent and word correlation hence is neglected. To address this\nproblem, in this paper, we propose a model called Word Related Latent Dirichlet\nAllocation (WR-LDA) by incorporating word correlation into LDA topic models.\nThis leads to new capabilities that standard LDA model does not have such as\nestimating infrequently occurring words or multi-language topic modeling.\nExperimental results demonstrate the effectiveness of our model compared with\nstandard LDA.", "published": "2014-11-10 05:24:41", "link": "http://arxiv.org/abs/1411.2328v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language\n  Models", "abstract": "Inspired by recent advances in multimodal learning and machine translation,\nwe introduce an encoder-decoder pipeline that learns (a): a multimodal joint\nembedding space with images and text and (b): a novel language model for\ndecoding distributed representations from our space. Our pipeline effectively\nunifies joint image-text embedding models with multimodal neural language\nmodels. We introduce the structure-content neural language model that\ndisentangles the structure of a sentence to its content, conditioned on\nrepresentations produced by the encoder. The encoder allows one to rank images\nand sentences while the decoder can generate novel descriptions from scratch.\nUsing LSTM to encode sentences, we match the state-of-the-art performance on\nFlickr8K and Flickr30K without using object detections. We also set new best\nresults when using the 19-layer Oxford convolutional network. Furthermore we\nshow that with linear encoders, the learned embedding space captures multimodal\nregularities in terms of vector space arithmetic e.g. *image of a blue car* -\n\"blue\" + \"red\" is near images of red cars. Sample captions generated for 800\nimages are made available for comparison.", "published": "2014-11-10 19:09:41", "link": "http://arxiv.org/abs/1411.2539v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Non-crossing dependencies: least effort, not grammar", "abstract": "The use of null hypotheses (in a statistical sense) is common in hard\nsciences but not in theoretical linguistics. Here the null hypothesis that the\nlow frequency of syntactic dependency crossings is expected by an arbitrary\nordering of words is rejected. It is shown that this would require star\ndependency structures, which are both unrealistic and too restrictive. The\nhypothesis of the limited resources of the human brain is revisited. Stronger\nnull hypotheses taking into account actual dependency lengths for the\nlikelihood of crossings are presented. Those hypotheses suggests that crossings\nare likely to reduce when dependencies are shortened. A hypothesis based on\npressure to reduce dependency lengths is more parsimonious than a principle of\nminimization of crossings or a grammatical ban that is totally dissociated from\nthe general and non-linguistic principle of economy.", "published": "2014-11-10 22:12:56", "link": "http://arxiv.org/abs/1411.2645v1", "categories": ["cs.CL", "cs.SI", "physics.soc-ph"], "primary_category": "cs.CL"}
