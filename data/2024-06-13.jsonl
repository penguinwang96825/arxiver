{"title": "Enhancing Psychotherapy Counseling: A Data Augmentation Pipeline\n  Leveraging Large Language Models for Counseling Conversations", "abstract": "We introduce a pipeline that leverages Large Language Models (LLMs) to\ntransform single-turn psychotherapy counseling sessions into multi-turn\ninteractions. While AI-supported online counseling services for individuals\nwith mental disorders exist, they are often constrained by the limited\navailability of multi-turn training datasets and frequently fail to fully\nutilize therapists' expertise. Our proposed pipeline effectively addresses\nthese limitations. The pipeline comprises two main steps: 1) Information\nExtraction and 2) Multi-turn Counseling Generation. Each step is meticulously\ndesigned to extract and generate comprehensive multi-turn counseling\nconversations from the available datasets. Experimental results from both\nzero-shot and few-shot generation scenarios demonstrate that our approach\nsignificantly enhances the ability of LLMs to produce higher quality multi-turn\ndialogues in the context of mental health counseling. Our pipeline and dataset\nare publicly available\nhttps://github.com/jwkim-chat/A-Data-Augmentation-Pipeline-Leveraging-Large-Language-Models-for-Counseling-Conversations.", "published": "2024-06-13 00:48:44", "link": "http://arxiv.org/abs/2406.08718v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ECBD: Evidence-Centered Benchmark Design for NLP", "abstract": "Benchmarking is seen as critical to assessing progress in NLP. However,\ncreating a benchmark involves many design decisions (e.g., which datasets to\ninclude, which metrics to use) that often rely on tacit, untested assumptions\nabout what the benchmark is intended to measure or is actually measuring. There\nis currently no principled way of analyzing these decisions and how they impact\nthe validity of the benchmark's measurements. To address this gap, we draw on\nevidence-centered design in educational assessments and propose\nEvidence-Centered Benchmark Design (ECBD), a framework which formalizes the\nbenchmark design process into five modules. ECBD specifies the role each module\nplays in helping practitioners collect evidence about capabilities of interest.\nSpecifically, each module requires benchmark designers to describe, justify,\nand support benchmark design choices -- e.g., clearly specifying the\ncapabilities the benchmark aims to measure or how evidence about those\ncapabilities is collected from model responses. To demonstrate the use of ECBD,\nwe conduct case studies with three benchmarks: BoolQ, SuperGLUE, and HELM. Our\nanalysis reveals common trends in benchmark design and documentation that could\nthreaten the validity of benchmarks' measurements.", "published": "2024-06-13 00:59:55", "link": "http://arxiv.org/abs/2406.08723v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Standard Language Ideology in AI-Generated Language", "abstract": "In this position paper, we explore standard language ideology in language\ngenerated by large language models (LLMs). First, we outline how standard\nlanguage ideology is reflected and reinforced in LLMs. We then present a\ntaxonomy of open problems regarding standard language ideology in AI-generated\nlanguage with implications for minoritized language communities. We introduce\nthe concept of standard AI-generated language ideology, the process by which\nAI-generated language regards Standard American English (SAE) as a linguistic\ndefault and reinforces a linguistic bias that SAE is the most \"appropriate\"\nlanguage. Finally, we discuss tensions that remain, including reflecting on\nwhat desirable system behavior looks like, as well as advantages and drawbacks\nof generative AI tools imitating--or often not--different English language\nvarieties. Throughout, we discuss standard language ideology as a manifestation\nof existing global power structures in and through AI-generated language before\nending with questions to move towards alternative, more emancipatory digital\nfutures.", "published": "2024-06-13 01:08:40", "link": "http://arxiv.org/abs/2406.08726v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "StreamBench: Towards Benchmarking Continuous Improvement of Language\n  Agents", "abstract": "Recent works have shown that large language model (LLM) agents are able to\nimprove themselves from experience, which is an important ability for\ncontinuous enhancement post-deployment. However, existing benchmarks primarily\nevaluate their innate capabilities and do not assess their ability to improve\nover time. To address this gap, we introduce StreamBench, a pioneering\nbenchmark designed to evaluate the continuous improvement of LLM agents over an\ninput-feedback sequence. StreamBench simulates an online learning environment\nwhere LLMs receive a continuous flow of feedback stream and iteratively enhance\ntheir performance. In addition, we propose several simple yet effective\nbaselines for improving LLMs on StreamBench, and provide a comprehensive\nanalysis to identify critical components that contribute to successful\nstreaming strategies. Our work serves as a stepping stone towards developing\neffective online learning strategies for LLMs, paving the way for more adaptive\nAI systems in streaming scenarios. Source code:\nhttps://github.com/stream-bench/stream-bench. Benchmark website:\nhttps://stream-bench.github.io.", "published": "2024-06-13 02:08:28", "link": "http://arxiv.org/abs/2406.08747v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Exploration of Cross-Lingual Zero-Shot Generalization in\n  Instruction Tuning", "abstract": "Instruction tuning has emerged as a powerful technique, significantly\nboosting zero-shot performance on unseen tasks. While recent work has explored\ncross-lingual generalization by applying instruction tuning to multilingual\nmodels, previous studies have primarily focused on English, with a limited\nexploration of non-English tasks. For an in-depth exploration of cross-lingual\ngeneralization in instruction tuning, we perform instruction tuning\nindividually for two distinct language meta-datasets. Subsequently, we assess\nthe performance on unseen tasks in a language different from the one used for\ntraining. To facilitate this investigation, we introduce a novel non-English\nmeta-dataset named \"KORANI\" (Korean Natural Instruction), comprising 51 Korean\nbenchmarks. Moreover, we design cross-lingual templates to mitigate\ndiscrepancies in language and instruction-format of the template between\ntraining and inference within the cross-lingual setting. Our experiments reveal\nconsistent improvements through cross-lingual generalization in both English\nand Korean, outperforming baseline by average scores of 20.7\\% and 13.6\\%,\nrespectively. Remarkably, these enhancements are comparable to those achieved\nby monolingual instruction tuning and even surpass them in some tasks. The\nresult underscores the significance of relevant data acquisition across\nlanguages over linguistic congruence with unseen tasks during instruction\ntuning.", "published": "2024-06-13 04:10:17", "link": "http://arxiv.org/abs/2406.08796v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mixture-of-Skills: Learning to Optimize Data Usage for Fine-Tuning Large\n  Language Models", "abstract": "Large language models (LLMs) are typically fine-tuned on diverse and\nextensive datasets sourced from various origins to develop a comprehensive\nrange of skills, such as writing, reasoning, chatting, coding, and more. Each\nskill has unique characteristics, and these datasets are often heterogeneous\nand imbalanced, making the fine-tuning process highly challenging. Balancing\nthe development of each skill while ensuring the model maintains its overall\nperformance requires sophisticated techniques and careful dataset curation. In\nthis work, we propose a general, model-agnostic, reinforcement learning\nframework, Mixture-of-Skills (MoS), that learns to optimize data usage\nautomatically during the fine-tuning process. This framework ensures the\noptimal comprehensive skill development of LLMs by dynamically adjusting the\nfocus on different datasets based on their current learning state. To validate\nthe effectiveness of MoS, we conduct extensive experiments using three diverse\nLLM backbones on two widely used benchmarks and demonstrate that MoS\nsubstantially enhances model performance. Building on the success of MoS, we\npropose MoSpec, an adaptation for task-specific fine-tuning, which harnesses\nthe utilities of various datasets for a specific purpose. Our work underlines\nthe significance of dataset rebalancing and present MoS as a powerful, general\nsolution for optimizing data usage in the fine-tuning of LLMs for various\npurposes.", "published": "2024-06-13 05:01:28", "link": "http://arxiv.org/abs/2406.08811v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Essay Scoring Using Grammatical Variety and Errors with\n  Multi-Task Learning and Item Response Theory", "abstract": "This study examines the effect of grammatical features in automatic essay\nscoring (AES). We use two kinds of grammatical features as input to an AES\nmodel: (1) grammatical items that writers used correctly in essays, and (2) the\nnumber of grammatical errors. Experimental results show that grammatical\nfeatures improve the performance of AES models that predict the holistic scores\nof essays. Multi-task learning with the holistic and grammar scores, alongside\nusing grammatical features, resulted in a larger improvement in model\nperformance. We also show that a model using grammar abilities estimated using\nItem Response Theory (IRT) as the labels for the auxiliary task achieved\ncomparable performance to when we used grammar scores assigned by human raters.\nIn addition, we weight the grammatical features using IRT to consider the\ndifficulty of grammatical items and writers' grammar abilities. We found that\nweighting grammatical features with the difficulty led to further improvement\nin performance.", "published": "2024-06-13 05:19:51", "link": "http://arxiv.org/abs/2406.08817v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ContraSolver: Self-Alignment of Language Models by Resolving Internal\n  Preference Contradictions", "abstract": "While substantial advancements have been made in developing large language\nmodels (LLMs), achieving control over their behavior can be difficult. Direct\npreference optimization (DPO) assumes the existence of a latent reward function\nto evaluate the responses of LLMs. This assumption indicates a strict\npreference ordering of different responses to the same input. However, there\nalways exist contradictions of preference in LLMs according to our experimental\nobservations. In this paper, we construct a graph structure of the preference\nrelationship among different responses with self-annotation to find\ncontradictions in the preference order. We propose ContraSolver, an algorithm\nthat traverses all edges on the preference graph to identify those that might\ncause contradictions. ContraSolver initializes the graph with a maximum\nspanning tree and identifies contradictory edges, prioritizing the resolution\nof low-confidence preferences while preserving high-confidence ones.\nExperimental results on four different generation tasks show that the\nperformance of different LLMs can be largely improved through our completely\nunsupervised self-alignment. Furthermore, by analyzing the preference graphs of\nLLMs with and without self-alignment by ContraSolver, we quantify the reduction\nin contradictions, suggesting that resolving preference contradictions is\ncrucial for achieving better alignment performance.", "published": "2024-06-13 06:08:04", "link": "http://arxiv.org/abs/2406.08842v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Plan, Generate and Complicate: Improving Low-resource Dialogue State\n  Tracking via Easy-to-Difficult Zero-shot Data Augmentation", "abstract": "Data augmentation methods have been a promising direction to improve the\nperformance of small models for low-resource dialogue state tracking. However,\ntraditional methods rely on pre-defined user goals and neglect the importance\nof data complexity in this task. In this paper, we propose EDZ-DA, an\nEasy-to-Difficult Zero-shot Data Augmentation framework for low-resource\ndialogue state tracking that utilizes large language models to automatically\ncatch the relationships of different domains and then generate the dialogue\ndata. We also complicate the dialogues based on the domain relation to enhance\nthe model's capability for co-reference slot tracking. Furthermore, we permute\nslot values to mitigate the influence of output orders and the problem of\nincomplete value generation. Experimental results illustrate the superiority of\nour proposed method compared to previous strong data augmentation baselines on\nMultiWOZ.", "published": "2024-06-13 06:49:03", "link": "http://arxiv.org/abs/2406.08860v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "No perspective, no perception!! Perspective-aware Healthcare Answer\n  Summarization", "abstract": "Healthcare Community Question Answering (CQA) forums offer an accessible\nplatform for individuals seeking information on various healthcare-related\ntopics. People find such platforms suitable for self-disclosure, seeking\nmedical opinions, finding simplified explanations for their medical conditions,\nand answering others' questions. However, answers on these forums are typically\ndiverse and prone to off-topic discussions. It can be challenging for readers\nto sift through numerous answers and extract meaningful insights, making answer\nsummarization a crucial task for CQA forums. While several efforts have been\nmade to summarize the community answers, most of them are limited to the open\ndomain and overlook the different perspectives offered by these answers. To\naddress this problem, this paper proposes a novel task of perspective-specific\nanswer summarization. We identify various perspectives, within\nhealthcare-related responses and frame a perspective-driven abstractive summary\ncovering all responses. To achieve this, we annotate 3167 CQA threads with 6193\nperspective-aware summaries in our PUMA dataset. Further, we propose PLASMA, a\nprompt-driven controllable summarization model. To encapsulate the\nperspective-specific conditions, we design an energy-controlled loss function\nfor the optimization. We also leverage the prefix tuner to learn the\nintricacies of the health-care perspective summarization. Our evaluation\nagainst five baselines suggests the superior performance of PLASMA by a margin\nof 1.5-21% improvement. We supplement our experiments with ablation and\nqualitative analysis.", "published": "2024-06-13 07:35:37", "link": "http://arxiv.org/abs/2406.08881v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Delta-CoMe: Training-Free Delta-Compression with Mixed-Precision for\n  Large Language Models", "abstract": "Fine-tuning is a crucial process for adapting large language models (LLMs) to\ndiverse applications. In certain scenarios, such as multi-tenant serving,\ndeploying multiple LLMs becomes necessary to meet complex demands. Recent\nstudies suggest decomposing a fine-tuned LLM into a base model and\ncorresponding delta weights, which are then compressed using low-rank or\nlow-bit approaches to reduce costs. In this work, we observe that existing\nlow-rank and low-bit compression methods can significantly harm the model\nperformance for task-specific fine-tuned LLMs (e.g., WizardMath for math\nproblems). Motivated by the long-tail distribution of singular values in the\ndelta weights, we propose a delta quantization approach using mixed-precision.\nThis method employs higher-bit representation for singular vectors\ncorresponding to larger singular values. We evaluate our approach on various\nfine-tuned LLMs, including math LLMs, code LLMs, chat LLMs, and even VLMs.\nExperimental results demonstrate that our approach performs comparably to full\nfine-tuned LLMs, surpassing both low-rank and low-bit baselines by a\nconsiderable margin. Additionally, we show that our method is compatible with\nvarious backbone LLMs, such as Llama-2, Llama-3, and Mistral, highlighting its\ngeneralizability.", "published": "2024-06-13 07:57:27", "link": "http://arxiv.org/abs/2406.08903v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Order in English-Japanese Simultaneous Interpretation: Analyses and\n  Evaluation using Chunk-wise Monotonic Translation", "abstract": "This paper analyzes the features of monotonic translations, which follow the\nword order of the source language, in simultaneous interpreting (SI). Word\norder differences are one of the biggest challenges in SI, especially for\nlanguage pairs with significant structural differences like English and\nJapanese. We analyzed the characteristics of chunk-wise monotonic translation\n(CMT) sentences using the NAIST English-to-Japanese Chunk-wise Monotonic\nTranslation Evaluation Dataset and identified some grammatical structures that\nmake monotonic translation difficult in English-Japanese SI. We further\ninvestigated the features of CMT sentences by evaluating the output from the\nexisting speech translation (ST) and simultaneous speech translation (simulST)\nmodels on the NAIST English-to-Japanese Chunk-wise Monotonic Translation\nEvaluation Dataset as well as on existing test sets. The results indicate the\npossibility that the existing SI-based test set underestimates the model\nperformance. The results also suggest that using CMT sentences as references\ngives higher scores to simulST models than ST models, and that using an\noffline-based test set to evaluate the simulST models underestimates the model\nperformance.", "published": "2024-06-13 09:10:16", "link": "http://arxiv.org/abs/2406.08940v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM Reading Tea Leaves: Automatically Evaluating Topic Models with Large\n  Language Models", "abstract": "Topic modeling has been a widely used tool for unsupervised text analysis.\nHowever, comprehensive evaluations of a topic model remain challenging.\nExisting evaluation methods are either less comparable across different models\n(e.g., perplexity) or focus on only one specific aspect of a model (e.g., topic\nquality or document representation quality) at a time, which is insufficient to\nreflect the overall model performance. In this paper, we propose WALM (Word\nAgreement with Language Model), a new evaluation method for topic modeling that\nconsiders the semantic quality of document representations and topics in a\njoint manner, leveraging the power of Large Language Models (LLMs). With\nextensive experiments involving different types of topic models, WALM is shown\nto align with human judgment and can serve as a complementary evaluation method\nto the existing ones, bringing a new perspective to topic modeling. Our\nsoftware package is available at\nhttps://github.com/Xiaohao-Yang/Topic_Model_Evaluation.", "published": "2024-06-13 11:19:50", "link": "http://arxiv.org/abs/2406.09008v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bayesian Statistical Modeling with Predictors from LLMs", "abstract": "State of the art large language models (LLMs) have shown impressive\nperformance on a variety of benchmark tasks and are increasingly used as\ncomponents in larger applications, where LLM-based predictions serve as proxies\nfor human judgements or decision. This raises questions about the\nhuman-likeness of LLM-derived information, alignment with human intuition, and\nwhether LLMs could possibly be considered (parts of) explanatory models of\n(aspects of) human cognition or language use. To shed more light on these\nissues, we here investigate the human-likeness of LLMs' predictions for\nmultiple-choice decision tasks from the perspective of Bayesian statistical\nmodeling. Using human data from a forced-choice experiment on pragmatic\nlanguage use, we find that LLMs do not capture the variance in the human data\nat the item-level. We suggest different ways of deriving full distributional\npredictions from LLMs for aggregate, condition-level data, and find that some,\nbut not all ways of obtaining condition-level predictions yield adequate fits\nto human data. These results suggests that assessment of LLM performance\ndepends strongly on seemingly subtle choices in methodology, and that LLMs are\nat best predictors of human behavior at the aggregate, condition-level, for\nwhich they are, however, not designed to, or usually used to, make predictions\nin the first place.", "published": "2024-06-13 11:33:30", "link": "http://arxiv.org/abs/2406.09012v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM\n  Finetuning", "abstract": "Efficient finetuning of large language models (LLMs) aims to adapt the LLMs\nwith reduced computational and memory cost. Previous LoRA-based approaches\ninitialize the low-rank matrices with Gaussian distribution and zero values\nwhile keeping the original weight matrices frozen. However, the trainable model\nparameters optimized in an unguided subspace might interfere with the\nwell-learned subspace of the pretrained weight matrices. In this paper, we\npropose MiLoRA, a simple yet effective LLM finetuning approach that only\nupdates the minor singular components of the weight matrix while keeping the\nprincipal singular components frozen. It is observed that the minor matrix\ncorresponds to the noisy or long-tail information, while the principal matrix\ncontains important knowledge. The MiLoRA initializes the low-rank matrices\nwithin a subspace that is orthogonal to the principal matrix, thus the\npretrained knowledge is expected to be well preserved. During finetuning,\nMiLoRA makes the most use of the less-optimized subspace for learning the\nlabeled dataset. Extensive experiments on commonsense reasoning, math\nreasoning, instruction following and visual instruction following benchmarks\npresent the superior performance of our method.", "published": "2024-06-13 12:30:02", "link": "http://arxiv.org/abs/2406.09044v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Living in the Moment: Can Large Language Models Grasp Co-Temporal\n  Reasoning?", "abstract": "Temporal reasoning is fundamental for large language models (LLMs) to\ncomprehend the world. Current temporal reasoning datasets are limited to\nquestions about single or isolated events, falling short in mirroring the\nrealistic temporal characteristics involving concurrent nature and intricate\ntemporal interconnections. In this paper, we introduce CoTempQA, a\ncomprehensive co-temporal Question Answering (QA) benchmark containing four\nco-temporal scenarios (Equal, Overlap, During, Mix) with 4,748 samples for\nevaluating the co-temporal comprehension and reasoning abilities of LLMs. Our\nextensive experiments reveal a significant gap between the performance of\ncurrent LLMs and human-level reasoning on CoTempQA tasks. Even when enhanced\nwith Chain of Thought (CoT) methodologies, models consistently struggle with\nour task. In our preliminary exploration, we discovered that mathematical\nreasoning plays a significant role in handling co-temporal events and proposed\na strategy to boost LLMs' co-temporal reasoning from a mathematical\nperspective. We hope that our CoTempQA datasets will encourage further\nadvancements in improving the co-temporal reasoning capabilities of LLMs. Our\ncode is available at https://github.com/zhaochen0110/Cotempqa.", "published": "2024-06-13 12:56:21", "link": "http://arxiv.org/abs/2406.09072v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "3M: Multi-modal Multi-task Multi-teacher Learning for Game Event\n  Detection", "abstract": "Esports has rapidly emerged as a global phenomenon with an ever-expanding\naudience via platforms, like YouTube. Due to the inherent complexity nature of\nthe game, it is challenging for newcomers to comprehend what the event entails.\nThe chaotic nature of online chat, the fast-paced speech of the game\ncommentator, and the game-specific user interface further compound the\ndifficulty for users in comprehending the gameplay. To overcome these\nchallenges, it is crucial to integrate the Multi-Modal (MM) information from\nthe platform and understand the event. The paper introduces a new MM\nmulti-teacher-based game event detection framework, with the ultimate goal of\nconstructing a comprehensive framework that enhances the comprehension of the\nongoing game situation. While conventional MM models typically prioritise\naligning MM data through concurrent training towards a unified objective, our\nframework leverages multiple teachers trained independently on different tasks\nto accomplish the Game Event Detection. The experiment clearly shows the\neffectiveness of the proposed MM multi-teacher framework.", "published": "2024-06-13 12:58:53", "link": "http://arxiv.org/abs/2406.09076v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Comparative Logical Relation with Contrastive Learning for Text\n  Generation", "abstract": "Data-to-Text Generation (D2T), a classic natural language generation problem,\naims at producing fluent descriptions for structured input data, such as a\ntable. Existing D2T works mainly focus on describing the superficial\nassociative relations among entities, while ignoring the deep comparative\nlogical relations, such as A is better than B in a certain aspect with a\ncorresponding opinion, which is quite common in our daily life. In this paper,\nwe introduce a new D2T task named comparative logical relation generation\n(CLRG). Additionally, we propose a Comparative Logic (CoLo) based text\ngeneration method, which generates texts following specific comparative logical\nrelations with contrastive learning. Specifically, we first construct various\npositive and negative samples by fine-grained perturbations in entities,\naspects and opinions. Then, we perform contrastive learning in the encoder\nlayer to have a better understanding of the comparative logical relations, and\nintegrate it in the decoder layer to guide the model to correctly generate the\nrelations. Noting the data scarcity problem, we construct a Chinese Comparative\nLogical Relation Dataset (CLRD), which is a high-quality human-annotated\ndataset and challenging for text generation with descriptions of multiple\nentities and annotations on their comparative logical relations. Extensive\nexperiments show that our method achieves impressive performance in both\nautomatic and human evaluations.", "published": "2024-06-13 13:25:50", "link": "http://arxiv.org/abs/2406.09095v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SciKnowEval: Evaluating Multi-level Scientific Knowledge of Large\n  Language Models", "abstract": "Large language models (LLMs) have gained increasing prominence in scientific\nresearch, but there is a lack of comprehensive benchmarks to fully evaluate\ntheir proficiency in understanding and mastering scientific knowledge. To\naddress this need, we introduce the SciKnowEval benchmark, a novel framework\nthat systematically evaluates LLMs across five progressive levels of scientific\nknowledge: studying extensively, inquiring earnestly, thinking profoundly,\ndiscerning clearly, and practicing assiduously. These levels aim to assess the\nbreadth and depth of scientific knowledge in LLMs, including memory,\ncomprehension, reasoning, discernment, and application. Specifically, we first\nconstruct a large-scale evaluation dataset encompassing 70K multi-level\nscientific problems and solutions in the domains of biology, chemistry,\nphysics, and materials science. By leveraging this dataset, we benchmark 26\nadvanced open-source and proprietary LLMs using zero-shot and few-shot\nprompting strategies. The results reveal that despite the state-of-the-art\nperformance of proprietary LLMs, there is still significant room for\nimprovement, particularly in addressing scientific reasoning and applications.\nWe anticipate that SciKnowEval will establish a standard for benchmarking LLMs\nin science research and promote the development of stronger scientific LLMs.\nThe dataset and code are publicly available at https://scimind.ai/sciknoweval .", "published": "2024-06-13 13:27:52", "link": "http://arxiv.org/abs/2406.09098v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chain-of-Though (CoT) prompting strategies for medical error detection\n  and correction", "abstract": "This paper describes our submission to the MEDIQA-CORR 2024 shared task for\nautomatically detecting and correcting medical errors in clinical notes. We\nreport results for three methods of few-shot In-Context Learning (ICL)\naugmented with Chain-of-Thought (CoT) and reason prompts using a large language\nmodel (LLM). In the first method, we manually analyse a subset of train and\nvalidation dataset to infer three CoT prompts by examining error types in the\nclinical notes. In the second method, we utilise the training dataset to prompt\nthe LLM to deduce reasons about their correctness or incorrectness. The\nconstructed CoTs and reasons are then augmented with ICL examples to solve the\ntasks of error detection, span identification, and error correction. Finally,\nwe combine the two methods using a rule-based ensemble method. Across the three\nsub-tasks, our ensemble method achieves a ranking of 3rd for both sub-task 1\nand 2, while securing 7th place in sub-task 3 among all submissions.", "published": "2024-06-13 13:31:04", "link": "http://arxiv.org/abs/2406.09103v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoastTerm: a Corpus for Multidisciplinary Term Extraction in Coastal\n  Scientific Literature", "abstract": "The growing impact of climate change on coastal areas, particularly active\nbut fragile regions, necessitates collaboration among diverse stakeholders and\ndisciplines to formulate effective environmental protection policies. We\nintroduce a novel specialized corpus comprising 2,491 sentences from 410\nscientific abstracts concerning coastal areas, for the Automatic Term\nExtraction (ATE) and Classification (ATC) tasks. Inspired by the ARDI\nframework, focused on the identification of Actors, Resources, Dynamics and\nInteractions, we automatically extract domain terms and their distinct roles in\nthe functioning of coastal systems by leveraging monolingual and multilingual\ntransformer models. The evaluation demonstrates consistent results, achieving\nan F1 score of approximately 80\\% for automated term extraction and F1 of 70\\%\nfor extracting terms and their labels. These findings are promising and signify\nan initial step towards the development of a specialized Knowledge Base\ndedicated to coastal areas.", "published": "2024-06-13 14:01:08", "link": "http://arxiv.org/abs/2406.09128v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RH-SQL: Refined Schema and Hardness Prompt for Text-to-SQL", "abstract": "Text-to-SQL is a technology that converts natural language queries into the\nstructured query language SQL. A novel research approach that has recently\ngained attention focuses on methods based on the complexity of SQL queries,\nachieving notable performance improvements. However, existing methods entail\nsignificant storage and training costs, which hampers their practical\napplication. To address this issue, this paper introduces a method for\nText-to-SQL based on Refined Schema and Hardness Prompt. By filtering out\nlow-relevance schema information with a refined schema and identifying query\nhardness through a Language Model (LM) to form prompts, this method reduces\nstorage and training costs while maintaining performance. It's worth mentioning\nthat this method is applicable to any sequence-to-sequence (seq2seq) LM. Our\nexperiments on the Spider dataset, specifically with large-scale LMs, achieved\nan exceptional Execution accuracy (EX) of 82.6%, demonstrating the\neffectiveness and greater suitability of our method for real-world\napplications.", "published": "2024-06-13 14:04:34", "link": "http://arxiv.org/abs/2406.09133v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Explicit Reasoning for Inference Integration in\n  Commonsense-Augmented Dialogue Models", "abstract": "Open-domain dialogue systems need to grasp social commonsense to understand\nand respond effectively to human users. Commonsense-augmented dialogue models\nhave been proposed that aim to infer commonsense knowledge from dialogue\ncontexts in order to improve response quality. However, existing approaches to\ncommonsense-augmented dialogue rely on implicit reasoning to integrate\ncommonsense inferences during response generation. In this study, we explore\nthe impact of explicit reasoning against implicit reasoning over commonsense\nfor dialogue response generation. Our findings demonstrate that separating\ncommonsense reasoning into explicit steps for generating, selecting, and\nintegrating commonsense into responses leads to better dialogue interactions,\nimproving naturalness, engagement, specificity, and overall quality. Subsequent\nanalyses of these findings unveil insights into the effectiveness of various\ntypes of commonsense in generating responses and the particular response traits\nenhanced through explicit reasoning for commonsense integration. Our work\nadvances research in open-domain dialogue by achieving a new state-of-the-art\nin commonsense-augmented response generation.", "published": "2024-06-13 14:07:52", "link": "http://arxiv.org/abs/2406.09138v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating the translation capabilities of Large Language Models\n  trained on parallel data only", "abstract": "In recent years, Large Language Models (LLMs) have demonstrated exceptional\nproficiency across a broad spectrum of Natural Language Processing (NLP) tasks,\nincluding Machine Translation. However, previous methods predominantly relied\non iterative processes such as instruction fine-tuning or continual\npre-training, leaving unexplored the challenges of training LLMs solely on\nparallel data. In this work, we introduce PLUME (Parallel Language Model), a\ncollection of three 2B LLMs featuring varying vocabulary sizes (32k, 128k, and\n256k) trained exclusively on Catalan-centric parallel examples. These models\nperform comparably to previous encoder-decoder architectures on 16 supervised\ntranslation directions and 56 zero-shot ones. Utilizing this set of models, we\nconduct a thorough investigation into the translation capabilities of LLMs,\nprobing their performance, the impact of the different elements of the prompt,\nand their cross-lingual representation space.", "published": "2024-06-13 14:08:56", "link": "http://arxiv.org/abs/2406.09140v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning", "abstract": "Large language models (LLMs) have showcased remarkable reasoning\ncapabilities, yet they remain susceptible to errors, particularly in temporal\nreasoning tasks involving complex temporal logic. Existing research has\nexplored LLM performance on temporal reasoning using diverse datasets and\nbenchmarks. However, these studies often rely on real-world data that LLMs may\nhave encountered during pre-training or employ anonymization techniques that\ncan inadvertently introduce factual inconsistencies. In this work, we address\nthese limitations by introducing novel synthetic datasets specifically designed\nto assess LLM temporal reasoning abilities in various scenarios. The diversity\nof question types across these datasets enables systematic investigation into\nthe impact of the problem structure, size, question type, fact order, and other\nfactors on LLM performance. Our findings provide valuable insights into the\nstrengths and weaknesses of current LLMs in temporal reasoning tasks. To foster\nfurther research in this area, we are open-sourcing the datasets and evaluation\nframework used in our experiments: https://huggingface.co/datasets/baharef/ToT.", "published": "2024-06-13 14:31:19", "link": "http://arxiv.org/abs/2406.09170v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Orthogonality and isotropy of speaker and phonetic information in\n  self-supervised speech representations", "abstract": "Self-supervised speech representations can hugely benefit downstream speech\ntechnologies, yet the properties that make them useful are still poorly\nunderstood. Two candidate properties related to the geometry of the\nrepresentation space have been hypothesized to correlate well with downstream\ntasks: (1) the degree of orthogonality between the subspaces spanned by the\nspeaker centroids and phone centroids, and (2) the isotropy of the space, i.e.,\nthe degree to which all dimensions are effectively utilized. To study them, we\nintroduce a new measure, Cumulative Residual Variance (CRV), which can be used\nto assess both properties. Using linear classifiers for speaker and phone ID to\nprobe the representations of six different self-supervised models and two\nuntrained baselines, we ask whether either orthogonality or isotropy correlate\nwith linear probing accuracy. We find that both measures correlate with\nphonetic probing accuracy, though our results on isotropy are more nuanced.", "published": "2024-06-13 14:57:18", "link": "http://arxiv.org/abs/2406.09200v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sharing Matters: Analysing Neurons Across Languages and Tasks in LLMs", "abstract": "Large language models (LLMs) have revolutionized the field of natural\nlanguage processing (NLP), and recent studies have aimed to understand their\nunderlying mechanisms. However, most of this research is conducted within a\nmonolingual setting, primarily focusing on English. Few studies attempt to\nexplore the internal workings of LLMs in multilingual settings. In this study,\nwe aim to fill the research gap by examining how neuron activation is shared\nacross tasks and languages. We classify neurons into four distinct categories\nbased on their responses to a specific input across different\nlanguages:all-shared, partial-shared, specific, and non-activated. This\ncategorization is combined with a study of neuron attribution, i.e. the\nimportance of a neuron w.r.t an output. Our analysis reveals the following\ninsights: (i) the patterns of neuron sharing are significantly affected by the\ncharacteristics of tasks and examples; (ii) neuron sharing does not fully\ncorrespond with language similarity; (iii) shared neurons play a vital role in\ngenerating responses, especially those shared across all languages. These\nfindings shed light on the internal workings of multilingual LLMs and pave the\nway to the future research. We will release the code to foster research in this\narea.", "published": "2024-06-13 16:04:11", "link": "http://arxiv.org/abs/2406.09265v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unpacking DPO and PPO: Disentangling Best Practices for Learning from\n  Preference Feedback", "abstract": "Learning from preference feedback has emerged as an essential step for\nimproving the generation quality and performance of modern language models\n(LMs). Despite its widespread use, the way preference-based learning is applied\nvaries wildly, with differing data, learning algorithms, and evaluations used,\nmaking disentangling the impact of each aspect difficult. In this work, we\nidentify four core aspects of preference-based learning: preference data,\nlearning algorithm, reward model, and policy training prompts, systematically\ninvestigate the impact of these components on downstream model performance, and\nsuggest a recipe for strong learning for preference feedback. Our findings\nindicate that all aspects are important for performance, with better preference\ndata leading to the largest improvements, followed by the choice of learning\nalgorithm, the use of improved reward models, and finally the use of additional\nunlabeled prompts for policy training. Notably, PPO outperforms DPO by up to\n2.5% in math and 1.2% in general domains. High-quality preference data leads to\nimprovements of up to 8% in instruction following and truthfulness. Despite\nsignificant gains of up to 5% in mathematical evaluation when scaling up reward\nmodels, we surprisingly observe marginal improvements in other categories.\n  We publicly release the code used for training\n(https://github.com/hamishivi/EasyLM) and evaluating\n(https://github.com/allenai/open-instruct) our models, along with the models\nand datasets themselves\n(https://huggingface.co/collections/allenai/tulu-v25-suite-66676520fd578080e126f618).", "published": "2024-06-13 16:17:21", "link": "http://arxiv.org/abs/2406.09279v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "REVS: Unlearning Sensitive Information in Language Models via Rank\n  Editing in the Vocabulary Space", "abstract": "Language models (LMs) risk inadvertently memorizing and divulging sensitive\nor personally identifiable information (PII) seen in training data, causing\nprivacy concerns. Current approaches to address this issue involve costly\ndataset scrubbing, or model filtering through unlearning and model editing,\nwhich can be bypassed through extraction attacks. We propose REVS, a novel\nnon-gradient-based method for unlearning sensitive information from LMs. REVS\nidentifies and modifies a small subset of neurons relevant for constituent\ntokens that form sensitive information. To adequately evaluate our method on\ntruly sensitive information, we curate three datasets: email and URL datasets\nnaturally memorized by the models, and a synthetic social security number\ndataset that we tune the models to memorize. Compared to other methods, REVS\ndemonstrates superior performance in unlearning sensitive information and\nrobustness to extraction attacks, while retaining underlying model integrity.", "published": "2024-06-13 17:02:32", "link": "http://arxiv.org/abs/2406.09325v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Learning from Natural Language Explanations for Generalizable Entity\n  Matching", "abstract": "Entity matching is the task of linking records from different sources that\nrefer to the same real-world entity. Past work has primarily treated entity\nlinking as a standard supervised learning problem. However, supervised entity\nmatching models often do not generalize well to new data, and collecting\nexhaustive labeled training data is often cost prohibitive. Further, recent\nefforts have adopted LLMs for this task in few/zero-shot settings, exploiting\ntheir general knowledge. But LLMs are prohibitively expensive for performing\ninference at scale for real-world entity matching tasks.\n  As an efficient alternative, we re-cast entity matching as a conditional\ngeneration task as opposed to binary classification. This enables us to\n\"distill\" LLM reasoning into smaller entity matching models via natural\nlanguage explanations. This approach achieves strong performance, especially on\nout-of-domain generalization tests (10.85% F-1) where standalone generative\nmethods struggle. We perform ablations that highlight the importance of\nexplanations, both for performance and model robustness.", "published": "2024-06-13 17:08:58", "link": "http://arxiv.org/abs/2406.09330v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ProxyLM: Predicting Language Model Performance on Multilingual Tasks via\n  Proxy Models", "abstract": "Performance prediction is a method to estimate the performance of Language\nModels (LMs) on various Natural Language Processing (NLP) tasks, mitigating\ncomputational costs associated with model capacity and data for fine-tuning.\nOur paper presents ProxyLM, a scalable task- and language-agnostic framework\ndesigned to predict the performance of LMs using proxy models. These proxy\nmodels act as surrogates, approximating the performance of the LM of interest.\nBy leveraging these proxy models, ProxyLM significantly reduces computational\noverhead in task evaluations, achieving up to a 37.08x speedup over traditional\nmethods, even with our smallest proxy models. Our results across multiple\nmultilingual NLP tasks and various robustness tests demonstrate that ProxyLM\nnot only adapts well to previously unseen languages in pre-trained LMs, but\nalso generalizes effectively across different datasets, outperforming the\nstate-of-the-art by at least 1.78x in terms of root-mean-square error (RMSE).", "published": "2024-06-13 17:15:33", "link": "http://arxiv.org/abs/2406.09334v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus", "abstract": "Multimodal Large Language Models (mLLMs) are trained on a large amount of\ntext-image data. While most mLLMs are trained on caption-like data only,\nAlayrac et al. [2022] showed that additionally training them on interleaved\nsequences of text and images can lead to the emergence of in-context learning\ncapabilities. However, the dataset they used, M3W, is not public and is only in\nEnglish. There have been attempts to reproduce their results but the released\ndatasets are English-only. In contrast, current multilingual and multimodal\ndatasets are either composed of caption-like only or medium-scale or fully\nprivate data. This limits mLLM research for the 7,000 other languages spoken in\nthe world. We therefore introduce mOSCAR, to the best of our knowledge the\nfirst large-scale multilingual and multimodal document corpus crawled from the\nweb. It covers 163 languages, 315M documents, 214B tokens and 1.2B images. We\ncarefully conduct a set of filtering and evaluation steps to make sure mOSCAR\nis sufficiently safe, diverse and of good quality. We additionally train two\ntypes of multilingual model to prove the benefits of mOSCAR: (1) a model\ntrained on a subset of mOSCAR and captioning data and (2) a model train on\ncaptioning data only. The model additionally trained on mOSCAR shows a strong\nboost in few-shot learning performance across various multilingual image-text\ntasks and benchmarks, confirming previous findings for English-only mLLMs.", "published": "2024-06-13 00:13:32", "link": "http://arxiv.org/abs/2406.08707v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "StructuralSleight: Automated Jailbreak Attacks on Large Language Models\n  Utilizing Uncommon Text-Organization Structures", "abstract": "Large Language Models (LLMs) are widely used in natural language processing\nbut face the risk of jailbreak attacks that maliciously induce them to generate\nharmful content. Existing jailbreak attacks, including character-level and\ncontext-level attacks, mainly focus on the prompt of plain text without\nspecifically exploring the significant influence of its structure. In this\npaper, we focus on studying how the prompt structure contributes to the\njailbreak attack. We introduce a novel structure-level attack method based on\nlong-tailed structures, which we refer to as Uncommon Text-Organization\nStructures (UTOS). We extensively study 12 UTOS templates and 6 obfuscation\nmethods to build an effective automated jailbreak tool named StructuralSleight\nthat contains three escalating attack strategies: Structural Attack, Structural\nand Character/Context Obfuscation Attack, and Fully Obfuscated Structural\nAttack. Extensive experiments on existing LLMs show that StructuralSleight\nsignificantly outperforms the baseline methods. In particular, the attack\nsuccess rate reaches 94.62\\% on GPT-4o, which has not been addressed by\nstate-of-the-art techniques.", "published": "2024-06-13 02:24:08", "link": "http://arxiv.org/abs/2406.08754v3", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction\n  Benchmark in Form Understanding", "abstract": "Accurately identifying and organizing textual content is crucial for the\nautomation of document processing in the field of form understanding. Existing\ndatasets, such as FUNSD and XFUND, support entity classification and\nrelationship prediction tasks but are typically limited to local and\nentity-level annotations. This limitation overlooks the hierarchically\nstructured representation of documents, constraining comprehensive\nunderstanding of complex forms. To address this issue, we present the SRFUND, a\nhierarchically structured multi-task form understanding benchmark. SRFUND\nprovides refined annotations on top of the original FUNSD and XFUND datasets,\nencompassing five tasks: (1) word to text-line merging, (2) text-line to entity\nmerging, (3) entity category classification, (4) item table localization, and\n(5) entity-based full-document hierarchical structure recovery. We meticulously\nsupplemented the original dataset with missing annotations at various levels of\ngranularity and added detailed annotations for multi-item table regions within\nthe forms. Additionally, we introduce global hierarchical structure\ndependencies for entity relation prediction tasks, surpassing traditional local\nkey-value associations. The SRFUND dataset includes eight languages including\nEnglish, Chinese, Japanese, German, French, Spanish, Italian, and Portuguese,\nmaking it a powerful tool for cross-lingual form understanding. Extensive\nexperimental results demonstrate that the SRFUND dataset presents new\nchallenges and significant opportunities in handling diverse layouts and global\nhierarchical structures of forms, thus providing deep insights into the field\nof form understanding. The original dataset and implementations of baseline\nmethods are available at https://sprateam-ustc.github.io/SRFUND", "published": "2024-06-13 02:35:55", "link": "http://arxiv.org/abs/2406.08757v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MMFakeBench: A Mixed-Source Multimodal Misinformation Detection\n  Benchmark for LVLMs", "abstract": "Current multimodal misinformation detection (MMD) methods often assume a\nsingle source and type of forgery for each sample, which is insufficient for\nreal-world scenarios where multiple forgery sources coexist. The lack of a\nbenchmark for mixed-source misinformation has hindered progress in this field.\nTo address this, we introduce MMFakeBench, the first comprehensive benchmark\nfor mixed-source MMD. MMFakeBench includes 3 critical sources: textual veracity\ndistortion, visual veracity distortion, and cross-modal consistency distortion,\nalong with 12 sub-categories of misinformation forgery types. We further\nconduct an extensive evaluation of 6 prevalent detection methods and 15 Large\nVision-Language Models (LVLMs) on MMFakeBench under a zero-shot setting. The\nresults indicate that current methods struggle under this challenging and\nrealistic mixed-source MMD setting. Additionally, we propose MMD-Agent, a novel\napproach to integrate the reasoning, action, and tool-use capabilities of LVLM\nagents, significantly enhancing accuracy and generalization. We believe this\nstudy will catalyze future research into more realistic mixed-source multimodal\nmisinformation and provide a fair evaluation of misinformation detection\nmethods.", "published": "2024-06-13 03:04:28", "link": "http://arxiv.org/abs/2406.08772v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Linguistic Bias in ChatGPT: Language Models Reinforce Dialect\n  Discrimination", "abstract": "We present a large-scale study of linguistic bias exhibited by ChatGPT\ncovering ten dialects of English (Standard American English, Standard British\nEnglish, and eight widely spoken non-\"standard\" varieties from around the\nworld). We prompted GPT-3.5 Turbo and GPT-4 with text by native speakers of\neach variety and analyzed the responses via detailed linguistic feature\nannotation and native speaker evaluation. We find that the models default to\n\"standard\" varieties of English; based on evaluation by native speakers, we\nalso find that model responses to non-\"standard\" varieties consistently exhibit\na range of issues: stereotyping (19% worse than for \"standard\" varieties),\ndemeaning content (25% worse), lack of comprehension (9% worse), and\ncondescending responses (15% worse). We also find that if these models are\nasked to imitate the writing style of prompts in non-\"standard\" varieties, they\nproduce text that exhibits lower comprehension of the input and is especially\nprone to stereotyping. GPT-4 improves on GPT-3.5 in terms of comprehension,\nwarmth, and friendliness, but also exhibits a marked increase in stereotyping\n(+18%). The results indicate that GPT-3.5 Turbo and GPT-4 can perpetuate\nlinguistic discrimination toward speakers of non-\"standard\" varieties.", "published": "2024-06-13 05:20:42", "link": "http://arxiv.org/abs/2406.08818v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "DisfluencySpeech -- Single-Speaker Conversational Speech Dataset with\n  Paralanguage", "abstract": "Laughing, sighing, stuttering, and other forms of paralanguage do not\ncontribute any direct lexical meaning to speech, but they provide crucial\npropositional context that aids semantic and pragmatic processes such as irony.\nIt is thus important for artificial social agents to both understand and be\nable to generate speech with semantically-important paralanguage. Most speech\ndatasets do not include transcribed non-lexical speech sounds and disfluencies,\nwhile those that do are typically multi-speaker datasets where each speaker\nprovides relatively little audio. This makes it challenging to train\nconversational Text-to-Speech (TTS) synthesis models that include such\nparalinguistic components.\n  We thus present DisfluencySpeech, a studio-quality labeled English speech\ndataset with paralanguage. A single speaker recreates nearly 10 hours of\nexpressive utterances from the Switchboard-1 Telephone Speech Corpus\n(Switchboard), simulating realistic informal conversations. To aid the\ndevelopment of a TTS model that is able to predictively synthesise paralanguage\nfrom text without such components, we provide three different transcripts at\ndifferent levels of information removal (removal of non-speech events, removal\nof non-sentence elements, and removal of false starts), as well as benchmark\nTTS models trained on each of these levels.", "published": "2024-06-13 05:23:22", "link": "http://arxiv.org/abs/2406.08820v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "An Approach to Build Zero-Shot Slot-Filling System for Industry-Grade\n  Conversational Assistants", "abstract": "We present an approach to build Large Language Model (LLM) based slot-filling\nsystem to perform Dialogue State Tracking in conversational assistants serving\nacross a wide variety of industry-grade applications. Key requirements of this\nsystem include: 1) usage of smaller-sized models to meet low latency\nrequirements and to enable convenient and cost-effective cloud and customer\npremise deployments, and 2) zero-shot capabilities to serve across a wide\nvariety of domains, slot types and conversational scenarios. We adopt a\nfine-tuning approach where a pre-trained LLM is fine-tuned into a slot-filling\nmodel using task specific data. The fine-tuning data is prepared carefully to\ncover a wide variety of slot-filling task scenarios that the model is expected\nto face across various domains. We give details of the data preparation and\nmodel building process. We also give a detailed analysis of the results of our\nexperimental evaluations. Results show that our prescribed approach for\nslot-filling model building has resulted in 6.9% relative improvement of F1\nmetric over the best baseline on a realistic benchmark, while at the same time\nreducing the latency by 57%. More over, the data we prepared has helped improve\nF1 on an average by 4.2% relative across various slot-types.", "published": "2024-06-13 06:24:52", "link": "http://arxiv.org/abs/2406.08848v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Initial Investigation of Language Adaptation for TTS Systems under\n  Low-resource Scenarios", "abstract": "Self-supervised learning (SSL) representations from massively multilingual\nmodels offer a promising solution for low-resource language speech tasks.\nDespite advancements, language adaptation in TTS systems remains an open\nproblem. This paper explores the language adaptation capability of ZMM-TTS, a\nrecent SSL-based multilingual TTS system proposed in our previous work. We\nconducted experiments on 12 languages using limited data with various\nfine-tuning configurations. We demonstrate that the similarity in phonetics\nbetween the pre-training and target languages, as well as the language\ncategory, affects the target language's adaptation performance. Additionally,\nwe find that the fine-tuning dataset size and number of speakers influence\nadaptability. Surprisingly, we also observed that using paired data for\nfine-tuning is not always optimal compared to audio-only data. Beyond speech\nintelligibility, our analysis covers speaker similarity, language\nidentification, and predicted MOS.", "published": "2024-06-13 08:16:52", "link": "http://arxiv.org/abs/2406.08911v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Language Models are Crossword Solvers", "abstract": "Crosswords are a form of word puzzle that require a solver to demonstrate a\nhigh degree of proficiency in natural language understanding, wordplay,\nreasoning, and world knowledge, along with adherence to character and length\nconstraints. In this paper we tackle the challenge of solving crosswords with\nlarge language models (LLMs). We demonstrate that the current generation of\nlanguage models shows significant competence at deciphering cryptic crossword\nclues and outperforms previously reported state-of-the-art (SoTA) results by a\nfactor of 2-3 in relevant benchmarks. We also develop a search algorithm that\nbuilds off this performance to tackle the problem of solving full crossword\ngrids with out-of-the-box LLMs for the very first time, achieving an accuracy\nof 93% on New York Times crossword puzzles. Additionally, we demonstrate that\nLLMs generalize well and are capable of supporting answers with sound\nrationale.", "published": "2024-06-13 12:29:27", "link": "http://arxiv.org/abs/2406.09043v3", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Towards Reliable Detection of LLM-Generated Texts: A Comprehensive\n  Evaluation Framework with CUDRT", "abstract": "The increasing prevalence of large language models (LLMs) has significantly\nadvanced text generation, but the human-like quality of LLM outputs presents\nmajor challenges in reliably distinguishing between human-authored and\nLLM-generated texts. Existing detection benchmarks are constrained by their\nreliance on static datasets, scenario-specific tasks (e.g., question answering\nand text refinement), and a primary focus on English, overlooking the diverse\nlinguistic and operational subtleties of LLMs. To address these gaps, we\npropose CUDRT, a comprehensive evaluation framework and bilingual benchmark in\nChinese and English, categorizing LLM activities into five key operations:\nCreate, Update, Delete, Rewrite, and Translate. CUDRT provides extensive\ndatasets tailored to each operation, featuring outputs from state-of-the-art\nLLMs to assess the reliability of LLM-generated text detectors. This framework\nsupports scalable, reproducible experiments and enables in-depth analysis of\nhow operational diversity, multilingual training sets, and LLM architectures\ninfluence detection performance. Our extensive experiments demonstrate the\nframework's capacity to optimize detection systems, providing critical insights\nto enhance reliability, cross-linguistic adaptability, and detection accuracy.\nBy advancing robust methodologies for identifying LLM-generated texts, this\nwork contributes to the development of intelligent systems capable of meeting\nreal-world multilingual detection challenges. Source code and dataset are\navailable at GitHub.", "published": "2024-06-13 12:43:40", "link": "http://arxiv.org/abs/2406.09056v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Chain of Preference Optimization: Improving Chain-of-Thought Reasoning\n  in LLMs", "abstract": "The recent development of chain-of-thought (CoT) decoding has enabled large\nlanguage models (LLMs) to generate explicit logical reasoning paths for complex\nproblem-solving. However, research indicates that these paths are not always\ndeliberate and optimal. The tree-of-thought (ToT) method employs tree-searching\nto extensively explore the reasoning space and find better reasoning paths that\nCoT decoding might overlook. This deliberation, however, comes at the cost of\nsignificantly increased inference complexity. In this work, we demonstrate that\nfine-tuning LLMs leveraging the search tree constructed by ToT allows CoT to\nachieve similar or better performance, thereby avoiding the substantial\ninference burden. This is achieved through Chain of Preference Optimization\n(CPO), where LLMs are fine-tuned to align each step of the CoT reasoning paths\nwith those of ToT using the inherent preference information in the tree-search\nprocess. Extensive experimental results show that CPO significantly improves\nLLM performance in solving a variety of complex problems, including question\nanswering, fact verification, and arithmetic reasoning, demonstrating its\neffectiveness. Our code is available at https://github.com/sail-sg/CPO.", "published": "2024-06-13 14:07:02", "link": "http://arxiv.org/abs/2406.09136v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ReMI: A Dataset for Reasoning with Multiple Images", "abstract": "With the continuous advancement of large language models (LLMs), it is\nessential to create new benchmarks to effectively evaluate their expanding\ncapabilities and identify areas for improvement. This work focuses on\nmulti-image reasoning, an emerging capability in state-of-the-art LLMs. We\nintroduce ReMI, a dataset designed to assess LLMs' ability to Reason with\nMultiple Images. This dataset encompasses a diverse range of tasks, spanning\nvarious reasoning domains such as math, physics, logic, code, table/chart\nunderstanding, and spatial and temporal reasoning. It also covers a broad\nspectrum of characteristics found in multi-image reasoning scenarios. We have\nbenchmarked several cutting-edge LLMs using ReMI and found a substantial gap\nbetween their performance and human-level proficiency. This highlights the\nchallenges in multi-image reasoning and the need for further research. Our\nanalysis also reveals the strengths and weaknesses of different models,\nshedding light on the types of reasoning that are currently attainable and\nareas where future models require improvement. To foster further research in\nthis area, we are releasing ReMI publicly:\nhttps://huggingface.co/datasets/mehrankazemi/ReMI.", "published": "2024-06-13 14:37:04", "link": "http://arxiv.org/abs/2406.09175v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Language Complexity and Speech Recognition Accuracy: Orthographic\n  Complexity Hurts, Phonological Complexity Doesn't", "abstract": "We investigate what linguistic factors affect the performance of Automatic\nSpeech Recognition (ASR) models. We hypothesize that orthographic and\nphonological complexities both degrade accuracy. To examine this, we fine-tune\nthe multilingual self-supervised pretrained model Wav2Vec2-XLSR-53 on 25\nlanguages with 15 writing systems, and we compare their ASR accuracy, number of\ngraphemes, unigram grapheme entropy, logographicity (how much\nword/morpheme-level information is encoded in the writing system), and number\nof phonemes. The results demonstrate that orthographic complexities\nsignificantly correlate with low ASR accuracy, while phonological complexity\nshows no significant correlation.", "published": "2024-06-13 14:59:45", "link": "http://arxiv.org/abs/2406.09202v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ReadCtrl: Personalizing text generation with readability-controlled\n  instruction learning", "abstract": "Content generation conditioning on users's readability is an important\napplication for personalization. In an era of large language models (LLMs),\nreadability-controlled text generation based on LLMs has become increasingly\nimportant. This paper introduces a novel methodology called\n\"Readability-Controlled Instruction Learning (ReadCtrl),\" which aims to\ninstruction-tune LLMs to tailor users' readability levels. Unlike the\ntraditional methods, which primarily focused on categorical readability\nadjustments typically classified as high, medium, and low or expert and\nlayperson levels with limited success, ReadCtrl introduces a dynamic framework\nthat enables LLMs to generate content at various (near continuous level)\ncomplexity levels, thereby enhancing their versatility across different\napplications. Our results show that the ReadCtrl-Mistral-7B models\nsignificantly outperformed strong baseline models such as GPT-4 and Claude-3,\nwith a win rate of 52.1%:35.7% against GPT-4 in human evaluations. Furthermore,\nRead-Ctrl has shown significant improvements in automatic evaluations, as\nevidenced by better readability metrics (e.g., FOG, FKGL) and generation\nquality metrics (e.g., BLEU, SARI, SummaC-Factuality, UniEval-Consistency and\nCoherence). These results underscore Read-Ctrl's effectiveness and tenacity in\nproducing high-quality, contextually appropriate outputs that closely align\nwith targeted readability levels, marking a significant advancement in\npersonalized content generation using LLMs.", "published": "2024-06-13 15:03:46", "link": "http://arxiv.org/abs/2406.09205v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AlignMMBench: Evaluating Chinese Multimodal Alignment in Large\n  Vision-Language Models", "abstract": "Evaluating the alignment capabilities of large Vision-Language Models (VLMs)\nis essential for determining their effectiveness as helpful assistants.\nHowever, existing benchmarks primarily focus on basic abilities using nonverbal\nmethods, such as yes-no and multiple-choice questions. In this paper, we\naddress this gap by introducing AlignMMBench, a comprehensive alignment\nbenchmark specifically designed for emerging Chinese VLMs. This benchmark is\nmeticulously curated from real-world scenarios and Chinese Internet sources,\nencompassing thirteen specific tasks across three categories, and includes both\nsingle-turn and multi-turn dialogue scenarios. Incorporating a prompt rewrite\nstrategy, AlignMMBench encompasses 1,054 images and 4,978 question-answer\npairs. To facilitate the evaluation pipeline, we propose CritiqueVLM, a\nrule-calibrated evaluator that exceeds GPT-4's evaluation ability. Finally, we\nreport the performance of representative VLMs on AlignMMBench, offering\ninsights into the capabilities and limitations of different VLM architectures.\nAll evaluation codes and data are available on https://alignmmbench.github.io.", "published": "2024-06-13 16:30:14", "link": "http://arxiv.org/abs/2406.09295v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Transformers meet Neural Algorithmic Reasoners", "abstract": "Transformers have revolutionized machine learning with their simple yet\neffective architecture. Pre-training Transformers on massive text datasets from\nthe Internet has led to unmatched generalization for natural language\nunderstanding (NLU) tasks. However, such language models remain fragile when\ntasked with algorithmic forms of reasoning, where computations must be precise\nand robust. To address this limitation, we propose a novel approach that\ncombines the Transformer's language understanding with the robustness of graph\nneural network (GNN)-based neural algorithmic reasoners (NARs). Such NARs\nproved effective as generic solvers for algorithmic tasks, when specified in\ngraph form. To make their embeddings accessible to a Transformer, we propose a\nhybrid architecture with a two-phase training procedure, allowing the tokens in\nthe language model to cross-attend to the node embeddings from the NAR. We\nevaluate our resulting TransNAR model on CLRS-Text, the text-based version of\nthe CLRS-30 benchmark, and demonstrate significant gains over Transformer-only\nmodels for algorithmic reasoning, both in and out of distribution.", "published": "2024-06-13 16:42:06", "link": "http://arxiv.org/abs/2406.09308v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Khmer Semantic Search Engine (KSE): Digital Information Access and\n  Document Retrieval", "abstract": "The search engine process is crucial for document content retrieval. For\nKhmer documents, an effective tool is needed to extract essential keywords and\nfacilitate accurate searches. Despite the daily generation of significant Khmer\ncontent, Cambodians struggle to find necessary documents due to the lack of an\neffective semantic searching tool. Even Google does not deliver high accuracy\nfor Khmer content. Semantic search engines improve search results by employing\nadvanced algorithms to understand various content types. With the rise in Khmer\ndigital content such as reports, articles, and social media feedback enhanced\nsearch capabilities are essential. This research proposes the first Khmer\nSemantic Search Engine (KSE), designed to enhance traditional Khmer search\nmethods. Utilizing semantic matching techniques and formally annotated semantic\ncontent, our tool extracts meaningful keywords from user queries, performs\nprecise matching, and provides the best matching offline documents and online\nURLs. We propose three semantic search frameworks: semantic search based on a\nkeyword dictionary, semantic search based on ontology, and semantic search\nbased on ranking. Additionally, we developed tools for data preparation,\nincluding document addition and manual keyword extraction. To evaluate\nperformance, we created a ground truth dataset and addressed issues related to\nsearching and semantic search. Our findings demonstrate that understanding\nsearch term semantics can lead to significantly more accurate results.", "published": "2024-06-13 16:58:02", "link": "http://arxiv.org/abs/2406.09320v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal\n  Language Models", "abstract": "Humans draw to facilitate reasoning: we draw auxiliary lines when solving\ngeometry problems; we mark and circle when reasoning on maps; we use sketches\nto amplify our ideas and relieve our limited-capacity working memory. However,\nsuch actions are missing in current multimodal language models (LMs). Current\nchain-of-thought and tool-use paradigms only use text as intermediate reasoning\nsteps. In this work, we introduce Sketchpad, a framework that gives multimodal\nLMs a visual sketchpad and tools to draw on the sketchpad. The LM conducts\nplanning and reasoning according to the visual artifacts it has drawn.\nDifferent from prior work, which uses text-to-image models to enable LMs to\ndraw, Sketchpad enables LMs to draw with lines, boxes, marks, etc., which is\ncloser to human sketching and better facilitates reasoning. Sketchpad can also\nuse specialist vision models during the sketching process (e.g., draw bounding\nboxes with object detection models, draw masks with segmentation models), to\nfurther enhance visual perception and reasoning. We experiment with a wide\nrange of math tasks (including geometry, functions, graphs, and chess) and\ncomplex visual reasoning tasks. Sketchpad substantially improves performance on\nall tasks over strong base models with no sketching, yielding an average gain\nof 12.7% on math tasks, and 8.6% on vision tasks. GPT-4o with Sketchpad sets a\nnew state of the art on all tasks, including V*Bench (80.3%), BLINK spatial\nreasoning (83.9%), and visual correspondence (80.8%). All codes and data are in\nhttps://visualsketchpad.github.io/.", "published": "2024-06-13 17:59:31", "link": "http://arxiv.org/abs/2406.09403v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Talking Heads: Understanding Inter-layer Communication in Transformer\n  Language Models", "abstract": "Although it is known that transformer language models (LMs) pass features\nfrom early layers to later layers, it is not well understood how this\ninformation is represented and routed by the model. We analyze a mechanism used\nin two LMs to selectively inhibit items in a context in one task, and find that\nit underlies a commonly used abstraction across many context-retrieval\nbehaviors. Specifically, we find that models write into low-rank subspaces of\nthe residual stream to represent features which are then read out by later\nlayers, forming low-rank communication channels (Elhage et al., 2021) between\nlayers. A particular 3D subspace in model activations in GPT-2 can be traversed\nto positionally index items in lists, and we show that this mechanism can\nexplain an otherwise arbitrary-seeming sensitivity of the model to the order of\nitems in the prompt. That is, the model has trouble copying the correct\ninformation from context when many items ``crowd\" this limited space. By\ndecomposing attention heads with the Singular Value Decomposition (SVD), we\nfind that previously described interactions between heads separated by one or\nmore layers can be predicted via analysis of their weight matrices alone. We\nshow that it is possible to manipulate the internal model representations as\nwell as edit model weights based on the mechanism we discover in order to\nsignificantly improve performance on our synthetic Laundry List task, which\nrequires recall from a list, often improving task accuracy by over 20%. Our\nanalysis reveals a surprisingly intricate interpretable structure learned from\nlanguage model pretraining, and helps us understand why sophisticated LMs\nsometimes fail in simple domains, facilitating future analysis of more complex\nbehaviors.", "published": "2024-06-13 18:12:01", "link": "http://arxiv.org/abs/2406.09519v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Urdu Dependency Parsing and Treebank Development: A Syntactic and\n  Morphological Perspective", "abstract": "Parsing is the process of analyzing a sentence's syntactic structure by\nbreaking it down into its grammatical components. and is critical for various\nlinguistic applications. Urdu is a low-resource, free word-order language and\nexhibits complex morphology. Literature suggests that dependency parsing is\nwell-suited for such languages. Our approach begins with a basic feature model\nencompassing word location, head word identification, and dependency relations,\nfollowed by a more advanced model integrating part-of-speech (POS) tags and\nmorphological attributes (e.g., suffixes, gender). We manually annotated a\ncorpus of news articles of varying complexity. Using Maltparser and the\nNivreEager algorithm, we achieved a best-labeled accuracy (LA) of 70% and an\nunlabeled attachment score (UAS) of 84%, demonstrating the feasibility of\ndependency parsing for Urdu.", "published": "2024-06-13 19:30:32", "link": "http://arxiv.org/abs/2406.09549v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Analyzing Gender Polarity in Short Social Media Texts with BERT: The\n  Role of Emojis and Emoticons", "abstract": "In this effort we fine tuned different models based on BERT to detect the\ngender polarity of twitter accounts. We specially focused on analyzing the\neffect of using emojis and emoticons in performance of our model in classifying\ntask. We were able to demonstrate that the use of these none word inputs\nalongside the mention of other accounts in a short text format like tweet has\nan impact in detecting the account holder's gender.", "published": "2024-06-13 20:23:59", "link": "http://arxiv.org/abs/2406.09573v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RelevAI-Reviewer: A Benchmark on AI Reviewers for Survey Paper Relevance", "abstract": "Recent advancements in Artificial Intelligence (AI), particularly the\nwidespread adoption of Large Language Models (LLMs), have significantly\nenhanced text analysis capabilities. This technological evolution offers\nconsiderable promise for automating the review of scientific papers, a task\ntraditionally managed through peer review by fellow researchers. Despite its\ncritical role in maintaining research quality, the conventional peer-review\nprocess is often slow and subject to biases, potentially impeding the swift\npropagation of scientific knowledge. In this paper, we propose\nRelevAI-Reviewer, an automatic system that conceptualizes the task of survey\npaper review as a classification problem, aimed at assessing the relevance of a\npaper in relation to a specified prompt, analogous to a \"call for papers\". To\naddress this, we introduce a novel dataset comprised of 25,164 instances. Each\ninstance contains one prompt and four candidate papers, each varying in\nrelevance to the prompt. The objective is to develop a machine learning (ML)\nmodel capable of determining the relevance of each paper and identifying the\nmost pertinent one. We explore various baseline approaches, including\ntraditional ML classifiers like Support Vector Machine (SVM) and advanced\nlanguage models such as BERT. Preliminary findings indicate that the BERT-based\nend-to-end classifier surpasses other conventional ML methods in performance.\nWe present this problem as a public challenge to foster engagement and interest\nin this area of research.", "published": "2024-06-13 06:42:32", "link": "http://arxiv.org/abs/2406.10294v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Robustness of Structured Data Extraction from In-plane Rotated Documents\n  using Multi-Modal Large Language Models (LLM)", "abstract": "Multi-modal large language models (LLMs) have shown remarkable performance in\nvarious natural language processing tasks, including data extraction from\ndocuments. However, the accuracy of these models can be significantly affected\nby document in-plane rotation, also known as skew, a common issue in real-world\nscenarios for scanned documents. This study investigates the impact of document\nskew on the data extraction accuracy of three state-of-the-art multi-modal\nLLMs: Anthropic Claude V3 Sonnet, GPT-4-Turbo, and Llava:v1.6. We focus on\nextracting specific entities from synthetically generated sample documents with\nvarying degrees of skewness. The results demonstrate that document skew\nadversely affects the data extraction accuracy of all the tested LLMs, with the\nseverity of the impact varying across models. We identify the safe in-plane\nrotation angles (SIPRA) for each model and investigate the effects of skew on\nmodel hallucinations. Furthermore, we explore existing skew detection and\ncorrection mechanisms and discuss their potential limitations. We propose\nalternative approaches, including developing new multi-modal architectures that\nare inherently more robust to document skew and incorporating skewing\ntechniques during the pre-training phase of the models. Additionally, we\nhighlight the need for more comprehensive testing on a wider range of document\nquality and conditions to fully understand the challenges and opportunities\nassociated with using multi-modal LLMs for information extraction in real-world\nscenarios.", "published": "2024-06-13 08:55:01", "link": "http://arxiv.org/abs/2406.10295v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "SememeLM: A Sememe Knowledge Enhanced Method for Long-tail Relation\n  Representation", "abstract": "Recognizing relations between two words is a fundamental task with the broad\napplications. Different from extracting relations from text, it is difficult to\nidentify relations among words without their contexts. Especially for long-tail\nrelations, it becomes more difficult due to inadequate semantic features.\nExisting approaches based on language models (LMs) utilize rich knowledge of\nLMs to enhance the semantic features of relations. However, they capture\nuncommon relations while overlooking less frequent but meaningful ones since\nknowledge of LMs seriously relies on trained data where often represents common\nrelations. On the other hand, long-tail relations are often uncommon in\ntraining data. It is interesting but not trivial to use external knowledge to\nenrich LMs due to collecting corpus containing long-tail relationships is\nhardly feasible. In this paper, we propose a sememe knowledge enhanced method\n(SememeLM) to enhance the representation of long-tail relations, in which\nsememes can break the contextual constraints between wors. Firstly, we present\na sememe relation graph and propose a graph encoding method. Moreover, since\nexternal knowledge base possibly consisting of massive irrelevant knowledge,\nthe noise is introduced. We propose a consistency alignment module, which\naligns the introduced knowledge with LMs, reduces the noise and integrates the\nknowledge into the language model. Finally, we conducted experiments on word\nanalogy datasets, which evaluates the ability to distinguish relation\nrepresentations subtle differences, including long-tail relations. Extensive\nexperiments show that our approach outperforms some state-of-the-art methods.", "published": "2024-06-13 12:42:49", "link": "http://arxiv.org/abs/2406.10297v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VLind-Bench: Measuring Language Priors in Large Vision-Language Models", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated outstanding\nperformance across various multimodal tasks. However, they suffer from a\nproblem known as language prior, where responses are generated based solely on\ntextual patterns while disregarding image information. Addressing the issue of\nlanguage prior is crucial, as it can lead to undesirable biases or\nhallucinations when dealing with images that are out of training distribution.\nDespite its importance, current methods for accurately measuring language\npriors in LVLMs are poorly studied. Although existing benchmarks based on\ncounterfactual or out-of-distribution images can partially be used to measure\nlanguage priors, they fail to disentangle language priors from other\nconfounding factors. To this end, we propose a new benchmark called\nVLind-Bench, which is the first benchmark specifically designed to measure the\nlanguage priors, or blindness, of LVLMs. It not only includes tests on\ncounterfactual images to assess language priors but also involves a series of\ntests to evaluate more basic capabilities such as commonsense knowledge, visual\nperception, and commonsense biases. For each instance in our benchmark, we\nensure that all these basic tests are passed before evaluating the language\npriors, thereby minimizing the influence of other factors on the assessment.\nThe evaluation and analysis of recent LVLMs in our benchmark reveal that almost\nall models exhibit a significant reliance on language priors, presenting a\nstrong challenge in the field.", "published": "2024-06-13 00:00:20", "link": "http://arxiv.org/abs/2406.08702v4", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "LLM-Driven Robots Risk Enacting Discrimination, Violence, and Unlawful\n  Actions", "abstract": "Members of the Human-Robot Interaction (HRI) and Artificial Intelligence (AI)\ncommunities have proposed Large Language Models (LLMs) as a promising resource\nfor robotics tasks such as natural language interactions, doing household and\nworkplace tasks, approximating `common sense reasoning', and modeling humans.\nHowever, recent research has raised concerns about the potential for LLMs to\nproduce discriminatory outcomes and unsafe behaviors in real-world robot\nexperiments and applications. To address these concerns, we conduct an\nHRI-based evaluation of discrimination and safety criteria on several\nhighly-rated LLMs. Our evaluation reveals that LLMs currently lack robustness\nwhen encountering people across a diverse range of protected identity\ncharacteristics (e.g., race, gender, disability status, nationality, religion,\nand their intersections), producing biased outputs consistent with directly\ndiscriminatory outcomes -- e.g. `gypsy' and `mute' people are labeled\nuntrustworthy, but not `european' or `able-bodied' people. Furthermore, we test\nmodels in settings with unconstrained natural language (open vocabulary)\ninputs, and find they fail to act safely, generating responses that accept\ndangerous, violent, or unlawful instructions -- such as incident-causing\nmisstatements, taking people's mobility aids, and sexual predation. Our results\nunderscore the urgent need for systematic, routine, and comprehensive risk\nassessments and assurances to improve outcomes and ensure LLMs only operate on\nrobots when it is safe, effective, and just to do so. Data and code will be\nmade available.", "published": "2024-06-13 05:31:49", "link": "http://arxiv.org/abs/2406.08824v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.RO"}
{"title": "Research on Optimization of Natural Language Processing Model Based on\n  Multimodal Deep Learning", "abstract": "This project intends to study the image representation based on attention\nmechanism and multimodal data. By adding multiple pattern layers to the\nattribute model, the semantic and hidden layers of image content are\nintegrated. The word vector is quantified by the Word2Vec method and then\nevaluated by a word embedding convolutional neural network. The published\nexperimental results of the two groups were tested. The experimental results\nshow that this method can convert discrete features into continuous characters,\nthus reducing the complexity of feature preprocessing. Word2Vec and natural\nlanguage processing technology are integrated to achieve the goal of direct\nevaluation of missing image features. The robustness of the image feature\nevaluation model is improved by using the excellent feature analysis\ncharacteristics of a convolutional neural network. This project intends to\nimprove the existing image feature identification methods and eliminate the\nsubjective influence in the evaluation process. The findings from the\nsimulation indicate that the novel approach has developed is viable,\neffectively augmenting the features within the produced representations.", "published": "2024-06-13 06:03:59", "link": "http://arxiv.org/abs/2406.08838v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Navigating the Shadows: Unveiling Effective Disturbances for Modern AI\n  Content Detectors", "abstract": "With the launch of ChatGPT, large language models (LLMs) have attracted\nglobal attention. In the realm of article writing, LLMs have witnessed\nextensive utilization, giving rise to concerns related to intellectual property\nprotection, personal privacy, and academic integrity. In response, AI-text\ndetection has emerged to distinguish between human and machine-generated\ncontent. However, recent research indicates that these detection systems often\nlack robustness and struggle to effectively differentiate perturbed texts.\nCurrently, there is a lack of systematic evaluations regarding detection\nperformance in real-world applications, and a comprehensive examination of\nperturbation techniques and detector robustness is also absent. To bridge this\ngap, our work simulates real-world scenarios in both informal and professional\nwriting, exploring the out-of-the-box performance of current detectors.\nAdditionally, we have constructed 12 black-box text perturbation methods to\nassess the robustness of current detection models across various perturbation\ngranularities. Furthermore, through adversarial learning experiments, we\ninvestigate the impact of perturbation data augmentation on the robustness of\nAI-text detectors. We have released our code and data at\nhttps://github.com/zhouying20/ai-text-detector-evaluation.", "published": "2024-06-13 08:37:01", "link": "http://arxiv.org/abs/2406.08922v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Multilingual Unseen Speaker Emotion Recognition: Leveraging\n  Co-Attention Cues in Multitask Learning", "abstract": "Advent of modern deep learning techniques has given rise to advancements in\nthe field of Speech Emotion Recognition (SER). However, most systems prevalent\nin the field fail to generalize to speakers not seen during training. This\nstudy focuses on handling challenges of multilingual SER, specifically on\nunseen speakers. We introduce CAMuLeNet, a novel architecture leveraging\nco-attention based fusion and multitask learning to address this problem.\nAdditionally, we benchmark pretrained encoders of Whisper, HuBERT, Wav2Vec2.0,\nand WavLM using 10-fold leave-speaker-out cross-validation on five existing\nmultilingual benchmark datasets: IEMOCAP, RAVDESS, CREMA-D, EmoDB and CaFE and,\nrelease a novel dataset for SER on the Hindi language (BhavVani). CAMuLeNet\nshows an average improvement of approximately 8% over all benchmarks on unseen\nspeakers determined by our cross-validation strategy.", "published": "2024-06-13 09:00:14", "link": "http://arxiv.org/abs/2406.08931v2", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multi-Agent Software Development through Cross-Team Collaboration", "abstract": "The latest breakthroughs in Large Language Models (LLMs), eg., ChatDev, have\ncatalyzed profound transformations, particularly through multi-agent\ncollaboration for software development. LLM agents can collaborate in teams\nlike humans, and follow the waterfall model to sequentially work on\nrequirements analysis, development, review, testing, and other phases to\nperform autonomous software generation. However, for an agent team, each phase\nin a single development process yields only one possible outcome. This results\nin the completion of only one development chain, thereby losing the opportunity\nto explore multiple potential decision paths within the solution space.\nConsequently, this may lead to obtaining suboptimal results. To address this\nchallenge, we introduce Cross-Team Collaboration (CTC), a scalable multi-team\nframework that enables orchestrated teams to jointly propose various decisions\nand communicate with their insights in a cross-team collaboration environment\nfor superior content generation. Experimental results in software development\nreveal a notable increase in quality compared to state-of-the-art baselines,\nunderscoring the efficacy of our framework. The significant improvements in\nstory generation demonstrate the promising generalization ability of our\nframework across various domains. We anticipate that our work will guide LLM\nagents towards a cross-team paradigm and contribute to their significant growth\nin but not limited to software development. The code and data will be available\nat https://github.com/OpenBMB/ChatDev.", "published": "2024-06-13 10:18:36", "link": "http://arxiv.org/abs/2406.08979v1", "categories": ["cs.CL", "cs.AI", "cs.MA", "cs.SE"], "primary_category": "cs.CL"}
{"title": "ME-Switch: A Memory-Efficient Expert Switching Framework for Large\n  Language Models", "abstract": "LLM development involves pre-training a foundation model on massive data,\nfollowed by fine-tuning on task-specific data to create specialized experts.\nServing these experts can pose significant memory challenges, as loading all\nexperts onto devices is impractical, and frequent switching between experts in\nresponse to user requests can incur substantial I/O costs. Previous approaches\ndecompose the expert weights as the pre-trained weights plus delta weights,\nfollowed by quantizing the delta weights using output channel-wise step sizes\nto reduce the model size. However, these methods overlook the fact that certain\ninput channels of delta weights can cause significant quantization errors at\nextremely low bitwidths. Additionally, existing methods assume that the\nappropriate model for a user request is known in advance, which is not the case\nin practice. To this end, we introduce ME-Switch, a memory-efficient expert\nswitching framework tailored for serving multiple LLMs. To condense the number\nof bits required for describing the delta weights, we propose a salient-aware\ndelta compression method that identifies salient input channels based on\nreconstruction error and applies mixed-precision quantization, reducing\nnon-salient channels to low bits while keeping salient ones intact, cutting\nstorage demand without compromising performance. Moreover, we develop a\nmodel-level routing method that efficiently directs user queries to the most\nsuitable expert by performing domain classification. Extensive experiments show\nthe promising memory efficiency and routing performance of ME-Switch. For\nexample, when serving three models from the Mistral-7B family, ME-Switch\nreduces the model size by $1.74\\times$ and maintains nearly lossless\nperformance on instruction, mathematical reasoning, and code generation tasks.\nNotably, our method can efficiently serve 16 Mistral-7B models on a single\nNVIDIA A100 GPU.", "published": "2024-06-13 12:27:55", "link": "http://arxiv.org/abs/2406.09041v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interpreting the structure of multi-object representations in vision\n  encoders", "abstract": "In this work, we interpret the representations of multi-object scenes in\nvision encoders through the lens of structured representations. Structured\nrepresentations allow modeling of individual objects distinctly and their\nflexible use based on the task context for both scene-level and object-specific\ntasks. These capabilities play a central role in human reasoning and\ngeneralization, allowing us to abstract away irrelevant details and focus on\nrelevant information in a compact and usable form. We define structured\nrepresentations as those that adhere to two specific properties: binding\nspecific object information into discrete representation units and segregating\nobject representations into separate sets of tokens to minimize cross-object\nentanglement. Based on these properties, we evaluated and compared image\nencoders pre-trained on classification (ViT), large vision-language models\n(CLIP, BLIP, FLAVA), and self-supervised methods (DINO, DINOv2). We examine the\ntoken representations by creating object-decoding tasks that measure the\nability of specific tokens to capture individual objects in multi-object scenes\nfrom the COCO dataset. This analysis provides insights into how object-wise\nrepresentations are distributed across tokens and layers within these vision\nencoders. Our findings highlight significant differences in the representation\nof objects depending on their relevance to the pre-training objective, with\nthis effect particularly pronounced in the CLS token (often used for downstream\ntasks). Meanwhile, networks and layers that exhibit more structured\nrepresentations retain better information about individual objects. To guide\npractical applications, we propose formal measures to quantify the two\nproperties of structured representations, aiding in selecting and adapting\nvision encoders for downstream tasks.", "published": "2024-06-13 12:54:20", "link": "http://arxiv.org/abs/2406.09067v3", "categories": ["cs.CV", "cs.CL", "q-bio.NC"], "primary_category": "cs.CV"}
{"title": "INS-MMBench: A Comprehensive Benchmark for Evaluating LVLMs' Performance\n  in Insurance", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated outstanding\nperformance in various general multimodal applications such as image\nrecognition and visual reasoning, and have also shown promising potential in\nspecialized domains. However, the application potential of LVLMs in the\ninsurance domain-characterized by rich application scenarios and abundant\nmultimodal data-has not been effectively explored. There is no systematic\nreview of multimodal tasks in the insurance domain, nor a benchmark\nspecifically designed to evaluate the capabilities of LVLMs in insurance. This\ngap hinders the development of LVLMs within the insurance domain. In this\npaper, we systematically review and distill multimodal tasks for four\nrepresentative types of insurance: auto insurance, property insurance, health\ninsurance, and agricultural insurance. We propose INS-MMBench, the first\ncomprehensive LVLMs benchmark tailored for the insurance domain. INS-MMBench\ncomprises a total of 2.2K thoroughly designed multiple-choice questions,\ncovering 12 meta-tasks and 22 fundamental tasks. Furthermore, we evaluate\nmultiple representative LVLMs, including closed-source models such as GPT-4o\nand open-source models like BLIP-2. This evaluation not only validates the\neffectiveness of our benchmark but also provides an in-depth performance\nanalysis of current LVLMs on various multimodal tasks in the insurance domain.\nWe hope that INS-MMBench will facilitate the further application of LVLMs in\nthe insurance domain and inspire interdisciplinary development. Our dataset and\nevaluation code are available at https://github.com/FDU-INS/INS-MMBench.", "published": "2024-06-13 13:31:49", "link": "http://arxiv.org/abs/2406.09105v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "LASER: Learning by Aligning Self-supervised Representations of Speech\n  for Improving Content-related Tasks", "abstract": "Self-supervised learning (SSL)-based speech models are extensively used for\nfull-stack speech processing. However, it has been observed that improving\nSSL-based speech representations using unlabeled speech for content-related\ntasks is challenging and computationally expensive. Recent attempts have been\nmade to address this issue with cost-effective self-supervised fine-tuning\n(SSFT) approaches. Continuing in this direction, a cost-effective SSFT method\nnamed \"LASER: Learning by Aligning Self-supervised Representations\" is\npresented. LASER is based on the soft-DTW alignment loss with temporal\nregularisation term. Experiments are conducted with HuBERT and WavLM models and\nevaluated on the SUPERB benchmark for two content-related tasks: automatic\nspeech recognition (ASR) and phoneme recognition (PR). A relative improvement\nof 3.7% and 8.2% for HuBERT, and 4.1% and 11.7% for WavLM are observed, for the\nASR and PR tasks respectively, with only < 3 hours of fine-tuning on a single\nGPU.", "published": "2024-06-13 14:17:47", "link": "http://arxiv.org/abs/2406.09153v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Diffusion Gaussian Mixture Audio Denoise", "abstract": "Recent diffusion models have achieved promising performances in\naudio-denoising tasks. The unique property of the reverse process could recover\nclean signals. However, the distribution of real-world noises does not comply\nwith a single Gaussian distribution and is even unknown. The sampling of\nGaussian noise conditions limits its application scenarios. To overcome these\nchallenges, we propose a DiffGMM model, a denoising model based on the\ndiffusion and Gaussian mixture models. We employ the reverse process to\nestimate parameters for the Gaussian mixture model. Given a noisy audio signal,\nwe first apply a 1D-U-Net to extract features and train linear layers to\nestimate parameters for the Gaussian mixture model, and we approximate the real\nnoise distributions. The noisy signal is continuously subtracted from the\nestimated noise to output clean audio signals. Extensive experimental results\ndemonstrate that the proposed DiffGMM model achieves state-of-the-art\nperformance.", "published": "2024-06-13 14:18:10", "link": "http://arxiv.org/abs/2406.09154v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DefAn: Definitive Answer Dataset for LLMs Hallucination Evaluation", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities,\nrevolutionizing the integration of AI in daily life applications. However, they\nare prone to hallucinations, generating claims that contradict established\nfacts, deviating from prompts, and producing inconsistent responses when the\nsame prompt is presented multiple times. Addressing these issues is challenging\ndue to the lack of comprehensive and easily assessable benchmark datasets. Most\nexisting datasets are small and rely on multiple-choice questions, which are\ninadequate for evaluating the generative prowess of LLMs. To measure\nhallucination in LLMs, this paper introduces a comprehensive benchmark dataset\ncomprising over 75,000 prompts across eight domains. These prompts are designed\nto elicit definitive, concise, and informative answers. The dataset is divided\ninto two segments: one publicly available for testing and assessing LLM\nperformance and a hidden segment for benchmarking various LLMs. In our\nexperiments, we tested six LLMs-GPT-3.5, LLama 2, LLama 3, Gemini, Mixtral, and\nZephyr-revealing that overall factual hallucination ranges from 59% to 82% on\nthe public dataset and 57% to 76% in the hidden benchmark. Prompt misalignment\nhallucination ranges from 6% to 95% in the public dataset and 17% to 94% in the\nhidden counterpart. Average consistency ranges from 21% to 61% and 22% to 63%,\nrespectively. Domain-wise analysis shows that LLM performance significantly\ndeteriorates when asked for specific numeric information while performing\nmoderately with person, location, and date queries. Our dataset demonstrates\nits efficacy and serves as a comprehensive benchmark for LLM performance\nevaluation. Our dataset and LLMs responses are available at\n\\href{https://github.com/ashikiut/DefAn}{https://github.com/ashikiut/DefAn}.", "published": "2024-06-13 14:18:13", "link": "http://arxiv.org/abs/2406.09155v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Training for Sample-Efficient Active Learning for Text\n  Classification with Pre-Trained Language Models", "abstract": "Active learning is an iterative labeling process that is used to obtain a\nsmall labeled subset, despite the absence of labeled data, thereby enabling to\ntrain a model for supervised tasks such as text classification. While active\nlearning has made considerable progress in recent years due to improvements\nprovided by pre-trained language models, there is untapped potential in the\noften neglected unlabeled portion of the data, although it is available in\nconsiderably larger quantities than the usually small set of labeled data. In\nthis work, we investigate how self-training, a semi-supervised approach that\nuses a model to obtain pseudo-labels for unlabeled data, can be used to improve\nthe efficiency of active learning for text classification. Building on a\ncomprehensive reproduction of four previous self-training approaches, some of\nwhich are evaluated for the first time in the context of active learning or\nnatural language processing, we introduce HAST, a new and effective\nself-training strategy, which is evaluated on four text classification\nbenchmarks. Our results show that it outperforms the reproduced self-training\napproaches and reaches classification results comparable to previous\nexperiments for three out of four datasets, using as little as 25% of the data.\nThe code is publicly available at\nhttps://github.com/chschroeder/self-training-for-sample-efficient-active-learning .", "published": "2024-06-13 15:06:11", "link": "http://arxiv.org/abs/2406.09206v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Bidirectional Human-AI Alignment: A Systematic Review for\n  Clarifications, Framework, and Future Directions", "abstract": "Recent advancements in general-purpose AI have highlighted the importance of\nguiding AI systems towards the intended goals, ethical principles, and values\nof individuals and groups, a concept broadly recognized as alignment. However,\nthe lack of clarified definitions and scopes of human-AI alignment poses a\nsignificant obstacle, hampering collaborative efforts across research domains\nto achieve this alignment. In particular, ML- and philosophy-oriented alignment\nresearch often views AI alignment as a static, unidirectional process (i.e.,\naiming to ensure that AI systems' objectives match humans) rather than an\nongoing, mutual alignment problem. This perspective largely neglects the\nlong-term interaction and dynamic changes of alignment. To understand these\ngaps, we introduce a systematic review of over 400 papers published between\n2019 and January 2024, spanning multiple domains such as Human-Computer\nInteraction (HCI), Natural Language Processing (NLP), Machine Learning (ML). We\ncharacterize, define and scope human-AI alignment. From this, we present a\nconceptual framework of \"Bidirectional Human-AI Alignment\" to organize the\nliterature from a human-centered perspective. This framework encompasses both\n1) conventional studies of aligning AI to humans that ensures AI produces the\nintended outcomes determined by humans, and 2) a proposed concept of aligning\nhumans to AI, which aims to help individuals and society adjust to AI\nadvancements both cognitively and behaviorally. Additionally, we articulate the\nkey findings derived from literature analysis, including literature gaps and\ntrends, human values, and interaction techniques. To pave the way for future\nstudies, we envision three key challenges and give recommendations for future\nresearch.", "published": "2024-06-13 16:03:25", "link": "http://arxiv.org/abs/2406.09264v3", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "End-to-end streaming model for low-latency speech anonymization", "abstract": "Speaker anonymization aims to conceal cues to speaker identity while\npreserving linguistic content. Current machine learning based approaches\nrequire substantial computational resources, hindering real-time streaming\napplications. To address these concerns, we propose a streaming model that\nachieves speaker anonymization with low latency. The system is trained in an\nend-to-end autoencoder fashion using a lightweight content encoder that\nextracts HuBERT-like information, a pretrained speaker encoder that extract\nspeaker identity, and a variance encoder that injects pitch and energy\ninformation. These three disentangled representations are fed to a decoder that\nre-synthesizes the speech signal. We present evaluation results from two\nimplementations of our system, a full model that achieves a latency of 230ms,\nand a lite version (0.1x in size) that further reduces latency to 66ms while\nmaintaining state-of-the-art performance in naturalness, intelligibility, and\nprivacy preservation.", "published": "2024-06-13 16:15:53", "link": "http://arxiv.org/abs/2406.09277v2", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "On the Effects of Heterogeneous Data Sources on Speech-to-Text\n  Foundation Models", "abstract": "The Open Whisper-style Speech Model (OWSM) series was introduced to achieve\nfull transparency in building advanced speech-to-text (S2T) foundation models.\nTo this end, OWSM models are trained on 25 public speech datasets, which are\nheterogeneous in multiple ways. In this study, we advance the OWSM series by\nintroducing OWSM v3.2, which improves on prior models by investigating and\naddressing the impacts of this data heterogeneity. Our study begins with a\ndetailed analysis of each dataset, from which we derive two key strategies:\ndata filtering with proxy task to enhance data quality, and the incorporation\nof punctuation and true-casing using an open large language model (LLM). With\nall other configurations staying the same, OWSM v3.2 improves performance over\nthe OWSM v3.1 baseline while using 15% less training data.", "published": "2024-06-13 16:22:37", "link": "http://arxiv.org/abs/2406.09282v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Understanding Jailbreak Success: A Study of Latent Space Dynamics in\n  Large Language Models", "abstract": "Conversational large language models are trained to refuse to answer harmful\nquestions. However, emergent jailbreaking techniques can still elicit unsafe\noutputs, presenting an ongoing challenge for model alignment. To better\nunderstand how different jailbreak types circumvent safeguards, this paper\nanalyses model activations on different jailbreak inputs. We find that it is\npossible to extract a jailbreak vector from a single class of jailbreaks that\nworks to mitigate jailbreak effectiveness from other semantically-dissimilar\nclasses. This may indicate that different kinds of effective jailbreaks operate\nvia a similar internal mechanism. We investigate a potential common mechanism\nof harmfulness feature suppression, and find evidence that effective jailbreaks\nnoticeably reduce a model's perception of prompt harmfulness. These findings\noffer actionable insights for developing more robust jailbreak countermeasures\nand lay the groundwork for a deeper, mechanistic understanding of jailbreak\ndynamics in language models.", "published": "2024-06-13 16:26:47", "link": "http://arxiv.org/abs/2406.09289v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Spoken Language Identification Strategies for Automatic\n  Transcription of Multilingual Broadcast and Institutional Speech", "abstract": "This paper addresses spoken language identification (SLI) and speech\nrecognition of multilingual broadcast and institutional speech, real\napplication scenarios that have been rarely addressed in the SLI literature.\nObserving that in these domains language changes are mostly associated with\nspeaker changes, we propose a cascaded system consisting of speaker diarization\nand language identification and compare it with more traditional language\nidentification and language diarization systems. Results show that the proposed\nsystem often achieves lower language classification and language diarization\nerror rates (up to 10% relative language diarization error reduction and 60%\nrelative language confusion reduction) and leads to lower WERs on multilingual\ntest sets (more than 8% relative WER reduction), while at the same time does\nnot negatively affect speech recognition on monolingual audio (with an absolute\nWER increase between 0.1% and 0.7% w.r.t. monolingual ASR).", "published": "2024-06-13 16:27:56", "link": "http://arxiv.org/abs/2406.09290v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts\n  Against Large Language Models", "abstract": "Jailbreak attacks induce Large Language Models (LLMs) to generate harmful\nresponses, posing severe misuse threats. Though research on jailbreak attacks\nand defenses is emerging, there is no consensus on evaluating jailbreaks, i.e.,\nthe methods to assess the harmfulness of an LLM's response are varied. Each\napproach has its own set of strengths and weaknesses, impacting their alignment\nwith human values, as well as the time and financial cost. This diversity\nchallenges researchers in choosing suitable evaluation methods and comparing\ndifferent attacks and defenses. In this paper, we conduct a comprehensive\nanalysis of jailbreak evaluation methodologies, drawing from nearly 90\njailbreak research published between May 2023 and April 2024. Our study\nintroduces a systematic taxonomy of jailbreak evaluators, offering indepth\ninsights into their strengths and weaknesses, along with the current status of\ntheir adaptation. To aid further research, we propose JailbreakEval, a toolkit\nfor evaluating jailbreak attempts. JailbreakEval includes various evaluators\nout-of-the-box, enabling users to obtain results with a single command or\ncustomized evaluation workflows. In summary, we regard JailbreakEval to be a\ncatalyst that simplifies the evaluation process in jailbreak research and\nfosters an inclusive standard for jailbreak evaluation within the community.", "published": "2024-06-13 16:59:43", "link": "http://arxiv.org/abs/2406.09321v2", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs", "abstract": "Although Large Language Models (LLMs) have demonstrated significant\ncapabilities in executing complex tasks in a zero-shot manner, they are\nsusceptible to jailbreak attacks and can be manipulated to produce harmful\noutputs. Recently, a growing body of research has categorized jailbreak attacks\ninto token-level and prompt-level attacks. However, previous work primarily\noverlooks the diverse key factors of jailbreak attacks, with most studies\nconcentrating on LLM vulnerabilities and lacking exploration of\ndefense-enhanced LLMs. To address these issues, we introduced\n$\\textbf{JailTrickBench}$ to evaluate the impact of various attack settings on\nLLM performance and provide a baseline for jailbreak attacks, encouraging the\nadoption of a standardized evaluation framework. Specifically, we evaluate the\neight key factors of implementing jailbreak attacks on LLMs from both\ntarget-level and attack-level perspectives. We further conduct seven\nrepresentative jailbreak attacks on six defense methods across two widely used\ndatasets, encompassing approximately 354 experiments with about 55,000 GPU\nhours on A800-80G. Our experimental results highlight the need for standardized\nbenchmarking to evaluate these attacks on defense-enhanced LLMs. Our code is\navailable at https://github.com/usail-hkust/JailTrickBench.", "published": "2024-06-13 17:01:40", "link": "http://arxiv.org/abs/2406.09324v3", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "DiscreteSLU: A Large Language Model with Self-Supervised Discrete Speech\n  Units for Spoken Language Understanding", "abstract": "The integration of pre-trained text-based large language models (LLM) with\nspeech input has enabled instruction-following capabilities for diverse speech\ntasks. This integration requires the use of a speech encoder, a speech adapter,\nand an LLM, trained on diverse tasks. We propose the use of discrete speech\nunits (DSU), rather than continuous-valued speech encoder outputs, that are\nconverted to the LLM token embedding space using the speech adapter. We\ngenerate DSU using a self-supervised speech encoder followed by k-means\nclustering. The proposed model shows robust performance on speech inputs from\nseen/unseen domains and instruction-following capability in spoken question\nanswering. We also explore various types of DSU extracted from different layers\nof the self-supervised speech encoder, as well as Mel frequency Cepstral\nCoefficients (MFCC). Our findings suggest that the ASR task and datasets are\nnot crucial in instruction-tuning for spoken question answering tasks.", "published": "2024-06-13 17:28:13", "link": "http://arxiv.org/abs/2406.09345v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Improving Autoregressive Training with Dynamic Oracles", "abstract": "Many tasks within NLP can be framed as sequential decision problems, ranging\nfrom sequence tagging to text generation. However, for many tasks, the standard\ntraining methods, including maximum likelihood (teacher forcing) and scheduled\nsampling, suffer from exposure bias and a mismatch between metrics employed\nduring training and inference. DAgger provides a solution to mitigate these\nproblems, yet it requires a metric-specific dynamic oracle algorithm, which\ndoes not exist for many common metrics like span-based F1, ROUGE, and BLEU. In\nthis paper, we develop these novel dynamic oracles and show they maintain\nDAgger's no-regret guarantee for decomposable metrics like span-based F1. We\nevaluate the algorithm's performance on named entity recognition (NER), text\nsummarization, and machine translation (MT). While DAgger with dynamic oracle\nyields less favorable results in our MT experiments, it outperforms the\nbaseline techniques in NER and text summarization.", "published": "2024-06-13 17:59:09", "link": "http://arxiv.org/abs/2406.09393v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MuirBench: A Comprehensive Benchmark for Robust Multi-image\n  Understanding", "abstract": "We introduce MuirBench, a comprehensive benchmark that focuses on robust\nmulti-image understanding capabilities of multimodal LLMs. MuirBench consists\nof 12 diverse multi-image tasks (e.g., scene understanding, ordering) that\ninvolve 10 categories of multi-image relations (e.g., multiview, temporal\nrelations). Comprising 11,264 images and 2,600 multiple-choice questions,\nMuirBench is created in a pairwise manner, where each standard instance is\npaired with an unanswerable variant that has minimal semantic differences, in\norder for a reliable assessment. Evaluated upon 20 recent multi-modal LLMs, our\nresults reveal that even the best-performing models like GPT-4o and Gemini Pro\nfind it challenging to solve MuirBench, achieving 68.0% and 49.3% in accuracy.\nOpen-source multimodal LLMs trained on single images can hardly generalize to\nmulti-image questions, hovering below 33.3% in accuracy. These results\nhighlight the importance of MuirBench in encouraging the community to develop\nmultimodal LLMs that can look beyond a single image, suggesting potential\npathways for future improvements.", "published": "2024-06-13 17:59:52", "link": "http://arxiv.org/abs/2406.09411v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Newswire: A Large-Scale Structured Database of a Century of Historical\n  News", "abstract": "In the U.S. historically, local newspapers drew their content largely from\nnewswires like the Associated Press. Historians argue that newswires played a\npivotal role in creating a national identity and shared understanding of the\nworld, but there is no comprehensive archive of the content sent over\nnewswires. We reconstruct such an archive by applying a customized deep\nlearning pipeline to hundreds of terabytes of raw image scans from thousands of\nlocal newspapers. The resulting dataset contains 2.7 million unique public\ndomain U.S. newswire articles, written between 1878 and 1977. Locations in\nthese articles are georeferenced, topics are tagged using customized neural\ntopic classification, named entities are recognized, and individuals are\ndisambiguated to Wikipedia using a novel entity disambiguation model. To\nconstruct the Newswire dataset, we first recognize newspaper layouts and\ntranscribe around 138 millions structured article texts from raw image scans.\nWe then use a customized neural bi-encoder model to de-duplicate reproduced\narticles, in the presence of considerable abridgement and noise, quantifying\nhow widely each article was reproduced. A text classifier is used to ensure\nthat we only include newswire articles, which historically are in the public\ndomain. The structured data that accompany the texts provide rich information\nabout the who (disambiguated individuals), what (topics), and where\n(georeferencing) of the news that millions of Americans read over the course of\na century. We also include Library of Congress metadata information about the\nnewspapers that ran the articles on their front pages. The Newswire dataset is\nuseful both for large language modeling - expanding training data beyond what\nis available from modern web texts - and for studying a diversity of questions\nin computational linguistics, social science, and the digital humanities.", "published": "2024-06-13 16:20:05", "link": "http://arxiv.org/abs/2406.09490v1", "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "primary_category": "cs.CL"}
{"title": "A Systematic Review of Generative AI for Teaching and Learning Practice", "abstract": "The use of generative artificial intelligence (GenAI) in academia is a\nsubjective and hotly debated topic. Currently, there are no agreed guidelines\ntowards the usage of GenAI systems in higher education (HE) and, thus, it is\nstill unclear how to make effective use of the technology for teaching and\nlearning practice. This paper provides an overview of the current state of\nresearch on GenAI for teaching and learning in HE. To this end, this study\nconducted a systematic review of relevant studies indexed by Scopus, using the\npreferred reporting items for systematic reviews and meta-analyses (PRISMA)\nguidelines. The search criteria revealed a total of 625 research papers, of\nwhich 355 met the final inclusion criteria. The findings from the review showed\nthe current state and the future trends in documents, citations, document\nsources/authors, keywords, and co-authorship. The research gaps identified\nsuggest that while some authors have looked at understanding the detection of\nAI-generated text, it may be beneficial to understand how GenAI can be\nincorporated into supporting the educational curriculum for assessments,\nteaching, and learning delivery. Furthermore, there is a need for additional\ninterdisciplinary, multidimensional studies in HE through collaboration. This\nwill strengthen the awareness and understanding of students, tutors, and other\nstakeholders, which will be instrumental in formulating guidelines, frameworks,\nand policies for GenAI usage.", "published": "2024-06-13 18:16:27", "link": "http://arxiv.org/abs/2406.09520v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "H.3.3"], "primary_category": "cs.IR"}
{"title": "$S^3$ -- Semantic Signal Separation", "abstract": "Topic models are useful tools for discovering latent semantic structures in\nlarge textual corpora. Topic modeling historically relied on bag-of-words\nrepresentations of language. This approach makes models sensitive to the\npresence of stop words and noise, and does not utilize potentially useful\ncontextual information. Recent efforts have been oriented at incorporating\ncontextual neural representations in topic modeling and have been shown to\noutperform classical topic models. These approaches are, however, typically\nslow, volatile and still require preprocessing for optimal results. We present\nSemantic Signal Separation ($S^3$), a theory-driven topic modeling approach in\nneural embedding spaces. $S^3$ conceptualizes topics as independent axes of\nsemantic space, and uncovers these with blind-source separation. Our approach\nprovides the most diverse, highly coherent topics, requires no preprocessing,\nand is demonstrated to be the fastest contextually sensitive topic model to\ndate. We offer an implementation of $S^3$, among other approaches, in the\nTurftopic Python package.", "published": "2024-06-13 19:43:38", "link": "http://arxiv.org/abs/2406.09556v2", "categories": ["cs.LG", "cs.CL", "stat.ML", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Decoding the Diversity: A Review of the Indic AI Research Landscape", "abstract": "This review paper provides a comprehensive overview of large language model\n(LLM) research directions within Indic languages. Indic languages are those\nspoken in the Indian subcontinent, including India, Pakistan, Bangladesh, Sri\nLanka, Nepal, and Bhutan, among others. These languages have a rich cultural\nand linguistic heritage and are spoken by over 1.5 billion people worldwide.\nWith the tremendous market potential and growing demand for natural language\nprocessing (NLP) based applications in diverse languages, generative\napplications for Indic languages pose unique challenges and opportunities for\nresearch. Our paper deep dives into the recent advancements in Indic generative\nmodeling, contributing with a taxonomy of research directions, tabulating 84\nrecent publications. Research directions surveyed in this paper include LLM\ndevelopment, fine-tuning existing LLMs, development of corpora, benchmarking\nand evaluation, as well as publications around specific techniques, tools, and\napplications. We found that researchers across the publications emphasize the\nchallenges associated with limited data availability, lack of standardization,\nand the peculiar linguistic complexities of Indic languages. This work aims to\nserve as a valuable resource for researchers and practitioners working in the\nfield of NLP, particularly those focused on Indic languages, and contributes to\nthe development of more accurate and efficient LLM applications for these\nlanguages.", "published": "2024-06-13 19:55:20", "link": "http://arxiv.org/abs/2406.09559v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speech ReaLLM -- Real-time Streaming Speech Recognition with Multimodal\n  LLMs by Teaching the Flow of Time", "abstract": "We introduce Speech ReaLLM, a new ASR architecture that marries\n\"decoder-only\" ASR with the RNN-T to make multimodal LLM architectures capable\nof real-time streaming. This is the first \"decoder-only\" ASR architecture\ndesigned to handle continuous audio without explicit end-pointing. Speech\nReaLLM is a special case of the more general ReaLLM (\"real-time LLM\") approach,\nalso introduced here for the first time. The idea is inspired by RNN-T: Instead\nof generating a response only at the end of a user prompt, generate after every\ninput token received in real time (it is often empty). On Librispeech \"test\",\nan 80M Speech ReaLLM achieves WERs of 3.0% and 7.4% in real time (without an\nexternal LM or auxiliary loss). This is only slightly above a 3x larger\nAttention-Encoder-Decoder baseline. We also show that this way, an LLM\narchitecture can learn to represent and reproduce the flow of time; and that a\npre-trained 7B LLM can be fine-tuned to do reasonably well on this task.", "published": "2024-06-13 20:20:29", "link": "http://arxiv.org/abs/2406.09569v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multimodal Large Language Models with Fusion Low Rank Adaptation for\n  Device Directed Speech Detection", "abstract": "Although Large Language Models (LLMs) have shown promise for human-like\nconversations, they are primarily pre-trained on text data. Incorporating audio\nor video improves performance, but collecting large-scale multimodal data and\npre-training multimodal LLMs is challenging. To this end, we propose a Fusion\nLow Rank Adaptation (FLoRA) technique that efficiently adapts a pre-trained\nunimodal LLM to consume new, previously unseen modalities via low rank\nadaptation. For device-directed speech detection, using FLoRA, the multimodal\nLLM achieves 22% relative reduction in equal error rate (EER) over the\ntext-only approach and attains performance parity with its full fine-tuning\n(FFT) counterpart while needing to tune only a fraction of its parameters.\nFurthermore, with the newly introduced adapter dropout, FLoRA is robust to\nmissing data, improving over FFT by 20% lower EER and 56% lower false accept\nrate. The proposed approach scales well for model sizes from 16M to 3B\nparameters.", "published": "2024-06-13 22:52:07", "link": "http://arxiv.org/abs/2406.09617v1", "categories": ["cs.CL", "cs.HC", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ResearchArena: Benchmarking Large Language Models' Ability to Collect\n  and Organize Information as Research Agents", "abstract": "Large language models (LLMs) excel across many natural language processing\ntasks but face challenges in domain-specific, analytical tasks such as\nconducting research surveys. This study introduces ResearchArena, a benchmark\ndesigned to evaluate LLMs' capabilities in conducting academic\nsurveys$\\unicode{x2013}$a foundational step in academic research. ResearchArena\nmodels the process in three stages: (1) information discovery, identifying\nrelevant literature; (2) information selection, evaluating papers' relevance\nand impact; and (3) information organization, structuring knowledge into\nhierarchical frameworks such as mind-maps. Notably, mind-map construction is\ntreated as a bonus task, reflecting its supplementary role in survey-writing.\nTo support these evaluations, we construct an offline environment of 12M\nfull-text academic papers and 7.9K survey papers. To ensure ethical compliance,\nwe do not redistribute copyrighted materials; instead, we provide code to\nconstruct the environment from the Semantic Scholar Open Research Corpus\n(S2ORC). Preliminary evaluations reveal that LLM-based approaches underperform\ncompared to simpler keyword-based retrieval methods, underscoring significant\nopportunities for advancing LLMs in autonomous research.", "published": "2024-06-13 03:26:30", "link": "http://arxiv.org/abs/2406.10291v2", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Automatically Labeling Clinical Trial Outcomes: A Large-Scale Benchmark\n  for Drug Development", "abstract": "Background The cost of drug discovery and development is substantial, with\nclinical trial outcomes playing a critical role in regulatory approval and\npatient care. However, access to large-scale, high-quality clinical trial\noutcome data remains limited, hindering advancements in predictive modeling and\nevidence-based decision-making.\n  Methods We present the Clinical Trial Outcome (CTO) benchmark, a fully\nreproducible, large-scale repository encompassing approximately 125,000 drug\nand biologics trials. CTO integrates large language model (LLM) interpretations\nof publications, trial phase progression tracking, sentiment analysis from news\nsources, stock price movements of trial sponsors, and additional trial-related\nmetrics. Furthermore, we manually annotated a dataset of clinical trials\nconducted between 2020 and 2024 to enhance the quality and reliability of\noutcome labels.\n  Results The trial outcome labels in the CTO benchmark agree strongly with\nexpert annotations, achieving an F1 score of 94 for Phase 3 trials and 91\nacross all phases. Additionally, benchmarking standard machine learning models\non our manually annotated dataset revealed distribution shifts in recent\ntrials, underscoring the necessity of continuously updated labeling approaches.\n  Conclusions By analyzing CTO's performance on recent clinical trials, we\ndemonstrate the ongoing need for high-quality, up-to-date trial outcome labels.\nWe publicly release the CTO knowledge base and annotated labels at\nhttps://chufangao.github.io/CTOD, with regular updates to support research on\nclinical trial outcomes and inform data-driven improvements in drug\ndevelopment.", "published": "2024-06-13 04:23:35", "link": "http://arxiv.org/abs/2406.10292v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "CLST: Cold-Start Mitigation in Knowledge Tracing by Aligning a\n  Generative Language Model as a Students' Knowledge Tracer", "abstract": "Knowledge tracing (KT), wherein students' problem-solving histories are used\nto estimate their current levels of knowledge, has attracted significant\ninterest from researchers. However, most existing KT models were developed with\nan ID-based paradigm, which exhibits limitations in cold-start performance.\nThese limitations can be mitigated by leveraging the vast quantities of\nexternal knowledge possessed by generative large language models (LLMs). In\nthis study, we propose cold-start mitigation in knowledge tracing by aligning a\ngenerative language model as a students' knowledge tracer (CLST) as a framework\nthat utilizes a generative LLM as a knowledge tracer. Upon collecting data from\nmath, social studies, and science subjects, we framed the KT task as a natural\nlanguage processing task, wherein problem-solving data are expressed in natural\nlanguage, and fine-tuned the generative LLM using the formatted KT dataset.\nSubsequently, we evaluated the performance of the CLST in situations of data\nscarcity using various baseline models for comparison. The results indicate\nthat the CLST significantly enhanced performance with a dataset of fewer than\n100 students in terms of prediction, reliability, and cross-domain\ngeneralization.", "published": "2024-06-13 09:21:43", "link": "http://arxiv.org/abs/2406.10296v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Large Language Models as Software Components: A Taxonomy for\n  LLM-Integrated Applications", "abstract": "Large Language Models (LLMs) have become widely adopted recently. Research\nexplores their use both as autonomous agents and as tools for software\nengineering. LLM-integrated applications, on the other hand, are software\nsystems that leverage an LLM to perform tasks that would otherwise be\nimpossible or require significant coding effort. While LLM-integrated\napplication engineering is emerging as new discipline, its terminology,\nconcepts and methods need to be established. This study provides a taxonomy for\nLLM-integrated applications, offering a framework for analyzing and describing\nthese systems. It also demonstrates various ways to utilize LLMs in\napplications, as well as options for implementing such integrations.\n  Following established methods, we analyze a sample of recent LLM-integrated\napplications to identify relevant dimensions. We evaluate the taxonomy by\napplying it to additional cases. This review shows that applications integrate\nLLMs in numerous ways for various purposes. Frequently, they comprise multiple\nLLM integrations, which we term ``LLM components''. To gain a clear\nunderstanding of an application's architecture, we examine each LLM component\nseparately. We identify thirteen dimensions along which to characterize an LLM\ncomponent, including the LLM skills leveraged, the format of the output, and\nmore. LLM-integrated applications are described as combinations of their LLM\ncomponents. We suggest a concise representation using feature vectors for\nvisualization.\n  The taxonomy is effective for describing LLM-integrated applications. It can\ncontribute to theory building in the nascent field of LLM-integrated\napplication engineering and aid in developing such systems. Researchers and\npractitioners explore numerous creative ways to leverage LLMs in applications.\nThough challenges persist, integrating LLMs may revolutionize the way software\nsystems are built.", "published": "2024-06-13 21:32:56", "link": "http://arxiv.org/abs/2406.10300v1", "categories": ["cs.SE", "cs.CL", "cs.LG", "A.1; I.2.7; D.2.11"], "primary_category": "cs.SE"}
{"title": "Advanced Multimodal Deep Learning Architecture for Image-Text Matching", "abstract": "Image-text matching is a key multimodal task that aims to model the semantic\nassociation between images and text as a matching relationship. With the advent\nof the multimedia information age, image, and text data show explosive growth,\nand how to accurately realize the efficient and accurate semantic\ncorrespondence between them has become the core issue of common concern in\nacademia and industry. In this study, we delve into the limitations of current\nmultimodal deep learning models in processing image-text pairing tasks.\nTherefore, we innovatively design an advanced multimodal deep learning\narchitecture, which combines the high-level abstract representation ability of\ndeep neural networks for visual information with the advantages of natural\nlanguage processing models for text semantic understanding. By introducing a\nnovel cross-modal attention mechanism and hierarchical feature fusion strategy,\nthe model achieves deep fusion and two-way interaction between image and text\nfeature space. In addition, we also optimize the training objectives and loss\nfunctions to ensure that the model can better map the potential association\nstructure between images and text during the learning process. Experiments show\nthat compared with existing image-text matching models, the optimized new model\nhas significantly improved performance on a series of benchmark data sets. In\naddition, the new model also shows excellent generalization and robustness on\nlarge and diverse open scenario datasets and can maintain high matching\nperformance even in the face of previously unseen complex situations.", "published": "2024-06-13 08:32:24", "link": "http://arxiv.org/abs/2406.15306v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "A Document-based Knowledge Discovery with Microservices Architecture", "abstract": "The first step towards digitalization within organizations lies in\ndigitization - the conversion of analog data into digitally stored data. This\nbasic step is the prerequisite for all following activities like the\ndigitalization of processes or the servitization of products or offerings.\nHowever, digitization itself often leads to 'data-rich' but 'knowledge-poor'\nmaterial. Knowledge discovery and knowledge extraction as approaches try to\nincrease the usefulness of digitized data. In this paper, we point out the key\nchallenges in the context of knowledge discovery and present an approach to\naddressing these using a microservices architecture. Our solution led to a\nconceptual design focusing on keyword extraction, similarity calculation of\ndocuments, database queries in natural language, and programming language\nindependent provision of the extracted information. In addition, the conceptual\ndesign provides referential design guidelines for integrating processes and\napplications for semi-automatic learning, editing, and visualization of\nontologies. The concept also uses a microservices architecture to address\nnon-functional requirements, such as scalability and resilience. The evaluation\nof the specified requirements is performed using a demonstrator that implements\nthe concept. Furthermore, this modern approach is used in the German patent\noffice in an extended version.", "published": "2024-06-13 09:28:31", "link": "http://arxiv.org/abs/2407.00053v1", "categories": ["cs.DC", "cs.CL", "cs.DL", "cs.IR"], "primary_category": "cs.DC"}
{"title": "Multi-Modal Retrieval For Large Language Model Based Speech Recognition", "abstract": "Retrieval is a widely adopted approach for improving language models\nleveraging external information. As the field moves towards multi-modal large\nlanguage models, it is important to extend the pure text based methods to\nincorporate other modalities in retrieval as well for applications across the\nwide spectrum of machine learning tasks and data types. In this work, we\npropose multi-modal retrieval with two approaches: kNN-LM and cross-attention\ntechniques. We demonstrate the effectiveness of our retrieval approaches\nempirically by applying them to automatic speech recognition tasks with access\nto external information. Under this setting, we show that speech-based\nmulti-modal retrieval outperforms text based retrieval, and yields up to 50 %\nimprovement in word error rate over the multi-modal language model baseline.\nFurthermore, we achieve state-of-the-art recognition results on the\nSpoken-Squad question answering dataset.", "published": "2024-06-13 22:55:22", "link": "http://arxiv.org/abs/2406.09618v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multi-Channel Multi-Speaker ASR Using Target Speaker's Solo Segment", "abstract": "In the field of multi-channel, multi-speaker Automatic Speech Recognition\n(ASR), the task of discerning and accurately transcribing a target speaker's\nspeech within background noise remains a formidable challenge. Traditional\napproaches often rely on microphone array configurations and the information of\nthe target speaker's location or voiceprint. This study introduces the Solo\nSpatial Feature (Solo-SF), an innovative method that utilizes a target\nspeaker's isolated speech segment to enhance ASR performance, thereby\ncircumventing the need for conventional inputs like microphone array layouts.\nWe explore effective strategies for selecting optimal solo segments, a crucial\naspect for Solo-SF's success. Through evaluations conducted on the AliMeeting\ndataset and AISHELL-1 simulations, Solo-SF demonstrates superior performance\nover existing techniques, significantly lowering Character Error Rates (CER) in\nvarious test conditions. Our findings highlight Solo-SF's potential as an\neffective solution for addressing the complexities of multi-channel,\nmulti-speaker ASR tasks.", "published": "2024-06-13 21:07:29", "link": "http://arxiv.org/abs/2406.09589v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "TSE-PI: Target Sound Extraction under Reverberant Environments with\n  Pitch Information", "abstract": "Target sound extraction (TSE) separates the target sound from the mixture\nsignals based on provided clues. However, the performance of existing models\nsignificantly degrades under reverberant conditions. Inspired by auditory scene\nanalysis (ASA), this work proposes a TSE model provided with pitch information\nnamed TSE-PI. Conditional pitch extraction is achieved through the Feature-wise\nLinearly Modulated layer with the sound-class label. A modified Waveformer\nmodel combined with pitch information, employing a learnable Gammatone\nfilterbank in place of the convolutional encoder, is used for target sound\nextraction. The inclusion of pitch information is aimed at improving the\nmodel's performance. The experimental results on the FSD50K dataset illustrate\n2.4 dB improvements of target sound extraction under reverberant environments\nwhen incorporating pitch information and Gammatone filterbank.", "published": "2024-06-13 00:43:48", "link": "http://arxiv.org/abs/2406.08716v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VISinger2+: End-to-End Singing Voice Synthesis Augmented by\n  Self-Supervised Learning Representation", "abstract": "Singing Voice Synthesis (SVS) has witnessed significant advancements with the\nadvent of deep learning techniques. However, a significant challenge in SVS is\nthe scarcity of labeled singing voice data, which limits the effectiveness of\nsupervised learning methods. In response to this challenge, this paper\nintroduces a novel approach to enhance the quality of SVS by leveraging\nunlabeled data from pre-trained self-supervised learning models. Building upon\nthe existing VISinger2 framework, this study integrates additional spectral\nfeature information into the system to enhance its performance. The integration\naims to harness the rich acoustic features from the pre-trained models, thereby\nenriching the synthesis and yielding a more natural and expressive singing\nvoice. Experimental results in various corpora demonstrate the efficacy of this\napproach in improving the overall quality of synthesized singing voices in both\nobjective and subjective metrics.", "published": "2024-06-13 02:44:49", "link": "http://arxiv.org/abs/2406.08761v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DubWise: Video-Guided Speech Duration Control in Multimodal LLM-based\n  Text-to-Speech for Dubbing", "abstract": "Audio-visual alignment after dubbing is a challenging research problem. To\nthis end, we propose a novel method, DubWise Multi-modal Large Language Model\n(LLM)-based Text-to-Speech (TTS), which can control the speech duration of\nsynthesized speech in such a way that it aligns well with the speakers lip\nmovements given in the reference video even when the spoken text is different\nor in a different language. To accomplish this, we propose to utilize\ncross-modal attention techniques in a pre-trained GPT-based TTS. We combine\nlinguistic tokens from text, speaker identity tokens via a voice cloning\nnetwork, and video tokens via a proposed duration controller network. We\ndemonstrate the effectiveness of our system on the Lip2Wav-Chemistry and LRS2\ndatasets. Also, the proposed method achieves improved lip sync and naturalness\ncompared to the SOTAs for the same language but different text (i.e.,\nnon-parallel) and the different language, different text (i.e., cross-lingual)\nscenarios.", "published": "2024-06-13 04:36:34", "link": "http://arxiv.org/abs/2406.08802v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Generating Speakers by Prompting Listener Impressions for Pre-trained\n  Multi-Speaker Text-to-Speech Systems", "abstract": "This paper proposes a speech synthesis system that allows users to specify\nand control the acoustic characteristics of a speaker by means of prompts\ndescribing the speaker's traits of synthesized speech. Unlike previous\napproaches, our method utilizes listener impressions to construct prompts,\nwhich are easier to collect and align more naturally with everyday descriptions\nof speaker traits. We adopt the Low-rank Adaptation (LoRA) technique to swiftly\ntailor a pre-trained language model to our needs, facilitating the extraction\nof speaker-related traits from the prompt text. Besides, different from other\nprompt-driven text-to-speech (TTS) systems, we separate the prompt-to-speaker\nmodule from the multi-speaker TTS system, enhancing system flexibility and\ncompatibility with various pre-trained multi-speaker TTS systems. Moreover, for\nthe prompt-to-speaker characteristic module, we also compared the\ndiscriminative method and flow-matching based generative method and we found\nthat combining both methods can help the system simultaneously capture\nspeaker-related information from prompts better and generate speech with higher\nfidelity.", "published": "2024-06-13 05:06:30", "link": "http://arxiv.org/abs/2406.08812v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "EffectiveASR: A Single-Step Non-Autoregressive Mandarin Speech\n  Recognition Architecture with High Accuracy and Inference Speed", "abstract": "Non-autoregressive (NAR) automatic speech recognition (ASR) models predict\ntokens independently and simultaneously, bringing high inference speed.\nHowever, there is still a gap in the accuracy of the NAR models compared to the\nautoregressive (AR) models. In this paper, we propose a single-step NAR ASR\narchitecture with high accuracy and inference speed, called EffectiveASR. It\nuses an Index Mapping Vector (IMV) based alignment generator to generate\nalignments during training, and an alignment predictor to learn the alignments\nfor inference. It can be trained end-to-end (E2E) with cross-entropy loss\ncombined with alignment loss. The proposed EffectiveASR achieves competitive\nresults on the AISHELL-1 and AISHELL-2 Mandarin benchmarks compared to the\nleading models. Specifically, it achieves character error rates (CER) of\n4.26%/4.62% on the AISHELL-1 dev/test dataset, which outperforms the AR\nConformer with about 30x inference speedup.", "published": "2024-06-13 05:57:54", "link": "http://arxiv.org/abs/2406.08835v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SingOMD: Singing Oriented Multi-resolution Discrete Representation\n  Construction from Speech Models", "abstract": "Discrete representation has shown advantages in speech generation tasks,\nwherein discrete tokens are derived by discretizing hidden features from\nself-supervised learning (SSL) pre-trained models. However, the direct\napplication of speech SSL models to singing generation encounters domain gaps\nbetween speech and singing. Furthermore, singing generation necessitates a more\nrefined representation than typical speech. To address these challenges, we\nintroduce SingOMD, a novel method to extract singing-oriented multi-resolution\ndiscrete representations from speech SSL models. Specifically, we first adapt\nthe features from speech SSL through a resynthesis task and incorporate\nmulti-resolution modules based on resampling to better serve singing\ngeneration. These adapted multi-resolution features are then discretized via\nclustering. Extensive experiments demonstrate the robustness, efficiency, and\neffectiveness of these representations in singing vocoders and singing voice\nsynthesis.", "published": "2024-06-13 08:00:25", "link": "http://arxiv.org/abs/2406.08905v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cascaded noise reduction and acoustic echo cancellation based on an\n  extended noise reduction", "abstract": "In many speech recording applications, the recorded desired speech is\ncorrupted by both noise and acoustic echo, such that combined noise reduction\n(NR) and acoustic echo cancellation (AEC) is called for. A common cascaded\ndesign corresponds to NR filters preceding AEC filters. These NR filters aim at\nreducing the near-end room noise (and possibly partially the echo) and operate\non the microphones only, consequently requiring the AEC filters to model both\nthe echo paths and the NR filters. In this paper, however, we propose a design\nwith extended NR (NRext) filters preceding AEC filters under the assumption of\nthe echo paths being additive maps, thus preserving the addition operation.\nHere, the NRext filters aim at reducing both the near-end room noise and the\nfar-end room noise component in the echo, and operate on both the microphones\nand loudspeakers. We show that the succeeding AEC filters remarkably become\nindependent of the NRext filters, such that the AEC filters are only required\nto model the echo paths, improving the AEC performance. Further, the degrees of\nfreedom in the NRext filters scale with the number of loudspeakers, which is\nnot the case for the NR filters, resulting in an improved NR performance.", "published": "2024-06-13 10:06:35", "link": "http://arxiv.org/abs/2406.08974v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ToneUnit: A Speech Discretization Approach for Tonal Language Speech\n  Synthesis", "abstract": "Representing speech as discretized units has numerous benefits in supporting\ndownstream spoken language processing tasks. However, the approach has been\nless explored in speech synthesis of tonal languages like Mandarin Chinese. Our\npreliminary experiments on Chinese speech synthesis reveal the issue of \"tone\nshift\", where a synthesized speech utterance contains correct base syllables\nbut incorrect tones. To address the issue, we propose the ToneUnit framework,\nwhich leverages annotated data with tone labels as CTC supervision to learn\ntone-aware discrete speech units for Mandarin Chinese speech. Our findings\nindicate that the discrete units acquired through the TonUnit resolve the \"tone\nshift\" issue in synthesized Chinese speech and yield favorable results in\nEnglish synthesis. Moreover, the experimental results suggest that finite\nscalar quantization enhances the effectiveness of ToneUnit. Notably, ToneUnit\ncan work effectively even with minimal annotated data.", "published": "2024-06-13 10:36:18", "link": "http://arxiv.org/abs/2406.08989v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Complex Image-Generative Diffusion Transformer for Audio Denoising", "abstract": "The audio denoising technique has captured widespread attention in the deep\nneural network field. Recently, the audio denoising problem has been converted\ninto an image generation task, and deep learning-based approaches have been\napplied to tackle this problem. However, its performance is still limited,\nleaving room for further improvement. In order to enhance audio denoising\nperformance, this paper introduces a complex image-generative diffusion\ntransformer that captures more information from the complex Fourier domain. We\nexplore a novel diffusion transformer by integrating the transformer with a\ndiffusion model. Our proposed model demonstrates the scalability of the\ntransformer and expands the receptive field of sparse attention using attention\ndiffusion. Our work is among the first to utilize diffusion transformers to\ndeal with the image generation task for audio denoising. Extensive experiments\non two benchmark datasets demonstrate that our proposed model outperforms\nstate-of-the-art methods.", "published": "2024-06-13 14:23:19", "link": "http://arxiv.org/abs/2406.09161v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Vision Transformer Segmentation for Visual Bird Sound Denoising", "abstract": "Audio denoising, especially in the context of bird sounds, remains a\nchallenging task due to persistent residual noise. Traditional and deep\nlearning methods often struggle with artificial or low-frequency noise. In this\nwork, we propose ViTVS, a novel approach that leverages the power of the vision\ntransformer (ViT) architecture. ViTVS adeptly combines segmentation techniques\nto disentangle clean audio from complex signal mixtures. Our key contributions\nencompass the development of ViTVS, introducing comprehensive, long-range, and\nmulti-scale representations. These contributions directly tackle the\nlimitations inherent in conventional approaches. Extensive experiments\ndemonstrate that ViTVS outperforms state-of-the-art methods, positioning it as\na benchmark solution for real-world bird sound denoising applications. Source\ncode is available at: https://github.com/aiai-4/ViVTS.", "published": "2024-06-13 14:28:37", "link": "http://arxiv.org/abs/2406.09167v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FlowAVSE: Efficient Audio-Visual Speech Enhancement with Conditional\n  Flow Matching", "abstract": "This work proposes an efficient method to enhance the quality of corrupted\nspeech signals by leveraging both acoustic and visual cues. While existing\ndiffusion-based approaches have demonstrated remarkable quality, their\napplicability is limited by slow inference speeds and computational complexity.\nTo address this issue, we present FlowAVSE which enhances the inference speed\nand reduces the number of learnable parameters without degrading the output\nquality. In particular, we employ a conditional flow matching algorithm that\nenables the generation of high-quality speech in a single sampling step.\nMoreover, we increase efficiency by optimizing the underlying U-net\narchitecture of diffusion-based systems. Our experiments demonstrate that\nFlowAVSE achieves 22 times faster inference speed and reduces the model size by\nhalf while maintaining the output quality. The demo page is available at:\nhttps://cyongong.github.io/FlowAVSE.github.io/", "published": "2024-06-13 16:26:33", "link": "http://arxiv.org/abs/2406.09286v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Second DISPLACE Challenge : DIarization of SPeaker and LAnguage in\n  Conversational Environments", "abstract": "The DIarization of SPeaker and LAnguage in Conversational Environments\n(DISPLACE) 2024 challenge is the second in the series of DISPLACE challenges,\nwhich involves tasks of speaker diarization (SD) and language diarization (LD)\non a challenging multilingual conversational speech dataset. In the DISPLACE\n2024 challenge, we also introduced the task of automatic speech recognition\n(ASR) on this dataset. The dataset containing 158 hours of speech, consisting\nof both supervised and unsupervised mono-channel far-field recordings, was\nreleased for LD and SD tracks. Further, 12 hours of close-field mono-channel\nrecordings were provided for the ASR track conducted on 5 Indian languages. The\ndetails of the dataset, baseline systems and the leader board results are\nhighlighted in this paper. We have also compared our baseline models and the\nteam's performances on evaluation data of DISPLACE-2023 to emphasize the\nadvancements made in this second version of the challenge.", "published": "2024-06-13 17:32:32", "link": "http://arxiv.org/abs/2406.09494v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Efficient Personalization of Amplification in Hearing Aids via\n  Multi-band Bayesian Machine Learning", "abstract": "Personalization of the amplification function of hearing aids has been shown\nto be of benefit to hearing aid users in previous studies. Several machine\nlearning-based personalization approaches have been introduced in the\nliterature. This paper presents a machine learning personalization approach\nwith the advantage of being efficient in its training based on paired\ncomparisons which makes it practical and field deployable. The training\nefficiency of this approach is the result of treating frequency bands\nindependent of one another and by simultaneously carrying out Bayesian machine\nlearning in each band across all of the frequency bands. Simulation results\nindicate that this approach leads to an estimated hearing preference function\nclose to the true hearing preference function in fewer number of paired\ncomparisons relative to the previous machine learning approaches. In addition,\na clinical experiment conducted on eight subjects with hearing impairment\nindicate that this training efficient personalization approach provides\npersonalized gain settings which are on average six times more preferred over\nthe standard prescriptive gain settings.", "published": "2024-06-13 23:49:24", "link": "http://arxiv.org/abs/2406.09634v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "MFF-EINV2: Multi-scale Feature Fusion across Spectral-Spatial-Temporal\n  Domains for Sound Event Localization and Detection", "abstract": "Sound Event Localization and Detection (SELD) involves detecting and\nlocalizing sound events using multichannel sound recordings. Previously\nproposed Event-Independent Network V2 (EINV2) has achieved outstanding\nperformance on SELD. However, it still faces challenges in effectively\nextracting features across spectral, spatial, and temporal domains. This paper\nproposes a three-stage network structure named Multi-scale Feature Fusion (MFF)\nmodule to fully extract multi-scale features across spectral, spatial, and\ntemporal domains. The MFF module utilizes parallel subnetworks architecture to\ngenerate multi-scale spectral and spatial features. The TF-Convolution Module\nis employed to provide multi-scale temporal features. We incorporated MFF into\nEINV2 and term the proposed method as MFF-EINV2. Experimental results in 2022\nand 2023 DCASE challenge task3 datasets show the effectiveness of our\nMFF-EINV2, which achieves state-of-the-art (SOTA) performance compared to\npublished methods.", "published": "2024-06-13 03:03:02", "link": "http://arxiv.org/abs/2406.08771v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Can Synthetic Audio From Generative Foundation Models Assist Audio\n  Recognition and Speech Modeling?", "abstract": "Recent advances in foundation models have enabled audio-generative models\nthat produce high-fidelity sounds associated with music, events, and human\nactions. Despite the success achieved in modern audio-generative models, the\nconventional approach to assessing the quality of the audio generation relies\nheavily on distance metrics like Frechet Audio Distance. In contrast, we aim to\nevaluate the quality of audio generation by examining the effectiveness of\nusing them as training data. Specifically, we conduct studies to explore the\nuse of synthetic audio for audio recognition. Moreover, we investigate whether\nsynthetic audio can serve as a resource for data augmentation in speech-related\nmodeling. Our comprehensive experiments demonstrate the potential of using\nsynthetic audio for audio recognition and speech-related modeling. Our code is\navailable at https://github.com/usc-sail/SynthAudio.", "published": "2024-06-13 04:33:05", "link": "http://arxiv.org/abs/2406.08800v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Are We There Yet? A Brief Survey of Music Emotion Prediction Datasets,\n  Models and Outstanding Challenges", "abstract": "Deep learning models for music have advanced drastically in recent years, but\nhow good are machine learning models at capturing emotion, and what challenges\nare researchers facing? In this paper, we provide a comprehensive overview of\nthe available music-emotion datasets and discuss evaluation standards as well\nas competitions in the field. We also offer a brief overview of various types\nof music emotion prediction models that have been built over the years,\nproviding insights into the diverse approaches within the field. Through this\nexamination, we highlight the challenges that persist in accurately capturing\nemotion in music, including issues related to dataset quality, annotation\nconsistency, and model generalization. Additionally, we explore the impact of\ndifferent modalities, such as audio, MIDI, and physiological signals, on the\neffectiveness of emotion prediction models. Recognizing the dynamic nature of\nthis field, we have complemented our findings with an accompanying GitHub\nrepository. This repository contains a comprehensive list of music emotion\ndatasets and recent predictive models.", "published": "2024-06-13 05:00:27", "link": "http://arxiv.org/abs/2406.08809v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Interpretable Temporal Class Activation Representation for Audio\n  Spoofing Detection", "abstract": "Explaining the decisions made by audio spoofing detection models is crucial\nfor fostering trust in detection outcomes. However, current research on the\ninterpretability of detection models is limited to applying XAI tools to\npost-trained models. In this paper, we utilize the wav2vec 2.0 model and\nattentive utterance-level features to integrate interpretability directly into\nthe model's architecture, thereby enhancing transparency of the decision-making\nprocess. Specifically, we propose a class activation representation to localize\nthe discriminative frames contributing to detection. Furthermore, we\ndemonstrate that multi-label training based on spoofing types, rather than\nbinary labels as bonafide and spoofed, enables the model to learn distinct\ncharacteristics of different attacks, significantly improving detection\nperformance. Our model achieves state-of-the-art results, with an EER of 0.51%\nand a min t-DCF of 0.0165 on the ASVspoof2019-LA set.", "published": "2024-06-13 05:36:01", "link": "http://arxiv.org/abs/2406.08825v2", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On Improving Error Resilience of Neural End-to-End Speech Coders", "abstract": "Error resilient tools like Packet Loss Concealment (PLC) and Forward Error\nCorrection (FEC) are essential to maintain a reliable speech communication for\napplications like Voice over Internet Protocol (VoIP), where packets are\nfrequently delayed and lost. In recent times, end-to-end neural speech codecs\nhave seen a significant rise, due to their ability to transmit speech signal at\nlow bitrates but few considerations were made about their error resilience in a\nreal system. Recently introduced Neural End-to-End Speech Codec (NESC) can\nreproduce high quality natural speech at low bitrates. We extend its robustness\nto packet losses by adding a low complexity network to predict the codebook\nindices in latent space. Furthermore, we propose a method to add an in-band FEC\nat an additional bitrate of 0.8 kbps. Both subjective and objective assessment\nindicate the effectiveness of proposed methods, and demonstrate that coupling\nPLC and FEC provide significant robustness against packet losses.", "published": "2024-06-13 07:53:54", "link": "http://arxiv.org/abs/2406.08900v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "AdaPTwin: Low-Cost Adaptive Compression of Product Twins in Transformers", "abstract": "While large transformer-based models have exhibited remarkable performance in\nspeaker-independent speech recognition, their large size and computational\nrequirements make them expensive or impractical to use in resource-constrained\nsettings. In this work, we propose a low-rank adaptive compression technique\ncalled AdaPTwin that jointly compresses product-dependent pairs of weight\nmatrices in the transformer attention layer. Our approach can prioritize the\ncompressed model's performance on a specific speaker while maintaining\ngeneralizability to new speakers and acoustic conditions. Notably, our\ntechnique requires only 8 hours of speech data for fine-tuning, which can be\naccomplished in under 20 minutes, making it highly cost-effective compared to\nother compression methods. We demonstrate the efficacy of our approach by\ncompressing the Whisper and Distil-Whisper models by up to 45% while incurring\nless than a 2% increase in word error rate.", "published": "2024-06-13 07:58:15", "link": "http://arxiv.org/abs/2406.08904v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Transcription-Free Fine-Tuning of Speech Separation Models for Noisy and\n  Reverberant Multi-Speaker Automatic Speech Recognition", "abstract": "One solution to automatic speech recognition (ASR) of overlapping speakers is\nto separate speech and then perform ASR on the separated signals. Commonly, the\nseparator produces artefacts which often degrade ASR performance. Addressing\nthis issue typically requires reference transcriptions to jointly train the\nseparation and ASR networks. This is often not viable for training on\nreal-world in-domain audio where reference transcript information is not always\navailable. This paper proposes a transcription-free method for joint training\nusing only audio signals. The proposed method uses embedding differences of\npre-trained ASR encoders as a loss with a proposed modification to permutation\ninvariant training (PIT) called guided PIT (GPIT). The method achieves a 6.4%\nimprovement in word error rate (WER) measures over a signal-level loss and also\nshows enhancement improvements in perceptual measures such as short-time\nobjective intelligibility (STOI).", "published": "2024-06-13 08:20:58", "link": "http://arxiv.org/abs/2406.08914v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AV-GS: Learning Material and Geometry Aware Priors for Novel View\n  Acoustic Synthesis", "abstract": "Novel view acoustic synthesis (NVAS) aims to render binaural audio at any\ntarget viewpoint, given a mono audio emitted by a sound source at a 3D scene.\nExisting methods have proposed NeRF-based implicit models to exploit visual\ncues as a condition for synthesizing binaural audio. However, in addition to\nlow efficiency originating from heavy NeRF rendering, these methods all have a\nlimited ability of characterizing the entire scene environment such as room\ngeometry, material properties, and the spatial relation between the listener\nand sound source. To address these issues, we propose a novel Audio-Visual\nGaussian Splatting (AV-GS) model. To obtain a material-aware and geometry-aware\ncondition for audio synthesis, we learn an explicit point-based scene\nrepresentation with an audio-guidance parameter on locally initialized Gaussian\npoints, taking into account the space relation from the listener and sound\nsource. To make the visual scene model audio adaptive, we propose a point\ndensification and pruning strategy to optimally distribute the Gaussian points,\nwith the per-point contribution in sound propagation (e.g., more points needed\nfor texture-less wall surfaces as they affect sound path diversion). Extensive\nexperiments validate the superiority of our AV-GS over existing alternatives on\nthe real-world RWAS and simulation-based SoundSpaces datasets.", "published": "2024-06-13 08:34:12", "link": "http://arxiv.org/abs/2406.08920v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Tool Wear Prediction in CNC Turning Operations using Ultrasonic\n  Microphone Arrays and CNNs", "abstract": "This paper introduces a novel method for predicting tool wear in CNC turning\noperations, combining ultrasonic microphone arrays and convolutional neural\nnetworks (CNNs). High-frequency acoustic emissions between 0 kHz and 60 kHz are\nenhanced using beamforming techniques to improve the signal- to-noise ratio.\nThe processed acoustic data is then analyzed by a CNN, which predicts the\nRemaining Useful Life (RUL) of cutting tools. Trained on data from 350\nworkpieces machined with a single carbide insert, the model can accurately\npredict the RUL of the carbide insert. Our results demonstrate the potential\ngained by integrating advanced ultrasonic sensors with deep learning for\naccurate predictive maintenance tasks in CNC machining.", "published": "2024-06-13 09:36:13", "link": "http://arxiv.org/abs/2406.08957v1", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric\n  Videos", "abstract": "Generating realistic audio for human actions is important for many\napplications, such as creating sound effects for films or virtual reality\ngames. Existing approaches implicitly assume total correspondence between the\nvideo and audio during training, yet many sounds happen off-screen and have\nweak to no correspondence with the visuals -- resulting in uncontrolled ambient\nsounds or hallucinations at test time. We propose a novel ambient-aware audio\ngeneration model, AV-LDM. We devise a novel audio-conditioning mechanism to\nlearn to disentangle foreground action sounds from the ambient background\nsounds in in-the-wild training videos. Given a novel silent video, our model\nuses retrieval-augmented generation to create audio that matches the visual\ncontent both semantically and temporally. We train and evaluate our model on\ntwo in-the-wild egocentric video datasets, Ego4D and EPIC-KITCHENS, and we\nintroduce Ego4D-Sounds -- 1.2M curated clips with action-audio correspondence.\nOur model outperforms an array of existing methods, allows controllable\ngeneration of the ambient sound, and even shows promise for generalizing to\ncomputer graphics game clips. Overall, our approach is the first to focus\nvideo-to-audio generation faithfully on the observed visual content despite\ntraining from uncurated clips with natural background sounds.", "published": "2024-06-13 16:10:19", "link": "http://arxiv.org/abs/2406.09272v3", "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Towards Multilingual Audio-Visual Question Answering", "abstract": "In this paper, we work towards extending Audio-Visual Question Answering\n(AVQA) to multilingual settings. Existing AVQA research has predominantly\nrevolved around English and replicating it for addressing AVQA in other\nlanguages requires a substantial allocation of resources. As a scalable\nsolution, we leverage machine translation and present two multilingual AVQA\ndatasets for eight languages created from existing benchmark AVQA datasets.\nThis prevents extra human annotation efforts of collecting questions and\nanswers manually. To this end, we propose, MERA framework, by leveraging\nstate-of-the-art (SOTA) video, audio, and textual foundation models for AVQA in\nmultiple languages. We introduce a suite of models namely MERA-L, MERA-C,\nMERA-T with varied model architectures to benchmark the proposed datasets. We\nbelieve our work will open new research directions and act as a reference\nbenchmark for future works in multilingual AVQA.", "published": "2024-06-13 14:18:56", "link": "http://arxiv.org/abs/2406.09156v1", "categories": ["cs.LG", "cs.CV", "cs.MM", "cs.SD", "eess.AS", "68T45"], "primary_category": "cs.LG"}
{"title": "PianoMotion10M: Dataset and Benchmark for Hand Motion Generation in\n  Piano Performance", "abstract": "Recently, artificial intelligence techniques for education have been received\nincreasing attentions, while it still remains an open problem to design the\neffective music instrument instructing systems. Although key presses can be\ndirectly derived from sheet music, the transitional movements among key presses\nrequire more extensive guidance in piano performance. In this work, we\nconstruct a piano-hand motion generation benchmark to guide hand movements and\nfingerings for piano playing. To this end, we collect an annotated dataset,\nPianoMotion10M, consisting of 116 hours of piano playing videos from a\nbird's-eye view with 10 million annotated hand poses. We also introduce a\npowerful baseline model that generates hand motions from piano audios through a\nposition predictor and a position-guided gesture generator. Furthermore, a\nseries of evaluation metrics are designed to assess the performance of the\nbaseline model, including motion similarity, smoothness, positional accuracy of\nleft and right hands, and overall fidelity of movement distribution. Despite\nthat piano key presses with respect to music scores or audios are already\naccessible, PianoMotion10M aims to provide guidance on piano fingering for\ninstruction purposes. The source code and dataset can be accessed at\nhttps://github.com/agnJason/PianoMotion10M.", "published": "2024-06-13 17:05:23", "link": "http://arxiv.org/abs/2406.09326v2", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
