{"title": "Words, Concepts, and the Geometry of Analogy", "abstract": "This paper presents a geometric approach to the problem of modelling the\nrelationship between words and concepts, focusing in particular on analogical\nphenomena in language and cognition. Grounded in recent theories regarding\ngeometric conceptual spaces, we begin with an analysis of existing static\ndistributional semantic models and move on to an exploration of a dynamic\napproach to using high dimensional spaces of word meaning to project subspaces\nwhere analogies can potentially be solved in an online, contextualised way. The\ncrucial element of this analysis is the positioning of statistics in a\ngeometric environment replete with opportunities for interpretation.", "published": "2016-08-04 00:36:48", "link": "http://arxiv.org/abs/1608.01403v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entailment Relations on Distributions", "abstract": "In this paper we give an overview of partial orders on the space of\nprobability distributions that carry a notion of information content and serve\nas a generalisation of the Bayesian order given in (Coecke and Martin, 2011).\nWe investigate what constraints are necessary in order to get a unique notion\nof information content. These partial orders can be used to give an ordering on\nwords in vector space models of natural language meaning relating to the\ncontexts in which words are used, which is useful for a notion of entailment\nand word disambiguation. The construction used also points towards a way to\ncreate orderings on the space of density operators which allow a more\nfine-grained study of entailment. The partial orders in this paper are directed\ncomplete and form domains in the sense of domain theory.", "published": "2016-08-04 00:37:07", "link": "http://arxiv.org/abs/1608.01405v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Solving General Arithmetic Word Problems", "abstract": "This paper presents a novel approach to automatically solving arithmetic word\nproblems. This is the first algorithmic approach that can handle arithmetic\nproblems with multiple steps and operations, without depending on additional\nannotations or predefined templates. We develop a theory for expression trees\nthat can be used to represent and evaluate the target arithmetic expressions;\nwe use it to uniquely decompose the target arithmetic problem to multiple\nclassification problems; we then compose an expression tree, combining these\nwith world knowledge through a constrained inference framework. Our classifiers\ngain from the use of {\\em quantity schemas} that supports better extraction of\nfeatures. Experimental results show that our method outperforms existing\nsystems, achieving state of the art performance on benchmark datasets of\narithmetic word problems.", "published": "2016-08-04 01:47:23", "link": "http://arxiv.org/abs/1608.01413v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Segmentation on Micro-blog Texts with External Lexicon and\n  Heterogeneous Data", "abstract": "This paper describes our system designed for the NLPCC 2016 shared task on\nword segmentation on micro-blog texts.", "published": "2016-08-04 07:37:54", "link": "http://arxiv.org/abs/1608.01448v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UsingWord Embeddings for Query Translation for Hindi to English Cross\n  Language Information Retrieval", "abstract": "Cross-Language Information Retrieval (CLIR) has become an important problem\nto solve in the recent years due to the growth of content in multiple languages\nin the Web. One of the standard methods is to use query translation from source\nto target language. In this paper, we propose an approach based on word\nembeddings, a method that captures contextual clues for a particular word in\nthe source language and gives those words as translations that occur in a\nsimilar context in the target language. Once we obtain the word embeddings of\nthe source and target language pairs, we learn a projection from source to\ntarget word embeddings, making use of a dictionary with word translation\npairs.We then propose various methods of query translation and aggregation. The\nadvantage of this approach is that it does not require the corpora to be\naligned (which is difficult to obtain for resource-scarce languages), a\ndictionary with word translation pairs is enough to train the word vectors for\ntranslation. We experiment with Forum for Information Retrieval and Evaluation\n(FIRE) 2008 and 2012 datasets for Hindi to English CLIR. The proposed word\nembedding based approach outperforms the basic dictionary based approach by 70%\nand when the word embeddings are combined with the dictionary, the hybrid\napproach beats the baseline dictionary based method by 77%. It outperforms the\nEnglish monolingual baseline by 15%, when combined with the translations\nobtained from Google Translate and Dictionary.", "published": "2016-08-04 14:44:52", "link": "http://arxiv.org/abs/1608.01561v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantum Algorithms for Compositional Natural Language Processing", "abstract": "We propose a new application of quantum computing to the field of natural\nlanguage processing. Ongoing work in this field attempts to incorporate\ngrammatical structure into algorithms that compute meaning. In (Coecke,\nSadrzadeh and Clark, 2010), the authors introduce such a model (the CSC model)\nbased on tensor product composition. While this algorithm has many advantages,\nits implementation is hampered by the large classical computational resources\nthat it requires. In this work we show how computational shortcomings of the\nCSC approach could be resolved using quantum computation (possibly in addition\nto existing techniques for dimension reduction). We address the value of\nquantum RAM (Giovannetti,2008) for this model and extend an algorithm from\nWiebe, Braun and Lloyd (2012) into a quantum algorithm to categorize sentences\nin CSC. Our new algorithm demonstrates a quadratic speedup over classical\nmethods under certain conditions.", "published": "2016-08-04 00:37:16", "link": "http://arxiv.org/abs/1608.01406v1", "categories": ["cs.CL", "quant-ph"], "primary_category": "cs.CL"}
{"title": "Dual Density Operators and Natural Language Meaning", "abstract": "Density operators allow for representing ambiguity about a vector\nrepresentation, both in quantum theory and in distributional natural language\nmeaning. Formally equivalently, they allow for discarding part of the\ndescription of a composite system, where we consider the discarded part to be\nthe context. We introduce dual density operators, which allow for two\nindependent notions of context. We demonstrate the use of dual density\noperators within a grammatical-compositional distributional framework for\nnatural language meaning. We show that dual density operators can be used to\nsimultaneously represent: (i) ambiguity about word meanings (e.g. queen as a\nperson vs. queen as a band), and (ii) lexical entailment (e.g. tiger ->\nmammal). We provide a proof-of-concept example.", "published": "2016-08-04 00:36:12", "link": "http://arxiv.org/abs/1608.01401v1", "categories": ["cs.CL", "cs.LO", "quant-ph"], "primary_category": "cs.CL"}
{"title": "Interacting Conceptual Spaces", "abstract": "We propose applying the categorical compositional scheme of [6] to conceptual\nspace models of cognition. In order to do this we introduce the category of\nconvex relations as a new setting for categorical compositional semantics,\nemphasizing the convex structure important to conceptual space applications. We\nshow how conceptual spaces for composite types such as adjectives and verbs can\nbe constructed. We illustrate this new model on detailed examples.", "published": "2016-08-04 00:36:21", "link": "http://arxiv.org/abs/1608.01402v1", "categories": ["cs.AI", "cs.CL", "cs.LO"], "primary_category": "cs.AI"}
{"title": "Quantifier Scope in Categorical Compositional Distributional Semantics", "abstract": "In previous work with J. Hedges, we formalised a generalised quantifiers\ntheory of natural language in categorical compositional distributional\nsemantics with the help of bialgebras. In this paper, we show how quantifier\nscope ambiguity can be represented in that setting and how this representation\ncan be generalised to branching quantifiers.", "published": "2016-08-04 00:36:57", "link": "http://arxiv.org/abs/1608.01404v1", "categories": ["cs.CL", "cs.AI", "cs.LO"], "primary_category": "cs.CL"}
{"title": "An improved uncertainty decoding scheme with weighted samples for\n  DNN-HMM hybrid systems", "abstract": "In this paper, we advance a recently-proposed uncertainty decoding scheme for\nDNN-HMM (deep neural network - hidden Markov model) hybrid systems. This\nnumerical sampling concept averages DNN outputs produced by a finite set of\nfeature samples (drawn from a probabilistic distortion model) to approximate\nthe posterior likelihoods of the context-dependent HMM states. As main\ninnovation, we propose a weighted DNN-output averaging based on a minimum\nclassification error criterion and apply it to a probabilistic distortion model\nfor spatial diffuseness features. The experimental evaluation is performed on\nthe 8-channel REVERB Challenge task using a DNN-HMM hybrid system with\nmultichannel front-end signal enhancement. We show that the recognition\naccuracy of the DNN-HMM hybrid system improves by incorporating uncertainty\ndecoding based on random sampling and that the proposed weighted DNN-output\naveraging further reduces the word error rate scores.", "published": "2016-08-04 10:11:24", "link": "http://arxiv.org/abs/1609.02082v1", "categories": ["cs.LG", "cs.CL", "cs.SD"], "primary_category": "cs.LG"}
