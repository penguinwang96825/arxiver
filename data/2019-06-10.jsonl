{"title": "Out-of-Vocabulary Embedding Imputation with Grounded Language\n  Information by Graph Convolutional Networks", "abstract": "Due to the ubiquitous use of embeddings as input representations for a wide\nrange of natural language tasks, imputation of embeddings for rare and unseen\nwords is a critical problem in language processing. Embedding imputation\ninvolves learning representations for rare or unseen words during the training\nof an embedding model, often in a post-hoc manner. In this paper, we propose an\napproach for embedding imputation which uses grounded information in the form\nof a knowledge graph. This is in contrast to existing approaches which\ntypically make use of vector space properties or subword information. We\npropose an online method to construct a graph from grounded information and\ndesign an algorithm to map from the resulting graphical structure to the space\nof the pre-trained embeddings. Finally, we evaluate our approach on a range of\nrare and unseen word tasks across various domains and show that our model can\nlearn better representations. For example, on the Card-660 task our method\nimproves Pearson's and Spearman's correlation coefficients upon the\nstate-of-the-art by 11% and 17.8% respectively using GloVe embeddings.", "published": "2019-06-10 01:10:34", "link": "http://arxiv.org/abs/1906.03753v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequence-to-Nuggets: Nested Entity Mention Detection via Anchor-Region\n  Networks", "abstract": "Sequential labeling-based NER approaches restrict each word belonging to at\nmost one entity mention, which will face a serious problem when recognizing\nnested entity mentions. In this paper, we propose to resolve this problem by\nmodeling and leveraging the head-driven phrase structures of entity mentions,\ni.e., although a mention can nest other mentions, they will not share the same\nhead word. Specifically, we propose Anchor-Region Networks (ARNs), a\nsequence-to-nuggets architecture for nested mention detection. ARNs first\nidentify anchor words (i.e., possible head words) of all mentions, and then\nrecognize the mention boundaries for each anchor word by exploiting regular\nphrase structures. Furthermore, we also design Bag Loss, an objective function\nwhich can train ARNs in an end-to-end manner without using any anchor word\nannotation. Experiments show that ARNs achieve the state-of-the-art performance\non three standard nested entity mention detection benchmarks.", "published": "2019-06-10 03:15:00", "link": "http://arxiv.org/abs/1906.03783v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generalized Data Augmentation for Low-Resource Translation", "abstract": "Translation to or from low-resource languages LRLs poses challenges for\nmachine translation in terms of both adequacy and fluency. Data augmentation\nutilizing large amounts of monolingual data is regarded as an effective way to\nalleviate these problems. In this paper, we propose a general framework for\ndata augmentation in low-resource machine translation that not only uses\ntarget-side monolingual data, but also pivots through a related high-resource\nlanguage HRL. Specifically, we experiment with a two-step pivoting method to\nconvert high-resource data to the LRL, making use of available resources to\nbetter approximate the true data distribution of the LRL. First, we inject LRL\nwords into HRL sentences through an induced bilingual dictionary. Second, we\nfurther edit these modified sentences using a modified unsupervised machine\ntranslation framework. Extensive experiments on four low-resource datasets show\nthat under extreme low-resource settings, our data augmentation techniques\nimprove translation quality by up to~1.5 to~8 BLEU points compared to\nsupervised back-translation baselines", "published": "2019-06-10 03:28:57", "link": "http://arxiv.org/abs/1906.03785v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open-Domain Targeted Sentiment Analysis via Span-Based Extraction and\n  Classification", "abstract": "Open-domain targeted sentiment analysis aims to detect opinion targets along\nwith their sentiment polarities from a sentence. Prior work typically\nformulates this task as a sequence tagging problem. However, such formulation\nsuffers from problems such as huge search space and sentiment inconsistency. To\naddress these problems, we propose a span-based extract-then-classify\nframework, where multiple opinion targets are directly extracted from the\nsentence under the supervision of target span boundaries, and corresponding\npolarities are then classified using their span representations. We further\ninvestigate three approaches under this framework, namely the pipeline, joint,\nand collapsed models. Experiments on three benchmark datasets show that our\napproach consistently outperforms the sequence tagging baseline. Moreover, we\nfind that the pipeline model achieves the best performance compared with the\nother two models.", "published": "2019-06-10 07:22:05", "link": "http://arxiv.org/abs/1906.03820v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic-Aware Neural Keyphrase Generation for Social Media Language", "abstract": "A huge volume of user-generated content is daily produced on social media. To\nfacilitate automatic language understanding, we study keyphrase prediction,\ndistilling salient information from massive posts. While most existing methods\nextract words from source posts to form keyphrases, we propose a\nsequence-to-sequence (seq2seq) based neural keyphrase generation framework,\nenabling absent keyphrases to be created. Moreover, our model, being\ntopic-aware, allows joint modeling of corpus-level latent topic\nrepresentations, which helps alleviate the data sparsity that widely exhibited\nin social media language. Experiments on three datasets collected from English\nand Chinese social media platforms show that our model significantly\noutperforms both extraction and generation models that do not exploit latent\ntopics. Further discussions show that our model learns meaningful topics, which\ninterprets its superiority in social media keyphrase generation.", "published": "2019-06-10 10:40:39", "link": "http://arxiv.org/abs/1906.03889v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Logical Inference System for Visual-Textual Entailment", "abstract": "A large amount of research about multimodal inference across text and vision\nhas been recently developed to obtain visually grounded word and sentence\nrepresentations. In this paper, we use logic-based representations as unified\nmeaning representations for texts and images and present an unsupervised\nmultimodal logical inference system that can effectively prove entailment\nrelations between them. We show that by combining semantic parsing and theorem\nproving, the system can handle semantically complex sentences for\nvisual-textual inference.", "published": "2019-06-10 12:57:56", "link": "http://arxiv.org/abs/1906.03952v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The University of Helsinki submissions to the WMT19 news translation\n  task", "abstract": "In this paper, we present the University of Helsinki submissions to the WMT\n2019 shared task on news translation in three language pairs: English-German,\nEnglish-Finnish and Finnish-English. This year, we focused first on cleaning\nand filtering the training data using multiple data-filtering approaches,\nresulting in much smaller and cleaner training sets. For English-German, we\ntrained both sentence-level transformer models and compared different\ndocument-level translation approaches. For Finnish-English and English-Finnish\nwe focused on different segmentation approaches, and we also included a\nrule-based system for English-Finnish.", "published": "2019-06-10 14:48:10", "link": "http://arxiv.org/abs/1906.04040v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CAiRE_HKUST at SemEval-2019 Task 3: Hierarchical Attention for Dialogue\n  Emotion Classification", "abstract": "Detecting emotion from dialogue is a challenge that has not yet been\nextensively surveyed. One could consider the emotion of each dialogue turn to\nbe independent, but in this paper, we introduce a hierarchical approach to\nclassify emotion, hypothesizing that the current emotional state depends on\nprevious latent emotions. We benchmark several feature-based classifiers using\npre-trained word and emotion embeddings, state-of-the-art end-to-end neural\nnetwork models, and Gaussian processes for automatic hyper-parameter search. In\nour experiments, hierarchical architectures consistently give significant\nimprovements, and our best model achieves a 76.77% F1-score on the test set.", "published": "2019-06-10 14:48:54", "link": "http://arxiv.org/abs/1906.04041v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Representation in Neural Language Models: Suppression and\n  Recovery of Expectations", "abstract": "Deep learning sequence models have led to a marked increase in performance\nfor a range of Natural Language Processing tasks, but it remains an open\nquestion whether they are able to induce proper hierarchical generalizations\nfor representing natural language from linear input alone. Work using\nartificial languages as training input has shown that LSTMs are capable of\ninducing the stack-like data structures required to represent context-free and\ncertain mildly context-sensitive languages---formal language classes which\ncorrespond in theory to the hierarchical structures of natural language. Here\nwe present a suite of experiments probing whether neural language models\ntrained on linguistic data induce these stack-like data structures and deploy\nthem while incrementally predicting words. We study two natural language\nphenomena: center embedding sentences and syntactic island constraints on the\nfiller--gap dependency. In order to properly predict words in these structures,\na model must be able to temporarily suppress certain expectations and then\nrecover those expectations later, essentially pushing and popping these\nexpectations on a stack. Our results provide evidence that models can\nsuccessfully suppress and recover expectations in many cases, but do not fully\nrecover their previous grammatical state.", "published": "2019-06-10 15:20:32", "link": "http://arxiv.org/abs/1906.04068v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Char-RNN for Word Stress Detection in East Slavic Languages", "abstract": "We explore how well a sequence labeling approach, namely, recurrent neural\nnetwork, is suited for the task of resource-poor and POS tagging free word\nstress detection in the Russian, Ukranian, Belarusian languages. We present new\ndatasets, annotated with the word stress, for the three languages and compare\nseveral RNN models trained on three languages and explore possible applications\nof the transfer learning for the task. We show that it is possible to train a\nmodel in a cross-lingual setting and that using additional languages improves\nthe quality of the results.", "published": "2019-06-10 15:53:20", "link": "http://arxiv.org/abs/1906.04082v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AGRR-2019: A Corpus for Gapping Resolution in Russian", "abstract": "This paper provides a comprehensive overview of the gapping dataset for\nRussian that consists of 7.5k sentences with gapping (as well as 15k relevant\nnegative sentences) and comprises data from various genres: news, fiction,\nsocial media and technical texts. The dataset was prepared for the Automatic\nGapping Resolution Shared Task for Russian (AGRR-2019) - a competition aimed at\nstimulating the development of NLP tools and methods for processing of\nellipsis.\n  In this paper, we pay special attention to the gapping resolution methods\nthat were introduced within the shared task as well as an alternative test set\nthat illustrates that our corpus is a diverse and representative subset of\nRussian language gapping sufficient for effective utilization of machine\nlearning techniques.", "published": "2019-06-10 16:20:48", "link": "http://arxiv.org/abs/1906.04099v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Keyphrase Generation via Reinforcement Learning with Adaptive\n  Rewards", "abstract": "Generating keyphrases that summarize the main points of a document is a\nfundamental task in natural language processing. Although existing generative\nmodels are capable of predicting multiple keyphrases for an input document as\nwell as determining the number of keyphrases to generate, they still suffer\nfrom the problem of generating too few keyphrases. To address this problem, we\npropose a reinforcement learning (RL) approach for keyphrase generation, with\nan adaptive reward function that encourages a model to generate both sufficient\nand accurate keyphrases. Furthermore, we introduce a new evaluation method that\nincorporates name variations of the ground-truth keyphrases using the Wikipedia\nknowledge base. Thus, our evaluation method can more robustly evaluate the\nquality of predicted keyphrases. Extensive experiments on five real-world\ndatasets of different scales demonstrate that our RL approach consistently and\nsignificantly improves the performance of the state-of-the-art generative\nmodels with both conventional and new evaluation methods.", "published": "2019-06-10 16:27:34", "link": "http://arxiv.org/abs/1906.04106v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Noisiness to Recognize Named Entities using Multitask Neural\n  Networks on Social Media", "abstract": "Recognizing named entities in a document is a key task in many NLP\napplications. Although current state-of-the-art approaches to this task reach a\nhigh performance on clean text (e.g. newswire genres), those algorithms\ndramatically degrade when they are moved to noisy environments such as social\nmedia domains. We present two systems that address the challenges of processing\nsocial media data using character-level phonetics and phonology, word\nembeddings, and Part-of-Speech tags as features. The first model is a multitask\nend-to-end Bidirectional Long Short-Term Memory (BLSTM)-Conditional Random\nField (CRF) network whose output layer contains two CRF classifiers. The second\nmodel uses a multitask BLSTM network as feature extractor that transfers the\nlearning to a CRF classifier for the final prediction. Our systems outperform\nthe current F1 scores of the state of the art on the Workshop on Noisy\nUser-generated Text 2017 dataset by 2.45% and 3.69%, establishing a more\nsuitable approach for social media environments.", "published": "2019-06-10 17:08:29", "link": "http://arxiv.org/abs/1906.04129v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multi-task Approach for Named Entity Recognition in Social Media Data", "abstract": "Named Entity Recognition for social media data is challenging because of its\ninherent noisiness. In addition to improper grammatical structures, it contains\nspelling inconsistencies and numerous informal abbreviations. We propose a\nnovel multi-task approach by employing a more general secondary task of Named\nEntity (NE) segmentation together with the primary task of fine-grained NE\ncategorization. The multi-task neural network architecture learns higher order\nfeature representations from word and character sequences along with basic\nPart-of-Speech tags and gazetteer information. This neural network acts as a\nfeature extractor to feed a Conditional Random Fields classifier. We were able\nto obtain the first position in the 3rd Workshop on Noisy User-generated Text\n(WNUT-2017) with a 41.86% entity F1-score and a 40.24% surface F1-score.", "published": "2019-06-10 17:12:30", "link": "http://arxiv.org/abs/1906.04135v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Named Entity Recognition on Code-Switched Data: Overview of the CALCS\n  2018 Shared Task", "abstract": "In the third shared task of the Computational Approaches to Linguistic\nCode-Switching (CALCS) workshop, we focus on Named Entity Recognition (NER) on\ncode-switched social-media data. We divide the shared task into two\ncompetitions based on the English-Spanish (ENG-SPA) and Modern Standard\nArabic-Egyptian (MSA-EGY) language pairs. We use Twitter data and 9 entity\ntypes to establish a new dataset for code-switched NER benchmarks. In addition\nto the CS phenomenon, the diversity of the entities and the social media\nchallenges make the task considerably hard to process. As a result, the best\nscores of the competitions are 63.76% and 71.61% for ENG-SPA and MSA-EGY,\nrespectively. We present the scores of 9 participants and discuss the most\ncommon challenges among submissions.", "published": "2019-06-10 17:17:45", "link": "http://arxiv.org/abs/1906.04138v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Label-Agnostic Sequence Labeling by Copying Nearest Neighbors", "abstract": "Retrieve-and-edit based approaches to structured prediction, where structures\nassociated with retrieved neighbors are edited to form new structures, have\nrecently attracted increased interest. However, much recent work merely\nconditions on retrieved structures (e.g., in a sequence-to-sequence framework),\nrather than explicitly manipulating them. We show we can perform accurate\nsequence labeling by explicitly (and only) copying labels from retrieved\nneighbors. Moreover, because this copying is label-agnostic, we can achieve\nimpressive performance when transferring to new sequence-labeling tasks without\nretraining. We additionally consider a dynamic programming approach to sequence\nlabeling in the presence of retrieved neighbors, which allows for controlling\nthe number of distinct (copied) segments used to form a prediction, and leads\nto both more interpretable and accurate predictions.", "published": "2019-06-10 18:53:18", "link": "http://arxiv.org/abs/1906.04225v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Psycholinguistics meets Continual Learning: Measuring Catastrophic\n  Forgetting in Visual Question Answering", "abstract": "We study the issue of catastrophic forgetting in the context of neural\nmultimodal approaches to Visual Question Answering (VQA). Motivated by evidence\nfrom psycholinguistics, we devise a set of linguistically-informed VQA tasks,\nwhich differ by the types of questions involved (Wh-questions and polar\nquestions). We test what impact task difficulty has on continual learning, and\nwhether the order in which a child acquires question types facilitates\ncomputational models. Our results show that dramatic forgetting is at play and\nthat task difficulty and order matter. Two well-known current continual\nlearning methods mitigate the problem only to a limiting degree.", "published": "2019-06-10 19:00:59", "link": "http://arxiv.org/abs/1906.04229v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Curriculum Learning for Turn-level Spoken Language\n  Understanding with Weak Supervision", "abstract": "We propose a learning approach for turn-level spoken language understanding,\nwhich facilitates a user to speak one or more utterances compositionally in a\nturn for completing a task (e.g., voice ordering). A typical pipelined approach\nfor these understanding tasks requires non-trivial annotation effort for\ndeveloping its multiple components. Also, the pipeline is difficult to port to\na new domain or scale up. To address these problems, we propose an end-to-end\nstatistical model with weak supervision. We employ randomized beam search with\nmemory augmentation (RBSMA) to solve complicated problems for which long\npromising trajectories are usually difficult to explore. Furthermore,\nconsidering the diversity of problem complexity, we explore automated\ncurriculum learning (CL) for weak supervision to accelerate exploration and\nlearning. We evaluate the proposed approach on real-world user logs of a\ncommercial voice ordering system. Results demonstrate that when trained on a\nsmall number of end-to-end annotated sessions collected with low cost, our\nmodel performs comparably to the deployed pipelined system, saving the\ndevelopment labor over an order of magnitude. The RBSMA algorithm improves the\ntest set accuracy by 7.8% relative compared to the standard beam search.\nAutomated CL leads to better generalization and further improves the test set\naccuracy by 5% relative.", "published": "2019-06-10 21:54:33", "link": "http://arxiv.org/abs/1906.04291v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent\n  Summarization", "abstract": "Most existing text summarization datasets are compiled from the news domain,\nwhere summaries have a flattened discourse structure. In such datasets,\nsummary-worthy content often appears in the beginning of input articles.\nMoreover, large segments from input articles are present verbatim in their\nrespective summaries. These issues impede the learning and evaluation of\nsystems that can understand an article's global content structure as well as\nproduce abstractive summaries with high compression ratio. In this work, we\npresent a novel dataset, BIGPATENT, consisting of 1.3 million records of U.S.\npatent documents along with human written abstractive summaries. Compared to\nexisting summarization datasets, BIGPATENT has the following properties: i)\nsummaries contain a richer discourse structure with more recurring entities,\nii) salient content is evenly distributed in the input, and iii) lesser and\nshorter extractive fragments are present in the summaries. Finally, we train\nand evaluate baselines and popular learning models on BIGPATENT to shed light\non new challenges and motivate future directions for summarization research.", "published": "2019-06-10 00:24:26", "link": "http://arxiv.org/abs/1906.03741v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey on Neural Machine Reading Comprehension", "abstract": "Enabling a machine to read and comprehend the natural language documents so\nthat it can answer some questions remains an elusive challenge. In recent\nyears, the popularity of deep learning and the establishment of large-scale\ndatasets have both promoted the prosperity of Machine Reading Comprehension.\nThis paper aims to present how to utilize the Neural Network to build a Reader\nand introduce some classic models, analyze what improvements they make.\nFurther, we also point out the defects of existing models and future research\ndirections", "published": "2019-06-10 07:49:14", "link": "http://arxiv.org/abs/1906.03824v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatically Identifying Complaints in Social Media", "abstract": "Complaining is a basic speech act regularly used in human and computer\nmediated communication to express a negative mismatch between reality and\nexpectations in a particular situation. Automatically identifying complaints in\nsocial media is of utmost importance for organizations or brands to improve the\ncustomer experience or in developing dialogue systems for handling and\nresponding to complaints. In this paper, we introduce the first systematic\nanalysis of complaints in computational linguistics. We collect a new annotated\ndata set of written complaints expressed in English on Twitter.\\footnote{Data\nand code is available here:\n\\url{https://github.com/danielpreotiuc/complaints-social-media}} We present an\nextensive linguistic analysis of complaining as a speech act in social media\nand train strong feature-based and neural models of complaints across nine\ndomains achieving a predictive performance of up to 79 F1 using distant\nsupervision.", "published": "2019-06-10 10:44:23", "link": "http://arxiv.org/abs/1906.03890v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Detecting Everyday Scenarios in Narrative Texts", "abstract": "Script knowledge consists of detailed information on everyday activities.\nSuch information is often taken for granted in text and needs to be inferred by\nreaders. Therefore, script knowledge is a central component to language\ncomprehension. Previous work on representing scripts is mostly based on\nextensive manual work or limited to scenarios that can be found with sufficient\nredundancy in large corpora. We introduce the task of scenario detection, in\nwhich we identify references to scripts. In this task, we address a wide range\nof different scripts (200 scenarios) and we attempt to identify all references\nto them in a collection of narrative texts. We present a first benchmark data\nset and a baseline model that tackles scenario detection using techniques from\ntopic segmentation and text classification.", "published": "2019-06-10 16:23:07", "link": "http://arxiv.org/abs/1906.04102v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Estimating Causal Effects of Tone in Online Debates", "abstract": "Statistical methods applied to social media posts shed light on the dynamics\nof online dialogue. For example, users' wording choices predict their\npersuasiveness and users adopt the language patterns of other dialogue\nparticipants. In this paper, we estimate the causal effect of reply tones in\ndebates on linguistic and sentiment changes in subsequent responses. The\nchallenge for this estimation is that a reply's tone and subsequent responses\nare confounded by the users' ideologies on the debate topic and their emotions.\nTo overcome this challenge, we learn representations of ideology using\ngenerative models of text. We study debates from 4Forums and compare annotated\ntones of replying such as emotional versus factual, or reasonable versus\nattacking. We show that our latent confounder representation reduces bias in\nATE estimation. Our results suggest that factual and asserting tones affect\ndialogue and provide a methodology for estimating causal effects from text.", "published": "2019-06-10 16:39:16", "link": "http://arxiv.org/abs/1906.04177v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Identifying Visible Actions in Lifestyle Vlogs", "abstract": "We consider the task of identifying human actions visible in online videos.\nWe focus on the widely spread genre of lifestyle vlogs, which consist of videos\nof people performing actions while verbally describing them. Our goal is to\nidentify if actions mentioned in the speech description of a video are visually\npresent. We construct a dataset with crowdsourced manual annotations of visible\nactions, and introduce a multimodal algorithm that leverages information\nderived from visual and linguistic clues to automatically infer which actions\nare visible in a video. We demonstrate that our multimodal algorithm\noutperforms algorithms based only on one modality at a time.", "published": "2019-06-10 19:11:01", "link": "http://arxiv.org/abs/1906.04236v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Deep Two-path Semi-supervised Learning for Fake News Detection", "abstract": "News in social media such as Twitter has been generated in high volume and\nspeed. However, very few of them can be labeled (as fake or true news) in a\nshort time. In order to achieve timely detection of fake news in social media,\na novel deep two-path semi-supervised learning model is proposed, where one\npath is for supervised learning and the other is for unsupervised learning.\nThese two paths implemented with convolutional neural networks are jointly\noptimized to enhance detection performance. In addition, we build a shared\nconvolutional neural networks between these two paths to share the low level\nfeatures. Experimental results using Twitter datasets show that the proposed\nmodel can recognize fake news effectively with very few labeled data.", "published": "2019-06-10 21:19:41", "link": "http://arxiv.org/abs/1906.05659v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Neural Language Modeling via Adversarial Training", "abstract": "Recently, substantial progress has been made in language modeling by using\ndeep neural networks. However, in practice, large scale neural language models\nhave been shown to be prone to overfitting. In this paper, we present a simple\nyet highly effective adversarial training mechanism for regularizing neural\nlanguage models. The idea is to introduce adversarial noise to the output\nembedding layer while training the models. We show that the optimal adversarial\nnoise yields a simple closed-form solution, thus allowing us to develop a\nsimple and time efficient algorithm. Theoretically, we show that our\nadversarial mechanism effectively encourages the diversity of the embedding\nvectors, helping to increase the robustness of models. Empirically, we show\nthat our method improves on the single model state-of-the-art results for\nlanguage modeling on Penn Treebank (PTB) and Wikitext-2, achieving test\nperplexity scores of 46.01 and 38.07, respectively. When applied to machine\ntranslation, our method improves over various transformer-based translation\nbaselines in BLEU scores on the WMT14 English-German and IWSLT14 German-English\ntasks.", "published": "2019-06-10 05:55:08", "link": "http://arxiv.org/abs/1906.03805v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Learning to combine Grammatical Error Corrections", "abstract": "The field of Grammatical Error Correction (GEC) has produced various systems\nto deal with focused phenomena or general text editing. We propose an automatic\nway to combine black-box systems. Our method automatically detects the strength\nof a system or the combination of several systems per error type, improving\nprecision and recall while optimizing $F$ score directly. We show consistent\nimprovement over the best standalone system in all the configurations tested.\nThis approach also outperforms average ensembling of different RNN models with\nrandom initializations.\n  In addition, we analyze the use of BERT for GEC - reporting promising results\non this end. We also present a spellchecker created for this task which\noutperforms standard spellcheckers tested on the task of spellchecking.\n  This paper describes a system submission to Building Educational Applications\n2019 Shared Task: Grammatical Error Correction.\n  Combining the output of top BEA 2019 shared task systems using our approach,\ncurrently holds the highest reported score in the open phase of the BEA 2019\nshared task, improving F0.5 by 3.7 points over the best result reported.", "published": "2019-06-10 10:57:47", "link": "http://arxiv.org/abs/1906.03897v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey of Reinforcement Learning Informed by Natural Language", "abstract": "To be successful in real-world tasks, Reinforcement Learning (RL) needs to\nexploit the compositional, relational, and hierarchical structure of the world,\nand learn to transfer it to the task at hand. Recent advances in representation\nlearning for language make it possible to build models that acquire world\nknowledge from text corpora and integrate this knowledge into downstream\ndecision making problems. We thus argue that the time is right to investigate a\ntight integration of natural language understanding into RL in particular. We\nsurvey the state of the field, including work on instruction following, text\ngames, and learning from textual domain knowledge. Finally, we call for the\ndevelopment of new environments as well as further investigation into the\npotential uses of recent Natural Language Processing (NLP) techniques for such\ntasks.", "published": "2019-06-10 12:17:45", "link": "http://arxiv.org/abs/1906.03926v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "GLTR: Statistical Detection and Visualization of Generated Text", "abstract": "The rapid improvement of language models has raised the specter of abuse of\ntext generation systems. This progress motivates the development of simple\nmethods for detecting generated text that can be used by and explained to\nnon-experts. We develop GLTR, a tool to support humans in detecting whether a\ntext was generated by a model. GLTR applies a suite of baseline statistical\nmethods that can detect generation artifacts across common sampling schemes. In\na human-subjects study, we show that the annotation scheme provided by GLTR\nimproves the human detection-rate of fake text from 54% to 72% without any\nprior training. GLTR is open-source and publicly deployed, and has already been\nwidely used to detect generated outputs", "published": "2019-06-10 14:52:41", "link": "http://arxiv.org/abs/1906.04043v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Word-level Speech Recognition with a Letter to Word Encoder", "abstract": "We propose a direct-to-word sequence model which uses a word network to learn\nword embeddings from letters. The word network can be integrated seamlessly\nwith arbitrary sequence models including Connectionist Temporal Classification\nand encoder-decoder models with attention. We show our direct-to-word model can\nachieve word error rate gains over sub-word level models for speech\nrecognition. We also show that our direct-to-word approach retains the ability\nto predict words not seen at training time without any retraining. Finally, we\ndemonstrate that a word-level model can use a larger stride than a sub-word\nlevel model while maintaining accuracy. This makes the model more efficient\nboth for training and inference.", "published": "2019-06-10 23:31:56", "link": "http://arxiv.org/abs/1906.04323v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Using generative modelling to produce varied intonation for speech\n  synthesis", "abstract": "Unlike human speakers, typical text-to-speech (TTS) systems are unable to\nproduce multiple distinct renditions of a given sentence. This has previously\nbeen addressed by adding explicit external control. In contrast, generative\nmodels are able to capture a distribution over multiple renditions and thus\nproduce varied renditions using sampling. Typical neural TTS models learn the\naverage of the data because they minimise mean squared error. In the context of\nprosody, taking the average produces flatter, more boring speech: an \"average\nprosody\". A generative model that can synthesise multiple prosodies will, by\ndesign, not model average prosody. We use variational autoencoders (VAEs) which\nexplicitly place the most \"average\" data close to the mean of the Gaussian\nprior. We propose that by moving towards the tails of the prior distribution,\nthe model will transition towards generating more idiosyncratic, varied\nrenditions. Focusing here on intonation, we investigate the trade-off between\nnaturalness and intonation variation and find that typical acoustic models can\neither be natural, or varied, but not both. However, sampling from the tails of\nthe VAE prior produces much more varied intonation than the traditional\napproaches, whilst maintaining the same level of naturalness.", "published": "2019-06-10 19:05:03", "link": "http://arxiv.org/abs/1906.04233v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Transfer Learning for Ultrasound Tongue Contour Extraction with\n  Different Domains", "abstract": "Medical ultrasound technology is widely used in routine clinical applications\nsuch as disease diagnosis and treatment as well as other applications like\nreal-time monitoring of human tongue shapes and motions as visual feedback in\nsecond language training. Due to the low-contrast characteristic and noisy\nnature of ultrasound images, it might require expertise for non-expert users to\nrecognize tongue gestures. Manual tongue segmentation is a cumbersome,\nsubjective, and error-prone task. Furthermore, it is not a feasible solution\nfor real-time applications. In the last few years, deep learning methods have\nbeen used for delineating and tracking tongue dorsum. Deep convolutional neural\nnetworks (DCNNs), which have shown to be successful in medical image analysis\ntasks, are typically weak for the same task on different domains. In many\ncases, DCNNs trained on data acquired with one ultrasound device, do not\nperform well on data of varying ultrasound device or acquisition protocol.\nDomain adaptation is an alternative solution for this difficulty by\ntransferring the weights from the model trained on a large annotated legacy\ndataset to a new model for adapting on another different dataset using\nfine-tuning. In this study, after conducting extensive experiments, we\naddressed the problem of domain adaptation on small ultrasound datasets for\ntongue contour extraction. We trained a U-net network comprises of an\nencoder-decoder path from scratch, and then with several surrogate scenarios,\nsome parts of the trained network were fine-tuned on another dataset as the\ndomain-adapted networks. We repeat scenarios from target to source domains to\nfind a balance point for knowledge transfer from source to target and vice\nversa. The performance of new fine-tuned networks was evaluated on the same\ntask with images from different domains.", "published": "2019-06-10 22:17:08", "link": "http://arxiv.org/abs/1906.04301v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS", "eess.IV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Deep Learning-Based Automatic Downbeat Tracking: A Brief Review", "abstract": "As an important format of multimedia, music has filled almost everyone's\nlife. Automatic analyzing music is a significant step to satisfy people's need\nfor music retrieval and music recommendation in an effortless way. Thereinto,\ndownbeat tracking has been a fundamental and continuous problem in Music\nInformation Retrieval (MIR) area. Despite significant research efforts,\ndownbeat tracking still remains a challenge. Previous researches either focus\non feature engineering (extracting certain features by signal processing, which\nare semi-automatic solutions); or have some limitations: they can only model\nmusic audio recordings within limited time signatures and tempo ranges.\nRecently, deep learning has surpassed traditional machine learning methods and\nhas become the primary algorithm in feature learning; the combination of\ntraditional and deep learning methods also has made better performance. In this\npaper, we begin with a background introduction of downbeat tracking problem.\nThen, we give detailed discussions of the following topics: system\narchitecture, feature extraction, deep neural network algorithms, datasets, and\nevaluation strategy. In addition, we take a look at the results from the annual\nbenchmark evaluation--Music Information Retrieval Evaluation eXchange\n(MIREX)--as well as the developments in software implementations. Although much\nhas been achieved in the area of automatic downbeat tracking, some problems\nstill remain. We point out these problems and conclude with possible directions\nand challenges for future research.", "published": "2019-06-10 09:38:38", "link": "http://arxiv.org/abs/1906.03870v1", "categories": ["cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "\"Did You Hear That?\" Learning to Play Video Games from Audio Cues", "abstract": "Game-playing AI research has focused for a long time on learning to play\nvideo games from visual input or symbolic information. However, humans benefit\nfrom a wider array of sensors which we utilise in order to navigate the world\naround us. In particular, sounds and music are key to how many of us perceive\nthe world and influence the decisions we make. In this paper, we present\ninitial experiments on game-playing agents learning to play video games solely\nfrom audio cues. We expand the Video Game Description Language to allow for\naudio specification, and the General Video Game AI framework to provide new\naudio games and an API for learning agents to make use of audio observations.\nWe analyse the games and the audio game design process, include initial results\nwith simple Q~Learning agents, and encourage further research in this area.", "published": "2019-06-10 14:38:17", "link": "http://arxiv.org/abs/1906.04027v2", "categories": ["cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Learning Individual Styles of Conversational Gesture", "abstract": "Human speech is often accompanied by hand and arm gestures. Given audio\nspeech input, we generate plausible gestures to go along with the sound.\nSpecifically, we perform cross-modal translation from \"in-the-wild'' monologue\nspeech of a single speaker to their hand and arm motion. We train on unlabeled\nvideos for which we only have noisy pseudo ground truth from an automatic pose\ndetection system. Our proposed model significantly outperforms baseline methods\nin a quantitative comparison. To support research toward obtaining a\ncomputational understanding of the relationship between gesture and speech, we\nrelease a large video dataset of person-specific gestures. The project website\nwith video, code and data can be found at\nhttp://people.eecs.berkeley.edu/~shiry/speech2gesture .", "published": "2019-06-10 17:58:08", "link": "http://arxiv.org/abs/1906.04160v1", "categories": ["cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Estimation of 2D Velocity Model using Acoustic Signals and Convolutional\n  Neural Networks", "abstract": "The parameters estimation of a system using indirect measurements over the\nsame system is a problem that occurs in many fields of engineering, known as\nthe inverse problem. It also happens in the field of underwater acoustic,\nespecially in mediums that are not transparent enough. In those cases, shape\nidentification of objects using only acoustic signals is a challenge because it\nis carried out with information of echoes that are produced by objects with\ndifferent densities from that of the medium. In general, these echoes are\ndifficult to understand since their information is usually noisy and redundant.\nIn this paper, we propose a model of convolutional neural network with an\nEncoder-Decoder configuration to estimate both localization and shape of\nobjects, which produce reflected signals. This model allows us to obtain a 2D\nvelocity model. The model was trained with data generated by the\nfinite-difference method, and it achieved a value of 98.58% in the intersection\nover union metric 75.88% in precision and 64.69% in sensibility.", "published": "2019-06-10 22:36:53", "link": "http://arxiv.org/abs/1906.04310v1", "categories": ["eess.SP", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "CNN depth analysis with different channel inputs for Acoustic Scene\n  Classification", "abstract": "Acoustic scene classification (ASC) has been approached in the last years\nusing deep learning techniques such as convolutional neural networks or\nrecurrent neural networks. Many state-of-the-art solutions are based on image\nclassification frameworks and, as such, a 2D representation of the audio signal\nis considered for training these networks. Finding the most suitable audio\nrepresentation is still a research area of interest. In this paper, different\nlog-Mel representations and combinations are analyzed. Experiments show that\nthe best results are obtained using the harmonic and percussive components plus\nthe difference between left and right stereo channels, (L-R). On the other\nhand, it is a common strategy to ensemble different models in order to increase\nthe final accuracy. Even though averaging different model predictions is a\ncommon choice, an exhaustive analysis of different ensemble techniques has not\nbeen presented in ASC problems. In this paper, geometric and arithmetic mean\nplus the Ordered Weighted Averaging (OWA) operator are studied as aggregation\noperators for the output of the different models of the ensemble. Finally, the\nwork carried out in this paper is highly oriented towards real-time\nimplementations. In this context, as the number of applications for audio\nclassification on edge devices is increasing exponentially, we also analyze\ndifferent network depths and efficient solutions for aggregating ensemble\npredictions.", "published": "2019-06-10 09:32:41", "link": "http://arxiv.org/abs/1906.04591v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "BowNet: Dilated Convolution Neural Network for Ultrasound Tongue Contour\n  Extraction", "abstract": "Ultrasound imaging is safe, relatively affordable, and capable of real-time\nperformance. One application of this technology is to visualize and to\ncharacterize human tongue shape and motion during a real-time speech to study\nhealthy or impaired speech production. Due to the noisy nature of ultrasound\nimages with low-contrast characteristic, it might require expertise for\nnon-expert users to recognize organ shape such as tongue surface (dorsum). To\nalleviate this difficulty for quantitative analysis of tongue shape and motion,\ntongue surface can be extracted, tracked, and visualized instead of the whole\ntongue region. Delineating the tongue surface from each frame is a cumbersome,\nsubjective, and error-prone task. Furthermore, the rapidity and complexity of\ntongue gestures have made it a challenging task, and manual segmentation is not\na feasible solution for real-time applications. Employing the power of\nstate-of-the-art deep neural network models and training techniques, it is\nfeasible to implement new fully-automatic, accurate, and robust segmentation\nmethods with the capability of real-time performance, applicable for tracking\nof the tongue contours during the speech. This paper presents two novel deep\nneural network models named BowNet and wBowNet benefits from the ability of\nglobal prediction of decoding-encoding models, with integrated multi-scale\ncontextual information, and capability of full-resolution (local) extraction of\ndilated convolutions. Experimental results using several ultrasound tongue\nimage datasets revealed that the combination of both localization and\nglobalization searching could improve prediction result significantly.\nAssessment of BowNet models using both qualitatively and quantitatively studies\nshowed them outstanding achievements in terms of accuracy and robustness in\ncomparison with similar techniques.", "published": "2019-06-10 19:04:09", "link": "http://arxiv.org/abs/1906.04232v1", "categories": ["eess.IV", "cs.CV", "cs.LG", "cs.NE", "cs.SD", "eess.AS"], "primary_category": "eess.IV"}
