{"title": "VIPHY: Probing \"Visible\" Physical Commonsense Knowledge", "abstract": "In recent years, vision-language models (VLMs) have shown remarkable\nperformance on visual reasoning tasks (e.g. attributes, location). While such\ntasks measure the requisite knowledge to ground and reason over a given visual\ninstance, they do not, however, measure the ability of VLMs to retain and\ngeneralize such knowledge. In this work, we evaluate their ability to acquire\n\"visible\" physical knowledge -- the information that is easily accessible from\nimages of static scenes, particularly across the dimensions of object color,\nsize and space. We build an automatic pipeline to derive a comprehensive\nknowledge resource for calibrating and probing these models. Our results\nindicate a severe gap between model and human performance across all three\ntasks. Furthermore, our caption pretrained baseline (CapBERT) significantly\noutperforms VLMs on both size and spatial tasks -- highlighting that despite\nsufficient access to ground language with visual modality, they struggle to\nretain such knowledge. The dataset and code are available at\nhttps://github.com/Axe--/ViPhy .", "published": "2022-09-15 02:06:25", "link": "http://arxiv.org/abs/2209.07000v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A semantic hierarchical graph neural network for text classification", "abstract": "The key to the text classification task is language representation and\nimportant information extraction, and there are many related studies. In recent\nyears, the research on graph neural network (GNN) in text classification has\ngradually emerged and shown its advantages, but the existing models mainly\nfocus on directly inputting words as graph nodes into the GNN models ignoring\nthe different levels of semantic structure information in the samples. To\naddress the issue, we propose a new hierarchical graph neural network (HieGNN)\nwhich extracts corresponding information from word-level, sentence-level and\ndocument-level respectively. Experimental results on several benchmark datasets\nachieve better or similar results compared to several baseline methods, which\ndemonstrate that our model is able to obtain more useful information for\nclassification from samples.", "published": "2022-09-15 03:59:31", "link": "http://arxiv.org/abs/2209.07031v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Accuracy of the Uzbek stop words detection: a case study on \"School\n  corpus\"", "abstract": "Stop words are very important for information retrieval and text analysis\ninvestigation tasks of natural language processing. Current work presents a\nmethod to evaluate the quality of a list of stop words aimed at automatically\ncreating techniques. Although the method proposed in this paper was tested on\nan automatically-generated list of stop words for the Uzbek language, it can\nbe, with some modifications, applied to similar languages either from the same\nfamily or the ones that have an agglutinative nature. Since the Uzbek language\nbelongs to the family of agglutinative languages, it can be explained that the\nautomatic detection of stop words in the language is a more complex process\nthan in inflected languages. Moreover, we integrated our previous work on stop\nwords detection in the example of the \"School corpus\" by investigating how to\nautomatically analyse the detection of stop words in Uzbek texts. This work is\ndevoted to answering whether there is a good way of evaluating available stop\nwords for Uzbek texts, or whether it is possible to determine what part of the\nUzbek sentence contains the majority of the stop words by studying the\nnumerical characteristics of the probability of unique words. The results show\nacceptable accuracy of the stop words lists.", "published": "2022-09-15 05:14:31", "link": "http://arxiv.org/abs/2209.07053v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "uChecker: Masked Pretrained Language Models as Unsupervised Chinese\n  Spelling Checkers", "abstract": "The task of Chinese Spelling Check (CSC) is aiming to detect and correct\nspelling errors that can be found in the text. While manually annotating a\nhigh-quality dataset is expensive and time-consuming, thus the scale of the\ntraining dataset is usually very small (e.g., SIGHAN15 only contains 2339\nsamples for training), therefore supervised-learning based models usually\nsuffer the data sparsity limitation and over-fitting issue, especially in the\nera of big language models. In this paper, we are dedicated to investigating\nthe \\textbf{unsupervised} paradigm to address the CSC problem and we propose a\nframework named \\textbf{uChecker} to conduct unsupervised spelling error\ndetection and correction. Masked pretrained language models such as BERT are\nintroduced as the backbone model considering their powerful language diagnosis\ncapability. Benefiting from the various and flexible MASKing operations, we\npropose a Confusionset-guided masking strategy to fine-train the masked\nlanguage model to further improve the performance of unsupervised detection and\ncorrection. Experimental results on standard datasets demonstrate the\neffectiveness of our proposed model uChecker in terms of character-level and\nsentence-level Accuracy, Precision, Recall, and F1-Measure on tasks of spelling\nerror detection and correction respectively.", "published": "2022-09-15 05:57:12", "link": "http://arxiv.org/abs/2209.07068v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Impact of Edge Displacement Vaserstein Distance on UD Parsing\n  Performance", "abstract": "We contribute to the discussion on parsing performance in NLP by introducing\na measurement that evaluates the differences between the distributions of edge\ndisplacement (the directed distance of edges) seen in training and test data.\nWe hypothesize that this measurement will be related to differences observed in\nparsing performance across treebanks. We motivate this by building upon\nprevious work and then attempt to falsify this hypothesis by using a number of\nstatistical methods. We establish that there is a statistical correlation\nbetween this measurement and parsing performance even when controlling for\npotential covariants. We then use this to establish a sampling technique that\ngives us an adversarial and complementary split. This gives an idea of the\nlower and upper bounds of parsing systems for a given treebank in lieu of\nfreshly sampled data. In a broader sense, the methodology presented here can\nact as a reference for future correlation-based exploratory work in NLP.", "published": "2022-09-15 08:37:12", "link": "http://arxiv.org/abs/2209.07139v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "TempoWiC: An Evaluation Benchmark for Detecting Meaning Shift in Social\n  Media", "abstract": "Language evolves over time, and word meaning changes accordingly. This is\nespecially true in social media, since its dynamic nature leads to faster\nsemantic shifts, making it challenging for NLP models to deal with new content\nand trends. However, the number of datasets and models that specifically\naddress the dynamic nature of these social platforms is scarce. To bridge this\ngap, we present TempoWiC, a new benchmark especially aimed at accelerating\nresearch in social media-based meaning shift. Our results show that TempoWiC is\na challenging benchmark, even for recently-released language models specialized\nin social media.", "published": "2022-09-15 11:17:56", "link": "http://arxiv.org/abs/2209.07216v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UBARv2: Towards Mitigating Exposure Bias in Task-Oriented Dialogs", "abstract": "This paper studies the exposure bias problem in task-oriented dialog systems,\nwhere the model's generated content over multiple turns drives the dialog\ncontext away from the ground-truth distribution at training time, introducing\nerror propagation and damaging the robustness of the TOD system. To bridge the\ngap between training and inference for multi-turn task-oriented dialogs, we\npropose session-level sampling which explicitly exposes the model to sampled\ngenerated content of dialog context during training. Additionally, we employ a\ndropout-based consistency regularization with the masking strategy R-Mask to\nfurther improve the robustness and performance of the model. The proposed\nUBARv2 achieves state-of-the-art performance on the standardized evaluation\nbenchmark MultiWOZ and extensive experiments show the effectiveness of the\nproposed methods.", "published": "2022-09-15 12:14:46", "link": "http://arxiv.org/abs/2209.07239v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linear Transformations for Cross-lingual Sentiment Analysis", "abstract": "This paper deals with cross-lingual sentiment analysis in Czech, English and\nFrench languages. We perform zero-shot cross-lingual classification using five\nlinear transformations combined with LSTM and CNN based classifiers. We compare\nthe performance of the individual transformations, and in addition, we confront\nthe transformation-based approach with existing state-of-the-art BERT-like\nmodels. We show that the pre-trained embeddings from the target domain are\ncrucial to improving the cross-lingual classification results, unlike in the\nmonolingual classification, where the effect is not so distinctive.", "published": "2022-09-15 12:27:16", "link": "http://arxiv.org/abs/2209.07244v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph-to-Text Generation with Dynamic Structure Pruning", "abstract": "Most graph-to-text works are built on the encoder-decoder framework with\ncross-attention mechanism. Recent studies have shown that explicitly modeling\nthe input graph structure can significantly improve the performance. However,\nthe vanilla structural encoder cannot capture all specialized information in a\nsingle forward pass for all decoding steps, resulting in inaccurate semantic\nrepresentations. Meanwhile, the input graph is flatted as an unordered sequence\nin the cross attention, ignoring the original graph structure. As a result, the\nobtained input graph context vector in the decoder may be flawed. To address\nthese issues, we propose a Structure-Aware Cross-Attention (SACA) mechanism to\nre-encode the input graph representation conditioning on the newly generated\ncontext at each decoding step in a structure aware manner. We further adapt\nSACA and introduce its variant Dynamic Graph Pruning (DGP) mechanism to\ndynamically drop irrelevant nodes in the decoding process. We achieve new\nstate-of-the-art results on two graph-to-text datasets, LDC2020T02 and\nENT-DESC, with only minor increase on computational cost.", "published": "2022-09-15 12:48:10", "link": "http://arxiv.org/abs/2209.07258v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\u00daFAL CorPipe at CRAC 2022: Effectivity of Multilingual Models for\n  Coreference Resolution", "abstract": "We describe the winning submission to the CRAC 2022 Shared Task on\nMultilingual Coreference Resolution. Our system first solves mention detection\nand then coreference linking on the retrieved spans with an\nantecedent-maximization approach, and both tasks are fine-tuned jointly with\nshared Transformer weights. We report results of fine-tuning a wide range of\npretrained models. The center of this contribution are fine-tuned multilingual\nmodels. We found one large multilingual model with sufficiently large encoder\nto increase performance on all datasets across the board, with the benefit not\nlimited only to the underrepresented languages or groups of typologically\nrelative languages. The source code is available at\nhttps://github.com/ufal/crac2022-corpipe.", "published": "2022-09-15 13:11:39", "link": "http://arxiv.org/abs/2209.07278v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Is Flat: A Seq2Seq Generative Framework for Various Knowledge\n  Graph Completion", "abstract": "Knowledge Graph Completion (KGC) has been recently extended to multiple\nknowledge graph (KG) structures, initiating new research directions, e.g.\nstatic KGC, temporal KGC and few-shot KGC. Previous works often design KGC\nmodels closely coupled with specific graph structures, which inevitably results\nin two drawbacks: 1) structure-specific KGC models are mutually incompatible;\n2) existing KGC methods are not adaptable to emerging KGs. In this paper, we\npropose KG-S2S, a Seq2Seq generative framework that could tackle different\nverbalizable graph structures by unifying the representation of KG facts into\n\"flat\" text, regardless of their original form. To remedy the KG structure\ninformation loss from the \"flat\" text, we further improve the input\nrepresentations of entities and relations, and the inference algorithm in\nKG-S2S. Experiments on five benchmarks show that KG-S2S outperforms many\ncompetitive baselines, setting new state-of-the-art performance. Finally, we\nanalyze KG-S2S's ability on the different relations and the Non-entity\nGenerations.", "published": "2022-09-15 13:49:40", "link": "http://arxiv.org/abs/2209.07299v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Round-Trip Translation for Machine Translation Evaluation", "abstract": "Automatic evaluation on low-resource language translation suffers from a\ndeficiency of parallel corpora. Round-trip translation could be served as a\nclever and straightforward technique to alleviate the requirement of the\nparallel evaluation corpus. However, there was an observation of obscure\ncorrelations between the evaluation scores by forward and round-trip\ntranslations in the era of statistical machine translation (SMT). In this\npaper, we report the surprising finding that round-trip translation can be used\nfor automatic evaluation without the references. Firstly, our revisit on the\nround-trip translation in SMT evaluation unveils that its long-standing\nmisunderstanding is essentially caused by copying mechanism. After removing\ncopying mechanism in SMT, round-trip translation scores can appropriately\nreflect the forward translation performance. Then, we demonstrate the\nrectification is overdue as round-trip translation could benefit multiple\nmachine translation evaluation tasks. To be more specific, round-trip\ntranslation could be used i) to predict corresponding forward translation\nscores; ii) to improve the performance of the recently advanced quality\nestimation model; and iii) to identify adversarial competitors in shared tasks\nvia cross-system verification.", "published": "2022-09-15 15:06:20", "link": "http://arxiv.org/abs/2209.07351v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Geographic Performance Disparities of Offensive Language\n  Classifiers", "abstract": "Text classifiers are applied at scale in the form of one-size-fits-all\nsolutions. Nevertheless, many studies show that classifiers are biased\nregarding different languages and dialects. When measuring and discovering\nthese biases, some gaps present themselves and should be addressed. First,\n``Does language, dialect, and topical content vary across geographical\nregions?'' and secondly ``If there are differences across the regions, do they\nimpact model performance?''. We introduce a novel dataset called GeoOLID with\nmore than 14 thousand examples across 15 geographically and demographically\ndiverse cities to address these questions. We perform a comprehensive analysis\nof geographical-related content and their impact on performance disparities of\noffensive language detection models. Overall, we find that current models do\nnot generalize across locations. Likewise, we show that while offensive\nlanguage models produce false positives on African American English, model\nperformance is not correlated with each city's minority population proportions.\nWarning: This paper contains offensive language.", "published": "2022-09-15 15:08:18", "link": "http://arxiv.org/abs/2209.07353v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine Reading, Fast and Slow: When Do Models \"Understand\" Language?", "abstract": "Two of the most fundamental challenges in Natural Language Understanding\n(NLU) at present are: (a) how to establish whether deep learning-based models\nscore highly on NLU benchmarks for the 'right' reasons; and (b) to understand\nwhat those reasons would even be. We investigate the behavior of reading\ncomprehension models with respect to two linguistic 'skills': coreference\nresolution and comparison. We propose a definition for the reasoning steps\nexpected from a system that would be 'reading slowly', and compare that with\nthe behavior of five models of the BERT family of various sizes, observed\nthrough saliency scores and counterfactual explanations. We find that for\ncomparison (but not coreference) the systems based on larger encoders are more\nlikely to rely on the 'right' information, but even they struggle with\ngeneralization, suggesting that they still learn specific lexical patterns\nrather than the general principles of comparison.", "published": "2022-09-15 16:25:44", "link": "http://arxiv.org/abs/2209.07430v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Error Analysis for Document-level Information Extraction", "abstract": "Document-level information extraction (IE) tasks have recently begun to be\nrevisited in earnest using the end-to-end neural network techniques that have\nbeen successful on their sentence-level IE counterparts. Evaluation of the\napproaches, however, has been limited in a number of dimensions. In particular,\nthe precision/recall/F1 scores typically reported provide few insights on the\nrange of errors the models make. We build on the work of Kummerfeld and Klein\n(2013) to propose a transformation-based framework for automating error\nanalysis in document-level event and (N-ary) relation extraction. We employ our\nframework to compare two state-of-the-art document-level template-filling\napproaches on datasets from three domains; and then, to gauge progress in IE\nsince its inception 30 years ago, vs. four systems from the MUC-4 (1992)\nevaluation.", "published": "2022-09-15 16:43:09", "link": "http://arxiv.org/abs/2209.07442v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Opinion Summarization Using Approximate Geodesics", "abstract": "Opinion summarization is the task of creating summaries capturing popular\nopinions from user reviews. In this paper, we introduce Geodesic Summarizer\n(GeoSumm), a novel system to perform unsupervised extractive opinion\nsummarization. GeoSumm involves an encoder-decoder based representation\nlearning model, that generates representations of text as a distribution over\nlatent semantic units. GeoSumm generates these representations by performing\ndictionary learning over pre-trained text representations at multiple decoder\nlayers. We then use these representations to quantify the relevance of review\nsentences using a novel approximate geodesic distance based scoring mechanism.\nWe use the relevance scores to identify popular opinions in order to compose\ngeneral and aspect-specific summaries. Our proposed model, GeoSumm, achieves\nstate-of-the-art performance on three opinion summarization datasets. We\nperform additional experiments to analyze the functioning of our model and\nshowcase the generalization ability of {\\X} across different domains.", "published": "2022-09-15 17:37:08", "link": "http://arxiv.org/abs/2209.07496v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for\n  Multilingual Tweet Representations at Twitter", "abstract": "Pre-trained language models (PLMs) are fundamental for natural language\nprocessing applications. Most existing PLMs are not tailored to the noisy\nuser-generated text on social media, and the pre-training does not factor in\nthe valuable social engagement logs available in a social network. We present\nTwHIN-BERT, a multilingual language model productionized at Twitter, trained on\nin-domain data from the popular social network. TwHIN-BERT differs from prior\npre-trained language models as it is trained with not only text-based\nself-supervision, but also with a social objective based on the rich social\nengagements within a Twitter heterogeneous information network (TwHIN). Our\nmodel is trained on 7 billion tweets covering over 100 distinct languages,\nproviding a valuable representation to model short, noisy, user-generated text.\nWe evaluate our model on various multilingual social recommendation and\nsemantic understanding tasks and demonstrate significant metric improvement\nover established pre-trained language models. We open-source TwHIN-BERT and our\ncurated hashtag prediction and social engagement benchmark datasets to the\nresearch community.", "published": "2022-09-15 19:01:21", "link": "http://arxiv.org/abs/2209.07562v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stateful Memory-Augmented Transformers for Efficient Dialogue Modeling", "abstract": "Transformer encoder-decoder models have achieved great performance in\ndialogue generation tasks, however, their inability to process long dialogue\nhistory often leads to truncation of the context To address this problem, we\npropose a novel memory-augmented transformer that is compatible with existing\npre-trained encoder-decoder models and enables efficient preservation of the\ndialogue history information. By incorporating a separate memory module\nalongside the pre-trained transformer, the model can effectively interchange\ninformation between the memory states and the current input context. We\nevaluate our model on three dialogue datasets and two language modeling\ndatasets. Experimental results show that our method has achieved superior\nefficiency and performance compared to other pre-trained Transformer baselines.", "published": "2022-09-15 22:37:22", "link": "http://arxiv.org/abs/2209.07634v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting the Practical Effectiveness of Constituency Parse Extraction\n  from Pre-trained Language Models", "abstract": "Constituency Parse Extraction from Pre-trained Language Models (CPE-PLM) is a\nrecent paradigm that attempts to induce constituency parse trees relying only\non the internal knowledge of pre-trained language models. While attractive in\nthe perspective that similar to in-context learning, it does not require\ntask-specific fine-tuning, the practical effectiveness of such an approach\nstill remains unclear, except that it can function as a probe for investigating\nlanguage models' inner workings. In this work, we mathematically reformulate\nCPE-PLM and propose two advanced ensemble methods tailored for it,\ndemonstrating that the new parsing paradigm can be competitive with common\nunsupervised parsers by introducing a set of heterogeneous PLMs combined using\nour techniques. Furthermore, we explore some scenarios where the trees\ngenerated by CPE-PLM are practically useful. Specifically, we show that CPE-PLM\nis more effective than typical supervised parsers in few-shot settings.", "published": "2022-09-15 09:41:19", "link": "http://arxiv.org/abs/2211.00479v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Offline Reinforcement Learning Help Natural Language Understanding?", "abstract": "Pre-training has been a useful method for learning implicit transferable\nknowledge and it shows the benefit of offering complementary features across\ndifferent modalities. Recent work mainly focuses on the modalities such as\nimage and text, for example, studies show that visual features learned from\nimages can help visual-grounded language understanding. In this paper, we\nconsider investigating the potential connection between offline reinforcement\nlearning (RL) and language modeling (LM). Intuitively, RL and LM are similar in\npredicting the next states based on the current and previous states, which rely\non both local and long-range dependency across states. To validate such an\nassumption, we pre-trained different offline RL tasks using Transformer and\nthen evaluate these models on various language-related tasks. Experimental\nresults show that our RL pre-trained models can give close performance compared\nwith the models using the LM training objective, showing that there exist\ncommon useful features across these two modalities. To further explore the\npotential relationship, we investigate some factors such as Markov property and\nthe sequential nature of RL trajectory.", "published": "2022-09-15 02:55:10", "link": "http://arxiv.org/abs/2212.03864v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cold-Start Data Selection for Few-shot Language Model Fine-tuning: A\n  Prompt-Based Uncertainty Propagation Approach", "abstract": "Large Language Models have demonstrated remarkable few-shot performance, but\nthe performance can be sensitive to the selection of few-shot instances. We\npropose PATRON, a new method that uses prompt-based uncertainty estimation for\ndata selection for pre-trained language model fine-tuning under cold-start\nscenarios, i.e., no initial labeled data are available. In PATRON, we design\n(1) a prompt-based uncertainty propagation approach to estimate the importance\nof data points and (2) a partition-then-rewrite (PTR) strategy to promote\nsample diversity when querying for annotations. Experiments on six text\nclassification datasets show that PATRON outperforms the strongest cold-start\ndata selection baselines by up to 6.9%. Besides, with 128 labels only, PATRON\nachieves 91.0% and 92.1% of the fully supervised performance based on vanilla\nfine-tuning and prompt-based learning respectively. Our implementation of\nPATRON is available at \\url{https://github.com/yueyu1030/Patron}.", "published": "2022-09-15 01:51:22", "link": "http://arxiv.org/abs/2209.06995v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Completion with Pre-trained Multimodal Transformer and\n  Twins Negative Sampling", "abstract": "Knowledge graphs (KGs) that modelings the world knowledge as structural\ntriples are inevitably incomplete. Such problems still exist for multimodal\nknowledge graphs (MMKGs). Thus, knowledge graph completion (KGC) is of great\nimportance to predict the missing triples in the existing KGs. As for the\nexisting KGC methods, embedding-based methods rely on manual design to leverage\nmultimodal information while finetune-based approaches are not superior to\nembedding-based methods in link prediction. To address these problems, we\npropose a VisualBERT-enhanced Knowledge Graph Completion model (VBKGC for\nshort). VBKGC could capture deeply fused multimodal information for entities\nand integrate them into the KGC model. Besides, we achieve the co-design of the\nKGC model and negative sampling by designing a new negative sampling strategy\ncalled twins negative sampling. Twins negative sampling is suitable for\nmultimodal scenarios and could align different embeddings for entities. We\nconduct extensive experiments to show the outstanding performance of VBKGC on\nthe link prediction task and make further exploration of VBKGC.", "published": "2022-09-15 06:50:31", "link": "http://arxiv.org/abs/2209.07084v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Multi-Modal Masked Autoencoders for Medical Vision-and-Language\n  Pre-Training", "abstract": "Medical vision-and-language pre-training provides a feasible solution to\nextract effective vision-and-language representations from medical images and\ntexts. However, few studies have been dedicated to this field to facilitate\nmedical vision-and-language understanding. In this paper, we propose a\nself-supervised learning paradigm with multi-modal masked autoencoders\n(M$^3$AE), which learn cross-modal domain knowledge by reconstructing missing\npixels and tokens from randomly masked images and texts. There are three key\ndesigns to make this simple approach work. First, considering the different\ninformation densities of vision and language, we adopt different masking ratios\nfor the input image and text, where a considerably larger masking ratio is used\nfor images. Second, we use visual and textual features from different layers to\nperform the reconstruction to deal with different levels of abstraction in\nvisual and language. Third, we develop different designs for vision and\nlanguage decoders (i.e., a Transformer for vision and a multi-layer perceptron\nfor language). To perform a comprehensive evaluation and facilitate further\nresearch, we construct a medical vision-and-language benchmark including three\ntasks. Experimental results demonstrate the effectiveness of our approach,\nwhere state-of-the-art results are achieved on all downstream tasks. Besides,\nwe conduct further analysis to better verify the effectiveness of different\ncomponents of our approach and various settings of pre-training. The source\ncode is available at~\\url{https://github.com/zhjohnchan/M3AE}.", "published": "2022-09-15 07:26:43", "link": "http://arxiv.org/abs/2209.07098v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Align, Reason and Learn: Enhancing Medical Vision-and-Language\n  Pre-training with Knowledge", "abstract": "Medical vision-and-language pre-training (Med-VLP) has received considerable\nattention owing to its applicability to extracting generic vision-and-language\nrepresentations from medical images and texts. Most existing methods mainly\ncontain three elements: uni-modal encoders (i.e., a vision encoder and a\nlanguage encoder), a multi-modal fusion module, and pretext tasks, with few\nstudies considering the importance of medical domain expert knowledge and\nexplicitly exploiting such knowledge to facilitate Med-VLP. Although there\nexist knowledge-enhanced vision-and-language pre-training (VLP) methods in the\ngeneral domain, most require off-the-shelf toolkits (e.g., object detectors and\nscene graph parsers), which are unavailable in the medical domain. In this\npaper, we propose a systematic and effective approach to enhance Med-VLP by\nstructured medical knowledge from three perspectives. First, considering\nknowledge can be regarded as the intermediate medium between vision and\nlanguage, we align the representations of the vision encoder and the language\nencoder through knowledge. Second, we inject knowledge into the multi-modal\nfusion model to enable the model to perform reasoning using knowledge as the\nsupplementation of the input image and text. Third, we guide the model to put\nemphasis on the most critical information in images and texts by designing\nknowledge-induced pretext tasks. To perform a comprehensive evaluation and\nfacilitate further research, we construct a medical vision-and-language\nbenchmark including three tasks. Experimental results illustrate the\neffectiveness of our approach, where state-of-the-art performance is achieved\non all downstream tasks. Further analyses explore the effects of different\ncomponents of our approach and various settings of pre-training.", "published": "2022-09-15 08:00:01", "link": "http://arxiv.org/abs/2209.07118v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Examining Large Pre-Trained Language Models for Machine Translation:\n  What You Don't Know About It", "abstract": "Pre-trained language models (PLMs) often take advantage of the monolingual\nand multilingual dataset that is freely available online to acquire general or\nmixed domain knowledge before deployment into specific tasks. Extra-large PLMs\n(xLPLMs) are proposed very recently to claim supreme performances over\nsmaller-sized PLMs such as in machine translation (MT) tasks. These xLPLMs\ninclude Meta-AI's wmt21-dense-24-wide-en-X (2021) and NLLB (2022). In this\nwork, we examine if xLPLMs are absolutely superior to smaller-sized PLMs in\nfine-tuning toward domain-specific MTs. We use two different in-domain data of\ndifferent sizes: commercial automotive in-house data and clinical shared task\ndata from the ClinSpEn2022 challenge at WMT2022. We choose popular Marian\nHelsinki as smaller sized PLM and two massive-sized Mega-Transformers from\nMeta-AI as xLPLMs.\n  Our experimental investigation shows that 1) on smaller-sized in-domain\ncommercial automotive data, xLPLM wmt21-dense-24-wide-en-X indeed shows much\nbetter evaluation scores using SacreBLEU and hLEPOR metrics than smaller-sized\nMarian, even though its score increase rate is lower than Marian after\nfine-tuning; 2) on relatively larger-size well prepared clinical data\nfine-tuning, the xLPLM NLLB tends to lose its advantage over smaller-sized\nMarian on two sub-tasks (clinical terms and ontology concepts) using ClinSpEn\noffered metrics METEOR, COMET, and ROUGE-L, and totally lost to Marian on\nTask-1 (clinical cases) on all official metrics including SacreBLEU and BLEU;\n3) metrics do not always agree with each other on the same tasks using the same\nmodel outputs; 4) clinic-Marian ranked No.2 on Task- 1 (via SACREBLEU/BLEU) and\nTask-3 (via METEOR and ROUGE) among all submissions.", "published": "2022-09-15 16:12:26", "link": "http://arxiv.org/abs/2209.07417v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hierarchical Attention Network for Explainable Depression Detection on\n  Twitter Aided by Metaphor Concept Mappings", "abstract": "Automatic depression detection on Twitter can help individuals privately and\nconveniently understand their mental health status in the early stages before\nseeing mental health professionals. Most existing black-box-like deep learning\nmethods for depression detection largely focused on improving classification\nperformance. However, explaining model decisions is imperative in health\nresearch because decision-making can often be high-stakes and life-and-death.\nReliable automatic diagnosis of mental health problems including depression\nshould be supported by credible explanations justifying models' predictions. In\nthis work, we propose a novel explainable model for depression detection on\nTwitter. It comprises a novel encoder combining hierarchical attention\nmechanisms and feed-forward neural networks. To support psycholinguistic\nstudies, our model leverages metaphorical concept mappings as input. Thus, it\nnot only detects depressed individuals, but also identifies features of such\nusers' tweets and associated metaphor concept mappings.", "published": "2022-09-15 17:36:18", "link": "http://arxiv.org/abs/2209.07494v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Corpus-Guided Contrast Sets for Morphosyntactic Feature Detection in\n  Low-Resource English Varieties", "abstract": "The study of language variation examines how language varies between and\nwithin different groups of speakers, shedding light on how we use language to\nconstruct identities and how social contexts affect language use. A common\nmethod is to identify instances of a certain linguistic feature - say, the zero\ncopula construction - in a corpus, and analyze the feature's distribution\nacross speakers, topics, and other variables, to either gain a qualitative\nunderstanding of the feature's function or systematically measure variation. In\nthis paper, we explore the challenging task of automatic morphosyntactic\nfeature detection in low-resource English varieties. We present a\nhuman-in-the-loop approach to generate and filter effective contrast sets via\ncorpus-guided edits. We show that our approach improves feature detection for\nboth Indian English and African American English, demonstrate how it can assist\nlinguistic research, and release our fine-tuned models for use by other\nresearchers.", "published": "2022-09-15 21:19:31", "link": "http://arxiv.org/abs/2209.07611v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CommunityLM: Probing Partisan Worldviews from Language Models", "abstract": "As political attitudes have diverged ideologically in the United States,\npolitical speech has diverged lingusitically. The ever-widening polarization\nbetween the US political parties is accelerated by an erosion of mutual\nunderstanding between them. We aim to make these communities more\ncomprehensible to each other with a framework that probes community-specific\nresponses to the same survey questions using community language models\nCommunityLM. In our framework we identify committed partisan members for each\ncommunity on Twitter and fine-tune LMs on the tweets authored by them. We then\nassess the worldviews of the two groups using prompt-based probing of their\ncorresponding LMs, with prompts that elicit opinions about public figures and\ngroups surveyed by the American National Election Studies (ANES) 2020\nExploratory Testing Survey. We compare the responses generated by the LMs to\nthe ANES survey results, and find a level of alignment that greatly exceeds\nseveral baseline methods. Our work aims to show that we can use community LMs\nto query the worldview of any group of people given a sufficiently large sample\nof their social media discussions or media diet.", "published": "2022-09-15 05:52:29", "link": "http://arxiv.org/abs/2209.07065v1", "categories": ["cs.SI", "cs.AI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Distribution Aware Metrics for Conditional Natural Language Generation", "abstract": "Traditional automated metrics for evaluating conditional natural language\ngeneration use pairwise comparisons between a single generated text and the\nbest-matching gold-standard ground truth text. When multiple ground truths are\navailable, scores are aggregated using an average or max operation across\nreferences. While this approach works well when diversity in the ground truth\ndata (i.e. dispersion of the distribution of conditional texts) can be ascribed\nto noise, such as in automated speech recognition, it does not allow for robust\nevaluation in the case where diversity in the ground truths represents signal\nfor the model. In this work we argue that existing metrics are not appropriate\nfor domains such as visual description or summarization where ground truths are\nsemantically diverse, and where the diversity in those captions captures useful\nadditional information about the context. We propose a novel paradigm for\nmulti-candidate evaluation of conditional language generation models, and a new\nfamily of metrics that compare the distributions of reference and\nmodel-generated caption sets using small sample sets of each. We demonstrate\nthe utility of our approach with a case study in visual description: where we\nshow that existing models optimize for single-description quality over\ndiversity, and gain some insights into how sampling methods and temperature\nimpact description quality and diversity.", "published": "2022-09-15 17:58:13", "link": "http://arxiv.org/abs/2209.07518v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LAVIS: A Library for Language-Vision Intelligence", "abstract": "We introduce LAVIS, an open-source deep learning library for LAnguage-VISion\nresearch and applications. LAVIS aims to serve as a one-stop comprehensive\nlibrary that brings recent advancements in the language-vision field accessible\nfor researchers and practitioners, as well as fertilizing future research and\ndevelopment. It features a unified interface to easily access state-of-the-art\nimage-language, video-language models and common datasets. LAVIS supports\ntraining, evaluation and benchmarking on a rich variety of tasks, including\nmultimodal classification, retrieval, captioning, visual question answering,\ndialogue and pre-training. In the meantime, the library is also highly\nextensible and configurable, facilitating future development and customization.\nIn this technical report, we describe design principles, key components and\nfunctionalities of the library, and also present benchmarking results across\ncommon language-vision tasks. The library is available at:\nhttps://github.com/salesforce/LAVIS.", "published": "2022-09-15 18:04:10", "link": "http://arxiv.org/abs/2209.09019v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Public Wisdom Matters! Discourse-Aware Hyperbolic Fourier Co-Attention\n  for Social-Text Classification", "abstract": "Social media has become the fulcrum of all forms of communication.\nClassifying social texts such as fake news, rumour, sarcasm, etc. has gained\nsignificant attention. The surface-level signals expressed by a social-text\nitself may not be adequate for such tasks; therefore, recent methods attempted\nto incorporate other intrinsic signals such as user behavior and the underlying\ngraph structure. Oftentimes, the `public wisdom' expressed through the\ncomments/replies to a social-text acts as a surrogate of crowd-sourced view and\nmay provide us with complementary signals. State-of-the-art methods on\nsocial-text classification tend to ignore such a rich hierarchical signal.\nHere, we propose Hyphen, a discourse-aware hyperbolic spectral co-attention\nnetwork. Hyphen is a fusion of hyperbolic graph representation learning with a\nnovel Fourier co-attention mechanism in an attempt to generalise the\nsocial-text classification tasks by incorporating public discourse. We parse\npublic discourse as an Abstract Meaning Representation (AMR) graph and use the\npowerful hyperbolic geometric representation to model graphs with hierarchical\nstructure. Finally, we equip it with a novel Fourier co-attention mechanism to\ncapture the correlation between the source post and public discourse. Extensive\nexperiments on four different social-text classification tasks, namely\ndetecting fake news, hate speech, rumour, and sarcasm, show that Hyphen\ngeneralises well, and achieves state-of-the-art results on ten benchmark\ndatasets. We also employ a sentence-level fact-checked and annotated dataset to\nevaluate how Hyphen is capable of producing explanations as analogous evidence\nto the final prediction.", "published": "2022-09-15 16:04:32", "link": "http://arxiv.org/abs/2209.13017v2", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Beat Transformer: Demixed Beat and Downbeat Tracking with Dilated\n  Self-Attention", "abstract": "We propose Beat Transformer, a novel Transformer encoder architecture for\njoint beat and downbeat tracking. Different from previous models that track\nbeats solely based on the spectrogram of an audio mixture, our model deals with\ndemixed spectrograms with multiple instrument channels. This is inspired by the\nfact that humans perceive metrical structures from richer musical contexts,\nsuch as chord progression and instrumentation. To this end, we develop a\nTransformer model with both time-wise attention and instrument-wise attention\nto capture deep-buried metrical cues. Moreover, our model adopts a novel\ndilated self-attention mechanism, which achieves powerful hierarchical\nmodelling with only linear complexity. Experiments demonstrate a significant\nimprovement in demixed beat tracking over the non-demixed version. Also, Beat\nTransformer achieves up to 4% point improvement in downbeat tracking accuracy\nover the TCN architectures. We further discover an interpretable attention\npattern that mirrors our understanding of hierarchical metrical structures.", "published": "2022-09-15 08:38:58", "link": "http://arxiv.org/abs/2209.07140v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Domain Adversarial Training on Conditional Variational Auto-Encoder for\n  Controllable Music Generation", "abstract": "The variational auto-encoder has become a leading framework for symbolic\nmusic generation, and a popular research direction is to study how to\neffectively control the generation process. A straightforward way is to control\na model using different conditions during inference. However, in music\npractice, conditions are usually sequential (rather than simple categorical\nlabels), involving rich information that overlaps with the learned\nrepresentation. Consequently, the decoder gets confused about whether to\n\"listen to\" the latent representation or the condition, and sometimes just\nignores the condition. To solve this problem, we leverage domain adversarial\ntraining to disentangle the representation from condition cues for better\ncontrol. Specifically, we propose a condition corruption objective that uses\nthe representation to denoise a corrupted condition. Minimized by a\ndiscriminator and maximized by the VAE encoder, this objective adversarially\ninduces a condition-invariant representation. In this paper, we focus on the\ntask of melody harmonization to illustrate our idea, while our methodology can\nbe generalized to other controllable generative tasks. Demos and experiments\nshow that our methodology facilitates not only condition-invariant\nrepresentation learning but also higher-quality controllability compared to\nbaselines.", "published": "2022-09-15 08:45:36", "link": "http://arxiv.org/abs/2209.07144v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Open Challenges in Synthetic Speech Detection", "abstract": "In this paper the current status and open challenges of synthetic speech\ndetection are addressed. The work comprises an initial analysis of available\nopen datasets and of existing detection methods, a description of the\nrequirements for new research datasets compliant with regulations and better\nrepresenting real-case scenarios, and a discussion of the desired\ncharacteristics of future trustworthy detection methods in terms of both\nfunctional and non-functional requirements. Compared to other works, based on\nspecific detection solutions or presenting single dataset of synthetic\nspeeches, our paper is meant to orient future state-of-the-art research in the\ndomain, to quickly lessen the current gap between synthesis and detection\napproaches.", "published": "2022-09-15 09:59:09", "link": "http://arxiv.org/abs/2209.07180v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Environment Classification via Blind Roomprints Estimation", "abstract": "In this paper we present a novel approach for environment classification for\nspeech recordings, which does not require the selection of decaying\nreverberation tails. It is based on a multi-band RT60 analysis of blind channel\nestimates and achieves an accuracy of up to 93.6% on test recordings derived\nfrom the ACE corpus.", "published": "2022-09-15 10:11:46", "link": "http://arxiv.org/abs/2209.07196v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MVNet: Memory Assistance and Vocal Reinforcement Network for Speech\n  Enhancement", "abstract": "Speech enhancement improves speech quality and promotes the performance of\nvarious downstream tasks. However, most current speech enhancement work was\nmainly devoted to improving the performance of downstream automatic speech\nrecognition (ASR), only a relatively small amount of work focused on the\nautomatic speaker verification (ASV) task. In this work, we propose a MVNet\nconsisted of a memory assistance module which improves the performance of\ndownstream ASR and a vocal reinforcement module which boosts the performance of\nASV. In addition, we design a new loss function to improve speaker vocal\nsimilarity. Experimental results on the Libri2mix dataset show that our method\noutperforms baseline methods in several metrics, including speech quality,\nintelligibility, and speaker vocal similarity et al.", "published": "2022-09-15 13:57:48", "link": "http://arxiv.org/abs/2209.07302v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Open Set Recognition For Music Genre Classification", "abstract": "We explore segmentation of known and unknown genre classes using the open\nsource GTZAN and FMA datasets. For each, we begin with best-case closed set\ngenre classification, then we apply open set recognition methods. We offer an\nalgorithm for the music genre classification task using OSR. We demonstrate the\nability to retrieve known genres and as well identification of aural patterns\nfor novel genres (not appearing in a training set). We conduct four\nexperiments, each containing a different set of known and unknown classes,\nusing the GTZAN and the FMA datasets to establish a baseline capacity for novel\ngenre detection. We employ grid search on both OpenMax and softmax to determine\nthe optimal total classification accuracy for each experimental setup, and\nillustrate interaction between genre labelling and open set recognition\naccuracy.", "published": "2022-09-15 18:06:01", "link": "http://arxiv.org/abs/2209.07548v2", "categories": ["eess.AS", "math.OC", "90C90, 6208", "E.4; G.4"], "primary_category": "eess.AS"}
{"title": "Non-Parallel Voice Conversion for ASR Augmentation", "abstract": "Automatic speech recognition (ASR) needs to be robust to speaker differences.\nVoice Conversion (VC) modifies speaker characteristics of input speech. This is\nan attractive feature for ASR data augmentation. In this paper, we demonstrate\nthat voice conversion can be used as a data augmentation technique to improve\nASR performance, even on LibriSpeech, which contains 2,456 speakers. For ASR\naugmentation, it is necessary that the VC model be robust to a wide range of\ninput speech. This motivates the use of a non-autoregressive, non-parallel VC\nmodel, and the use of a pretrained ASR encoder within the VC model. This work\nsuggests that despite including many speakers, speaker diversity may remain a\nlimitation to ASR quality. Finally, interrogation of our VC performance has\nprovided useful metrics for objective evaluation of VC quality.", "published": "2022-09-15 00:40:35", "link": "http://arxiv.org/abs/2209.06987v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "68T10", "I.2.7"], "primary_category": "cs.SD"}
{"title": "Self-Supervised Attention Networks and Uncertainty Loss Weighting for\n  Multi-Task Emotion Recognition on Vocal Bursts", "abstract": "Vocal bursts play an important role in communicating affect, making them\nvaluable for improving speech emotion recognition. Here, we present our\napproach for classifying vocal bursts and predicting their emotional\nsignificance in the ACII Affective Vocal Burst Workshop & Challenge 2022\n(A-VB). We use a large self-supervised audio model as shared feature extractor\nand compare multiple architectures built on classifier chains and attention\nnetworks, combined with uncertainty loss weighting strategies. Our approach\nsurpasses the challenge baseline by a wide margin on all four tasks.", "published": "2022-09-15 15:50:27", "link": "http://arxiv.org/abs/2209.07384v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-Relation Attention and Temporal Awareness for Emotion Recognition\n  via Vocal Burst", "abstract": "The technical report presents our emotion recognition pipeline for\nhigh-dimensional emotion task (A-VB High) in The ACII Affective Vocal Bursts\n(A-VB) 2022 Workshop \\& Competition. Our proposed method contains three stages.\nFirstly, we extract the latent features from the raw audio signal and its\nMel-spectrogram by self-supervised learning methods. Then, the features from\nthe raw signal are fed to the self-relation attention and temporal awareness\n(SA-TA) module for learning the valuable information between these latent\nfeatures. Finally, we concatenate all the features and utilize a\nfully-connected layer to predict each emotion's score. By empirical\nexperiments, our proposed method achieves a mean concordance correlation\ncoefficient (CCC) of 0.7295 on the test set, compared to 0.5686 on the baseline\nmodel. The code of our method is available at\nhttps://github.com/linhtd812/A-VB2022.", "published": "2022-09-15 22:06:42", "link": "http://arxiv.org/abs/2209.07629v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
