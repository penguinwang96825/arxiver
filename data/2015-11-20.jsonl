{"title": "Polysemy in Controlled Natural Language Texts", "abstract": "Computational semantics and logic-based controlled natural languages (CNL) do\nnot address systematically the word sense disambiguation problem of content\nwords, i.e., they tend to interpret only some functional words that are crucial\nfor construction of discourse representation structures. We show that\nmicro-ontologies and multi-word units allow integration of the rich and\npolysemous multi-domain background knowledge into CNL thus providing\ninterpretation for the content words. The proposed approach is demonstrated by\nextending the Attempto Controlled English (ACE) with polysemous and procedural\nconstructs resulting in a more natural CNL named PAO covering narrative\nmulti-domain texts.", "published": "2015-11-20 13:41:31", "link": "http://arxiv.org/abs/1511.06591v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Neural Machine Translation Models with Monolingual Data", "abstract": "Neural Machine Translation (NMT) has obtained state-of-the art performance\nfor several language pairs, while only using parallel data for training.\nTarget-side monolingual data plays an important role in boosting fluency for\nphrase-based statistical machine translation, and we investigate the use of\nmonolingual data for NMT. In contrast to previous work, which combines NMT\nmodels with separately trained language models, we note that encoder-decoder\nNMT architectures already have the capacity to learn the same information as a\nlanguage model, and we explore strategies to train with monolingual data\nwithout changing the neural network architecture. By pairing monolingual\ntraining data with an automatic back-translation, we can treat it as additional\nparallel training data, and we obtain substantial improvements on the WMT 15\ntask English<->German (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task\nTurkish->English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. We\nalso show that fine-tuning on in-domain monolingual and parallel data gives\nsubstantial improvements for the IWSLT 15 task English->German.", "published": "2015-11-20 17:58:37", "link": "http://arxiv.org/abs/1511.06709v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stories in the Eye: Contextual Visual Interactions for Efficient Video\n  to Language Translation", "abstract": "Integrating higher level visual and linguistic interpretations is at the\nheart of human intelligence. As automatic visual category recognition in images\nis approaching human performance, the high level understanding in the dynamic\nspatiotemporal domain of videos and its translation into natural language is\nstill far from being solved. While most works on vision-to-text translations\nuse pre-learned or pre-established computational linguistic models, in this\npaper we present an approach that uses vision alone to efficiently learn how to\ntranslate into language the video content. We discover, in simple form, the\nstory played by main actors, while using only visual cues for representing\nobjects and their interactions. Our method learns in a hierarchical manner\nhigher level representations for recognizing subjects, actions and objects\ninvolved, their relevant contextual background and their interaction to one\nanother over time. We have a three stage approach: first we take in\nconsideration features of the individual entities at the local level of\nappearance, then we consider the relationship between these objects and actions\nand their video background, and third, we consider their spatiotemporal\nrelations as inputs to classifiers at the highest level of interpretation.\nThus, our approach finds a coherent linguistic description of videos in the\nform of a subject, verb and object based on their role played in the overall\nvisual story learned directly from training data, without using a known\nlanguage model. We test the efficiency of our approach on a large scale dataset\ncontaining YouTube clips taken in the wild and demonstrate state-of-the-art\nperformance, often superior to current approaches that use more complex,\npre-learned linguistic knowledge.", "published": "2015-11-20 16:33:13", "link": "http://arxiv.org/abs/1511.06674v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Sequence Level Training with Recurrent Neural Networks", "abstract": "Many natural language processing applications use language models to generate\ntext. These models are typically trained to predict the next word in a\nsequence, given the previous words and some context such as an image. However,\nat test time the model is expected to generate the entire sequence from\nscratch. This discrepancy makes generation brittle, as errors may accumulate\nalong the way. We address this issue by proposing a novel sequence level\ntraining algorithm that directly optimizes the metric used at test time, such\nas BLEU or ROUGE. On three different tasks, our approach outperforms several\nstrong baselines for greedy generation. The method is also competitive when\nthese baselines employ beam search, while being several times faster.", "published": "2015-11-20 19:25:54", "link": "http://arxiv.org/abs/1511.06732v7", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Conducting sparse feature selection on arbitrarily long phrases in text\n  corpora with a focus on interpretability", "abstract": "We propose a general framework for topic-specific summarization of large text\ncorpora, and illustrate how it can be used for analysis in two quite different\ncontexts: an OSHA database of fatality and catastrophe reports (to facilitate\nsurveillance for patterns in circumstances leading to injury or death) and\nlegal decisions on workers' compensation claims (to explore relevant case law).\nOur summarization framework, built on sparse classification methods, is a\ncompromise between simple word frequency based methods currently in wide use,\nand more heavyweight, model-intensive methods such as Latent Dirichlet\nAllocation (LDA). For a particular topic of interest (e.g., mental health\ndisability, or chemical reactions), we regress a labeling of documents onto the\nhigh-dimensional counts of all the other words and phrases in the documents.\nThe resulting small set of phrases found as predictive are then harvested as\nthe summary. Using a branch-and-bound approach, this method can be extended to\nallow for phrases of arbitrary length, which allows for potentially rich\nsummarization. We discuss how focus on the purpose of the summaries can inform\nchoices of regularization parameters and model constraints. We evaluate this\ntool by comparing computational time and summary statistics of the resulting\nword lists to three other methods in the literature. We also present a new R\npackage, textreg. Overall, we argue that sparse methods have much to offer text\nanalysis, and is a branch of research that should be considered further in this\ncontext.", "published": "2015-11-20 23:39:48", "link": "http://arxiv.org/abs/1511.06798v2", "categories": ["cs.CL", "cs.IR", "stat.AP"], "primary_category": "cs.CL"}
