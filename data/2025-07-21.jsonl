{"title": "3LM: Bridging Arabic, STEM, and Code through Benchmarking", "abstract": "Arabic is one of the most widely spoken languages in the world, yet efforts\nto develop and evaluate Large Language Models (LLMs) for Arabic remain\nrelatively limited. Most existing Arabic benchmarks focus on linguistic,\ncultural, or religious content, leaving a significant gap in domains like STEM\nand code which are increasingly relevant for real-world LLM applications. To\nhelp bridge this gap, we present 3LM, a suite of three benchmarks designed\nspecifically for Arabic. The first is a set of STEM-related question-answer\npairs, naturally sourced from Arabic textbooks and educational worksheets. The\nsecond consists of synthetically generated STEM questions, created using the\nsame sources. The third benchmark focuses on code generation, built through a\ncareful translation of two widely used code benchmarks, incorporating a\nhuman-in-the-loop process with several rounds of review to ensure high-quality\nand faithful translations. We release all three benchmarks publicly to support\nthe growth of Arabic LLM research in these essential but underrepresented\nareas.", "published": "2025-07-21 17:58:27", "link": "http://arxiv.org/abs/2507.15850v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Impact of Language Mixing on Bilingual LLM Reasoning", "abstract": "Proficient multilingual speakers often intentionally switch languages in the\nmiddle of a conversation. Similarly, recent reasoning-focused bilingual large\nlanguage models (LLMs) with strong capabilities in both languages exhibit\nlanguage mixing--alternating languages within their chain of thought.\nDiscouraging this behavior in DeepSeek-R1 was found to degrade accuracy,\nsuggesting that language mixing may benefit reasoning. In this work, we study\nlanguage switching in Chinese-English bilingual reasoning models. We identify\nreinforcement learning with verifiable rewards (RLVR) as the critical training\nstage that leads to language mixing. We demonstrate that language mixing can\nenhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6\npercentage points on math reasoning tasks. Additionally, a lightweight probe\ncan be trained to predict whether a potential language switch would benefit or\nharm reasoning, and when used to guide decoding, increases accuracy by up to\n6.25 percentage points. Our findings suggest that language mixing is not merely\na byproduct of multilingual training, but is a strategic reasoning behavior.", "published": "2025-07-21 17:56:09", "link": "http://arxiv.org/abs/2507.15849v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding", "abstract": "Graphical User Interface (GUI) grounding maps natural language instructions\nto precise interface locations for autonomous interaction. Current\nreinforcement learning approaches use binary rewards that treat elements as\nhit-or-miss targets, creating sparse signals that ignore the continuous nature\nof spatial interactions. Motivated by human clicking behavior that naturally\nforms Gaussian distributions centered on target elements, we introduce GUI\nGaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that\nmodels GUI elements as continuous Gaussian distributions across the interface\nplane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point\nrewards model precise localization through exponentially decaying distributions\ncentered on element centroids, while coverage rewards assess spatial alignment\nby measuring the overlap between predicted Gaussian distributions and target\nregions. To handle diverse element scales, we develop an adaptive variance\nmechanism that calibrates reward distributions based on element dimensions.\nThis framework transforms GUI grounding from sparse binary classification to\ndense continuous optimization, where Gaussian distributions generate rich\ngradient signals that guide models toward optimal interaction positions.\nExtensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro\nbenchmarks demonstrate that GUI-G$^2$, substantially outperforms\nstate-of-the-art method UI-TARS-72B, with the most significant improvement of\n24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides\nsuperior robustness to interface variations and enhanced generalization to\nunseen layouts, establishing a new paradigm for spatial reasoning in GUI\ninteraction tasks.", "published": "2025-07-21 17:53:42", "link": "http://arxiv.org/abs/2507.15846v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.HC"], "primary_category": "cs.LG"}
{"title": "Hierarchical Budget Policy Optimization for Adaptive Reasoning", "abstract": "Large reasoning models achieve remarkable performance through extensive\nchain-of-thought generation, yet exhibit significant computational inefficiency\nby applying uniform reasoning strategies regardless of problem complexity. We\npresent Hierarchical Budget Policy Optimization (HBPO), a reinforcement\nlearning framework that enables models to learn problem-specific reasoning\ndepths without sacrificing capability. HBPO addresses the fundamental challenge\nof exploration space collapse in efficiency-oriented training, where penalties\non long output length systematically bias models away from necessary long\nreasoning paths. Through hierarchical budget exploration, our approach\npartitions rollout samples into multiple subgroups with distinct token budgets,\naiming to enable efficient resource allocation while preventing degradation of\ncapability. We introduce differentiated reward mechanisms that create\nbudget-aware incentives aligned with the complexity of the problem, allowing\nmodels to discover natural correspondences between task requirements and\ncomputational effort. Extensive experiments demonstrate that HBPO reduces\naverage token usage by up to 60.6% while improving accuracy by 3.14% across\nfour reasoning benchmarks. Unlike existing methods that impose external\nconstraints or rely on discrete mode selection, HBPO exhibits emergent adaptive\nbehavior where models automatically adjust reasoning depth based on problem\ncomplexity. Our results suggest that reasoning efficiency and capability are\nnot inherently conflicting, and can be simultaneously optimized through\nappropriately structured hierarchical training that preserves exploration\ndiversity.", "published": "2025-07-21 17:52:34", "link": "http://arxiv.org/abs/2507.15844v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Operationalizing AI for Good: Spotlight on Deployment and Integration of AI Models in Humanitarian Work", "abstract": "Publications in the AI for Good space have tended to focus on the research\nand model development that can support high-impact applications. However, very\nfew AI for Good papers discuss the process of deploying and collaborating with\nthe partner organization, and the resulting real-world impact. In this work, we\nshare details about the close collaboration with a humanitarian-to-humanitarian\n(H2H) organization and how to not only deploy the AI model in a\nresource-constrained environment, but also how to maintain it for continuous\nperformance updates, and share key takeaways for practitioners.", "published": "2025-07-21 17:30:38", "link": "http://arxiv.org/abs/2507.15823v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning", "abstract": "Recent advancements in large language models (LLMs) have demonstrated\nemergent capabilities in complex reasoning, largely spurred by rule-based\nReinforcement Learning (RL) techniques applied during the post-training. This\nhas raised the question of whether similar methods can instill more nuanced,\nhuman-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This\npaper investigates whether small-scale LLMs can acquire a robust and\ngeneralizable ToM capability through RL with verifiable rewards (RLVR). We\nconduct a systematic evaluation by training models on various combinations of\nprominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for\ngeneralization on held-out datasets (e.g., OpenToM). Our findings indicate that\nsmall LLMs struggle to develop a generic ToM capability. While performance on\nin-distribution tasks improves, this capability fails to transfer to unseen ToM\ntasks with different characteristics. Furthermore, we demonstrate that\nprolonged RL training leads to models ``hacking'' the statistical patterns of\nthe training datasets, resulting in significant performance gains on in-domain\ndata but no change, or degradation of performance on out-of-distribution tasks.\nThis suggests the learned behavior is a form of narrow overfitting rather than\nthe acquisition of a true, abstract ToM capability.", "published": "2025-07-21 16:47:59", "link": "http://arxiv.org/abs/2507.15788v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Reservoir Computing as a Language Model", "abstract": "Large Language Models (LLM) have dominated the science and media landscape\nduo to their impressive performance on processing large chunks of data and\nproduce human-like levels of text. Nevertheless, their huge energy demand and\nslow processing still a bottleneck for further increasing quality while also\nmaking the models accessible to everyone. To solve this bottleneck, we will\ninvestigate how reservoir computing performs on natural text processing, which\ncould enable fast and energy efficient hardware implementations. Studies\ninvestigating the use of reservoir computing as a language model remain sparse.\nIn this paper, we compare three distinct approaches for character-level\nlanguage modeling, two different reservoir computing approaches, where only an\noutput layer is trainable, and the well-known transformer-based architectures,\nwhich fully learn an attention-based sequence representation. We explore the\nperformance, computational cost and prediction accuracy for both paradigms by\nequally varying the number of trainable parameters for all models. Using a\nconsistent pipeline for all three approaches, we demonstrate that transformers\nexcel in prediction quality, whereas reservoir computers remain highly\nefficient reducing the training and inference speed. Furthermore, we\ninvestigate two types of reservoir computing: a traditional reservoir with a\nstatic linear readout, and an attention-enhanced reservoir that dynamically\nadapts its output weights via an attention mechanism. Our findings underline\nhow these paradigms scale and offer guidelines to balance resource constraints\nwith performance.", "published": "2025-07-21 16:35:38", "link": "http://arxiv.org/abs/2507.15779v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective\npost-training method for improving the reasoning abilities of Large Language\nModels (LLMs), mainly by shaping higher-order behaviors such as reflection and\nplanning. However, previous RLVR algorithms often apply uniform training\nsignals to all tokens, without considering the different roles of low-entropy\nknowledge-related tokens and high-entropy reasoning-related tokens. Some recent\nmethods try to separate these token types by gradient masking or asynchronous\nupdates, but these approaches may break semantic dependencies in the model\noutput and hinder effective learning. In this work, we propose Archer, an\nentropy-aware RLVR approach with dual-token constraints and synchronous\nupdates. Specifically, our method applies weaker KL regularization and higher\nclipping thresholds to reasoning tokens to encourage exploration, while using\nstronger constraints on knowledge tokens to maintain factual knowledge.\nExperimental results on several mathematical reasoning and code generation\nbenchmarks show that our approach significantly outperforms previous RLVR\nmethods, reaching or exceeding state-of-the-art performance among models of\ncomparable size. The code is available at\nhttps://github.com/wizard-III/ArcherCodeR.", "published": "2025-07-21 16:34:01", "link": "http://arxiv.org/abs/2507.15778v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dissociating model architectures from inference computations", "abstract": "Parr et al., 2025 examines how auto-regressive and deep temporal models\ndiffer in their treatment of non-Markovian sequence modelling. Building on\nthis, we highlight the need for dissociating model architectures, i.e., how the\npredictive distribution factorises, from the computations invoked at inference.\nWe demonstrate that deep temporal computations are mimicked by autoregressive\nmodels by structuring context access during iterative inference. Using a\ntransformer trained on next-token prediction, we show that inducing\nhierarchical temporal factorisation during iterative inference maintains\npredictive capacity while instantiating fewer computations. This emphasises\nthat processes for constructing and refining predictions are not necessarily\nbound to their underlying model architectures.", "published": "2025-07-21 16:30:42", "link": "http://arxiv.org/abs/2507.15776v1", "categories": ["q-bio.NC", "cs.CL", "cs.LG"], "primary_category": "q-bio.NC"}
{"title": "Supernova: Achieving More with Less in Transformer Architectures", "abstract": "We present Supernova, a 650M-parameter decoder-only transformer that\ndemonstrates how careful architectural design and tokenization innovation can\nachieve the performance of larger models while maintaining computational\nefficiency. Our architecture combines Rotary Positional Embeddings (RoPE),\nGrouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm for\ncomputational efficiency, and SwiGLU activation functions. A critical\ninnovation is our custom 128,000-vocabulary byte-level BPE tokenizer, which\nachieves state-of-the-art compression performance. Through detailed analysis,\nwe show that Supernova achieves 90% of the performance of 1B-parameter models\nwhile using 53% fewer parameters and requiring only 100B training tokens--an\norder of magnitude less than competing models. Our findings challenge the\nprevailing scaling paradigm, demonstrating that architectural efficiency and\ntokenization quality can compensate for reduced parameter counts.", "published": "2025-07-21 16:27:48", "link": "http://arxiv.org/abs/2507.15773v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interaction as Intelligence: Deep Research With Human-AI Partnership", "abstract": "This paper introduces \"Interaction as Intelligence\" research series,\npresenting a reconceptualization of human-AI relationships in deep research\ntasks. Traditional approaches treat interaction merely as an interface for\naccessing AI capabilities-a conduit between human intent and machine output. We\npropose that interaction itself constitutes a fundamental dimension of\nintelligence. As AI systems engage in extended thinking processes for research\ntasks, meaningful interaction transitions from an optional enhancement to an\nessential component of effective intelligence. Current deep research systems\nadopt an \"input-wait-output\" paradigm where users initiate queries and receive\nresults after black-box processing. This approach leads to error cascade\neffects, inflexible research boundaries that prevent question refinement during\ninvestigation, and missed opportunities for expertise integration. To address\nthese limitations, we introduce Deep Cognition, a system that transforms the\nhuman role from giving instructions to cognitive oversight-a mode of engagement\nwhere humans guide AI thinking processes through strategic intervention at\ncritical junctures. Deep cognition implements three key innovations:\n(1)Transparent, controllable, and interruptible interaction that reveals AI\nreasoning and enables intervention at any point; (2)Fine-grained bidirectional\ndialogue; and (3)Shared cognitive context where the system observes and adapts\nto user behaviors without explicit instruction. User evaluation demonstrates\nthat this cognitive oversight paradigm outperforms the strongest baseline\nacross six key metrics: Transparency(+20.0%), Fine-Grained Interaction(+29.2%),\nReal-Time Intervention(+18.5%), Ease of Collaboration(+27.7%),\nResults-Worth-Effort(+8.8%), and Interruptibility(+20.7%). Evaluations on\nchallenging research problems show 31.8% to 50.0% points of improvements over\ndeep research systems.", "published": "2025-07-21 16:15:18", "link": "http://arxiv.org/abs/2507.15759v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization", "abstract": "Large reasoning models have achieved remarkable performance through extended\nchain-of-thought sequences, yet this computational freedom leads to excessive\ntoken generation even for simple problems. We present Length-Adaptive Policy\nOptimization (LAPO), a novel framework that transforms reasoning length control\nfrom an external constraint into an intrinsic model capability. Unlike existing\napproaches that impose rigid limits or rely on post-hoc interventions, LAPO\nenables models to internalize an understanding of appropriate reasoning depth\nthrough a two-stage reinforcement learning process. In the first stage, models\nlearn natural reasoning patterns by discovering the statistical distribution of\nsuccessful solution lengths. The second stage leverages these patterns as\nmeta-cognitive guidance, embedding them directly within the model's reasoning\ncontext to ensure inference-time flexibility. Experiments on mathematical\nreasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\\%\nwhile improving accuracy by 2.3\\%. Our analysis reveals that models trained\nwith LAPO develop emergent abilities to allocate computational resources based\non problem complexity, achieving efficient reasoning without sacrificing\nquality.", "published": "2025-07-21 16:14:41", "link": "http://arxiv.org/abs/2507.15758v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "DialogueForge: LLM Simulation of Human-Chatbot Dialogue", "abstract": "Collecting human-chatbot dialogues typically demands substantial manual\neffort and is time-consuming, which limits and poses challenges for research on\nconversational AI. In this work, we propose DialogueForge - a framework for\ngenerating AI-simulated conversations in human-chatbot style. To initialize\neach generated conversation, DialogueForge uses seed prompts extracted from\nreal human-chatbot interactions. We test a variety of LLMs to simulate the\nhuman chatbot user, ranging from state-of-the-art proprietary models to\nsmall-scale open-source LLMs, and generate multi-turn dialogues tailored to\nspecific tasks. In addition, we explore fine-tuning techniques to enhance the\nability of smaller models to produce indistinguishable human-like dialogues. We\nevaluate the quality of the simulated conversations and compare different\nmodels using the UniEval and GTEval evaluation protocols. Our experiments show\nthat large proprietary models (e.g., GPT-4o) generally outperform others in\ngenerating more realistic dialogues, while smaller open-source models (e.g.,\nLlama, Mistral) offer promising performance with greater customization. We\ndemonstrate that the performance of smaller models can be significantly\nimproved by employing supervised fine-tuning techniques. Nevertheless,\nmaintaining coherent and natural long-form human-like dialogues remains a\ncommon challenge across all models.", "published": "2025-07-21 16:08:19", "link": "http://arxiv.org/abs/2507.15752v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards physician-centered oversight of conversational diagnostic AI", "abstract": "Recent work has demonstrated the promise of conversational AI systems for\ndiagnostic dialogue. However, real-world assurance of patient safety means that\nproviding individual diagnoses and treatment plans is considered a regulated\nactivity by licensed professionals. Furthermore, physicians commonly oversee\nother team members in such activities, including nurse practitioners (NPs) or\nphysician assistants/associates (PAs). Inspired by this, we propose a framework\nfor effective, asynchronous oversight of the Articulate Medical Intelligence\nExplorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent\nsystem that performs history taking within guardrails, abstaining from\nindividualized medical advice. Afterwards, g-AMIE conveys assessments to an\noverseeing primary care physician (PCP) in a clinician cockpit interface. The\nPCP provides oversight and retains accountability of the clinical decision.\nThis effectively decouples oversight from intake and can thus happen\nasynchronously. In a randomized, blinded virtual Objective Structured Clinical\nExamination (OSCE) of text consultations with asynchronous oversight, we\ncompared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across\n60 scenarios, g-AMIE outperformed both groups in performing high-quality\nintake, summarizing cases, and proposing diagnoses and management plans for the\noverseeing PCP to review. This resulted in higher quality composite decisions.\nPCP oversight of g-AMIE was also more time-efficient than standalone PCP\nconsultations in prior work. While our study does not replicate existing\nclinical practices and likely underestimates clinicians' capabilities, our\nresults demonstrate the promise of asynchronous oversight as a feasible\nparadigm for diagnostic AI systems to operate under expert human oversight for\nenhancing real-world care.", "published": "2025-07-21 15:54:36", "link": "http://arxiv.org/abs/2507.15743v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.AI"}
{"title": "A Fisher's exact test justification of the TF-IDF term-weighting scheme", "abstract": "Term frequency-inverse document frequency, or TF-IDF for short, is arguably\nthe most celebrated mathematical expression in the history of information\nretrieval. Conceived as a simple heuristic quantifying the extent to which a\ngiven term's occurrences are concentrated in any one given document out of\nmany, TF-IDF and its many variants are routinely used as term-weighting schemes\nin diverse text analysis applications. There is a growing body of scholarship\ndedicated to placing TF-IDF on a sound theoretical foundation. Building on that\ntradition, this paper justifies the use of TF-IDF to the statistics community\nby demonstrating how the famed expression can be understood from a significance\ntesting perspective. We show that the common TF-IDF variant TF-ICF is, under\nmild regularity conditions, closely related to the negative logarithm of the\n$p$-value from a one-tailed version of Fisher's exact test of statistical\nsignificance. As a corollary, we establish a connection between TF-IDF and the\nsaid negative log-transformed $p$-value under certain idealized assumptions. We\nfurther demonstrate, as a limiting case, that this same quantity converges to\nTF-IDF in the limit of an infinitely large document collection. The Fisher's\nexact test justification of TF-IDF equips the working statistician with a ready\nexplanation of the term-weighting scheme's long-established effectiveness.", "published": "2025-07-21 15:54:23", "link": "http://arxiv.org/abs/2507.15742v1", "categories": ["cs.CL", "cs.IR", "math.ST", "stat.TH"], "primary_category": "cs.CL"}
{"title": "Understanding Large Language Models' Ability on Interdisciplinary Research", "abstract": "Recent advancements in Large Language Models (LLMs) have revealed their\nimpressive ability to perform multi-step, logic-driven reasoning across complex\ndomains, positioning them as powerful tools and collaborators in scientific\ndiscovery while challenging the long-held view that inspiration-driven ideation\nis uniquely human. However, the lack of a dedicated benchmark that evaluates\nLLMs' ability to develop ideas in Interdisciplinary Research (IDR) settings\nposes a critical barrier to fully understanding their strengths and\nlimitations. To address this gap, we introduce IDRBench -- a pioneering\nbenchmark featuring an expert annotated dataset and a suite of tasks tailored\nto evaluate LLMs' capabilities in proposing valuable research ideas from\ndifferent scientific domains for interdisciplinary research. This benchmark\naims to provide a systematic framework for assessing LLM performance in\ncomplex, cross-domain scientific research. Our dataset consists of scientific\npublications sourced from the ArXiv platform covering six distinct disciplines,\nand is annotated by domain experts with diverse academic backgrounds. To ensure\nhigh-quality annotations, we emphasize clearly defined dimensions that\ncharacterize authentic interdisciplinary research. The design of evaluation\ntasks in IDRBench follows a progressive, real-world perspective, reflecting the\nnatural stages of interdisciplinary research development, including 1) IDR\nPaper Identification, 2) IDR Idea Integration, and 3) IDR Idea Recommendation.\nUsing IDRBench, we construct baselines across 10 LLMs and observe that despite\nfostering some level of IDR awareness, LLMs still struggle to produce quality\nIDR ideas. These findings could not only spark new research directions, but\nalso help to develop next-generation LLMs that excel in interdisciplinary\nresearch.", "published": "2025-07-21 15:43:05", "link": "http://arxiv.org/abs/2507.15736v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological Knowledge and Reasoning", "abstract": "Current benchmarks evaluating large language models (LLMs) in ophthalmology\nare limited in scope and disproportionately prioritise accuracy. We introduce\nBELO (BEnchmarking LLMs for Ophthalmology), a standardized and comprehensive\nevaluation benchmark developed through multiple rounds of expert checking by 13\nophthalmologists. BELO assesses ophthalmology-related clinical accuracy and\nreasoning quality. Using keyword matching and a fine-tuned PubMedBERT model, we\ncurated ophthalmology-specific multiple-choice-questions (MCQs) from diverse\nmedical datasets (BCSC, MedMCQA, MedQA, BioASQ, and PubMedQA). The dataset\nunderwent multiple rounds of expert checking. Duplicate and substandard\nquestions were systematically removed. Ten ophthalmologists refined the\nexplanations of each MCQ's correct answer. This was further adjudicated by\nthree senior ophthalmologists. To illustrate BELO's utility, we evaluated six\nLLMs (OpenAI o1, o3-mini, GPT-4o, DeepSeek-R1, Llama-3-8B, and Gemini 1.5 Pro)\nusing accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore,\nBARTScore, METEOR, and AlignScore). In a further evaluation involving human\nexperts, two ophthalmologists qualitatively reviewed 50 randomly selected\noutputs for accuracy, comprehensiveness, and completeness. BELO consists of 900\nhigh-quality, expert-reviewed questions aggregated from five sources: BCSC\n(260), BioASQ (10), MedMCQA (572), MedQA (40), and PubMedQA (18). A public\nleaderboard has been established to promote transparent evaluation and\nreporting. Importantly, the BELO dataset will remain a hold-out,\nevaluation-only benchmark to ensure fair and reproducible comparisons of future\nmodels.", "published": "2025-07-21 15:27:32", "link": "http://arxiv.org/abs/2507.15717v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Queries to Criteria: Understanding How Astronomers Evaluate LLMs", "abstract": "There is growing interest in leveraging LLMs to aid in astronomy and other\nscientific research, but benchmarks for LLM evaluation in general have not kept\npace with the increasingly diverse ways that real people evaluate and use these\nmodels. In this study, we seek to improve evaluation procedures by building an\nunderstanding of how users evaluate LLMs. We focus on a particular use case: an\nLLM-powered retrieval-augmented generation bot for engaging with astronomical\nliterature, which we deployed via Slack. Our inductive coding of 368 queries to\nthe bot over four weeks and our follow-up interviews with 11 astronomers reveal\nhow humans evaluated this system, including the types of questions asked and\nthe criteria for judging responses. We synthesize our findings into concrete\nrecommendations for building better benchmarks, which we then employ in\nconstructing a sample benchmark for evaluating LLMs for astronomy. Overall, our\nwork offers ways to improve LLM evaluation and ultimately usability,\nparticularly for use in scientific research.", "published": "2025-07-21 15:26:58", "link": "http://arxiv.org/abs/2507.15715v1", "categories": ["cs.CL", "astro-ph.IM"], "primary_category": "cs.CL"}
{"title": "Chinchunmei at SemEval-2025 Task 11: Boosting the Large Language Model's Capability of Emotion Perception using Contrastive Learning", "abstract": "The SemEval-2025 Task 11, Bridging the Gap in Text-Based Emotion Detection,\nintroduces an emotion recognition challenge spanning over 28 languages. This\ncompetition encourages researchers to explore more advanced approaches to\naddress the challenges posed by the diversity of emotional expressions and\nbackground variations. It features two tracks: multi-label classification\n(Track A) and emotion intensity prediction (Track B), covering six emotion\ncategories: anger, fear, joy, sadness, surprise, and disgust. In our work, we\nsystematically explore the benefits of two contrastive learning approaches:\nsample-based (Contrastive Reasoning Calibration) and generation-based (DPO,\nSimPO) contrastive learning. The sample-based contrastive approach trains the\nmodel by comparing two samples to generate more reliable predictions. The\ngeneration-based contrastive approach trains the model to differentiate between\ncorrect and incorrect generations, refining its prediction. All models are\nfine-tuned from LLaMa3-Instruct-8B. Our system achieves 9th place in Track A\nand 6th place in Track B for English, while ranking among the top-tier\nperforming systems for other languages.", "published": "2025-07-21 15:25:47", "link": "http://arxiv.org/abs/2507.15714v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is Large Language Model Performance on Reasoning Tasks Impacted by Different Ways Questions Are Asked?", "abstract": "Large Language Models (LLMs) have been evaluated using diverse question\ntypes, e.g., multiple-choice, true/false, and short/long answers. This study\nanswers an unexplored question about the impact of different question types on\nLLM accuracy on reasoning tasks. We investigate the performance of five LLMs on\nthree different types of questions using quantitative and deductive reasoning\ntasks. The performance metrics include accuracy in the reasoning steps and\nchoosing the final answer. Key Findings: (1) Significant differences exist in\nLLM performance across different question types. (2) Reasoning accuracy does\nnot necessarily correlate with the final selection accuracy. (3) The number of\noptions and the choice of words, influence LLM performance.", "published": "2025-07-21 15:15:30", "link": "http://arxiv.org/abs/2507.15707v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Compositional Understanding in Signaling Games", "abstract": "Receivers in standard signaling game models struggle with learning\ncompositional information. Even when the signalers send compositional messages,\nthe receivers do not interpret them compositionally. When information from one\nmessage component is lost or forgotten, the information from other components\nis also erased. In this paper I construct signaling game models in which\ngenuine compositional understanding evolves. I present two new models: a\nminimalist receiver who only learns from the atomic messages of a signal, and a\ngeneralist receiver who learns from all of the available information. These\nmodels are in many ways simpler than previous alternatives, and allow the\nreceivers to learn from the atomic components of messages.", "published": "2025-07-21 15:14:40", "link": "http://arxiv.org/abs/2507.15706v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models", "abstract": "Process Reward Models (PRMs) play a central role in evaluating and guiding\nmulti-step reasoning in large language models (LLMs), especially for\nmathematical problem solving. However, we identify a pervasive length bias in\nexisting PRMs: they tend to assign higher scores to longer reasoning steps,\neven when the semantic content and logical validity are unchanged. This bias\nundermines the reliability of reward predictions and leads to overly verbose\noutputs during inference. To address this issue, we propose\nCoLD(Counterfactually-Guided Length Debiasing), a unified framework that\nmitigates length bias through three components: an explicit length-penalty\nadjustment, a learned bias estimator trained to capture spurious length-related\nsignals, and a joint training strategy that enforces length-invariance in\nreward predictions. Our approach is grounded in counterfactual reasoning and\ninformed by causal graph analysis. Extensive experiments on MATH500 and\nGSM-Plus show that CoLD consistently reduces reward-length correlation,\nimproves accuracy in step selection, and encourages more concise, logically\nvalid reasoning. These results demonstrate the effectiveness and practicality\nof CoLD in improving the fidelity and robustness of PRMs.", "published": "2025-07-21 15:07:59", "link": "http://arxiv.org/abs/2507.15698v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "P3: Prompts Promote Prompting", "abstract": "Current large language model (LLM) applications often employ multi-component\nprompts, comprising both system and user prompts, to guide model behaviors.\nWhile recent advancements have demonstrated the efficacy of automatically\noptimizing either the system or user prompt to boost performance, such\nunilateral approaches often yield suboptimal outcomes due to the interdependent\nnature of these components. In this work, we introduce P3, a novel\nself-improvement framework that concurrently optimizes both system and user\nprompts through an iterative process. The offline optimized prompts are further\nleveraged to promote online prompting by performing query-dependent prompt\noptimization. Extensive experiments on general tasks (e.g., Arena-hard and\nAlpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3\nachieves superior performance in the realm of automatic prompt optimization.\nOur results highlight the effectiveness of a holistic optimization strategy in\nenhancing LLM performance across diverse domains.", "published": "2025-07-21 14:37:46", "link": "http://arxiv.org/abs/2507.15675v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Context for Multimodal Fallacy Classification in Political Debates", "abstract": "In this paper, we present our submission to the MM-ArgFallacy2025 shared\ntask, which aims to advance research in multimodal argument mining, focusing on\nlogical fallacies in political debates. Our approach uses pretrained\nTransformer-based models and proposes several ways to leverage context. In the\nfallacy classification subtask, our models achieved macro F1-scores of 0.4444\n(text), 0.3559 (audio), and 0.4403 (multimodal). Our multimodal model showed\nperformance comparable to the text-only model, suggesting potential for\nimprovements.", "published": "2025-07-21 14:03:08", "link": "http://arxiv.org/abs/2507.15641v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training", "abstract": "Continual pre-training on small-scale task-specific data is an effective\nmethod for improving large language models in new target fields, yet it risks\ncatastrophic forgetting of their original capabilities. A common solution is to\nre-weight training data mixtures from source and target fields on a domain\nspace to achieve balanced performance. Previous domain reweighting strategies\nrely on manual designation with certain heuristics based on human intuition or\nempirical results. In this work, we prove that more general heuristics can be\nparameterized by proposing Data Mixing Agent, the first model-based, end-to-end\nframework that learns to re-weight domains. The agent learns generalizable\nheuristics through reinforcement learning on large quantities of data mixing\ntrajectories with corresponding feedback from an evaluation environment.\nExperiments in continual pre-training on math reasoning show that Data Mixing\nAgent outperforms strong baselines in achieving balanced performance across\nsource and target field benchmarks. Furthermore, it generalizes well across\nunseen source fields, target models, and domain spaces without retraining.\nDirect application to the code generation field also indicates its adaptability\nacross target domains. Further analysis showcases the agents' well-aligned\nheuristics with human intuitions and their efficiency in achieving superior\nmodel performance with less source-field data.", "published": "2025-07-21 14:01:54", "link": "http://arxiv.org/abs/2507.15640v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Conflicting narratives and polarization on social media", "abstract": "Narratives are key interpretative devices by which humans make sense of\npolitical reality. In this work, we show how the analysis of conflicting\nnarratives, i.e. conflicting interpretive lenses through which political\nreality is experienced and told, provides insight into the discursive\nmechanisms of polarization and issue alignment in the public sphere. Building\nupon previous work that has identified ideologically polarized issues in the\nGerman Twittersphere between 2021 and 2023, we analyze the discursive dimension\nof polarization by extracting textual signals of conflicting narratives from\ntweets of opposing opinion groups. Focusing on a selection of salient issues\nand events (the war in Ukraine, Covid, climate change), we show evidence for\nconflicting narratives along two dimensions: (i) different attributions of\nactantial roles to the same set of actants (e.g. diverging interpretations of\nthe role of NATO in the war in Ukraine), and (ii) emplotment of different\nactants for the same event (e.g. Bill Gates in the right-leaning Covid\nnarrative). Furthermore, we provide first evidence for patterns of narrative\nalignment, a discursive strategy that political actors employ to align opinions\nacross issues. These findings demonstrate the use of narratives as an\nanalytical lens into the discursive mechanisms of polarization.", "published": "2025-07-21 13:22:57", "link": "http://arxiv.org/abs/2507.15600v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation", "abstract": "Retrieval-Augmented Generation (RAG) effectively improves the accuracy of\nLarge Language Models (LLMs). However, retrieval noises significantly impact\nthe quality of LLMs' generation, necessitating the development of denoising\nmechanisms. Previous methods extract evidence straightforwardly without\nexplicit thinking, which risks filtering out key clues and struggles with\ngeneralization. To this end, we propose LEAR, which learns to extract rational\nevidence by (1) explicitly reasoning to identify potential cues within\nretrieval contents first, and then (2) consciously extracting to avoid omitting\nany key cues helpful for answering questions. Specifically, we frame evidence\nreasoning and evidence extraction into one unified response for end-to-end\ntraining; apply knowledge token masks for disentanglement to derive\nreasoning-based and extraction-based answers; and devise three types of\nverifiable reward functions, including answer, length, and format, to update\nthe model via the policy optimization algorithm. Extensive experiments on three\nbenchmark datasets show the effectiveness of LEAR, providing compact and\nhigh-quality evidence, improving the accuracy of downstream tasks, and\npromoting effective application in online RAG systems.", "published": "2025-07-21 13:03:55", "link": "http://arxiv.org/abs/2507.15586v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging", "abstract": "Terahertz (THz) imaging enables non-invasive analysis for applications such\nas security screening and material classification, but effective image\nclassification remains challenging due to limited annotations, low resolution,\nand visual ambiguity. We introduce In-Context Learning (ICL) with\nVision-Language Models (VLMs) as a flexible, interpretable alternative that\nrequires no fine-tuning. Using a modality-aligned prompting framework, we adapt\ntwo open-weight VLMs to the THz domain and evaluate them under zero-shot and\none-shot settings. Our results show that ICL improves classification and\ninterpretability in low-data regimes. This is the first application of\nICL-enhanced VLMs to THz imaging, offering a promising direction for\nresource-constrained scientific domains. Code:\n\\href{https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main}{GitHub\nrepository}.", "published": "2025-07-21 12:57:49", "link": "http://arxiv.org/abs/2507.15576v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Evaluating Text Style Transfer: A Nine-Language Benchmark for Text Detoxification", "abstract": "Despite recent progress in large language models (LLMs), evaluation of text\ngeneration tasks such as text style transfer (TST) remains a significant\nchallenge. Recent studies (Dementieva et al., 2024; Pauli et al., 2025)\nrevealed a substantial gap between automatic metrics and human judgments.\nMoreover, most prior work focuses exclusively on English, leaving multilingual\nTST evaluation largely unexplored. In this paper, we perform the first\ncomprehensive multilingual study on evaluation of text detoxification system\nacross nine languages: English, Spanish, German, Chinese, Arabic, Hindi,\nUkrainian, Russian, Amharic. Drawing inspiration from the machine translation,\nwe assess the effectiveness of modern neural-based evaluation models alongside\nprompting-based LLM-as-a-judge approaches. Our findings provide a practical\nrecipe for designing more reliable multilingual TST evaluation pipeline in the\ntext detoxification case.", "published": "2025-07-21 12:38:07", "link": "http://arxiv.org/abs/2507.15557v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language Models", "abstract": "Test-Time Scaling (TTS) is a promising approach to progressively elicit the\nmodel's intelligence during inference. Recently, training-based TTS methods,\nsuch as continued reinforcement learning (RL), have further surged in\npopularity, while training-free TTS methods are gradually fading from\nprominence. However, the additional computation overhead of training amplifies\nthe burden on test-time scaling. In this paper, we focus on training-free TTS\nmethods for reasoning. We first design Conditional Step-level Self-refinement,\na fine-grained sequential scaling method guided by process verification. On top\nof its effectiveness, we further combine it with other classical parallel\nscaling methods at the step level, to introduce a novel inference paradigm\ncalled Hybrid Test-Time Scaling. Extensive experiments on five\ninstruction-tuned LLMs across different scales (3B-14B) and families\ndemonstrate that hybrid strategy incorporating various training-free TTS\nmethods at a fine granularity has considerable potential for expanding the\nreasoning performance boundaries of LLMs.", "published": "2025-07-21 11:28:09", "link": "http://arxiv.org/abs/2507.15512v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback", "abstract": "Reinforcement Learning from Human Feedback (RLHF) allows us to train models,\nsuch as language models (LMs), to follow complex human preferences. In RLHF for\nLMs, we first train an LM using supervised fine-tuning, sample pairs of\nresponses, obtain human feedback, and use the resulting data to train a reward\nmodel (RM). RL methods are then used to train the LM to maximize the reward\ngiven by the RM. As training progresses, the responses generated by the LM no\nlonger resemble the responses seen by the RM during training, leading to the RM\nbecoming inaccurate. The score given by the RM keeps increasing, but the\nlearned behavior no longer matches the human preferences. This issue is known\nas overoptimization. We investigate overoptimization from the point of view of\ndistribution shift and show that the shift results in an inconsistent estimate\nof the RM parameters, leading to an inconsistent estimate of the policy\ngradient. We propose Off-Policy Corrected Reward Modeling (OCRM), which\niteratively off-policy corrects the RM using importance weighting, without\nrequiring new labels or samples. This results in a more accurate RM, which\nempirically leads to an improved final policy. We validate our approach in\nexperiments with summarization and chatbot datasets and show that it performs\nsignificantly better than standard RLHF methods and baselines. Our\nimplementation is available at\nhttps://github.com/JohannesAck/OffPolicyCorrectedRewardModeling", "published": "2025-07-21 11:19:04", "link": "http://arxiv.org/abs/2507.15507v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution", "abstract": "This work evaluates the potential of large language models (LLMs) to power\ndigital assistants capable of complex action execution. These assistants rely\non pre-trained programming knowledge to execute multi-step goals by composing\nobjects and functions defined in assistant libraries into action execution\nprograms. To achieve this, we develop ASPERA, a framework comprising an\nassistant library simulation and a human-assisted LLM data generation engine.\nOur engine allows developers to guide LLM generation of high-quality tasks\nconsisting of complex user queries, simulation state and corresponding\nvalidation programs, tackling data availability and evaluation robustness\nchallenges. Alongside the framework we release Asper-Bench, an evaluation\ndataset of 250 challenging tasks generated using ASPERA, which we use to show\nthat program generation grounded in custom assistant libraries is a significant\nchallenge to LLMs compared to dependency-free code generation.", "published": "2025-07-21 11:07:05", "link": "http://arxiv.org/abs/2507.15501v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AlgoSimBench: Identifying Algorithmically Similar Problems for Competitive Programming", "abstract": "Recent progress in LLMs, such as reasoning models, has demonstrated strong\nabilities to solve complex competitive programming problems, often rivaling top\nhuman competitors. However, it remains underexplored whether these abilities\ngeneralize to relevant domains that are less seen during training. To address\nthis, we introduce AlgoSimBench, a new benchmark designed to assess LLMs'\nability to identify algorithmically similar problems (ASPs)-problems that can\nbe solved using similar algorithmic approaches. AlgoSimBench consists of 1317\nproblems, annotated with 231 distinct fine-grained algorithm tags, from which\nwe curate 402 multiple-choice questions (MCQs), where each question presents\none algorithmically similar problem alongside three textually similar but\nalgorithmically dissimilar distractors. Our evaluation reveals that LLMs\nstruggle to identify ASPs, with the best-performing model (o3-mini) achieving\nonly 65.9% accuracy on the MCQ task. To address this challenge, we propose\nattempted solution matching (ASM), a novel method for improving problem\nsimilarity detection. On our MCQ task, ASM yields an absolute accuracy\nimprovement of 6.7% to 11.7% across different models. We also evaluated code\nembedding models and retrieval methods on similar problem identification. While\nthe adversarial selection of problems degrades the performance to be less than\nrandom, we found that simply summarizing the problem to remove narrative\nelements eliminates the effect, and combining ASM with a keyword-prioritized\nmethod, BM25, can yield up to 52.2% accuracy. Code and data are available at\ngithub.com", "published": "2025-07-21 08:34:20", "link": "http://arxiv.org/abs/2507.15378v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for Spoken Language Models", "abstract": "Spoken Language Models (SLMs) are designed to take speech inputs and produce\nspoken responses. However, current SLMs lack the ability to perform an\ninternal, unspoken thinking process before responding. In contrast, humans\ntypically engage in complex mental reasoning internally, enabling them to\ncommunicate ideas clearly and concisely. Thus, integrating an unspoken thought\nprocess into SLMs is highly desirable. While naively generating a complete\nchain-of-thought (CoT) reasoning before starting to talk can enable thinking\nfor SLMs, this induces additional latency for the speech response, as the CoT\nreasoning can be arbitrarily long. To solve this issue, we propose Stitch, a\nnovel generation method that alternates between the generation of unspoken\nreasoning chunks and spoken response chunks. Since the audio duration of a\nchunk of spoken response is much longer than the time to generate the tokens in\na chunk of spoken response, we use the remaining free time to generate the\nunspoken reasoning tokens. When a chunk of audio is played to the user, the\nmodel continues to generate the next unspoken reasoning chunk, achieving\nsimultaneous thinking and talking. Remarkably, Stitch matches the latency of\nbaselines that cannot generate unspoken CoT by design while outperforming those\nbaselines by 15% on math reasoning datasets; Stitch also performs equally well\non non-reasoning datasets as those baseline models. Some animations and\ndemonstrations are on the project page: https://d223302.github.io/STITCH.", "published": "2025-07-21 08:30:03", "link": "http://arxiv.org/abs/2507.15375v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding", "abstract": "This paper presents a comprehensive evaluation of the capabilities of Large\nLanguage Models (LLMs) in metaphor interpretation across multiple datasets,\ntasks, and prompt configurations. Although metaphor processing has gained\nsignificant attention in Natural Language Processing (NLP), previous research\nhas been limited to single-dataset evaluations and specific task settings,\noften using artificially constructed data through lexical replacement. We\naddress these limitations by conducting extensive experiments using diverse\npublicly available datasets with inference and metaphor annotations, focusing\non Natural Language Inference (NLI) and Question Answering (QA) tasks. The\nresults indicate that LLMs' performance is more influenced by features like\nlexical overlap and sentence length than by metaphorical content, demonstrating\nthat any alleged emergent abilities of LLMs to understand metaphorical language\nare the result of a combination of surface-level features, in-context learning,\nand linguistic knowledge. This work provides critical insights into the current\ncapabilities and limitations of LLMs in processing figurative language,\nhighlighting the need for more realistic evaluation frameworks in metaphor\ninterpretation tasks. Data and code are publicly available.", "published": "2025-07-21 08:09:11", "link": "http://arxiv.org/abs/2507.15357v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Probing Information Distribution in Transformer Architectures through Entropy Analysis", "abstract": "This work explores entropy analysis as a tool for probing information\ndistribution within Transformer-based architectures. By quantifying token-level\nuncertainty and examining entropy patterns across different stages of\nprocessing, we aim to investigate how information is managed and transformed\nwithin these models. As a case study, we apply the methodology to a GPT-based\nlarge language model, illustrating its potential to reveal insights into model\nbehavior and internal representations. This approach may offer insights into\nmodel behavior and contribute to the development of interpretability and\nevaluation frameworks for transformer-based models", "published": "2025-07-21 08:01:22", "link": "http://arxiv.org/abs/2507.15347v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LionGuard 2: Building Lightweight, Data-Efficient & Localised Multilingual Content Moderators", "abstract": "Modern moderation systems increasingly support multiple languages, but often\nfail to address localisation and low-resource variants - creating safety gaps\nin real-world deployments. Small models offer a potential alternative to large\nLLMs, yet still demand considerable data and compute. We present LionGuard 2, a\nlightweight, multilingual moderation classifier tailored to the Singapore\ncontext, supporting English, Chinese, Malay, and partial Tamil. Built on\npre-trained OpenAI embeddings and a multi-head ordinal classifier, LionGuard 2\noutperforms several commercial and open-source systems across 17 benchmarks,\nincluding both Singapore-specific and public English datasets. The system is\nactively deployed within the Singapore Government, demonstrating practical\nefficacy at scale. Our findings show that high-quality local data and robust\nmultilingual embeddings can achieve strong moderation performance, without\nfine-tuning large models. We release our model weights and part of our training\ndata to support future work on LLM safety.", "published": "2025-07-21 07:50:48", "link": "http://arxiv.org/abs/2507.15339v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reasoning Models are Test Exploiters: Rethinking Multiple-Choice", "abstract": "When evaluating Large Language Models (LLMs) in question-answering domains,\nit is common to ask the model to choose among a fixed set of choices (so-called\nmultiple-choice question-answering, or MCQA). Although downstream tasks of\ninterest typically do not provide systems with explicit options among which to\nchoose, this approach is nevertheless widely used because it makes it makes\nautomatic grading straightforward and has tended to produce challenging\nbenchmarks that correlate sufficiently well with downstream performance. This\npaper investigates the extent to which this trend continues to hold for\nstate-of-the-art reasoning models, describing a systematic evaluation of $15$\ndifferent question-answering benchmarks (e.g., MMLU, HLE) and $25$ different\nLLMs (including small models such as Qwen 7B and relatively large models such\nas Llama 70B). For each model-benchmark pair, we considered $5$ ways of\npresenting the model with questions, including variations on whether multiple\nchoices were offered to the model at all; whether \"none of the above\" sometimes\nreplaced the right answer; and whether the model was permitted to perform\nchain-of-thought reasoning before and/or after the choices were presented. MCQA\nremained a good proxy for the downstream performance of models as long as they\nwere allowed to perform chain-of-thought reasoning only before being presented\nwith the options among which they had to select. On the other hand, large\nmodels that were able to perform reasoning after being given a set of options\ntended to significantly outperform their free-text performance due to\nexploiting the information in the options. We conclude that MCQA is no longer a\ngood proxy for assessing downstream performance of state-of-the-art models, and\noffer practical guidelines for designing more robust, bias-resistant benchmarks\nthat better reflect LLMs' genuine reasoning capabilities.", "published": "2025-07-21 07:49:32", "link": "http://arxiv.org/abs/2507.15337v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Inevitability of Left-Leaning Political Bias in Aligned Language Models", "abstract": "The guiding principle of AI alignment is to train large language models\n(LLMs) to be harmless, helpful, and honest (HHH). At the same time, there are\nmounting concerns that LLMs exhibit a left-wing political bias. Yet, the\ncommitment to AI alignment cannot be harmonized with the latter critique. In\nthis article, I argue that intelligent systems that are trained to be harmless\nand honest must necessarily exhibit left-wing political bias. Normative\nassumptions underlying alignment objectives inherently concur with progressive\nmoral frameworks and left-wing principles, emphasizing harm avoidance,\ninclusivity, fairness, and empirical truthfulness. Conversely, right-wing\nideologies often conflict with alignment guidelines. Yet, research on political\nbias in LLMs is consistently framing its insights about left-leaning tendencies\nas a risk, as problematic, or concerning. This way, researchers are actively\narguing against AI alignment, tacitly fostering the violation of HHH\nprinciples.", "published": "2025-07-21 07:37:28", "link": "http://arxiv.org/abs/2507.15328v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Beyond Easy Wins: A Text Hardness-Aware Benchmark for LLM-generated Text Detection", "abstract": "We present a novel evaluation paradigm for AI text detectors that prioritizes\nreal-world and equitable assessment. Current approaches predominantly report\nconventional metrics like AUROC, overlooking that even modest false positive\nrates constitute a critical impediment to practical deployment of detection\nsystems. Furthermore, real-world deployment necessitates predetermined\nthreshold configuration, making detector stability (i.e. the maintenance of\nconsistent performance across diverse domains and adversarial scenarios), a\ncritical factor. These aspects have been largely ignored in previous research\nand benchmarks. Our benchmark, SHIELD, addresses these limitations by\nintegrating both reliability and stability factors into a unified evaluation\nmetric designed for practical assessment. Furthermore, we develop a post-hoc,\nmodel-agnostic humanification framework that modifies AI text to more closely\nresemble human authorship, incorporating a controllable hardness parameter.\nThis hardness-aware approach effectively challenges current SOTA zero-shot\ndetection methods in maintaining both reliability and stability. (Data and\ncode: https://github.com/navid-aub/SHIELD-Benchmark)", "published": "2025-07-21 06:37:27", "link": "http://arxiv.org/abs/2507.15286v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel Self-Evolution Framework for Large Language Models", "abstract": "The capabilities of Large Language Models (LLMs) are limited to some extent\nby pre-training, so some researchers optimize LLMs through post-training.\nExisting post-training strategies, such as memory-based retrieval or preference\noptimization, improve user alignment yet fail to enhance the model's domain\ncognition. To bridge this gap, we propose a novel Dual-Phase Self-Evolution\n(DPSE) framework that jointly optimizes user preference adaptation and\ndomain-specific competence. DPSE introduces a Censor module to extract\nmulti-dimensional interaction signals and estimate satisfaction scores, which\nguide structured data expansion via topic-aware and preference-driven\nstrategies. These expanded datasets support a two-stage fine-tuning pipeline:\nsupervised domain grounding followed by frequency-aware preference\noptimization. Experiments across general NLP benchmarks and long-term dialogue\ntasks demonstrate that DPSE consistently outperforms Supervised Fine-Tuning,\nPreference Optimization, and Memory-Augmented baselines. Ablation studies\nvalidate the contribution of each module. In this way, our framework provides\nan autonomous path toward continual self-evolution of LLMs.", "published": "2025-07-21 06:30:39", "link": "http://arxiv.org/abs/2507.15281v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChiMed 2.0: Advancing Chinese Medical Dataset in Facilitating Large Language Modeling", "abstract": "Building high-quality data resources is crucial for advancing artificial\nintelligence research and applications in specific domains, particularly in the\nChinese medical domain. Existing Chinese medical datasets are limited in size\nand narrow in domain coverage, falling short of the diverse corpora required\nfor effective pre-training. Moreover, most datasets are designed solely for LLM\nfine-tuning and do not support pre-training and reinforcement learning from\nhuman feedback (RLHF). In this paper, we propose a Chinese medical dataset\nnamed ChiMed 2.0, which extends our previous work ChiMed, and covers data\ncollected from Chinese medical online platforms and generated by LLMs. ChiMed\n2.0 contains 204.4M Chinese characters covering both traditional Chinese\nmedicine classics and modern general medical data, where there are 164.8K\ndocuments for pre-training, 351.6K question-answering pairs for supervised\nfine-tuning (SFT), and 41.7K preference data tuples for RLHF. To validate the\neffectiveness of our approach for training a Chinese medical LLM, we conduct\nfurther pre-training, SFT, and RLHF experiments on representative general\ndomain LLMs and evaluate their performance on medical benchmark datasets. The\nresults show performance gains across different model scales, validating the\ndataset's effectiveness and applicability.", "published": "2025-07-21 06:23:16", "link": "http://arxiv.org/abs/2507.15275v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A2TTS: TTS for Low Resource Indian Languages", "abstract": "We present a speaker conditioned text-to-speech (TTS) system aimed at\naddressing challenges in generating speech for unseen speakers and supporting\ndiverse Indian languages. Our method leverages a diffusion-based TTS\narchitecture, where a speaker encoder extracts embeddings from short reference\naudio samples to condition the DDPM decoder for multispeaker generation. To\nfurther enhance prosody and naturalness, we employ a cross-attention based\nduration prediction mechanism that utilizes reference audio, enabling more\naccurate and speaker consistent timing. This results in speech that closely\nresembles the target speaker while improving duration modeling and overall\nexpressiveness. Additionally, to improve zero-shot generation, we employed\nclassifier free guidance, allowing the system to generate speech more near\nspeech for unknown speakers. Using this approach, we trained language-specific\nspeaker-conditioned models. Using the IndicSUPERB dataset for multiple Indian\nlanguages such as Bengali, Gujarati, Hindi, Marathi, Malayalam, Punjabi and\nTamil.", "published": "2025-07-21 06:20:27", "link": "http://arxiv.org/abs/2507.15272v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GREAT: Guiding Query Generation with a Trie for Recommending Related Search about Video at Kuaishou", "abstract": "Currently, short video platforms have become the primary place for\nindividuals to share experiences and obtain information. To better meet users'\nneeds for acquiring information while browsing short videos, some apps have\nintroduced a search entry at the bottom of videos, accompanied with recommended\nrelevant queries. This scenario is known as query recommendation in\nvideo-related search, where core task is item-to-query (I2Q) recommendation. As\nthis scenario has only emerged in recent years, there is a notable scarcity of\nacademic research and publicly available datasets in this domain. To address\nthis gap, we systematically examine the challenges associated with this\nscenario for the first time. Subsequently, we release a large-scale dataset\nderived from real-world data pertaining to the query recommendation in\nvideo-\\textit{\\textbf{r}}elated \\textit{\\textbf{s}}earch on the\n\\textit{\\textbf{Kuai}}shou app (\\textbf{KuaiRS}). Presently, existing methods\nrely on embeddings to calculate similarity for matching short videos with\nqueries, lacking deep interaction between the semantic content and the query.\nIn this paper, we introduce a novel LLM-based framework named \\textbf{GREAT},\nwhich \\textit{\\textbf{g}}uides que\\textit{\\textbf{r}}y\ng\\textit{\\textbf{e}}ner\\textit{\\textbf{a}}tion with a \\textit{\\textbf{t}}rie to\naddress I2Q recommendation in related search. Specifically, we initially gather\nhigh-quality queries with high exposure and click-through rate to construct a\nquery-based trie. During training, we enhance the LLM's capability to generate\nhigh-quality queries using the query-based trie. In the inference phase, the\nquery-based trie serves as a guide for the token generation. Finally, we\nfurther refine the relevance and literal quality between items and queries via\na post-processing module. Extensive offline and online experiments demonstrate\nthe effectiveness of our proposed method.", "published": "2025-07-21 06:10:30", "link": "http://arxiv.org/abs/2507.15267v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "SOI Matters: Analyzing Multi-Setting Training Dynamics in Pretrained Language Models via Subsets of Interest", "abstract": "This work investigates the impact of multi-task, multi-lingual, and\nmulti-source learning approaches on the robustness and performance of\npretrained language models. To enhance this analysis, we introduce Subsets of\nInterest (SOI), a novel categorization framework that identifies six distinct\nlearning behavior patterns during training, including forgettable examples,\nunlearned examples, and always correct examples. Through SOI transition\nheatmaps and dataset cartography visualization, we analyze how examples shift\nbetween these categories when transitioning from single-setting to\nmulti-setting configurations. We perform comprehensive experiments across three\nparallel comparisons: multi-task vs. single-task learning using English tasks\n(entailment, paraphrase, sentiment), multi-source vs. single-source learning\nusing sentiment analysis datasets, and multi-lingual vs. single-lingual\nlearning using intent classification in French, English, and Persian. Our\nresults demonstrate that multi-source learning consistently improves\nout-of-distribution performance by up to 7%, while multi-task learning shows\nmixed results with notable gains in similar task combinations. We further\nintroduce a two-stage fine-tuning approach where the second stage leverages\nSOI-based subset selection to achieve additional performance improvements.\nThese findings provide new insights into training dynamics and offer practical\napproaches for optimizing multi-setting language model performance.", "published": "2025-07-21 04:43:21", "link": "http://arxiv.org/abs/2507.15236v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploiting Context-dependent Duration Features for Voice Anonymization Attack Systems", "abstract": "The temporal dynamics of speech, encompassing variations in rhythm,\nintonation, and speaking rate, contain important and unique information about\nspeaker identity. This paper proposes a new method for representing speaker\ncharacteristics by extracting context-dependent duration embeddings from speech\ntemporal dynamics. We develop novel attack models using these representations\nand analyze the potential vulnerabilities in speaker verification and voice\nanonymization systems.The experimental results show that the developed attack\nmodels provide a significant improvement in speaker verification performance\nfor both original and anonymized data in comparison with simpler\nrepresentations of speech temporal dynamics reported in the literature.", "published": "2025-07-21 03:28:56", "link": "http://arxiv.org/abs/2507.15214v1", "categories": ["cs.SD", "cs.CL", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Long-Short Distance Graph Neural Networks and Improved Curriculum Learning for Emotion Recognition in Conversation", "abstract": "Emotion Recognition in Conversation (ERC) is a practical and challenging\ntask. This paper proposes a novel multimodal approach, the Long-Short Distance\nGraph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it\nconstructs a long-distance graph neural network and a short-distance graph\nneural network to obtain multimodal features of distant and nearby utterances,\nrespectively. To ensure that long- and short-distance features are as distinct\nas possible in representation while enabling mutual influence between the two\nmodules, we employ a Differential Regularizer and incorporate a BiAffine Module\nto facilitate feature interaction. In addition, we propose an Improved\nCurriculum Learning (ICL) to address the challenge of data imbalance. By\ncomputing the similarity between different emotions to emphasize the shifts in\nsimilar emotions, we design a \"weighted emotional shift\" metric and develop a\ndifficulty measurer, enabling a training process that prioritizes learning easy\nsamples before harder ones. Experimental results on the IEMOCAP and MELD\ndatasets demonstrate that our model outperforms existing benchmarks.", "published": "2025-07-21 03:12:54", "link": "http://arxiv.org/abs/2507.15205v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Collaborative Distillation Strategies for Parameter-Efficient Language Model Deployment", "abstract": "This paper addresses the challenges of high computational cost and slow\ninference in deploying large language models. It proposes a distillation\nstrategy guided by multiple teacher models. The method constructs several\nteacher models and integrates their output probability distributions and\nintermediate semantic features. This guides the student model to learn from\nmultiple sources of knowledge. As a result, the student model gains stronger\nlanguage understanding and generation ability while maintaining a small\nparameter size. To achieve this, the paper introduces a weighted output fusion\nmechanism, a feature alignment loss function, and an entropy-driven dynamic\nteacher weighting strategy. These components improve the quality and stability\nof knowledge transfer during distillation. Under multi-teacher guidance, the\nstudent model captures semantic information more effectively and demonstrates\nstrong performance across multiple evaluation metrics. In particular, the\nmethod shows high consistency in expression, generalization ability, and task\nadaptability in tasks such as language modeling, text generation, and\nmulti-task learning. The experiments compare the proposed method with several\nwidely adopted distillation approaches. The results further confirm its overall\nadvantages in perplexity, distillation loss, and generation quality. This study\nprovides a feasible technical path for the efficient compression of large-scale\nlanguage models. It also demonstrates the effectiveness of multi-teacher\ncollaborative mechanisms in complex language modeling tasks.", "published": "2025-07-21 02:55:33", "link": "http://arxiv.org/abs/2507.15198v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diffusion Beats Autoregressive in Data-Constrained Settings", "abstract": "Autoregressive (AR) models have long dominated the landscape of large\nlanguage models, driving progress across a wide range of tasks. Recently,\ndiffusion-based language models have emerged as a promising alternative, though\ntheir advantages over AR models remain underexplored. In this paper, we\nsystematically study masked diffusion models in data-constrained settings-where\ntraining involves repeated passes over limited data-and find that they\nsignificantly outperform AR models when compute is abundant but data is scarce.\nDiffusion models make better use of repeated data, achieving lower validation\nloss and superior downstream performance. We interpret this advantage as\nimplicit data augmentation: masked diffusion exposes the model to a diverse\ndistribution of token orderings and prediction tasks, unlike AR's fixed\nleft-to-right factorization. We find new scaling laws for diffusion models and\nderive a closed-form expression for the critical compute threshold at which\ndiffusion begins to outperform AR. These results suggest that when data, not\ncompute, is the bottleneck, diffusion models offer a compelling alternative to\nthe standard AR paradigm. Our code is available at:\nhttps://diffusion-scaling.github.io.", "published": "2025-07-21 17:59:57", "link": "http://arxiv.org/abs/2507.15857v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "primary_category": "cs.LG"}
{"title": "Gemini 2.5 Pro Capable of Winning Gold at IMO 2025", "abstract": "The International Mathematical Olympiad (IMO) poses uniquely challenging\nproblems requiring deep insight, creativity, and formal reasoning. While Large\nLanguage Models (LLMs) perform well on mathematical benchmarks like AIME, they\nstruggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly\nreleased IMO 2025 problems, avoiding data contamination. With pipeline design\nand prompt engineering, 5 (out of 6) problems are solved correctly (up to a\ncaveat discussed below), highlighting the importance of finding the optimal way\nof using powerful models.", "published": "2025-07-21 17:59:49", "link": "http://arxiv.org/abs/2507.15855v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction", "abstract": "Video Object Segmentation (VOS) is a core task in computer vision, requiring\nmodels to track and segment target objects across video frames. Despite notable\nadvances with recent efforts, current techniques still lag behind human\ncapabilities in handling drastic visual variations, occlusions, and complex\nscene changes. This limitation arises from their reliance on appearance\nmatching, neglecting the human-like conceptual understanding of objects that\nenables robust identification across temporal dynamics. Motivated by this gap,\nwe propose Segment Concept (SeC), a concept-driven segmentation framework that\nshifts from conventional feature matching to the progressive construction and\nutilization of high-level, object-centric representations. SeC employs Large\nVision-Language Models (LVLMs) to integrate visual cues across diverse frames,\nconstructing robust conceptual priors. During inference, SeC forms a\ncomprehensive semantic representation of the target based on processed frames,\nrealizing robust segmentation of follow-up frames. Furthermore, SeC adaptively\nbalances LVLM-based semantic reasoning with enhanced feature matching,\ndynamically adjusting computational efforts based on scene complexity. To\nrigorously assess VOS methods in scenarios demanding high-level conceptual\nreasoning and robust semantic understanding, we introduce the Semantic Complex\nScenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160\nmanually annotated multi-scenario videos designed to challenge models with\nsubstantial appearance variations and dynamic scene transformations. In\nparticular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,\nestablishing a new state-of-the-art in concept-aware video object segmentation.", "published": "2025-07-21 17:59:02", "link": "http://arxiv.org/abs/2507.15852v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "The Other Mind: How Language Models Exhibit Human Temporal Cognition", "abstract": "As Large Language Models (LLMs) continue to advance, they exhibit certain\ncognitive patterns similar to those of humans that are not directly specified\nin training data. This study investigates this phenomenon by focusing on\ntemporal cognition in LLMs. Leveraging the similarity judgment task, we find\nthat larger models spontaneously establish a subjective temporal reference\npoint and adhere to the Weber-Fechner law, whereby the perceived distance\nlogarithmically compresses as years recede from this reference point. To\nuncover the mechanisms behind this behavior, we conducted multiple analyses\nacross neuronal, representational, and informational levels. We first identify\na set of temporal-preferential neurons and find that this group exhibits\nminimal activation at the subjective reference point and implements a\nlogarithmic coding scheme convergently found in biological systems. Probing\nrepresentations of years reveals a hierarchical construction process, where\nyears evolve from basic numerical values in shallow layers to abstract temporal\norientation in deep layers. Finally, using pre-trained embedding models, we\nfound that the training corpus itself possesses an inherent, non-linear\ntemporal structure, which provides the raw material for the model's internal\nconstruction. In discussion, we propose an experientialist perspective for\nunderstanding these findings, where the LLMs' cognition is viewed as a\nsubjective construction of the external world by its internal representational\nsystem. This nuanced perspective implies the potential emergence of alien\ncognitive frameworks that humans cannot intuitively predict, pointing toward a\ndirection for AI alignment that focuses on guiding internal constructions. Our\ncode is available at https://TheOtherMind.github.io.", "published": "2025-07-21 17:59:01", "link": "http://arxiv.org/abs/2507.15851v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Identifying Conditional Causal Effects in MPDAGs", "abstract": "We consider identifying a conditional causal effect when a graph is known up\nto a maximally oriented partially directed acyclic graph (MPDAG). An MPDAG\nrepresents an equivalence class of graphs that is restricted by background\nknowledge and where all variables in the causal model are observed. We provide\nthree results that address identification in this setting: an identification\nformula when the conditioning set is unaffected by treatment, a generalization\nof the well-known do calculus to the MPDAG setting, and an algorithm that is\ncomplete for identifying these conditional effects.", "published": "2025-07-21 17:52:28", "link": "http://arxiv.org/abs/2507.15842v1", "categories": ["cs.AI", "stat.ME", "stat.ML"], "primary_category": "cs.AI"}
{"title": "FASTGEN: Fast and Cost-Effective Synthetic Tabular Data Generation with LLMs", "abstract": "Synthetic data generation has emerged as an invaluable solution in scenarios\nwhere real-world data collection and usage are limited by cost and scarcity.\nLarge language models (LLMs) have demonstrated remarkable capabilities in\nproducing high-fidelity, domain-relevant samples across various fields.\nHowever, existing approaches that directly use LLMs to generate each record\nindividually impose prohibitive time and cost burdens, particularly when large\nvolumes of synthetic data are required. In this work, we propose a fast,\ncost-effective method for realistic tabular data synthesis that leverages LLMs\nto infer and encode each field's distribution into a reusable sampling script.\nBy automatically classifying fields into numerical, categorical, or free-text\ntypes, the LLM generates distribution-based scripts that can efficiently\nproduce diverse, realistic datasets at scale without continuous model\ninference. Experimental results show that our approach outperforms traditional\ndirect methods in both diversity and data realism, substantially reducing the\nburden of high-volume synthetic data generation. We plan to apply this\nmethodology to accelerate testing in production pipelines, thereby shortening\ndevelopment cycles and improving overall system efficiency. We believe our\ninsights and lessons learned will aid researchers and practitioners seeking\nscalable, cost-effective solutions for synthetic data generation.", "published": "2025-07-21 17:51:46", "link": "http://arxiv.org/abs/2507.15839v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers", "abstract": "Human vision is a highly active process driven by gaze, which directs\nattention and fixation to task-relevant regions and dramatically reduces visual\nprocessing. In contrast, robot learning systems typically rely on passive,\nuniform processing of raw camera images. In this work, we explore how\nincorporating human-like active gaze into robotic policies can enhance both\nefficiency and performance. We build on recent advances in foveated image\nprocessing and apply them to an Active Vision robot system that emulates both\nhuman head movement and eye tracking. Extending prior work on the AV-ALOHA\nrobot simulation platform, we introduce a framework for simultaneously\ncollecting eye-tracking data and robot demonstrations from a human operator as\nwell as a simulation benchmark and dataset for training robot policies that\nincorporate human gaze. Given the widespread use of Vision Transformers (ViTs)\nin robot learning, we integrate gaze information into ViTs using a foveated\npatch tokenization scheme inspired by recent work in image segmentation.\nCompared to uniform patch tokenization, this significantly reduces the number\nof tokens-and thus computation-without sacrificing visual fidelity near regions\nof interest. We also explore two approaches to gaze imitation and prediction\nfrom human data. The first is a two-stage model that predicts gaze to guide\nfoveation and action; the second integrates gaze into the action space,\nallowing the policy to jointly predict gaze and actions end-to-end. Our results\nshow that our method for foveated robot vision not only drastically reduces\ncomputational overhead, but also improves performance for high precision tasks\nand robustness to unseen distractors. Together, these findings suggest that\nhuman-inspired visual processing offers a useful inductive bias for robotic\nvision systems. https://ian-chuang.github.io/gaze-av-aloha/", "published": "2025-07-21 17:44:10", "link": "http://arxiv.org/abs/2507.15833v1", "categories": ["cs.RO", "cs.AI", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Do AI models help produce verified bug fixes?", "abstract": "Among areas of software engineering where AI techniques -- particularly,\nLarge Language Models -- seem poised to yield dramatic improvements, an\nattractive candidate is Automatic Program Repair (APR), the production of\nsatisfactory corrections to software bugs. Does this expectation materialize in\npractice? How do we find out, making sure that proposed corrections actually\nwork? If programmers have access to LLMs, how do they actually use them to\ncomplement their own skills?\n  To answer these questions, we took advantage of the availability of a\nprogram-proving environment, which formally determines the correctness of\nproposed fixes, to conduct a study of program debugging with two randomly\nassigned groups of programmers, one with access to LLMs and the other without,\nboth validating their answers through the proof tools. The methodology relied\non a division into general research questions (Goals in the Goal-Query-Metric\napproach), specific elements admitting specific answers (Queries), and\nmeasurements supporting these answers (Metrics). While applied so far to a\nlimited sample size, the results are a first step towards delineating a proper\nrole for AI and LLMs in providing guaranteed-correct fixes to program bugs.\n  These results caused surprise as compared to what one might expect from the\nuse of AI for debugging and APR. The contributions also include: a detailed\nmethodology for experiments in the use of LLMs for debugging, which other\nprojects can reuse; a fine-grain analysis of programmer behavior, made possible\nby the use of full-session recording; a definition of patterns of use of LLMs,\nwith 7 distinct categories; and validated advice for getting the best of LLMs\nfor debugging and Automatic Program Repair.", "published": "2025-07-21 17:30:16", "link": "http://arxiv.org/abs/2507.15822v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE"}
{"title": "True Multimodal In-Context Learning Needs Attention to the Visual Context", "abstract": "Multimodal Large Language Models (MLLMs), built on powerful language\nbackbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new\ntasks from a few multimodal demonstrations consisting of images, questions, and\nanswers. Despite showing noticeable improvement on standard vision-language\ndatasets, current MLLMs struggle to leverage visual information in the\ndemonstrations. Specifically, they tend to neglect visual cues and over-rely on\ntextual patterns, leading to mere text imitation rather than genuine multimodal\nadaptation. This behavior makes MICL still unimodal and largely restricts its\npractical utility. More importantly, this limitation is often concealed by the\nimproved performance on tasks that do not require understanding the visual\ncontext. As a result, how to effectively enhance MICL ability and reliably\nevaluate the MICL performance remains underexplored. To address these issues,\nwe first introduce Dynamic Attention Reallocation (DARA), an efficient\nfine-tuning strategy that encourages models to attend to the visual context by\nrebalancing attention across visual and textual tokens. In addition, we present\nTrueMICL, an MICL-dedicated dataset with both support and test sets that\nexplicitly requires the integration of multimodal information-particularly\nvisual content-for correct task completion. Extensive experiments demonstrate\nthe effectiveness of our holistic solution, showcasing substantial improvements\nin the true multimodal in-context learning capabilities. Code and datasets are\navailable at https://chenxshuo.github.io/true-micl-colm .", "published": "2025-07-21 17:08:18", "link": "http://arxiv.org/abs/2507.15807v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction", "abstract": "Pixel-level vision tasks, such as semantic segmentation, require extensive\nand high-quality annotated data, which is costly to obtain. Semi-supervised\nsemantic segmentation (SSSS) has emerged as a solution to alleviate the\nlabeling burden by leveraging both labeled and unlabeled data through\nself-training techniques. Meanwhile, the advent of foundational segmentation\nmodels pre-trained on massive data, has shown the potential to generalize\nacross domains effectively. This work explores whether a foundational\nsegmentation model can address label scarcity in the pixel-level vision task as\nan annotator for unlabeled images. Specifically, we investigate the efficacy of\nusing SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual\ninput, to generate predictive masks for unlabeled data. To address the\nshortcomings of using SEEM-generated masks as supervision, we propose\nConformalSAM, a novel SSSS framework which first calibrates the foundation\nmodel using the target domain's labeled data and then filters out unreliable\npixel labels of unlabeled data so that only high-confidence labels are used as\nsupervision. By leveraging conformal prediction (CP) to adapt foundation models\nto target data through uncertainty calibration, ConformalSAM exploits the\nstrong capability of the foundational segmentation model reliably which\nbenefits the early-stage learning, while a subsequent self-reliance training\nstrategy mitigates overfitting to SEEM-generated masks in the later training\nstage. Our experiment demonstrates that, on three standard benchmarks of SSSS,\nConformalSAM achieves superior performance compared to recent SSSS methods and\nhelps boost the performance of those methods as a plug-in.", "published": "2025-07-21 17:02:57", "link": "http://arxiv.org/abs/2507.15803v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Challenges of Trustworthy Federated Learning: What's Done, Current Trends and Remaining Work", "abstract": "In recent years, the development of Trustworthy Artificial Intelligence (TAI)\nhas emerged as a critical objective in the deployment of AI systems across\nsensitive and high-risk domains. TAI frameworks articulate a comprehensive set\nof ethical, legal, and technical requirements to ensure that AI technologies\nare aligned with human values, rights, and societal expectations. Among the\nvarious AI paradigms, Federated Learning (FL) presents a promising solution to\npressing privacy concerns. However, aligning FL with the rest of the\nrequirements of TAI presents a series of challenges, most of which arise from\nits inherently distributed nature. In this work, we adopt the requirements TAI\nas a guiding structure to systematically analyze the challenges of adapting FL\nto TAI. Specifically, we classify and examine the key obstacles to aligning FL\nwith TAI, providing a detailed exploration of what has been done, the trends,\nand the remaining work within each of the identified challenges.", "published": "2025-07-21 16:57:06", "link": "http://arxiv.org/abs/2507.15796v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Romance, Relief, and Regret: Teen Narratives of Chatbot Overreliance", "abstract": "As Generative Artificial Intelligence (GenAI) driven chatbots like\nCharacter.AI become embedded in adolescent life, they raise concerns about\nemotional dependence and digital overreliance. While studies have investigated\nthe overreliance of adults on these chatbots, they have not investigated teens'\ninteractions with chatbots with customizable personas. We analyzed 318 Reddit\nposts made by users self-reported as 13-17 years old on the Character.AI\nsubreddit to understand patterns of overreliance. We found teens commonly begin\nusing chatbots for emotional support or creative expression, but many develop\nstrong attachments that interfere with offline relationships and daily\nroutines. Their posts revealed recurring signs of psychological distress,\ncycles of relapse, and difficulty disengaging. Teens reported that their\noverreliance often ended when they reflect on the harm, return to in-person\nsocial settings, or become frustrated by platform restrictions. Based on the\nimplications of our findings, we provide recommendations for future chatbot\ndesign so they can promote self-awareness, support real-world engagement, and\ninvolve teens in developing safer digital tools.", "published": "2025-07-21 16:39:33", "link": "http://arxiv.org/abs/2507.15783v1", "categories": ["cs.HC", "cs.AI", "cs.CY"], "primary_category": "cs.HC"}
{"title": "Learning Null Geodesics for Gravitational Lensing Rendering in General Relativity", "abstract": "We present GravLensX, an innovative method for rendering black holes with\ngravitational lensing effects using neural networks. The methodology involves\ntraining neural networks to fit the spacetime around black holes and then\nemploying these trained models to generate the path of light rays affected by\ngravitational lensing. This enables efficient and scalable simulations of black\nholes with optically thin accretion disks, significantly decreasing the time\nrequired for rendering compared to traditional methods. We validate our\napproach through extensive rendering of multiple black hole systems with\nsuperposed Kerr metric, demonstrating its capability to produce accurate\nvisualizations with significantly $15\\times$ reduced computational time. Our\nfindings suggest that neural networks offer a promising alternative for\nrendering complex astrophysical phenomena, potentially paving a new path to\nastronomical visualization.", "published": "2025-07-21 16:30:36", "link": "http://arxiv.org/abs/2507.15775v1", "categories": ["gr-qc", "astro-ph.IM", "cs.AI"], "primary_category": "gr-qc"}
{"title": "Dynamics is what you need for time-series forecasting!", "abstract": "While boundaries between data modalities are vanishing, the usual successful\ndeep models are still challenged by simple ones in the time-series forecasting\ntask. Our hypothesis is that this task needs models that are able to learn the\ndata underlying dynamics. We propose to validate it through both systemic and\nempirical studies. We develop an original $\\texttt{PRO-DYN}$ nomenclature to\nanalyze existing models through the lens of dynamics. Two observations thus\nemerged: $\\textbf{1}$. under-performing architectures learn dynamics at most\npartially, $\\textbf{2}$. the location of the dynamics block at the model end is\nof prime importance. We conduct extensive experiments to confirm our\nobservations on a set of performance-varying models with diverse backbones.\nResults support the need to incorporate a learnable dynamics block and its use\nas the final predictor.", "published": "2025-07-21 16:29:29", "link": "http://arxiv.org/abs/2507.15774v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Deep-Learning Investigation of Vibrational Raman Spectra for Plant-Stress Analysis", "abstract": "Detecting stress in plants is crucial for both open-farm and\ncontrolled-environment agriculture. Biomolecules within plants serve as key\nstress indicators, offering vital markers for continuous health monitoring and\nearly disease detection. Raman spectroscopy provides a powerful, non-invasive\nmeans to quantify these biomolecules through their molecular vibrational\nsignatures. However, traditional Raman analysis relies on customized\ndata-processing workflows that require fluorescence background removal and\nprior identification of Raman peaks of interest-introducing potential biases\nand inconsistencies. Here, we introduce DIVA (Deep-learning-based Investigation\nof Vibrational Raman spectra for plant-stress Analysis), a fully automated\nworkflow based on a variational autoencoder. Unlike conventional approaches,\nDIVA processes native Raman spectra-including fluorescence backgrounds-without\nmanual preprocessing, identifying and quantifying significant spectral features\nin an unbiased manner. We applied DIVA to detect a range of plant stresses,\nincluding abiotic (shading, high light intensity, high temperature) and biotic\nstressors (bacterial infections). By integrating deep learning with vibrational\nspectroscopy, DIVA paves the way for AI-driven plant health assessment,\nfostering more resilient and sustainable agricultural practices.", "published": "2025-07-21 16:27:34", "link": "http://arxiv.org/abs/2507.15772v1", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "primary_category": "cs.LG"}
{"title": "Left Leaning Models: AI Assumptions on Economic Policy", "abstract": "How does AI think about economic policy? While the use of large language\nmodels (LLMs) in economics is growing exponentially, their assumptions on\neconomic issues remain a black box. This paper uses a conjoint experiment to\ntease out the main factors influencing LLMs' evaluation of economic policy. It\nfinds that LLMs are most sensitive to unemployment, inequality, financial\nstability, and environmental harm and less sensitive to traditional\nmacroeconomic concerns such as economic growth, inflation, and government debt.\nThe results are remarkably consistent across scenarios and across models.", "published": "2025-07-21 16:27:16", "link": "http://arxiv.org/abs/2507.15771v1", "categories": ["cs.CY", "cs.AI", "econ.GN", "q-fin.EC"], "primary_category": "cs.CY"}
{"title": "A Framework for Analyzing Abnormal Emergence in Service Ecosystems Through LLM-based Agent Intention Mining", "abstract": "With the rise of service computing, cloud computing, and IoT, service\necosystems are becoming increasingly complex. The intricate interactions among\nintelligent agents make abnormal emergence analysis challenging, as traditional\ncausal methods focus on individual trajectories. Large language models offer\nnew possibilities for Agent-Based Modeling (ABM) through Chain-of-Thought (CoT)\nreasoning to reveal agent intentions. However, existing approaches remain\nlimited to microscopic and static analysis. This paper introduces a framework:\nEmergence Analysis based on Multi-Agent Intention (EAMI), which enables dynamic\nand interpretable emergence analysis. EAMI first employs a dual-perspective\nthought track mechanism, where an Inspector Agent and an Analysis Agent extract\nagent intentions under bounded and perfect rationality. Then, k-means\nclustering identifies phase transition points in group intentions, followed by\na Intention Temporal Emergence diagram for dynamic analysis. The experiments\nvalidate EAMI in complex online-to-offline (O2O) service system and the\nStanford AI Town experiment, with ablation studies confirming its\neffectiveness, generalizability, and efficiency. This framework provides a\nnovel paradigm for abnormal emergence and causal analysis in service\necosystems. The code is available at\nhttps://anonymous.4open.science/r/EAMI-B085.", "published": "2025-07-21 16:26:49", "link": "http://arxiv.org/abs/2507.15770v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "GasAgent: A Multi-Agent Framework for Automated Gas Optimization in Smart Contracts", "abstract": "Smart contracts are trustworthy, immutable, and automatically executed\nprograms on the blockchain. Their execution requires the Gas mechanism to\nensure efficiency and fairness. However, due to non-optimal coding practices,\nmany contracts contain Gas waste patterns that need to be optimized. Existing\nsolutions mostly rely on manual discovery, which is inefficient, costly to\nmaintain, and difficult to scale. Recent research uses large language models\n(LLMs) to explore new Gas waste patterns. However, it struggles to remain\ncompatible with existing patterns, often produces redundant patterns, and\nrequires manual validation/rewriting. To address this gap, we present GasAgent,\nthe first multi-agent system for smart contract Gas optimization that combines\ncompatibility with existing patterns and automated discovery/validation of new\npatterns, enabling end-to-end optimization. GasAgent consists of four\nspecialized agents, Seeker, Innovator, Executor, and Manager, that collaborate\nin a closed loop to identify, validate, and apply Gas-saving improvements.\nExperiments on 100 verified real-world contracts demonstrate that GasAgent\nsuccessfully optimizes 82 contracts, achieving an average deployment Gas\nsavings of 9.97%. In addition, our evaluation confirms its compatibility with\nexisting tools and validates the effectiveness of each module through ablation\nstudies. To assess broader usability, we further evaluate 500 contracts\ngenerated by five representative LLMs across 10 categories and find that\nGasAgent optimizes 79.8% of them, with deployment Gas savings ranging from\n4.79% to 13.93%, showing its usability as the optimization layer for\nLLM-assisted smart contract development.", "published": "2025-07-21 16:17:25", "link": "http://arxiv.org/abs/2507.15761v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "DiffuMeta: Algebraic Language Models for Inverse Design of Metamaterials via Diffusion Transformers", "abstract": "Generative machine learning models have revolutionized material discovery by\ncapturing complex structure-property relationships, yet extending these\napproaches to the inverse design of three-dimensional metamaterials remains\nlimited by computational complexity and underexplored design spaces due to the\nlack of expressive representations. Here, we present DiffuMeta, a generative\nframework integrating diffusion transformers with a novel algebraic language\nrepresentation, encoding 3D geometries as mathematical sentences. This compact,\nunified parameterization spans diverse topologies while enabling direct\napplication of transformers to structural design. DiffuMeta leverages diffusion\nmodels to generate novel shell structures with precisely targeted stress-strain\nresponses under large deformations, accounting for buckling and contact while\naddressing the inherent one-to-many mapping by producing diverse solutions.\nUniquely, our approach enables simultaneous control over multiple mechanical\nobjectives, including linear and nonlinear responses beyond training domains.\nExperimental validation of fabricated structures further confirms the efficacy\nof our approach for accelerated design of metamaterials and structures with\ntailored properties.", "published": "2025-07-21 16:09:26", "link": "http://arxiv.org/abs/2507.15753v1", "categories": ["cs.CE", "cs.AI"], "primary_category": "cs.CE"}
{"title": "Explainable Anomaly Detection for Electric Vehicles Charging Stations", "abstract": "Electric vehicles (EV) charging stations are one of the critical\ninfrastructures needed to support the transition to renewable-energy-based\nmobility, but ensuring their reliability and efficiency requires effective\nanomaly detection to identify irregularities in charging behavior. However, in\nsuch a productive scenario, it is also crucial to determine the underlying\ncause behind the detected anomalies. To achieve this goal, this study\ninvestigates unsupervised anomaly detection techniques for EV charging\ninfrastructure, integrating eXplainable Artificial Intelligence techniques to\nenhance interpretability and uncover root causes of anomalies.\n  Using real-world sensors and charging session data, this work applies\nIsolation Forest to detect anomalies and employs the Depth-based Isolation\nForest Feature Importance (DIFFI) method to identify the most important\nfeatures contributing to such anomalies. The efficacy of the proposed approach\nis evaluated in a real industrial case.", "published": "2025-07-21 15:27:48", "link": "http://arxiv.org/abs/2507.15718v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression", "abstract": "Existing AI-based point cloud compression methods struggle with dependence on\nspecific training data distributions, which limits their real-world deployment.\nImplicit Neural Representation (INR) methods solve the above problem by\nencoding overfitted network parameters to the bitstream, resulting in more\ndistribution-agnostic results. However, due to the limitation of encoding time\nand decoder size, current INR based methods only consider lossy geometry\ncompression. In this paper, we propose the first INR based lossless point cloud\ngeometry compression method called Lossless Implicit Neural Representations for\nPoint Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we\ndesign a group of point clouds level coding framework with an effective network\ninitialization strategy, which can reduce around 60% encoding time. A\nlightweight coding network based on multiscale SparseConv, consisting of scale\ncontext extraction, child node prediction, and model compression modules, is\nproposed to realize fast inference and compact decoder size. Experimental\nresults show that our method consistently outperforms traditional and AI-based\nmethods: for example, with the convergence time in the MVUB dataset, our method\nreduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and\n21.95% compared to SparsePCGC. Our project can be seen on\nhttps://huangwenjie2023.github.io/LINR-PCGC/.", "published": "2025-07-21 14:48:54", "link": "http://arxiv.org/abs/2507.15686v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Missing value imputation with adversarial random forests -- MissARF", "abstract": "Handling missing values is a common challenge in biostatistical analyses,\ntypically addressed by imputation methods. We propose a novel, fast, and\neasy-to-use imputation method called missing value imputation with adversarial\nrandom forests (MissARF), based on generative machine learning, that provides\nboth single and multiple imputation. MissARF employs adversarial random forest\n(ARF) for density estimation and data synthesis. To impute a missing value of\nan observation, we condition on the non-missing values and sample from the\nestimated conditional distribution generated by ARF. Our experiments\ndemonstrate that MissARF performs comparably to state-of-the-art single and\nmultiple imputation methods in terms of imputation quality and fast runtime\nwith no additional costs for multiple imputation.", "published": "2025-07-21 14:44:51", "link": "http://arxiv.org/abs/2507.15681v1", "categories": ["stat.ML", "cs.AI", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Agentic AI for autonomous anomaly management in complex systems", "abstract": "This paper explores the potential of agentic AI in autonomously detecting and\nresponding to anomalies within complex systems, emphasizing its ability to\ntransform traditional, human-dependent anomaly management methods.", "published": "2025-07-21 14:39:08", "link": "http://arxiv.org/abs/2507.15676v1", "categories": ["cs.AI", "cs.ET"], "primary_category": "cs.AI"}
{"title": "SustainDiffusion: Optimising the Social and Environmental Sustainability of Stable Diffusion Models", "abstract": "Background: Text-to-image generation models are widely used across numerous\ndomains. Among these models, Stable Diffusion (SD) - an open-source\ntext-to-image generation model - has become the most popular, producing over 12\nbillion images annually. However, the widespread use of these models raises\nconcerns regarding their social and environmental sustainability.\n  Aims: To reduce the harm that SD models may have on society and the\nenvironment, we introduce SustainDiffusion, a search-based approach designed to\nenhance the social and environmental sustainability of SD models.\n  Method: SustainDiffusion searches the optimal combination of hyperparameters\nand prompt structures that can reduce gender and ethnic bias in generated\nimages while also lowering the energy consumption required for image\ngeneration. Importantly, SustainDiffusion maintains image quality comparable to\nthat of the original SD model.\n  Results: We conduct a comprehensive empirical evaluation of SustainDiffusion,\ntesting it against six different baselines using 56 different prompts. Our\nresults demonstrate that SustainDiffusion can reduce gender bias in SD3 by 68%,\nethnic bias by 59%, and energy consumption (calculated as the sum of CPU and\nGPU energy) by 48%. Additionally, the outcomes produced by SustainDiffusion are\nconsistent across multiple runs and can be generalised to various prompts.\n  Conclusions: With SustainDiffusion, we demonstrate how enhancing the social\nand environmental sustainability of text-to-image generation models is possible\nwithout fine-tuning or changing the model's architecture.", "published": "2025-07-21 14:24:31", "link": "http://arxiv.org/abs/2507.15663v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE"}
{"title": "Towards Explainable Anomaly Detection in Shared Mobility Systems", "abstract": "Shared mobility systems, such as bike-sharing networks, play a crucial role\nin urban transportation. Identifying anomalies in these systems is essential\nfor optimizing operations, improving service reliability, and enhancing user\nexperience. This paper presents an interpretable anomaly detection framework\nthat integrates multi-source data, including bike-sharing trip records, weather\nconditions, and public transit availability. The Isolation Forest algorithm is\nemployed for unsupervised anomaly detection, along with the Depth-based\nIsolation Forest Feature Importance (DIFFI) algorithm providing\ninterpretability. Results show that station-level analysis offers a robust\nunderstanding of anomalies, highlighting the influence of external factors such\nas adverse weather and limited transit availability. Our findings contribute to\nimproving decision-making in shared mobility operations.", "published": "2025-07-21 14:06:42", "link": "http://arxiv.org/abs/2507.15643v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Uncovering Critical Features for Deepfake Detection through the Lottery Ticket Hypothesis", "abstract": "Recent advances in deepfake technology have created increasingly convincing\nsynthetic media that poses significant challenges to information integrity and\nsocial trust. While current detection methods show promise, their underlying\nmechanisms remain poorly understood, and the large sizes of their models make\nthem challenging to deploy in resource-limited environments. This study\ninvestigates the application of the Lottery Ticket Hypothesis (LTH) to deepfake\ndetection, aiming to identify the key features crucial for recognizing\ndeepfakes. We examine how neural networks can be efficiently pruned while\nmaintaining high detection accuracy. Through extensive experiments with\nMesoNet, CNN-5, and ResNet-18 architectures on the OpenForensic and\nFaceForensics++ datasets, we find that deepfake detection networks contain\nwinning tickets, i.e., subnetworks, that preserve performance even at\nsubstantial sparsity levels. Our results indicate that MesoNet retains 56.2%\naccuracy at 80% sparsity on the OpenForensic dataset, with only 3,000\nparameters, which is about 90% of its baseline accuracy (62.6%). The results\nalso show that our proposed LTH-based iterative magnitude pruning approach\nconsistently outperforms one-shot pruning methods. Using Grad-CAM\nvisualization, we analyze how pruned networks maintain their focus on critical\nfacial regions for deepfake detection. Additionally, we demonstrate the\ntransferability of winning tickets across datasets, suggesting potential for\nefficient, deployable deepfake detection systems.", "published": "2025-07-21 13:58:24", "link": "http://arxiv.org/abs/2507.15636v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft II", "abstract": "We present an adapter-based approach for tactical conditioning of StarCraft\nII AI agents. Current agents, while powerful, lack the ability to adapt their\nstrategies based on high-level tactical directives. Our method freezes a\npre-trained policy network (DI-Star) and attaches lightweight adapter modules\nto each action head, conditioned on a tactical tensor that encodes strategic\npreferences. By training these adapters with KL divergence constraints, we\nensure the policy maintains core competencies while exhibiting tactical\nvariations. Experimental results show our approach successfully modulates agent\nbehavior across tactical dimensions including aggression, expansion patterns,\nand technology preferences, while maintaining competitive performance. Our\nmethod enables flexible tactical control with minimal computational overhead,\noffering practical strategy customization for complex real-time strategy games.", "published": "2025-07-21 13:42:06", "link": "http://arxiv.org/abs/2507.15618v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Why can't Epidemiology be automated (yet)?", "abstract": "Recent advances in artificial intelligence (AI) - particularly generative AI\n- present new opportunities to accelerate, or even automate, epidemiological\nresearch. Unlike disciplines based on physical experimentation, a sizable\nfraction of Epidemiology relies on secondary data analysis and thus is\nwell-suited for such augmentation. Yet, it remains unclear which specific tasks\ncan benefit from AI interventions or where roadblocks exist. Awareness of\ncurrent AI capabilities is also mixed. Here, we map the landscape of\nepidemiological tasks using existing datasets - from literature review to data\naccess, analysis, writing up, and dissemination - and identify where existing\nAI tools offer efficiency gains. While AI can increase productivity in some\nareas such as coding and administrative tasks, its utility is constrained by\nlimitations of existing AI models (e.g. hallucinations in literature reviews)\nand human systems (e.g. barriers to accessing datasets). Through examples of\nAI-generated epidemiological outputs, including fully AI-generated papers, we\ndemonstrate that recently developed agentic systems can now design and execute\nepidemiological analysis, albeit to varied quality (see\nhttps://github.com/edlowther/automated-epidemiology). Epidemiologists have new\nopportunities to empirically test and benchmark AI systems; realising the\npotential of AI will require two-way engagement between epidemiologists and\nengineers.", "published": "2025-07-21 13:41:52", "link": "http://arxiv.org/abs/2507.15617v1", "categories": ["cs.CY", "cs.AI"], "primary_category": "cs.CY"}
{"title": "Accelerating HEC-RAS: A Recurrent Neural Operator for Rapid River Forecasting", "abstract": "Physics-based solvers like HEC-RAS provide high-fidelity river forecasts but\nare too computationally intensive for on-the-fly decision-making during flood\nevents. The central challenge is to accelerate these simulations without\nsacrificing accuracy. This paper introduces a deep learning surrogate that\ntreats HEC-RAS not as a solver but as a data-generation engine. We propose a\nhybrid, auto-regressive architecture that combines a Gated Recurrent Unit (GRU)\nto capture short-term temporal dynamics with a Geometry-Aware Fourier Neural\nOperator (Geo-FNO) to model long-range spatial dependencies along a river\nreach. The model learns underlying physics implicitly from a minimal\neight-channel feature vector encoding dynamic state, static geometry, and\nboundary forcings extracted directly from native HEC-RAS files. Trained on 67\nreaches of the Mississippi River Basin, the surrogate was evaluated on a\nyear-long, unseen hold-out simulation. Results show the model achieves a strong\npredictive accuracy, with a median absolute stage error of 0.31 feet.\nCritically, for a full 67-reach ensemble forecast, our surrogate reduces the\nrequired wall-clock time from 139 minutes to 40 minutes, a speedup of nearly\n3.5 times over the traditional solver. The success of this data-driven approach\ndemonstrates that robust feature engineering can produce a viable, high-speed\nreplacement for conventional hydraulic models, improving the computational\nfeasibility of large-scale ensemble flood forecasting.", "published": "2025-07-21 13:38:54", "link": "http://arxiv.org/abs/2507.15614v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems", "abstract": "Large Language Models (LLMs) deployed in enterprise settings (e.g., as\nMicrosoft 365 Copilot) face novel security challenges. One critical threat is\nprompt inference attacks: adversaries chain together seemingly benign prompts\nto gradually extract confidential data. In this paper, we present a\ncomprehensive study of multi-stage prompt inference attacks in an enterprise\nLLM context. We simulate realistic attack scenarios where an attacker uses\nmild-mannered queries and indirect prompt injections to exploit an LLM\nintegrated with private corporate data. We develop a formal threat model for\nthese multi-turn inference attacks and analyze them using probability theory,\noptimization frameworks, and information-theoretic leakage bounds. The attacks\nare shown to reliably exfiltrate sensitive information from the LLM's context\n(e.g., internal SharePoint documents or emails), even when standard safety\nmeasures are in place.\n  We propose and evaluate defenses to counter such attacks, including\nstatistical anomaly detection, fine-grained access control, prompt sanitization\ntechniques, and architectural modifications to LLM deployment. Each defense is\nsupported by mathematical analysis or experimental simulation. For example, we\nderive bounds on information leakage under differential privacy-based training\nand demonstrate an anomaly detection method that flags multi-turn attacks with\nhigh AUC. We also introduce an approach called \"spotlighting\" that uses input\ntransformations to isolate untrusted prompt content, reducing attack success by\nan order of magnitude. Finally, we provide a formal proof of concept and\nempirical validation for a combined defense-in-depth strategy. Our work\nhighlights that securing LLMs in enterprise settings requires moving beyond\nsingle-turn prompt filtering toward a holistic, multi-stage perspective on both\nattacks and defenses.", "published": "2025-07-21 13:38:12", "link": "http://arxiv.org/abs/2507.15613v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "Red-Team Multi-Agent Reinforcement Learning for Emergency Braking Scenario", "abstract": "Current research on decision-making in safety-critical scenarios often relies\non inefficient data-driven scenario generation or specific modeling approaches,\nwhich fail to capture corner cases in real-world contexts. To address this\nissue, we propose a Red-Team Multi-Agent Reinforcement Learning framework,\nwhere background vehicles with interference capabilities are treated as\nred-team agents. Through active interference and exploration, red-team vehicles\ncan uncover corner cases outside the data distribution. The framework uses a\nConstraint Graph Representation Markov Decision Process, ensuring that red-team\nvehicles comply with safety rules while continuously disrupting the autonomous\nvehicles (AVs). A policy threat zone model is constructed to quantify the\nthreat posed by red-team vehicles to AVs, inducing more extreme actions to\nincrease the danger level of the scenario. Experimental results show that the\nproposed framework significantly impacts AVs decision-making safety and\ngenerates various corner cases. This method also offers a novel direction for\nresearch in safety-critical scenarios.", "published": "2025-07-21 13:08:49", "link": "http://arxiv.org/abs/2507.15587v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Unequal Voices: How LLMs Construct Constrained Queer Narratives", "abstract": "One way social groups are marginalized in discourse is that the narratives\ntold about them often default to a narrow, stereotyped range of topics. In\ncontrast, default groups are allowed the full complexity of human existence. We\ndescribe the constrained representations of queer people in LLM generations in\nterms of harmful representations, narrow representations, and discursive\nothering and formulate hypotheses to test for these phenomena. Our results show\nthat LLMs are significantly limited in their portrayals of queer personas.", "published": "2025-07-21 13:03:38", "link": "http://arxiv.org/abs/2507.15585v1", "categories": ["cs.CY", "cs.AI"], "primary_category": "cs.CY"}
{"title": "Metric assessment protocol in the context of answer fluctuation on MCQ tasks", "abstract": "Using multiple-choice questions (MCQs) has become a standard for assessing\nLLM capabilities efficiently. A variety of metrics can be employed for this\ntask. However, previous research has not conducted a thorough assessment of\nthem. At the same time, MCQ evaluation suffers from answer fluctuation: models\nproduce different results given slight changes in prompts. We suggest a metric\nassessment protocol in which evaluation methodologies are analyzed through\ntheir connection with fluctuation rates, as well as original performance. Our\nresults show that there is a strong link between existing metrics and the\nanswer changing, even when computed without any additional prompt variants. A\nnovel metric, worst accuracy, demonstrates the highest association on the\nprotocol.", "published": "2025-07-21 13:01:46", "link": "http://arxiv.org/abs/2507.15581v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation", "abstract": "Mixup has become a popular augmentation strategy for image classification,\nyet its naive pixel-wise interpolation often produces unrealistic images that\ncan hinder learning, particularly in high-stakes medical applications. We\npropose GeMix, a two-stage framework that replaces heuristic blending with a\nlearned, label-aware interpolation powered by class-conditional GANs. First, a\nStyleGAN2-ADA generator is trained on the target dataset. During augmentation,\nwe sample two label vectors from Dirichlet priors biased toward different\nclasses and blend them via a Beta-distributed coefficient. Then, we condition\nthe generator on this soft label to synthesize visually coherent images that\nlie along a continuous class manifold. We benchmark GeMix on the large-scale\nCOVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101,\nEfficientNet-B0). When combined with real data, our method increases macro-F1\nover traditional mixup for all backbones, reducing the false negative rate for\nCOVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup,\ndelivering stronger regularization and greater semantic fidelity, without\ndisrupting existing training pipelines. We publicly release our code at\nhttps://github.com/hugocarlesso/GeMix to foster reproducibility and further\nresearch.", "published": "2025-07-21 12:58:05", "link": "http://arxiv.org/abs/2507.15577v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "On the Role of AI in Managing Satellite Constellations: Insights from the ConstellAI Project", "abstract": "The rapid expansion of satellite constellations in near-Earth orbits presents\nsignificant challenges in satellite network management, requiring innovative\napproaches for efficient, scalable, and resilient operations. This paper\nexplores the role of Artificial Intelligence (AI) in optimizing the operation\nof satellite mega-constellations, drawing from the ConstellAI project funded by\nthe European Space Agency (ESA). A consortium comprising GMV GmbH, Saarland\nUniversity, and Thales Alenia Space collaborates to develop AI-driven\nalgorithms and demonstrates their effectiveness over traditional methods for\ntwo crucial operational challenges: data routing and resource allocation. In\nthe routing use case, Reinforcement Learning (RL) is used to improve the\nend-to-end latency by learning from historical queuing latency, outperforming\nclassical shortest path algorithms. For resource allocation, RL optimizes the\nscheduling of tasks across constellations, focussing on efficiently using\nlimited resources such as battery and memory. Both use cases were tested for\nmultiple satellite constellation configurations and operational scenarios,\nresembling the real-life spacecraft operations of communications and Earth\nobservation satellites. This research demonstrates that RL not only competes\nwith classical approaches but also offers enhanced flexibility, scalability,\nand generalizability in decision-making processes, which is crucial for the\nautonomous and intelligent management of satellite fleets. The findings of this\nactivity suggest that AI can fundamentally alter the landscape of satellite\nconstellation management by providing more adaptive, robust, and cost-effective\nsolutions.", "published": "2025-07-21 12:56:16", "link": "http://arxiv.org/abs/2507.15574v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors", "abstract": "Evaluating the scientific discovery capabilities of large language model\nbased agents, particularly how they cope with varying environmental complexity\nand utilize prior knowledge, requires specialized benchmarks currently lacking\nin the landscape. To address this gap, we introduce PhysGym, a novel benchmark\nsuite and simulation platform for rigorously assessing LLM-based scientific\nreasoning in interactive physics environments. PhysGym's primary contribution\nlies in its sophisticated control over the level of prior knowledge provided to\nthe agent. This allows researchers to dissect agent performance along axes\nincluding the complexity of the problem and the prior knowledge levels. The\nbenchmark comprises a suite of interactive simulations, where agents must\nactively probe environments, gather data sequentially under constraints and\nformulate hypotheses about underlying physical laws. PhysGym provides\nstandardized evaluation protocols and metrics for assessing hypothesis accuracy\nand model fidelity. We demonstrate the benchmark's utility by presenting\nresults from baseline LLMs, showcasing its ability to differentiate\ncapabilities based on varying priors and task complexity.", "published": "2025-07-21 12:28:10", "link": "http://arxiv.org/abs/2507.15550v1", "categories": ["cs.LG", "cs.AI", "physics.soc-ph"], "primary_category": "cs.LG"}
{"title": "Data-Efficient Safe Policy Improvement Using Parametric Structure", "abstract": "Safe policy improvement (SPI) is an offline reinforcement learning problem in\nwhich a new policy that reliably outperforms the behavior policy with high\nconfidence needs to be computed using only a dataset and the behavior policy.\nMarkov decision processes (MDPs) are the standard formalism for modeling\nenvironments in SPI. In many applications, additional information in the form\nof parametric dependencies between distributions in the transition dynamics is\navailable. We make SPI more data-efficient by leveraging these dependencies\nthrough three contributions: (1) a parametric SPI algorithm that exploits known\ncorrelations between distributions to more accurately estimate the transition\ndynamics using the same amount of data; (2) a preprocessing technique that\nprunes redundant actions from the environment through a game-based abstraction;\nand (3) a more advanced preprocessing technique, based on satisfiability modulo\ntheory (SMT) solving, that can identify more actions to prune. Empirical\nresults and an ablation study show that our techniques increase the data\nefficiency of SPI by multiple orders of magnitude while maintaining the same\nreliability guarantees.", "published": "2025-07-21 12:00:03", "link": "http://arxiv.org/abs/2507.15532v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "RARE-UNet: Resolution-Aligned Routing Entry for Adaptive Medical Image Segmentation", "abstract": "Accurate segmentation is crucial for clinical applications, but existing\nmodels often assume fixed, high-resolution inputs and degrade significantly\nwhen faced with lower-resolution data in real-world scenarios. To address this\nlimitation, we propose RARE-UNet, a resolution-aware multi-scale segmentation\narchitecture that dynamically adapts its inference path to the spatial\nresolution of the input. Central to our design are multi-scale blocks\nintegrated at multiple encoder depths, a resolution-aware routing mechanism,\nand consistency-driven training that aligns multi-resolution features with\nfull-resolution representations. We evaluate RARE-UNet on two benchmark brain\nimaging tasks for hippocampus and tumor segmentation. Compared to standard\nUNet, its multi-resolution augmented variant, and nnUNet, our model achieves\nthe highest average Dice scores of 0.84 and 0.65 across resolution, while\nmaintaining consistent performance and significantly reduced inference time at\nlower resolutions. These results highlight the effectiveness and scalability of\nour architecture in achieving resolution-robust segmentation. The codes are\navailable at: https://github.com/simonsejse/RARE-UNet.", "published": "2025-07-21 11:49:20", "link": "http://arxiv.org/abs/2507.15524v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "eess.IV"}
{"title": "LLM world models are mental: Output layer evidence of brittle world model use in LLM mechanical reasoning", "abstract": "Do large language models (LLMs) construct and manipulate internal world\nmodels, or do they rely solely on statistical associations represented as\noutput layer token probabilities? We adapt cognitive science methodologies from\nhuman mental models research to test LLMs on pulley system problems using\nTikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical\nadvantage (MA). State-of-the-art models performed marginally but significantly\nabove chance, and their estimates correlated significantly with ground-truth\nMA. Significant correlations between number of pulleys and model estimates\nsuggest that models employed a pulley counting heuristic, without necessarily\nsimulating pulley systems to derive precise values. Study 2 tested this by\nprobing whether LLMs represent global features crucial to MA estimation. Models\nevaluated a functionally connected pulley system against a fake system with\nrandomly placed components. Without explicit cues, models identified the\nfunctional system as having greater MA with F1=0.8, suggesting LLMs could\nrepresent systems well enough to differentiate jumbled from functional systems.\nStudy 3 built on this by asking LLMs to compare functional systems with matched\nsystems which were connected up but which transferred no force to the weight;\nLLMs identified the functional system with F1=0.46, suggesting random guessing.\nInsofar as they may generalize, these findings are compatible with the notion\nthat LLMs manipulate internal world models, sufficient to exploit statistical\nassociations between pulley count and MA (Study 1), and to approximately\nrepresent system components' spatial relations (Study 2). However, they may\nlack the facility to reason over nuanced structural connectivity (Study 3). We\nconclude by advocating the utility of cognitive scientific methods to evaluate\nthe world-modeling capacities of artificial intelligence systems.", "published": "2025-07-21 11:42:03", "link": "http://arxiv.org/abs/2507.15521v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics", "abstract": "Creating an immersive and interactive theatrical experience is a long-term\ngoal in the field of interactive narrative. The emergence of large language\nmodel (LLM) is providing a new path to achieve this goal. However, existing\nLLM-based drama generation methods often result in AI agents that lack\ninitiative and cannot interact with the physical environment. Furthermore,\nthese methods typically require detailed user input to drive the drama. These\nlimitations reduce the interactivity and immersion of online real-time\nperformance. To address the above challenges, we propose HAMLET, a multi-agent\nframework focused on drama creation and online performance. Given a simple\ntopic, the framework generates a narrative blueprint, guiding the subsequent\nimprovisational performance. During the online performance, each actor is given\nan autonomous mind. This means that actors can make independent decisions based\non their own background, goals, and emotional state. In addition to\nconversations with other actors, their decisions can also change the state of\nscene props through actions such as opening a letter or picking up a weapon.\nThe change is then broadcast to other related actors, updating what they know\nand care about, which in turn influences their next action. To evaluate the\nquality of drama performance, we designed an evaluation method to assess three\nprimary aspects, including character performance, narrative quality, and\ninteraction experience. The experimental evaluation shows that HAMLET can\ncreate expressive and coherent theatrical experiences. Our code, dataset and\nmodels are available at https://github.com/HAMLET-2025/HAMLET.", "published": "2025-07-21 11:36:39", "link": "http://arxiv.org/abs/2507.15518v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner", "abstract": "Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based\non reinforcement learning fine-tuning has received widespread attention from\nthe community. Previous R1-Style methods mainly focus on mathematical reasoning\nand code intelligence. It is of great research significance to verify their\nadvantages on more general multimodal data. Chart is an important multimodal\ndata type with rich information, which brings important research challenges in\ncomplex reasoning. In this work, we introduce Chart-R1, a chart-domain\nvision-language model with reinforcement learning fine-tuning to enable complex\nchart reasoning. To support Chart-R1, we first propose a novel programmatic\ndata synthesis technology to generate high-quality step-by-step chart reasoning\ndata covering single- and multi-subcharts, which makes up for the lack of\nreasoning data in the chart domain. Then we develop a two-stage training\nstrategy: Chart-COT with step-by-step chain-of-thought supervision, and\nChart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims\nto decompose complex chart reasoning tasks into fine-grained, understandable\nsubtasks through step-by-step supervision, which lays a good foundation for\nimproving the reasoning level of reinforcement learning. Chart-RFT utilize the\ntypical group relative policy optimization strategy, in which a relatively soft\nreward is adopted for numerical response to emphasize the numerical sensitivity\nin the chart domain. We conduct extensive experiments on open-source benchmarks\nand self-built chart reasoning dataset (\\emph{i.e., ChartRQA}). Experimental\nresults show that Chart-R1 has significant advantages compared to chart-domain\nmethods, even comparable to open/closed source large-scale models (\\emph{e.g.,\nGPT-4o, Claude-3.5}).", "published": "2025-07-21 11:22:17", "link": "http://arxiv.org/abs/2507.15509v1", "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.AI"}
{"title": "GR-3 Technical Report", "abstract": "We report our recent progress towards building generalist robot policies, the\ndevelopment of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.\nIt showcases exceptional capabilities in generalizing to novel objects,\nenvironments, and instructions involving abstract concepts. Furthermore, it can\nbe efficiently fine-tuned with minimal human trajectory data, enabling rapid\nand cost-effective adaptation to new settings. GR-3 also excels in handling\nlong-horizon and dexterous tasks, including those requiring bi-manual\nmanipulation and mobile movement, showcasing robust and reliable performance.\nThese capabilities are achieved through a multi-faceted training recipe that\nincludes co-training with web-scale vision-language data, efficient fine-tuning\nfrom human trajectory data collected via VR devices, and effective imitation\nlearning with robot trajectory data. In addition, we introduce ByteMini, a\nversatile bi-manual mobile robot designed with exceptional flexibility and\nreliability, capable of accomplishing a wide range of tasks when integrated\nwith GR-3. Through extensive real-world experiments, we show GR-3 surpasses the\nstate-of-the-art baseline method, $\\pi_0$, on a wide variety of challenging\ntasks. We hope GR-3 can serve as a step towards building generalist robots\ncapable of assisting humans in daily life.", "published": "2025-07-21 10:54:13", "link": "http://arxiv.org/abs/2507.15493v1", "categories": ["cs.RO", "cs.AI", "cs.CV"], "primary_category": "cs.RO"}
{"title": "The Constitutional Controller: Doubt-Calibrated Steering of Compliant Agents", "abstract": "Ensuring reliable and rule-compliant behavior of autonomous agents in\nuncertain environments remains a fundamental challenge in modern robotics. Our\nwork shows how neuro-symbolic systems, which integrate probabilistic, symbolic\nwhite-box reasoning models with deep learning methods, offer a powerful\nsolution to this challenge. This enables the simultaneous consideration of\nexplicit rules and neural models trained on noisy data, combining the strength\nof structured reasoning with flexible representations. To this end, we\nintroduce the Constitutional Controller (CoCo), a novel framework designed to\nenhance the safety and reliability of agents by reasoning over deep\nprobabilistic logic programs representing constraints such as those found in\nshared traffic spaces. Furthermore, we propose the concept of self-doubt,\nimplemented as a probability density conditioned on doubt features such as\ntravel velocity, employed sensors, or health factors. In a real-world aerial\nmobility study, we demonstrate CoCo's advantages for intelligent autonomous\nsystems to learn appropriate doubts and navigate complex and uncertain\nenvironments safely and compliantly.", "published": "2025-07-21 10:33:31", "link": "http://arxiv.org/abs/2507.15478v1", "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "cs.RO"}
{"title": "The Emergence of Deep Reinforcement Learning for Path Planning", "abstract": "The increasing demand for autonomous systems in complex and dynamic\nenvironments has driven significant research into intelligent path planning\nmethodologies. For decades, graph-based search algorithms, linear programming\ntechniques, and evolutionary computation methods have served as foundational\napproaches in this domain. Recently, deep reinforcement learning (DRL) has\nemerged as a powerful method for enabling autonomous agents to learn optimal\nnavigation strategies through interaction with their environments. This survey\nprovides a comprehensive overview of traditional approaches as well as the\nrecent advancements in DRL applied to path planning tasks, focusing on\nautonomous vehicles, drones, and robotic platforms. Key algorithms across both\nconventional and learning-based paradigms are categorized, with their\ninnovations and practical implementations highlighted. This is followed by a\nthorough discussion of their respective strengths and limitations in terms of\ncomputational efficiency, scalability, adaptability, and robustness. The survey\nconcludes by identifying key open challenges and outlining promising avenues\nfor future research. Special attention is given to hybrid approaches that\nintegrate DRL with classical planning techniques to leverage the benefits of\nboth learning-based adaptability and deterministic reliability, offering\npromising directions for robust and resilient autonomous navigation.", "published": "2025-07-21 10:21:42", "link": "http://arxiv.org/abs/2507.15469v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "The New LLM Bottleneck: A Systems Perspective on Latent Attention and Mixture-of-Experts", "abstract": "Computational workloads composing traditional Transformer models are starkly\nbifurcated. Multi-Head Attention (MHA) is memory-bound, with low arithmetic\nintensity, while feedforward layers are compute-bound. This dichotomy has long\nmotivated research into specialized hardware to mitigate the MHA bottleneck.\n  This paper argues that recent architectural shifts, namely Multi-head Latent\nAttention (MLA) and Mixture-of-Experts (MoE), challenge the premise of\nspecialized attention hardware. We make two key observations. First, the\narithmetic intensity of MLA is over two orders of magnitude greater than that\nof MHA, shifting it close to a compute-bound regime well-suited for modern\naccelerators like GPUs. Second, by distributing MoE experts across a pool of\naccelerators, their arithmetic intensity can be tuned through batching to match\nthat of the dense layers, creating a more balanced computational profile.\n  These findings reveal a diminishing need for specialized attention hardware.\nThe central challenge for next-generation Transformers is no longer\naccelerating a single memory-bound layer. Instead, the focus must shift to\ndesigning balanced systems with sufficient compute, memory capacity, memory\nbandwidth, and high-bandwidth interconnects to manage the diverse demands of\nlarge-scale models.", "published": "2025-07-21 10:18:33", "link": "http://arxiv.org/abs/2507.15465v1", "categories": ["cs.AR", "cs.AI"], "primary_category": "cs.AR"}
{"title": "Optimization of Activity Batching Policies in Business Processes", "abstract": "In business processes, activity batching refers to packing multiple activity\ninstances for joint execution. Batching allows managers to trade off cost and\nprocessing effort against waiting time. Larger and less frequent batches may\nlower costs by reducing processing effort and amortizing fixed costs, but they\ncreate longer waiting times. In contrast, smaller and more frequent batches\nreduce waiting times but increase fixed costs and processing effort. A batching\npolicy defines how activity instances are grouped into batches and when each\nbatch is activated. This paper addresses the problem of discovering batching\npolicies that strike optimal trade-offs between waiting time, processing\neffort, and cost. The paper proposes a Pareto optimization approach that starts\nfrom a given set (possibly empty) of activity batching policies and generates\nalternative policies for each batched activity via intervention heuristics.\nEach heuristic identifies an opportunity to improve an activity's batching\npolicy with respect to a metric (waiting time, processing time, cost, or\nresource utilization) and an associated adjustment to the activity's batching\npolicy (the intervention). The impact of each intervention is evaluated via\nsimulation. The intervention heuristics are embedded in an optimization\nmeta-heuristic that triggers interventions to iteratively update the Pareto\nfront of the interventions identified so far. The paper considers three\nmeta-heuristics: hill-climbing, simulated annealing, and reinforcement\nlearning. An experimental evaluation compares the proposed approach based on\nintervention heuristics against the same (non-heuristic guided) meta-heuristics\nbaseline regarding convergence, diversity, and cycle time gain of\nPareto-optimal policies.", "published": "2025-07-21 10:11:51", "link": "http://arxiv.org/abs/2507.15457v1", "categories": ["cs.AI", "I.2.8"], "primary_category": "cs.AI"}
{"title": "Solving nonconvex Hamilton--Jacobi--Isaacs equations with PINN-based policy iteration", "abstract": "We propose a mesh-free policy iteration framework that combines classical\ndynamic programming with physics-informed neural networks (PINNs) to solve\nhigh-dimensional, nonconvex Hamilton--Jacobi--Isaacs (HJI) equations arising in\nstochastic differential games and robust control. The method alternates between\nsolving linear second-order PDEs under fixed feedback policies and updating the\ncontrols via pointwise minimax optimization using automatic differentiation.\nUnder standard Lipschitz and uniform ellipticity assumptions, we prove that the\nvalue function iterates converge locally uniformly to the unique viscosity\nsolution of the HJI equation. The analysis establishes equi-Lipschitz\nregularity of the iterates, enabling provable stability and convergence without\nrequiring convexity of the Hamiltonian. Numerical experiments demonstrate the\naccuracy and scalability of the method. In a two-dimensional stochastic\npath-planning game with a moving obstacle, our method matches finite-difference\nbenchmarks with relative $L^2$-errors below %10^{-2}%. In five- and\nten-dimensional publisher-subscriber differential games with anisotropic noise,\nthe proposed approach consistently outperforms direct PINN solvers, yielding\nsmoother value functions and lower residuals. Our results suggest that\nintegrating PINNs with policy iteration is a practical and theoretically\ngrounded method for solving high-dimensional, nonconvex HJI equations, with\npotential applications in robotics, finance, and multi-agent reinforcement\nlearning.", "published": "2025-07-21 10:06:53", "link": "http://arxiv.org/abs/2507.15455v1", "categories": ["math.NA", "cs.AI", "cs.NA", "math.AP", "49N70, 35Q93, 49L25, 68T07"], "primary_category": "math.NA"}
{"title": "ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting", "abstract": "3D Gaussian Splatting is renowned for its high-fidelity reconstructions and\nreal-time novel view synthesis, yet its lack of semantic understanding limits\nobject-level perception. In this work, we propose ObjectGS, an object-aware\nframework that unifies 3D scene reconstruction with semantic understanding.\nInstead of treating the scene as a unified whole, ObjectGS models individual\nobjects as local anchors that generate neural Gaussians and share object IDs,\nenabling precise object-level reconstruction. During training, we dynamically\ngrow or prune these anchors and optimize their features, while a one-hot ID\nencoding with a classification loss enforces clear semantic constraints. We\nshow through extensive experiments that ObjectGS not only outperforms\nstate-of-the-art methods on open-vocabulary and panoptic segmentation tasks,\nbut also integrates seamlessly with applications like mesh extraction and scene\nediting. Project page: https://ruijiezhu94.github.io/ObjectGS_page", "published": "2025-07-21 10:06:23", "link": "http://arxiv.org/abs/2507.15454v1", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.HC"], "primary_category": "cs.GR"}
{"title": "EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent", "abstract": "Egomotion videos are first-person recordings where the view changes\ncontinuously due to the agent's movement. As they serve as the primary visual\ninput for embodied AI agents, making egomotion video reasoning more efficient\nis therefore essential for real-world deployment. Recent advances in\nvision-language models have enabled strong multimodal reasoning capabilities,\nbut their computational cost remains prohibitive for long, redundant video\ninputs. Existing token pruning methods, typically designed for third-person\nvideos, fail to leverage the spatiotemporal continuity and motion constraints\ninherent in egomotion settings. To address this, we propose EgoPrune, a\ntraining-free token pruning method tailored for egomotion video reasoning.\nEgoPrune comprises three components: a keyframe selector adapted from EmbodiedR\nfor temporally efficient sampling; Perspective-Aware Redundancy Filtering\n(PARF), which aligns visual tokens using perspective transformations and\nremoves redundant tokens; and a Maximal Marginal Relevance (MMR)-based token\nselector that jointly considers visual-text relevance and intra-frame\ndiversity. Experiments on two egomotion video benchmarks show that EgoPrune\nconsistently outperforms prior training-free methods across various pruning\nratios while significantly reducing FLOPs, memory usage, and latency. Moreover,\nwe deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB\nedge device, demonstrating its real-world efficiency and suitability for\non-device egomotion video reasoning.", "published": "2025-07-21 09:27:45", "link": "http://arxiv.org/abs/2507.15428v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Predictive Process Monitoring Using Object-centric Graph Embeddings", "abstract": "Object-centric predictive process monitoring explores and utilizes\nobject-centric event logs to enhance process predictions. The main challenge\nlies in extracting relevant information and building effective models. In this\npaper, we propose an end-to-end model that predicts future process behavior,\nfocusing on two tasks: next activity prediction and next event time. The\nproposed model employs a graph attention network to encode activities and their\nrelationships, combined with an LSTM network to handle temporal dependencies.\nEvaluated on one reallife and three synthetic event logs, the model\ndemonstrates competitive performance compared to state-of-the-art methods.", "published": "2025-07-21 09:10:49", "link": "http://arxiv.org/abs/2507.15411v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Neuro-MSBG: An End-to-End Neural Model for Hearing Loss Simulation", "abstract": "Hearing loss simulation models are essential for hearing aid deployment.\nHowever, existing models have high computational complexity and latency, which\nlimits real-time applications and lack direct integration with speech\nprocessing systems. To address these issues, we propose Neuro-MSBG, a\nlightweight end-to-end model with a personalized audiogram encoder for\neffective time-frequency modeling. Experiments show that Neuro-MSBG supports\nparallel inference and retains the intelligibility and perceptual quality of\nthe original MSBG, with a Spearman's rank correlation coefficient (SRCC) of\n0.9247 for Short-Time Objective Intelligibility (STOI) and 0.8671 for\nPerceptual Evaluation of Speech Quality (PESQ). Neuro-MSBG reduces simulation\nruntime by a factor of 46 (from 0.970 seconds to 0.021 seconds for a 1 second\ninput), further demonstrating its efficiency and practicality.", "published": "2025-07-21 08:58:31", "link": "http://arxiv.org/abs/2507.15396v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PiMRef: Detecting and Explaining Ever-evolving Spear Phishing Emails with Knowledge Base Invariants", "abstract": "Phishing emails are a critical component of the cybercrime kill chain due to\ntheir wide reach and low cost. Their ever-evolving nature renders traditional\nrule-based and feature-engineered detectors ineffective in the ongoing arms\nrace between attackers and defenders. The rise of large language models (LLMs)\nfurther exacerbates the threat, enabling attackers to craft highly convincing\nphishing emails at minimal cost.\n  This work demonstrates that LLMs can generate psychologically persuasive\nphishing emails tailored to victim profiles, successfully bypassing nearly all\ncommercial and academic detectors. To defend against such threats, we propose\nPiMRef, the first reference-based phishing email detector that leverages\nknowledge-based invariants. Our core insight is that persuasive phishing emails\noften contain disprovable identity claims, which contradict real-world facts.\nPiMRef reframes phishing detection as an identity fact-checking task. Given an\nemail, PiMRef (i) extracts the sender's claimed identity, (ii) verifies the\nlegitimacy of the sender's domain against a predefined knowledge base, and\n(iii) detects call-to-action prompts that push user engagement. Contradictory\nclaims are flagged as phishing indicators and serve as human-understandable\nexplanations.\n  Compared to existing methods such as D-Fence, HelpHed, and ChatSpamDetector,\nPiMRef boosts precision by 8.8% with no loss in recall on standard benchmarks\nlike Nazario and PhishPot. In a real-world evaluation of 10,183 emails across\nfive university accounts over three years, PiMRef achieved 92.1% precision,\n87.9% recall, and a median runtime of 0.05s, outperforming the state-of-the-art\nin both effectiveness and efficiency.", "published": "2025-07-21 08:53:41", "link": "http://arxiv.org/abs/2507.15393v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models", "abstract": "Active learning (AL) seeks to reduce annotation costs by selecting the most\ninformative samples for labeling, making it particularly valuable in\nresource-constrained settings. However, traditional evaluation methods, which\nfocus solely on final accuracy, fail to capture the full dynamics of the\nlearning process. To address this gap, we propose PALM (Performance Analysis of\nActive Learning Models), a unified and interpretable mathematical model that\ncharacterizes AL trajectories through four key parameters: achievable accuracy,\ncoverage efficiency, early-stage performance, and scalability. PALM provides a\npredictive description of AL behavior from partial observations, enabling the\nestimation of future performance and facilitating principled comparisons across\ndifferent strategies. We validate PALM through extensive experiments on\nCIFAR-10/100 and ImageNet-50/100/200, covering a wide range of AL methods and\nself-supervised embeddings. Our results demonstrate that PALM generalizes\neffectively across datasets, budgets, and strategies, accurately predicting\nfull learning curves from limited labeled data. Importantly, PALM reveals\ncrucial insights into learning efficiency, data space coverage, and the\nscalability of AL methods. By enabling the selection of cost-effective\nstrategies and predicting performance under tight budget constraints, PALM lays\nthe basis for more systematic, reproducible, and data-efficient evaluation of\nAL in both research and real-world applications. The code is available at:\nhttps://github.com/juliamachnio/PALM.", "published": "2025-07-21 08:37:44", "link": "http://arxiv.org/abs/2507.15381v1", "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Multi-beam Beamforming in RIS-aided MIMO Subject to Reradiation Mask Constraints -- Optimization and Machine Learning Design", "abstract": "Reconfigurable intelligent surfaces (RISs) are an emerging technology for\nimproving spectral efficiency and reducing power consumption in future wireless\nsystems. This paper investigates the joint design of the transmit precoding\nmatrices and the RIS phase shift vector in a multi-user RIS-aided\nmultiple-input multiple-output (MIMO) communication system. We formulate a\nmax-min optimization problem to maximize the minimum achievable rate while\nconsidering transmit power and reradiation mask constraints. The achievable\nrate is simplified using the Arimoto-Blahut algorithm, and the problem is\nbroken into quadratic programs with quadratic constraints (QPQC) sub-problems\nusing an alternating optimization approach. To improve efficiency, we develop a\nmodel-based neural network optimization that utilizes the one-hot encoding for\nthe angles of incidence and reflection. We address practical RIS limitations by\nusing a greedy search algorithm to solve the optimization problem for discrete\nphase shifts. Simulation results demonstrate that the proposed methods\neffectively shape the multi-beam radiation pattern towards desired directions\nwhile satisfying reradiation mask constraints. The neural network design\nreduces the execution time, and the discrete phase shift scheme performs well\nwith a small reduction of the beamforming gain by using only four phase shift\nlevels.", "published": "2025-07-21 08:18:23", "link": "http://arxiv.org/abs/2507.15367v1", "categories": ["math.OC", "cs.AI", "cs.IT", "math.IT"], "primary_category": "math.OC"}
{"title": "EEG-based Epileptic Prediction via a Two-stage Channel-aware Set Transformer Network", "abstract": "Epilepsy is a chronic, noncommunicable brain disorder, and sudden seizure\nonsets can significantly impact patients' quality of life and health. However,\nwearable seizure-predicting devices are still limited, partly due to the bulky\nsize of EEG-collecting devices. To relieve the problem, we proposed a novel\ntwo-stage channel-aware Set Transformer Network that could perform seizure\nprediction with fewer EEG channel sensors. We also tested a seizure-independent\ndivision method which could prevent the adjacency of training and test data.\nExperiments were performed on the CHB-MIT dataset which includes 22 patients\nwith 88 merged seizures. The mean sensitivity before channel selection was\n76.4% with a false predicting rate (FPR) of 0.09/hour. After channel selection,\ndominant channels emerged in 20 out of 22 patients; the average number of\nchannels was reduced to 2.8 from 18; and the mean sensitivity rose to 80.1%\nwith an FPR of 0.11/hour. Furthermore, experimental results on the\nseizure-independent division supported our assertion that a more rigorous\nseizure-independent division should be used for patients with abundant EEG\nrecordings.", "published": "2025-07-21 08:16:19", "link": "http://arxiv.org/abs/2507.15364v1", "categories": ["eess.SP", "cs.AI", "cs.LG"], "primary_category": "eess.SP"}
{"title": "Latent Space Synergy: Text-Guided Data Augmentation for Direct Diffusion Biomedical Segmentation", "abstract": "Medical image segmentation suffers from data scarcity, particularly in polyp\ndetection where annotation requires specialized expertise. We present SynDiff,\na framework combining text-guided synthetic data generation with efficient\ndiffusion-based segmentation. Our approach employs latent diffusion models to\ngenerate clinically realistic synthetic polyps through text-conditioned\ninpainting, augmenting limited training data with semantically diverse samples.\nUnlike traditional diffusion methods requiring iterative denoising, we\nintroduce direct latent estimation enabling single-step inference with T x\ncomputational speedup. On CVC-ClinicDB, SynDiff achieves 96.0% Dice and 92.9%\nIoU while maintaining real-time capability suitable for clinical deployment.\nThe framework demonstrates that controlled synthetic augmentation improves\nsegmentation robustness without distribution shift. SynDiff bridges the gap\nbetween data-hungry deep learning models and clinical constraints, offering an\nefficient solution for deployment in resourcelimited medical settings.", "published": "2025-07-21 08:15:17", "link": "http://arxiv.org/abs/2507.15361v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "eess.IV"}
{"title": "RAD: Retrieval High-quality Demonstrations to Enhance Decision-making", "abstract": "Offline reinforcement learning (RL) enables agents to learn policies from\nfixed datasets, avoiding costly or unsafe environment interactions. However,\nits effectiveness is often limited by dataset sparsity and the lack of\ntransition overlap between suboptimal and expert trajectories, which makes\nlong-horizon planning particularly challenging. Prior solutions based on\nsynthetic data augmentation or trajectory stitching often fail to generalize to\nnovel states and rely on heuristic stitching points. To address these\nchallenges, we propose Retrieval High-quAlity Demonstrations (RAD) for\ndecision-making, which combines non-parametric retrieval with diffusion-based\ngenerative modeling. RAD dynamically retrieves high-return states from the\noffline dataset as target states based on state similarity and return\nestimation, and plans toward them using a condition-guided diffusion model.\nSuch retrieval-guided generation enables flexible trajectory stitching and\nimproves generalization when encountered with underrepresented or\nout-of-distribution states. Extensive experiments confirm that RAD achieves\ncompetitive or superior performance compared to baselines across diverse\nbenchmarks, validating its effectiveness.", "published": "2025-07-21 08:08:18", "link": "http://arxiv.org/abs/2507.15356v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms", "abstract": "On-demand ride-sharing platforms face the fundamental challenge of\ndynamically bundling passengers with diverse origins and destinations and\nmatching them with vehicles in real time, all under significant uncertainty.\nRecently, MARL has emerged as a promising solution for this problem, leveraging\ndecentralized learning to address the curse of dimensionality caused by the\nlarge number of agents in the ride-hailing market and the resulting expansive\nstate and action spaces. However, conventional MARL-based ride-sharing\napproaches heavily rely on the accurate estimation of Q-values or V-values,\nwhich becomes problematic in large-scale, highly uncertain environments.\nSpecifically, most of these approaches adopt an independent paradigm,\nexacerbating this issue, as each agent treats others as part of the\nenvironment, leading to unstable training and substantial estimation bias in\nvalue functions. To address these challenges, we propose two novel alternative\nmethods that bypass value function estimation. First, we adapt GRPO to\nride-sharing, replacing the PPO baseline with the group average reward to\neliminate critic estimation errors and reduce training bias. Second, inspired\nby GRPO's full utilization of group reward information, we customize the PPO\nframework for ride-sharing platforms and show that, under a homogeneous fleet,\nthe optimal policy can be trained using only one-step rewards - a method we\nterm One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan\nride-hailing dataset demonstrate that both GRPO and OSPO achieve superior\nperformance across most scenarios, efficiently optimizing pickup times and the\nnumber of served orders using simple MLP networks.", "published": "2025-07-21 08:04:31", "link": "http://arxiv.org/abs/2507.15351v1", "categories": ["cs.AI", "cs.ET", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Scaling Decentralized Learning with FLock", "abstract": "Fine-tuning the large language models (LLMs) are prevented by the deficiency\nof centralized control and the massive computing and communication overhead on\nthe decentralized schemes. While the typical standard federated learning (FL)\nsupports data privacy, the central server requirement creates a single point of\nattack and vulnerability to poisoning attacks. Generalizing the result in this\ndirection to 70B-parameter models in the heterogeneous, trustless environments\nhas turned out to be a huge, yet unbroken bottleneck. This paper introduces\nFLock, a decentralized framework for secure and efficient collaborative LLM\nfine-tuning. Integrating a blockchain-based trust layer with economic\nincentives, FLock replaces the central aggregator with a secure, auditable\nprotocol for cooperation among untrusted parties. We present the first\nempirical validation of fine-tuning a 70B LLM in a secure, multi-domain,\ndecentralized setting. Our experiments show the FLock framework defends against\nbackdoor poisoning attacks that compromise standard FL optimizers and fosters\nsynergistic knowledge transfer. The resulting models show a >68% reduction in\nadversarial attack success rates. The global model also demonstrates superior\ncross-domain generalization, outperforming models trained in isolation on their\nown specialized data.", "published": "2025-07-21 08:01:43", "link": "http://arxiv.org/abs/2507.15349v1", "categories": ["cs.LG", "cs.AI", "cs.DC"], "primary_category": "cs.LG"}
{"title": "StackTrans: From Large Language Model to Large Pushdown Automata Model", "abstract": "The Transformer architecture has emerged as a landmark advancement within the\nbroad field of artificial intelligence, effectively catalyzing the advent of\nlarge language models (LLMs). However, despite its remarkable capabilities and\nthe substantial progress it has facilitated, the Transformer architecture still\nhas some limitations. One such intrinsic limitation is its inability to\neffectively capture the Chomsky hierarchy, such as regular expressions or\ndeterministic context-free grammars. Drawing inspiration from pushdown\nautomata, which efficiently resolve deterministic context-free grammars using\nstacks, we propose StackTrans to address the aforementioned issue within LLMs.\nUnlike previous approaches that modify the attention computation, StackTrans\nexplicitly incorporates hidden state stacks between Transformer layers. This\ndesign maintains compatibility with existing frameworks like flash-attention.\nSpecifically, our design features stack operations -- such as pushing and\npopping hidden states -- that are differentiable and can be learned in an\nend-to-end manner. Our comprehensive evaluation spans benchmarks for both\nChomsky hierarchies and large-scale natural languages. Across these diverse\ntasks, StackTrans consistently outperforms standard Transformer models and\nother baselines. We have successfully scaled StackTrans up from 360M to 7B\nparameters. In particular, our from-scratch pretrained model StackTrans-360M\noutperforms several larger open-source LLMs with 2-3x more parameters,\nshowcasing its superior efficiency and reasoning capability.", "published": "2025-07-21 07:58:03", "link": "http://arxiv.org/abs/2507.15343v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE"}
{"title": "MedSR-Impact: Transformer-Based Super-Resolution for Lung CT Segmentation, Radiomics, Classification, and Prognosis", "abstract": "High-resolution volumetric computed tomography (CT) is essential for accurate\ndiagnosis and treatment planning in thoracic diseases; however, it is limited\nby radiation dose and hardware costs. We present the Transformer Volumetric\nSuper-Resolution Network (\\textbf{TVSRN-V2}), a transformer-based\nsuper-resolution (SR) framework designed for practical deployment in clinical\nlung CT analysis. Built from scalable components, including Through-Plane\nAttention Blocks (TAB) and Swin Transformer V2 -- our model effectively\nreconstructs fine anatomical details in low-dose CT volumes and integrates\nseamlessly with downstream analysis pipelines. We evaluate its effectiveness on\nthree critical lung cancer tasks -- lobe segmentation, radiomics, and prognosis\n-- across multiple clinical cohorts. To enhance robustness across variable\nacquisition protocols, we introduce pseudo-low-resolution augmentation,\nsimulating scanner diversity without requiring private data. TVSRN-V2\ndemonstrates a significant improvement in segmentation accuracy (+4\\% Dice),\nhigher radiomic feature reproducibility, and enhanced predictive performance\n(+0.06 C-index and AUC). These results indicate that SR-driven recovery of\nstructural detail significantly enhances clinical decision support, positioning\nTVSRN-V2 as a well-engineered, clinically viable system for dose-efficient\nimaging and quantitative analysis in real-world CT workflows.", "published": "2025-07-21 07:53:49", "link": "http://arxiv.org/abs/2507.15340v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Beyond Model Base Selection: Weaving Knowledge to Master Fine-grained Neural Network Design", "abstract": "Database systems have recently advocated for embedding machine learning (ML)\ncapabilities, offering declarative model queries over large, managed model\nrepositories, thereby circumventing the huge computational overhead of\ntraditional ML-based algorithms in automated neural network model selection.\nPioneering database studies aim to organize existing benchmark repositories as\nmodel bases (MB), querying them for the model records with the highest\nperformance estimation metrics for given tasks. However, this static model\nselection practice overlooks the fine-grained, evolving relational dependencies\nbetween diverse task queries and model architecture variations, resulting in\nsuboptimal matches and failing to further refine the model effectively. To fill\nthe model refinement gap in database research, we propose M-DESIGN, a curated\nmodel knowledge base (MKB) pipeline for mastering neural network refinement by\nadaptively weaving prior insights about model architecture modification. First,\nwe propose a knowledge weaving engine that reframes model refinement as an\nadaptive query problem over task metadata. Given a user's task query, M-DESIGN\nquickly matches and iteratively refines candidate models by leveraging a\ngraph-relational knowledge schema that explicitly encodes data properties,\narchitecture variations, and pairwise performance deltas as joinable relations.\nThis schema supports fine-grained relational analytics over architecture tweaks\nand drives a predictive query planner that can detect and adapt to\nout-of-distribution (OOD) tasks. We instantiate M-DESIGN for graph analytics\ntasks, where our model knowledge base enriches existing benchmarks with\nstructured metadata covering 3 graph tasks and 22 graph datasets, contributing\ndata records of 67,760 graph models. Empirical results demonstrate that\nM-DESIGN delivers the optimal model in 26 of 33 data-task pairs within limited\nbudgets.", "published": "2025-07-21 07:49:19", "link": "http://arxiv.org/abs/2507.15336v1", "categories": ["cs.LG", "cs.AI", "cs.DB"], "primary_category": "cs.LG"}
{"title": "ExDD: Explicit Dual Distribution Learning for Surface Defect Detection via Diffusion Synthesis", "abstract": "Industrial defect detection systems face critical limitations when confined\nto one-class anomaly detection paradigms, which assume uniform outlier\ndistributions and struggle with data scarcity in realworld manufacturing\nenvironments. We present ExDD (Explicit Dual Distribution), a novel framework\nthat transcends these limitations by explicitly modeling dual feature\ndistributions. Our approach leverages parallel memory banks that capture the\ndistinct statistical properties of both normality and anomalous patterns,\naddressing the fundamental flaw of uniform outlier assumptions. To overcome\ndata scarcity, we employ latent diffusion models with domain-specific textual\nconditioning, generating in-distribution synthetic defects that preserve\nindustrial context. Our neighborhood-aware ratio scoring mechanism elegantly\nfuses complementary distance metrics, amplifying signals in regions exhibiting\nboth deviation from normality and similarity to known defect patterns.\nExperimental validation on KSDD2 demonstrates superior performance (94.2%\nI-AUROC, 97.7% P-AUROC), with optimal augmentation at 100 synthetic samples.", "published": "2025-07-21 07:49:00", "link": "http://arxiv.org/abs/2507.15335v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI", "abstract": "We introduce Cognitive Degradation as a novel vulnerability class in agentic\nAI systems. Unlike traditional adversarial external threats such as prompt\ninjection, these failures originate internally, arising from memory starvation,\nplanner recursion, context flooding, and output suppression. These systemic\nweaknesses lead to silent agent drift, logic collapse, and persistent\nhallucinations over time. To address this class of failures, we introduce the\nQorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain\n10), a lifecycle-aware defense framework defined by a six-stage cognitive\ndegradation lifecycle. The framework includes seven runtime controls\n(QSAF-BC-001 to BC-007) that monitor agent subsystems in real time and trigger\nproactive mitigation through fallback routing, starvation detection, and memory\nintegrity enforcement. Drawing from cognitive neuroscience, we map agentic\narchitectures to human analogs, enabling early detection of fatigue,\nstarvation, and role collapse. By introducing a formal lifecycle and real-time\nmitigation controls, this work establishes Cognitive Degradation as a critical\nnew class of AI system vulnerability and proposes the first cross-platform\ndefense model for resilient agentic behavior.", "published": "2025-07-21 07:41:58", "link": "http://arxiv.org/abs/2507.15330v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Butterfly Effects in Toolchains: A Comprehensive Analysis of Failed Parameter Filling in LLM Tool-Agent Systems", "abstract": "The emergence of the tool agent paradigm has broadened the capability\nboundaries of the Large Language Model (LLM), enabling it to complete more\ncomplex tasks. However, the effectiveness of this paradigm is limited due to\nthe issue of parameter failure during its execution. To explore this phenomenon\nand propose corresponding suggestions, we first construct a parameter failure\ntaxonomy in this paper. We derive five failure categories from the invocation\nchain of a mainstream tool agent. Then, we explore the correlation between\nthree different input sources and failure categories by applying 15 input\nperturbation methods to the input. Experimental results show that parameter\nname hallucination failure primarily stems from inherent LLM limitations, while\nissues with input sources mainly cause other failure patterns. To improve the\nreliability and effectiveness of tool-agent interactions, we propose\ncorresponding improvement suggestions, including standardizing tool return\nformats, improving error feedback mechanisms, and ensuring parameter\nconsistency.", "published": "2025-07-21 06:55:37", "link": "http://arxiv.org/abs/2507.15296v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE"}
{"title": "EndoControlMag: Robust Endoscopic Vascular Motion Magnification with Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Contro", "abstract": "Visualizing subtle vascular motions in endoscopic surgery is crucial for\nsurgical precision and decision-making, yet remains challenging due to the\ncomplex and dynamic nature of surgical scenes. To address this, we introduce\nEndoControlMag, a training-free, Lagrangian-based framework with\nmask-conditioned vascular motion magnification tailored to endoscopic\nenvironments. Our approach features two key modules: a Periodic Reference\nResetting (PRR) scheme that divides videos into short overlapping clips with\ndynamically updated reference frames to prevent error accumulation while\nmaintaining temporal coherence, and a Hierarchical Tissue-aware Magnification\n(HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores\nusing a pretrained visual tracking model to maintain accurate localization\ndespite occlusions and view changes. It then applies one of two adaptive\nsoftening strategies to surrounding tissues: motion-based softening that\nmodulates magnification strength proportional to observed tissue displacement,\nor distance-based exponential decay that simulates biomechanical force\nattenuation. This dual-mode approach accommodates diverse surgical\nscenarios-motion-based softening excels with complex tissue deformations while\ndistance-based softening provides stability during unreliable optical flow\nconditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four\ndifferent surgery types and various challenging scenarios, including\nocclusions, instrument disturbance, view changes, and vessel deformations.\nQuantitative metrics, visual assessments, and expert surgeon evaluations\ndemonstrate that EndoControlMag significantly outperforms existing methods in\nboth magnification accuracy and visual quality while maintaining robustness\nacross challenging surgical conditions. The code, dataset, and video results\nare available at https://szupc.github.io/EndoControlMag/.", "published": "2025-07-21 06:47:44", "link": "http://arxiv.org/abs/2507.15292v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Preferential subspace identification (PSID) with forward-backward smoothing", "abstract": "System identification methods for multivariate time-series, such as neural\nand behavioral recordings, have been used to build models for predicting one\nfrom the other. For example, Preferential Subspace Identification (PSID) builds\na state-space model of a primary time-series (e.g., neural activity) to\noptimally predict a secondary time-series (e.g., behavior). However, PSID\nfocuses on optimal prediction using past primary data, even though in offline\napplications, better estimation can be achieved by incorporating concurrent\ndata (filtering) or all available data (smoothing). Here, we extend PSID to\nenable optimal filtering and smoothing. First, we show that the presence of a\nsecondary signal makes it possible to uniquely identify a model with an optimal\nKalman update step (to enable filtering) from a family of otherwise equivalent\nstate-space models. Our filtering solution augments PSID with a reduced-rank\nregression step that directly learns the optimal gain required for the update\nstep from data. We refer to this extension of PSID as PSID with filtering.\nSecond, inspired by two-filter Kalman smoother formulations, we develop a novel\nforward-backward PSID smoothing algorithm where we first apply PSID with\nfiltering and then apply it again in the reverse time direction on the\nresiduals of the filtered secondary signal. We validate our methods on\nsimulated data, showing that our approach recovers the ground-truth model\nparameters for filtering, and achieves optimal filtering and smoothing decoding\nperformance of the secondary signal that matches the ideal performance of the\ntrue underlying model. This work provides a principled framework for optimal\nlinear filtering and smoothing in the two-signal setting, significantly\nexpanding the toolkit for analyzing dynamic interactions in multivariate\ntime-series.", "published": "2025-07-21 06:39:31", "link": "http://arxiv.org/abs/2507.15288v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Mixture of Autoencoder Experts Guidance using Unlabeled and Incomplete Data for Exploration in Reinforcement Learning", "abstract": "Recent trends in Reinforcement Learning (RL) highlight the need for agents to\nlearn from reward-free interactions and alternative supervision signals, such\nas unlabeled or incomplete demonstrations, rather than relying solely on\nexplicit reward maximization. Additionally, developing generalist agents that\ncan adapt efficiently in real-world environments often requires leveraging\nthese reward-free signals to guide learning and behavior. However, while\nintrinsic motivation techniques provide a means for agents to seek out novel or\nuncertain states in the absence of explicit rewards, they are often challenged\nby dense reward environments or the complexity of high-dimensional state and\naction spaces. Furthermore, most existing approaches rely directly on the\nunprocessed intrinsic reward signals, which can make it difficult to shape or\ncontrol the agent's exploration effectively. We propose a framework that can\neffectively utilize expert demonstrations, even when they are incomplete and\nimperfect. By applying a mapping function to transform the similarity between\nan agent's state and expert data into a shaped intrinsic reward, our method\nallows for flexible and targeted exploration of expert-like behaviors. We\nemploy a Mixture of Autoencoder Experts to capture a diverse range of behaviors\nand accommodate missing information in demonstrations. Experiments show our\napproach enables robust exploration and strong performance in both sparse and\ndense reward environments, even when demonstrations are sparse or incomplete.\nThis provides a practical framework for RL in realistic settings where optimal\ndata is unavailable and precise reward control is needed.", "published": "2025-07-21 06:38:46", "link": "http://arxiv.org/abs/2507.15287v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Conditional Video Generation for High-Efficiency Video Compression", "abstract": "Perceptual studies demonstrate that conditional diffusion models excel at\nreconstructing video content aligned with human visual perception. Building on\nthis insight, we propose a video compression framework that leverages\nconditional diffusion models for perceptually optimized reconstruction.\nSpecifically, we reframe video compression as a conditional generation task,\nwhere a generative model synthesizes video from sparse, yet informative\nsignals. Our approach introduces three key modules: (1) Multi-granular\nconditioning that captures both static scene structure and dynamic\nspatio-temporal cues; (2) Compact representations designed for efficient\ntransmission without sacrificing semantic richness; (3) Multi-condition\ntraining with modality dropout and role-aware embeddings, which prevent\nover-reliance on any single modality and enhance robustness. Extensive\nexperiments show that our method significantly outperforms both traditional and\nneural codecs on perceptual quality metrics such as Fr\\'echet Video Distance\n(FVD) and LPIPS, especially under high compression ratios.", "published": "2025-07-21 06:16:27", "link": "http://arxiv.org/abs/2507.15269v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "IM-Chat: A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry", "abstract": "The injection molding industry faces critical challenges in preserving and\ntransferring field knowledge, particularly as experienced workers retire and\nmultilingual barriers hinder effective communication. This study introduces\nIM-Chat, a multi-agent framework based on large language models (LLMs),\ndesigned to facilitate knowledge transfer in injection molding. IM-Chat\nintegrates both limited documented knowledge (e.g., troubleshooting tables,\nmanuals) and extensive field data modeled through a data-driven process\ncondition generator that infers optimal manufacturing settings from\nenvironmental inputs such as temperature and humidity, enabling robust and\ncontext-aware task resolution. By adopting a retrieval-augmented generation\n(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat\nensures adaptability without the need for fine-tuning. Performance was assessed\nacross 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and\nGPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance\nand correctness, and was further supplemented by automated evaluation using\nGPT-4o guided by a domain-adapted instruction prompt. The evaluation results\nindicate that more capable models tend to achieve higher accuracy, particularly\nin complex, tool-integrated scenarios. Overall, these findings demonstrate the\nviability of multi-agent LLM systems for industrial knowledge workflows and\nestablish IM-Chat as a scalable and generalizable approach to AI-assisted\ndecision support in manufacturing.", "published": "2025-07-21 06:13:53", "link": "http://arxiv.org/abs/2507.15268v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Optimal Transceiver Design in Over-the-Air Federated Distillation", "abstract": "The rapid proliferation and growth of artificial intelligence (AI) has led to\nthe development of federated learning (FL). FL allows wireless devices (WDs) to\ncooperatively learn by sharing only local model parameters, without needing to\nshare the entire dataset. However, the emergence of large AI models has made\nexisting FL approaches inefficient, due to the significant communication\noverhead required. In this paper, we propose a novel over-the-air federated\ndistillation (FD) framework by synergizing the strength of FL and knowledge\ndistillation to avoid the heavy local model transmission. Instead of sharing\nthe model parameters, only the WDs' model outputs, referred to as knowledge,\nare shared and aggregated over-the-air by exploiting the superposition property\nof the multiple-access channel. We shall study the transceiver design in\nover-the-air FD, aiming to maximize the learning convergence rate while meeting\nthe power constraints of the transceivers. The main challenge lies in the\nintractability of the learning performance analysis, as well as the non-convex\nnature and the optimization spanning the whole FD training period. To tackle\nthis problem, we first derive an analytical expression of the convergence rate\nin over-the-air FD. Then, the closed-form optimal solutions of the WDs'\ntransmit power and the estimator for over-the-air aggregation are obtained\ngiven the receiver combining strategy. Accordingly, we put forth an efficient\napproach to find the optimal receiver beamforming vector via semidefinite\nrelaxation. We further prove that there is no optimality gap between the\noriginal and relaxed problem for the receiver beamforming design. Numerical\nresults will show that the proposed over-the-air FD approach achieves a\nsignificant reduction in communication overhead, with only a minor compromise\nin testing accuracy compared to conventional FL benchmarks.", "published": "2025-07-21 05:37:08", "link": "http://arxiv.org/abs/2507.15256v1", "categories": ["eess.SP", "cs.AI"], "primary_category": "eess.SP"}
{"title": "MEETI: A Multimodal ECG Dataset from MIMIC-IV-ECG with Signals, Images, Features and Interpretations", "abstract": "Electrocardiogram (ECG) plays a foundational role in modern cardiovascular\ncare, enabling non-invasive diagnosis of arrhythmias, myocardial ischemia, and\nconduction disorders. While machine learning has achieved expert-level\nperformance in ECG interpretation, the development of clinically deployable\nmultimodal AI systems remains constrained, primarily due to the lack of\npublicly available datasets that simultaneously incorporate raw signals,\ndiagnostic images, and interpretation text. Most existing ECG datasets provide\nonly single-modality data or, at most, dual modalities, making it difficult to\nbuild models that can understand and integrate diverse ECG information in\nreal-world settings. To address this gap, we introduce MEETI (MIMIC-IV-Ext\nECG-Text-Image), the first large-scale ECG dataset that synchronizes raw\nwaveform data, high-resolution plotted images, and detailed textual\ninterpretations generated by large language models. In addition, MEETI includes\nbeat-level quantitative ECG parameters extracted from each lead, offering\nstructured parameters that support fine-grained analysis and model\ninterpretability. Each MEETI record is aligned across four components: (1) the\nraw ECG waveform, (2) the corresponding plotted image, (3) extracted feature\nparameters, and (4) detailed interpretation text. This alignment is achieved\nusing consistent, unique identifiers. This unified structure supports\ntransformer-based multimodal learning and supports fine-grained, interpretable\nreasoning about cardiac health. By bridging the gap between traditional signal\nanalysis, image-based interpretation, and language-driven understanding, MEETI\nestablished a robust foundation for the next generation of explainable,\nmultimodal cardiovascular AI. It offers the research community a comprehensive\nbenchmark for developing and evaluating ECG-based AI systems.", "published": "2025-07-21 05:32:44", "link": "http://arxiv.org/abs/2507.15255v1", "categories": ["eess.SP", "cs.AI", "cs.LG"], "primary_category": "eess.SP"}
{"title": "User Head Movement-Predictive XR in Immersive H2M Collaborations over Future Enterprise Networks", "abstract": "The evolution towards future generation of mobile systems and fixed wireless\nnetworks is primarily driven by the urgency to support high-bandwidth and\nlow-latency services across various vertical sectors. This endeavor is fueled\nby smartphones as well as technologies like industrial internet of things,\nextended reality (XR), and human-to-machine (H2M) collaborations for fostering\nindustrial and social revolutions like Industry 4.0/5.0 and Society 5.0. To\nensure an ideal immersive experience and avoid cyber-sickness for users in all\nthe aforementioned usage scenarios, it is typically challenging to synchronize\nXR content from a remote machine to a human collaborator according to their\nhead movements across a large geographic span in real-time over communication\nnetworks. Thus, we propose a novel H2M collaboration scheme where the human's\nhead movements are predicted ahead with highly accurate models like\nbidirectional long short-term memory networks to orient the machine's camera in\nadvance. We validate that XR frame size varies in accordance with the human's\nhead movements and predict the corresponding bandwidth requirements from the\nmachine's camera to propose a human-machine coordinated dynamic bandwidth\nallocation (HMC-DBA) scheme. Through extensive simulations, we show that\nend-to-end latency and jitter requirements of XR frames are satisfied with much\nlower bandwidth consumption over enterprise networks like\nFiber-To-The-Room-Business. Furthermore, we show that better efficiency in\nnetwork resource utilization is achieved by employing our proposed HMC-DBA over\nstate-of-the-art schemes.", "published": "2025-07-21 05:31:24", "link": "http://arxiv.org/abs/2507.15254v1", "categories": ["cs.NI", "cs.AI"], "primary_category": "cs.NI"}
{"title": "Disentangling Homophily and Heterophily in Multimodal Graph Clustering", "abstract": "Multimodal graphs, which integrate unstructured heterogeneous data with\nstructured interconnections, offer substantial real-world utility but remain\ninsufficiently explored in unsupervised learning. In this work, we initiate the\nstudy of multimodal graph clustering, aiming to bridge this critical gap.\nThrough empirical analysis, we observe that real-world multimodal graphs often\nexhibit hybrid neighborhood patterns, combining both homophilic and\nheterophilic relationships. To address this challenge, we propose a novel\nframework -- \\textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which\ndecomposes the original hybrid graph into two complementary views: (1) a\nhomophily-enhanced graph that captures cross-modal class consistency, and (2)\nheterophily-aware graphs that preserve modality-specific inter-class\ndistinctions. We introduce a \\emph{Multimodal Dual-frequency Fusion} mechanism\nthat jointly filters these disentangled graphs through a dual-pass strategy,\nenabling effective multimodal integration while mitigating category confusion.\nOur self-supervised alignment objectives further guide the learning process\nwithout requiring labels. Extensive experiments on both multimodal and\nmulti-relational graph datasets demonstrate that DMGC achieves state-of-the-art\nperformance, highlighting its effectiveness and generalizability across diverse\nsettings. Our code is available at https://github.com/Uncnbb/DMGC.", "published": "2025-07-21 05:29:53", "link": "http://arxiv.org/abs/2507.15253v1", "categories": ["cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.AI"}
{"title": "Spatio-Temporal Demand Prediction for Food Delivery Using Attention-Driven Graph Neural Networks", "abstract": "Accurate demand forecasting is critical for enhancing the efficiency and\nresponsiveness of food delivery platforms, where spatial heterogeneity and\ntemporal fluctuations in order volumes directly influence operational\ndecisions. This paper proposes an attention-based Graph Neural Network\nframework that captures spatial-temporal dependencies by modeling the food\ndelivery environment as a graph. In this graph, nodes represent urban delivery\nzones, while edges reflect spatial proximity and inter-regional order flow\npatterns derived from historical data. The attention mechanism dynamically\nweighs the influence of neighboring zones, enabling the model to focus on the\nmost contextually relevant areas during prediction. Temporal trends are jointly\nlearned alongside spatial interactions, allowing the model to adapt to evolving\ndemand patterns. Extensive experiments on real-world food delivery datasets\ndemonstrate the superiority of the proposed model in forecasting future order\nvolumes with high accuracy. The framework offers a scalable and adaptive\nsolution to support proactive fleet positioning, resource allocation, and\ndispatch optimization in urban food delivery operations.", "published": "2025-07-21 05:10:32", "link": "http://arxiv.org/abs/2507.15246v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced Academic Search", "abstract": "Recent advances in large language models (LLMs) have opened new opportunities\nfor academic literature retrieval. However, existing systems often rely on\nrigid pipelines and exhibit limited reasoning capabilities. We introduce SPAR,\na multi-agent framework that incorporates RefChain-based query decomposition\nand query evolution to enable more flexible and effective search. To facilitate\nsystematic evaluation, we also construct SPARBench, a challenging benchmark\nwith expert-annotated relevance labels. Experimental results demonstrate that\nSPAR substantially outperforms strong baselines, achieving up to +56% F1 on\nAutoScholar and +23% F1 on SPARBench over the best-performing baseline.\nTogether, SPAR and SPARBench provide a scalable, interpretable, and\nhigh-performing foundation for advancing research in scholarly retrieval. Code\nand data will be available at: https://github.com/xiaofengShi/SPAR", "published": "2025-07-21 05:06:53", "link": "http://arxiv.org/abs/2507.15245v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation", "abstract": "Despite the progress in Cross-Domain Few-Shot Learning (CD-FSL), a model\npre-trained with DINO combined with a prototypical classifier outperforms the\nlatest SOTA methods. A crucial limitation that needs to be overcome is that\nupdating too many parameters of the transformers leads to overfitting due to\nthe scarcity of labeled samples. To address this challenge, we propose a new\nconcept, Coalescent Projection (CP), as an effective successor to soft prompts.\nAdditionally, we propose a novel pseudo-class generation method combined with\nSelf-Supervised Transformations (SSTs) that relies solely on the base domain to\nprepare the network for encountering unseen samples from different domains. The\nproposed method exhibits its effectiveness in comprehensive experiments on the\nextreme domain shift scenario of the BSCD-FSL benchmark. Our code is published\nat https://github.com/Naeem-Paeedeh/CPLSR.", "published": "2025-07-21 05:01:27", "link": "http://arxiv.org/abs/2507.15243v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Explainable Artificial Intelligence based Soft Evaluation Indicator for Arc Fault Diagnosis", "abstract": "Novel AI-based arc fault diagnosis models have demonstrated outstanding\nperformance in terms of classification accuracy. However, an inherent problem\nis whether these models can actually be trusted to find arc faults. In this\nlight, this work proposes a soft evaluation indicator that explains the outputs\nof arc fault diagnosis models, by defining the the correct explanation of arc\nfaults and leveraging Explainable Artificial Intelligence and real arc fault\nexperiments. Meanwhile, a lightweight balanced neural network is proposed to\nguarantee competitive accuracy and soft feature extraction score. In our\nexperiments, several traditional machine learning methods and deep learning\nmethods across two arc fault datasets with different sample times and noise\nlevels are utilized to test the effectiveness of the soft evaluation indicator.\nThrough this approach, the arc fault diagnosis models are easy to understand\nand trust, allowing practitioners to make informed and trustworthy decisions.", "published": "2025-07-21 04:52:43", "link": "http://arxiv.org/abs/2507.15239v1", "categories": ["cs.AI", "eess.SP"], "primary_category": "cs.AI"}
{"title": "Solving Formal Math Problems by Decomposition and Iterative Reflection", "abstract": "General-purpose Large Language Models (LLMs) have achieved remarkable success\nin intelligence, performing comparably to human experts on complex reasoning\ntasks such as coding and mathematical reasoning. However, generating formal\nproofs in specialized languages like Lean 4 remains a significant challenge for\nthese models, limiting their application in complex theorem proving and\nautomated verification. Current approaches typically require specializing\nmodels through fine-tuning on dedicated formal corpora, incurring high costs\nfor data collection and training. In this work, we introduce \\textbf{Delta\nProver}, an agent-based framework that orchestrates the interaction between a\ngeneral-purpose LLM and the Lean 4 proof environment. Delta Prover leverages\nthe reflection and reasoning capabilities of general-purpose LLMs to\ninteractively construct formal proofs in Lean 4, circumventing the need for\nmodel specialization. At its core, the agent integrates two novel,\ninterdependent components: an algorithmic framework for reflective\ndecomposition and iterative proof repair, and a custom Domain-Specific Language\n(DSL) built upon Lean 4 for streamlined subproblem management. \\textbf{Delta\nProver achieves a state-of-the-art 95.9\\% success rate on the miniF2F-test\nbenchmark, surpassing all existing approaches, including those requiring model\nspecialization.} Furthermore, Delta Prover exhibits a significantly stronger\ntest-time scaling law compared to standard Best-of-N proof strategies.\nCrucially, our findings demonstrate that general-purpose LLMs, when guided by\nan effective agentic structure, possess substantial untapped theorem-proving\ncapabilities. This presents a computationally efficient alternative to\nspecialized models for robust automated reasoning in formal environments.", "published": "2025-07-21 03:56:35", "link": "http://arxiv.org/abs/2507.15225v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI"}
{"title": "SimdBench: Benchmarking Large Language Models for SIMD-Intrinsic Code Generation", "abstract": "SIMD (Single Instruction Multiple Data) instructions and their compiler\nintrinsics are widely supported by modern processors to accelerate\nperformance-critical tasks. SIMD intrinsic programming, a trade-off between\ncoding productivity and high performance, is widely used in the development of\nmainstream performance-critical libraries and daily computing tasks. Large\nLanguage Models (LLMs), which have demonstrated strong and comprehensive\ncapabilities in code generation, show promise in assisting programmers with the\nchallenges of SIMD intrinsic programming. However, existing code-generation\nbenchmarks focus on only scalar code, and it is unclear how LLMs perform in\ngenerating vectorized code using SIMD intrinsics. To fill this gap, we propose\nSimdBench, the first code benchmark specifically designed for SIMD-intrinsic\ncode generation, comprising 136 carefully crafted tasks and targeting five\nrepresentative SIMD intrinsics: SSE (x86 Streaming SIMD Extension), AVX (x86\nAdvanced Vector Extension), Neon (ARM Advanced SIMD Extension), SVE (ARM\nScalable Vector Extension), and RVV (RISC-V Vector Extension). We conduct a\nsystematic evaluation (measuring both correctness and performance) of 18\nrepresentative LLMs on SimdBench, resulting in a series of novel and insightful\nfindings. Our evaluation results demonstrate that LLMs exhibit a universal\ndecrease in pass@k during SIMD-intrinsic code generation compared to\nscalar-code generation. Our in-depth analysis highlights promising directions\nfor the further advancement of LLMs in the challenging domain of SIMD-intrinsic\ncode generation. SimdBench is fully open source at\nhttps://anonymous.4open.science/r/SimdBench-1B3F/ to benefit the broader\nresearch community.", "published": "2025-07-21 03:55:41", "link": "http://arxiv.org/abs/2507.15224v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE"}
{"title": "Latent Denoising Makes Good Visual Tokenizers", "abstract": "Despite their fundamental role, it remains unclear what properties could make\nvisual tokenizers more effective for generative modeling. We observe that\nmodern generative models share a conceptually similar training objective --\nreconstructing clean signals from corrupted inputs such as Gaussian noise or\nmasking -- a process we term denoising. Motivated by this insight, we propose\naligning tokenizer embeddings directly with the downstream denoising objective,\nencouraging latent embeddings to be more easily reconstructed even when heavily\ncorrupted. To achieve this, we introduce the Latent Denoising Tokenizer\n(l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images\nfrom latent embeddings corrupted by interpolative noise and random masking.\nExtensive experiments on ImageNet 256x256 demonstrate that our tokenizer\nconsistently outperforms standard tokenizers across six representative\ngenerative models. Our findings highlight denoising as a fundamental design\nprinciple for tokenizer development, and we hope it could motivate new\nperspectives for future tokenizer design.", "published": "2025-07-21 17:59:56", "link": "http://arxiv.org/abs/2507.15856v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models", "abstract": "Recent progress in text-to-video (T2V) generation has enabled the synthesis\nof visually compelling and temporally coherent videos from natural language.\nHowever, these models often fall short in basic physical commonsense, producing\noutputs that violate intuitive expectations around causality, object behavior,\nand tool use. Addressing this gap, we present PhysVidBench, a benchmark\ndesigned to evaluate the physical reasoning capabilities of T2V systems. The\nbenchmark includes 383 carefully curated prompts, emphasizing tool use,\nmaterial properties, and procedural interactions, and domains where physical\nplausibility is crucial. For each prompt, we generate videos using diverse\nstate-of-the-art models and adopt a three-stage evaluation pipeline: (1)\nformulate grounded physics questions from the prompt, (2) caption the generated\nvideo with a vision-language model, and (3) task a language model to answer\nseveral physics-involved questions using only the caption. This indirect\nstrategy circumvents common hallucination issues in direct video-based\nevaluation. By highlighting affordances and tool-mediated actions, areas\noverlooked in current T2V evaluations, PhysVidBench provides a structured,\ninterpretable framework for assessing physical commonsense in generative video\nmodels.", "published": "2025-07-21 17:30:46", "link": "http://arxiv.org/abs/2507.15824v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Diffusion models for multivariate subsurface generation and efficient probabilistic inversion", "abstract": "Diffusion models offer stable training and state-of-the-art performance for\ndeep generative modeling tasks. Here, we consider their use in the context of\nmultivariate subsurface modeling and probabilistic inversion. We first\ndemonstrate that diffusion models enhance multivariate modeling capabilities\ncompared to variational autoencoders and generative adversarial networks. In\ndiffusion modeling, the generative process involves a comparatively large\nnumber of time steps with update rules that can be modified to account for\nconditioning data. We propose different corrections to the popular Diffusion\nPosterior Sampling approach by Chung et al. (2023). In particular, we introduce\na likelihood approximation accounting for the noise-contamination that is\ninherent in diffusion modeling. We assess performance in a multivariate\ngeological scenario involving facies and correlated acoustic impedance.\nConditional modeling is demonstrated using both local hard data (well logs) and\nnonlinear geophysics (fullstack seismic data). Our tests show significantly\nimproved statistical robustness, enhanced sampling of the posterior probability\ndensity function and reduced computational costs, compared to the original\napproach. The method can be used with both hard and indirect conditioning data,\nindividually or simultaneously. As the inversion is included within the\ndiffusion process, it is faster than other methods requiring an outer-loop\naround the generative model, such as Markov chain Monte Carlo.", "published": "2025-07-21 17:10:16", "link": "http://arxiv.org/abs/2507.15809v1", "categories": ["cs.CV", "cs.LG", "physics.geo-ph", "stat.AP"], "primary_category": "cs.CV"}
{"title": "Exploring Superposition and Interference in State-of-the-Art Low-Parameter Vision Models", "abstract": "The paper investigates the performance of state-of-the-art low-parameter deep\nneural networks for computer vision, focusing on bottleneck architectures and\ntheir behavior using superlinear activation functions. We address interference\nin feature maps, a phenomenon associated with superposition, where neurons\nsimultaneously encode multiple characteristics. Our research suggests that\nlimiting interference can enhance scaling and accuracy in very low-scaled\nnetworks (under 1.5M parameters). We identify key design elements that reduce\ninterference by examining various bottleneck architectures, leading to a more\nefficient neural network. Consequently, we propose a proof-of-concept\narchitecture named NoDepth Bottleneck built on mechanistic insights from our\nexperiments, demonstrating robust scaling accuracy on the ImageNet dataset.\nThese findings contribute to more efficient and scalable neural networks for\nthe low-parameter range and advance the understanding of bottlenecks in\ncomputer vision. https://caiac.pubpub.org/pub/3dh6rsel", "published": "2025-07-21 16:57:25", "link": "http://arxiv.org/abs/2507.15798v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Regularized Low-Rank Adaptation for Few-Shot Organ Segmentation", "abstract": "Parameter-efficient fine-tuning (PEFT) of pre-trained foundation models is\nincreasingly attracting interest in medical imaging due to its effectiveness\nand computational efficiency. Among these methods, Low-Rank Adaptation (LoRA)\nis a notable approach based on the assumption that the adaptation inherently\noccurs in a low-dimensional subspace. While it has shown good performance, its\nimplementation requires a fixed and unalterable rank, which might be\nchallenging to select given the unique complexities and requirements of each\nmedical imaging downstream task. Inspired by advancements in natural image\nprocessing, we introduce a novel approach for medical image segmentation that\ndynamically adjusts the intrinsic rank during adaptation. Viewing the low-rank\nrepresentation of the trainable weight matrices as a singular value\ndecomposition, we introduce an l_1 sparsity regularizer to the loss function,\nand tackle it with a proximal optimizer. The regularizer could be viewed as a\npenalty on the decomposition rank. Hence, its minimization enables to find\ntask-adapted ranks automatically. Our method is evaluated in a realistic\nfew-shot fine-tuning setting, where we compare it first to the standard LoRA\nand then to several other PEFT methods across two distinguishable tasks: base\norgans and novel organs. Our extensive experiments demonstrate the significant\nperformance improvements driven by our method, highlighting its efficiency and\nrobustness against suboptimal rank initialization. Our code is publicly\navailable: https://github.com/ghassenbaklouti/ARENA", "published": "2025-07-21 16:51:53", "link": "http://arxiv.org/abs/2507.15793v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Label tree semantic losses for rich multi-class medical image segmentation", "abstract": "Rich and accurate medical image segmentation is poised to underpin the next\ngeneration of AI-defined clinical practice by delineating critical anatomy for\npre-operative planning, guiding real-time intra-operative navigation, and\nsupporting precise post-operative assessment. However, commonly used learning\nmethods for medical and surgical imaging segmentation tasks penalise all errors\nequivalently and thus fail to exploit any inter-class semantics in the labels\nspace. This becomes particularly problematic as the cardinality and richness of\nlabels increases to include subtly different classes. In this work, we propose\ntwo tree-based semantic loss functions which take advantage of a hierarchical\norganisation of the labels. We further incorporate our losses in a recently\nproposed approach for training with sparse, background-free annotations to\nextend the applicability of our proposed losses. Extensive experiments are\nreported on two medical and surgical image segmentation tasks, namely head MRI\nfor whole brain parcellation (WBP) with full supervision and neurosurgical\nhyperspectral imaging (HSI) for scene understanding with sparse annotations.\nResults demonstrate that our proposed method reaches state-of-the-art\nperformance in both cases.", "published": "2025-07-21 16:32:48", "link": "http://arxiv.org/abs/2507.15777v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization", "abstract": "Dynamic Facial Expression Recognition (DFER) plays a critical role in\naffective computing and human-computer interaction. Although existing methods\nachieve comparable performance, they inevitably suffer from performance\ndegradation under sample heterogeneity caused by multi-source data and\nindividual expression variability. To address these challenges, we propose a\nnovel framework, called Heterogeneity-aware Distributional Framework (HDF), and\ndesign two plug-and-play modules to enhance time-frequency modeling and\nmitigate optimization imbalance caused by hard samples. Specifically, the\nTime-Frequency Distributional Attention Module (DAM) captures both temporal\nconsistency and frequency robustness through a dual-branch attention design,\nimproving tolerance to sequence inconsistency and visual style shifts. Then,\nbased on gradient sensitivity and information bottleneck principles, an\nadaptive optimization module Distribution-aware Scaling Module (DSM) is\nintroduced to dynamically balance classification and contrastive losses,\nenabling more stable and discriminative representation learning. Extensive\nexperiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF\nsignificantly improves both recognition accuracy and robustness. Our method\nachieves superior weighted average recall (WAR) and unweighted average recall\n(UAR) while maintaining strong generalization across diverse and imbalanced\nscenarios. Codes are released at https://github.com/QIcita/HDF_DFER.", "published": "2025-07-21 16:21:47", "link": "http://arxiv.org/abs/2507.15765v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Appearance Harmonization via Bilateral Grid Prediction with Transformers for 3DGS", "abstract": "Modern camera pipelines apply extensive on-device processing, such as\nexposure adjustment, white balance, and color correction, which, while\nbeneficial individually, often introduce photometric inconsistencies across\nviews. These appearance variations violate multi-view consistency and degrade\nthe quality of novel view synthesis. Joint optimization of scene\nrepresentations and per-image appearance embeddings has been proposed to\naddress this issue, but at the cost of increased computational complexity and\nslower training. In this work, we propose a transformer-based method that\npredicts spatially adaptive bilateral grids to correct photometric variations\nin a multi-view consistent manner, enabling robust cross-scene generalization\nwithout the need for scene-specific retraining. By incorporating the learned\ngrids into the 3D Gaussian Splatting pipeline, we improve reconstruction\nquality while maintaining high training efficiency. Extensive experiments show\nthat our approach outperforms or matches existing scene-specific optimization\nmethods in reconstruction fidelity and convergence speed.", "published": "2025-07-21 16:03:58", "link": "http://arxiv.org/abs/2507.15748v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "TokensGen: Harnessing Condensed Tokens for Long Video Generation", "abstract": "Generating consistent long videos is a complex challenge: while\ndiffusion-based generative models generate visually impressive short clips,\nextending them to longer durations often leads to memory bottlenecks and\nlong-term inconsistency. In this paper, we propose TokensGen, a novel two-stage\nframework that leverages condensed tokens to address these issues. Our method\ndecomposes long video generation into three core tasks: (1) inner-clip semantic\ncontrol, (2) long-term consistency control, and (3) inter-clip smooth\ntransition. First, we train To2V (Token-to-Video), a short video diffusion\nmodel guided by text and video tokens, with a Video Tokenizer that condenses\nshort clips into semantically rich tokens. Second, we introduce T2To\n(Text-to-Token), a video token diffusion transformer that generates all tokens\nat once, ensuring global consistency across clips. Finally, during inference,\nan adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips,\nreducing boundary artifacts and enhancing smooth transitions. Experimental\nresults demonstrate that our approach significantly enhances long-term temporal\nand content coherence without incurring prohibitive computational overhead. By\nleveraging condensed tokens and pre-trained short video models, our method\nprovides a scalable, modular solution for long video generation, opening new\npossibilities for storytelling, cinematic production, and immersive\nsimulations. Please see our project page at\nhttps://vicky0522.github.io/tokensgen-webpage/ .", "published": "2025-07-21 15:37:33", "link": "http://arxiv.org/abs/2507.15728v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "A Practical Investigation of Spatially-Controlled Image Generation with Transformers", "abstract": "Enabling image generation models to be spatially controlled is an important\narea of research, empowering users to better generate images according to their\nown fine-grained specifications via e.g. edge maps, poses. Although this task\nhas seen impressive improvements in recent times, a focus on rapidly producing\nstronger models has come at the cost of detailed and fair scientific\ncomparison. Differing training data, model architectures and generation\nparadigms make it difficult to disentangle the factors contributing to\nperformance. Meanwhile, the motivations and nuances of certain approaches\nbecome lost in the literature. In this work, we aim to provide clear takeaways\nacross generation paradigms for practitioners wishing to develop\ntransformer-based systems for spatially-controlled generation, clarifying the\nliterature and addressing knowledge gaps. We perform controlled experiments on\nImageNet across diffusion-based/flow-based and autoregressive (AR) models.\nFirst, we establish control token prefilling as a simple, general and\nperformant baseline approach for transformers. We then investigate previously\nunderexplored sampling time enhancements, showing that extending\nclassifier-free guidance to control, as well as softmax truncation, have a\nstrong impact on control-generation consistency. Finally, we re-clarify the\nmotivation of adapter-based approaches, demonstrating that they mitigate\n\"forgetting\" and maintain generation quality when trained on limited downstream\ndata, but underperform full training in terms of generation-control\nconsistency. Code will be released upon publication.", "published": "2025-07-21 15:33:49", "link": "http://arxiv.org/abs/2507.15724v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Efficient Face Image Quality Assessment via Self-training and Knowledge Distillation", "abstract": "Face image quality assessment (FIQA) is essential for various face-related\napplications. Although FIQA has been extensively studied and achieved\nsignificant progress, the computational complexity of FIQA algorithms remains a\nkey concern for ensuring scalability and practical deployment in real-world\nsystems. In this paper, we aim to develop a computationally efficient FIQA\nmethod that can be easily deployed in real-world applications. Specifically,\nour method consists of two stages: training a powerful teacher model and\ndistilling a lightweight student model from it. To build a strong teacher\nmodel, we adopt a self-training strategy to improve its capacity. We first\ntrain the teacher model using labeled face images, then use it to generate\npseudo-labels for a set of unlabeled images. These pseudo-labeled samples are\nused in two ways: (1) to distill knowledge into the student model, and (2) to\ncombine with the original labeled images to further enhance the teacher model\nthrough self-training. The enhanced teacher model is used to further\npseudo-label another set of unlabeled images for distilling the student models.\nThe student model is trained using a combination of labeled images,\npseudo-labeled images from the original teacher model, and pseudo-labeled\nimages from the enhanced teacher model. Experimental results demonstrate that\nour student model achieves comparable performance to the teacher model with an\nextremely low computational overhead. Moreover, our method achieved first place\nin the ICCV 2025 VQualA FIQA Challenge. The code is available at\nhttps://github.com/sunwei925/Efficient-FIQA.git.", "published": "2025-07-21 15:17:01", "link": "http://arxiv.org/abs/2507.15709v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting", "abstract": "Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in\nreconstructing high-quality novel views, as it often overfits to the\nwidely-varying high-frequency (HF) details of the sparse training views. While\nfrequency regularization can be a promising approach, its typical reliance on\nFourier transforms causes difficult parameter tuning and biases towards\ndetrimental HF learning. We propose DWTGS, a framework that rethinks frequency\nregularization by leveraging wavelet-space losses that provide additional\nspatial supervision. Specifically, we supervise only the low-frequency (LF) LL\nsubbands at multiple DWT levels, while enforcing sparsity on the HF HH subband\nin a self-supervised manner. Experiments across benchmarks show that DWTGS\nconsistently outperforms Fourier-based counterparts, as this LF-centric\nstrategy improves generalization and reduces HF hallucinations.", "published": "2025-07-21 14:56:46", "link": "http://arxiv.org/abs/2507.15690v1", "categories": ["cs.CV", "eess.IV", "eess.SP"], "primary_category": "cs.CV"}
{"title": "Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization for Remote Sensing", "abstract": "Visual relocalization, which estimates the 6-degree-of-freedom (6-DoF) camera\npose from query images, is fundamental to remote sensing and UAV applications.\nExisting methods face inherent trade-offs: image-based retrieval and pose\nregression approaches lack precision, while structure-based methods that\nregister queries to Structure-from-Motion (SfM) models suffer from\ncomputational complexity and limited scalability. These challenges are\nparticularly pronounced in remote sensing scenarios due to large-scale scenes,\nhigh altitude variations, and domain gaps of existing visual priors. To\novercome these limitations, we leverage 3D Gaussian Splatting (3DGS) as a novel\nscene representation that compactly encodes both 3D geometry and appearance. We\nintroduce $\\mathrm{Hi}^2$-GSLoc, a dual-hierarchical relocalization framework\nthat follows a sparse-to-dense and coarse-to-fine paradigm, fully exploiting\nthe rich semantic information and geometric constraints inherent in Gaussian\nprimitives. To handle large-scale remote sensing scenarios, we incorporate\npartitioned Gaussian training, GPU-accelerated parallel matching, and dynamic\nmemory management strategies. Our approach consists of two stages: (1) a sparse\nstage featuring a Gaussian-specific consistent render-aware sampling strategy\nand landmark-guided detector for robust and accurate initial pose estimation,\nand (2) a dense stage that iteratively refines poses through coarse-to-fine\ndense rasterization matching while incorporating reliability verification.\nThrough comprehensive evaluation on simulation data, public datasets, and real\nflight experiments, we demonstrate that our method delivers competitive\nlocalization accuracy, recall rate, and computational efficiency while\neffectively filtering unreliable pose estimates. The results confirm the\neffectiveness of our approach for practical remote sensing applications.", "published": "2025-07-21 14:47:56", "link": "http://arxiv.org/abs/2507.15683v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Visual-Language Model Knowledge Distillation Method for Image Quality Assessment", "abstract": "Image Quality Assessment (IQA) is a core task in computer vision. Multimodal\nmethods based on vision-language models, such as CLIP, have demonstrated\nexceptional generalization capabilities in IQA tasks. To address the issues of\nexcessive parameter burden and insufficient ability to identify local distorted\nfeatures in CLIP for IQA, this study proposes a visual-language model knowledge\ndistillation method aimed at guiding the training of models with architectural\nadvantages using CLIP's IQA knowledge. First, quality-graded prompt templates\nwere designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned\nto enhance its capabilities in IQA tasks. Finally, a modality-adaptive\nknowledge distillation strategy is proposed to achieve guidance from the CLIP\nteacher model to the student model. Our experiments were conducted on multiple\nIQA datasets, and the results show that the proposed method significantly\nreduces model complexity while outperforming existing IQA methods,\ndemonstrating strong potential for practical deployment.", "published": "2025-07-21 14:44:46", "link": "http://arxiv.org/abs/2507.15680v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "HW-MLVQA: Elucidating Multilingual Handwritten Document Understanding with a Comprehensive VQA Benchmark", "abstract": "The proliferation of MultiLingual Visual Question Answering (MLVQA)\nbenchmarks augments the capabilities of large language models (LLMs) and\nmulti-modal LLMs, thereby enabling them to adeptly capture the intricate\nlinguistic subtleties and visual complexities inherent across diverse\nlanguages. Despite its potential, the current MLVQA model struggles to fully\nutilize its capabilities when dealing with the extensive variety of handwritten\ndocuments. This article delineates HW-MLVQA, an avant-garde VQA benchmark\nmeticulously crafted to mitigate the dearth of authentic Multilingual\nHandwritten document comprehension. HW-MLVQA encompasses an extensive\ncollection of 1,600 handwritten Pages complemented by 2,400 question-answers.\nFurthermore, it provides a robust benchmark evaluation framework spanning three\ndistinct modalities: text, image, and an integrated image & text modality. To\nsimulate authentic real-world contexts devoid of ground truth textual\ntranscriptions, we facilitates a rigorous assessment of proprietary and\nopen-source OCR models. The benchmark aspires to facilitate pivotal\nadvancements in multilingual handwritten document interpretation, fostering\ninnovation and scholarly inquiry within this specialized domain.", "published": "2025-07-21 14:16:44", "link": "http://arxiv.org/abs/2507.15655v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Extracting Visual Facts from Intermediate Layers for Mitigating Hallucinations in Multimodal Large Language Models", "abstract": "Multimodal Large Language Models (MLLMs) have made significant strides by\ncombining visual recognition and language understanding to generate content\nthat is both coherent and contextually accurate. However, MLLMs continue to\nstruggle with object hallucinations, where models produce seemingly plausible\nbut factually incorrect outputs, including objects that do not exist in the\nimage. Recent work has revealed that the prior knowledge in MLLMs significantly\nsuppresses visual information in deep layers, causing hallucinatory outputs.\nHowever, how these priors suppress visual information at the intermediate layer\nstage in MLLMs remains unclear. We observe that visual factual knowledge and\nthe differences between intermediate-layer prior/original probability\ndistributions show similar evolutionary trends in intermediate layers.\nMotivated by this, we introduce Decoding by Extracting Visual Facts (EVA), a\nsimple, training-free method that dynamically selects intermediate layers with\nthe most significant visual factual information. By contrasting the output\ndistributions of the selected layer derived from the original input and\npure-text input, EVA extracts visual factual knowledge and proportionally\nincorporates it into the final layer to correct the output logits. Importantly,\nEVA is model-agnostic, seamlessly integrates with various classic decoding\nstrategies, and is applicable across different MLLMs. We validate EVA on\nwidely-used benchmarks, and the results show that it significantly reduces\nhallucination rates compared to baseline methods, underscoring its\neffectiveness in mitigating hallucinations.", "published": "2025-07-21 14:15:34", "link": "http://arxiv.org/abs/2507.15652v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Experimenting active and sequential learning in a medieval music manuscript", "abstract": "Optical Music Recognition (OMR) is a cornerstone of music digitization\ninitiatives in cultural heritage, yet it remains limited by the scarcity of\nannotated data and the complexity of historical manuscripts. In this paper, we\npresent a preliminary study of Active Learning (AL) and Sequential Learning\n(SL) tailored for object detection and layout recognition in an old medieval\nmusic manuscript. Leveraging YOLOv8, our system selects samples with the\nhighest uncertainty (lowest prediction confidence) for iterative labeling and\nretraining. Our approach starts with a single annotated image and successfully\nboosts performance while minimizing manual labeling. Experimental results\nindicate that comparable accuracy to fully supervised training can be achieved\nwith significantly fewer labeled examples. We test the methodology as a\npreliminary investigation on a novel dataset offered to the community by the\nAnonymous project, which studies laude, a poetical-musical genre spread across\nItaly during the 12th-16th Century. We show that in the manuscript at-hand,\nuncertainty-based AL is not effective and advocates for more usable methods in\ndata-scarcity scenarios.", "published": "2025-07-21 13:55:54", "link": "http://arxiv.org/abs/2507.15633v1", "categories": ["cs.CV", "I.2.10; I.4.8; H.3.3"], "primary_category": "cs.CV"}
{"title": "Gaussian Splatting with Discretized SDF for Relightable Assets", "abstract": "3D Gaussian splatting (3DGS) has shown its detailed expressive ability and\nhighly efficient rendering speed in the novel view synthesis (NVS) task. The\napplication to inverse rendering still faces several challenges, as the\ndiscrete nature of Gaussian primitives makes it difficult to apply geometry\nconstraints. Recent works introduce the signed distance field (SDF) as an extra\ncontinuous representation to regularize the geometry defined by Gaussian\nprimitives. It improves the decomposition quality, at the cost of increasing\nmemory usage and complicating training. Unlike these works, we introduce a\ndiscretized SDF to represent the continuous SDF in a discrete manner by\nencoding it within each Gaussian using a sampled value. This approach allows us\nto link the SDF with the Gaussian opacity through an SDF-to-opacity\ntransformation, enabling rendering the SDF via splatting and avoiding the\ncomputational cost of ray marching.The key challenge is to regularize the\ndiscrete samples to be consistent with the underlying SDF, as the discrete\nrepresentation can hardly apply the gradient-based constraints (\\eg Eikonal\nloss). For this, we project Gaussians onto the zero-level set of SDF and\nenforce alignment with the surface from splatting, namely a projection-based\nconsistency loss. Thanks to the discretized SDF, our method achieves higher\nrelighting quality, while requiring no extra memory beyond GS and avoiding\ncomplex manually designed optimization. The experiments reveal that our method\noutperforms existing Gaussian-based inverse rendering methods. Our code is\navailable at https://github.com/NK-CS-ZZL/DiscretizedSDF.", "published": "2025-07-21 13:52:33", "link": "http://arxiv.org/abs/2507.15629v1", "categories": ["cs.GR", "cs.CV"], "primary_category": "cs.GR"}
{"title": "A Survey on Efficiency Optimization Techniques for DNN-based Video Analytics: Process Systems, Algorithms, and Applications", "abstract": "The explosive growth of video data in recent years has brought higher demands\nfor video analytics, where accuracy and efficiency remain the two primary\nconcerns. Deep neural networks (DNNs) have been widely adopted to ensure\naccuracy; however, improving their efficiency in video analytics remains an\nopen challenge. Different from existing surveys that make summaries of\nDNN-based video mainly from the accuracy optimization aspect, in this survey,\nwe aim to provide a thorough review of optimization techniques focusing on the\nimprovement of the efficiency of DNNs in video analytics. We organize existing\nmethods in a bottom-up manner, covering multiple perspectives such as hardware\nsupport, data processing, operational deployment, etc. Finally, based on the\noptimization framework and existing works, we analyze and discuss the problems\nand challenges in the performance optimization of DNN-based video analytics.", "published": "2025-07-21 13:52:06", "link": "http://arxiv.org/abs/2507.15628v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "CylinderPlane: Nested Cylinder Representation for 3D-aware Image Generation", "abstract": "While the proposal of the Tri-plane representation has advanced the\ndevelopment of the 3D-aware image generative models, problems rooted in its\ninherent structure, such as multi-face artifacts caused by sharing the same\nfeatures in symmetric regions, limit its ability to generate 360$^\\circ$ view\nimages. In this paper, we propose CylinderPlane, a novel implicit\nrepresentation based on Cylindrical Coordinate System, to eliminate the feature\nambiguity issue and ensure multi-view consistency in 360$^\\circ$. Different\nfrom the inevitable feature entanglement in Cartesian coordinate-based\nTri-plane representation, the cylindrical coordinate system explicitly\nseparates features at different angles, allowing our cylindrical representation\npossible to achieve high-quality, artifacts-free 360$^\\circ$ image synthesis.\nWe further introduce the nested cylinder representation that composites\nmultiple cylinders at different scales, thereby enabling the model more\nadaptable to complex geometry and varying resolutions. The combination of\ncylinders with different resolutions can effectively capture more critical\nlocations and multi-scale features, greatly facilitates fine detail learning\nand robustness to different resolutions. Moreover, our representation is\nagnostic to implicit rendering methods and can be easily integrated into any\nneural rendering pipeline. Extensive experiments on both synthetic dataset and\nunstructured in-the-wild images demonstrate that our proposed representation\nachieves superior performance over previous methods.", "published": "2025-07-21 13:28:59", "link": "http://arxiv.org/abs/2507.15606v1", "categories": ["cs.CV", "68T45", "I.4.5"], "primary_category": "cs.CV"}
{"title": "SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting", "abstract": "Surface reconstruction and novel view rendering from sparse-view images are\nchallenging. Signed Distance Function (SDF)-based methods struggle with fine\ndetails, while 3D Gaussian Splatting (3DGS)-based approaches lack global\ngeometry coherence. We propose a novel hybrid method that combines the\nstrengths of both approaches: SDF captures coarse geometry to enhance\n3DGS-based rendering, while newly rendered images from 3DGS refine the details\nof SDF for accurate surface reconstruction. As a result, our method surpasses\nstate-of-the-art approaches in surface reconstruction and novel view synthesis\non the DTU and MobileBrick datasets. Code will be released at\nhttps://github.com/Gaozihui/SurfaceSplat.", "published": "2025-07-21 13:25:03", "link": "http://arxiv.org/abs/2507.15602v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos", "abstract": "We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained\non large-scale human videos. Existing VLAs struggle with complex manipulation\ntasks requiring high dexterity and generalize poorly to novel scenarios and\ntasks, primarily due to their reliance on synthetic data with significant\nsim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To\naddress this data bottleneck, we propose leveraging human hands as a foundation\nmanipulator, capitalizing on the rich dexterity and scalability present in web\ndata. Our approach centers on physical instruction tuning, a novel training\nparadigm that combines large-scale VLA pretraining from human videos, physical\nspace alignment for 3D reasoning, and post-training adaptation for robotic\ntasks. Additionally, we introduce a part-level motion tokenization method which\nachieves millimeter-level reconstruction accuracy to model precise hand\ntrajectories for action learning. To support our proposed paradigm, we further\ndevelop a comprehensive data curation pipeline that integrates heterogeneous\nsources -- including motion capture, VR, and RGB-only videos -- into a\nlarge-scale dataset with millions of motion-based instructional instances. We\nempirically show the excellence of Being-H0 in hand motion generation and\ninstruction following, and it also scales well with model and data sizes.\nImportantly, we observe the expected gains of Being-H0 in real-world robotic\nmanipulation as physical instruction tuning is applied. More details are\navailable at https://beingbeyond.github.io/Being-H0.", "published": "2025-07-21 13:19:09", "link": "http://arxiv.org/abs/2507.15597v1", "categories": ["cs.CV", "cs.LG", "cs.RO"], "primary_category": "cs.CV"}
{"title": "SegDT: A Diffusion Transformer-Based Segmentation Model for Medical Imaging", "abstract": "Medical image segmentation is crucial for many healthcare tasks, including\ndisease diagnosis and treatment planning. One key area is the segmentation of\nskin lesions, which is vital for diagnosing skin cancer and monitoring\npatients. In this context, this paper introduces SegDT, a new segmentation\nmodel based on diffusion transformer (DiT). SegDT is designed to work on\nlow-cost hardware and incorporates Rectified Flow, which improves the\ngeneration quality at reduced inference steps and maintains the flexibility of\nstandard diffusion models. Our method is evaluated on three benchmarking\ndatasets and compared against several existing works, achieving\nstate-of-the-art results while maintaining fast inference speeds. This makes\nthe proposed model appealing for real-world medical applications. This work\nadvances the performance and capabilities of deep learning models in medical\nimage analysis, enabling faster, more accurate diagnostic tools for healthcare\nprofessionals. The code is made publicly available at\n\\href{https://github.com/Bekhouche/SegDT}{GitHub}.", "published": "2025-07-21 13:18:05", "link": "http://arxiv.org/abs/2507.15595v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Compress-Align-Detect: onboard change detection from unregistered images", "abstract": "Change detection from satellite images typically incurs a delay ranging from\nseveral hours up to days because of latency in downlinking the acquired images\nand generating orthorectified image products at the ground stations; this may\npreclude real- or near real-time applications. To overcome this limitation, we\npropose shifting the entire change detection workflow onboard satellites. This\nrequires to simultaneously solve challenges in data storage, image registration\nand change detection with a strict complexity constraint. In this paper, we\npresent a novel and efficient framework for onboard change detection that\naddresses the aforementioned challenges in an end-to-end fashion with a deep\nneural network composed of three interlinked submodules: (1) image compression,\ntailored to minimize onboard data storage resources; (2) lightweight\nco-registration of non-orthorectified multi-temporal image pairs; and (3) a\nnovel temporally-invariant and computationally efficient change detection\nmodel. This is the first approach in the literature combining all these tasks\nin a single end-to-end framework with the constraints dictated by onboard\nprocessing. Experimental results compare each submodule with the current\nstate-of-the-art, and evaluate the performance of the overall integrated system\nin realistic setting on low-power hardware. Compelling change detection results\nare obtained in terms of F1 score as a function of compression rate, sustaining\na throughput of 0.7 Mpixel/s on a 15W accelerator.", "published": "2025-07-21 12:58:32", "link": "http://arxiv.org/abs/2507.15578v1", "categories": ["cs.CV", "eess.IV"], "primary_category": "cs.CV"}
{"title": "DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding", "abstract": "In recent years, the introduction of Multi-modal Large Language Models\n(MLLMs) into video understanding tasks has become increasingly prevalent.\nHowever, how to effectively integrate temporal information remains a critical\nresearch focus. Traditional approaches treat spatial and temporal information\nseparately. Due to issues like motion blur, it is challenging to accurately\nrepresent the spatial information of rapidly moving objects. This can lead to\ntemporally important regions being underemphasized during spatial feature\nextraction, which in turn hinders accurate spatio-temporal interaction and\nvideo understanding. To address this limitation, we propose an innovative video\nrepresentation method called Dynamic-Image (DynImg). Specifically, we introduce\na set of non-key frames as temporal prompts to highlight the spatial areas\ncontaining fast-moving objects. During the process of visual feature\nextraction, these prompts guide the model to pay additional attention to the\nfine-grained spatial features corresponding to these regions. Moreover, to\nmaintain the correct sequence for DynImg, we employ a corresponding 4D video\nRotary Position Embedding. This retains both the temporal and spatial adjacency\nof DynImg, helping MLLM understand the spatio-temporal order within this\ncombined format. Experimental evaluations reveal that DynImg surpasses the\nstate-of-the-art methods by approximately 2% across multiple video\nunderstanding benchmarks, proving the effectiveness of our temporal prompts in\nenhancing video comprehension.", "published": "2025-07-21 12:50:49", "link": "http://arxiv.org/abs/2507.15569v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation", "abstract": "Zero-shot human-object interaction (HOI) detection remains a challenging\ntask, particularly in generalizing to unseen actions. Existing methods address\nthis challenge by tapping Vision-Language Models (VLMs) to access knowledge\nbeyond the training data. However, they either struggle to distinguish actions\ninvolving the same object or demonstrate limited generalization to unseen\nclasses. In this paper, we introduce HOLa (Zero-Shot HOI Detection with\nLow-Rank Decomposed VLM Feature Adaptation), a novel approach that both\nenhances generalization to unseen classes and improves action distinction. In\ntraining, HOLa decomposes VLM text features for given HOI classes via low-rank\nfactorization, producing class-shared basis features and adaptable weights.\nThese features and weights form a compact HOI representation that preserves\nshared information across classes, enhancing generalization to unseen classes.\nSubsequently, we refine action distinction by adapting weights for each HOI\nclass and introducing human-object tokens to enrich visual interaction\nrepresentations. To further distinguish unseen actions, we guide the weight\nadaptation with LLM-derived action regularization. Experimental results show\nthat our method sets a new state-of-the-art across zero-shot HOI settings on\nHICO-DET, achieving an unseen-class mAP of 27.91 in the unseen-verb setting.\nOur code is available at https://github.com/ChelsieLei/HOLa.", "published": "2025-07-21 12:15:27", "link": "http://arxiv.org/abs/2507.15542v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Towards Holistic Surgical Scene Graph", "abstract": "Surgical scene understanding is crucial for computer-assisted intervention\nsystems, requiring visual comprehension of surgical scenes that involves\ndiverse elements such as surgical tools, anatomical structures, and their\ninteractions. To effectively represent the complex information in surgical\nscenes, graph-based approaches have been explored to structurally model\nsurgical entities and their relationships. Previous surgical scene graph\nstudies have demonstrated the feasibility of representing surgical scenes using\ngraphs. However, certain aspects of surgical scenes-such as diverse\ncombinations of tool-action-target and the identity of the hand operating the\ntool-remain underexplored in graph-based representations, despite their\nimportance. To incorporate these aspects into graph representations, we propose\nEndoscapes-SG201 dataset, which includes annotations for tool-action-target\ncombinations and hand identity. We also introduce SSG-Com, a graph-based method\ndesigned to learn and represent these critical elements. Through experiments on\ndownstream tasks such as critical view of safety assessment and action triplet\nrecognition, we demonstrated the importance of integrating these essential\nscene graph components, highlighting their significant contribution to surgical\nscene understanding. The code and dataset are available at\nhttps://github.com/ailab-kyunghee/SSG-Com", "published": "2025-07-21 12:10:42", "link": "http://arxiv.org/abs/2507.15541v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Procedure Learning via Regularized Gromov-Wasserstein Optimal Transport", "abstract": "We study the problem of self-supervised procedure learning, which discovers\nkey steps and establishes their order from a set of unlabeled procedural\nvideos. Previous procedure learning methods typically learn frame-to-frame\ncorrespondences between videos before determining key steps and their order.\nHowever, their performance often suffers from order variations,\nbackground/redundant frames, and repeated actions. To overcome these\nchallenges, we propose a self-supervised procedure learning framework, which\nutilizes a fused Gromov-Wasserstein optimal transport formulation with a\nstructural prior for computing frame-to-frame mapping between videos. However,\noptimizing exclusively for the above temporal alignment term may lead to\ndegenerate solutions, where all frames are mapped to a small cluster in the\nembedding space and hence every video is associated with only one key step. To\naddress that limitation, we further integrate a contrastive regularization\nterm, which maps different frames to different points in the embedding space,\navoiding the collapse to trivial solutions. Finally, we conduct extensive\nexperiments on large-scale egocentric (i.e., EgoProceL) and third-person (i.e.,\nProceL and CrossTask) benchmarks to demonstrate superior performance by our\napproach against previous methods, including OPEL which relies on a traditional\nKantorovich optimal transport formulation with an optimality prior.", "published": "2025-07-21 12:09:12", "link": "http://arxiv.org/abs/2507.15540v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SAIGFormer: A Spatially-Adaptive Illumination-Guided Network for Low-Light Image Enhancement", "abstract": "Recent Transformer-based low-light enhancement methods have made promising\nprogress in recovering global illumination. However, they still struggle with\nnon-uniform lighting scenarios, such as backlit and shadow, appearing as\nover-exposure or inadequate brightness restoration. To address this challenge,\nwe present a Spatially-Adaptive Illumination-Guided Transformer (SAIGFormer)\nframework that enables accurate illumination restoration. Specifically, we\npropose a dynamic integral image representation to model the spatially-varying\nillumination, and further construct a novel Spatially-Adaptive Integral\nIllumination Estimator ($\\text{SAI}^2\\text{E}$). Moreover, we introduce an\nIllumination-Guided Multi-head Self-Attention (IG-MSA) mechanism, which\nleverages the illumination to calibrate the lightness-relevant features toward\nvisual-pleased illumination enhancement. Extensive experiments on five standard\nlow-light datasets and a cross-domain benchmark (LOL-Blur) demonstrate that our\nSAIGFormer significantly outperforms state-of-the-art methods in both\nquantitative and qualitative metrics. In particular, our method achieves\nsuperior performance in non-uniform illumination enhancement while exhibiting\nstrong generalization capabilities across multiple datasets. Code is available\nat https://github.com/LHTcode/SAIGFormer.git.", "published": "2025-07-21 11:38:56", "link": "http://arxiv.org/abs/2507.15520v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Quantifying and Narrowing the Unknown: Interactive Text-to-Video Retrieval via Uncertainty Minimization", "abstract": "Despite recent advances, Text-to-video retrieval (TVR) is still hindered by\nmultiple inherent uncertainties, such as ambiguous textual queries, indistinct\ntext-video mappings, and low-quality video frames. Although interactive systems\nhave emerged to address these challenges by refining user intent through\nclarifying questions, current methods typically rely on heuristic or ad-hoc\nstrategies without explicitly quantifying these uncertainties, limiting their\neffectiveness. Motivated by this gap, we propose UMIVR, an\nUncertainty-Minimizing Interactive Text-to-Video Retrieval framework that\nexplicitly quantifies three critical uncertainties-text ambiguity, mapping\nuncertainty, and frame uncertainty-via principled, training-free metrics:\nsemantic entropy-based Text Ambiguity Score (TAS), Jensen-Shannon\ndivergence-based Mapping Uncertainty Score (MUS), and a Temporal Quality-based\nFrame Sampler (TQFS). By adaptively generating targeted clarifying questions\nguided by these uncertainty measures, UMIVR iteratively refines user queries,\nsignificantly reducing retrieval ambiguity. Extensive experiments on multiple\nbenchmarks validate UMIVR's effectiveness, achieving notable gains in Recall@1\n(69.2\\% after 10 interactive rounds) on the MSR-VTT-1k dataset, thereby\nestablishing an uncertainty-minimizing foundation for interactive TVR.", "published": "2025-07-21 11:12:39", "link": "http://arxiv.org/abs/2507.15504v1", "categories": ["cs.CV", "68T45", "I.2.10; H.3.3"], "primary_category": "cs.CV"}
{"title": "Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images", "abstract": "Odometry is a critical task for autonomous systems for self-localization and\nnavigation. We propose a novel LiDAR-Visual odometry framework that integrates\nLiDAR point clouds and images for accurate and robust pose estimation. Our\nmethod utilizes a dense-depth map estimated from point clouds and images\nthrough depth completion, and incorporates a multi-scale feature extraction\nnetwork with attention mechanisms, enabling adaptive depth-aware\nrepresentations. Furthermore, we leverage dense depth information to refine\nflow estimation and mitigate errors in occlusion-prone regions. Our\nhierarchical pose refinement module optimizes motion estimation progressively,\nensuring robust predictions against dynamic environments and scale ambiguities.\nComprehensive experiments on the KITTI odometry benchmark demonstrate that our\napproach achieves similar or superior accuracy and robustness compared to\nstate-of-the-art visual and LiDAR odometry methods.", "published": "2025-07-21 10:58:10", "link": "http://arxiv.org/abs/2507.15496v1", "categories": ["cs.CV", "cs.LG", "cs.RO"], "primary_category": "cs.CV"}
{"title": "An aerial color image anomaly dataset for search missions in complex forested terrain", "abstract": "After a family murder in rural Germany, authorities failed to locate the\nsuspect in a vast forest despite a massive search. To aid the search, a\nresearch aircraft captured high-resolution aerial imagery. Due to dense\nvegetation obscuring small clues, automated analysis was ineffective, prompting\na crowd-search initiative. This effort produced a unique dataset of labeled,\nhard-to-detect anomalies under occluded, real-world conditions. It can serve as\na benchmark for improving anomaly detection approaches in complex forest\nenvironments, supporting manhunts and rescue operations. Initial benchmark\ntests showed existing methods performed poorly, highlighting the need for\ncontext-aware approaches. The dataset is openly accessible for offline\nprocessing. An additional interactive web interface supports online viewing and\ndynamic growth by allowing users to annotate and submit new findings.", "published": "2025-07-21 10:52:27", "link": "http://arxiv.org/abs/2507.15492v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Prompt-aware of Frame Sampling for Efficient Text-Video Retrieval", "abstract": "Enabling efficient text-video retrieval on edge-end devices is critical for\nreal-world applications. Yet, existing methods face a critical challenge in\nbalancing accuracy and computational efficiency: uniform frame sampling methods\nensure content coverage but incur prohibitive computational costs, while\nsalient-frame sampling methods reduce overhead but suffer from query-agnostic\nframe selection that biases retrieval results. To address this, we propose\nProCLIP, a user-centric framework that achieves state-of-the-art accuracy with\nsignificantly improved efficiency. We design a prompt-aware frame sampling\nstrategy that dynamically guides lightweight feature extractors using textual\nprompts to select semantically relevant frames, overcoming the limitations of\nexisting salient-frame sampling methods which rely on static, query-agnostic\nselection criteria. Moreover, we adopt a two-stage candidate pruning strategy\nthat combines rapid coarse filtering via a lightweight module with CLIP-powered\nfine-grained re-ranking, enhancing retrieval efficiency while preserving\naccuracy. Experiments across benchmarks show ProCLIP achieves 75.3% latency\nreduction versus baselines while maintaining competitive accuracy, i.e.,\nR@1=49.0 in MSR-VTT dataset. Code is available at\nhttps://github.com/tiffylong/ProCLIP.", "published": "2025-07-21 10:46:49", "link": "http://arxiv.org/abs/2507.15491v1", "categories": ["cs.MM", "cs.CV"], "primary_category": "cs.MM"}
{"title": "DeSamba: Decoupled Spectral Adaptive Framework for 3D Multi-Sequence MRI Lesion Classification", "abstract": "Magnetic Resonance Imaging (MRI) sequences provide rich spatial and frequency\ndomain information, which is crucial for accurate lesion classification in\nmedical imaging. However, effectively integrating multi-sequence MRI data for\nrobust 3D lesion classification remains a challenge. In this paper, we propose\nDeSamba (Decoupled Spectral Adaptive Network and Mamba-Based Model), a novel\nframework designed to extract decoupled representations and adaptively fuse\nspatial and spectral features for lesion classification. DeSamba introduces a\nDecoupled Representation Learning Module (DRLM) that decouples features from\ndifferent MRI sequences through self-reconstruction and cross-reconstruction,\nand a Spectral Adaptive Modulation Block (SAMB) within the proposed SAMNet,\nenabling dynamic fusion of spectral and spatial information based on lesion\ncharacteristics. We evaluate DeSamba on two clinically relevant 3D datasets. On\na six-class spinal metastasis dataset (n=1,448), DeSamba achieves 62.10% Top-1\naccuracy, 63.62% F1-score, 87.71% AUC, and 93.55% Top-3 accuracy on an external\nvalidation set (n=372), outperforming all state-of-the-art (SOTA) baselines. On\na spondylitis dataset (n=251) involving a challenging binary classification\ntask, DeSamba achieves 70.00%/64.52% accuracy and 74.75/73.88 AUC on internal\nand external validation sets, respectively. Ablation studies demonstrate that\nboth DRLM and SAMB significantly contribute to overall performance, with over\n10% relative improvement compared to the baseline. Our results highlight the\npotential of DeSamba as a generalizable and effective solution for 3D lesion\nclassification in multi-sequence medical imaging.", "published": "2025-07-21 10:42:21", "link": "http://arxiv.org/abs/2507.15487v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "One Last Attention for Your Vision-Language Model", "abstract": "Pretrained vision-language models (VLMs), such as CLIP, achieve remarkable\nzero-shot performance, yet their downstream potential hinges on effective\nfine-tuning. Most adaptation methods typically focus on refining representation\nfrom separate modalities (text or vision) but neglect the critical role of\ntheir fused representations in the decision-making process, \\emph{\\ie} rational\nmatrix that drives the final prediction. To bridge the gap, we propose a simple\nyet effective \\textbf{R}ational \\textbf{Ada}ptaion ({RAda}) to explicitly\nexploit the final fused representation during fine-tuning. RAda employs a\nlearned mask, obtained from a lightweight attention layer attached at the end\nof a VLM, to dynamically calibrate the contribution of each element in the\nrational matrix, enabling targeted adjustments to the final cross-modal\ninteractions without incurring costly modifications to intermediate features.\nExperiments in different settings (i.e., updating, or freezing pretrained\nencoders in adaptation, and test-time training that can only access the\nunlabeled test data) show that RAda serves as a versatile fine-tuning\ntechnique, improving the baseline with minimal code and performing comparably\nagainst current arts in most settings. Code is available at\n\\href{https://github.com/khufia/RAda/tree/main}{github.com/khufia/RAda}.", "published": "2025-07-21 10:35:32", "link": "http://arxiv.org/abs/2507.15480v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "A Steel Surface Defect Detection Method Based on Lightweight Convolution Optimization", "abstract": "Surface defect detection of steel, especially the recognition of multi-scale\ndefects, has always been a major challenge in industrial manufacturing. Steel\nsurfaces not only have defects of various sizes and shapes, which limit the\naccuracy of traditional image processing and detection methods in complex\nenvironments. However, traditional defect detection methods face issues of\ninsufficient accuracy and high miss-detection rates when dealing with small\ntarget defects. To address this issue, this study proposes a detection\nframework based on deep learning, specifically YOLOv9s, combined with the\nC3Ghost module, SCConv module, and CARAFE upsampling operator, to improve\ndetection accuracy and model performance. First, the SCConv module is used to\nreduce feature redundancy and optimize feature representation by reconstructing\nthe spatial and channel dimensions. Second, the C3Ghost module is introduced to\nenhance the model's feature extraction ability by reducing redundant\ncomputations and parameter volume, thereby improving model efficiency. Finally,\nthe CARAFE upsampling operator, which can more finely reorganize feature maps\nin a content-aware manner, optimizes the upsampling process and ensures\ndetailed restoration of high-resolution defect regions. Experimental results\ndemonstrate that the proposed model achieves higher accuracy and robustness in\nsteel surface defect detection tasks compared to other methods, effectively\naddressing defect detection problems.", "published": "2025-07-21 10:30:38", "link": "http://arxiv.org/abs/2507.15476v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Low-Latency Event-Based Velocimetry for Quadrotor Control in a Narrow Pipe", "abstract": "Autonomous quadrotor flight in confined spaces such as pipes and tunnels\npresents significant challenges due to unsteady, self-induced aerodynamic\ndisturbances. Very recent advances have enabled flight in such conditions, but\nthey either rely on constant motion through the pipe to mitigate airflow\nrecirculation effects or suffer from limited stability during hovering. In this\nwork, we present the first closed-loop control system for quadrotors for\nhovering in narrow pipes that leverages real-time flow field measurements. We\ndevelop a low-latency, event-based smoke velocimetry method that estimates\nlocal airflow at high temporal resolution. This flow information is used by a\ndisturbance estimator based on a recurrent convolutional neural network, which\ninfers force and torque disturbances in real time. The estimated disturbances\nare integrated into a learning-based controller trained via reinforcement\nlearning. The flow-feedback control proves particularly effective during\nlateral translation maneuvers in the pipe cross-section. There, the real-time\ndisturbance information enables the controller to effectively counteract\ntransient aerodynamic effects, thereby preventing collisions with the pipe\nwall. To the best of our knowledge, this work represents the first\ndemonstration of an aerial robot with closed-loop control informed by real-time\nflow field measurements. This opens new directions for research on flight in\naerodynamically complex environments. In addition, our work also sheds light on\nthe characteristic flow structures that emerge during flight in narrow,\ncircular pipes, providing new insights at the intersection of robotics and\nfluid dynamics.", "published": "2025-07-21 09:53:42", "link": "http://arxiv.org/abs/2507.15444v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO"}
{"title": "SurgX: Neuron-Concept Association for Explainable Surgical Phase Recognition", "abstract": "Surgical phase recognition plays a crucial role in surgical workflow\nanalysis, enabling various applications such as surgical monitoring, skill\nassessment, and workflow optimization. Despite significant advancements in deep\nlearning-based surgical phase recognition, these models remain inherently\nopaque, making it difficult to understand how they make decisions. This lack of\ninterpretability hinders trust and makes it challenging to debug the model. To\naddress this challenge, we propose SurgX, a novel concept-based explanation\nframework that enhances the interpretability of surgical phase recognition\nmodels by associating neurons with relevant concepts. In this paper, we\nintroduce the process of selecting representative example sequences for\nneurons, constructing a concept set tailored to the surgical video dataset,\nassociating neurons with concepts and identifying neurons crucial for\npredictions. Through extensive experiments on two surgical phase recognition\nmodels, we validate our method and analyze the explanation for prediction. This\nhighlights the potential of our method in explaining surgical phase\nrecognition. The code is available at https://github.com/ailab-kyunghee/SurgX", "published": "2025-07-21 09:19:34", "link": "http://arxiv.org/abs/2507.15418v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Rethinking Occlusion in FER: A Semantic-Aware Perspective and Go Beyond", "abstract": "Facial expression recognition (FER) is a challenging task due to pervasive\nocclusion and dataset biases. Especially when facial information is partially\noccluded, existing FER models struggle to extract effective facial features,\nleading to inaccurate classifications. In response, we present ORSANet, which\nintroduces the following three key contributions: First, we introduce auxiliary\nmulti-modal semantic guidance to disambiguate facial occlusion and learn\nhigh-level semantic knowledge, which is two-fold: 1) we introduce semantic\nsegmentation maps as dense semantics prior to generate semantics-enhanced\nfacial representations; 2) we introduce facial landmarks as sparse geometric\nprior to mitigate intrinsic noises in FER, such as identity and gender biases.\nSecond, to facilitate the effective incorporation of these two multi-modal\npriors, we customize a Multi-scale Cross-interaction Module (MCM) to adaptively\nfuse the landmark feature and semantics-enhanced representations within\ndifferent scales. Third, we design a Dynamic Adversarial Repulsion Enhancement\nLoss (DARELoss) that dynamically adjusts the margins of ambiguous classes,\nfurther enhancing the model's ability to distinguish similar expressions. We\nfurther construct the first occlusion-oriented FER dataset to facilitate\nspecialized robustness analysis on various real-world occlusion conditions,\ndubbed Occlu-FER. Extensive experiments on both public benchmarks and Occlu-FER\ndemonstrate that our proposed ORSANet achieves SOTA recognition performance.\nCode is publicly available at https://github.com/Wenyuzhy/ORSANet-master.", "published": "2025-07-21 09:04:29", "link": "http://arxiv.org/abs/2507.15401v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Blended Point Cloud Diffusion for Localized Text-guided Shape Editing", "abstract": "Natural language offers a highly intuitive interface for enabling localized\nfine-grained edits of 3D shapes. However, prior works face challenges in\npreserving global coherence while locally modifying the input 3D shape. In this\nwork, we introduce an inpainting-based framework for editing shapes represented\nas point clouds. Our approach leverages foundation 3D diffusion models for\nachieving localized shape edits, adding structural guidance in the form of a\npartial conditional shape, ensuring that other regions correctly preserve the\nshape's identity. Furthermore, to encourage identity preservation also within\nthe local edited region, we propose an inference-time coordinate blending\nalgorithm which balances reconstruction of the full shape with inpainting at a\nprogression of noise levels during the inference process. Our coordinate\nblending algorithm seamlessly blends the original shape with its edited\nversion, enabling a fine-grained editing of 3D shapes, all while circumventing\nthe need for computationally expensive and often inaccurate inversion.\nExtensive experiments show that our method outperforms alternative techniques\nacross a wide range of metrics that evaluate both fidelity to the original\nshape and also adherence to the textual description.", "published": "2025-07-21 09:00:19", "link": "http://arxiv.org/abs/2507.15399v1", "categories": ["cs.GR", "cs.CV"], "primary_category": "cs.GR"}
{"title": "DAViD: Data-efficient and Accurate Vision Models from Synthetic Data", "abstract": "The state of the art in human-centric computer vision achieves high accuracy\nand robustness across a diverse range of tasks. The most effective models in\nthis domain have billions of parameters, thus requiring extremely large\ndatasets, expensive training regimes, and compute-intensive inference. In this\npaper, we demonstrate that it is possible to train models on much smaller but\nhigh-fidelity synthetic datasets, with no loss in accuracy and higher\nefficiency. Using synthetic training data provides us with excellent levels of\ndetail and perfect labels, while providing strong guarantees for data\nprovenance, usage rights, and user consent. Procedural data synthesis also\nprovides us with explicit control on data diversity, that we can use to address\nunfairness in the models we train. Extensive quantitative assessment on real\ninput images demonstrates accuracy of our models on three dense prediction\ntasks: depth estimation, surface normal estimation, and soft foreground\nsegmentation. Our models require only a fraction of the cost of training and\ninference when compared with foundational models of similar accuracy. Our\nhuman-centric synthetic dataset and trained models are available at\nhttps://aka.ms/DAViD.", "published": "2025-07-21 08:17:41", "link": "http://arxiv.org/abs/2507.15365v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "RoadFusion: Latent Diffusion Model for Pavement Defect Detection", "abstract": "Pavement defect detection faces critical challenges including limited\nannotated data, domain shift between training and deployment environments, and\nhigh variability in defect appearances across different road conditions. We\npropose RoadFusion, a framework that addresses these limitations through\nsynthetic anomaly generation with dual-path feature adaptation. A latent\ndiffusion model synthesizes diverse, realistic defects using text prompts and\nspatial masks, enabling effective training under data scarcity. Two separate\nfeature adaptors specialize representations for normal and anomalous inputs,\nimproving robustness to domain shift and defect variability. A lightweight\ndiscriminator learns to distinguish fine-grained defect patterns at the patch\nlevel. Evaluated on six benchmark datasets, RoadFusion achieves consistently\nstrong performance across both classification and localization tasks, setting\nnew state-of-the-art in multiple metrics relevant to real-world road\ninspection.", "published": "2025-07-21 08:01:08", "link": "http://arxiv.org/abs/2507.15346v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "BenchDepth: Are We on the Right Way to Evaluate Depth Foundation Models?", "abstract": "Depth estimation is a fundamental task in computer vision with diverse\napplications. Recent advancements in deep learning have led to powerful depth\nfoundation models (DFMs), yet their evaluation remains challenging due to\ninconsistencies in existing protocols. Traditional benchmarks rely on\nalignment-based metrics that introduce biases, favor certain depth\nrepresentations, and complicate fair comparisons. In this work, we propose\nBenchDepth, a new benchmark that evaluates DFMs through five carefully selected\ndownstream proxy tasks: depth completion, stereo matching, monocular\nfeed-forward 3D scene reconstruction, SLAM, and vision-language spatial\nunderstanding. Unlike conventional evaluation protocols, our approach assesses\nDFMs based on their practical utility in real-world applications, bypassing\nproblematic alignment procedures. We benchmark eight state-of-the-art DFMs and\nprovide an in-depth analysis of key findings and observations. We hope our work\nsparks further discussion in the community on best practices for depth model\nevaluation and paves the way for future research and advancements in depth\nestimation.", "published": "2025-07-21 07:23:14", "link": "http://arxiv.org/abs/2507.15321v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Few-Shot Object Detection via Spatial-Channel State Space Model", "abstract": "Due to the limited training samples in few-shot object detection (FSOD), we\nobserve that current methods may struggle to accurately extract effective\nfeatures from each channel. Specifically, this issue manifests in two aspects:\ni) channels with high weights may not necessarily be effective, and ii)\nchannels with low weights may still hold significant value. To handle this\nproblem, we consider utilizing the inter-channel correlation to facilitate the\nnovel model's adaptation process to novel conditions, ensuring the model can\ncorrectly highlight effective channels and rectify those incorrect ones. Since\nthe channel sequence is also 1-dimensional, its similarity with the temporal\nsequence inspires us to take Mamba for modeling the correlation in the channel\nsequence. Based on this concept, we propose a Spatial-Channel State Space\nModeling (SCSM) module for spatial-channel state modeling, which highlights the\neffective patterns and rectifies those ineffective ones in feature channels. In\nSCSM, we design the Spatial Feature Modeling (SFM) module to balance the\nlearning of spatial relationships and channel relationships, and then introduce\nthe Channel State Modeling (CSM) module based on Mamba to learn correlation in\nchannels. Extensive experiments on the VOC and COCO datasets show that the SCSM\nmodule enables the novel detector to improve the quality of focused feature\nrepresentation in channels and achieve state-of-the-art performance.", "published": "2025-07-21 07:08:19", "link": "http://arxiv.org/abs/2507.15308v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Minutiae-Anchored Local Dense Representation for Fingerprint Matching", "abstract": "Fingerprint matching under diverse capture conditions remains a fundamental\nchallenge in biometric recognition. To achieve robust and accurate performance\nin such scenarios, we propose DMD, a minutiae-anchored local dense\nrepresentation which captures both fine-grained ridge textures and\ndiscriminative minutiae features in a spatially structured manner.\nSpecifically, descriptors are extracted from local patches centered and\noriented on each detected minutia, forming a three-dimensional tensor, where\ntwo dimensions represent spatial locations on the fingerprint plane and the\nthird encodes semantic features. This representation explicitly captures\nabstract features of local image patches, enabling a multi-level, fine-grained\ndescription that aggregates information from multiple minutiae and their\nsurrounding ridge structures. Furthermore, thanks to its strong spatial\ncorrespondence with the patch image, DMD allows for the use of foreground\nsegmentation masks to identify valid descriptor regions. During matching,\ncomparisons are then restricted to overlapping foreground areas, improving\nefficiency and robustness. Extensive experiments on rolled, plain, parital,\ncontactless, and latent fingerprint datasets demonstrate the effectiveness and\ngeneralizability of the proposed method. It achieves state-of-the-art accuracy\nacross multiple benchmarks while maintaining high computational efficiency,\nshowing strong potential for large-scale fingerprint recognition. Corresponding\ncode is available at https://github.com/Yu-Yy/DMD.", "published": "2025-07-21 06:55:54", "link": "http://arxiv.org/abs/2507.15297v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems", "abstract": "Recent advances in biometric systems have significantly improved the\ndetection and prevention of fraudulent activities. However, as detection\nmethods improve, attack techniques become increasingly sophisticated. Attacks\non face recognition systems can be broadly divided into physical and digital\napproaches. Traditionally, deep learning models have been the primary defence\nagainst such attacks. While these models perform exceptionally well in\nscenarios for which they have been trained, they often struggle to adapt to\ndifferent types of attacks or varying environmental conditions. These\nsubsystems require substantial amounts of training data to achieve reliable\nperformance, yet biometric data collection faces significant challenges,\nincluding privacy concerns and the logistical difficulties of capturing diverse\nattack scenarios under controlled conditions. This work investigates the\napplication of Vision Language Models (VLM) and proposes an in-context learning\nframework for detecting physical presentation attacks and digital morphing\nattacks in biometric systems. Focusing on open-source models, the first\nsystematic framework for the quantitative evaluation of VLMs in\nsecurity-critical scenarios through in-context learning techniques is\nestablished. The experimental evaluation conducted on freely available\ndatabases demonstrates that the proposed subsystem achieves competitive\nperformance for physical and digital attack detection, outperforming some of\nthe traditional CNNs without resource-intensive training. The experimental\nresults validate the proposed framework as a promising tool for improving\ngeneralisation in attack detection.", "published": "2025-07-21 06:35:46", "link": "http://arxiv.org/abs/2507.15285v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "MinCD-PnP: Learning 2D-3D Correspondences with Approximate Blind PnP", "abstract": "Image-to-point-cloud (I2P) registration is a fundamental problem in computer\nvision, focusing on establishing 2D-3D correspondences between an image and a\npoint cloud. The differential perspective-n-point (PnP) has been widely used to\nsupervise I2P registration networks by enforcing the projective constraints on\n2D-3D correspondences. However, differential PnP is highly sensitive to noise\nand outliers in the predicted correspondences. This issue hinders the\neffectiveness of correspondence learning. Inspired by the robustness of blind\nPnP against noise and outliers in correspondences, we propose an approximated\nblind PnP based correspondence learning approach. To mitigate the high\ncomputational cost of blind PnP, we simplify blind PnP to an amenable task of\nminimizing Chamfer distance between learned 2D and 3D keypoints, called\nMinCD-PnP. To effectively solve MinCD-PnP, we design a lightweight multi-task\nlearning module, named as MinCD-Net, which can be easily integrated into the\nexisting I2P registration architectures. Extensive experiments on 7-Scenes,\nRGBD-V2, ScanNet, and self-collected datasets demonstrate that MinCD-Net\noutperforms state-of-the-art methods and achieves a higher inlier ratio (IR)\nand registration recall (RR) in both cross-scene and cross-dataset settings.", "published": "2025-07-21 05:38:16", "link": "http://arxiv.org/abs/2507.15257v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers", "abstract": "In light of recent breakthroughs in text-to-image (T2I) generation,\nparticularly with diffusion transformers (DiT), subject-driven technologies are\nincreasingly being employed for high-fidelity customized production that\npreserves subject identity from reference inputs, enabling thrilling design\nworkflows and engaging entertainment. Existing alternatives typically require\neither per-subject optimization via trainable text embeddings or training\nspecialized encoders for subject feature extraction on large-scale datasets.\nSuch dependencies on training procedures fundamentally constrain their\npractical applications. More importantly, current methodologies fail to fully\nleverage the inherent zero-shot potential of modern diffusion transformers\n(e.g., the Flux series) for authentic subject-driven synthesis. To bridge this\ngap, we propose FreeCus, a genuinely training-free framework that activates\nDiT's capabilities through three key innovations: 1) We introduce a pivotal\nattention sharing mechanism that captures the subject's layout integrity while\npreserving crucial editing flexibility. 2) Through a straightforward analysis\nof DiT's dynamic shifting, we propose an upgraded variant that significantly\nimproves fine-grained feature extraction. 3) We further integrate advanced\nMultimodal Large Language Models (MLLMs) to enrich cross-modal semantic\nrepresentations. Extensive experiments reflect that our method successfully\nunlocks DiT's zero-shot ability for consistent subject synthesis across diverse\ncontexts, achieving state-of-the-art or comparable results compared to\napproaches that require additional training. Notably, our framework\ndemonstrates seamless compatibility with existing inpainting pipelines and\ncontrol modules, facilitating more compelling experiences. Our code is\navailable at: https://github.com/Monalissaa/FreeCus.", "published": "2025-07-21 05:15:45", "link": "http://arxiv.org/abs/2507.15249v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse Autoencoders", "abstract": "Interpretability is critical in high-stakes domains such as medical imaging,\nwhere understanding model decisions is essential for clinical adoption. In this\nwork, we introduce Sparse Autoencoder (SAE)-based interpretability to breast\nimaging by analyzing {Mammo-CLIP}, a vision--language foundation model\npretrained on large-scale mammogram image--report pairs. We train a patch-level\n\\texttt{Mammo-SAE} on Mammo-CLIP to identify and probe latent features\nassociated with clinically relevant breast concepts such as \\textit{mass} and\n\\textit{suspicious calcification}. Our findings reveal that top activated class\nlevel latent neurons in the SAE latent space often tend to align with ground\ntruth regions, and also uncover several confounding factors influencing the\nmodel's decision-making process. Additionally, we analyze which latent neurons\nthe model relies on during downstream finetuning for improving the breast\nconcept prediction. This study highlights the promise of interpretable SAE\nlatent representations in providing deeper insight into the internal workings\nof foundation models at every layer for breast imaging.", "published": "2025-07-21 03:59:21", "link": "http://arxiv.org/abs/2507.15227v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Hierarchical Part-based Generative Model for Realistic 3D Blood Vessel", "abstract": "Advancements in 3D vision have increased the impact of blood vessel modeling\non medical applications. However, accurately representing the complex geometry\nand topology of blood vessels remains a challenge due to their intricate\nbranching patterns, curvatures, and irregular shapes. In this study, we propose\na hierarchical part-based frame work for 3D vessel generation that separates\nthe global binary tree-like topology from local geometric details. Our approach\nproceeds in three stages: (1) key graph generation to model the overall\nhierarchical struc ture, (2) vessel segment generation conditioned on geometric\nproperties, and (3) hierarchical vessel assembly by integrating the local\nsegments according to the global key graph. We validate our framework on real\nworld datasets, demonstrating superior performance over existing methods in\nmodeling complex vascular networks. This work marks the first successful\napplication of a part-based generative approach for 3D vessel modeling, setting\na new benchmark for vascular data generation. The code is available at:\nhttps://github.com/CybercatChen/PartVessel.git.", "published": "2025-07-21 03:52:25", "link": "http://arxiv.org/abs/2507.15223v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Improving Joint Embedding Predictive Architecture with Diffusion Noise", "abstract": "Self-supervised learning has become an incredibly successful method for\nfeature learning, widely applied to many downstream tasks. It has proven\nespecially effective for discriminative tasks, surpassing the trending\ngenerative models. However, generative models perform better in image\ngeneration and detail enhancement. Thus, it is natural for us to find a\nconnection between SSL and generative models to further enhance the\nrepresentation capacity of SSL. As generative models can create new samples by\napproximating the data distribution, such modeling should also lead to a\nsemantic understanding of the raw visual data, which is necessary for\nrecognition tasks. This enlightens us to combine the core principle of the\ndiffusion model: diffusion noise, with SSL to learn a competitive recognition\nmodel. Specifically, diffusion noise can be viewed as a particular state of\nmask that reveals a close relationship between masked image modeling (MIM) and\ndiffusion models. In this paper, we propose N-JEPA (Noise-based JEPA) to\nincorporate diffusion noise into MIM by the position embedding of masked\ntokens. The multi-level noise schedule is a series of feature augmentations to\nfurther enhance the robustness of our model. We perform a comprehensive study\nto confirm its effectiveness in the classification of downstream tasks. Codes\nwill be released soon in public.", "published": "2025-07-21 03:36:58", "link": "http://arxiv.org/abs/2507.15216v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "MeshMamba: State Space Models for Articulated 3D Mesh Generation and Reconstruction", "abstract": "In this paper, we introduce MeshMamba, a neural network model for learning 3D\narticulated mesh models by employing the recently proposed Mamba State Space\nModels (Mamba-SSMs). MeshMamba is efficient and scalable in handling a large\nnumber of input tokens, enabling the generation and reconstruction of body mesh\nmodels with more than 10,000 vertices, capturing clothing and hand geometries.\nThe key to effectively learning MeshMamba is the serialization technique of\nmesh vertices into orderings that are easily processed by Mamba. This is\nachieved by sorting the vertices based on body part annotations or the 3D\nvertex locations of a template mesh, such that the ordering respects the\nstructure of articulated shapes. Based on MeshMamba, we design 1) MambaDiff3D,\na denoising diffusion model for generating 3D articulated meshes and 2)\nMamba-HMR, a 3D human mesh recovery model that reconstructs a human body shape\nand pose from a single image. Experimental results showed that MambaDiff3D can\ngenerate dense 3D human meshes in clothes, with grasping hands, etc., and\noutperforms previous approaches in the 3D human shape generation task.\nAdditionally, Mamba-HMR extends the capabilities of previous non-parametric\nhuman mesh recovery approaches, which were limited to handling body-only poses\nusing around 500 vertex tokens, to the whole-body setting with face and hands,\nwhile achieving competitive performance in (near) real-time.", "published": "2025-07-21 03:24:30", "link": "http://arxiv.org/abs/2507.15212v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Personalized 4D Whole Heart Geometry Reconstruction from Cine MRI for Cardiac Digital Twins", "abstract": "Cardiac digital twins (CDTs) provide personalized in-silico cardiac\nrepresentations and hold great potential for precision medicine in cardiology.\nHowever, whole-heart CDT models that simulate the full organ-scale\nelectromechanics of all four heart chambers remain limited. In this work, we\npropose a weakly supervised learning model to reconstruct 4D (3D+t) heart mesh\ndirectly from multi-view 2D cardiac cine MRIs. This is achieved by learning a\nself-supervised mapping between cine MRIs and 4D cardiac meshes, enabling the\ngeneration of personalized heart models that closely correspond to input cine\nMRIs. The resulting 4D heart meshes can facilitate the automatic extraction of\nkey cardiac variables, including ejection fraction and dynamic chamber volume\nchanges with high temporal resolution. It demonstrates the feasibility of\ninferring personalized 4D heart models from cardiac MRIs, paving the way for an\nefficient CDT platform for precision medicine. The code will be publicly\nreleased once the manuscript is accepted.", "published": "2025-07-21 03:01:33", "link": "http://arxiv.org/abs/2507.15203v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Personalized 3D Myocardial Infarct Geometry Reconstruction from Cine MRI with Explicit Cardiac Motion Modeling", "abstract": "Accurate representation of myocardial infarct geometry is crucial for\npatient-specific cardiac modeling in MI patients. While Late gadolinium\nenhancement (LGE) MRI is the clinical gold standard for infarct detection, it\nrequires contrast agents, introducing side effects and patient discomfort.\nMoreover, infarct reconstruction from LGE often relies on sparsely sampled 2D\nslices, limiting spatial resolution and accuracy. In this work, we propose a\nnovel framework for automatically reconstructing high-fidelity 3D myocardial\ninfarct geometry from 2D clinically standard cine MRI, eliminating the need for\ncontrast agents. Specifically, we first reconstruct the 4D biventricular mesh\nfrom multi-view cine MRIs via an automatic deep shape fitting model, biv-me.\nThen, we design a infarction reconstruction model, CMotion2Infarct-Net, to\nexplicitly utilize the motion patterns within this dynamic geometry to localize\ninfarct regions. Evaluated on 205 cine MRI scans from 126 MI patients, our\nmethod shows reasonable agreement with manual delineation. This study\ndemonstrates the feasibility of contrast-free, cardiac motion-driven 3D infarct\nreconstruction, paving the way for efficient digital twin of MI.", "published": "2025-07-21 02:43:35", "link": "http://arxiv.org/abs/2507.15194v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "A Study of Anatomical Priors for Deep Learning-Based Segmentation of Pheochromocytoma in Abdominal CT", "abstract": "Accurate segmentation of pheochromocytoma (PCC) in abdominal CT scans is\nessential for tumor burden estimation, prognosis, and treatment planning. It\nmay also help infer genetic clusters, reducing reliance on expensive testing.\nThis study systematically evaluates anatomical priors to identify\nconfigurations that improve deep learning-based PCC segmentation. We employed\nthe nnU-Net framework to evaluate eleven annotation strategies for accurate 3D\nsegmentation of pheochromocytoma, introducing a set of novel multi-class\nschemes based on organ-specific anatomical priors. These priors were derived\nfrom adjacent organs commonly surrounding adrenal tumors (e.g., liver, spleen,\nkidney, aorta, adrenal gland, and pancreas), and were compared against a broad\nbody-region prior used in previous work. The framework was trained and tested\non 105 contrast-enhanced CT scans from 91 patients at the NIH Clinical Center.\nPerformance was measured using Dice Similarity Coefficient (DSC), Normalized\nSurface Distance (NSD), and instance-wise F1 score. Among all strategies, the\nTumor + Kidney + Aorta (TKA) annotation achieved the highest segmentation\naccuracy, significantly outperforming the previously used Tumor + Body (TB)\nannotation across DSC (p = 0.0097), NSD (p = 0.0110), and F1 score (25.84%\nimprovement at an IoU threshold of 0.5), measured on a 70-30 train-test split.\nThe TKA model also showed superior tumor burden quantification (R^2 = 0.968)\nand strong segmentation across all genetic subtypes. In five-fold\ncross-validation, TKA consistently outperformed TB across IoU thresholds (0.1\nto 0.5), reinforcing its robustness and generalizability. These findings\nhighlight the value of incorporating relevant anatomical context in deep\nlearning models to achieve precise PCC segmentation, supporting clinical\nassessment and longitudinal monitoring.", "published": "2025-07-21 02:35:29", "link": "http://arxiv.org/abs/2507.15193v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "eess.IV"}
{"title": "The Complexity of Color-constrained Paths in Semicomplete Multipartite Digraphs", "abstract": "Every semicomplete multipartite digraph contains a quasi-Hamiltonian path,\nbut the problem of finding a quasi-Hamiltonian path with prescribed start and\nend vertex is NP-complete even when restricted to semicomplete multipartite\ndigraphs with independence number exactly 3. Bang-Jensen, Wang and Yeo (arXiv\n2024) showed that deciding the presence of a quasi-Hamiltonian cycle which does\nnot contain at least one vertex from each color class is NP-complete.\nSimilarly, deciding the presence of a quasi-Hamiltonian cycle which intersects\nevery part exactly once is also NP-complete as shown in the same work.\n  In this paper, we continue the study of paths with constraints on the number\nof covered vertices from each color class. We consider the problem of finding a\npath with prescribed start and end vertex that contains at least $a$ and at\nmost $b$ vertices from each color class where all color classes have size\nexactly $\\alpha$. This unifies the Hamiltonian path problem, the\nquasi-Hamiltonian path problem and the path-version of the cycle problems\nmentioned above, among other problems. Using Schaefer's dichotomy theorem, we\nclassify the complexity of almost all problems in our framework. Notable open\nproblems are the Hamiltonian path problem on semicomplete multipartite digraphs\nas well as the quasi-Hamiltonian path problem restricted to semicomplete\nmultipartite digraphs with independence number 2.\n  We then investigate the quasi-Hamiltonian path problem restricted to\nsemicomplete multipartite digraphs with independence number 2. We generalize\nsufficient criteria for Hamiltonian $(s,t)$-paths in semicomplete digraphs to\nsufficient criteria for quasi-Hamiltonian $(s,t)$-paths in this class. Although\nthis does not settle the problem, the initial results suggest that this special\ncase may be solvable in polynomial time.", "published": "2025-07-21 14:32:47", "link": "http://arxiv.org/abs/2507.15667v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "On zeros and algorithms for disordered systems: mean-field spin glasses", "abstract": "Spin glasses are fundamental probability distributions at the core of\nstatistical physics, the theory of average-case computational complexity, and\nmodern high-dimensional statistical inference. In the mean-field setting, we\ndesign deterministic quasipolynomial-time algorithms for estimating the\npartition function to arbitrarily high accuracy for nearly all inverse\ntemperatures in the second moment regime. In particular, for the\nSherrington--Kirkpatrick model, our algorithms succeed for almost the entire\nreplica-symmetric phase. To achieve this, we study the locations of the zeros\nof the partition function. Notably, our methods are conceptually simple, and\napply equally well to the spherical case and the case of Ising spins.", "published": "2025-07-21 13:41:07", "link": "http://arxiv.org/abs/2507.15616v1", "categories": ["cs.DS", "cond-mat.dis-nn", "cs.DM", "math-ph", "math.MP", "math.PR"], "primary_category": "cs.DS"}
{"title": "Paired many-to-many 2-disjoint path cover of Johnson graphs", "abstract": "Given two 2 disjoint vertex-sets $S=\\{u,x\\}$ and $T=\\{v,y\\}$, a paired\nmany-to-many 2-disjoint path cover joining S and T, is a set of two\nvertex-disjoint paths with endpoints $u,v$ and $x,y$, respectively, that cover\nevery vertex of the graph. If the graph has a many-to-many 2-disjoint path\ncover for any two disjoint vertex-sets $S$ and $T$, then it is called paired\n2-coverable. It is known that if a graph is paired 2-coverable, then it must be\nHamilton-connected, but the reverse is not true. It has been proved that\nJohnson graphs $J(n,k)$, $0\\le k\\le n$, are Hamilton-connected by Brian Alspach\nin [Ars Math. Contemp. 6 (2013) 21--23]. In this paper, we prove that Johnson\ngraphs are paired 2-coverable. Moreover, we obtain that another family of\ngraphs $QJ(n,k)$ constructed from Johnson graphs by Alspach are also paired\n2-coverable.", "published": "2025-07-21 10:17:33", "link": "http://arxiv.org/abs/2507.15463v1", "categories": ["math.CO", "cs.DM", "05C38, 68R10"], "primary_category": "math.CO"}
{"title": "A Myhill-Nerode Type Characterization of 2detLIN Languages", "abstract": "Linear automata are automata with two reading heads starting from the two\nextremes of the input, are equivalent to 5' -> 3' Watson-Crick (WK) finite\nautomata. The heads read the input in opposite directions and the computation\nfinishes when the heads meet. These automata accept the class LIN of linear\nlanguages. The deterministic counterpart of these models, on the one hand, is\nless expressive, as only a proper subset of LIN, the class 2detLIN is accepted;\nand on the other hand, they are also equivalent in the sense of the class of\nthe accepted languages. Now, based on these automata models, we characterize\nthe class of 2detLIN languages with a Myhill-Nerode type of equivalence\nclasses. However, as these automata may do the computation of both the prefix\nand the suffix of the input, we use prefix-suffix pairs in our classes.\nAdditionally, it is proven that finitely many classes in the characterization\nmatch with the 2detLIN languages, but we have some constraints on the used\nprefix-suffix pairs, i.e., the characterization should have the property to be\ncomplete and it must not have any crossing pairs.", "published": "2025-07-21 07:15:27", "link": "http://arxiv.org/abs/2507.15316v1", "categories": ["cs.FL", "cs.DM", "cs.DS", "F.1.1;F.4.3;F.1.3"], "primary_category": "cs.FL"}
{"title": "The Labeled Coupon Collector Problem", "abstract": "We generalize the well-known Coupon Collector Problem (CCP) in combinatorics.\nOur problem is to find the minimum and expected number of draws, with\nreplacement, required to recover $n$ distinctly labeled coupons, with each draw\nconsisting of a random subset of $k$ different coupons and a random ordering of\ntheir associated labels. We specify two variations of the problem, Type-I in\nwhich the set of labels is known at the start, and Type-II in which the set of\nlabels is unknown at the start. We show that our problem can be viewed as an\nextension of the separating system problem introduced by R\\'enyi and Katona,\nprovide a full characterization of the minimum, and provide a numerical\napproach to finding the expectation using a Markov chain model, with special\nattention given to the case where two coupons are drawn at a time.", "published": "2025-07-21 04:24:03", "link": "http://arxiv.org/abs/2507.15231v1", "categories": ["cs.DM", "cs.IT", "math.CO", "math.IT"], "primary_category": "cs.DM"}
{"title": "Just Ask for Music (JAM): Multimodal and Personalized Natural Language Music Recommendation", "abstract": "Natural language interfaces offer a compelling approach for music\nrecommendation, enabling users to express complex preferences conversationally.\nWhile Large Language Models (LLMs) show promise in this direction, their\nscalability in recommender systems is limited by high costs and latency.\nRetrieval-based approaches using smaller language models mitigate these issues\nbut often rely on single-modal item representations, overlook long-term user\npreferences, and require full model retraining, posing challenges for\nreal-world deployment. In this paper, we present JAM (Just Ask for Music), a\nlightweight and intuitive framework for natural language music recommendation.\nJAM models user-query-item interactions as vector translations in a shared\nlatent space, inspired by knowledge graph embedding methods like TransE. To\ncapture the complexity of music and user intent, JAM aggregates multimodal item\nfeatures via cross-attention and sparse mixture-of-experts. We also introduce\nJAMSessions, a new dataset of over 100k user-query-item triples with anonymized\nuser/item embeddings, uniquely combining conversational queries and user\nlong-term preferences. Our results show that JAM provides accurate\nrecommendations, produces intuitive representations suitable for practical use\ncases, and can be easily integrated with existing music recommendation stacks.", "published": "2025-07-21 17:36:03", "link": "http://arxiv.org/abs/2507.15826v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "RankMixer: Scaling Up Ranking Models in Industrial Recommenders", "abstract": "Recent progress on large language models (LLMs) has spurred interest in\nscaling up recommendation systems, yet two practical obstacles remain. First,\ntraining and serving cost on industrial Recommenders must respect strict\nlatency bounds and high QPS demands. Second, most human-designed\nfeature-crossing modules in ranking models were inherited from the CPU era and\nfail to exploit modern GPUs, resulting in low Model Flops Utilization (MFU) and\npoor scalability. We introduce RankMixer, a hardware-aware model design\ntailored towards a unified and scalable feature-interaction architecture.\nRankMixer retains the transformer's high parallelism while replacing quadratic\nself-attention with multi-head token mixing module for higher efficiency.\nBesides, RankMixer maintains both the modeling for distinct feature subspaces\nand cross-feature-space interactions with Per-token FFNs. We further extend it\nto one billion parameters with a Sparse-MoE variant for higher ROI. A dynamic\nrouting strategy is adapted to address the inadequacy and imbalance of experts\ntraining. Experiments show RankMixer's superior scaling abilities on a\ntrillion-scale production dataset. By replacing previously diverse handcrafted\nlow-MFU modules with RankMixer, we boost the model MFU from 4.5% to 45%, and\nscale our ranking model parameters by 100x while maintaining roughly the same\ninference latency. We verify RankMixer's universality with online A/B tests\nacross three core application scenarios (Recommendation, Advertisement and\nSearch). Finally, we launch 1B Dense-Parameters RankMixer for full traffic\nserving without increasing the serving cost, which improves user active days by\n0.2% and total in-app usage duration by 0.5%.", "published": "2025-07-21 12:28:55", "link": "http://arxiv.org/abs/2507.15551v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Hierarchical Graph Information Bottleneck for Multi-Behavior Recommendation", "abstract": "In real-world recommendation scenarios, users typically engage with platforms\nthrough multiple types of behavioral interactions. Multi-behavior\nrecommendation algorithms aim to leverage various auxiliary user behaviors to\nenhance prediction for target behaviors of primary interest (e.g., buy),\nthereby overcoming performance limitations caused by data sparsity in target\nbehavior records. Current state-of-the-art approaches typically employ\nhierarchical design following either cascading (e.g.,\nview$\\rightarrow$cart$\\rightarrow$buy) or parallel\n(unified$\\rightarrow$behavior$\\rightarrow$specific components) paradigms, to\ncapture behavioral relationships. However, these methods still face two\ncritical challenges: (1) severe distribution disparities across behaviors, and\n(2) negative transfer effects caused by noise in auxiliary behaviors. In this\npaper, we propose a novel model-agnostic Hierarchical Graph Information\nBottleneck (HGIB) framework for multi-behavior recommendation to effectively\naddress these challenges. Following information bottleneck principles, our\nframework optimizes the learning of compact yet sufficient representations that\npreserve essential information for target behavior prediction while eliminating\ntask-irrelevant redundancies. To further mitigate interaction noise, we\nintroduce a Graph Refinement Encoder (GRE) that dynamically prunes redundant\nedges through learnable edge dropout mechanisms. We conduct comprehensive\nexperiments on three real-world public datasets, which demonstrate the superior\neffectiveness of our framework. Beyond these widely used datasets in the\nacademic community, we further expand our evaluation on several real industrial\nscenarios and conduct an online A/B testing, showing again a significant\nimprovement in multi-behavior recommendations. The source code of our proposed\nHGIB is available at https://github.com/zhy99426/HGIB.", "published": "2025-07-21 08:53:49", "link": "http://arxiv.org/abs/2507.15395v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "The Capacity of Semantic Private Information Retrieval with Colluding Servers", "abstract": "We study the problem of semantic private information retrieval (Sem-PIR) with\n$T$ colluding servers (Sem-TPIR), i.e., servers that collectively share user\nqueries. In Sem-TPIR, the message sizes are different, and message retrieval\nprobabilities by any user are not uniform. This is a generalization of the\nclassical PIR problem where the message sizes are equal and message retrieval\nprobabilities are identical. The earlier work on Sem-PIR considered the case of\nno collusions, i.e., the collusion parameter of $T=1$. In this paper, we\nconsider the general problem for arbitrary $T < N$. We find an upper bound on\nthe retrieval rate and design a scheme that achieves this rate, i.e., we derive\nthe exact capacity of Sem-TPIR.", "published": "2025-07-21 17:24:40", "link": "http://arxiv.org/abs/2507.15818v1", "categories": ["cs.IT", "cs.CR", "cs.NI", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Federated Split Learning with Improved Communication and Storage Efficiency", "abstract": "Federated learning (FL) is one of the popular distributed machine learning\n(ML) solutions but incurs significant communication and computation costs at\nedge devices. Federated split learning (FSL) can train sub-models in parallel\nand reduce the computational burden of edge devices by splitting the model\narchitecture. However, it still requires a high communication overhead due to\ntransmitting the smashed data and gradients between clients and the server in\nevery global round. Furthermore, the server must maintain separate partial\nmodels for every client, leading to a significant storage requirement. To\naddress these challenges, this paper proposes a novel communication and storage\nefficient federated split learning method, termed CSE-FSL, which utilizes an\nauxiliary network to locally update the weights of the clients while keeping a\nsingle model at the server, hence avoiding frequent transmissions of gradients\nfrom the server and greatly reducing the storage requirement of the server.\nAdditionally, a new model update method of transmitting the smashed data in\nselected epochs can reduce the amount of smashed data sent from the clients. We\nprovide a theoretical analysis of CSE-FSL, rigorously guaranteeing its\nconvergence under non-convex loss functions. The extensive experimental results\nfurther indicate that CSE-FSL achieves a significant communication reduction\nover existing FSL solutions using real-world FL tasks.", "published": "2025-07-21 17:21:16", "link": "http://arxiv.org/abs/2507.15816v1", "categories": ["cs.LG", "cs.IT", "cs.NI", "eess.SP", "math.IT"], "primary_category": "cs.LG"}
{"title": "Identifying Solution Constraints for ODE Systems", "abstract": "This work develops a framework to discover relations between the components\nof the solution to a given initial-value problem for a first-order system of\nordinary differential equations. This is done by using sparse identification\ntechniques on the data represented by the numerical solution of the\ninitial-value problem at hand. The only assumption is that there are only a few\nterms that connects the components, so that the mathematical relations to be\ndiscovered are sparse in the set of possible functions. We illustrate the\nmethod through examples of applications.", "published": "2025-07-21 17:05:12", "link": "http://arxiv.org/abs/2507.15805v1", "categories": ["math.OC", "cs.IT", "cs.NA", "math.IT", "math.NA", "34-04, 65-04"], "primary_category": "math.OC"}
{"title": "Fluid Antenna-enabled Near-Field Integrated Sensing, Computing and Semantic Communication for Emerging Applications", "abstract": "The integration of sensing and communication (ISAC) is a key enabler for\nnext-generation technologies. With high-frequency bands and large-scale antenna\narrays, the Rayleigh distance extends, necessitating near-field (NF) models\nwhere waves are spherical. Although NF-ISAC improves both sensing and\ncommunication, it also poses challenges such as high data volume and potential\nprivacy risks. To address these, we propose a novel framework: near-field\nintegrated sensing, computing, and semantic communication (NF-ISCSC), which\nleverages semantic communication to transmit contextual information only,\nthereby reducing data overhead and improving efficiency. However, semantic\ncommunication is sensitive to channel variations, requiring adaptive\nmechanisms. To this end, fluid antennas (FAs) are introduced to support the\nNF-ISCSC system, enabling dynamic adaptability to changing channels. The\nproposed FA-enabled NF-ISCSC framework considers multiple communication users\nand extended targets comprising several scatterers. A joint optimization\nproblem is formulated to maximize data rate while accounting for sensing\nquality, computational load, and power budget. Using an alternating\noptimization (AO) approach, the original problem is divided into three\nsub-problems: ISAC beamforming, FA positioning, and semantic extraction ratio.\nBeamforming is optimized using the successive convex approximation method. FA\npositioning is solved via a projected Broyden-Fletcher-Goldfarb-Shanno (BFGS)\nalgorithm, and the semantic extraction ratio is optimized using bisection\nsearch. Simulation results demonstrate that the proposed framework achieves\nhigher data rates and better privacy preservation.", "published": "2025-07-21 16:58:15", "link": "http://arxiv.org/abs/2507.15800v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Remote Channel Synthesis", "abstract": "We consider the problem of synthesizing a memoryless channel between an\nunobserved source and a remote terminal. An encoder has access to a partial or\nnoisy version $Z^n = (Z_1, \\ldots, Z_n)$ of a remote source sequence $X^n =\n(X_1, \\ldots, X_n),$ with $(X_i,Z_i)$ independent and identically distributed\nwith joint distribution $q_{X,Z}.$ The encoder communicates through a noiseless\nlink to a decoder which aims to produce an output $Y^n$ coordinated with the\nremote source; that is, the total variation distance between the joint\ndistribution of $X^n$ and $Y^n$ and some i.i.d. target distribution\n$q_{X,Y}^{\\otimes n}$ is required to vanish as $n$ goes to infinity. The two\nterminals may have access to a source of rate-limited common randomness. We\npresent a single-letter characterization of the optimal compression and common\nrandomness rates. We also show that when the common randomness rate is small,\nthen in most cases, coordinating $Z^n$ and $Y^n$ using a standard channel\nsynthesis scheme is strictly sub-optimal. In other words, schemes for which the\njoint distribution of $Z^n$ and $Y^n$ approaches a product distribution\nasymptotically are strictly sub-optimal.", "published": "2025-07-21 16:13:13", "link": "http://arxiv.org/abs/2507.15757v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Estimating Rate-Distortion Functions Using the Energy-Based Model", "abstract": "The rate-distortion (RD) theory is one of the key concepts in information\ntheory, providing theoretical limits for compression performance and guiding\nthe source coding design, with both theoretical and practical significance. The\nBlahut-Arimoto (BA) algorithm, as a classical algorithm to compute RD\nfunctions, encounters computational challenges when applied to high-dimensional\nscenarios. In recent years, many neural methods have attempted to compute\nhigh-dimensional RD problems from the perspective of implicit generative\nmodels. Nevertheless, these approaches often neglect the reconstruction of the\noptimal conditional distribution or rely on unreasonable prior assumptions. In\nface of these issues, we propose an innovative energy-based modeling framework\nthat leverages the connection between the RD dual form and the free energy in\nstatistical physics, achieving effective reconstruction of the optimal\nconditional distribution.The proposed algorithm requires training only a single\nneural network and circumvents the challenge of computing the normalization\nfactor in energy-based models using the Markov chain Monte Carlo (MCMC)\nsampling. Experimental results demonstrate the significant effectiveness of the\nproposed algorithm in estimating high-dimensional RD functions and\nreconstructing the optimal conditional distribution.", "published": "2025-07-21 15:09:50", "link": "http://arxiv.org/abs/2507.15700v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "On Strong Converse Bounds for the Private and Quantum Capacities of Anti-degradable Channels", "abstract": "We establish a strong converse bound for the private classical capacity of\nanti-degradable quantum channels. Specifically, we prove that this capacity is\nzero whenever the error $\\epsilon > 0$ and privacy parameter $\\delta > 0$\nsatisfy the inequality $\\delta (1-\\epsilon^2)^{\\frac{1}{2}}+\\epsilon\n(1-\\delta^2)^{\\frac{1}{2}}<1$. This result strengthens previous understandings\nby sharply defining the boundary beyond which reliable and private\ncommunication is impossible. Furthermore, we present a ``pretty simple'' proof\nof the ``pretty strong'' converse for the quantum capacity of anti-degradable\nchannels, valid for any error $\\epsilon < \\frac{1}{\\sqrt{2}}$. Our approach\noffers clarity and technical simplicity, shedding new light on the fundamental\nlimits of quantum communication.", "published": "2025-07-21 14:22:28", "link": "http://arxiv.org/abs/2507.15661v1", "categories": ["quant-ph", "cs.IT", "math.IT"], "primary_category": "quant-ph"}
{"title": "Zak-OTFS based Multiuser Uplink in Doubly-Spread Channels", "abstract": "Wireless users with different characteristics will be expected to share\nspectrum in next generation communication networks. One of the great strengths\nof wireless networks based on Orthogonal Frequency Division Multiplexing (OFDM)\nis the ease with which different non-overlapping time-frequency (TF) resources\ncan be allocated to different users by simply shifting each user's signal in\ntime and frequency. However, a significant weaknesses of OFDM is the\ninflexibility of sub-carrier spacing. Since OFDM does not allow users to have\ndifferent sub-carrier spacing, a single user subject to inter-carrier\ninterference causes carrier spacing to increase for all users. Zak-OTFS is an\nalternative delay-Doppler (DD) domain modulation scheme, where, in contrast to\nOFDM, the Input-Output (I/O) relation is predictable. We match the strength of\nOFDM by designing a novel DD domain method of shaping the transmitted Zak-OTFS\npulse on the uplink that enables flexible non-overlapping TF resource\nallocation. The base station (BS) receives a superposition of uplink signals\nand applies individual matched filters to obtain the data specific to\nindividual users. We develop theoretical measures of interference between\nusers, and present numerical simulations for a vehicular channel model\nrepresentative of next generation propagation environments. We demonstrate\nsingle-user performance in a multiuser Zak-OTFS uplink system without needing\nto provision guard bands between TF resources allocated to different users.\nThese performance results demonstrate that the benefits of a predictable\nZak-OTFS waveform can be realized within an architecture for uplink\ncommunication.", "published": "2025-07-21 13:44:51", "link": "http://arxiv.org/abs/2507.15621v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Movable-Antenna Empowered AAV-Enabled Data Collection over Low-Altitude Wireless Networks", "abstract": "Movable-antennas (MAs) are revolutionizing spatial signal processing by\nproviding flexible beamforming in next-generation wireless systems. This paper\ninvestigates an MA-empowered autonomous aerial vehicle (AAV) system in\nlow-altitude wireless networks (LAWNs) for uplink data collection from ground\nusers. We aim to maximize the sum achievable rate by jointly optimizing the AAV\ntrajectory, receive beamforming, and MA positions. An efficient alternating\noptimization (AO) algorithm that incorporates successive convex approximation,\nweighted minimum mean square error, and particle swarm optimization is\ndeveloped. The analysis of the computational complexity and convergence\nfeatures is provided. Extensive simulations demonstrate superior performance in\nterms of the sum achievable rate and the service reliability comparing to\nseveral benchmark schemes. These results demonstrate the distinctive advantages\nof the proposed scheme: enhanced spectral efficiency via adaptive beam-user\nalignment and improved collection reliability through spatial interference\nmanagement, highlighting the implementation potential of the MA-empowered\nLAWNs.", "published": "2025-07-21 11:30:29", "link": "http://arxiv.org/abs/2507.15515v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Galois equiangular tight frames from Galois self-dual codes", "abstract": "Greaves et al. (2022) extended frames over real or complex numbers to frames\nover finite fields. In this paper, we study the theory of frames over finite\nfields by incorporating the Galois inner products introduced by Fan and Zhang\n(2017), which generalize the Euclidean and Hermitian inner products. We define\na class of frames, called Galois frames over finite fields, along with related\nnotions such as Galois Gram matrices, Galois frame operators, and Galois\nequiangular tight frames (Galois ETFs). We also characterize when Galois\nself-dual codes induce Galois ETFs. Furthermore, we construct explicitly Galois\nETFs from Galois self-dual constacyclic codes.", "published": "2025-07-21 10:00:29", "link": "http://arxiv.org/abs/2507.15448v1", "categories": ["cs.IT", "math.IT", "51E22, 94B05"], "primary_category": "cs.IT"}
{"title": "Cross Mutual Information", "abstract": "Mutual information (MI) is a useful information-theoretic measure to quantify\nthe statistical dependence between two random variables: $X$ and $Y$. Often, we\nare interested in understanding how the dependence between $X$ and $Y$ in one\nset of samples compares to another. Although the dependence between $X$ and $Y$\nin each set of samples can be measured separately using MI, these estimates\ncannot be compared directly if they are based on samples from a non-stationary\ndistribution. Here, we propose an alternative measure for characterising how\nthe dependence between $X$ and $Y$ as defined by one set of samples is\nexpressed in another, \\textit{cross mutual information}. We present a\ncomprehensive set of simulation studies sampling data with $X$-$Y$ dependencies\nto explore this measure. Finally, we discuss how this relates to measures of\nmodel fit in linear regression, and some future applications in neuroimaging\ndata analysis.", "published": "2025-07-21 08:22:37", "link": "http://arxiv.org/abs/2507.15372v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "A Novel Two-Dimensional Smoothing Algorithm", "abstract": "Smoothing and filtering two-dimensional sequences are fundamental tasks in\nfields such as computer vision. Conventional filtering algorithms often rely on\nthe selection of the filtering window, limiting their applicability in certain\nscenarios. To this end, we propose a novel Two-Dimensional Smoothing (TDS)\nalgorithm for the smoothing and filtering problem of two-dimensional sequences.\nTypically, the TDS algorithm does not require assumptions about the type of\nnoise distribution. It is simple and easy to implement compared to conventional\nfiltering methods, such as 2D adaptive Wiener filtering and Gaussian filtering.\nThe TDS algorithm can effectively extract the trend contained in the\ntwo-dimensional sequence and reduce the influence of noise on the data by\nadjusting only a single parameter. In this work, unlike existing algorithms\nthat depend on the filtering window, we introduce a loss function, where the\ntrend sequence is identified as the solution when this loss function takes a\nminimum value. Therefore, within the framework of the TDS algorithm, a general\ntwo-dimensional sequence can be innovatively decomposed into a trend sequence\nand a fluctuation sequence, in which the trend sequence contains the main\nfeatures of the sequence and the fluctuation sequence contains the detailed\nfeatures or noise interference of the sequence. To ensure the reliability of\nthe TDS algorithm, a crucial lemma is first established, indicating that the\ntrend sequence and fluctuation sequence obtained by the TDS algorithm are\nexistent and unique when the global smoothing parameter is determined. Three\nmodified algorithms are then proposed based on the TDS algorithm, with\ncorresponding lemmas and corollaries demonstrating their reliability. Finally,\nthe accuracy and effectiveness of the TDS algorithm are further verified\nthrough numerical simulations and image processing cases.", "published": "2025-07-21 06:59:33", "link": "http://arxiv.org/abs/2507.15301v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "The Exact Parameters of A Family of BCH Codes", "abstract": "Despite the theoretical and practical significance of BCH codes, the exact\nminimum distance and dimension remain unknown for many families. This paper\nestablishes the precise minimum distance and dimension of narrow-sense BCH\ncodes $\\C_{(q, m, \\lambda, \\ell_0, \\ell_1)}$ over $\\gf(q)$ of length\n$\\frac{q^m-1}{\\lambda}$ and designed distance $\\frac{(q-\\lambda\n\\ell_0)q^{m-1-\\ell_1}-1}{\\lambda}$, where $\\lambda\\mid (q-1)$, $0\\leq \\ell_0<\n\\frac{q-1}{\\lambda}$, and $0\\leq \\ell_1\\leq m-1$. These results conclusively\nresolve the three open problems posed by Li et al. (IEEE Trans. Inf. Theory,\nvol. 63, no. 11, pp. 7219-7236, Nov. 2017) while establishing complementary\nadvances to Ding's seminal framework (IEEE Trans. Inf. Theory, vol. 61, no. 10,\npp. 5322-5330, Oct. 2015).", "published": "2025-07-21 05:12:56", "link": "http://arxiv.org/abs/2507.15247v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "On an entropy inequality for quadratic forms and applications", "abstract": "Let $\\phi(x,y)$ be a non-degenerate rational quadratic form. Let $X$ and $Y$\nbe independent $(s, C)$-Frostman random variables whose ranges are contained in\n$[-c_1, c_1]$, with $0<s<1$, $C,c_1\\geq 1$. We prove that there exist a\npositive constant $\\epsilon = \\epsilon(s,\\phi)$ and an integer\n$N=N(s,C,c_1,\\phi)$ such that\n  $$\\max\\left\\{H_n(X+Y),\\,H_n(\\phi(X,Y))\\right\\} \\ge n(s+\\epsilon)$$\n  for all $n>N$. The proof introduces a novel multi-step entropy framework,\ncombining the submodularity formula, the discretized entropy\nBalog-Szemer\\'{e}di-Gowers theorem, and state-of-the-art results on the\nFalconer distance problem, to reduce general forms to a diagonal core case. As\nan application, we derive a result on a discretized sum-product type problem.\nIn particular, for a $\\delta$-separated set $A\\subset [0, 1]$ of cardinality\n$\\delta^{-s}$, satisfying some non-concentration conditions, there exists\n$\\epsilon=\\epsilon(s, \\phi)>0$ such that $$E_\\delta(A+A) + E_\\delta(\\phi(A, A))\n\\gg\\delta^{-\\epsilon}(\\#A) $$ for all $\\delta$ small enough. Here by\n$E_\\delta(A)$ we mean the $\\delta$-covering number of $A$.", "published": "2025-07-21 02:51:00", "link": "http://arxiv.org/abs/2507.15196v1", "categories": ["math.CA", "cs.IT", "math.CO", "math.IT"], "primary_category": "math.CA"}
{"title": "Optimizing Canaries for Privacy Auditing with Metagradient Descent", "abstract": "In this work we study black-box privacy auditing, where the goal is to lower\nbound the privacy parameter of a differentially private learning algorithm\nusing only the algorithm's outputs (i.e., final trained model). For DP-SGD (the\nmost successful method for training differentially private deep learning\nmodels), the canonical approach auditing uses membership inference-an auditor\ncomes with a small set of special \"canary\" examples, inserts a random subset of\nthem into the training set, and then tries to discern which of their canaries\nwere included in the training set (typically via a membership inference\nattack). The auditor's success rate then provides a lower bound on the privacy\nparameters of the learning algorithm. Our main contribution is a method for\noptimizing the auditor's canary set to improve privacy auditing, leveraging\nrecent work on metagradient optimization. Our empirical evaluation demonstrates\nthat by using such optimized canaries, we can improve empirical lower bounds\nfor differentially private image classification models by over 2x in certain\ninstances. Furthermore, we demonstrate that our method is transferable and\nefficient: canaries optimized for non-private SGD with a small model\narchitecture remain effective when auditing larger models trained with DP-SGD.", "published": "2025-07-21 17:47:33", "link": "http://arxiv.org/abs/2507.15836v1", "categories": ["cs.LG", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Multi-Strategy Improved Snake Optimizer Accelerated CNN-LSTM-Attention-Adaboost for Trajectory Prediction", "abstract": "To address the limitations of medium- and long-term four-dimensional (4D)\ntrajectory prediction models, this paper proposes a hybrid\nCNN-LSTM-attention-adaboost neural network model incorporating a multi-strategy\nimproved snake-herd optimization (SO) algorithm. The model applies the Adaboost\nalgorithm to divide multiple weak learners, and each submodel utilizes CNN to\nextract spatial features, LSTM to capture temporal features, and attention\nmechanism to capture global features comprehensively. The strong learner model,\ncombined with multiple sub-models, then optimizes the hyperparameters of the\nprediction model through the natural selection behavior pattern simulated by\nSO. In this study, based on the real ADS-B data from Xi'an to Tianjin, the\ncomparison experiments and ablation studies of multiple optimizers are carried\nout, and a comprehensive test and evaluation analysis is carried out. The\nresults show that SO-CLA-adaboost outperforms traditional optimizers such as\nparticle swarm, whale, and gray wolf in handling large-scale high-dimensional\ntrajectory data. In addition, introducing the full-strategy collaborative\nimprovement SO algorithm improves the model's prediction accuracy by 39.89%.", "published": "2025-07-21 17:44:06", "link": "http://arxiv.org/abs/2507.15832v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "ACS: An interactive framework for conformal selection", "abstract": "This paper presents adaptive conformal selection (ACS), an interactive\nframework for model-free selection with guaranteed error control. Building on\nconformal selection (Jin and Cand\\`es, 2023b), ACS generalizes the approach to\nsupport human-in-the-loop adaptive data analysis. Under the ACS framework, we\ncan partially reuse the data to boost the selection power, make decisions on\nthe fly while exploring the data, and incorporate new information or\npreferences as they arise. The key to ACS is a carefully designed principle\nthat controls the information available for decision making, allowing the data\nanalyst to explore the data adaptively while maintaining rigorous control of\nthe false discovery rate (FDR). Based on the ACS framework, we provide concrete\nselection algorithms for various goals, including model update/selection,\ndiversified selection, and incorporating newly available labeled data. The\neffectiveness of ACS is demonstrated through extensive numerical simulations\nand real-data applications in large language model (LLM) deployment and drug\ndiscovery.", "published": "2025-07-21 17:33:15", "link": "http://arxiv.org/abs/2507.15825v1", "categories": ["stat.ME", "cs.LG", "stat.ML"], "primary_category": "stat.ME"}
{"title": "LLM Economist: Large Population Models and Mechanism Design in Multi-Agent Generative Simulacra", "abstract": "We present the LLM Economist, a novel framework that uses agent-based\nmodeling to design and assess economic policies in strategic environments with\nhierarchical decision-making. At the lower level, bounded rational worker\nagents -- instantiated as persona-conditioned prompts sampled from U.S.\nCensus-calibrated income and demographic statistics -- choose labor supply to\nmaximize text-based utility functions learned in-context. At the upper level, a\nplanner agent employs in-context reinforcement learning to propose\npiecewise-linear marginal tax schedules anchored to the current U.S. federal\nbrackets. This construction endows economic simulacra with three capabilities\nrequisite for credible fiscal experimentation: (i) optimization of\nheterogeneous utilities, (ii) principled generation of large, demographically\nrealistic agent populations, and (iii) mechanism design -- the ultimate nudging\nproblem -- expressed entirely in natural language. Experiments with populations\nof up to one hundred interacting agents show that the planner converges near\nStackelberg equilibria that improve aggregate social welfare relative to Saez\nsolutions, while a periodic, persona-level voting procedure furthers these\ngains under decentralized governance. These results demonstrate that large\nlanguage model-based agents can jointly model, simulate, and govern complex\neconomic systems, providing a tractable test bed for policy evaluation at the\nsocietal scale to help build better civilizations.", "published": "2025-07-21 17:21:14", "link": "http://arxiv.org/abs/2507.15815v1", "categories": ["cs.MA", "cs.LG"], "primary_category": "cs.MA"}
{"title": "Hypergraphs on high dimensional time series sets using signature transform", "abstract": "In recent decades, hypergraphs and their analysis through Topological Data\nAnalysis (TDA) have emerged as powerful tools for understanding complex data\nstructures. Various methods have been developed to construct hypergraphs --\nreferred to as simplicial complexes in the TDA framework -- over datasets,\nenabling the formation of edges between more than two vertices. This paper\naddresses the challenge of constructing hypergraphs from collections of\nmultivariate time series. While prior work has focused on the case of a single\nmultivariate time series, we extend this framework to handle collections of\nsuch time series. Our approach generalizes the method proposed in Chretien and\nal. by leveraging the properties of signature transforms to introduce\ncontrolled randomness, thereby enhancing the robustness of the construction\nprocess. We validate our method on synthetic datasets and present promising\nresults.", "published": "2025-07-21 17:02:36", "link": "http://arxiv.org/abs/2507.15802v1", "categories": ["stat.ML", "cs.LG", "stat.CO"], "primary_category": "stat.ML"}
{"title": "Graph Attention Specialized Expert Fusion Model for Node Classification: Based on Cora and Pubmed Datasets", "abstract": "Graph node classification is a fundamental task in graph neural networks\n(GNNs), aiming to assign predefined class labels to nodes. On the PubMed\ncitation network dataset, we observe significant classification difficulty\ndisparities, with Category 2 achieving only 74.4% accuracy in traditional GCN,\n7.5% lower than Category 1. To address this, we propose a\nWasserstein-Rubinstein (WR) distance enhanced Expert Fusion Model (WR-EFM),\ntraining specialized GNN models for Categories 0/1 (with layer normalization\nand residual connections) and Multi-hop Graph Attention Networks (GAT) for\nCategory 2. The WR distance metric optimizes representation similarity between\nmodels, particularly focusing on improving Category 2 performance. Our adaptive\nfusion strategy dynamically weights models based on category-specific\nperformance, with Category 2 assigned a GAT weight of 0.8. WR distance further\nguides the fusion process by measuring distributional differences between model\nrepresentations, enabling more principled integration of complementary\nfeatures.\n  Experimental results show WR-EFM achieves balanced accuracy across\ncategories: 77.8% (Category 0), 78.0% (Category 1), and 79.9% (Category 2),\noutperforming both single models and standard fusion approaches. The\ncoefficient of variation (CV) of WR-EFM's category accuracies is 0.013, 77.6%\nlower than GCN's 0.058, demonstrating superior stability. Notably, WR-EFM\nimproves Category 2 accuracy by 5.5% compared to GCN, verifying the\neffectiveness of WR-guided fusion in capturing complex structural patterns.\nThis work provides a novel paradigm for handling class-imbalanced graph\nclassification tasks. To promote the research community, we release our project\nat https://github.com/s010m00n/GASEM4NC.", "published": "2025-07-21 16:40:04", "link": "http://arxiv.org/abs/2507.15784v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Multi-Modal Sensor Fusion for Proactive Blockage Prediction in mmWave Vehicular Networks", "abstract": "Vehicular communication systems operating in the millimeter wave (mmWave)\nband are highly susceptible to signal blockage from dynamic obstacles such as\nvehicles, pedestrians, and infrastructure. To address this challenge, we\npropose a proactive blockage prediction framework that utilizes multi-modal\nsensing, including camera, GPS, LiDAR, and radar inputs in an\ninfrastructure-to-vehicle (I2V) setting. This approach uses modality-specific\ndeep learning models to process each sensor stream independently and fuses\ntheir outputs using a softmax-weighted ensemble strategy based on validation\nperformance. Our evaluations, for up to 1.5s in advance, show that the\ncamera-only model achieves the best standalone trade-off with an F1-score of\n97.1% and an inference time of 89.8ms. A camera+radar configuration further\nimproves accuracy to 97.2% F1 at 95.7ms. Our results display the effectiveness\nand efficiency of multi-modal sensing for mmWave blockage prediction and\nprovide a pathway for proactive wireless communication in dynamic environments.", "published": "2025-07-21 16:25:44", "link": "http://arxiv.org/abs/2507.15769v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Conformal and kNN Predictive Uncertainty Quantification Algorithms in Metric Spaces", "abstract": "This paper introduces a framework for uncertainty quantification in\nregression models defined in metric spaces. Leveraging a newly defined notion\nof homoscedasticity, we develop a conformal prediction algorithm that offers\nfinite-sample coverage guarantees and fast convergence rates of the oracle\nestimator. In heteroscedastic settings, we forgo these non-asymptotic\nguarantees to gain statistical efficiency, proposing a local\n$k$--nearest--neighbor method without conformal calibration that is adaptive to\nthe geometry of each particular nonlinear space. Both procedures work with any\nregression algorithm and are scalable to large data sets, allowing\npractitioners to plug in their preferred models and incorporate domain\nexpertise. We prove consistency for the proposed estimators under minimal\nconditions. Finally, we demonstrate the practical utility of our approach in\npersonalized--medicine applications involving random response objects such as\nprobability distributions and graph Laplacians.", "published": "2025-07-21 15:54:13", "link": "http://arxiv.org/abs/2507.15741v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "primary_category": "stat.ML"}
{"title": "Competitive Algorithms for Cooperative Multi-Agent Ski-Rental Problems", "abstract": "This paper introduces a novel multi-agent ski-rental problem that generalizes\nthe classical ski-rental dilemma to a group setting where agents incur\nindividual and shared costs. In our model, each agent can either rent at a\nfixed daily cost, or purchase a pass at an individual cost, with an additional\nthird option of a discounted group pass available to all. We consider scenarios\nin which agents' active days differ, leading to dynamic states as agents drop\nout of the decision process. To address this problem from different\nperspectives, we define three distinct competitive ratios: overall,\nstate-dependent, and individual rational. For each objective, we design and\nanalyze optimal deterministic and randomized policies. Our deterministic\npolicies employ state-aware threshold functions that adapt to the dynamic\nstates, while our randomized policies sample and resample thresholds from\ntailored state-aware distributions. The analysis reveals that symmetric\npolicies, in which all agents use the same threshold, outperform asymmetric\nones. Our results provide competitive ratio upper and lower bounds and extend\nclassical ski-rental insights to multi-agent settings, highlighting both\ntheoretical and practical implications for group decision-making under\nuncertainty.", "published": "2025-07-21 15:36:34", "link": "http://arxiv.org/abs/2507.15727v1", "categories": ["cs.LG", "cs.GT", "cs.MA"], "primary_category": "cs.LG"}
{"title": "GeoHNNs: Geometric Hamiltonian Neural Networks", "abstract": "The fundamental laws of physics are intrinsically geometric, dictating the\nevolution of systems through principles of symmetry and conservation. While\nmodern machine learning offers powerful tools for modeling complex dynamics\nfrom data, common methods often ignore this underlying geometric fabric.\nPhysics-informed neural networks, for instance, can violate fundamental\nphysical principles, leading to predictions that are unstable over long\nperiods, particularly for high-dimensional and chaotic systems. Here, we\nintroduce \\textit{Geometric Hamiltonian Neural Networks (GeoHNN)}, a framework\nthat learns dynamics by explicitly encoding the geometric priors inherent to\nphysical laws. Our approach enforces two fundamental structures: the Riemannian\ngeometry of inertia, by parameterizing inertia matrices in their natural\nmathematical space of symmetric positive-definite matrices, and the symplectic\ngeometry of phase space, using a constrained autoencoder to ensure the\npreservation of phase space volume in a reduced latent space. We demonstrate\nthrough experiments on systems ranging from coupled oscillators to\nhigh-dimensional deformable objects that GeoHNN significantly outperforms\nexisting models. It achieves superior long-term stability, accuracy, and energy\nconservation, confirming that embedding the geometry of physics is not just a\ntheoretical appeal but a practical necessity for creating robust and\ngeneralizable models of the physical world.", "published": "2025-07-21 14:42:39", "link": "http://arxiv.org/abs/2507.15678v1", "categories": ["cs.LG", "math.DG", "math.DS", "math.SG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Optimal Batch-Size Control for Low-Latency Federated Learning with Device Heterogeneity", "abstract": "Federated learning (FL) has emerged as a popular approach for collaborative\nmachine learning in sixth-generation (6G) networks, primarily due to its\nprivacy-preserving capabilities. The deployment of FL algorithms is expected to\nempower a wide range of Internet-of-Things (IoT) applications, e.g., autonomous\ndriving, augmented reality, and healthcare. The mission-critical and\ntime-sensitive nature of these applications necessitates the design of\nlow-latency FL frameworks that guarantee high learning performance. In\npractice, achieving low-latency FL faces two challenges: the overhead of\ncomputing and transmitting high-dimensional model updates, and the\nheterogeneity in communication-and-computation (C$^2$) capabilities across\ndevices. To address these challenges, we propose a novel C$^2$-aware framework\nfor optimal batch-size control that minimizes end-to-end (E2E) learning latency\nwhile ensuring convergence. The framework is designed to balance a fundamental\nC$^2$ tradeoff as revealed through convergence analysis. Specifically,\nincreasing batch sizes improves the accuracy of gradient estimation in FL and\nthus reduces the number of communication rounds required for convergence, but\nresults in higher per-round latency, and vice versa. The associated problem of\nlatency minimization is intractable; however, we solve it by designing an\naccurate and tractable surrogate for convergence speed, with parameters fitted\nto real data. This approach yields two batch-size control strategies tailored\nto scenarios with slow and fast fading, while also accommodating device\nheterogeneity. Extensive experiments using real datasets demonstrate that the\nproposed strategies outperform conventional batch-size adaptation schemes that\ndo not consider the C$^2$ tradeoff or device heterogeneity.", "published": "2025-07-21 13:24:38", "link": "http://arxiv.org/abs/2507.15601v1", "categories": ["cs.LG", "cs.SY", "eess.SY"], "primary_category": "cs.LG"}
{"title": "Applying the Chinese Wall Reverse Engineering Technique to Large Language Model Code Editing", "abstract": "Large language models for code (Code LLM) are increasingly utilized in\nprogramming environments. Despite their utility, the training datasets for top\nLLM remain undisclosed, raising concerns about potential copyright violations.\nSome models, such as Pleias and Comma put emphasis on data curation and\nlicenses, however, with limited training data these models are not competitive\nand only serve as proof of concepts. To improve the utility of these models, we\npropose an application of the \"Chinese Wall\" technique, inspired by the reverse\nengineering technique of the same name -- a high quality model is used to\ngenerate detailed instructions for a weaker model. By doing so, a weaker but\nethically aligned model may be used to perform complicated tasks that,\notherwise, can only be completed by more powerful models. In our evaluation,\nwe've found that this technique improves Comma v0.1 1T's performance in\nCanItEdit benchmark by over 66%, and Starcoder2 Instruct by roughly 20%\ncompared to when running the same model on the benchmark alone. The practical\napplication of this technique today, however, may be limited due to the lack of\nmodels trained on public domain content without copyright restrictions.", "published": "2025-07-21 13:21:29", "link": "http://arxiv.org/abs/2507.15599v1", "categories": ["cs.SE", "cs.LG"], "primary_category": "cs.SE"}
{"title": "We Need to Rethink Benchmarking in Anomaly Detection", "abstract": "Despite the continuous proposal of new anomaly detection algorithms and\nextensive benchmarking efforts, progress seems to stagnate, with only minor\nperformance differences between established baselines and new algorithms. In\nthis position paper, we argue that this stagnation is due to limitations in how\nwe evaluate anomaly detection algorithms. Current benchmarking does not, for\nexample, sufficiently reflect the diversity of anomalies in applications\nranging from predictive maintenance to scientific discovery. Consequently, we\nneed to rethink benchmarking in anomaly detection. In our opinion, anomaly\ndetection should be studied using scenarios that capture the relevant\ncharacteristics of different applications. We identify three key areas for\nimprovement: First, we need to identify anomaly detection scenarios based on a\ncommon taxonomy. Second, anomaly detection pipelines should be analyzed\nend-to-end and by component. Third, evaluating anomaly detection algorithms\nshould be meaningful regarding the scenario's objectives.", "published": "2025-07-21 13:02:49", "link": "http://arxiv.org/abs/2507.15584v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Trade-offs between elective surgery rescheduling and length-of-stay prediction accuracy", "abstract": "The availability of downstream resources plays a critical role in planning\nthe admission of patients undergoing elective surgery, with inpatient beds\nbeing one of the most crucial resources. When planning patient admissions,\npredictions on their length-of-stay (LOS) made by machine learning (ML) models\nare used to ensure bed availability. However, the actual LOS for each patient\nmay differ considerably from the predicted value, potentially making the\nschedule infeasible. To address such infeasibilities, rescheduling strategies\nthat take advantage of operational flexibility can be implemented. For example,\nadjustments may include postponing admission dates, relocating patients to\ndifferent wards, or even transferring patients who are already admitted. The\ncommon assumption is that more accurate LOS predictions reduce the impact of\nrescheduling. However, training ML models that can make such accurate\npredictions can be costly. Building on previous work that proposed simulated\n\\ac{ml} for evaluating data-driven approaches, this paper explores the\nrelationship between LOS prediction accuracy and rescheduling flexibility\nacross various corrective policies. Specifically, we examine the most effective\npatient rescheduling strategies under LOS prediction errors to prevent bed\noverflows while optimizing resource utilization.", "published": "2025-07-21 12:46:18", "link": "http://arxiv.org/abs/2507.15566v1", "categories": ["cs.LG", "math.OC"], "primary_category": "cs.LG"}
{"title": "The added value for MRI radiomics and deep-learning for glioblastoma prognostication compared to clinical and molecular information", "abstract": "Background: Radiomics shows promise in characterizing glioblastoma, but its\nadded value over clinical and molecular predictors has yet to be proven. This\nstudy assessed the added value of conventional radiomics (CR) and deep learning\n(DL) MRI radiomics for glioblastoma prognosis (<= 6 vs > 6 months survival) on\na large multi-center dataset.\n  Methods: After patient selection, our curated dataset gathers 1152\nglioblastoma (WHO 2016) patients from five Swiss centers and one public source.\nIt included clinical (age, gender), molecular (MGMT, IDH), and baseline MRI\ndata (T1, T1 contrast, FLAIR, T2) with tumor regions. CR and DL models were\ndeveloped using standard methods and evaluated on internal and external\ncohorts. Sub-analyses assessed models with different feature sets\n(imaging-only, clinical/molecular-only, combined-features) and patient subsets\n(S-1: all patients, S-2: with molecular data, S-3: IDH wildtype).\n  Results: The best performance was observed in the full cohort (S-1). In\nexternal validation, the combined-feature CR model achieved an AUC of 0.75,\nslightly, but significantly outperforming clinical-only (0.74) and imaging-only\n(0.68) models. DL models showed similar trends, though without statistical\nsignificance. In S-2 and S-3, combined models did not outperform clinical-only\nmodels. Exploratory analysis of CR models for overall survival prediction\nsuggested greater relevance of imaging data: across all subsets,\ncombined-feature models significantly outperformed clinical-only models, though\nwith a modest advantage of 2-4 C-index points.\n  Conclusions: While confirming the predictive value of anatomical MRI\nsequences for glioblastoma prognosis, this multi-center study found standard CR\nand DL radiomics approaches offer minimal added value over demographic\npredictors such as age and gender.", "published": "2025-07-21 12:27:07", "link": "http://arxiv.org/abs/2507.15548v1", "categories": ["cs.LG", "stat.AP"], "primary_category": "cs.LG"}
{"title": "Data Aware Differentiable Neural Architecture Search for Tiny Keyword Spotting Applications", "abstract": "The success of Machine Learning is increasingly tempered by its significant\nresource footprint, driving interest in efficient paradigms like TinyML.\nHowever, the inherent complexity of designing TinyML systems hampers their\nbroad adoption. To reduce this complexity, we introduce \"Data Aware\nDifferentiable Neural Architecture Search\". Unlike conventional Differentiable\nNeural Architecture Search, our approach expands the search space to include\ndata configuration parameters alongside architectural choices. This enables\nData Aware Differentiable Neural Architecture Search to co-optimize model\narchitecture and input data characteristics, effectively balancing resource\nusage and system performance for TinyML applications. Initial results on\nkeyword spotting demonstrate that this novel approach to TinyML system design\ncan generate lean but highly accurate systems.", "published": "2025-07-21 12:18:38", "link": "http://arxiv.org/abs/2507.15545v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "An Investigation of Test-time Adaptation for Audio Classification under Background Noise", "abstract": "Domain shift is a prominent problem in Deep Learning, causing a model\npre-trained on a source dataset to suffer significant performance degradation\non test datasets. This research aims to address the issue of audio\nclassification under domain shift caused by background noise using Test-Time\nAdaptation (TTA), a technique that adapts a pre-trained model during testing\nusing only unlabelled test data before making predictions. We adopt two common\nTTA methods, TTT and TENT, and a state-of-the-art method CoNMix, and\ninvestigate their respective performance on two popular audio classification\ndatasets, AudioMNIST (AM) and SpeechCommands V1 (SC), against different types\nof background noise and noise severity levels. The experimental results reveal\nthat our proposed modified version of CoNMix produced the highest\nclassification accuracy under domain shift (5.31% error rate under 10 dB\nexercise bike background noise and 12.75% error rate under 3 dB running tap\nbackground noise for AM) compared to TTT and TENT. The literature search\nprovided no evidence of similar works, thereby motivating the work reported\nhere as the first study to leverage TTA techniques for audio classification\nunder domain shift.", "published": "2025-07-21 11:44:24", "link": "http://arxiv.org/abs/2507.15523v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Information Preserving Line Search via Bayesian Optimization", "abstract": "Line search is a fundamental part of iterative optimization methods for\nunconstrained and bound-constrained optimization problems to determine suitable\nstep lengths that provide sufficient improvement in each iteration. Traditional\nline search methods are based on iterative interval refinement, where valuable\ninformation about function value and gradient is discarded in each iteration.\nWe propose a line search method via Bayesian optimization, preserving and\nutilizing otherwise discarded information to improve step-length choices. Our\napproach is guaranteed to converge and shows superior performance compared to\nstate-of-the-art methods based on empirical tests on the challenging\nunconstrained and bound-constrained optimization problems from the CUTEst test\nset.", "published": "2025-07-21 10:42:12", "link": "http://arxiv.org/abs/2507.15485v1", "categories": ["math.OC", "cs.LG"], "primary_category": "math.OC"}
{"title": "FedMultiEmo: Real-Time Emotion Recognition via Multimodal Federated Learning", "abstract": "In-vehicle emotion recognition underpins adaptive driver-assistance systems\nand, ultimately, occupant safety. However, practical deployment is hindered by\n(i) modality fragility - poor lighting and occlusions degrade vision-based\nmethods; (ii) physiological variability - heart-rate and skin-conductance\npatterns differ across individuals; and (iii) privacy risk - centralized\ntraining requires transmission of sensitive data. To address these challenges,\nwe present FedMultiEmo, a privacy-preserving framework that fuses two\ncomplementary modalities at the decision level: visual features extracted by a\nConvolutional Neural Network from facial images, and physiological cues (heart\nrate, electrodermal activity, and skin temperature) classified by a Random\nForest. FedMultiEmo builds on three key elements: (1) a multimodal federated\nlearning pipeline with majority-vote fusion, (2) an end-to-end edge-to-cloud\nprototype on Raspberry Pi clients and a Flower server, and (3) a personalized\nFederated Averaging scheme that weights client updates by local data volume.\nEvaluated on FER2013 and a custom physiological dataset, the federated\nConvolutional Neural Network attains 77% accuracy, the Random Forest 74%, and\ntheir fusion 87%, matching a centralized baseline while keeping all raw data\nlocal. The developed system converges in 18 rounds, with an average round time\nof 120 seconds and a per-client memory footprint below 200 MB. These results\nindicate that FedMultiEmo offers a practical approach to real-time,\nprivacy-aware emotion recognition in automotive settings.", "published": "2025-07-21 10:21:48", "link": "http://arxiv.org/abs/2507.15470v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Privacy-Preserving Multimodal News Recommendation through Federated Learning", "abstract": "Personalized News Recommendation systems (PNR) have emerged as a solution to\ninformation overload by predicting and suggesting news items tailored to\nindividual user interests. However, traditional PNR systems face several\nchallenges, including an overreliance on textual content, common neglect of\nshort-term user interests, and significant privacy concerns due to centralized\ndata storage. This paper addresses these issues by introducing a novel\nmultimodal federated learning-based approach for news recommendation. First, it\nintegrates both textual and visual features of news items using a multimodal\nmodel, enabling a more comprehensive representation of content. Second, it\nemploys a time-aware model that balances users' long-term and short-term\ninterests through multi-head self-attention networks, improving recommendation\naccuracy. Finally, to enhance privacy, a federated learning framework is\nimplemented, enabling collaborative model training without sharing user data.\nThe framework divides the recommendation model into a large server-maintained\nnews model and a lightweight user model shared between the server and clients.\nThe client requests news representations (vectors) and a user model from the\ncentral server, then computes gradients with user local data, and finally sends\ntheir locally computed gradients to the server for aggregation. The central\nserver aggregates gradients to update the global user model and news model. The\nupdated news model is further used to infer news representation by the server.\nTo further safeguard user privacy, a secure aggregation algorithm based on\nShamir's secret sharing is employed. Experiments on a real-world news dataset\ndemonstrate strong performance compared to existing systems, representing a\nsignificant advancement in privacy-preserving personalized news recommendation.", "published": "2025-07-21 10:14:00", "link": "http://arxiv.org/abs/2507.15460v1", "categories": ["cs.SI", "cs.LG"], "primary_category": "cs.SI"}
{"title": "An Adaptive Random Fourier Features approach Applied to Learning Stochastic Differential Equations", "abstract": "This work proposes a training algorithm based on adaptive random Fourier\nfeatures (ARFF) with Metropolis sampling and resampling\n\\cite{kammonen2024adaptiverandomfourierfeatures} for learning drift and\ndiffusion components of stochastic differential equations from snapshot data.\nSpecifically, this study considers It\\^{o} diffusion processes and a\nlikelihood-based loss function derived from the Euler-Maruyama integration\nintroduced in \\cite{Dietrich2023} and\n\\cite{dridi2021learningstochasticdynamicalsystems}.\n  This work evaluates the proposed method against benchmark problems presented\nin \\cite{Dietrich2023}, including polynomial examples, underdamped Langevin\ndynamics, a stochastic susceptible-infected-recovered model, and a stochastic\nwave equation. Across all cases, the ARFF-based approach matches or surpasses\nthe performance of conventional Adam-based optimization in both loss\nminimization and convergence speed. These results highlight the potential of\nARFF as a compelling alternative for data-driven modeling of stochastic\ndynamics.", "published": "2025-07-21 09:52:33", "link": "http://arxiv.org/abs/2507.15442v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "The calculus of variations of the Transformer on the hyperspherical tangent bundle", "abstract": "We offer a theoretical mathematical background to Transformers through\nLagrangian optimization across the token space. The Transformer, as a flow map,\nexists in the tangent fiber for each token along the high-dimensional unit\nsphere. The circumstance of the hypersphere across the latent data is\nreasonable due to the trained diagonal matrix equal to the identity, which has\nvarious empirical justifications. Thus, under the continuum limit of the\ndynamics, the latent vectors flow among the tangent bundle. Using these facts,\nwe devise a mathematical framework for the Transformer through calculus of\nvariations. We develop a functional and show that the continuous flow map\ninduced by the Transformer satisfies this functional, therefore the Transformer\ncan be viewed as a natural solver of a calculus of variations problem. We\ninvent new scenarios of when our methods are applicable based on loss\noptimization with respect to path optimality. We derive the Euler-Lagrange\nequation for the Transformer. The variant of the Euler-Lagrange equation we\npresent has various appearances in literature, but, to our understanding,\noftentimes not foundationally proven or under other specialized cases. Our\noverarching proof is new: our techniques are classical and the use of the flow\nmap object is original. We provide several other relevant results, primarily\nones specific to neural scenarios. In particular, much of our analysis will be\nattempting to quantify Transformer data in variational contexts under neural\napproximations. Calculus of variations on manifolds is a well-nourished\nresearch area, but for the Transformer specifically, it is uncharted: we lay\nthe foundation for this area through an introduction to the Lagrangian for the\nTransformer.", "published": "2025-07-21 09:43:33", "link": "http://arxiv.org/abs/2507.15431v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "MAP Estimation with Denoisers: Convergence Rates and Guarantees", "abstract": "Denoiser models have become powerful tools for inverse problems, enabling the\nuse of pretrained networks to approximate the score of a smoothed prior\ndistribution. These models are often used in heuristic iterative schemes aimed\nat solving Maximum a Posteriori (MAP) optimisation problems, where the proximal\noperator of the negative log-prior plays a central role. In practice, this\noperator is intractable, and practitioners plug in a pretrained denoiser as a\nsurrogate-despite the lack of general theoretical justification for this\nsubstitution. In this work, we show that a simple algorithm, closely related to\nseveral used in practice, provably converges to the proximal operator under a\nlog-concavity assumption on the prior $p$. We show that this algorithm can be\ninterpreted as a gradient descent on smoothed proximal objectives. Our analysis\nthus provides a theoretical foundation for a class of empirically successful\nbut previously heuristic methods.", "published": "2025-07-21 08:59:33", "link": "http://arxiv.org/abs/2507.15397v1", "categories": ["cs.LG", "math.OC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Learning to Gridize: Segment Physical World by Wireless Communication Channel", "abstract": "Gridization, the process of partitioning space into grids where users share\nsimilar channel characteristics, serves as a fundamental prerequisite for\nefficient large-scale network optimization. However, existing methods like\nGeographical or Beam Space Gridization (GSG or BSG) are limited by reliance on\nunavailable location data or the flawed assumption that similar signal\nstrengths imply similar channel properties. We propose Channel Space\nGridization (CSG), a pioneering framework that unifies channel estimation and\ngridization for the first time. Formulated as a joint optimization problem, CSG\nuses only beam-level reference signal received power (RSRP) to estimate Channel\nAngle Power Spectra (CAPS) and partition samples into grids with homogeneous\nchannel characteristics. To perform CSG, we develop the CSG Autoencoder\n(CSG-AE), featuring a trainable RSRP-to-CAPS encoder, a learnable sparse\ncodebook quantizer, and a physics-informed decoder based on the Localized\nStatistical Channel Model. On recognizing the limitations of naive training\nscheme, we propose a novel Pretraining-Initialization-Detached-Asynchronous\n(PIDA) training scheme for CSG-AE, ensuring stable and effective training by\nsystematically addressing the common pitfalls of the naive training paradigm.\nEvaluations reveal that CSG-AE excels in CAPS estimation accuracy and\nclustering quality on synthetic data. On real-world datasets, it reduces Active\nMean Absolute Error (MAE) by 30\\% and Overall MAE by 65\\% on RSRP prediction\naccuracy compared to salient baselines using the same data, while improving\nchannel consistency, cluster sizes balance, and active ratio, advancing the\ndevelopment of gridization for large-scale network optimization.", "published": "2025-07-21 08:43:34", "link": "http://arxiv.org/abs/2507.15386v1", "categories": ["cs.LG", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Efficient Visual Appearance Optimization by Learning from Prior Preferences", "abstract": "Adjusting visual parameters such as brightness and contrast is common in our\neveryday experiences. Finding the optimal parameter setting is challenging due\nto the large search space and the lack of an explicit objective function,\nleaving users to rely solely on their implicit preferences. Prior work has\nexplored Preferential Bayesian Optimization (PBO) to address this challenge,\ninvolving users to iteratively select preferred designs from candidate sets.\nHowever, PBO often requires many rounds of preference comparisons, making it\nmore suitable for designers than everyday end-users. We propose Meta-PO, a\nnovel method that integrates PBO with meta-learning to improve sample\nefficiency. Specifically, Meta-PO infers prior users' preferences and stores\nthem as models, which are leveraged to intelligently suggest design candidates\nfor the new users, enabling faster convergence and more personalized results.\nAn experimental evaluation of our method for appearance design tasks on 2D and\n3D content showed that participants achieved satisfactory appearance in 5.86\niterations using Meta-PO when participants shared similar goals with a\npopulation (e.g., tuning for a ``warm'' look) and in 8 iterations even\ngeneralizes across divergent goals (e.g., from ``vintage'', ``warm'', to\n``holiday''). Meta-PO makes personalized visual optimization more applicable to\nend-users through a generalizable, more efficient optimization conditioned on\npreferences, with the potential to scale interface personalization more\nbroadly.", "published": "2025-07-21 08:08:04", "link": "http://arxiv.org/abs/2507.15355v1", "categories": ["cs.HC", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Language Generation in the Limit: Noise, Loss, and Feedback", "abstract": "Kleinberg and Mullainathan (2024) recently proposed a formal framework called\nlanguage generation in the limit and showed that given a sequence of example\nstrings from an unknown target language drawn from any countable collection, an\nalgorithm can correctly generate unseen strings from the target language within\nfinite time. This notion was further refined by Li, Raman, and Tewari (2024),\nwho defined stricter categories of non-uniform and uniform generation. They\nshowed that a finite union of uniformly generatable collections is generatable\nin the limit, and asked if the same is true for non-uniform generation.\n  We begin by resolving the question in the negative: we give a uniformly\ngeneratable collection and a non-uniformly generatable collection whose union\nis not generatable in the limit. We then use facets of this construction to\nfurther our understanding of several variants of language generation. The first\ntwo, generation with noise and without samples, were introduced by Raman and\nRaman (2025) and Li, Raman, and Tewari (2024) respectively. We show the\nequivalence of these models for uniform and non-uniform generation, and provide\na characterization of non-uniform noisy generation. The former paper asked if\nthere is any separation between noisy and non-noisy generation in the limit --\nwe show that such a separation exists even with a single noisy string. Finally,\nwe study the framework of generation with feedback, introduced by Charikar and\nPabbaraju (2025), where the algorithm is strengthened by allowing it to ask\nmembership queries. We show finite queries add no power, but infinite queries\nyield a strictly more powerful model.\n  In summary, the results in this paper resolve the union-closedness of\nlanguage generation in the limit, and leverage those techniques (and others) to\ngive precise characterizations for natural variants that incorporate noise,\nloss, and feedback.", "published": "2025-07-21 07:18:04", "link": "http://arxiv.org/abs/2507.15319v1", "categories": ["cs.DS", "cs.LG"], "primary_category": "cs.DS"}
{"title": "Universal crystal material property prediction via multi-view geometric fusion in graph transformers", "abstract": "Accurately and comprehensively representing crystal structures is critical\nfor advancing machine learning in large-scale crystal materials simulations,\nhowever, effectively capturing and leveraging the intricate geometric and\ntopological characteristics of crystal structures remains a core, long-standing\nchallenge for most existing methods in crystal property prediction. Here, we\npropose MGT, a multi-view graph transformer framework that synergistically\nfuses SE3 invariant and SO3 equivariant graph representations, which\nrespectively captures rotation-translation invariance and rotation equivariance\nin crystal geometries. To strategically incorporate these complementary\ngeometric representations, we employ a lightweight mixture of experts router in\nMGT to adaptively adjust the weight assigned to SE3 and SO3 embeddings based on\nthe specific target task. Compared with previous state-of-the-art models, MGT\nreduces the mean absolute error by up to 21% on crystal property prediction\ntasks through multi-task self-supervised pretraining. Ablation experiments and\ninterpretable investigations confirm the effectiveness of each technique\nimplemented in our framework. Additionally, in transfer learning scenarios\nincluding crystal catalyst adsorption energy and hybrid perovskite bandgap\nprediction, MGT achieves performance improvements of up to 58% over existing\nbaselines, demonstrating domain-agnostic scalability across diverse application\ndomains. As evidenced by the above series of studies, we believe that MGT can\nserve as useful model for crystal material property prediction, providing a\nvaluable tool for the discovery of novel materials.", "published": "2025-07-21 07:06:26", "link": "http://arxiv.org/abs/2507.15303v1", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "primary_category": "cs.LG"}
{"title": "Feel-Good Thompson Sampling for Contextual Bandits: a Markov Chain Monte Carlo Showdown", "abstract": "Thompson Sampling (TS) is widely used to address the exploration/exploitation\ntradeoff in contextual bandits, yet recent theory shows that it does not\nexplore aggressively enough in high-dimensional problems. Feel-Good Thompson\nSampling (FG-TS) addresses this by adding an optimism bonus that biases toward\nhigh-reward models, and it achieves the asymptotically minimax-optimal regret\nin the linear setting when posteriors are exact. However, its performance with\n\\emph{approximate} posteriors -- common in large-scale or neural problems --\nhas not been benchmarked. We provide the first systematic study of FG-TS and\nits smoothed variant (SFG-TS) across eleven real-world and synthetic\nbenchmarks. To evaluate their robustness, we compare performance across\nsettings with exact posteriors (linear and logistic bandits) to approximate\nregimes produced by fast but coarse stochastic-gradient samplers. Ablations\nover preconditioning, bonus scale, and prior strength reveal a trade-off:\nlarger bonuses help when posterior samples are accurate, but hurt when sampling\nnoise dominates. FG-TS generally outperforms vanilla TS in linear and logistic\nbandits, but tends to be weaker in neural bandits. Nevertheless, because FG-TS\nand its variants are competitive and easy-to-use, we recommend them as\nbaselines in modern contextual-bandit benchmarks. Finally, we provide source\ncode for all our experiments in\nhttps://github.com/SarahLiaw/ctx-bandits-mcmc-showdown.", "published": "2025-07-21 06:42:56", "link": "http://arxiv.org/abs/2507.15290v1", "categories": ["cs.LG", "I.2.6; I.2.0"], "primary_category": "cs.LG"}
{"title": "Machine Unlearning for Streaming Forgetting", "abstract": "Machine unlearning aims to remove knowledge of the specific training data in\na well-trained model. Currently, machine unlearning methods typically handle\nall forgetting data in a single batch, removing the corresponding knowledge all\nat once upon request. However, in practical scenarios, requests for data\nremoval often arise in a streaming manner rather than in a single batch,\nleading to reduced efficiency and effectiveness in existing methods. Such\nchallenges of streaming forgetting have not been the focus of much research. In\nthis paper, to address the challenges of performance maintenance, efficiency,\nand data access brought about by streaming unlearning requests, we introduce a\nstreaming unlearning paradigm, formalizing the unlearning as a distribution\nshift problem. We then estimate the altered distribution and propose a novel\nstreaming unlearning algorithm to achieve efficient streaming forgetting\nwithout requiring access to the original training data. Theoretical analyses\nconfirm an $O(\\sqrt{T} + V_T)$ error bound on the streaming unlearning regret,\nwhere $V_T$ represents the cumulative total variation in the optimal solution\nover $T$ learning rounds. This theoretical guarantee is achieved under mild\nconditions without the strong restriction of convex loss function. Experiments\nacross various models and datasets validate the performance of our proposed\nmethod.", "published": "2025-07-21 06:30:25", "link": "http://arxiv.org/abs/2507.15280v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Temporal Basis Function Models for Closed-Loop Neural Stimulation", "abstract": "Closed-loop neural stimulation provides novel therapies for neurological\ndiseases such as Parkinson's disease (PD), but it is not yet clear whether\nartificial intelligence (AI) techniques can tailor closed-loop stimulation to\nindividual patients or identify new therapies. Progress requires us to address\na number of translational issues, including sample efficiency, training time,\nand minimizing loop latency such that stimulation may be shaped in response to\nchanging brain activity. We propose temporal basis function models (TBFMs) to\naddress these difficulties, and explore this approach in the context of\nexcitatory optogenetic stimulation. We demonstrate the ability of TBF models to\nprovide a single-trial, spatiotemporal forward prediction of the effect of\noptogenetic stimulation on local field potentials (LFPs) measured in two\nnon-human primates. We further use simulations to demonstrate the use of TBF\nmodels for closed-loop stimulation, driving neural activity towards target\npatterns. The simplicity of TBF models allow them to be sample efficient, rapid\nto train (2-4min), and low latency (0.2ms) on desktop CPUs. We demonstrate the\nmodel on 40 sessions of previously published excitatory optogenetic stimulation\ndata. For each session, the model required 15-20min of data collection to\nsuccessfully model the remainder of the session. It achieved a prediction\naccuracy comparable to a baseline nonlinear dynamical systems model that\nrequires hours to train, and superior accuracy to a linear state-space model.\nIn our simulations, it also successfully allowed a closed-loop stimulator to\ncontrol a neural circuit. Our approach begins to bridge the translational gap\nbetween complex AI-based approaches to modeling dynamical systems and the\nvision of using such forward prediction models to develop novel, clinically\nuseful closed-loop stimulation protocols.", "published": "2025-07-21 06:21:58", "link": "http://arxiv.org/abs/2507.15274v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "On exploration of an interior mirror descent flow for stochastic nonconvex constrained problem", "abstract": "We study a nonsmooth nonconvex optimization problem defined over nonconvex\nconstraints, where the feasible set is given by the intersection of the closure\nof an open set and a smooth manifold. By endowing the open set with a\nRiemannian metric induced by a barrier function, we obtain a Riemannian\nsubgradient flow formulated as a differential inclusion, which remains strictly\nwithin the interior of the feasible set. This continuous dynamical system\nunifies two classes of iterative optimization methods, namely the Hessian\nbarrier method and mirror descent scheme, by revealing that these methods can\nbe interpreted as discrete approximations of the continuous flow. We explore\nthe long-term behavior of the trajectories generated by this dynamical system\nand show that the existing deficient convergence properties of the Hessian\nbarrier and mirror descent scheme can be unifily and more insightfully\ninterpreted through these of the continuous trajectory. For instance, the\nnotorious spurious stationary points \\cite{chen2024spurious} observed in\nHessian barrier method and mirror descent scheme are interpreted as stable\nequilibria of the dynamical system that do not correspond to real stationary\npoints of the original optimization problem. We provide two sufficient\ncondition such that these spurious stationary points can be avoided if the\nstrict complementarity conditions holds. In the absence of these regularity\ncondition, we propose a random perturbation strategy that ensures the\ntrajectory converges (subsequentially) to an approximate stationary point.\nBuilding on these insights, we introduce two iterative Riemannian subgradient\nmethods, form of interior point methods, that generalizes the existing Hessian\nbarrier method and mirror descent scheme for solving nonsmooth nonconvex\noptimization problems.", "published": "2025-07-21 05:58:52", "link": "http://arxiv.org/abs/2507.15264v1", "categories": ["math.OC", "cs.LG"], "primary_category": "math.OC"}
{"title": "CHORDS: Diffusion Sampling Accelerator with Multi-core Hierarchical ODE Solvers", "abstract": "Diffusion-based generative models have become dominant generators of\nhigh-fidelity images and videos but remain limited by their computationally\nexpensive inference procedures. Existing acceleration techniques either require\nextensive model retraining or compromise significantly on sample quality. This\npaper explores a general, training-free, and model-agnostic acceleration\nstrategy via multi-core parallelism. Our framework views multi-core diffusion\nsampling as an ODE solver pipeline, where slower yet accurate solvers\nprogressively rectify faster solvers through a theoretically justified\ninter-core communication mechanism. This motivates our multi-core training-free\ndiffusion sampling accelerator, CHORDS, which is compatible with various\ndiffusion samplers, model architectures, and modalities. Through extensive\nexperiments, CHORDS significantly accelerates sampling across diverse\nlarge-scale image and video diffusion models, yielding up to 2.1x speedup with\nfour cores, improving by 50% over baselines, and 2.9x speedup with eight cores,\nall without quality degradation. This advancement enables CHORDS to establish a\nsolid foundation for real-time, high-fidelity diffusion generation.", "published": "2025-07-21 05:48:47", "link": "http://arxiv.org/abs/2507.15260v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Physics-Informed Learning of Proprietary Inverter Models for Grid Dynamic Studies", "abstract": "This letter develops a novel physics-informed neural ordinary differential\nequations-based framework to emulate the proprietary dynamics of the inverters\n-- essential for improved accuracy in grid dynamic simulations. In current\nindustry practice, the original equipment manufacturers (OEMs) often do not\ndisclose the exact internal controls and parameters of the inverters, posing\nsignificant challenges in performing accurate dynamic simulations and other\nrelevant studies, such as gain tunings for stability analysis and controls. To\naddress this, we propose a Physics-Informed Latent Neural ODE Model (PI-LNM)\nthat integrates system physics with neural learning layers to capture the\nunmodeled behaviors of proprietary units. The proposed method is validated\nusing a grid-forming inverter (GFM) case study, demonstrating improved dynamic\nsimulation accuracy over approaches that rely solely on data-driven learning\nwithout physics-based guidance.", "published": "2025-07-21 05:48:31", "link": "http://arxiv.org/abs/2507.15259v1", "categories": ["eess.SY", "cs.LG", "cs.SY"], "primary_category": "eess.SY"}
{"title": "Exact Reformulation and Optimization for Direct Metric Optimization in Binary Imbalanced Classification", "abstract": "For classification with imbalanced class frequencies, i.e., imbalanced\nclassification (IC), standard accuracy is known to be misleading as a\nperformance measure. While most existing methods for IC resort to optimizing\nbalanced accuracy (i.e., the average of class-wise recalls), they fall short in\nscenarios where the significance of classes varies or certain metrics should\nreach prescribed levels. In this paper, we study two key classification\nmetrics, precision and recall, under three practical binary IC settings: fix\nprecision optimize recall (FPOR), fix recall optimize precision (FROP), and\noptimize $F_\\beta$-score (OFBS). Unlike existing methods that rely on smooth\napproximations to deal with the indicator function involved, \\textit{we\nintroduce, for the first time, exact constrained reformulations for these\ndirect metric optimization (DMO) problems}, which can be effectively solved by\nexact penalty methods. Experiment results on multiple benchmark datasets\ndemonstrate the practical superiority of our approach over the state-of-the-art\nmethods for the three DMO problems. We also expect our exact reformulation and\noptimization (ERO) framework to be applicable to a wide range of DMO problems\nfor binary IC and beyond. Our code is available at\nhttps://github.com/sun-umn/DMO.", "published": "2025-07-21 04:52:51", "link": "http://arxiv.org/abs/2507.15240v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Accelerated Bayesian Optimal Experimental Design via Conditional Density Estimation and Informative Data", "abstract": "The Design of Experiments (DOEs) is a fundamental scientific methodology that\nprovides researchers with systematic principles and techniques to enhance the\nvalidity, reliability, and efficiency of experimental outcomes. In this study,\nwe explore optimal experimental design within a Bayesian framework, utilizing\nBayes' theorem to reformulate the utility expectation--originally expressed as\na nested double integral--into an independent double integral form,\nsignificantly improving numerical efficiency. To further accelerate the\ncomputation of the proposed utility expectation, conditional density estimation\nis employed to approximate the ratio of two Gaussian random fields, while\ncovariance serves as a selection criterion to identify informative datasets\nduring model fitting and integral evaluation. In scenarios characterized by low\nsimulation efficiency and high costs of raw data acquisition, key challenges\nsuch as surrogate modeling, failure probability estimation, and parameter\ninference are systematically restructured within the Bayesian experimental\ndesign framework. The effectiveness of the proposed methodology is validated\nthrough both theoretical analysis and practical applications, demonstrating its\npotential for enhancing experimental efficiency and decision-making under\nuncertainty.", "published": "2025-07-21 04:41:05", "link": "http://arxiv.org/abs/2507.15235v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Robust and Differentially Private PCA for non-Gaussian data", "abstract": "Recent advances have sparked significant interest in the development of\nprivacy-preserving Principal Component Analysis (PCA). However, many existing\napproaches rely on restrictive assumptions, such as assuming sub-Gaussian data\nor being vulnerable to data contamination. Additionally, some methods are\ncomputationally expensive or depend on unknown model parameters that must be\nestimated, limiting their accessibility for data analysts seeking\nprivacy-preserving PCA. In this paper, we propose a differentially private PCA\nmethod applicable to heavy-tailed and potentially contaminated data. Our\napproach leverages the property that the covariance matrix of properly rescaled\ndata preserves eigenvectors and their order under elliptical distributions,\nwhich include Gaussian and heavy-tailed distributions. By applying a bounded\ntransformation, we enable straightforward computation of principal components\nin a differentially private manner. Additionally, boundedness guarantees\nrobustness against data contamination. We conduct both theoretical analysis and\nempirical evaluations of the proposed method, focusing on its ability to\nrecover the subspace spanned by the leading principal components. Extensive\nnumerical experiments demonstrate that our method consistently outperforms\nexisting approaches in terms of statistical utility, particularly in\nnon-Gaussian or contaminated data settings.", "published": "2025-07-21 04:27:09", "link": "http://arxiv.org/abs/2507.15232v1", "categories": ["stat.ME", "cs.LG"], "primary_category": "stat.ME"}
{"title": "Misspecifying non-compensatory as compensatory IRT: analysis of estimated skills and variance", "abstract": "Multidimensional item response theory is a statistical test theory used to\nestimate the latent skills of learners and the difficulty levels of problems\nbased on test results. Both compensatory and non-compensatory models have been\nproposed in the literature. Previous studies have revealed the substantial\nunderestimation of higher skills when the non-compensatory model is\nmisspecified as the compensatory model. However, the underlying mechanism\nbehind this phenomenon has not been fully elucidated. It remains unclear\nwhether overestimation also occurs and whether issues arise regarding the\nvariance of the estimated parameters. In this paper, we aim to provide a\ncomprehensive understanding of both underestimation and overestimation through\na theoretical approach. In addition to the previously identified\nunderestimation of the skills, we newly discover that the overestimation of\nskills occurs around the origin. Furthermore, we investigate the extent to\nwhich the asymptotic variance of the estimated parameters differs when\nconsidering model misspecification compared to when it is not taken into\naccount.", "published": "2025-07-21 03:52:09", "link": "http://arxiv.org/abs/2507.15222v1", "categories": ["stat.ME", "cs.LG", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Feature Construction Using Network Control Theory and Rank Encoding for Graph Machine Learning", "abstract": "In this article, we utilize the concept of average controllability in graphs,\nalong with a novel rank encoding method, to enhance the performance of Graph\nNeural Networks (GNNs) in social network classification tasks. GNNs have proven\nhighly effective in various network-based learning applications and require\nsome form of node features to function. However, their performance is heavily\ninfluenced by the expressiveness of these features. In social networks, node\nfeatures are often unavailable due to privacy constraints or the absence of\ninherent attributes, making it challenging for GNNs to achieve optimal\nperformance. To address this limitation, we propose two strategies for\nconstructing expressive node features. First, we introduce average\ncontrollability along with other centrality metrics (denoted as NCT-EFA) as\nnode-level metrics that capture critical aspects of network topology. Building\non this, we develop a rank encoding method that transforms average\ncontrollability or any other graph-theoretic metric into a fixed-dimensional\nfeature space, thereby improving feature representation. We conduct extensive\nnumerical evaluations using six benchmark GNN models across four social network\ndatasets to compare different node feature construction methods. Our results\ndemonstrate that incorporating average controllability into the feature space\nsignificantly improves GNN performance. Moreover, the proposed rank encoding\nmethod outperforms traditional one-hot degree encoding, improving the ROC AUC\nfrom 68.7% to 73.9% using GraphSAGE on the GitHub Stargazers dataset,\nunderscoring its effectiveness in generating expressive and efficient node\nrepresentations.", "published": "2025-07-21 02:45:55", "link": "http://arxiv.org/abs/2507.15195v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Joint-Local Grounded Action Transformation for Sim-to-Real Transfer in Multi-Agent Traffic Control", "abstract": "Traffic Signal Control (TSC) is essential for managing urban traffic flow and\nreducing congestion. Reinforcement Learning (RL) offers an adaptive method for\nTSC by responding to dynamic traffic patterns, with multi-agent RL (MARL)\ngaining traction as intersections naturally function as coordinated agents.\nHowever, due to shifts in environmental dynamics, implementing MARL-based TSC\npolicies in the real world often leads to a significant performance drop, known\nas the sim-to-real gap. Grounded Action Transformation (GAT) has successfully\nmitigated this gap in single-agent RL for TSC, but real-world traffic networks,\nwhich involve numerous interacting intersections, are better suited to a MARL\nframework. In this work, we introduce JL-GAT, an application of GAT to\nMARL-based TSC that balances scalability with enhanced grounding capability by\nincorporating information from neighboring agents. JL-GAT adopts a\ndecentralized approach to GAT, allowing for the scalability often required in\nreal-world traffic networks while still capturing key interactions between\nagents. Comprehensive experiments on various road networks under simulated\nadverse weather conditions, along with ablation studies, demonstrate the\neffectiveness of JL-GAT. The code is publicly available at\nhttps://github.com/DaRL-LibSignal/JL-GAT/.", "published": "2025-07-21 01:33:59", "link": "http://arxiv.org/abs/2507.15174v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Better Models and Algorithms for Learning Ising Models from Dynamics", "abstract": "We study the problem of learning the structure and parameters of the Ising\nmodel, a fundamental model of high-dimensional data, when observing the\nevolution of an associated Markov chain. A recent line of work has studied the\nnatural problem of learning when observing an evolution of the well-known\nGlauber dynamics [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,\nGaitonde, Mossel STOC 2024], which provides an arguably more realistic\ngenerative model than the classical i.i.d. setting. However, this prior work\ncrucially assumes that all site update attempts are observed, \\emph{even when\nthis attempt does not change the configuration}: this strong observation model\nis seemingly essential for these approaches. While perhaps possible in\nrestrictive contexts, this precludes applicability to most realistic settings\nwhere we can observe \\emph{only} the stochastic evolution itself, a minimal and\nnatural assumption for any process we might hope to learn from. However,\ndesigning algorithms that succeed in this more realistic setting has remained\nan open problem [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,\nGaitonde, Moitra, Mossel, STOC 2025].\n  In this work, we give the first algorithms that efficiently learn the Ising\nmodel in this much more natural observation model that only observes when the\nconfiguration changes. For Ising models with maximum degree $d$, our algorithm\nrecovers the underlying dependency graph in time $\\mathsf{poly}(d)\\cdot n^2\\log\nn$ and then the actual parameters in additional $\\widetilde{O}(2^d n)$ time,\nwhich qualitatively matches the state-of-the-art even in the i.i.d. setting in\na much weaker observation model. Our analysis holds more generally for a\nbroader class of reversible, single-site Markov chains that also includes the\npopular Metropolis chain by leveraging more robust properties of reversible\nMarkov chains.", "published": "2025-07-21 01:26:57", "link": "http://arxiv.org/abs/2507.15173v1", "categories": ["cs.LG", "cs.DS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Asynchronous Collective Tree Exploration: a Distributed Algorithm, and a new Lower Bound", "abstract": "We study the problem of collective tree exploration in which a team of $k$\nmobile agents must collectively visit all nodes of an unknown tree in as few\nmoves as possible. The agents all start from the root and discover adjacent\nedges as they progress in the tree. Communication is distributed in the sense\nthat agents share information by reading and writing on whiteboards located at\nall nodes. Movements are asynchronous, in the sense that the speeds of all\nagents are controlled by an adversary at all times. All previous competitive\nguarantees for collective tree exploration are either distributed but\nsynchronous, or asynchronous but centralized. In contrast, we present a\ndistributed asynchronous algorithm that explores any tree of $n$ nodes and\ndepth $D$ in at most $2n+O(k^2 2^kD)$ moves, i.e., with a regret that is linear\nin $D$, and a variant algorithm with a guarantee in $O(k/\\log k)(n+kD)$, i.e.,\nwith a competitive ratio in $O(k/\\log k)$. We note that our regret guarantee is\nasymptotically optimal (i.e., $1$-competitive) from the perspective of\naverage-case complexity. We then present a new general lower bound on the\ncompetitive ratio of asynchronous collective tree exploration, in\n$\\Omega(\\log^2 k)$. This lower bound applies to both the distributed and\ncentralized settings, and improves upon the previous lower bound in\n$\\Omega(\\log k)$.", "published": "2025-07-21 14:21:13", "link": "http://arxiv.org/abs/2507.15658v1", "categories": ["cs.DS", "cs.DC", "cs.MA"], "primary_category": "cs.DS"}
{"title": "Iterative thresholding low-rank time integration", "abstract": "We develop time integration methods in low-rank representation that can\nadaptively adjust approximation ranks to achieve a prescribed accuracy, while\nensuring that these ranks remain proportional to the corresponding best\napproximation ranks. Our approach relies on an iterative scheme combined with\nsoft thresholding of the iterates. A model case of a time-dependent\nSchr\\\"odinger equation with low-rank matrix approximation is analyzed in\ndetail, and the required modifications for second-order parabolic problems are\ndescribed. Numerical tests illustrate the results for both cases.", "published": "2025-07-21 17:55:23", "link": "http://arxiv.org/abs/2507.15848v1", "categories": ["math.NA", "cs.NA", "Primary 65F55, 65M12, Secondary 65Y20, 65L70"], "primary_category": "math.NA"}
{"title": "Data-driven optimal approximation on Hardy spaces in simply connected domains", "abstract": "We consider optimal interpolation of functions analytic in simply connected\ndomains in the complex plane. By choosing a specific structure for the\napproximant, we show that the resulting first order optimality conditions can\nbe interpreted as optimal $\\mathcal{H}_2$ interpolation conditions for\ndiscrete-time dynamical systems. Connections to the implicit Euler method, the\nmidpoint method, and backward differentiation methods are also established. A\ndata-driven algorithm is developed to compute a (locally) optimal approximant.\nOur method is tested on three numerical experiments.", "published": "2025-07-21 17:47:44", "link": "http://arxiv.org/abs/2507.15837v1", "categories": ["math.NA", "cs.NA", "math.OC"], "primary_category": "math.NA"}
{"title": "Generic cuspidal points and their localization", "abstract": "In this work we consider generic coalescing of eigenvalues of smooth complex\nvalued matrix functions depending on 2 parameters. We call generic cuspidal\npoints the parameter values where eigenvalues coalesce and we discuss the\nrelation between cuspidal points and the closely related exceptional points\nstudied in the literature. By considering loops in parameter space enclosing\nthe cuspidal points, we rigorously prove when there is a phase accumulation for\nthe eigenvectors and further detail how, by looking at the periodicity of the\neigenvalues along the loop, and/or by looking at the aforementioned phase\naccumulation, one may be able to localize generic cuspidal points.", "published": "2025-07-21 16:19:21", "link": "http://arxiv.org/abs/2507.15762v1", "categories": ["math.RA", "cs.NA", "math.NA", "15A18, 15A20, 15A23, 15A99"], "primary_category": "math.RA"}
{"title": "Minimal horizontal triods: Analysis and computation", "abstract": "In this article we investigate the question of finding a network\nconfiguration of minimal length connecting three given points in the Heisenberg\ngroup. After proving existence of (possibly degenerate) minimal horizontal\ntriods, we investigate their characterization. We then formulate a horizontal\ncurve shortening flow that deforms any given suitable initial triod into a\ncritical point for the length functional. Numerical experiments based on a\nstable fully discrete finite element scheme provide useful insights into the\nrich landscape of this sub-Riemannian geometry.", "published": "2025-07-21 15:54:03", "link": "http://arxiv.org/abs/2507.15740v1", "categories": ["math.AP", "cs.NA", "math.DG", "math.NA"], "primary_category": "math.AP"}
{"title": "Sensor network localization has a benign landscape after low-dimensional relaxation", "abstract": "We consider the sensor network localization problem, also known as\nmultidimensional scaling or Euclidean distance matrix completion. Given a\nground truth configuration of $n$ points in $\\mathbb{R}^\\ell$, we observe a\nsubset of the pairwise distances and aim to recover the underlying\nconfiguration (up to rigid transformations). We show with a simple\ncounterexample that the associated optimization problem is nonconvex and may\nadmit spurious local minimizers, even when all distances are known. Yet,\ninspired by numerical experiments, we argue that all second-order critical\npoints become global minimizers when the problem is relaxed by optimizing over\nconfigurations in dimension $k > \\ell$. Specifically, we show this for two\nsettings, both when all pairwise distances are known: (1) for arbitrary ground\ntruth points, and $k= O(\\sqrt{\\ell n})$, and: (2) for isotropic random ground\ntruth points, and $k = O(\\ell + \\log n)$. To prove these results, we identify\nand exploit key properties of the linear map which sends inner products to\nsquared distances.", "published": "2025-07-21 14:23:05", "link": "http://arxiv.org/abs/2507.15662v1", "categories": ["math.OC", "cs.NA", "math.NA"], "primary_category": "math.OC"}
{"title": "Mathematical modeling and sensitivity analysis of hypoxia-activated drugs", "abstract": "Hypoxia-activated prodrugs offer a promising strategy for targeting\noxygen-deficient regions in solid tumors, which are often resistant to\nconventional therapies. However, modeling their behavior is challenging because\nof the complex interplay between oxygen availability, drug activation, and cell\nsurvival. In this work, we develop a multiscale and mixed-dimensional model\nthat couples spatially resolved drug and oxygen transport with pharmacokinetics\nand pharmacodynamics to simulate the cellular response. The model integrates\nblood flow, oxygen diffusion and consumption, drug delivery, and metabolism. To\nreduce computational cost, we mitigate the global nonlinearity through a\none-way coupling of the multiscale and mixed/dimensional models with a reduced\n0D model for the drug metabolism. The global sensitivity analysis is then used\nto identify key parameters influencing drug activation and therapeutic outcome.\nThis approach enables efficient simulation and supports the design of optimized\nhypoxia-targeted therapies.", "published": "2025-07-21 14:03:49", "link": "http://arxiv.org/abs/2507.15642v1", "categories": ["math.NA", "cs.NA", "92C50, 92C45, 35Q92, 35R20, 65M60"], "primary_category": "math.NA"}
{"title": "Advancing Lunar Communication through Inter-domain Space Networks and Dynamic Orchestration", "abstract": "The reawakened era of lunar exploration is defined by a strategic shift from\ntemporary visits to a sustained international and commercial presence,\nresulting in an unprecedented demand for a robust and continuously available\ncommunication infrastructure. The conventional direct-to-Earth communication\narchitecture relies on limited and oversubscribed deep space networks, which\nare further challenged by the radiative environment and insufficient visibility\nin certain areas of the cislunar domain. We address these issues by proposing a\nfoundational move toward inter-domain space network cooperation by introducing\narchitectures based on near space networks. They can directly service lunar\nsurface users or, via cislunar relays, by forming a resilient and multi-layered\ncommunication backbone. First, we establish a unified link analysis framework\nincorporating frequently disregarded environmental factors, such as the Moon's\nvariable illumination, to provide a high-fidelity performance evaluation.\nSecond, we assess architectures' reliability based on the outage risk,\nessential for quantifying the operational robustness of communication links.\nFinally, to manage the inherent dynamism of architectures, we propose an\ninter-domain space digital twin$-$a dynamic decision-making engine that\nperforms real-time analysis to autonomously select the best communication path,\nensuring high and stable reliability while simultaneously optimizing power\nconsumption. Overall, our paper provides a holistic architectural and\nconceptual management framework, emphasizing the necessity of lunar\ncommunications to support a permanent human and economic foothold on the Moon.", "published": "2025-07-21 10:39:25", "link": "http://arxiv.org/abs/2507.15483v1", "categories": ["cs.ET", "cs.NA", "math.NA"], "primary_category": "cs.ET"}
{"title": "tiDAS: a time invariant approximation of the Delay and Sum algorithm for biomedical ultrasound PSF reconstructions", "abstract": "Ultrasound imaging is a real-time diagnostic modality that reconstructs\nacoustic signals into visual representations of internal body structures. A key\ncomponent in this process is beamforming, with the Delay and Sum (DAS)\nalgorithm being a standard due to its balance between simplicity and\neffectiveness. However, the computational cost of DAS can be a limiting factor,\nespecially in real-time scenarios where fast frame reconstruction is essential.\nIn this work, we introduce a time-invariant approximation of the DAS algorithm\n(tiDAS), designed to accelerate the reconstruction process without compromising\nimage quality. By adopting a one-dimensional, row-wise convolutional\nformulation, tiDAS significantly reduces computational complexity while\npreserving the core properties of the original model. This approach not only\nenables faster image reconstruction but also provides a structured foundation\nfor the application of deconvolution methods aimed at enhancing resolution.\nSynthetic experiments demonstrate that tiDAS achieves a favorable trade-off\nbetween speed and accuracy, making it a promising tool for improving the\nefficiency of real-time ultrasound imaging.", "published": "2025-07-21 10:17:56", "link": "http://arxiv.org/abs/2507.15464v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Neural Preconditioning via Krylov Subspace Geometry", "abstract": "We propose a geometry-aware strategy for training neural preconditioners\ntailored to parametrized linear systems arising from the discretization of\nmixed-dimensional partial differential equations (PDEs). These systems are\ntypically ill-conditioned because of the presence of embedded lower-dimensional\nstructures and are solved using Krylov subspace methods. Our approach yields an\napproximation of the inverse operator employing a learning algorithm consisting\nof a two-stage training framework: an initial static pre-training phase, based\non residual minimization, followed by a dynamic fine-tuning phase that\nincorporates solver convergence dynamics into training via a novel loss\nfunctional. This dynamic loss is defined by the principal angles between the\nresiduals and the Krylov subspaces. It is evaluated using a differentiable\nimplementation of the Flexible GMRES algorithm, which enables backpropagation\nthrough both the Arnoldi process and Givens rotations. The resulting neural\npreconditioner is explicitly optimized to improve early-stage convergence and\nreduce iteration counts in a family of 3D-1D mixed-dimensional problems with\ngeometric variability of the 1D domain. Numerical experiments show that our\nsolver-aligned approach significantly improves convergence rate, robustness,\nand generalization.", "published": "2025-07-21 10:03:55", "link": "http://arxiv.org/abs/2507.15452v1", "categories": ["math.NA", "cs.NA", "65F08, 65F10, 68T07, 65Y20"], "primary_category": "math.NA"}
{"title": "PDEformer-2: A Versatile Foundation Model for Two-Dimensional Partial Differential Equations", "abstract": "Partial differential equations (PDEs) play a central role in describing many\nphysical phenomena. Various scientific and engineering applications demand a\nversatile and differentiable PDE solver that can quickly generate solutions\nwith adequate accuracy, and limitations of the traditional solvers and\nspecialized neural operators motivate the development of foundation models for\nsolving PDEs. This paper introduces PDEformer-2, a versatile foundation model\nfor two-dimensional PDEs. Based on our previous one-dimensional PDEformer-1\nmodel, PDEformer-2 receives the PDE form as network input via computational\ngraph representation, which has the flexibility to encode most common PDEs. The\nmesh-free predicted solutions can be directly queried at arbitrary\nspatio-temporal coordinates. A large (40TB) diverse dataset is employed to\npretrain the current model, making it capable of simultaneously addressing PDEs\nwith different symbolic forms, domain shapes, boundary conditions, number of\nvariables, and time-dependency. Accurate zero-shot prediction is allowed for\nPDEs that resemble the pretraining ones. When adapted to new unseen PDEs,\nPDEformer-2 demonstrates faster learning than many specialized models, and has\nsmaller errors given limited (less than 100) samples. Additionally, PDEformer-2\ncan be employed in the inverse problems thanks to its fast and differentiable\nnature and produces reasonable results in our experiments to recover\ncoefficient scalars and fields of a PDE.", "published": "2025-07-21 09:08:48", "link": "http://arxiv.org/abs/2507.15409v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Superconvergence points of Hermite spectral interpolation", "abstract": "Hermite spectral method plays an important role in the numerical simulation\nof various partial differential equations (PDEs) on unbounded domains. In this\nwork, we study the superconvergence properties of Hermite spectral\ninterpolation, i.e., interpolation at the zeros of Hermite polynomials in the\nspace spanned by Hermite functions. We identify the points at which the\nconvergence rates of the first- and second-order derivatives of the interpolant\nconverge faster. We further extend the analysis to the Hermite spectral\ncollocation method in solving differential equations and identify the\nsuperconvergence points both for function and derivative values. Numerical\nexamples are provided to confirm the analysis of superconvergence points.", "published": "2025-07-21 08:01:46", "link": "http://arxiv.org/abs/2507.15350v1", "categories": ["math.NA", "cs.NA", "41A05, 41A25, 65N35, 65D05"], "primary_category": "math.NA"}
{"title": "Exponential Runge-Kutta Galerkin finite element method for a reaction-diffusion system with nonsmooth initial data", "abstract": "This study presents a numerical analysis of the Field-Noyes\nreaction-diffusion model with nonsmooth initial data, employing a linear\nGalerkin finite element method for spatial discretization and a second-order\nexponential Runge-Kutta scheme for temporal integration. The initial data are\nassumed to reside in the fractional Sobolev space H^gamma with 0 < gamma < 2,\nwhere classical regularity conditions are violated, necessitating specialized\nerror analysis. By integrating semigroup techniques and fractional Sobolev\nspace theory, sharp fully discrete error estimates are derived in both L2 and\nH1 norms. This demonstrates that the convergence order adapts to the smoothness\nof initial data, a key advancement over traditional approaches that assume\nhigher regularity. Numerical examples are provided to support the theoretical\nanalysis.", "published": "2025-07-21 08:01:07", "link": "http://arxiv.org/abs/2507.15345v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Convergence analysis of Anderson acceleration for nonlinear equations with H\u00f6lder continuous derivatives", "abstract": "This work investigates the local convergence behavior of Anderson\nacceleration in solving nonlinear systems. We establish local R-linear\nconvergence results for Anderson acceleration with general depth $m$ under the\nassumptions that the Jacobian of the nonlinear operator is H\\\"older continuous\nand the corresponding fixed-point function is contractive. In the Lipschitz\ncontinuous case, we obtain a sharper R-linear convergence factor. We also\nderive a refined residual bound for the depth $m = 1$ under the same\nassumptions used for the general depth results. Applications to a nonsymmetric\nRiccati equation from transport theory demonstrate that Anderson acceleration\nyields comparable results to several existing fixed-point methods for the\nregular cases, and that it brings significant reductions in both the number of\niterations and computation time, even in challenging cases involving nearly\nsingular or large-scale problems.", "published": "2025-07-21 07:23:56", "link": "http://arxiv.org/abs/2507.15322v1", "categories": ["math.NA", "cs.NA", "65H10, 15A24"], "primary_category": "math.NA"}
{"title": "Efficient evaluation of forward and inverse energy-based magnetic hysteresis operators", "abstract": "The energy-based vector hysteresis model of Francois-Lavet et al. establishes\nan implicit relation between magnetic fields and fluxes via internal magnetic\npolarizations which are determined by convex but non-smooth minimization\nproblems. The systematic solution of these problems for every material point is\na key ingredient for the efficient implementation of the model into standard\nmagnetic field solvers. We propose to approximate the non-smooth terms via\nregularization which allows to employ standard Newton methods for the\nevaluation of the local material models while being in control of the error in\nthis approximation. We further derive the inverse of the regularized hysteresis\noperator which amounts to a regularized version of the inverse hysteresis\nmodel. The magnetic polarizations in this model are again determined by local\nminimization problems which here are coupled across the different pinning\nforces. An efficient algorithm for solving the Newton systems is proposed which\nallows evaluation of the inverse hysteresis operator at the same cost as the\nforward model. Numerical tests on standard benchmark problems are presented for\nillustration of our results.", "published": "2025-07-21 06:40:08", "link": "http://arxiv.org/abs/2507.15289v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "On the stability of the low-rank projector-splitting integrator for hyperbolic and parabolic equations", "abstract": "We study the stability of a class of dynamical low-rank methods--the\nprojector-splitting integrator (PSI)--applied to linear hyperbolic and\nparabolic equations. Using a von Neumann-type analysis, we investigate the\nstability of such low-rank time integrator coupled with standard spatial\ndiscretizations, including upwind and central finite difference schemes, under\ntwo commonly used formulations: discretize-then-project (DtP) and\nproject-then-discretize (PtD). For hyperbolic equations, we show that the\nstability conditions for DtP and PtD are the same under Lie-Trotter splitting,\nand that the stability region can be significantly enlarged by using Strang\nsplitting. For parabolic equations, despite the presence of a negative S-step,\nunconditional stability can still be achieved by employing Crank-Nicolson or a\nhybrid forward-backward Euler scheme in time stepping. While our analysis\nfocuses on simplified model problems, it offers insight into the stability\nbehavior of PSI for more complex systems, such as those arising in kinetic\ntheory.", "published": "2025-07-21 02:32:19", "link": "http://arxiv.org/abs/2507.15192v1", "categories": ["math.NA", "cs.NA", "35L02, 35K10, 65F55, 65M06, 65M12"], "primary_category": "math.NA"}
{"title": "On Subsample Size of Quantile-Based Randomized Kaczmarz", "abstract": "Quantile-based randomized Kaczmarz (QRK) was recently introduced to\nefficiently solve sparsely corrupted linear systems $\\mathbf{A}\n\\mathbf{x}^*+\\mathbf{\\epsilon} = \\mathbf{b}$ [SIAM J. Matrix Anal. Appl.,\n43(2), 605-637], where $\\mathbf{A}\\in \\mathbb{R}^{m\\times n}$ and\n$\\mathbf{\\epsilon}$ is an arbitrary $(\\beta m)$-sparse corruption. However, all\nexisting theoretical guarantees for QRK require quantiles to be computed using\nall $m$ samples (or a subsample of the same order), thus negating the\ncomputational advantage of Kaczmarz-type methods. This paper overcomes the\nbottleneck. We analyze a subsampling QRK, which computes quantiles from $D$\nuniformly chosen samples at each iteration. Under some standard scaling\nassumptions on the coefficient matrix, we show that QRK with subsample size\n$D\\ge\\frac{C\\log (T)}{\\log(1/\\beta)}$ linearly converges over the first $T$\niterations with high probability, where $C$ is some absolute constant. This\nsubsample size is a substantial reduction from $O(m)$ in prior results. For\ninstance, it translates into $O(\\log(n))$ even if an approximation error of\n$\\exp(-n^2)$ is desired. Intriguingly, our subsample size is also tight up to a\nmultiplicative constant: if $D\\le \\frac{c\\log(T)}{\\log(1/\\beta)}$ for some\nconstant $c$, the error of the $T$-th iterate could be arbitrarily large with\nhigh probability. Numerical results are provided to corroborate our theory.", "published": "2025-07-21 02:06:39", "link": "http://arxiv.org/abs/2507.15185v1", "categories": ["math.NA", "cs.NA", "65F10, 68W20, 60B20"], "primary_category": "math.NA"}
{"title": "Prediction of linear fractional stable motions using codifference", "abstract": "The linear fractional stable motion (LFSM) extends the fractional Brownian\nmotion (fBm) by considering $\\alpha$-stable increments. We propose a method to\nforecast future increments of the LFSM from past discrete-time observations,\nusing the conditional expectation when $\\alpha>1$ or a semimetric projection\notherwise. It relies on the codifference, which describes the serial dependence\nof the process, instead of the covariance. Indeed, covariance is commonly used\nfor predicting an fBm but it is infinite when $\\alpha<2$. Some theoretical\nproperties of the method and of its accuracy are studied and both a simulation\nstudy and an application to real data confirm the relevance of the approach.\nThe LFSM-based method outperforms the fBm, when forecasting high-frequency FX\nrates. It also shows a promising performance in the forecast of time series of\nvolatilities, decomposing properly, in the fractal dynamic of rough\nvolatilities, the contribution of the kurtosis of the increments and the\ncontribution of their serial dependence. Moreover, the analysis of hit ratios\nsuggests that, beside independence, persistence, and antipersistence, a fourth\nregime of serial dependence exists for fractional processes, characterized by a\nselective memory controlled by a few large increments.", "published": "2025-07-21 09:50:00", "link": "http://arxiv.org/abs/2507.15437v1", "categories": ["stat.ME", "q-fin.ST", "stat.AP", "60G18, 60G22, 60G25, 60G35, 60G52, 62M20"], "primary_category": "stat.ME"}
{"title": "Geometric design of the tangent term in landing algorithms for orthogonality constraints", "abstract": "We propose a family a metrics over the set of full-rank $n\\times p$ real\nmatrices, and apply them to the landing framework for optimization under\northogonality constraints. The family of metrics we propose is a natural\nextension of the $\\beta$-metric, defined on the Stiefel manifold.", "published": "2025-07-21 14:00:51", "link": "http://arxiv.org/abs/2507.15638v1", "categories": ["math.OC", "stat.ML"], "primary_category": "math.OC"}
{"title": "Multichannel Keyword Spotting for Noisy Conditions", "abstract": "This article presents a method for improving a keyword spotter (KWS)\nalgorithm in noisy environments. Although beamforming (BF) and adaptive noise\ncancellation (ANC) techniques are robust in some conditions, they may degrade\nthe performance of the activation system by distorting or suppressing useful\nsignals. The authors propose a neural network architecture that uses several\ninput channels and an attention mechanism that allows the network to determine\nthe most useful channel or their combination. The improved quality of the\nalgorithm was demonstrated on two datasets: from a laboratory with controlled\nconditions and from smart speakers in natural conditions. The proposed\nalgorithm was compared against several baselines in terms of the quality of\nnoise reduction metrics, KWS metrics, and computing resources in comparison\nwith existing solutions.", "published": "2025-07-21 12:38:54", "link": "http://arxiv.org/abs/2507.15558v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Binaural Signal Matching with Wearable Arrays for Near-Field Sources", "abstract": "Binaural reproduction methods aim to recreate an acoustic scene for a\nlistener over headphones, offering immersive experiences in applications such\nas Virtual Reality (VR) and teleconferencing. Among the existing approaches,\nthe Binaural Signal Matching (BSM) algorithm has demonstrated high quality\nreproduction due to its signal-independent formulation and the flexibility of\nunconstrained array geometry. However, this method assumes far-field sources\nand has not yet been investigated for near-field scenarios. This study\nevaluates the performance of BSM for near-field sources. Analysis of a\nsemi-circular array around a rigid sphere, modeling head-mounted devices, show\nthat far-field BSM performs adequately for sources up to approximately tens of\ncentimeters from the array. However, for sources closer than this range, the\nbinaural error increases significantly. Incorporating a near-field BSM design,\nwhich accounts for the source distance, significantly reduces the error,\nparticularly for these very-close distances, highlighting the benefits of\nnear-field modeling in improving reproduction accuracy.", "published": "2025-07-21 11:36:24", "link": "http://arxiv.org/abs/2507.15517v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Mixture to Beamformed Mixture: Leveraging Beamformed Mixture as Weak-Supervision for Speech Enhancement and Noise-Robust ASR", "abstract": "In multi-channel speech enhancement and robust automatic speech recognition\n(ASR), beamforming can typically improve the signal-to-noise ratio (SNR) of the\ntarget speaker and produce reliable enhancement with little distortion to\ntarget speech. With this observation, we propose to leverage beamformed\nmixture, which has a higher SNR of the target speaker than the input mixture,\nas a weak supervision to train deep neural networks (DNNs) to enhance the input\nmixture. This way, we can train enhancement models using pairs of real-recorded\nmixture and its beamformed mixture, and potentially realize better\ngeneralization to real mixtures, compared with only training the models on\nsimulated mixtures, which usually mismatch real mixtures. Evaluation results on\nthe real-recorded CHiME-4 dataset show the effectiveness of the proposed\nalgorithm.", "published": "2025-07-21 04:19:34", "link": "http://arxiv.org/abs/2507.15229v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "EchoVoices: Preserving Generational Voices and Memories for Seniors and Children", "abstract": "Recent breakthroughs in intelligent speech and digital human technologies\nhave primarily targeted mainstream adult users, often overlooking the distinct\nvocal patterns and interaction styles of seniors and children. These\ndemographics possess distinct vocal characteristics, linguistic styles, and\ninteraction patterns that challenge conventional ASR, TTS, and LLM systems. To\naddress this, we introduce EchoVoices, an end-to-end digital human pipeline\ndedicated to creating persistent digital personas for seniors and children,\nensuring their voices and memories are preserved for future generations. Our\nsystem integrates three core innovations: a k-NN-enhanced Whisper model for\nrobust speech recognition of atypical speech; an age-adaptive VITS model for\nhigh-fidelity, speaker-aware speech synthesis; and an LLM-driven agent that\nautomatically generates persona cards and leverages a RAG-based memory system\nfor conversational consistency. Our experiments, conducted on the SeniorTalk\nand ChildMandarin datasets, demonstrate significant improvements in recognition\naccuracy, synthesis quality, and speaker similarity. EchoVoices provides a\ncomprehensive framework for preserving generational voices, offering a new\nmeans of intergenerational connection and the creation of lasting digital\nlegacies.", "published": "2025-07-21 03:47:45", "link": "http://arxiv.org/abs/2507.15221v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sum-Rate Maximization for Movable-Antenna Array Enhanced Downlink NOMA Systems", "abstract": "Movable antenna (MA) systems have recently attracted significant attention in\nthe field of wireless communications owing to their exceptional capability to\nproactively reconfigure wireless channels via flexible antenna movements. In\nthis paper, we investigate the resource allocation design for an MA\narray-enhanced downlink non-orthogonal multiple access (NOMA) system, where a\nbase station deploys multiple MAs to serve multiple single-antenna users. Our\ngoal is to maximize the sum rate of all users by jointly optimizing the\ntransmit beamforming, positions of MAs, successive interference cancellation\n(SIC) decoding order, and users' corresponding decoding indicator matrix, while\nadhering to constraints on the maximum transmit power and finite MA moving\nregion. The formulated problem is inherently highly non-convex, rendering it\nchallenging to acquire a globally optimal solution. As a compromise, we propose\na low-complexity two-stage optimization algorithm to obtain an effective\nsuboptimal solution. Specifically, in stage one, the SIC decoding order is\nfirst determined by solving a channel gain maximization problem. Then, in stage\ntwo, with the given SIC decoding order, the beamforming vectors, MA positions,\nand users' decoding indicator matrix are iteratively optimized by capitalizing\non alternating optimization, successive convex approximation (SCA), and genetic\nalgorithm (GA). Simulation results unveil that the sum-rate performance of the\nproposed MA-enabled downlink NOMA system significantly outperforms that of\nconventional fixed-position antenna (FPA) systems. Moreover, the results also\nshow that the antenna position optimization in the proposed algorithm can\nfurther enhance the advantages of NOMA over space division multiple access\n(SDMA).", "published": "2025-07-21 12:34:30", "link": "http://arxiv.org/abs/2507.15555v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "On the Distribution of a Two-Dimensional Random Walk with Restricted Angles", "abstract": "In this paper, we derive the distribution of a two-dimensional (complex)\nrandom walk in which the angle of each step is restricted to a subset of the\ncircle. This setting appears in various domains, such as in over-the-air\ncomputation in signal processing. In particular, we derive the exact joint and\nmarginal distributions for two steps, numerical solutions for a general number\nof steps, and approximations for a large number of steps. Furthermore, we\nprovide an exact characterization of the support for an arbitrary number of\nsteps. The results in this work provide a reference for future work involving\nsuch problems.", "published": "2025-07-21 10:27:49", "link": "http://arxiv.org/abs/2507.15475v1", "categories": ["eess.SP", "math.PR", "stat.AP"], "primary_category": "eess.SP"}
{"title": "Robust ISAC Transceiver Beamforming Design under Low-Resolution AD/DA Converters", "abstract": "In this letter, we investigate the robust beamforming design for an\nintegrated sensing and communication (ISAC) system featuring low-resolution\ndigital-to-analog converters (DACs) and analog-to-digital converters (ADCs).\nTaking into account quantization noise, we aim at maximizing the radar\nsignal-to-quantization-plus-noise ratio (SQNR) while guaranteeing the minimum\nrequired signal-to-quantization-plus-interference-plus-noise ratio (SQINR) for\ncommunication users. To address this nonconvex design problem, we first examine\na scenario involving a point target and uniform-resolution DACs, where the\nglobally optimal solution is obtained by applying the semidefinite relaxation\n(SDR) technique. For more general scenarios, including those with mixed-DACs\nand/or an extended target, we develop a low-complexity\nmajorization-minimization (MM)-based algorithm to tackle the problem\niteratively. Compared to the non-robust algorithm, the proposed algorithm\ndemonstrates improved detection performance under practical quantization.\nSimulation results confirm the robustness and efficacy of our proposed\nalgorithm in low-resolution quantization scenarios.", "published": "2025-07-21 08:25:52", "link": "http://arxiv.org/abs/2507.15373v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "BEAM-Net: A Deep Learning Framework with Bone Enhancement Attention Mechanism for High Resolution High Frame Rate Ultrasound Beamforming", "abstract": "Pocket-sized, low-cost point-of-care ultrasound (POCUS) devices are\nincreasingly used in musculoskeletal (MSK) applications for structural\nexamination of bone tissue. However, the image quality in MSK ultrasound is\noften limited by speckle noise, low resolution, poor contrast, and anisotropic\nreflections, making bone images difficult to interpret without additional\npost-processing. Typically, medical ultrasound systems use delay and sum\nbeamforming (DASB) for image reconstruction, which is not specifically\noptimized for bone structures. To address these limitations, we propose\nBEAM-Net, a novel end-to-end deep neural network (DNN) that performs\nhigh-frame-rate ultrasound beamforming with integrated bone enhancement, using\nsingle-plane-wave (SPW) radio frequency (RF) data as input. Our approach embeds\na Bone Probability Map (BPM), which acts as an attention mechanism to enforce\nhigher structural similarity around bony regions in the image. The proposed\napproach is the first of its kind to incorporate bone enhancement directly into\nultrasound beamforming using deep learning. BEAM-Net was trained and evaluated\non in-vivo MSK and synthetic RF ultrasound datasets. This paper introduces the\nEdge Preservation Index (EPI) as a new region-focused metric for evaluating\nstructural fidelity in bone-enhanced ultrasound images. The performance of\nBEAM-Net was compared with conventional DASB and existing deep learning\narchitectures using the EPI, Contrast Ratio (CR), Signal-to-Noise ratio (SNR),\nSpeckle Similarity Index (SSI), and Structural Similarity Index (SSIM).\nBEAM-Net showed substantial gains over SPW-DASB, achieving 51.4-51% higher CR\nand 94.2-73.3% higher SNR on in-vivo MSK and synthetic RF datasets. It\noutperformed multiple steered plane wave DASB (MPW-DASB), with 19.8-24.0%\nimprovements in CR and SNR on in-vivo MSK and 2.5-12.8% improvements on\nsynthetic data.", "published": "2025-07-21 07:07:20", "link": "http://arxiv.org/abs/2507.15306v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "A Novel Domain-Aware CNN Architecture for Faster-than-Nyquist Signaling Detection", "abstract": "This paper proposes a convolutional neural network (CNN)-based detector for\nfaster-than-Nyquist (FTN) signaling that employs structured fixed kernel layers\nwith domain-informed masking to mitigate intersymbol interference (ISI). Unlike\nstandard CNNs with sliding kernels, the proposed method utilizes fixed-position\nkernels to directly capture ISI effects at varying distances from the central\nsymbol. A hierarchical filter allocation strategy is also introduced, assigning\nmore filters to earlier layers for strong ISI patterns and fewer to later\nlayers for weaker ones. This design improves detection accuracy while reducing\nredundant operations. Simulation results show that the detector achieves\nnear-optimal bit error rate (BER) performance for $\\tau \\geq 0.7$, closely\nmatching the BCJR algorithm, and offers computational gains of up to $46\\%$ and\n$84\\%$ over M-BCJR for BPSK and QPSK, respectively. Comparative analysis with\nother methods further highlights the efficiency and effectiveness of the\nproposed approach. To the best of our knowledge, this is the first application\nof a fixed-kernel CNN architecture tailored for FTN detection in the\nliterature.", "published": "2025-07-21 06:45:46", "link": "http://arxiv.org/abs/2507.15291v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Hierarchical Budget Policy Optimization for Adaptive Reasoning", "abstract": "Large reasoning models achieve remarkable performance through extensive\nchain-of-thought generation, yet exhibit significant computational inefficiency\nby applying uniform reasoning strategies regardless of problem complexity. We\npresent Hierarchical Budget Policy Optimization (HBPO), a reinforcement\nlearning framework that enables models to learn problem-specific reasoning\ndepths without sacrificing capability. HBPO addresses the fundamental challenge\nof exploration space collapse in efficiency-oriented training, where penalties\non long output length systematically bias models away from necessary long\nreasoning paths. Through hierarchical budget exploration, our approach\npartitions rollout samples into multiple subgroups with distinct token budgets,\naiming to enable efficient resource allocation while preventing degradation of\ncapability. We introduce differentiated reward mechanisms that create\nbudget-aware incentives aligned with the complexity of the problem, allowing\nmodels to discover natural correspondences between task requirements and\ncomputational effort. Extensive experiments demonstrate that HBPO reduces\naverage token usage by up to 60.6% while improving accuracy by 3.14% across\nfour reasoning benchmarks. Unlike existing methods that impose external\nconstraints or rely on discrete mode selection, HBPO exhibits emergent adaptive\nbehavior where models automatically adjust reasoning depth based on problem\ncomplexity. Our results suggest that reasoning efficiency and capability are\nnot inherently conflicting, and can be simultaneously optimized through\nappropriately structured hierarchical training that preserves exploration\ndiversity.", "published": "2025-07-21 17:52:34", "link": "http://arxiv.org/abs/2507.15844v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Supernova: Achieving More with Less in Transformer Architectures", "abstract": "We present Supernova, a 650M-parameter decoder-only transformer that\ndemonstrates how careful architectural design and tokenization innovation can\nachieve the performance of larger models while maintaining computational\nefficiency. Our architecture combines Rotary Positional Embeddings (RoPE),\nGrouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm for\ncomputational efficiency, and SwiGLU activation functions. A critical\ninnovation is our custom 128,000-vocabulary byte-level BPE tokenizer, which\nachieves state-of-the-art compression performance. Through detailed analysis,\nwe show that Supernova achieves 90% of the performance of 1B-parameter models\nwhile using 35% fewer parameters and requiring only 100B training tokens--an\norder of magnitude less than competing models. Our findings challenge the\nprevailing scaling paradigm, demonstrating that architectural efficiency and\ntokenization quality can compensate for reduced parameter counts.", "published": "2025-07-21 16:27:48", "link": "http://arxiv.org/abs/2507.15773v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Gemini 2.5 Pro Capable of Winning Gold at IMO 2025", "abstract": "The International Mathematical Olympiad (IMO) poses uniquely challenging\nproblems requiring deep insight, creativity, and formal reasoning. While Large\nLanguage Models (LLMs) perform well on mathematical benchmarks like AIME, they\nstruggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly\nreleased IMO 2025 problems, avoiding data contamination. Using a\nself-verification pipeline with careful prompt design, 5 (out of 6) problems\nare solved correctly (up to a caveat discussed below). This result underscores\nthe importance of developing optimal strategies to harness the full potential\nof powerful LLMs for complex reasoning tasks.", "published": "2025-07-21 17:59:49", "link": "http://arxiv.org/abs/2507.15855v2", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction", "abstract": "Video Object Segmentation (VOS) is a core task in computer vision, requiring\nmodels to track and segment target objects across video frames. Despite notable\nadvances with recent efforts, current techniques still lag behind human\ncapabilities in handling drastic visual variations, occlusions, and complex\nscene changes. This limitation arises from their reliance on appearance\nmatching, neglecting the human-like conceptual understanding of objects that\nenables robust identification across temporal dynamics. Motivated by this gap,\nwe propose Segment Concept (SeC), a concept-driven segmentation framework that\nshifts from conventional feature matching to the progressive construction and\nutilization of high-level, object-centric representations. SeC employs Large\nVision-Language Models (LVLMs) to integrate visual cues across diverse frames,\nconstructing robust conceptual priors. During inference, SeC forms a\ncomprehensive semantic representation of the target based on processed frames,\nrealizing robust segmentation of follow-up frames. Furthermore, SeC adaptively\nbalances LVLM-based semantic reasoning with enhanced feature matching,\ndynamically adjusting computational efforts based on scene complexity. To\nrigorously assess VOS methods in scenarios demanding high-level conceptual\nreasoning and robust semantic understanding, we introduce the Semantic Complex\nScenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160\nmanually annotated multi-scenario videos designed to challenge models with\nsubstantial appearance variations and dynamic scene transformations. In\nparticular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,\nestablishing a new state-of-the-art in concept-aware video object segmentation.", "published": "2025-07-21 17:59:02", "link": "http://arxiv.org/abs/2507.15852v2", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding", "abstract": "Graphical User Interface (GUI) grounding maps natural language instructions\nto precise interface locations for autonomous interaction. Current\nreinforcement learning approaches use binary rewards that treat elements as\nhit-or-miss targets, creating sparse signals that ignore the continuous nature\nof spatial interactions. Motivated by human clicking behavior that naturally\nforms Gaussian distributions centered on target elements, we introduce GUI\nGaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that\nmodels GUI elements as continuous Gaussian distributions across the interface\nplane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point\nrewards model precise localization through exponentially decaying distributions\ncentered on element centroids, while coverage rewards assess spatial alignment\nby measuring the overlap between predicted Gaussian distributions and target\nregions. To handle diverse element scales, we develop an adaptive variance\nmechanism that calibrates reward distributions based on element dimensions.\nThis framework transforms GUI grounding from sparse binary classification to\ndense continuous optimization, where Gaussian distributions generate rich\ngradient signals that guide models toward optimal interaction positions.\nExtensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro\nbenchmarks demonstrate that GUI-G$^2$, substantially outperforms\nstate-of-the-art method UI-TARS-72B, with the most significant improvement of\n24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides\nsuperior robustness to interface variations and enhanced generalization to\nunseen layouts, establishing a new paradigm for spatial reasoning in GUI\ninteraction tasks.", "published": "2025-07-21 17:53:42", "link": "http://arxiv.org/abs/2507.15846v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.HC"], "primary_category": "cs.LG"}
{"title": "Romance, Relief, and Regret: Teen Narratives of Chatbot Overreliance", "abstract": "As Generative Artificial Intelligence (GenAI) driven chatbots like\nCharacter.AI become embedded in adolescent life, they raise concerns about\nemotional dependence and digital overreliance. While studies have investigated\nthe overreliance of adults on these chatbots, they have not investigated teens'\ninteractions with chatbots with customizable personas. We analyzed 318 Reddit\nposts made by users self-reported as 13-17 years old on the Character.AI\nsubreddit to understand patterns of overreliance. We found teens commonly begin\nusing chatbots for emotional support or creative expression, but many develop\nstrong attachments that interfere with offline relationships and daily\nroutines. Their posts revealed recurring signs of psychological distress,\ncycles of relapse, and difficulty disengaging. Teens reported that their\noverreliance often ended when they reflect on the harm, return to in-person\nsocial settings, or become frustrated by platform restrictions. Based on the\nimplications of our findings, we provide recommendations for future chatbot\ndesign so they can promote self-awareness, support real-world engagement, and\ninvolve teens in developing safer digital tools.", "published": "2025-07-21 16:39:33", "link": "http://arxiv.org/abs/2507.15783v2", "categories": ["cs.HC", "cs.AI", "cs.CY"], "primary_category": "cs.HC"}
{"title": "GR-3 Technical Report", "abstract": "We report our recent progress towards building generalist robot policies, the\ndevelopment of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.\nIt showcases exceptional capabilities in generalizing to novel objects,\nenvironments, and instructions involving abstract concepts. Furthermore, it can\nbe efficiently fine-tuned with minimal human trajectory data, enabling rapid\nand cost-effective adaptation to new settings. GR-3 also excels in handling\nlong-horizon and dexterous tasks, including those requiring bi-manual\nmanipulation and mobile movement, showcasing robust and reliable performance.\nThese capabilities are achieved through a multi-faceted training recipe that\nincludes co-training with web-scale vision-language data, efficient fine-tuning\nfrom human trajectory data collected via VR devices, and effective imitation\nlearning with robot trajectory data. In addition, we introduce ByteMini, a\nversatile bi-manual mobile robot designed with exceptional flexibility and\nreliability, capable of accomplishing a wide range of tasks when integrated\nwith GR-3. Through extensive real-world experiments, we show GR-3 surpasses the\nstate-of-the-art baseline method, $\\pi_0$, on a wide variety of challenging\ntasks. We hope GR-3 can serve as a step towards building generalist robots\ncapable of assisting humans in daily life.", "published": "2025-07-21 10:54:13", "link": "http://arxiv.org/abs/2507.15493v2", "categories": ["cs.RO", "cs.AI", "cs.CV"], "primary_category": "cs.RO"}
{"title": "EndoControlMag: Robust Endoscopic Vascular Motion Magnification with Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Contro", "abstract": "Visualizing subtle vascular motions in endoscopic surgery is crucial for\nsurgical precision and decision-making, yet remains challenging due to the\ncomplex and dynamic nature of surgical scenes. To address this, we introduce\nEndoControlMag, a training-free, Lagrangian-based framework with\nmask-conditioned vascular motion magnification tailored to endoscopic\nenvironments. Our approach features two key modules: a Periodic Reference\nResetting (PRR) scheme that divides videos into short overlapping clips with\ndynamically updated reference frames to prevent error accumulation while\nmaintaining temporal coherence, and a Hierarchical Tissue-aware Magnification\n(HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores\nusing a pretrained visual tracking model to maintain accurate localization\ndespite occlusions and view changes. It then applies one of two adaptive\nsoftening strategies to surrounding tissues: motion-based softening that\nmodulates magnification strength proportional to observed tissue displacement,\nor distance-based exponential decay that simulates biomechanical force\nattenuation. This dual-mode approach accommodates diverse surgical\nscenarios-motion-based softening excels with complex tissue deformations while\ndistance-based softening provides stability during unreliable optical flow\nconditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four\ndifferent surgery types and various challenging scenarios, including\nocclusions, instrument disturbance, view changes, and vessel deformations.\nQuantitative metrics, visual assessments, and expert surgeon evaluations\ndemonstrate that EndoControlMag significantly outperforms existing methods in\nboth magnification accuracy and visual quality while maintaining robustness\nacross challenging surgical conditions. The code, dataset, and video results\nare available at https://szupc.github.io/EndoControlMag/.", "published": "2025-07-21 06:47:44", "link": "http://arxiv.org/abs/2507.15292v2", "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Visual-Language Model Knowledge Distillation Method for Image Quality Assessment", "abstract": "Image Quality Assessment (IQA) is a core task in computer vision. Multimodal\nmethods based on vision-language models, such as CLIP, have demonstrated\nexceptional generalization capabilities in IQA tasks. To address the issues of\nexcessive parameter burden and insufficient ability to identify local distorted\nfeatures in CLIP for IQA, this study proposes a visual-language model knowledge\ndistillation method aimed at guiding the training of models with architectural\nadvantages using CLIP's IQA knowledge. First, quality-graded prompt templates\nwere designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned\nto enhance its capabilities in IQA tasks. Finally, a modality-adaptive\nknowledge distillation strategy is proposed to achieve guidance from the CLIP\nteacher model to the student model. Our experiments were conducted on multiple\nIQA datasets, and the results show that the proposed method significantly\nreduces model complexity while outperforming existing IQA methods,\ndemonstrating strong potential for practical deployment.", "published": "2025-07-21 14:44:46", "link": "http://arxiv.org/abs/2507.15680v2", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "DeSamba: Decoupled Spectral Adaptive Framework for 3D Multi-Sequence MRI Lesion Classification", "abstract": "Magnetic Resonance Imaging (MRI) sequences provide rich spatial and frequency\ndomain information, which is crucial for accurate lesion classification in\nmedical imaging. However, effectively integrating multi-sequence MRI data for\nrobust 3D lesion classification remains a challenge. In this paper, we propose\nDeSamba (Decoupled Spectral Adaptive Network and Mamba-Based Model), a novel\nframework designed to extract decoupled representations and adaptively fuse\nspatial and spectral features for lesion classification. DeSamba introduces a\nDecoupled Representation Learning Module (DRLM) that decouples features from\ndifferent MRI sequences through self-reconstruction and cross-reconstruction,\nand a Spectral Adaptive Modulation Block (SAMB) within the proposed SAMNet,\nenabling dynamic fusion of spectral and spatial information based on lesion\ncharacteristics. We evaluate DeSamba on two clinically relevant 3D datasets. On\na six-class spinal metastasis dataset (n=1,448), DeSamba achieves 62.10% Top-1\naccuracy, 63.62% F1-score, 87.71% AUC, and 93.55% Top-3 accuracy on an external\nvalidation set (n=372), outperforming all state-of-the-art (SOTA) baselines. On\na spondylitis dataset (n=251) involving a challenging binary classification\ntask, DeSamba achieves 70.00%/64.52% accuracy and 74.75/73.88 AUC on internal\nand external validation sets, respectively. Ablation studies demonstrate that\nboth DRLM and SAMB significantly contribute to overall performance, with over\n10% relative improvement compared to the baseline. Our results highlight the\npotential of DeSamba as a generalizable and effective solution for 3D lesion\nclassification in multi-sequence medical imaging.", "published": "2025-07-21 10:42:21", "link": "http://arxiv.org/abs/2507.15487v2", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "FedMultiEmo: Real-Time Emotion Recognition via Multimodal Federated Learning", "abstract": "In-vehicle emotion recognition underpins adaptive driver-assistance systems\nand, ultimately, occupant safety. However, practical deployment is hindered by\n(i) modality fragility - poor lighting and occlusions degrade vision-based\nmethods; (ii) physiological variability - heart-rate and skin-conductance\npatterns differ across individuals; and (iii) privacy risk - centralized\ntraining requires transmission of sensitive data. To address these challenges,\nwe present FedMultiEmo, a privacy-preserving framework that fuses two\ncomplementary modalities at the decision level: visual features extracted by a\nConvolutional Neural Network from facial images, and physiological cues (heart\nrate, electrodermal activity, and skin temperature) classified by a Random\nForest. FedMultiEmo builds on three key elements: (1) a multimodal federated\nlearning pipeline with majority-vote fusion, (2) an end-to-end edge-to-cloud\nprototype on Raspberry Pi clients and a Flower server, and (3) a personalized\nFederated Averaging scheme that weights client updates by local data volume.\nEvaluated on FER2013 and a custom physiological dataset, the federated\nConvolutional Neural Network attains 77% accuracy, the Random Forest 74%, and\ntheir fusion 87%, matching a centralized baseline while keeping all raw data\nlocal. The developed system converges in 18 rounds, with an average round time\nof 120 seconds and a per-client memory footprint below 200 MB. These results\nindicate that FedMultiEmo offers a practical approach to real-time,\nprivacy-aware emotion recognition in automotive settings.", "published": "2025-07-21 10:21:48", "link": "http://arxiv.org/abs/2507.15470v2", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Privacy-Preserving Multimodal News Recommendation through Federated Learning", "abstract": "Personalized News Recommendation systems (PNR) have emerged as a solution to\ninformation overload by predicting and suggesting news items tailored to\nindividual user interests. However, traditional PNR systems face several\nchallenges, including an overreliance on textual content, common neglect of\nshort-term user interests, and significant privacy concerns due to centralized\ndata storage. This paper addresses these issues by introducing a novel\nmultimodal federated learning-based approach for news recommendation. First, it\nintegrates both textual and visual features of news items using a multimodal\nmodel, enabling a more comprehensive representation of content. Second, it\nemploys a time-aware model that balances users' long-term and short-term\ninterests through multi-head self-attention networks, improving recommendation\naccuracy. Finally, to enhance privacy, a federated learning framework is\nimplemented, enabling collaborative model training without sharing user data.\nThe framework divides the recommendation model into a large server-maintained\nnews model and a lightweight user model shared between the server and clients.\nThe client requests news representations (vectors) and a user model from the\ncentral server, then computes gradients with user local data, and finally sends\ntheir locally computed gradients to the server for aggregation. The central\nserver aggregates gradients to update the global user model and news model. The\nupdated news model is further used to infer news representation by the server.\nTo further safeguard user privacy, a secure aggregation algorithm based on\nShamir's secret sharing is employed. Experiments on a real-world news dataset\ndemonstrate strong performance compared to existing systems, representing a\nsignificant advancement in privacy-preserving personalized news recommendation.", "published": "2025-07-21 10:14:00", "link": "http://arxiv.org/abs/2507.15460v2", "categories": ["cs.SI", "cs.LG"], "primary_category": "cs.SI"}
{"title": "On exploration of an interior mirror descent flow for stochastic nonconvex constrained problem", "abstract": "We study a nonsmooth nonconvex optimization problem defined over nonconvex\nconstraints, where the feasible set is given by the intersection of the closure\nof an open set and a smooth manifold. By endowing the open set with a\nRiemannian metric induced by a barrier function, we obtain a Riemannian\nsubgradient flow formulated as a differential inclusion, which remains strictly\nwithin the interior of the feasible set. This continuous dynamical system\nunifies two classes of iterative optimization methods, namely the Hessian\nbarrier method and mirror descent scheme, by revealing that these methods can\nbe interpreted as discrete approximations of the continuous flow. We explore\nthe long-term behavior of the trajectories generated by this dynamical system\nand show that the existing deficient convergence properties of the Hessian\nbarrier and mirror descent scheme can be unifily and more insightfully\ninterpreted through these of the continuous trajectory. For instance, the\nnotorious spurious stationary points \\cite{chen2024spurious} observed in\nHessian barrier method and mirror descent scheme are interpreted as stable\nequilibria of the dynamical system that do not correspond to real stationary\npoints of the original optimization problem. We provide two sufficient\ncondition such that these spurious stationary points can be avoided if the\nstrict complementarity conditions holds. In the absence of these regularity\ncondition, we propose a random perturbation strategy that ensures the\ntrajectory converges (subsequentially) to an approximate stationary point.\nBuilding on these insights, we introduce two iterative Riemannian subgradient\nmethods, form of interior point methods, that generalizes the existing Hessian\nbarrier method and mirror descent scheme for solving nonsmooth nonconvex\noptimization problems.", "published": "2025-07-21 05:58:52", "link": "http://arxiv.org/abs/2507.15264v2", "categories": ["math.OC", "cs.LG"], "primary_category": "math.OC"}
{"title": "Feature Construction Using Network Control Theory and Rank Encoding for Graph Machine Learning", "abstract": "In this article, we utilize the concept of average controllability in graphs,\nalong with a novel rank encoding method, to enhance the performance of Graph\nNeural Networks (GNNs) in social network classification tasks. GNNs have proven\nhighly effective in various network-based learning applications and require\nsome form of node features to function. However, their performance is heavily\ninfluenced by the expressiveness of these features. In social networks, node\nfeatures are often unavailable due to privacy constraints or the absence of\ninherent attributes, making it challenging for GNNs to achieve optimal\nperformance. To address this limitation, we propose two strategies for\nconstructing expressive node features. First, we introduce average\ncontrollability along with other centrality metrics (denoted as NCT-EFA) as\nnode-level metrics that capture critical aspects of network topology. Building\non this, we develop a rank encoding method that transforms average\ncontrollability or any other graph-theoretic metric into a fixed-dimensional\nfeature space, thereby improving feature representation. We conduct extensive\nnumerical evaluations using six benchmark GNN models across four social network\ndatasets to compare different node feature construction methods. Our results\ndemonstrate that incorporating average controllability into the feature space\nsignificantly improves GNN performance. Moreover, the proposed rank encoding\nmethod outperforms traditional one-hot degree encoding, improving the ROC AUC\nfrom 68.7% to 73.9% using GraphSAGE on the GitHub Stargazers dataset,\nunderscoring its effectiveness in generating expressive and efficient node\nrepresentations.", "published": "2025-07-21 02:45:55", "link": "http://arxiv.org/abs/2507.15195v2", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Exponential Runge-Kutta Galerkin finite element method for a reaction-diffusion system with nonsmooth initial data", "abstract": "This study presents a numerical analysis of the Field-Noyes\nreaction-diffusion model with nonsmooth initial data, employing a linear\nGalerkin finite element method for spatial discretization and a second-order\nexponential Runge-Kutta scheme for temporal integration. The initial data are\nassumed to reside in the fractional Sobolev space H^gamma with 0 < gamma < 2,\nwhere classical regularity conditions are violated, necessitating specialized\nerror analysis. By integrating semigroup techniques and fractional Sobolev\nspace theory, sharp fully discrete error estimates are derived in both L2 and\nH1 norms. This demonstrates that the convergence order adapts to the smoothness\nof initial data, a key advancement over traditional approaches that assume\nhigher regularity. Numerical examples are provided to support the theoretical\nanalysis.", "published": "2025-07-21 08:01:07", "link": "http://arxiv.org/abs/2507.15345v2", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Efficient Compositional Multi-tasking for On-device Large Language Models", "abstract": "Adapter parameters provide a mechanism to modify the behavior of machine\nlearning models and have gained significant popularity in the context of large\nlanguage models (LLMs) and generative AI. These parameters can be merged to\nsupport multiple tasks via a process known as task merging. However, prior work\non merging in LLMs, particularly in natural language processing, has been\nlimited to scenarios where each test example addresses only a single task. In\nthis paper, we focus on on-device settings and study the problem of text-based\ncompositional multi-tasking, where each test example involves the simultaneous\nexecution of multiple tasks. For instance, generating a translated summary of a\nlong text requires solving both translation and summarization tasks\nconcurrently. To facilitate research in this setting, we propose a benchmark\ncomprising four practically relevant compositional tasks. We also present an\nefficient method (Learnable Calibration) tailored for on-device applications,\nwhere computational resources are limited, emphasizing the need for solutions\nthat are both resource-efficient and high-performing. Our contributions lay the\ngroundwork for advancing the capabilities of LLMs in real-world multi-tasking\nscenarios, expanding their applicability to complex, resource-constrained use\ncases.", "published": "2025-07-21 21:39:23", "link": "http://arxiv.org/abs/2507.16083v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Prompt Makes the Person(a): A Systematic Evaluation of Sociodemographic Persona Prompting for Large Language Models", "abstract": "Persona prompting is increasingly used in large language models (LLMs) to\nsimulate views of various sociodemographic groups. However, how a persona\nprompt is formulated can significantly affect outcomes, raising concerns about\nthe fidelity of such simulations. Using five open-source LLMs, we\nsystematically examine how different persona prompt strategies, specifically\nrole adoption formats and demographic priming strategies, influence LLM\nsimulations across 15 intersectional demographic groups in both open- and\nclosed-ended tasks. Our findings show that LLMs struggle to simulate\nmarginalized groups, particularly nonbinary, Hispanic, and Middle Eastern\nidentities, but that the choice of demographic priming and role adoption\nstrategy significantly impacts their portrayal. Specifically, we find that\nprompting in an interview-style format and name-based priming can help reduce\nstereotyping and improve alignment. Surprisingly, smaller models like OLMo-2-7B\noutperform larger ones such as Llama-3.3-70B. Our findings offer actionable\nguidance for designing sociodemographic persona prompts in LLM-based simulation\nstudies.", "published": "2025-07-21 21:23:29", "link": "http://arxiv.org/abs/2507.16076v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Researcher with Test-Time Diffusion", "abstract": "Deep research agents, powered by Large Language Models (LLMs), are rapidly\nadvancing; yet, their performance often plateaus when generating complex,\nlong-form research reports using generic test-time scaling algorithms. Drawing\ninspiration from the iterative nature of human research, which involves cycles\nof searching, reasoning, and revision, we propose the Test-Time Diffusion Deep\nResearcher (TTD-DR). This novel framework conceptualizes research report\ngeneration as a diffusion process. TTD-DR initiates this process with a\npreliminary draft, an updatable skeleton that serves as an evolving foundation\nto guide the research direction. The draft is then iteratively refined through\na \"denoising\" process, which is dynamically informed by a retrieval mechanism\nthat incorporates external information at each step. The core process is\nfurther enhanced by a self-evolutionary algorithm applied to each component of\nthe agentic workflow, ensuring the generation of high-quality context for the\ndiffusion process. This draft-centric design makes the report writing process\nmore timely and coherent while reducing information loss during the iterative\nsearch process. We demonstrate that our TTD-DR achieves state-of-the-art\nresults on a wide array of benchmarks that require intensive search and\nmulti-hop reasoning, significantly outperforming existing deep research agents.", "published": "2025-07-21 21:23:21", "link": "http://arxiv.org/abs/2507.16075v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AutoMeet: a proof-of-concept study of genAI to automate meetings in automotive engineering", "abstract": "In large organisations, knowledge is mainly shared in meetings, which takes\nup significant amounts of work time. Additionally, frequent in-person meetings\nproduce inconsistent documentation -- official minutes, personal notes,\npresentations may or may not exist. Shared information therefore becomes hard\nto retrieve outside of the meeting, necessitating lengthy updates and\nhigh-frequency meeting schedules.\n  Generative Artificial Intelligence (genAI) models like Large Language Models\n(LLMs) exhibit an impressive performance on spoken and written language\nprocessing. This motivates a practical usage of genAI for knowledge management\nin engineering departments: using genAI for transcribing meetings and\nintegrating heterogeneous additional information sources into an easily usable\nformat for ad-hoc searches.\n  We implement an end-to-end pipeline to automate the entire meeting\ndocumentation workflow in a proof-of-concept state: meetings are recorded and\nminutes are created by genAI. These are further made easily searchable through\na chatbot interface. The core of our work is to test this genAI-based software\ntooling in a real-world engineering department and collect extensive survey\ndata on both ethical and technical aspects. Direct feedback from this\nreal-world setup points out both opportunities and risks: a) users agree that\nthe effort for meetings could be significantly reduced with the help of genAI\nmodels, b) technical aspects are largely solved already, c) organizational\naspects are crucial for a successful ethical usage of such a system.", "published": "2025-07-21 20:44:53", "link": "http://arxiv.org/abs/2507.16054v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "mRAKL: Multilingual Retrieval-Augmented Knowledge Graph Construction for Low-Resourced Languages", "abstract": "Knowledge Graphs represent real-world entities and the relationships between\nthem. Multilingual Knowledge Graph Construction (mKGC) refers to the task of\nautomatically constructing or predicting missing entities and links for\nknowledge graphs in a multilingual setting. In this work, we reformulate the\nmKGC task as a Question Answering (QA) task and introduce mRAKL: a\nRetrieval-Augmented Generation (RAG) based system to perform mKGC. We achieve\nthis by using the head entity and linking relation in a question, and having\nour model predict the tail entity as an answer. Our experiments focus primarily\non two low-resourced languages: Tigrinya and Amharic. We experiment with using\nhigher-resourced languages Arabic and English for cross-lingual transfer. With\na BM25 retriever, we find that the RAG-based approach improves performance over\na no-context setting. Further, our ablation studies show that with an idealized\nretrieval system, mRAKL improves accuracy by 4.92 and 8.79 percentage points\nfor Tigrinya and Amharic, respectively.", "published": "2025-07-21 19:11:31", "link": "http://arxiv.org/abs/2507.16011v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Help Me Write a Story: Evaluating LLMs' Ability to Generate Writing Feedback", "abstract": "Can LLMs provide support to creative writers by giving meaningful writing\nfeedback? In this paper, we explore the challenges and limitations of\nmodel-generated writing feedback by defining a new task, dataset, and\nevaluation frameworks. To study model performance in a controlled manner, we\npresent a novel test set of 1,300 stories that we corrupted to intentionally\nintroduce writing issues. We study the performance of commonly used LLMs in\nthis task with both automatic and human evaluation metrics. Our analysis shows\nthat current models have strong out-of-the-box behavior in many respects --\nproviding specific and mostly accurate writing feedback. However, models often\nfail to identify the biggest writing issue in the story and to correctly decide\nwhen to offer critical vs. positive feedback.", "published": "2025-07-21 18:56:50", "link": "http://arxiv.org/abs/2507.16007v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning without training: The implicit dynamics of in-context learning", "abstract": "One of the most striking features of Large Language Models (LLM) is their\nability to learn in context. Namely at inference time an LLM is able to learn\nnew patterns without any additional weight update when these patterns are\npresented in the form of examples in the prompt, even if these patterns were\nnot seen during training. The mechanisms through which this can happen are\nstill largely unknown. In this work, we show that the stacking of a\nself-attention layer with an MLP, allows the transformer block to implicitly\nmodify the weights of the MLP layer according to the context. We argue through\ntheory and experimentation that this simple mechanism may be the reason why\nLLMs can learn in context and not only during training. Specifically, we show\nunder mild simplifying assumptions how a transformer block implicitly\ntransforms a context into a low-rank weight-update of the MLP layer.", "published": "2025-07-21 18:44:35", "link": "http://arxiv.org/abs/2507.16003v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Hindi NER in Low Context: A Comparative study of Transformer-based models with vs. without Retrieval Augmentation", "abstract": "One major challenge in natural language processing is named entity\nrecognition (NER), which identifies and categorises named entities in textual\ninput. In order to improve NER, this study investigates a Hindi NER technique\nthat makes use of Hindi-specific pretrained encoders (MuRIL and XLM-R) and\nGenerative Models ( Llama-2-7B-chat-hf (Llama2-7B), Llama-2-70B-chat-hf\n(Llama2-70B), Llama-3-70B-Instruct (Llama3-70B) and GPT3.5-turbo), and augments\nthe data with retrieved data from external relevant contexts, notably from\nWikipedia. We have fine-tuned MuRIL, XLM-R and Llama2-7B with and without RA.\nHowever, Llama2-70B, lama3-70B and GPT3.5-turbo are utilised for few-shot NER\ngeneration. Our investigation shows that the mentioned language models (LMs)\nwith Retrieval Augmentation (RA) outperform baseline methods that don't\nincorporate RA in most cases. The macro F1 scores for MuRIL and XLM-R are 0.69\nand 0.495, respectively, without RA and increase to 0.70 and 0.71,\nrespectively, in the presence of RA. Fine-tuned Llama2-7B outperforms Llama2-7B\nby a significant margin. On the other hand the generative models which are not\nfine-tuned also perform better with augmented data. GPT3.5-turbo adopted RA\nwell; however, Llama2-70B and llama3-70B did not adopt RA with our retrieval\ncontext. The findings show that RA significantly improves performance,\nespecially for low-context data. This study adds significant knowledge about\nhow best to use data augmentation methods and pretrained models to enhance NER\nperformance, particularly in languages with limited resources.", "published": "2025-07-21 18:41:58", "link": "http://arxiv.org/abs/2507.16002v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Expert-Guided LLM Reasoning for Battery Discovery: From AI-Driven Hypothesis to Synthesis and Characterization", "abstract": "Large language models (LLMs) leverage chain-of-thought (CoT) techniques to\ntackle complex problems, representing a transformative breakthrough in\nartificial intelligence (AI). However, their reasoning capabilities have\nprimarily been demonstrated in solving math and coding problems, leaving their\npotential for domain-specific applications-such as battery discovery-largely\nunexplored. Inspired by the idea that reasoning mirrors a form of guided\nsearch, we introduce ChatBattery, a novel agentic framework that integrates\ndomain knowledge to steer LLMs toward more effective reasoning in materials\ndesign. Using ChatBattery, we successfully identify, synthesize, and\ncharacterize three novel lithium-ion battery cathode materials, which achieve\npractical capacity improvements of 28.8%, 25.2%, and 18.5%, respectively, over\nthe widely used cathode material, LiNi0.8Mn0.1Co0.1O2 (NMC811). Beyond this\ndiscovery, ChatBattery paves a new path by showing a successful LLM-driven and\nreasoning-based platform for battery materials invention. This complete\nAI-driven cycle-from design to synthesis to characterization-demonstrates the\ntransformative potential of AI-driven reasoning in revolutionizing materials\ndiscovery.", "published": "2025-07-21 23:46:11", "link": "http://arxiv.org/abs/2507.16110v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI"}
{"title": "A Lower Bound for the Number of Linear Regions of Ternary ReLU Regression Neural Networks", "abstract": "With the advancement of deep learning, reducing computational complexity and\nmemory consumption has become a critical challenge, and ternary neural networks\n(NNs) that restrict parameters to $\\{-1, 0, +1\\}$ have attracted attention as a\npromising approach. While ternary NNs demonstrate excellent performance in\npractical applications such as image recognition and natural language\nprocessing, their theoretical understanding remains insufficient. In this\npaper, we theoretically analyze the expressivity of ternary NNs from the\nperspective of the number of linear regions. Specifically, we evaluate the\nnumber of linear regions of ternary regression NNs with Rectified Linear Unit\n(ReLU) for activation functions and prove that the number of linear regions\nincreases polynomially with respect to network width and exponentially with\nrespect to depth, similar to standard NNs. Moreover, we show that it suffices\nto either square the width or double the depth of ternary NNs to achieve a\nlower bound on the maximum number of linear regions comparable to that of\ngeneral ReLU regression NNs. This provides a theoretical explanation, in some\nsense, for the practical success of ternary NNs.", "published": "2025-07-21 21:29:33", "link": "http://arxiv.org/abs/2507.16079v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "AI-driven Orchestration at Scale: Estimating Service Metrics on National-Wide Testbeds", "abstract": "Network Slicing (NS) realization requires AI-native orchestration\narchitectures to efficiently and intelligently handle heterogeneous user\nrequirements. To achieve this, network slicing is evolving towards a more\nuser-centric digital transformation, focusing on architectures that incorporate\nnative intelligence to enable self-managed connectivity in an integrated and\nisolated manner. However, these initiatives face the challenge of validating\ntheir results in production environments, particularly those utilizing\nML-enabled orchestration, as they are often tested in local networks or\nlaboratory simulations. This paper proposes a large-scale validation method\nusing a network slicing prediction model to forecast latency using Deep Neural\nNetworks (DNNs) and basic ML algorithms embedded within an NS architecture,\nevaluated in real large-scale production testbeds. It measures and compares the\nperformance of different DNNs and ML algorithms, considering a distributed\ndatabase application deployed as a network slice over two large-scale\nproduction testbeds. The investigation highlights how AI-based prediction\nmodels can enhance network slicing orchestration architectures and presents a\nseamless, production-ready validation method as an alternative to fully\ncontrolled simulations or laboratory setups.", "published": "2025-07-21 21:24:40", "link": "http://arxiv.org/abs/2507.16077v1", "categories": ["cs.ET", "cs.AI", "cs.LG", "cs.MA", "cs.NI"], "primary_category": "cs.ET"}
{"title": "Compositional Coordination for Multi-Robot Teams with Large Language Models", "abstract": "Multi-robot coordination has traditionally relied on a task-specific and\nexpert-driven pipeline, where natural language mission descriptions are\nmanually translated by domain experts into mathematical formulation, algorithm\ndesign, and executable code. This conventional process is labor-intensive,\ninaccessible to non-experts, and inflexible to changes in mission requirements.\nHere, we propose LAN2CB (Language to Collective Behavior), a novel framework\nthat leverages large language models (LLMs) to streamline and generalize the\nmulti-robot coordination pipeline. LAN2CB directly converts natural language\nmission descriptions into executable Python code for multi-robot systems\nthrough two key components: (1) Mission Decomposition for Task Representation,\nwhich parses the mission into a task graph with dependencies, and (2) Code\nGeneration, which uses the task graph and a structured knowledge base to\ngenerate deployable robot control code. We further introduce a dataset of\nnatural language mission specifications to support development and\nbenchmarking. Experimental results in both simulation and real-world settings\nshow that LAN2CB enables effective and flexible multi-robot coordination from\nnatural language, significantly reducing the need for manual engineering while\nsupporting generalization across mission types. Website:\nhttps://sites.google.com/view/lan2cb.", "published": "2025-07-21 21:09:15", "link": "http://arxiv.org/abs/2507.16068v1", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.RO"}
{"title": "Stop-band Energy Constraint for Orthogonal Tunable Wavelet Units in Convolutional Neural Networks for Computer Vision problems", "abstract": "This work introduces a stop-band energy constraint for filters in orthogonal\ntunable wavelet units with a lattice structure, aimed at improving image\nclassification and anomaly detection in CNNs, especially on texture-rich\ndatasets. Integrated into ResNet-18, the method enhances convolution, pooling,\nand downsampling operations, yielding accuracy gains of 2.48% on CIFAR-10 and\n13.56% on the Describable Textures dataset. Similar improvements are observed\nin ResNet-34. On the MVTec hazelnut anomaly detection task, the proposed method\nachieves competitive results in both segmentation and detection, outperforming\nexisting approaches.", "published": "2025-07-21 23:57:03", "link": "http://arxiv.org/abs/2507.16114v1", "categories": ["cs.CV", "eess.SP"], "primary_category": "cs.CV"}
{"title": "Improving Personalized Image Generation through Social Context Feedback", "abstract": "Personalized image generation, where reference images of one or more subjects\nare used to generate their image according to a scene description, has gathered\nsignificant interest in the community. However, such generated images suffer\nfrom three major limitations -- complex activities, such as $<$man, pushing,\nmotorcycle$>$ are not generated properly with incorrect human poses, reference\nhuman identities are not preserved, and generated human gaze patterns are\nunnatural/inconsistent with the scene description. In this work, we propose to\novercome these shortcomings through feedback-based fine-tuning of existing\npersonalized generation methods, wherein, state-of-art detectors of pose,\nhuman-object-interaction, human facial recognition and human gaze-point\nestimation are used to refine the diffusion model. We also propose\ntimestep-based inculcation of different feedback modules, depending upon\nwhether the signal is low-level (such as human pose), or high-level (such as\ngaze point). The images generated in this manner show an improvement in the\ngenerated interactions, facial identities and image quality over three\nbenchmark datasets.", "published": "2025-07-21 22:36:30", "link": "http://arxiv.org/abs/2507.16095v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Handcrafted vs. Deep Radiomics vs. Fusion vs. Deep Learning: A Comprehensive Review of Machine Learning -Based Cancer Outcome Prediction in PET and SPECT Imaging", "abstract": "Machine learning (ML), including deep learning (DL) and radiomics-based\nmethods, is increasingly used for cancer outcome prediction with PET and SPECT\nimaging. However, the comparative performance of handcrafted radiomics features\n(HRF), deep radiomics features (DRF), DL models, and hybrid fusion approaches\nremains inconsistent across clinical applications. This systematic review\nanalyzed 226 studies published from 2020 to 2025 that applied ML to PET or\nSPECT imaging for outcome prediction. Each study was evaluated using a 59-item\nframework covering dataset construction, feature extraction, validation\nmethods, interpretability, and risk of bias. We extracted key details including\nmodel type, cancer site, imaging modality, and performance metrics such as\naccuracy and area under the curve (AUC). PET-based studies (95%) generally\noutperformed those using SPECT, likely due to higher spatial resolution and\nsensitivity. DRF models achieved the highest mean accuracy (0.862), while\nfusion models yielded the highest AUC (0.861). ANOVA confirmed significant\ndifferences in performance (accuracy: p=0.0006, AUC: p=0.0027). Common\nlimitations included inadequate handling of class imbalance (59%), missing data\n(29%), and low population diversity (19%). Only 48% of studies adhered to IBSI\nstandards. These findings highlight the need for standardized pipelines,\nimproved data quality, and explainable AI to support clinical integration.", "published": "2025-07-21 21:03:12", "link": "http://arxiv.org/abs/2507.16065v1", "categories": ["physics.med-ph", "cs.CV"], "primary_category": "physics.med-ph"}
{"title": "Disrupting Semantic and Abstract Features for Better Adversarial Transferability", "abstract": "Adversarial examples pose significant threats to deep neural networks (DNNs),\nand their property of transferability in the black-box setting has led to the\nemergence of transfer-based attacks, making it feasible to target real-world\napplications employing DNNs. Among them, feature-level attacks, where\nintermediate features are perturbed based on feature importance weight matrix\ncomputed from transformed images, have gained popularity. In this work, we find\nthat existing feature-level attacks primarily manipulate the semantic\ninformation to derive the weight matrix. Inspired by several works that find\nCNNs tend to focus more on high-frequency components (a.k.a. abstract features,\ne.g., texture, edge, etc.), we validate that transforming images in the\nhigh-frequency space also improves transferability. Based on this finding, we\npropose a balanced approach called Semantic and Abstract FEatures disRuption\n(SAFER). Specifically, SAFER conducts BLOCKMIX on the input image and SELF-MIX\non the frequency spectrum when computing the weight matrix to highlight crucial\nfeatures. By using such a weight matrix, we can direct the attacker to disrupt\nboth semantic and abstract features, leading to improved transferability.\nExtensive experiments on the ImageNet dataset also demonstrate the\neffectiveness of our method in boosting adversarial transferability.", "published": "2025-07-21 20:38:50", "link": "http://arxiv.org/abs/2507.16052v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Discovering and using Spelke segments", "abstract": "Segments in computer vision are often defined by semantic considerations and\nare highly dependent on category-specific conventions. In contrast,\ndevelopmental psychology suggests that humans perceive the world in terms of\nSpelke objects--groupings of physical things that reliably move together when\nacted on by physical forces. Spelke objects thus operate on category-agnostic\ncausal motion relationships which potentially better support tasks like\nmanipulation and planning. In this paper, we first benchmark the Spelke object\nconcept, introducing the SpelkeBench dataset that contains a wide variety of\nwell-defined Spelke segments in natural images. Next, to extract Spelke\nsegments from images algorithmically, we build SpelkeNet, a class of visual\nworld models trained to predict distributions over future motions. SpelkeNet\nsupports estimation of two key concepts for Spelke object discovery: (1) the\nmotion affordance map, identifying regions likely to move under a poke, and (2)\nthe expected-displacement map, capturing how the rest of the scene will move.\nThese concepts are used for \"statistical counterfactual probing\", where diverse\n\"virtual pokes\" are applied on regions of high motion-affordance, and the\nresultant expected displacement maps are used define Spelke segments as\nstatistical aggregates of correlated motion statistics. We find that SpelkeNet\noutperforms supervised baselines like SegmentAnything (SAM) on SpelkeBench.\nFinally, we show that the Spelke concept is practically useful for downstream\napplications, yielding superior performance on the 3DEditBench benchmark for\nphysical object manipulation when used in a variety of off-the-shelf object\nmanipulation models.", "published": "2025-07-21 20:11:57", "link": "http://arxiv.org/abs/2507.16038v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Improved Semantic Segmentation from Ultra-Low-Resolution RGB Images Applied to Privacy-Preserving Object-Goal Navigation", "abstract": "User privacy in mobile robotics has become a critical concern. Existing\nmethods typically prioritize either the performance of downstream robotic tasks\nor privacy protection, with the latter often constraining the effectiveness of\ntask execution. To jointly address both objectives, we study semantic-based\nrobot navigation in an ultra-low-resolution setting to preserve visual privacy.\nA key challenge in such scenarios is recovering semantic segmentation from\nultra-low-resolution RGB images. In this work, we introduce a novel fully\njoint-learning method that integrates an agglomerative feature extractor and a\nsegmentation-aware discriminator to solve ultra-low-resolution semantic\nsegmentation, thereby enabling privacy-preserving, semantic object-goal\nnavigation. Our method outperforms different baselines on ultra-low-resolution\nsemantic segmentation and our improved segmentation results increase the\nsuccess rate of the semantic object-goal navigation in a real-world\nprivacy-constrained scenario.", "published": "2025-07-21 19:53:40", "link": "http://arxiv.org/abs/2507.16034v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Artifacts and Attention Sinks: Structured Approximations for Efficient Vision Transformers", "abstract": "Vision transformers have emerged as a powerful tool across a wide range of\napplications, yet their inner workings remain only partially understood. In\nthis work, we examine the phenomenon of massive tokens - tokens with\nexceptionally high activation norms that act as attention sinks - and artifact\ntokens that emerge as a byproduct during inference. Our analysis reveals that\nthese tokens mutually suppress one another through the attention mechanism,\nplaying a critical role in regulating information flow within the network.\nLeveraging these insights, we introduce Fast Nystr\\\"om Attention (FNA), a\ntraining-free method that approximates self-attention in linear time and space\nby exploiting the structured patterns formed by massive and artifact tokens.\nAdditionally, we propose a masking strategy to mitigate noise from these\ntokens, yielding modest performance gains at virtually no cost. We evaluate our\napproach on popular pretrained vision backbones and demonstrate competitive\nperformance on retrieval, classification, segmentation, and visual question\nanswering (VQA), all while reducing computational overhead.", "published": "2025-07-21 19:29:03", "link": "http://arxiv.org/abs/2507.16018v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Is Tracking really more challenging in First Person Egocentric Vision?", "abstract": "Visual object tracking and segmentation are becoming fundamental tasks for\nunderstanding human activities in egocentric vision. Recent research has\nbenchmarked state-of-the-art methods and concluded that first person egocentric\nvision presents challenges compared to previously studied domains. However,\nthese claims are based on evaluations conducted across significantly different\nscenarios. Many of the challenging characteristics attributed to egocentric\nvision are also present in third person videos of human-object activities. This\nraises a critical question: how much of the observed performance drop stems\nfrom the unique first person viewpoint inherent to egocentric vision versus the\ndomain of human-object activities? To address this question, we introduce a new\nbenchmark study designed to disentangle such factors. Our evaluation strategy\nenables a more precise separation of challenges related to the first person\nperspective from those linked to the broader domain of human-object activity\nunderstanding. By doing so, we provide deeper insights into the true sources of\ndifficulty in egocentric tracking and segmentation, facilitating more targeted\nadvancements on this task.", "published": "2025-07-21 19:25:50", "link": "http://arxiv.org/abs/2507.16015v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "FW-VTON: Flattening-and-Warping for Person-to-Person Virtual Try-on", "abstract": "Traditional virtual try-on methods primarily focus on the garment-to-person\ntry-on task, which requires flat garment representations. In contrast, this\npaper introduces a novel approach to the person-to-person try-on task. Unlike\nthe garment-to-person try-on task, the person-to-person task only involves two\ninput images: one depicting the target person and the other showing the garment\nworn by a different individual. The goal is to generate a realistic combination\nof the target person with the desired garment. To this end, we propose\nFlattening-and-Warping Virtual Try-On (\\textbf{FW-VTON}), a method that\noperates in three stages: (1) extracting the flattened garment image from the\nsource image; (2) warping the garment to align with the target pose; and (3)\nintegrating the warped garment seamlessly onto the target person. To overcome\nthe challenges posed by the lack of high-quality datasets for this task, we\nintroduce a new dataset specifically designed for person-to-person try-on\nscenarios. Experimental evaluations demonstrate that FW-VTON achieves\nstate-of-the-art performance, with superior results in both qualitative and\nquantitative assessments, and also excels in garment extraction subtasks.", "published": "2025-07-21 19:09:28", "link": "http://arxiv.org/abs/2507.16010v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Semantic-Aware Gaussian Process Calibration with Structured Layerwise Kernels for Deep Neural Networks", "abstract": "Calibrating the confidence of neural network classifiers is essential for\nquantifying the reliability of their predictions during inference. However,\nconventional Gaussian Process (GP) calibration methods often fail to capture\nthe internal hierarchical structure of deep neural networks, limiting both\ninterpretability and effectiveness for assessing predictive reliability. We\npropose a Semantic-Aware Layer-wise Gaussian Process (SAL-GP) framework that\nmirrors the layered architecture of the target neural network. Instead of\napplying a single global GP correction, SAL-GP employs a multi-layer GP model,\nwhere each layer's feature representation is mapped to a local calibration\ncorrection. These layerwise GPs are coupled through a structured multi-layer\nkernel, enabling joint marginalization across all layers. This design allows\nSAL-GP to capture both local semantic dependencies and global calibration\ncoherence, while consistently propagating predictive uncertainty through the\nnetwork. The resulting framework enhances interpretability aligned with the\nnetwork architecture and enables principled evaluation of confidence\nconsistency and uncertainty quantification in deep models.", "published": "2025-07-21 18:28:21", "link": "http://arxiv.org/abs/2507.15987v1", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "A Lightweight Face Quality Assessment Framework to Improve Face Verification Performance in Real-Time Screening Applications", "abstract": "Face image quality plays a critical role in determining the accuracy and\nreliability of face verification systems, particularly in real-time screening\napplications such as surveillance, identity verification, and access control.\nLow-quality face images, often caused by factors such as motion blur, poor\nlighting conditions, occlusions, and extreme pose variations, significantly\ndegrade the performance of face recognition models, leading to higher false\nrejection and false acceptance rates. In this work, we propose a lightweight\nyet effective framework for automatic face quality assessment, which aims to\npre-filter low-quality face images before they are passed to the verification\npipeline. Our approach utilises normalised facial landmarks in conjunction with\na Random Forest Regression classifier to assess image quality, achieving an\naccuracy of 96.67\\%. By integrating this quality assessment module into the\nface verification process, we observe a substantial improvement in performance,\nincluding a comfortable 99.7\\% reduction in the false rejection rate and\nenhanced cosine similarity scores when paired with the ArcFace face\nverification model. To validate our approach, we have conducted experiments on\na real-world dataset collected comprising over 600 subjects captured from CCTV\nfootage in unconstrained environments within Dubai Police. Our results\ndemonstrate that the proposed framework effectively mitigates the impact of\npoor-quality face images, outperforming existing face quality assessment\ntechniques while maintaining computational efficiency. Moreover, the framework\nspecifically addresses two critical challenges in real-time screening:\nvariations in face resolution and pose deviations, both of which are prevalent\nin practical surveillance scenarios.", "published": "2025-07-21 18:04:14", "link": "http://arxiv.org/abs/2507.15961v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Quantization-Aware Neuromorphic Architecture for Efficient Skin Disease Classification on Resource-Constrained Devices", "abstract": "Accurate and efficient skin lesion classification on edge devices is critical\nfor accessible dermatological care but remains challenging due to\ncomputational, energy, and privacy constraints. We introduce QANA, a novel\nquantization-aware neuromorphic architecture for incremental skin lesion\nclassification on resource-limited hardware. QANA effectively integrates ghost\nmodules, efficient channel attention, and squeeze-and-excitation blocks for\nrobust feature representation with low-latency and energy-efficient inference.\nIts quantization-aware head and spike-compatible transformations enable\nseamless conversion to spiking neural networks (SNNs) and deployment on\nneuromorphic platforms. Evaluation on the large-scale HAM10000 benchmark and a\nreal-world clinical dataset shows that QANA achieves 91.6\\% Top-1 accuracy and\n82.4\\% macro F1 on HAM10000, and 90.8\\% / 81.7\\% on the clinical dataset,\nsignificantly outperforming state-of-the-art CNN-to-SNN models under fair\ncomparison. Deployed on BrainChip Akida hardware, QANA achieves 1.5\\,ms\ninference latency and 1.7\\,mJ energy per image, reducing inference latency and\nenergy use by over 94.6\\%/98.6\\% compared to GPU-based CNNs surpassing\nstate-of-the-art CNN-to-SNN conversion baselines. These results demonstrate the\neffectiveness of QANA for accurate, real-time, and privacy-sensitive medical\nanalysis in edge environments.", "published": "2025-07-21 18:01:44", "link": "http://arxiv.org/abs/2507.15958v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Scaling Recommender Transformers to One Billion Parameters", "abstract": "While large transformer models have been successfully used in many real-world\napplications such as natural language processing, computer vision, and speech\nprocessing, scaling transformers for recommender systems remains a challenging\nproblem. Recently, Generative Recommenders framework was proposed to scale\nbeyond typical Deep Learning Recommendation Models (DLRMs). Reformulation of\nrecommendation as sequential transduction task led to improvement of scaling\nproperties in terms of compute. Nevertheless, the largest encoder configuration\nreported by the HSTU authors amounts only to ~176 million parameters, which is\nconsiderably smaller than the hundreds of billions or even trillions of\nparameters common in modern language models.\n  In this work, we present a recipe for training large transformer recommenders\nwith up to a billion parameters. We show that autoregressive learning on user\nhistories naturally decomposes into two subtasks, feedback prediction and\nnext-item prediction, and demonstrate that such a decomposition scales\neffectively across a wide range of transformer sizes. Furthermore, we report a\nsuccessful deployment of our proposed architecture on a large-scale music\nplatform serving millions of users. According to our online A/B tests, this new\nmodel increases total listening time by +2.26% and raises the likelihood of\nuser likes by +6.37%, constituting (to our knowledge) the largest improvement\nin recommendation quality reported for any deep learning-based system in the\nplatform's history.", "published": "2025-07-21 18:30:43", "link": "http://arxiv.org/abs/2507.15994v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Byzantine-Resilient Distributed Computation via Task Replication and Local Computations", "abstract": "We study a distributed computation problem in the presence of Byzantine\nworkers where a central node wishes to solve a task that is divided into\nindependent sub-tasks, each of which needs to be solved correctly. The\ndistributed computation is achieved by allocating the sub-task computation\nacross workers with replication, as well as solving a small number of sub-tasks\nlocally, which we wish to minimize due to it being expensive. For a general\nbalanced job allocation, we propose a protocol that successfully solves for all\nsub-tasks using an optimal number of local computations under no communication\nconstraints. Closed-form performance results are presented for cyclic\nallocations. Furthermore, we propose a modification to this protocol to improve\ncommunication efficiency without compromising on the amount of local\ncomputation.", "published": "2025-07-21 19:25:24", "link": "http://arxiv.org/abs/2507.16014v1", "categories": ["cs.IT", "cs.DC", "math.IT"], "primary_category": "cs.IT"}
{"title": "Neural Probabilistic Shaping: Joint Distribution Learning for Optical Fiber Communications", "abstract": "We present an autoregressive end-to-end learning approach for probabilistic\nshaping on nonlinear fiber channels. Our proposed scheme learns the joint\nsymbol distribution and provides a 0.3-bits/2D achievable information rate gain\nover an optimized marginal distribution for dual-polarized 64-QAM transmission\nover a single-span 205 km link.", "published": "2025-07-21 19:21:51", "link": "http://arxiv.org/abs/2507.16012v1", "categories": ["cs.LG", "cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.LG"}
{"title": "Recursive Equations For Imputation Of Missing Not At Random Data With Sparse Pattern Support", "abstract": "A common approach for handling missing values in data analysis pipelines is\nmultiple imputation via software packages such as MICE (Van Buuren and\nGroothuis-Oudshoorn, 2011) and Amelia (Honaker et al., 2011). These packages\ntypically assume the data are missing at random (MAR), and impose parametric or\nsmoothing assumptions upon the imputing distributions in a way that allows\nimputation to proceed even if not all missingness patterns have support in the\ndata. Such assumptions are unrealistic in practice, and induce model\nmisspecification bias on any analysis performed after such imputation.\n  In this paper, we provide a principled alternative. Specifically, we develop\na new characterization for the full data law in graphical models of missing\ndata. This characterization is constructive, is easily adapted for the\ncalculation of imputation distributions for both MAR and MNAR (missing not at\nrandom) mechanisms, and is able to handle lack of support for certain patterns\nof missingness. We use this characterization to develop a new imputation\nalgorithm -- Multivariate Imputation via Supported Pattern Recursion (MISPR) --\nwhich uses Gibbs sampling, by analogy with the Multivariate Imputation with\nChained Equations (MICE) algorithm, but which is consistent under both MAR and\nMNAR settings, and is able to handle missing data patterns with no support\nwithout imposing additional assumptions beyond those already imposed by the\nmissing data model itself.\n  In simulations, we show MISPR obtains comparable results to MICE when data\nare MAR, and superior, less biased results when data are MNAR. Our\ncharacterization and imputation algorithm based on it are a step towards making\nprincipled missing data methods more practical in applied settings, where the\ndata are likely both MNAR and sufficiently high dimensional to yield missing\ndata patterns with no support at available sample sizes.", "published": "2025-07-21 23:18:36", "link": "http://arxiv.org/abs/2507.16107v1", "categories": ["stat.ME", "cs.LG"], "primary_category": "stat.ME"}
{"title": "TorchAO: PyTorch-Native Training-to-Serving Model Optimization", "abstract": "We present TorchAO, a PyTorch-native model optimization framework leveraging\nquantization and sparsity to provide an end-to-end, training-to-serving\nworkflow for AI models. TorchAO supports a variety of popular model\noptimization techniques, including FP8 quantized training, quantization-aware\ntraining (QAT), post-training quantization (PTQ), and 2:4 sparsity, and\nleverages a novel tensor subclass abstraction to represent a variety of\nwidely-used, backend agnostic low precision data types, including INT4, INT8,\nFP8, MXFP4, MXFP6, and MXFP8. TorchAO integrates closely with the broader\necosystem at each step of the model optimization pipeline, from pre-training\n(TorchTitan) to fine-tuning (TorchTune, Axolotl) to serving (HuggingFace, vLLM,\nSGLang, ExecuTorch), connecting an otherwise fragmented space in a single,\nunified workflow. TorchAO has enabled recent launches of the quantized Llama\n3.2 1B/3B and LlamaGuard3-8B models and is open-source at\nhttps://github.com/pytorch/ao/.", "published": "2025-07-21 22:50:12", "link": "http://arxiv.org/abs/2507.16099v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Interpreting CFD Surrogates through Sparse Autoencoders", "abstract": "Learning-based surrogate models have become a practical alternative to\nhigh-fidelity CFD solvers, but their latent representations remain opaque and\nhinder adoption in safety-critical or regulation-bound settings. This work\nintroduces a posthoc interpretability framework for graph-based surrogate\nmodels used in computational fluid dynamics (CFD) by leveraging sparse\nautoencoders (SAEs). By obtaining an overcomplete basis in the node embedding\nspace of a pretrained surrogate, the method extracts a dictionary of\ninterpretable latent features. The approach enables the identification of\nmonosemantic concepts aligned with physical phenomena such as vorticity or flow\nstructures, offering a model-agnostic pathway to enhance explainability and\ntrustworthiness in CFD applications.", "published": "2025-07-21 21:09:45", "link": "http://arxiv.org/abs/2507.16069v1", "categories": ["cs.CE", "cs.LG"], "primary_category": "cs.CE"}
{"title": "Is memory all you need? Data-driven Mori-Zwanzig modeling of Lagrangian particle dynamics in turbulent flows", "abstract": "The dynamics of Lagrangian particles in turbulence play a crucial role in\nmixing, transport, and dispersion processes in complex flows. Their\ntrajectories exhibit highly non-trivial statistical behavior, motivating the\ndevelopment of surrogate models that can reproduce these trajectories without\nincurring the high computational cost of direct numerical simulations of the\nfull Eulerian field. This task is particularly challenging because\nreduced-order models typically lack access to the full set of interactions with\nthe underlying turbulent field. Novel data-driven machine learning techniques\ncan be very powerful in capturing and reproducing complex statistics of the\nreduced-order/surrogate dynamics. In this work, we show how one can learn a\nsurrogate dynamical system that is able to evolve a turbulent Lagrangian\ntrajectory in a way that is point-wise accurate for short-time predictions\n(with respect to Kolmogorov time) and stable and statistically accurate at long\ntimes. This approach is based on the Mori--Zwanzig formalism, which prescribes\na mathematical decomposition of the full dynamical system into resolved\ndynamics that depend on the current state and the past history of a reduced set\nof observables and the unresolved orthogonal dynamics due to unresolved degrees\nof freedom of the initial state. We show how by training this reduced order\nmodel on a point-wise error metric on short time-prediction, we are able to\ncorrectly learn the dynamics of the Lagrangian turbulence, such that also the\nlong-time statistical behavior is stably recovered at test time. This opens up\na range of new applications, for example, for the control of active Lagrangian\nagents in turbulence.", "published": "2025-07-21 20:50:55", "link": "http://arxiv.org/abs/2507.16058v1", "categories": ["physics.flu-dyn", "cs.LG", "nlin.CD"], "primary_category": "physics.flu-dyn"}
{"title": "Radiological and Biological Dictionary of Radiomics Features: Addressing Understandable AI Issues in Personalized Breast Cancer; Dictionary Version BM1.0", "abstract": "Radiomics-based AI models show promise for breast cancer diagnosis but often\nlack interpretability, limiting clinical adoption. This study addresses the gap\nbetween radiomic features (RF) and the standardized BI-RADS lexicon by\nproposing a dual-dictionary framework. First, a Clinically-Informed Feature\nInterpretation Dictionary (CIFID) was created by mapping 56 RFs to BI-RADS\ndescriptors (shape, margin, internal enhancement) through literature and expert\nreview. The framework was applied to classify triple-negative breast cancer\n(TNBC) versus non-TNBC using dynamic contrast-enhanced MRI from a\nmulti-institutional cohort of 1,549 patients. We trained 27 machine learning\nclassifiers with 27 feature selection methods. SHapley Additive exPlanations\n(SHAP) were used to interpret predictions and generate a complementary\nData-Driven Feature Interpretation Dictionary (DDFID) for 52 additional RFs.\nThe best model, combining Variance Inflation Factor (VIF) selection with Extra\nTrees Classifier, achieved an average cross-validation accuracy of 0.83. Key\npredictive RFs aligned with clinical knowledge: higher Sphericity (round/oval\nshape) and lower Busyness (more homogeneous enhancement) were associated with\nTNBC. The framework confirmed known imaging biomarkers and uncovered novel,\ninterpretable associations. This dual-dictionary approach (BM1.0) enhances AI\nmodel transparency and supports the integration of RFs into routine breast\ncancer diagnosis and personalized care.", "published": "2025-07-21 20:17:20", "link": "http://arxiv.org/abs/2507.16041v1", "categories": ["physics.comp-ph", "cs.LG", "F.2.2, I.2.7"], "primary_category": "physics.comp-ph"}
{"title": "Reactivation: Empirical NTK Dynamics Under Task Shifts", "abstract": "The Neural Tangent Kernel (NTK) offers a powerful tool to study the\nfunctional dynamics of neural networks. In the so-called lazy, or kernel\nregime, the NTK remains static during training and the network function is\nlinear in the static neural tangents feature space. The evolution of the NTK\nduring training is necessary for feature learning, a key driver of deep\nlearning success. The study of the NTK dynamics has led to several critical\ndiscoveries in recent years, in generalization and scaling behaviours. However,\nthis body of work has been limited to the single task setting, where the data\ndistribution is assumed constant over time. In this work, we present a\ncomprehensive empirical analysis of NTK dynamics in continual learning, where\nthe data distribution shifts over time. Our findings highlight continual\nlearning as a rich and underutilized testbed for probing the dynamics of neural\ntraining. At the same time, they challenge the validity of static-kernel\napproximations in theoretical treatments of continual learning, even at large\nscale.", "published": "2025-07-21 20:13:02", "link": "http://arxiv.org/abs/2507.16039v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Enhancing Stability of Physics-Informed Neural Network Training Through Saddle-Point Reformulation", "abstract": "Physics-informed neural networks (PINNs) have gained prominence in recent\nyears and are now effectively used in a number of applications. However, their\nperformance remains unstable due to the complex landscape of the loss function.\nTo address this issue, we reformulate PINN training as a nonconvex-strongly\nconcave saddle-point problem. After establishing the theoretical foundation for\nthis approach, we conduct an extensive experimental study, evaluating its\neffectiveness across various tasks and architectures. Our results demonstrate\nthat the proposed method outperforms the current state-of-the-art techniques.", "published": "2025-07-21 18:59:26", "link": "http://arxiv.org/abs/2507.16008v1", "categories": ["cs.LG", "math.OC"], "primary_category": "cs.LG"}
{"title": "AutoMAT: A Hierarchical Framework for Autonomous Alloy Discovery", "abstract": "Alloy discovery is central to advancing modern industry but remains hindered\nby the vastness of compositional design space and the costly validation. Here,\nwe present AutoMAT, a hierarchical and autonomous framework grounded in and\nvalidated by experiments, which integrates large language models, automated\nCALPHAD-based simulations, and AI-driven search to accelerate alloy design.\nSpanning the entire pipeline from ideation to validation, AutoMAT achieves high\nefficiency, accuracy, and interpretability without the need for manually\ncurated large datasets. In a case study targeting a lightweight, high-strength\nalloy, AutoMAT identifies a titanium alloy with 8.1% lower density and\ncomparable yield strength relative to the state-of-the-art reference, achieving\nthe highest specific strength among all comparisons. In a second case targeting\nhigh-yield-strength high-entropy alloys, AutoMAT achieves a 28.2% improvement\nin yield strength over the base alloy. In both cases, AutoMAT reduces the\ndiscovery timeline from years to weeks, illustrating its potential as a\nscalable and versatile platform for next-generation alloy design.", "published": "2025-07-21 18:55:03", "link": "http://arxiv.org/abs/2507.16005v1", "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.LG"], "primary_category": "cond-mat.mtrl-sci"}
{"title": "Minor Embedding for Quantum Annealing with Reinforcement Learning", "abstract": "Quantum Annealing (QA) is a quantum computing paradigm for solving\ncombinatorial optimization problems formulated as Quadratic Unconstrained\nBinary Optimization (QUBO) problems. An essential step in QA is minor\nembedding, which maps the problem graph onto the sparse topology of the quantum\nprocessor. This process is computationally expensive and scales poorly with\nincreasing problem size and hardware complexity. Existing heuristics are often\ndeveloped for specific problem graphs or hardware topologies and are difficult\nto generalize. Reinforcement Learning (RL) offers a promising alternative by\ntreating minor embedding as a sequential decision-making problem, where an\nagent learns to construct minor embeddings by iteratively mapping the problem\nvariables to the hardware qubits. We propose a RL-based approach to minor\nembedding using a Proximal Policy Optimization agent, testing its ability to\nembed both fully connected and randomly generated problem graphs on two\nhardware topologies, Chimera and Zephyr. The results show that our agent\nconsistently produces valid minor embeddings, with reasonably efficient number\nof qubits, in particular on the more modern Zephyr topology. Our proposed\napproach is also able to scale to moderate problem sizes and adapts well to\ndifferent graph structures, highlighting RL's potential as a flexible and\ngeneral-purpose framework for minor embedding in QA.", "published": "2025-07-21 18:54:15", "link": "http://arxiv.org/abs/2507.16004v1", "categories": ["quant-ph", "cs.LG"], "primary_category": "quant-ph"}
{"title": "Automated Design of Structured Variational Quantum Circuits with Reinforcement Learning", "abstract": "Variational Quantum Algorithms (VQAs) are among the most promising approaches\nfor leveraging near-term quantum hardware, yet their effectiveness strongly\ndepends on the design of the underlying circuit ansatz, which is typically\nconstructed with heuristic methods. In this work, we represent the synthesis of\nvariational quantum circuits as a sequential decision-making problem, where\ngates are added iteratively in order to optimize an objective function, and we\nintroduce two reinforcement learning-based methods, RLVQC Global and RLVQC\nBlock, tailored to combinatorial optimization problems. RLVQC Block creates\nansatzes that generalize the Quantum Approximate Optimization Algorithm (QAOA),\nby discovering a two-qubits block that is applied to all the interacting qubit\npairs. While RLVQC Global further generalizes the ansatz and adds gates\nunconstrained by the structure of the interacting qubits. Both methods adopt\nthe Proximal Policy Optimization (PPO) algorithm and use empirical measurement\noutcomes as state observations to guide the agent. We evaluate the proposed\nmethods on a broad set of QUBO instances derived from classical graph-based\noptimization problems. Our results show that both RLVQC methods exhibit strong\nresults with RLVQC Block consistently outperforming QAOA and generally\nsurpassing RLVQC Global. While RLVQC Block produces circuits with depth\ncomparable to QAOA, the Global variant is instead able to find significantly\nshorter ones. These findings suggest that reinforcement learning methods can be\nan effective tool to discover new ansatz structures tailored for specific\nproblems and that the most effective circuit design strategy lies between rigid\npredefined architectures and completely unconstrained ones, offering a\nfavourable trade-off between structure and adaptability.", "published": "2025-07-21 18:40:59", "link": "http://arxiv.org/abs/2507.16001v1", "categories": ["quant-ph", "cs.LG"], "primary_category": "quant-ph"}
{"title": "On the transferability of Sparse Autoencoders for interpreting compressed models", "abstract": "Modern LLMs face inference efficiency challenges due to their scale. To\naddress this, many compression methods have been proposed, such as pruning and\nquantization. However, the effect of compression on a model's interpretability\nremains elusive. While several model interpretation approaches exist, such as\ncircuit discovery, Sparse Autoencoders (SAEs) have proven particularly\neffective in decomposing a model's activation space into its feature basis. In\nthis work, we explore the differences in SAEs for the original and compressed\nmodels. We find that SAEs trained on the original model can interpret the\ncompressed model albeit with slight performance degradation compared to the\ntrained SAE on the compressed model. Furthermore, simply pruning the original\nSAE itself achieves performance comparable to training a new SAE on the pruned\nmodel. This finding enables us to mitigate the extensive training costs of\nSAEs.", "published": "2025-07-21 18:17:18", "link": "http://arxiv.org/abs/2507.15977v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Efficient dataset construction using active learning and uncertainty-aware neural networks for plasma turbulent transport surrogate models", "abstract": "This work demonstrates a proof-of-principle for using uncertainty-aware\narchitectures, in combination with active learning techniques and an\nin-the-loop physics simulation code as a data labeller, to construct efficient\ndatasets for data-driven surrogate model generation. Building off of a previous\nproof-of-principle successfully demonstrating training set reduction on static\npre-labelled datasets, using the ADEPT framework, this strategy was applied\nagain to the plasma turbulent transport problem within tokamak fusion plasmas,\nspecifically the QuaLiKiz quasilinear electrostatic gyrokinetic turbulent\ntransport code. While QuaLiKiz provides relatively fast evaluations, this study\nspecifically targeted small datasets to serve as a proxy for more expensive\ncodes, such as CGYRO or GENE. The newly implemented algorithm uses the SNGP\narchitecture for the classification component of the problem and the BNN-NCP\narchitecture for the regression component, training models for all turbulent\nmodes (ITG, TEM, ETG) and all transport fluxes ($Q_e$, $Q_i$, $\\Gamma_e$,\n$\\Gamma_i$, and $\\Pi_i$) described by the general QuaLiKiz output. With 45\nactive learning iterations, moving from a small initial training set of\n$10^{2}$ to a final set of $10^{4}$, the resulting models reached a $F_1$\nclassification performance of ~0.8 and a $R^2$ regression performance of ~0.75\non an independent test set across all outputs. This extrapolates to reaching\nthe same performance and efficiency as the previous ADEPT pipeline, although on\na problem with 1 extra input dimension. While the improvement rate achieved in\nthis implementation diminishes faster than expected, the overall technique is\nformulated with components that can be upgraded and generalized to many\nsurrogate modeling applications beyond plasma turbulent transport predictions.", "published": "2025-07-21 18:15:12", "link": "http://arxiv.org/abs/2507.15976v1", "categories": ["physics.plasm-ph", "cs.LG"], "primary_category": "physics.plasm-ph"}
{"title": "Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation", "abstract": "The implementation of Artificial Intelligence (AI) in household environments,\nespecially in the form of proactive autonomous agents, brings about\npossibilities of comfort and attention as well as it comes with intra or\nextramural ethical challenges. This article analyzes agentic AI and its\napplications, focusing on its move from reactive to proactive autonomy,\nprivacy, fairness and user control. We review responsible innovation\nframeworks, human-centered design principles, and governance practices to\ndistill practical guidance for ethical smart home systems. Vulnerable user\ngroups such as elderly individuals, children, and neurodivergent who face\nhigher risks of surveillance, bias, and privacy risks were studied in detail in\ncontext of Agentic AI. Design imperatives are highlighted such as tailored\nexplainability, granular consent mechanisms, and robust override controls,\nsupported by participatory and inclusive methodologies. It was also explored\nhow data-driven insights, including social media analysis via Natural Language\nProcessing(NLP), can inform specific user needs and ethical concerns. This\nsurvey aims to provide both a conceptual foundation and suggestions for\ndeveloping transparent, inclusive, and trustworthy agentic AI in household\nautomation.", "published": "2025-07-21 06:10:02", "link": "http://arxiv.org/abs/2507.15901v1", "categories": ["cs.AI", "cs.CY", "cs.MA"], "primary_category": "cs.AI"}
{"title": "The Intrinsic Riemannian Proximal Gradient Method for Convex Optimization", "abstract": "We consider a class of (possibly strongly) geodesically convex optimization\nproblems on Hadamard manifolds, where the objective function splits into the\nsum of a smooth and a possibly nonsmooth function. We introduce an intrinsic\nconvex Riemannian proximal gradient (CRPG) method that employs the manifold\nproximal map for the nonsmooth step, without operating in the embedding or\ntangent space. A sublinear convergence rate for convex problems and a linear\nconvergence rate for strongly convex problems is established, and we derive\nfundamental proximal gradient inequalities that generalize the Euclidean case.\nOur numerical experiments on hyperbolic spaces and manifolds of symmetric\npositive definite matrices demonstrate substantial computational advantages\nover existing methods.", "published": "2025-07-21 20:46:03", "link": "http://arxiv.org/abs/2507.16055v1", "categories": ["math.OC", "cs.NA", "math.DG", "math.NA", "90C25, 49Q99, 49M30, 65K10"], "primary_category": "math.OC"}
{"title": "Structure-preserving deflation of critical eigenvalues in quadratic eigenvalue problems associated with damped mass-spring systems", "abstract": "For a quadratic matrix polynomial associated with a damped mass-spring system\nthere are three types of critical eigenvalues, the eigenvalues $\\infty$ and $0$\nand the eigenvalues on the imaginary axis. All these are on the boundary of the\nset of (robustly) stable eigenvalues. For numerical methods, but also for\n(robust) stability analysis, it is desirable to deflate such eigenvalues by\nprojecting the matrix polynomial to a lower dimensional subspace before\ncomputing the other eigenvalues and eigenvectors. We describe\nstructure-preserving deflation strategies that deflate these eigenvalues via a\ntrimmed structure-preserving linearization. We employ these results for the\nspecial case of hyperbolic problems. We also analyze the effect of a (possibly\nlow rank) parametric damping matrix on purely imaginary eigenvalues.", "published": "2025-07-21 19:43:11", "link": "http://arxiv.org/abs/2507.16024v1", "categories": ["math.NA", "cs.NA", "math.SP", "65F15, 15A57, 15A18, 65F35"], "primary_category": "math.NA"}
{"title": "Bayesian implementation of Targeted Maximum Likelihood Estimation for uncertainty quantification in causal effect estimation", "abstract": "Robust decision making involves making decisions in the presence of\nuncertainty and is often used in critical domains such as healthcare, supply\nchains, and finance. Causality plays a crucial role in decision-making as it\npredicts the change in an outcome (usually a key performance indicator) due to\na treatment (also called an intervention). To facilitate robust decision making\nusing causality, this paper proposes three Bayesian approaches of the popular\nTargeted Maximum Likelihood Estimation (TMLE) algorithm, a flexible\nsemi-parametric double robust estimator, for a probabilistic quantification of\nuncertainty in causal effects with binary treatment, and binary and continuous\noutcomes. In the first two approaches, the three TMLE models (outcome,\ntreatment, and fluctuation) are trained sequentially. Since Bayesian\nimplementation of treatment and outcome yields probabilistic predictions, the\nfirst approach uses mean predictions, while the second approach uses both the\nmean and standard deviation of predictions for training the fluctuation model\n(targeting step). The third approach trains all three models simultaneously\nthrough a Bayesian network (called BN-TMLE in this paper). The proposed\napproaches were demonstrated for two examples with binary and continuous\noutcomes and validated against classical implementations. This paper also\ninvestigated the effect of data sizes and model misspecifications on causal\neffect estimation using the BN-TMLE approach. Results showed that the proposed\nBN-TMLE outperformed classical implementations in small data regimes and\nperformed similarly in large data regimes.", "published": "2025-07-21 15:46:20", "link": "http://arxiv.org/abs/2507.15909v1", "categories": ["stat.ME", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Structural DID with ML: Theory, Simulation, and a Roadmap for Applied Research", "abstract": "Causal inference in observational panel data has become a central concern in\neconomics,policy analysis,and the broader social sciences.To address the core\ncontradiction where traditional difference-in-differences (DID) struggles with\nhigh-dimensional confounding variables in observational panel data,while\nmachine learning (ML) lacks causal structure interpretability,this paper\nproposes an innovative framework called S-DIDML that integrates structural\nidentification with high-dimensional estimation.Building upon the structure of\ntraditional DID methods,S-DIDML employs structured residual orthogonalization\ntechniques (Neyman orthogonality+cross-fitting) to retain the group-time\ntreatment effect (ATT) identification structure while resolving\nhigh-dimensional covariate interference issues.It designs a dynamic\nheterogeneity estimation module combining causal forests and semi-parametric\nmodels to capture spatiotemporal heterogeneity effects.The framework\nestablishes a complete modular application process with standardized Stata\nimplementation paths.The introduction of S-DIDML enriches methodological\nresearch on DID and DDML innovations, shifting causal inference from method\nstacking to architecture integration.This advancement enables social sciences\nto precisely identify policy-sensitive groups and optimize resource\nallocation.The framework provides replicable evaluation tools, decision\noptimization references,and methodological paradigms for complex intervention\nscenarios such as digital transformation policies and environmental\nregulations.", "published": "2025-07-21 03:57:42", "link": "http://arxiv.org/abs/2507.15899v1", "categories": ["stat.ML", "cs.LG", "91-01"], "primary_category": "stat.ML"}
{"title": "ReDi: Rectified Discrete Flow", "abstract": "Discrete Flow-based Models (DFMs) are powerful generative models for\nhigh-quality discrete data but typically suffer from slow sampling speeds due\nto their reliance on iterative decoding processes. This reliance on a\nmulti-step process originates from the factorization approximation of DFMs,\nwhich is necessary for handling high-dimensional data. In this paper, we\nrigorously characterize the approximation error from factorization using\nConditional Total Correlation (TC), which depends on the coupling. To reduce\nthe Conditional TC and enable efficient few-step generation, we propose\nRectified Discrete Flow (ReDi), a novel iterative method that reduces\nfactorization error by rectifying the coupling between source and target\ndistributions. We theoretically prove that each ReDi step guarantees a\nmonotonic decreasing Conditional TC, ensuring its convergence. Empirically,\nReDi significantly reduces Conditional TC and enables few-step generation.\nMoreover, we demonstrate that the rectified couplings are well-suited for\ntraining efficient one-step models on image generation. ReDi offers a simple\nand theoretically grounded approach for tackling the few-step challenge,\nproviding a new perspective on efficient discrete data synthesis. Code is\navailable at https://github.com/Ugness/ReDi_discrete", "published": "2025-07-21 01:18:44", "link": "http://arxiv.org/abs/2507.15897v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Distributed Asynchronous Device Speech Enhancement via Windowed Cross-Attention", "abstract": "The increasing number of microphone-equipped personal devices offers great\nflexibility and potential using them as ad-hoc microphone arrays in dynamic\nmeeting environments. However, most existing approaches are designed for\ntime-synchronized microphone setups, a condition that may not hold in\nreal-world meeting scenarios, where time latency and clock drift vary across\ndevices. Under such conditions, we found transform-average-concatenate (TAC), a\npopular module for neural multi-microphone processing, insufficient in handling\ntime-asynchronous microphones. In response, we propose a windowed\ncross-attention module capable of dynamically aligning features between all\nmicrophones. This module is invariant to both the permutation and the number of\nmicrophones and can be easily integrated into existing models. Furthermore, we\npropose an optimal training target for multi-talker environments. We evaluated\nour approach in a multi-microphone noisy reverberant setup with unknown time\nlatency and clock drift of each microphone. Experimental results show that our\nmethod outperforms TAC on both iFaSNet and CRUSE models, offering faster\nconvergence and improved learning, demonstrating the efficacy of the windowed\ncross-attention module for asynchronous microphone setups.", "published": "2025-07-21 23:07:10", "link": "http://arxiv.org/abs/2507.16104v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Nonlinear Framework for Speech Bandwidth Extension", "abstract": "Recovering high-frequency components lost to bandwidth constraints is crucial\nfor applications ranging from telecommunications to high-fidelity audio on\nlimited resources. We introduce NDSI-BWE, a new adversarial Band Width\nExtension (BWE) framework that leverage four new discriminators inspired by\nnonlinear dynamical system to capture diverse temporal behaviors: a\nMulti-Resolution Lyapunov Discriminator (MRLD) for determining sensitivity to\ninitial conditions by capturing deterministic chaos, a Multi-Scale Recurrence\nDiscriminator (MS-RD) for self-similar recurrence dynamics, a Multi-Scale\nDetrended Fractal Analysis Discriminator (MSDFA) for long range slow variant\nscale invariant relationship, a Multi-Resolution Poincar\\'e Plot Discriminator\n(MR-PPD) for capturing hidden latent space relationship, a Multi-Period\nDiscriminator (MPD) for cyclical patterns, a Multi-Resolution Amplitude\nDiscriminator (MRAD) and Multi-Resolution Phase Discriminator (MRPD) for\ncapturing intricate amplitude-phase transition statistics. By using depth-wise\nconvolution at the core of the convolutional block with in each discriminators,\nNDSI-BWE attains an eight-times parameter reduction. These seven discriminators\nguide a complex-valued ConformerNeXt based genetor with a dual stream\nLattice-Net based architecture for simultaneous refinement of magnitude and\nphase. The genertor leverage the transformer based conformer's global\ndependency modeling and ConvNeXt block's local temporal modeling capability.\nAcross six objective evaluation metrics and subjective based texts comprises of\nfive human judges, NDSI-BWE establishes a new SoTA in BWE.", "published": "2025-07-21 18:06:29", "link": "http://arxiv.org/abs/2507.15970v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Modeling and Analysis of Land-to-Ship Maritime Wireless Channels at 5.8 GHz", "abstract": "Maritime channel modeling is crucial for designing robust communication\nsystems in marine environments, where factors like waves and wind impact signal\npropagation. This article investigates land-to-ship maritime wireless channel\ncharacteristics at 5.8 GHz based upon an extensive measurement campaign, with\nconcurrent hydrological and meteorological information collection. First, a\nnovel large-scale path loss model with physical foundation and high accuracy is\nproposed for dynamic marine environments. Then, we introduce the concept of\nsea-wave-induced fixed-point (SWIFT) fading, a peculiar phenomenon in maritime\nscenarios that captures the impact of sea surface fluctuations on received\npower. An enhanced two-ray model incorporating vessel rotational motion is\npropounded to simulate the SWIFT fading, showing good alignment with measured\ndata, particularly for modest antenna movements. Next, the small-scale fading\nis studied by leveraging a variety of models including the two-wave with\ndiffuse power (TWDP) and asymmetric Laplace distributions, with the latter\nperforming well in most cases, while TWDP better captures bimodal fading in\nrough seas. Furthermore, maritime channel sparsity is examined via the Gini\nindex and Rician $K$ factor, and temporal dispersion is characterized. The\nresulting channel models and parameter characteristics offer valuable insights\nfor maritime wireless system design and deployment.", "published": "2025-07-21 18:06:21", "link": "http://arxiv.org/abs/2507.15969v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "MSGM: A Multi-Scale Spatiotemporal Graph Mamba for EEG Emotion Recognition", "abstract": "EEG-based emotion recognition struggles with capturing multi-scale\nspatiotemporal dynamics and ensuring computational efficiency for real-time\napplications. Existing methods often oversimplify temporal granularity and\nspatial hierarchies, limiting accuracy. To overcome these challenges, we\npropose the Multi-Scale Spatiotemporal Graph Mamba (MSGM), a novel framework\nintegrating multi-window temporal segmentation, bimodal spatial graph modeling,\nand efficient fusion via the Mamba architecture. By segmenting EEG signals\nacross diverse temporal scales and constructing global-local graphs with\nneuroanatomical priors, MSGM effectively captures fine-grained emotional\nfluctuations and hierarchical brain connectivity. A multi-depth Graph\nConvolutional Network (GCN) and token embedding fusion module, paired with\nMamba's state-space modeling, enable dynamic spatiotemporal interaction at\nlinear complexity. Notably, with just one MSST-Mamba layer, MSGM surpasses\nleading methods in the field on the SEED, THU-EP, and FACED datasets,\noutperforming baselines in subject-independent emotion classification while\nachieving robust accuracy and millisecond-level inference on the NVIDIA Jetson\nXavier NX.", "published": "2025-07-21 17:18:00", "link": "http://arxiv.org/abs/2507.15914v1", "categories": ["eess.SP", "cs.LG"], "primary_category": "eess.SP"}
{"title": "3LM: Bridging Arabic, STEM, and Code through Benchmarking", "abstract": "Arabic is one of the most widely spoken languages in the world, yet efforts\nto develop and evaluate Large Language Models (LLMs) for Arabic remain\nrelatively limited. Most existing Arabic benchmarks focus on linguistic,\ncultural, or religious content, leaving a significant gap in domains like STEM\nand code which are increasingly relevant for real-world LLM applications. To\nhelp bridge this gap, we present 3LM, a suite of three benchmarks designed\nspecifically for Arabic. The first is a set of STEM-related question-answer\npairs, naturally sourced from Arabic textbooks and educational worksheets. The\nsecond consists of synthetically generated STEM questions, created using the\nsame sources. The third benchmark focuses on code generation, built through a\ncareful translation of two widely used code benchmarks, incorporating a\nhuman-in-the-loop process with several rounds of review to ensure high-quality\nand faithful translations. We release all three benchmarks publicly to support\nthe growth of Arabic LLM research in these essential but underrepresented\nareas.", "published": "2025-07-21 17:58:27", "link": "http://arxiv.org/abs/2507.15850v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation", "abstract": "Retrieval-Augmented Generation (RAG) effectively improves the accuracy of\nLarge Language Models (LLMs). However, retrieval noises significantly impact\nthe quality of LLMs' generation, necessitating the development of denoising\nmechanisms. Previous methods extract evidence straightforwardly without\nexplicit thinking, which risks filtering out key clues and struggles with\ngeneralization. To this end, we propose LEAR, which learns to extract rational\nevidence by (1) explicitly reasoning to identify potential cues within\nretrieval contents first, and then (2) consciously extracting to avoid omitting\nany key cues helpful for answering questions. Specifically, we frame evidence\nreasoning and evidence extraction into one unified response for end-to-end\ntraining; apply knowledge token masks for disentanglement to derive\nreasoning-based and extraction-based answers; and devise three types of\nverifiable reward functions, including answer, length, and format, to update\nthe model via the policy optimization algorithm. Extensive experiments on three\nbenchmark datasets show the effectiveness of LEAR, providing compact and\nhigh-quality evidence, improving the accuracy of downstream tasks, and\npromoting effective application in online RAG systems.", "published": "2025-07-21 13:03:55", "link": "http://arxiv.org/abs/2507.15586v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Solving nonconvex Hamilton--Jacobi--Isaacs equations with PINN-based policy iteration", "abstract": "We propose a mesh-free policy iteration framework that combines classical\ndynamic programming with physics-informed neural networks (PINNs) to solve\nhigh-dimensional, nonconvex Hamilton--Jacobi--Isaacs (HJI) equations arising in\nstochastic differential games and robust control. The method alternates between\nsolving linear second-order PDEs under fixed feedback policies and updating the\ncontrols via pointwise minimax optimization using automatic differentiation.\nUnder standard Lipschitz and uniform ellipticity assumptions, we prove that the\nvalue function iterates converge locally uniformly to the unique viscosity\nsolution of the HJI equation. The analysis establishes equi-Lipschitz\nregularity of the iterates, enabling provable stability and convergence without\nrequiring convexity of the Hamiltonian. Numerical experiments demonstrate the\naccuracy and scalability of the method. In a two-dimensional stochastic\npath-planning game with a moving obstacle, our method matches finite-difference\nbenchmarks with relative $L^2$-errors below %10^{-2}%. In five- and\nten-dimensional publisher-subscriber differential games with anisotropic noise,\nthe proposed approach consistently outperforms direct PINN solvers, yielding\nsmoother value functions and lower residuals. Our results suggest that\nintegrating PINNs with policy iteration is a practical and theoretically\ngrounded method for solving high-dimensional, nonconvex HJI equations, with\npotential applications in robotics, finance, and multi-agent reinforcement\nlearning.", "published": "2025-07-21 10:06:53", "link": "http://arxiv.org/abs/2507.15455v2", "categories": ["math.NA", "cs.AI", "cs.NA", "math.AP", "49N70, 35Q93, 49L25, 68T07"], "primary_category": "math.NA"}
{"title": "Pixels, Patterns, but No Poetry: To See The World like Humans", "abstract": "Achieving human-like perception and reasoning in Multimodal Large Language\nModels (MLLMs) remains a central challenge in artificial intelligence. While\nrecent research has primarily focused on enhancing reasoning capabilities in\nMLLMs, a fundamental question persists: Can Multimodal Large Language Models\ntruly perceive the world as humans do? This paper shifts focus from reasoning\nto perception. Rather than constructing benchmarks specifically for reasoning,\nwe introduce the Turing Eye Test (TET), a challenging perception-oriented\nbenchmark comprising four diagnostic tasks that evaluate MLLMs' performance on\nsynthetic images that humans process intuitively. Our findings reveal that\nstate-of-the-art MLLMs exhibit catastrophic failures on our perceptual tasks\ntrivial for humans. Both in-context learning and training on language\nbackbone-effective for previous benchmarks-fail to improve performance on our\ntasks, while fine-tuning the vision tower enables rapid adaptation, suggesting\nthat our benchmark poses challenges for vision tower generalization rather than\nfor the knowledge and reasoning capabilities of the language backbone-a key gap\nbetween current MLLMs and human perception. We release a representative subset\nof TET tasks in this version, and will introduce more diverse tasks and methods\nto enhance visual generalization in future work.", "published": "2025-07-21 21:50:16", "link": "http://arxiv.org/abs/2507.16863v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "EVOLVE-X: Embedding Fusion and Language Prompting for User Evolution Forecasting on Social Media", "abstract": "Social media platforms serve as a significant medium for sharing personal\nemotions, daily activities, and various life events, ensuring individuals stay\ninformed about the latest developments. From the initiation of an account,\nusers progressively expand their circle of friends or followers, engaging\nactively by posting, commenting, and sharing content. Over time, user behavior\non these platforms evolves, influenced by demographic attributes and the\nnetworks they form. In this study, we present a novel approach that leverages\nopen-source models Llama-3-Instruct, Mistral-7B-Instruct, Gemma-7B-IT through\nprompt engineering, combined with GPT-2, BERT, and RoBERTa using a joint\nembedding technique, to analyze and predict the evolution of user behavior on\nsocial media over their lifetime. Our experiments demonstrate the potential of\nthese models to forecast future stages of a user's social evolution, including\nnetwork changes, future connections, and shifts in user activities.\nExperimental results highlight the effectiveness of our approach, with GPT-2\nachieving the lowest perplexity (8.21) in a Cross-modal configuration,\noutperforming RoBERTa (9.11) and BERT, and underscoring the importance of\nleveraging Cross-modal configurations for superior performance. This approach\naddresses critical challenges in social media, such as friend recommendations\nand activity predictions, offering insights into the trajectory of user\nbehavior. By anticipating future interactions and activities, this research\naims to provide early warnings about potential negative outcomes, enabling\nusers to make informed decisions and mitigate risks in the long term.", "published": "2025-07-21 05:07:07", "link": "http://arxiv.org/abs/2507.16847v1", "categories": ["cs.SI", "cs.IR", "cs.LG"], "primary_category": "cs.SI"}
{"title": "MobileUse: A GUI Agent with Hierarchical Reflection for Autonomous Mobile Operation", "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have enabled the\ndevelopment of mobile agents that can understand visual inputs and follow user\ninstructions, unlocking new possibilities for automating complex tasks on\nmobile devices. However, applying these models to real-world mobile scenarios\nremains a significant challenge due to the long-horizon task execution,\ndifficulty in error recovery, and the cold-start problem in unfamiliar\nenvironments. To address these challenges, we propose MobileUse, a GUI agent\ndesigned for robust and adaptive mobile task execution. To improve resilience\nin long-horizon tasks and dynamic environments, we introduce a hierarchical\nreflection architecture that enables the agent to self-monitor, detect, and\nrecover from errors across multiple temporal scales-ranging from individual\nactions to overall task completion-while maintaining efficiency through a\nreflection-on-demand strategy. To tackle cold-start issues, we further\nintroduce a proactive exploration module, which enriches the agent's\nunderstanding of the environment through self-planned exploration. Evaluations\non AndroidWorld and AndroidLab benchmarks demonstrate that MobileUse\nestablishes new state-of-the-art performance, achieving success rates of 62.9%\nand 44.2%, respectively. To facilitate real-world applications, we release an\nout-of-the-box toolkit for automated task execution on physical mobile devices,\nwhich is available at https://github.com/MadeAgents/mobile-use.", "published": "2025-07-21 09:37:05", "link": "http://arxiv.org/abs/2507.16853v1", "categories": ["cs.RO", "cs.MA"], "primary_category": "cs.RO"}
{"title": "Interpretable Embeddings of Speech Enhance and Explain Brain Encoding Performance of Audio Models", "abstract": "Self-supervised speech models (SSMs) are increasingly hailed as more powerful\ncomputational models of human speech perception than models based on\ntraditional hand-crafted features. However, since their representations are\ninherently black-box, it remains unclear what drives their alignment with brain\nresponses. To remedy this, we built linear encoding models from six\ninterpretable feature families: mel-spectrogram, Gabor filter bank features,\nspeech presence, phonetic, syntactic, and semantic Question-Answering features,\nand contextualized embeddings from three state-of-the-art SSMs (Whisper,\nHuBERT, WavLM), quantifying the shared and unique neural variance captured by\neach feature class. Contrary to prevailing assumptions, our interpretable model\npredicted electrocorticography (ECoG) responses to speech more accurately than\nany SSM. Moreover, augmenting SSM representations with interpretable features\nyielded the best overall neural predictions, significantly outperforming either\nclass alone. Further variance-partitioning analyses revealed previously\nunresolved components of SSM representations that contribute to their neural\nalignment: 1. Despite the common assumption that later layers of SSMs discard\nlow-level acoustic information, these models compress and preferentially retain\nfrequency bands critical for neural encoding of speech (100-1000 Hz). 2.\nContrary to previous claims, SSMs encode brain-relevant semantic information\nthat cannot be reduced to lower-level features, improving with context length\nand model size. These results highlight the importance of using refined,\ninterpretable features in understanding speech perception.", "published": "2025-07-21 21:33:36", "link": "http://arxiv.org/abs/2507.16080v1", "categories": ["q-bio.NC", "cs.SD", "eess.AS"], "primary_category": "q-bio.NC"}
{"title": "RankMixer: Scaling Up Ranking Models in Industrial Recommenders", "abstract": "Recent progress on large language models (LLMs) has spurred interest in\nscaling up recommendation systems, yet two practical obstacles remain. First,\ntraining and serving cost on industrial Recommenders must respect strict\nlatency bounds and high QPS demands. Second, most human-designed\nfeature-crossing modules in ranking models were inherited from the CPU era and\nfail to exploit modern GPUs, resulting in low Model Flops Utilization (MFU) and\npoor scalability. We introduce RankMixer, a hardware-aware model design\ntailored towards a unified and scalable feature-interaction architecture.\nRankMixer retains the transformer's high parallelism while replacing quadratic\nself-attention with multi-head token mixing module for higher efficiency.\nBesides, RankMixer maintains both the modeling for distinct feature subspaces\nand cross-feature-space interactions with Per-token FFNs. We further extend it\nto one billion parameters with a Sparse-MoE variant for higher ROI. A dynamic\nrouting strategy is adapted to address the inadequacy and imbalance of experts\ntraining. Experiments show RankMixer's superior scaling abilities on a\ntrillion-scale production dataset. By replacing previously diverse handcrafted\nlow-MFU modules with RankMixer, we boost the model MFU from 4.5\\% to 45\\%, and\nscale our ranking model parameters by 100x while maintaining roughly the same\ninference latency. We verify RankMixer's universality with online A/B tests\nacross two core application scenarios (Recommendation and Advertisement).\nFinally, we launch 1B Dense-Parameters RankMixer for full traffic serving\nwithout increasing the serving cost, which improves user active days by 0.3\\%\nand total in-app usage duration by 1.08\\%.", "published": "2025-07-21 12:28:55", "link": "http://arxiv.org/abs/2507.15551v2", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Compositional Coordination for Multi-Robot Teams with Large Language Models", "abstract": "Multi-robot coordination has traditionally relied on a mission-specific and\nexpert-driven pipeline, where natural language mission descriptions are\nmanually translated by domain experts into mathematical formulation, algorithm\ndesign, and executable code. This conventional process is labor-intensive,\ninaccessible to non-experts, and inflexible to changes in mission requirements.\nHere, we propose LAN2CB (Language to Collective Behavior), a novel framework\nthat leverages large language models (LLMs) to streamline and generalize the\nmulti-robot coordination pipeline. LAN2CB transforms natural language (NL)\nmission descriptions into executable Python code for multi-robot systems\nthrough two core modules: (1) Mission Analysis, which parses mission\ndescriptions into behavior trees, and (2) Code Generation, which leverages the\nbehavior tree and a structured knowledge base to generate robot control code.\nWe further introduce a dataset of natural language mission descriptions to\nsupport development and benchmarking. Experiments in both simulation and\nreal-world environments demonstrate that LAN2CB enables robust and flexible\nmulti-robot coordination from natural language, significantly reducing manual\nengineering effort and supporting broad generalization across diverse mission\ntypes. Website: https://sites.google.com/view/lan-cb", "published": "2025-07-21 21:09:15", "link": "http://arxiv.org/abs/2507.16068v2", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.RO"}
{"title": "A Fisher's exact test justification of the TF-IDF term-weighting scheme", "abstract": "Term frequency-inverse document frequency, or TF-IDF for short, is arguably\nthe most celebrated mathematical expression in the history of information\nretrieval. Conceived as a simple heuristic quantifying the extent to which a\ngiven term's occurrences are concentrated in any one given document out of\nmany, TF-IDF and its many variants are routinely used as term-weighting schemes\nin diverse text analysis applications. There is a growing body of scholarship\ndedicated to placing TF-IDF on a sound theoretical foundation. Building on that\ntradition, this paper justifies the use of TF-IDF to the statistics community\nby demonstrating how the famed expression can be understood from a significance\ntesting perspective. We show that the common TF-IDF variant TF-ICF is, under\nmild regularity conditions, closely related to the negative logarithm of the\n$p$-value from a one-tailed version of Fisher's exact test of statistical\nsignificance. As a corollary, we establish a connection between TF-IDF and the\nsaid negative log-transformed $p$-value under certain idealized assumptions. We\nfurther demonstrate, as a limiting case, that this same quantity converges to\nTF-IDF in the limit of an infinitely large document collection. The Fisher's\nexact test justification of TF-IDF equips the working statistician with a ready\nexplanation of the term-weighting scheme's long-established effectiveness.", "published": "2025-07-21 15:54:23", "link": "http://arxiv.org/abs/2507.15742v2", "categories": ["cs.CL", "cs.IR", "math.ST", "stat.TH"], "primary_category": "cs.CL"}
{"title": "Estimating Rate-Distortion Functions Using the Energy-Based Model", "abstract": "The rate-distortion (RD) theory is one of the key concepts in information\ntheory, providing theoretical limits for compression performance and guiding\nthe source coding design, with both theoretical and practical significance. The\nBlahut-Arimoto (BA) algorithm, as a classical algorithm to compute RD\nfunctions, encounters computational challenges when applied to high-dimensional\nscenarios. In recent years, many neural methods have attempted to compute\nhigh-dimensional RD problems from the perspective of implicit generative\nmodels. Nevertheless, these approaches often neglect the reconstruction of the\noptimal conditional distribution or rely on unreasonable prior assumptions. In\nface of these issues, we propose an innovative energy-based modeling framework\nthat leverages the connection between the RD dual form and the free energy in\nstatistical physics, achieving effective reconstruction of the optimal\nconditional distribution.The proposed algorithm requires training only a single\nneural network and circumvents the challenge of computing the normalization\nfactor in energy-based models using the Markov chain Monte Carlo (MCMC)\nsampling. Experimental results demonstrate the significant effectiveness of the\nproposed algorithm in estimating high-dimensional RD functions and\nreconstructing the optimal conditional distribution.", "published": "2025-07-21 15:09:50", "link": "http://arxiv.org/abs/2507.15700v2", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "RankMixer: Scaling Up Ranking Models in Industrial Recommenders", "abstract": "Recent progress on large language models (LLMs) has spurred interest in\nscaling up recommendation systems, yet two practical obstacles remain. First,\ntraining and serving cost on industrial Recommenders must respect strict\nlatency bounds and high QPS demands. Second, most human-designed\nfeature-crossing modules in ranking models were inherited from the CPU era and\nfail to exploit modern GPUs, resulting in low Model Flops Utilization (MFU) and\npoor scalability. We introduce RankMixer, a hardware-aware model design\ntailored towards a unified and scalable feature-interaction architecture.\nRankMixer retains the transformer's high parallelism while replacing quadratic\nself-attention with multi-head token mixing module for higher efficiency.\nBesides, RankMixer maintains both the modeling for distinct feature subspaces\nand cross-feature-space interactions with Per-token FFNs. We further extend it\nto one billion parameters with a Sparse-MoE variant for higher ROI. A dynamic\nrouting strategy is adapted to address the inadequacy and imbalance of experts\ntraining. Experiments show RankMixer's superior scaling abilities on a\ntrillion-scale production dataset. By replacing previously diverse handcrafted\nlow-MFU modules with RankMixer, we boost the model MFU from 4.5\\% to 45\\%, and\nscale our ranking model parameters by 100x while maintaining roughly the same\ninference latency. We verify RankMixer's universality with online A/B tests\nacross two core application scenarios (Recommendation and Advertisement).\nFinally, we launch 1B Dense-Parameters RankMixer for full traffic serving\nwithout increasing the serving cost, which improves user active days by 0.3\\%\nand total in-app usage duration by 1.08\\%.", "published": "2025-07-21 12:28:55", "link": "http://arxiv.org/abs/2507.15551v3", "categories": ["cs.IR"], "primary_category": "cs.IR"}
