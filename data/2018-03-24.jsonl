{"title": "Near-lossless Binarization of Word Embeddings", "abstract": "Word embeddings are commonly used as a starting point in many NLP models to\nachieve state-of-the-art performances. However, with a large vocabulary and\nmany dimensions, these floating-point representations are expensive both in\nterms of memory and calculations which makes them unsuitable for use on\nlow-resource devices. The method proposed in this paper transforms real-valued\nembeddings into binary embeddings while preserving semantic information,\nrequiring only 128 or 256 bits for each vector. This leads to a small memory\nfootprint and fast vector operations. The model is based on an autoencoder\narchitecture, which also allows to reconstruct original vectors from the binary\nones. Experimental results on semantic similarity, text classification and\nsentiment analysis tasks show that the binarization of word embeddings only\nleads to a loss of ~2% in accuracy while vector size is reduced by 97%.\nFurthermore, a top-k benchmark demonstrates that using these binary vectors is\n30 times faster than using real-valued vectors.", "published": "2018-03-24 06:21:56", "link": "http://arxiv.org/abs/1803.09065v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simple Large-scale Relation Extraction from Unstructured Text", "abstract": "Knowledge-based question answering relies on the availability of facts, the\nmajority of which cannot be found in structured sources (e.g. Wikipedia\ninfo-boxes, Wikidata). One of the major components of extracting facts from\nunstructured text is Relation Extraction (RE). In this paper we propose a novel\nmethod for creating distant (weak) supervision labels for training a\nlarge-scale RE system. We also provide new evidence about the effectiveness of\nneural network approaches by decoupling the model architecture from the feature\ndesign of a state-of-the-art neural network system. Surprisingly, a much\nsimpler classifier trained on similar features performs on par with the highly\ncomplex neural network system (at 75x reduction to the training time),\nsuggesting that the features are a bigger contributor to the final performance.", "published": "2018-03-24 10:57:41", "link": "http://arxiv.org/abs/1803.09091v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine Learning and Applied Linguistics", "abstract": "This entry introduces the topic of machine learning and provides an overview\nof its relevance for applied linguistics and language learning. The discussion\nwill focus on giving an introduction to the methods and applications of machine\nlearning in applied linguistics, and will provide references for further study.", "published": "2018-03-24 13:08:56", "link": "http://arxiv.org/abs/1803.09103v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low-Resource Speech-to-Text Translation", "abstract": "Speech-to-text translation has many potential applications for low-resource\nlanguages, but the typical approach of cascading speech recognition with\nmachine translation is often impossible, since the transcripts needed to train\na speech recognizer are usually not available for low-resource languages.\nRecent work has found that neural encoder-decoder models can learn to directly\ntranslate foreign speech in high-resource scenarios, without the need for\nintermediate transcription. We investigate whether this approach also works in\nsettings where both data and computation are limited. To make the approach\nefficient, we make several architectural changes, including a change from\ncharacter-level to word-level decoding. We find that this choice yields crucial\nspeed improvements that allow us to train with fewer computational resources,\nyet still performs well on frequent words. We explore models trained on between\n20 and 160 hours of data, and find that although models trained on less data\nhave considerably lower BLEU scores, they can still predict words with\nrelatively high precision and recall---around 50% for a model trained on 50\nhours of data, versus around 60% for the full 160 hour model. Thus, they may\nstill be useful for some low-resource scenarios.", "published": "2018-03-24 21:17:52", "link": "http://arxiv.org/abs/1803.09164v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with\n  Tacotron", "abstract": "We present an extension to the Tacotron speech synthesis architecture that\nlearns a latent embedding space of prosody, derived from a reference acoustic\nrepresentation containing the desired prosody. We show that conditioning\nTacotron on this learned embedding space results in synthesized audio that\nmatches the prosody of the reference signal with fine time detail even when the\nreference and synthesis speakers are different. Additionally, we show that a\nreference prosody embedding can be used to synthesize text that is different\nfrom that of the reference utterance. We define several quantitative and\nsubjective metrics for evaluating prosody transfer, and report results with\naccompanying audio samples from single-speaker and 44-speaker Tacotron models\non a prosody transfer task.", "published": "2018-03-24 02:52:58", "link": "http://arxiv.org/abs/1803.09047v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multi-range Reasoning for Machine Comprehension", "abstract": "We propose MRU (Multi-Range Reasoning Units), a new fast compositional\nencoder for machine comprehension (MC). Our proposed MRU encoders are\ncharacterized by multi-ranged gating, executing a series of parameterized\ncontract-and-expand layers for learning gating vectors that benefit from long\nand short-term dependencies. The aims of our approach are as follows: (1)\nlearning representations that are concurrently aware of long and short-term\ncontext, (2) modeling relationships between intra-document blocks and (3) fast\nand efficient sequence encoding. We show that our proposed encoder demonstrates\npromising results both as a standalone encoder and as well as a complementary\nbuilding block. We conduct extensive experiments on three challenging MC\ndatasets, namely RACE, SearchQA and NarrativeQA, achieving highly competitive\nperformance on all. On the RACE benchmark, our model outperforms DFN (Dynamic\nFusion Networks) by 1.5%-6% without using any recurrent or convolution layers.\nSimilarly, we achieve competitive performance relative to AMANDA on the\nSearchQA benchmark and BiDAF on the NarrativeQA benchmark without using any\nLSTM/GRU layers. Finally, incorporating MRU encoders with standard BiLSTM\narchitectures further improves performance, achieving state-of-the-art results.", "published": "2018-03-24 08:10:04", "link": "http://arxiv.org/abs/1803.09074v1", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Equation Embeddings", "abstract": "We present an unsupervised approach for discovering semantic representations\nof mathematical equations. Equations are challenging to analyze because each is\nunique, or nearly unique. Our method, which we call equation embeddings, finds\ngood representations of equations by using the representations of their\nsurrounding words. We used equation embeddings to analyze four collections of\nscientific articles from the arXiv, covering four computer science domains\n(NLP, IR, AI, and ML) and $\\sim$98.5k equations. Quantitatively, we found that\nequation embeddings provide better models when compared to existing word\nembedding approaches. Qualitatively, we found that equation embeddings provide\ncoherent semantic representations of equations and can capture semantic\nsimilarity to other equations and to words.", "published": "2018-03-24 15:04:17", "link": "http://arxiv.org/abs/1803.09123v1", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "MTGAN: Speaker Verification through Multitasking Triplet Generative\n  Adversarial Networks", "abstract": "In this paper, we propose an enhanced triplet method that improves the\nencoding process of embeddings by jointly utilizing generative adversarial\nmechanism and multitasking optimization. We extend our triplet encoder with\nGenerative Adversarial Networks (GANs) and softmax loss function. GAN is\nintroduced for increasing the generality and diversity of samples, while\nsoftmax is for reinforcing features about speakers. For simplification, we term\nour method Multitasking Triplet Generative Adversarial Networks (MTGAN).\nExperiment on short utterances demonstrates that MTGAN reduces the verification\nequal error rate (EER) by 67% (relatively) and 32% (relatively) over\nconventional i-vector method and state-of-the-art triplet loss method\nrespectively. This effectively indicates that MTGAN outperforms triplet methods\nin the aspect of expressing the high-level feature of speaker information.", "published": "2018-03-24 05:49:07", "link": "http://arxiv.org/abs/1803.09059v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automatic Music Accompanist", "abstract": "Automatic musical accompaniment is where a human musician is accompanied by a\ncomputer musician. The computer musician is able to produce musical\naccompaniment that relates musically to the human performance. The\naccompaniment should follow the performance using observations of the notes\nthey are playing. This paper describes a complete and detailed construction of\na score following and accompanying system using Hidden Markov Models (HMMs). It\ndetails how to train a score HMM, how to deal with polyphonic input, how this\nHMM work when following score, how to build up a musical accompanist. It\nproposes a new parallel hidden Markov model for score following and a fast\ndecoding algorithm to deal with performance errors.", "published": "2018-03-24 02:06:11", "link": "http://arxiv.org/abs/1803.09033v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
