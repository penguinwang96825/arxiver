{"title": "Calculating Probabilities Simplifies Word Learning", "abstract": "Children can use the statistical regularities of their environment to learn\nword meanings, a mechanism known as cross-situational learning. We take a\ncomputational approach to investigate how the information present during each\nobservation in a cross-situational framework can affect the overall acquisition\nof word meanings. We do so by formulating various in-the-moment learning\nmechanisms that are sensitive to different statistics of the environment, such\nas counts and conditional probabilities. Each mechanism introduces a unique\nsource of competition or mutual exclusivity bias to the model; the mechanism\nthat maximally uses the model's knowledge of word meanings performs the best.\nMoreover, the gap between this mechanism and others is amplified in more\nchallenging learning scenarios, such as learning from few examples.", "published": "2017-02-22 04:30:09", "link": "http://arxiv.org/abs/1702.06672v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-Aware Prediction of Derivational Word-forms", "abstract": "Derivational morphology is a fundamental and complex characteristic of\nlanguage. In this paper we propose the new task of predicting the derivational\nform of a given base-form lemma that is appropriate for a given context. We\npresent an encoder--decoder style neural network to produce a derived form\ncharacter-by-character, based on its corresponding character-level\nrepresentation of the base form and the context. We demonstrate that our model\nis able to generate valid context-sensitive derivations from known base forms,\nbut is less accurate under a lexicon agnostic setting.", "published": "2017-02-22 04:50:23", "link": "http://arxiv.org/abs/1702.06675v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One Representation per Word - Does it make Sense for Composition?", "abstract": "In this paper, we investigate whether an a priori disambiguation of word\nsenses is strictly necessary or whether the meaning of a word in context can be\ndisambiguated through composition alone. We evaluate the performance of\noff-the-shelf single-vector and multi-sense vector models on a benchmark phrase\nsimilarity task and a novel task for word-sense discrimination. We find that\nsingle-sense vector models perform as well or better than multi-sense vector\nmodels despite arguably less clean elementary representations. Our findings\nfurthermore show that simple composition functions such as pointwise addition\nare able to recover sense specific information from a single-sense vector model\nremarkably well.", "published": "2017-02-22 07:41:08", "link": "http://arxiv.org/abs/1702.06696v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Distillation for Controlling Specificity in Dialogue Generation", "abstract": "People speak at different levels of specificity in different situations.\nDepending on their knowledge, interlocutors, mood, etc.} A conversational agent\nshould have this ability and know when to be specific and when to be general.\nWe propose an approach that gives a neural network--based conversational agent\nthis ability. Our approach involves alternating between \\emph{data\ndistillation} and model training : removing training examples that are closest\nto the responses most commonly produced by the model trained from the last\nround and then retrain the model on the remaining dataset. Dialogue generation\nmodels trained with different degrees of data distillation manifest different\nlevels of specificity.\n  We then train a reinforcement learning system for selecting among this pool\nof generation models, to choose the best level of specificity for a given\ninput. Compared to the original generative model trained without distillation,\nthe proposed system is capable of generating more interesting and\nhigher-quality responses, in addition to appropriately adjusting specificity\ndepending on the context.\n  Our research constitutes a specific case of a broader approach involving\ntraining multiple subsystems from a single dataset distinguished by differences\nin a specific property one wishes to model. We show that from such a set of\nsubsystems, one can use reinforcement learning to build a system that tailors\nits output to different input contexts at test time.", "published": "2017-02-22 08:32:47", "link": "http://arxiv.org/abs/1702.06703v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Grained Entity Type Classification by Jointly Learning\n  Representations and Label Embeddings", "abstract": "Fine-grained entity type classification (FETC) is the task of classifying an\nentity mention to a broad set of types. Distant supervision paradigm is\nextensively used to generate training data for this task. However, generated\ntraining data assigns same set of labels to every mention of an entity without\nconsidering its local context. Existing FETC systems have two major drawbacks:\nassuming training data to be noise free and use of hand crafted features. Our\nwork overcomes both drawbacks. We propose a neural network model that jointly\nlearns entity mentions and their context representation to eliminate use of\nhand crafted features. Our model treats training data as noisy and uses\nnon-parametric variant of hinge loss function. Experiments show that the\nproposed model outperforms previous state-of-the-art methods on two publicly\navailable datasets, namely FIGER (GOLD) and BBN with an average relative\nimprovement of 2.69% in micro-F1 score. Knowledge learnt by our model on one\ndataset can be transferred to other datasets while using same model or other\nFETC systems. These approaches of transferring knowledge further improve the\nperformance of respective models.", "published": "2017-02-22 08:59:37", "link": "http://arxiv.org/abs/1702.06709v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving a Strong Neural Parser with Conjunction-Specific Features", "abstract": "While dependency parsers reach very high overall accuracy, some dependency\nrelations are much harder than others. In particular, dependency parsers\nperform poorly in coordination construction (i.e., correctly attaching the\n\"conj\" relation). We extend a state-of-the-art dependency parser with\nconjunction-specific features, focusing on the similarity between the conjuncts\nhead words. Training the extended parser yields an improvement in \"conj\"\nattachment as well as in overall dependency parsing accuracy on the Stanford\ndependency conversion of the Penn TreeBank.", "published": "2017-02-22 10:10:44", "link": "http://arxiv.org/abs/1702.06733v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Chinese SRL with Heterogeneous Annotations", "abstract": "Previous studies on Chinese semantic role labeling (SRL) have concentrated on\nsingle semantically annotated corpus. But the training data of single corpus is\noften limited. Meanwhile, there usually exists other semantically annotated\ncorpora for Chinese SRL scattered across different annotation frameworks. Data\nsparsity remains a bottleneck. This situation calls for larger training\ndatasets, or effective approaches which can take advantage of highly\nheterogeneous data. In these papers, we focus mainly on the latter, that is, to\nimprove Chinese SRL by using heterogeneous corpora together. We propose a novel\nprogressive learning model which augments the Progressive Neural Network with\nGated Recurrent Adapters. The model can accommodate heterogeneous inputs and\neffectively transfer knowledge between them. We also release a new corpus,\nChinese SemBank, for Chinese SRL. Experiments on CPB 1.0 show that ours model\noutperforms state-of-the-art methods.", "published": "2017-02-22 10:34:47", "link": "http://arxiv.org/abs/1702.06740v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tackling Error Propagation through Reinforcement Learning: A Case of\n  Greedy Dependency Parsing", "abstract": "Error propagation is a common problem in NLP. Reinforcement learning explores\nerroneous states during training and can therefore be more robust when mistakes\nare made early in a process. In this paper, we apply reinforcement learning to\ngreedy dependency parsing which is known to suffer from error propagation.\nReinforcement learning improves accuracy of both labeled and unlabeled\ndependencies of the Stanford Neural Dependency Parser, a high performance\ngreedy parser, while maintaining its efficiency. We investigate the portion of\nerrors which are the result of error propagation and confirm that reinforcement\nlearning reduces the occurrence of error propagation.", "published": "2017-02-22 13:49:18", "link": "http://arxiv.org/abs/1702.06794v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EVE: Explainable Vector Based Embedding Technique Using Wikipedia", "abstract": "We present an unsupervised explainable word embedding technique, called EVE,\nwhich is built upon the structure of Wikipedia. The proposed model defines the\ndimensions of a semantic vector representing a word using human-readable\nlabels, thereby it readily interpretable. Specifically, each vector is\nconstructed using the Wikipedia category graph structure together with the\nWikipedia article link structure. To test the effectiveness of the proposed\nword embedding model, we consider its usefulness in three fundamental tasks: 1)\nintruder detection - to evaluate its ability to identify a non-coherent vector\nfrom a list of coherent vectors, 2) ability to cluster - to evaluate its\ntendency to group related vectors together while keeping unrelated vectors in\nseparate clusters, and 3) sorting relevant items first - to evaluate its\nability to rank vectors (items) relevant to the query in the top order of the\nresult. For each task, we also propose a strategy to generate a task-specific\nhuman-interpretable explanation from the model. These demonstrate the overall\neffectiveness of the explainable embeddings generated by EVE. Finally, we\ncompare EVE with the Word2Vec, FastText, and GloVe embedding techniques across\nthe three tasks, and report improvements over the state-of-the-art.", "published": "2017-02-22 16:50:25", "link": "http://arxiv.org/abs/1702.06891v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Learning of Morphological Forests", "abstract": "This paper focuses on unsupervised modeling of morphological families,\ncollectively comprising a forest over the language vocabulary. This formulation\nenables us to capture edgewise properties reflecting single-step morphological\nderivations, along with global distributional properties of the entire forest.\nThese global properties constrain the size of the affix set and encourage\nformation of tight morphological families. The resulting objective is solved\nusing Integer Linear Programming (ILP) paired with contrastive estimation. We\ntrain the model by alternating between optimizing the local log-linear model\nand the global ILP objective. We evaluate our system on three tasks: root\ndetection, clustering of morphological families and segmentation. Our\nexperiments demonstrate that our model yields consistent gains in all three\ntasks compared with the best published results.", "published": "2017-02-22 21:44:02", "link": "http://arxiv.org/abs/1702.07015v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Feature Generation for Robust Semantic Role Labeling", "abstract": "Hand-engineered feature sets are a well understood method for creating robust\nNLP models, but they require a lot of expertise and effort to create. In this\nwork we describe how to automatically generate rich feature sets from simple\nunits called featlets, requiring less engineering. Using information gain to\nguide the generation process, we train models which rival the state of the art\non two standard Semantic Role Labeling datasets with almost no task or\nlinguistic insight.", "published": "2017-02-22 23:39:03", "link": "http://arxiv.org/abs/1702.07046v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset", "abstract": "Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.", "published": "2017-02-22 07:57:14", "link": "http://arxiv.org/abs/1703.10661v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Guided Deep List: Automating the Generation of Epidemiological Line\n  Lists from Open Sources", "abstract": "Real-time monitoring and responses to emerging public health threats rely on\nthe availability of timely surveillance data. During the early stages of an\nepidemic, the ready availability of line lists with detailed tabular\ninformation about laboratory-confirmed cases can assist epidemiologists in\nmaking reliable inferences and forecasts. Such inferences are crucial to\nunderstand the epidemiology of a specific disease early enough to stop or\ncontrol the outbreak. However, construction of such line lists requires\nconsiderable human supervision and therefore, difficult to generate in\nreal-time. In this paper, we motivate Guided Deep List, the first tool for\nbuilding automated line lists (in near real-time) from open source reports of\nemerging disease outbreaks. Specifically, we focus on deriving epidemiological\ncharacteristics of an emerging disease and the affected population from reports\nof illness. Guided Deep List uses distributed vector representations (ala\nword2vec) to discover a set of indicators for each line list feature. This\ndiscovery of indicators is followed by the use of dependency parsing based\ntechniques for final extraction in tabular form. We evaluate the performance of\nGuided Deep List against a human annotated line list provided by HealthMap\ncorresponding to MERS outbreaks in Saudi Arabia. We demonstrate that Guided\nDeep List extracts line list features with increased accuracy compared to a\nbaseline method. We further show how these automatically extracted line list\nfeatures can be used for making epidemiological inferences, such as inferring\ndemographics and symptoms-to-hospitalization period of affected individuals.", "published": "2017-02-22 03:14:36", "link": "http://arxiv.org/abs/1702.06663v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Discussion quality diffuses in the digital public square", "abstract": "Studies of online social influence have demonstrated that friends have\nimportant effects on many types of behavior in a wide variety of settings.\nHowever, we know much less about how influence works among relative strangers\nin digital public squares, despite important conversations happening in such\nspaces. We present the results of a study on large public Facebook pages where\nwe randomly used two different methods--most recent and social feedback--to\norder comments on posts. We find that the social feedback condition results in\nhigher quality viewed comments and response comments. After measuring the\naverage quality of comments written by users before the study, we find that\nsocial feedback has a positive effect on response quality for both low and high\nquality commenters. We draw on a theoretical framework of social norms to\nexplain this empirical result. In order to examine the influence mechanism\nfurther, we measure the similarity between comments viewed and written during\nthe study, finding that similarity increases for the highest quality\ncontributors under the social feedback condition. This suggests that, in\naddition to norms, some individuals may respond with increased relevance to\nhigh-quality comments.", "published": "2017-02-22 04:54:43", "link": "http://arxiv.org/abs/1702.06677v1", "categories": ["cs.CY", "cs.CL", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Task-driven Visual Saliency and Attention-based Visual Question\n  Answering", "abstract": "Visual question answering (VQA) has witnessed great progress since May, 2015\nas a classic problem unifying visual and textual data into a system. Many\nenlightening VQA works explore deep into the image and question encodings and\nfusing methods, of which attention is the most effective and infusive\nmechanism. Current attention based methods focus on adequate fusion of visual\nand textual features, but lack the attention to where people focus to ask\nquestions about the image. Traditional attention based methods attach a single\nvalue to the feature at each spatial location, which losses many useful\ninformation. To remedy these problems, we propose a general method to perform\nsaliency-like pre-selection on overlapped region features by the interrelation\nof bidirectional LSTM (BiLSTM), and use a novel element-wise multiplication\nbased attention method to capture more competent correlation information\nbetween visual and textual features. We conduct experiments on the large-scale\nCOCO-VQA dataset and analyze the effectiveness of our model demonstrated by\nstrong empirical results.", "published": "2017-02-22 08:19:38", "link": "http://arxiv.org/abs/1702.06700v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.CV"}
{"title": "Dialectometric analysis of language variation in Twitter", "abstract": "In the last few years, microblogging platforms such as Twitter have given\nrise to a deluge of textual data that can be used for the analysis of informal\ncommunication between millions of individuals. In this work, we propose an\ninformation-theoretic approach to geographic language variation using a corpus\nbased on Twitter. We test our models with tens of concepts and their associated\nkeywords detected in Spanish tweets geolocated in Spain. We employ\ndialectometric measures (cosine similarity and Jensen-Shannon divergence) to\nquantify the linguistic distance on the lexical level between cells created in\na uniform grid over the map. This can be done for a single concept or in the\ngeneral case taking into account an average of the considered variants. The\nlatter permits an analysis of the dialects that naturally emerge from the data.\nInterestingly, our results reveal the existence of two dialect macrovarieties.\nThe first group includes a region-specific speech spoken in small towns and\nrural areas whereas the second cluster encompasses cities that tend to use a\nmore uniform variety. Since the results obtained with the two different metrics\nqualitatively agree, our work suggests that social media corpora can be\nefficiently used for dialectometric analyses.", "published": "2017-02-22 12:42:06", "link": "http://arxiv.org/abs/1702.06777v1", "categories": ["cs.CL", "cs.IR", "cs.SI", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Triaging Content Severity in Online Mental Health Forums", "abstract": "Mental health forums are online communities where people express their issues\nand seek help from moderators and other users. In such forums, there are often\nposts with severe content indicating that the user is in acute distress and\nthere is a risk of attempted self-harm. Moderators need to respond to these\nsevere posts in a timely manner to prevent potential self-harm. However, the\nlarge volume of daily posted content makes it difficult for the moderators to\nlocate and respond to these critical posts. We present a framework for triaging\nuser content into four severity categories which are defined based on\nindications of self-harm ideation. Our models are based on a feature-rich\nclassification framework which includes lexical, psycholinguistic, contextual\nand topic modeling features. Our approaches improve the state of the art in\ntriaging the content severity in mental health forums by large margins (up to\n17% improvement over the F-1 scores). Using the proposed model, we analyze the\nmental state of users and we show that overall, long-term users of the forum\ndemonstrate a decreased severity of risk over time. Our analysis on the\ninteraction of the moderators with the users further indicates that without an\nautomatic way to identify critical content, it is indeed challenging for the\nmoderators to provide timely response to the users in need.", "published": "2017-02-22 16:14:12", "link": "http://arxiv.org/abs/1702.06875v1", "categories": ["cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "A new cosine series antialiasing function and its application to\n  aliasing-free glottal source models for speech and singing synthesis", "abstract": "We formulated and implemented a procedure to generate aliasing-free\nexcitation source signals. It uses a new antialiasing filter in the continuous\ntime domain followed by an IIR digital filter for response equalization. We\nintroduced a cosine-series-based general design procedure for the new\nantialiasing function. We applied this new procedure to implement the\nantialiased Fujisaki-Ljungqvist model. We also applied it to revise our\nprevious implementation of the antialiased Fant-Liljencrants model. A\ncombination of these signals and a lattice implementation of the time varying\nvocal tract model provides a reliable and flexible basis to test fo extractors\nand source aperiodicity analysis methods. MATLAB implementations of these\nantialiased excitation source models are available as part of our open source\ntools for speech science.", "published": "2017-02-22 09:36:48", "link": "http://arxiv.org/abs/1702.06724v4", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
