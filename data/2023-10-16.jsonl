{"title": "Empirical Study of Zero-Shot NER with ChatGPT", "abstract": "Large language models (LLMs) exhibited powerful capability in various natural\nlanguage processing tasks. This work focuses on exploring LLM performance on\nzero-shot information extraction, with a focus on the ChatGPT and named entity\nrecognition (NER) task. Inspired by the remarkable reasoning capability of LLM\non symbolic and arithmetic reasoning, we adapt the prevalent reasoning methods\nto NER and propose reasoning strategies tailored for NER. First, we explore a\ndecomposed question-answering paradigm by breaking down the NER task into\nsimpler subproblems by labels. Second, we propose syntactic augmentation to\nstimulate the model's intermediate thinking in two ways: syntactic prompting,\nwhich encourages the model to analyze the syntactic structure itself, and tool\naugmentation, which provides the model with the syntactic information generated\nby a parsing tool. Besides, we adapt self-consistency to NER by proposing a\ntwo-stage majority voting strategy, which first votes for the most consistent\nmentions, then the most consistent types. The proposed methods achieve\nremarkable improvements for zero-shot NER across seven benchmarks, including\nChinese and English datasets, and on both domain-specific and general-domain\nscenarios. In addition, we present a comprehensive analysis of the error types\nwith suggestions for optimization directions. We also verify the effectiveness\nof the proposed methods on the few-shot setting and other LLMs.", "published": "2023-10-16 03:40:03", "link": "http://arxiv.org/abs/2310.10035v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Large Language Model Fine-tuning for Solving Math Problems", "abstract": "Despite their success in many natural language tasks, solving math problems\nremains a significant challenge for large language models (LLMs). A large gap\nexists between LLMs' pass-at-one and pass-at-N performance in solving math\nproblems, suggesting LLMs might be close to finding correct solutions,\nmotivating our exploration of fine-tuning methods to unlock LLMs' performance.\nUsing the challenging MATH dataset, we investigate three fine-tuning\nstrategies: (1) solution fine-tuning, where we fine-tune to generate a detailed\nsolution for a given math problem; (2) solution-cluster re-ranking, where the\nLLM is fine-tuned as a solution verifier/evaluator to choose among generated\ncandidate solution clusters; (3) multi-task sequential fine-tuning, which\nintegrates both solution generation and evaluation tasks together efficiently\nto enhance the LLM performance. With these methods, we present a thorough\nempirical study on a series of PaLM 2 models and find: (1) The quality and\nstyle of the step-by-step solutions used for fine-tuning can make a significant\nimpact on the model performance; (2) While solution re-ranking and majority\nvoting are both effective for improving the model performance when used\nseparately, they can also be used together for an even greater performance\nboost; (3) Multi-task fine-tuning that sequentially separates the solution\ngeneration and evaluation tasks can offer improved performance compared with\nthe solution fine-tuning baseline. Guided by these insights, we design a\nfine-tuning recipe that yields approximately 58.8% accuracy on the MATH dataset\nwith fine-tuned PaLM 2-L models, an 11.2% accuracy improvement over the\nfew-shot performance of pre-trained PaLM 2-L model with majority voting.", "published": "2023-10-16 04:11:19", "link": "http://arxiv.org/abs/2310.10047v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt Packer: Deceiving LLMs through Compositional Instruction with\n  Hidden Attacks", "abstract": "Recently, Large language models (LLMs) with powerful general capabilities\nhave been increasingly integrated into various Web applications, while\nundergoing alignment training to ensure that the generated content aligns with\nuser intent and ethics. Unfortunately, they remain the risk of generating\nharmful content like hate speech and criminal activities in practical\napplications. Current approaches primarily rely on detecting, collecting, and\ntraining against harmful prompts to prevent such risks. However, they typically\nfocused on the \"superficial\" harmful prompts with a solitary intent, ignoring\ncomposite attack instructions with multiple intentions that can easily elicit\nharmful content in real-world scenarios. In this paper, we introduce an\ninnovative technique for obfuscating harmful instructions: Compositional\nInstruction Attacks (CIA), which refers to attacking by combination and\nencapsulation of multiple instructions. CIA hides harmful prompts within\ninstructions of harmless intentions, making it impossible for the model to\nidentify underlying malicious intentions. Furthermore, we implement two\ntransformation methods, known as T-CIA and W-CIA, to automatically disguise\nharmful instructions as talking or writing tasks, making them appear harmless\nto LLMs. We evaluated CIA on GPT-4, ChatGPT, and ChatGLM2 with two safety\nassessment datasets and two harmful prompt datasets. It achieves an attack\nsuccess rate of 95%+ on safety assessment datasets, and 83%+ for GPT-4, 91%+\nfor ChatGPT (gpt-3.5-turbo backed) and ChatGLM2-6B on harmful prompt datasets.\nOur approach reveals the vulnerability of LLMs to such compositional\ninstruction attacks that harbor underlying harmful intentions, contributing\nsignificantly to LLM security development. Warning: this paper may contain\noffensive or upsetting content!", "published": "2023-10-16 05:19:25", "link": "http://arxiv.org/abs/2310.10077v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Let's reward step by step: Step-Level reward model as the Navigators for\n  Reasoning", "abstract": "Recent years have seen considerable advancements in multi-step reasoning with\nLarge Language Models (LLMs). The previous studies have elucidated the merits\nof integrating feedback or search mechanisms during model inference to improve\nthe reasoning accuracy. The Process-Supervised Reward Model (PRM), typically\nfurnishes LLMs with step-by-step feedback during the training phase, akin to\nProximal Policy Optimization (PPO) or reject sampling. Our objective is to\nexamine the efficacy of PRM in the inference phase to help discern the optimal\nsolution paths for multi-step tasks such as mathematical reasoning and code\ngeneration. To this end, we propose a heuristic greedy search algorithm that\nemploys the step-level feedback from PRM to optimize the reasoning pathways\nexplored by LLMs. This tailored PRM demonstrated enhanced results compared to\nthe Chain of Thought (CoT) on mathematical benchmarks like GSM8K and MATH.\nAdditionally, to explore the versatility of our approach, we develop a novel\nmethod to automatically generate step-level reward dataset for coding tasks and\nobserved similar improved performance in the code generation tasks. Thus\nhighlighting the robust nature of our reward-model-based approach to inference\nfor reasoning tasks.", "published": "2023-10-16 05:21:50", "link": "http://arxiv.org/abs/2310.10080v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JMedLoRA:Medical Domain Adaptation on Japanese Large Language Models\n  using Instruction-tuning", "abstract": "In the ongoing wave of impact driven by large language models (LLMs) like\nChatGPT, the adaptation of LLMs to medical domain has emerged as a crucial\nresearch frontier. Since mainstream LLMs tend to be designed for\ngeneral-purpose applications, constructing a medical LLM through domain\nadaptation is a huge challenge. While instruction-tuning is used to fine-tune\nsome LLMs, its precise roles in domain adaptation remain unknown. Here we show\nthe contribution of LoRA-based instruction-tuning to performance in Japanese\nmedical question-answering tasks. In doing so, we employ a multifaceted\nevaluation for multiple-choice questions, including scoring based on \"Exact\nmatch\" and \"Gestalt distance\" in addition to the conventional accuracy. Our\nfindings suggest that LoRA-based instruction-tuning can partially incorporate\ndomain-specific knowledge into LLMs, with larger models demonstrating more\npronounced effects. Furthermore, our results underscore the potential of\nadapting English-centric models for Japanese applications in domain adaptation,\nwhile also highlighting the persisting limitations of Japanese-centric models.\nThis initiative represents a pioneering effort in enabling medical institutions\nto fine-tune and operate models without relying on external services.", "published": "2023-10-16 05:28:28", "link": "http://arxiv.org/abs/2310.10083v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decomposed Prompt Tuning via Low-Rank Reparameterization", "abstract": "While prompt tuning approaches have achieved competitive performance with\nhigh efficiency, we observe that they invariably employ the same initialization\nprocess, wherein the soft prompt is either randomly initialized or derived from\nan existing embedding vocabulary. In contrast to these conventional methods,\nthis study aims to investigate an alternative way to derive soft prompt. Our\nempirical studies show that the soft prompt typically exhibits a low intrinsic\nrank characteristic. With such observations, we propose decomposed prompt\ntuning, a novel approach that utilizes low-rank matrices to initialize the soft\nprompt. Through the low-rank reparameterization, our method significantly\nreduces the number of trainable parameters while maintaining effectiveness.\nExperimental results on the SuperGLUE benchmark in both high-resource and\nlow-resource scenarios demonstrate the effectiveness of the proposed method.", "published": "2023-10-16 05:56:06", "link": "http://arxiv.org/abs/2310.10094v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Rank Context for Named Entity Recognition Using a Synthetic\n  Dataset", "abstract": "While recent pre-trained transformer-based models can perform named entity\nrecognition (NER) with great accuracy, their limited range remains an issue\nwhen applied to long documents such as whole novels. To alleviate this issue, a\nsolution is to retrieve relevant context at the document level. Unfortunately,\nthe lack of supervision for such a task means one has to settle for\nunsupervised approaches. Instead, we propose to generate a synthetic context\nretrieval training dataset using Alpaca, an instructiontuned large language\nmodel (LLM). Using this dataset, we train a neural context retriever based on a\nBERT model that is able to find relevant context for NER. We show that our\nmethod outperforms several retrieval baselines for the NER task on an English\nliterary dataset composed of the first chapter of 40 books.", "published": "2023-10-16 06:53:12", "link": "http://arxiv.org/abs/2310.10118v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative\n  Language Models", "abstract": "Automated theorem proving (ATP) has become an appealing domain for exploring\nthe reasoning ability of the recent successful generative language models.\nHowever, current ATP benchmarks mainly focus on symbolic inference, but rarely\ninvolve the understanding of complex number combination reasoning. In this\nwork, we propose TRIGO, an ATP benchmark that not only requires a model to\nreduce a trigonometric expression with step-by-step proofs but also evaluates a\ngenerative LM's reasoning ability on formulas and its capability to manipulate,\ngroup, and factor number terms. We gather trigonometric expressions and their\nreduced forms from the web, annotate the simplification process manually, and\ntranslate it into the Lean formal language system. We then automatically\ngenerate additional examples from the annotated samples to expand the dataset.\nFurthermore, we develop an automatic generator based on Lean-Gym to create\ndataset splits of varying difficulties and distributions in order to thoroughly\nanalyze the model's generalization ability. Our extensive experiments show our\nproposed TRIGO poses a new challenge for advanced generative LM's including\nGPT-4 which is pre-trained on a considerable amount of open-source formal\ntheorem-proving language data, and provide a new tool to study the generative\nLM's ability on both formal and mathematical reasoning.", "published": "2023-10-16 08:42:39", "link": "http://arxiv.org/abs/2310.10180v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VIBE: Topic-Driven Temporal Adaptation for Twitter Classification", "abstract": "Language features are evolving in real-world social media, resulting in the\ndeteriorating performance of text classification in dynamics. To address this\nchallenge, we study temporal adaptation, where models trained on past data are\ntested in the future. Most prior work focused on continued pretraining or\nknowledge updating, which may compromise their performance on noisy social\nmedia data. To tackle this issue, we reflect feature change via modeling latent\ntopic evolution and propose a novel model, VIBE: Variational Information\nBottleneck for Evolutions. Concretely, we first employ two Information\nBottleneck (IB) regularizers to distinguish past and future topics. Then, the\ndistinguished topics work as adaptive features via multi-task training with\ntimestamp and class label prediction. In adaptive learning, VIBE utilizes\nretrieved unlabeled data from online streams created posterior to training data\ntime. Substantial Twitter experiments on three classification tasks show that\nour model, with only 3% of data, significantly outperforms previous\nstate-of-the-art continued-pretraining methods.", "published": "2023-10-16 08:53:57", "link": "http://arxiv.org/abs/2310.10191v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Repetition In Repetition Out: Towards Understanding Neural Text\n  Degeneration from the Data Perspective", "abstract": "There are a number of diverging hypotheses about the neural text degeneration\nproblem, i.e., generating repetitive and dull loops, which makes this problem\nboth interesting and confusing. In this work, we aim to advance our\nunderstanding by presenting a straightforward and fundamental explanation from\nthe data perspective. Our preliminary investigation reveals a strong\ncorrelation between the degeneration issue and the presence of repetitions in\ntraining data. Subsequent experiments also demonstrate that by selectively\ndropping out the attention to repetitive words in training data, degeneration\ncan be significantly minimized. Furthermore, our empirical analysis illustrates\nthat prior works addressing the degeneration issue from various standpoints,\nsuch as the high-inflow words, the likelihood objective, and the\nself-reinforcement phenomenon, can be interpreted by one simple explanation.\nThat is, penalizing the repetitions in training data is a common and\nfundamental factor for their effectiveness. Moreover, our experiments reveal\nthat penalizing the repetitions in training data remains critical even when\nconsidering larger model sizes and instruction tuning.", "published": "2023-10-16 09:35:42", "link": "http://arxiv.org/abs/2310.10226v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Interpretability using Human Similarity Judgements to Prune\n  Word Embeddings", "abstract": "Interpretability methods in NLP aim to provide insights into the semantics\nunderlying specific system architectures. Focusing on word embeddings, we\npresent a supervised-learning method that, for a given domain (e.g., sports,\nprofessions), identifies a subset of model features that strongly improve\nprediction of human similarity judgments. We show this method keeps only 20-40%\nof the original embeddings, for 8 independent semantic domains, and that it\nretains different feature sets across domains. We then present two approaches\nfor interpreting the semantics of the retained features. The first obtains the\nscores of the domain words (co-hyponyms) on the first principal component of\nthe retained embeddings, and extracts terms whose co-occurrence with the\nco-hyponyms tracks these scores' profile. This analysis reveals that humans\ndifferentiate e.g. sports based on how gender-inclusive and international they\nare. The second approach uses the retained sets as variables in a probing task\nthat predicts values along 65 semantically annotated dimensions for a dataset\nof 535 words. The features retained for professions are best at predicting\ncognitive, emotional and social dimensions, whereas features retained for\nfruits or vegetables best predict the gustation (taste) dimension. We discuss\nimplications for alignment between AI systems and human knowledge.", "published": "2023-10-16 10:38:49", "link": "http://arxiv.org/abs/2310.10262v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generative Calibration for In-context Learning", "abstract": "As one of the most exciting features of large language models (LLMs),\nin-context learning is a mixed blessing. While it allows users to\nfast-prototype a task solver with only a few training examples, the performance\nis generally sensitive to various configurations of the prompt such as the\nchoice or order of the training examples. In this paper, we for the first time\ntheoretically and empirically identify that such a paradox is mainly due to the\nlabel shift of the in-context model to the data distribution, in which LLMs\nshift the label marginal $p(y)$ while having a good label conditional $p(x|y)$.\nWith this understanding, we can simply calibrate the in-context predictive\ndistribution by adjusting the label marginal, which is estimated via\nMonte-Carlo sampling over the in-context model, i.e., generation of LLMs. We\ncall our approach as generative calibration. We conduct exhaustive experiments\nwith 12 text classification tasks and 12 LLMs scaling from 774M to 33B,\ngenerally find that the proposed method greatly and consistently outperforms\nthe ICL as well as state-of-the-art calibration methods, by up to 27% absolute\nin macro-F1. Meanwhile, the proposed method is also stable under different\nprompt configurations.", "published": "2023-10-16 10:45:02", "link": "http://arxiv.org/abs/2310.10266v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Stage Pre-training Enhanced by ChatGPT for Multi-Scenario\n  Multi-Domain Dialogue Summarization", "abstract": "Dialogue summarization involves a wide range of scenarios and domains.\nHowever, existing methods generally only apply to specific scenarios or\ndomains. In this study, we propose a new pre-trained model specifically\ndesigned for multi-scenario multi-domain dialogue summarization. It adopts a\nmulti-stage pre-training strategy to reduce the gap between the pre-training\nobjective and fine-tuning objective. Specifically, we first conduct\ndomain-aware pre-training using large-scale multi-scenario multi-domain\ndialogue data to enhance the adaptability of our pre-trained model. Then, we\nconduct task-oriented pre-training using large-scale multi-scenario\nmulti-domain \"dialogue-summary\" parallel data annotated by ChatGPT to enhance\nthe dialogue summarization ability of our pre-trained model. Experimental\nresults on three dialogue summarization datasets from different scenarios and\ndomains indicate that our pre-trained model significantly outperforms previous\nstate-of-the-art models in full fine-tuning, zero-shot, and few-shot settings.", "published": "2023-10-16 11:16:07", "link": "http://arxiv.org/abs/2310.10285v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Bias in Multilingual Language Models: Cross-Lingual\n  Transfer of Debiasing Techniques", "abstract": "This paper investigates the transferability of debiasing techniques across\ndifferent languages within multilingual models. We examine the applicability of\nthese techniques in English, French, German, and Dutch. Using multilingual BERT\n(mBERT), we demonstrate that cross-lingual transfer of debiasing techniques is\nnot only feasible but also yields promising results. Surprisingly, our findings\nreveal no performance disadvantages when applying these techniques to\nnon-English languages. Using translations of the CrowS-Pairs dataset, our\nanalysis identifies SentenceDebias as the best technique across different\nlanguages, reducing bias in mBERT by an average of 13%. We also find that\ndebiasing techniques with additional pretraining exhibit enhanced cross-lingual\neffectiveness for the languages included in the analyses, particularly in\nlower-resource languages. These novel insights contribute to a deeper\nunderstanding of bias mitigation in multilingual language models and provide\npractical guidance for debiasing techniques in different language contexts.", "published": "2023-10-16 11:43:30", "link": "http://arxiv.org/abs/2310.10310v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Untying the Reversal Curse via Bidirectional Language Model Editing", "abstract": "Recent studies have demonstrated that large language models (LLMs) store\nmassive factual knowledge within their parameters. But existing LLMs are prone\nto hallucinate unintended text due to false or outdated knowledge. Since\nretraining LLMs is resource intensive, there has been a growing interest in the\nconcept of model editing. Despite the emergence of benchmarks and approaches,\nthese unidirectional editing and evaluation have failed to explore the reversal\ncurse. Intuitively, if \"The capital of France is\" is edited to be a counterfact\n\"London\" within a model, then it should be able to naturally reason and recall\nthe reverse fact, i.e., \"London is the capital of\" followed by \"France\" instead\nof \"England\". In this paper, we study bidirectional language model editing,\naiming to provide rigorous model editing evaluation to assess if edited LLMs\ncan recall the editing knowledge bidirectionally. A new evaluation metric of\nreversibility is introduced, and a benchmark dubbed as Bidirectional Assessment\nfor Knowledge Editing (BAKE) is constructed to evaluate the reversibility of\nedited models in recalling knowledge in the reverse direction of editing. We\nsurprisingly observe that while current editing methods and LLMs can\neffectively recall editing facts in the direction of editing, they suffer\nserious deficiencies when evaluated in the reverse direction. To mitigate the\nreversal curse, a method named Bidirectionally Inversible Relationship moDeling\n(BIRD) is proposed. A set of editing objectives that incorporate bidirectional\nrelationships between subject and object into the updated model weights are\ndesigned. Experiments show that BIRD improves the performance of four\nrepresentative LLMs of different sizes via question answering and judgement.", "published": "2023-10-16 12:04:13", "link": "http://arxiv.org/abs/2310.10322v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual Data Augmentation for Task-Oriented Dialog Systems", "abstract": "Collection of annotated dialogs for training task-oriented dialog systems\nhave been one of the key bottlenecks in improving current models. While dialog\nresponse generation has been widely studied on the agent side, it is not\nevident if similar generative models can be used to generate a large variety\nof, and often unexpected, user inputs that real dialog systems encounter in\npractice. Existing data augmentation techniques such as paraphrase generation\ndo not take the dialog context into consideration. In this paper, we develop a\nnovel dialog augmentation model that generates a user turn, conditioning on\nfull dialog context. Additionally, with a new prompt design for language model,\nand output re-ranking, the dialogs generated from our model can be directly\nused to train downstream dialog systems. On common benchmark datasets MultiWoZ\nand SGD, we show that our dialog augmentation model generates high quality\ndialogs and improves dialog success rate by as much as $8\\%$ over baseline.", "published": "2023-10-16 13:22:34", "link": "http://arxiv.org/abs/2310.10380v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "$\\textit{Swap and Predict}$ -- Predicting the Semantic Changes in Words\n  across Corpora by Context Swapping", "abstract": "Meanings of words change over time and across domains. Detecting the semantic\nchanges of words is an important task for various NLP applications that must\nmake time-sensitive predictions. We consider the problem of predicting whether\na given target word, $w$, changes its meaning between two different text\ncorpora, $\\mathcal{C}_1$ and $\\mathcal{C}_2$. For this purpose, we propose\n$\\textit{Swapping-based Semantic Change Detection}$ (SSCD), an unsupervised\nmethod that randomly swaps contexts between $\\mathcal{C}_1$ and $\\mathcal{C}_2$\nwhere $w$ occurs. We then look at the distribution of contextualised word\nembeddings of $w$, obtained from a pretrained masked language model (MLM),\nrepresenting the meaning of $w$ in its occurrence contexts in $\\mathcal{C}_1$\nand $\\mathcal{C}_2$. Intuitively, if the meaning of $w$ does not change between\n$\\mathcal{C}_1$ and $\\mathcal{C}_2$, we would expect the distributions of\ncontextualised word embeddings of $w$ to remain the same before and after this\nrandom swapping process. Despite its simplicity, we demonstrate that even by\nusing pretrained MLMs without any fine-tuning, our proposed context swapping\nmethod accurately predicts the semantic changes of words in four languages\n(English, German, Swedish, and Latin) and across different time spans (over 50\nyears and about five years). Moreover, our method achieves significant\nperformance improvements compared to strong baselines for the English semantic\nchange prediction task. Source code is available at\nhttps://github.com/a1da4/svp-swap .", "published": "2023-10-16 13:39:44", "link": "http://arxiv.org/abs/2310.10397v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "G-SPEED: General SParse Efficient Editing MoDel", "abstract": "Large Language Models~(LLMs) have demonstrated incredible capabilities in\nunderstanding, generating, and manipulating languages. Through human-model\ninteractions, LLMs can automatically understand human-issued instructions and\noutput the expected contents, which can significantly increase working\nefficiency. In various types of real-world demands, editing-oriented tasks\naccount for a considerable proportion, which involves an interactive process\nthat entails the continuous refinement of existing texts to meet specific\ncriteria. Due to the need for multi-round human-model interaction and the\ngeneration of complicated editing tasks, there is an emergent need for\nefficient general editing models. In this paper, we propose\n\\underline{\\textbf{G}}eneral \\underline{\\textbf{SP}}arse\n\\underline{\\textbf{E}}fficient \\underline{\\textbf{E}}diting\nMo\\underline{\\textbf{D}}el~(\\textbf{G-SPEED}), which can fulfill diverse\nediting requirements through a single model while maintaining low computational\ncosts. Specifically, we first propose a novel unsupervised text editing data\nclustering algorithm to deal with the data scarcity problem. Subsequently, we\nintroduce a sparse editing model architecture to mitigate the inherently\nlimited learning capabilities of small language models. The experimental\noutcomes indicate that G-SPEED, with its 508M parameters, can surpass LLMs\nequipped with 175B parameters. Our code and model checkpoints are available at\n\\url{https://github.com/Banner-Z/G-SPEED}.", "published": "2023-10-16 15:01:18", "link": "http://arxiv.org/abs/2310.10480v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "xCOMET: Transparent Machine Translation Evaluation through Fine-grained\n  Error Detection", "abstract": "Widely used learned metrics for machine translation evaluation, such as COMET\nand BLEURT, estimate the quality of a translation hypothesis by providing a\nsingle sentence-level score. As such, they offer little insight into\ntranslation errors (e.g., what are the errors and what is their severity). On\nthe other hand, generative large language models (LLMs) are amplifying the\nadoption of more granular strategies to evaluation, attempting to detail and\ncategorize translation errors. In this work, we introduce xCOMET, an\nopen-source learned metric designed to bridge the gap between these approaches.\nxCOMET integrates both sentence-level evaluation and error span detection\ncapabilities, exhibiting state-of-the-art performance across all types of\nevaluation (sentence-level, system-level, and error span detection). Moreover,\nit does so while highlighting and categorizing error spans, thus enriching the\nquality assessment. We also provide a robustness analysis with stress tests,\nand show that xCOMET is largely capable of identifying localized critical\nerrors and hallucinations.", "published": "2023-10-16 15:03:14", "link": "http://arxiv.org/abs/2310.10482v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UNO-DST: Leveraging Unlabelled Data in Zero-Shot Dialogue State Tracking", "abstract": "Previous zero-shot dialogue state tracking (DST) methods only apply transfer\nlearning, ignoring unlabelled data in the target domain. We transform zero-shot\nDST into few-shot DST by utilising such unlabelled data via joint and\nself-training methods. Our method incorporates auxiliary tasks that generate\nslot types as inverse prompts for main tasks, creating slot values during joint\ntraining. Cycle consistency between these two tasks enables the generation and\nselection of quality samples in unknown target domains for subsequent\nfine-tuning. This approach also facilitates automatic label creation, thereby\noptimizing the training and fine-tuning of DST models. We demonstrate this\nmethod's effectiveness on general language models in zero-shot scenarios,\nimproving average joint goal accuracy by 8% across all domains in MultiWOZ.", "published": "2023-10-16 15:16:16", "link": "http://arxiv.org/abs/2310.10492v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Metric Ensembles For Hallucination Detection", "abstract": "Abstractive text summarization has garnered increased interest as of late, in\npart due to the proliferation of large language models (LLMs). One of the most\npressing problems related to generation of abstractive summaries is the need to\nreduce \"hallucinations,\" information that was not included in the document\nbeing summarized, and which may be wholly incorrect. Due to this need, a wide\narray of metrics estimating consistency with the text being summarized have\nbeen proposed. We examine in particular a suite of unsupervised metrics for\nsummary consistency, and measure their correlations with each other and with\nhuman evaluation scores in the wiki_bio_gpt3_hallucination dataset. We then\ncompare these evaluations to models made from a simple linear ensemble of these\nmetrics. We find that LLM-based methods outperform other unsupervised metrics\nfor hallucination detection. We also find that ensemble methods can improve\nthese scores even further, provided that the metrics in the ensemble have\nsufficiently similar and uncorrelated error rates. Finally, we present an\nensemble method for LLM-based evaluations that we show improves over this\nprevious SOTA.", "published": "2023-10-16 15:17:22", "link": "http://arxiv.org/abs/2310.10495v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One For All & All For One: Bypassing Hyperparameter Tuning with Model\n  Averaging For Cross-Lingual Transfer", "abstract": "Multilingual language models enable zero-shot cross-lingual transfer\n(ZS-XLT): fine-tuned on sizable source-language task data, they perform the\ntask in target languages without labeled instances. The effectiveness of ZS-XLT\nhinges on the linguistic proximity between languages and the amount of\npretraining data for a language. Because of this, model selection based on\nsource-language validation is unreliable: it picks model snapshots with\nsuboptimal target-language performance. As a remedy, some work optimizes ZS-XLT\nby extensively tuning hyperparameters: the follow-up work then routinely\nstruggles to replicate the original results. Other work searches over narrower\nhyperparameter grids, reporting substantially lower performance. In this work,\nwe therefore propose an unsupervised evaluation protocol for ZS-XLT that\ndecouples performance maximization from hyperparameter tuning. As a robust and\nmore transparent alternative to extensive hyperparameter tuning, we propose to\naccumulatively average snapshots from different runs into a single model. We\nrun broad ZS-XLT experiments on both higher-level semantic tasks (NLI,\nextractive QA) and a lower-level token classification task (NER) and find that\nconventional model selection based on source-language validation quickly\nplateaus to suboptimal ZS-XLT performance. On the other hand, our accumulative\nrun-by-run averaging of models trained with different hyperparameters boosts\nZS-XLT performance and closely correlates with \"oracle\" ZS-XLT, i.e., model\nselection based on target-language validation performance.", "published": "2023-10-16 15:50:34", "link": "http://arxiv.org/abs/2310.10532v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RegaVAE: A Retrieval-Augmented Gaussian Mixture Variational Auto-Encoder\n  for Language Modeling", "abstract": "Retrieval-augmented language models show promise in addressing issues like\noutdated information and hallucinations in language models (LMs). However,\ncurrent research faces two main problems: 1) determining what information to\nretrieve, and 2) effectively combining retrieved information during generation.\nWe argue that valuable retrieved information should not only be related to the\ncurrent source text but also consider the future target text, given the nature\nof LMs that model future tokens. Moreover, we propose that aggregation using\nlatent variables derived from a compact latent space is more efficient than\nutilizing explicit raw text, which is limited by context length and susceptible\nto noise. Therefore, we introduce RegaVAE, a retrieval-augmented language model\nbuilt upon the variational auto-encoder (VAE). It encodes the text corpus into\na latent space, capturing current and future information from both source and\ntarget text. Additionally, we leverage the VAE to initialize the latent space\nand adopt the probabilistic form of the retrieval generation paradigm by\nexpanding the Gaussian prior distribution into a Gaussian mixture distribution.\nTheoretical analysis provides an optimizable upper bound for RegaVAE.\nExperimental results on various datasets demonstrate significant improvements\nin text generation quality and hallucination removal.", "published": "2023-10-16 16:42:01", "link": "http://arxiv.org/abs/2310.10567v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Context Utilization in Summarization with Large Language Models", "abstract": "Large language models (LLMs) excel in abstractive summarization tasks,\ndelivering fluent and pertinent summaries. Recent advancements have extended\ntheir capabilities to handle long-input contexts, exceeding 100k tokens.\nHowever, in question answering, language models exhibit uneven utilization of\ntheir input context. They tend to favor the initial and final segments,\nresulting in a U-shaped performance pattern concerning where the answer is\nlocated within the input. This bias raises concerns, particularly in\nsummarization where crucial content may be dispersed throughout the source\ndocument(s). Besides, in summarization, mapping facts from the source to the\nsummary is not trivial as salient content is usually re-phrased. In this paper,\nwe conduct the first comprehensive study on context utilization and position\nbias in summarization. Our analysis encompasses 6 LLMs, 10 datasets, and 5\nevaluation metrics. We introduce a new evaluation benchmark called MiddleSum on\nthe which we benchmark two alternative inference methods to alleviate position\nbias: hierarchical summarization and incremental summarization. Our code and\ndata can be found here: https://github.com/ntunlp/MiddleSum.", "published": "2023-10-16 16:45:12", "link": "http://arxiv.org/abs/2310.10570v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mastering the Task of Open Information Extraction with Large Language\n  Models and Consistent Reasoning Environment", "abstract": "Open Information Extraction (OIE) aims to extract objective structured\nknowledge from natural texts, which has attracted growing attention to build\ndedicated models with human experience. As the large language models (LLMs)\nhave exhibited remarkable in-context learning capabilities, a question arises\nas to whether the task of OIE can be effectively tackled with this paradigm? In\nthis paper, we explore solving the OIE problem by constructing an appropriate\nreasoning environment for LLMs. Specifically, we first propose a method to\neffectively estimate the discrepancy of syntactic distribution between a LLM\nand test samples, which can serve as correlation evidence for preparing\npositive demonstrations. Upon the evidence, we introduce a simple yet effective\nmechanism to establish the reasoning environment for LLMs on specific tasks.\nWithout bells and whistles, experimental results on the standard CaRB benchmark\ndemonstrate that our $6$-shot approach outperforms state-of-the-art supervised\nmethod, achieving an $55.3$ $F_1$ score. Further experiments on TACRED and\nACE05 show that our method can naturally generalize to other information\nextraction tasks, resulting in improvements of $5.7$ and $6.8$ $F_1$ scores,\nrespectively.", "published": "2023-10-16 17:11:42", "link": "http://arxiv.org/abs/2310.10590v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Contamination Through the Lens of Time", "abstract": "Recent claims about the impressive abilities of large language models (LLMs)\nare often supported by evaluating publicly available benchmarks. Since LLMs\ntrain on wide swaths of the internet, this practice raises concerns of data\ncontamination, i.e., evaluating on examples that are explicitly or implicitly\nincluded in the training data. Data contamination remains notoriously\nchallenging to measure and mitigate, even with partial attempts like controlled\nexperimentation of training data, canary strings, or embedding similarities. In\nthis work, we conduct the first thorough longitudinal analysis of data\ncontamination in LLMs by using the natural experiment of training cutoffs in\nGPT models to look at benchmarks released over time. Specifically, we consider\ntwo code/mathematical problem-solving datasets, Codeforces and Project Euler,\nand find statistically significant trends among LLM pass rate vs. GitHub\npopularity and release date that provide strong evidence of contamination. By\nopen-sourcing our dataset, raw results, and evaluation framework, our work\npaves the way for rigorous analyses of data contamination in modern models. We\nconclude with a discussion of best practices and future steps for publicly\nreleasing benchmarks in the age of LLMs that train on webscale data.", "published": "2023-10-16 17:51:29", "link": "http://arxiv.org/abs/2310.10628v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Mistakes Help Us Grow\": Facilitating and Evaluating Growth Mindset\n  Supportive Language in Classrooms", "abstract": "Teachers' growth mindset supportive language (GMSL)--rhetoric emphasizing\nthat one's skills can be improved over time--has been shown to significantly\nreduce disparities in academic achievement and enhance students' learning\noutcomes. Although teachers espouse growth mindset principles, most find it\ndifficult to adopt GMSL in their practice due the lack of effective coaching in\nthis area. We explore whether large language models (LLMs) can provide\nautomated, personalized coaching to support teachers' use of GMSL. We establish\nan effective coaching tool to reframe unsupportive utterances to GMSL by\ndeveloping (i) a parallel dataset containing GMSL-trained teacher reframings of\nunsupportive statements with an accompanying annotation guide, (ii) a GMSL\nprompt framework to revise teachers' unsupportive language, and (iii) an\nevaluation framework grounded in psychological theory for evaluating GMSL with\nthe help of students and teachers. We conduct a large-scale evaluation\ninvolving 174 teachers and 1,006 students, finding that both teachers and\nstudents perceive GMSL-trained teacher and model reframings as more effective\nin fostering a growth mindset and promoting challenge-seeking behavior, among\nother benefits. We also find that model-generated reframings outperform those\nfrom the GMSL-trained teachers. These results show promise for harnessing LLMs\nto provide automated GMSL feedback for teachers and, more broadly, LLMs'\npotentiality for supporting students' learning in the classroom. Our findings\nalso demonstrate the benefit of large-scale human evaluations when applying\nLLMs in educational domains.", "published": "2023-10-16 17:56:07", "link": "http://arxiv.org/abs/2310.10637v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging Code Semantic and LLMs: Semantic Chain-of-Thought Prompting for\n  Code Generation", "abstract": "Large language models (LLMs) have showcased remarkable prowess in code\ngeneration. However, automated code generation is still challenging since it\nrequires a high-level semantic mapping between natural language requirements\nand codes. Most existing LLMs-based approaches for code generation rely on\ndecoder-only causal language models often treate codes merely as plain text\ntokens, i.e., feeding the requirements as a prompt input, and outputing code as\nflat sequence of tokens, potentially missing the rich semantic features\ninherent in source code. To bridge this gap, this paper proposes the \"Semantic\nChain-of-Thought\" approach to intruduce semantic information of code, named\nSeCoT. Our motivation is that the semantic information of the source code (\\eg\ndata flow and control flow) describes more precise program execution behavior,\nintention and function. By guiding LLM consider and integrate semantic\ninformation, we can achieve a more granular understanding and representation of\ncode, enhancing code generation accuracy. Meanwhile, while traditional\ntechniques leveraging such semantic information require complex static or\ndynamic code analysis to obtain features such as data flow and control flow,\nSeCoT demonstrates that this process can be fully automated via the intrinsic\ncapabilities of LLMs (i.e., in-context learning), while being generalizable and\napplicable to challenging domains. While SeCoT can be applied with different\nLLMs, this paper focuses on the powerful GPT-style models: ChatGPT(close-source\nmodel) and WizardCoder(open-source model). The experimental study on three\npopular DL benchmarks (i.e., HumanEval, HumanEval-ET and MBPP) shows that SeCoT\ncan achieves state-of-the-art performance, greatly improving the potential for\nlarge models and code generation.", "published": "2023-10-16 05:09:58", "link": "http://arxiv.org/abs/2310.10698v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building Persona Consistent Dialogue Agents with Offline Reinforcement\n  Learning", "abstract": "Maintaining a consistent persona is a key quality for any open domain\ndialogue system. Current state-of-the-art systems do this by training agents\nwith supervised learning or online reinforcement learning (RL). However,\nsystems trained with supervised learning often lack consistency as they are\nnever punished for uttering contradictions. Additional training with RL can\nalleviate some of these issues, however the training process is expensive.\nInstead, we propose an offline RL framework to improve the persona consistency\nof dialogue systems. Our framework allows us to combine the advantages of\nprevious methods as we can inexpensively train our model on existing data as in\nsupervised learning, while punishing and rewarding specific utterances as in\nRL. We also introduce a simple importance sampling method to reduce the\nvariance of importance weights in offline RL training which we call\nVariance-Reducing MLE-Initialized (VaRMI) importance sampling. Our automatic\nand human evaluations show that our framework improves both the persona\nconsistency and dialogue quality of a state-of-the-art social chatbot.", "published": "2023-10-16 18:05:54", "link": "http://arxiv.org/abs/2310.10735v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BanglaNLP at BLP-2023 Task 1: Benchmarking different Transformer Models\n  for Violence Inciting Text Detection in Bengali", "abstract": "This paper presents the system that we have developed while solving this\nshared task on violence inciting text detection in Bangla. We explain both the\ntraditional and the recent approaches that we have used to make our models\nlearn. Our proposed system helps to classify if the given text contains any\nthreat. We studied the impact of data augmentation when there is a limited\ndataset available. Our quantitative results show that finetuning a\nmultilingual-e5-base model performed the best in our task compared to other\ntransformer-based architectures. We obtained a macro F1 of 68.11\\% in the test\nset and our performance in this shared task is ranked at 23 in the leaderboard.", "published": "2023-10-16 19:35:04", "link": "http://arxiv.org/abs/2310.10781v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fake News in Sheep's Clothing: Robust Fake News Detection Against\n  LLM-Empowered Style Attacks", "abstract": "It is commonly perceived that fake news and real news exhibit distinct\nwriting styles, such as the use of sensationalist versus objective language.\nHowever, we emphasize that style-related features can also be exploited for\nstyle-based attacks. Notably, the advent of powerful Large Language Models\n(LLMs) has empowered malicious actors to mimic the style of trustworthy news\nsources, doing so swiftly, cost-effectively, and at scale. Our analysis reveals\nthat LLM-camouflaged fake news content significantly undermines the\neffectiveness of state-of-the-art text-based detectors (up to 38% decrease in\nF1 Score), implying a severe vulnerability to stylistic variations. To address\nthis, we introduce SheepDog, a style-robust fake news detector that prioritizes\ncontent over style in determining news veracity. SheepDog achieves this\nresilience through (1) LLM-empowered news reframings that inject style\ndiversity into the training process by customizing articles to match different\nstyles; (2) a style-agnostic training scheme that ensures consistent veracity\npredictions across style-diverse reframings; and (3) content-focused veracity\nattributions that distill content-centric guidelines from LLMs for debunking\nfake news, offering supplementary cues and potential intepretability that\nassist veracity prediction. Extensive experiments on three real-world\nbenchmarks demonstrate SheepDog's style robustness and adaptability to various\nbackbones.", "published": "2023-10-16 21:05:12", "link": "http://arxiv.org/abs/2310.10830v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Will the Prince Get True Love's Kiss? On the Model Sensitivity to Gender\n  Perturbation over Fairytale Texts", "abstract": "In this paper, we study whether language models are affected by learned\ngender stereotypes during the comprehension of stories. Specifically, we\ninvestigate how models respond to gender stereotype perturbations through\ncounterfactual data augmentation. Focusing on Question Answering (QA) tasks in\nfairytales, we modify the FairytaleQA dataset by swapping gendered character\ninformation and introducing counterfactual gender stereotypes during training.\nThis allows us to assess model robustness and examine whether learned biases\ninfluence story comprehension. Our results show that models exhibit slight\nperformance drops when faced with gender perturbations in the test set,\nindicating sensitivity to learned stereotypes. However, when fine-tuned on\ncounterfactual training data, models become more robust to anti-stereotypical\nnarratives. Additionally, we conduct a case study demonstrating how\nincorporating counterfactual anti-stereotype examples can improve inclusivity\nin downstream applications.", "published": "2023-10-16 22:25:09", "link": "http://arxiv.org/abs/2310.10865v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IDEAL: Influence-Driven Selective Annotations Empower In-Context\n  Learners in Large Language Models", "abstract": "In-context learning is a promising paradigm that utilizes in-context examples\nas prompts for the predictions of large language models. These prompts are\ncrucial for achieving strong performance. However, since the prompts need to be\nsampled from a large volume of annotated examples, finding the right prompt may\nresult in high annotation costs. To address this challenge, this paper\nintroduces an influence-driven selective annotation method that aims to\nminimize annotation costs while improving the quality of in-context examples.\nThe essence of our method is to select a pivotal subset from a large-scale\nunlabeled data pool to annotate for the subsequent sampling of prompts.\nSpecifically, a directed graph is first constructed to represent unlabeled\ndata. Afterward, the influence of candidate unlabeled subsets is quantified\nwith a diffusion process. A simple yet effective greedy algorithm for unlabeled\ndata selection is lastly introduced. It iteratively selects the data if it\nprovides a maximum marginal gain with respect to quantified influence. Compared\nwith previous efforts on selective annotations, our influence-driven method\nworks in an end-to-end manner, avoids an intractable explicit balance between\ndata diversity and representativeness, and enjoys theoretical support.\nExperiments confirm the superiority of the proposed method on various\nbenchmarks, achieving better performance under lower time consumption during\nsubset selection. The project page is available at\nhttps://skzhang1.github.io/IDEAL/.", "published": "2023-10-16 22:53:54", "link": "http://arxiv.org/abs/2310.10873v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Evaluation of Tool-Assisted Generation Strategies", "abstract": "A growing area of research investigates augmenting language models with tools\n(e.g., search engines, calculators) to overcome their shortcomings (e.g.,\nmissing or incorrect knowledge, incorrect logical inferences). Various few-shot\ntool-usage strategies have been proposed. However, there is no systematic and\nfair comparison across different strategies, or between these strategies and\nstrong baselines that do not leverage tools. We conduct an extensive empirical\nanalysis, finding that (1) across various datasets, example difficulty levels,\nand models, strong no-tool baselines are competitive to tool-assisted\nstrategies, implying that effectively using tools with in-context\ndemonstrations is a difficult unsolved problem; (2) for knowledge-retrieval\ntasks, strategies that *refine* incorrect outputs with tools outperform\nstrategies that retrieve relevant information *ahead of* or *during\ngeneration*; (3) tool-assisted strategies are expensive in the number of tokens\nthey require to work -- incurring additional costs by orders of magnitude --\nwhich does not translate into significant improvement in performance. Overall,\nour findings suggest that few-shot tool integration is still an open challenge,\nemphasizing the need for comprehensive evaluations of future strategies to\naccurately assess their *benefits* and *costs*.", "published": "2023-10-16 04:53:22", "link": "http://arxiv.org/abs/2310.10062v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fine-tuning ChatGPT for Automatic Scoring", "abstract": "This study highlights the potential of fine-tuned ChatGPT (GPT-3.5) for\nautomatically scoring student written constructed responses using example\nassessment tasks in science education. Recent studies on OpenAI's generative\nmodel GPT-3.5 proved its superiority in predicting the natural language with\nhigh accuracy and human-like responses. GPT-3.5 has been trained over enormous\nonline language materials such as journals and Wikipedia; therefore, more than\ndirect usage of pre-trained GPT-3.5 is required for automatic scoring as\nstudents utilize a different language than trained material. These imply that a\ndomain-specific model, fine-tuned over data for specific tasks, can enhance\nmodel performance. In this study, we fine-tuned GPT-3.5 on six assessment tasks\nwith a diverse dataset of middle-school and high-school student responses and\nexpert scoring. The six tasks comprise two multi-label and four multi-class\nassessment tasks. We compare the performance of fine-tuned GPT-3.5 with the\nfine-tuned state-of-the-art Google's generated language model, BERT. The\nresults show that in-domain training corpora constructed from science questions\nand responses for BERT achieved average accuracy = 0.838, SD = 0.069. GPT-3.5\nshows a remarkable average increase (9.1%) in automatic scoring accuracy (mean\n= 9.15, SD = 0.042) for the six tasks, p =0.001 < 0.05. Specifically, for\nmulti-label tasks (item 1 with 5 labels; item 2 with 10 labels), GPT-3.5\nachieved significantly higher scoring accuracy than BERT across all the labels,\nwith the second item achieving a 7.1% increase. The average scoring increase\nfor the four multi-class items for GPT-3.5 was 10.6% compared to BERT. Our\nstudy confirmed the effectiveness of fine-tuned GPT-3.5 for automatic scoring\nof student responses on domain-specific data in education with high accuracy.\nWe have released fine-tuned models for public use and community engagement.", "published": "2023-10-16 05:09:16", "link": "http://arxiv.org/abs/2310.10072v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Verbosity Bias in Preference Labeling by Large Language Models", "abstract": "In recent years, Large Language Models (LLMs) have witnessed a remarkable\nsurge in prevalence, altering the landscape of natural language processing and\nmachine learning. One key factor in improving the performance of LLMs is\nalignment with humans achieved with Reinforcement Learning from Human Feedback\n(RLHF), as for many LLMs such as GPT-4, Bard, etc. In addition, recent studies\nare investigating the replacement of human feedback with feedback from other\nLLMs named Reinforcement Learning from AI Feedback (RLAIF). We examine the\nbiases that come along with evaluating LLMs with other LLMs and take a closer\nlook into verbosity bias -- a bias where LLMs sometimes prefer more verbose\nanswers even if they have similar qualities. We see that in our problem\nsetting, GPT-4 prefers longer answers more than humans. We also propose a\nmetric to measure this bias.", "published": "2023-10-16 05:19:02", "link": "http://arxiv.org/abs/2310.10076v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Character-LLM: A Trainable Agent for Role-Playing", "abstract": "Large language models (LLMs) can be used to serve as agents to simulate human\nbehaviors, given the powerful ability to understand human instructions and\nprovide high-quality generated texts. Such ability stimulates us to wonder\nwhether LLMs can simulate a person in a higher form than simple human\nbehaviors. Therefore, we aim to train an agent with the profile, experience,\nand emotional states of a specific person instead of using limited prompts to\ninstruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs\nto act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar,\netc. Our method focuses on editing profiles as experiences of a certain\ncharacter and training models to be personal simulacra with these experiences.\nTo assess the effectiveness of our approach, we build a test playground that\ninterviews trained agents and evaluates whether the agents \\textit{memorize}\ntheir characters and experiences. Experimental results show interesting\nobservations that help build future simulacra of humankind.", "published": "2023-10-16 07:58:56", "link": "http://arxiv.org/abs/2310.10158v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Battle of the Large Language Models: Dolly vs LLaMA vs Vicuna vs Guanaco\n  vs Bard vs ChatGPT -- A Text-to-SQL Parsing Comparison", "abstract": "The success of ChatGPT has ignited an AI race, with researchers striving to\ndevelop new large language models (LLMs) that can match or surpass the language\nunderstanding and generation abilities of commercial ones. In recent times, a\nnumber of models have emerged, claiming performance near that of GPT-3.5 or\nGPT-4 through various instruction-tuning methods. As practitioners of\nText-to-SQL parsing, we are grateful for their valuable contributions to\nopen-source research. However, it is important to approach these claims with a\nsense of scrutiny and ascertain the actual effectiveness of these models.\nTherefore, we pit six popular large language models against each other,\nsystematically evaluating their Text-to-SQL parsing capability on nine\nbenchmark datasets with five different prompting strategies, covering both\nzero-shot and few-shot scenarios. Regrettably, the open-sourced models fell\nsignificantly short of the performance achieved by closed-source models like\nGPT-3.5, highlighting the need for further work to bridge the performance gap\nbetween these models.", "published": "2023-10-16 08:52:41", "link": "http://arxiv.org/abs/2310.10190v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AdaLomo: Low-memory Optimization with Adaptive Learning Rate", "abstract": "Large language models have achieved remarkable success, but their extensive\nparameter size necessitates substantial memory for training, thereby setting a\nhigh threshold. While the recently proposed low-memory optimization (LOMO)\nreduces memory footprint, its optimization technique, akin to stochastic\ngradient descent, is sensitive to hyper-parameters and exhibits suboptimal\nconvergence, failing to match the performance of the prevailing optimizer for\nlarge language models, AdamW. Through empirical analysis of the Adam optimizer,\nwe found that, compared to momentum, the adaptive learning rate is more\ncritical for bridging the gap. Building on this insight, we introduce the\nlow-memory optimization with adaptive learning rate (AdaLomo), which offers an\nadaptive learning rate for each parameter. To maintain memory efficiency, we\nemploy non-negative matrix factorization for the second-order moment estimation\nin the optimizer state. Additionally, we suggest the use of a grouped update\nnormalization to stabilize convergence. Our experiments with instruction-tuning\nand further pre-training demonstrate that AdaLomo achieves results on par with\nAdamW, while significantly reducing memory requirements, thereby lowering the\nhardware barrier to training large language models. The code is accessible at\nhttps://github.com/OpenLMLab/LOMO.", "published": "2023-10-16 09:04:28", "link": "http://arxiv.org/abs/2310.10195v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Key-phrase boosted unsupervised summary generation for FinTech\n  organization", "abstract": "With the recent advances in social media, the use of NLP techniques in social\nmedia data analysis has become an emerging research direction. Business\norganizations can particularly benefit from such an analysis of social media\ndiscourse, providing an external perspective on consumer behavior. Some of the\nNLP applications such as intent detection, sentiment classification, text\nsummarization can help FinTech organizations to utilize the social media\nlanguage data to find useful external insights and can be further utilized for\ndownstream NLP tasks. Particularly, a summary which highlights the intents and\nsentiments of the users can be very useful for these organizations to get an\nexternal perspective. This external perspective can help organizations to\nbetter manage their products, offers, promotional campaigns, etc. However,\ncertain challenges, such as a lack of labeled domain-specific datasets impede\nfurther exploration of these tasks in the FinTech domain. To overcome these\nchallenges, we design an unsupervised phrase-based summary generation from\nsocial media data, using 'Action-Object' pairs (intent phrases). We evaluated\nthe proposed method with other key-phrase based summary generation methods in\nthe direction of contextual information of various Reddit discussion threads,\navailable in the different summaries. We introduce certain \"Context Metrics\"\nsuch as the number of Unique words, Action-Object pairs, and Noun chunks to\nevaluate the contextual information retrieved from the source text in these\nphrase-based summaries. We demonstrate that our methods significantly\noutperform the baseline on these metrics, thus providing a qualitative and\nquantitative measure of their efficacy. Proposed framework has been leveraged\nas a web utility portal hosted within Amex.", "published": "2023-10-16 11:30:47", "link": "http://arxiv.org/abs/2310.10294v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Interpreting and Exploiting Functional Specialization in Multi-Head\n  Attention under Multi-task Learning", "abstract": "Transformer-based models, even though achieving super-human performance on\nseveral downstream tasks, are often regarded as a black box and used as a\nwhole. It is still unclear what mechanisms they have learned, especially their\ncore module: multi-head attention. Inspired by functional specialization in the\nhuman brain, which helps to efficiently handle multiple tasks, this work\nattempts to figure out whether the multi-head attention module will evolve\nsimilar function separation under multi-tasking training. If it is, can this\nmechanism further improve the model performance? To investigate these\nquestions, we introduce an interpreting method to quantify the degree of\nfunctional specialization in multi-head attention. We further propose a simple\nmulti-task training method to increase functional specialization and mitigate\nnegative information transfer in multi-task learning. Experimental results on\nseven pre-trained transformer models have demonstrated that multi-head\nattention does evolve functional specialization phenomenon after multi-task\ntraining which is affected by the similarity of tasks. Moreover, the multi-task\ntraining strategy based on functional specialization boosts performance in both\nmulti-task learning and transfer learning without adding any parameters.", "published": "2023-10-16 11:55:53", "link": "http://arxiv.org/abs/2310.10318v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tabular Representation, Noisy Operators, and Impacts on Table Structure\n  Understanding Tasks in LLMs", "abstract": "Large language models (LLMs) are increasingly applied for tabular tasks using\nin-context learning. The prompt representation for a table may play a role in\nthe LLMs ability to process the table. Inspired by prior work, we generate a\ncollection of self-supervised structural tasks (e.g. navigate to a cell and\nrow; transpose the table) and evaluate the performance differences when using 8\nformats. In contrast to past work, we introduce 8 noise operations inspired by\nreal-world messy data and adversarial inputs, and show that such operations can\nimpact LLM performance across formats for different structural understanding\ntasks.", "published": "2023-10-16 12:51:24", "link": "http://arxiv.org/abs/2310.10358v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Privacy in Large Language Models: Attacks, Defenses and Future\n  Directions", "abstract": "The advancement of large language models (LLMs) has significantly enhanced\nthe ability to effectively tackle various downstream NLP tasks and unify these\ntasks into generative pipelines. On the one hand, powerful language models,\ntrained on massive textual data, have brought unparalleled accessibility and\nusability for both models and users. On the other hand, unrestricted access to\nthese models can also introduce potential malicious and unintentional privacy\nrisks. Despite ongoing efforts to address the safety and privacy concerns\nassociated with LLMs, the problem remains unresolved. In this paper, we provide\na comprehensive analysis of the current privacy attacks targeting LLMs and\ncategorize them according to the adversary's assumed capabilities to shed light\non the potential vulnerabilities present in LLMs. Then, we present a detailed\noverview of prominent defense strategies that have been developed to counter\nthese privacy attacks. Beyond existing works, we identify upcoming privacy\nconcerns as LLMs evolve. Lastly, we point out several potential avenues for\nfuture exploration.", "published": "2023-10-16 13:23:54", "link": "http://arxiv.org/abs/2310.10383v2", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Towards a Better Understanding of Variations in Zero-Shot Neural Machine\n  Translation Performance", "abstract": "Multilingual Neural Machine Translation (MNMT) facilitates knowledge sharing\nbut often suffers from poor zero-shot (ZS) translation qualities. While prior\nwork has explored the causes of overall low ZS performance, our work introduces\na fresh perspective: the presence of high variations in ZS performance. This\nsuggests that MNMT does not uniformly exhibit poor ZS capability; instead,\ncertain translation directions yield reasonable results. Through systematic\nexperimentation involving 1,560 language directions spanning 40 languages, we\nidentify three key factors contributing to high variations in ZS NMT\nperformance: 1) target side translation capability 2) vocabulary overlap 3)\nlinguistic properties. Our findings highlight that the target side translation\nquality is the most influential factor, with vocabulary overlap consistently\nimpacting ZS performance. Additionally, linguistic properties, such as language\nfamily and writing system, play a role, particularly with smaller models.\nFurthermore, we suggest that the off-target issue is a symptom of inadequate ZS\nperformance, emphasizing that zero-shot translation challenges extend beyond\naddressing the off-target problem. We release the data and models serving as a\nbenchmark to study zero-shot for future research at\nhttps://github.com/Smu-Tan/ZS-NMT-Variations", "published": "2023-10-16 13:26:05", "link": "http://arxiv.org/abs/2310.10385v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MechGPT, a language-based strategy for mechanics and materials modeling\n  that connects knowledge across scales, disciplines and modalities", "abstract": "For centuries, researchers have sought out ways to connect disparate areas of\nknowledge. While early scholars (Galileo, da Vinci, etc.) were experts across\nfields, specialization has taken hold later. With the advent of Artificial\nIntelligence, we can now explore relationships across areas (e.g.,\nmechanics-biology) or disparate domains (e.g., failure mechanics-art). To\nachieve this, we use a fine-tuned Large Language Model (LLM), here for a subset\nof knowledge in multiscale materials failure. The approach includes the use of\na general-purpose LLM to distill question-answer pairs from raw sources\nfollowed by LLM fine-tuning. The resulting MechGPT LLM foundation model is used\nin a series of computational experiments to explore its capacity for knowledge\nretrieval, various language tasks, hypothesis generation, and connecting\nknowledge across disparate areas. While the model has some ability to recall\nknowledge from training, we find that LLMs are particularly useful to extract\nstructural insights through Ontological Knowledge Graphs. These interpretable\ngraph structures provide explanatory insights, frameworks for new research\nquestions, and visual representations of knowledge that also can be used in\nretrieval-augmented generation. Three versions of MechGPT are discussed,\nfeaturing different sizes from 13 billion to 70 billion parameters, and\nreaching context lengths of more than 10,000 tokens. This provides ample\ncapacity for sophisticated retrieval augmented strategies, as well as\nagent-based modeling where multiple LLMs interact collaboratively and/or\nadversarially, the incorporation of new data from the literature or web\nsearches, as well as multimodality.", "published": "2023-10-16 14:29:35", "link": "http://arxiv.org/abs/2310.10445v1", "categories": ["cs.CL", "cond-mat.mtrl-sci"], "primary_category": "cs.CL"}
{"title": "Stance Detection with Collaborative Role-Infused LLM-Based Agents", "abstract": "Stance detection automatically detects the stance in a text towards a target,\nvital for content analysis in web and social media research. Despite their\npromising capabilities, LLMs encounter challenges when directly applied to\nstance detection. First, stance detection demands multi-aspect knowledge, from\ndeciphering event-related terminologies to understanding the expression styles\nin social media platforms. Second, stance detection requires advanced reasoning\nto infer authors' implicit viewpoints, as stance are often subtly embedded\nrather than overtly stated in the text. To address these challenges, we design\na three-stage framework COLA (short for Collaborative rOle-infused LLM-based\nAgents) in which LLMs are designated distinct roles, creating a collaborative\nsystem where each role contributes uniquely. Initially, in the multidimensional\ntext analysis stage, we configure the LLMs to act as a linguistic expert, a\ndomain specialist, and a social media veteran to get a multifaceted analysis of\ntexts, thus overcoming the first challenge. Next, in the reasoning-enhanced\ndebating stage, for each potential stance, we designate a specific LLM-based\nagent to advocate for it, guiding the LLM to detect logical connections between\ntext features and stance, tackling the second challenge. Finally, in the stance\nconclusion stage, a final decision maker agent consolidates prior insights to\ndetermine the stance. Our approach avoids extra annotated data and model\ntraining and is highly usable. We achieve state-of-the-art performance across\nmultiple datasets. Ablation studies validate the effectiveness of each design\nrole in handling stance detection. Further experiments have demonstrated the\nexplainability and the versatility of our approach. Our approach excels in\nusability, accuracy, effectiveness, explainability and versatility,\nhighlighting its value.", "published": "2023-10-16 14:46:52", "link": "http://arxiv.org/abs/2310.10467v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications\n  with Programmable Rails", "abstract": "NeMo Guardrails is an open-source toolkit for easily adding programmable\nguardrails to LLM-based conversational systems. Guardrails (or rails for short)\nare a specific way of controlling the output of an LLM, such as not talking\nabout topics considered harmful, following a predefined dialogue path, using a\nparticular language style, and more. There are several mechanisms that allow\nLLM providers and developers to add guardrails that are embedded into a\nspecific model at training, e.g. using model alignment. Differently, using a\nruntime inspired from dialogue management, NeMo Guardrails allows developers to\nadd programmable rails to LLM applications - these are user-defined,\nindependent of the underlying LLM, and interpretable. Our initial results show\nthat the proposed approach can be used with several LLM providers to develop\ncontrollable and safe LLM applications using programmable rails.", "published": "2023-10-16 15:20:30", "link": "http://arxiv.org/abs/2310.10501v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ViPE: Visualise Pretty-much Everything", "abstract": "Figurative and non-literal expressions are profoundly integrated in human\ncommunication. Visualising such expressions allow us to convey our creative\nthoughts, and evoke nuanced emotions. Recent text-to-image models like Stable\nDiffusion, on the other hand, struggle to depict non-literal expressions.\nRecent works primarily deal with this issue by compiling humanly annotated\ndatasets on a small scale, which not only demands specialised expertise but\nalso proves highly inefficient. To address this issue, we introduce ViPE:\nVisualise Pretty-much Everything. ViPE offers a series of lightweight and\nrobust language models that have been trained on a large-scale set of lyrics\nwith noisy visual descriptions that represent their implicit meaning. The\nsynthetic visual descriptions are generated by GPT3.5 relying on neither human\nannotations nor images. ViPE effectively expresses any arbitrary piece of text\ninto a visualisable description, enabling meaningful and high-quality image\ngeneration. We provide compelling evidence that ViPE is more robust than GPT3.5\nin synthesising visual elaborations. ViPE also exhibits an understanding of\nfigurative expressions comparable to human experts, providing a powerful and\nopen-source backbone to many downstream applications such as music video and\ncaption generation.", "published": "2023-10-16 16:14:20", "link": "http://arxiv.org/abs/2310.10543v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Emerging Challenges in Personalized Medicine: Assessing Demographic\n  Effects on Biomedical Question Answering Systems", "abstract": "State-of-the-art question answering (QA) models exhibit a variety of social\nbiases (e.g., with respect to sex or race), generally explained by similar\nissues in their training data. However, what has been overlooked so far is that\nin the critical domain of biomedicine, any unjustified change in model output\ndue to patient demographics is problematic: it results in the unfair treatment\nof patients. Selecting only questions on biomedical topics whose answers do not\ndepend on ethnicity, sex, or sexual orientation, we ask the following research\nquestions: (RQ1) Do the answers of QA models change when being provided with\nirrelevant demographic information? (RQ2) Does the answer of RQ1 differ between\nknowledge graph (KG)-grounded and text-based QA systems? We find that\nirrelevant demographic information change up to 15% of the answers of a\nKG-grounded system and up to 23% of the answers of a text-based system,\nincluding changes that affect accuracy. We conclude that unjustified answer\nchanges caused by patient demographics are a frequent phenomenon, which raises\nfairness concerns and should be paid more attention to.", "published": "2023-10-16 16:45:52", "link": "http://arxiv.org/abs/2310.10571v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Who Are All The Stochastic Parrots Imitating? They Should Tell Us!", "abstract": "Both standalone language models (LMs) as well as LMs within downstream-task\nsystems have been shown to generate statements which are factually untrue. This\nproblem is especially severe for low-resource languages, where training data is\nscarce and of worse quality than for high-resource languages. In this opinion\npiece, we argue that LMs in their current state will never be fully trustworthy\nin critical settings and suggest a possible novel strategy to handle this\nissue: by building LMs such that can cite their sources - i.e., point a user to\nthe parts of their training data that back up their outputs. We first discuss\nwhich current NLP tasks would or would not benefit from such models. We then\nhighlight the expected benefits such models would bring, e.g., quick\nverifiability of statements. We end by outlining the individual tasks that\nwould need to be solved on the way to developing LMs with the ability to cite.\nWe hope to start a discussion about the field's current approach to building\nLMs, especially for low-resource languages, and the role of the training data\nin explaining model generations.", "published": "2023-10-16 16:57:55", "link": "http://arxiv.org/abs/2310.10583v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VidCoM: Fast Video Comprehension through Large Language Models with\n  Multimodal Tools", "abstract": "Building models that comprehends videos and responds specific user\ninstructions is a practical and challenging topic, as it requires mastery of\nboth vision understanding and knowledge reasoning. Compared to language and\nimage modalities, training efficiency remains a serious problem as existing\nstudies train models on massive sparse videos paired with brief descriptions.\nIn this paper, we introduce \\textbf{VidCoM}, a fast adaptive framework that\nleverages Large Language Models (LLMs) to reason about videos using lightweight\nvisual tools. Specifically, we reveal that the key to responding to specific\ninstructions is focusing on relevant video events, and utilize two visual\ntools, structured scene graph generation and descriptive image caption\ngeneration, to gather and represent the event information. Thus, a LLM enriched\nwith world knowledge is adopted as the reasoning agent to achieve the responses\nby performing multiple reasoning steps on specific video events. To address the\ndifficulty of LLMs identifying video events, we further propose an\nInstruction-oriented Video Events Recognition (InsOVER) algorithm. This\nalgorithm locates the corresponding video events based on an efficient\nHungarian matching between decompositions of linguistic instructions and video\nevents, thereby enabling LLMs to interact effectively with extended videos.\nExtensive experiments on two typical video comprehension tasks show that the\nproposed tuning-free framework outperforms the pre-trained models including\nFlamingo-80B, to achieve the state-of-the-art performance. Our source code and\nsystem will be publicly available.", "published": "2023-10-16 17:05:56", "link": "http://arxiv.org/abs/2310.10586v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Motion2Language, unsupervised learning of synchronized semantic motion\n  segmentation", "abstract": "In this paper, we investigate building a sequence to sequence architecture\nfor motion to language translation and synchronization. The aim is to translate\nmotion capture inputs into English natural-language descriptions, such that the\ndescriptions are generated synchronously with the actions performed, enabling\nsemantic segmentation as a byproduct, but without requiring synchronized\ntraining data. We propose a new recurrent formulation of local attention that\nis suited for synchronous/live text generation, as well as an improved motion\nencoder architecture better suited to smaller data and for synchronous\ngeneration. We evaluate both contributions in individual experiments, using the\nstandard BLEU4 metric, as well as a simple semantic equivalence measure, on the\nKIT motion language dataset. In a follow-up experiment, we assess the quality\nof the synchronization of generated text in our proposed approaches through\nmultiple evaluation metrics. We find that both contributions to the attention\nmechanism and the encoder architecture additively improve the quality of\ngenerated text (BLEU and semantic equivalence), but also of synchronization.\nOur code is available at\nhttps://github.com/rd20karim/M2T-Segmentation/tree/main", "published": "2023-10-16 17:16:32", "link": "http://arxiv.org/abs/2310.10594v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Factored Verification: Detecting and Reducing Hallucination in Summaries\n  of Academic Papers", "abstract": "Hallucination plagues even frontier LLMs--but how bad is it really for\nsummarizing academic papers? We evaluate Factored Verification, a simple\nautomated method for detecting hallucinations in abstractive summaries. This\nmethod sets a new SotA on hallucination detection in the summarization task of\nthe HaluEval benchmark, achieving 76.2% accuracy. We then use this method to\nestimate how often language models hallucinate when summarizing across multiple\nacademic papers and find 0.62 hallucinations in the average ChatGPT (16k)\nsummary, 0.84 for GPT-4, and 1.55 for Claude 2. We ask models to self-correct\nusing Factored Critiques and find that this lowers the number of hallucinations\nto 0.49 for ChatGPT, 0.46 for GPT-4, and 0.95 for Claude 2. The hallucinations\nwe find are often subtle, so we advise caution when using models to synthesize\nacademic papers.", "published": "2023-10-16 17:51:17", "link": "http://arxiv.org/abs/2310.10627v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OpenAgents: An Open Platform for Language Agents in the Wild", "abstract": "Language agents show potential in being capable of utilizing natural language\nfor varied and intricate tasks in diverse environments, particularly when built\nupon large language models (LLMs). Current language agent frameworks aim to\nfacilitate the construction of proof-of-concept language agents while\nneglecting the non-expert user access to agents and paying little attention to\napplication-level designs. We present OpenAgents, an open platform for using\nand hosting language agents in the wild of everyday life. OpenAgents includes\nthree agents: (1) Data Agent for data analysis with Python/SQL and data tools;\n(2) Plugins Agent with 200+ daily API tools; (3) Web Agent for autonomous web\nbrowsing. OpenAgents enables general users to interact with agent\nfunctionalities through a web user interface optimized for swift responses and\ncommon failures while offering developers and researchers a seamless deployment\nexperience on local setups, providing a foundation for crafting innovative\nlanguage agents and facilitating real-world evaluations. We elucidate the\nchallenges and opportunities, aspiring to set a foundation for future research\nand development of real-world language agents.", "published": "2023-10-16 17:54:53", "link": "http://arxiv.org/abs/2310.10634v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bridging the Novice-Expert Gap via Models of Decision-Making: A Case\n  Study on Remediating Math Mistakes", "abstract": "Scaling high-quality tutoring remains a major challenge in education. Due to\ngrowing demand, many platforms employ novice tutors who, unlike experienced\neducators, struggle to address student mistakes and thus fail to seize prime\nlearning opportunities. Our work explores the potential of large language\nmodels (LLMs) to close the novice-expert knowledge gap in remediating math\nmistakes. We contribute Bridge, a method that uses cognitive task analysis to\ntranslate an expert's latent thought process into a decision-making model for\nremediation. This involves an expert identifying (A) the student's error, (B) a\nremediation strategy, and (C) their intention before generating a response. We\nconstruct a dataset of 700 real tutoring conversations, annotated by experts\nwith their decisions. We evaluate state-of-the-art LLMs on our dataset and find\nthat the expert's decision-making model is critical for LLMs to close the gap:\nresponses from GPT4 with expert decisions (e.g., \"simplify the problem\") are\n+76% more preferred than without. Additionally, context-sensitive decisions are\ncritical to closing pedagogical gaps: random decisions decrease GPT4's response\nquality by -97% than expert decisions. Our work shows the potential of\nembedding expert thought processes in LLM generations to enhance their\ncapability to bridge novice-expert knowledge gaps. Our dataset and code can be\nfound at: \\url{https://github.com/rosewang2008/bridge}.", "published": "2023-10-16 17:59:50", "link": "http://arxiv.org/abs/2310.10648v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Theory of Mind for Multi-Agent Collaboration via Large Language Models", "abstract": "While Large Language Models (LLMs) have demonstrated impressive\naccomplishments in both reasoning and planning, their abilities in multi-agent\ncollaborations remains largely unexplored. This study evaluates LLM-based\nagents in a multi-agent cooperative text game with Theory of Mind (ToM)\ninference tasks, comparing their performance with Multi-Agent Reinforcement\nLearning (MARL) and planning-based baselines. We observed evidence of emergent\ncollaborative behaviors and high-order Theory of Mind capabilities among\nLLM-based agents. Our results reveal limitations in LLM-based agents' planning\noptimization due to systematic failures in managing long-horizon contexts and\nhallucination about the task state. We explore the use of explicit belief state\nrepresentations to mitigate these issues, finding that it enhances task\nperformance and the accuracy of ToM inferences for LLM-based agents.", "published": "2023-10-16 07:51:19", "link": "http://arxiv.org/abs/2310.10701v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Harnessing the Power of LLMs: Evaluating Human-AI Text Co-Creation\n  through the Lens of News Headline Generation", "abstract": "To explore how humans can best leverage LLMs for writing and how interacting\nwith these models affects feelings of ownership and trust in the writing\nprocess, we compared common human-AI interaction types (e.g., guiding system,\nselecting from system outputs, post-editing outputs) in the context of\nLLM-assisted news headline generation. While LLMs alone can generate\nsatisfactory news headlines, on average, human control is needed to fix\nundesirable model outputs. Of the interaction methods, guiding and selecting\nmodel output added the most benefit with the lowest cost (in time and effort).\nFurther, AI assistance did not harm participants' perception of control\ncompared to freeform editing.", "published": "2023-10-16 15:11:01", "link": "http://arxiv.org/abs/2310.10706v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Demonstrations Are All You Need: Advancing Offensive Content\n  Paraphrasing using In-Context Learning", "abstract": "Paraphrasing of offensive content is a better alternative to content removal\nand helps improve civility in a communication environment. Supervised\nparaphrasers; however, rely heavily on large quantities of labelled data to\nhelp preserve meaning and intent. They also often retain a large portion of the\noffensiveness of the original content, which raises questions on their overall\nusability. In this paper we aim to assist practitioners in developing usable\nparaphrasers by exploring In-Context Learning (ICL) with large language models\n(LLMs), i.e., using a limited number of input-label demonstration pairs to\nguide the model in generating desired outputs for specific queries. Our study\nfocuses on key factors such as - number and order of demonstrations, exclusion\nof prompt instruction, and reduction in measured toxicity. We perform\nprincipled evaluation on three datasets, including our proposed Context-Aware\nPolite Paraphrase (CAPP) dataset, comprising of dialogue-style rude utterances,\npolite paraphrases, and additional dialogue context. We evaluate our approach\nusing four closed source and one open source LLM. Our results reveal that ICL\nis comparable to supervised methods in generation quality, while being\nqualitatively better by 25% on human evaluation and attaining lower toxicity by\n76%. Also, ICL-based paraphrasers only show a slight reduction in performance\neven with just 10% training data.", "published": "2023-10-16 16:18:55", "link": "http://arxiv.org/abs/2310.10707v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Models of Speech Infer Universal Articulatory Kinematics", "abstract": "Self-Supervised Learning (SSL) based models of speech have shown remarkable\nperformance on a range of downstream tasks. These state-of-the-art models have\nremained blackboxes, but many recent studies have begun \"probing\" models like\nHuBERT, to correlate their internal representations to different aspects of\nspeech. In this paper, we show \"inference of articulatory kinematics\" as\nfundamental property of SSL models, i.e., the ability of these models to\ntransform acoustics into the causal articulatory dynamics underlying the speech\nsignal. We also show that this abstraction is largely overlapping across the\nlanguage of the data used to train the model, with preference to the language\nwith similar phonological system. Furthermore, we show that with simple affine\ntransformations, Acoustic-to-Articulatory inversion (AAI) is transferrable\nacross speakers, even across genders, languages, and dialects, showing the\ngeneralizability of this property. Together, these results shed new light on\nthe internals of SSL models that are critical to their superior performance,\nand open up new avenues into language-agnostic universal models for speech\nengineering, that are interpretable and grounded in speech science.", "published": "2023-10-16 19:50:01", "link": "http://arxiv.org/abs/2310.10788v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "SD-HuBERT: Sentence-Level Self-Distillation Induces Syllabic\n  Organization in HuBERT", "abstract": "Data-driven unit discovery in self-supervised learning (SSL) of speech has\nembarked on a new era of spoken language processing. Yet, the discovered units\noften remain in phonetic space and the units beyond phonemes are largely\nunderexplored. Here, we demonstrate that a syllabic organization emerges in\nlearning sentence-level representation of speech. In particular, we adopt\n\"self-distillation\" objective to fine-tune the pretrained HuBERT with an\naggregator token that summarizes the entire sentence. Without any supervision,\nthe resulting model draws definite boundaries in speech, and the\nrepresentations across frames exhibit salient syllabic structures. We\ndemonstrate that this emergent structure largely corresponds to the ground\ntruth syllables. Furthermore, we propose a new benchmark task, Spoken Speech\nABX, for evaluating sentence-level representation of speech. When compared to\nprevious models, our model outperforms in both unsupervised syllable discovery\nand learning sentence-level representation. Together, we demonstrate that the\nself-distillation of HuBERT gives rise to syllabic organization without relying\non external labels or modalities, and potentially provides novel data-driven\nunits for spoken language modeling.", "published": "2023-10-16 20:05:36", "link": "http://arxiv.org/abs/2310.10803v3", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "CoTFormer: A Chain-of-Thought Driven Architecture with Budget-Adaptive\n  Computation Cost at Inference", "abstract": "Scaling language models to larger and deeper sizes has led to significant\nboosts in performance. Even though the size of these models limits their\napplication in compute-constrained environments, the race to continually\ndevelop ever larger and deeper foundational models is underway. At the same\ntime -- regardless of the model size -- task-specific techniques continue to\nplay a pivotal role in achieving optimal downstream performance. One of these\ntechniques, called Chain-of-Thought (CoT), is particularly interesting since,\nas we point out in this work, it resembles employing a deeper transformer\nthrough re-applying the model multiple times. However, a key subtlety in\ncomputing the attention of past tokens differentiates CoT from simply applying\nthe model several times. Based on this insight, we propose CoTFormer, a novel\narchitecture which closely mimics CoT at the token level, allowing us to obtain\nsignificantly improved accuracies close to much larger models. While applying\nCoT introduces additional computation costs, we compensate for it by leveraging\nCoTFormer's special compatibility with token-wise variable depth. Through a\ncompute adaptive model -- which automatically allocates the compute to tokens\nthat need it most -- we show that it is possible to reduce the computation cost\nsignificantly without any reduction in accuracy, and with further compute cost\nreductions possible while maintaining a competitive accuracy.", "published": "2023-10-16 21:37:34", "link": "http://arxiv.org/abs/2310.10845v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EfficientOCR: An Extensible, Open-Source Package for Efficiently\n  Digitizing World Knowledge", "abstract": "Billions of public domain documents remain trapped in hard copy or lack an\naccurate digitization. Modern natural language processing methods cannot be\nused to index, retrieve, and summarize their texts; conduct computational\ntextual analyses; or extract information for statistical analyses, and these\ntexts cannot be incorporated into language model training. Given the diversity\nand sheer quantity of public domain texts, liberating them at scale requires\noptical character recognition (OCR) that is accurate, extremely cheap to\ndeploy, and sample-efficient to customize to novel collections, languages, and\ncharacter sets. Existing OCR engines, largely designed for small-scale\ncommercial applications in high resource languages, often fall short of these\nrequirements. EffOCR (EfficientOCR), a novel open-source OCR package, meets\nboth the computational and sample efficiency requirements for liberating texts\nat scale by abandoning the sequence-to-sequence architecture typically used for\nOCR, which takes representations from a learned vision model as inputs to a\nlearned language model. Instead, EffOCR models OCR as a character or word-level\nimage retrieval problem. EffOCR is cheap and sample efficient to train, as the\nmodel only needs to learn characters' visual appearance and not how they are\nused in sequence to form language. Models in the EffOCR model zoo can be\ndeployed off-the-shelf with only a few lines of code. Importantly, EffOCR also\nallows for easy, sample efficient customization with a simple model training\ninterface and minimal labeling requirements due to its sample efficiency. We\nillustrate the utility of EffOCR by cheaply and accurately digitizing 20\nmillion historical U.S. newspaper scans, evaluating zero-shot performance on\nrandomly selected documents from the U.S. National Archives, and accurately\ndigitizing Japanese documents for which all other OCR solutions failed.", "published": "2023-10-16 04:20:16", "link": "http://arxiv.org/abs/2310.10050v1", "categories": ["cs.CV", "cs.CL", "econ.GN", "q-fin.EC"], "primary_category": "cs.CV"}
{"title": "NASH: A Simple Unified Framework of Structured Pruning for Accelerating\n  Encoder-Decoder Language Models", "abstract": "Structured pruning methods have proven effective in reducing the model size\nand accelerating inference speed in various network architectures such as\nTransformers. Despite the versatility of encoder-decoder models in numerous NLP\ntasks, the structured pruning methods on such models are relatively less\nexplored compared to encoder-only models. In this study, we investigate the\nbehavior of the structured pruning of the encoder-decoder models in the\ndecoupled pruning perspective of the encoder and decoder component,\nrespectively. Our findings highlight two insights: (1) the number of decoder\nlayers is the dominant factor of inference speed, and (2) low sparsity in the\npruned encoder network enhances generation quality. Motivated by these\nfindings, we propose a simple and effective framework, NASH, that narrows the\nencoder and shortens the decoder networks of encoder-decoder models. Extensive\nexperiments on diverse generation and inference tasks validate the\neffectiveness of our method in both speedup and output quality.", "published": "2023-10-16 04:27:36", "link": "http://arxiv.org/abs/2310.10054v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Navigation with Large Language Models: Semantic Guesswork as a Heuristic\n  for Planning", "abstract": "Navigation in unfamiliar environments presents a major challenge for robots:\nwhile mapping and planning techniques can be used to build up a representation\nof the world, quickly discovering a path to a desired goal in unfamiliar\nsettings with such methods often requires lengthy mapping and exploration.\nHumans can rapidly navigate new environments, particularly indoor environments\nthat are laid out logically, by leveraging semantics -- e.g., a kitchen often\nadjoins a living room, an exit sign indicates the way out, and so forth.\nLanguage models can provide robots with such knowledge, but directly using\nlanguage models to instruct a robot how to reach some destination can also be\nimpractical: while language models might produce a narrative about how to reach\nsome goal, because they are not grounded in real-world observations, this\nnarrative might be arbitrarily wrong. Therefore, in this paper we study how the\n``semantic guesswork'' produced by language models can be utilized as a guiding\nheuristic for planning algorithms. Our method, Language Frontier Guide (LFG),\nuses the language model to bias exploration of novel real-world environments by\nincorporating the semantic knowledge stored in language models as a search\nheuristic for planning with either topological or metric maps. We evaluate LFG\nin challenging real-world environments and simulated benchmarks, outperforming\nuninformed exploration and other ways of using language models.", "published": "2023-10-16 06:21:06", "link": "http://arxiv.org/abs/2310.10103v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.RO"}
{"title": "End-to-end Multichannel Speaker-Attributed ASR: Speaker Guided Decoder\n  and Input Feature Analysis", "abstract": "We present an end-to-end multichannel speaker-attributed automatic speech\nrecognition (MC-SA-ASR) system that combines a Conformer-based encoder with\nmulti-frame crosschannel attention and a speaker-attributed Transformer-based\ndecoder. To the best of our knowledge, this is the first model that efficiently\nintegrates ASR and speaker identification modules in a multichannel setting. On\nsimulated mixtures of LibriSpeech data, our system reduces the word error rate\n(WER) by up to 12% and 16% relative compared to previously proposed\nsingle-channel and multichannel approaches, respectively. Furthermore, we\ninvestigate the impact of different input features, including multichannel\nmagnitude and phase information, on the ASR performance. Finally, our\nexperiments on the AMI corpus confirm the effectiveness of our system for\nreal-world multichannel meeting transcription.", "published": "2023-10-16 06:40:18", "link": "http://arxiv.org/abs/2310.10106v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation\n  and Generalization", "abstract": "Language agents have shown some ability to interact with an external\nenvironment, e.g., a virtual world such as ScienceWorld, to perform complex\ntasks, e.g., growing a plant, without the startup costs of reinforcement\nlearning. However, despite their zero-shot capabilities, these agents to date\ndo not continually improve over time beyond performance refinement on a\nspecific task. Here we present CLIN, the first language-based agent to achieve\nthis, so that it continually improves over multiple trials, including when both\nthe environment and task are varied, and without requiring parameter updates.\nOur approach is to use a persistent, dynamic, textual memory centered on causal\nabstractions (rather than general \"helpful hints\") that is regularly updated\nafter each trial so that the agent gradually learns useful knowledge for new\ntrials. In the ScienceWorld benchmark, CLIN is able to continually improve on\nrepeated trials on the same task and environment, outperforming\nstate-of-the-art reflective language agents like Reflexion by 23 absolute\npoints. CLIN can also transfer its learning to new environments (or new tasks),\nimproving its zero-shot performance by 4 points (13 for new tasks) and can\nfurther improve performance there through continual memory updates, enhancing\nperformance by an additional 17 points (7 for new tasks). This suggests a new\narchitecture for agents built on frozen models that can still continually and\nrapidly improve over time.", "published": "2023-10-16 07:17:27", "link": "http://arxiv.org/abs/2310.10134v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Node-based Knowledge Graph Contrastive Learning for Medical Relationship\n  Prediction", "abstract": "The embedding of Biomedical Knowledge Graphs (BKGs) generates robust\nrepresentations, valuable for a variety of artificial intelligence\napplications, including predicting drug combinations and reasoning disease-drug\nrelationships. Meanwhile, contrastive learning (CL) is widely employed to\nenhance the distinctiveness of these representations. However, constructing\nsuitable contrastive pairs for CL, especially within Knowledge Graphs (KGs),\nhas been challenging. In this paper, we proposed a novel node-based contrastive\nlearning method for knowledge graph embedding, NC-KGE. NC-KGE enhances\nknowledge extraction in embeddings and speeds up training convergence by\nconstructing appropriate contrastive node pairs on KGs. This scheme can be\neasily integrated with other knowledge graph embedding (KGE) methods. For\ndownstream task such as biochemical relationship prediction, we have\nincorporated a relation-aware attention mechanism into NC-KGE, focusing on the\nsemantic relationships and node interactions. Extensive experiments show that\nNC-KGE performs competitively with state-of-the-art models on public datasets\nlike FB15k-237 and WN18RR. Particularly in biomedical relationship prediction\ntasks, NC-KGE outperforms all baselines on datasets such as PharmKG8k-28,\nDRKG17k-21, and BioKG72k-14, especially in predicting drug combination\nrelationships. We release our code at https://github.com/zhi520/NC-KGE.", "published": "2023-10-16 07:27:43", "link": "http://arxiv.org/abs/2310.10138v1", "categories": ["cs.DB", "cs.CL", "q-bio.QM"], "primary_category": "cs.DB"}
{"title": "DNA: Denoised Neighborhood Aggregation for Fine-grained Category\n  Discovery", "abstract": "Discovering fine-grained categories from coarsely labeled data is a practical\nand challenging task, which can bridge the gap between the demand for\nfine-grained analysis and the high annotation cost. Previous works mainly focus\non instance-level discrimination to learn low-level features, but ignore\nsemantic similarities between data, which may prevent these models learning\ncompact cluster representations. In this paper, we propose Denoised\nNeighborhood Aggregation (DNA), a self-supervised framework that encodes\nsemantic structures of data into the embedding space. Specifically, we retrieve\nk-nearest neighbors of a query as its positive keys to capture semantic\nsimilarities between data and then aggregate information from the neighbors to\nlearn compact cluster representations, which can make fine-grained categories\nmore separatable. However, the retrieved neighbors can be noisy and contain\nmany false-positive keys, which can degrade the quality of learned embeddings.\nTo cope with this challenge, we propose three principles to filter out these\nfalse neighbors for better representation learning. Furthermore, we\ntheoretically justify that the learning objective of our framework is\nequivalent to a clustering loss, which can capture semantic similarities\nbetween data to form compact fine-grained clusters. Extensive experiments on\nthree benchmark datasets show that our method can retrieve more accurate\nneighbors (21.31% accuracy improvement) and outperform state-of-the-art models\nby a large margin (average 9.96% improvement on three metrics). Our code and\ndata are available at https://github.com/Lackel/DNA.", "published": "2023-10-16 07:43:30", "link": "http://arxiv.org/abs/2310.10151v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Joint Music and Language Attention Models for Zero-shot Music Tagging", "abstract": "Music tagging is a task to predict the tags of music recordings. However,\nprevious music tagging research primarily focuses on close-set music tagging\ntasks which can not be generalized to new tags. In this work, we propose a\nzero-shot music tagging system modeled by a joint music and language attention\n(JMLA) model to address the open-set music tagging problem. The JMLA model\nconsists of an audio encoder modeled by a pretrained masked autoencoder and a\ndecoder modeled by a Falcon7B. We introduce preceiver resampler to convert\narbitrary length audio into fixed length embeddings. We introduce dense\nattention connections between encoder and decoder layers to improve the\ninformation flow between the encoder and decoder layers. We collect a\nlarge-scale music and description dataset from the internet. We propose to use\nChatGPT to convert the raw descriptions into formalized and diverse\ndescriptions to train the JMLA models. Our proposed JMLA system achieves a\nzero-shot audio tagging accuracy of $ 64.82\\% $ on the GTZAN dataset,\noutperforming previous zero-shot systems and achieves comparable results to\nprevious systems on the FMA and the MagnaTagATune datasets.", "published": "2023-10-16 08:00:16", "link": "http://arxiv.org/abs/2310.10159v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DemoNSF: A Multi-task Demonstration-based Generative Framework for Noisy\n  Slot Filling Task", "abstract": "Recently, prompt-based generative frameworks have shown impressive\ncapabilities in sequence labeling tasks. However, in practical dialogue\nscenarios, relying solely on simplistic templates and traditional corpora\npresents a challenge for these methods in generalizing to unknown input\nperturbations. To address this gap, we propose a multi-task demonstration based\ngenerative framework for noisy slot filling, named DemoNSF. Specifically, we\nintroduce three noisy auxiliary tasks, namely noisy recovery (NR), random mask\n(RM), and hybrid discrimination (HD), to implicitly capture semantic structural\ninformation of input perturbations at different granularities. In the\ndownstream main task, we design a noisy demonstration construction strategy for\nthe generative framework, which explicitly incorporates task-specific\ninformation and perturbed distribution during training and inference.\nExperiments on two benchmarks demonstrate that DemoNSF outperforms all baseline\nmethods and achieves strong generalization. Further analysis provides empirical\nguidance for the practical application of generative frameworks. Our code is\nreleased at https://github.com/dongguanting/Demo-NSF.", "published": "2023-10-16 08:16:53", "link": "http://arxiv.org/abs/2310.10169v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models Meet Open-World Intent Discovery and Recognition:\n  An Evaluation of ChatGPT", "abstract": "The tasks of out-of-domain (OOD) intent discovery and generalized intent\ndiscovery (GID) aim to extend a closed intent classifier to open-world intent\nsets, which is crucial to task-oriented dialogue (TOD) systems. Previous\nmethods address them by fine-tuning discriminative models. Recently, although\nsome studies have been exploring the application of large language models\n(LLMs) represented by ChatGPT to various downstream tasks, it is still unclear\nfor the ability of ChatGPT to discover and incrementally extent OOD intents. In\nthis paper, we comprehensively evaluate ChatGPT on OOD intent discovery and\nGID, and then outline the strengths and weaknesses of ChatGPT. Overall, ChatGPT\nexhibits consistent advantages under zero-shot settings, but is still at a\ndisadvantage compared to fine-tuned models. More deeply, through a series of\nanalytical experiments, we summarize and discuss the challenges faced by LLMs\nincluding clustering, domain-specific understanding, and cross-domain\nin-context learning scenarios. Finally, we provide empirical guidance for\nfuture directions to address these challenges.", "published": "2023-10-16 08:34:44", "link": "http://arxiv.org/abs/2310.10176v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Continual Generalized Intent Discovery: Marching Towards Dynamic and\n  Open-world Intent Recognition", "abstract": "In a practical dialogue system, users may input out-of-domain (OOD) queries.\nThe Generalized Intent Discovery (GID) task aims to discover OOD intents from\nOOD queries and extend them to the in-domain (IND) classifier. However, GID\nonly considers one stage of OOD learning, and needs to utilize the data in all\nprevious stages for joint training, which limits its wide application in\nreality. In this paper, we introduce a new task, Continual Generalized Intent\nDiscovery (CGID), which aims to continuously and automatically discover OOD\nintents from dynamic OOD data streams and then incrementally add them to the\nclassifier with almost no previous data, thus moving towards dynamic intent\nrecognition in an open world. Next, we propose a method called Prototype-guided\nLearning with Replay and Distillation (PLRD) for CGID, which bootstraps new\nintent discovery through class prototypes and balances new and old intents\nthrough data replay and feature distillation. Finally, we conduct detailed\nexperiments and analysis to verify the effectiveness of PLRD and understand the\nkey challenges of CGID for future research.", "published": "2023-10-16 08:48:07", "link": "http://arxiv.org/abs/2310.10184v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prediction of Arabic Legal Rulings using Large Language Models", "abstract": "In the intricate field of legal studies, the analysis of court decisions is a\ncornerstone for the effective functioning of the judicial system. The ability\nto predict court outcomes helps judges during the decision-making process and\nequips lawyers with invaluable insights, enhancing their strategic approaches\nto cases. Despite its significance, the domain of Arabic court analysis remains\nunder-explored. This paper pioneers a comprehensive predictive analysis of\nArabic court decisions on a dataset of 10,813 commercial court real cases,\nleveraging the advanced capabilities of the current state-of-the-art large\nlanguage models. Through a systematic exploration, we evaluate three prevalent\nfoundational models (LLaMA-7b, JAIS-13b, and GPT3.5-turbo) and three training\nparadigms: zero-shot, one-shot, and tailored fine-tuning. Besides, we assess\nthe benefit of summarizing and/or translating the original Arabic input texts.\nThis leads to a spectrum of 14 model variants, for which we offer a granular\nperformance assessment with a series of different metrics (human assessment,\nGPT evaluation, ROUGE, and BLEU scores). We show that all variants of LLaMA\nmodels yield limited performance, whereas GPT-3.5-based models outperform all\nother models by a wide margin, surpassing the average score of the dedicated\nArabic-centric JAIS model by 50%. Furthermore, we show that all scores except\nhuman evaluation are inconsistent and unreliable for assessing the performance\nof large language models on court decision predictions. This study paves the\nway for future research, bridging the gap between computational linguistics and\nArabic legal analytics.", "published": "2023-10-16 10:37:35", "link": "http://arxiv.org/abs/2310.10260v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Legal NLP Meets MiCAR: Advancing the Analysis of Crypto White Papers", "abstract": "In the rapidly evolving field of crypto assets, white papers are essential\ndocuments for investor guidance, and are now subject to unprecedented content\nrequirements under the European Union's Markets in Crypto-Assets Regulation\n(MiCAR). Natural Language Processing (NLP) can serve as a powerful tool for\nboth analyzing these documents and assisting in regulatory compliance. This\npaper delivers two contributions to the topic. First, we survey existing\napplications of textual analysis to unregulated crypto asset white papers,\nuncovering a research gap that could be bridged with interdisciplinary\ncollaboration. We then conduct an analysis of the changes introduced by MiCAR,\nhighlighting the opportunities and challenges of integrating NLP within the new\nregulatory framework. The findings set the stage for further research, with the\npotential to benefit regulators, crypto asset issuers, and investors.", "published": "2023-10-16 12:17:11", "link": "http://arxiv.org/abs/2310.10333v3", "categories": ["cs.CY", "cs.CL", "q-fin.GN"], "primary_category": "cs.CY"}
{"title": "Attribution Patching Outperforms Automated Circuit Discovery", "abstract": "Automated interpretability research has recently attracted attention as a\npotential research direction that could scale explanations of neural network\nbehavior to large models. Existing automated circuit discovery work applies\nactivation patching to identify subnetworks responsible for solving specific\ntasks (circuits). In this work, we show that a simple method based on\nattribution patching outperforms all existing methods while requiring just two\nforward passes and a backward pass. We apply a linear approximation to\nactivation patching to estimate the importance of each edge in the\ncomputational subgraph. Using this approximation, we prune the least important\nedges of the network. We survey the performance and limitations of this method,\nfinding that averaged over all tasks our method has greater AUC from circuit\nrecovery than other methods.", "published": "2023-10-16 12:34:43", "link": "http://arxiv.org/abs/2310.10348v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Cross-Lingual Consistency of Factual Knowledge in Multilingual Language\n  Models", "abstract": "Multilingual large-scale Pretrained Language Models (PLMs) have been shown to\nstore considerable amounts of factual knowledge, but large variations are\nobserved across languages. With the ultimate goal of ensuring that users with\ndifferent language backgrounds obtain consistent feedback from the same model,\nwe study the cross-lingual consistency (CLC) of factual knowledge in various\nmultilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC)\nmetric to evaluate knowledge consistency across languages independently from\naccuracy. Using this metric, we conduct an in-depth analysis of the determining\nfactors for CLC, both at model level and at language-pair level. Among other\nresults, we find that increasing model size leads to higher factual probing\naccuracy in most languages, but does not improve cross-lingual consistency.\nFinally, we conduct a case study on CLC when new factual associations are\ninserted in the PLMs via model editing. Results on a small sample of facts\ninserted in English reveal a clear pattern whereby the new piece of knowledge\ntransfers only to languages with which English has a high RankC score.", "published": "2023-10-16 13:19:17", "link": "http://arxiv.org/abs/2310.10378v4", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can Word Sense Distribution Detect Semantic Changes of Words?", "abstract": "Semantic Change Detection (SCD) of words is an important task for various NLP\napplications that must make time-sensitive predictions. Some words are used\nover time in novel ways to express new meanings, and these new meanings\nestablish themselves as novel senses of existing words. On the other hand, Word\nSense Disambiguation (WSD) methods associate ambiguous words with sense ids,\ndepending on the context in which they occur. Given this relationship between\nWSD and SCD, we explore the possibility of predicting whether a target word has\nits meaning changed between two corpora collected at different time steps, by\ncomparing the distributions of senses of that word in each corpora. For this\npurpose, we use pretrained static sense embeddings to automatically annotate\neach occurrence of the target word in a corpus with a sense id. Next, we\ncompute the distribution of sense ids of a target word in a given corpus.\nFinally, we use different divergence or distance measures to quantify the\nsemantic change of the target word across the two given corpora. Our\nexperimental results on SemEval 2020 Task 1 dataset show that word sense\ndistributions can be accurately used to predict semantic changes of words in\nEnglish, German, Swedish and Latin.", "published": "2023-10-16 13:41:27", "link": "http://arxiv.org/abs/2310.10400v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploiting User Comments for Early Detection of Fake News Prior to\n  Users' Commenting", "abstract": "Both accuracy and timeliness are key factors in detecting fake news on social\nmedia. However, most existing methods encounter an accuracy-timeliness dilemma:\nContent-only methods guarantee timeliness but perform moderately because of\nlimited available information, while social con-text-based ones generally\nperform better but inevitably lead to latency because of social context\naccumulation needs. To break such a dilemma, a feasible but not well-studied\nsolution is to leverage social contexts (e.g., comments) from historical news\nfor training a detection model and apply it to newly emerging news without\nsocial contexts. This requires the model to (1) sufficiently learn helpful\nknowledge from social contexts, and (2) be well compatible with situations that\nsocial contexts are available or not. To achieve this goal, we propose to\nabsorb and parameterize useful knowledge from comments in historical news and\nthen inject it into a content-only detection model. Specifically, we design the\nComments ASsisted FakE News Detection method (CAS-FEND), which transfers useful\nknowledge from a comment-aware teacher model to a content-only student model\nand detects newly emerging news with the student model. Experiments show that\nthe CAS-FEND student model outperforms all content-only methods and even\ncomment-aware ones with 1/4 comments as inputs, demonstrating its superiority\nfor early detection.", "published": "2023-10-16 14:13:38", "link": "http://arxiv.org/abs/2310.10429v2", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Text Summarization Using Large Language Models: A Comparative Study of\n  MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models", "abstract": "Text summarization is a critical Natural Language Processing (NLP) task with\napplications ranging from information retrieval to content generation.\nLeveraging Large Language Models (LLMs) has shown remarkable promise in\nenhancing summarization techniques. This paper embarks on an exploration of\ntext summarization with a diverse set of LLMs, including MPT-7b-instruct,\nfalcon-7b-instruct, and OpenAI ChatGPT text-davinci-003 models. The experiment\nwas performed with different hyperparameters and evaluated the generated\nsummaries using widely accepted metrics such as the Bilingual Evaluation\nUnderstudy (BLEU) Score, Recall-Oriented Understudy for Gisting Evaluation\n(ROUGE) Score, and Bidirectional Encoder Representations from Transformers\n(BERT) Score. According to the experiment, text-davinci-003 outperformed the\nothers. This investigation involved two distinct datasets: CNN Daily Mail and\nXSum. Its primary objective was to provide a comprehensive understanding of the\nperformance of Large Language Models (LLMs) when applied to different datasets.\nThe assessment of these models' effectiveness contributes valuable insights to\nresearchers and practitioners within the NLP domain. This work serves as a\nresource for those interested in harnessing the potential of LLMs for text\nsummarization and lays the foundation for the development of advanced\nGenerative AI applications aimed at addressing a wide spectrum of business\nchallenges.", "published": "2023-10-16 14:33:02", "link": "http://arxiv.org/abs/2310.10449v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake\n  Analysis", "abstract": "The rapid development of large language models (LLMs) has not only provided\nnumerous opportunities but also presented significant challenges. This becomes\nparticularly evident when LLMs inadvertently generate harmful or toxic content,\neither unintentionally or because of intentional inducement. Existing alignment\nmethods usually direct LLMs toward the favorable outcomes by utilizing\nhuman-annotated, flawless instruction-response pairs. Conversely, this study\nproposes a novel alignment technique based on mistake analysis, which\ndeliberately exposes LLMs to erroneous content to learn the reasons for\nmistakes and how to avoid them. In this case, mistakes are repurposed into\nvaluable data for alignment, effectively helping to avoid the production of\nerroneous responses. Without external models or human annotations, our method\nleverages a model's intrinsic ability to discern undesirable mistakes and\nimproves the safety of its generated responses. Experimental results reveal\nthat our method outperforms existing alignment approaches in enhancing model\nsafety while maintaining the overall utility.", "published": "2023-10-16 14:59:10", "link": "http://arxiv.org/abs/2310.10477v6", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DemoSG: Demonstration-enhanced Schema-guided Generation for Low-resource\n  Event Extraction", "abstract": "Most current Event Extraction (EE) methods focus on the high-resource\nscenario, which requires a large amount of annotated data and can hardly be\napplied to low-resource domains. To address EE more effectively with limited\nresources, we propose the Demonstration-enhanced Schema-guided Generation\n(DemoSG) model, which benefits low-resource EE from two aspects: Firstly, we\npropose the demonstration-based learning paradigm for EE to fully use the\nannotated data, which transforms them into demonstrations to illustrate the\nextraction process and help the model learn effectively. Secondly, we formulate\nEE as a natural language generation task guided by schema-based prompts,\nthereby leveraging label semantics and promoting knowledge transfer in\nlow-resource scenarios. We conduct extensive experiments under in-domain and\ndomain adaptation low-resource settings on three datasets, and study the\nrobustness of DemoSG. The results show that DemoSG significantly outperforms\ncurrent methods in low-resource scenarios.", "published": "2023-10-16 15:02:37", "link": "http://arxiv.org/abs/2310.10481v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Type-aware Decoding via Explicitly Aggregating Event Information for\n  Document-level Event Extraction", "abstract": "Document-level event extraction (DEE) faces two main challenges:\narguments-scattering and multi-event. Although previous methods attempt to\naddress these challenges, they overlook the interference of event-unrelated\nsentences during event detection and neglect the mutual interference of\ndifferent event roles during argument extraction. Therefore, this paper\nproposes a novel Schema-based Explicitly Aggregating~(SEA) model to address\nthese limitations. SEA aggregates event information into event type and role\nrepresentations, enabling the decoding of event records based on specific\ntype-aware representations. By detecting each event based on its event type\nrepresentation, SEA mitigates the interference caused by event-unrelated\ninformation. Furthermore, SEA extracts arguments for each role based on its\nrole-aware representations, reducing mutual interference between different\nroles. Experimental results on the ChFinAnn and DuEE-fin datasets show that SEA\noutperforms the SOTA methods.", "published": "2023-10-16 15:10:42", "link": "http://arxiv.org/abs/2310.10487v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantic Parsing by Large Language Models for Intricate Updating\n  Strategies of Zero-Shot Dialogue State Tracking", "abstract": "Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring\nand annotating task-oriented dialogues, which can be time-consuming and costly.\nHowever, DST extends beyond simple slot-filling and requires effective updating\nstrategies for tracking dialogue state as conversations progress. In this\npaper, we propose ParsingDST, a new In-Context Learning (ICL) method, to\nintroduce additional intricate updating strategies in zero-shot DST. Our\napproach reformulates the DST task by leveraging powerful Large Language Models\n(LLMs) and translating the original dialogue text to JSON through semantic\nparsing as an intermediate state. We also design a novel framework that\nincludes more modules to ensure the effectiveness of updating strategies in the\ntext-to-JSON process. Experimental results demonstrate that our approach\noutperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant\nimprovements in Joint Goal Accuracy (JGA) and slot accuracy compared to\nexisting ICL methods. Our code has been released.", "published": "2023-10-16 15:38:02", "link": "http://arxiv.org/abs/2310.10520v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generating Summaries with Controllable Readability Levels", "abstract": "Readability refers to how easily a reader can understand a written text.\nSeveral factors affect the readability level, such as the complexity of the\ntext, its subject matter, and the reader's background knowledge. Generating\nsummaries based on different readability levels is critical for enabling\nknowledge consumption by diverse audiences. However, current text generation\napproaches lack refined control, resulting in texts that are not customized to\nreaders' proficiency levels. In this work, we bridge this gap and study\ntechniques to generate summaries at specified readability levels. Unlike\nprevious methods that focus on a specific readability level (e.g., lay\nsummarization), we generate summaries with fine-grained control over their\nreadability. We develop three text generation techniques for controlling\nreadability: (1) instruction-based readability control, (2) reinforcement\nlearning to minimize the gap between requested and observed readability and (3)\na decoding approach that uses lookahead to estimate the readability of upcoming\ndecoding steps. We show that our generation methods significantly improve\nreadability control on news summarization (CNN/DM dataset), as measured by\nvarious readability metrics and human judgement, establishing strong baselines\nfor controllable readability in summarization.", "published": "2023-10-16 17:46:26", "link": "http://arxiv.org/abs/2310.10623v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Llemma: An Open Language Model For Mathematics", "abstract": "We present Llemma, a large language model for mathematics. We continue\npretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web\ndata containing mathematics, and mathematical code, yielding Llemma. On the\nMATH benchmark Llemma outperforms all known open base models, as well as the\nunreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is\ncapable of tool use and formal theorem proving without any further finetuning.\nWe openly release all artifacts, including 7 billion and 34 billion parameter\nmodels, the Proof-Pile-2, and code to replicate our experiments.", "published": "2023-10-16 17:54:07", "link": "http://arxiv.org/abs/2310.10631v3", "categories": ["cs.CL", "cs.AI", "cs.LO"], "primary_category": "cs.CL"}
{"title": "BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology", "abstract": "The ability to automatically generate accurate protocols for scientific\nexperiments would represent a major step towards the automation of science.\nLarge Language Models (LLMs) have impressive capabilities on a wide range of\ntasks, such as question answering and the generation of coherent text and code.\nHowever, LLMs can struggle with multi-step problems and long-term planning,\nwhich are crucial for designing scientific experiments. Moreover, evaluation of\nthe accuracy of scientific protocols is challenging, because experiments can be\ndescribed correctly in many different ways, require expert knowledge to\nevaluate, and cannot usually be executed automatically. Here we present an\nautomatic evaluation framework for the task of planning experimental protocols,\nand we introduce BioProt: a dataset of biology protocols with corresponding\npseudocode representations. To measure performance on generating scientific\nprotocols, we use an LLM to convert a natural language protocol into\npseudocode, and then evaluate an LLM's ability to reconstruct the pseudocode\nfrom a high-level description and a list of admissible pseudocode functions. We\nevaluate GPT-3 and GPT-4 on this task and explore their robustness. We\nexternally validate the utility of pseudocode representations of text by\ngenerating accurate novel protocols using retrieved pseudocode, and we run a\ngenerated protocol successfully in our biological laboratory. Our framework is\nextensible to the evaluation and improvement of language model planning\nabilities in other areas of science or other areas that lack automatic\nevaluation.", "published": "2023-10-16 17:54:20", "link": "http://arxiv.org/abs/2310.10632v1", "categories": ["cs.CL", "cs.AI", "cs.RO"], "primary_category": "cs.CL"}
{"title": "In-context Pretraining: Language Modeling Beyond Document Boundaries", "abstract": "Large language models (LMs) are currently trained to predict tokens given\ndocument prefixes, enabling them to directly perform long-form generation and\nprompting-style tasks which can be reduced to document completion. Existing\npretraining pipelines train LMs by concatenating random sets of short documents\nto create input contexts but the prior documents provide no signal for\npredicting the next document. We instead present In-Context Pretraining, a new\napproach where language models are pretrained on a sequence of related\ndocuments, thereby explicitly encouraging them to read and reason across\ndocument boundaries. We can do In-Context Pretraining by simply changing the\ndocument ordering so that each context contains related documents, and directly\napplying existing pretraining pipelines. However, this document sorting problem\nis challenging. There are billions of documents and we would like the sort to\nmaximize contextual similarity for every document without repeating any data.\nTo do this, we introduce approximate algorithms for finding related documents\nwith efficient nearest neighbor search and constructing coherent input contexts\nwith a graph traversal algorithm. Our experiments show In-Context Pretraining\noffers a simple and scalable approach to significantly enhance LMs'performance:\nwe see notable improvements in tasks that require more complex contextual\nreasoning, including in-context learning (+8%), reading comprehension (+15%),\nfaithfulness to previous contexts (+16%), long-context reasoning (+5%), and\nretrieval augmentation (+9%).", "published": "2023-10-16 17:57:12", "link": "http://arxiv.org/abs/2310.10638v6", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interactive Task Planning with Language Models", "abstract": "An interactive robot framework accomplishes long-horizon task planning and\ncan easily generalize to new goals and distinct tasks, even during execution.\nHowever, most traditional methods require predefined module design, making it\nhard to generalize to different goals. Recent large language model based\napproaches can allow for more open-ended planning but often require heavy\nprompt engineering or domain specific pretrained models. To tackle this, we\npropose a simple framework that achieves interactive task planning with\nlanguage models by incorporating both high-level planning and low-level skill\nexecution through function calling, leveraging pretrained vision models to\nground the scene in language. We verify the robustness of our system on the\nreal world task of making milk tea drinks. Our system is able to generate novel\nhigh-level instructions for unseen objectives and successfully accomplishes\nuser tasks. Furthermore, when the user sends a new request, our system is able\nto replan accordingly with precision based on the new request, task guidelines\nand previously executed steps. Our approach is easy to adapt to different tasks\nby simply substituting the task guidelines, without the need for additional\ncomplex prompt engineering. Please check more details on our\nhttps://wuphilipp.github.io/itp_site and https://youtu.be/TrKLuyv26_g.", "published": "2023-10-16 17:59:12", "link": "http://arxiv.org/abs/2310.10645v2", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.RO"}
{"title": "Optimized Tokenization for Transcribed Error Correction", "abstract": "The challenges facing speech recognition systems, such as variations in\npronunciations, adverse audio conditions, and the scarcity of labeled data,\nemphasize the necessity for a post-processing step that corrects recurring\nerrors. Previous research has shown the advantages of employing dedicated error\ncorrection models, yet training such models requires large amounts of labeled\ndata which is not easily obtained. To overcome this limitation, synthetic\ntranscribed-like data is often utilized, however, bridging the distribution gap\nbetween transcribed errors and synthetic noise is not trivial. In this paper,\nwe demonstrate that the performance of correction models can be significantly\nincreased by training solely using synthetic data. Specifically, we empirically\nshow that: (1) synthetic data generated using the error distribution derived\nfrom a set of transcribed data outperforms the common approach of applying\nrandom perturbations; (2) applying language-specific adjustments to the\nvocabulary of a BPE tokenizer strike a balance between adapting to unseen\ndistributions and retaining knowledge of transcribed errors. We showcase the\nbenefits of these key observations, and evaluate our approach using multiple\nlanguages, speech recognition systems and prominent speech recognition\ndatasets.", "published": "2023-10-16 12:14:21", "link": "http://arxiv.org/abs/2310.10704v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards reducing hallucination in extracting information from financial\n  reports using Large Language Models", "abstract": "For a financial analyst, the question and answer (Q\\&A) segment of the\ncompany financial report is a crucial piece of information for various analysis\nand investment decisions. However, extracting valuable insights from the Q\\&A\nsection has posed considerable challenges as the conventional methods such as\ndetailed reading and note-taking lack scalability and are susceptible to human\nerrors, and Optical Character Recognition (OCR) and similar techniques\nencounter difficulties in accurately processing unstructured transcript text,\noften missing subtle linguistic nuances that drive investor decisions. Here, we\ndemonstrate the utilization of Large Language Models (LLMs) to efficiently and\nrapidly extract information from earnings report transcripts while ensuring\nhigh accuracy transforming the extraction process as well as reducing\nhallucination by combining retrieval-augmented generation technique as well as\nmetadata. We evaluate the outcomes of various LLMs with and without using our\nproposed approach based on various objective metrics for evaluating Q\\&A\nsystems, and empirically demonstrate superiority of our method.", "published": "2023-10-16 18:45:38", "link": "http://arxiv.org/abs/2310.10760v1", "categories": ["cs.CL", "q-fin.PM", "q-fin.ST", "stat.AP"], "primary_category": "cs.CL"}
{"title": "BiomedJourney: Counterfactual Biomedical Image Generation by\n  Instruction-Learning from Multimodal Patient Journeys", "abstract": "Rapid progress has been made in instruction-learning for image editing with\nnatural-language instruction, as exemplified by InstructPix2Pix. In\nbiomedicine, such methods can be applied to counterfactual image generation,\nwhich helps differentiate causal structure from spurious correlation and\nfacilitate robust image interpretation for disease progression modeling.\nHowever, generic image-editing models are ill-suited for the biomedical domain,\nand counterfactual biomedical image generation is largely underexplored. In\nthis paper, we present BiomedJourney, a novel method for counterfactual\nbiomedical image generation by instruction-learning from multimodal patient\njourneys. Given a patient with two biomedical images taken at different time\npoints, we use GPT-4 to process the corresponding imaging reports and generate\na natural language description of disease progression. The resulting triples\n(prior image, progression description, new image) are then used to train a\nlatent diffusion model for counterfactual biomedical image generation. Given\nthe relative scarcity of image time series data, we introduce a two-stage\ncurriculum that first pretrains the denoising network using the much more\nabundant single image-report pairs (with dummy prior image), and then continues\ntraining using the counterfactual triples. Experiments using the standard\nMIMIC-CXR dataset demonstrate the promise of our method. In a comprehensive\nbattery of tests on counterfactual medical image generation, BiomedJourney\nsubstantially outperforms prior state-of-the-art methods in instruction image\nediting and medical image generation such as InstructPix2Pix and RoentGen. To\nfacilitate future study in counterfactual medical generation, we plan to\nrelease our instruction-learning code and pretrained models.", "published": "2023-10-16 18:59:31", "link": "http://arxiv.org/abs/2310.10765v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Survey of Vulnerabilities in Large Language Models Revealed by\n  Adversarial Attacks", "abstract": "Large Language Models (LLMs) are swiftly advancing in architecture and\ncapability, and as they integrate more deeply into complex systems, the urgency\nto scrutinize their security properties grows. This paper surveys research in\nthe emerging interdisciplinary field of adversarial attacks on LLMs, a subfield\nof trustworthy ML, combining the perspectives of Natural Language Processing\nand Security. Prior work has shown that even safety-aligned LLMs (via\ninstruction tuning and reinforcement learning through human feedback) can be\nsusceptible to adversarial attacks, which exploit weaknesses and mislead AI\nsystems, as evidenced by the prevalence of `jailbreak' attacks on models like\nChatGPT and Bard. In this survey, we first provide an overview of large\nlanguage models, describe their safety alignment, and categorize existing\nresearch based on various learning structures: textual-only attacks,\nmulti-modal attacks, and additional attack methods specifically targeting\ncomplex systems, such as federated learning or multi-agent systems. We also\noffer comprehensive remarks on works that focus on the fundamental sources of\nvulnerabilities and potential defenses. To make this field more accessible to\nnewcomers, we present a systematic review of existing works, a structured\ntypology of adversarial attack concepts, and additional resources, including\nslides for presentations on related topics at the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (ACL'24).", "published": "2023-10-16 21:37:24", "link": "http://arxiv.org/abs/2310.10844v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DavIR: Data Selection via Implicit Reward for Large Language Models", "abstract": "We introduce DavIR, a model-based data selection method for post-training\nLarge Language Models. DavIR generalizes Reducible Holdout Loss to core-set\nselection problem of causal language modeling, and quantifies the learnability\nof a given datum with respect to a pre-trained LLM based on relative reduction\nin loss during fine-tuning, a metric we show to be closely related to the\nimplicit reward model described in Direct Preference Optimization (DPO). We\nshow that 6% of Alpaca dataset selected with DavIR can steer both the LLaMA and\nGemma model family to produce superior performance compared to the same models\ntrained on the full 52K dataset. We also show that Alpaca dataset compressed\nwith DavIR can be combined with GSM8K dataset to effectively balance\nopen-domain freeform QA and mathematical reasoning capabilities. Finally, we\napply the DavIR objective to DPO and develop a normalized DavIR-DPO objective\nwhich improves alignment performance of Zephyr-7B-SFT model by 8% (relative) on\nAlpacaEval, compared against training on vanilla DPO objective.", "published": "2023-10-16 07:26:24", "link": "http://arxiv.org/abs/2310.13008v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ForceGen: End-to-end de novo protein generation based on nonlinear\n  mechanical unfolding responses using a protein language diffusion model", "abstract": "Through evolution, nature has presented a set of remarkable protein\nmaterials, including elastins, silks, keratins and collagens with superior\nmechanical performances that play crucial roles in mechanobiology. However,\ngoing beyond natural designs to discover proteins that meet specified\nmechanical properties remains challenging. Here we report a generative model\nthat predicts protein designs to meet complex nonlinear mechanical\nproperty-design objectives. Our model leverages deep knowledge on protein\nsequences from a pre-trained protein language model and maps mechanical\nunfolding responses to create novel proteins. Via full-atom molecular\nsimulations for direct validation, we demonstrate that the designed proteins\nare novel, and fulfill the targeted mechanical properties, including unfolding\nenergy and mechanical strength, as well as the detailed unfolding\nforce-separation curves. Our model offers rapid pathways to explore the\nenormous mechanobiological protein sequence space unconstrained by biological\nsynthesis, using mechanical features as target to enable the discovery of\nprotein materials with superior mechanical properties.", "published": "2023-10-16 17:31:34", "link": "http://arxiv.org/abs/2310.10605v3", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall", "cs.CL", "cs.LG", "q-bio.BM"], "primary_category": "cond-mat.mtrl-sci"}
{"title": "Real-time Speech Enhancement and Separation with a Unified Deep Neural\n  Network for Single/Dual Talker Scenarios", "abstract": "This paper introduces a practical approach for leveraging a real-time deep\nlearning model to alternate between speech enhancement and joint speech\nenhancement and separation depending on whether the input mixture contains one\nor two active speakers. Scale-invariant signal-to-distortion ratio (SI-SDR) has\nshown to be a highly effective training measure in time-domain speech\nseparation. However, the SI-SDR metric is ill-defined for zero-energy target\nsignals, which is a problem when training a speech separation model using\nutterances with varying numbers of talkers. Unlike existing solutions that\nfocus on modifying the loss function to accommodate zero-energy target signals,\nthe proposed approach circumvents this problem by training the model to extract\nspeech on both its output channels regardless if the input is a single or\ndual-talker mixture. A lightweight speaker overlap detection (SOD) module is\nalso introduced to differentiate between single and dual-talker segments in\nreal-time. The proposed module takes advantage of the new formulation by\noperating directly on the separated masks, given by the separation model,\ninstead of the original mixture, thus effectively simplifying the detection\ntask. Experimental results show that the proposed training approach outperforms\nexisting solutions, and the SOD module exhibits high accuracy.", "published": "2023-10-16 03:02:29", "link": "http://arxiv.org/abs/2310.10026v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Advancing Audio Emotion and Intent Recognition with Large Pre-Trained\n  Models and Bayesian Inference", "abstract": "Large pre-trained models are essential in paralinguistic systems,\ndemonstrating effectiveness in tasks like emotion recognition and stuttering\ndetection. In this paper, we employ large pre-trained models for the ACM\nMultimedia Computational Paralinguistics Challenge, addressing the Requests and\nEmotion Share tasks. We explore audio-only and hybrid solutions leveraging\naudio and text modalities. Our empirical results consistently show the\nsuperiority of the hybrid approaches over the audio-only models. Moreover, we\nintroduce a Bayesian layer as an alternative to the standard linear output\nlayer. The multimodal fusion approach achieves an 85.4% UAR on HC-Requests and\n60.2% on HC-Complaints. The ensemble model for the Emotion Share task yields\nthe best rho value of .614. The Bayesian wav2vec2 approach, explored in this\nstudy, allows us to easily build ensembles, at the cost of fine-tuning only one\nmodel. Moreover, we can have usable confidence values instead of the usual\noverconfident posterior probabilities.", "published": "2023-10-16 08:40:11", "link": "http://arxiv.org/abs/2310.10179v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Generation or Replication: Auscultating Audio Latent Diffusion Models", "abstract": "The introduction of audio latent diffusion models possessing the ability to\ngenerate realistic sound clips on demand from a text description has the\npotential to revolutionize how we work with audio. In this work, we make an\ninitial attempt at understanding the inner workings of audio latent diffusion\nmodels by investigating how their audio outputs compare with the training data,\nsimilar to how a doctor auscultates a patient by listening to the sounds of\ntheir organs. Using text-to-audio latent diffusion models trained on the\nAudioCaps dataset, we systematically analyze memorization behavior as a\nfunction of training set size. We also evaluate different retrieval metrics for\nevidence of training data memorization, finding the similarity between mel\nspectrograms to be more robust in detecting matches than learned embedding\nvectors. In the process of analyzing memorization in audio latent diffusion\nmodels, we also discover a large amount of duplicated audio clips within the\nAudioCaps database.", "published": "2023-10-16 17:31:26", "link": "http://arxiv.org/abs/2310.10604v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Detecting Speech Abnormalities with a Perceiver-based Sequence\n  Classifier that Leverages a Universal Speech Model", "abstract": "We propose a Perceiver-based sequence classifier to detect abnormalities in\nspeech reflective of several neurological disorders. We combine this classifier\nwith a Universal Speech Model (USM) that is trained (unsupervised) on 12\nmillion hours of diverse audio recordings. Our model compresses long sequences\ninto a small set of class-specific latent representations and a factorized\nprojection is used to predict different attributes of the disordered input\nspeech. The benefit of our approach is that it allows us to model different\nregions of the input for different classes and is at the same time data\nefficient. We evaluated the proposed model extensively on a curated corpus from\nthe Mayo Clinic. Our model outperforms standard transformer (80.9%) and\nperceiver (81.8%) models and achieves an average accuracy of 83.1%. With\nlimited task-specific data, we find that pretraining is important and\nsurprisingly pretraining with the unrelated automatic speech recognition (ASR)\ntask is also beneficial. Encodings from the middle layers provide a mix of both\nacoustic and phonetic information and achieve best prediction results compared\nto just using the final layer encodings (83.1% vs. 79.6%). The results are\npromising and with further refinements may help clinicians detect speech\nabnormalities without needing access to highly specialized speech-language\npathologists.", "published": "2023-10-16 21:07:12", "link": "http://arxiv.org/abs/2310.13010v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "BeatDance: A Beat-Based Model-Agnostic Contrastive Learning Framework\n  for Music-Dance Retrieval", "abstract": "Dance and music are closely related forms of expression, with mutual\nretrieval between dance videos and music being a fundamental task in various\nfields like education, art, and sports. However, existing methods often suffer\nfrom unnatural generation effects or fail to fully explore the correlation\nbetween music and dance. To overcome these challenges, we propose BeatDance, a\nnovel beat-based model-agnostic contrastive learning framework. BeatDance\nincorporates a Beat-Aware Music-Dance InfoExtractor, a Trans-Temporal Beat\nBlender, and a Beat-Enhanced Hubness Reducer to improve dance-music retrieval\nperformance by utilizing the alignment between music beats and dance movements.\nWe also introduce the Music-Dance (MD) dataset, a large-scale collection of\nover 10,000 music-dance video pairs for training and testing. Experimental\nresults on the MD dataset demonstrate the superiority of our method over\nexisting baselines, achieving state-of-the-art performance. The code and\ndataset will be made public available upon acceptance.", "published": "2023-10-16 11:36:38", "link": "http://arxiv.org/abs/2310.10300v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LocSelect: Target Speaker Localization with an Auditory Selective\n  Hearing Mechanism", "abstract": "The prevailing noise-resistant and reverberation-resistant localization\nalgorithms primarily emphasize separating and providing directional output for\neach speaker in multi-speaker scenarios, without association with the identity\nof speakers. In this paper, we present a target speaker localization algorithm\nwith a selective hearing mechanism. Given a reference speech of the target\nspeaker, we first produce a speaker-dependent spectrogram mask to eliminate\ninterfering speakers' speech. Subsequently, a Long short-term memory (LSTM)\nnetwork is employed to extract the target speaker's location from the filtered\nspectrogram. Experiments validate the superiority of our proposed method over\nthe existing algorithms for different scale invariant signal-to-noise ratios\n(SNR) conditions. Specifically, at SNR = -10 dB, our proposed network LocSelect\nachieves a mean absolute error (MAE) of 3.55 and an accuracy (ACC) of 87.40%.", "published": "2023-10-16 15:19:05", "link": "http://arxiv.org/abs/2310.10497v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unsupervised Lead Sheet Generation via Semantic Compression", "abstract": "Lead sheets have become commonplace in generative music research, being used\nas an initial compressed representation for downstream tasks like multitrack\nmusic generation and automatic arrangement. Despite this, researchers have\noften fallen back on deterministic reduction methods (such as the skyline\nalgorithm) to generate lead sheets when seeking paired lead sheets and full\nscores, with little attention being paid toward the quality of the lead sheets\nthemselves and how they accurately reflect their orchestrated counterparts. To\naddress these issues, we propose the problem of conditional lead sheet\ngeneration (i.e. generating a lead sheet given its full score version), and\nshow that this task can be formulated as an unsupervised music compression\ntask, where the lead sheet represents a compressed latent version of the score.\nWe introduce a novel model, called Lead-AE, that models the lead sheets as a\ndiscrete subselection of the original sequence, using a differentiable top-k\noperator to allow for controllable local sparsity constraints. Across both\nautomatic proxy tasks and direct human evaluations, we find that our method\nimproves upon the established deterministic baseline and produces coherent\nreductions of large multitrack scores.", "published": "2023-10-16 19:12:20", "link": "http://arxiv.org/abs/2310.10772v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
