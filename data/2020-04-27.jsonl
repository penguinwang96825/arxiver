{"title": "On the Importance of Word and Sentence Representation Learning in\n  Implicit Discourse Relation Classification", "abstract": "Implicit discourse relation classification is one of the most difficult parts\nin shallow discourse parsing as the relation prediction without explicit\nconnectives requires the language understanding at both the text span level and\nthe sentence level. Previous studies mainly focus on the interactions between\ntwo arguments. We argue that a powerful contextualized representation module, a\nbilateral multi-perspective matching module, and a global information fusion\nmodule are all important to implicit discourse analysis. We propose a novel\nmodel to combine these modules together. Extensive experiments show that our\nproposed model outperforms BERT and other state-of-the-art systems on the PDTB\ndataset by around 8% and CoNLL 2016 datasets around 16%. We also analyze the\neffectiveness of different modules in the implicit discourse relation\nclassification task and demonstrate how different levels of representation\nlearning can affect the results.", "published": "2020-04-27 07:41:02", "link": "http://arxiv.org/abs/2004.12617v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less\n  Forgetting", "abstract": "Deep pretrained language models have achieved great success in the way of\npretraining first and then fine-tuning. But such a sequential transfer learning\nparadigm often confronts the catastrophic forgetting problem and leads to\nsub-optimal performance. To fine-tune with less forgetting, we propose a recall\nand learn mechanism, which adopts the idea of multi-task learning and jointly\nlearns pretraining tasks and downstream tasks. Specifically, we propose a\nPretraining Simulation mechanism to recall the knowledge from pretraining tasks\nwithout data, and an Objective Shifting mechanism to focus the learning on\ndownstream tasks gradually. Experiments show that our method achieves\nstate-of-the-art performance on the GLUE benchmark. Our method also enables\nBERT-base to achieve better performance than directly fine-tuning of\nBERT-large. Further, we provide the open-source RecAdam optimizer, which\nintegrates the proposed mechanisms into Adam optimizer, to facility the NLP\ncommunity.", "published": "2020-04-27 08:59:57", "link": "http://arxiv.org/abs/2004.12651v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lexically Constrained Neural Machine Translation with Levenshtein\n  Transformer", "abstract": "This paper proposes a simple and effective algorithm for incorporating\nlexical constraints in neural machine translation. Previous work either\nrequired re-training existing models with the lexical constraints or\nincorporating them during beam search decoding with significantly higher\ncomputational overheads. Leveraging the flexibility and speed of a recently\nproposed Levenshtein Transformer model (Gu et al., 2019), our method injects\nterminology constraints at inference time without any impact on decoding speed.\nOur method does not require any modification to the training procedure and can\nbe easily applied at runtime with custom dictionaries. Experiments on\nEnglish-German WMT datasets show that our approach improves an unconstrained\nbaseline and previous approaches.", "published": "2020-04-27 09:59:27", "link": "http://arxiv.org/abs/2004.12681v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Graphs for Generating Deep Questions", "abstract": "This paper proposes the problem of Deep Question Generation (DQG), which aims\nto generate complex questions that require reasoning over multiple pieces of\ninformation of the input passage. In order to capture the global structure of\nthe document and facilitate reasoning, we propose a novel framework which first\nconstructs a semantic-level graph for the input document and then encodes the\nsemantic graph by introducing an attention-based GGNN (Att-GGNN). Afterwards,\nwe fuse the document-level and graph-level representations to perform joint\ntraining of content selection and question decoding. On the HotpotQA\ndeep-question centric dataset, our model greatly improves performance over\nquestions requiring reasoning over multiple facts, leading to state-of-the-art\nperformance. The code is publicly available at\nhttps://github.com/WING-NUS/SG-Deep-Question-Generation.", "published": "2020-04-27 10:52:52", "link": "http://arxiv.org/abs/2004.12704v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Screenplay Summarization Using Latent Narrative Structure", "abstract": "Most general-purpose extractive summarization models are trained on news\narticles, which are short and present all important information upfront. As a\nresult, such models are biased on position and often perform a smart selection\nof sentences from the beginning of the document. When summarizing long\nnarratives, which have complex structure and present information piecemeal,\nsimple position heuristics are not sufficient. In this paper, we propose to\nexplicitly incorporate the underlying structure of narratives into general\nunsupervised and supervised extractive summarization models. We formalize\nnarrative structure in terms of key narrative events (turning points) and treat\nit as latent in order to summarize screenplays (i.e., extract an optimal\nsequence of scenes). Experimental results on the CSI corpus of TV screenplays,\nwhich we augment with scene-level summarization labels, show that latent\nturning points correlate with important aspects of a CSI episode and improve\nsummarization performance over general extractive algorithms leading to more\ncomplete and diverse summaries.", "published": "2020-04-27 11:54:19", "link": "http://arxiv.org/abs/2004.12727v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Augmenting Transformers with KNN-Based Composite Memory for Dialogue", "abstract": "Various machine learning tasks can benefit from access to external\ninformation of different modalities, such as text and images. Recent work has\nfocused on learning architectures with large memories capable of storing this\nknowledge. We propose augmenting generative Transformer neural networks with\nKNN-based Information Fetching (KIF) modules. Each KIF module learns a read\noperation to access fixed external knowledge. We apply these modules to\ngenerative dialog modeling, a challenging task where information must be\nflexibly retrieved and incorporated to maintain the topic and flow of\nconversation. We demonstrate the effectiveness of our approach by identifying\nrelevant knowledge required for knowledgeable but engaging dialog from\nWikipedia, images, and human-written dialog utterances, and show that\nleveraging this retrieved information improves model performance, measured by\nautomatic and human evaluation.", "published": "2020-04-27 12:37:26", "link": "http://arxiv.org/abs/2004.12744v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Gutenberg Dialogue Dataset", "abstract": "Large datasets are essential for neural modeling of many NLP tasks. Current\npublicly available open-domain dialogue datasets offer a trade-off between\nquality (e.g., DailyDialog) and size (e.g., Opensubtitles). We narrow this gap\nby building a high-quality dataset of 14.8M utterances in English, and smaller\ndatasets in German, Dutch, Spanish, Portuguese, Italian, and Hungarian. We\nextract and process dialogues from public-domain books made available by\nProject Gutenberg. We describe our dialogue extraction pipeline, analyze the\neffects of the various heuristics used, and present an error analysis of\nextracted dialogues. Finally, we conduct experiments showing that better\nresponse quality can be achieved in zero-shot and finetuning settings by\ntraining on our data than on the larger but much noisier Opensubtitles dataset.\nOur open-source pipeline (https://github.com/ricsinaruto/gutenberg-dialog) can\nbe extended to further languages with little additional effort. Researchers can\nalso build their versions of existing datasets by adjusting various trade-off\nparameters. We also built a web demo for interacting with our models:\nhttps://ricsinaruto.github.io/chatbot.html.", "published": "2020-04-27 12:52:20", "link": "http://arxiv.org/abs/2004.12752v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DeSePtion: Dual Sequence Prediction and Adversarial Examples for\n  Improved Fact-Checking", "abstract": "The increased focus on misinformation has spurred development of data and\nsystems for detecting the veracity of a claim as well as retrieving\nauthoritative evidence. The Fact Extraction and VERification (FEVER) dataset\nprovides such a resource for evaluating end-to-end fact-checking, requiring\nretrieval of evidence from Wikipedia to validate a veracity prediction. We show\nthat current systems for FEVER are vulnerable to three categories of realistic\nchallenges for fact-checking -- multiple propositions, temporal reasoning, and\nambiguity and lexical variation -- and introduce a resource with these types of\nclaims. Then we present a system designed to be resilient to these \"attacks\"\nusing multiple pointer networks for document selection and jointly modeling a\nsequence of evidence sentences and veracity relation predictions. We find that\nin handling these attacks we obtain state-of-the-art results on FEVER, largely\ndue to improved evidence retrieval.", "published": "2020-04-27 15:18:49", "link": "http://arxiv.org/abs/2004.12864v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intelligent Translation Memory Matching and Retrieval with Sentence\n  Encoders", "abstract": "Matching and retrieving previously translated segments from a Translation\nMemory is the key functionality in Translation Memories systems. However this\nmatching and retrieving process is still limited to algorithms based on edit\ndistance which we have identified as a major drawback in Translation Memories\nsystems. In this paper we introduce sentence encoders to improve the matching\nand retrieving process in Translation Memories systems - an effective and\nefficient solution to replace edit distance based algorithms.", "published": "2020-04-27 15:54:29", "link": "http://arxiv.org/abs/2004.12894v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SCDE: Sentence Cloze Dataset with High Quality Distractors From\n  Examinations", "abstract": "We introduce SCDE, a dataset to evaluate the performance of computational\nmodels through sentence prediction. SCDE is a human-created sentence cloze\ndataset, collected from public school English examinations. Our task requires a\nmodel to fill up multiple blanks in a passage from a shared candidate set with\ndistractors designed by English teachers. Experimental results demonstrate that\nthis task requires the use of non-local, discourse-level context beyond the\nimmediate sentence neighborhood. The blanks require joint solving and\nsignificantly impair each other's context. Furthermore, through ablations, we\nshow that the distractors are of high quality and make the task more\nchallenging. Our experiments show that there is a significant performance gap\nbetween advanced models (72%) and humans (87%), encouraging future models to\nbridge this gap.", "published": "2020-04-27 16:48:54", "link": "http://arxiv.org/abs/2004.12934v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural language processing for achieving sustainable development: the\n  case of neural labelling to enhance community profiling", "abstract": "In recent years, there has been an increasing interest in the application of\nArtificial Intelligence - and especially Machine Learning - to the field of\nSustainable Development (SD). However, until now, NLP has not been applied in\nthis context. In this research paper, we show the high potential of NLP\napplications to enhance the sustainability of projects. In particular, we focus\non the case of community profiling in developing countries, where, in contrast\nto the developed world, a notable data gap exists. In this context, NLP could\nhelp to address the cost and time barrier of structuring qualitative data that\nprohibits its widespread use and associated benefits. We propose the new task\nof Automatic UPV classification, which is an extreme multi-class multi-label\nclassification problem. We release Stories2Insights, an expert-annotated\ndataset, provide a detailed corpus analysis, and implement a number of strong\nneural baselines to address the task. Experimental results show that the\nproblem is challenging, and leave plenty of room for future research at the\nintersection of NLP and SD.", "published": "2020-04-27 16:51:21", "link": "http://arxiv.org/abs/2004.12935v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Octa: Omissions and Conflicts in Target-Aspect Sentiment Analysis", "abstract": "Sentiments in opinionated text are often determined by both aspects and\ntarget words (or targets). We observe that targets and aspects interrelate in\nsubtle ways, often yielding conflicting sentiments. Thus, a naive aggregation\nof sentiments from aspects and targets treated separately, as in existing\nsentiment analysis models, impairs performance.\n  We propose Octa, an approach that jointly considers aspects and targets when\ninferring sentiments. To capture and quantify relationships between targets and\ncontext words, Octa uses a selective self-attention mechanism that handles\nimplicit or missing targets. Specifically, Octa involves two layers of\nattention mechanisms for, respectively, selective attention between targets and\ncontext words and attention over words based on aspects. On benchmark datasets,\nOcta outperforms leading models by a large margin, yielding (absolute) gains in\naccuracy of 1.6% to 4.3%.", "published": "2020-04-27 20:11:50", "link": "http://arxiv.org/abs/2004.13150v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simultaneous Translation Policies: From Fixed to Adaptive", "abstract": "Adaptive policies are better than fixed policies for simultaneous\ntranslation, since they can flexibly balance the tradeoff between translation\nquality and latency based on the current context information. But previous\nmethods on obtaining adaptive policies either rely on complicated training\nprocess, or underperform simple fixed policies. We design an algorithm to\nachieve adaptive policies via a simple heuristic composition of a set of fixed\npolicies. Experiments on Chinese -> English and German -> English show that our\nadaptive policies can outperform fixed ones by up to 4 BLEU points for the same\nlatency, and more surprisingly, it even surpasses the BLEU score of\nfull-sentence translation in the greedy mode (and very close to beam mode), but\nwith much lower latency.", "published": "2020-04-27 20:56:20", "link": "http://arxiv.org/abs/2004.13169v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Summary of the First Workshop on Language Technology for Language\n  Documentation and Revitalization", "abstract": "Despite recent advances in natural language processing and other language\ntechnology, the application of such technology to language documentation and\nconservation has been limited. In August 2019, a workshop was held at Carnegie\nMellon University in Pittsburgh to attempt to bring together language community\nmembers, documentary linguists, and technologists to discuss how to bridge this\ngap and create prototypes of novel and practical language revitalization\ntechnologies. This paper reports the results of this workshop, including issues\ndiscussed, and various conceived and implemented technologies for nine\nlanguages: Arapaho, Cayuga, Inuktitut, Irish Gaelic, Kidaw'ida, Kwak'wala,\nOjibwe, San Juan Quiahije Chatino, and Seneca.", "published": "2020-04-27 22:55:55", "link": "http://arxiv.org/abs/2004.13203v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Faster Depth-Adaptive Transformers", "abstract": "Depth-adaptive neural networks can dynamically adjust depths according to the\nhardness of input words, and thus improve efficiency. The main challenge is how\nto measure such hardness and decide the required depths (i.e., layers) to\nconduct. Previous works generally build a halting unit to decide whether the\ncomputation should continue or stop at each layer. As there is no specific\nsupervision of depth selection, the halting unit may be under-optimized and\ninaccurate, which results in suboptimal and unstable performance when modeling\nsentences. In this paper, we get rid of the halting unit and estimate the\nrequired depths in advance, which yields a faster depth-adaptive model.\nSpecifically, two approaches are proposed to explicitly measure the hardness of\ninput words and estimate corresponding adaptive depth, namely 1) mutual\ninformation (MI) based estimation and 2) reconstruction loss based estimation.\nWe conduct experiments on the text classification task with 24 datasets in\nvarious sizes and domains. Results confirm that our approaches can speed up the\nvanilla Transformer (up to 7x) while preserving high accuracy. Moreover,\nefficiency and robustness are significantly improved when compared with other\ndepth-adaptive approaches.", "published": "2020-04-27 15:08:10", "link": "http://arxiv.org/abs/2004.13542v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Batch Normalized Inference Network Keeps the KL Vanishing Away", "abstract": "Variational Autoencoder (VAE) is widely used as a generative model to\napproximate a model's posterior on latent variables by combining the amortized\nvariational inference and deep neural networks. However, when paired with\nstrong autoregressive decoders, VAE often converges to a degenerated local\noptimum known as \"posterior collapse\". Previous approaches consider the\nKullback Leibler divergence (KL) individual for each datapoint. We propose to\nlet the KL follow a distribution across the whole dataset, and analyze that it\nis sufficient to prevent posterior collapse by keeping the expectation of the\nKL's distribution positive. Then we propose Batch Normalized-VAE (BN-VAE), a\nsimple but effective approach to set a lower bound of the expectation by\nregularizing the distribution of the approximate posterior's parameters.\nWithout introducing any new model component or modifying the objective, our\napproach can avoid the posterior collapse effectively and efficiently. We\nfurther show that the proposed BN-VAE can be extended to conditional VAE\n(CVAE). Empirically, our approach surpasses strong autoregressive baselines on\nlanguage modeling, text classification and dialogue generation, and rivals more\ncomplex approaches while keeping almost the same training time as VAE.", "published": "2020-04-27 05:20:01", "link": "http://arxiv.org/abs/2004.12585v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "BLEU Neighbors: A Reference-less Approach to Automatic Evaluation", "abstract": "Evaluation is a bottleneck in the development of natural language generation\n(NLG) models. Automatic metrics such as BLEU rely on references, but for tasks\nsuch as open-ended generation, there are no references to draw upon. Although\nlanguage diversity can be estimated using statistical measures such as\nperplexity, measuring language quality requires human evaluation. However,\nbecause human evaluation at scale is slow and expensive, it is used sparingly;\nit cannot be used to rapidly iterate on NLG models, in the way BLEU is used for\nmachine translation. To this end, we propose BLEU Neighbors, a nearest\nneighbors model for estimating language quality by using the BLEU score as a\nkernel function. On existing datasets for chitchat dialogue and open-ended\nsentence generation, we find that -- on average -- the quality estimation from\na BLEU Neighbors model has a lower mean squared error and higher Spearman\ncorrelation with the ground truth than individual human annotators. Despite its\nsimplicity, BLEU Neighbors even outperforms state-of-the-art models on\nautomatically grading essays, including models that have access to a\ngold-standard reference essay.", "published": "2020-04-27 11:51:28", "link": "http://arxiv.org/abs/2004.12726v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ColBERT: Using BERT Sentence Embedding in Parallel Neural Networks for\n  Computational Humor", "abstract": "Automation of humor detection and rating has interesting use cases in modern\ntechnologies, such as humanoid robots, chatbots, and virtual assistants. In\nthis paper, we propose a novel approach for detecting and rating humor in short\ntexts based on a popular linguistic theory of humor. The proposed technical\nmethod initiates by separating sentences of the given text and utilizing the\nBERT model to generate embeddings for each one. The embeddings are fed to\nseparate lines of hidden layers in a neural network (one line for each\nsentence) to extract latent features. At last, the parallel lines are\nconcatenated to determine the congruity and other relationships between the\nsentences and predict the target value. We accompany the paper with a novel\ndataset for humor detection consisting of 200,000 formal short texts. In\naddition to evaluating our work on the novel dataset, we participated in a live\nmachine learning competition focused on rating humor in Spanish tweets. The\nproposed model obtained F1 scores of 0.982 and 0.869 in the humor detection\nexperiments which outperform general and state-of-the-art models. The\nevaluation performed on two contrasting settings confirm the strength and\nrobustness of the model and suggests two important factors in achieving high\naccuracy in the current task: 1) usage of sentence embeddings and 2) utilizing\nthe linguistic structure of humor in designing the proposed model.", "published": "2020-04-27 13:10:11", "link": "http://arxiv.org/abs/2004.12765v7", "categories": ["cs.CL", "cs.LG", "I.7; I.2"], "primary_category": "cs.CL"}
{"title": "LightPAFF: A Two-Stage Distillation Framework for Pre-training and\n  Fine-tuning", "abstract": "While pre-training and fine-tuning, e.g., BERT~\\citep{devlin2018bert},\nGPT-2~\\citep{radford2019language}, have achieved great success in language\nunderstanding and generation tasks, the pre-trained models are usually too big\nfor online deployment in terms of both memory cost and inference speed, which\nhinders them from practical online usage. In this paper, we propose LightPAFF,\na Lightweight Pre-training And Fine-tuning Framework that leverages two-stage\nknowledge distillation to transfer knowledge from a big teacher model to a\nlightweight student model in both pre-training and fine-tuning stages. In this\nway the lightweight model can achieve similar accuracy as the big teacher\nmodel, but with much fewer parameters and thus faster online inference speed.\nLightPAFF can support different pre-training methods (such as BERT, GPT-2 and\nMASS~\\citep{song2019mass}) and be applied to many downstream tasks. Experiments\non three language understanding tasks, three language modeling tasks and three\nsequence to sequence generation tasks demonstrate that while achieving similar\naccuracy with the big BERT, GPT-2 and MASS models, LightPAFF reduces the model\nsize by nearly 5x and improves online inference speed by 5x-7x.", "published": "2020-04-27 14:00:09", "link": "http://arxiv.org/abs/2004.12817v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late\n  Interaction over BERT", "abstract": "Recent progress in Natural Language Understanding (NLU) is driving fast-paced\nadvances in Information Retrieval (IR), largely owed to fine-tuning deep\nlanguage models (LMs) for document ranking. While remarkably effective, the\nranking models based on these LMs increase computational cost by orders of\nmagnitude over prior approaches, particularly as they must feed each\nquery-document pair through a massive neural network to compute a single\nrelevance score. To tackle this, we present ColBERT, a novel ranking model that\nadapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT\nintroduces a late interaction architecture that independently encodes the query\nand the document using BERT and then employs a cheap yet powerful interaction\nstep that models their fine-grained similarity. By delaying and yet retaining\nthis fine-granular interaction, ColBERT can leverage the expressiveness of deep\nLMs while simultaneously gaining the ability to pre-compute document\nrepresentations offline, considerably speeding up query processing. Beyond\nreducing the cost of re-ranking the documents retrieved by a traditional model,\nColBERT's pruning-friendly interaction mechanism enables leveraging\nvector-similarity indexes for end-to-end retrieval directly from a large\ndocument collection. We extensively evaluate ColBERT using two recent passage\nsearch datasets. Results show that ColBERT's effectiveness is competitive with\nexisting BERT-based models (and outperforms every non-BERT baseline), while\nexecuting two orders-of-magnitude faster and requiring four orders-of-magnitude\nfewer FLOPs per query.", "published": "2020-04-27 14:21:03", "link": "http://arxiv.org/abs/2004.12832v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference", "abstract": "Large-scale pre-trained language models such as BERT have brought significant\nimprovements to NLP applications. However, they are also notorious for being\nslow in inference, which makes them difficult to deploy in real-time\napplications. We propose a simple but effective method, DeeBERT, to accelerate\nBERT inference. Our approach allows samples to exit earlier without passing\nthrough the entire model. Experiments show that DeeBERT is able to save up to\n~40% inference time with minimal degradation in model quality. Further analyses\nshow different behaviors in the BERT transformer layers and also reveal their\nredundancy. Our work provides new ideas to efficiently apply deep\ntransformer-based models to downstream tasks. Code is available at\nhttps://github.com/castorini/DeeBERT.", "published": "2020-04-27 17:58:05", "link": "http://arxiv.org/abs/2004.12993v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Context-aware Helpfulness Prediction for Online Product Reviews", "abstract": "Modeling and prediction of review helpfulness has become more predominant due\nto proliferation of e-commerce websites and online shops. Since the\nfunctionality of a product cannot be tested before buying, people often rely on\ndifferent kinds of user reviews to decide whether or not to buy a product.\nHowever, quality reviews might be buried deep in the heap of a large amount of\nreviews. Therefore, recommending reviews to customers based on the review\nquality is of the essence. Since there is no direct indication of review\nquality, most reviews use the information that ''X out of Y'' users found the\nreview helpful for obtaining the review quality. However, this approach\nundermines helpfulness prediction because not all reviews have statistically\nabundant votes. In this paper, we propose a neural deep learning model that\npredicts the helpfulness score of a review. This model is based on\nconvolutional neural network (CNN) and a context-aware encoding mechanism which\ncan directly capture relationships between words irrespective of their distance\nin a long sequence. We validated our model on human annotated dataset and the\nresult shows that our model significantly outperforms existing models for\nhelpfulness prediction.", "published": "2020-04-27 18:19:26", "link": "http://arxiv.org/abs/2004.13078v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Conversational Question Answering over Passages by Leveraging Word\n  Proximity Networks", "abstract": "Question answering (QA) over text passages is a problem of long-standing\ninterest in information retrieval. Recently, the conversational setting has\nattracted attention, where a user asks a sequence of questions to satisfy her\ninformation needs around a topic. While this setup is a natural one and similar\nto humans conversing with each other, it introduces two key research\nchallenges: understanding the context left implicit by the user in follow-up\nquestions, and dealing with ad hoc question formulations. In this work, we\ndemonstrate CROWN (Conversational passage ranking by Reasoning Over Word\nNetworks): an unsupervised yet effective system for conversational QA with\npassage responses, that supports several modes of context propagation over\nmultiple turns. To this end, CROWN first builds a word proximity network (WPN)\nfrom large corpora to store statistically significant term co-occurrences. At\nanswering time, passages are ranked by a combination of their similarity to the\nquestion, and coherence of query terms within: these factors are measured by\nreading off node and edge weights from the WPN. CROWN provides an interface\nthat is both intuitive for end-users, and insightful for experts for\nreconfiguration to individual setups. CROWN was evaluated on TREC CAsT data,\nwhere it achieved above-median performance in a pool of neural methods.", "published": "2020-04-27 19:30:47", "link": "http://arxiv.org/abs/2004.13117v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "PuzzLing Machines: A Challenge on Learning From Small Data", "abstract": "Deep neural models have repeatedly proved excellent at memorizing surface\npatterns from large datasets for various ML and NLP benchmarks. They struggle\nto achieve human-like thinking, however, because they lack the skill of\niterative reasoning upon knowledge. To expose this problem in a new light, we\nintroduce a challenge on learning from small data, PuzzLing Machines, which\nconsists of Rosetta Stone puzzles from Linguistic Olympiads for high school\nstudents. These puzzles are carefully designed to contain only the minimal\namount of parallel text necessary to deduce the form of unseen expressions.\nSolving them does not require external information (e.g., knowledge bases,\nvisual signals) or linguistic expertise, but meta-linguistic awareness and\ndeductive skills. Our challenge contains around 100 puzzles covering a wide\nrange of linguistic phenomena from 81 languages. We show that both simple\nstatistical algorithms and state-of-the-art deep neural models perform\ninadequately on this challenge, as expected. We hope that this benchmark,\navailable at https://ukplab.github.io/PuzzLing-Machines/, inspires further\nefforts towards a new paradigm in NLP---one that is grounded in human-like\nreasoning and understanding.", "published": "2020-04-27 20:34:26", "link": "http://arxiv.org/abs/2004.13161v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation with Monte-Carlo Tree Search", "abstract": "Recent algorithms in machine translation have included a value network to\nassist the policy network when deciding which word to output at each step of\nthe translation. The addition of a value network helps the algorithm perform\nbetter on evaluation metrics like the BLEU score. After training the policy and\nvalue networks in a supervised setting, the policy and value networks can be\njointly improved through common actor-critic methods. The main idea of our\nproject is to instead leverage Monte-Carlo Tree Search (MCTS) to search for\ngood output words with guidance from a combined policy and value network\narchitecture in a similar fashion as AlphaZero. This network serves both as a\nlocal and a global look-ahead reference that uses the result of the search to\nimprove itself. Experiments using the IWLST14 German to English translation\ndataset show that our method outperforms the actor-critic methods used in\nrecent machine translation papers.", "published": "2020-04-27 01:03:26", "link": "http://arxiv.org/abs/2004.12527v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "\"Call me sexist, but...\": Revisiting Sexism Detection Using\n  Psychological Scales and Adversarial Samples", "abstract": "Research has focused on automated methods to effectively detect sexism\nonline. Although overt sexism seems easy to spot, its subtle forms and manifold\nexpressions are not. In this paper, we outline the different dimensions of\nsexism by grounding them in their implementation in psychological scales. From\nthe scales, we derive a codebook for sexism in social media, which we use to\nannotate existing and novel datasets, surfacing their limitations in breadth\nand validity with respect to the construct of sexism. Next, we leverage the\nannotated datasets to generate adversarial examples, and test the reliability\nof sexism detection methods. Results indicate that current machine learning\nmodels pick up on a very narrow set of linguistic markers of sexism and do not\ngeneralize well to out-of-domain examples. Yet, including diverse data and\nadversarial examples at training time results in models that generalize better\nand that are more robust to artifacts of data collection. By providing a\nscale-based codebook and insights regarding the shortcomings of the\nstate-of-the-art, we hope to contribute to the development of better and\nbroader models for sexism detection, including reflections on theory-driven\napproaches to data collection.", "published": "2020-04-27 13:07:46", "link": "http://arxiv.org/abs/2004.12764v2", "categories": ["cs.CY", "cs.CL", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Intuitive Contrasting Map for Antonym Embeddings", "abstract": "This paper shows that, modern word embeddings contain information that\ndistinguishes synonyms and antonyms despite small cosine similarities between\ncorresponding vectors. This information is encoded in the geometry of the\nembeddings and could be extracted with a straight-forward and intuitive\nmanifold learning procedure or a contrasting map. Such a map is trained on a\nsmall labeled subset of the data and can produce new embeddings that explicitly\nhighlight specific semantic attributes of the word. The new embeddings produced\nby the map are shown to improve the performance on downstream tasks.", "published": "2020-04-27 14:33:37", "link": "http://arxiv.org/abs/2004.12835v2", "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50, 68T35", "I.2.7; E.4"], "primary_category": "cs.CL"}
{"title": "Knowledge Base Completion for Constructing Problem-Oriented Medical\n  Records", "abstract": "Both electronic health records and personal health records are typically\norganized by data type, with medical problems, medications, procedures, and\nlaboratory results chronologically sorted in separate areas of the chart. As a\nresult, it can be difficult to find all of the relevant information for\nanswering a clinical question about a given medical problem. A promising\nalternative is to instead organize by problems, with related medications,\nprocedures, and other pertinent information all grouped together. A recent\neffort by Buchanan (2017) manually defined, through expert consensus, 11\nmedical problems and the relevant labs and medications for each. We show how to\nuse machine learning on electronic health records to instead automatically\nconstruct these problem-based groupings of relevant medications, procedures,\nand laboratory tests. We formulate the learning task as one of knowledge base\ncompletion, and annotate a dataset that expands the set of problems from 11 to\n32. We develop a model architecture that exploits both pre-trained concept\nembeddings and usage data relating the concepts contained in a longitudinal\ndataset from a large health system. We evaluate our algorithms' ability to\nsuggest relevant medications, procedures, and lab tests, and find that the\napproach provides feasible suggestions even for problems that are hidden during\ntraining. The dataset, along with code to reproduce our results, is available\nat https://github.com/asappresearch/kbc-pomr.", "published": "2020-04-27 16:05:23", "link": "http://arxiv.org/abs/2004.12905v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Novel Attention-based Aggregation Function to Combine Vision and\n  Language", "abstract": "The joint understanding of vision and language has been recently gaining a\nlot of attention in both the Computer Vision and Natural Language Processing\ncommunities, with the emergence of tasks such as image captioning, image-text\nmatching, and visual question answering. As both images and text can be encoded\nas sets or sequences of elements -- like regions and words -- proper reduction\nfunctions are needed to transform a set of encoded elements into a single\nresponse, like a classification or similarity score. In this paper, we propose\na novel fully-attentive reduction method for vision and language. Specifically,\nour approach computes a set of scores for each element of each modality\nemploying a novel variant of cross-attention, and performs a learnable and\ncross-modal reduction, which can be used for both classification and ranking.\nWe test our approach on image-text matching and visual question answering,\nbuilding fair comparisons with other reduction choices, on both COCO and VQA\n2.0 datasets. Experimentally, we demonstrate that our approach leads to a\nperformance increase on both tasks. Further, we conduct ablation studies to\nvalidate the role of each component of the approach.", "published": "2020-04-27 18:09:46", "link": "http://arxiv.org/abs/2004.13073v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Word Interdependence Exposes How LSTMs Compose Representations", "abstract": "Recent work in NLP shows that LSTM language models capture compositional\nstructure in language data. For a closer look at how these representations are\ncomposed hierarchically, we present a novel measure of interdependence between\nword meanings in an LSTM, based on their interactions at the internal gates. To\nexplore how compositional representations arise over training, we conduct\nsimple experiments on synthetic data, which illustrate our measure by showing\nhow high interdependence can hurt generalization. These synthetic experiments\nalso illustrate a specific hypothesis about how hierarchical structures are\ndiscovered over the course of training: that parent constituents rely on\neffective representations of their children, rather than on learning long-range\nrelations independently. We further support this measure with experiments on\nEnglish language data, where interdependence is higher for more closely\nsyntactically linked word pairs.", "published": "2020-04-27 21:48:08", "link": "http://arxiv.org/abs/2004.13195v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Time-Frequency Analysis and Parameterisation of Knee Sounds for\n  Non-invasive Detection of Osteoarthritis", "abstract": "Objective: In this work the potential of non-invasive detection of knee\nosteoarthritis is investigated using the sounds generated by the knee joint\nduring walking. Methods: The information contained in the time-frequency domain\nof these signals and its compressed representations is exploited and their\ndiscriminant properties are studied. Their efficacy for the task of normal vs\nabnormal signal classification is evaluated using a comprehensive experimental\nframework. Based on this, the impact of the feature extraction parameters on\nthe classification performance is investigated using Classification and\nRegression Trees (CART), Linear Discriminant Analysis (LDA) and Support Vector\nMachine (SVM) classifiers. Results: It is shown that classification is\nsuccessful with an area under the Receiver Operating Characteristic (ROC) curve\nof 0.92. Conclusion: The analysis indicates improvements in classification\nperformance when using non-uniform frequency scaling and identifies specific\nfrequency bands that contain discriminative features. Significance: Contrary to\nother studies that focus on sit-to-stand movements and knee flexion/extension,\nthis study used knee sounds obtained during walking. The analysis of such\nsignals leads to non-invasive detection of knee osteoarthritis with high\naccuracy and could potentially extend the range of available tools for the\nassessment of the disease as a more practical and cost effective method without\nrequiring clinical setups.", "published": "2020-04-27 12:40:47", "link": "http://arxiv.org/abs/2004.12745v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DWT-GBT-SVD-based Robust Speech Steganography", "abstract": "Steganography is a method that can improve network security and make\ncommunications safer. In this method, a secret message is hidden in content\nlike audio signals that should not be perceptible by listening to the audio or\nseeing the signal waves. Also, it should be robust against different common\nattacks such as noise and compression. In this paper, we propose a new speech\nsteganography method based on a combination of Discrete Wavelet Transform,\nGraph-based Transform, and Singular Value Decomposition (SVD). In this method,\nwe first find voiced frames based on energy and zero-crossing counts of the\nframes and then embed a binary message into voiced frames. Experimental results\non the NOIZEUS database show that the proposed method is imperceptible and also\nrobust against Gaussian noise, re-sampling, re-quantization, high pass filter,\nand low pass filter. Also, it is robust against MP3 compression and scaling for\nwatermarking applications.", "published": "2020-04-27 03:35:32", "link": "http://arxiv.org/abs/2004.12569v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Autoencoding Neural Networks as Musical Audio Synthesizers", "abstract": "A method for musical audio synthesis using autoencoding neural networks is\nproposed. The autoencoder is trained to compress and reconstruct magnitude\nshort-time Fourier transform frames. The autoencoder produces a spectrogram by\nactivating its smallest hidden layer, and a phase response is calculated using\nreal-time phase gradient heap integration. Taking an inverse short-time Fourier\ntransform produces the audio signal. Our algorithm is light-weight when\ncompared to current state-of-the-art audio-producing machine learning\nalgorithms. We outline our design process, produce metrics, and detail an\nopen-source Python implementation of our model.", "published": "2020-04-27 20:58:03", "link": "http://arxiv.org/abs/2004.13172v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
