{"title": "Incorporating Constituent Syntax for Coreference Resolution", "abstract": "Syntax has been shown to benefit Coreference Resolution from incorporating\nlong-range dependencies and structured information captured by syntax trees,\neither in traditional statistical machine learning based systems or recently\nproposed neural models. However, most leading systems use only dependency\ntrees. We argue that constituent trees also encode important information, such\nas explicit span-boundary signals captured by nested multi-word phrases, extra\nlinguistic labels and hierarchical structures useful for detecting anaphora. In\nthis work, we propose a simple yet effective graph-based method to incorporate\nconstituent syntactic structures. Moreover, we also explore to utilise\nhigher-order neighbourhood information to encode rich structures in constituent\ntrees. A novel message propagation mechanism is therefore proposed to enable\ninformation flow among elements in syntax trees. Experiments on the English and\nChinese portions of OntoNotes 5.0 benchmark show that our proposed model either\nbeats a strong baseline or achieves new state-of-the-art performance. (Code is\navailable at https://github.com/Fantabulous-J/Coref-Constituent-Graph)", "published": "2022-02-22 07:40:42", "link": "http://arxiv.org/abs/2202.10710v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CorefDRE: Document-level Relation Extraction with coreference resolution", "abstract": "Document-level relation extraction is to extract relation facts from a\ndocument consisting of multiple sentences, in which pronoun crossed sentences\nare a ubiquitous phenomenon against a single sentence. However, most of the\nprevious works focus more on mentions coreference resolution except for\npronouns, and rarely pay attention to mention-pronoun coreference and capturing\nthe relations. To represent multi-sentence features by pronouns, we imitate the\nreading process of humans by leveraging coreference information when\ndynamically constructing a heterogeneous graph to enhance semantic information.\nSince the pronoun is notoriously ambiguous in the graph, a mention-pronoun\ncoreference resolution is introduced to calculate the affinity between pronouns\nand corresponding mentions, and the noise suppression mechanism is proposed to\nreduce the noise caused by pronouns. Experiments on the public dataset, DocRED,\nDialogRE and MPDD, show that Coref-aware Doc-level Relation Extraction based on\nGraph Inference Network outperforms the state-of-the-art.", "published": "2022-02-22 09:03:59", "link": "http://arxiv.org/abs/2202.10744v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Semi-supervised Learning Approach with Two Teachers to Improve\n  Breakdown Identification in Dialogues", "abstract": "Identifying breakdowns in ongoing dialogues helps to improve communication\neffectiveness. Most prior work on this topic relies on human annotated data and\ndata augmentation to learn a classification model. While quality labeled\ndialogue data requires human annotation and is usually expensive to obtain,\nunlabeled data is easier to collect from various sources. In this paper, we\npropose a novel semi-supervised teacher-student learning framework to tackle\nthis task. We introduce two teachers which are trained on labeled data and\nperturbed labeled data respectively. We leverage unlabeled data to improve\nclassification in student training where we employ two teachers to refine the\nlabeling of unlabeled data through teacher-student learning in a bootstrapping\nmanner. Through our proposed training approach, the student can achieve\nimprovements over single-teacher performance. Experimental results on the\nDialogue Breakdown Detection Challenge dataset DBDC5 and Learning to Identify\nFollow-Up Questions dataset LIF show that our approach outperforms all previous\npublished approaches as well as other supervised and semi-supervised baseline\nmethods.", "published": "2022-02-22 14:39:51", "link": "http://arxiv.org/abs/2202.10948v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Cluster Patterns for Abstractive Summarization", "abstract": "Nowadays, pre-trained sequence-to-sequence models such as BERTSUM and BART\nhave shown state-of-the-art results in abstractive summarization. In these\nmodels, during fine-tuning, the encoder transforms sentences to context vectors\nin the latent space and the decoder learns the summary generation task based on\nthe context vectors. In our approach, we consider two clusters of salient and\nnon-salient context vectors, using which the decoder can attend more to salient\ncontext vectors for summary generation. For this, we propose a novel clustering\ntransformer layer between the encoder and the decoder, which first generates\ntwo clusters of salient and non-salient vectors, and then normalizes and\nshrinks the clusters to make them apart in the latent space. Our experimental\nresult shows that the proposed model outperforms the existing BART model by\nlearning these distinct cluster patterns, improving up to 4% in ROUGE and 0.3%\nin BERTScore on average in CNN/DailyMail and XSUM data sets.", "published": "2022-02-22 15:15:24", "link": "http://arxiv.org/abs/2202.10967v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatically Generating Counterfactuals for Relation Classification", "abstract": "The goal of relation classification (RC) is to extract the semantic relations\nbetween/among entities in the text. As a fundamental task in natural language\nprocessing, it is crucial to ensure the robustness of RC models. Despite the\nhigh accuracy current deep neural models have achieved in RC tasks, they are\neasily affected by spurious correlations. One solution to this problem is to\ntrain the model with counterfactually augmented data (CAD) such that it can\nlearn the causation rather than the confounding. However, no attempt has been\nmade on generating counterfactuals for RC tasks. In this paper, we formulate\nthe problem of automatically generating CAD for RC tasks from an entity-centric\nviewpoint, and develop a novel approach to derive contextual counterfactuals\nfor entities. Specifically, we exploit two elementary topological properties,\ni.e., the centrality and the shortest path, in syntactic and semantic\ndependency graphs, to first identify and then intervene on the contextual\ncausal features for entities. We conduct a comprehensive evaluation on four RC\ndatasets by combining our proposed approach with a variety of backbone RC\nmodels. The results demonstrate that our approach not only improves the\nperformance of the backbones, but also makes them more robust in the\nout-of-domain test.", "published": "2022-02-22 04:46:10", "link": "http://arxiv.org/abs/2202.10668v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "JAMES: Normalizing Job Titles with Multi-Aspect Graph Embeddings and\n  Reasoning", "abstract": "In online job marketplaces, it is important to establish a well-defined job\ntitle taxonomy for various downstream tasks (e.g., job recommendation, users'\ncareer analysis, and turnover prediction). Job Title Normalization (JTN) is\nsuch a cleaning step to classify user-created non-standard job titles into\nnormalized ones. However, solving the JTN problem is non-trivial with\nchallenges: (1) semantic similarity of different job titles, (2) non-normalized\nuser-created job titles, and (3) large-scale and long-tailed job titles in\nreal-world applications. To this end, we propose a novel solution, named JAMES,\nthat constructs three unique embeddings (i.e., graph, contextual, and\nsyntactic) of a target job title to effectively capture its various traits. We\nfurther propose a multi-aspect co-attention mechanism to attentively combine\nthese embeddings, and employ neural logical reasoning representations to\ncollaboratively estimate similarities between messy job titles and normalized\njob titles in a reasoning space. To evaluate JAMES, we conduct comprehensive\nexperiments against ten competing models on a large-scale real-world dataset\nwith over 350,000 job titles. Our experimental results show that JAMES\nsignificantly outperforms the best baseline by 10.06% in Precision@10 and by\n17.52% in NDCG@10, respectively.", "published": "2022-02-22 08:57:08", "link": "http://arxiv.org/abs/2202.10739v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "NU HLT at CMCL 2022 Shared Task: Multilingual and Crosslingual\n  Prediction of Human Reading Behavior in Universal Language Space", "abstract": "In this paper, we present a unified model that works for both multilingual\nand crosslingual prediction of reading times of words in various languages. The\nsecret behind the success of this model is in the preprocessing step where all\nwords are transformed to their universal language representation via the\nInternational Phonetic Alphabet (IPA). To the best of our knowledge, this is\nthe first study to favorable exploit this phonological property of language for\nthe two tasks. Various feature types were extracted covering basic frequencies,\nn-grams, information theoretic, and psycholinguistically-motivated predictors\nfor model training. A finetuned Random Forest model obtained best performance\nfor both tasks with 3.8031 and 3.9065 MAE scores for mean first fixation\nduration (FFDAvg) and mean total reading time (TRTAvg) respectively.", "published": "2022-02-22 12:39:16", "link": "http://arxiv.org/abs/2202.10855v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Persian Tokenizers", "abstract": "Tokenization plays a significant role in the process of lexical analysis.\nTokens become the input for other natural language processing tasks, like\nsemantic parsing and language modeling. Natural Language Processing in Persian\nis challenging due to Persian's exceptional cases, such as half-spaces. Thus,\nit is crucial to have a precise tokenizer for Persian. This article provides a\nnovel work by introducing the most widely used tokenizers for Persian and\ncomparing and evaluating their performance on Persian texts using a simple\nalgorithm with a pre-tagged Persian dependency dataset. After evaluating\ntokenizers with the F1-Score, the hybrid version of the Farsi Verb and Hazm\nwith bounded morphemes fixing showed the best performance with an F1 score of\n98.97%.", "published": "2022-02-22 13:27:24", "link": "http://arxiv.org/abs/2202.10879v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Overview on Machine Translation Evaluation", "abstract": "Since the 1950s, machine translation (MT) has become one of the important\ntasks of AI and development, and has experienced several different periods and\nstages of development, including rule-based methods, statistical methods, and\nrecently proposed neural network-based learning methods. Accompanying these\nstaged leaps is the evaluation research and development of MT, especially the\nimportant role of evaluation methods in statistical translation and neural\ntranslation research. The evaluation task of MT is not only to evaluate the\nquality of machine translation, but also to give timely feedback to machine\ntranslation researchers on the problems existing in machine translation itself,\nhow to improve and how to optimise. In some practical application fields, such\nas in the absence of reference translations, the quality estimation of machine\ntranslation plays an important role as an indicator to reveal the credibility\nof automatically translated target languages. This report mainly includes the\nfollowing contents: a brief history of machine translation evaluation (MTE),\nthe classification of research methods on MTE, and the the cutting-edge\nprogress, including human evaluation, automatic evaluation, and evaluation of\nevaluation methods (meta-evaluation). Manual evaluation and automatic\nevaluation include reference-translation based and reference-translation\nindependent participation; automatic evaluation methods include traditional\nn-gram string matching, models applying syntax and semantics, and deep learning\nmodels; evaluation of evaluation methods includes estimating the credibility of\nhuman evaluations, the reliability of the automatic evaluation, the reliability\nof the test set, etc. Advances in cutting-edge evaluation methods include\ntask-based evaluation, using pre-trained language models based on big data, and\nlightweight optimisation models using distillation techniques.", "published": "2022-02-22 16:58:28", "link": "http://arxiv.org/abs/2202.11027v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DialMed: A Dataset for Dialogue-based Medication Recommendation", "abstract": "Medication recommendation is a crucial task for intelligent healthcare\nsystems. Previous studies mainly recommend medications with electronic health\nrecords (EHRs). However, some details of interactions between doctors and\npatients may be ignored or omitted in EHRs, which are essential for automatic\nmedication recommendation. Therefore, we make the first attempt to recommend\nmedications with the conversations between doctors and patients. In this work,\nwe construct DIALMED, the first high-quality dataset for medical dialogue-based\nmedication recommendation task. It contains 11,996 medical dialogues related to\n16 common diseases from 3 departments and 70 corresponding common medications.\nFurthermore, we propose a Dialogue structure and Disease knowledge aware\nNetwork (DDN), where a QA Dialogue Graph mechanism is designed to model the\ndialogue structure and the knowledge graph is used to introduce external\ndisease knowledge. The extensive experimental results demonstrate that the\nproposed method is a promising solution to recommend medications with medical\ndialogues. The dataset and code are available at\nhttps://github.com/f-window/DialMed.", "published": "2022-02-22 05:12:29", "link": "http://arxiv.org/abs/2203.07094v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Base Question Answering by Case-based Reasoning over Subgraphs", "abstract": "Question answering (QA) over knowledge bases (KBs) is challenging because of\nthe diverse, essentially unbounded, types of reasoning patterns needed.\nHowever, we hypothesize in a large KB, reasoning patterns required to answer a\nquery type reoccur for various entities in their respective subgraph\nneighborhoods. Leveraging this structural similarity between local\nneighborhoods of different subgraphs, we introduce a semiparametric model\n(CBR-SUBG) with (i) a nonparametric component that for each query, dynamically\nretrieves other similar $k$-nearest neighbor (KNN) training queries along with\nquery-specific subgraphs and (ii) a parametric component that is trained to\nidentify the (latent) reasoning patterns from the subgraphs of KNN queries and\nthen apply them to the subgraph of the target query. We also propose an\nadaptive subgraph collection strategy to select a query-specific compact\nsubgraph, allowing us to scale to full Freebase KB containing billions of\nfacts. We show that CBR-SUBG can answer queries requiring subgraph reasoning\npatterns and performs competitively with the best models on several KBQA\nbenchmarks. Our subgraph collection strategy also produces more compact\nsubgraphs (e.g. 55\\% reduction in size for WebQSP while increasing answer\nrecall by 4.85\\%)\\footnote{Code, model, and subgraphs are available at\n\\url{https://github.com/rajarshd/CBR-SUBG}}.", "published": "2022-02-22 01:34:35", "link": "http://arxiv.org/abs/2202.10610v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Cross-lingual Speech Synthesis with Triplet Training Scheme", "abstract": "Recent advances in cross-lingual text-to-speech (TTS) made it possible to\nsynthesize speech in a language foreign to a monolingual speaker. However,\nthere is still a large gap between the pronunciation of generated cross-lingual\nspeech and that of native speakers in terms of naturalness and intelligibility.\nIn this paper, a triplet training scheme is proposed to enhance the\ncross-lingual pronunciation by allowing previously unseen content and speaker\ncombinations to be seen during training. Proposed method introduces an extra\nfine-tune stage with triplet loss during training, which efficiently draws the\npronunciation of the synthesized foreign speech closer to those from the native\nanchor speaker, while preserving the non-native speaker's timbre. Experiments\nare conducted based on a state-of-the-art baseline cross-lingual TTS system and\nits enhanced variants. All the objective and subjective evaluations show the\nproposed method brings significant improvement in both intelligibility and\nnaturalness of the synthesized cross-lingual speech.", "published": "2022-02-22 08:40:43", "link": "http://arxiv.org/abs/2202.10729v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Systematic Generalization Through Modularity and Augmentation", "abstract": "Systematic generalization is the ability to combine known parts into novel\nmeaning; an important aspect of efficient human learning, but a weakness of\nneural network learning. In this work, we investigate how two well-known\nmodeling principles -- modularity and data augmentation -- affect systematic\ngeneralization of neural networks in grounded language learning. We analyze how\nlarge the vocabulary needs to be to achieve systematic generalization and how\nsimilar the augmented data needs to be to the problem at hand. Our findings\nshow that even in the controlled setting of a synthetic benchmark, achieving\nsystematic generalization remains very difficult. After training on an\naugmented dataset with almost forty times more adverbs than the original\nproblem, a non-modular baseline is not able to systematically generalize to a\nnovel combination of a known verb and adverb. When separating the task into\ncognitive processes like perception and navigation, a modular neural network is\nable to utilize the augmented data and generalize more systematically,\nachieving 70% and 40% exact match increase over state-of-the-art on two gSCAN\ntests that have not previously been improved. We hope that this work gives\ninsight into the drivers of systematic generalization, and what we still need\nto improve for neural networks to learn more like humans do.", "published": "2022-02-22 09:04:35", "link": "http://arxiv.org/abs/2202.10745v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "VU-BERT: A Unified framework for Visual Dialog", "abstract": "The visual dialog task attempts to train an agent to answer multi-turn\nquestions given an image, which requires the deep understanding of interactions\nbetween the image and dialog history. Existing researches tend to employ the\nmodality-specific modules to model the interactions, which might be troublesome\nto use. To fill in this gap, we propose a unified framework for image-text\njoint embedding, named VU-BERT, and apply patch projection to obtain vision\nembedding firstly in visual dialog tasks to simplify the model. The model is\ntrained over two tasks: masked language modeling and next utterance retrieval.\nThese tasks help in learning visual concepts, utterances dependence, and the\nrelationships between these two modalities. Finally, our VU-BERT achieves\ncompetitive performance (0.7287 NDCG scores) on VisDial v1.0 Datasets.", "published": "2022-02-22 10:20:14", "link": "http://arxiv.org/abs/2202.10787v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Statistical and Spatio-temporal Hand Gesture Features for Sign Language\n  Recognition using the Leap Motion Sensor", "abstract": "In modern society, people should not be identified based on their disability,\nrather, it is environments that can disable people with impairments.\nImprovements to automatic Sign Language Recognition (SLR) will lead to more\nenabling environments via digital technology. Many state-of-the-art approaches\nto SLR focus on the classification of static hand gestures, but communication\nis a temporal activity, which is reflected by many of the dynamic gestures\npresent. Given this, temporal information during the delivery of a gesture is\nnot often considered within SLR. The experiments in this work consider the\nproblem of SL gesture recognition regarding how dynamic gestures change during\ntheir delivery, and this study aims to explore how single types of features as\nwell as mixed features affect the classification ability of a machine learning\nmodel. 18 common gestures recorded via a Leap Motion Controller sensor provide\na complex classification problem. Two sets of features are extracted from a 0.6\nsecond time window, statistical descriptors and spatio-temporal attributes.\nFeatures from each set are compared by their ANOVA F-Scores and p-values,\narranged into bins grown by 10 features per step to a limit of the 250\nhighest-ranked features. Results show that the best statistical model selected\n240 features and scored 85.96% accuracy, the best spatio-temporal model\nselected 230 features and scored 80.98%, and the best mixed-feature model\nselected 240 features from each set leading to a classification accuracy of\n86.75%. When all three sets of results are compared (146 individual machine\nlearning models), the overall distribution shows that the minimum results are\nincreased when inputs are any number of mixed features compared to any number\nof either of the two single sets of features.", "published": "2022-02-22 16:16:47", "link": "http://arxiv.org/abs/2202.11005v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A New Generation of Perspective API: Efficient Multilingual\n  Character-level Transformers", "abstract": "On the world wide web, toxic content detectors are a crucial line of defense\nagainst potentially hateful and offensive messages. As such, building highly\neffective classifiers that enable a safer internet is an important research\narea. Moreover, the web is a highly multilingual, cross-cultural community that\ndevelops its own lingo over time. As such, it is crucial to develop models that\nare effective across a diverse range of languages, usages, and styles. In this\npaper, we present the fundamentals behind the next version of the Perspective\nAPI from Google Jigsaw. At the heart of the approach is a single multilingual\ntoken-free Charformer model that is applicable across a range of languages,\ndomains, and tasks. We demonstrate that by forgoing static vocabularies, we\ngain flexibility across a variety of settings. We additionally outline the\ntechniques employed to make such a byte-level model efficient and feasible for\nproductionization. Through extensive experiments on multilingual toxic comment\nclassification benchmarks derived from real API traffic and evaluation on an\narray of code-switching, covert toxicity, emoji-based hate, human-readable\nobfuscation, distribution shift, and bias evaluation settings, we show that our\nproposed approach outperforms strong baselines. Finally, we present our\nfindings from deploying this system in production.", "published": "2022-02-22 20:55:31", "link": "http://arxiv.org/abs/2202.11176v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning to Combine Instructions in LLVM Compiler", "abstract": "Instruction combiner (IC) is a critical compiler optimization pass, which\nreplaces a sequence of instructions with an equivalent and optimized\ninstruction sequence at basic block level. There can be thousands of\ninstruction-combining patterns which need to be frequently updated as new\ncoding idioms/applications and novel hardware evolve over time. This results in\nfrequent updates to the IC optimization pass thereby incurring considerable\nhuman effort and high software maintenance costs. To mitigate these challenges\nassociated with the traditional IC, we design and implement a Neural\nInstruction Combiner (NIC) and demonstrate its feasibility by integrating it\ninto the standard LLVM compiler optimization pipeline. NIC leverages neural\nsequence-to-sequence (Seq2Seq) models for generating optimized encoded IR\nsequence from the unoptimized encoded IR sequence. To the best of our\nknowledge, ours is the first work demonstrating the feasibility of a neural\ninstruction combiner built into a full-fledged compiler pipeline. Given the\nnovelty of this task, we built a new dataset for training our NIC neural model.\nWe show that NIC achieves exact match results percentage of 72% for optimized\nsequences as compared to traditional IC and neural machine translation metric\nBleu precision score of 0.94, demonstrating its feasibility in a production\ncompiler pipeline.", "published": "2022-02-22 06:20:51", "link": "http://arxiv.org/abs/2202.12379v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Improving CTC-based speech recognition via knowledge transferring from\n  pre-trained language models", "abstract": "Recently, end-to-end automatic speech recognition models based on\nconnectionist temporal classification (CTC) have achieved impressive results,\nespecially when fine-tuned from wav2vec2.0 models. Due to the conditional\nindependence assumption, CTC-based models are always weaker than\nattention-based encoder-decoder models and require the assistance of external\nlanguage models (LMs). To solve this issue, we propose two knowledge\ntransferring methods that leverage pre-trained LMs, such as BERT and GPT2, to\nimprove CTC-based models. The first method is based on representation learning,\nin which the CTC-based models use the representation produced by BERT as an\nauxiliary learning target. The second method is based on joint classification\nlearning, which combines GPT2 for text modeling with a hybrid CTC/attention\narchitecture. Experiment on AISHELL-1 corpus yields a character error rate\n(CER) of 4.2% on the test set. When compared to the vanilla CTC-based models\nfine-tuned from the wav2vec2.0 models, our knowledge transferring method\nreduces CER by 16.1% relatively without external LMs.", "published": "2022-02-22 11:30:55", "link": "http://arxiv.org/abs/2203.03582v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Korean Tokenization for Beam Search Rescoring in Speech Recognition", "abstract": "The performance of automatic speech recognition (ASR) models can be greatly\nimproved by proper beam-search decoding with external language model (LM).\nThere has been an increasing interest in Korean speech recognition, but not\nmany studies have been focused on the decoding procedure. In this paper, we\npropose a Korean tokenization method for neural network-based LM used for\nKorean ASR. Although the common approach is to use the same tokenization method\nfor external LM as the ASR model, we show that it may not be the best choice\nfor Korean. We propose a new tokenization method that inserts a special token,\nSkipTC, when there is no trailing consonant in a Korean syllable. By utilizing\nthe proposed SkipTC token, the input sequence for LM becomes very regularly\npatterned so that the LM can better learn the linguistic characteristics. Our\nexperiments show that the proposed approach achieves a lower word error rate\ncompared to the same LM model without SkipTC. In addition, we are the first to\nreport the ASR performance for the recently introduced large-scale 7,600h\nKorean speech dataset.", "published": "2022-02-22 11:25:01", "link": "http://arxiv.org/abs/2203.03583v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "VADOI:Voice-Activity-Detection Overlapping Inference For End-to-end\n  Long-form Speech Recognition", "abstract": "While end-to-end models have shown great success on the Automatic Speech\nRecognition task, performance degrades severely when target sentences are\nlong-form. The previous proposed methods, (partial) overlapping inference are\nshown to be effective on long-form decoding. For both methods, word error rate\n(WER) decreases monotonically when overlapping percentage decreases. Setting\naside computational cost, the setup with 50% overlapping during inference can\nachieve the best performance. However, a lower overlapping percentage has an\nadvantage of fast inference speed. In this paper, we first conduct\ncomprehensive experiments comparing overlapping inference and partial\noverlapping inference with various configurations. We then propose\nVoice-Activity-Detection Overlapping Inference to provide a trade-off between\nWER and computation cost. Results show that the proposed method can achieve a\n20% relative computation cost reduction on Librispeech and Microsoft Speech\nLanguage Translation long-form corpus while maintaining the WER performance\nwhen comparing to the best performing overlapping inference algorithm. We also\npropose Soft-Match to compensate for similar words mis-aligned problem.", "published": "2022-02-22 00:13:50", "link": "http://arxiv.org/abs/2202.10593v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "nnSpeech: Speaker-Guided Conditional Variational Autoencoder for\n  Zero-shot Multi-speaker Text-to-Speech", "abstract": "Multi-speaker text-to-speech (TTS) using a few adaption data is a challenge\nin practical applications. To address that, we propose a zero-shot\nmulti-speaker TTS, named nnSpeech, that could synthesis a new speaker voice\nwithout fine-tuning and using only one adaption utterance. Compared with using\na speaker representation module to extract the characteristics of new speakers,\nour method bases on a speaker-guided conditional variational autoencoder and\ncan generate a variable Z, which contains both speaker characteristics and\ncontent information. The latent variable Z distribution is approximated by\nanother variable conditioned on reference mel-spectrogram and phoneme.\nExperiments on the English corpus, Mandarin corpus, and cross-dataset proves\nthat our model could generate natural and similar speech with only one adaption\nspeech.", "published": "2022-02-22 07:43:30", "link": "http://arxiv.org/abs/2202.10712v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DRVC: A Framework of Any-to-Any Voice Conversion with Self-Supervised\n  Learning", "abstract": "Any-to-any voice conversion problem aims to convert voices for source and\ntarget speakers, which are out of the training data. Previous works wildly\nutilize the disentangle-based models. The disentangle-based model assumes the\nspeech consists of content and speaker style information and aims to untangle\nthem to change the style information for conversion. Previous works focus on\nreducing the dimension of speech to get the content information. But the size\nis hard to determine to lead to the untangle overlapping problem. We propose\nthe Disentangled Representation Voice Conversion (DRVC) model to address the\nissue. DRVC model is an end-to-end self-supervised model consisting of the\ncontent encoder, timbre encoder, and generator. Instead of the previous work\nfor reducing speech size to get content, we propose a cycle for restricting the\ndisentanglement by the Cycle Reconstruct Loss and Same Loss. The experiments\nshow there is an improvement for converted speech on quality and voice\nsimilarity.", "published": "2022-02-22 15:30:44", "link": "http://arxiv.org/abs/2202.10976v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Modal Estimation on a Warped Frequency Axis for Linear System Modeling", "abstract": "Linear systems such as room acoustics and string oscillations may be modeled\nas the sum of mode responses, each characterized by a frequency, damping and\namplitude. Here, we consider finding the mode parameters from impulse response\nmeasurements, and estimate the mode frequencies and decay rates as the\ngeneralized eigenvalues of Hankel matrices of system response samples, similar\nto ESPRIT. For greater resolution at low frequencies, such as desired in room\nacoustics and musical instrument modeling, the estimation is done on a warped\nfrequency axis. The approach has the benefit of selecting the number of modes\nto achieve a desired fidelity to the measured impulse response. An optimization\nto further refine the frequency and damping parameters is presented. The method\nis used to model coupled piano strings and room impulse responses, with its\nperformance comparing favorably to FZ-ARMA.", "published": "2022-02-22 21:59:42", "link": "http://arxiv.org/abs/2202.11192v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Adversarial Attacks on Speech Recognition Systems for Mission-Critical\n  Applications: A Survey", "abstract": "A Machine-Critical Application is a system that is fundamentally necessary to\nthe success of specific and sensitive operations such as search and recovery,\nrescue, military, and emergency management actions. Recent advances in Machine\nLearning, Natural Language Processing, voice recognition, and speech processing\ntechnologies have naturally allowed the development and deployment of\nspeech-based conversational interfaces to interact with various\nmachine-critical applications. While these conversational interfaces have\nallowed users to give voice commands to carry out strategic and critical\nactivities, their robustness to adversarial attacks remains uncertain and\nunclear. Indeed, Adversarial Artificial Intelligence (AI) which refers to a set\nof techniques that attempt to fool machine learning models with deceptive data,\nis a growing threat in the AI and machine learning research community, in\nparticular for machine-critical applications. The most common reason of\nadversarial attacks is to cause a malfunction in a machine learning model. An\nadversarial attack might entail presenting a model with inaccurate or\nfabricated samples as it's training data, or introducing maliciously designed\ndata to deceive an already trained model. While focusing on speech recognition\nfor machine-critical applications, in this paper, we first review existing\nspeech recognition techniques, then, we investigate the effectiveness of\nadversarial attacks and defenses against these systems, before outlining\nresearch challenges, defense recommendations, and future work. This paper is\nexpected to serve researchers and practitioners as a reference to help them in\nunderstanding the challenges, position themselves and, ultimately, help them to\nimprove existing models of speech recognition for mission-critical\napplications. Keywords: Mission-Critical Applications, Adversarial AI, Speech\nRecognition Systems.", "published": "2022-02-22 00:29:40", "link": "http://arxiv.org/abs/2202.10594v1", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Hidden bawls, whispers, and yelps: can text be made to sound more than\n  just its words?", "abstract": "Whether a word was bawled, whispered, or yelped, captions will typically\nrepresent it in the same way. If they are your only way to access what is being\nsaid, subjective nuances expressed in the voice will be lost. Since so much of\ncommunication is carried by these nuances, we posit that if captions are to be\nused as an accurate representation of speech, embedding visual representations\nof paralinguistic qualities into captions could help readers use them to better\nunderstand speech beyond its mere textual content. This paper presents a model\nfor processing vocal prosody (its loudness, pitch, and duration) and mapping it\ninto visual dimensions of typography (respectively, font-weight, baseline\nshift, and letter-spacing), creating a visual representation of these lost\nvocal subtleties that can be embedded directly into the typographical form of\ntext. An evaluation was carried out where participants were exposed to this\nspeech-modulated typography and asked to match it to its originating audio,\npresented between similar alternatives. Participants (n=117) were able to\ncorrectly identify the original audios with an average accuracy of 65%, with no\nsignificant difference when showing them modulations as animated or static\ntext. Additionally, participants' comments showed their mental models of\nspeech-modulated typography varied widely.", "published": "2022-02-22 02:35:25", "link": "http://arxiv.org/abs/2202.10631v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Contrastive-mixup learning for improved speaker verification", "abstract": "This paper proposes a novel formulation of prototypical loss with mixup for\nspeaker verification. Mixup is a simple yet efficient data augmentation\ntechnique that fabricates a weighted combination of random data point and label\npairs for deep neural network training. Mixup has attracted increasing\nattention due to its ability to improve robustness and generalization of deep\nneural networks. Although mixup has shown success in diverse domains, most\napplications have centered around closed-set classification tasks. In this\nwork, we propose contrastive-mixup, a novel augmentation strategy that learns\ndistinguishing representations based on a distance metric. During training,\nmixup operations generate convex interpolations of both inputs and virtual\nlabels. Moreover, we have reformulated the prototypical loss function such that\nmixup is enabled on metric learning objectives. To demonstrate its\ngeneralization given limited training data, we conduct experiments by varying\nthe number of available utterances from each speaker in the VoxCeleb database.\nExperimental results show that applying contrastive-mixup outperforms the\nexisting baseline, reducing error rate by 16% relatively, especially when the\nnumber of training utterances per speaker is limited.", "published": "2022-02-22 05:09:22", "link": "http://arxiv.org/abs/2202.10672v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Continuous Speech for Improved Learning Pathological Voice Disorders", "abstract": "Goal: Numerous studies had successfully differentiated normal and abnormal\nvoice samples. Nevertheless, further classification had rarely been attempted.\nThis study proposes a novel approach, using continuous Mandarin speech instead\nof a single vowel, to classify four common voice disorders (i.e. functional\ndysphonia, neoplasm, phonotrauma, and vocal palsy). Methods: In the proposed\nframework, acoustic signals are transformed into mel-frequency cepstral\ncoefficients, and a bi-directional long-short term memory network (BiLSTM) is\nadopted to model the sequential features. The experiments were conducted on a\nlarge-scale database, wherein 1,045 continuous speech were collected by the\nspeech clinic of a hospital from 2012 to 2019. Results: Experimental results\ndemonstrated that the proposed framework yields significant accuracy and\nunweighted average recall improvements of 78.12-89.27% and 50.92-80.68%,\nrespectively, compared with systems that use a single vowel. Conclusions: The\nresults are consistent with other machine learning algorithms, including gated\nrecurrent units, random forest, deep neural networks, and LSTM. The\nsensitivities for each disorder were also analyzed, and the model capabilities\nwere visualized via principal component analysis. An alternative experiment\nbased on a balanced dataset again confirms the advantages of using continuous\nspeech for learning voice disorders.", "published": "2022-02-22 09:58:31", "link": "http://arxiv.org/abs/2202.10777v1", "categories": ["eess.AS", "cs.AI", "cs.SD", "q-bio.QM"], "primary_category": "eess.AS"}
{"title": "Sound Adversarial Audio-Visual Navigation", "abstract": "Audio-visual navigation task requires an agent to find a sound source in a\nrealistic, unmapped 3D environment by utilizing egocentric audio-visual\nobservations. Existing audio-visual navigation works assume a clean environment\nthat solely contains the target sound, which, however, would not be suitable in\nmost real-world applications due to the unexpected sound noise or intentional\ninterference. In this work, we design an acoustically complex environment in\nwhich, besides the target sound, there exists a sound attacker playing a\nzero-sum game with the agent. More specifically, the attacker can move and\nchange the volume and category of the sound to make the agent suffer from\nfinding the sounding object while the agent tries to dodge the attack and\nnavigate to the goal under the intervention. Under certain constraints to the\nattacker, we can improve the robustness of the agent towards unexpected sound\nattacks in audio-visual navigation. For better convergence, we develop a joint\ntraining mechanism by employing the property of a centralized critic with\ndecentralized actors. Experiments on two real-world 3D scan datasets, Replica,\nand Matterport3D, verify the effectiveness and the robustness of the agent\ntrained under our designed environment when transferred to the clean\nenvironment or the one containing sound attackers with random policy. Project:\n\\url{https://yyf17.github.io/SAAVN}.", "published": "2022-02-22 14:19:42", "link": "http://arxiv.org/abs/2202.10910v1", "categories": ["cs.SD", "cs.CV", "cs.RO", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Wavebender GAN: An architecture for phonetically meaningful speech\n  manipulation", "abstract": "Deep learning has revolutionised synthetic speech quality. However, it has\nthus far delivered little value to the speech science community. The new\nmethods do not meet the controllability demands that practitioners in this area\nrequire e.g.: in listening tests with manipulated speech stimuli. Instead,\ncontrol of different speech properties in such stimuli is achieved by using\nlegacy signal-processing methods. This limits the range, accuracy, and speech\nquality of the manipulations. Also, audible artefacts have a negative impact on\nthe methodological validity of results in speech perception studies.\n  This work introduces a system capable of manipulating speech properties\nthrough learning rather than design. The architecture learns to control\narbitrary speech properties and leverages progress in neural vocoders to obtain\nrealistic output. Experiments with copy synthesis and manipulation of a small\nset of core speech features (pitch, formants, and voice quality measures)\nillustrate the promise of the approach for producing speech stimuli that have\naccurate control and high perceptual quality.", "published": "2022-02-22 15:26:26", "link": "http://arxiv.org/abs/2202.10973v1", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.SD", "68T07", "I.2.7; I.2.6; J.5; H.5.5"], "primary_category": "eess.AS"}
{"title": "ProtoSound: A Personalized and Scalable Sound Recognition System for\n  Deaf and Hard-of-Hearing Users", "abstract": "Recent advances have enabled automatic sound recognition systems for deaf and\nhard of hearing (DHH) users on mobile devices. However, these tools use\npre-trained, generic sound recognition models, which do not meet the diverse\nneeds of DHH users. We introduce ProtoSound, an interactive system for\ncustomizing sound recognition models by recording a few examples, thereby\nenabling personalized and fine-grained categories. ProtoSound is motivated by\nprior work examining sound awareness needs of DHH people and by a survey we\nconducted with 472 DHH participants. To evaluate ProtoSound, we characterized\nperformance on two real-world sound datasets, showing significant improvement\nover state-of-the-art (e.g., +9.7% accuracy on the first dataset). We then\ndeployed ProtoSound's end-user training and real-time recognition through a\nmobile application and recruited 19 hearing participants who listened to the\nreal-world sounds and rated the accuracy across 56 locations (e.g., homes,\nrestaurants, parks). Results show that ProtoSound personalized the model\non-device in real-time and accurately learned sounds across diverse acoustic\ncontexts. We close by discussing open challenges in personalizable sound\nrecognition, including the need for better recording interfaces and algorithmic\nimprovements.", "published": "2022-02-22 19:21:13", "link": "http://arxiv.org/abs/2202.11134v1", "categories": ["cs.HC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "FlowSense: Monitoring Airflow in Building Ventilation Systems Using\n  Audio Sensing", "abstract": "Proper indoor ventilation through buildings' heating, ventilation, and air\nconditioning (HVAC) systems has become an increasing public health concern that\nsignificantly impacts individuals' health and safety at home, work, and school.\nWhile much work has progressed in providing energy-efficient and user comfort\nfor HVAC systems through IoT devices and mobile-sensing approaches, ventilation\nis an aspect that has received lesser attention despite its importance. With a\nmotivation to monitor airflow from building ventilation systems through\ncommodity sensing devices, we present FlowSense, a machine learning-based\nalgorithm to predict airflow rate from sensed audio data in indoor spaces. Our\nML technique can predict the state of an air vent-whether it is on or off-as\nwell as the rate of air flowing through active vents. By exploiting a low-pass\nfilter to obtain low-frequency audio signals, we put together a\nprivacy-preserving pipeline that leverages a silence detection algorithm to\nonly sense for sounds of air from HVAC air vent when no human speech is\ndetected. We also propose the Minimum Persistent Sensing (MPS) as a\npost-processing algorithm to reduce interference from ambient noise, including\nongoing human conversation, office machines, and traffic noises. Together,\nthese techniques ensure user privacy and improve the robustness of FlowSense.\nWe validate our approach yielding over 90% accuracy in predicting vent status\nand 0.96 MSE in predicting airflow rate when the device is placed within 2.25\nmeters away from an air vent. Additionally, we demonstrate how our approach as\na mobile audio-sensing platform is robust to smartphone models, distance, and\norientation. Finally, we evaluate FlowSense privacy-preserving pipeline through\na user study and a Google Speech Recognition service, confirming that the audio\nsignals we used as input data are inaudible and inconstructible.", "published": "2022-02-22 19:22:36", "link": "http://arxiv.org/abs/2202.11136v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Neural Speech Synthesis on a Shoestring: Improving the Efficiency of\n  LPCNet", "abstract": "Neural speech synthesis models can synthesize high quality speech but\ntypically require a high computational complexity to do so. In previous work,\nwe introduced LPCNet, which uses linear prediction to significantly reduce the\ncomplexity of neural synthesis. In this work, we further improve the efficiency\nof LPCNet -- targeting both algorithmic and computational improvements -- to\nmake it usable on a wide variety of devices. We demonstrate an improvement in\nsynthesis quality while operating 2.5x faster. The resulting open-source LPCNet\nalgorithm can perform real-time neural synthesis on most existing phones and is\neven usable in some embedded devices.", "published": "2022-02-22 20:42:00", "link": "http://arxiv.org/abs/2202.11169v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Benchmarking Generative Latent Variable Models for Speech", "abstract": "Stochastic latent variable models (LVMs) achieve state-of-the-art performance\non natural image generation but are still inferior to deterministic models on\nspeech. In this paper, we develop a speech benchmark of popular temporal LVMs\nand compare them against state-of-the-art deterministic models. We report the\nlikelihood, which is a much used metric in the image domain, but rarely, or\nincomparably, reported for speech models. To assess the quality of the learned\nrepresentations, we also compare their usefulness for phoneme recognition.\nFinally, we adapt the Clockwork VAE, a state-of-the-art temporal LVM for video\ngeneration, to the speech domain. Despite being autoregressive only in latent\nspace, we find that the Clockwork VAE can outperform previous LVMs and reduce\nthe gap to deterministic models by using a hierarchy of latent variables.", "published": "2022-02-22 14:35:35", "link": "http://arxiv.org/abs/2202.12707v2", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
