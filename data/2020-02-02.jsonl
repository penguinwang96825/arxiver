{"title": "Beat the AI: Investigating Adversarial Human Annotation for Reading\n  Comprehension", "abstract": "Innovations in annotation methodology have been a catalyst for Reading\nComprehension (RC) datasets and models. One recent trend to challenge current\nRC models is to involve a model in the annotation process: humans create\nquestions adversarially, such that the model fails to answer them correctly. In\nthis work we investigate this annotation methodology and apply it in three\ndifferent settings, collecting a total of 36,000 samples with progressively\nstronger models in the annotation loop. This allows us to explore questions\nsuch as the reproducibility of the adversarial effect, transfer from data\ncollected with varying model-in-the-loop strengths, and generalisation to data\ncollected without a model. We find that training on adversarially collected\nsamples leads to strong generalisation to non-adversarially collected datasets,\nyet with progressive performance deterioration with increasingly stronger\nmodels-in-the-loop. Furthermore, we find that stronger models can still learn\nfrom datasets collected with substantially weaker models-in-the-loop. When\ntrained on data collected with a BiDAF model in the loop, RoBERTa achieves\n39.9F1 on questions that it cannot answer when trained on SQuAD - only\nmarginally lower than when trained on data collected using RoBERTa itself\n(41.0F1).", "published": "2020-02-02 00:22:55", "link": "http://arxiv.org/abs/2002.00293v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explaining Relationships Between Scientific Documents", "abstract": "We address the task of explaining relationships between two scientific\ndocuments using natural language text. This task requires modeling the complex\ncontent of long technical documents, deducing a relationship between these\ndocuments, and expressing the details of that relationship in text. In addition\nto the theoretical interest of this task, successful solutions can help improve\nresearcher efficiency in search and review. In this paper we establish a\ndataset of 622K examples from 154K documents. We pretrain a large language\nmodel to serve as the foundation for autoregressive approaches to the task. We\nexplore the impact of taking different views on the two documents, including\nthe use of dense representations extracted with scientific IE systems. We\nprovide extensive automatic and human evaluations which show the promise of\nsuch models, but make clear challenges for future work.", "published": "2020-02-02 03:54:47", "link": "http://arxiv.org/abs/2002.00317v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessment of Amazon Comprehend Medical: Medication Information\n  Extraction", "abstract": "In November 27, 2018, Amazon Web Services (AWS) released Amazon Comprehend\nMedical (ACM), a deep learning based system that automatically extracts\nclinical concepts (which include anatomy, medical conditions, protected health\ninformation (PH)I, test names, treatment names, and medical procedures, and\nmedications) from clinical text notes. Uptake and trust in any new data product\nrelies on independent validation across benchmark datasets and tools to\nestablish and confirm expected quality of results. This work focuses on the\nmedication extraction task, and particularly, ACM was evaluated using the\nofficial test sets from the 2009 i2b2 Medication Extraction Challenge and 2018\nn2c2 Track 2: Adverse Drug Events and Medication Extraction in EHRs. Overall,\nACM achieved F-scores of 0.768 and 0.828. These scores ranked the lowest when\ncompared to the three best systems in the respective challenges. To further\nestablish the generalizability of its medication extraction performance, a set\nof random internal clinical text notes from NYU Langone Medical Center were\nalso included in this work. And in this corpus, ACM garnered an F-score of\n0.753.", "published": "2020-02-02 20:08:34", "link": "http://arxiv.org/abs/2002.00481v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Schema-Guided Dialogue State Tracking Task at DSTC8", "abstract": "This paper gives an overview of the Schema-Guided Dialogue State Tracking\ntask of the 8th Dialogue System Technology Challenge. The goal of this task is\nto develop dialogue state tracking models suitable for large-scale virtual\nassistants, with a focus on data-efficient joint modeling across domains and\nzero-shot generalization to new APIs. This task provided a new dataset\nconsisting of over 16000 dialogues in the training set spanning 16 domains to\nhighlight these challenges, and a baseline model capable of zero-shot\ngeneralization to new APIs. Twenty-five teams participated, developing a range\nof neural network models, exceeding the performance of the baseline model by a\nvery high margin. The submissions incorporated a variety of pre-trained\nencoders and data augmentation techniques. This paper describes the task\ndefinition, dataset and evaluation methodology. We also summarize the approach\nand results of the submitted systems to highlight the overall trends in the\nstate-of-the-art.", "published": "2020-02-02 05:59:27", "link": "http://arxiv.org/abs/2002.01359v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Knowledge Graphs: Representation, Acquisition and\n  Applications", "abstract": "Human knowledge provides a formal understanding of the world. Knowledge\ngraphs that represent structural relations between entities have become an\nincreasingly popular research direction towards cognition and human-level\nintelligence. In this survey, we provide a comprehensive review of knowledge\ngraph covering overall research topics about 1) knowledge graph representation\nlearning, 2) knowledge acquisition and completion, 3) temporal knowledge graph,\nand 4) knowledge-aware applications, and summarize recent breakthroughs and\nperspective directions to facilitate future research. We propose a full-view\ncategorization and new taxonomies on these topics. Knowledge graph embedding is\norganized from four aspects of representation space, scoring function, encoding\nmodels, and auxiliary information. For knowledge acquisition, especially\nknowledge graph completion, embedding methods, path inference, and logical rule\nreasoning, are reviewed. We further explore several emerging topics, including\nmeta relational learning, commonsense reasoning, and temporal knowledge graphs.\nTo facilitate future research on knowledge graphs, we also provide a curated\ncollection of datasets and open-source libraries on different tasks. In the\nend, we have a thorough outlook on several promising research directions.", "published": "2020-02-02 13:17:31", "link": "http://arxiv.org/abs/2002.00388v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WaveTTS: Tacotron-based TTS with Joint Time-Frequency Domain Loss", "abstract": "Tacotron-based text-to-speech (TTS) systems directly synthesize speech from\ntext input. Such frameworks typically consist of a feature prediction network\nthat maps character sequences to frequency-domain acoustic features, followed\nby a waveform reconstruction algorithm or a neural vocoder that generates the\ntime-domain waveform from acoustic features. As the loss function is usually\ncalculated only for frequency-domain acoustic features, that doesn't directly\ncontrol the quality of the generated time-domain waveform. To address this\nproblem, we propose a new training scheme for Tacotron-based TTS, referred to\nas WaveTTS, that has 2 loss functions: 1) time-domain loss, denoted as the\nwaveform loss, that measures the distortion between the natural and generated\nwaveform; and 2) frequency-domain loss, that measures the Mel-scale acoustic\nfeature loss between the natural and generated acoustic features. WaveTTS\nensures both the quality of the acoustic features and the resulting speech\nwaveform. To our best knowledge, this is the first implementation of Tacotron\nwith joint time-frequency domain loss. Experimental results show that the\nproposed framework outperforms the baselines and achieves high-quality\nsynthesized speech.", "published": "2020-02-02 15:51:22", "link": "http://arxiv.org/abs/2002.00417v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Neural Machine Translation System of Indic Languages -- An Attention\n  based Approach", "abstract": "Neural machine translation (NMT) is a recent and effective technique which\nled to remarkable improvements in comparison of conventional machine\ntranslation techniques. Proposed neural machine translation model developed for\nthe Gujarati language contains encoder-decoder with attention mechanism. In\nIndia, almost all the languages are originated from their ancestral language -\nSanskrit. They are having inevitable similarities including lexical and named\nentity similarity. Translating into Indic languages is always be a challenging\ntask. In this paper, we have presented the neural machine translation system\n(NMT) that can efficiently translate Indic languages like Hindi and Gujarati\nthat together covers more than 58.49 percentage of total speakers in the\ncountry. We have compared the performance of our NMT model with automatic\nevaluation matrices such as BLEU, perplexity and TER matrix. The comparison of\nour network with Google translate is also presented where it outperformed with\na margin of 6 BLEU score on English-Gujarati translation.", "published": "2020-02-02 07:15:18", "link": "http://arxiv.org/abs/2002.02758v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "The FFSVC 2020 Evaluation Plan", "abstract": "The Far-Field Speaker Verification Challenge 2020 (FFSVC20) is designed to\nboost the speaker verification research with special focus on far-field\ndistributed microphone arrays under noisy conditions in real scenarios. The\nobjectives of this challenge are to: 1) benchmark the current speech\nverification technology under this challenging condition, 2) promote the\ndevelopment of new ideas and technologies in speaker verification, 3) provide\nan open, free, and large scale speech database to the community that exhibits\nthe far-field characteristics in real scenes.", "published": "2020-02-02 13:14:37", "link": "http://arxiv.org/abs/2002.00387v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Single Channel Speech Enhancement Using Temporal Convolutional Recurrent\n  Neural Networks", "abstract": "In recent decades, neural network based methods have significantly improved\nthe performace of speech enhancement. Most of them estimate time-frequency\n(T-F) representation of target speech directly or indirectly, then resynthesize\nwaveform using the estimated T-F representation. In this work, we proposed the\ntemporal convolutional recurrent network (TCRN), an end-to-end model that\ndirectly map noisy waveform to clean waveform. The TCRN, which is combined\nconvolution and recurrent neural network, is able to efficiently and\neffectively leverage short-term ang long-term information. Futuremore, we\npresent the architecture that repeatedly downsample and upsample speech during\nforward propagation. We show that our model is able to improve the performance\nof model, compared with existing convolutional recurrent networks. Futuremore,\nWe present several key techniques to stabilize the training process. The\nexperimental results show that our model consistently outperforms existing\nspeech enhancement approaches, in terms of speech intelligibility and quality.", "published": "2020-02-02 04:26:50", "link": "http://arxiv.org/abs/2002.00319v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DropClass and DropAdapt: Dropping classes for deep speaker\n  representation learning", "abstract": "Many recent works on deep speaker embeddings train their feature extraction\nnetworks on large classification tasks, distinguishing between all speakers in\na training set. Empirically, this has been shown to produce\nspeaker-discriminative embeddings, even for unseen speakers. However, it is not\nclear that this is the optimal means of training embeddings that generalize\nwell. This work proposes two approaches to learning embeddings, based on the\nnotion of dropping classes during training. We demonstrate that both approaches\ncan yield performance gains in speaker verification tasks. The first proposed\nmethod, DropClass, works via periodically dropping a random subset of classes\nfrom the training data and the output layer throughout training, resulting in a\nfeature extractor trained on many different classification tasks. Combined with\nan additive angular margin loss, this method can yield a 7.9% relative\nimprovement in equal error rate (EER) over a strong baseline on VoxCeleb. The\nsecond proposed method, DropAdapt, is a means of adapting a trained model to a\nset of enrolment speakers in an unsupervised manner. This is performed by\nfine-tuning a model on only those classes which produce high probability\npredictions when the enrolment speakers are used as input, again also dropping\nthe relevant rows from the output layer. This method yields a large 13.2%\nrelative improvement in EER on VoxCeleb. The code for this paper has been made\npublicly available.", "published": "2020-02-02 18:43:50", "link": "http://arxiv.org/abs/2002.00453v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sound Event Detection with Depthwise Separable and Dilated Convolutions", "abstract": "State-of-the-art sound event detection (SED) methods usually employ a series\nof convolutional neural networks (CNNs) to extract useful features from the\ninput audio signal, and then recurrent neural networks (RNNs) to model longer\ntemporal context in the extracted features. The number of the channels of the\nCNNs and size of the weight matrices of the RNNs have a direct effect on the\ntotal amount of parameters of the SED method, which is to a couple of millions.\nAdditionally, the usually long sequences that are used as an input to an SED\nmethod along with the employment of an RNN, introduce implications like\nincreased training time, difficulty at gradient flow, and impeding the\nparallelization of the SED method. To tackle all these problems, we propose the\nreplacement of the CNNs with depthwise separable convolutions and the\nreplacement of the RNNs with dilated convolutions. We compare the proposed\nmethod to a baseline convolutional neural network on a SED task, and achieve a\nreduction of the amount of parameters by 85% and average training time per\nepoch by 78%, and an increase the average frame-wise F1 score and reduction of\nthe average error rate by 4.6% and 3.8%, respectively.", "published": "2020-02-02 19:50:51", "link": "http://arxiv.org/abs/2002.00476v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Music2Dance: DanceNet for Music-driven Dance Generation", "abstract": "Synthesize human motions from music, i.e., music to dance, is appealing and\nattracts lots of research interests in recent years. It is challenging due to\nnot only the requirement of realistic and complex human motions for dance, but\nmore importantly, the synthesized motions should be consistent with the style,\nrhythm and melody of the music. In this paper, we propose a novel\nautoregressive generative model, DanceNet, to take the style, rhythm and melody\nof music as the control signals to generate 3D dance motions with high realism\nand diversity. To boost the performance of our proposed model, we capture\nseveral synchronized music-dance pairs by professional dancers, and build a\nhigh-quality music-dance pair dataset. Experiments have demonstrated that the\nproposed method can achieve the state-of-the-art results.", "published": "2020-02-02 17:18:31", "link": "http://arxiv.org/abs/2002.03761v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
