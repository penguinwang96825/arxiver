{"title": "Informative Language Representation Learning for Massively Multilingual\n  Neural Machine Translation", "abstract": "In a multilingual neural machine translation model that fully shares\nparameters across all languages, an artificial language token is usually used\nto guide translation into the desired target language. However, recent studies\nshow that prepending language tokens sometimes fails to navigate the\nmultilingual neural machine translation models into right translation\ndirections, especially on zero-shot translation. To mitigate this issue, we\npropose two methods, language embedding embodiment and language-aware\nmulti-head attention, to learn informative language representations to channel\ntranslation into right directions. The former embodies language embeddings into\ndifferent critical switching points along the information flow from the source\nto the target, aiming at amplifying translation direction guiding signals. The\nlatter exploits a matrix, instead of a vector, to represent a language in the\ncontinuous space. The matrix is chunked into multiple heads so as to learn\nlanguage representations in multiple subspaces. Experiment results on two\ndatasets for massively multilingual neural machine translation demonstrate that\nlanguage-aware multi-head attention benefits both supervised and zero-shot\ntranslation and significantly alleviates the off-target translation issue.\nFurther linguistic typology prediction experiments show that matrix-based\nlanguage representations learned by our methods are capable of capturing rich\nlinguistic typology features.", "published": "2022-09-04 04:27:17", "link": "http://arxiv.org/abs/2209.01530v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interpretable Fake News Detection with Topic and Deep Variational Models", "abstract": "The growing societal dependence on social media and user generated content\nfor news and information has increased the influence of unreliable sources and\nfake content, which muddles public discourse and lessens trust in the media.\nValidating the credibility of such information is a difficult task that is\nsusceptible to confirmation bias, leading to the development of algorithmic\ntechniques to distinguish between fake and real news. However, most existing\nmethods are challenging to interpret, making it difficult to establish trust in\npredictions, and make assumptions that are unrealistic in many real-world\nscenarios, e.g., the availability of audiovisual features or provenance. In\nthis work, we focus on fake news detection of textual content using\ninterpretable features and methods. In particular, we have developed a deep\nprobabilistic model that integrates a dense representation of textual news\nusing a variational autoencoder and bi-directional Long Short-Term Memory\n(LSTM) networks with semantic topic-related features inferred from a Bayesian\nadmixture model. Extensive experimental studies with 3 real-world datasets\ndemonstrate that our model achieves comparable performance to state-of-the-art\ncompeting models while facilitating model interpretability from the learned\ntopics. Finally, we have conducted model ablation studies to justify the\neffectiveness and accuracy of integrating neural embeddings and topic features\nboth quantitatively by evaluating performance and qualitatively through\nseparability in lower dimensional embeddings.", "published": "2022-09-04 05:31:00", "link": "http://arxiv.org/abs/2209.01536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Every picture tells a story: Image-grounded controllable stylistic story\n  generation", "abstract": "Generating a short story out of an image is arduous. Unlike image captioning,\nstory generation from an image poses multiple challenges: preserving the story\ncoherence, appropriately assessing the quality of the story, steering the\ngenerated story into a certain style, and addressing the scarcity of\nimage-story pair reference datasets limiting supervision during training. In\nthis work, we introduce Plug-and-Play Story Teller (PPST) and improve\nimage-to-story generation by: 1) alleviating the data scarcity problem by\nincorporating large pre-trained models, namely CLIP and GPT-2, to facilitate a\nfluent image-to-text generation with minimal supervision, and 2) enabling a\nmore style-relevant generation by incorporating stylistic adapters to control\nthe story generation. We conduct image-to-story generation experiments with\nnon-styled, romance-styled, and action-styled PPST approaches and compare our\ngenerated stories with those of previous work over three aspects, i.e., story\ncoherence, image-story relevance, and style fitness, using both automatic and\nhuman evaluation. The results show that PPST improves story coherence and has\nbetter image-story relevance, but has yet to be adequately stylistic.", "published": "2022-09-04 15:07:53", "link": "http://arxiv.org/abs/2209.01638v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SCL-RAI: Span-based Contrastive Learning with Retrieval Augmented\n  Inference for Unlabeled Entity Problem in NER", "abstract": "Named Entity Recognition is the task to locate and classify the entities in\nthe text. However, Unlabeled Entity Problem in NER datasets seriously hinders\nthe improvement of NER performance. This paper proposes SCL-RAI to cope with\nthis problem. Firstly, we decrease the distance of span representations with\nthe same label while increasing it for different ones via span-based\ncontrastive learning, which relieves the ambiguity among entities and improves\nthe robustness of the model over unlabeled entities. Then we propose retrieval\naugmented inference to mitigate the decision boundary shifting problem. Our\nmethod significantly outperforms the previous SOTA method by 4.21% and 8.64%\nF1-score on two real-world datasets.", "published": "2022-09-04 15:40:10", "link": "http://arxiv.org/abs/2209.01646v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ArgLegalSumm: Improving Abstractive Summarization of Legal Documents\n  with Argument Mining", "abstract": "A challenging task when generating summaries of legal documents is the\nability to address their argumentative nature. We introduce a simple technique\nto capture the argumentative structure of legal documents by integrating\nargument role labeling into the summarization process. Experiments with\npretrained language models show that our proposed approach improves performance\nover strong baselines", "published": "2022-09-04 15:55:56", "link": "http://arxiv.org/abs/2209.01650v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Large Language Models know what humans know?", "abstract": "Humans can attribute beliefs to others. However, it is unknown to what extent\nthis ability results from an innate biological endowment or from experience\naccrued through child development, particularly exposure to language describing\nothers' mental states. We test the viability of the language exposure\nhypothesis by assessing whether models exposed to large quantities of human\nlanguage display sensitivity to the implied knowledge states of characters in\nwritten passages. In pre-registered analyses, we present a linguistic version\nof the False Belief Task to both human participants and a Large Language Model,\nGPT-3. Both are sensitive to others' beliefs, but while the language model\nsignificantly exceeds chance behavior, it does not perform as well as the\nhumans, nor does it explain the full extent of their behavior -- despite being\nexposed to more language than a human would in a lifetime. This suggests that\nwhile statistical learning from language exposure may in part explain how\nhumans develop the ability to reason about the mental states of others, other\nmechanisms are also responsible.", "published": "2022-09-04 01:29:53", "link": "http://arxiv.org/abs/2209.01515v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Quantitative Stopword Generation for Sentiment Analysis via Recursive\n  and Iterative Deletion", "abstract": "Stopwords carry little semantic information and are often removed from text\ndata to reduce dataset size and improve machine learning model performance.\nConsequently, researchers have sought to develop techniques for generating\neffective stopword sets. Previous approaches have ranged from qualitative\ntechniques relying upon linguistic experts, to statistical approaches that\nextract word importance using correlations or frequency-dependent metrics\ncomputed on a corpus. We present a novel quantitative approach that employs\niterative and recursive feature deletion algorithms to see which words can be\ndeleted from a pre-trained transformer's vocabulary with the least degradation\nto its performance, specifically for the task of sentiment analysis.\nEmpirically, stopword lists generated via this approach drastically reduce\ndataset size while negligibly impacting model performance, in one such example\nshrinking the corpus by 28.4% while improving the accuracy of a trained\nlogistic regression model by 0.25%. In another instance, the corpus was shrunk\nby 63.7% with a 2.8% decrease in accuracy. These promising results indicate\nthat our approach can generate highly effective stopword sets for specific NLP\ntasks.", "published": "2022-09-04 03:04:10", "link": "http://arxiv.org/abs/2209.01519v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Selective Text Augmentation with Word Roles for Low-Resource Text\n  Classification", "abstract": "Data augmentation techniques are widely used in text classification tasks to\nimprove the performance of classifiers, especially in low-resource scenarios.\nMost previous methods conduct text augmentation without considering the\ndifferent functionalities of the words in the text, which may generate\nunsatisfactory samples. Different words may play different roles in text\nclassification, which inspires us to strategically select the proper roles for\ntext augmentation. In this work, we first identify the relationships between\nthe words in a text and the text category from the perspectives of statistical\ncorrelation and semantic similarity and then utilize them to divide the words\ninto four roles -- Gold, Venture, Bonus, and Trivial words, which have\ndifferent functionalities for text classification. Based on these word roles,\nwe present a new augmentation technique called STA (Selective Text\nAugmentation) where different text-editing operations are selectively applied\nto words with specific roles. STA can generate diverse and relatively clean\nsamples, while preserving the original core semantics, and is also quite simple\nto implement. Extensive experiments on 5 benchmark low-resource text\nclassification datasets illustrate that augmented samples produced by STA\nsuccessfully boost the performance of classification models which significantly\noutperforms previous non-selective methods, including two large language\nmodel-based techniques. Cross-dataset experiments further indicate that STA can\nhelp the classifiers generalize better to other datasets than previous methods.", "published": "2022-09-04 08:13:11", "link": "http://arxiv.org/abs/2209.01560v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Review of Sparse Expert Models in Deep Learning", "abstract": "Sparse expert models are a thirty-year old concept re-emerging as a popular\narchitecture in deep learning. This class of architecture encompasses\nMixture-of-Experts, Switch Transformers, Routing Networks, BASE layers, and\nothers, all with the unifying idea that each example is acted on by a subset of\nthe parameters. By doing so, the degree of sparsity decouples the parameter\ncount from the compute per example allowing for extremely large, but efficient\nmodels. The resulting models have demonstrated significant improvements across\ndiverse domains such as natural language processing, computer vision, and\nspeech recognition. We review the concept of sparse expert models, provide a\nbasic description of the common algorithms, contextualize the advances in the\ndeep learning era, and conclude by highlighting areas for future work.", "published": "2022-09-04 18:00:29", "link": "http://arxiv.org/abs/2209.01667v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The Effectiveness of Bidirectional Generative Patent Language Models", "abstract": "Generative patent language models can assist humans to write patent text more\neffectively. The question is how to measure effectiveness from a human-centric\nperspective and how to improve effectiveness. In this manuscript, a simplified\ndesign of the autocomplete function is proposed to increase effectiveness by\nmore than 10%. With the new design, the effectiveness of autocomplete can reach\nmore than 60%, which means that more than 60% of keystrokes can be saved by\nautocomplete. Since writing patent text does not necessarily start from the\nbeginning to the end, a question is whether the generative model can assist a\nuser no matter where to start writing. To answer the question, the generative\nmodels in this manuscript are pre-trained with training data in both\ndirections. The generative models become bidirectional. Since text generation\nis bidirectional, the calculation of autocomplete effectiveness can be\nbidirectional and starts from anywhere in the text. After thorough experiments,\na key finding is that the autocomplete effectiveness of a model for the same\ntext remains similar no matter where the calculation starts. The finding\nindicates that such bidirectional models can assist a user at a similar level,\nno matter where the user starts to write.", "published": "2022-09-04 03:12:27", "link": "http://arxiv.org/abs/2211.09690v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interactive Question Answering Systems: Literature Review", "abstract": "Question answering systems are recognized as popular and frequently effective\nmeans of information seeking on the web. In such systems, information seekers\ncan receive a concise response to their query by presenting their questions in\nnatural language. Interactive question answering is a recently proposed and\nincreasingly popular solution that resides at the intersection of question\nanswering and dialogue systems. On the one hand, the user can ask questions in\nnormal language and locate the actual response to her inquiry; on the other\nhand, the system can prolong the question-answering session into a dialogue if\nthere are multiple probable replies, very few, or ambiguities in the initial\nrequest. By permitting the user to ask more questions, interactive question\nanswering enables users to dynamically interact with the system and receive\nmore precise results. This survey offers a detailed overview of the interactive\nquestion-answering methods that are prevalent in current literature. It begins\nby explaining the foundational principles of question-answering systems, hence\ndefining new notations and taxonomies to combine all identified works inside a\nunified framework. The reviewed published work on interactive\nquestion-answering systems is then presented and examined in terms of its\nproposed methodology, evaluation approaches, and dataset/application domain. We\nalso describe trends surrounding specific tasks and issues raised by the\ncommunity, so shedding light on the future interests of scholars. Our work is\nfurther supported by a GitHub page with a synthesis of all the major topics\ncovered in this literature study.\nhttps://sisinflab.github.io/interactive-question-answering-systems-survey/", "published": "2022-09-04 13:46:54", "link": "http://arxiv.org/abs/2209.01621v3", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR", "A.1; H.3.3; I.2.7"], "primary_category": "cs.CL"}
{"title": "Time-domain speech super-resolution with GAN based modeling for\n  telephony speaker verification", "abstract": "Automatic Speaker Verification (ASV) technology has become commonplace in\nvirtual assistants. However, its performance suffers when there is a mismatch\nbetween the train and test domains. Mixed bandwidth training, i.e., pooling\ntraining data from both domains, is a preferred choice for developing a\nuniversal model that works for both narrowband and wideband domains. We propose\ncomplementing this technique by performing neural upsampling of narrowband\nsignals, also known as bandwidth extension. Our main goal is to discover and\nanalyze high-performing time-domain Generative Adversarial Network (GAN) based\nmodels to improve our downstream state-of-the-art ASV system. We choose GANs\nsince they (1) are powerful for learning conditional distribution and (2) allow\nflexible plug-in usage as a pre-processor during the training of downstream\ntask (ASV) with data augmentation. Prior works mainly focus on feature-domain\nbandwidth extension and limited experimental setups. We address these\nlimitations by 1) using time-domain extension models, 2) reporting results on\nthree real test sets, 2) extending training data, and 3) devising new test-time\nschemes. We compare supervised (conditional GAN) and unsupervised GANs\n(CycleGAN) and demonstrate average relative improvement in Equal Error Rate of\n8.6% and 7.7%, respectively. For further analysis, we study changes in\nspectrogram visual quality, audio perceptual quality, t-SNE embeddings, and ASV\nscore distributions. We show that our bandwidth extension leads to phenomena\nsuch as a shift of telephone (test) embeddings towards wideband (train)\nsignals, a negative correlation of perceptual quality with downstream\nperformance, and condition-independent score calibration.", "published": "2022-09-04 22:34:41", "link": "http://arxiv.org/abs/2209.01702v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
