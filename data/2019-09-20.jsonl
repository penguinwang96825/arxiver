{"title": "Cross-lingual Dependency Parsing with Unlabeled Auxiliary Languages", "abstract": "Cross-lingual transfer learning has become an important weapon to battle the\nunavailability of annotated resources for low-resource languages. One of the\nfundamental techniques to transfer across languages is learning\n\\emph{language-agnostic} representations, in the form of word embeddings or\ncontextual encodings. In this work, we propose to leverage unannotated\nsentences from auxiliary languages to help learning language-agnostic\nrepresentations. Specifically, we explore adversarial training for learning\ncontextual encoders that produce invariant representations across languages to\nfacilitate cross-lingual transfer. We conduct experiments on cross-lingual\ndependency parsing where we train a dependency parser on a source language and\ntransfer it to a wide range of target languages. Experiments on 28 target\nlanguages demonstrate that adversarial training significantly improves the\noverall transfer performances under several different settings. We conduct a\ncareful analysis to evaluate the language-agnostic representations resulted\nfrom adversarial training.", "published": "2019-09-20 00:12:39", "link": "http://arxiv.org/abs/1909.09265v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Working Hard or Hardly Working: Challenges of Integrating Typology into\n  Neural Dependency Parsers", "abstract": "This paper explores the task of leveraging typology in the context of\ncross-lingual dependency parsing. While this linguistic information has shown\ngreat promise in pre-neural parsing, results for neural architectures have been\nmixed. The aim of our investigation is to better understand this\nstate-of-the-art. Our main findings are as follows: 1) The benefit of\ntypological information is derived from coarsely grouping languages into\nsyntactically-homogeneous clusters rather than from learning to leverage\nvariations along individual typological dimensions in a compositional manner;\n2) Typology consistent with the actual corpus statistics yields better transfer\nperformance; 3) Typological similarity is only a rough proxy of cross-lingual\ntransferability with respect to parsing.", "published": "2019-09-20 01:07:11", "link": "http://arxiv.org/abs/1909.09279v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Jointly Learning Entity and Relation Representations for Entity\n  Alignment", "abstract": "Entity alignment is a viable means for integrating heterogeneous knowledge\namong different knowledge graphs (KGs). Recent developments in the field often\ntake an embedding-based approach to model the structural information of KGs so\nthat entity alignment can be easily performed in the embedding space. However,\nmost existing works do not explicitly utilize useful relation representations\nto assist in entity alignment, which, as we will show in the paper, is a simple\nyet effective way for improving entity alignment. This paper presents a novel\njoint learning framework for entity alignment. At the core of our approach is a\nGraph Convolutional Network (GCN) based framework for learning both entity and\nrelation representations. Rather than relying on pre-aligned relation seeds to\nlearn relation representations, we first approximate them using entity\nembeddings learned by the GCN. We then incorporate the relation approximation\ninto entities to iteratively learn better representations for both. Experiments\nperformed on three real-world cross-lingual datasets show that our approach\nsubstantially outperforms state-of-the-art entity alignment methods.", "published": "2019-09-20 04:35:07", "link": "http://arxiv.org/abs/1909.09317v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Designing dialogue systems: A mean, grumpy, sarcastic chatbot in the\n  browser", "abstract": "In this work we explore a deep learning-based dialogue system that generates\nsarcastic and humorous responses from a conversation design perspective. We\ntrained a seq2seq model on a carefully curated dataset of 3000\nquestion-answering pairs, the core of our mean, grumpy, sarcastic chatbot. We\nshow that end-to-end systems learn patterns very quickly from small datasets\nand thus, are able to transfer simple linguistic structures representing\nabstract concepts to unseen settings. We also deploy our LSTM-based\nencoder-decoder model in the browser, where users can directly interact with\nthe chatbot. Human raters evaluated linguistic quality, creativity and\nhuman-like traits, revealing the system's strengths, limitations and potential\nfor future research.", "published": "2019-09-20 14:37:05", "link": "http://arxiv.org/abs/1909.09531v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Named Entity Recognition with Partially Annotated Training Data", "abstract": "Supervised machine learning assumes the availability of fully-labeled data,\nbut in many cases, such as low-resource languages, the only data available is\npartially annotated. We study the problem of Named Entity Recognition (NER)\nwith partially annotated training data in which a fraction of the named\nentities are labeled, and all other tokens, entities or otherwise, are labeled\nas non-entity by default. In order to train on this noisy dataset, we need to\ndistinguish between the true and false negatives. To this end, we introduce a\nconstraint-driven iterative algorithm that learns to detect false negatives in\nthe noisy set and downweigh them, resulting in a weighted training set. With\nthis set, we train a weighted NER model. We evaluate our algorithm with\nweighted variants of neural and non-neural NER models on data in 8 languages\nfrom several language and script families, showing strong ability to learn from\npartial data. Finally, to show real-world efficacy, we evaluate on a Bengali\nNER corpus annotated by non-speakers, outperforming the prior state-of-the-art\nby over 5 points F1.", "published": "2019-09-20 00:39:07", "link": "http://arxiv.org/abs/1909.09270v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sampling Bias in Deep Active Classification: An Empirical Study", "abstract": "The exploding cost and time needed for data labeling and model training are\nbottlenecks for training DNN models on large datasets. Identifying smaller\nrepresentative data samples with strategies like active learning can help\nmitigate such bottlenecks. Previous works on active learning in NLP identify\nthe problem of sampling bias in the samples acquired by uncertainty-based\nquerying and develop costly approaches to address it. Using a large empirical\nstudy, we demonstrate that active set selection using the posterior entropy of\ndeep models like FastText.zip (FTZ) is robust to sampling biases and to various\nalgorithmic choices (query size and strategies) unlike that suggested by\ntraditional literature. We also show that FTZ based query strategy produces\nsample sets similar to those from more sophisticated approaches (e.g ensemble\nnetworks). Finally, we show the effectiveness of the selected samples by\ncreating tiny high-quality datasets, and utilizing them for fast and cheap\ntraining of large models. Based on the above, we propose a simple baseline for\ndeep active text classification that outperforms the state-of-the-art. We\nexpect the presented work to be useful and informative for dataset compression\nand for problems involving active, semi-supervised or online learning\nscenarios. Code and models are available at:\nhttps://github.com/drimpossible/Sampling-Bias-Active-Learning", "published": "2019-09-20 09:35:20", "link": "http://arxiv.org/abs/1909.09389v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Critical Analysis of Biased Parsers in Unsupervised Parsing", "abstract": "A series of recent papers has used a parsing algorithm due to Shen et al.\n(2018) to recover phrase-structure trees based on proxies for \"syntactic\ndepth.\" These proxy depths are obtained from the representations learned by\nrecurrent language models augmented with mechanisms that encourage the\n(unsupervised) discovery of hierarchical structure latent in natural language\nsentences. Using the same parser, we show that proxies derived from a\nconventional LSTM language model produce trees comparably well to the\nspecialized architectures used in previous work. However, we also provide a\ndetailed analysis of the parsing algorithm, showing (1) that it is\nincomplete---that is, it can recover only a fraction of possible trees---and\n(2) that it has a marked bias for right-branching structures which results in\ninflated performance in right-branching languages like English. Our analysis\nshows that evaluating with biased parsing algorithms can inflate the apparent\nstructural competence of language models.", "published": "2019-09-20 11:09:52", "link": "http://arxiv.org/abs/1909.09428v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pivot-based Transfer Learning for Neural Machine Translation between\n  Non-English Languages", "abstract": "We present effective pre-training strategies for neural machine translation\n(NMT) using parallel corpora involving a pivot language, i.e., source-pivot and\npivot-target, leading to a significant improvement in source-target\ntranslation. We propose three methods to increase the relation among source,\npivot, and target languages in the pre-training: 1) step-wise training of a\nsingle model for different language pairs, 2) additional adapter component to\nsmoothly connect pre-trained encoder and decoder, and 3) cross-lingual encoder\ntraining via autoencoding of the pivot language. Our methods greatly outperform\nmultilingual models up to +2.6% BLEU in WMT 2019 French-German and German-Czech\ntasks. We show that our improvements are valid also in zero-shot/zero-resource\nscenarios.", "published": "2019-09-20 14:16:27", "link": "http://arxiv.org/abs/1909.09524v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Creative GANs for generating poems, lyrics, and metaphors", "abstract": "Generative models for text have substantially contributed to tasks like\nmachine translation and language modeling, using maximum likelihood\noptimization (MLE). However, for creative text generation, where multiple\noutputs are possible and originality and uniqueness are encouraged, MLE falls\nshort. Methods optimized for MLE lead to outputs that can be generic,\nrepetitive and incoherent. In this work, we use a Generative Adversarial\nNetwork framework to alleviate this problem. We evaluate our framework on\npoetry, lyrics and metaphor datasets, each with widely different\ncharacteristics, and report better performance of our objective function over\nother generative models.", "published": "2019-09-20 14:40:18", "link": "http://arxiv.org/abs/1909.09534v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Natural Language Processing via LDA Topic Model in Recommendation\n  Systems", "abstract": "Today, Internet is one of the widest available media worldwide.\nRecommendation systems are increasingly being used in various applications such\nas movie recommendation, mobile recommendation, article recommendation and etc.\nCollaborative Filtering (CF) and Content-Based (CB) are Well-known techniques\nfor building recommendation systems. Topic modeling based on LDA, is a powerful\ntechnique for semantic mining and perform topic extraction. In the past few\nyears, many articles have been published based on LDA technique for building\nrecommendation systems. In this paper, we present taxonomy of recommendation\nsystems and applications based on LDA. In addition, we utilize LDA and Gibbs\nsampling algorithms to evaluate ISWC and WWW conference publications in\ncomputer science. Our study suggest that the recommendation systems based on\nLDA could be effective in building smart recommendation system in online\ncommunities.", "published": "2019-09-20 15:08:51", "link": "http://arxiv.org/abs/1909.09551v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Towards Neural Language Evaluators", "abstract": "We review three limitations of BLEU and ROUGE -- the most popular metrics\nused to assess reference summaries against hypothesis summaries, come up with\ncriteria for what a good metric should behave like and propose concrete ways to\nuse recent Transformers-based Language Models to assess reference summaries\nagainst hypothesis summaries.", "published": "2019-09-20 00:24:59", "link": "http://arxiv.org/abs/1909.09268v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BERT Meets Chinese Word Segmentation", "abstract": "Chinese word segmentation (CWS) is a fundamental task for Chinese language\nunderstanding. Recently, neural network-based models have attained superior\nperformance in solving the in-domain CWS task. Last year, Bidirectional Encoder\nRepresentation from Transformers (BERT), a new language representation model,\nhas been proposed as a backbone model for many natural language tasks and\nredefined the corresponding performance. The excellent performance of BERT\nmotivates us to apply it to solve the CWS task. By conducting intensive\nexperiments in the benchmark datasets from the second International Chinese\nWord Segmentation Bake-off, we obtain several keen observations. BERT can\nslightly improve the performance even when the datasets contain the issue of\nlabeling inconsistency. When applying sufficiently learned features, Softmax, a\nsimpler classifier, can attain the same performance as that of a more\ncomplicated classifier, e.g., Conditional Random Field (CRF). The performance\nof BERT usually increases as the model size increases. The features extracted\nby BERT can be also applied as good candidates for other neural network models.", "published": "2019-09-20 01:53:19", "link": "http://arxiv.org/abs/1909.09292v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Measuring Conceptual Entanglement in Collections of Documents", "abstract": "Conceptual entanglement is a crucial phenomenon in quantum cognition because\nit implies that classical probabilities cannot model non--compositional\nconceptual phenomena. While several psychological experiments have been\ndeveloped to test conceptual entanglement, this has not been explored in the\ncontext of Natural Language Processing. In this paper, we apply the hypothesis\nthat words of a document are traces of the concepts that a person has in mind\nwhen writing the document. Therefore, if these concepts are entangled, we\nshould be able to observe traces of their entanglement in the documents. In\nparticular, we test conceptual entanglement by contrasting language simulations\nwith results obtained from a text corpus. Our analysis indicates that\nconceptual entanglement is strongly linked to the way in which language is\nstructured. We discuss the implications of this finding in the context of\nconceptual modeling and of Natural Language Processing.", "published": "2019-09-20 20:22:15", "link": "http://arxiv.org/abs/1909.09708v1", "categories": ["cs.CL", "cs.AI", "quant-ph"], "primary_category": "cs.CL"}
{"title": "Learning to Create Sentence Semantic Relation Graphs for Multi-Document\n  Summarization", "abstract": "Linking facts across documents is a challenging task, as the language used to\nexpress the same information in a sentence can vary significantly, which\ncomplicates the task of multi-document summarization. Consequently, existing\napproaches heavily rely on hand-crafted features, which are domain-dependent\nand hard to craft, or additional annotated data, which is costly to gather. To\novercome these limitations, we present a novel method, which makes use of two\ntypes of sentence embeddings: universal embeddings, which are trained on a\nlarge unrelated corpus, and domain-specific embeddings, which are learned\nduring training.\n  To this end, we develop SemSentSum, a fully data-driven model able to\nleverage both types of sentence embeddings by building a sentence semantic\nrelation graph. SemSentSum achieves competitive results on two types of\nsummary, consisting of 665 bytes and 100 words. Unlike other state-of-the-art\nmodels, neither hand-crafted features nor additional annotated data are\nnecessary, and the method is easily adaptable for other tasks. To our\nknowledge, we are the first to use multiple sentence embeddings for the task of\nmulti-document summarization.", "published": "2019-09-20 10:21:55", "link": "http://arxiv.org/abs/1909.12231v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Towards Multimodal Understanding of Passenger-Vehicle Interactions in\n  Autonomous Vehicles: Intent/Slot Recognition Utilizing Audio-Visual Data", "abstract": "Understanding passenger intents from spoken interactions and car's vision\n(both inside and outside the vehicle) are important building blocks towards\ndeveloping contextual dialog systems for natural interactions in autonomous\nvehicles (AV). In this study, we continued exploring AMIE (Automated-vehicle\nMultimodal In-cabin Experience), the in-cabin agent responsible for handling\ncertain multimodal passenger-vehicle interactions. When the passengers give\ninstructions to AMIE, the agent should parse such commands properly considering\navailable three modalities (language/text, audio, video) and trigger the\nappropriate functionality of the AV system. We had collected a multimodal\nin-cabin dataset with multi-turn dialogues between the passengers and AMIE\nusing a Wizard-of-Oz scheme via realistic scavenger hunt game. In our previous\nexplorations, we experimented with various RNN-based models to detect\nutterance-level intents (set destination, change route, go faster, go slower,\nstop, park, pull over, drop off, open door, and others) along with intent\nkeywords and relevant slots (location, position/direction, object,\ngesture/gaze, time-guidance, person) associated with the action to be performed\nin our AV scenarios. In this recent work, we propose to discuss the benefits of\nmultimodal understanding of in-cabin utterances by incorporating\nverbal/language input (text and speech embeddings) together with the\nnon-verbal/acoustic and visual input from inside and outside the vehicle (i.e.,\npassenger gestures and gaze from in-cabin video stream, referred objects\noutside of the vehicle from the road view camera stream). Our experimental\nresults outperformed text-only baselines and with multimodality, we achieved\nimproved performances for utterance-level intent detection and slot filling.", "published": "2019-09-20 00:00:41", "link": "http://arxiv.org/abs/1909.13714v1", "categories": ["cs.MM", "cs.CL", "cs.CV"], "primary_category": "cs.MM"}
{"title": "MIMII Dataset: Sound Dataset for Malfunctioning Industrial Machine\n  Investigation and Inspection", "abstract": "Factory machinery is prone to failure or breakdown, resulting in significant\nexpenses for companies. Hence, there is a rising interest in machine monitoring\nusing different sensors including microphones. In the scientific community, the\nemergence of public datasets has led to advancements in acoustic detection and\nclassification of scenes and events, but there are no public datasets that\nfocus on the sound of industrial machines under normal and anomalous operating\nconditions in real factory environments. In this paper, we present a new\ndataset of industrial machine sounds that we call a sound dataset for\nmalfunctioning industrial machine investigation and inspection (MIMII dataset).\nNormal sounds were recorded for different types of industrial machines (i.e.,\nvalves, pumps, fans, and slide rails), and to resemble a real-life scenario,\nvarious anomalous sounds were recorded (e.g., contamination, leakage, rotating\nunbalance, and rail damage). The purpose of releasing the MIMII dataset is to\nassist the machine-learning and signal-processing community with their\ndevelopment of automated facility maintenance. The MIMII dataset is freely\navailable for download at: https://zenodo.org/record/3384388", "published": "2019-09-20 07:17:34", "link": "http://arxiv.org/abs/1909.09347v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
