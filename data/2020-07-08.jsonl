{"title": "Research on multi-dimensional end-to-end phrase recognition algorithm\n  based on background knowledge", "abstract": "At present, the deep end-to-end method based on supervised learning is used\nin entity recognition and dependency analysis. There are two problems in this\nmethod: firstly, background knowledge cannot be introduced; secondly, multi\ngranularity and nested features of natural language cannot be recognized. In\norder to solve these problems, the annotation rules based on phrase window are\nproposed, and the corresponding multi-dimensional end-to-end phrase recognition\nalgorithm is designed. This annotation rule divides sentences into seven types\nof nested phrases, and indicates the dependency between phrases. The algorithm\ncan not only introduce background knowledge, recognize all kinds of nested\nphrases in sentences, but also recognize the dependency between phrases. The\nexperimental results show that the annotation rule is easy to use and has no\nambiguity; the matching algorithm is more consistent with the multi granularity\nand diversity characteristics of syntax than the traditional end-to-end\nalgorithm. The experiment on CPWD dataset, by introducing background knowledge,\nthe new algorithm improves the accuracy of the end-to-end method by more than\none point. The corresponding method was applied to the CCL 2018 competition and\nwon the first place in the task of Chinese humor type recognition.", "published": "2020-07-08 02:30:00", "link": "http://arxiv.org/abs/2007.03860v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KQA Pro: A Dataset with Explicit Compositional Programs for Complex\n  Question Answering over Knowledge Base", "abstract": "Complex question answering over knowledge base (Complex KBQA) is challenging\nbecause it requires various compositional reasoning capabilities, such as\nmulti-hop inference, attribute comparison, set operation. Existing benchmarks\nhave some shortcomings that limit the development of Complex KBQA: 1) they only\nprovide QA pairs without explicit reasoning processes; 2) questions are poor in\ndiversity or scale. To this end, we introduce KQA Pro, a dataset for Complex\nKBQA including ~120K diverse natural language questions. We introduce a\ncompositional and interpretable programming language KoPL to represent the\nreasoning process of complex questions. For each question, we provide the\ncorresponding KoPL program and SPARQL query, so that KQA Pro serves for both\nKBQA and semantic parsing tasks. Experimental results show that SOTA KBQA\nmethods cannot achieve promising results on KQA Pro as on current datasets,\nwhich suggests that KQA Pro is challenging and Complex KBQA requires further\nresearch efforts. We also treat KQA Pro as a diagnostic dataset for testing\nmultiple reasoning skills, conduct a thorough evaluation of existing models and\ndiscuss further directions for Complex KBQA. Our codes and datasets can be\nobtained from https://github.com/shijx12/KQAPro_Baselines.", "published": "2020-07-08 03:28:04", "link": "http://arxiv.org/abs/2007.03875v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Audio-Visual Understanding of Passenger Intents for In-Cabin\n  Conversational Agents", "abstract": "Building multimodal dialogue understanding capabilities situated in the\nin-cabin context is crucial to enhance passenger comfort in autonomous vehicle\n(AV) interaction systems. To this end, understanding passenger intents from\nspoken interactions and vehicle vision systems is a crucial component for\ndeveloping contextual and visually grounded conversational agents for AV.\nTowards this goal, we explore AMIE (Automated-vehicle Multimodal In-cabin\nExperience), the in-cabin agent responsible for handling multimodal\npassenger-vehicle interactions. In this work, we discuss the benefits of a\nmultimodal understanding of in-cabin utterances by incorporating\nverbal/language input together with the non-verbal/acoustic and visual clues\nfrom inside and outside the vehicle. Our experimental results outperformed\ntext-only baselines as we achieved improved performances for intent detection\nwith a multimodal approach.", "published": "2020-07-08 03:31:03", "link": "http://arxiv.org/abs/2007.03876v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generalizing Tensor Decomposition for N-ary Relational Knowledge Bases", "abstract": "With the rapid development of knowledge bases (KBs), link prediction task,\nwhich completes KBs with missing facts, has been broadly studied in especially\nbinary relational KBs (a.k.a knowledge graph) with powerful tensor\ndecomposition related methods. However, the ubiquitous n-ary relational KBs\nwith higher-arity relational facts are paid less attention, in which existing\ntranslation based and neural network based approaches have weak expressiveness\nand high complexity in modeling various relations. Tensor decomposition has not\nbeen considered for n-ary relational KBs, while directly extending tensor\ndecomposition related methods of binary relational KBs to the n-ary case does\nnot yield satisfactory results due to exponential model complexity and their\nstrong assumptions on binary relations. To generalize tensor decomposition for\nn-ary relational KBs, in this work, we propose GETD, a generalized model based\non Tucker decomposition and Tensor Ring decomposition. The existing negative\nsampling technique is also generalized to the n-ary case for GETD. In addition,\nwe theoretically prove that GETD is fully expressive to completely represent\nany KBs. Extensive evaluations on two representative n-ary relational KB\ndatasets demonstrate the superior performance of GETD, significantly improving\nthe state-of-the-art methods by over 15\\%. Moreover, GETD further obtains the\nstate-of-the-art results on the benchmark binary relational KB datasets.", "published": "2020-07-08 09:49:38", "link": "http://arxiv.org/abs/2007.03988v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Neural Textual Representations for Citation Recommendation", "abstract": "With the rapid growth of the scientific literature, manually selecting\nappropriate citations for a paper is becoming increasingly challenging and\ntime-consuming. While several approaches for automated citation recommendation\nhave been proposed in the recent years, effective document representations for\ncitation recommendation are still elusive to a large extent. For this reason,\nin this paper we propose a novel approach to citation recommendation which\nleverages a deep sequential representation of the documents (Sentence-BERT)\ncascaded with Siamese and triplet networks in a submodular scoring function. To\nthe best of our knowledge, this is the first approach to combine deep\nrepresentations and submodular selection for a task of citation recommendation.\nExperiments have been carried out using a popular benchmark dataset - the ACL\nAnthology Network corpus - and evaluated against baselines and a\nstate-of-the-art approach using metrics such as the MRR and F1-at-k score. The\nresults show that the proposed approach has been able to outperform all the\ncompared approaches in every measured metric.", "published": "2020-07-08 12:38:50", "link": "http://arxiv.org/abs/2007.04070v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discourse Coherence, Reference Grounding and Goal Oriented Dialogue", "abstract": "Prior approaches to realizing mixed-initiative human--computer referential\ncommunication have adopted information-state or collaborative problem-solving\napproaches. In this paper, we argue for a new approach, inspired by\ncoherence-based models of discourse such as SDRT \\cite{asher-lascarides:2003a},\nin which utterances attach to an evolving discourse structure and the\nassociated knowledge graph of speaker commitments serves as an interface to\nreal-world reasoning and conversational strategy. As first steps towards\nimplementing the approach, we describe a simple dialogue system in a\nreferential communication domain that accumulates constraints across discourse,\ninterprets them using a learned probabilistic model, and plans clarification\nusing reinforcement learning.", "published": "2020-07-08 20:53:14", "link": "http://arxiv.org/abs/2007.04428v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Graph Representation Learning for Video Dialog via Multi-Modal\n  Shuffled Transformers", "abstract": "Given an input video, its associated audio, and a brief caption, the\naudio-visual scene aware dialog (AVSD) task requires an agent to indulge in a\nquestion-answer dialog with a human about the audio-visual content. This task\nthus poses a challenging multi-modal representation learning and reasoning\nscenario, advancements into which could influence several human-machine\ninteraction applications. To solve this task, we introduce a\nsemantics-controlled multi-modal shuffled Transformer reasoning framework,\nconsisting of a sequence of Transformer modules, each taking a modality as\ninput and producing representations conditioned on the input question. Our\nproposed Transformer variant uses a shuffling scheme on their multi-head\noutputs, demonstrating better regularization. To encode fine-grained visual\ninformation, we present a novel dynamic scene graph representation learning\npipeline that consists of an intra-frame reasoning layer producing\nspatio-semantic graph representations for every frame, and an inter-frame\naggregation module capturing temporal cues. Our entire pipeline is trained\nend-to-end. We present experiments on the benchmark AVSD dataset, both on\nanswer generation and selection tasks. Our results demonstrate state-of-the-art\nperformances on all evaluation metrics.", "published": "2020-07-08 02:00:22", "link": "http://arxiv.org/abs/2007.03848v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Best-First Beam Search", "abstract": "Decoding for many NLP tasks requires an effective heuristic algorithm for\napproximating exact search since the problem of searching the full output space\nis often intractable, or impractical in many settings. The default algorithm\nfor this job is beam search -- a pruned version of breadth-first search. Quite\nsurprisingly, beam search often returns better results than exact inference due\nto beneficial search bias for NLP tasks. In this work, we show that the\nstandard implementation of beam search can be made up to 10x faster in\npractice. Our method assumes that the scoring function is monotonic in the\nsequence length, which allows us to safely prune hypotheses that cannot be in\nthe final set of hypotheses early on. We devise effective monotonic\napproximations to popular nonmonontic scoring functions, including length\nnormalization and mutual information decoding. Lastly, we propose a\nmemory-reduced variant of Best-First Beam Search, which has a similar\nbeneficial search bias in terms of downstream performance, but runs in a\nfraction of the time.", "published": "2020-07-08 05:56:01", "link": "http://arxiv.org/abs/2007.03909v5", "categories": ["cs.CL", "cs.DS"], "primary_category": "cs.CL"}
{"title": "Analysis of Predictive Coding Models for Phonemic Representation\n  Learning in Small Datasets", "abstract": "Neural network models using predictive coding are interesting from the\nviewpoint of computational modelling of human language acquisition, where the\nobjective is to understand how linguistic units could be learned from speech\nwithout any labels. Even though several promising predictive coding -based\nlearning algorithms have been proposed in the literature, it is currently\nunclear how well they generalise to different languages and training dataset\nsizes. In addition, despite that such models have shown to be effective\nphonemic feature learners, it is unclear whether minimisation of the predictive\nloss functions of these models also leads to optimal phoneme-like\nrepresentations. The present study investigates the behaviour of two predictive\ncoding models, Autoregressive Predictive Coding and Contrastive Predictive\nCoding, in a phoneme discrimination task (ABX task) for two languages with\ndifferent dataset sizes. Our experiments show a strong correlation between the\nautoregressive loss and the phoneme discrimination scores with the two\ndatasets. However, to our surprise, the CPC model shows rapid convergence\nalready after one pass over the training data, and, on average, its\nrepresentations outperform those of APC on both languages.", "published": "2020-07-08 15:46:13", "link": "http://arxiv.org/abs/2007.04205v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "IQ-VQA: Intelligent Visual Question Answering", "abstract": "Even though there has been tremendous progress in the field of Visual\nQuestion Answering, models today still tend to be inconsistent and brittle. To\nthis end, we propose a model-independent cyclic framework which increases\nconsistency and robustness of any VQA architecture. We train our models to\nanswer the original question, generate an implication based on the answer and\nthen also learn to answer the generated implication correctly. As a part of the\ncyclic framework, we propose a novel implication generator which can generate\nimplied questions from any question-answer pair. As a baseline for future works\non consistency, we provide a new human annotated VQA-Implications dataset. The\ndataset consists of ~30k questions containing implications of 3 types - Logical\nEquivalence, Necessary Condition and Mutual Exclusion - made from the VQA v2.0\nvalidation dataset. We show that our framework improves consistency of VQA\nmodels by ~15% on the rule-based dataset, ~7% on VQA-Implications dataset and\nrobustness by ~2%, without degrading their performance. In addition, we also\nquantitatively show improvement in attention maps which highlights better\nmulti-modal understanding of vision and language.", "published": "2020-07-08 20:41:52", "link": "http://arxiv.org/abs/2007.04422v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Language Modeling with Reduced Densities", "abstract": "This work originates from the observation that today's state-of-the-art\nstatistical language models are impressive not only for their performance, but\nalso - and quite crucially - because they are built entirely from correlations\nin unstructured text data. The latter observation prompts a fundamental\nquestion that lies at the heart of this paper: What mathematical structure\nexists in unstructured text data? We put forth enriched category theory as a\nnatural answer. We show that sequences of symbols from a finite alphabet, such\nas those found in a corpus of text, form a category enriched over\nprobabilities. We then address a second fundamental question: How can this\ninformation be stored and modeled in a way that preserves the categorical\nstructure? We answer this by constructing a functor from our enriched category\nof text to a particular enriched category of reduced density operators. The\nlatter leverages the Loewner order on positive semidefinite operators, which\ncan further be interpreted as a toy example of entailment.", "published": "2020-07-08 00:41:53", "link": "http://arxiv.org/abs/2007.03834v4", "categories": ["cs.CL", "cs.LG", "math.CT", "quant-ph"], "primary_category": "cs.CL"}
{"title": "Streaming End-to-End Bilingual ASR Systems with Joint Language\n  Identification", "abstract": "Multilingual ASR technology simplifies model training and deployment, but its\naccuracy is known to depend on the availability of language information at\nruntime. Since language identity is seldom known beforehand in real-world\nscenarios, it must be inferred on-the-fly with minimum latency. Furthermore, in\nvoice-activated smart assistant systems, language identity is also required for\ndownstream processing of ASR output. In this paper, we introduce streaming,\nend-to-end, bilingual systems that perform both ASR and language identification\n(LID) using the recurrent neural network transducer (RNN-T) architecture. On\nthe input side, embeddings from pretrained acoustic-only LID classifiers are\nused to guide RNN-T training and inference, while on the output side, language\ntargets are jointly modeled with ASR targets. The proposed method is applied to\ntwo language pairs: English-Spanish as spoken in the United States, and\nEnglish-Hindi as spoken in India. Experiments show that for English-Spanish,\nthe bilingual joint ASR-LID architecture matches monolingual ASR and\nacoustic-only LID accuracies. For the more challenging (owing to\nwithin-utterance code switching) case of English-Hindi, English ASR and LID\nmetrics show degradation. Overall, in scenarios where users switch dynamically\nbetween languages, the proposed architecture offers a promising simplification\nover running multiple monolingual ASR models and an LID classifier in parallel.", "published": "2020-07-08 05:00:25", "link": "http://arxiv.org/abs/2007.03900v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Conversational Recommender Systems via Knowledge Graph based\n  Semantic Fusion", "abstract": "Conversational recommender systems (CRS) aim to recommend high-quality items\nto users through interactive conversations. Although several efforts have been\nmade for CRS, two major issues still remain to be solved. First, the\nconversation data itself lacks of sufficient contextual information for\naccurately understanding users' preference. Second, there is a semantic gap\nbetween natural language expression and item-level user preference. To address\nthese issues, we incorporate both word-oriented and entity-oriented knowledge\ngraphs (KG) to enhance the data representations in CRSs, and adopt Mutual\nInformation Maximization to align the word-level and entity-level semantic\nspaces. Based on the aligned semantic representations, we further develop a\nKG-enhanced recommender component for making accurate recommendations, and a\nKG-enhanced dialog component that can generate informative keywords or entities\nin the response text. Extensive experiments have demonstrated the effectiveness\nof our approach in yielding better performance on both recommendation and\nconversation tasks.", "published": "2020-07-08 11:14:23", "link": "http://arxiv.org/abs/2007.04032v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Automatic Detection of Sexist Statements Commonly Used at the Workplace", "abstract": "Detecting hate speech in the workplace is a unique classification task, as\nthe underlying social context implies a subtler version of conventional hate\nspeech. Applications regarding a state-of the-art workplace sexism detection\nmodel include aids for Human Resources departments, AI chatbots and sentiment\nanalysis. Most existing hate speech detection methods, although robust and\naccurate, focus on hate speech found on social media, specifically Twitter. The\ncontext of social media is much more anonymous than the workplace, therefore it\ntends to lend itself to more aggressive and \"hostile\" versions of sexism.\nTherefore, datasets with large amounts of \"hostile\" sexism have a slightly\neasier detection task since \"hostile\" sexist statements can hinge on a couple\nwords that, regardless of context, tip the model off that a statement is\nsexist. In this paper we present a dataset of sexist statements that are more\nlikely to be said in the workplace as well as a deep learning model that can\nachieve state-of-the art results. Previous research has created\nstate-of-the-art models to distinguish \"hostile\" and \"benevolent\" sexism based\nsimply on aggregated Twitter data. Our deep learning methods, initialized with\nGloVe or random word embeddings, use LSTMs with attention mechanisms to\noutperform those models on a more diverse, filtered dataset that is more\ntargeted towards workplace sexism, leading to an F1 score of 0.88.", "published": "2020-07-08 15:14:29", "link": "http://arxiv.org/abs/2007.04181v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Speech Representations from Raw Audio by Joint Audiovisual\n  Self-Supervision", "abstract": "The intuitive interaction between the audio and visual modalities is valuable\nfor cross-modal self-supervised learning. This concept has been demonstrated\nfor generic audiovisual tasks like video action recognition and acoustic scene\nclassification. However, self-supervision remains under-explored for\naudiovisual speech. We propose a method to learn self-supervised speech\nrepresentations from the raw audio waveform. We train a raw audio encoder by\ncombining audio-only self-supervision (by predicting informative audio\nattributes) with visual self-supervision (by generating talking faces from\naudio). The visual pretext task drives the audio representations to capture\ninformation related to lip movements. This enriches the audio encoder with\nvisual information and the encoder can be used for evaluation without the\nvisual modality. Our method attains competitive performance with respect to\nexisting self-supervised audio features on established isolated word\nclassification benchmarks, and significantly outperforms other methods at\nlearning from fewer labels. Notably, our method also outperforms fully\nsupervised training, thus providing a strong initialization for speech related\ntasks. Our results demonstrate the potential of multimodal self-supervision in\naudiovisual speech for learning good audio representations.", "published": "2020-07-08 14:07:06", "link": "http://arxiv.org/abs/2007.04134v1", "categories": ["eess.AS", "cs.CL", "cs.CV", "cs.LG", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Information, communication and music: Recognition of musical dissonance\n  and consonance in a simple reservoir computing system", "abstract": "Reservoir computing is an emerging, but very successful approach towards\nprocessing and classification of various signals. It can be described as a\nmodel of a transient computation, where influence of input changes internal\ndynamics of chosen computational reservoir. Trajectory of these changes\nrepresents computation performed by the system. The selection of a suitable\ncomputational substrate capable of non-linear response and rich internal\ndynamics ensures the implementation of simple readout protocols. Signal\nevolution based on the rich dynamics of the reservoir layer helps to emphasize\ndifferences between given signals thus enabling their easier classification.\nHere we present a simple reservoir computing system (single node echo-state\nmachine) implemented on Multisim platform as a tool for classification of\nmusical intervals according to their consonant or dissonant character. The\nresult of this classification closely resembled sensory dissonance curve, with\nsome significant differences. A deeper analysis of the received signals\nindicates the geometric relationships between the consonant and dissonant\nintervals, enabling their classification.", "published": "2020-07-08 18:39:58", "link": "http://arxiv.org/abs/2007.04360v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Resolution Beta-Divergence NMF for Blind Spectral Unmixing", "abstract": "Many datasets are obtained as a resolution trade-off between two adversarial\ndimensions; for example between the frequency and the temporal resolutions for\nthe spectrogram of an audio signal, and between the number of wavelengths and\nthe spatial resolution for a hyper/multi-spectral image. To perform blind\nsource separation using observations with different resolutions, a standard\napproach is to use coupled nonnegative matrix factorizations (NMF). Most\nprevious works have focused on the least squares error measure, which is the\n$\\beta$-divergence for $\\beta = 2$. In this paper, we formulate this\nmulti-resolution NMF problem for any $\\beta$-divergence, and propose an\nalgorithm based on multiplicative updates (MU). We show on numerical\nexperiments that the MU are able to obtain high resolutions in both dimensions\non two applications: (1) blind unmixing of audio spectrograms: to the best of\nour knowledge, this is the first time a coupled NMF model is used in this\ncontext, and (2) the fusion of hyperspectral and multispectral images: we show\nthat the MU compete favorable with state-of-the-art algorithms in particular in\nthe presence of non-Gaussian noise.", "published": "2020-07-08 04:50:05", "link": "http://arxiv.org/abs/2007.03893v3", "categories": ["eess.SP", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "eess.SP"}
{"title": "Training Sound Event Detection On A Heterogeneous Dataset", "abstract": "Training a sound event detection algorithm on a heterogeneous dataset\nincluding both recorded and synthetic soundscapes that can have various\nlabeling granularity is a non-trivial task that can lead to systems requiring\nseveral technical choices. These technical choices are often passed from one\nsystem to another without being questioned. We propose to perform a detailed\nanalysis of DCASE 2020 task 4 sound event detection baseline with regards to\nseveral aspects such as the type of data used for training, the parameters of\nthe mean-teacher or the transformations applied while generating the synthetic\nsoundscapes. Some of the parameters that are usually used as default are shown\nto be sub-optimal.", "published": "2020-07-08 07:29:35", "link": "http://arxiv.org/abs/2007.03931v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Improving Sound Event Detection In Domestic Environments Using Sound\n  Separation", "abstract": "Performing sound event detection on real-world recordings often implies\ndealing with overlapping target sound events and non-target sounds, also\nreferred to as interference or noise. Until now these problems were mainly\ntackled at the classifier level. We propose to use sound separation as a\npre-processing for sound event detection. In this paper we start from a sound\nseparation model trained on the Free Universal Sound Separation dataset and the\nDCASE 2020 task 4 sound event detection baseline. We explore different methods\nto combine separated sound sources and the original mixture within the sound\nevent detection. Furthermore, we investigate the impact of adapting the sound\nseparation model to the sound event detection data on both the sound separation\nand the sound event detection.", "published": "2020-07-08 07:31:30", "link": "http://arxiv.org/abs/2007.03932v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
