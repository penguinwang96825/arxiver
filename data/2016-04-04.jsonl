{"title": "Capturing Semantic Similarity for Entity Linking with Convolutional\n  Neural Networks", "abstract": "A key challenge in entity linking is making effective use of contextual\ninformation to disambiguate mentions that might refer to different entities in\ndifferent contexts. We present a model that uses convolutional neural networks\nto capture semantic correspondence between a mention's context and a proposed\ntarget entity. These convolutional networks operate at multiple granularities\nto exploit various kinds of topic information, and their rich parameterization\ngives them the capacity to learn which n-grams characterize different topics.\nWe combine these networks with a sparse linear model to achieve\nstate-of-the-art performance on multiple entity linking datasets, outperforming\nthe prior systems of Durrett and Klein (2014) and Nguyen et al. (2014).", "published": "2016-04-04 03:58:31", "link": "http://arxiv.org/abs/1604.00734v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Field Structural Decomposition for Question Answering", "abstract": "This paper presents a precursory yet novel approach to the question answering\ntask using structural decomposition. Our system first generates linguistic\nstructures such as syntactic and semantic trees from text, decomposes them into\nmultiple fields, then indexes the terms in each field. For each question, it\ndecomposes the question into multiple fields, measures the relevance score of\neach field to the indexed ones, then ranks all documents by their relevance\nscores and weights associated with the fields, where the weights are learned\nthrough statistical modeling. Our final model gives an absolute improvement of\nover 40% to the baseline approach using simple search for detecting documents\ncontaining answers.", "published": "2016-04-04 16:33:15", "link": "http://arxiv.org/abs/1604.00938v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Achieving Open Vocabulary Neural Machine Translation with Hybrid\n  Word-Character Models", "abstract": "Nearly all previous work on neural machine translation (NMT) has used quite\nrestricted vocabularies, perhaps with a subsequent method to patch in unknown\nwords. This paper presents a novel word-character solution to achieving open\nvocabulary NMT. We build hybrid systems that translate mostly at the word level\nand consult the character components for rare words. Our character-level\nrecurrent neural networks compute source word representations and recover\nunknown target words when needed. The twofold advantage of such a hybrid\napproach is that it is much faster and easier to train than character-based\nones; at the same time, it never produces unknown words as in the case of\nword-based models. On the WMT'15 English to Czech translation task, this hybrid\napproach offers an addition boost of +2.1-11.4 BLEU points over models that\nalready handle unknown words. Our best system achieves a new state-of-the-art\nresult with 20.7 BLEU score. We demonstrate that our character models can\nsuccessfully learn to not only generate well-formed words for Czech, a\nhighly-inflected language with a very complex vocabulary, but also build\ncorrect representations for English source words.", "published": "2016-04-04 09:30:54", "link": "http://arxiv.org/abs/1604.00788v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "In narrative texts punctuation marks obey the same statistics as words", "abstract": "From a grammar point of view, the role of punctuation marks in a sentence is\nformally defined and well understood. In semantic analysis punctuation plays\nalso a crucial role as a method of avoiding ambiguity of the meaning. A\ndifferent situation can be observed in the statistical analyses of language\nsamples, where the decision on whether the punctuation marks should be\nconsidered or should be neglected is seen rather as arbitrary and at present it\nbelongs to a researcher's preference. An objective of this work is to shed some\nlight onto this problem by providing us with an answer to the question whether\nthe punctuation marks may be treated as ordinary words and whether they should\nbe included in any analysis of the word co-occurences. We already know from our\nprevious study (S.~Dro\\.zd\\.z {\\it et al.}, Inf. Sci. 331 (2016) 32-44) that\nfull stops that determine the length of sentences are the main carrier of\nlong-range correlations. Now we extend that study and analyze statistical\nproperties of the most common punctuation marks in a few Indo-European\nlanguages, investigate their frequencies, and locate them accordingly in the\nZipf rank-frequency plots as well as study their role in the word-adjacency\nnetworks. We show that, from a statistical viewpoint, the punctuation marks\nreveal properties that are qualitatively similar to the properties of the most\nfrequent words like articles, conjunctions, pronouns, and prepositions. This\nrefers to both the Zipfian analysis and the network analysis. By adding the\npunctuation marks to the Zipf plots, we also show that these plots that are\nnormally described by the Zipf-Mandelbrot distribution largely restore the\npower-law Zipfian behaviour for the most frequent items.", "published": "2016-04-04 12:29:00", "link": "http://arxiv.org/abs/1604.00834v2", "categories": ["cs.CL", "physics.data-an"], "primary_category": "cs.CL"}
{"title": "Entity Type Recognition using an Ensemble of Distributional Semantic\n  Models to Enhance Query Understanding", "abstract": "We present an ensemble approach for categorizing search query entities in the\nrecruitment domain. Understanding the types of entities expressed in a search\nquery (Company, Skill, Job Title, etc.) enables more intelligent information\nretrieval based upon those entities compared to a traditional keyword-based\nsearch. Because search queries are typically very short, leveraging a\ntraditional bag-of-words model to identify entity types would be inappropriate\ndue to the lack of contextual information. Our approach instead combines clues\nfrom different sources of varying complexity in order to collect real-world\nknowledge about query entities. We employ distributional semantic\nrepresentations of query entities through two models: 1) contextual vectors\ngenerated from encyclopedic corpora like Wikipedia, and 2) high dimensional\nword embedding vectors generated from millions of job postings using word2vec.\nAdditionally, our approach utilizes both entity linguistic properties obtained\nfrom WordNet and ontological properties extracted from DBpedia. We evaluate our\napproach on a data set created at CareerBuilder; the largest job board in the\nUS. The data set contains entities extracted from millions of job\nseekers/recruiters search queries, job postings, and resume documents. After\nconstructing the distributional vectors of search entities, we use supervised\nmachine learning to infer search entity types. Empirical results show that our\napproach outperforms the state-of-the-art word2vec distributional semantics\nmodel trained on Wikipedia. Moreover, we achieve micro-averaged F 1 score of\n97% using the proposed distributional representations ensemble.", "published": "2016-04-04 16:18:44", "link": "http://arxiv.org/abs/1604.00933v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Character-Level Question Answering with Attention", "abstract": "We show that a character-level encoder-decoder framework can be successfully\napplied to question answering with a structured knowledge base. We use our\nmodel for single-relation question answering and demonstrate the effectiveness\nof our approach on the SimpleQuestions dataset (Bordes et al., 2015), where we\nimprove state-of-the-art accuracy from 63.9% to 70.9%, without use of\nensembles. Importantly, our character-level model has 16x fewer parameters than\nan equivalent word-level model, can be learned with significantly less data\ncompared to previous work, which relies on data augmentation, and is robust to\nnew entities in testing.", "published": "2016-04-04 02:43:23", "link": "http://arxiv.org/abs/1604.00727v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Image Captioning with Deep Bidirectional LSTMs", "abstract": "This work presents an end-to-end trainable deep bidirectional LSTM\n(Long-Short Term Memory) model for image captioning. Our model builds on a deep\nconvolutional neural network (CNN) and two separate LSTM networks. It is\ncapable of learning long term visual-language interactions by making use of\nhistory and future context information at high level semantic space. Two novel\ndeep bidirectional variant models, in which we increase the depth of\nnonlinearity transition in different way, are proposed to learn hierarchical\nvisual-language embeddings. Data augmentation techniques such as multi-crop,\nmulti-scale and vertical mirror are proposed to prevent overfitting in training\ndeep models. We visualize the evolution of bidirectional LSTM internal states\nover time and qualitatively analyze how our models \"translate\" image to\nsentence. Our proposed models are evaluated on caption generation and\nimage-sentence retrieval tasks with three benchmark datasets: Flickr8K,\nFlickr30K and MSCOCO datasets. We demonstrate that bidirectional LSTM models\nachieve highly competitive performance to the state-of-the-art results on\ncaption generation even without integrating additional mechanism (e.g. object\ndetection, attention model etc.) and significantly outperform recent methods on\nretrieval task.", "published": "2016-04-04 09:43:04", "link": "http://arxiv.org/abs/1604.00790v3", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
