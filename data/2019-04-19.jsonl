{"title": "A Novel Task-Oriented Text Corpus in Silent Speech Recognition and its\n  Natural Language Generation Construction Method", "abstract": "Millions of people with severe speech disorders around the world may regain\ntheir communication capabilities through techniques of silent speech\nrecognition (SSR). Using electroencephalography (EEG) as a biomarker for speech\ndecoding has been popular for SSR. However, the lack of SSR text corpus has\nimpeded the development of this technique. Here, we construct a novel\ntask-oriented text corpus, which is utilized in the field of SSR. In the\nprocess of construction, we propose a task-oriented hybrid construction method\nbased on natural language generation algorithm. The algorithm focuses on the\nstrategy of data-to-text generation, and has two advantages including\nlinguistic quality and high diversity. These two advantages use template-based\nmethod and deep neural networks respectively. In an SSR experiment with the\ngenerated text corpus, analysis results show that the performance of our hybrid\nconstruction method outperforms the pure method such as template-based natural\nlanguage generation or neural natural language generation models.", "published": "2019-04-19 08:21:01", "link": "http://arxiv.org/abs/1905.01974v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Query-focused Sentence Compression in Linear Time", "abstract": "Search applications often display shortened sentences which must contain\ncertain query terms and must fit within the space constraints of a user\ninterface. This work introduces a new transition-based sentence compression\ntechnique developed for such settings. Our query-focused method constructs\nlength and lexically constrained compressions in linear time, by growing a\nsubgraph in the dependency parse of a sentence. This theoretically efficient\napproach achieves an 11X empirical speedup over baseline ILP methods, while\nbetter reconstructing gold constrained shortenings. Such speedups help\nquery-focused applications, because users are measurably hindered by interface\nlags. Additionally, our technique does not require an ILP solver or a GPU.", "published": "2019-04-19 02:19:43", "link": "http://arxiv.org/abs/1904.09051v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Offensive Posts and Targeted Offense from Twitter", "abstract": "In this paper we present our approach and the system description for Sub-task\nA and Sub Task B of SemEval 2019 Task 6: Identifying and Categorizing Offensive\nLanguage in Social Media. Sub-task A involves identifying if a given tweet is\noffensive or not, and Sub Task B involves detecting if an offensive tweet is\ntargeted towards someone (group or an individual). Our models for Sub-task A is\nbased on an ensemble of Convolutional Neural Network, Bidirectional LSTM with\nattention, and Bidirectional LSTM + Bidirectional GRU, whereas for Sub-task B,\nwe rely on a set of heuristics derived from the training data and manual\nobservation. We provide detailed analysis of the results obtained using the\ntrained models. Our team ranked 5th out of 103 participants in Sub-task A,\nachieving a macro F1 score of 0.807, and ranked 8th out of 75 participants in\nSub Task B achieving a macro F1 of 0.695.", "published": "2019-04-19 04:26:36", "link": "http://arxiv.org/abs/1904.09072v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Suggestion Mining from Online Reviews using ULMFiT", "abstract": "In this paper we present our approach and the system description for Sub Task\nA of SemEval 2019 Task 9: Suggestion Mining from Online Reviews and Forums.\nGiven a sentence, the task asks to predict whether the sentence consists of a\nsuggestion or not. Our model is based on Universal Language Model Fine-tuning\nfor Text Classification. We apply various pre-processing techniques before\ntraining the language and the classification model. We further provide detailed\nanalysis of the results obtained using the trained model. Our team ranked 10th\nout of 34 participants, achieving an F1 score of 0.7011. We publicly share our\nimplementation at https://github.com/isarth/SemEval9_MIDAS", "published": "2019-04-19 04:38:32", "link": "http://arxiv.org/abs/1904.09076v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT", "abstract": "Pretrained contextual representation models (Peters et al., 2018; Devlin et\nal., 2018) have pushed forward the state-of-the-art on many NLP tasks. A new\nrelease of BERT (Devlin, 2018) includes a model simultaneously pretrained on\n104 languages with impressive performance for zero-shot cross-lingual transfer\non a natural language inference task. This paper explores the broader\ncross-lingual potential of mBERT (multilingual) as a zero shot language\ntransfer model on 5 NLP tasks covering a total of 39 languages from various\nlanguage families: NLI, document classification, NER, POS tagging, and\ndependency parsing. We compare mBERT with the best-published methods for\nzero-shot cross-lingual transfer and find mBERT competitive on each task.\nAdditionally, we investigate the most effective strategy for utilizing mBERT in\nthis manner, determine to what extent mBERT generalizes away from language\nspecific features, and measure factors that influence cross-lingual transfer.", "published": "2019-04-19 04:45:44", "link": "http://arxiv.org/abs/1904.09077v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Programmatic Idioms for Scalable Semantic Parsing", "abstract": "Programmers typically organize executable source code using high-level coding\npatterns or idiomatic structures such as nested loops, exception handlers and\nrecursive blocks, rather than as individual code tokens. In contrast, state of\nthe art (SOTA) semantic parsers still map natural language instructions to\nsource code by building the code syntax tree one node at a time. In this paper,\nwe introduce an iterative method to extract code idioms from large source code\ncorpora by repeatedly collapsing most-frequent depth-2 subtrees of their syntax\ntrees, and train semantic parsers to apply these idioms during decoding.\nApplying idiom-based decoding on a recent context-dependent semantic parsing\ntask improves the SOTA by 2.2\\% BLEU score while reducing training time by more\nthan 50\\%. This improved speed enables us to scale up the model by training on\nan extended training set that is 5$\\times$ larger, to further move up the SOTA\nby an additional 2.3\\% BLEU and 0.9\\% exact match. Finally, idioms also\nsignificantly improve accuracy of semantic parsing to SQL on the ATIS-SQL\ndataset, when training data is limited.", "published": "2019-04-19 05:56:45", "link": "http://arxiv.org/abs/1904.09086v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Code-Switching for Enhancing NMT with Pre-Specified Translation", "abstract": "Leveraging user-provided translation to constrain NMT has practical\nsignificance. Existing methods can be classified into two main categories,\nnamely the use of placeholder tags for lexicon words and the use of hard\nconstraints during decoding. Both methods can hurt translation fidelity for\nvarious reasons. We investigate a data augmentation method, making\ncode-switched training data by replacing source phrases with their target\ntranslations. Our method does not change the MNT model or decoding algorithm,\nallowing the model to learn lexicon translations by copying source-side target\nwords. Extensive experiments show that our method achieves consistent\nimprovements over existing approaches, improving translation of constrained\nwords without hurting unconstrained words.", "published": "2019-04-19 07:53:06", "link": "http://arxiv.org/abs/1904.09107v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recognizing the vocabulary of Brazilian popular newspapers with a\n  free-access computational dictionary", "abstract": "We report an experiment to check the identification of a set of words in\npopular written Portuguese with two versions of a computational dictionary of\nBrazilian Portuguese, DELAF PB 2004 and DELAF PB 2015. This dictionary is\nfreely available for use in linguistic analyses of Brazilian Portuguese and\nother researches, which justifies critical study. The vocabulary comes from the\nPorPopular corpus, made of popular newspapers Di{\\'a}rio Ga{\\'u}cho (DG) and\nMassa! (MA). From DG, we retained a set of texts with 984.465 words (tokens),\npublished in 2008, with the spelling used before the Portuguese Language\nOrthographic Agreement adopted in 2009. From MA, we examined papers of 2012,\n2014 e 2015, with 215.776 words (tokens), all with the new spelling. The\nchecking involved: a) generating lists of words (types) occurring in DG and MA;\nb) comparing them with the entry lists of both versions of DELAF PB; c)\nassessing the coverage of this vocabulary; d) proposing ways of incorporating\nthe items not covered. The results of the work show that an average of 19% of\nthe types in DG were not found in DELAF PB 2004 or 2015. In MA, this average is\n13%. Switching versions of the dictionary affected slightly the performance in\nrecognizing the words.", "published": "2019-04-19 07:59:25", "link": "http://arxiv.org/abs/1904.09108v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ERNIE: Enhanced Representation through Knowledge Integration", "abstract": "We present a novel language representation model enhanced by knowledge called\nERNIE (Enhanced Representation through kNowledge IntEgration). Inspired by the\nmasking strategy of BERT, ERNIE is designed to learn language representation\nenhanced by knowledge masking strategies, which includes entity-level masking\nand phrase-level masking. Entity-level strategy masks entities which are\nusually composed of multiple words.Phrase-level strategy masks the whole phrase\nwhich is composed of several words standing together as a conceptual\nunit.Experimental results show that ERNIE outperforms other baseline methods,\nachieving new state-of-the-art results on five Chinese natural language\nprocessing tasks including natural language inference, semantic similarity,\nnamed entity recognition, sentiment analysis and question answering. We also\ndemonstrate that ERNIE has more powerful knowledge inference capacity on a\ncloze test.", "published": "2019-04-19 15:10:56", "link": "http://arxiv.org/abs/1904.09223v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unifying Question Answering, Text Classification, and Regression via\n  Span Extraction", "abstract": "Even as pre-trained language encoders such as BERT are shared across many\ntasks, the output layers of question answering, text classification, and\nregression models are significantly different. Span decoders are frequently\nused for question answering, fixed-class, classification layers for text\nclassification, and similarity-scoring layers for regression tasks, We show\nthat this distinction is not necessary and that all three can be unified as\nspan extraction. A unified, span-extraction approach leads to superior or\ncomparable performance in supplementary supervised pre-trained, low-data, and\nmulti-task learning experiments on several question answering, text\nclassification, and regression benchmarks.", "published": "2019-04-19 17:58:29", "link": "http://arxiv.org/abs/1904.09286v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Evaluation of Transfer Learning for Classifying Sales Engagement\n  Emails at Large Scale", "abstract": "This paper conducts an empirical investigation to evaluate transfer learning\nfor classifying sales engagement emails arising from digital sales engagement\nplatforms. Given the complexity of content and context of sales engagement,\nlack of standardized large corpora and benchmarks, limited labeled examples and\nheterogenous context of intent, this real-world use case poses both a challenge\nand an opportunity for adopting a transfer learning approach. We propose an\nevaluation framework to assess a high performance transfer learning (HPTL)\napproach in three key areas in addition to commonly used accuracy metrics: 1)\neffective embeddings and pretrained language model usage, 2) minimum labeled\nsamples requirement and 3) transfer learning implementation strategies. We use\nin-house sales engagement email samples as the experiment dataset, which\nincludes over 3000 emails labeled as positive, objection, unsubscribe, or\nnot-sure. We discuss our findings on evaluating BERT, ELMo, Flair and GloVe\nembeddings with both feature-based and fine-tuning approaches and their\nscalability on a GPU cluster with increasingly larger labeled samples. Our\nresults show that fine-tuning of the BERT model outperforms with as few as 300\nlabeled samples, but underperforms with fewer than 300 labeled samples,\nrelative to all the feature-based approaches using different embeddings.", "published": "2019-04-19 18:11:54", "link": "http://arxiv.org/abs/1905.01971v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Hybrid Retrieval-Generation Neural Conversation Model", "abstract": "Intelligent personal assistant systems that are able to have multi-turn\nconversations with human users are becoming increasingly popular. Most previous\nresearch has been focused on using either retrieval-based or generation-based\nmethods to develop such systems. Retrieval-based methods have the advantage of\nreturning fluent and informative responses with great diversity. However, the\nperformance of the methods is limited by the size of the response repository.\nOn the other hand, generation-based methods can produce highly coherent\nresponses on any topics. But the generated responses are often generic and not\ninformative due to the lack of grounding knowledge. In this paper, we propose a\nhybrid neural conversation model that combines the merits of both response\nretrieval and generation methods. Experimental results on Twitter and\nFoursquare data show that the proposed model outperforms both retrieval-based\nmethods and generation-based methods (including a recently proposed\nknowledge-grounded neural conversation model) under both automatic evaluation\nmetrics and human evaluation. We hope that the findings in this study provide\nnew insights on how to integrate text retrieval and text generation models for\nbuilding conversation systems.", "published": "2019-04-19 04:10:03", "link": "http://arxiv.org/abs/1904.09068v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Zero-Shot Cross-Lingual Opinion Target Extraction", "abstract": "Aspect-based sentiment analysis involves the recognition of so called opinion\ntarget expressions (OTEs). To automatically extract OTEs, supervised learning\nalgorithms are usually employed which are trained on manually annotated\ncorpora. The creation of these corpora is labor-intensive and sufficiently\nlarge datasets are therefore usually only available for a very narrow selection\nof languages and domains. In this work, we address the lack of available\nannotated data for specific languages by proposing a zero-shot cross-lingual\napproach for the extraction of opinion target expressions. We leverage\nmultilingual word embeddings that share a common vector space across various\nlanguages and incorporate these into a convolutional neural network\narchitecture for OTE extraction. Our experiments with 5 languages give\npromising results: We can successfully train a model on annotated data of a\nsource language and perform accurate prediction on a target language without\never using any annotated samples in that target language. Depending on the\nsource and target language pairs, we reach performances in a zero-shot regime\nof up to 77% of a model trained on target language data. Furthermore, we can\nincrease this performance up to 87% of a baseline model trained on target\nlanguage data by performing cross-lingual learning from multiple source\nlanguages.", "published": "2019-04-19 08:59:13", "link": "http://arxiv.org/abs/1904.09122v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OpenTapioca: Lightweight Entity Linking for Wikidata", "abstract": "We propose a simple Named Entity Linking system that can be trained from\nWikidata only. This demonstrates the strengths and weaknesses of this data\nsource for this task and provides an easily reproducible baseline to compare\nother systems against. Our model is lightweight to train, to run and to keep\nsynchronous with Wikidata in real time.", "published": "2019-04-19 09:44:22", "link": "http://arxiv.org/abs/1904.09131v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Looking Beyond Label Noise: Shifted Label Distribution Matters in\n  Distantly Supervised Relation Extraction", "abstract": "In recent years there is a surge of interest in applying distant supervision\n(DS) to automatically generate training data for relation extraction (RE). In\nthis paper, we study the problem what limits the performance of DS-trained\nneural models, conduct thorough analyses, and identify a factor that can\ninfluence the performance greatly, shifted label distribution. Specifically, we\nfound this problem commonly exists in real-world DS datasets, and without\nspecial handing, typical DS-RE models cannot automatically adapt to this shift,\nthus achieving deteriorated performance. To further validate our intuition, we\ndevelop a simple yet effective adaptation method for DS-trained models, bias\nadjustment, which updates models learned over the source domain (i.e., DS\ntraining set) with a label distribution estimated on the target domain (i.e.,\ntest set). Experiments demonstrate that bias adjustment achieves consistent\nperformance gains on DS-trained models, especially on neural models, with an up\nto 23% relative F1 improvement, which verifies our assumptions. Our code and\ndata can be found at\n\\url{https://github.com/INK-USC/shifted-label-distribution}.", "published": "2019-04-19 20:23:27", "link": "http://arxiv.org/abs/1904.09331v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Relation Discovery with Out-of-Relation Knowledge Base as Supervision", "abstract": "Unsupervised relation discovery aims to discover new relations from a given\ntext corpus without annotated data. However, it does not consider existing\nhuman annotated knowledge bases even when they are relevant to the relations to\nbe discovered. In this paper, we study the problem of how to use\nout-of-relation knowledge bases to supervise the discovery of unseen relations,\nwhere out-of-relation means that relations to discover from the text corpus and\nthose in knowledge bases are not overlapped. We construct a set of constraints\nbetween entity pairs based on the knowledge base embedding and then incorporate\nconstraints into the relation discovery by a variational auto-encoder based\nalgorithm. Experiments show that our new approach can improve the\nstate-of-the-art relation discovery performance by a large margin.", "published": "2019-04-19 02:30:59", "link": "http://arxiv.org/abs/1905.01959v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Who wrote this book? A challenge for e-commerce", "abstract": "Modern e-commerce catalogs contain millions of references, associated with\ntextual and visual information that is of paramount importance for the products\nto be found via search or browsing. Of particular significance is the book\ncategory, where the author name(s) field poses a significant challenge. Indeed,\nbooks written by a given author (such as F. Scott Fitzgerald) might be listed\nwith different authors' names in a catalog due to abbreviations and spelling\nvariants and mistakes, among others. To solve this problem at scale, we design\na composite system involving open data sources for books as well as machine\nlearning components leveraging deep learning-based techniques for natural\nlanguage processing. In particular, we use Siamese neural networks for an\napproximate match with known author names, and direct correction of the\nprovided author's name using sequence-to-sequence learning with neural\nnetworks. We evaluate this approach on product data from the e-commerce website\nRakuten France, and find that the top proposal of the system is the normalized\nauthor name with 72% accuracy.", "published": "2019-04-19 10:13:07", "link": "http://arxiv.org/abs/1905.01973v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "An Investigation of End-to-End Multichannel Speech Recognition for\n  Reverberant and Mismatch Conditions", "abstract": "Sequence-to-sequence (S2S) modeling is becoming a popular paradigm for\nautomatic speech recognition (ASR) because of its ability to jointly optimize\nall the conventional ASR components in an end-to-end (E2E) fashion. This report\ninvestigates the ability of E2E ASR from standard close-talk to far-field\napplications by encompassing entire multichannel speech enhancement and ASR\ncomponents within the S2S model. There have been previous studies on jointly\noptimizing neural beamforming alongside E2E ASR for denoising. It is clear from\nboth recent challenge outcomes and successful products that far-field systems\nwould be incomplete without solving both denoising and dereverberation\nsimultaneously. This report uses a recently developed architecture for\nfar-field ASR by composing neural extensions of dereverberation and beamforming\nmodules with the S2S ASR module as a single differentiable neural network and\nalso clearly defining the role of each subnetwork. The original implementation\nof this architecture was successfully applied to the noisy speech recognition\ntask (CHiME-4), while we applied this implementation to noisy reverberant tasks\n(DIRHA and REVERB). Our investigation shows that the method achieves better\nperformance than conventional pipeline methods on the DIRHA English dataset and\ncomparable performance on the REVERB dataset. It also has additional advantages\nof being neither iterative nor requiring parallel noisy and clean speech data.", "published": "2019-04-19 01:36:37", "link": "http://arxiv.org/abs/1904.09049v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Emergence of Compositional Language with Deep Generational Transmission", "abstract": "Recent work has studied the emergence of language among deep reinforcement\nlearning agents that must collaborate to solve a task. Of particular interest\nare the factors that cause language to be compositional -- i.e., express\nmeaning by combining words which themselves have meaning. Evolutionary\nlinguists have found that in addition to structural priors like those already\nstudied in deep learning, the dynamics of transmitting language from generation\nto generation contribute significantly to the emergence of compositionality. In\nthis paper, we introduce these cultural evolutionary dynamics into language\nemergence by periodically replacing agents in a population to create a\nknowledge gap, implicitly inducing cultural transmission of language. We show\nthat this implicit cultural transmission encourages the resulting languages to\nexhibit better compositional generalization.", "published": "2019-04-19 04:09:12", "link": "http://arxiv.org/abs/1904.09067v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Mask-Predict: Parallel Decoding of Conditional Masked Language Models", "abstract": "Most machine translation systems generate text autoregressively from left to\nright. We, instead, use a masked language modeling objective to train a model\nto predict any subset of the target words, conditioned on both the input text\nand a partially masked target translation. This approach allows for efficient\niterative decoding, where we first predict all of the target words\nnon-autoregressively, and then repeatedly mask out and regenerate the subset of\nwords that the model is least confident about. By applying this strategy for a\nconstant number of iterations, our model improves state-of-the-art performance\nlevels for non-autoregressive and parallel decoding translation models by over\n4 BLEU on average. It is also able to reach within about 1 BLEU point of a\ntypical left-to-right transformer model, while decoding significantly faster.", "published": "2019-04-19 19:53:01", "link": "http://arxiv.org/abs/1904.09324v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Challenges and Prospects in Vision and Language Research", "abstract": "Language grounded image understanding tasks have often been proposed as a\nmethod for evaluating progress in artificial intelligence. Ideally, these tasks\nshould test a plethora of capabilities that integrate computer vision,\nreasoning, and natural language understanding. However, rather than behaving as\nvisual Turing tests, recent studies have demonstrated state-of-the-art systems\nare achieving good performance through flaws in datasets and evaluation\nprocedures. We review the current state of affairs and outline a path forward.", "published": "2019-04-19 19:04:12", "link": "http://arxiv.org/abs/1904.09317v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.NE", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Listen to the Image", "abstract": "Visual-to-auditory sensory substitution devices can assist the blind in\nsensing the visual environment by translating the visual information into a\nsound pattern. To improve the translation quality, the task performances of the\nblind are usually employed to evaluate different encoding schemes. In contrast\nto the toilsome human-based assessment, we argue that machine model can be also\ndeveloped for evaluation, and more efficient. To this end, we firstly propose\ntwo distinct cross-modal perception model w.r.t. the late-blind and\ncongenitally-blind cases, which aim to generate concrete visual contents based\non the translated sound. To validate the functionality of proposed models, two\nnovel optimization strategies w.r.t. the primary encoding scheme are presented.\nFurther, we conduct sets of human-based experiments to evaluate and compare\nthem with the conducted machine-based assessments in the cross-modal generation\ntask. Their highly consistent results w.r.t. different encoding schemes\nindicate that using machine model to accelerate optimization evaluation and\nreduce experimental cost is feasible to some extent, which could dramatically\npromote the upgrading of encoding scheme then help the blind to improve their\nvisual perception ability.", "published": "2019-04-19 08:13:34", "link": "http://arxiv.org/abs/1904.09115v1", "categories": ["cs.CV", "cs.HC", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
