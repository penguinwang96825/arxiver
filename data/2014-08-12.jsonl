{"title": "Internal and external dynamics in language: Evidence from verb\n  regularity in a historical corpus of English", "abstract": "Human languages are rule governed, but almost invariably these rules have\nexceptions in the form of irregularities. Since rules in language are efficient\nand productive, the persistence of irregularity is an anomaly. How does\nirregularity linger in the face of internal (endogenous) and external\n(exogenous) pressures to conform to a rule? Here we address this problem by\ntaking a detailed look at simple past tense verbs in the Corpus of Historical\nAmerican English. The data show that the language is open, with many new verbs\nentering. At the same time, existing verbs might tend to regularize or\nirregularize as a consequence of internal dynamics, but overall, the amount of\nirregularity sustained by the language stays roughly constant over time.\nDespite continuous vocabulary growth, and presumably, an attendant increase in\nexpressive power, there is no corresponding growth in irregularity. We analyze\nthe set of irregulars, showing they may adhere to a set of minority rules,\nallowing for increased stability of irregularity over time. These findings\ncontribute to the debate on how language systems become rule governed, and how\nand why they sustain exceptions to rules, providing insight into the interplay\nbetween the emergence and maintenance of rules and exceptions in language.", "published": "2014-08-12 12:07:27", "link": "http://arxiv.org/abs/1408.2699v1", "categories": ["physics.soc-ph", "cs.CL"], "primary_category": "physics.soc-ph"}
{"title": "First-Pass Large Vocabulary Continuous Speech Recognition using\n  Bi-Directional Recurrent DNNs", "abstract": "We present a method to perform first-pass large vocabulary continuous speech\nrecognition using only a neural network and language model. Deep neural network\nacoustic models are now commonplace in HMM-based speech recognition systems,\nbut building such systems is a complex, domain-specific task. Recent work\ndemonstrated the feasibility of discarding the HMM sequence modeling framework\nby directly predicting transcript text from audio. This paper extends this\napproach in two ways. First, we demonstrate that a straightforward recurrent\nneural network architecture can achieve a high level of accuracy. Second, we\npropose and evaluate a modified prefix-search decoding algorithm. This approach\nto decoding enables first-pass speech recognition with a language model,\ncompletely unaided by the cumbersome infrastructure of HMM-based systems.\nExperiments on the Wall Street Journal corpus demonstrate fairly competitive\nword error rates, and the importance of bi-directional network recurrence.", "published": "2014-08-12 22:40:21", "link": "http://arxiv.org/abs/1408.2873v2", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
