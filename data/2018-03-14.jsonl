{"title": "MCScript: A Novel Dataset for Assessing Machine Comprehension Using\n  Script Knowledge", "abstract": "We introduce a large dataset of narrative texts and questions about these\ntexts, intended to be used in a machine comprehension task that requires\nreasoning using commonsense knowledge. Our dataset complements similar datasets\nin that we focus on stories about everyday activities, such as going to the\nmovies or working in the garden, and that the questions require commonsense\nknowledge, or more specifically, script knowledge, to be answered. We show that\nour mode of data collection via crowdsourcing results in a substantial amount\nof such inference questions. The dataset forms the basis of a shared task on\ncommonsense and script knowledge organized at SemEval 2018 and provides\nchallenging test cases for the broader natural language understanding\ncommunity.", "published": "2018-03-14 11:59:13", "link": "http://arxiv.org/abs/1803.05223v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FEVER: a large-scale dataset for Fact Extraction and VERification", "abstract": "In this paper we introduce a new publicly available dataset for verification\nagainst textual sources, FEVER: Fact Extraction and VERification. It consists\nof 185,445 claims generated by altering sentences extracted from Wikipedia and\nsubsequently verified without knowledge of the sentence they were derived from.\nThe claims are classified as Supported, Refuted or NotEnoughInfo by annotators\nachieving 0.6841 in Fleiss $\\kappa$. For the first two classes, the annotators\nalso recorded the sentence(s) forming the necessary evidence for their\njudgment. To characterize the challenge of the dataset presented, we develop a\npipeline approach and compare it to suitably designed oracles. The best\naccuracy we achieve on labeling a claim accompanied by the correct evidence is\n31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe that\nFEVER is a challenging testbed that will help stimulate progress on claim\nverification against textual sources.", "published": "2018-03-14 15:30:37", "link": "http://arxiv.org/abs/1803.05355v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SentEval: An Evaluation Toolkit for Universal Sentence Representations", "abstract": "We introduce SentEval, a toolkit for evaluating the quality of universal\nsentence representations. SentEval encompasses a variety of tasks, including\nbinary and multi-class classification, natural language inference and sentence\nsimilarity. The set of tasks was selected based on what appears to be the\ncommunity consensus regarding the appropriate evaluations for universal\nsentence representations. The toolkit comes with scripts to download and\npreprocess datasets, and an easy interface to evaluate sentence encoders. The\naim is to provide a fairer, less cumbersome and more centralized way for\nevaluating sentence representations.", "published": "2018-03-14 18:01:15", "link": "http://arxiv.org/abs/1803.05449v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Challenges in Discriminating Profanity from Hate Speech", "abstract": "In this study we approach the problem of distinguishing general profanity\nfrom hate speech in social media, something which has not been widely\nconsidered. Using a new dataset annotated specifically for this task, we employ\nsupervised classification along with a set of features that includes n-grams,\nskip-grams and clustering-based word representations. We apply approaches based\non single classifiers as well as more advanced ensemble classifiers and stacked\ngeneralization, achieving the best result of 80% accuracy for this 3-class\nclassification task. Analysis of the results reveals that discriminating hate\nspeech and profanity is not a simple task, which may require features that\ncapture a deeper understanding of the text not always possible with surface\nn-grams. The variability of gold labels in the annotated data, due to\ndifferences in the subjective adjudications of the annotators, is also an\nissue. Other directions for future work are discussed.", "published": "2018-03-14 20:00:08", "link": "http://arxiv.org/abs/1803.05495v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ISIS at its apogee: the Arabic discourse on Twitter and what we can\n  learn from that about ISIS support and Foreign Fighters", "abstract": "We analyze 26.2 million comments published in Arabic language on Twitter,\nfrom July 2014 to January 2015, when ISIS' strength reached its peak and the\ngroup was prominently expanding the territorial area under its control. By\ndoing that, we are able to measure the share of support and aversion toward the\nIslamic State within the online Arab communities. We then investigate two\nspecific topics. First, by exploiting the time-granularity of the tweets, we\nlink the opinions with daily events to understand the main determinants of the\nchanging trend in support toward ISIS. Second, by taking advantage of the\ngeographical locations of tweets, we explore the relationship between online\nopinions across countries and the number of foreign fighters joining ISIS.", "published": "2018-03-14 05:29:30", "link": "http://arxiv.org/abs/1804.04059v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "How to evaluate sentiment classifiers for Twitter time-ordered data?", "abstract": "Social media are becoming an increasingly important source of information\nabout the public mood regarding issues such as elections, Brexit, stock market,\netc. In this paper we focus on sentiment classification of Twitter data.\nConstruction of sentiment classifiers is a standard text mining task, but here\nwe address the question of how to properly evaluate them as there is no settled\nway to do so. Sentiment classes are ordered and unbalanced, and Twitter\nproduces a stream of time-ordered data. The problem we address concerns the\nprocedures used to obtain reliable estimates of performance measures, and\nwhether the temporal ordering of the training and test data matters. We\ncollected a large set of 1.5 million tweets in 13 European languages. We\ncreated 138 sentiment models and out-of-sample datasets, which are used as a\ngold standard for evaluations. The corresponding 138 in-sample datasets are\nused to empirically compare six different estimation procedures: three variants\nof cross-validation, and three variants of sequential validation (where test\nset always follows the training set). We find no significant difference between\nthe best cross-validation and sequential validation. However, we observe that\nall cross-validation variants tend to overestimate the performance, while the\nsequential methods tend to underestimate it. Standard cross-validation with\nrandom selection of examples is significantly worse than the blocked\ncross-validation, and should not be used to evaluate classifiers in\ntime-ordered data scenarios.", "published": "2018-03-14 08:16:48", "link": "http://arxiv.org/abs/1803.05160v1", "categories": ["cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning\n  Challenge", "abstract": "We present a new question set, text corpus, and baselines assembled to\nencourage AI research in advanced question answering. Together, these\nconstitute the AI2 Reasoning Challenge (ARC), which requires far more powerful\nknowledge and reasoning than previous challenges such as SQuAD or SNLI. The ARC\nquestion set is partitioned into a Challenge Set and an Easy Set, where the\nChallenge Set contains only questions answered incorrectly by both a\nretrieval-based algorithm and a word co-occurence algorithm. The dataset\ncontains only natural, grade-school science questions (authored for human\ntests), and is the largest public-domain set of this kind (7,787 questions). We\ntest several baselines on the Challenge Set, including leading neural models\nfrom the SQuAD and SNLI tasks, and find that none are able to significantly\noutperform a random baseline, reflecting the difficult nature of this task. We\nare also releasing the ARC Corpus, a corpus of 14M science sentences relevant\nto the task, and implementations of the three neural baseline models tested.\nCan your model perform better? We pose ARC as a challenge to the community.", "published": "2018-03-14 18:04:21", "link": "http://arxiv.org/abs/1803.05457v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Speaker Verification using Convolutional Neural Networks", "abstract": "In this paper, a novel Convolutional Neural Network architecture has been\ndeveloped for speaker verification in order to simultaneously capture and\ndiscard speaker and non-speaker information, respectively. In training phase,\nthe network is trained to distinguish between different speaker identities for\ncreating the background model. One of the crucial parts is to create the\nspeaker models. Most of the previous approaches create speaker models based on\naveraging the speaker representations provided by the background model. We\noverturn this problem by further fine-tuning the trained model using the\nSiamese framework for generating a discriminative feature space to distinguish\nbetween same and different speakers regardless of their identity. This provides\na mechanism which simultaneously captures the speaker-related information and\ncreate robustness to within-speaker variations. It is demonstrated that the\nproposed method outperforms the traditional verification methods which create\nspeaker models directly from the background model.", "published": "2018-03-14 20:12:32", "link": "http://arxiv.org/abs/1803.05427v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
