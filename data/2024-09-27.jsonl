{"title": "Volatility Forecasting in Global Financial Markets Using TimeMixer", "abstract": "Predicting volatility in financial markets, including stocks, index ETFs,\nforeign exchange, and cryptocurrencies, remains a challenging task due to the\ninherent complexity and non-linear dynamics of these time series. In this\nstudy, I apply TimeMixer, a state-of-the-art time series forecasting model, to\npredict the volatility of global financial assets. TimeMixer utilizes a\nmultiscale-mixing approach that effectively captures both short-term and\nlong-term temporal patterns by analyzing data across different scales. My\nempirical results reveal that while TimeMixer performs exceptionally well in\nshort-term volatility forecasting, its accuracy diminishes for longer-term\npredictions, particularly in highly volatile markets. These findings highlight\nTimeMixer's strength in capturing short-term volatility, making it highly\nsuitable for practical applications in financial risk management, where precise\nshort-term forecasts are critical. However, the model's limitations in\nlong-term forecasting point to potential areas for further refinement.", "published": "2024-09-27 17:35:28", "link": "http://arxiv.org/abs/2410.09062v1", "categories": ["q-fin.ST", "cs.AI", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "Exploring Language Model Generalization in Low-Resource Extractive QA", "abstract": "In this paper, we investigate Extractive Question Answering (EQA) with Large\nLanguage Models (LLMs) under domain drift, i.e., can LLMs generalize to domains\nthat require specific knowledge such as medicine and law in a zero-shot fashion\nwithout additional in-domain training? To this end, we devise a series of\nexperiments to explain the performance gap empirically. Our findings suggest\nthat: (a) LLMs struggle with dataset demands of closed domains such as\nretrieving long answer spans; (b) Certain LLMs, despite showing strong overall\nperformance, display weaknesses in meeting basic requirements as discriminating\nbetween domain-specific senses of words which we link to pre-processing\ndecisions; (c) Scaling model parameters is not always effective for cross\ndomain generalization; and (d) Closed-domain datasets are quantitatively much\ndifferent than open-domain EQA datasets and current LLMs struggle to deal with\nthem. Our findings point out important directions for improving existing LLMs.", "published": "2024-09-27 05:06:43", "link": "http://arxiv.org/abs/2409.18446v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of OpenAI o1: Opportunities and Challenges of AGI", "abstract": "This comprehensive study evaluates the performance of OpenAI's o1-preview\nlarge language model across a diverse array of complex reasoning tasks,\nspanning multiple domains, including computer science, mathematics, natural\nsciences, medicine, linguistics, and social sciences. Through rigorous testing,\no1-preview demonstrated remarkable capabilities, often achieving human-level or\nsuperior performance in areas ranging from coding challenges to scientific\nreasoning and from language processing to creative problem-solving. Key\nfindings include:\n  -83.3% success rate in solving complex competitive programming problems,\nsurpassing many human experts.\n  -Superior ability in generating coherent and accurate radiology reports,\noutperforming other evaluated models.\n  -100% accuracy in high school-level mathematical reasoning tasks, providing\ndetailed step-by-step solutions.\n  -Advanced natural language inference capabilities across general and\nspecialized domains like medicine.\n  -Impressive performance in chip design tasks, outperforming specialized\nmodels in areas such as EDA script generation and bug analysis.\n  -Remarkable proficiency in anthropology and geology, demonstrating deep\nunderstanding and reasoning in these specialized fields.\n  -Strong capabilities in quantitative investing. O1 has comprehensive\nfinancial knowledge and statistical modeling skills.\n  -Effective performance in social media analysis, including sentiment analysis\nand emotion recognition.\n  The model excelled particularly in tasks requiring intricate reasoning and\nknowledge integration across various fields. While some limitations were\nobserved, including occasional errors on simpler problems and challenges with\ncertain highly specialized concepts, the overall results indicate significant\nprogress towards artificial general intelligence.", "published": "2024-09-27 06:57:00", "link": "http://arxiv.org/abs/2409.18486v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Complex Tasks for Goal-Directed Interactive Agents", "abstract": "Goal-directed interactive agents, which autonomously complete tasks through\ninteractions with their environment, can assist humans in various domains of\ntheir daily lives. Recent advances in large language models (LLMs) led to a\nsurge of new, more and more challenging tasks to evaluate such agents. To\nproperly contextualize performance across these tasks, it is imperative to\nunderstand the different challenges they pose to agents. To this end, this\nsurvey compiles relevant tasks and environments for evaluating goal-directed\ninteractive agents, structuring them along dimensions relevant for\nunderstanding current obstacles. An up-to-date compilation of relevant\nresources can be found on our project website:\nhttps://coli-saar.github.io/interactive-agents.", "published": "2024-09-27 08:17:53", "link": "http://arxiv.org/abs/2409.18538v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hit the Sweet Spot! Span-Level Ensemble for Large Language Models", "abstract": "Ensembling various LLMs to unlock their complementary potential and leverage\ntheir individual strengths is highly valuable. Previous studies typically focus\non two main paradigms: sample-level and token-level ensembles. Sample-level\nensemble methods either select or blend fully generated outputs, which hinders\ndynamic correction and enhancement of outputs during the generation process. On\nthe other hand, token-level ensemble methods enable real-time correction\nthrough fine-grained ensemble at each generation step. However, the information\ncarried by an individual token is quite limited, leading to suboptimal\ndecisions at each step. To address these issues, we propose SweetSpan, a\nspan-level ensemble method that effectively balances the need for real-time\nadjustments and the information required for accurate ensemble decisions. Our\napproach involves two key steps: First, we have each candidate model\nindependently generate candidate spans based on the shared prefix. Second, we\ncalculate perplexity scores to facilitate mutual evaluation among the candidate\nmodels and achieve robust span selection by filtering out unfaithful scores. To\ncomprehensively evaluate ensemble methods, we propose a new challenging setting\n(ensemble models with significant performance gaps) in addition to the standard\nsetting (ensemble the best-performing models) to assess the performance of\nmodel ensembles in more realistic scenarios. Experimental results in both\nstandard and challenging settings across various language generation tasks\ndemonstrate the effectiveness, robustness, and versatility of our approach\ncompared with previous ensemble methods.", "published": "2024-09-27 09:41:29", "link": "http://arxiv.org/abs/2409.18583v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do LLMs suffer from Multi-Party Hangover? A Diagnostic Approach to\n  Addressee Recognition and Response Selection in Conversations", "abstract": "Assessing the performance of systems to classify Multi-Party Conversations\n(MPC) is challenging due to the interconnection between linguistic and\nstructural characteristics of conversations. Conventional evaluation methods\noften overlook variances in model behavior across different levels of\nstructural complexity on interaction graphs. In this work, we propose a\nmethodological pipeline to investigate model performance across specific\nstructural attributes of conversations. As a proof of concept we focus on\nResponse Selection and Addressee Recognition tasks, to diagnose model\nweaknesses. To this end, we extract representative diagnostic subdatasets with\na fixed number of users and a good structural variety from a large and open\ncorpus of online MPCs. We further frame our work in terms of data minimization,\navoiding the use of original usernames to preserve privacy, and propose\nalternatives to using original text messages. Results show that response\nselection relies more on the textual content of conversations, while addressee\nrecognition requires capturing their structural dimension. Using an LLM in a\nzero-shot setting, we further highlight how sensitivity to prompt variations is\ntask-dependent.", "published": "2024-09-27 10:07:33", "link": "http://arxiv.org/abs/2409.18602v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Precedents for Legal Judgement Prediction on European\n  Court of Human Rights Cases", "abstract": "Inspired by the legal doctrine of stare decisis, which leverages precedents\n(prior cases) for informed decision-making, we explore methods to integrate\nthem into LJP models. To facilitate precedent retrieval, we train a retriever\nwith a fine-grained relevance signal based on the overlap ratio of alleged\narticles between cases. We investigate two strategies to integrate precedents:\ndirect incorporation at inference via label interpolation based on case\nproximity and during training via a precedent fusion module using a\nstacked-cross attention model. We employ joint training of the retriever and\nLJP models to address latent space divergence between them. Our experiments on\nLJP tasks from the ECHR jurisdiction reveal that integrating precedents during\ntraining coupled with joint training of the retriever and LJP model,\noutperforms models without precedents or with precedents incorporated only at\ninference, particularly benefiting sparser articles.", "published": "2024-09-27 11:24:35", "link": "http://arxiv.org/abs/2409.18644v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Craft of Selective Prediction: Towards Reliable Case Outcome\n  Classification -- An Empirical Study on European Court of Human Rights Cases", "abstract": "In high-stakes decision-making tasks within legal NLP, such as Case Outcome\nClassification (COC), quantifying a model's predictive confidence is crucial.\nConfidence estimation enables humans to make more informed decisions,\nparticularly when the model's certainty is low, or where the consequences of a\nmistake are significant. However, most existing COC works prioritize high task\nperformance over model reliability. This paper conducts an empirical\ninvestigation into how various design choices including pre-training corpus,\nconfidence estimator and fine-tuning loss affect the reliability of COC models\nwithin the framework of selective prediction. Our experiments on the\nmulti-label COC task, focusing on European Court of Human Rights (ECtHR) cases,\nhighlight the importance of a diverse yet domain-specific pre-training corpus\nfor better calibration. Additionally, we demonstrate that larger models tend to\nexhibit overconfidence, Monte Carlo dropout methods produce reliable confidence\nestimates, and confident error regularization effectively mitigates\noverconfidence. To our knowledge, this is the first systematic exploration of\nselective prediction in legal NLP. Our findings underscore the need for further\nresearch on enhancing confidence measurement and improving the trustworthiness\nof models in the legal domain.", "published": "2024-09-27 11:25:10", "link": "http://arxiv.org/abs/2409.18645v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HiCuLR: Hierarchical Curriculum Learning for Rhetorical Role Labeling of\n  Legal Documents", "abstract": "Rhetorical Role Labeling (RRL) of legal documents is pivotal for various\ndownstream tasks such as summarization, semantic case search and argument\nmining. Existing approaches often overlook the varying difficulty levels\ninherent in legal document discourse styles and rhetorical roles. In this work,\nwe propose HiCuLR, a hierarchical curriculum learning framework for RRL. It\nnests two curricula: Rhetorical Role-level Curriculum (RC) on the outer layer\nand Document-level Curriculum (DC) on the inner layer. DC categorizes documents\nbased on their difficulty, utilizing metrics like deviation from a standard\ndiscourse structure and exposes the model to them in an easy-to-difficult\nfashion. RC progressively strengthens the model to discern\ncoarse-to-fine-grained distinctions between rhetorical roles. Our experiments\non four RRL datasets demonstrate the efficacy of HiCuLR, highlighting the\ncomplementary nature of DC and RC.", "published": "2024-09-27 11:28:01", "link": "http://arxiv.org/abs/2409.18647v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Co-Trained Retriever-Generator Framework for Question Generation in\n  Earnings Calls", "abstract": "In diverse professional environments, ranging from academic conferences to\ncorporate earnings calls, the ability to anticipate audience questions stands\nparamount. Traditional methods, which rely on manual assessment of an\naudience's background, interests, and subject knowledge, often fall short -\nparticularly when facing large or heterogeneous groups, leading to imprecision\nand inefficiency. While NLP has made strides in text-based question generation,\nits primary focus remains on academic settings, leaving the intricate\nchallenges of professional domains, especially earnings call conferences,\nunderserved. Addressing this gap, our paper pioneers the multi-question\ngeneration (MQG) task specifically designed for earnings call contexts. Our\nmethodology involves an exhaustive collection of earnings call transcripts and\na novel annotation technique to classify potential questions. Furthermore, we\nintroduce a retriever-enhanced strategy to extract relevant information. With a\ncore aim of generating a spectrum of potential questions that analysts might\npose, we derive these directly from earnings call content. Empirical\nevaluations underscore our approach's edge, revealing notable excellence in the\naccuracy, consistency, and perplexity of the questions generated.", "published": "2024-09-27 12:04:58", "link": "http://arxiv.org/abs/2409.18677v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rehearsing Answers to Probable Questions with Perspective-Taking", "abstract": "Question answering (QA) has been a long-standing focus in the NLP field,\npredominantly addressing reading comprehension and common sense QA. However,\nscenarios involving the preparation of answers to probable questions during\nprofessional oral presentations remain underexplored. In this paper, we pioneer\nthe examination of this crucial yet overlooked topic by utilizing real-world QA\nconversation transcripts between company managers and professional analysts. We\nexplore the proposed task using three causal knowledge graphs (KGs) and three\nlarge language models (LLMs). This work provides foundational insights into the\napplication of LLMs in professional QA scenarios, highlighting the importance\nof causal KGs and perspective-taking in generating effective responses.", "published": "2024-09-27 12:05:05", "link": "http://arxiv.org/abs/2409.18678v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Why\" Has the Least Side Effect on Model Editing", "abstract": "Training large language models (LLMs) from scratch is an expensive endeavor,\nparticularly as world knowledge continually evolves. To maintain relevance and\naccuracy of LLMs, model editing has emerged as a pivotal research area. While\nthese methods hold promise, they can also produce unintended side effects.\nTheir underlying factors and causes remain largely unexplored. This paper\ndelves into a critical factor-question type-by categorizing model editing\nquestions. Our findings reveal that the extent of performance degradation\nvaries significantly across different question types, providing new insights\nfor experimental design in knowledge editing. Furthermore, we investigate\nwhether insights from smaller models can be extrapolated to larger models. Our\nresults indicate discrepancies in findings between models of different sizes,\nsuggesting that insights from smaller models may not necessarily apply to\nlarger models. Additionally, we examine the impact of batch size on side\neffects, discovering that increasing the batch size can mitigate performance\ndrops.", "published": "2024-09-27 12:05:12", "link": "http://arxiv.org/abs/2409.18679v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Local Transcription Models in Home Care Nursing in Switzerland: an\n  Interdisciplinary Case Study", "abstract": "Latest advances in the field of natural language processing (NLP) enable new\nuse cases for different domains, including the medical sector. In particular,\ntranscription can be used to support automation in the nursing documentation\nprocess and give nurses more time to interact with the patients. However,\ndifferent challenges including (a) data privacy, (b) local languages and\ndialects, and (c) domain-specific vocabulary need to be addressed. In this case\nstudy, we investigate the case of home care nursing documentation in\nSwitzerland. We assessed different transcription tools and models, and\nconducted several experiments with OpenAI Whisper, involving different\nvariations of German (i.e., dialects, foreign accent) and manually curated\nexample texts by a domain expert of home care nursing. Our results indicate\nthat even the used out-of-the-box model performs sufficiently well to be a good\nstarting point for future research in the field.", "published": "2024-09-27 15:15:50", "link": "http://arxiv.org/abs/2409.18819v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IDGen: Item Discrimination Induced Prompt Generation for LLM Evaluation", "abstract": "As Large Language Models (LLMs) grow increasingly adept at managing complex\ntasks, the evaluation set must keep pace with these advancements to ensure it\nremains sufficiently discriminative. Item Discrimination (ID) theory, which is\nwidely used in educational assessment, measures the ability of individual test\nitems to differentiate between high and low performers. Inspired by this\ntheory, we propose an ID-induced prompt synthesis framework for evaluating LLMs\nto ensure the evaluation set can continually update and refine according to\nmodel abilities. Our data synthesis framework prioritizes both breadth and\nspecificity. It can generate prompts that comprehensively evaluate the\ncapabilities of LLMs while revealing meaningful performance differences between\nmodels, allowing for effective discrimination of their relative strengths and\nweaknesses across various tasks and domains. To produce high-quality data, we\nincorporate a self-correct mechanism into our generalization framework, and\ndevelop two models to predict prompt discrimination and difficulty score to\nfacilitate our data synthesis framework, contributing valuable tools to\nevaluation data synthesis research. We apply our generated data to evaluate\nfive SOTA models. Our data achieves an average score of 51.92, accompanied by a\nvariance of 10.06. By contrast, previous works (i.e., SELF-INSTRUCT and\nWizardLM) obtain an average score exceeding 67, with a variance below 3.2. The\nresults demonstrate that the data generated by our framework is more\nchallenging and discriminative compared to previous works. We will release a\ndataset of over 3,000 carefully crafted prompts to facilitate evaluation\nresearch of LLMs.", "published": "2024-09-27 16:29:12", "link": "http://arxiv.org/abs/2409.18892v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ruler: A Model-Agnostic Method to Control Generated Length for Large\n  Language Models", "abstract": "The instruction-following ability of large language models enables humans to\ninteract with AI agents in a natural way. However, when required to generate\nresponses of a specific length, large language models often struggle to meet\nusers' needs due to their inherent difficulty in accurately perceiving\nnumerical constraints. To explore the ability of large language models to\ncontrol the length of generated responses, we propose the Target Length\nGeneration Task (TLG) and design two metrics, Precise Match (PM) and Flexible\nMatch (FM) to evaluate the model's performance in adhering to specified\nresponse lengths. Furthermore, we introduce a novel, model-agnostic approach\ncalled Ruler, which employs Meta Length Tokens (MLTs) to enhance the\ninstruction-following ability of large language models under length-constrained\ninstructions. Specifically, Ruler equips LLMs with the ability to generate\nresponses of a specified length based on length constraints within the\ninstructions. Moreover, Ruler can automatically generate appropriate MLT when\nlength constraints are not explicitly provided, demonstrating excellent\nversatility and generalization. Comprehensive experiments show the\neffectiveness of Ruler across different LLMs on Target Length Generation Task,\ne.g., at All Level 27.97 average gain on PM, 29.57 average gain on FM. In\naddition, we conduct extensive ablation experiments to further substantiate the\nefficacy and generalization of Ruler. Our code and data is available at\nhttps://github.com/Geaming2002/Ruler.", "published": "2024-09-27 17:44:58", "link": "http://arxiv.org/abs/2409.18943v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncovering Differences in Persuasive Language in Russian versus English\n  Wikipedia", "abstract": "We study how differences in persuasive language across Wikipedia articles,\nwritten in either English and Russian, can uncover each culture's distinct\nperspective on different subjects. We develop a large language model (LLM)\npowered system to identify instances of persuasive language in multilingual\ntexts. Instead of directly prompting LLMs to detect persuasion, which is\nsubjective and difficult, we propose to reframe the task to instead ask\nhigh-level questions (HLQs) which capture different persuasive aspects.\nImportantly, these HLQs are authored by LLMs themselves. LLMs over-generate a\nlarge set of HLQs, which are subsequently filtered to a small set aligned with\nhuman labels for the original task. We then apply our approach to a\nlarge-scale, bilingual dataset of Wikipedia articles (88K total), using a\ntwo-stage identify-then-extract prompting strategy to find instances of\npersuasion.\n  We quantify the amount of persuasion per article, and explore the differences\nin persuasion through several experiments on the paired articles. Notably, we\ngenerate rankings of articles by persuasion in both languages. These rankings\nmatch our intuitions on the culturally-salient subjects; Russian Wikipedia\nhighlights subjects on Ukraine, while English Wikipedia highlights the Middle\nEast. Grouping subjects into larger topics, we find politically-related events\ncontain more persuasion than others. We further demonstrate that HLQs obtain\nsimilar performance when posed in either English or Russian. Our methodology\nenables cross-lingual, cross-cultural understanding at scale, and we release\nour code, prompts, and data.", "published": "2024-09-27 21:23:19", "link": "http://arxiv.org/abs/2409.19148v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Power of Decision Trees in Auto-Regressive Language Modeling", "abstract": "Originally proposed for handling time series data, Auto-regressive Decision\nTrees (ARDTs) have not yet been explored for language modeling. This paper\ndelves into both the theoretical and practical applications of ARDTs in this\nnew context. We theoretically demonstrate that ARDTs can compute complex\nfunctions, such as simulating automata, Turing machines, and sparse circuits,\nby leveraging \"chain-of-thought\" computations. Our analysis provides bounds on\nthe size, depth, and computational efficiency of ARDTs, highlighting their\nsurprising computational power. Empirically, we train ARDTs on simple language\ngeneration tasks, showing that they can learn to generate coherent and\ngrammatically correct text on par with a smaller Transformer model.\nAdditionally, we show that ARDTs can be used on top of transformer\nrepresentations to solve complex reasoning tasks. This research reveals the\nunique computational abilities of ARDTs, aiming to broaden the architectural\ndiversity in language model development.", "published": "2024-09-27 21:25:00", "link": "http://arxiv.org/abs/2409.19150v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can LLMs Really Learn to Translate a Low-Resource Language from One\n  Grammar Book?", "abstract": "Extremely low-resource (XLR) languages lack substantial corpora for training\nNLP models, motivating the use of all available resources such as dictionaries\nand grammar books. Machine Translation from One Book (Tanzer et al., 2024)\nsuggests prompting long-context LLMs with one grammar book enables\nEnglish-Kalamang translation, an unseen XLR language - a noteworthy case of\nlinguistic knowledge helping an NLP task. We investigate whether the book's\ngrammatical explanations or its parallel examples are most effective for\nlearning XLR translation, finding almost all improvement stems from the\nparallel examples. Further, we find similar results for Nepali, a seen\nlow-resource language, and achieve performance comparable to an LLM with a\ngrammar book by simply fine-tuning an encoder-decoder translation model. We\nthen investigate where grammar books help by testing two linguistic tasks,\ngrammaticality judgment and gloss prediction, and we explore what kind of\ngrammatical knowledge helps by introducing a typological feature prompt that\nachieves leading results on these more relevant tasks. We thus emphasise the\nimportance of task-appropriate data for XLR languages: parallel examples for\ntranslation, and grammatical data for linguistic tasks. As we find no evidence\nthat long-context LLMs can make effective use of grammatical explanations for\nXLR translation, we suggest data collection for multilingual XLR tasks such as\ntranslation is best focused on parallel data over linguistic description.", "published": "2024-09-27 21:27:32", "link": "http://arxiv.org/abs/2409.19151v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SciDFM: A Large Language Model with Mixture-of-Experts for Science", "abstract": "Recently, there has been a significant upsurge of interest in leveraging\nlarge language models (LLMs) to assist scientific discovery. However, most LLMs\nonly focus on general science, while they lack domain-specific knowledge, such\nas chemical molecules and amino acid sequences. To bridge these gaps, we\nintroduce SciDFM, a mixture-of-experts LLM, which is trained from scratch and\nis able to conduct college-level scientific reasoning and understand molecules\nand amino acid sequences. We collect a large-scale training corpus containing\nnumerous scientific papers and books from different disciplines as well as data\nfrom domain-specific databases. We further fine-tune the pre-trained model on\nlots of instruction data to improve performances on downstream benchmarks. From\nexperiment results, we show that SciDFM achieves strong performance on general\nscientific benchmarks such as SciEval and SciQ, and it reaches a SOTA\nperformance on domain-specific benchmarks among models of similar size. We\nfurther analyze the expert layers and show that the results of expert selection\nvary with data from different disciplines. To benefit the broader research\ncommunity, we open-source SciDFM at\nhttps://huggingface.co/OpenDFM/SciDFM-MoE-A5.6B-v1.0.", "published": "2024-09-27 03:00:29", "link": "http://arxiv.org/abs/2409.18412v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leveraging Long-Context Large Language Models for Multi-Document\n  Understanding and Summarization in Enterprise Applications", "abstract": "The rapid increase in unstructured data across various fields has made\nmulti-document comprehension and summarization a critical task. Traditional\napproaches often fail to capture relevant context, maintain logical\nconsistency, and extract essential information from lengthy documents. This\npaper explores the use of Long-context Large Language Models (LLMs) for\nmulti-document summarization, demonstrating their exceptional capacity to grasp\nextensive connections, provide cohesive summaries, and adapt to various\nindustry domains and integration with enterprise applications/systems. The\npaper discusses the workflow of multi-document summarization for effectively\ndeploying long-context LLMs, supported by case studies in legal applications,\nenterprise functions such as HR, finance, and sourcing, as well as in the\nmedical and news domains. These case studies show notable enhancements in both\nefficiency and accuracy. Technical obstacles, such as dataset diversity, model\nscalability, and ethical considerations like bias mitigation and factual\naccuracy, are carefully analyzed. Prospective research avenues are suggested to\naugment the functionalities and applications of long-context LLMs, establishing\nthem as pivotal tools for transforming information processing across diverse\nsectors and enterprise applications.", "published": "2024-09-27 05:29:31", "link": "http://arxiv.org/abs/2409.18454v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "URIEL+: Enhancing Linguistic Inclusion and Usability in a Typological\n  and Multilingual Knowledge Base", "abstract": "URIEL is a knowledge base offering geographical, phylogenetic, and\ntypological vector representations for 7970 languages. It includes distance\nmeasures between these vectors for 4005 languages, which are accessible via the\nlang2vec tool. Despite being frequently cited, URIEL is limited in terms of\nlinguistic inclusion and overall usability. To tackle these challenges, we\nintroduce URIEL+, an enhanced version of URIEL and lang2vec that addresses\nthese limitations. In addition to expanding typological feature coverage for\n2898 languages, URIEL+ improves the user experience with robust, customizable\ndistance calculations to better suit the needs of users. These upgrades also\noffer competitive performance on downstream tasks and provide distances that\nbetter align with linguistic distance studies.", "published": "2024-09-27 06:18:55", "link": "http://arxiv.org/abs/2409.18472v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Do We Need Domain-Specific Embedding Models? An Empirical Investigation", "abstract": "Embedding models play a crucial role in representing and retrieving\ninformation across various NLP applications. Recent advancements in Large\nLanguage Models (LLMs) have further enhanced the performance of embedding\nmodels, which are trained on massive amounts of text covering almost every\ndomain. These models are often benchmarked on general-purpose datasets like\nMassive Text Embedding Benchmark (MTEB), where they demonstrate superior\nperformance. However, a critical question arises: Is the development of\ndomain-specific embedding models necessary when general-purpose models are\ntrained on vast corpora that already include specialized domain texts? In this\npaper, we empirically investigate this question, choosing the finance domain as\nan example. We introduce the Finance Massive Text Embedding Benchmark\n(FinMTEB), a counterpart to MTEB that consists of financial domain-specific\ntext datasets. We evaluate the performance of seven state-of-the-art embedding\nmodels on FinMTEB and observe a significant performance drop compared to their\nperformance on MTEB. To account for the possibility that this drop is driven by\nFinMTEB's higher complexity, we propose four measures to quantify dataset\ncomplexity and control for this factor in our analysis. Our analysis provides\ncompelling evidence that state-of-the-art embedding models struggle to capture\ndomain-specific linguistic and semantic patterns. Moreover, we find that the\nperformance of general-purpose embedding models on MTEB is not correlated with\ntheir performance on FinMTEB, indicating the need for domain-specific embedding\nbenchmarks for domain-specific embedding models. This study sheds light on\ndeveloping domain-specific embedding models in the LLM era. FinMTEB comes with\nopen-source code at https://github.com/yixuantt/FinMTEB", "published": "2024-09-27 07:46:06", "link": "http://arxiv.org/abs/2409.18511v4", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Research on Predicting Public Opinion Event Heat Levels Based on Large\n  Language Models", "abstract": "In recent years, with the rapid development of large language models, serval\nmodels such as GPT-4o have demonstrated extraordinary capabilities, surpassing\nhuman performance in various language tasks. As a result, many researchers have\nbegun exploring their potential applications in the field of public opinion\nanalysis. This study proposes a novel large-language-models-based method for\npublic opinion event heat level prediction. First, we preprocessed and\nclassified 62,836 Chinese hot event data collected between July 2022 and\nDecember 2023. Then, based on each event's online dissemination heat index, we\nused the MiniBatchKMeans algorithm to automatically cluster the events and\ncategorize them into four heat levels (ranging from low heat to very high\nheat). Next, we randomly selected 250 events from each heat level, totalling\n1,000 events, to build the evaluation dataset. During the evaluation process,\nwe employed various large language models to assess their accuracy in\npredicting event heat levels in two scenarios: without reference cases and with\nsimilar case references. The results showed that GPT-4o and DeepseekV2\nperformed the best in the latter case, achieving prediction accuracies of 41.4%\nand 41.5%, respectively. Although the overall prediction accuracy remains\nrelatively low, it is worth noting that for low-heat (Level 1) events, the\nprediction accuracies of these two models reached 73.6% and 70.4%,\nrespectively. Additionally, the prediction accuracy showed a downward trend\nfrom Level 1 to Level 4, which correlates with the uneven distribution of data\nacross the heat levels in the actual dataset. This suggests that with the more\nrobust dataset, public opinion event heat level prediction based on large\nlanguage models will have significant research potential for the future.", "published": "2024-09-27 08:34:42", "link": "http://arxiv.org/abs/2409.18548v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Model-based Preference Optimization in Abstractive Summarization without\n  Human Feedback", "abstract": "In abstractive summarization, the challenge of producing concise and accurate\nsummaries arises from the vast amount of information contained in the source\ndocument. Consequently, although Large Language Models (LLMs) can generate\nfluent text, they often introduce inaccuracies by hallucinating content not\nfound in the original source. While supervised fine-tuning methods that\nmaximize likelihood contribute to this issue, they do not consistently enhance\nthe faithfulness of the summaries. Preference-based optimization methods, such\nas Direct Preference Optimization (DPO), can further refine the model to align\nwith human preferences. However, these methods still heavily depend on costly\nhuman feedback. In this work, we introduce a novel and straightforward approach\ncalled Model-based Preference Optimization (MPO) to fine-tune LLMs for improved\nsummarization abilities without any human feedback. By leveraging the model's\ninherent summarization capabilities, we create a preference dataset that is\nfully generated by the model using different decoding strategies. Our\nexperiments on standard summarization datasets and various metrics demonstrate\nthat our proposed MPO significantly enhances the quality of generated summaries\nwithout relying on human feedback.", "published": "2024-09-27 10:35:45", "link": "http://arxiv.org/abs/2409.18618v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Charting the Future: Using Chart Question-Answering for Scalable\n  Evaluation of LLM-Driven Data Visualizations", "abstract": "We propose a novel framework that leverages Visual Question Answering (VQA)\nmodels to automate the evaluation of LLM-generated data visualizations.\nTraditional evaluation methods often rely on human judgment, which is costly\nand unscalable, or focus solely on data accuracy, neglecting the effectiveness\nof visual communication. By employing VQA models, we assess data representation\nquality and the general communicative clarity of charts. Experiments were\nconducted using two leading VQA benchmark datasets, ChartQA and PlotQA, with\nvisualizations generated by OpenAI's GPT-3.5 Turbo and Meta's Llama 3.1\n70B-Instruct models. Our results indicate that LLM-generated charts do not\nmatch the accuracy of the original non-LLM-generated charts based on VQA\nperformance measures. Moreover, while our results demonstrate that few-shot\nprompting significantly boosts the accuracy of chart generation, considerable\nprogress remains to be made before LLMs can fully match the precision of\nhuman-generated graphs. This underscores the importance of our work, which\nexpedites the research process by enabling rapid iteration without the need for\nhuman annotation, thus accelerating advancements in this field.", "published": "2024-09-27 14:02:48", "link": "http://arxiv.org/abs/2409.18764v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Survey on the Honesty of Large Language Models", "abstract": "Honesty is a fundamental principle for aligning large language models (LLMs)\nwith human values, requiring these models to recognize what they know and don't\nknow and be able to faithfully express their knowledge. Despite promising,\ncurrent LLMs still exhibit significant dishonest behaviors, such as confidently\npresenting wrong answers or failing to express what they know. In addition,\nresearch on the honesty of LLMs also faces challenges, including varying\ndefinitions of honesty, difficulties in distinguishing between known and\nunknown knowledge, and a lack of comprehensive understanding of related\nresearch. To address these issues, we provide a survey on the honesty of LLMs,\ncovering its clarification, evaluation approaches, and strategies for\nimprovement. Moreover, we offer insights for future research, aiming to inspire\nfurther exploration in this important area.", "published": "2024-09-27 14:34:54", "link": "http://arxiv.org/abs/2409.18786v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AIPatient: Simulating Patients with EHRs and LLM Powered Agentic\n  Workflow", "abstract": "Simulated patient systems play a crucial role in modern medical education and\nresearch, providing safe, integrative learning environments and enabling\nclinical decision-making simulations. Large Language Models (LLM) could advance\nsimulated patient systems by replicating medical conditions and patient-doctor\ninteractions with high fidelity and low cost. However, ensuring the\neffectiveness and trustworthiness of these systems remains a challenge, as they\nrequire a large, diverse, and precise patient knowledgebase, along with a\nrobust and stable knowledge diffusion to users. Here, we developed AIPatient,\nan advanced simulated patient system with AIPatient Knowledge Graph (AIPatient\nKG) as the input and the Reasoning Retrieval-Augmented Generation (Reasoning\nRAG) agentic workflow as the generation backbone. AIPatient KG samples data\nfrom Electronic Health Records (EHRs) in the Medical Information Mart for\nIntensive Care (MIMIC)-III database, producing a clinically diverse and\nrelevant cohort of 1,495 patients with high knowledgebase validity (F1 0.89).\nReasoning RAG leverages six LLM powered agents spanning tasks including\nretrieval, KG query generation, abstraction, checker, rewrite, and\nsummarization. This agentic framework reaches an overall accuracy of 94.15% in\nEHR-based medical Question Answering (QA), outperforming benchmarks that use\neither no agent or only partial agent integration. Our system also presents\nhigh readability (median Flesch Reading Ease 77.23; median Flesch Kincaid Grade\n5.6), robustness (ANOVA F-value 0.6126, p>0.1), and stability (ANOVA F-value\n0.782, p>0.1). The promising performance of the AIPatient system highlights its\npotential to support a wide range of applications, including medical education,\nmodel evaluation, and system integration.", "published": "2024-09-27 17:17:15", "link": "http://arxiv.org/abs/2409.18924v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-Replicating Mechanical Universal Turing Machine", "abstract": "This paper presents the implementation of a self-replicating finite-state\nmachine (FSM) and a self-replicating Turing Machine (TM) using bio-inspired\nmechanisms. Building on previous work that introduced self-replicating\nstructures capable of sorting, copying, and reading information, this study\ndemonstrates the computational power of these mechanisms by explicitly\nconstructing a functioning FSM and TM. This study demonstrates the universality\nof the system by emulating the UTM(5,5) of Neary and Woods.", "published": "2024-09-27 08:28:37", "link": "http://arxiv.org/abs/2409.19037v1", "categories": ["cs.FL", "cs.CL"], "primary_category": "cs.FL"}
{"title": "Show and Guide: Instructional-Plan Grounded Vision and Language Model", "abstract": "Guiding users through complex procedural plans is an inherently multimodal\ntask in which having visually illustrated plan steps is crucial to deliver an\neffective plan guidance. However, existing works on plan-following language\nmodels (LMs) often are not capable of multimodal input and output. In this\nwork, we present MM-PlanLLM, the first multimodal LLM designed to assist users\nin executing instructional tasks by leveraging both textual plans and visual\ninformation. Specifically, we bring cross-modality through two key tasks:\nConversational Video Moment Retrieval, where the model retrieves relevant\nstep-video segments based on user queries, and Visually-Informed Step\nGeneration, where the model generates the next step in a plan, conditioned on\nan image of the user's current progress. MM-PlanLLM is trained using a novel\nmultitask-multistage approach, designed to gradually expose the model to\nmultimodal instructional-plans semantic layers, achieving strong performance on\nboth multimodal and textual dialogue in a plan-grounded setting. Furthermore,\nwe show that the model delivers cross-modal temporal and plan-structure\nrepresentations aligned between textual plan steps and instructional video\nmoments.", "published": "2024-09-27 18:20:24", "link": "http://arxiv.org/abs/2409.19074v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Meta-RTL: Reinforcement-Based Meta-Transfer Learning for Low-Resource\n  Commonsense Reasoning", "abstract": "Meta learning has been widely used to exploit rich-resource source tasks to\nimprove the performance of low-resource target tasks. Unfortunately, most\nexisting meta learning approaches treat different source tasks equally,\nignoring the relatedness of source tasks to the target task in knowledge\ntransfer. To mitigate this issue, we propose a reinforcement-based multi-source\nmeta-transfer learning framework (Meta-RTL) for low-resource commonsense\nreasoning. In this framework, we present a reinforcement-based approach to\ndynamically estimating source task weights that measure the contribution of the\ncorresponding tasks to the target task in the meta-transfer learning. The\ndifferences between the general loss of the meta model and task-specific losses\nof source-specific temporal meta models on sampled target data are fed into the\npolicy network of the reinforcement learning module as rewards. The policy\nnetwork is built upon LSTMs that capture long-term dependencies on source task\nweight estimation across meta learning iterations. We evaluate the proposed\nMeta-RTL using both BERT and ALBERT as the backbone of the meta model on three\ncommonsense reasoning benchmark datasets. Experimental results demonstrate that\nMeta-RTL substantially outperforms strong baselines and previous task selection\nstrategies and achieves larger improvements on extremely low-resource settings.", "published": "2024-09-27 18:22:22", "link": "http://arxiv.org/abs/2409.19075v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Confidential Prompting: Protecting User Prompts from Cloud LLM Providers", "abstract": "Our work tackles the challenge of securing user inputs in cloud-hosted large\nlanguage model (LLM) serving while ensuring model confidentiality, output\ninvariance, and compute efficiency. We introduce Secure Partitioned Decoding\n(SPD), which uses confidential computing to confine user prompts to a trusted\nexecution environment (TEE), namely a confidential virtual machine (CVM), while\nallowing service providers to generate tokens efficiently. We also introduce a\nnovel cryptographic method, Prompt Obfuscation (PO), to ensure robustness\nagainst reconstruction attacks on SPD. We demonstrate our approach preserves\nboth prompt confidentiality and LLM serving efficiency. Our solution enables\nprivacy-preserving cloud LLM serving that handles sensitive prompts, such as\nclinical records, financial data, and personal information.", "published": "2024-09-27 20:32:42", "link": "http://arxiv.org/abs/2409.19134v3", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "HM3: Heterogeneous Multi-Class Model Merging", "abstract": "Foundation language model deployments often include auxiliary guard-rail\nmodels to filter or classify text, detecting jailbreak attempts, biased or\ntoxic output, or ensuring topic adherence. These additional models increase the\ncomplexity and cost of model inference, especially since many are also large\nlanguage models. To address this issue, we explore training-free model merging\ntechniques to consolidate these models into a single, multi-functional model.\nWe propose Heterogeneous Multi-Class Model Merging (HM3) as a simple technique\nfor merging multi-class classifiers with heterogeneous label spaces. Unlike\nparameter-efficient fine-tuning techniques like LoRA, which require extensive\ntraining and add complexity during inference, recent advancements allow models\nto be merged in a training-free manner. We report promising results for merging\nBERT-based guard models, some of which attain an average F1-score higher than\nthe source models while reducing the inference time by up to 44%. We introduce\nself-merging to assess the impact of reduced task-vector density, finding that\nthe more poorly performing hate speech classifier benefits from self-merging\nwhile higher-performing classifiers do not, which raises questions about using\ntask vector reduction for model tuning.", "published": "2024-09-27 22:42:45", "link": "http://arxiv.org/abs/2409.19173v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Defect Prediction with Content-based Features", "abstract": "Traditional defect prediction approaches often use metrics that measure the\ncomplexity of the design or implementing code of a software system, such as the\nnumber of lines of code in a source file. In this paper, we explore a different\napproach based on content of source code. Our key assumption is that source\ncode of a software system contains information about its technical aspects and\nthose aspects might have different levels of defect-proneness. Thus,\ncontent-based features such as words, topics, data types, and package names\nextracted from a source code file could be used to predict its defects. We have\nperformed an extensive empirical evaluation and found that: i) such\ncontent-based features have higher predictive power than code complexity\nmetrics and ii) the use of feature selection, reduction, and combination\nfurther improves the prediction performance.", "published": "2024-09-27 00:49:27", "link": "http://arxiv.org/abs/2409.18365v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Improving Multilingual ASR in the Wild Using Simple N-best Re-ranking", "abstract": "Multilingual Automatic Speech Recognition (ASR) models are typically\nevaluated in a setting where the ground-truth language of the speech utterance\nis known, however, this is often not the case for most practical settings.\nAutomatic Spoken Language Identification (SLID) models are not perfect and\nmisclassifications have a substantial impact on the final ASR accuracy. In this\npaper, we present a simple and effective N-best re-ranking approach to improve\nmultilingual ASR accuracy for several prominent acoustic models by employing\nexternal features such as language models and text-based language\nidentification models. Our results on FLEURS using the MMS and Whisper models\nshow spoken language identification accuracy improvements of 8.7% and 6.1%,\nrespectively and word error rates which are 3.3% and 2.0% lower on these\nbenchmarks.", "published": "2024-09-27 03:31:32", "link": "http://arxiv.org/abs/2409.18428v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM\n  Performance and Generalization", "abstract": "While generalization over tasks from easy to hard is crucial to profile\nlanguage models (LLMs), the datasets with fine-grained difficulty annotations\nfor each problem across a broad range of complexity are still blank. Aiming to\naddress this limitation, we present Easy2Hard-Bench, a consistently formatted\ncollection of 6 benchmark datasets spanning various domains, such as\nmathematics and programming problems, chess puzzles, and reasoning questions.\nEach problem within these datasets is annotated with numerical difficulty\nscores. To systematically estimate problem difficulties, we collect abundant\nperformance data on attempts to each problem by humans in the real world or\nLLMs on the prominent leaderboard. Leveraging the rich performance data, we\napply well-established difficulty ranking systems, such as Item Response Theory\n(IRT) and Glicko-2 models, to uniformly assign numerical difficulty scores to\nproblems. Moreover, datasets in Easy2Hard-Bench distinguish themselves from\nprevious collections by a higher proportion of challenging problems. Through\nextensive experiments with six state-of-the-art LLMs, we provide a\ncomprehensive analysis of their performance and generalization capabilities\nacross varying levels of difficulty, with the aim of inspiring future research\nin LLM generalization. The datasets are available at\nhttps://huggingface.co/datasets/furonghuang-lab/Easy2Hard-Bench.", "published": "2024-09-27 03:49:56", "link": "http://arxiv.org/abs/2409.18433v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "EmoPro: A Prompt Selection Strategy for Emotional Expression in LM-based\n  Speech Synthesis", "abstract": "Recent advancements in speech synthesis models, trained on extensive\ndatasets, have demonstrated remarkable zero-shot capabilities. These models can\ncontrol content, timbre, and emotion in generated speech based on prompt\ninputs. Despite these advancements, the choice of prompts significantly impacts\nthe output quality, yet most existing selection schemes do not adequately\naddress the control of emotional intensity. To address this question, this\npaper proposes a two-stage prompt selection strategy EmoPro, which is\nspecifically designed for emotionally controllable speech synthesis. This\nstrategy focuses on selecting highly expressive and high-quality prompts by\nevaluating them from four perspectives: emotional expression strength, speech\nquality, text-emotion consistency, and model generation performance.\nExperimental results show that prompts selected using the proposed method\nresult in more emotionally expressive and engaging synthesized speech compared\nto those obtained through baseline. Audio samples and codes will be available\nat https://whyrrrrun.github.io/EmoPro/.", "published": "2024-09-27 07:46:52", "link": "http://arxiv.org/abs/2409.18512v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "\"Oh LLM, I'm Asking Thee, Please Give Me a Decision Tree\": Zero-Shot\n  Decision Tree Induction and Embedding with Large Language Models", "abstract": "Large language models (LLMs) provide powerful means to leverage prior\nknowledge for predictive modeling when data is limited. In this work, we\ndemonstrate how LLMs can use their compressed world knowledge to generate\nintrinsically interpretable machine learning models, i.e., decision trees,\nwithout any training data. We find that these zero-shot decision trees can\nsurpass data-driven trees on some small-sized tabular datasets and that\nembeddings derived from these trees perform on par with data-driven tree-based\nembeddings on average. Our knowledge-driven decision tree induction and\nembedding approaches therefore serve as strong new baselines for data-driven\nmachine learning methods in the low-data regime.", "published": "2024-09-27 09:53:48", "link": "http://arxiv.org/abs/2409.18594v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "ASAG2024: A Combined Benchmark for Short Answer Grading", "abstract": "Open-ended questions test a more thorough understanding than closed-ended\nquestions and are often a preferred assessment method. However, open-ended\nquestions are tedious to grade and subject to personal bias. Therefore, there\nhave been efforts to speed up the grading process through automation. Short\nAnswer Grading (SAG) systems aim to automatically score students' answers.\nDespite growth in SAG methods and capabilities, there exists no comprehensive\nshort-answer grading benchmark across different subjects, grading scales, and\ndistributions. Thus, it is hard to assess the capabilities of current automated\ngrading methods in terms of their generalizability. In this preliminary work,\nwe introduce the combined ASAG2024 benchmark to facilitate the comparison of\nautomated grading systems. Combining seven commonly used short-answer grading\ndatasets in a common structure and grading scale. For our benchmark, we\nevaluate a set of recent SAG methods, revealing that while LLM-based approaches\nreach new high scores, they still are far from reaching human performance. This\nopens up avenues for future research on human-machine SAG systems.", "published": "2024-09-27 09:56:02", "link": "http://arxiv.org/abs/2409.18596v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "KALE-LM: Unleash The Power Of AI For Science Via Knowledge And Logic\n  Enhanced Large Model", "abstract": "Artificial intelligence is gradually demonstrating its immense potential, and\nincreasing attention is being given to how AI can be harnessed to advance\nscientific research. In this vision paper, we present our perspectives on how\nAI can better assist scientific inquiry and explore corresponding technical\napproach. We have proposed and open-sourced two large models of our KALE-LM\nmodel series, KALE-LM-Chem(-1.5), which have achieved outstanding performance\nin tasks related to the field of chemistry. We hope that our work serves as a\nstrong starting point, helping to realize more intelligent AI and promoting the\nadvancement of human science and technology, as well as societal development.", "published": "2024-09-27 12:33:57", "link": "http://arxiv.org/abs/2409.18695v2", "categories": ["cs.AI", "cs.CE", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with\n  ASCII Art to Mask Profanity", "abstract": "We introduce a novel family of adversarial attacks that exploit the inability\nof language models to interpret ASCII art. To evaluate these attacks, we\npropose the ToxASCII benchmark and develop two custom ASCII art fonts: one\nleveraging special tokens and another using text-filled letter shapes. Our\nattacks achieve a perfect 1.0 Attack Success Rate across ten models, including\nOpenAI's o1-preview and LLaMA 3.1.\n  Warning: this paper contains examples of toxic language used for research\npurposes.", "published": "2024-09-27 12:54:13", "link": "http://arxiv.org/abs/2409.18708v4", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Cross-Domain Keyword Extraction with Keyness Patterns", "abstract": "Domain dependence and annotation subjectivity pose challenges for supervised\nkeyword extraction. Based on the premises that second-order keyness patterns\nare existent at the community level and learnable from annotated keyword\nextraction datasets, this paper proposes a supervised ranking approach to\nkeyword extraction that ranks keywords with keyness patterns consisting of\nindependent features (such as sublanguage domain and term length) and three\ncategories of dependent features -- heuristic features, specificity features,\nand representavity features. The approach uses two convolutional-neural-network\nbased models to learn keyness patterns from keyword datasets and overcomes\nannotation subjectivity by training the two models with bootstrap sampling\nstrategy. Experiments demonstrate that the approach not only achieves\nstate-of-the-art performance on ten keyword datasets in general supervised\nkeyword extraction with an average top-10-F-measure of 0.316 , but also robust\ncross-domain performance with an average top-10-F-measure of 0.346 on four\ndatasets that are excluded in the training process. Such cross-domain\nrobustness is attributed to the fact that community-level keyness patterns are\nlimited in number and temperately independent of language domains, the\ndistinction between independent features and dependent features, and the\nsampling training strategy that balances excess risk and lack of negative\ntraining data.", "published": "2024-09-27 13:19:19", "link": "http://arxiv.org/abs/2409.18724v1", "categories": ["cs.IR", "cs.CL", "cs.NE", "H.3.1; H.3.3"], "primary_category": "cs.IR"}
{"title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific\n  Synthesis", "abstract": "In response to the growing complexity and volume of scientific literature,\nthis paper introduces the LLMs4Synthesis framework, designed to enhance the\ncapabilities of Large Language Models (LLMs) in generating high-quality\nscientific syntheses. This framework addresses the need for rapid, coherent,\nand contextually rich integration of scientific insights, leveraging both\nopen-source and proprietary LLMs. It also examines the effectiveness of LLMs in\nevaluating the integrity and reliability of these syntheses, alleviating\ninadequacies in current quantitative metrics. Our study contributes to this\nfield by developing a novel methodology for processing scientific papers,\ndefining new synthesis types, and establishing nine detailed quality criteria\nfor evaluating syntheses. The integration of LLMs with reinforcement learning\nand AI feedback is proposed to optimize synthesis quality, ensuring alignment\nwith established criteria. The LLMs4Synthesis framework and its components are\nmade available, promising to enhance both the generation and evaluation\nprocesses in scientific research synthesis.", "published": "2024-09-27 15:04:39", "link": "http://arxiv.org/abs/2409.18812v1", "categories": ["cs.CL", "cs.AI", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Individuation in Neural Models with and without Visual Grounding", "abstract": "We show differences between a language-and-vision model CLIP and two\ntext-only models - FastText and SBERT - when it comes to the encoding of\nindividuation information. We study latent representations that CLIP provides\nfor substrates, granular aggregates, and various numbers of objects. We\ndemonstrate that CLIP embeddings capture quantitative differences in\nindividuation better than models trained on text-only data. Moreover, the\nindividuation hierarchy we deduce from the CLIP embeddings agrees with the\nhierarchies proposed in linguistics and cognitive science.", "published": "2024-09-27 16:04:06", "link": "http://arxiv.org/abs/2409.18868v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.4; J.4; I.6.8; I.2.10"], "primary_category": "cs.CL"}
{"title": "Suicide Phenotyping from Clinical Notes in Safety-Net Psychiatric\n  Hospital Using Multi-Label Classification with Pre-Trained Language Models", "abstract": "Accurate identification and categorization of suicidal events can yield\nbetter suicide precautions, reducing operational burden, and improving care\nquality in high-acuity psychiatric settings. Pre-trained language models offer\npromise for identifying suicidality from unstructured clinical narratives. We\nevaluated the performance of four BERT-based models using two fine-tuning\nstrategies (multiple single-label and single multi-label) for detecting\ncoexisting suicidal events from 500 annotated psychiatric evaluation notes. The\nnotes were labeled for suicidal ideation (SI), suicide attempts (SA), exposure\nto suicide (ES), and non-suicidal self-injury (NSSI). RoBERTa outperformed\nother models using multiple single-label classification strategy (acc=0.86,\nF1=0.78). MentalBERT (acc=0.83, F1=0.74) also exceeded BioClinicalBERT\n(acc=0.82, F1=0.72) which outperformed BERT (acc=0.80, F1=0.70). RoBERTa\nfine-tuned with single multi-label classification further improved the model\nperformance (acc=0.88, F1=0.81). The findings highlight that the model\noptimization, pretraining with domain-relevant data, and the single multi-label\nclassification strategy enhance the model performance of suicide phenotyping.\nKeywords: EHR-based Phenotyping; Natural Language Processing; Secondary Use of\nEHR Data; Suicide Classification; BERT-based Model; Psychiatry; Mental Health", "published": "2024-09-27 16:13:38", "link": "http://arxiv.org/abs/2409.18878v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Soft Measures for Extracting Causal Collective Intelligence", "abstract": "Understanding and modeling collective intelligence is essential for\naddressing complex social systems. Directed graphs called fuzzy cognitive maps\n(FCMs) offer a powerful tool for encoding causal mental models, but extracting\nhigh-integrity FCMs from text is challenging. This study presents an approach\nusing large language models (LLMs) to automate FCM extraction. We introduce\nnovel graph-based similarity measures and evaluate them by correlating their\noutputs with human judgments through the Elo rating system. Results show\npositive correlations with human evaluations, but even the best-performing\nmeasure exhibits limitations in capturing FCM nuances. Fine-tuning LLMs\nimproves performance, but existing measures still fall short. This study\nhighlights the need for soft similarity measures tailored to FCM extraction,\nadvancing collective intelligence modeling with NLP.", "published": "2024-09-27 16:54:36", "link": "http://arxiv.org/abs/2409.18911v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "LML-DAP: Language Model Learning a Dataset for Data-Augmented Prediction", "abstract": "Classification tasks are typically handled using Machine Learning (ML)\nmodels, which lack a balance between accuracy and interpretability. This paper\nintroduces a new approach for classification tasks using Large Language Models\n(LLMs) in an explainable method. Unlike ML models, which rely heavily on data\ncleaning and feature engineering, this method streamlines the process using\nLLMs. This paper proposes a method called \"Language Model Learning (LML)\"\npowered by a new method called \"Data-Augmented Prediction (DAP).\" The\nclassification is performed by LLMs using a method similar to that used by\nhumans who manually explore and understand the data to decide classifications.\nIn the process of LML, a dataset is summarized and evaluated to determine the\nfeatures leading to each label the most. In the DAP process, the system uses\nthe data summary and a row of the testing dataset to automatically generate a\nquery to retrieve relevant rows from the dataset for context-aware\nclassification. LML and DAP unlock new possibilities in areas that require\nexplainable and context-aware decisions by ensuring satisfactory accuracy even\nwith complex data. The system scored an accuracy above 90% in some test cases,\nconfirming the effectiveness and potential of the system to outperform ML\nmodels in various scenarios. The source code is available at\nhttps://github.com/Pro-GenAI/LML-DAP", "published": "2024-09-27 17:58:50", "link": "http://arxiv.org/abs/2409.18957v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Inductive Bias of Stacking Towards Improving Reasoning", "abstract": "Given the increasing scale of model sizes, novel training strategies like\ngradual stacking [Gong et al., 2019, Reddi et al., 2023] have garnered\ninterest. Stacking enables efficient training by gradually growing the depth of\na model in stages and using layers from a smaller model in an earlier stage to\ninitialize the next stage. Although efficient for training, the model biases\ninduced by such growing approaches are largely unexplored. In this work, we\nexamine this fundamental aspect of gradual stacking, going beyond its\nefficiency benefits. We propose a variant of gradual stacking called MIDAS that\ncan speed up language model training by up to 40%. Furthermore we discover an\nintriguing phenomenon: MIDAS is not only training-efficient but surprisingly\nalso has an inductive bias towards improving downstream tasks, especially tasks\nthat require reasoning abilities like reading comprehension and math problems,\ndespite having similar or slightly worse perplexity compared to baseline\ntraining. To further analyze this inductive bias, we construct reasoning\nprimitives -- simple synthetic tasks that are building blocks for reasoning --\nand find that a model pretrained with stacking is significantly better than\nstandard pretraining on these primitives, with and without fine-tuning. This\nprovides stronger and more robust evidence for this inductive bias towards\nreasoning. These findings of training efficiency and inductive bias towards\nreasoning are verified at 1B, 2B and 8B parameter language models. Finally, we\nconjecture the underlying reason for this inductive bias by exploring the\nconnection of stacking to looped models and provide strong supporting empirical\nanalysis.", "published": "2024-09-27 17:58:21", "link": "http://arxiv.org/abs/2409.19044v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CLLMate: A Multimodal Benchmark for Weather and Climate Events\n  Forecasting", "abstract": "Forecasting weather and climate events is crucial for making appropriate\nmeasures to mitigate environmental hazards and minimize losses. However,\nexisting environmental forecasting research focuses narrowly on predicting\nnumerical meteorological variables (e.g., temperature), neglecting the\ntranslation of these variables into actionable textual narratives of events and\ntheir consequences. To bridge this gap, we proposed Weather and Climate Event\nForecasting (WCEF), a new task that leverages numerical meteorological raster\ndata and textual event data to predict weather and climate events. This task is\nchallenging to accomplish due to difficulties in aligning multimodal data and\nthe lack of supervised datasets. To address these challenges, we present\nCLLMate, the first multimodal dataset for WCEF, using 26,156 environmental news\narticles aligned with ERA5 reanalysis data. We systematically benchmark 23\nexisting MLLMs on CLLMate, including closed-source, open-source, and our\nfine-tuned models. Our experiments reveal the advantages and limitations of\nexisting MLLMs and the value of CLLMate for the training and benchmarking of\nthe WCEF task.", "published": "2024-09-27 18:00:13", "link": "http://arxiv.org/abs/2409.19058v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "physics.ao-ph"], "primary_category": "cs.LG"}
{"title": "Evidence Is All You Need: Ordering Imaging Studies via Language Model\n  Alignment with the ACR Appropriateness Criteria", "abstract": "Diagnostic imaging studies are an increasingly important component of the\nworkup and management of acutely presenting patients. However, ordering\nappropriate imaging studies according to evidence-based medical guidelines is a\nchallenging task with a high degree of variability between healthcare\nproviders. To address this issue, recent work has investigated if generative AI\nand large language models can be leveraged to help clinicians order relevant\nimaging studies for patients. However, it is challenging to ensure that these\ntools are correctly aligned with medical guidelines, such as the American\nCollege of Radiology's Appropriateness Criteria (ACR AC). In this study, we\nintroduce a framework to intelligently leverage language models by recommending\nimaging studies for patient cases that are aligned with evidence-based\nguidelines. We make available a novel dataset of patient \"one-liner\" scenarios\nto power our experiments, and optimize state-of-the-art language models to\nachieve an accuracy on par with clinicians in image ordering. Finally, we\ndemonstrate that our language model-based pipeline can be used as intelligent\nassistants by clinicians to support image ordering workflows and improve the\naccuracy of imaging study ordering according to the ACR AC. Our work\ndemonstrates and validates a strategy to leverage AI-based software to improve\ntrustworthy clinical decision making in alignment with expert evidence-based\nguidelines.", "published": "2024-09-27 23:13:17", "link": "http://arxiv.org/abs/2409.19177v2", "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Building a Chinese Medical Dialogue System: Integrating Large-scale\n  Corpora and Novel Models", "abstract": "The global COVID-19 pandemic underscored major deficiencies in traditional\nhealthcare systems, hastening the advancement of online medical services,\nespecially in medical triage and consultation. However, existing studies face\ntwo main challenges. First, the scarcity of large-scale, publicly available,\ndomain-specific medical datasets due to privacy concerns, with current datasets\nbeing small and limited to a few diseases, limiting the effectiveness of triage\nmethods based on Pre-trained Language Models (PLMs). Second, existing methods\nlack medical knowledge and struggle to accurately understand professional terms\nand expressions in patient-doctor consultations. To overcome these obstacles,\nwe construct the Large-scale Chinese Medical Dialogue Corpora (LCMDC), thereby\naddressing the data shortage in this field. Moreover, we further propose a\nnovel triage system that combines BERT-based supervised learning with prompt\nlearning, as well as a GPT-based medical consultation model. To enhance domain\nknowledge acquisition, we pre-trained PLMs using our self-constructed\nbackground corpus. Experimental results on the LCMDC demonstrate the efficacy\nof our proposed systems.", "published": "2024-09-27 00:01:32", "link": "http://arxiv.org/abs/2410.03521v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Revisiting the Superficial Alignment Hypothesis", "abstract": "The Superficial Alignment Hypothesis posits that almost all of a language\nmodel's abilities and knowledge are learned during pre-training, while\npost-training is about giving a model the right style and format. We re-examine\nthese claims by empirically studying the scaling behavior of post-training with\nincreasing finetuning examples and evaluating them using objective\ntask-specific standardized benchmarks. Through experiments with the Llama-3,\nMistral, and Llama-2 model families of multiple sizes, we observe that, similar\nto the pre-training scaling laws, post-training task performance scales as a\npower law against the number of finetuning examples. This power law\nrelationship holds across a broad array of capabilities, including mathematical\nreasoning, coding, instruction following, and multihop-reasoning. In addition,\nfor tasks like math and multihop reasoning, we observe that a handful of\nexamples merely align the model stylistically but do not saturate performance\non the benchmarks. Model performance is instead correlated with its reasoning\nability and it improves significantly with more examples, illustrating the need\nfor holistic evaluation programs leveraging objective benchmarks in addition to\nmeasurement of alignment to human preferences. We also observe that language\nmodels are not necessarily limited to using knowledge learned during\npre-training. With appropriate post-training, a model's ability to integrate\nnew knowledge greatly improves on downstream tasks like multihop\nquestion-answering. Taken together, these results shed new light on the\nSuperficial Alignment Hypothesis, suggesting that it is, at best, an\nover-simplification.", "published": "2024-09-27 22:14:10", "link": "http://arxiv.org/abs/2410.03717v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large\n  Language Models", "abstract": "Various audio-LLMs (ALLMs) have been explored recently for tackling different\naudio tasks simultaneously using a single, unified model. While existing\nevaluations of ALLMs primarily focus on single-audio tasks, real-world\napplications often involve processing multiple audio streams simultaneously. To\nbridge this gap, we propose the first multi-audio evaluation (MAE) benchmark\nthat consists of 20 datasets from 11 multi-audio tasks encompassing both speech\nand sound scenarios. Comprehensive experiments on MAE demonstrate that the\nexisting ALLMs, while being powerful in comprehending primary audio elements in\nindividual audio inputs, struggling to handle multi-audio scenarios. To this\nend, we propose a novel multi-audio-LLM (MALLM) to capture audio context among\nmultiple similar audios using discriminative learning on our proposed synthetic\ndata. The results demonstrate that the proposed MALLM outperforms all baselines\nand achieves high data efficiency using synthetic data without requiring human\nannotations. The proposed MALLM opens the door for ALLMs towards multi-audio\nprocessing era and brings us closer to replicating human auditory capabilities\nin machines.", "published": "2024-09-27 12:06:53", "link": "http://arxiv.org/abs/2409.18680v3", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A GEN AI Framework for Medical Note Generation", "abstract": "The increasing administrative burden of medical documentation, particularly\nthrough Electronic Health Records (EHR), significantly reduces the time\navailable for direct patient care and contributes to physician burnout. To\naddress this issue, we propose MediNotes, an advanced generative AI framework\ndesigned to automate the creation of SOAP (Subjective, Objective, Assessment,\nPlan) notes from medical conversations. MediNotes integrates Large Language\nModels (LLMs), Retrieval-Augmented Generation (RAG), and Automatic Speech\nRecognition (ASR) to capture and process both text and voice inputs in real\ntime or from recorded audio, generating structured and contextually accurate\nmedical notes. The framework also incorporates advanced techniques like\nQuantized Low-Rank Adaptation (QLoRA) and Parameter-Efficient Fine-Tuning\n(PEFT) for efficient model fine-tuning in resource-constrained environments.\nAdditionally, MediNotes offers a query-based retrieval system, allowing\nhealthcare providers and patients to access relevant medical information\nquickly and accurately. Evaluations using the ACI-BENCH dataset demonstrate\nthat MediNotes significantly improves the accuracy, efficiency, and usability\nof automated medical documentation, offering a robust solution to reduce the\nadministrative burden on healthcare professionals while improving the quality\nof clinical workflows.", "published": "2024-09-27 23:05:02", "link": "http://arxiv.org/abs/2410.01841v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.IR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "VickreyFeedback: Cost-efficient Data Construction for Reinforcement\n  Learning from Human Feedback", "abstract": "This paper addresses the cost-efficiency aspect of Reinforcement Learning\nfrom Human Feedback (RLHF). RLHF leverages datasets of human preferences over\noutputs of large language models (LLM)s to instill human expectations into\nLLMs. Although preference annotation comes with a monetized cost, the economic\nutility of a preference dataset has not been considered by far. What\nexacerbates this situation is that, given complex intransitive or cyclic\nrelationships in preference datasets, existing algorithms for fine-tuning LLMs\nare still far from capturing comprehensive preferences. This raises severe\ncost-efficiency concerns in production environments, where preference data\naccumulate over time. In this paper, we discuss the fine-tuning of LLMs as a\nmonetized economy and introduce an auction mechanism to improve the efficiency\nof preference data collection in dollar terms. We show that introducing an\nauction mechanism can play an essential role in enhancing the cost-efficiency\nof RLHF, while maintaining satisfactory model performance. Experimental results\ndemonstrate that our proposed auction-based protocol is cost-effective for\nfine-tuning LLMs concentrating on high-quality feedback.", "published": "2024-09-27 03:15:07", "link": "http://arxiv.org/abs/2409.18417v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.GT", "econ.GN", "q-fin.EC"], "primary_category": "cs.LG"}
{"title": "XWSB: A Blend System Utilizing XLS-R and WavLM with SLS Classifier\n  detection system for SVDD 2024 Challenge", "abstract": "This paper introduces the model structure used in the SVDD 2024 Challenge.\nThe SVDD 2024 challenge has been introduced this year for the first time.\nSinging voice deepfake detection (SVDD) which faces complexities due to\ninformal speech intonations and varying speech rates. In this paper, we propose\nthe XWSB system, which achieved SOTA per-formance in the SVDD challenge. XWSB\nstands for XLS-R, WavLM, and SLS Blend, representing the integration of these\ntechnologies for the purpose of SVDD. Specifically, we used the best performing\nmodel structure XLS-R&SLS from the ASVspoof DF dataset, and applied SLS to\nWavLM to form the WavLM&SLS structure. Finally, we integrated two models to\nform the XWSB system. Experimental results show that our system demonstrates\nadvanced recognition capabilities in the SVDD challenge, specifically achieving\nan EER of 2.32% in the CtrSVDD track. The code and data can be found at\nhttps://github.com/QiShanZhang/XWSB_for_ SVDD2024.", "published": "2024-09-27 08:55:51", "link": "http://arxiv.org/abs/2409.18558v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The IEEE-IS2 2024 Music Packet Loss Concealment Challenge", "abstract": "We present the IEEE-IS2 2024 Music Packet Loss Concealment Challenge. We\nbegin by detailing the challenge rules, followed by an overview of the provided\nbaseline system, the blind test set, and the evaluation methodology used to\ndetermine the final ranking. This inaugural edition aimed to foster\ncollaboration between researchers and practitioners from the fields of signal\nprocessing, machine learning, and networked music performance, while also\nlaying the groundwork for future advancements in packet loss concealment for\nmusic signals.", "published": "2024-09-27 09:04:46", "link": "http://arxiv.org/abs/2409.18564v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ChildMandarin: A Comprehensive Mandarin Speech Dataset for Young\n  Children Aged 3-5", "abstract": "Automatic speech recognition (ASR) systems have advanced significantly with\nmodels like Whisper, Conformer, and self-supervised frameworks such as Wav2vec\n2.0 and HuBERT. However, developing robust ASR models for young children's\nspeech remains challenging due to differences in pronunciation, tone, and pace\ncompared to adult speech. In this paper, we introduce a new Mandarin speech\ndataset focused on children aged 3 to 5, addressing the scarcity of resources\nin this area. The dataset comprises 41.25 hours of speech with carefully\ncrafted manual transcriptions, collected from 397 speakers across various\nprovinces in China, with balanced gender representation. We provide a\ncomprehensive analysis of speaker demographics, speech duration distribution\nand geographic coverage. Additionally, we evaluate ASR performance on models\ntrained from scratch, such as Conformer, as well as fine-tuned pre-trained\nmodels like HuBERT and Whisper, where fine-tuning demonstrates significant\nperformance improvements. Furthermore, we assess speaker verification (SV) on\nour dataset, showing that, despite the challenges posed by the unique vocal\ncharacteristics of young children, the dataset effectively supports both ASR\nand SV tasks. This dataset is a valuable contribution to Mandarin child speech\nresearch. The dataset is now open-source and freely available for all academic\npurposes on https://github.com/flageval-baai/ChildMandarin.", "published": "2024-09-27 09:42:27", "link": "http://arxiv.org/abs/2409.18584v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-Based Linguistic Feature Extraction for Enhancing Multi-lingual\n  and Low-Resource Text-to-Speech", "abstract": "The difficulty of acquiring abundant, high-quality data, especially in\nmulti-lingual contexts, has sparked interest in addressing low-resource\nscenarios. Moreover, current literature rely on fixed expressions from language\nIDs, which results in the inadequate learning of language representations, and\nthe failure to generate speech in unseen languages. To address these\nchallenges, we propose a novel method that directly extracts linguistic\nfeatures from audio input while effectively filtering out miscellaneous\nacoustic information including speaker-specific attributes like timbre.\nSubjective and objective evaluations affirm the effectiveness of our approach\nfor multi-lingual text-to-speech, and highlight its superiority in low-resource\ntransfer learning for previously unseen language.", "published": "2024-09-27 10:46:09", "link": "http://arxiv.org/abs/2409.18622v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech-Mamba: Long-Context Speech Recognition with Selective State\n  Spaces Models", "abstract": "Current automatic speech recognition systems struggle with modeling long\nspeech sequences due to high quadratic complexity of Transformer-based models.\nSelective state space models such as Mamba has performed well on long-sequence\nmodeling in natural language processing and computer vision tasks. However,\nresearch endeavors in speech technology tasks has been under-explored. We\npropose Speech-Mamba, which incorporates selective state space modeling in\nTransformer neural architectures. Long sequence representations with selective\nstate space models in Speech-Mamba is complemented with lower-level\nrepresentations from Transformer-based modeling. Speech-mamba achieves better\ncapacity to model long-range dependencies, as it scales near-linearly with\nsequence length.", "published": "2024-09-27 11:36:31", "link": "http://arxiv.org/abs/2409.18654v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Text2FX: Harnessing CLAP Embeddings for Text-Guided Audio Effects", "abstract": "This work introduces Text2FX, a method that leverages CLAP embeddings and\ndifferentiable digital signal processing to control audio effects, such as\nequalization and reverberation, using open-vocabulary natural language prompts\n(e.g., \"make this sound in-your-face and bold\"). Text2FX operates without\nretraining any models, relying instead on single-instance optimization within\nthe existing embedding space, thus enabling a flexible, scalable approach to\nopen-vocabulary sound transformations through interpretable and disentangled FX\nmanipulation. We show that CLAP encodes valuable information for controlling\naudio effects and propose two optimization approaches using CLAP to map text to\naudio effect parameters. While we demonstrate with CLAP, this approach is\napplicable to any shared text-audio embedding space. Similarly, while we\ndemonstrate with equalization and reverberation, any differentiable audio\neffect may be controlled. We conduct a listener study with diverse text prompts\nand source audio to evaluate the quality and alignment of these methods with\nhuman perception. Demos and code are available at anniejchu.github.io/text2fx.", "published": "2024-09-27 15:43:53", "link": "http://arxiv.org/abs/2409.18847v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Probing mental health information in speech foundation models", "abstract": "Non-invasive methods for diagnosing mental health conditions, such as speech\nanalysis, offer promising potential in modern medicine. Recent advancements in\nmachine learning, particularly speech foundation models, have shown significant\npromise in detecting mental health states by capturing diverse features. This\nstudy investigates which pretext tasks in these models best transfer to mental\nhealth detection and examines how different model layers encode features\nrelevant to mental health conditions. We also probed the optimal length of\naudio segments and the best pooling strategies to improve detection accuracy.\nUsing the Callyope-GP and Androids datasets, we evaluated the models'\neffectiveness across different languages and speech tasks, aiming to enhance\nthe generalizability of speech-based mental health diagnostics. Our approach\nachieved SOTA scores in depression detection on the Androids dataset.", "published": "2024-09-27 17:38:34", "link": "http://arxiv.org/abs/2409.19042v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MIMII-Gen: Generative Modeling Approach for Simulated Evaluation of\n  Anomalous Sound Detection System", "abstract": "Insufficient recordings and the scarcity of anomalies present significant\nchallenges in developing and validating robust anomaly detection systems for\nmachine sounds. To address these limitations, we propose a novel approach for\ngenerating diverse anomalies in machine sound using a latent diffusion-based\nmodel that integrates an encoder-decoder framework. Our method utilizes the\nFlan-T5 model to encode captions derived from audio file metadata, enabling\nconditional generation through a carefully designed U-Net architecture. This\napproach aids our model in generating audio signals within the EnCodec latent\nspace, ensuring high contextual relevance and quality. We objectively evaluated\nthe quality of our generated sounds using the Fr\\'echet Audio Distance (FAD)\nscore and other metrics, demonstrating that our approach surpasses existing\nmodels in generating reliable machine audio that closely resembles actual\nabnormal conditions. The evaluation of the anomaly detection system using our\ngenerated data revealed a strong correlation, with the area under the curve\n(AUC) score differing by 4.8\\% from the original, validating the effectiveness\nof our generated data. These results demonstrate the potential of our approach\nto enhance the evaluation and robustness of anomaly detection systems across\nvaried and previously unseen conditions. Audio samples can be found at\n\\url{https://hpworkhub.github.io/MIMII-Gen.github.io/}.", "published": "2024-09-27 08:21:31", "link": "http://arxiv.org/abs/2409.18542v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Boosting: Low-Latency Live Speech Enhancement for TWS Earbuds", "abstract": "This paper introduces a speech enhancement solution tailored for true\nwireless stereo (TWS) earbuds on-device usage. The solution was specifically\ndesigned to support conversations in noisy environments, with active noise\ncancellation (ANC) activated. The primary challenges for speech enhancement\nmodels in this context arise from computational complexity that limits\non-device usage and latency that must be less than 3 ms to preserve a live\nconversation. To address these issues, we evaluated several crucial design\nelements, including the network architecture and domain, design of loss\nfunctions, pruning method, and hardware-specific optimization. Consequently, we\ndemonstrated substantial improvements in speech enhancement quality compared\nwith that in baseline models, while simultaneously reducing the computational\ncomplexity and algorithmic latency.", "published": "2024-09-27 12:47:36", "link": "http://arxiv.org/abs/2409.18705v1", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Differential privacy enables fair and accurate AI-based analysis of\n  speech disorders while protecting patient data", "abstract": "Speech pathology has impacts on communication abilities and quality of life.\nWhile deep learning-based models have shown potential in diagnosing these\ndisorders, the use of sensitive data raises critical privacy concerns. Although\ndifferential privacy (DP) has been explored in the medical imaging domain, its\napplication in pathological speech analysis remains largely unexplored despite\nthe equally critical privacy concerns. This study is the first to investigate\nDP's impact on pathological speech data, focusing on the trade-offs between\nprivacy, diagnostic accuracy, and fairness. Using a large, real-world dataset\nof 200 hours of recordings from 2,839 German-speaking participants, we observed\na maximum accuracy reduction of 3.85% when training with DP with high privacy\nlevels. To highlight real-world privacy risks, we demonstrated the\nvulnerability of non-private models to explicit gradient inversion attacks,\nreconstructing identifiable speech samples and showcasing DP's effectiveness in\nmitigating these risks. To generalize our findings across languages and\ndisorders, we validated our approach on a dataset of Spanish-speaking\nParkinson's disease patients, leveraging pretrained models from healthy\nEnglish-speaking datasets, and demonstrated that careful pretraining on\nlarge-scale task-specific datasets can maintain favorable accuracy under DP\nconstraints. A comprehensive fairness analysis revealed minimal gender bias at\nreasonable privacy levels but underscored the need for addressing age-related\ndisparities. Our results establish that DP can balance privacy and utility in\nspeech disorder detection, while highlighting unique challenges in\nprivacy-fairness trade-offs for speech data. This provides a foundation for\nrefining DP methodologies and improving fairness across diverse patient groups\nin real-world deployments.", "published": "2024-09-27 18:25:54", "link": "http://arxiv.org/abs/2409.19078v2", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "From Vision to Audio and Beyond: A Unified Model for Audio-Visual\n  Representation and Generation", "abstract": "Video encompasses both visual and auditory data, creating a perceptually rich\nexperience where these two modalities complement each other. As such, videos\nare a valuable type of media for the investigation of the interplay between\naudio and visual elements. Previous studies of audio-visual modalities\nprimarily focused on either audio-visual representation learning or generative\nmodeling of a modality conditioned on the other, creating a disconnect between\nthese two branches. A unified framework that learns representation and\ngenerates modalities has not been developed yet. In this work, we introduce a\nnovel framework called Vision to Audio and Beyond (VAB) to bridge the gap\nbetween audio-visual representation learning and vision-to-audio generation.\nThe key approach of VAB is that rather than working with raw video frames and\naudio data, VAB performs representation learning and generative modeling within\nlatent spaces. In particular, VAB uses a pre-trained audio tokenizer and an\nimage encoder to obtain audio tokens and visual features, respectively. It then\nperforms the pre-training task of visual-conditioned masked audio token\nprediction. This training strategy enables the model to engage in contextual\nlearning and simultaneous video-to-audio generation. After the pre-training\nphase, VAB employs the iterative-decoding approach to rapidly generate audio\ntokens conditioned on visual features. Since VAB is a unified model, its\nbackbone can be fine-tuned for various audio-visual downstream tasks. Our\nexperiments showcase the efficiency of VAB in producing high-quality audio from\nvideo, and its capability to acquire semantic audio-visual features, leading to\ncompetitive results in audio-visual retrieval and classification.", "published": "2024-09-27 20:26:34", "link": "http://arxiv.org/abs/2409.19132v1", "categories": ["cs.MM", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
