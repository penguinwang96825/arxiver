{"title": "CAMS: An Annotated Corpus for Causal Analysis of Mental Health Issues in\n  Social Media Posts", "abstract": "Research community has witnessed substantial growth in the detection of\nmental health issues and their associated reasons from analysis of social\nmedia. We introduce a new dataset for Causal Analysis of Mental health issues\nin Social media posts (CAMS). Our contributions for causal analysis are\ntwo-fold: causal interpretation and causal categorization. We introduce an\nannotation schema for this task of causal analysis. We demonstrate the efficacy\nof our schema on two different datasets: (i) crawling and annotating 3155\nReddit posts and (ii) re-annotating the publicly available SDCNL dataset of\n1896 instances for interpretable causal analysis. We further combine these into\nthe CAMS dataset and make this resource publicly available along with\nassociated source code: https://github.com/drmuskangarg/CAMS. We present\nexperimental results of models learned from CAMS dataset and demonstrate that a\nclassic Logistic Regression model outperforms the next best (CNN-LSTM) model by\n4.9\\% accuracy.", "published": "2022-07-11 07:38:18", "link": "http://arxiv.org/abs/2207.04674v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GMN: Generative Multi-modal Network for Practical Document Information\n  Extraction", "abstract": "Document Information Extraction (DIE) has attracted increasing attention due\nto its various advanced applications in the real world. Although recent\nliterature has already achieved competitive results, these approaches usually\nfail when dealing with complex documents with noisy OCR results or mutative\nlayouts. This paper proposes Generative Multi-modal Network (GMN) for\nreal-world scenarios to address these problems, which is a robust multi-modal\ngeneration method without predefined label categories. With the carefully\ndesigned spatial encoder and modal-aware mask module, GMN can deal with complex\ndocuments that are hard to serialized into sequential order. Moreover, GMN\ntolerates errors in OCR results and requires no character-level annotation,\nwhich is vital because fine-grained annotation of numerous documents is\nlaborious and even requires annotators with specialized domain knowledge.\nExtensive experiments show that GMN achieves new state-of-the-art performance\non several public DIE datasets and surpasses other methods by a large margin,\nespecially in realistic scenes.", "published": "2022-07-11 08:52:36", "link": "http://arxiv.org/abs/2207.04713v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TArC: Tunisian Arabish Corpus First complete release", "abstract": "In this paper we present the final result of a project on Tunisian Arabic\nencoded in Arabizi, the Latin-based writing system for digital conversations.\nThe project led to the creation of two integrated and independent resources: a\ncorpus and a NLP tool created to annotate the former with various levels of\nlinguistic information: word classification, transliteration, tokenization,\nPOS-tagging, lemmatization. We discuss our choices in terms of computational\nand linguistic methodology and the strategies adopted to improve our results.\nWe report on the experiments performed in order to outline our research path.\nFinally, we explain why we believe in the potential of these resources for both\ncomputational and linguistic researches. Keywords: Tunisian Arabizi, Annotated\nCorpus, Neural Network Architecture", "published": "2022-07-11 11:46:59", "link": "http://arxiv.org/abs/2207.04796v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Embedding Recycling for Language Models", "abstract": "Real-world applications of neural language models often involve running many\ndifferent models over the same corpus. The high computational cost of these\nruns has led to interest in techniques that can reuse the contextualized\nembeddings produced in previous runs to speed training and inference of future\nones. We refer to this approach as embedding recycling (ER). While multiple ER\ntechniques have been proposed, their practical effectiveness is still unknown\nbecause existing evaluations consider very few models and do not adequately\naccount for overhead costs. We perform an extensive evaluation of ER across\neight different models (17 to 900 million parameters) and fourteen tasks in\nEnglish. We show how a simple ER technique that caches activations from an\nintermediate layer of a pretrained model, and learns task-specific adapters on\nthe later layers, is broadly effective. For the best-performing baseline in our\nexperiments (DeBERTa-v2 XL), adding a precomputed cache results in a >90%\nspeedup during training and 87-91% speedup for inference, with negligible\nimpact on accuracy. Our analysis reveals important areas of future work.", "published": "2022-07-11 16:36:14", "link": "http://arxiv.org/abs/2207.04993v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A description of Turkish Discourse Bank 1.2 and an examination of common\n  dependencies in Turkish discourse", "abstract": "We describe Turkish Discourse Bank 1.2, the latest version of a discourse\ncorpus annotated for explicitly or implicitly conveyed discourse relations,\ntheir constitutive units, and senses in the Penn Discourse Treebank style. We\npresent an evaluation of the recently added tokens and examine three commonly\noccurring dependency patterns that hold among the constitutive units of a pair\nof adjacent discourse relations, namely, shared arguments, full embedding and\npartial containment of a discourse relation. We present three major findings:\n(a) implicitly conveyed relations occur more often than explicitly conveyed\nrelations in the data; (b) it is much more common for two adjacent implicit\ndiscourse relations to share an argument than for two adjacent explicit\nrelations to do so; (c) both full embedding and partial containment of\ndiscourse relations are pervasive in the corpus, which can be partly due to\nsubordinator connectives whose preposed subordinate clause tends to be selected\ntogether with the matrix clause rather than being selected alone. Finally, we\nbriefly discuss the implications of our findings for Turkish discourse parsing.", "published": "2022-07-11 16:57:00", "link": "http://arxiv.org/abs/2207.05008v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "UrduFake@FIRE2021: Shared Track on Fake News Identification in Urdu", "abstract": "This study reports the second shared task named as UrduFake@FIRE2021 on\nidentifying fake news detection in Urdu language. This is a binary\nclassification problem in which the task is to classify a given news article\ninto two classes: (i) real news, or (ii) fake news. In this shared task, 34\nteams from 7 different countries (China, Egypt, Israel, India, Mexico,\nPakistan, and UAE) registered to participate in the shared task, 18 teams\nsubmitted their experimental results and 11 teams submitted their technical\nreports. The proposed systems were based on various count-based features and\nused different classifiers as well as neural network architectures. The\nstochastic gradient descent (SGD) algorithm outperformed other classifiers and\nachieved 0.679 F-score.", "published": "2022-07-11 19:15:04", "link": "http://arxiv.org/abs/2207.05144v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Neural Numeric-To-Text Generation From Temporal Personal Health\n  Data", "abstract": "With an increased interest in the production of personal health technologies\ndesigned to track user data (e.g., nutrient intake, step counts), there is now\nmore opportunity than ever to surface meaningful behavioral insights to\neveryday users in the form of natural language. This knowledge can increase\ntheir behavioral awareness and allow them to take action to meet their health\ngoals. It can also bridge the gap between the vast collection of personal\nhealth data and the summary generation required to describe an individual's\nbehavioral tendencies. Previous work has focused on rule-based time-series data\nsummarization methods designed to generate natural language summaries of\ninteresting patterns found within temporal personal health data. We examine\nrecurrent, convolutional, and Transformer-based encoder-decoder models to\nautomatically generate natural language summaries from numeric temporal\npersonal health data. We showcase the effectiveness of our models on real user\nhealth data logged in MyFitnessPal and show that we can automatically generate\nhigh-quality natural language summaries. Our work serves as a first step\ntowards the ambitious goal of automatically generating novel and meaningful\ntemporal summaries from personal health data.", "published": "2022-07-11 21:16:48", "link": "http://arxiv.org/abs/2207.05194v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Large-scale Universal User Representation with Sparse Mixture\n  of Experts", "abstract": "Learning user sequence behaviour embedding is very sophisticated and\nchallenging due to the complicated feature interactions over time and high\ndimensions of user features. Recent emerging foundation models, e.g., BERT and\nits variants, encourage a large body of researchers to investigate in this\nfield. However, unlike natural language processing (NLP) tasks, the parameters\nof user behaviour model come mostly from user embedding layer, which makes most\nexisting works fail in training a universal user embedding of large scale.\nFurthermore, user representations are learned from multiple downstream tasks,\nand the past research work do not address the seesaw phenomenon. In this paper,\nwe propose SUPERMOE, a generic framework to obtain high quality user\nrepresentation from multiple tasks. Specifically, the user behaviour sequences\nare encoded by MoE transformer, and we can thus increase the model capacity to\nbillions of parameters, or even to trillions of parameters. In order to deal\nwith seesaw phenomenon when learning across multiple tasks, we design a new\nloss function with task indicators. We perform extensive offline experiments on\npublic datasets and online experiments on private real-world business\nscenarios. Our approach achieves the best performance over state-of-the-art\nmodels, and the results demonstrate the effectiveness of our framework.", "published": "2022-07-11 06:19:03", "link": "http://arxiv.org/abs/2207.04648v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Topic-Grained Text Representation-based Model for Document Retrieval", "abstract": "Document retrieval enables users to find their required documents accurately\nand quickly. To satisfy the requirement of retrieval efficiency, prevalent deep\nneural methods adopt a representation-based matching paradigm, which saves\nonline matching time by pre-storing document representations offline. However,\nthe above paradigm consumes vast local storage space, especially when storing\nthe document as word-grained representations. To tackle this, we present TGTR,\na Topic-Grained Text Representation-based Model for document retrieval.\nFollowing the representation-based matching paradigm, TGTR stores the document\nrepresentations offline to ensure retrieval efficiency, whereas it\nsignificantly reduces the storage requirements by using novel topicgrained\nrepresentations rather than traditional word-grained. Experimental results\ndemonstrate that compared to word-grained baselines, TGTR is consistently\ncompetitive with them on TREC CAR and MS MARCO in terms of retrieval accuracy,\nbut it requires less than 1/10 of the storage space required by them. Moreover,\nTGTR overwhelmingly surpasses global-grained baselines in terms of retrieval\naccuracy.", "published": "2022-07-11 06:31:21", "link": "http://arxiv.org/abs/2207.04656v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "SummScore: A Comprehensive Evaluation Metric for Summary Quality Based\n  on Cross-Encoder", "abstract": "Text summarization models are often trained to produce summaries that meet\nhuman quality requirements. However, the existing evaluation metrics for\nsummary text are only rough proxies for summary quality, suffering from low\ncorrelation with human scoring and inhibition of summary diversity. To solve\nthese problems, we propose SummScore, a comprehensive metric for summary\nquality evaluation based on CrossEncoder. Firstly, by adopting the\noriginal-summary measurement mode and comparing the semantics of the original\ntext, SummScore gets rid of the inhibition of summary diversity. With the help\nof the text-matching pre-training Cross-Encoder, SummScore can effectively\ncapture the subtle differences between the semantics of summaries. Secondly, to\nimprove the comprehensiveness and interpretability, SummScore consists of four\nfine-grained submodels, which measure Coherence, Consistency, Fluency, and\nRelevance separately. We use semi-supervised multi-rounds of training to\nimprove the performance of our model on extremely limited annotated data.\nExtensive experiments show that SummScore significantly outperforms existing\nevaluation metrics in the above four dimensions in correlation with human\nscoring. We also provide the quality evaluation results of SummScore on 16\nmainstream summarization models for later research.", "published": "2022-07-11 06:47:29", "link": "http://arxiv.org/abs/2207.04660v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "No Language Left Behind: Scaling Human-Centered Machine Translation", "abstract": "Driven by the goal of eradicating language barriers on a global scale,\nmachine translation has solidified itself as a key focus of artificial\nintelligence research today. However, such efforts have coalesced around a\nsmall subset of languages, leaving behind the vast majority of mostly\nlow-resource languages. What does it take to break the 200 language barrier\nwhile ensuring safe, high quality results, all while keeping ethical\nconsiderations in mind? In No Language Left Behind, we took on this challenge\nby first contextualizing the need for low-resource language translation support\nthrough exploratory interviews with native speakers. Then, we created datasets\nand models aimed at narrowing the performance gap between low and high-resource\nlanguages. More specifically, we developed a conditional compute model based on\nSparsely Gated Mixture of Experts that is trained on data obtained with novel\nand effective data mining techniques tailored for low-resource languages. We\npropose multiple architectural and training improvements to counteract\noverfitting while training on thousands of tasks. Critically, we evaluated the\nperformance of over 40,000 different translation directions using a\nhuman-translated benchmark, Flores-200, and combined human evaluation with a\nnovel toxicity benchmark covering all languages in Flores-200 to assess\ntranslation safety. Our model achieves an improvement of 44% BLEU relative to\nthe previous state-of-the-art, laying important groundwork towards realizing a\nuniversal translation system. Finally, we open source all contributions\ndescribed in this work, accessible at\nhttps://github.com/facebookresearch/fairseq/tree/nllb.", "published": "2022-07-11 07:33:36", "link": "http://arxiv.org/abs/2207.04672v3", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Identifying public values and spatial conflicts in urban planning", "abstract": "Identifying the diverse and often competing values of citizens, and resolving\nthe consequent public value conflicts, are of significant importance for\ninclusive and integrated urban development. Scholars have highlighted that\nrelational, value-laden urban space gives rise to many diverse conflicts that\nvary both spatially and temporally. Although notions of public value conflicts\nhave been conceived in theory, there are very few empirical studies that\nidentify such values and their conflicts in urban space. Building on public\nvalue theory and using a case-study mixed-methods approach, this paper proposes\na new approach to empirically investigate public value conflicts in urban\nspace. Using unstructured participatory data of 4,528 citizen contributions\nfrom a Public Participation Geographic Information Systems in Hamburg, Germany,\nnatural language processing and spatial clustering techniques are used to\nidentify areas of potential value conflicts. Four expert workshops assess and\ninterpret these quantitative findings. Integrating both quantitative and\nqualitative results, 19 general public values and a total of 9 archetypical\nconflicts are identified. On the basis of these results, this paper proposes a\nnew conceptual tool of Public Value Spheres that extends the theoretical notion\nof public-value conflicts and helps to further account for the value-laden\nnature of urban space.", "published": "2022-07-11 08:59:43", "link": "http://arxiv.org/abs/2207.04719v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Cross-modal Prototype Driven Network for Radiology Report Generation", "abstract": "Radiology report generation (RRG) aims to describe automatically a radiology\nimage with human-like language and could potentially support the work of\nradiologists, reducing the burden of manual reporting. Previous approaches\noften adopt an encoder-decoder architecture and focus on single-modal feature\nlearning, while few studies explore cross-modal feature interaction. Here we\npropose a Cross-modal PROtotype driven NETwork (XPRONET) to promote cross-modal\npattern learning and exploit it to improve the task of radiology report\ngeneration. This is achieved by three well-designed, fully differentiable and\ncomplementary modules: a shared cross-modal prototype matrix to record the\ncross-modal prototypes; a cross-modal prototype network to learn the\ncross-modal prototypes and embed the cross-modal information into the visual\nand textual features; and an improved multi-label contrastive loss to enable\nand enhance multi-label prototype learning. XPRONET obtains substantial\nimprovements on the IU-Xray and MIMIC-CXR benchmarks, where its performance\nexceeds recent state-of-the-art approaches by a large margin on IU-Xray and\ncomparable performance on MIMIC-CXR.", "published": "2022-07-11 12:29:33", "link": "http://arxiv.org/abs/2207.04818v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Exploring Length Generalization in Large Language Models", "abstract": "The ability to extrapolate from short problem instances to longer ones is an\nimportant form of out-of-distribution generalization in reasoning tasks, and is\ncrucial when learning from datasets where longer problem instances are rare.\nThese include theorem proving, solving quantitative mathematics problems, and\nreading/summarizing novels. In this paper, we run careful empirical studies\nexploring the length generalization capabilities of transformer-based language\nmodels. We first establish that naively finetuning transformers on length\ngeneralization tasks shows significant generalization deficiencies independent\nof model scale. We then show that combining pretrained large language models'\nin-context learning abilities with scratchpad prompting (asking the model to\noutput solution steps before producing an answer) results in a dramatic\nimprovement in length generalization. We run careful failure analyses on each\nof the learning modalities and identify common sources of mistakes that\nhighlight opportunities in equipping language models with the ability to\ngeneralize to longer problems.", "published": "2022-07-11 14:24:38", "link": "http://arxiv.org/abs/2207.04901v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Overview of the Shared Task on Fake News Detection in Urdu at FIRE 2021", "abstract": "Automatic detection of fake news is a highly important task in the\ncontemporary world. This study reports the 2nd shared task called\nUrduFake@FIRE2021 on identifying fake news detection in Urdu. The goal of the\nshared task is to motivate the community to come up with efficient methods for\nsolving this vital problem, particularly for the Urdu language. The task is\nposed as a binary classification problem to label a given news article as a\nreal or a fake news article. The organizers provide a dataset comprising news\nin five domains: (i) Health, (ii) Sports, (iii) Showbiz, (iv) Technology, and\n(v) Business, split into training and testing sets. The training set contains\n1300 annotated news articles -- 750 real news, 550 fake news, while the testing\nset contains 300 news articles -- 200 real, 100 fake news. 34 teams from 7\ndifferent countries (China, Egypt, Israel, India, Mexico, Pakistan, and UAE)\nregistered to participate in the UrduFake@FIRE2021 shared task. Out of those,\n18 teams submitted their experimental results, and 11 of those submitted their\ntechnical reports, which is substantially higher compared to the UrduFake\nshared task in 2020 when only 6 teams submitted their technical reports. The\ntechnical reports submitted by the participants demonstrated different data\nrepresentation techniques ranging from count-based BoW features to word vector\nembeddings as well as the use of numerous machine learning algorithms ranging\nfrom traditional SVM to various neural network architectures including\nTransformers such as BERT and RoBERTa. In this year's competition, the best\nperforming system obtained an F1-macro score of 0.679, which is lower than the\npast year's best result of 0.907 F1-macro. Admittedly, while training sets from\nthe past and the current years overlap to a large extent, the testing set\nprovided this year is completely different.", "published": "2022-07-11 18:58:36", "link": "http://arxiv.org/abs/2207.05133v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LIP: Lightweight Intelligent Preprocessor for meaningful text-to-speech", "abstract": "Existing Text-to-Speech (TTS) systems need to read messages from the email\nwhich may have Personal Identifiable Information (PII) to text messages that\ncan have a streak of emojis and punctuation. 92% of the world's online\npopulation use emoji with more than 10 billion emojis sent everyday. Lack of\npreprocessor leads to messages being read as-is including punctuation and\ninfographics like emoticons. This problem worsens if there is a continuous\nsequence of punctuation/emojis that are quite common in real-world\ncommunications like messaging, Social Networking Site (SNS) interactions, etc.\nIn this work, we aim to introduce a lightweight intelligent preprocessor (LIP)\nthat can enhance the readability of a message before being passed downstream to\nexisting TTS systems. We propose multiple sub-modules including: expanding\ncontraction, censoring swear words, and masking of PII, as part of our\npreprocessor to enhance the readability of text. With a memory footprint of\nonly 3.55 MB and inference time of 4 ms for up to 50-character text, our\nsolution is suitable for real-time deployment. This work being the first of its\nkind, we try to benchmark with an open independent survey, the result of which\nshows 76.5% preference towards LIP enabled TTS engine as compared to standard\nTTS.", "published": "2022-07-11 18:42:52", "link": "http://arxiv.org/abs/2207.07118v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Multi-level Fusion of Wav2vec 2.0 and BERT for Multimodal Emotion\n  Recognition", "abstract": "The research and applications of multimodal emotion recognition have become\nincreasingly popular recently. However, multimodal emotion recognition faces\nthe challenge of lack of data. To solve this problem, we propose to use\ntransfer learning which leverages state-of-the-art pre-trained models including\nwav2vec 2.0 and BERT for this task. Multi-level fusion approaches including\ncoattention-based early fusion and late fusion with the models trained on both\nembeddings are explored. Also, a multi-granularity framework which extracts not\nonly frame-level speech embeddings but also segment-level embeddings including\nphone, syllable and word-level speech embeddings is proposed to further boost\nthe performance. By combining our coattention-based early fusion model and late\nfusion model with the multi-granularity feature extraction framework, we obtain\nresult that outperforms best baseline approaches by 1.3% unweighted accuracy\n(UA) on the IEMOCAP dataset.", "published": "2022-07-11 08:20:53", "link": "http://arxiv.org/abs/2207.04697v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Whois? Deep Author Name Disambiguation using Bibliographic Data", "abstract": "As the number of authors is increasing exponentially over years, the number\nof authors sharing the same names is increasing proportionally. This makes it\nchallenging to assign newly published papers to their adequate authors.\nTherefore, Author Name Ambiguity (ANA) is considered a critical open problem in\ndigital libraries. This paper proposes an Author Name Disambiguation (AND)\napproach that links author names to their real-world entities by leveraging\ntheir co-authors and domain of research. To this end, we use a collection from\nthe DBLP repository that contains more than 5 million bibliographic records\nauthored by around 2.6 million co-authors. Our approach first groups authors\nwho share the same last names and same first name initials. The author within\neach group is identified by capturing the relation with his/her co-authors and\narea of research, which is represented by the titles of the validated\npublications of the corresponding author. To this end, we train a neural\nnetwork model that learns from the representations of the co-authors and\ntitles. We validated the effectiveness of our approach by conducting extensive\nexperiments on a large dataset.", "published": "2022-07-11 11:03:39", "link": "http://arxiv.org/abs/2207.04772v2", "categories": ["cs.DL", "cs.CL", "cs.LG"], "primary_category": "cs.DL"}
{"title": "UM4: Unified Multilingual Multiple Teacher-Student Model for\n  Zero-Resource Neural Machine Translation", "abstract": "Most translation tasks among languages belong to the zero-resource\ntranslation problem where parallel corpora are unavailable. Multilingual neural\nmachine translation (MNMT) enables one-pass translation using shared semantic\nspace for all languages compared to the two-pass pivot translation but often\nunderperforms the pivot-based method. In this paper, we propose a novel method,\nnamed as Unified Multilingual Multiple teacher-student Model for NMT (UM4). Our\nmethod unifies source-teacher, target-teacher, and pivot-teacher models to\nguide the student model for the zero-resource translation. The source teacher\nand target teacher force the student to learn the direct source to target\ntranslation by the distilled knowledge on both source and target sides. The\nmonolingual corpus is further leveraged by the pivot-teacher model to enhance\nthe student model. Experimental results demonstrate that our model of 72\ndirections significantly outperforms previous methods on the WMT benchmark.", "published": "2022-07-11 14:22:59", "link": "http://arxiv.org/abs/2207.04900v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HLT-MT: High-resource Language-specific Training for Multilingual Neural\n  Machine Translation", "abstract": "Multilingual neural machine translation (MNMT) trained in multiple language\npairs has attracted considerable attention due to fewer model parameters and\nlower training costs by sharing knowledge among multiple languages.\nNonetheless, multilingual training is plagued by language interference\ndegeneration in shared parameters because of the negative interference among\ndifferent translation directions, especially on high-resource languages. In\nthis paper, we propose the multilingual translation model with the\nhigh-resource language-specific training (HLT-MT) to alleviate the negative\ninterference, which adopts the two-stage training with the language-specific\nselection mechanism. Specifically, we first train the multilingual model only\nwith the high-resource pairs and select the language-specific modules at the\ntop of the decoder to enhance the translation quality of high-resource\ndirections. Next, the model is further trained on all available corpora to\ntransfer knowledge from high-resource languages (HRLs) to low-resource\nlanguages (LRLs). Experimental results show that HLT-MT outperforms various\nstrong baselines on WMT-10 and OPUS-100 benchmarks. Furthermore, the analytic\nexperiments validate the effectiveness of our method in mitigating the negative\ninterference in multilingual training.", "published": "2022-07-11 14:33:13", "link": "http://arxiv.org/abs/2207.04906v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TweetDIS: A Large Twitter Dataset for Natural Disasters Built using Weak\n  Supervision", "abstract": "Social media is often utilized as a lifeline for communication during natural\ndisasters. Traditionally, natural disaster tweets are filtered from the Twitter\nstream using the name of the natural disaster and the filtered tweets are sent\nfor human annotation. The process of human annotation to create labeled sets\nfor machine learning models is laborious, time consuming, at times inaccurate,\nand more importantly not scalable in terms of size and real-time use. In this\nwork, we curate a silver standard dataset using weak supervision. In order to\nvalidate its utility, we train machine learning models on the weakly supervised\ndata to identify three different types of natural disasters i.e earthquakes,\nhurricanes and floods. Our results demonstrate that models trained on the\nsilver standard dataset achieved performance greater than 90% when classifying\na manually curated, gold-standard dataset. To enable reproducible research and\nadditional downstream utility, we release the silver standard dataset for the\nscientific community.", "published": "2022-07-11 15:30:09", "link": "http://arxiv.org/abs/2207.04947v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Induction enabling Recommending and Trend Analysis: A\n  Corporate Research Community Use Case", "abstract": "A research division plays an important role of driving innovation in an\norganization. Drawing insights, following trends, keeping abreast of new\nresearch, and formulating strategies are increasingly becoming more challenging\nfor both researchers and executives as the amount of information grows in both\nvelocity and volume. In this paper we present a use case of how a corporate\nresearch community, IBM Research, utilizes Semantic Web technologies to induce\na unified Knowledge Graph from both structured and textual data obtained by\nintegrating various applications used by the community related to research\nprojects, academic papers, datasets, achievements and recognition. In order to\nmake the Knowledge Graph more accessible to application developers, we\nidentified a set of common patterns for exploiting the induced knowledge and\nexposed them as APIs. Those patterns were born out of user research which\nidentified the most valuable use cases or user pain points to be alleviated. We\noutline two distinct scenarios: recommendation and analytics for business use.\nWe will discuss these scenarios in detail and provide an empirical evaluation\non entity recommendation specifically. The methodology used and the lessons\nlearned from this work can be applied to other organizations facing similar\nchallenges.", "published": "2022-07-11 20:51:28", "link": "http://arxiv.org/abs/2207.05188v3", "categories": ["cs.AI", "cs.CL", "cs.IR", "68T01, 68T30", "I.2.7; I.2.4; H.5"], "primary_category": "cs.AI"}
{"title": "Language Models (Mostly) Know What They Know", "abstract": "We study whether language models can evaluate the validity of their own\nclaims and predict which questions they will be able to answer correctly. We\nfirst show that larger models are well-calibrated on diverse multiple choice\nand true/false questions when they are provided in the right format. Thus we\ncan approach self-evaluation on open-ended sampling tasks by asking models to\nfirst propose answers, and then to evaluate the probability \"P(True)\" that\ntheir answers are correct. We find encouraging performance, calibration, and\nscaling for P(True) on a diverse array of tasks. Performance at self-evaluation\nfurther improves when we allow models to consider many of their own samples\nbefore predicting the validity of one specific possibility. Next, we\ninvestigate whether models can be trained to predict \"P(IK)\", the probability\nthat \"I know\" the answer to a question, without reference to any particular\nproposed answer. Models perform well at predicting P(IK) and partially\ngeneralize across tasks, though they struggle with calibration of P(IK) on new\ntasks. The predicted P(IK) probabilities also increase appropriately in the\npresence of relevant source materials in the context, and in the presence of\nhints towards the solution of mathematical word problems. We hope these\nobservations lay the groundwork for training more honest models, and for\ninvestigating how honesty generalizes to cases where models are trained on\nobjectives other than the imitation of human writing.", "published": "2022-07-11 22:59:39", "link": "http://arxiv.org/abs/2207.05221v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bootstrapping a User-Centered Task-Oriented Dialogue System", "abstract": "We present TacoBot, a task-oriented dialogue system built for the inaugural\nAlexa Prize TaskBot Challenge, which assists users in completing multi-step\ncooking and home improvement tasks. TacoBot is designed with a user-centered\nprinciple and aspires to deliver a collaborative and accessible dialogue\nexperience. Towards that end, it is equipped with accurate language\nunderstanding, flexible dialogue management, and engaging response generation.\nFurthermore, TacoBot is backed by a strong search engine and an automated\nend-to-end test suite. In bootstrapping the development of TacoBot, we explore\na series of data augmentation strategies to train advanced neural language\nprocessing models and continuously improve the dialogue experience with\ncollected real conversations. At the end of the semifinals, TacoBot achieved an\naverage rating of 3.55/5.0.", "published": "2022-07-11 23:32:54", "link": "http://arxiv.org/abs/2207.05223v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PoeticTTS -- Controllable Poetry Reading for Literary Studies", "abstract": "Speech synthesis for poetry is challenging due to specific intonation\npatterns inherent to poetic speech. In this work, we propose an approach to\nsynthesise poems with almost human like naturalness in order to enable literary\nscholars to systematically examine hypotheses on the interplay between text,\nspoken realisation, and the listener's perception of poems. To meet these\nspecial requirements for literary studies, we resynthesise poems by cloning\nprosodic values from a human reference recitation, and afterwards make use of\nfine-grained prosody control to manipulate the synthetic speech in a\nhuman-in-the-loop setting to alter the recitation w.r.t. specific phenomena. We\nfind that finetuning our TTS model on poetry captures poetic intonation\npatterns to a large extent which is beneficial for prosody cloning and\nmanipulation and verify the success of our approach both in an objective\nevaluation as well as in human studies.", "published": "2022-07-11 13:15:27", "link": "http://arxiv.org/abs/2207.05549v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaker consistency loss and step-wise optimization for semi-supervised\n  joint training of TTS and ASR using unpaired text data", "abstract": "In this paper, we investigate the semi-supervised joint training of text to\nspeech (TTS) and automatic speech recognition (ASR), where a small amount of\npaired data and a large amount of unpaired text data are available.\nConventional studies form a cycle called the TTS-ASR pipeline, where the\nmultispeaker TTS model synthesizes speech from text with a reference speech and\nthe ASR model reconstructs the text from the synthesized speech, after which\nboth models are trained with a cycle-consistency loss. However, the synthesized\nspeech does not reflect the speaker characteristics of the reference speech and\nthe synthesized speech becomes overly easy for the ASR model to recognize after\ntraining. This not only decreases the TTS model quality but also limits the ASR\nmodel improvement. To solve this problem, we propose improving the\ncycleconsistency-based training with a speaker consistency loss and step-wise\noptimization. The speaker consistency loss brings the speaker characteristics\nof the synthesized speech closer to that of the reference speech. In the\nstep-wise optimization, we first freeze the parameter of the TTS model before\nboth models are trained to avoid over-adaptation of the TTS model to the ASR\nmodel. Experimental results demonstrate the efficacy of the proposed method.", "published": "2022-07-11 06:47:00", "link": "http://arxiv.org/abs/2207.04659v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The HCCL System for the NIST SRE21", "abstract": "This paper describes the systems developed by the HCCL team for the NIST 2021\nspeaker recognition evaluation (NIST SRE21).We first explore various\nstate-of-the-art speaker embedding extractors combined with a novel circle loss\nto obtain discriminative deep speaker embeddings. Considering that\ncross-channel and cross-linguistic speaker recognition are the key challenges\nof SRE21, we introduce several techniques to reduce the cross-domain mismatch.\nSpecifically, Codec and speech enhancement are directly applied to the raw\nspeech to eliminate the codecs and the environment noise mismatch. We denote\nthe methods that work directly on speech to eliminate the relatively explicit\nmismatches collectively as data adaptation methods. Experiments show that data\nadaption methods achieve 15\\% improvements over our baseline. Furthermore, some\npopular back-ends domain adaptation algorithms are deployed on speaker\nembeddings to alleviate speaker performance degradation caused by the implicit\nmismatch. Score calibration is a major failure for us in SRE21. The reason is\nthat score calibration with too many parameters easily lead to overfitting\nproblems.", "published": "2022-07-11 07:42:26", "link": "http://arxiv.org/abs/2207.04676v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "pMCT: Patched Multi-Condition Training for Robust Speech Recognition", "abstract": "We propose a novel Patched Multi-Condition Training (pMCT) method for robust\nAutomatic Speech Recognition (ASR). pMCT employs Multi-condition Audio\nModification and Patching (MAMP) via mixing {\\it patches} of the same utterance\nextracted from clean and distorted speech. Training using patch-modified\nsignals improves robustness of models in noisy reverberant scenarios. Our\nproposed pMCT is evaluated on the LibriSpeech dataset showing improvement over\nusing vanilla Multi-Condition Training (MCT). For analyses on robust ASR, we\nemployed pMCT on the VOiCES dataset which is a noisy reverberant dataset\ncreated using utterances from LibriSpeech. In the analyses, pMCT achieves 23.1%\nrelative WER reduction compared to the MCT.", "published": "2022-07-11 15:34:42", "link": "http://arxiv.org/abs/2207.04949v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DelightfulTTS 2: End-to-End Speech Synthesis with Adversarial\n  Vector-Quantized Auto-Encoders", "abstract": "Current text to speech (TTS) systems usually leverage a cascaded acoustic\nmodel and vocoder pipeline with mel-spectrograms as the intermediate\nrepresentations, which suffer from two limitations: 1) the acoustic model and\nvocoder are separately trained instead of jointly optimized, which incurs\ncascaded errors; 2) the intermediate speech representations (e.g.,\nmel-spectrogram) are pre-designed and lose phase information, which are\nsub-optimal. To solve these problems, in this paper, we develop DelightfulTTS\n2, a new end-to-end speech synthesis system with automatically learned speech\nrepresentations and jointly optimized acoustic model and vocoder. Specifically,\n1) we propose a new codec network based on vector-quantized auto-encoders with\nadversarial training (VQ-GAN) to extract intermediate frame-level speech\nrepresentations (instead of traditional representations like mel-spectrograms)\nand reconstruct speech waveform; 2) we jointly optimize the acoustic model\n(based on DelightfulTTS) and the vocoder (the decoder of VQ-GAN), with an\nauxiliary loss on the acoustic model to predict intermediate speech\nrepresentations. Experiments show that DelightfulTTS 2 achieves a CMOS gain\n+0.14 over DelightfulTTS, and more method analyses further verify the\neffectiveness of the developed system.", "published": "2022-07-11 06:15:45", "link": "http://arxiv.org/abs/2207.04646v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Speaker Anonymization with Phonetic Intermediate Representations", "abstract": "In this work, we propose a speaker anonymization pipeline that leverages high\nquality automatic speech recognition and synthesis systems to generate speech\nconditioned on phonetic transcriptions and anonymized speaker embeddings. Using\nphones as the intermediate representation ensures near complete elimination of\nspeaker identity information from the input while preserving the original\nphonetic content as much as possible. Our experimental results on LibriSpeech\nand VCTK corpora reveal two key findings: 1) although automatic speech\nrecognition produces imperfect transcriptions, our neural speech synthesis\nsystem can handle such errors, making our system feasible and robust, and 2)\ncombining speaker embeddings from different resources is beneficial and their\nappropriate normalization is crucial. Overall, our final best system\noutperforms significantly the baselines provided in the Voice Privacy Challenge\n2020 in terms of privacy robustness against a lazy-informed attacker while\nmaintaining high intelligibility and naturalness of the anonymized speech.", "published": "2022-07-11 13:02:08", "link": "http://arxiv.org/abs/2207.04834v1", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Online Continual Learning of End-to-End Speech Recognition Models", "abstract": "Continual Learning, also known as Lifelong Learning, aims to continually\nlearn from new data as it becomes available. While prior research on continual\nlearning in automatic speech recognition has focused on the adaptation of\nmodels across multiple different speech recognition tasks, in this paper we\npropose an experimental setting for \\textit{online continual learning} for\nautomatic speech recognition of a single task. Specifically focusing on the\ncase where additional training data for the same task becomes available\nincrementally over time, we demonstrate the effectiveness of performing\nincremental model updates to end-to-end speech recognition models with an\nonline Gradient Episodic Memory (GEM) method. Moreover, we show that with\nonline continual learning and a selective sampling strategy, we can maintain an\naccuracy that is similar to retraining a model from scratch while requiring\nsignificantly lower computation costs. We have also verified our method with\nself-supervised learning (SSL) features.", "published": "2022-07-11 05:35:06", "link": "http://arxiv.org/abs/2207.05071v1", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Audio-Visual Segmentation", "abstract": "We propose to explore a new problem called audio-visual segmentation (AVS),\nin which the goal is to output a pixel-level map of the object(s) that produce\nsound at the time of the image frame. To facilitate this research, we construct\nthe first audio-visual segmentation benchmark (AVSBench), providing pixel-wise\nannotations for the sounding objects in audible videos. Two settings are\nstudied with this benchmark: 1) semi-supervised audio-visual segmentation with\na single sound source and 2) fully-supervised audio-visual segmentation with\nmultiple sound sources. To deal with the AVS problem, we propose a novel method\nthat uses a temporal pixel-wise audio-visual interaction module to inject audio\nsemantics as guidance for the visual segmentation process. We also design a\nregularization loss to encourage the audio-visual mapping during training.\nQuantitative and qualitative experiments on the AVSBench compare our approach\nto several existing methods from related tasks, demonstrating that the proposed\nmethod is promising for building a bridge between the audio and pixel-wise\nvisual semantics. Code is available at https://github.com/OpenNLPLab/AVSBench.", "published": "2022-07-11 17:50:36", "link": "http://arxiv.org/abs/2207.05042v3", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
