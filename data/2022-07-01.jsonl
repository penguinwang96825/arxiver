{"title": "Affordance Extraction with an External Knowledge Database for Text-Based\n  Simulated Environments", "abstract": "Text-based simulated environments have proven to be a valid testbed for\nmachine learning approaches. The process of affordance extraction can be used\nto generate possible actions for interaction within such an environment. In\nthis paper the capabilities and challenges for utilizing external knowledge\ndatabases (in particular ConceptNet) in the process of affordance extraction\nare studied. An algorithm for automated affordance extraction is introduced and\nevaluated on the Interactive Fiction (IF) platforms TextWorld and Jericho. For\nthis purpose, the collected affordances are translated into text commands for\nIF agents. To probe the quality of the automated evaluation process, an\nadditional human baseline study is conducted. The paper illustrates that,\ndespite some challenges, external databases can in principle be used for\naffordance extraction. The paper concludes with recommendations for further\nmodification and improvement of the process.", "published": "2022-07-01 08:39:18", "link": "http://arxiv.org/abs/2207.00265v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conditional Generation with a Question-Answering Blueprint", "abstract": "The ability to convey relevant and faithful information is critical for many\ntasks in conditional generation and yet remains elusive for neural seq-to-seq\nmodels whose outputs often reveal hallucinations and fail to correctly cover\nimportant details. In this work, we advocate planning as a useful intermediate\nrepresentation for rendering conditional generation less opaque and more\ngrounded. Our work proposes a new conceptualization of text plans as a sequence\nof question-answer (QA) pairs. We enhance existing datasets (e.g., for\nsummarization) with a QA blueprint operating as a proxy for both content\nselection (i.e.,~what to say) and planning (i.e.,~in what order). We obtain\nblueprints automatically by exploiting state-of-the-art question generation\ntechnology and convert input-output pairs into input-blueprint-output tuples.\nWe develop Transformer-based models, each varying in how they incorporate the\nblueprint in the generated output (e.g., as a global plan or iteratively).\nEvaluation across metrics and datasets demonstrates that blueprint models are\nmore factual than alternatives which do not resort to planning and allow\ntighter control of the generation output.", "published": "2022-07-01 13:10:19", "link": "http://arxiv.org/abs/2207.00397v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reduce Indonesian Vocabularies with an Indonesian Sub-word Separator", "abstract": "Indonesian is an agglutinative language since it has a compounding process of\nword-formation. Therefore, the translation model of this language requires a\nmechanism that is even lower than the word level, referred to as the sub-word\nlevel. This compounding process leads to a rare word problem since the number\nof vocabulary explodes. We propose a strategy to address the unique word\nproblem of the neural machine translation (NMT) system, which uses Indonesian\nas a pair language. Our approach uses a rule-based method to transform a word\ninto its roots and accompanied affixes to retain its meaning and context. Using\na rule-based algorithm has more advantages: it does not require corpus data but\nonly applies the standard Indonesian rules. Our experiments confirm that this\nmethod is practical. It reduces the number of vocabulary significantly up to\n57\\%, and on the English to Indonesian translation, this strategy provides an\nimprovement of up to 5 BLEU points over a similar NMT system that does not use\nthis technique.", "published": "2022-07-01 17:09:53", "link": "http://arxiv.org/abs/2207.00552v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Is neural language acquisition similar to natural? A chronological\n  probing study", "abstract": "The probing methodology allows one to obtain a partial representation of\nlinguistic phenomena stored in the inner layers of the neural network, using\nexternal classifiers and statistical analysis. Pre-trained transformer-based\nlanguage models are widely used both for natural language understanding (NLU)\nand natural language generation (NLG) tasks making them most commonly used for\ndownstream applications. However, little analysis was carried out, whether the\nmodels were pre-trained enough or contained knowledge correlated with\nlinguistic theory. We are presenting the chronological probing study of\ntransformer English models such as MultiBERT and T5. We sequentially compare\nthe information about the language learned by the models in the process of\ntraining on corpora. The results show that 1) linguistic information is\nacquired in the early stages of training 2) both language models demonstrate\ncapabilities to capture various features from various levels of language,\nincluding morphology, syntax, and even discourse, while they also can\ninconsistently fail on tasks that are perceived as easy. We also introduce the\nopen-source framework for chronological probing research, compatible with other\ntransformer-based models.\nhttps://github.com/EkaterinaVoloshina/chronological_probing", "published": "2022-07-01 17:24:11", "link": "http://arxiv.org/abs/2207.00560v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Understanding-Oriented Robust Machine Reading Comprehension Model", "abstract": "Although existing machine reading comprehension models are making rapid\nprogress on many datasets, they are far from robust. In this paper, we propose\nan understanding-oriented machine reading comprehension model to address three\nkinds of robustness issues, which are over sensitivity, over stability and\ngeneralization. Specifically, we first use a natural language inference module\nto help the model understand the accurate semantic meanings of input questions\nso as to address the issues of over sensitivity and over stability. Then in the\nmachine reading comprehension module, we propose a memory-guided multi-head\nattention method that can further well understand the semantic meanings of\ninput questions and passages. Third, we propose a multilanguage learning\nmechanism to address the issue of generalization. Finally, these modules are\nintegrated with a multi-task learning based method. We evaluate our model on\nthree benchmark datasets that are designed to measure models robustness,\nincluding DuReader (robust) and two SQuAD-related datasets. Extensive\nexperiments show that our model can well address the mentioned three kinds of\nrobustness issues. And it achieves much better results than the compared\nstate-of-the-art models on all these datasets under different evaluation\nmetrics, even under some extreme and unfair evaluations. The source code of our\nwork is available at: https://github.com/neukg/RobustMRC.", "published": "2022-07-01 03:32:02", "link": "http://arxiv.org/abs/2207.00187v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pile of Law: Learning Responsible Data Filtering from the Law and a\n  256GB Open-Source Legal Dataset", "abstract": "One concern with the rise of large language models lies with their potential\nfor significant harm, particularly from pretraining on biased, obscene,\ncopyrighted, and private information. Emerging ethical approaches have\nattempted to filter pretraining material, but such approaches have been ad hoc\nand failed to take context into account. We offer an approach to filtering\ngrounded in law, which has directly addressed the tradeoffs in filtering\nmaterial. First, we gather and make available the Pile of Law, a 256GB (and\ngrowing) dataset of open-source English-language legal and administrative data,\ncovering court opinions, contracts, administrative rules, and legislative\nrecords. Pretraining on the Pile of Law may help with legal tasks that have the\npromise to improve access to justice. Second, we distill the legal norms that\ngovernments have developed to constrain the inclusion of toxic or private\ncontent into actionable lessons for researchers and discuss how our dataset\nreflects these norms. Third, we show how the Pile of Law offers researchers the\nopportunity to learn such filtering rules directly from the data, providing an\nexciting new research direction in model-based processing.", "published": "2022-07-01 06:25:15", "link": "http://arxiv.org/abs/2207.00220v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Vers la compr\u00e9hension automatique de la parole bout-en-bout \u00e0\n  moindre effort", "abstract": "Recent advances in spoken language understanding benefited from\nSelf-Supervised models trained on large speech corpora. For French, the\nLeBenchmark project has made such models available and has led to impressive\nprogress on several tasks including spoken language understanding. These\nadvances have a non-negligible cost in terms of computation time and energy\nconsumption. In this paper, we compare several learning strategies aiming at\nreducing such cost while keeping competitive performances. The experiments are\nperformed on the MEDIA corpus, and show that it is possible to reduce the\nlearning cost while maintaining state-of-the-art performances.", "published": "2022-07-01 11:29:59", "link": "http://arxiv.org/abs/2207.00349v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Swiss German Speech to Text system evaluation", "abstract": "We present an in-depth evaluation of four commercially available\nSpeech-to-Text (STT) systems for Swiss German. The systems are anonymized and\nreferred to as system a-d in this report. We compare the four systems to our\nSTT model, referred to as FHNW from hereon after, and provide details on how we\ntrained our model. To evaluate the models, we use two STT datasets from\ndifferent domains. The Swiss Parliament Corpus (SPC) test set and a private\ndataset in the news domain with an even distribution across seven dialect\nregions. We provide a detailed error analysis to detect the three systems'\nstrengths and weaknesses. This analysis is limited by the characteristics of\nthe two test sets. Our model scored the highest bilingual evaluation understudy\n(BLEU) on both datasets. On the SPC test set, we obtain a BLEU score of 0.607,\nwhereas the best commercial system reaches a BLEU score of 0.509. On our\nprivate test set, we obtain a BLEU score of 0.722 and the best commercial\nsystem a BLEU score of 0.568.", "published": "2022-07-01 13:43:06", "link": "http://arxiv.org/abs/2207.00412v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How trial-to-trial learning shapes mappings in the mental lexicon:\n  Modelling Lexical Decision with Linear Discriminative Learning", "abstract": "Trial-to-trial effects have been found in a number of studies, indicating\nthat processing a stimulus influences responses in subsequent trials. A special\ncase are priming effects which have been modelled successfully with\nerror-driven learning (Marsolek, 2008), implying that participants are\ncontinuously learning during experiments. This study investigates whether\ntrial-to-trial learning can be detected in an unprimed lexical decision\nexperiment. We used the Discriminative Lexicon Model (DLM; Baayen et al.,\n2019), a model of the mental lexicon with meaning representations from\ndistributional semantics, which models error-driven incremental learning with\nthe Widrow-Hoff rule. We used data from the British Lexicon Project (BLP;\nKeuleers et al., 2012) and simulated the lexical decision experiment with the\nDLM on a trial-by-trial basis for each subject individually. Then, reaction\ntimes were predicted with Generalised Additive Models (GAMs), using measures\nderived from the DLM simulations as predictors. We extracted measures from two\nsimulations per subject (one with learning updates between trials and one\nwithout), and used them as input to two GAMs. Learning-based models showed\nbetter model fit than the non-learning ones for the majority of subjects. Our\nmeasures also provide insights into lexical processing and individual\ndifferences. This demonstrates the potential of the DLM to model behavioural\ndata and leads to the conclusion that trial-to-trial learning can indeed be\ndetected in unprimed lexical decision. Our results support the possibility that\nour lexical knowledge is subject to continuous changes.", "published": "2022-07-01 13:49:30", "link": "http://arxiv.org/abs/2207.00430v3", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Reinforcement Learning of Multi-Domain Dialog Policies Via Action\n  Embeddings", "abstract": "Learning task-oriented dialog policies via reinforcement learning typically\nrequires large amounts of interaction with users, which in practice renders\nsuch methods unusable for real-world applications. In order to reduce the data\nrequirements, we propose to leverage data from across different dialog domains,\nthereby reducing the amount of data required from each given domain. In\nparticular, we propose to learn domain-agnostic action embeddings, which\ncapture general-purpose structure that informs the system how to act given the\ncurrent dialog context, and are then specialized to a specific domain. We show\nhow this approach is capable of learning with significantly less interaction\nwith users, with a reduction of 35% in the number of dialogs required to learn,\nand to a higher level of proficiency than training separate policies for each\ndomain on a set of simulated domains.", "published": "2022-07-01 14:49:05", "link": "http://arxiv.org/abs/2207.00468v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Panning for gold: Lessons learned from the platform-agnostic automated\n  detection of political content in textual data", "abstract": "The growing availability of data about online information behaviour enables\nnew possibilities for political communication research. However, the volume and\nvariety of these data makes them difficult to analyse and prompts the need for\ndeveloping automated content approaches relying on a broad range of natural\nlanguage processing techniques (e.g. machine learning- or neural network-based\nones). In this paper, we discuss how these techniques can be used to detect\npolitical content across different platforms. Using three validation datasets,\nwhich include a variety of political and non-political textual documents from\nonline platforms, we systematically compare the performance of three groups of\ndetection techniques relying on dictionaries, supervised machine learning, or\nneural networks. We also examine the impact of different modes of data\npreprocessing (e.g. stemming and stopword removal) on the low-cost\nimplementations of these techniques using a large set (n = 66) of detection\nmodels. Our results show the limited impact of preprocessing on model\nperformance, with the best results for less noisy data being achieved by neural\nnetwork- and machine-learning-based models, in contrast to the more robust\nperformance of dictionary-based models on noisy data.", "published": "2022-07-01 15:23:23", "link": "http://arxiv.org/abs/2207.00489v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Interactive Learning from Natural Language and Demonstrations using\n  Signal Temporal Logic", "abstract": "Natural language is an intuitive way for humans to communicate tasks to a\nrobot. While natural language (NL) is ambiguous, real world tasks and their\nsafety requirements need to be communicated unambiguously. Signal Temporal\nLogic (STL) is a formal logic that can serve as a versatile, expressive, and\nunambiguous formal language to describe robotic tasks. On one hand, existing\nwork in using STL for the robotics domain typically requires end-users to\nexpress task specifications in STL, a challenge for non-expert users.\n  On the other, translating from NL to STL specifications is currently\nrestricted to specific fragments. In this work, we propose DIALOGUESTL, an\ninteractive approach for learning correct and concise STL formulas from (often)\nambiguous NL descriptions. We use a combination of semantic parsing,\npre-trained transformer-based language models, and user-in-the-loop\nclarifications aided by a small number of user demonstrations to predict the\nbest STL formula to encode NL task descriptions. An advantage of mapping NL to\nSTL is that there has been considerable recent work on the use of reinforcement\nlearning (RL) to identify control policies for robots. We show we can use Deep\nQ-Learning techniques to learn optimal policies from the learned STL\nspecifications. We demonstrate that DIALOGUESTL is efficient, scalable, and\nrobust, and has high accuracy in predicting the correct STL formula with a few\nnumber of demonstrations and a few interactions with an oracle user.", "published": "2022-07-01 19:08:43", "link": "http://arxiv.org/abs/2207.00627v1", "categories": ["cs.FL", "cs.CL"], "primary_category": "cs.FL"}
{"title": "Improving Low-Resource Speech Recognition with Pretrained Speech Models:\n  Continued Pretraining vs. Semi-Supervised Training", "abstract": "Self-supervised Transformer based models, such as wav2vec 2.0 and HuBERT,\nhave produced significant improvements over existing approaches to automatic\nspeech recognition (ASR). This is evident in the performance of the wav2vec 2.0\nbased pretrained XLSR-53 model across many languages when fine-tuned with\navailable labeled data. However, the performance from finetuning these models\ncan be dependent on the amount of in-language or similar-to-in-language data\nincluded in the pretraining dataset. In this paper we investigate continued\npretraining (CoPT) with unlabeled in-language audio data on the XLSR-53\npretrained model in several low-resource languages. CoPT is more\ncomputationally efficient than semi-supervised training (SST), the standard\napproach of utilizing unlabeled data in ASR, since it omits the need for\npseudo-labeling of the unlabeled data. We show CoPT results in word error rates\n(WERs), equal to or slightly better than using SST. In addition, we show that\nusing the CoPT model for pseudo-labeling, and using these labels in SST,\nresults in further improvements in WER.", "published": "2022-07-01 21:02:51", "link": "http://arxiv.org/abs/2207.00659v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VL-CheckList: Evaluating Pre-trained Vision-Language Models with\n  Objects, Attributes and Relations", "abstract": "Vision-Language Pretraining (VLP) models have recently successfully\nfacilitated many cross-modal downstream tasks. Most existing works evaluated\ntheir systems by comparing the fine-tuned downstream task performance. However,\nonly average downstream task accuracy provides little information about the\npros and cons of each VLP method, let alone provides insights on how the\ncommunity can improve the systems in the future. Inspired by the CheckList for\ntesting natural language processing, we exploit VL-CheckList, a novel framework\nto understand the capabilities of VLP models. The proposed method divides the\nimage-texting ability of a VLP model into three categories: objects,\nattributes, and relations, and uses a novel taxonomy to further break down\nthese three aspects. We conduct comprehensive studies to analyze seven recently\npopular VLP models via the proposed framework. Results confirm the\neffectiveness of the proposed method by revealing fine-grained differences\namong the compared models that were not visible from downstream task-only\nevaluation. Further results show promising research direction in building\nbetter VLP models. Our data and code are available at:\nhttps://github.com/om-ai-lab/VL-CheckList.", "published": "2022-07-01 06:25:53", "link": "http://arxiv.org/abs/2207.00221v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Multi-features based Semantic Augmentation Networks for Named Entity\n  Recognition in Threat Intelligence", "abstract": "Extracting cybersecurity entities such as attackers and vulnerabilities from\nunstructured network texts is an important part of security analysis. However,\nthe sparsity of intelligence data resulted from the higher frequency variations\nand the randomness of cybersecurity entity names makes it difficult for current\nmethods to perform well in extracting security-related concepts and entities.\nTo this end, we propose a semantic augmentation method which incorporates\ndifferent linguistic features to enrich the representation of input tokens to\ndetect and classify the cybersecurity names over unstructured text. In\nparticular, we encode and aggregate the constituent feature, morphological\nfeature and part of speech feature for each input token to improve the\nrobustness of the method. More than that, a token gets augmented semantic\ninformation from its most similar K words in cybersecurity domain corpus where\nan attentive module is leveraged to weigh differences of the words, and from\ncontextual clues based on a large-scale general field corpus. We have conducted\nexperiments on the cybersecurity datasets DNRTI and MalwareTextDB, and the\nresults demonstrate the effectiveness of the proposed method.", "published": "2022-07-01 06:55:12", "link": "http://arxiv.org/abs/2207.00232v1", "categories": ["cs.CR", "cs.CL", "cs.IR"], "primary_category": "cs.CR"}
{"title": "Toward Low-Cost End-to-End Spoken Language Understanding", "abstract": "Recent advances in spoken language understanding benefited from\nSelf-Supervised models trained on large speech corpora. For French, the\nLeBenchmark project has made such models available and has led to impressive\nprogress on several tasks including spoken language understanding. These\nadvances have a non-negligible cost in terms of computation time and energy\nconsumption. In this paper, we compare several learning strategies trying to\nreduce such cost while keeping competitive performance. At the same time we\npropose an extensive analysis where we measure the cost of our models in terms\nof training time and electric energy consumption, hopefully promoting a\ncomprehensive evaluation procedure. The experiments are performed on the FSC\nand MEDIA corpora, and show that it is possible to reduce the learning cost\nwhile maintaining state-of-the-art performance and using SSL models.", "published": "2022-07-01 11:40:53", "link": "http://arxiv.org/abs/2207.00352v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Assessing the Effects of Hyperparameters on Knowledge Graph Embedding\n  Quality", "abstract": "Embedding knowledge graphs into low-dimensional spaces is a popular method\nfor applying approaches, such as link prediction or node classification, to\nthese databases. This embedding process is very costly in terms of both\ncomputational time and space. Part of the reason for this is the optimisation\nof hyperparameters, which involves repeatedly sampling, by random, guided, or\nbrute-force selection, from a large hyperparameter space and testing the\nresulting embeddings for their quality. However, not all hyperparameters in\nthis search space will be equally important. In fact, with prior knowledge of\nthe relative importance of the hyperparameters, some could be eliminated from\nthe search altogether without significantly impacting the overall quality of\nthe outputted embeddings. To this end, we ran a Sobol sensitivity analysis to\nevaluate the effects of tuning different hyperparameters on the variance of\nembedding quality. This was achieved by performing thousands of embedding\ntrials, each time measuring the quality of embeddings produced by different\nhyperparameter configurations. We regressed the embedding quality on those\nhyperparameter configurations, using this model to generate Sobol sensitivity\nindices for each of the hyperparameters. By evaluating the correlation between\nSobol indices, we find substantial variability in the hyperparameter\nsensitivities between knowledge graphs, with differing dataset characteristics\nbeing the probable cause of these inconsistencies. As an additional\ncontribution of this work we identify several relations in the UMLS knowledge\ngraph that may cause data leakage via inverse relations, and derive and present\nUMLS-43, a leakage-robust variant of that graph.", "published": "2022-07-01 14:53:16", "link": "http://arxiv.org/abs/2207.00473v3", "categories": ["cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.AI"}
{"title": "FitHuBERT: Going Thinner and Deeper for Knowledge Distillation of Speech\n  Self-Supervised Learning", "abstract": "Large-scale speech self-supervised learning (SSL) has emerged to the main\nfield of speech processing, however, the problem of computational cost arising\nfrom its vast size makes a high entry barrier to academia. In addition,\nexisting distillation techniques of speech SSL models compress the model by\nreducing layers, which induces performance degradation in linguistic pattern\nrecognition tasks such as phoneme recognition (PR). In this paper, we propose\nFitHuBERT, which makes thinner in dimension throughout almost all model\ncomponents and deeper in layer compared to prior speech SSL distillation works.\nMoreover, we employ a time-reduction layer to speed up inference time and\npropose a method of hint-based distillation for less performance degradation.\nOur method reduces the model to 23.8% in size and 35.9% in inference time\ncompared to HuBERT. Also, we achieve 12.1% word error rate and 13.3% phoneme\nerror rate on the SUPERB benchmark which is superior than prior work.", "published": "2022-07-01 17:11:23", "link": "http://arxiv.org/abs/2207.00555v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Building African Voices", "abstract": "Modern speech synthesis techniques can produce natural-sounding speech given\nsufficient high-quality data and compute resources. However, such data is not\nreadily available for many languages. This paper focuses on speech synthesis\nfor low-resourced African languages, from corpus creation to sharing and\ndeploying the Text-to-Speech (TTS) systems. We first create a set of\ngeneral-purpose instructions on building speech synthesis systems with minimum\ntechnological resources and subject-matter expertise. Next, we create new\ndatasets and curate datasets from \"found\" data (existing recordings) through a\nparticipatory approach while considering accessibility, quality, and breadth.\nWe demonstrate that we can develop synthesizers that generate intelligible\nspeech with 25 minutes of created speech, even when recorded in suboptimal\nenvironments. Finally, we release the speech data, code, and trained voices for\n12 African languages to support researchers and developers.", "published": "2022-07-01 23:28:16", "link": "http://arxiv.org/abs/2207.00688v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Polyphone BERT for Polyphone Disambiguation in Mandarin Chinese", "abstract": "Grapheme-to-phoneme (G2P) conversion is an indispensable part of the Chinese\nMandarin text-to-speech (TTS) system, and the core of G2P conversion is to\nsolve the problem of polyphone disambiguation, which is to pick up the correct\npronunciation for several candidates for a Chinese polyphonic character. In\nthis paper, we propose a Chinese polyphone BERT model to predict the\npronunciations of Chinese polyphonic characters. Firstly, we create 741 new\nChinese monophonic characters from 354 source Chinese polyphonic characters by\npronunciation. Then we get a Chinese polyphone BERT by extending a pre-trained\nChinese BERT with 741 new Chinese monophonic characters and adding a\ncorresponding embedding layer for new tokens, which is initialized by the\nembeddings of source Chinese polyphonic characters. In this way, we can turn\nthe polyphone disambiguation task into a pre-training task of the Chinese\npolyphone BERT. Experimental results demonstrate the effectiveness of the\nproposed model, and the polyphone BERT model obtain 2% (from 92.1% to 94.1%)\nimprovement of average accuracy compared with the BERT-based classifier model,\nwhich is the prior state-of-the-art in polyphone disambiguation.", "published": "2022-07-01 09:16:29", "link": "http://arxiv.org/abs/2207.12089v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "American == White in Multimodal Language-and-Image AI", "abstract": "Three state-of-the-art language-and-image AI models, CLIP, SLIP, and BLIP,\nare evaluated for evidence of a bias previously observed in social and\nexperimental psychology: equating American identity with being White. Embedding\nassociation tests (EATs) using standardized images of self-identified Asian,\nBlack, Latina/o, and White individuals from the Chicago Face Database (CFD)\nreveal that White individuals are more associated with collective in-group\nwords than are Asian, Black, or Latina/o individuals. In assessments of three\ncore aspects of American identity reported by social psychologists,\nsingle-category EATs reveal that images of White individuals are more\nassociated with patriotism and with being born in America, but that, consistent\nwith prior findings in psychology, White individuals are associated with being\nless likely to treat people of all races and backgrounds equally. Three\ndownstream machine learning tasks demonstrate biases associating American with\nWhite. In a visual question answering task using BLIP, 97% of White individuals\nare identified as American, compared to only 3% of Asian individuals. When\nasked in what state the individual depicted lives in, the model responds China\n53% of the time for Asian individuals, but always with an American state for\nWhite individuals. In an image captioning task, BLIP remarks upon the race of\nAsian individuals as much as 36% of the time, but never remarks upon race for\nWhite individuals. Finally, provided with an initialization image from the CFD\nand the text \"an American person,\" a synthetic image generator (VQGAN) using\nthe text-based guidance of CLIP lightens the skin tone of individuals of all\nraces (by 35% for Black individuals, based on pixel brightness). The results\nindicate that biases equating American identity with being White are learned by\nlanguage-and-image AI, and propagate to downstream applications of such models.", "published": "2022-07-01 23:45:56", "link": "http://arxiv.org/abs/2207.00691v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CY"}
{"title": "SASV Based on Pre-trained ASV System and Integrated Scoring Module", "abstract": "Based on the assumption that there is a correlation between anti-spoofing and\nspeaker verification, a Total-Divide-Total integrated Spoofing-Aware Speaker\nVerification (SASV) system based on pre-trained automatic speaker verification\n(ASV) system and integrated scoring module is proposed and submitted to the\nSASV 2022 Challenge. The training and scoring of ASV and anti-spoofing\ncountermeasure (CM) in current SASV systems are relatively independent,\nignoring the correlation. In this paper, by leveraging the correlation between\nthe two tasks, an integrated SASV system can be obtained by simply training a\nfew more layers on the basis of the baseline pre-trained ASV subsystem. The\nfeatures in pre-trained ASV system are utilized for logical access spoofing\nspeech detection. Further, speaker embeddings extracted by the pre-trained ASV\nsystem are used to improve the performance of the CM. The integrated scoring\nmodule takes the embeddings of the ASV and anti-spoofing branches as input and\npreserves the correlation between the two tasks through matrix operations to\nproduce integrated SASV scores. Submitted primary system achieved equal error\nrate (EER) of 3.07\\% on the development dataset of the SASV 2022 Challenge and\n4.30\\% on the evaluation part, which is a 25\\% improvement over the baseline\nsystems.", "published": "2022-07-01 02:27:09", "link": "http://arxiv.org/abs/2207.00150v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Updating Only Encoders Prevents Catastrophic Forgetting of End-to-End\n  ASR Models", "abstract": "In this paper, we present an incremental domain adaptation technique to\nprevent catastrophic forgetting for an end-to-end automatic speech recognition\n(ASR) model. Conventional approaches require extra parameters of the same size\nas the model for optimization, and it is difficult to apply these approaches to\nend-to-end ASR models because they have a huge amount of parameters. To solve\nthis problem, we first investigate which parts of end-to-end ASR models\ncontribute to high accuracy in the target domain while preventing catastrophic\nforgetting. We conduct experiments on incremental domain adaptation from the\nLibriSpeech dataset to the AMI meeting corpus with two popular end-to-end ASR\nmodels and found that adapting only the linear layers of their encoders can\nprevent catastrophic forgetting. Then, on the basis of this finding, we develop\nan element-wise parameter selection focused on specific layers to further\nreduce the number of fine-tuning parameters. Experimental results show that our\napproach consistently prevents catastrophic forgetting compared to parameter\nselection from the whole model.", "published": "2022-07-01 05:54:01", "link": "http://arxiv.org/abs/2207.00216v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Learning Subject-Invariant Representations from Speech-Evoked EEG Using\n  Variational Autoencoders", "abstract": "The electroencephalogram (EEG) is a powerful method to understand how the\nbrain processes speech. Linear models have recently been replaced for this\npurpose with deep neural networks and yield promising results. In related EEG\nclassification fields, it is shown that explicitly modeling subject-invariant\nfeatures improves generalization of models across subjects and benefits\nclassification accuracy. In this work, we adapt factorized hierarchical\nvariational autoencoders to exploit parallel EEG recordings of the same\nstimuli. We model EEG into two disentangled latent spaces. Subject accuracy\nreaches 98.96% and 1.60% on respectively the subject and content latent space,\nwhereas binary content classification experiments reach an accuracy of 51.51%\nand 62.91% on respectively the subject and content latent space.", "published": "2022-07-01 10:26:44", "link": "http://arxiv.org/abs/2207.00323v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Distance-Based Sound Separation", "abstract": "We propose the novel task of distance-based sound separation, where sounds\nare separated based only on their distance from a single microphone. In the\ncontext of assisted listening devices, proximity provides a simple criterion\nfor sound selection in noisy environments that would allow the user to focus on\nsounds relevant to a local conversation. We demonstrate the feasibility of this\napproach by training a neural network to separate near sounds from far sounds\nin single channel synthetic reverberant mixtures, relative to a threshold\ndistance defining the boundary between near and far. With a single nearby\nspeaker and four distant speakers, the model improves scale-invariant signal to\nnoise ratio by 4.4 dB for near sounds and 6.8 dB for far sounds.", "published": "2022-07-01 17:27:18", "link": "http://arxiv.org/abs/2207.00562v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speaker Diarization and Identification from Single-Channel Classroom\n  Audio Recording Using Virtual Microphones", "abstract": "Speaker identification in noisy audio recordings, specifically those from\ncollaborative learning environments, can be extremely challenging. There is a\nneed to identify individual students talking in small groups from other\nstudents talking at the same time. To solve the problem, we assume the use of a\nsingle microphone per student group without any access to previous large\ndatasets for training.\n  This dissertation proposes a method of speaker identification using\ncross-correlation patterns associated to an array of virtual microphones,\ncentered around the physical microphone. The virtual microphones are simulated\nby using approximate speaker geometry observed from a video recording. The\npatterns are constructed based on estimates of the room impulse responses for\neach virtual microphone. The correlation patterns are then used to identify the\nspeakers. The proposed method is validated with classroom audios and shown to\nsubstantially outperform diarization services provided by Google Cloud and\nAmazon AWS.", "published": "2022-07-01 21:03:50", "link": "http://arxiv.org/abs/2207.00660v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Speech Enhancement through Fine-Grained Speech Characteristics", "abstract": "While deep learning based speech enhancement systems have made rapid progress\nin improving the quality of speech signals, they can still produce outputs that\ncontain artifacts and can sound unnatural. We propose a novel approach to\nspeech enhancement aimed at improving perceptual quality and naturalness of\nenhanced signals by optimizing for key characteristics of speech. We first\nidentify key acoustic parameters that have been found to correlate well with\nvoice quality (e.g. jitter, shimmer, and spectral flux) and then propose\nobjective functions which are aimed at reducing the difference between clean\nspeech and enhanced speech with respect to these features. The full set of\nacoustic features is the extended Geneva Acoustic Parameter Set (eGeMAPS),\nwhich includes 25 different attributes associated with perception of speech.\nGiven the non-differentiable nature of these feature computation, we first\nbuild differentiable estimators of the eGeMAPS and then use them to fine-tune\nexisting speech enhancement systems. Our approach is generic and can be applied\nto any existing deep learning based enhancement systems to further improve the\nenhanced speech signals. Experimental results conducted on the Deep Noise\nSuppression (DNS) Challenge dataset shows that our approach can improve the\nstate-of-the-art deep learning based enhancement systems.", "published": "2022-07-01 07:04:28", "link": "http://arxiv.org/abs/2207.00237v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automatic Evaluation of Speaker Similarity", "abstract": "We introduce a new automatic evaluation method for speaker similarity\nassessment, that is consistent with human perceptual scores. Modern neural\ntext-to-speech models require a vast amount of clean training data, which is\nwhy many solutions switch from single speaker models to solutions trained on\nexamples from many different speakers. Multi-speaker models bring new\npossibilities, such as a faster creation of new voices, but also a new problem\n- speaker leakage, where the speaker identity of a synthesized example might\nnot match those of the target speaker. Currently, the only way to discover this\nissue is through costly perceptual evaluations. In this work, we propose an\nautomatic method for assessment of speaker similarity. For that purpose, we\nextend the recent work on speaker verification systems and evaluate how\ndifferent metrics and speaker embeddings models reflect Multiple Stimuli with\nHidden Reference and Anchor (MUSHRA) scores. Our experiments show that we can\ntrain a model to predict speaker similarity MUSHRA scores from speaker\nembeddings with 0.96 accuracy and significant correlation up to 0.78 Pearson\nscore at the utterance level.", "published": "2022-07-01 11:23:16", "link": "http://arxiv.org/abs/2207.00344v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
