{"title": "A Comparative Analysis of Knowledge-Intensive and Data-Intensive\n  Semantic Parsers", "abstract": "We present a phenomenon-oriented comparative analysis of the two dominant\napproaches in task-independent semantic parsing: classic, knowledge-intensive\nand neural, data-intensive models. To reflect state-of-the-art neural NLP\ntechnologies, we introduce a new target structure-centric parser that can\nproduce semantic graphs much more accurately than previous data-driven parsers.\nWe then show that, in spite of comparable performance overall, knowledge- and\ndata-intensive models produce different types of errors, in a way that can be\nexplained by their theoretical properties. This analysis leads to new\ndirections for parser development.", "published": "2019-07-04 09:40:27", "link": "http://arxiv.org/abs/1907.02298v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interactive-Predictive Neural Machine Translation through Reinforcement\n  and Imitation", "abstract": "We propose an interactive-predictive neural machine translation framework for\neasier model personalization using reinforcement and imitation learning. During\nthe interactive translation process, the user is asked for feedback on\nuncertain locations identified by the system. Responses are weak feedback in\nthe form of \"keep\" and \"delete\" edits, and expert demonstrations in the form of\n\"substitute\" edits. Conditioning on the collected feedback, the system creates\nalternative translations via constrained beam search. In simulation experiments\non two language pairs our systems get close to the performance of supervised\ntraining with much less human effort.", "published": "2019-07-04 10:58:58", "link": "http://arxiv.org/abs/1907.02326v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Morphological Word Embeddings", "abstract": "Linguistic similarity is multi-faceted. For instance, two words may be\nsimilar with respect to semantics, syntax, or morphology inter alia. Continuous\nword-embeddings have been shown to capture most of these shades of similarity\nto some degree. This work considers guiding word-embeddings with\nmorphologically annotated data, a form of semi-supervised learning, encouraging\nthe vectors to encode a word's morphology, i.e., words close in the embedded\nspace share morphological features. We extend the log-bilinear model to this\nend and show that indeed our learned embeddings achieve this, using German as a\ncase study.", "published": "2019-07-04 14:32:39", "link": "http://arxiv.org/abs/1907.02423v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transfer Learning for Risk Classification of Social Media Posts: Model\n  Evaluation Study", "abstract": "Mental illness affects a significant portion of the worldwide population.\nOnline mental health forums can provide a supportive environment for those\nafflicted and also generate a large amount of data which can be mined to\npredict mental health states using machine learning methods. We benchmark\nmultiple methods of text feature representation for social media posts and\ncompare their downstream use with automated machine learning (AutoML) tools to\ntriage content for moderator attention. We used 1588 labeled posts from the\nCLPsych 2017 shared task collected from the Reachout.com forum (Milne et al.,\n2019). Posts were represented using lexicon based tools including VADER,\nEmpath, LIWC and also used pre-trained artificial neural network models\nincluding DeepMoji, Universal Sentence Encoder, and GPT-1. We used TPOT and\nauto-sklearn as AutoML tools to generate classifiers to triage the posts. The\ntop-performing system used features derived from the GPT-1 model, which was\nfinetuned on over 150,000 unlabeled posts from Reachout.com. Our top system had\na macro averaged F1 score of 0.572, providing a new state-of-the-art result on\nthe CLPsych 2017 task. This was achieved without additional information from\nmeta-data or preceding posts. Error analyses revealed that this top system\noften misses expressions of hopelessness. We additionally present\nvisualizations that aid understanding of the learned classifiers. We show that\ntransfer learning is an effective strategy for predicting risk with relatively\nlittle labeled data. We note that finetuning of pretrained language models\nprovides further gains when large amounts of unlabeled text is available.", "published": "2019-07-04 20:37:44", "link": "http://arxiv.org/abs/1907.02581v2", "categories": ["cs.CL", "H.4.0"], "primary_category": "cs.CL"}
{"title": "SEntiMoji: An Emoji-Powered Learning Approach for Sentiment Analysis in\n  Software Engineering", "abstract": "Sentiment analysis has various application scenarios in software engineering\n(SE), such as detecting developers' emotions in commit messages and identifying\ntheir opinions on Q&A forums. However, commonly used out-of-the-box sentiment\nanalysis tools cannot obtain reliable results on SE tasks and the\nmisunderstanding of technical jargon is demonstrated to be the main reason.\nThen, researchers have to utilize labeled SE-related texts to customize\nsentiment analysis for SE tasks via a variety of algorithms. However, the\nscarce labeled data can cover only very limited expressions and thus cannot\nguarantee the analysis quality. To address such a problem, we turn to the\neasily available emoji usage data for help. More specifically, we employ\nemotional emojis as noisy labels of sentiments and propose a representation\nlearning approach that uses both Tweets and GitHub posts containing emojis to\nlearn sentiment-aware representations for SE-related texts. These emoji-labeled\nposts can not only supply the technical jargon, but also incorporate more\ngeneral sentiment patterns shared across domains. They as well as labeled data\nare used to learn the final sentiment classifier. Compared to the existing\nsentiment analysis methods used in SE, the proposed approach can achieve\nsignificant improvement on representative benchmark datasets. By further\ncontrast experiments, we find that the Tweets make a key contribution to the\npower of our approach. This finding informs future research not to unilaterally\npursue the domain-specific resource, but try to transform knowledge from the\nopen domain through ubiquitous signals such as emojis.", "published": "2019-07-04 03:38:41", "link": "http://arxiv.org/abs/1907.02202v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "An External Knowledge Enhanced Multi-label Charge Prediction Approach\n  with Label Number Learning", "abstract": "Multi-label charge prediction is a task to predict the corresponding\naccusations for legal cases, and recently becomes a hot topic. However, current\nstudies use rough methods to deal with the label number. These methods manually\nset parameters to select label numbers, which has an effect in final prediction\nquality. We propose an external knowledge enhanced multi-label charge\nprediction approach that has two phases. One is charge label prediction phase\nwith external knowledge from law provisions, the other one is number learning\nphase with a number learning network (NLN) designed. Our approach enhanced by\nexternal knowledge can automatically adjust the threshold to get label number\nof law cases. It combines the output probabilities of samples and their\ncorresponding label numbers to get final prediction results. In experiments,\nour approach is connected to some state of-the art deep learning models. By\ntesting on the biggest published Chinese law dataset, we find that our approach\nhas improvements on these models. We future conduct experiments on multi-label\nsamples from the dataset. In items of macro-F1, the improvement of baselines\nwith our approach is 3%-5%; In items of micro-F1, the significant improvement\nof our approach is 5%-15%. The experiment results show the effectiveness our\napproach for multi-label charge prediction.", "published": "2019-07-04 03:50:21", "link": "http://arxiv.org/abs/1907.02205v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-Task Learning for Coherence Modeling", "abstract": "We address the task of assessing discourse coherence, an aspect of text\nquality that is essential for many NLP tasks, such as summarization and\nlanguage assessment. We propose a hierarchical neural network trained in a\nmulti-task fashion that learns to predict a document-level coherence score (at\nthe network's top layers) along with word-level grammatical roles (at the\nbottom layers), taking advantage of inductive transfer between the two tasks.\nWe assess the extent to which our framework generalizes to different domains\nand prediction tasks, and demonstrate its effectiveness not only on standard\nbinary evaluation coherence tasks, but also on real-world tasks involving the\nprediction of varying degrees of coherence, achieving a new state of the art.", "published": "2019-07-04 14:40:22", "link": "http://arxiv.org/abs/1907.02427v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fine-grained robust prosody transfer for single-speaker neural\n  text-to-speech", "abstract": "We present a neural text-to-speech system for fine-grained prosody transfer\nfrom one speaker to another. Conventional approaches for end-to-end prosody\ntransfer typically use either fixed-dimensional or variable-length prosody\nembedding via a secondary attention to encode the reference signal. However,\nwhen trained on a single-speaker dataset, the conventional prosody transfer\nsystems are not robust enough to speaker variability, especially in the case of\na reference signal coming from an unseen speaker. Therefore, we propose\ndecoupling of the reference signal alignment from the overall system. For this\npurpose, we pre-compute phoneme-level time stamps and use them to aggregate\nprosodic features per phoneme, injecting them into a sequence-to-sequence\ntext-to-speech system. We incorporate a variational auto-encoder to further\nenhance the latent representation of prosody embeddings. We show that our\nproposed approach is significantly more stable and achieves reliable prosody\ntransplantation from an unseen speaker. We also propose a solution to the use\ncase in which the transcription of the reference signal is absent. We evaluate\nall our proposed methods using both objective and subjective listening tests.", "published": "2019-07-04 16:20:42", "link": "http://arxiv.org/abs/1907.02479v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Collecting Indicators of Compromise from Unstructured Text of\n  Cybersecurity Articles using Neural-Based Sequence Labelling", "abstract": "Indicators of Compromise (IOCs) are artifacts observed on a network or in an\noperating system that can be utilized to indicate a computer intrusion and\ndetect cyber-attacks in an early stage. Thus, they exert an important role in\nthe field of cybersecurity. However, state-of-the-art IOCs detection systems\nrely heavily on hand-crafted features with expert knowledge of cybersecurity,\nand require large-scale manually annotated corpora to train an IOC classifier.\nIn this paper, we propose using an end-to-end neural-based sequence labelling\nmodel to identify IOCs automatically from cybersecurity articles without expert\nknowledge of cybersecurity. By using a multi-head self-attention module and\ncontextual features, we find that the proposed model is capable of gathering\ncontextual information from texts of cybersecurity articles and performs better\nin the task of IOC identification. Experiments show that the proposed model\noutperforms other sequence labelling models, achieving the average F1-score of\n89.0% on English cybersecurity article test set, and approximately the average\nF1-score of 81.8% on Chinese test set.", "published": "2019-07-04 02:54:02", "link": "http://arxiv.org/abs/1907.02636v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Un Modelo Ontol\u00f3gico para el Gobierno Electr\u00f3nico", "abstract": "Decision making often requires information that must be Provided with the\nrich data format. Addressing these new requirements appropriately makes it\nnecessary for government agencies to orchestrate large amounts of information\nfrom different sources and formats, to be efficiently delivered through the\ndevices commonly used by people, such as computers, netbooks, tablets and\nsmartphones. To overcome these problems, a model is proposed for the conceptual\nrepresentation of the State's organizational units, seen as georeferenced\nentities of Electronic Government, based on ontologies designed under the\nprinciples of Linked Open Data, which allows the automatic extraction of\ninformation through the machines, which supports the process of governmental\ndecision making and gives citizens full access to find and process through\nmobile technologies.", "published": "2019-07-04 21:42:26", "link": "http://arxiv.org/abs/1907.02964v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Answer Extraction for Why Arabic Questions Answering Systems: EWAQ", "abstract": "With the increasing amount of web information, questions answering systems\nbecomes very important to allow users to access to direct answers for their\nrequests. This paper presents an Arabic Questions Answering Systems based on\nentailment metrics. The type of questions which this paper focuses on is why\nquestions. There are many reasons lead us to develop this system: generally,\nthe lack of Arabic Questions Answering Systems and scarcity Arabic Questions\nAnswering Systems which focus on why questions. The goal of the proposed system\nin this research is to extract answers from re-ranked retrieved passages which\nare retrieved by search engines. This system extracts the answer only to why\nquestions. This system is called by EWAQ: Entailment based Why Arabic Questions\nAnswering. Each answer is scored with entailment metrics and ranked according\nto their scores in order to determine the most possible correct answer. EWAQ is\ncompared with search engines: yahoo, google and ask.com, the well-established\nweb-based Questions Answering systems, using manual test set. In EWAQ\nexperiments, it is showed that the accuracy is increased by implementing the\ntextual entailment in re-raking the retrieved relevant passages by search\nengines and deciding the correct answer. The obtained results show that using\nentailment based similarity can help significantly to tackle the why Answer\nExtraction module in Arabic language.", "published": "2019-07-04 17:25:28", "link": "http://arxiv.org/abs/1907.04149v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Road-map Towards Explainable Question Answering A Solution for\n  Information Pollution", "abstract": "The increasing rate of information pollution on the Web requires novel\nsolutions to tackle that. Question Answering (QA) interfaces are simplified and\nuser-friendly interfaces to access information on the Web. However, similar to\nother AI applications, they are black boxes which do not manifest the details\nof the learning or reasoning steps for augmenting an answer. The Explainable\nQuestion Answering (XQA) system can alleviate the pain of information pollution\nwhere it provides transparency to the underlying computational model and\nexposes an interface enabling the end-user to access and validate provenance,\nvalidity, context, circulation, interpretation, and feedbacks of information.\nThis position paper sheds light on the core concepts, expectations, and\nchallenges in favor of the following questions (i) What is an XQA system?, (ii)\nWhy do we need XQA?, (iii) When do we need XQA? (iv) How to represent the\nexplanations? (iv) How to evaluate XQA systems?", "published": "2019-07-04 21:42:29", "link": "http://arxiv.org/abs/1907.02606v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Formal Axiomatization of Computation", "abstract": "We introduce an axiomatization for the notion of computation. Based on the\nidea of Brouwer choice sequences, we construct a model, denoted by $E$, which\nsatisfies our axioms and $E \\models \\mathrm{ P \\neq NP}$. In other words,\nregarding \"effective computability\" in Brouwer intuitionism viewpoint, we show\n$\\mathrm{ P \\neq NP}$.", "published": "2019-07-04 04:52:45", "link": "http://arxiv.org/abs/1907.03533v2", "categories": ["cs.CC", "cs.CL", "cs.LO"], "primary_category": "cs.CC"}
{"title": "The DKU-SMIIP System for NIST 2018 Speaker Recognition Evaluation", "abstract": "In this paper, we present the system submission for the NIST 2018 Speaker\nRecognition Evaluation by DKU Speech and Multi-Modal Intelligent Information\nProcessing (SMIIP) Lab. We explore various kinds of state-of-the-art front-end\nextractors as well as back-end modeling for text-independent speaker\nverifications. Our submitted primary systems employ multiple state-of-the-art\nfront-end extractors, including the MFCC i-vector, the DNN tandem i-vector, the\nTDNN x-vector, and the deep ResNet. After speaker embedding is extracted, we\nexploit several kinds of back-end modeling to perform variability compensation\nand domain adaptation for mismatch training and testing conditions. The final\nsubmitted system on the fixed condition obtains actual detection cost of 0.392\nand 0.494 on CMN2 and VAST evaluation data respectively. After the official\nevaluation, we further extend our experiments by investigating multiple\nencoding layer designs and loss functions for the deep ResNet system.", "published": "2019-07-04 02:31:03", "link": "http://arxiv.org/abs/1907.02191v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "The DKU System for the Speaker Recognition Task of the 2019 VOiCES from\n  a Distance Challenge", "abstract": "In this paper, we present the DKU system for the speaker recognition task of\nthe VOiCES from a distance challenge 2019. We investigate the whole system\npipeline for the far-field speaker verification, including data pre-processing,\nshort-term spectral feature representation, utterance-level speaker modeling,\nback-end scoring, and score normalization. Our best single system employs a\nresidual neural network trained with angular softmax loss. Also, the weighted\nprediction error algorithms can further improve performance. It achieves 0.3668\nminDCF and 5.58% EER on the evaluation set by using a simple cosine similarity\nscoring. Finally, the submitted primary system obtains 0.3532 minDCF and 4.96%\nEER on the evaluation set.", "published": "2019-07-04 02:42:15", "link": "http://arxiv.org/abs/1907.02194v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Attention based Convolutional Recurrent Neural Network for Environmental\n  Sound Classification", "abstract": "Environmental sound classification (ESC) is a challenging problem due to the\ncomplexity of sounds. The ESC performance is heavily dependent on the\neffectiveness of representative features extracted from the environmental\nsounds. However, ESC often suffers from the semantically irrelevant frames and\nsilent frames. In order to deal with this, we employ a frame-level attention\nmodel to focus on the semantically relevant frames and salient frames.\nSpecifically, we first propose an convolutional recurrent neural network to\nlearn spectro-temporal features and temporal correlations. Then, we extend our\nconvolutional RNN model with a frame-level attention mechanism to learn\ndiscriminative feature representations for ESC. Experiments were conducted on\nESC-50 and ESC-10 datasets. Experimental results demonstrated the effectiveness\nof the proposed method and achieved the state-of-the-art performance in terms\nof classification accuracy.", "published": "2019-07-04 05:41:18", "link": "http://arxiv.org/abs/1907.02230v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Lumi\u00e8reNet: Lecture Video Synthesis from Audio", "abstract": "We present Lumi\\`ereNet, a simple, modular, and completely deep-learning\nbased architecture that synthesizes, high quality, full-pose headshot lecture\nvideos from instructor's new audio narration of any length. Unlike prior works,\nLumi\\`ereNet is entirely composed of trainable neural network modules to learn\nmapping functions from the audio to video through (intermediate) estimated\npose-based compact and abstract latent codes. Our video demos are available at\n[22] and [23].", "published": "2019-07-04 07:21:24", "link": "http://arxiv.org/abs/1907.02253v1", "categories": ["cs.LG", "cs.CV", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Supervised Symbolic Music Style Translation Using Synthetic Data", "abstract": "Research on style transfer and domain translation has clearly demonstrated\nthe ability of deep learning-based algorithms to manipulate images in terms of\nartistic style. More recently, several attempts have been made to extend such\napproaches to music (both symbolic and audio) in order to enable transforming\nmusical style in a similar manner. In this study, we focus on symbolic music\nwith the goal of altering the 'style' of a piece while keeping its original\n'content'. As opposed to the current methods, which are inherently restricted\nto be unsupervised due to the lack of 'aligned' data (i.e. the same musical\npiece played in multiple styles), we develop the first fully supervised\nalgorithm for this task. At the core of our approach lies a synthetic data\ngeneration scheme which allows us to produce virtually unlimited amounts of\naligned data, and hence avoid the above issue. In view of this data generation\nscheme, we propose an encoder-decoder model for translating symbolic music\naccompaniments between a number of different styles. Our experiments show that\nour models, although trained entirely on synthetic data, are capable of\nproducing musically meaningful accompaniments even for real (non-synthetic)\nMIDI recordings.", "published": "2019-07-04 08:16:20", "link": "http://arxiv.org/abs/1907.02265v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Adversarial Attacks in Sound Event Classification", "abstract": "Adversarial attacks refer to a set of methods that perturb the input to a\nclassification model in order to fool the classifier. In this paper we apply\ndifferent gradient based adversarial attack algorithms on five deep learning\nmodels trained for sound event classification. Four of the models use\nmel-spectrogram input and one model uses raw audio input. The models represent\nstandard architectures such as convolutional, recurrent and dense networks. The\ndataset used for training is the Freesound dataset released for task 2 of the\nDCASE 2018 challenge and the models used are from participants of the challenge\nwho open sourced their code. Our experiments show that adversarial attacks can\nbe generated with high confidence and low perturbation. In addition, we show\nthat the adversarial attacks are very effective across the different models.", "published": "2019-07-04 16:15:35", "link": "http://arxiv.org/abs/1907.02477v2", "categories": ["cs.LG", "cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Neural Drum Machine : An Interactive System for Real-time Synthesis of\n  Drum Sounds", "abstract": "In this work, we introduce a system for real-time generation of drum sounds.\nThis system is composed of two parts: a generative model for drum sounds\ntogether with a Max4Live plugin providing intuitive controls on the generative\nprocess. The generative model consists of a Conditional Wasserstein autoencoder\n(CWAE), which learns to generate Mel-scaled magnitude spectrograms of short\npercussion samples, coupled with a Multi-Head Convolutional Neural Network\n(MCNN) which estimates the corresponding audio signal from the magnitude\nspectrogram. The design of this model makes it lightweight, so that it allows\none to perform real-time generation of novel drum sounds on an average CPU,\nremoving the need for the users to possess dedicated hardware in order to use\nthis system. We then present our Max4Live interface designed to interact with\nthis generative model. With this setup, the system can be easily integrated\ninto a studio-production environment and enhance the creative process. Finally,\nwe discuss the advantages of our system and how the interaction of music\nproducers with such tools could change the way drum tracks are composed.", "published": "2019-07-04 17:22:27", "link": "http://arxiv.org/abs/1907.02637v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "68T99"], "primary_category": "cs.SD"}
{"title": "Blind Audio Source Separation with Minimum-Volume Beta-Divergence NMF", "abstract": "Considering a mixed signal composed of various audio sources and recorded\nwith a single microphone, we consider on this paper the blind audio source\nseparation problem which consists in isolating and extracting each of the\nsources. To perform this task, nonnegative matrix factorization (NMF) based on\nthe Kullback-Leibler and Itakura-Saito $\\beta$-divergences is a standard and\nstate-of-the-art technique that uses the time-frequency representation of the\nsignal. We present a new NMF model better suited for this task. It is based on\nthe minimization of $\\beta$-divergences along with a penalty term that promotes\nthe columns of the dictionary matrix to have a small volume. Under some mild\nassumptions and in noiseless conditions, we prove that this model is provably\nable to identify the sources. In order to solve this problem, we propose\nmultiplicative updates whose derivations are based on the standard\nmajorization-minimization framework. We show on several numerical experiments\nthat our new model is able to obtain more interpretable results than standard\nNMF models. Moreover, we show that it is able to recover the sources even when\nthe number of sources present into the mixed signal is overestimated. In fact,\nour model automatically sets sources to zero in this situation, hence performs\nmodel order selection automatically.", "published": "2019-07-04 13:56:25", "link": "http://arxiv.org/abs/1907.02404v2", "categories": ["eess.SP", "cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "eess.SP"}
