{"title": "A Pipeline for Creative Visual Storytelling", "abstract": "Computational visual storytelling produces a textual description of events\nand interpretations depicted in a sequence of images. These texts are made\npossible by advances and cross-disciplinary approaches in natural language\nprocessing, generation, and computer vision. We define a computational creative\nvisual storytelling as one with the ability to alter the telling of a story\nalong three aspects: to speak about different environments, to produce\nvariations based on narrative goals, and to adapt the narrative to the\naudience. These aspects of creative storytelling and their effect on the\nnarrative have yet to be explored in visual storytelling. This paper presents a\npipeline of task-modules, Object Identification, Single-Image Inferencing, and\nMulti-Image Narration, that serve as a preliminary design for building a\ncreative visual storyteller. We have piloted this design for a sequence of\nimages in an annotation task. We present and analyze the collected corpus and\ndescribe plans towards automation.", "published": "2018-07-21 03:33:50", "link": "http://arxiv.org/abs/1807.08077v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ScoutBot: A Dialogue System for Collaborative Navigation", "abstract": "ScoutBot is a dialogue interface to physical and simulated robots that\nsupports collaborative exploration of environments. The demonstration will\nallow users to issue unconstrained spoken language commands to ScoutBot.\nScoutBot will prompt for clarification if the user's instruction needs\nadditional input. It is trained on human-robot dialogue collected from\nWizard-of-Oz experiments, where robot responses were initiated by a human\nwizard in previous interactions. The demonstration will show a simulated ground\nrobot (Clearpath Jackal) in a simulated environment supported by ROS (Robot\nOperating System).", "published": "2018-07-21 03:12:36", "link": "http://arxiv.org/abs/1807.08074v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Towards Neural Theorem Proving at Scale", "abstract": "Neural models combining representation learning and reasoning in an\nend-to-end trainable manner are receiving increasing interest. However, their\nuse is severely limited by their computational complexity, which renders them\nunusable on real world datasets. We focus on the Neural Theorem Prover (NTP)\nmodel proposed by Rockt{\\\"{a}}schel and Riedel (2017), a continuous relaxation\nof the Prolog backward chaining algorithm where unification between terms is\nreplaced by the similarity between their embedding representations. For\nanswering a given query, this model needs to consider all possible proof paths,\nand then aggregate results - this quickly becomes infeasible even for small\nKnowledge Bases (KBs). We observe that we can accurately approximate the\ninference process in this model by considering only proof paths associated with\nthe highest proof scores. This enables inference and learning on previously\nimpracticable KBs.", "published": "2018-07-21 20:48:53", "link": "http://arxiv.org/abs/1807.08204v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Consequences and Factors of Stylistic Differences in Human-Robot\n  Dialogue", "abstract": "This paper identifies stylistic differences in instruction-giving observed in\na corpus of human-robot dialogue. Differences in verbosity and structure (i.e.,\nsingle-intent vs. multi-intent instructions) arose naturally without\nrestrictions or prior guidance on how users should speak with the robot.\nDifferent styles were found to produce different rates of miscommunication, and\ncorrelations were found between style differences and individual user\nvariation, trust, and interaction experience with the robot. Understanding\npotential consequences and factors that influence style can inform design of\ndialogue systems that are robust to natural variation from human users.", "published": "2018-07-21 03:21:56", "link": "http://arxiv.org/abs/1807.08076v1", "categories": ["cs.CL", "cs.HC", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Phonetic-and-Semantic Embedding of Spoken Words with Applications in\n  Spoken Content Retrieval", "abstract": "Word embedding or Word2Vec has been successful in offering semantics for text\nwords learned from the context of words. Audio Word2Vec was shown to offer\nphonetic structures for spoken words (signal segments for words) learned from\nsignals within spoken words. This paper proposes a two-stage framework to\nperform phonetic-and-semantic embedding on spoken words considering the context\nof the spoken words. Stage 1 performs phonetic embedding with speaker\ncharacteristics disentangled. Stage 2 then performs semantic embedding in\naddition. We further propose to evaluate the phonetic-and-semantic nature of\nthe audio embeddings obtained in Stage 2 by parallelizing with text embeddings.\nIn general, phonetic structure and semantics inevitably disturb each other. For\nexample the words \"brother\" and \"sister\" are close in semantics but very\ndifferent in phonetic structure, while the words \"brother\" and \"bother\" are in\nthe other way around. But phonetic-and-semantic embedding is attractive, as\nshown in the initial experiments on spoken document retrieval. Not only spoken\ndocuments including the spoken query can be retrieved based on the phonetic\nstructures, but spoken documents semantically related to the query but not\nincluding the query can also be retrieved based on the semantics.", "published": "2018-07-21 06:07:46", "link": "http://arxiv.org/abs/1807.08089v4", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ELICA: An Automated Tool for Dynamic Extraction of Requirements Relevant\n  Information", "abstract": "Requirements elicitation requires extensive knowledge and deep understanding\nof the problem domain where the final system will be situated. However, in many\nsoftware development projects, analysts are required to elicit the requirements\nfrom an unfamiliar domain, which often causes communication barriers between\nanalysts and stakeholders. In this paper, we propose a requirements ELICitation\nAid tool (ELICA) to help analysts better understand the target application\ndomain by dynamic extraction and labeling of requirements-relevant knowledge.\nTo extract the relevant terms, we leverage the flexibility and power of\nWeighted Finite State Transducers (WFSTs) in dynamic modeling of natural\nlanguage processing tasks. In addition to the information conveyed through\ntext, ELICA captures and processes non-linguistic information about the\nintention of speakers such as their confidence level, analytical tone, and\nemotions. The extracted information is made available to the analysts as a set\nof labeled snippets with highlighted relevant terms which can also be exported\nas an artifact of the Requirements Engineering (RE) process. The application\nand usefulness of ELICA are demonstrated through a case study. This study shows\nhow pre-existing relevant information about the application domain and the\ninformation captured during an elicitation meeting, such as the conversation\nand stakeholders' intentions, can be captured and used to support analysts\nachieving their tasks.", "published": "2018-07-21 00:19:13", "link": "http://arxiv.org/abs/1808.05857v1", "categories": ["cs.SE", "cs.CL", "cs.IR", "stat.ML"], "primary_category": "cs.SE"}
{"title": "What is not where: the challenge of integrating spatial representations\n  into deep learning architectures", "abstract": "This paper examines to what degree current deep learning architectures for\nimage caption generation capture spatial language. On the basis of the\nevaluation of examples of generated captions from the literature we argue that\nsystems capture what objects are in the image data but not where these objects\nare located: the captions generated by these systems are the output of a\nlanguage model conditioned on the output of an object detector that cannot\ncapture fine-grained location information. Although language models provide\nuseful knowledge for image captions, we argue that deep learning image\ncaptioning architectures should also model geometric relations between objects.", "published": "2018-07-21 11:55:17", "link": "http://arxiv.org/abs/1807.08133v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Modular Mechanistic Networks: On Bridging Mechanistic and\n  Phenomenological Models with Deep Neural Networks in Natural Language\n  Processing", "abstract": "Natural language processing (NLP) can be done using either top-down (theory\ndriven) and bottom-up (data driven) approaches, which we call mechanistic and\nphenomenological respectively. The approaches are frequently considered to\nstand in opposition to each other. Examining some recent approaches in deep\nlearning we argue that deep neural networks incorporate both perspectives and,\nfurthermore, that leveraging this aspect of deep learning may help in solving\ncomplex problems within language technology, such as modelling language and\nperception in the domain of spatial cognition.", "published": "2018-07-21 11:37:15", "link": "http://arxiv.org/abs/1807.09844v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
