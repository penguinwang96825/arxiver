{"title": "Speech SIMCLR: Combining Contrastive and Reconstruction Objective for\n  Self-supervised Speech Representation Learning", "abstract": "Self-supervised visual pretraining has shown significant progress recently.\nAmong those methods, SimCLR greatly advanced the state of the art in\nself-supervised and semi-supervised learning on ImageNet. The input feature\nrepresentations for speech and visual tasks are both continuous, so it is\nnatural to consider applying similar objective on speech representation\nlearning. In this paper, we propose Speech SimCLR, a new self-supervised\nobjective for speech representation learning. During training, Speech SimCLR\napplies augmentation on raw speech and its spectrogram. Its objective is the\ncombination of contrastive loss that maximizes agreement between differently\naugmented samples in the latent space and reconstruction loss of input\nrepresentation. The proposed method achieved competitive results on speech\nemotion recognition and speech recognition.", "published": "2020-10-27 02:09:06", "link": "http://arxiv.org/abs/2010.13991v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Volctrans Parallel Corpus Filtering System for WMT 2020", "abstract": "In this paper, we describe our submissions to the WMT20 shared task on\nparallel corpus filtering and alignment for low-resource conditions. The task\nrequires the participants to align potential parallel sentence pairs out of the\ngiven document pairs, and score them so that low-quality pairs can be filtered.\nOur system, Volctrans, is made of two modules, i.e., a mining module and a\nscoring module. Based on the word alignment model, the mining module adopts an\niterative mining strategy to extract latent parallel sentences. In the scoring\nmodule, an XLM-based scorer provides scores, followed by reranking mechanisms\nand ensemble. Our submissions outperform the baseline by 3.x/2.x and 2.x/2.x\nfor km-en and ps-en on From Scratch/Fine-Tune conditions, which is the highest\namong all submissions.", "published": "2020-10-27 03:20:04", "link": "http://arxiv.org/abs/2010.14029v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "To BERT or Not to BERT: Comparing Task-specific and Task-agnostic\n  Semi-Supervised Approaches for Sequence Tagging", "abstract": "Leveraging large amounts of unlabeled data using Transformer-like\narchitectures, like BERT, has gained popularity in recent times owing to their\neffectiveness in learning general representations that can then be further\nfine-tuned for downstream tasks to much success. However, training these models\ncan be costly both from an economic and environmental standpoint. In this work,\nwe investigate how to effectively use unlabeled data: by exploring the\ntask-specific semi-supervised approach, Cross-View Training (CVT) and comparing\nit with task-agnostic BERT in multiple settings that include domain and task\nrelevant English data. CVT uses a much lighter model architecture and we show\nthat it achieves similar performance to BERT on a set of sequence tagging\ntasks, with lesser financial and environmental impact.", "published": "2020-10-27 04:03:47", "link": "http://arxiv.org/abs/2010.14042v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Event Detection: Gate Diversity and Syntactic Importance Scoresfor Graph\n  Convolution Neural Networks", "abstract": "Recent studies on event detection (ED) haveshown that the syntactic\ndependency graph canbe employed in graph convolution neural net-works (GCN) to\nachieve state-of-the-art per-formance. However, the computation of thehidden\nvectors in such graph-based models isagnostic to the trigger candidate words,\npo-tentially leaving irrelevant information for thetrigger candidate for event\nprediction. In addi-tion, the current models for ED fail to exploitthe overall\ncontextual importance scores of thewords, which can be obtained via the\ndepen-dency tree, to boost the performance. In thisstudy, we propose a novel\ngating mechanismto filter noisy information in the hidden vec-tors of the GCN\nmodels for ED based on theinformation from the trigger candidate. Wealso\nintroduce novel mechanisms to achievethe contextual diversity for the gates and\ntheimportance score consistency for the graphsand models in ED. The experiments\nshow thatthe proposed model achieves state-of-the-artperformance on two ED\ndatasets", "published": "2020-10-27 08:28:28", "link": "http://arxiv.org/abs/2010.14123v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multitask Training with Text Data for End-to-End Speech Recognition", "abstract": "We propose a multitask training method for attention-based end-to-end speech\nrecognition models. We regularize the decoder in a listen, attend, and spell\nmodel by multitask training it on both audio-text and text-only data. Trained\non the 100-hour subset of LibriSpeech, the proposed method, without requiring\nan additional language model, leads to an 11% relative performance improvement\nover the baseline and approaches the performance of language model shallow\nfusion on the test-clean evaluation set. We observe a similar trend on the\nwhole 960-hour LibriSpeech training set. Analyses of different types of errors\nand sample output sentences demonstrate that the proposed method can\nincorporate language level information, suggesting its effectiveness in\nreal-world applications.", "published": "2020-10-27 14:29:28", "link": "http://arxiv.org/abs/2010.14318v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Listener's Social Identity Matters in Personalised Response Generation", "abstract": "Personalised response generation enables generating human-like responses by\nmeans of assigning the generator a social identity. However, pragmatics theory\nsuggests that human beings adjust the way of speaking based on not only who\nthey are but also whom they are talking to. In other words, when modelling\npersonalised dialogues, it might be favourable if we also take the listener's\nsocial identity into consideration. To validate this idea, we use gender as a\ntypical example of a social variable to investigate how the listener's identity\ninfluences the language used in Chinese dialogues on social media. Also, we\nbuild personalised generators. The experiment results demonstrate that the\nlistener's identity indeed matters in the language use of responses and that\nthe response generator can capture such differences in language use. More\ninterestingly, by additionally modelling the listener's identity, the\npersonalised response generator performs better in its own identity.", "published": "2020-10-27 14:57:21", "link": "http://arxiv.org/abs/2010.14342v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Gender Bias in Speech Translation", "abstract": "The scientific community is increasingly aware of the necessity to embrace\npluralism and consistently represent major and minor social groups. Currently,\nthere are no standard evaluation techniques for different types of biases.\nAccordingly, there is an urgent need to provide evaluation sets and protocols\nto measure existing biases in our automatic systems. Evaluating the biases\nshould be an essential step towards mitigating them in the systems.\n  This paper introduces WinoST, a new freely available challenge set for\nevaluating gender bias in speech translation. WinoST is the speech version of\nWinoMT which is a MT challenge set and both follow an evaluation protocol to\nmeasure gender accuracy. Using a state-of-the-art end-to-end speech translation\nsystem, we report the gender bias evaluation on four language pairs and we show\nthat gender accuracy in speech translation is more than 23% lower than in MT.", "published": "2020-10-27 17:24:27", "link": "http://arxiv.org/abs/2010.14465v4", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Unmasking Contextual Stereotypes: Measuring and Mitigating BERT's Gender\n  Bias", "abstract": "Contextualized word embeddings have been replacing standard embeddings as the\nrepresentational knowledge source of choice in NLP systems. Since a variety of\nbiases have previously been found in standard word embeddings, it is crucial to\nassess biases encoded in their replacements as well. Focusing on BERT (Devlin\net al., 2018), we measure gender bias by studying associations between\ngender-denoting target words and names of professions in English and German,\ncomparing the findings with real-world workforce statistics. We mitigate bias\nby fine-tuning BERT on the GAP corpus (Webster et al., 2018), after applying\nCounterfactual Data Substitution (CDS) (Maudslay et al., 2019). We show that\nour method of measuring bias is appropriate for languages such as English, but\nnot for languages with a rich morphology and gender-marking, such as German.\nOur results highlight the importance of investigating bias and mitigation\ntechniques cross-linguistically, especially in view of the current emphasis on\nlarge-scale, multilingual language models.", "published": "2020-10-27 18:06:09", "link": "http://arxiv.org/abs/2010.14534v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Strongly Incremental Constituency Parsing with Graph Neural Networks", "abstract": "Parsing sentences into syntax trees can benefit downstream applications in\nNLP. Transition-based parsers build trees by executing actions in a state\ntransition system. They are computationally efficient, and can leverage machine\nlearning to predict actions based on partial trees. However, existing\ntransition-based parsers are predominantly based on the shift-reduce transition\nsystem, which does not align with how humans are known to parse sentences.\nPsycholinguistic research suggests that human parsing is strongly incremental:\nhumans grow a single parse tree by adding exactly one token at each step. In\nthis paper, we propose a novel transition system called attach-juxtapose. It is\nstrongly incremental; it represents a partial sentence using a single tree;\neach action adds exactly one token into the partial tree. Based on our\ntransition system, we develop a strongly incremental parser. At each step, it\nencodes the partial tree using a graph neural network and predicts an action.\nWe evaluate our parser on Penn Treebank (PTB) and Chinese Treebank (CTB). On\nPTB, it outperforms existing parsers trained with only constituency trees; and\nit performs on par with state-of-the-art parsers that use dependency trees as\nadditional training data. On CTB, our parser establishes a new state of the\nart. Code is available at\nhttps://github.com/princeton-vl/attach-juxtapose-parser.", "published": "2020-10-27 19:19:38", "link": "http://arxiv.org/abs/2010.14568v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WNUT-2020 Task 1 Overview: Extracting Entities and Relations from Wet\n  Lab Protocols", "abstract": "This paper presents the results of the wet lab information extraction task at\nWNUT 2020. This task consisted of two sub tasks: (1) a Named Entity Recognition\n(NER) task with 13 participants and (2) a Relation Extraction (RE) task with 2\nparticipants. We outline the task, data annotation process, corpus statistics,\nand provide a high-level overview of the participating systems for each sub\ntask.", "published": "2020-10-27 19:34:53", "link": "http://arxiv.org/abs/2010.14576v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Themes within Complex Unstructured Texts: A Case Study on\n  Safeguarding Reports", "abstract": "The task of text and sentence classification is associated with the need for\nlarge amounts of labelled training data. The acquisition of high volumes of\nlabelled datasets can be expensive or unfeasible, especially for\nhighly-specialised domains for which documents are hard to obtain. Research on\nthe application of supervised classification based on small amounts of training\ndata is limited. In this paper, we address the combination of state-of-the-art\ndeep learning and classification methods and provide an insight into what\ncombination of methods fit the needs of small, domain-specific, and\nterminologically-rich corpora. We focus on a real-world scenario related to a\ncollection of safeguarding reports comprising learning experiences and\nreflections on tackling serious incidents involving children and vulnerable\nadults. The relatively small volume of available reports and their use of\nhighly domain-specific terminology makes the application of automated\napproaches difficult. We focus on the problem of automatically identifying the\nmain themes in a safeguarding report using supervised classification\napproaches. Our results show the potential of deep learning models to simulate\nsubject-expert behaviour even for complex tasks with limited labelled data.", "published": "2020-10-27 19:48:23", "link": "http://arxiv.org/abs/2010.14584v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the diminishing return of labeling clinical reports", "abstract": "Ample evidence suggests that better machine learning models may be steadily\nobtained by training on increasingly larger datasets on natural language\nprocessing (NLP) problems from non-medical domains. Whether the same holds true\nfor medical NLP has by far not been thoroughly investigated. This work shows\nthat this is indeed not always the case. We reveal the somehow\ncounter-intuitive observation that performant medical NLP models may be\nobtained with small amount of labeled data, quite the opposite to the common\nbelief, most likely due to the domain specificity of the problem. We show\nquantitatively the effect of training data size on a fixed test set composed of\ntwo of the largest public chest x-ray radiology report datasets on the task of\nabnormality classification. The trained models not only make use of the\ntraining data efficiently, but also outperform the current state-of-the-art\nrule-based systems by a significant margin.", "published": "2020-10-27 19:51:04", "link": "http://arxiv.org/abs/2010.14587v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predict and Use Latent Patterns for Short-Text Conversation", "abstract": "Many neural network models nowadays have achieved promising performances in\nChit-chat settings. The majority of them rely on an encoder for understanding\nthe post and a decoder for generating the response. Without given assigned\nsemantics, the models lack the fine-grained control over responses as the\nsemantic mapping between posts and responses is hidden on the fly within the\nend-to-end manners. Some previous works utilize sampled latent words as a\ncontrollable semantic form to drive the generated response around the work, but\nfew works attempt to use more complex semantic patterns to guide the\ngeneration. In this paper, we propose to use more detailed semantic forms,\nincluding latent responses and part-of-speech sequences sampled from the\ncorresponding distributions, as the controllable semantics to guide the\ngeneration. Our results show that the richer semantics are not only able to\nprovide informative and diverse responses, but also increase the overall\nperformance of response quality, including fluency and coherence.", "published": "2020-10-27 01:31:42", "link": "http://arxiv.org/abs/2010.13982v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Interpretation of NLP models through input marginalization", "abstract": "To demystify the \"black box\" property of deep neural networks for natural\nlanguage processing (NLP), several methods have been proposed to interpret\ntheir predictions by measuring the change in prediction probability after\nerasing each token of an input. Since existing methods replace each token with\na predefined value (i.e., zero), the resulting sentence lies out of the\ntraining data distribution, yielding misleading interpretations. In this study,\nwe raise the out-of-distribution problem induced by the existing interpretation\nmethods and present a remedy; we propose to marginalize each token out. We\ninterpret various NLP models trained for sentiment analysis and natural\nlanguage inference using the proposed method.", "published": "2020-10-27 01:40:41", "link": "http://arxiv.org/abs/2010.13984v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-XScience: A Large-scale Dataset for Extreme Multi-document\n  Summarization of Scientific Articles", "abstract": "Multi-document summarization is a challenging task for which there exists\nlittle large-scale datasets. We propose Multi-XScience, a large-scale\nmulti-document summarization dataset created from scientific articles.\nMulti-XScience introduces a challenging multi-document summarization task:\nwriting the related-work section of a paper based on its abstract and the\narticles it references. Our work is inspired by extreme summarization, a\ndataset construction protocol that favours abstractive modeling approaches.\nDescriptive statistics and empirical results---using several state-of-the-art\nmodels trained on the Multi-XScience dataset---reveal that Multi-XScience is\nwell suited for abstractive models.", "published": "2020-10-27 12:10:19", "link": "http://arxiv.org/abs/2010.14235v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RH-Net: Improving Neural Relation Extraction via Reinforcement Learning\n  and Hierarchical Relational Searching", "abstract": "Distant supervision (DS) aims to generate large-scale heuristic labeling\ncorpus, which is widely used for neural relation extraction currently. However,\nit heavily suffers from noisy labeling and long-tail distributions problem.\nMany advanced approaches usually separately address two problems, which ignore\ntheir mutual interactions. In this paper, we propose a novel framework named\nRH-Net, which utilizes Reinforcement learning and Hierarchical relational\nsearching module to improve relation extraction. We leverage reinforcement\nlearning to instruct the model to select high-quality instances. We then\npropose the hierarchical relational searching module to share the semantics\nfrom correlative instances between data-rich and data-poor classes. During the\niterative process, the two modules keep interacting to alleviate the noisy and\nlong-tail problem simultaneously. Extensive experiments on widely used NYT data\nset clearly show that our method significant improvements over state-of-the-art\nbaselines.", "published": "2020-10-27 12:50:27", "link": "http://arxiv.org/abs/2010.14255v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Machine Reading Comprehension with Language Branch\n  Knowledge Distillation", "abstract": "Cross-lingual Machine Reading Comprehension (CLMRC) remains a challenging\nproblem due to the lack of large-scale annotated datasets in low-source\nlanguages, such as Arabic, Hindi, and Vietnamese. Many previous approaches use\ntranslation data by translating from a rich-source language, such as English,\nto low-source languages as auxiliary supervision. However, how to effectively\nleverage translation data and reduce the impact of noise introduced by\ntranslation remains onerous. In this paper, we tackle this challenge and\nenhance the cross-lingual transferring performance by a novel augmentation\napproach named Language Branch Machine Reading Comprehension (LBMRC). A\nlanguage branch is a group of passages in one single language paired with\nquestions in all target languages. We train multiple machine reading\ncomprehension (MRC) models proficient in individual language based on LBMRC.\nThen, we devise a multilingual distillation approach to amalgamate knowledge\nfrom multiple language branch models to a single model for all target\nlanguages. Combining the LBMRC and multilingual distillation can be more robust\nto the data noises, therefore, improving the model's cross-lingual ability.\nMeanwhile, the produced single multilingual model is applicable to all target\nlanguages, which saves the cost of training, inference, and maintenance for\nmultiple models. Extensive experiments on two CLMRC benchmarks clearly show the\neffectiveness of our proposed method.", "published": "2020-10-27 13:12:17", "link": "http://arxiv.org/abs/2010.14271v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fast Interleaved Bidirectional Sequence Generation", "abstract": "Independence assumptions during sequence generation can speed up inference,\nbut parallel generation of highly inter-dependent tokens comes at a cost in\nquality. Instead of assuming independence between neighbouring tokens\n(semi-autoregressive decoding, SA), we take inspiration from bidirectional\nsequence generation and introduce a decoder that generates target words from\nthe left-to-right and right-to-left directions simultaneously. We show that we\ncan easily convert a standard architecture for unidirectional decoding into a\nbidirectional decoder by simply interleaving the two directions and adapting\nthe word positions and self-attention masks. Our interleaved bidirectional\ndecoder (IBDecoder) retains the model simplicity and training efficiency of the\nstandard Transformer, and on five machine translation tasks and two document\nsummarization tasks, achieves a decoding speedup of ~2X compared to\nautoregressive decoding with comparable quality. Notably, it outperforms\nleft-to-right SA because the independence assumptions in IBDecoder are more\nfelicitous. To achieve even higher speedups, we explore hybrid models where we\neither simultaneously predict multiple neighbouring tokens per direction, or\nperform multi-directional decoding by partitioning the target sequence. These\nmethods achieve speedups to 4X-11X across different tasks at the cost of <1\nBLEU or <0.5 ROUGE (on average). Source code is released at\nhttps://github.com/bzhangGo/zero.", "published": "2020-10-27 17:38:51", "link": "http://arxiv.org/abs/2010.14481v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DGST: a Dual-Generator Network for Text Style Transfer", "abstract": "We propose DGST, a novel and simple Dual-Generator network architecture for\ntext Style Transfer. Our model employs two generators only, and does not rely\non any discriminators or parallel corpus for training. Both quantitative and\nqualitative experiments on the Yelp and IMDb datasets show that our model gives\ncompetitive performance compared to several strong baselines with more\ncomplicated architecture designs.", "published": "2020-10-27 18:54:51", "link": "http://arxiv.org/abs/2010.14557v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language ID in the Wild: Unexpected Challenges on the Path to a\n  Thousand-Language Web Text Corpus", "abstract": "Large text corpora are increasingly important for a wide variety of Natural\nLanguage Processing (NLP) tasks, and automatic language identification (LangID)\nis a core technology needed to collect such datasets in a multilingual context.\nLangID is largely treated as solved in the literature, with models reported\nthat achieve over 90% average F1 on as many as 1,366 languages. We train LangID\nmodels on up to 1,629 languages with comparable quality on held-out test sets,\nbut find that human-judged LangID accuracy for web-crawl text corpora created\nusing these models is only around 5% for many lower-resource languages,\nsuggesting a need for more robust evaluation. Further analysis revealed a\nvariety of error modes, arising from domain mismatch, class imbalance, language\nsimilarity, and insufficiently expressive models. We propose two classes of\ntechniques to mitigate these errors: wordlist-based tunable-precision filters\n(for which we release curated lists in about 500 languages) and\ntransformer-based semi-supervised LangID models, which increase median dataset\nprecision from 5.5% to 71.2%. These techniques enable us to create an initial\ndata set covering 100K or more relatively clean sentences in each of 500+\nlanguages, paving the way towards a 1,000-language web text corpus.", "published": "2020-10-27 19:29:17", "link": "http://arxiv.org/abs/2010.14571v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Dictionary and Term Variation Analysis for COVID-19 and\n  SARS-CoV-2", "abstract": "The number of unique terms in the scientific literature used to refer to\neither SARS-CoV-2 or COVID-19 is remarkably large and has continued to increase\nrapidly despite well-established standardized terms. This high degree of term\nvariation makes high recall identification of these important entities\ndifficult. In this manuscript we present an extensive dictionary of terms used\nin the literature to refer to SARS-CoV-2 and COVID-19. We use a rule-based\napproach to iteratively generate new term variants, then locate these variants\nin a large text corpus. We compare our dictionary to an extensive collection of\nterminological resources, demonstrating that our resource provides a\nsubstantial number of additional terms. We use our dictionary to analyze the\nusage of SARS-CoV-2 and COVID-19 terms over time and show that the number of\nunique terms continues to grow rapidly. Our dictionary is freely available at\nhttps://github.com/ncbi-nlp/CovidTermVar.", "published": "2020-10-27 19:51:53", "link": "http://arxiv.org/abs/2010.14588v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "DualTKB: A Dual Learning Bridge between Text and Knowledge Base", "abstract": "In this work, we present a dual learning approach for unsupervised text to\npath and path to text transfers in Commonsense Knowledge Bases (KBs). We\ninvestigate the impact of weak supervision by creating a weakly supervised\ndataset and show that even a slight amount of supervision can significantly\nimprove the model performance and enable better-quality transfers. We examine\ndifferent model architectures, and evaluation metrics, proposing a novel\nCommonsense KB completion metric tailored for generative models. Extensive\nexperimental results show that the proposed method compares very favorably to\nthe existing baselines. This approach is a viable step towards a more advanced\nsystem for automatic KB construction/expansion and the reverse operation of KB\nconversion to coherent textual descriptions.", "published": "2020-10-27 22:56:18", "link": "http://arxiv.org/abs/2010.14660v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transformer in action: a comparative study of transformer-based acoustic\n  models for large scale speech recognition applications", "abstract": "In this paper, we summarize the application of transformer and its streamable\nvariant, Emformer based acoustic model for large scale speech recognition\napplications. We compare the transformer based acoustic models with their LSTM\ncounterparts on industrial scale tasks. Specifically, we compare Emformer with\nlatency-controlled BLSTM (LCBLSTM) on medium latency tasks and LSTM on low\nlatency tasks. On a low latency voice assistant task, Emformer gets 24% to 26%\nrelative word error rate reductions (WERRs). For medium latency scenarios,\ncomparing with LCBLSTM with similar model size and latency, Emformer gets\nsignificant WERR across four languages in video captioning datasets with 2-3\ntimes inference real-time factors reduction.", "published": "2020-10-27 23:04:21", "link": "http://arxiv.org/abs/2010.14665v2", "categories": ["cs.CL", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Emotion recognition by fusing time synchronous and time asynchronous\n  representations", "abstract": "In this paper, a novel two-branch neural network model structure is proposed\nfor multimodal emotion recognition, which consists of a time synchronous branch\n(TSB) and a time asynchronous branch (TAB). To capture correlations between\neach word and its acoustic realisation, the TSB combines speech and text\nmodalities at each input window frame and then does pooling across time to form\na single embedding vector. The TAB, by contrast, provides cross-utterance\ninformation by integrating sentence text embeddings from a number of context\nutterances into another embedding vector. The final emotion classification uses\nboth the TSB and the TAB embeddings. Experimental results on the IEMOCAP\ndataset demonstrate that the two-branch structure achieves state-of-the-art\nresults in 4-way classification with all common test setups. When using\nautomatic speech recognition (ASR) output instead of manually transcribed\nreference text, it is shown that the cross-utterance information considerably\nimproves the robustness against ASR errors. Furthermore, by incorporating an\nextra class for all the other emotions, the final 5-way classification system\nwith ASR hypotheses can be viewed as a prototype for more realistic emotion\nrecognition systems.", "published": "2020-10-27 07:14:31", "link": "http://arxiv.org/abs/2010.14102v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Co-attentional Transformers for Story-Based Video Understanding", "abstract": "Inspired by recent trends in vision and language learning, we explore\napplications of attention mechanisms for visio-lingual fusion within an\napplication to story-based video understanding. Like other video-based QA\ntasks, video story understanding requires agents to grasp complex temporal\ndependencies. However, as it focuses on the narrative aspect of video it also\nrequires understanding of the interactions between different characters, as\nwell as their actions and their motivations. We propose a novel co-attentional\ntransformer model to better capture long-term dependencies seen in visual\nstories such as dramas and measure its performance on the video question\nanswering task. We evaluate our approach on the recently introduced DramaQA\ndataset which features character-centered video story understanding questions.\nOur model outperforms the baseline model by 8 percentage points overall, at\nleast 4.95 and up to 12.8 percentage points on all difficulty levels and\nmanages to beat the winner of the DramaQA challenge.", "published": "2020-10-27 07:17:09", "link": "http://arxiv.org/abs/2010.14104v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Global Sentiment Analysis Of COVID-19 Tweets Over Time", "abstract": "The Coronavirus pandemic has affected the normal course of life. People\naround the world have taken to social media to express their opinions and\ngeneral emotions regarding this phenomenon that has taken over the world by\nstorm. The social networking site, Twitter showed an unprecedented increase in\ntweets related to the novel Coronavirus in a very short span of time. This\npaper presents the global sentiment analysis of tweets related to Coronavirus\nand how the sentiment of people in different countries has changed over time.\nFurthermore, to determine the impact of Coronavirus on daily aspects of life,\ntweets related to Work From Home (WFH) and Online Learning were scraped and the\nchange in sentiment over time was observed. In addition, various Machine\nLearning models such as Long Short Term Memory (LSTM) and Artificial Neural\nNetworks (ANN) were implemented for sentiment classification and their\naccuracies were determined. Exploratory data analysis was also performed for a\ndataset providing information about the number of confirmed cases on a per-day\nbasis in a few of the worst-hit countries to provide a comparison between the\nchange in sentiment with the change in cases since the start of this pandemic\ntill June 2020.", "published": "2020-10-27 12:10:10", "link": "http://arxiv.org/abs/2010.14234v2", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Discovering and Interpreting Biased Concepts in Online Communities", "abstract": "Language carries implicit human biases, functioning both as a reflection and\na perpetuation of stereotypes that people carry with them. Recently, ML-based\nNLP methods such as word embeddings have been shown to learn such language\nbiases with striking accuracy. This capability of word embeddings has been\nsuccessfully exploited as a tool to quantify and study human biases. However,\nprevious studies only consider a predefined set of biased concepts to attest\n(e.g., whether gender is more or less associated with particular jobs), or just\ndiscover biased words without helping to understand their meaning at the\nconceptual level. As such, these approaches can be either unable to find biased\nconcepts that have not been defined in advance, or the biases they find are\ndifficult to interpret and study. This could make existing approaches\nunsuitable to discover and interpret biases in online communities, as such\ncommunities may carry different biases than those in mainstream culture. This\npaper improves upon, extends, and evaluates our previous data-driven method to\nautomatically discover and help interpret biased concepts encoded in word\nembeddings. We apply this approach to study the biased concepts present in the\nlanguage used in online communities and experimentally show the validity and\nstability of our method", "published": "2020-10-27 17:07:12", "link": "http://arxiv.org/abs/2010.14448v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50, 68T09, 91D30"], "primary_category": "cs.CL"}
{"title": "Dynamic Boundary Time Warping for Sub-sequence Matching with Few\n  Examples", "abstract": "The paper presents a novel method of finding a fragment in a long temporal\nsequence similar to the set of shorter sequences. We are the first to propose\nan algorithm for such a search that does not rely on computing the average\nsequence from query examples. Instead, we use query examples as is, utilizing\nall of them simultaneously. The introduced method based on the Dynamic Time\nWarping (DTW) technique is suited explicitly for few-shot query-by-example\nretrieval tasks. We evaluate it on two different few-shot problems from the\nfield of Natural Language Processing. The results show it either outperforms\nbaselines and previous approaches or achieves comparable results when a low\nnumber of examples is available.", "published": "2020-10-27 17:23:18", "link": "http://arxiv.org/abs/2010.14464v2", "categories": ["cs.DS", "cs.CL", "cs.IR"], "primary_category": "cs.DS"}
{"title": "It's All in the Name: A Character Based Approach To Infer Religion", "abstract": "Demographic inference from text has received a surge of attention in the\nfield of natural language processing in the last decade. In this paper, we use\npersonal names to infer religion in South Asia - where religion is a salient\nsocial division, and yet, disaggregated data on it remains scarce. Existing\nwork predicts religion using dictionary based method, and therefore, can not\nclassify unseen names. We use character based models which learn character\npatterns and, therefore, can classify unseen names as well with high accuracy.\nThese models are also much faster and can easily be scaled to large data sets.\nWe improve our classifier by combining the name of an individual with that of\ntheir parent/spouse and achieve remarkably high accuracy. Finally, we trace the\nclassification decisions of a convolutional neural network model using\nlayer-wise relevance propagation which can explain the predictions of complex\nnon-linear classifiers and circumvent their purported black box nature. We show\nhow character patterns learned by the classifier are rooted in the linguistic\norigins of names.", "published": "2020-10-27 17:38:11", "link": "http://arxiv.org/abs/2010.14479v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cascaded encoders for unifying streaming and non-streaming ASR", "abstract": "End-to-end (E2E) automatic speech recognition (ASR) models, by now, have\nshown competitive performance on several benchmarks. These models are\nstructured to either operate in streaming or non-streaming mode. This work\npresents cascaded encoders for building a single E2E ASR model that can operate\nin both these modes simultaneously. The proposed model consists of streaming\nand non-streaming encoders. Input features are first processed by the streaming\nencoder; the non-streaming encoder operates exclusively on the output of the\nstreaming encoder. A single decoder then learns to decode either using the\noutput of the streaming or the non-streaming encoder. Results show that this\nmodel achieves similar word error rates (WER) as a standalone streaming model\nwhen operating in streaming mode, and obtains 10% -- 27% relative improvement\nwhen operating in non-streaming mode. Our results also show that the proposed\napproach outperforms existing E2E two-pass models, especially on long-form\nspeech.", "published": "2020-10-27 20:59:50", "link": "http://arxiv.org/abs/2010.14606v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning Contextualised Cross-lingual Word Embeddings and Alignments for\n  Extremely Low-Resource Languages Using Parallel Corpora", "abstract": "We propose a new approach for learning contextualised cross-lingual word\nembeddings based on a small parallel corpus (e.g. a few hundred sentence\npairs). Our method obtains word embeddings via an LSTM encoder-decoder model\nthat simultaneously translates and reconstructs an input sentence. Through\nsharing model parameters among different languages, our model jointly trains\nthe word embeddings in a common cross-lingual space. We also propose to combine\nword and subword embeddings to make use of orthographic similarities across\ndifferent languages. We base our experiments on real-world data from endangered\nlanguages, namely Yongning Na, Shipibo-Konibo, and Griko. Our experiments on\nbilingual lexicon induction and word alignment tasks show that our model\noutperforms existing methods by a large margin for most language pairs. These\nresults demonstrate that, contrary to common belief, an encoder-decoder\ntranslation model is beneficial for learning cross-lingual representations even\nin extremely low-resource conditions. Furthermore, our model also works well on\nhigh-resource conditions, achieving state-of-the-art performance on a\nGerman-English word-alignment task.", "published": "2020-10-27 22:24:01", "link": "http://arxiv.org/abs/2010.14649v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Acoustic echo cancellation with the dual-signal transformation LSTM\n  network", "abstract": "This paper applies the dual-signal transformation LSTM network (DTLN) to the\ntask of real-time acoustic echo cancellation (AEC). The DTLN combines a\nshort-time Fourier transformation and a learned feature representation in a\nstacked network approach, which enables robust information processing in the\ntime-frequency and in the time domain, which also includes phase information.\nThe model is only trained on 60~h of real and synthetic echo scenarios. The\ntraining setup includes multi-lingual speech, data augmentation, additional\nnoise and reverberation to create a model that should generalize well to a\nlarge variety of real-world conditions. The DTLN approach produces\nstate-of-the-art performance on clean and noisy echo conditions reducing\nacoustic echo and additional noise robustly. The method outperforms the\nAEC-Challenge baseline by 0.30 in terms of Mean Opinion Score (MOS).", "published": "2020-10-27 14:54:04", "link": "http://arxiv.org/abs/2010.14337v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Effective Decoder Masking for Transformer Based End-to-End Speech\n  Recognition", "abstract": "The attention-based encoder-decoder modeling paradigm has achieved promising\nresults on a variety of speech processing tasks like automatic speech\nrecognition (ASR), text-to-speech (TTS) and among others. This paradigm takes\nadvantage of the generalization ability of neural networks to learn a direct\nmapping from an input sequence to an output sequence, without recourse to prior\nknowledge such as audio-text alignments or pronunciation lexicons. However, ASR\nmodels stemming from this paradigm are prone to overfitting, especially when\nthe training data is limited. Inspired by SpecAugment and BERT-like masked\nlanguage modeling, we propose in the paper a decoder masking based training\napproach for end-to-end (E2E) ASR models. During the training phase we randomly\nreplace some portions of the decoder's historical text input with the symbol\n[mask], in order to encourage the decoder to robustly output a correct token\neven when parts of its decoding history are masked or corrupted. The proposed\napproach is instantiated with the top-of-the-line transformer-based E2E ASR\nmodel. Extensive experiments on the Librispeech960h and TedLium2 benchmark\ndatasets demonstrate the superior performance of our approach in comparison to\nsome existing strong E2E ASR systems.", "published": "2020-10-27 05:15:05", "link": "http://arxiv.org/abs/2010.14764v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "One-class Learning Towards Synthetic Voice Spoofing Detection", "abstract": "Human voices can be used to authenticate the identity of the speaker, but the\nautomatic speaker verification (ASV) systems are vulnerable to voice spoofing\nattacks, such as impersonation, replay, text-to-speech, and voice conversion.\nRecently, researchers developed anti-spoofing techniques to improve the\nreliability of ASV systems against spoofing attacks. However, most methods\nencounter difficulties in detecting unknown attacks in practical use, which\noften have different statistical distributions from known attacks. Especially,\nthe fast development of synthetic voice spoofing algorithms is generating\nincreasingly powerful attacks, putting the ASV systems at risk of unseen\nattacks. In this work, we propose an anti-spoofing system to detect unknown\nsynthetic voice spoofing attacks (i.e., text-to-speech or voice conversion)\nusing one-class learning. The key idea is to compact the bona fide speech\nrepresentation and inject an angular margin to separate the spoofing attacks in\nthe embedding space. Without resorting to any data augmentation methods, our\nproposed system achieves an equal error rate (EER) of 2.19% on the evaluation\nset of ASVspoof 2019 Challenge logical access scenario, outperforming all\nexisting single systems (i.e., those without model ensemble).", "published": "2020-10-27 02:13:35", "link": "http://arxiv.org/abs/2010.13995v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Universal ASR: Unifying Streaming and Non-Streaming ASR Using a Single\n  Encoder-Decoder Model", "abstract": "Recently, online end-to-end ASR has gained increasing attention. However, the\nperformance of online systems still lags far behind that of offline systems,\nwith a large gap in quality of recognition. For specific scenarios, we can\ntrade-off between performance and latency, and can train multiple systems with\ndifferent delays to match the performance and latency requirements of various\napplication scenarios. In this work, in contrast to trading-off between\nperformance and latency, we envisage a single system that can match the needs\nof different scenarios. We propose a novel architecture, termed Universal ASR\nthat can unify streaming and non-streaming ASR models into one system. The\nembedded streaming ASR model can configure different delays according to\nrequirements to obtain real-time recognition results, while the non-streaming\nmodel is able to refresh the final recognition result for better performance.\nWe have evaluated our approach on the public AISHELL-2 benchmark and an\nindustrial-level 20,000-hour Mandarin speech recognition task. The experimental\nresults show that the Universal ASR provides an efficient mechanism to\nintegrate streaming and non-streaming models that can recognize speech quickly\nand accurately. On the AISHELL-2 task, Universal ASR comfortably outperforms\nother state-of-the-art systems.", "published": "2020-10-27 06:46:00", "link": "http://arxiv.org/abs/2010.14099v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FragmentVC: Any-to-Any Voice Conversion by End-to-End Extracting and\n  Fusing Fine-Grained Voice Fragments With Attention", "abstract": "Any-to-any voice conversion aims to convert the voice from and to any\nspeakers even unseen during training, which is much more challenging compared\nto one-to-one or many-to-many tasks, but much more attractive in real-world\nscenarios. In this paper we proposed FragmentVC, in which the latent phonetic\nstructure of the utterance from the source speaker is obtained from Wav2Vec\n2.0, while the spectral features of the utterance(s) from the target speaker\nare obtained from log mel-spectrograms. By aligning the hidden structures of\nthe two different feature spaces with a two-stage training process, FragmentVC\nis able to extract fine-grained voice fragments from the target speaker\nutterance(s) and fuse them into the desired utterance, all based on the\nattention mechanism of Transformer as verified with analysis on attention maps,\nand is accomplished end-to-end. This approach is trained with reconstruction\nloss only without any disentanglement considerations between content and\nspeaker information and doesn't require parallel data. Objective evaluation\nbased on speaker verification and subjective evaluation with MOS both showed\nthat this approach outperformed SOTA approaches, such as AdaIN-VC and AutoVC.", "published": "2020-10-27 09:21:03", "link": "http://arxiv.org/abs/2010.14150v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "SSLIDE: Sound Source Localization for Indoors based on Deep Learning", "abstract": "This paper presents SSLIDE, Sound Source Localization for Indoors using DEep\nlearning, which applies deep neural networks (DNNs) with encoder-decoder\nstructure to localize sound sources with random positions in a continuous\nspace. The spatial features of sound signals received by each microphone are\nextracted and represented as likelihood surfaces for the sound source locations\nin each point. Our DNN consists of an encoder network followed by two decoders.\nThe encoder obtains a compressed representation of the input likelihoods. One\ndecoder resolves the multipath caused by reverberation, and the other decoder\nestimates the source location. Experiments based on both the simulated and\nexperimental data show that our method can not only outperform multiple signal\nclassification (MUSIC), steered response power with phase transform (SRP-PHAT),\nsparse Bayesian learning (SBL), and a competing convolutional neural network\n(CNN) approach in the reverberant environment but also achieve a good\ngeneralization performance.", "published": "2020-10-27 16:36:15", "link": "http://arxiv.org/abs/2010.14420v2", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "ByteCover: Cover Song Identification via Multi-Loss Training", "abstract": "We present in this paper ByteCover, which is a new feature learning method\nfor cover song identification (CSI). ByteCover is built based on the classical\nResNet model, and two major improvements are designed to further enhance the\ncapability of the model for CSI. In the first improvement, we introduce the\nintegration of instance normalization (IN) and batch normalization (BN) to\nbuild IBN blocks, which are major components of our ResNet-IBN model. With the\nhelp of the IBN blocks, our CSI model can learn features that are invariant to\nthe changes of musical attributes such as key, tempo, timbre and genre, while\npreserving the version information. In the second improvement, we employ the\nBNNeck method to allow a multi-loss training and encourage our method to\njointly optimize a classification loss and a triplet loss, and by this means,\nthe inter-class discrimination and intra-class compactness of cover songs, can\nbe ensured at the same time. A set of experiments demonstrated the\neffectiveness and efficiency of ByteCover on multiple datasets, and in the\nDa-TACOS dataset, ByteCover outperformed the best competitive system by 20.9\\%.", "published": "2020-10-27 02:59:54", "link": "http://arxiv.org/abs/2010.14022v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Phase Aware Speech Enhancement using Realisation of Complex-valued LSTM", "abstract": "Most of the deep learning based speech enhancement (SE) methods rely on\nestimating the magnitude spectrum of the clean speech signal from the observed\nnoisy speech signal, either by magnitude spectral masking or regression. These\nmethods reuse the noisy phase while synthesizing the time-domain waveform from\nthe estimated magnitude spectrum. However, there have been recent works\nhighlighting the importance of phase in SE. There was an attempt to estimate\nthe complex ratio mask taking phase into account using complex-valued\nfeed-forward neural network (FFNN). But FFNNs cannot capture the sequential\ninformation essential for phase estimation. In this work, we propose a\nrealisation of complex-valued long short-term memory (RCLSTM) network to\nestimate the complex ratio mask (CRM) using sequential information along time.\nThe proposed RCLSTM is designed to process the complex-valued sequences using\ncomplex arithmetic, and hence it preserves the dependencies between the real\nand imaginary parts of CRM and thereby the phase. The proposed method is\nevaluated on the noisy speech mixtures formed from the Voice-Bank corpus and\nDEMAND database. When compared to real value based masking methods, the\nproposed RCLSTM improves over them in several objective measures including\nperceptual evaluation of speech quality (PESQ), in which it improves by over\n4.3%", "published": "2020-10-27 08:16:58", "link": "http://arxiv.org/abs/2010.14122v1", "categories": ["eess.AS", "cs.AI", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Parallel waveform synthesis based on generative adversarial networks\n  with voicing-aware conditional discriminators", "abstract": "This paper proposes voicing-aware conditional discriminators for Parallel\nWaveGAN-based waveform synthesis systems. In this framework, we adopt a\nprojection-based conditioning method that can significantly improve the\ndiscriminator's performance. Furthermore, the conventional discriminator is\nseparated into two waveform discriminators for modeling voiced and unvoiced\nspeech. As each discriminator learns the distinctive characteristics of the\nharmonic and noise components, respectively, the adversarial training process\nbecomes more efficient, allowing the generator to produce more realistic speech\nwaveforms. Subjective test results demonstrate the superiority of the proposed\nmethod over the conventional Parallel WaveGAN and WaveNet systems. In\nparticular, our speaker-independently trained model within a FastSpeech 2 based\ntext-to-speech framework achieves the mean opinion scores of 4.20, 4.18, 4.21,\nand 4.31 for four Japanese speakers, respectively.", "published": "2020-10-27 09:26:30", "link": "http://arxiv.org/abs/2010.14151v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Rule-embedded network for audio-visual voice activity detection in live\n  musical video streams", "abstract": "Detecting anchor's voice in live musical streams is an important\npreprocessing for music and speech signal processing. Existing approaches to\nvoice activity detection (VAD) primarily rely on audio, however, audio-based\nVAD is difficult to effectively focus on the target voice in noisy\nenvironments. With the help of visual information, this paper proposes a\nrule-embedded network to fuse the audio-visual (A-V) inputs to help the model\nbetter detect target voice. The core role of the rule in the model is to\ncoordinate the relation between the bi-modal information and use visual\nrepresentations as the mask to filter out the information of non-target sound.\nExperiments show that: 1) with the help of cross-modal fusion by the proposed\nrule, the detection result of A-V branch outperforms that of audio branch; 2)\nthe performance of bi-modal model far outperforms that of audio-only models,\nindicating that the incorporation of both audio and visual signals is highly\nbeneficial for VAD. To attract more attention to the cross-modal music and\naudio signal processing, a new live musical video corpus with frame-level label\nis introduced.", "published": "2020-10-27 10:00:07", "link": "http://arxiv.org/abs/2010.14168v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "New interfaces for musical expression", "abstract": "The rapid evolution of electronics, digital media, advanced materials, and\nother areas of technology, is opening up unprecedented opportunities for\nmusical interface inventors and designers. The possibilities afforded by these\nnew technologies carry with them the challenges of a complex and often\nconfusing array of choices for musical composers and performers. New musical\ntechnologies are at least partly responsible for the current explosion of new\nmusical forms, some of which are controversial and challenge traditional\ndefinitions of music. Alternative musical controllers, currently the leading\nedge of the ongoing dialogue between technology and musical culture, involve\nmany of the issues covered at past CHI meetings. This workshop brings together\ninterface experts interested in musical controllers and musicians and composers\ninvolved in the development of new musical interfaces.", "published": "2020-10-27 11:59:44", "link": "http://arxiv.org/abs/2010.14228v1", "categories": ["cs.HC", "cs.SD", "eess.AS", "H.5.5"], "primary_category": "cs.HC"}
{"title": "Deep generative factorization for speech signal", "abstract": "Various information factors are blended in speech signals, which forms the\nprimary difficulty for most speech information processing tasks. An intuitive\nidea is to factorize speech signal into individual information factors (e.g.,\nphonetic content and speaker trait), though it turns out to be highly\nchallenging. This paper presents a speech factorization approach based on a\nnovel factorial discriminative normalization flow model (factorial DNF).\nExperiments conducted on a two-factor case that involves phonetic content and\nspeaker trait demonstrates that the proposed factorial DNF has powerful\ncapability to factorize speech signals and outperforms several comparative\nmodels in terms of information representation and manipulation.", "published": "2020-10-27 12:27:58", "link": "http://arxiv.org/abs/2010.14242v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Squeezing value of cross-domain labels: a decoupled scoring approach for\n  speaker verification", "abstract": "Domain mismatch often occurs in real applications and causes serious\nperformance reduction on speaker verification systems. The common wisdom is to\ncollect cross-domain data and train a multi-domain PLDA model, with the hope to\nlearn a domain-independent speaker subspace. In this paper, we firstly present\nan empirical study to show that simply adding cross-domain data does not help\nperformance in conditions with enrollment-test mismatch. Careful analysis shows\nthat this striking result is caused by the incoherent statistics between the\nenrollment and test conditions. Based on this analysis, we present a decoupled\nscoring approach that can maximally squeeze the value of cross-domain labels\nand obtain optimal verification scores when the enrollment and test are\nmismatched. When the statistics are coherent, the new formulation falls back to\nthe conventional PLDA. Experimental results on cross-channel test show that the\nproposed approach is highly effective and is a principle solution to domain\nmismatch.", "published": "2020-10-27 12:32:25", "link": "http://arxiv.org/abs/2010.14243v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Leveraging speaker attribute information using multi task learning for\n  speaker verification and diarization", "abstract": "Deep speaker embeddings have become the leading method for encoding speaker\nidentity in speaker recognition tasks. The embedding space should ideally\ncapture the variations between all possible speakers, encoding the multiple\nacoustic aspects that make up a speaker's identity, whilst being robust to\nnon-speaker acoustic variation. Deep speaker embeddings are normally trained\ndiscriminatively, predicting speaker identity labels on the training data. We\nhypothesise that additionally predicting speaker-related auxiliary variables --\nsuch as age and nationality -- may yield representations that are better able\nto generalise to unseen speakers. We propose a framework for making use of\nauxiliary label information, even when it is only available for speech corpora\nmismatched to the target application. On a test set of US Supreme Court\nrecordings, we show that by leveraging two additional forms of speaker\nattribute information derived respectively from the matched training data, and\nVoxCeleb corpus, we improve the performance of our deep speaker embeddings for\nboth verification and diarization tasks, achieving a relative improvement of\n26.2% in DER and 6.7% in EER compared to baselines using speaker labels only.\nThis improvement is obtained despite the auxiliary labels having been scraped\nfrom the web and being potentially noisy.", "published": "2020-10-27 13:10:51", "link": "http://arxiv.org/abs/2010.14269v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Upsampling artifacts in neural audio synthesis", "abstract": "A number of recent advances in neural audio synthesis rely on upsampling\nlayers, which can introduce undesired artifacts. In computer vision, upsampling\nartifacts have been studied and are known as checkerboard artifacts (due to\ntheir characteristic visual pattern). However, their effect has been overlooked\nso far in audio processing. Here, we address this gap by studying this problem\nfrom the audio signal processing perspective. We first show that the main\nsources of upsampling artifacts are: (i) the tonal and filtering artifacts\nintroduced by problematic upsampling operators, and (ii) the spectral replicas\nthat emerge while upsampling. We then compare different upsampling layers,\nshowing that nearest neighbor upsamplers can be an alternative to the\nproblematic (but state-of-the-art) transposed and subpixel convolutions which\nare prone to introduce tonal artifacts.", "published": "2020-10-27 15:09:28", "link": "http://arxiv.org/abs/2010.14356v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Remixing Music with Visual Conditioning", "abstract": "We propose a visually conditioned music remixing system by incorporating deep\nvisual and audio models. The method is based on a state of the art audio-visual\nsource separation model which performs music instrument source separation with\nvideo information. We modified the model to work with user-selected images\ninstead of videos as visual input during inference to enable separation of\naudio-only content. Furthermore, we propose a remixing engine that generalizes\nthe task of source separation into music remixing. The proposed method is able\nto achieve improved audio quality compared to remixing performed by the\nseparate-and-add method with a state-of-the-art audio-visual source separation\nmodel.", "published": "2020-10-27 19:12:08", "link": "http://arxiv.org/abs/2010.14565v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CopyPaste: An Augmentation Method for Speech Emotion Recognition", "abstract": "Data augmentation is a widely used strategy for training robust machine\nlearning models. It partially alleviates the problem of limited data for tasks\nlike speech emotion recognition (SER), where collecting data is expensive and\nchallenging. This study proposes CopyPaste, a perceptually motivated novel\naugmentation procedure for SER. Assuming that the presence of emotions other\nthan neutral dictates a speaker's overall perceived emotion in a recording,\nconcatenation of an emotional (emotion E) and a neutral utterance can still be\nlabeled with emotion E. We hypothesize that SER performance can be improved\nusing these concatenated utterances in model training. To verify this, three\nCopyPaste schemes are tested on two deep learning models: one trained\nindependently and another using transfer learning from an x-vector model, a\nspeaker recognition model. We observed that all three CopyPaste schemes improve\nSER performance on all the three datasets considered: MSP-Podcast, Crema-D, and\nIEMOCAP. Additionally, CopyPaste performs better than noise augmentation and,\nusing them together improves the SER performance further. Our experiments on\nnoisy test sets suggested that CopyPaste is effective even in noisy test\nconditions.", "published": "2020-10-27 20:52:47", "link": "http://arxiv.org/abs/2010.14602v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning Contextual Tag Embeddings for Cross-Modal Alignment of Audio\n  and Tags", "abstract": "Self-supervised audio representation learning offers an attractive\nalternative for obtaining generic audio embeddings, capable to be employed into\nvarious downstream tasks. Published approaches that consider both audio and\nwords/tags associated with audio do not employ text processing models that are\ncapable to generalize to tags unknown during training. In this work we propose\na method for learning audio representations using an audio autoencoder (AAE), a\ngeneral word embeddings model (WEM), and a multi-head self-attention (MHA)\nmechanism. MHA attends on the output of the WEM, providing a contextualized\nrepresentation of the tags associated with the audio, and we align the output\nof MHA with the output of the encoder of AAE using a contrastive loss. We\njointly optimize AAE and MHA and we evaluate the audio representations (i.e.\nthe output of the encoder of AAE) by utilizing them in three different\ndownstream tasks, namely sound, music genre, and music instrument\nclassification. Our results show that employing multi-head self-attention with\nmultiple heads in the tag-based network can induce better learned audio\nrepresentations.", "published": "2020-10-27 10:13:17", "link": "http://arxiv.org/abs/2010.14171v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
