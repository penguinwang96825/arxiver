{"title": "Scalable Zero-shot Entity Linking with Dense Entity Retrieval", "abstract": "This paper introduces a conceptually simple, scalable, and highly effective\nBERT-based entity linking model, along with an extensive evaluation of its\naccuracy-speed trade-off. We present a two-stage zero-shot linking algorithm,\nwhere each entity is defined only by a short textual description. The first\nstage does retrieval in a dense space defined by a bi-encoder that\nindependently embeds the mention context and the entity descriptions. Each\ncandidate is then re-ranked with a cross-encoder, that concatenates the mention\nand entity text. Experiments demonstrate that this approach is state of the art\non recent zero-shot benchmarks (6 point absolute gains) and also on more\nestablished non-zero-shot evaluations (e.g. TACKBP-2010), despite its relative\nsimplicity (e.g. no explicit entity embeddings or manually engineered mention\ntables). We also show that bi-encoder linking is very fast with nearest\nneighbour search (e.g. linking with 5.9 million candidates in 2 milliseconds),\nand that much of the accuracy gain from the more expensive cross-encoder can be\ntransferred to the bi-encoder via knowledge distillation. Our code and models\nare available at https://github.com/facebookresearch/BLINK.", "published": "2019-11-10 01:01:45", "link": "http://arxiv.org/abs/1911.03814v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Learning on the Latent Space for Diverse Dialog Generation", "abstract": "Generating relevant responses in a dialog is challenging, and requires not\nonly proper modeling of context in the conversation but also being able to\ngenerate fluent sentences during inference. In this paper, we propose a\ntwo-step framework based on generative adversarial nets for generating\nconditioned responses. Our model first learns a meaningful representation of\nsentences by autoencoding and then learns to map an input query to the response\nrepresentation, which is in turn decoded as a response sentence. Both\nquantitative and qualitative evaluations show that our model generates more\nfluent, relevant, and diverse responses than existing state-of-the-art methods.", "published": "2019-11-10 01:32:02", "link": "http://arxiv.org/abs/1911.03817v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generalizing Natural Language Analysis through Span-relation\n  Representations", "abstract": "Natural language processing covers a wide variety of tasks predicting syntax,\nsemantics, and information content, and usually each type of output is\ngenerated with specially designed architectures. In this paper, we provide the\nsimple insight that a great variety of tasks can be represented in a single\nunified format consisting of labeling spans and relations between spans, thus a\nsingle task-independent model can be used across different tasks. We perform\nextensive experiments to test this insight on 10 disparate tasks spanning\ndependency parsing (syntax), semantic role labeling (semantics), relation\nextraction (information content), aspect based sentiment analysis (sentiment),\nand many others, achieving performance comparable to state-of-the-art\nspecialized models. We further demonstrate benefits of multi-task learning, and\nalso show that the proposed method makes it easy to analyze differences and\nsimilarities in how the model handles different tasks. Finally, we convert\nthese datasets into a unified format to build a benchmark, which provides a\nholistic testbed for evaluating future models for generalized natural language\nanalysis.", "published": "2019-11-10 01:42:14", "link": "http://arxiv.org/abs/1911.03822v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Translationese as a Language in \"Multilingual\" NMT", "abstract": "Machine translation has an undesirable propensity to produce \"translationese\"\nartifacts, which can lead to higher BLEU scores while being liked less by human\nraters. Motivated by this, we model translationese and original (i.e. natural)\ntext as separate languages in a multilingual model, and pose the question: can\nwe perform zero-shot translation between original source text and original\ntarget text? There is no data with original source and original target, so we\ntrain sentence-level classifiers to distinguish translationese from original\ntarget text, and use this classifier to tag the training data for an NMT model.\nUsing this technique we bias the model to produce more natural outputs at test\ntime, yielding gains in human evaluation scores on both accuracy and fluency.\nAdditionally, we demonstrate that it is possible to bias the model to produce\ntranslationese and game the BLEU score, increasing it while decreasing\nhuman-rated quality. We analyze these models using metrics to measure the\ndegree of translationese in the output, and present an analysis of the\ncapriciousness of heuristically-based train-data tagging.", "published": "2019-11-10 01:43:22", "link": "http://arxiv.org/abs/1911.03823v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stylized Text Generation Using Wasserstein Autoencoders with a Mixture\n  of Gaussian Prior", "abstract": "Wasserstein autoencoders are effective for text generation. They do not\nhowever provide any control over the style and topic of the generated sentences\nif the dataset has multiple classes and includes different topics. In this\nwork, we present a semi-supervised approach for generating stylized sentences.\nOur model is trained on a multi-class dataset and learns the latent\nrepresentation of the sentences using a mixture of Gaussian prior without any\nadversarial losses. This allows us to generate sentences in the style of a\nspecified class or multiple classes by sampling from their corresponding prior\ndistributions. Moreover, we can train our model on relatively small datasets\nand learn the latent representation of a specified class by adding external\ndata with other styles/classes to our dataset. While a simple WAE or VAE cannot\ngenerate diverse sentences in this case, generated sentences with our approach\nare diverse, fluent, and preserve the style and the content of the desired\nclasses.", "published": "2019-11-10 02:06:23", "link": "http://arxiv.org/abs/1911.03828v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextualized End-to-End Neural Entity Linking", "abstract": "We propose yet another entity linking model (YELM) which links words to\nentities instead of spans. This overcomes any difficulties associated with the\nselection of good candidate mention spans and makes the joint training of\nmention detection (MD) and entity disambiguation (ED) easily possible. Our\nmodel is based on BERT and produces contextualized word embeddings which are\ntrained against a joint MD and ED objective. We achieve state-of-the-art\nresults on several standard entity linking (EL) datasets.", "published": "2019-11-10 02:26:19", "link": "http://arxiv.org/abs/1911.03834v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation", "abstract": "Models often easily learn biases present in the training data, and their\npredictions directly reflect this bias. We analyze gender bias in dialogue\ndata, and examine how this bias is actually amplified in subsequent generative\nchit-chat dialogue models. We measure gender bias in six existing dialogue\ndatasets, and focus on the most biased one, the multi-player text-based fantasy\nadventure dataset LIGHT, as a testbed for our bias mitigation techniques. The\nLIGHT dataset is highly imbalanced with respect to gender, containing\npredominantly male characters, likely because it is entirely collected by\ncrowdworkers and reflects common biases that exist in fantasy or medieval\nsettings. We consider three techniques to mitigate gender bias: counterfactual\ndata augmentation, targeted data collection, and bias controlled training. We\nshow that our proposed techniques mitigate gender bias in LIGHT by balancing\nthe genderedness of generated dialogue utterances and are particularly\neffective in combination. We quantify performance using various evaluation\nmethods---such as quantity of gendered words, a dialogue safety classifier, and\nhuman studies---all of which show that our models generate less gendered, but\nequally engaging chit-chat responses.", "published": "2019-11-10 03:10:50", "link": "http://arxiv.org/abs/1911.03842v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Say That! Making Inconsistent Dialogue Unlikely with Unlikelihood\n  Training", "abstract": "Generative dialogue models currently suffer from a number of problems which\nstandard maximum likelihood training does not address. They tend to produce\ngenerations that (i) rely too much on copying from the context, (ii) contain\nrepetitions within utterances, (iii) overuse frequent words, and (iv) at a\ndeeper level, contain logical flaws. In this work we show how all of these\nproblems can be addressed by extending the recently introduced unlikelihood\nloss (Welleck et al., 2019) to these cases. We show that appropriate loss\nfunctions which regularize generated outputs to match human distributions are\neffective for the first three issues. For the last important general issue, we\nshow applying unlikelihood to collected data of what a model should not do is\neffective for improving logical consistency, potentially paving the way to\ngenerative models with greater reasoning ability. We demonstrate the efficacy\nof our approach across several dialogue tasks.", "published": "2019-11-10 05:53:40", "link": "http://arxiv.org/abs/1911.03860v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Annotation of Phenotypic Abnormalities via Semantic Latent\n  Representations on Electronic Health Records", "abstract": "The extraction of phenotype information which is naturally contained in\nelectronic health records (EHRs) has been found to be useful in various\nclinical informatics applications such as disease diagnosis. However, due to\nimprecise descriptions, lack of gold standards and the demand for efficiency,\nannotating phenotypic abnormalities on millions of EHR narratives is still\nchallenging. In this work, we propose a novel unsupervised deep learning\nframework to annotate the phenotypic abnormalities from EHRs via semantic\nlatent representations. The proposed framework takes the advantage of Human\nPhenotype Ontology (HPO), which is a knowledge base of phenotypic\nabnormalities, to standardize the annotation results. Experiments have been\nconducted on 52,722 EHRs from MIMIC-III dataset. Quantitative and qualitative\nanalysis have shown the proposed framework achieves state-of-the-art annotation\nperformance and computational efficiency compared with other methods.", "published": "2019-11-10 06:04:35", "link": "http://arxiv.org/abs/1911.03862v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Neuro-Symbolic Knowledge Graph Construction for Zero-shot\n  Commonsense Question Answering", "abstract": "Understanding narratives requires reasoning about implicit world knowledge\nrelated to the causes, effects, and states of situations described in text. At\nthe core of this challenge is how to access contextually relevant knowledge on\ndemand and reason over it.\n  In this paper, we present initial studies toward zero-shot commonsense\nquestion answering by formulating the task as inference over dynamically\ngenerated commonsense knowledge graphs. In contrast to previous studies for\nknowledge integration that rely on retrieval of existing knowledge from static\nknowledge graphs, our study requires commonsense knowledge integration where\ncontextually relevant knowledge is often not present in existing knowledge\nbases. Therefore, we present a novel approach that generates\ncontextually-relevant symbolic knowledge structures on demand using generative\nneural commonsense knowledge models.\n  Empirical results on two datasets demonstrate the efficacy of our\nneuro-symbolic approach for dynamically constructing knowledge graphs for\nreasoning. Our approach achieves significant performance boosts over pretrained\nlanguage models and vanilla knowledge models, all while providing interpretable\nreasoning paths for its predictions.", "published": "2019-11-10 08:20:20", "link": "http://arxiv.org/abs/1911.03876v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Social Bias Frames: Reasoning about Social and Power Implications of\n  Language", "abstract": "Warning: this paper contains content that may be offensive or upsetting.\n  Language has the power to reinforce stereotypes and project social biases\nonto others. At the core of the challenge is that it is rarely what is stated\nexplicitly, but rather the implied meanings, that frame people's judgments\nabout others. For example, given a statement that \"we shouldn't lower our\nstandards to hire more women,\" most listeners will infer the implicature\nintended by the speaker -- that \"women (candidates) are less qualified.\" Most\nsemantic formalisms, to date, do not capture such pragmatic implications in\nwhich people express social biases and power differentials in language.\n  We introduce Social Bias Frames, a new conceptual formalism that aims to\nmodel the pragmatic frames in which people project social biases and\nstereotypes onto others. In addition, we introduce the Social Bias Inference\nCorpus to support large-scale modelling and evaluation with 150k structured\nannotations of social media posts, covering over 34k implications about a\nthousand demographic groups.\n  We then establish baseline approaches that learn to recover Social Bias\nFrames from unstructured text. We find that while state-of-the-art neural\nmodels are effective at high-level categorization of whether a given statement\nprojects unwanted social bias (80% F1), they are not effective at spelling out\nmore detailed explanations in terms of Social Bias Frames. Our study motivates\nfuture work that combines structured pragmatic inference with commonsense\nreasoning on social implications.", "published": "2019-11-10 10:38:27", "link": "http://arxiv.org/abs/1911.03891v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "INSET: Sentence Infilling with INter-SEntential Transformer", "abstract": "Missing sentence generation (or sentence infilling) fosters a wide range of\napplications in natural language generation, such as document auto-completion\nand meeting note expansion. This task asks the model to generate intermediate\nmissing sentences that can syntactically and semantically bridge the\nsurrounding context. Solving the sentence infilling task requires techniques in\nnatural language processing ranging from understanding to discourse-level\nplanning to generation. In this paper, we propose a framework to decouple the\nchallenge and address these three aspects respectively, leveraging the power of\nexisting large-scale pre-trained models such as BERT and GPT-2. We empirically\ndemonstrate the effectiveness of our model in learning a sentence\nrepresentation for generation and further generating a missing sentence that\nfits the context.", "published": "2019-11-10 10:41:52", "link": "http://arxiv.org/abs/1911.03892v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CamemBERT: a Tasty French Language Model", "abstract": "Pretrained language models are now ubiquitous in Natural Language Processing.\nDespite their success, most available models have either been trained on\nEnglish data or on the concatenation of data in multiple languages. This makes\npractical use of such models --in all languages except English-- very limited.\nIn this paper, we investigate the feasibility of training monolingual\nTransformer-based language models for other languages, taking French as an\nexample and evaluating our language models on part-of-speech tagging,\ndependency parsing, named entity recognition and natural language inference\ntasks. We show that the use of web crawled data is preferable to the use of\nWikipedia data. More surprisingly, we show that a relatively small web crawled\ndataset (4GB) leads to results that are as good as those obtained using larger\ndatasets (130+GB). Our best performing model CamemBERT reaches or improves the\nstate of the art in all four downstream tasks.", "published": "2019-11-10 10:46:37", "link": "http://arxiv.org/abs/1911.03894v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Two-Headed Monster And Crossed Co-Attention Networks", "abstract": "This paper presents some preliminary investigations of a new co-attention\nmechanism in neural transduction models. We propose a paradigm, termed\nTwo-Headed Monster (THM), which consists of two symmetric encoder modules and\none decoder module connected with co-attention. As a specific and concrete\nimplementation of THM, Crossed Co-Attention Networks (CCNs) are designed based\non the Transformer model. We demonstrate CCNs on WMT 2014 EN-DE and WMT 2016\nEN-FI translation tasks and our model outperforms the strong Transformer\nbaseline by 0.51 (big) and 0.74 (base) BLEU points on EN-DE and by 0.17 (big)\nand 0.47 (base) BLEU points on EN-FI.", "published": "2019-11-10 10:55:12", "link": "http://arxiv.org/abs/1911.03897v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Re-evaluation of Knowledge Graph Completion Methods", "abstract": "Knowledge Graph Completion (KGC) aims at automatically predicting missing\nlinks for large-scale knowledge graphs. A vast number of state-of-the-art KGC\ntechniques have got published at top conferences in several research fields,\nincluding data mining, machine learning, and natural language processing.\nHowever, we notice that several recent papers report very high performance,\nwhich largely outperforms previous state-of-the-art methods. In this paper, we\nfind that this can be attributed to the inappropriate evaluation protocol used\nby them and propose a simple evaluation protocol to address this problem. The\nproposed protocol is robust to handle bias in the model, which can\nsubstantially affect the final results. We conduct extensive experiments and\nreport the performance of several existing methods using our protocol. The\nreproducible code has been made publicly available", "published": "2019-11-10 11:19:08", "link": "http://arxiv.org/abs/1911.03903v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Noise Matters for Neural Natural Language Generation", "abstract": "Neural natural language generation (NNLG) systems are known for their\npathological outputs, i.e. generating text which is unrelated to the input\nspecification. In this paper, we show the impact of semantic noise on\nstate-of-the-art NNLG models which implement different semantic control\nmechanisms. We find that cleaned data can improve semantic correctness by up to\n97%, while maintaining fluency. We also find that the most common error is\nomitting information, rather than hallucination.", "published": "2019-11-10 11:24:02", "link": "http://arxiv.org/abs/1911.03905v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Efficient Dialogue State Tracking by Selectively Overwriting Memory", "abstract": "Recent works in dialogue state tracking (DST) focus on an open\nvocabulary-based setting to resolve scalability and generalization issues of\nthe predefined ontology-based approaches. However, they are inefficient in that\nthey predict the dialogue state at every turn from scratch. Here, we consider\ndialogue state as an explicit fixed-sized memory and propose a selectively\noverwriting mechanism for more efficient DST. This mechanism consists of two\nsteps: (1) predicting state operation on each of the memory slots, and (2)\noverwriting the memory with new values, of which only a few are generated\naccording to the predicted state operations. Our method decomposes DST into two\nsub-tasks and guides the decoder to focus only on one of the tasks, thus\nreducing the burden of the decoder. This enhances the effectiveness of training\nand DST performance. Our SOM-DST (Selectively Overwriting Memory for Dialogue\nState Tracking) model achieves state-of-the-art joint goal accuracy with 51.72%\nin MultiWOZ 2.0 and 53.01% in MultiWOZ 2.1 in an open vocabulary-based DST\nsetting. In addition, we analyze the accuracy gaps between the current and the\nground truth-given situations and suggest that it is a promising direction to\nimprove state operation prediction to boost the DST performance.", "published": "2019-11-10 11:27:53", "link": "http://arxiv.org/abs/1911.03906v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contract Discovery: Dataset and a Few-Shot Semantic Retrieval Challenge\n  with Competitive Baselines", "abstract": "We propose a new shared task of semantic retrieval from legal texts, in which\na so-called contract discovery is to be performed, where legal clauses are\nextracted from documents, given a few examples of similar clauses from other\nlegal acts. The task differs substantially from conventional NLI and shared\ntasks on legal information extraction (e.g., one has to identify text span\ninstead of a single document, page, or paragraph). The specification of the\nproposed task is followed by an evaluation of multiple solutions within the\nunified framework proposed for this branch of methods. It is shown that\nstate-of-the-art pretrained encoders fail to provide satisfactory results on\nthe task proposed. In contrast, Language Model-based solutions perform better,\nespecially when unsupervised fine-tuning is applied. Besides the ablation\nstudies, we addressed questions regarding detection accuracy for relevant text\nfragments depending on the number of examples available. In addition to the\ndataset and reference results, LMs specialized in the legal domain were made\npublicly available.", "published": "2019-11-10 11:50:09", "link": "http://arxiv.org/abs/1911.03911v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Monolingual Pretrained Models Help Cross-Lingual Classification?", "abstract": "Multilingual pretrained language models (such as multilingual BERT) have\nachieved impressive results for cross-lingual transfer. However, due to the\nconstant model capacity, multilingual pre-training usually lags behind the\nmonolingual competitors. In this work, we present two approaches to improve\nzero-shot cross-lingual classification, by transferring the knowledge from\nmonolingual pretrained models to multilingual ones. Experimental results on two\ncross-lingual classification benchmarks show that our methods outperform\nvanilla multilingual fine-tuning.", "published": "2019-11-10 11:53:26", "link": "http://arxiv.org/abs/1911.03913v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Fine-Grained Style Transfer: Leveraging Distributed Continuous\n  Style Representations to Transfer To Unseen Styles", "abstract": "Text style transfer is usually performed using attributes that can take a\nhandful of discrete values (e.g., positive to negative reviews). In this work,\nwe introduce an architecture that can leverage pre-trained consistent\ncontinuous distributed style representations and use them to transfer to an\nattribute unseen during training, without requiring any re-tuning of the style\ntransfer model. We demonstrate the method by training an architecture to\ntransfer text conveying one sentiment to another sentiment, using a\nfine-grained set of over 20 sentiment labels rather than the binary\npositive/negative often used in style transfer. Our experiments show that this\nmodel can then rewrite text to match a target sentiment that was unseen during\ntraining.", "published": "2019-11-10 11:53:43", "link": "http://arxiv.org/abs/1911.03914v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving BERT Fine-tuning with Embedding Normalization", "abstract": "Large pre-trained sentence encoders like BERT start a new chapter in natural\nlanguage processing. A common practice to apply pre-trained BERT to sequence\nclassification tasks (e.g., classification of sentences or sentence pairs) is\nby feeding the embedding of [CLS] token (in the last layer) to a task-specific\nclassification layer, and then fine tune the model parameters of BERT and\nclassifier jointly. In this paper, we conduct systematic analysis over several\nsequence classification datasets to examine the embedding values of [CLS] token\nbefore the fine tuning phase, and present the biased embedding distribution\nissue---i.e., embedding values of [CLS] concentrate on a few dimensions and are\nnon-zero centered. Such biased embedding brings challenge to the optimization\nprocess during fine-tuning as gradients of [CLS] embedding may explode and\nresult in degraded model performance. We further propose several simple yet\neffective normalization methods to modify the [CLS] embedding during the\nfine-tuning. Compared with the previous practice, neural classification model\nwith the normalized embedding shows improvements on several text classification\ntasks, demonstrates the effectiveness of our method.", "published": "2019-11-10 12:17:35", "link": "http://arxiv.org/abs/1911.03918v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Un systeme de lemmatisation pour les applications de TALN", "abstract": "This paper presents a method of stemming for the Arabian texts based on the\nlinguistic techniques of the natural language processing. This method leans on\nthe notion of scheme (one of the strong points of the morphology of the Arabian\nlanguage). The advantage of this approach is that it doesn't use a dictionary\nof inflexions but a smart dynamic recognition of the different words of the\nlanguage.", "published": "2019-11-10 12:53:38", "link": "http://arxiv.org/abs/1911.03922v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Model-Driven Unsupervised Neural Machine Translation", "abstract": "Unsupervised neural machine translation(NMT) is associated with noise and\nerrors in synthetic data when executing vanilla back-translations. Here, we\nexplicitly exploits language model(LM) to drive construction of an unsupervised\nNMT system. This features two steps. First, we initialize NMT models using\nsynthetic data generated via temporary statistical machine translation(SMT).\nSecond, unlike vanilla back-translation, we formulate a weight function, that\nscores synthetic data at each step of subsequent iterative training; this\nallows unsupervised training to an improved outcome. We present the detailed\nmathematical construction of our method. Experimental WMT2014 English-French,\nand WMT2016 English-German and English-Russian translation tasks revealed that\nour method outperforms the best prior systems by more than 3 BLEU points.", "published": "2019-11-10 14:04:59", "link": "http://arxiv.org/abs/1911.03937v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CCMatrix: Mining Billions of High-Quality Parallel Sentences on the WEB", "abstract": "We show that margin-based bitext mining in a multilingual sentence space can\nbe applied to monolingual corpora of billions of sentences. We are using ten\nsnapshots of a curated common crawl corpus (Wenzek et al., 2019) totalling 32.7\nbillion unique sentences. Using one unified approach for 38 languages, we were\nable to mine 4.5 billions parallel sentences, out of which 661 million are\naligned with English. 20 language pairs have more then 30 million parallel\nsentences, 112 more then 10 million, and most more than one million, including\ndirect alignments between many European or Asian languages.\n  To evaluate the quality of the mined bitexts, we train NMT systems for most\nof the language pairs and evaluate them on TED, WMT and WAT test sets. Using\nour mined bitexts only and no human translated parallel data, we achieve a new\nstate-of-the-art for a single system on the WMT'19 test set for translation\nbetween English and German, Russian and Chinese, as well as German/French. In\nparticular, our English/German system outperforms the best single one by close\nto 4 BLEU points and is almost on pair with best WMT'19 evaluation system which\nuses system combination and back-translation. We also achieve excellent results\nfor distant languages pairs like Russian/Japanese, outperforming the best\nsubmission at the 2019 workshop on Asian Translation (WAT).", "published": "2019-11-10 12:09:46", "link": "http://arxiv.org/abs/1911.04944v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distilling Knowledge Learned in BERT for Text Generation", "abstract": "Large-scale pre-trained language model such as BERT has achieved great\nsuccess in language understanding tasks. However, it remains an open question\nhow to utilize BERT for language generation. In this paper, we present a novel\napproach, Conditional Masked Language Modeling (C-MLM), to enable the\nfinetuning of BERT on target generation tasks. The finetuned BERT (teacher) is\nexploited as extra supervision to improve conventional Seq2Seq models (student)\nfor better text generation performance. By leveraging BERT's idiosyncratic\nbidirectional nature, distilling knowledge learned in BERT can encourage\nauto-regressive Seq2Seq models to plan ahead, imposing global sequence-level\nsupervision for coherent text generation. Experiments show that the proposed\napproach significantly outperforms strong Transformer baselines on multiple\nlanguage generation tasks such as machine translation and text summarization.\nOur proposed model also achieves new state of the art on IWSLT German-English\nand English-Vietnamese MT datasets. Code is available at\nhttps://github.com/ChenRocks/Distill-BERT-Textgen.", "published": "2019-11-10 02:12:38", "link": "http://arxiv.org/abs/1911.03829v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Not All Claims are Created Equal: Choosing the Right Statistical\n  Approach to Assess Hypotheses", "abstract": "Empirical research in Natural Language Processing (NLP) has adopted a narrow\nset of principles for assessing hypotheses, relying mainly on p-value\ncomputation, which suffers from several known issues. While alternative\nproposals have been well-debated and adopted in other fields, they remain\nrarely discussed or used within the NLP community. We address this gap by\ncontrasting various hypothesis assessment techniques, especially those not\ncommonly used in the field (such as evaluations based on Bayesian inference).\nSince these statistical techniques differ in the hypotheses they can support,\nwe argue that practitioners should first decide their target hypothesis before\nchoosing an assessment method. This is crucial because common fallacies,\nmisconceptions, and misinterpretation surrounding hypothesis assessment methods\noften stem from a discrepancy between what one would like to claim versus what\nthe method used actually assesses. Our survey reveals that these issues are\nomnipresent in the NLP research community. As a step forward, we provide best\npractices and guidelines tailored to NLP research, as well as an easy-to-use\npackage called 'HyBayes' for Bayesian assessment of hypotheses, complementing\nexisting tools.", "published": "2019-11-10 04:41:31", "link": "http://arxiv.org/abs/1911.03850v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Increasing Robustness to Spurious Correlations using Forgettable\n  Examples", "abstract": "Neural NLP models tend to rely on spurious correlations between labels and\ninput features to perform their tasks. Minority examples, i.e., examples that\ncontradict the spurious correlations present in the majority of data points,\nhave been shown to increase the out-of-distribution generalization of\npre-trained language models. In this paper, we first propose using example\nforgetting to find minority examples without prior knowledge of the spurious\ncorrelations present in the dataset. Forgettable examples are instances either\nlearned and then forgotten during training or never learned. We empirically\nshow how these examples are related to minorities in our training sets. Then,\nwe introduce a new approach to robustify models by fine-tuning our models\ntwice, first on the full training data and second on the minorities only. We\nobtain substantial improvements in out-of-distribution generalization when\napplying our approach to the MNLI, QQP, and FEVER datasets.", "published": "2019-11-10 05:56:41", "link": "http://arxiv.org/abs/1911.03861v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning to Few-Shot Learn Across Diverse Natural Language\n  Classification Tasks", "abstract": "Self-supervised pre-training of transformer models has shown enormous success\nin improving performance on a number of downstream tasks. However, fine-tuning\non a new task still requires large amounts of task-specific labelled data to\nachieve good performance. We consider this problem of learning to generalize to\nnew tasks with few examples as a meta-learning problem. While meta-learning has\nshown tremendous progress in recent years, its application is still limited to\nsimulated problems or problems with limited diversity across tasks. We develop\na novel method, LEOPARD, which enables optimization-based meta-learning across\ntasks with different number of classes, and evaluate different methods on\ngeneralization to diverse NLP classification tasks. LEOPARD is trained with the\nstate-of-the-art transformer architecture and shows better generalization to\ntasks not seen at all during training, with as few as 4 examples per label.\nAcross 17 NLP tasks, including diverse domains of entity typing, natural\nlanguage inference, sentiment analysis, and several other text classification\ntasks, we show that LEOPARD learns better initial parameters for few-shot\nlearning than self-supervised pre-training or multi-task training,\noutperforming many strong baselines, for example, yielding 14.5% average\nrelative gain in accuracy on unseen tasks with only 4 examples per label.", "published": "2019-11-10 06:10:47", "link": "http://arxiv.org/abs/1911.03863v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Transformer Models by Reordering their Sublayers", "abstract": "Multilayer transformer networks consist of interleaved self-attention and\nfeedforward sublayers. Could ordering the sublayers in a different pattern lead\nto better performance? We generate randomly ordered transformers and train them\nwith the language modeling objective. We observe that some of these models are\nable to achieve better performance than the interleaved baseline, and that\nthose successful variants tend to have more self-attention at the bottom and\nmore feedforward sublayers at the top. We propose a new transformer pattern\nthat adheres to this property, the sandwich transformer, and show that it\nimproves perplexity on multiple word-level and character-level language\nmodeling benchmarks, at no cost in parameters, memory, or training time.\nHowever, the sandwich reordering pattern does not guarantee performance gains\nacross every task, as we demonstrate on machine translation models. Instead, we\nsuggest that further exploration of task-specific sublayer reorderings is\nneeded in order to unlock additional gains.", "published": "2019-11-10 06:14:15", "link": "http://arxiv.org/abs/1911.03864v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge Guided Text Retrieval and Reading for Open Domain Question\n  Answering", "abstract": "We introduce an approach for open-domain question answering (QA) that\nretrieves and reads a passage graph, where vertices are passages of text and\nedges represent relationships that are derived from an external knowledge base\nor co-occurrence in the same article. Our goals are to boost coverage by using\nknowledge-guided retrieval to find more relevant passages than text-matching\nmethods, and to improve accuracy by allowing for better knowledge-guided fusion\nof information across related passages. Our graph retrieval method expands a\nset of seed keyword-retrieved passages by traversing the graph structure of the\nknowledge base. Our reader extends a BERT-based architecture and updates\npassage representations by propagating information from related passages and\ntheir relations, instead of reading each passage in isolation. Experiments on\nthree open-domain QA datasets, WebQuestions, Natural Questions and TriviaQA,\nshow improved performance over non-graph baselines by 2-11% absolute. Our\napproach also matches or exceeds the state-of-the-art in every case, without\nusing an expensive end-to-end training regime.", "published": "2019-11-10 06:58:44", "link": "http://arxiv.org/abs/1911.03868v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rethinking Self-Attention: Towards Interpretability in Neural Parsing", "abstract": "Attention mechanisms have improved the performance of NLP tasks while\nallowing models to remain explainable. Self-attention is currently widely used,\nhowever interpretability is difficult due to the numerous attention\ndistributions. Recent work has shown that model representations can benefit\nfrom label-specific information, while facilitating interpretation of\npredictions. We introduce the Label Attention Layer: a new form of\nself-attention where attention heads represent labels. We test our novel layer\nby running constituency and dependency parsing experiments and show our new\nmodel obtains new state-of-the-art results for both tasks on both the Penn\nTreebank (PTB) and Chinese Treebank. Additionally, our model requires fewer\nself-attention layers compared to existing work. Finally, we find that the\nLabel Attention heads learn relations between syntactic categories and show\npathways to analyze errors.", "published": "2019-11-10 08:17:11", "link": "http://arxiv.org/abs/1911.03875v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Bilingual Generative Transformer for Semantic Sentence Embedding", "abstract": "Semantic sentence embedding models encode natural language sentences into\nvectors, such that closeness in embedding space indicates closeness in the\nsemantics between the sentences. Bilingual data offers a useful signal for\nlearning such embeddings: properties shared by both sentences in a translation\npair are likely semantic, while divergent properties are likely stylistic or\nlanguage-specific. We propose a deep latent variable model that attempts to\nperform source separation on parallel sentences, isolating what they have in\ncommon in a latent semantic vector, and explaining what is left over with\nlanguage-specific latent vectors. Our proposed approach differs from past work\non semantic sentence encoding in two ways. First, by using a variational\nprobabilistic framework, we introduce priors that encourage source separation,\nand can use our model's posterior to predict sentence embeddings for\nmonolingual data at test time. Second, we use high-capacity transformers as\nboth data generating distributions and inference networks -- contrasting with\nmost past work on sentence embeddings. In experiments, our approach\nsubstantially outperforms the state-of-the-art on a standard suite of\nunsupervised semantic similarity evaluations. Further, we demonstrate that our\napproach yields the largest gains on more difficult subsets of these\nevaluations where simple word overlap is not a good indicator of similarity.", "published": "2019-11-10 10:48:09", "link": "http://arxiv.org/abs/1911.03895v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Understanding Multi-Head Attention in Abstractive Summarization", "abstract": "Attention mechanisms in deep learning architectures have often been used as a\nmeans of transparency and, as such, to shed light on the inner workings of the\narchitectures. Recently, there has been a growing interest in whether or not\nthis assumption is correct. In this paper we investigate the interpretability\nof multi-head attention in abstractive summarization, a sequence-to-sequence\ntask for which attention does not have an intuitive alignment role, such as in\nmachine translation. We first introduce three metrics to gain insight in the\nfocus of attention heads and observe that these heads specialize towards\nrelative positions, specific part-of-speech tags, and named entities. However,\nwe also find that ablating and pruning these heads does not lead to a\nsignificant drop in performance, indicating redundancy. By replacing the\nsoftmax activation functions with sparsemax activation functions, we find that\nattention heads behave seemingly more transparent: we can ablate fewer heads\nand heads score higher on our interpretability metrics. However, if we apply\npruning to the sparsemax model we find that we can prune even more heads,\nraising the question whether enforced sparsity actually improves transparency.\nFinally, we find that relative positions heads seem integral to summarization\nperformance and persistently remain after pruning.", "published": "2019-11-10 10:56:10", "link": "http://arxiv.org/abs/1911.03898v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Effectiveness of self-supervised pre-training for speech recognition", "abstract": "We compare self-supervised representation learning algorithms which either\nexplicitly quantize the audio data or learn representations without\nquantization. We find the former to be more accurate since it builds a good\nvocabulary of the data through vq-wav2vec [1] to enable learning of effective\nrepresentations in subsequent BERT training. Different to previous work, we\ndirectly fine-tune the pre-trained BERT models on transcribed speech using a\nConnectionist Temporal Classification (CTC) loss instead of feeding the\nrepresentations into a task-specific model. We also propose a BERT-style model\nlearning directly from the continuous audio data and compare pre-training on\nraw audio to spectral features. Fine-tuning a BERT model on 10 hour of labeled\nLibrispeech data with a vq-wav2vec vocabulary is almost as good as the best\nknown reported system trained on 100 hours of labeled data on testclean, while\nachieving a 25% WER reduction on test-other. When using only 10 minutes of\nlabeled data, WER is 25.2 on test-other and 16.3 on test-clean. This\ndemonstrates that self-supervision can enable speech recognition systems\ntrained on a near-zero amount of transcribed data.", "published": "2019-11-10 11:50:14", "link": "http://arxiv.org/abs/1911.03912v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TENER: Adapting Transformer Encoder for Named Entity Recognition", "abstract": "The Bidirectional long short-term memory networks (BiLSTM) have been widely\nused as an encoder in models solving the named entity recognition (NER) task.\nRecently, the Transformer is broadly adopted in various Natural Language\nProcessing (NLP) tasks owing to its parallelism and advantageous performance.\nNevertheless, the performance of the Transformer in NER is not as good as it is\nin other NLP tasks. In this paper, we propose TENER, a NER architecture\nadopting adapted Transformer Encoder to model the character-level features and\nword-level features. By incorporating the direction and relative distance aware\nattention and the un-scaled attention, we prove the Transformer-like encoder is\njust as effective for NER as other NLP tasks.", "published": "2019-11-10 15:05:48", "link": "http://arxiv.org/abs/1911.04474v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL\n  Parsers", "abstract": "When translating natural language questions into SQL queries to answer\nquestions from a database, contemporary semantic parsing models struggle to\ngeneralize to unseen database schemas. The generalization challenge lies in (a)\nencoding the database relations in an accessible way for the semantic parser,\nand (b) modeling alignment between database columns and their mentions in a\ngiven query. We present a unified framework, based on the relation-aware\nself-attention mechanism, to address schema encoding, schema linking, and\nfeature representation within a text-to-SQL encoder. On the challenging Spider\ndataset this framework boosts the exact match accuracy to 57.2%, surpassing its\nbest counterparts by 8.7% absolute improvement. Further augmented with BERT, it\nachieves the new state-of-the-art performance of 65.6% on the Spider\nleaderboard. In addition, we observe qualitative improvements in the model's\nunderstanding of schema linking and alignment. Our implementation will be\nopen-sourced at https://github.com/Microsoft/rat-sql.", "published": "2019-11-10 09:09:13", "link": "http://arxiv.org/abs/1911.04942v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adaptive Fusion Techniques for Multimodal Data", "abstract": "Effective fusion of data from multiple modalities, such as video, speech, and\ntext, is challenging due to the heterogeneous nature of multimodal data. In\nthis paper, we propose adaptive fusion techniques that aim to model context\nfrom different modalities effectively. Instead of defining a deterministic\nfusion operation, such as concatenation, for the network, we let the network\ndecide \"how\" to combine a given set of multimodal features more effectively. We\npropose two networks: 1) Auto-Fusion, which learns to compress information from\ndifferent modalities while preserving the context, and 2) GAN-Fusion, which\nregularizes the learned latent space given context from complementing\nmodalities. A quantitative evaluation on the tasks of multimodal machine\ntranslation and emotion recognition suggests that our lightweight, adaptive\nnetworks can better model context from other modalities than existing methods,\nmany of which employ massive transformer-based networks.", "published": "2019-11-10 01:39:46", "link": "http://arxiv.org/abs/1911.03821v2", "categories": ["cs.CL", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Modelling Bahdanau Attention using Election methods aided by Q-Learning", "abstract": "Neural Machine Translation has lately gained a lot of \"attention\" with the\nadvent of more and more sophisticated but drastically improved models.\nAttention mechanism has proved to be a boon in this direction by providing\nweights to the input words, making it easy for the decoder to identify words\nrepresenting the present context. But by and by, as newer attention models with\nmore complexity came into development, they involved large computation, making\ninference slow. In this paper, we have modelled the attention network using\ntechniques resonating with social choice theory. Along with that, the attention\nmechanism, being a Markov Decision Process, has been represented by\nreinforcement learning techniques. Thus, we propose to use an election method\n($k$-Borda), fine-tuned using Q-learning, as a replacement for attention\nnetworks. The inference time for this network is less than a standard Bahdanau\ntranslator, and the results of the translation are comparable. This not only\nexperimentally verifies the claims stated above but also helped provide a\nfaster inference.", "published": "2019-11-10 04:55:46", "link": "http://arxiv.org/abs/1911.03853v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "r/Fakeddit: A New Multimodal Benchmark Dataset for Fine-grained Fake\n  News Detection", "abstract": "Fake news has altered society in negative ways in politics and culture. It\nhas adversely affected both online social network systems as well as offline\ncommunities and conversations. Using automatic machine learning classification\nmodels is an efficient way to combat the widespread dissemination of fake news.\nHowever, a lack of effective, comprehensive datasets has been a problem for\nfake news research and detection model development. Prior fake news datasets do\nnot provide multimodal text and image data, metadata, comment data, and\nfine-grained fake news categorization at the scale and breadth of our dataset.\nWe present Fakeddit, a novel multimodal dataset consisting of over 1 million\nsamples from multiple categories of fake news. After being processed through\nseveral stages of review, the samples are labeled according to 2-way, 3-way,\nand 6-way classification categories through distant supervision. We construct\nhybrid text+image models and perform extensive experiments for multiple\nvariations of classification, demonstrating the importance of the novel aspect\nof multimodality and fine-grained classification unique to Fakeddit.", "published": "2019-11-10 05:06:38", "link": "http://arxiv.org/abs/1911.03854v2", "categories": ["cs.CL", "cs.CY", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Correcting Sociodemographic Selection Biases for Population Prediction\n  from Social Media", "abstract": "Social media is increasingly used for large-scale population predictions,\nsuch as estimating community health statistics. However, social media users are\nnot typically a representative sample of the intended population -- a\n\"selection bias\". Within the social sciences, such a bias is typically\naddressed with restratification techniques, where observations are reweighted\naccording to how under- or over-sampled their socio-demographic groups are.\nYet, restratifaction is rarely evaluated for improving prediction. In this\ntwo-part study, we first evaluate standard, \"out-of-the-box\" restratification\ntechniques, finding they provide no improvement and often even degraded\nprediction accuracies across four tasks of esimating U.S. county population\nhealth statistics from Twitter. The core reasons for degraded performance seem\nto be tied to their reliance on either sparse or shrunken estimates of each\npopulation's socio-demographics. In the second part of our study, we develop\nand evaluate Robust Poststratification, which consists of three methods to\naddress these problems: (1) estimator redistribution to account for shrinking,\nas well as (2) adaptive binning and (3) informed smoothing to handle sparse\nsocio-demographic estimates. We show that each of these methods leads to\nsignificant improvement in prediction accuracies over the standard\nrestratification approaches. Taken together, Robust Poststratification enables\nstate-of-the-art prediction accuracies, yielding a 53.0% increase in variance\nexplained (R^2) in the case of surveyed life satisfaction, and a 17.8% average\nincrease across all tasks.", "published": "2019-11-10 05:13:29", "link": "http://arxiv.org/abs/1911.03855v4", "categories": ["cs.SI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "Knowledge Guided Named Entity Recognition for BioMedical Text", "abstract": "In this work, we formulate the NER task as a multi-answer knowledge guided QA\ntask (KGQA) which helps to predict entities only by assigning B, I and O tags\nwithout associating entity types with the tags. We provide different knowledge\ncontexts, such as, entity types, questions, definitions and examples along with\nthe text and train on a combined dataset of 18 biomedical corpora. This\nformulation (a) enables systems to jointly learn NER specific features from\nvaried NER datasets, (b) can use knowledge-text attention to identify words\nhaving higher similarity to provided knowledge, improving performance, (c)\nreduces system confusion by reducing the prediction classes to B, I, O only,\nand (d) makes detection of nested entities easier. We perform extensive\nexperiments of this KGQA formulation on 18 biomedical NER datasets, and through\nexperiments we note that knowledge helps in achieving better performance. Our\nproblem formulation is able to achieve state-of-the-art results in 12 datasets.", "published": "2019-11-10 07:05:25", "link": "http://arxiv.org/abs/1911.03869v4", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pre-train and Plug-in: Flexible Conditional Text Generation with\n  Variational Auto-Encoders", "abstract": "Conditional Text Generation has drawn much attention as a topic of Natural\nLanguage Generation (NLG) which provides the possibility for humans to control\nthe properties of generated contents. Current conditional generation models\ncannot handle emerging conditions due to their joint end-to-end learning\nfashion. When a new condition added, these techniques require full retraining.\nIn this paper, we present a new framework named Pre-train and Plug-in\nVariational Auto-Encoder (PPVAE) towards flexible conditional text generation.\nPPVAE decouples the text generation module from the condition representation\nmodule to allow \"one-to-many\" conditional generation. When a fresh condition\nemerges, only a lightweight network needs to be trained and works as a plug-in\nfor PPVAE, which is efficient and desirable for real-world applications.\nExtensive experiments demonstrate the superiority of PPVAE against the existing\nalternatives with better conditionality and diversity but less training effort.", "published": "2019-11-10 09:23:42", "link": "http://arxiv.org/abs/1911.03882v4", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "HighwayGraph: Modelling Long-distance Node Relations for Improving\n  General Graph Neural Network", "abstract": "Graph Neural Networks (GNNs) are efficient approaches to process\ngraph-structured data. Modelling long-distance node relations is essential for\nGNN training and applications. However, conventional GNNs suffer from bad\nperformance in modelling long-distance node relations due to limited-layer\ninformation propagation. Existing studies focus on building deep GNN\narchitectures, which face the over-smoothing issue and cannot model node\nrelations in particularly long distance. To address this issue, we propose to\nmodel long-distance node relations by simply relying on shallow GNN\narchitectures with two solutions: (1) Implicitly modelling by learning to\npredict node pair relations (2) Explicitly modelling by adding edges between\nnodes that potentially have the same label. To combine our two solutions, we\npropose a model-agnostic training framework named HighwayGraph, which overcomes\nthe challenge of insufficient labeled nodes by sampling node pairs from the\ntraining set and adopting the self-training method. Extensive experimental\nresults show that our HighwayGraph achieves consistent and significant\nimprovements over four representative GNNs on three benchmark datasets.", "published": "2019-11-10 11:23:37", "link": "http://arxiv.org/abs/1911.03904v2", "categories": ["cs.LG", "cs.CL", "cs.SI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Evaluating Voice Conversion-based Privacy Protection against Informed\n  Attackers", "abstract": "Speech data conveys sensitive speaker attributes like identity or accent.\nWith a small amount of found data, such attributes can be inferred and\nexploited for malicious purposes: voice cloning, spoofing, etc. Anonymization\naims to make the data unlinkable, i.e., ensure that no utterance can be linked\nto its original speaker. In this paper, we investigate anonymization methods\nbased on voice conversion. In contrast to prior work, we argue that various\nlinkage attacks can be designed depending on the attackers' knowledge about the\nanonymization scheme. We compare two frequency warping-based conversion methods\nand a deep learning based method in three attack scenarios. The utility of\nconverted speech is measured via the word error rate achieved by automatic\nspeech recognition, while privacy protection is assessed by the increase in\nequal error rate achieved by state-of-the-art i-vector or x-vector based\nspeaker verification. Our results show that voice conversion schemes are unable\nto effectively protect against an attacker that has extensive knowledge of the\ntype of conversion and how it has been applied, but may provide some protection\nagainst less knowledgeable attackers.", "published": "2019-11-10 13:45:06", "link": "http://arxiv.org/abs/1911.03934v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Can Neural Image Captioning be Controlled via Forced Attention?", "abstract": "Learned dynamic weighting of the conditioning signal (attention) has been\nshown to improve neural language generation in a variety of settings. The\nweights applied when generating a particular output sequence have also been\nviewed as providing a potentially explanatory insight into the internal\nworkings of the generator. In this paper, we reverse the direction of this\nconnection and ask whether through the control of the attention of the model we\ncan control its output. Specifically, we take a standard neural image\ncaptioning model that uses attention, and fix the attention to pre-determined\nareas in the image. We evaluate whether the resulting output is more likely to\nmention the class of the object in that area than the normally generated\ncaption. We introduce three effective methods to control the attention and find\nthat these are producing expected results in up to 28.56% of the cases.", "published": "2019-11-10 14:00:27", "link": "http://arxiv.org/abs/1911.03936v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Classical linear logic, cobordisms and categorial grammars", "abstract": "We propose a categorial grammar based on classical multiplicative linear\nlogic.\n  This can be seen as an extension of abstract categorial grammars (ACG) and is\nat least as expressive. However, constituents of {\\it linear logic grammars\n(LLG)} are not abstract ${\\lambda}$-terms, but simply tuples of words with\nlabeled endpoints and supplied with specific {\\it plugging instructions}: the\nsets of endpoints are subdivided into the {\\it incoming} and the {\\it outgoing}\nparts. We call such objects {\\it word cobordisms}.\n  A key observation is that word cobordisms can be organized in a category,\nvery similar to the familiar category of topological cobordisms. This category\nis symmetric monoidal closed and compact closed and thus is a model of linear\n$\\lambda$-calculus and classical, as well as intuitionistic linear logic. This\nallows us using linear logic as a typing system for word cobordisms.\n  At least, this gives a concrete and intuitive representation of ACG.\n  We think, however, that the category of word cobordisms, which has a rich\nstructure and is independent of any grammar, might be interesting on its own\nright.", "published": "2019-11-10 16:56:25", "link": "http://arxiv.org/abs/1911.03962v3", "categories": ["math.LO", "cs.CL", "cs.LO"], "primary_category": "math.LO"}
{"title": "Improved Large-margin Softmax Loss for Speaker Diarisation", "abstract": "Speaker diarisation systems nowadays use embeddings generated from speech\nsegments in a bottleneck layer, which are needed to be discriminative for\nunseen speakers. It is well-known that large-margin training can improve the\ngeneralisation ability to unseen data, and its use in such open-set problems\nhas been widespread. Therefore, this paper introduces a general approach to the\nlarge-margin softmax loss without any approximations to improve the quality of\nspeaker embeddings for diarisation. Furthermore, a novel and simple way to\nstabilise training, when large-margin softmax is used, is proposed. Finally, to\ncombat the effect of overlapping speech, different training margins are used to\nreduce the negative effect overlapping speech has on creating discriminative\nembeddings. Experiments on the AMI meeting corpus show that the use of\nlarge-margin softmax significantly improves the speaker error rate (SER). By\nusing all hyper parameters of the loss in a unified way, further improvements\nwere achieved which reached a relative SER reduction of 24.6% over the\nbaseline. However, by training overlapping and single speaker speech samples\nwith different margins, the best result was achieved, giving overall a 29.5%\nSER reduction relative to the baseline.", "published": "2019-11-10 17:41:11", "link": "http://arxiv.org/abs/1911.03970v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On Posterior Collapse and Encoder Feature Dispersion in Sequence VAEs", "abstract": "Variational autoencoders (VAEs) hold great potential for modelling text, as\nthey could in theory separate high-level semantic and syntactic properties from\nlocal regularities of natural language. Practically, however, VAEs with\nautoregressive decoders often suffer from posterior collapse, a phenomenon\nwhere the model learns to ignore the latent variables, causing the sequence VAE\nto degenerate into a language model. In this paper, we argue that posterior\ncollapse is in part caused by the lack of dispersion in encoder features. We\nprovide empirical evidence to verify this hypothesis, and propose a\nstraightforward fix using pooling. This simple technique effectively prevents\nposterior collapse, allowing model to achieve significantly better data\nlog-likelihood than standard sequence VAEs. Comparing to existing work, our\nproposed method is able to achieve comparable or superior performances while\nbeing more computationally efficient.", "published": "2019-11-10 18:50:46", "link": "http://arxiv.org/abs/1911.03976v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Multimodal Intelligence: Representation Learning, Information Fusion,\n  and Applications", "abstract": "Deep learning methods have revolutionized speech recognition, image\nrecognition, and natural language processing since 2010. Each of these tasks\ninvolves a single modality in their input signals. However, many applications\nin the artificial intelligence field involve multiple modalities. Therefore, it\nis of broad interest to study the more difficult and complex problem of\nmodeling and learning across multiple modalities. In this paper, we provide a\ntechnical review of available models and learning methods for multimodal\nintelligence. The main focus of this review is the combination of vision and\nnatural language modalities, which has become an important topic in both the\ncomputer vision and natural language processing research communities. This\nreview provides a comprehensive analysis of recent works on multimodal deep\nlearning from three perspectives: learning multimodal representations, fusing\nmultimodal signals at various levels, and multimodal applications. Regarding\nmultimodal representation learning, we review the key concepts of embedding,\nwhich unify multimodal signals into a single vector space and thereby enable\ncross-modality signal processing. We also review the properties of many types\nof embeddings that are constructed and learned for general downstream tasks.\nRegarding multimodal fusion, this review focuses on special architectures for\nthe integration of representations of unimodal signals for a particular task.\nRegarding applications, selected areas of a broad interest in the current\nliterature are covered, including image-to-text caption generation,\ntext-to-image generation, and visual question answering. We believe that this\nreview will facilitate future studies in the emerging field of multimodal\nintelligence for related communities.", "published": "2019-11-10 18:58:20", "link": "http://arxiv.org/abs/1911.03977v3", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "CCAligned: A Massive Collection of Cross-Lingual Web-Document Pairs", "abstract": "Cross-lingual document alignment aims to identify pairs of documents in two\ndistinct languages that are of comparable content or translations of each\nother. In this paper, we exploit the signals embedded in URLs to label web\ndocuments at scale with an average precision of 94.5% across different language\npairs. We mine sixty-eight snapshots of the Common Crawl corpus and identify\nweb document pairs that are translations of each other. We release a new web\ndataset consisting of over 392 million URL pairs from Common Crawl covering\ndocuments in 8144 language pairs of which 137 pairs include English. In\naddition to curating this massive dataset, we introduce baseline methods that\nleverage cross-lingual representations to identify aligned documents based on\ntheir textual content. Finally, we demonstrate the value of this parallel\ndocuments dataset through a downstream task of mining parallel sentences and\nmeasuring the quality of machine translations from models trained on this mined\ndata. Our objective in releasing this dataset is to foster new research in\ncross-lingual NLP across a variety of low, medium, and high-resource languages.", "published": "2019-11-10 02:09:11", "link": "http://arxiv.org/abs/1911.06154v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Syntax-Infused Transformer and BERT models for Machine Translation and\n  Natural Language Understanding", "abstract": "Attention-based models have shown significant improvement over traditional\nalgorithms in several NLP tasks. The Transformer, for instance, is an\nillustrative example that generates abstract representations of tokens inputted\nto an encoder based on their relationships to all tokens in a sequence. Recent\nstudies have shown that although such models are capable of learning syntactic\nfeatures purely by seeing examples, explicitly feeding this information to deep\nlearning models can significantly enhance their performance. Leveraging\nsyntactic information like part of speech (POS) may be particularly beneficial\nin limited training data settings for complex models such as the Transformer.\nWe show that the syntax-infused Transformer with multiple features achieves an\nimprovement of 0.7 BLEU when trained on the full WMT 14 English to German\ntranslation dataset and a maximum improvement of 1.99 BLEU points when trained\non a fraction of the dataset. In addition, we find that the incorporation of\nsyntax into BERT fine-tuning outperforms baseline on a number of downstream\ntasks from the GLUE benchmark.", "published": "2019-11-10 04:42:13", "link": "http://arxiv.org/abs/1911.06156v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Transformation of low-quality device-recorded speech to high-quality\n  speech using improved SEGAN model", "abstract": "Nowadays vast amounts of speech data are recorded from low-quality recorder\ndevices such as smartphones, tablets, laptops, and medium-quality microphones.\nThe objective of this research was to study the automatic generation of\nhigh-quality speech from such low-quality device-recorded speech, which could\nthen be applied to many speech-generation tasks. In this paper, we first\nintroduce our new device-recorded speech dataset then propose an improved\nend-to-end method for automatically transforming the low-quality\ndevice-recorded speech into professional high-quality speech. Our method is an\nextension of a generative adversarial network (GAN)-based speech enhancement\nmodel called speech enhancement GAN (SEGAN), and we present two modifications\nto make model training more robust and stable. Finally, from a large-scale\nlistening test, we show that our method can significantly enhance the quality\nof device-recorded speech signals.", "published": "2019-11-10 16:05:21", "link": "http://arxiv.org/abs/1911.03952v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Characterizing dynamically varying acoustic scenes from egocentric audio\n  recordings in workplace setting", "abstract": "Devices capable of detecting and categorizing acoustic scenes have numerous\napplications such as providing context-aware user experiences. In this paper,\nwe address the task of characterizing acoustic scenes in a workplace setting\nfrom audio recordings collected with wearable microphones. The acoustic scenes,\ntracked with Bluetooth transceivers, vary dynamically with time from the\negocentric perspective of a mobile user. Our dataset contains experience\nsampled long audio recordings collected from clinical providers in a hospital,\nwho wore the audio badges during multiple work shifts. To handle the long\negocentric recordings, we propose a Time Delay Neural Network~(TDNN)-based\nsegment-level modeling. The experiments show that TDNN outperforms other models\nin the acoustic scene classification task. We investigate the effect of primary\nspeaker's speech in determining acoustic scenes from audio badges, and provide\na comparison between performance of different models. Moreover, we explore the\nrelationship between the sequence of acoustic scenes experienced by the users\nand the nature of their jobs, and find that the scene sequence predicted by our\nmodel tend to possess similar relationship. The initial promising results\nreveal numerous research directions for acoustic scene classification via\nwearable devices as well as egocentric analysis of dynamic acoustic scenes\nencountered by the users.", "published": "2019-11-10 04:11:47", "link": "http://arxiv.org/abs/1911.03843v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Robust Unsupervised Audio-visual Speech Enhancement Using a Mixture of\n  Variational Autoencoders", "abstract": "Recently, an audio-visual speech generative model based on variational\nautoencoder (VAE) has been proposed, which is combined with a nonnegative\nmatrix factorization (NMF) model for noise variance to perform unsupervised\nspeech enhancement. When visual data is clean, speech enhancement with\naudio-visual VAE shows a better performance than with audio-only VAE, which is\ntrained on audio-only data. However, audio-visual VAE is not robust against\nnoisy visual data, e.g., when for some video frames, speaker face is not\nfrontal or lips region is occluded. In this paper, we propose a robust\nunsupervised audio-visual speech enhancement method based on a per-frame VAE\nmixture model. This mixture model consists of a trained audio-only VAE and a\ntrained audio-visual VAE. The motivation is to skip noisy visual frames by\nswitching to the audio-only VAE model. We present a variational\nexpectation-maximization method to estimate the parameters of the model.\nExperiments show the promising performance of the proposed method.", "published": "2019-11-10 13:36:47", "link": "http://arxiv.org/abs/1911.03930v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Listen and Fill in the Missing Letters: Non-Autoregressive Transformer\n  for Speech Recognition", "abstract": "Recently very deep transformers have outperformed conventional bi-directional\nlong short-term memory networks by a large margin in speech recognition.\nHowever, to put it into production usage, inference computation cost is still a\nserious concern in real scenarios. In this paper, we study two different\nnon-autoregressive transformer structure for automatic speech recognition\n(ASR): A-CMLM and A-FMLM. During training, for both frameworks, input tokens\nfed to the decoder are randomly replaced by special mask tokens. The network is\nrequired to predict the tokens corresponding to those mask tokens by taking\nboth unmasked context and input speech into consideration. During inference, we\nstart from all mask tokens and the network iteratively predicts missing tokens\nbased on partial results. We show that this framework can support different\ndecoding strategies, including traditional left-to-right. A new decoding\nstrategy is proposed as an example, which starts from the easiest predictions\nto the most difficult ones. Results on Mandarin (Aishell) and Japanese (CSJ)\nASR benchmarks show the possibility to train such a non-autoregressive network\nfor ASR. Especially in Aishell, the proposed method outperformed the Kaldi ASR\nsystem and it matches the performance of the state-of-the-art autoregressive\ntransformer with 7x speedup. Pretrained models and code will be made available\nafter publication.", "published": "2019-11-10 06:05:14", "link": "http://arxiv.org/abs/1911.04908v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
