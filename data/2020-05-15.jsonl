{"title": "A chatbot architecture for promoting youth resilience", "abstract": "E-health technologies have the potential to provide scalable and accessible\ninterventions for youth mental health. As part of a developing an ecosystem of\ne-screening and e-therapy tools for New Zealand young people, a dialog agent,\nHeadstrong, has been designed to promote resilience with methods grounded in\ncognitive behavioral therapy and positive psychology. This paper describes the\narchitecture underlying the chatbot. The architecture supports a range of over\n20 activities delivered in a 4-week program by relatable personas. The\narchitecture provides a visual authoring interface to its content management\nsystem. In addition to supporting the original adolescent resilience chatbot,\nthe architecture has been reused to create a 3-week 'stress-detox' intervention\nfor undergraduates, and subsequently for a chatbot to support young people with\nthe impacts of the COVID-19 pandemic, with all three systems having been used\nin field trials. The Headstrong architecture illustrates the feasibility of\ncreating a domain-focused authoring environment in the context of e-therapy\nthat supports non-technical expert input and rapid deployment.", "published": "2020-05-15 04:36:06", "link": "http://arxiv.org/abs/2005.07355v1", "categories": ["cs.CL", "I.2.7; J.3; H.5.2"], "primary_category": "cs.CL"}
{"title": "Corpus and Models for Lemmatisation and POS-tagging of Classical French\n  Theatre", "abstract": "This paper describes the process of building an annotated corpus and training\nmodels for classical French literature, with a focus on theatre, and\nparticularly comedies in verse. It was originally developed as a preliminary\nstep to the stylometric analyses presented in Cafiero and Camps [2019]. The use\nof a recent lemmatiser based on neural networks and a CRF tagger allows to\nachieve accuracies beyond the current state-of-the art on the in-domain test,\nand proves to be robust during out-of-domain tests, i.e.up to 20th c.novels.", "published": "2020-05-15 12:47:54", "link": "http://arxiv.org/abs/2005.07505v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Challenges in Emotion Style Transfer: An Exploration with a Lexical\n  Substitution Pipeline", "abstract": "We propose the task of emotion style transfer, which is particularly\nchallenging, as emotions (here: anger, disgust, fear, joy, sadness, surprise)\nare on the fence between content and style. To understand the particular\ndifficulties of this task, we design a transparent emotion style transfer\npipeline based on three steps: (1) select the words that are promising to be\nsubstituted to change the emotion (with a brute-force approach and selection\nbased on the attention mechanism of an emotion classifier), (2) find sets of\nwords as candidates for substituting the words (based on lexical and\ndistributional semantics), and (3) select the most promising combination of\nsubstitutions with an objective function which consists of components for\ncontent (based on BERT sentence embeddings), emotion (based on an emotion\nclassifier), and fluency (based on a neural language model). This comparably\nstraight-forward setup enables us to explore the task and understand in what\ncases lexical substitution can vary the emotional load of texts, how changes in\ncontent and style interact and if they are at odds. We further evaluate our\npipeline quantitatively in an automated and an annotation study based on Tweets\nand find, indeed, that simultaneous adjustments of content and emotion are\nconflicting objectives: as we show in a qualitative analysis motivated by\nScherer's emotion component model, this is particularly the case for implicit\nemotion expressions based on cognitive appraisal or descriptions of bodily\nreactions.", "published": "2020-05-15 16:11:33", "link": "http://arxiv.org/abs/2005.07617v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncovering Gender Bias in Media Coverage of Politicians with Machine\n  Learning", "abstract": "This paper presents research uncovering systematic gender bias in the\nrepresentation of political leaders in the media, using artificial\nintelligence. Newspaper coverage of Irish ministers over a fifteen year period\nwas gathered and analysed with natural language processing techniques and\nmachine learning. Findings demonstrate evidence of gender bias in the portrayal\nof female politicians, the kind of policies they were associated with and how\nthey were evaluated in terms of their performance as political leaders. This\npaper also sets out a methodology whereby media content may be analysed on a\nlarge scale utilising techniques from artificial intelligence within a\ntheoretical framework founded in gender theory and feminist linguistics.", "published": "2020-05-15 18:37:56", "link": "http://arxiv.org/abs/2005.07734v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In Layman's Terms: Semi-Open Relation Extraction from Scientific Texts", "abstract": "Information Extraction (IE) from scientific texts can be used to guide\nreaders to the central information in scientific documents. But narrow IE\nsystems extract only a fraction of the information captured, and Open IE\nsystems do not perform well on the long and complex sentences encountered in\nscientific texts. In this work we combine the output of both types of systems\nto achieve Semi-Open Relation Extraction, a new task that we explore in the\nBiology domain. First, we present the Focused Open Biological Information\nExtraction (FOBIE) dataset and use FOBIE to train a state-of-the-art narrow\nscientific IE system to extract trade-off relations and arguments that are\ncentral to biology texts. We then run both the narrow IE system and a\nstate-of-the-art Open IE system on a corpus of 10k open-access scientific\nbiological texts. We show that a significant amount (65%) of erroneous and\nuninformative Open IE extractions can be filtered using narrow IE extractions.\nFurthermore, we show that the retained extractions are significantly more often\ninformative to a reader.", "published": "2020-05-15 19:18:26", "link": "http://arxiv.org/abs/2005.07751v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Scientific Information Extraction Dataset for Nature Inspired\n  Engineering", "abstract": "Nature has inspired various ground-breaking technological developments in\napplications ranging from robotics to aerospace engineering and the\nmanufacturing of medical devices. However, accessing the information captured\nin scientific biology texts is a time-consuming and hard task that requires\ndomain-specific knowledge. Improving access for outsiders can help\ninterdisciplinary research like Nature Inspired Engineering. This paper\ndescribes a dataset of 1,500 manually-annotated sentences that express\ndomain-independent relations between central concepts in a scientific biology\ntext, such as trade-offs and correlations. The arguments of these relations can\nbe Multi Word Expressions and have been annotated with modifying phrases to\nform non-projective graphs. The dataset allows for training and evaluating\nRelation Extraction algorithms that aim for coarse-grained typing of scientific\nbiological documents, enabling a high-level filter for engineers.", "published": "2020-05-15 19:25:12", "link": "http://arxiv.org/abs/2005.07753v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Behind the Scene: Revealing the Secrets of Pre-trained\n  Vision-and-Language Models", "abstract": "Recent Transformer-based large-scale pre-trained models have revolutionized\nvision-and-language (V+L) research. Models such as ViLBERT, LXMERT and UNITER\nhave significantly lifted state of the art across a wide range of V+L\nbenchmarks with joint image-text pre-training. However, little is known about\nthe inner mechanisms that destine their impressive success. To reveal the\nsecrets behind the scene of these powerful models, we present VALUE\n(Vision-And-Language Understanding Evaluation), a set of meticulously designed\nprobing tasks (e.g., Visual Coreference Resolution, Visual Relation Detection,\nLinguistic Probing Tasks) generalizable to standard pre-trained V+L models,\naiming to decipher the inner workings of multimodal pre-training (e.g., the\nimplicit knowledge garnered in individual attention heads, the inherent\ncross-modal alignment learned through contextualized multimodal embeddings).\nThrough extensive analysis of each archetypal model architecture via these\nprobing tasks, our key observations are: (i) Pre-trained models exhibit a\npropensity for attending over text rather than images during inference. (ii)\nThere exists a subset of attention heads that are tailored for capturing\ncross-modal interactions. (iii) Learned attention matrix in pre-trained models\ndemonstrates patterns coherent with the latent alignment between image regions\nand textual words. (iv) Plotted attention patterns reveal\nvisually-interpretable relations among image regions. (v) Pure linguistic\nknowledge is also effectively encoded in the attention heads. These are\nvaluable insights serving to guide future work towards designing better model\narchitecture and objectives for multimodal pre-training.", "published": "2020-05-15 01:06:54", "link": "http://arxiv.org/abs/2005.07310v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Is Your Goal-Oriented Dialog Model Performing Really Well? Empirical\n  Analysis of System-wise Evaluation", "abstract": "There is a growing interest in developing goal-oriented dialog systems which\nserve users in accomplishing complex tasks through multi-turn conversations.\nAlthough many methods are devised to evaluate and improve the performance of\nindividual dialog components, there is a lack of comprehensive empirical study\non how different components contribute to the overall performance of a dialog\nsystem. In this paper, we perform a system-wise evaluation and present an\nempirical analysis on different types of dialog systems which are composed of\ndifferent modules in different settings. Our results show that (1) a pipeline\ndialog system trained using fine-grained supervision signals at different\ncomponent levels often obtains better performance than the systems that use\njoint or end-to-end models trained on coarse-grained labels, (2)\ncomponent-wise, single-turn evaluation results are not always consistent with\nthe overall performance of a dialog system, and (3) despite the discrepancy\nbetween simulators and human users, simulated evaluation is still a valid\nalternative to the costly human evaluation especially in the early stage of\ndevelopment.", "published": "2020-05-15 05:20:06", "link": "http://arxiv.org/abs/2005.07362v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Spelling Error Correction with Soft-Masked BERT", "abstract": "Spelling error correction is an important yet challenging task because a\nsatisfactory solution of it essentially needs human-level language\nunderstanding ability. Without loss of generality we consider Chinese spelling\nerror correction (CSC) in this paper. A state-of-the-art method for the task\nselects a character from a list of candidates for correction (including\nnon-correction) at each position of the sentence on the basis of BERT, the\nlanguage representation model. The accuracy of the method can be sub-optimal,\nhowever, because BERT does not have sufficient capability to detect whether\nthere is an error at each position, apparently due to the way of pre-training\nit using mask language modeling. In this work, we propose a novel neural\narchitecture to address the aforementioned issue, which consists of a network\nfor error detection and a network for error correction based on BERT, with the\nformer being connected to the latter with what we call soft-masking technique.\nOur method of using `Soft-Masked BERT' is general, and it may be employed in\nother language detection-correction problems. Experimental results on two\ndatasets demonstrate that the performance of our proposed method is\nsignificantly better than the baselines including the one solely based on BERT.", "published": "2020-05-15 09:02:38", "link": "http://arxiv.org/abs/2005.07421v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Transfer of Sentiment Classifiers", "abstract": "Word embeddings represent words in a numeric space so that semantic relations\nbetween words are represented as distances and directions in the vector space.\nCross-lingual word embeddings transform vector spaces of different languages so\nthat similar words are aligned. This is done by constructing a mapping between\nvector spaces of two languages or learning a joint vector space for multiple\nlanguages. Cross-lingual embeddings can be used to transfer machine learning\nmodels between languages, thereby compensating for insufficient data in\nless-resourced languages. We use cross-lingual word embeddings to transfer\nmachine learning prediction models for Twitter sentiment between 13 languages.\nWe focus on two transfer mechanisms that recently show superior transfer\nperformance. The first mechanism uses the trained models whose input is the\njoint numerical space for many languages as implemented in the LASER library.\nThe second mechanism uses large pretrained multilingual BERT language models.\nOur experiments show that the transfer of models between similar languages is\nsensible, even with no target language data. The performance of cross-lingual\nmodels obtained with the multilingual BERT and LASER library is comparable, and\nthe differences are language-dependent. The transfer with CroSloEngual BERT,\npretrained on only three languages, is superior on these and some closely\nrelated languages.", "published": "2020-05-15 10:15:27", "link": "http://arxiv.org/abs/2005.07456v3", "categories": ["cs.CL", "cs.LG", "68T50 (Primary)", "I.2.7; J.4; K.4.2"], "primary_category": "cs.CL"}
{"title": "Adaptive Transformers for Learning Multimodal Representations", "abstract": "The usage of transformers has grown from learning about language semantics to\nforming meaningful visiolinguistic representations. These architectures are\noften over-parametrized, requiring large amounts of computation. In this work,\nwe extend adaptive approaches to learn more about model interpretability and\ncomputational efficiency. Specifically, we study attention spans, sparse, and\nstructured dropout methods to help understand how their attention mechanism\nextends for vision and language tasks. We further show that these approaches\ncan help us learn more about how the network perceives the complexity of input\nsequences, sparsity preferences for different modalities, and other related\nphenomena.", "published": "2020-05-15 12:12:57", "link": "http://arxiv.org/abs/2005.07486v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Entity Linking on Technical Service Tickets", "abstract": "Entity linking, the task of mapping textual mentions to known entities, has\nrecently been tackled using contextualized neural networks. We address the\nquestion whether these results -- reported for large, high-quality datasets\nsuch as Wikipedia -- transfer to practical business use cases, where labels are\nscarce, text is low-quality, and terminology is highly domain-specific. Using\nan entity linking model based on BERT, a popular transformer network in natural\nlanguage processing, we show that a neural approach outperforms and complements\nhand-coded heuristics, with improvements of about 20% top-1 accuracy. Also, the\nbenefits of transfer learning on a large corpus are demonstrated, while\nfine-tuning proves difficult. Finally, we compare different BERT-based\narchitectures and show that a simple sentence-wise encoding (Bi-Encoder) offers\na fast yet efficient search in practice.", "published": "2020-05-15 15:47:02", "link": "http://arxiv.org/abs/2005.07604v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Analyzing Temporal Relationships between Trending Terms on Twitter and\n  Urban Dictionary Activity", "abstract": "As an online, crowd-sourced, open English-language slang dictionary, the\nUrban Dictionary platform contains a wealth of opinions, jokes, and definitions\nof terms, phrases, acronyms, and more. However, it is unclear exactly how\nactivity on this platform relates to larger conversations happening elsewhere\non the web, such as discussions on larger, more popular social media platforms.\nIn this research, we study the temporal activity trends on Urban Dictionary and\nprovide the first analysis of how this activity relates to content being\ndiscussed on a major social network: Twitter. By collecting the whole of Urban\nDictionary, as well as a large sample of tweets over seven years, we explore\nthe connections between the words and phrases that are defined and searched for\non Urban Dictionary and the content that is talked about on Twitter. Through a\nseries of cross-correlation calculations, we identify cases in which Urban\nDictionary activity closely reflects the larger conversation happening on\nTwitter. Then, we analyze the types of terms that have a stronger connection to\ndiscussions on Twitter, finding that Urban Dictionary activity that is\npositively correlated with Twitter is centered around terms related to memes,\npopular public figures, and offline events. Finally, We explore the\nrelationship between periods of time when terms are trending on Twitter and the\ncorresponding activity on Urban Dictionary, revealing that new definitions are\nmore likely to be added to Urban Dictionary for terms that are currently\ntrending on Twitter.", "published": "2020-05-15 17:17:01", "link": "http://arxiv.org/abs/2005.07655v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning", "abstract": "Magnitude pruning is a widely used strategy for reducing model size in pure\nsupervised learning; however, it is less effective in the transfer learning\nregime that has become standard for state-of-the-art natural language\nprocessing applications. We propose the use of movement pruning, a simple,\ndeterministic first-order weight pruning method that is more adaptive to\npretrained model fine-tuning. We give mathematical foundations to the method\nand compare it to existing zeroth- and first-order pruning methods. Experiments\nshow that when pruning large pretrained language models, movement pruning shows\nsignificant improvements in high-sparsity regimes. When combined with\ndistillation, the approach achieves minimal accuracy loss with down to only 3%\nof the model parameters.", "published": "2020-05-15 17:54:15", "link": "http://arxiv.org/abs/2005.07683v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KEIS@JUST at SemEval-2020 Task 12: Identifying Multilingual Offensive\n  Tweets Using Weighted Ensemble and Fine-Tuned BERT", "abstract": "This research presents our team KEIS@JUST participation at SemEval-2020 Task\n12 which represents shared task on multilingual offensive language. We\nparticipated in all the provided languages for all subtasks except sub-task-A\nfor the English language. Two main approaches have been developed the first is\nperformed to tackle both languages Arabic and English, a weighted ensemble\nconsists of Bi-GRU and CNN followed by Gaussian noise and global pooling layer\nmultiplied by weights to improve the overall performance. The second is\nperformed for other languages, a transfer learning from BERT beside the\nrecurrent neural networks such as Bi-LSTM and Bi-GRU followed by a global\naverage pooling layer. Word embedding and contextual embedding have been used\nas features, moreover, data augmentation has been used only for the Arabic\nlanguage.", "published": "2020-05-15 23:11:03", "link": "http://arxiv.org/abs/2005.07820v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Contextualizing ASR Lattice Rescoring with Hybrid Pointer Network\n  Language Model", "abstract": "Videos uploaded on social media are often accompanied with textual\ndescriptions. In building automatic speech recognition (ASR) systems for\nvideos, we can exploit the contextual information provided by such video\nmetadata. In this paper, we explore ASR lattice rescoring by selectively\nattending to the video descriptions. We first use an attention based method to\nextract contextual vector representations of video metadata, and use these\nrepresentations as part of the inputs to a neural language model during lattice\nrescoring. Secondly, we propose a hybrid pointer network approach to explicitly\ninterpolate the word probabilities of the word occurrences in metadata. We\nperform experimental evaluations on both language modeling and ASR tasks, and\ndemonstrate that both proposed methods provide performance improvements by\nselectively leveraging the video metadata.", "published": "2020-05-15 07:47:33", "link": "http://arxiv.org/abs/2005.07394v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Predicting User Emotional Tone in Mental Disorder Online Communities", "abstract": "In recent years, Online Social Networks have become an important medium for\npeople who suffer from mental disorders to share moments of hardship, and\nreceive emotional and informational support. In this work, we analyze how\ndiscussions in Reddit communities related to mental disorders can help improve\nthe health conditions of their users. Using the emotional tone of users'\nwriting as a proxy for emotional state, we uncover relationships between user\ninteractions and state changes. First, we observe that authors of negative\nposts often write rosier comments after engaging in discussions, indicating\nthat users' emotional state can improve due to social support. Second, we build\nmodels based on SOTA text embedding techniques and RNNs to predict shifts in\nemotional tone. This differs from most of related work, which focuses primarily\non detecting mental disorders from user activity. We demonstrate the\nfeasibility of accurately predicting the users' reactions to the interactions\nexperienced in these platforms, and present some examples which illustrate that\nthe models are correctly capturing the effects of comments on the author's\nemotional tone. Our models hold promising implications for interventions to\nprovide support for people struggling with mental illnesses.", "published": "2020-05-15 11:25:08", "link": "http://arxiv.org/abs/2005.07473v2", "categories": ["cs.LG", "cs.CL", "cs.SI", "stat.ML", "J.3; I.2.7"], "primary_category": "cs.LG"}
{"title": "COVID-Twitter-BERT: A Natural Language Processing Model to Analyse\n  COVID-19 Content on Twitter", "abstract": "In this work, we release COVID-Twitter-BERT (CT-BERT), a transformer-based\nmodel, pretrained on a large corpus of Twitter messages on the topic of\nCOVID-19. Our model shows a 10-30% marginal improvement compared to its base\nmodel, BERT-Large, on five different classification datasets. The largest\nimprovements are on the target domain. Pretrained transformer models, such as\nCT-BERT, are trained on a specific target domain and can be used for a wide\nvariety of natural language processing tasks, including classification,\nquestion-answering and chatbots. CT-BERT is optimised to be used on COVID-19\ncontent, in particular social media posts from Twitter.", "published": "2020-05-15 12:40:46", "link": "http://arxiv.org/abs/2005.07503v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Finding Experts in Transformer Models", "abstract": "In this work we study the presence of expert units in pre-trained Transformer\nModels (TM), and how they impact a model's performance. We define expert units\nto be neurons that are able to classify a concept with a given average\nprecision, where a concept is represented by a binary set of sentences\ncontaining the concept (or not). Leveraging the OneSec dataset (Scarlini et\nal., 2019), we compile a dataset of 1641 concepts that allows diverse expert\nunits in TM to be discovered. We show that expert units are important in\nseveral ways: (1) The presence of expert units is correlated ($r^2=0.833$) with\nthe generalization power of TM, which allows ranking TM without requiring\nfine-tuning on suites of downstream tasks. We further propose an empirical\nmethod to decide how accurate such experts should be to evaluate\ngeneralization. (2) The overlap of top experts between concepts provides a\nsensible way to quantify concept co-learning, which can be used for\nexplainability of unknown concepts. (3) We show how to self-condition\noff-the-shelf pre-trained language models to generate text with a given concept\nby forcing the top experts to be active, without requiring re-training the\nmodel or using additional parameters.", "published": "2020-05-15 17:07:02", "link": "http://arxiv.org/abs/2005.07647v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Language Conditioned Imitation Learning over Unstructured Data", "abstract": "Natural language is perhaps the most flexible and intuitive way for humans to\ncommunicate tasks to a robot. Prior work in imitation learning typically\nrequires each task be specified with a task id or goal image -- something that\nis often impractical in open-world environments. On the other hand, previous\napproaches in instruction following allow agent behavior to be guided by\nlanguage, but typically assume structure in the observations, actuators, or\nlanguage that limit their applicability to complex settings like robotics. In\nthis work, we present a method for incorporating free-form natural language\nconditioning into imitation learning. Our approach learns perception from\npixels, natural language understanding, and multitask continuous control\nend-to-end as a single neural network. Unlike prior work in imitation learning,\nour method is able to incorporate unlabeled and unstructured demonstration data\n(i.e. no task or language labels). We show this dramatically improves language\nconditioned performance, while reducing the cost of language annotation to less\nthan 1% of total data. At test time, a single language conditioned visuomotor\npolicy trained with our method can perform a wide variety of robotic\nmanipulation skills in a 3D environment, specified only with natural language\ndescriptions of each task (e.g. \"open the drawer...now pick up the block...now\npress the green button...\"). To scale up the number of instructions an agent\ncan follow, we propose combining text conditioned policies with large\npretrained neural language models. We find this allows a policy to be robust to\nmany out-of-distribution synonym instructions, without requiring new\ndemonstrations. See videos of a human typing live text commands to our agent at\nlanguage-play.github.io", "published": "2020-05-15 17:08:50", "link": "http://arxiv.org/abs/2005.07648v2", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Benchmarking neural embeddings for link prediction in knowledge graphs\n  under semantic and structural changes", "abstract": "Recently, link prediction algorithms based on neural embeddings have gained\ntremendous popularity in the Semantic Web community, and are extensively used\nfor knowledge graph completion. While algorithmic advances have strongly\nfocused on efficient ways of learning embeddings, fewer attention has been\ndrawn to the different ways their performance and robustness can be evaluated.\nIn this work we propose an open-source evaluation pipeline, which benchmarks\nthe accuracy of neural embeddings in situations where knowledge graphs may\nexperience semantic and structural changes. We define relation-centric\nconnectivity measures that allow us to connect the link prediction capacity to\nthe structure of the knowledge graph. Such an evaluation pipeline is especially\nimportant to simulate the accuracy of embeddings for knowledge graphs that are\nexpected to be frequently updated.", "published": "2020-05-15 17:15:45", "link": "http://arxiv.org/abs/2005.07654v2", "categories": ["cs.AI", "cs.CL", "stat.ML", "I.2"], "primary_category": "cs.AI"}
{"title": "Recent Advances in SQL Query Generation: A Survey", "abstract": "Natural language is hypothetically the best user interface for many domains.\nHowever, general models that provide an interface between natural language and\nany other domain still do not exist. Providing natural language interface to\nrelational databases could possibly attract a vast majority of users that are\nor are not proficient with query languages. With the rise of deep learning\ntechniques, there is extensive ongoing research in designing a suitable natural\nlanguage interface to relational databases.\n  This survey aims to overview some of the latest methods and models proposed\nin the area of SQL query generation from natural language. We describe models\nwith various architectures such as convolutional neural networks, recurrent\nneural networks, pointer networks, reinforcement learning, etc. Several\ndatasets intended to address the problem of SQL query generation are\ninterpreted and briefly overviewed. In the end, evaluation metrics utilized in\nthe field are presented mainly as a combination of execution accuracy and\nlogical form accuracy.", "published": "2020-05-15 17:31:29", "link": "http://arxiv.org/abs/2005.07667v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "JDI-T: Jointly trained Duration Informed Transformer for Text-To-Speech\n  without Explicit Alignment", "abstract": "We propose Jointly trained Duration Informed Transformer (JDI-T), a\nfeed-forward Transformer with a duration predictor jointly trained without\nexplicit alignments in order to generate an acoustic feature sequence from an\ninput text. In this work, inspired by the recent success of the duration\ninformed networks such as FastSpeech and DurIAN, we further simplify its\nsequential, two-stage training pipeline to a single-stage training.\nSpecifically, we extract the phoneme duration from the autoregressive\nTransformer on the fly during the joint training instead of pretraining the\nautoregressive model and using it as a phoneme duration extractor. To our best\nknowledge, it is the first implementation to jointly train the feed-forward\nTransformer without relying on a pre-trained phoneme duration extractor in a\nsingle training pipeline. We evaluate the effectiveness of the proposed model\non the publicly available Korean Single speaker Speech (KSS) dataset compared\nto the baseline text-to-speech (TTS) models trained by ESPnet-TTS.", "published": "2020-05-15 22:06:13", "link": "http://arxiv.org/abs/2005.07799v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Feature Fusion Strategies for End-to-End Evaluation of Cognitive\n  Behavior Therapy Sessions", "abstract": "Cognitive Behavioral Therapy (CBT) is a goal-oriented psychotherapy for\nmental health concerns implemented in a conversational setting with broad\nempirical support for its effectiveness across a range of presenting problems\nand client populations. The quality of a CBT session is typically assessed by\ntrained human raters who manually assign pre-defined session-level behavioral\ncodes. In this paper, we develop an end-to-end pipeline that converts speech\naudio to diarized and transcribed text and extracts linguistic features to code\nthe CBT sessions automatically. We investigate both word-level and\nutterance-level features and propose feature fusion strategies to combine them.\nThe utterance level features include dialog act tags as well as behavioral\ncodes drawn from another well-known talk psychotherapy called Motivational\nInterviewing (MI). We propose a novel method to augment the word-based features\nwith the utterance level tags for subsequent CBT code estimation. Experiments\nshow that our new fusion strategy outperforms all the studied features, both\nwhen used individually and when fused by direct concatenation. We also find\nthat incorporating a sentence segmentation module can further improve the\noverall system given the preponderance of multi-utterance conversational turns\nin CBT sessions.", "published": "2020-05-15 22:26:58", "link": "http://arxiv.org/abs/2005.07809v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Weakly Supervised Training of Hierarchical Attention Networks for\n  Speaker Identification", "abstract": "Identifying multiple speakers without knowing where a speaker's voice is in a\nrecording is a challenging task. In this paper, a hierarchical attention\nnetwork is proposed to solve a weakly labelled speaker identification problem.\nThe use of a hierarchical structure, consisting of a frame-level encoder and a\nsegment-level encoder, aims to learn speaker related information locally and\nglobally. Speech streams are segmented into fragments. The frame-level encoder\nwith attention learns features and highlights the target related frames\nlocally, and output a fragment based embedding. The segment-level encoder works\nwith a second attention layer to emphasize the fragments probably related to\ntarget speakers. The global information is finally collected from segment-level\nmodule to predict speakers via a classifier. To evaluate the effectiveness of\nthe proposed approach, artificial datasets based on Switchboard Cellular part1\n(SWBC) and Voxceleb1 are constructed in two conditions, where speakers' voices\nare overlapped and not overlapped. Comparing to two baselines the obtained\nresults show that the proposed approach can achieve better performances.\nMoreover, further experiments are conducted to evaluate the impact of utterance\nsegmentation. The results show that a reasonable segmentation can slightly\nimprove identification performances.", "published": "2020-05-15 22:57:53", "link": "http://arxiv.org/abs/2005.07817v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaker Re-identification with Speaker Dependent Speech Enhancement", "abstract": "While the use of deep neural networks has significantly boosted speaker\nrecognition performance, it is still challenging to separate speakers in poor\nacoustic environments. Here speech enhancement methods have traditionally\nallowed improved performance. The recent works have shown that adapting speech\nenhancement can lead to further gains. This paper introduces a novel approach\nthat cascades speech enhancement and speaker recognition. In the first step, a\nspeaker embedding vector is generated , which is used in the second step to\nenhance the speech quality and re-identify the speakers. Models are trained in\nan integrated framework with joint optimisation. The proposed approach is\nevaluated using the Voxceleb1 dataset, which aims to assess speaker recognition\nin real world situations. In addition three types of noise at different\nsignal-noise-ratios were added for this work. The obtained results show that\nthe proposed approach using speaker dependent speech enhancement can yield\nbetter speaker recognition and speech enhancement performances than two\nbaselines in various noise conditions.", "published": "2020-05-15 23:02:10", "link": "http://arxiv.org/abs/2005.07818v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Reverberation Modeling for Source-Filter-based Neural Vocoder", "abstract": "This paper presents a reverberation module for source-filter-based neural\nvocoders that improves the performance of reverberant effect modeling. This\nmodule uses the output waveform of neural vocoders as an input and produces a\nreverberant waveform by convolving the input with a room impulse response\n(RIR). We propose two approaches to parameterizing and estimating the RIR. The\nfirst approach assumes a global time-invariant (GTI) RIR and directly learns\nthe values of the RIR on a training dataset. The second approach assumes an\nutterance-level time-variant (UTV) RIR, which is invariant within one utterance\nbut varies across utterances, and uses another neural network to predict the\nRIR values. We add the proposed reverberation module to the phase spectrum\npredictor (PSP) of a HiNet vocoder and jointly train the model. Experimental\nresults demonstrate that the proposed module was helpful for modeling the\nreverberation effect and improving the perceived quality of generated\nreverberant speech. The UTV-RIR was shown to be more robust than the GTI-RIR to\nunknown reverberation conditions and achieved a perceptually better\nreverberation effect.", "published": "2020-05-15 07:05:06", "link": "http://arxiv.org/abs/2005.07379v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "WG-WaveNet: Real-Time High-Fidelity Speech Synthesis without GPU", "abstract": "In this paper, we propose WG-WaveNet, a fast, lightweight, and high-quality\nwaveform generation model. WG-WaveNet is composed of a compact flow-based model\nand a post-filter. The two components are jointly trained by maximizing the\nlikelihood of the training data and optimizing loss functions on the frequency\ndomains. As we design a flow-based model that is heavily compressed, the\nproposed model requires much less computational resources compared to other\nwaveform generation models during both training and inference time; even though\nthe model is highly compressed, the post-filter maintains the quality of\ngenerated waveform. Our PyTorch implementation can be trained using less than 8\nGB GPU memory and generates audio samples at a rate of more than 960 kHz on an\nNVIDIA 1080Ti GPU. Furthermore, even if synthesizing on a CPU, we show that the\nproposed method is capable of generating 44.1 kHz speech waveform 1.2 times\nfaster than real-time. Experiments also show that the quality of generated\naudio is comparable to those of other methods. Audio samples are publicly\navailable online.", "published": "2020-05-15 08:38:46", "link": "http://arxiv.org/abs/2005.07412v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dual-Signal Transformation LSTM Network for Real-Time Noise Suppression", "abstract": "This paper introduces a dual-signal transformation LSTM network (DTLN) for\nreal-time speech enhancement as part of the Deep Noise Suppression Challenge\n(DNS-Challenge). This approach combines a short-time Fourier transform (STFT)\nand a learned analysis and synthesis basis in a stacked-network approach with\nless than one million parameters. The model was trained on 500 h of noisy\nspeech provided by the challenge organizers. The network is capable of\nreal-time processing (one frame in, one frame out) and reaches competitive\nresults. Combining these two types of signal transformations enables the DTLN\nto robustly extract information from magnitude spectra and incorporate phase\ninformation from the learned feature basis. The method shows state-of-the-art\nperformance and outperforms the DNS-Challenge baseline by 0.24 points absolute\nin terms of the mean opinion score (MOS).", "published": "2020-05-15 14:04:33", "link": "http://arxiv.org/abs/2005.07551v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ConVoice: Real-Time Zero-Shot Voice Style Transfer with Convolutional\n  Network", "abstract": "We propose a neural network for zero-shot voice conversion (VC) without any\nparallel or transcribed data. Our approach uses pre-trained models for\nautomatic speech recognition (ASR) and speaker embedding, obtained from a\nspeaker verification task. Our model is fully convolutional and\nnon-autoregressive except for a small pre-trained recurrent neural network for\nspeaker encoding. ConVoice can convert speech of any length without\ncompromising quality due to its convolutional architecture. Our model has\ncomparable quality to similar state-of-the-art models while being extremely\nfast.", "published": "2020-05-15 22:41:16", "link": "http://arxiv.org/abs/2005.07815v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On Bottleneck Features for Text-Dependent Speaker Verification Using\n  X-vectors", "abstract": "Applying x-vectors for speaker verification has recently attracted great\ninterest, with the focus being on text-independent speaker verification. In\nthis paper, we study x-vectors for text-dependent speaker verification (TD-SV),\nwhich remains unexplored. We further investigate the impact of the different\nbottleneck (BN) features on the performance of x-vectors, including the\nrecently-introduced time-contrastive-learning (TCL) BN features and\nphone-discriminant BN features. TCL is a weakly supervised learning approach\nthat constructs training data by uniformly partitioning each utterance into a\npredefined number of segments and then assigning each segment a class label\ndepending on their position in the utterance. We also compare TD-SV performance\nfor different modeling techniques, including the Gaussian mixture\nmodels-universal background model (GMM-UBM), i-vector, and x-vector.\nExperiments are conducted on the RedDots 2016 challenge database. It is found\nthat the type of features has a marginal impact on the performance of x-vectors\nwith the TCL BN feature achieving the lowest equal error rate, while the impact\nof features is significant for i-vector and GMM-UBM. The fusion of x-vector and\ni-vector systems gives a large gain in performance. The GMM-UBM technique shows\nits advantage for TD-SV using short utterances.", "published": "2020-05-15 07:10:53", "link": "http://arxiv.org/abs/2005.07383v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Siamese Neural Networks for Class Activity Detection", "abstract": "Classroom activity detection (CAD) aims at accurately recognizing speaker\nroles (either teacher or student) in classrooms. A CAD solution helps teachers\nget instant feedback on their pedagogical instructions. However, CAD is very\nchallenging because (1) classroom conversations contain many conversational\nturn-taking overlaps between teachers and students; (2) the CAD model needs to\nbe generalized well enough for different teachers and students; and (3)\nclassroom recordings may be very noisy and low-quality. In this work, we\naddress the above challenges by building a Siamese neural framework to\nautomatically identify teacher and student utterances from classroom\nrecordings. The proposed model is evaluated on real-world educational datasets.\nThe results demonstrate that (1) our approach is superior on the prediction\ntasks for both online and offline classroom environments; and (2) our framework\nexhibits robustness and generalization ability on new teachers (i.e., teachers\nnever appear in training data).", "published": "2020-05-15 14:03:35", "link": "http://arxiv.org/abs/2005.07549v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Context-Dependent Acoustic Modeling without Explicit Phone Clustering", "abstract": "Phoneme-based acoustic modeling of large vocabulary automatic speech\nrecognition takes advantage of phoneme context. The large number of\ncontext-dependent (CD) phonemes and their highly varying statistics require\ntying or smoothing to enable robust training. Usually, classification and\nregression trees are used for phonetic clustering, which is standard in hidden\nMarkov model (HMM)-based systems. However, this solution introduces a secondary\ntraining objective and does not allow for end-to-end training. In this work, we\naddress a direct phonetic context modeling for the hybrid deep neural network\n(DNN)/HMM, that does not build on any phone clustering algorithm for the\ndetermination of the HMM state inventory. By performing different\ndecompositions of the joint probability of the center phoneme state and its\nleft and right contexts, we obtain a factorized network consisting of different\ncomponents, trained jointly. Moreover, the representation of the phonetic\ncontext for the network relies on phoneme embeddings. The recognition accuracy\nof our proposed models on the Switchboard task is comparable and outperforms\nslightly the hybrid model using the standard state-tying decision trees.", "published": "2020-05-15 14:45:32", "link": "http://arxiv.org/abs/2005.07578v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "68T10", "I.2.7"], "primary_category": "eess.AS"}
{"title": "An Auto Encoder For Audio Dolphin Communication", "abstract": "Research in dolphin communication and cognition requires detailed inspection\nof audible dolphin signals. The manual analysis of these signals is cumbersome\nand time-consuming. We seek to automate parts of the analysis using modern deep\nlearning methods. We propose to learn an autoencoder constructed from\nconvolutional and recurrent layers trained in an unsupervised fashion. The\nresulting model embeds patterns in audible dolphin communication. In several\nexperiments, we show that the embeddings can be used for clustering as well as\nsignal detection and signal type classification.", "published": "2020-05-15 16:30:04", "link": "http://arxiv.org/abs/2005.07623v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Nonlinear Residual Echo Suppression Based on Multi-stream Conv-TasNet", "abstract": "Acoustic echo cannot be entirely removed by linear adaptive filters due to\nthe nonlinear relationship between the echo and far-end signal. Usually a post\nprocessing module is required to further suppress the echo. In this paper, we\npropose a residual echo suppression method based on the modification of fully\nconvolutional time-domain audio separation network (Conv-TasNet). Both the\nresidual signal of the linear acoustic echo cancellation system, and the output\nof the adaptive filter are adopted to form multiple streams for the\nConv-TasNet, resulting in more effective echo suppression while keeping a lower\nlatency of the whole system. Simulation results validate the efficacy of the\nproposed method in both single-talk and double-talk situations.", "published": "2020-05-15 16:41:16", "link": "http://arxiv.org/abs/2005.07631v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "\"I have vxxx bxx connexxxn!\": Facing Packet Loss in Deep Speech Emotion\n  Recognition", "abstract": "In applications that use emotion recognition via speech, frame-loss can be a\nsevere issue given manifold applications, where the audio stream loses some\ndata frames, for a variety of reasons like low bandwidth. In this contribution,\nwe investigate for the first time the effects of frame-loss on the performance\nof emotion recognition via speech. Reproducible extensive experiments are\nreported on the popular RECOLA corpus using a state-of-the-art end-to-end deep\nneural network, which mainly consists of convolution blocks and recurrent\nlayers. A simple environment based on a Markov Chain model is used to model the\nloss mechanism based on two main parameters. We explore matched, mismatched,\nand multi-condition training settings. As one expects, the matched setting\nyields the best performance, while the mismatched yields the lowest.\nFurthermore, frame-loss as a data augmentation technique is introduced as a\ngeneral-purpose strategy to overcome the effects of frame-loss. It can be used\nduring training, and we observed it to produce models that are more robust\nagainst frame-loss in run-time environments.", "published": "2020-05-15 19:33:40", "link": "http://arxiv.org/abs/2005.07757v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ConcealNet: An End-to-end Neural Network for Packet Loss Concealment in\n  Deep Speech Emotion Recognition", "abstract": "Packet loss is a common problem in data transmission, including speech data\ntransmission. This may affect a wide range of applications that stream audio\ndata, like streaming applications or speech emotion recognition (SER). Packet\nLoss Concealment (PLC) is any technique of facing packet loss. Simple PLC\nbaselines are 0-substitution or linear interpolation. In this paper, we present\na concealment wrapper, which can be used with stacked recurrent neural cells.\nThe concealment cell can provide a recurrent neural network (ConcealNet), that\nperforms real-time step-wise end-to-end PLC at inference time. Additionally,\nextending this with an end-to-end emotion prediction neural network provides a\nnetwork that performs SER from audio with lost frames, end-to-end. The proposed\nmodel is compared against the fore-mentioned baselines. Additionally, a\nbidirectional variant with better performance is utilised. For evaluation, we\nchose the public RECOLA dataset given its long audio tracks with continuous\nemotion labels. ConcealNet is evaluated on the reconstruction of the audio and\nthe quality of corresponding emotions predicted after that. The proposed\nConcealNet model has shown considerable improvement, for both audio\nreconstruction and the corresponding emotion prediction, in environments that\ndo not have losses with long duration, even when the losses occur frequently.", "published": "2020-05-15 20:43:02", "link": "http://arxiv.org/abs/2005.07777v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Reliable Local Explanations for Machine Listening", "abstract": "One way to analyse the behaviour of machine learning models is through local\nexplanations that highlight input features that maximally influence model\npredictions. Sensitivity analysis, which involves analysing the effect of input\nperturbations on model predictions, is one of the methods to generate local\nexplanations. Meaningful input perturbations are essential for generating\nreliable explanations, but there exists limited work on what such perturbations\nare and how to perform them. This work investigates these questions in the\ncontext of machine listening models that analyse audio. Specifically, we use a\nstate-of-the-art deep singing voice detection (SVD) model to analyse whether\nexplanations from SoundLIME (a local explanation method) are sensitive to how\nthe method perturbs model inputs. The results demonstrate that SoundLIME\nexplanations are sensitive to the content in the occluded input regions. We\nfurther propose and demonstrate a novel method for quantitatively identifying\nsuitable content type(s) for reliably occluding inputs of machine listening\nmodels. The results for the SVD model suggest that the average magnitude of\ninput mel-spectrogram bins is the most suitable content type for temporal\nexplanations.", "published": "2020-05-15 21:17:06", "link": "http://arxiv.org/abs/2005.07788v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "On Deep Speech Packet Loss Concealment: A Mini-Survey", "abstract": "Packet-loss is a common problem in data transmission, using Voice over IP.\nThe problem is an old problem, and there has been a variety of classical\napproaches that were developed to overcome this problem. However, with the rise\nof deep learning and generative models like Generative Adversarial Networks and\nAutoencoders, a new avenue has emerged for attempting to solve packet-loss\nusing deep learning, by generating replacements for lost packets. In this\nmini-survey, we review all the literature we found to date, that attempt to\nsolve the packet-loss in speech using deep learning methods. Additionally, we\nbriefly review how the problem of packet-loss in a realistic setting is\nmodelled, and how to evaluate Packet Loss Concealment techniques. Moreover, we\nreview a few modern deep learning techniques in related domains that have shown\npromising results. These techniques shed light on future potentially better\nsolutions for PLC and additional challenges that need to be considered\nsimultaneously with packet-loss.", "published": "2020-05-15 21:37:13", "link": "http://arxiv.org/abs/2005.07794v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Cross-Domain Speech-to-Speech Conversion with\n  Time-Frequency Consistency", "abstract": "In recent years generative adversarial network (GAN) based models have been\nsuccessfully applied for unsupervised speech-to-speech conversion.The rich\ncompact harmonic view of the magnitude spectrogram is considered a suitable\nchoice for training these models with audio data. To reconstruct the speech\nsignal first a magnitude spectrogram is generated by the neural network, which\nis then utilized by methods like the Griffin-Lim algorithm to reconstruct a\nphase spectrogram. This procedure bears the problem that the generated\nmagnitude spectrogram may not be consistent, which is required for finding a\nphase such that the full spectrogram has a natural-sounding speech waveform. In\nthis work, we approach this problem by proposing a condition encouraging\nspectrogram consistency during the adversarial training procedure. We\ndemonstrate our approach on the task of translating the voice of a male speaker\nto that of a female speaker, and vice versa. Our experimental results on the\nLibrispeech corpus show that the model trained with the TF consistency provides\na perceptually better quality of speech-to-speech conversion.", "published": "2020-05-15 22:27:07", "link": "http://arxiv.org/abs/2005.07810v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Novel Fusion of Attention and Sequence to Sequence Autoencoders to\n  Predict Sleepiness From Speech", "abstract": "Motivated by the attention mechanism of the human visual system and recent\ndevelopments in the field of machine translation, we introduce our\nattention-based and recurrent sequence to sequence autoencoders for fully\nunsupervised representation learning from audio files. In particular, we test\nthe efficacy of our novel approach on the task of speech-based sleepiness\nrecognition. We evaluate the learnt representations from both autoencoders, and\nthen conduct an early fusion to ascertain possible complementarity between\nthem. In our frameworks, we first extract Mel-spectrograms from raw audio\nfiles. Second, we train recurrent autoencoders on these spectrograms which are\nconsidered as time-dependent frequency vectors. Afterwards, we extract the\nactivations of specific fully connected layers of the autoencoders which\nrepresent the learnt features of spectrograms for the corresponding audio\ninstances. Finally, we train support vector regressors on these representations\nto obtain the predictions. On the development partition of the data, we achieve\nSpearman's correlation coefficients of .324, .283, and .320 with the targets on\nthe Karolinska Sleepiness Scale by utilising attention and non-attention\nautoencoders, and the fusion of both autoencoders' representations,\nrespectively. In the same order, we achieve .311, .359, and .367 Spearman's\ncorrelation coefficients on the test data, indicating the suitability of our\nproposed fusion strategy.", "published": "2020-05-15 12:02:52", "link": "http://arxiv.org/abs/2005.08722v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
