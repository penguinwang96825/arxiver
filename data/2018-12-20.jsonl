{"title": "Context, Attention and Audio Feature Explorations for Audio Visual\n  Scene-Aware Dialog", "abstract": "With the recent advancements in AI, Intelligent Virtual Assistants (IVA) have\nbecome a ubiquitous part of every home. Going forward, we are witnessing a\nconfluence of vision, speech and dialog system technologies that are enabling\nthe IVAs to learn audio-visual groundings of utterances and have conversations\nwith users about the objects, activities and events surrounding them. As a part\nof the 7th Dialog System Technology Challenges (DSTC7), for Audio Visual\nScene-Aware Dialog (AVSD) track, We explore `topics' of the dialog as an\nimportant contextual feature into the architecture along with explorations\naround multimodal Attention. We also incorporate an end-to-end audio\nclassification ConvNet, AclNet, into our models. We present detailed analysis\nof the experiments and show that some of our model variations outperform the\nbaseline system presented for this task.", "published": "2018-12-20 08:05:54", "link": "http://arxiv.org/abs/1812.08407v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Much Does Tokenization Affect Neural Machine Translation?", "abstract": "Tokenization or segmentation is a wide concept that covers simple processes\nsuch as separating punctuation from words, or more sophisticated processes such\nas applying morphological knowledge. Neural Machine Translation (NMT) requires\na limited-size vocabulary for computational cost and enough examples to\nestimate word embeddings. Separating punctuation and splitting tokens into\nwords or subwords has proven to be helpful to reduce vocabulary and increase\nthe number of examples of each word, improving the translation quality.\nTokenization is more challenging when dealing with languages with no separator\nbetween words. In order to assess the impact of the tokenization in the quality\nof the final translation on NMT, we experimented on five tokenizers over ten\nlanguage pairs. We reached the conclusion that the tokenization significantly\naffects the final translation quality and that the best tokenizer differs for\ndifferent language pairs.", "published": "2018-12-20 15:02:39", "link": "http://arxiv.org/abs/1812.08621v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RNNs Implicitly Implement Tensor Product Representations", "abstract": "Recurrent neural networks (RNNs) can learn continuous vector representations\nof symbolic structures such as sequences and sentences; these representations\noften exhibit linear regularities (analogies). Such regularities motivate our\nhypothesis that RNNs that show such regularities implicitly compile symbolic\nstructures into tensor product representations (TPRs; Smolensky, 1990), which\nadditively combine tensor products of vectors representing roles (e.g.,\nsequence positions) and vectors representing fillers (e.g., particular words).\nTo test this hypothesis, we introduce Tensor Product Decomposition Networks\n(TPDNs), which use TPRs to approximate existing vector representations. We\ndemonstrate using synthetic data that TPDNs can successfully approximate linear\nand tree-based RNN autoencoder representations, suggesting that these\nrepresentations exhibit interpretable compositional structure; we explore the\nsettings that lead RNNs to induce such structure-sensitive representations. By\ncontrast, further TPDN experiments show that the representations of four models\ntrained to encode naturally-occurring sentences can be largely approximated\nwith a bag of words, with only marginal improvements from more sophisticated\nstructures. We conclude that TPDNs provide a powerful method for interpreting\nvector representations, and that standard RNNs can induce compositional\nsequence representations that are remarkably well approximated by TPRs; at the\nsame time, existing training tasks for sentence representation learning may not\nbe sufficient for inducing robust structural representations.", "published": "2018-12-20 17:44:05", "link": "http://arxiv.org/abs/1812.08718v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recommendation System based on Semantic Scholar Mining and Topic\n  modeling: A behavioral analysis of researchers from six conferences", "abstract": "Recommendation systems have an important place to help online users in the\ninternet society. Recommendation Systems in computer science are of very\npractical use these days in various aspects of the Internet portals, such as\nsocial networks, and library websites. There are several approaches to\nimplement recommendation systems, Latent Dirichlet Allocation (LDA) is one the\npopular techniques in Topic Modeling. Recently, researchers have proposed many\napproaches based on Recommendation Systems and LDA. According to importance of\nthe subject, in this paper we discover the trends of the topics and find\nrelationship between LDA topics and Scholar-Context-documents. In fact, We\napply probabilistic topic modeling based on Gibbs sampling algorithms for a\nsemantic mining from six conference publications in computer science from DBLP\ndataset. According to our experimental results, our semantic framework can be\neffective to help organizations to better organize these conferences and cover\nfuture research topics.", "published": "2018-12-20 01:07:04", "link": "http://arxiv.org/abs/1812.08304v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "What are the biases in my word embedding?", "abstract": "This paper presents an algorithm for enumerating biases in word embeddings.\nThe algorithm exposes a large number of offensive associations related to\nsensitive features such as race and gender on publicly available embeddings,\nincluding a supposedly \"debiased\" embedding. These biases are concerning in\nlight of the widespread use of word embeddings. The associations are identified\nby geometric patterns in word embeddings that run parallel between people's\nnames and common lower-case tokens. The algorithm is highly unsupervised: it\ndoes not even require the sensitive features to be pre-specified. This is\ndesirable because: (a) many forms of discrimination--such as racial\ndiscrimination--are linked to social constructs that may vary depending on the\ncontext, rather than to categories with fixed definitions; and (b) it makes it\neasier to identify biases against intersectional groups, which depend on\ncombinations of sensitive features. The inputs to our algorithm are a list of\ntarget tokens, e.g. names, and a word embedding. It outputs a number of Word\nEmbedding Association Tests (WEATs) that capture various biases present in the\ndata. We illustrate the utility of our approach on publicly available word\nembeddings and lists of names, and evaluate its output using crowdsourcing. We\nalso show how removing names may not remove potential proxy bias.", "published": "2018-12-20 18:53:05", "link": "http://arxiv.org/abs/1812.08769v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Variational Cross-domain Natural Language Generation for Spoken Dialogue\n  Systems", "abstract": "Cross-domain natural language generation (NLG) is still a difficult task\nwithin spoken dialogue modelling. Given a semantic representation provided by\nthe dialogue manager, the language generator should generate sentences that\nconvey desired information. Traditional template-based generators can produce\nsentences with all necessary information, but these sentences are not\nsufficiently diverse. With RNN-based models, the diversity of the generated\nsentences can be high, however, in the process some information is lost. In\nthis work, we improve an RNN-based generator by considering latent information\nat the sentence level during generation using the conditional variational\nautoencoder architecture. We demonstrate that our model outperforms the\noriginal RNN-based generator, while yielding highly diverse sentences. In\naddition, our model performs better when the training data is limited.", "published": "2018-12-20 22:53:33", "link": "http://arxiv.org/abs/1812.08879v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generating lyrics with variational autoencoder and multi-modal artist\n  embeddings", "abstract": "We present a system for generating song lyrics lines conditioned on the style\nof a specified artist. The system uses a variational autoencoder with artist\nembeddings. We propose the pre-training of artist embeddings with the\nrepresentations learned by a CNN classifier, which is trained to predict\nartists based on MEL spectrograms of their song clips. This work is the first\nstep towards combining audio and text modalities of songs for generating lyrics\nconditioned on the artist's style. Our preliminary results suggest that there\nis a benefit in initializing artists' embeddings with the representations\nlearned by a spectrogram classifier.", "published": "2018-12-20 02:25:45", "link": "http://arxiv.org/abs/1812.08318v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Survey of Hierarchy Identification in Social Networks", "abstract": "Humans are social by nature. Throughout history, people have formed\ncommunities and built relationships. Most relationships with coworkers,\nfriends, and family are developed during face-to-face interactions. These\nrelationships are established through explicit means of communications such as\nwords and implicit such as intonation, body language, etc. By analyzing human\ninteractions we can derive information about the relationships and influence\namong conversation participants. However, with the development of the Internet,\npeople started to communicate through text in online social networks.\nInterestingly, they brought their communicational habits to the Internet. Many\nsocial network users form relationships with each other and establish\ncommunities with leaders and followers. Recognizing these hierarchical\nrelationships is an important task because it will help to understand social\nnetworks and predict future trends, improve recommendations, better target\nadvertisement, and improve national security by identifying leaders of\nanonymous terror groups. In this work, I provide an overview of current\nresearch in this area and present the state-of-the-art approaches to deal with\nthe problem of identifying hierarchical relationships in social networks.", "published": "2018-12-20 08:56:52", "link": "http://arxiv.org/abs/1812.08425v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "nocaps: novel object captioning at scale", "abstract": "Image captioning models have achieved impressive results on datasets\ncontaining limited visual concepts and large amounts of paired image-caption\ntraining data. However, if these models are to ever function in the wild, a\nmuch larger variety of visual concepts must be learned, ideally from less\nsupervision. To encourage the development of image captioning models that can\nlearn visual concepts from alternative data sources, such as object detection\ndatasets, we present the first large-scale benchmark for this task. Dubbed\n'nocaps', for novel object captioning at scale, our benchmark consists of\n166,100 human-generated captions describing 15,100 images from the OpenImages\nvalidation and test sets. The associated training data consists of COCO\nimage-caption pairs, plus OpenImages image-level labels and object bounding\nboxes. Since OpenImages contains many more classes than COCO, nearly 400 object\nclasses seen in test images have no or very few associated training captions\n(hence, nocaps). We extend existing novel object captioning models to establish\nstrong baselines for this benchmark and provide analysis to guide future work\non this task.", "published": "2018-12-20 16:04:05", "link": "http://arxiv.org/abs/1812.08658v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A unified convolutional beamformer for simultaneous denoising and\n  dereverberation", "abstract": "This paper proposes a method for estimating a convolutional beamformer that\ncan perform denoising and dereverberation simultaneously in an optimal way. The\napplication of dereverberation based on a weighted prediction error (WPE)\nmethod followed by denoising based on a minimum variance distortionless\nresponse (MVDR) beamformer has conventionally been considered a promising\napproach, however, the optimality of this approach cannot be guaranteed. To\nrealize the optimal integration of denoising and dereverberation, we present a\nmethod that unifies the WPE dereverberation method and a variant of the MVDR\nbeamformer, namely a minimum power distortionless response (MPDR) beamformer,\ninto a single convolutional beamformer, and we optimize it based on a single\nunified optimization criterion. The proposed beamformer is referred to as a\nWeighted Power minimization Distortionless response (WPD) beamformer.\nExperiments show that the proposed method substantially improves the speech\nenhancement performance in terms of both objective speech enhancement measures\nand automatic speech recognition (ASR) performance.", "published": "2018-12-20 07:36:24", "link": "http://arxiv.org/abs/1812.08400v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fr\u00e9chet Audio Distance: A Metric for Evaluating Music Enhancement\n  Algorithms", "abstract": "We propose the Fr\\'echet Audio Distance (FAD), a novel, reference-free\nevaluation metric for music enhancement algorithms. We demonstrate how typical\nevaluation metrics for speech enhancement and blind source separation can fail\nto accurately measure the perceived effect of a wide variety of distortions. As\nan alternative, we propose adapting the Fr\\'echet Inception Distance (FID)\nmetric used to evaluate generative image models to the audio domain. FAD is\nvalidated using a wide variety of artificial distortions and is compared to the\nsignal based metrics signal to distortion ratio (SDR), cosine distance and\nmagnitude L2 distance. We show that, with a correlation coefficient of 0.52,\nFAD correlates more closely with human perception than either SDR, cosine\ndistance or magnitude L2 distance, with correlation coefficients of 0.39, -0.15\nand -0.01 respectively.", "published": "2018-12-20 10:28:00", "link": "http://arxiv.org/abs/1812.08466v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multichannel Online Dereverberation based on Spectral Magnitude Inverse\n  Filtering", "abstract": "This paper addresses the problem of multichannel online dereverberation. The\nproposed method is carried out in the short-time Fourier transform (STFT)\ndomain, and for each frequency band independently. In the STFT domain, the\ntime-domain room impulse response is approximately represented by the\nconvolutive transfer function (CTF). The multichannel CTFs are adaptively\nidentified based on the cross-relation method, and using the recursive least\nsquare criterion. Instead of the complex-valued CTF convolution model, we use a\nnonnegative convolution model between the STFT magnitude of the source signal\nand the CTF magnitude, which is just a coarse approximation of the former\nmodel, but is shown to be more robust against the CTF perturbations. Based on\nthis nonnegative model, we propose an online STFT magnitude inverse filtering\nmethod. The inverse filters of the CTF magnitude are formulated based on the\nmultiple-input/output inverse theorem (MINT), and adaptively estimated based on\nthe gradient descent criterion. Finally, the inverse filtering is applied to\nthe STFT magnitude of the microphone signals, obtaining an estimate of the STFT\nmagnitude of the source signal. Experiments regarding both speech enhancement\nand automatic speech recognition are conducted, which demonstrate that the\nproposed method can effectively suppress reverberation, even for the difficult\ncase of a moving speaker.", "published": "2018-12-20 10:35:01", "link": "http://arxiv.org/abs/1812.08471v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
