{"title": "ADVISE: AI-accelerated Design of Evidence Synthesis for Global\n  Development", "abstract": "When designing evidence-based policies and programs, decision-makers must\ndistill key information from a vast and rapidly growing literature base.\nIdentifying relevant literature from raw search results is time and resource\nintensive, and is often done by manual screening. In this study, we develop an\nAI agent based on a bidirectional encoder representations from transformers\n(BERT) model and incorporate it into a human team designing an evidence\nsynthesis product for global development. We explore the effectiveness of the\nhuman-AI hybrid team in accelerating the evidence synthesis process. To further\nimprove team efficiency, we enhance the human-AI hybrid team through active\nlearning (AL). Specifically, we explore different sampling strategies,\nincluding random sampling, least confidence (LC) sampling, and highest priority\n(HP) sampling, to study their influence on the collaborative screening process.\nResults show that incorporating the BERT-based AI agent into the human team can\nreduce the human screening effort by 68.5% compared to the case of no AI\nassistance and by 16.8% compared to the case of using a support vector machine\n(SVM)-based AI agent for identifying 80% of all relevant documents. When we\napply the HP sampling strategy for AL, the human screening effort can be\nreduced even more: by 78.3% for identifying 80% of all relevant documents\ncompared to no AI assistance. We apply the AL-enhanced human-AI hybrid teaming\nworkflow in the design process of three evidence gap maps (EGMs) for USAID and\nfind it to be highly effective. These findings demonstrate how AI can\naccelerate the development of evidence synthesis products and promote timely\nevidence-based decision making in global development in a human-AI hybrid\nteaming context.", "published": "2023-05-02 01:29:53", "link": "http://arxiv.org/abs/2305.01145v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RadAdapt: Radiology Report Summarization via Lightweight Domain\n  Adaptation of Large Language Models", "abstract": "We systematically investigate lightweight strategies to adapt large language\nmodels (LLMs) for the task of radiology report summarization (RRS).\nSpecifically, we focus on domain adaptation via pretraining (on natural\nlanguage, biomedical text, or clinical text) and via discrete prompting or\nparameter-efficient fine-tuning. Our results consistently achieve best\nperformance by maximally adapting to the task via pretraining on clinical text\nand fine-tuning on RRS examples. Importantly, this method fine-tunes a mere\n0.32% of parameters throughout the model, in contrast to end-to-end fine-tuning\n(100% of parameters). Additionally, we study the effect of in-context examples\nand out-of-distribution (OOD) training before concluding with a radiologist\nreader study and qualitative analysis. Our findings highlight the importance of\ndomain adaptation in RRS and provide valuable insights toward developing\neffective natural language processing solutions for clinical tasks.", "published": "2023-05-02 01:33:02", "link": "http://arxiv.org/abs/2305.01146v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Paradigm Shift: The Future of Machine Translation Lies with Large\n  Language Models", "abstract": "Machine Translation (MT) has greatly advanced over the years due to the\ndevelopments in deep neural networks. However, the emergence of Large Language\nModels (LLMs) like GPT-4 and ChatGPT is introducing a new phase in the MT\ndomain. In this context, we believe that the future of MT is intricately tied\nto the capabilities of LLMs. These models not only offer vast linguistic\nunderstandings but also bring innovative methodologies, such as prompt-based\ntechniques, that have the potential to further elevate MT. In this paper, we\nprovide an overview of the significant enhancements in MT that are influenced\nby LLMs and advocate for their pivotal role in upcoming MT research and\nimplementations. We highlight several new MT directions, emphasizing the\nbenefits of LLMs in scenarios such as Long-Document Translation, Stylized\nTranslation, and Interactive Translation. Additionally, we address the\nimportant concern of privacy in LLM-driven MT and suggest essential\nprivacy-preserving strategies. By showcasing practical instances, we aim to\ndemonstrate the advantages that LLMs offer, particularly in tasks like\ntranslating extended documents. We conclude by emphasizing the critical role of\nLLMs in guiding the future evolution of MT and offer a roadmap for future\nexploration in the sector.", "published": "2023-05-02 03:27:27", "link": "http://arxiv.org/abs/2305.01181v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Role of Summarization in Generative Agents: A Preliminary\n  Perspective", "abstract": "Generative agents that simulate human society show tremendous potential for\nfurther research and practical applications. Specifically, the generative agent\narchitecture comprising several meticulously designed modules constitutes the\nmost critical component. To facilitate progress in this research, this report\npresents our integrated perspective on comprehending generative agents through\nsummarization, since we believe summarization is the most fundamental and\nindispensable capacity of generative agents manifested across diverse\nscenarios. We hope this report can provide insight into understanding the\nimportance of summarization capacity in generative agents and motivate future\nresearch.", "published": "2023-05-02 08:35:09", "link": "http://arxiv.org/abs/2305.01253v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Turning Flowchart into Dialog: Augmenting Flowchart-grounded\n  Troubleshooting Dialogs via Synthetic Data Generation", "abstract": "Flowchart-grounded troubleshooting dialogue (FTD) systems, which follow the\ninstructions of a flowchart to diagnose users' problems in specific domains\n(e.g., vehicle, laptop), have been gaining research interest in recent years.\nHowever, collecting sufficient dialogues that are naturally grounded on\nflowcharts is costly, thus FTD systems are impeded by scarce training data. To\nmitigate the data sparsity issue, we propose a plan-based synthetic data\ngeneration (PlanSDG) approach that generates diverse synthetic dialog data at\nscale by transforming concise flowchart into dialogues. Specifically, its\ngenerative model employs a variational-base framework with a hierarchical\nplanning strategy that includes global and local latent planning variables.\nExperiments on the FloDial dataset show that synthetic dialogue produced by\nPlanSDG improves the performance of downstream tasks, including flowchart path\nretrieval and response generation, in particular on the Out-of-Flowchart\nsettings. In addition, further analysis demonstrate the quality of synthetic\ndata generated by PlanSDG in paths that are covered by current sample dialogues\nand paths that are not covered.", "published": "2023-05-02 11:08:27", "link": "http://arxiv.org/abs/2305.01323v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OTIEA:Ontology-enhanced Triple Intrinsic-Correlation for Cross-lingual\n  Entity Alignment", "abstract": "Cross-lingual and cross-domain knowledge alignment without sufficient\nexternal resources is a fundamental and crucial task for fusing irregular data.\nAs the element-wise fusion process aiming to discover equivalent objects from\ndifferent knowledge graphs (KGs), entity alignment (EA) has been attracting\ngreat interest from industry and academic research recent years. Most of\nexisting EA methods usually explore the correlation between entities and\nrelations through neighbor nodes, structural information and external\nresources. However, the complex intrinsic interactions among triple elements\nand role information are rarely modeled in these methods, which may lead to the\ninadequate illustration for triple. In addition, external resources are usually\nunavailable in some scenarios especially cross-lingual and cross-domain\napplications, which reflects the little scalability of these methods. To tackle\nthe above insufficiency, a novel universal EA framework (OTIEA) based on\nontology pair and role enhancement mechanism via triple-aware attention is\nproposed in this paper without introducing external resources. Specifically, an\nontology-enhanced triple encoder is designed via mining intrinsic correlations\nand ontology pair information instead of independent elements. In addition, the\nEA-oriented representations can be obtained in triple-aware entity decoder by\nfusing role diversity. Finally, a bidirectional iterative alignment strategy is\ndeployed to expand seed entity pairs. The experimental results on three\nreal-world datasets show that our framework achieves a competitive performance\ncompared with baselines.", "published": "2023-05-02 16:03:54", "link": "http://arxiv.org/abs/2305.01561v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UNTER: A Unified Knowledge Interface for Enhancing Pre-trained Language\n  Models", "abstract": "Recent research demonstrates that external knowledge injection can advance\npre-trained language models (PLMs) in a variety of downstream NLP tasks.\nHowever, existing knowledge injection methods are either applicable to\nstructured knowledge or unstructured knowledge, lacking a unified usage. In\nthis paper, we propose a UNified knowledge inTERface, UNTER, to provide a\nunified perspective to exploit both structured knowledge and unstructured\nknowledge. In UNTER, we adopt the decoder as a unified knowledge interface,\naligning span representations obtained from the encoder with their\ncorresponding knowledge. This approach enables the encoder to uniformly invoke\nspan-related knowledge from its parameters for downstream applications.\nExperimental results show that, with both forms of knowledge injected, UNTER\ngains continuous improvements on a series of knowledge-driven NLP tasks,\nincluding entity typing, named entity recognition and relation extraction,\nespecially in low-resource scenarios.", "published": "2023-05-02 17:33:28", "link": "http://arxiv.org/abs/2305.01624v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unlimiformer: Long-Range Transformers with Unlimited Length Input", "abstract": "Since the proposal of transformers, these models have been limited to bounded\ninput lengths, because of their need to attend to every token in the input. In\nthis work, we propose Unlimiformer: a general approach that wraps any existing\npretrained encoder-decoder transformer, and offloads the cross-attention\ncomputation to a single k-nearest-neighbor (kNN) index, while the returned kNN\ndistances are the attention dot-product scores. This kNN index can be kept on\neither the GPU or CPU memory and queried in sub-linear time; this way, we can\nindex practically unlimited input sequences, while every attention head in\nevery decoder layer retrieves its top-k keys, instead of attending to every\nkey. We evaluate Unlimiformer on several long-document and book-summarization\nbenchmarks, showing that it can process even 500k token-long inputs from the\nBookSum dataset, without any input truncation at test time. We demonstrate that\nUnlimiformer improves pretrained models such as BART and Longformer by\nextending them to unlimited inputs without additional learned weights and\nwithout modifying their code. We make our code and models publicly available at\nhttps://github.com/abertsch72/unlimiformer .", "published": "2023-05-02 17:35:08", "link": "http://arxiv.org/abs/2305.01625v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Missing Information, Unresponsive Authors, Experimental Flaws: The\n  Impossibility of Assessing the Reproducibility of Previous Human Evaluations\n  in NLP", "abstract": "We report our efforts in identifying a set of previous human evaluations in\nNLP that would be suitable for a coordinated study examining what makes human\nevaluations in NLP more/less reproducible. We present our results and findings,\nwhich include that just 13\\% of papers had (i) sufficiently low barriers to\nreproduction, and (ii) enough obtainable information, to be considered for\nreproduction, and that all but one of the experiments we selected for\nreproduction was discovered to have flaws that made the meaningfulness of\nconducting a reproduction questionable. As a result, we had to change our\ncoordinated study design from a reproduce approach to a\nstandardise-then-reproduce-twice approach. Our overall (negative) finding that\nthe great majority of human evaluations in NLP is not repeatable and/or not\nreproducible and/or too flawed to justify reproduction, paints a dire picture,\nbut presents an opportunity for a rethink about how to design and report human\nevaluations in NLP.", "published": "2023-05-02 17:46:12", "link": "http://arxiv.org/abs/2305.01633v2", "categories": ["cs.CL", "68", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models", "abstract": "Fine-tuning large models is highly effective, however, inference can be\nexpensive and produces carbon emissions. Knowledge distillation has been shown\nto be a practical solution to reduce inference costs, but the distillation\nprocess itself requires significant computational resources. Rather than buying\nor renting GPUs to fine-tune, then distill a large model, an NLP practitioner\nmight instead choose to allocate the available budget to hire annotators and\nmanually label additional fine-tuning data. In this paper, we investigate how\nto most efficiently use a fixed budget to build a compact model. Through\nextensive experiments on six diverse tasks, we show that distilling from T5-XXL\n(11B) to T5-Small (60M) is almost always a cost-efficient strategy compared to\nannotating more data to directly train a compact model (T5-Small). We further\ninvestigate how the optimal budget allocated towards computation varies across\nscenarios. We will make our code, datasets, annotation cost estimates, and\nbaseline models available as a benchmark to support further work on\ncost-efficient training of compact models.", "published": "2023-05-02 17:56:16", "link": "http://arxiv.org/abs/2305.01645v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can LMs Learn New Entities from Descriptions? Challenges in Propagating\n  Injected Knowledge", "abstract": "Pre-trained language models (LMs) are used for knowledge intensive tasks like\nquestion answering, but their knowledge gets continuously outdated as the world\nchanges. Prior work has studied targeted updates to LMs, injecting individual\nfacts and evaluating whether the model learns these facts while not changing\npredictions on other contexts. We take a step forward and study LMs' abilities\nto make inferences based on injected facts (or propagate those facts): for\nexample, after learning that something is a TV show, does an LM predict that\nyou can watch it? We study this with two cloze-style tasks: an existing dataset\nof real-world sentences about novel entities (ECBD) as well as a new controlled\nbenchmark with manually designed templates requiring varying levels of\ninference about injected knowledge. Surprisingly, we find that existing methods\nfor updating knowledge (gradient-based fine-tuning and modifications of this\napproach) show little propagation of injected knowledge. These methods improve\nperformance on cloze instances only when there is lexical overlap between\ninjected facts and target inferences. Yet, prepending entity definitions in an\nLM's context improves performance across all settings, suggesting that there is\nsubstantial headroom for parameter-updating approaches for knowledge injection.", "published": "2023-05-02 17:59:46", "link": "http://arxiv.org/abs/2305.01651v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Stars to Insights: Exploration and Implementation of Unified\n  Sentiment Analysis with Distant Supervision", "abstract": "Sentiment analysis is integral to understanding the voice of the customer and\ninforming businesses' strategic decisions. Conventional sentiment analysis\ninvolves three separate tasks: aspect-category detection (ACD), aspect-category\nsentiment analysis (ACSA), and rating prediction (RP). However, independently\ntackling these tasks can overlook their interdependencies and often requires\nexpensive, fine-grained annotations. This paper introduces Unified Sentiment\nAnalysis (Uni-SA), a novel learning paradigm that unifies ACD, ACSA, and RP\ninto a coherent framework. To achieve this, we propose the Distantly Supervised\nPyramid Network (DSPN), which employs a pyramid structure to capture sentiment\nat word, aspect, and document levels in a hierarchical manner. Evaluations on\nmulti-aspect review datasets in English and Chinese show that DSPN, using only\nstar rating labels for supervision, demonstrates significant efficiency\nadvantages while performing comparably well to a variety of benchmark models.\nAdditionally, DSPN's pyramid structure enables the interpretability of its\noutputs. Our findings validate DSPN's effectiveness and efficiency,\nestablishing a robust, resource-efficient, unified framework for sentiment\nanalysis.", "published": "2023-05-02 18:23:50", "link": "http://arxiv.org/abs/2305.01710v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner", "abstract": "Language models (LMs) trained on vast quantities of unlabelled data have\ngreatly advanced the field of natural language processing (NLP). In this study,\nwe re-visit the widely accepted notion in NLP that continued pre-training LMs\non task-related texts improves the performance of fine-tuning (FT) in\ndownstream tasks. Through experiments on eight single-sentence tasks and eight\nsentence-pair tasks in both semi-supervised and fully-supervised settings, we\nfind that conventional continued pre-training does not consistently provide\nbenefits and can even be detrimental for sentence-pair tasks or when\nprompt-based FT is used. To tackle these issues, we propose Prompt-based\nContinued Pre-training (PCP), which combines the idea of instruction tuning\nwith conventional continued pre-training. Our approach aims to improve the\nperformance of prompt-based FT by presenting both task-related texts and prompt\ntemplates to LMs through unsupervised pre-training objectives before\nfine-tuning for the target task. Our empirical evaluations on 21 benchmarks\ndemonstrate that the PCP consistently improves the performance of\nstate-of-the-art prompt-based FT approaches (up to 20.1% absolute) in both\nsemi-supervised and fully-supervised settings, even with only hundreds of\nunlabelled examples. Additionally, prompt-based FT with the PCP outperforms\nstate-of-the-art semi-supervised approaches with greater simplicity,\neliminating the need for an iterative process and extra data augmentation. Our\nfurther analysis explores the performance lower bound of the PCP and reveals\nthat the advantages of PCP persist across different sizes of models and\ndatasets.", "published": "2023-05-02 18:25:30", "link": "http://arxiv.org/abs/2305.01711v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stance Detection: A Practical Guide to Classifying Political Beliefs in\n  Text", "abstract": "Stance detection is identifying expressed beliefs in a document. While\nresearchers widely use sentiment analysis for this, recent research\ndemonstrates that sentiment and stance are distinct. This paper advances text\nanalysis methods by precisely defining stance detection and presenting three\ndistinct approaches: supervised classification, natural language inference, and\nin-context learning with generative language models. I discuss how document\ncontext and trade-offs between resources and workload should inform your\nmethods. For all three approaches I provide guidance on application and\nvalidation techniques, as well as coding tutorials for implementation. Finally,\nI demonstrate how newer classification approaches can replicate supervised\nclassifiers.", "published": "2023-05-02 18:49:12", "link": "http://arxiv.org/abs/2305.01723v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DiffuSum: Generation Enhanced Extractive Summarization with Diffusion", "abstract": "Extractive summarization aims to form a summary by directly extracting\nsentences from the source document. Existing works mostly formulate it as a\nsequence labeling problem by making individual sentence label predictions. This\npaper proposes DiffuSum, a novel paradigm for extractive summarization, by\ndirectly generating the desired summary sentence representations with diffusion\nmodels and extracting sentences based on sentence representation matching. In\naddition, DiffuSum jointly optimizes a contrastive sentence encoder with a\nmatching loss for sentence representation alignment and a multi-class\ncontrastive loss for representation diversity. Experimental results show that\nDiffuSum achieves the new state-of-the-art extractive results on CNN/DailyMail\nwith ROUGE scores of $44.83/22.56/40.56$. Experiments on the other two datasets\nwith different summary lengths also demonstrate the effectiveness of DiffuSum.\nThe strong performance of our framework shows the great potential of adapting\ngenerative models for extractive summarization. To encourage more following\nwork in the future, we have released our codes at\n\\url{https://github.com/hpzhang94/DiffuSum}", "published": "2023-05-02 19:09:16", "link": "http://arxiv.org/abs/2305.01735v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Procedural Planning via Dual Text-Image Prompting", "abstract": "Embodied agents have achieved prominent performance in following human\ninstructions to complete tasks. However, the potential of providing\ninstructions informed by texts and images to assist humans in completing tasks\nremains underexplored. To uncover this capability, we present the multimodal\nprocedural planning (MPP) task, in which models are given a high-level goal and\ngenerate plans of paired text-image steps, providing more complementary and\ninformative guidance than unimodal plans. The key challenges of MPP are to\nensure the informativeness, temporal coherence,and accuracy of plans across\nmodalities. To tackle this, we propose Text-Image Prompting (TIP), a\ndual-modality prompting method that jointly leverages zero-shot reasoning\nability in large language models (LLMs) and compelling text-to-image generation\nability from diffusion-based models. TIP improves the interaction in the dual\nmodalities using Text-to-Image Bridge and Image-to-Text Bridge, allowing LLMs\nto guide the textual-grounded image plan generation and leveraging the\ndescriptions of image plans to ground the textual plan reversely. To address\nthe lack of relevant datasets, we collect WIKIPLAN and RECIPEPLAN as a testbed\nfor MPP. Our results show compelling human preferences and automatic scores\nagainst unimodal and multimodal baselines on WIKIPLAN and RECIPEPLAN in terms\nof informativeness, temporal coherence, and plan accuracy. Our code and data:\nhttps://github.com/YujieLu10/MPP.", "published": "2023-05-02 21:46:44", "link": "http://arxiv.org/abs/2305.01795v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Post-Abstention: Towards Reliably Re-Attempting the Abstained Instances\n  in QA", "abstract": "Despite remarkable progress made in natural language processing, even the\nstate-of-the-art models often make incorrect predictions. Such predictions\nhamper the reliability of systems and limit their widespread adoption in\nreal-world applications. 'Selective prediction' partly addresses the above\nconcern by enabling models to abstain from answering when their predictions are\nlikely to be incorrect. While selective prediction is advantageous, it leaves\nus with a pertinent question 'what to do after abstention'. To this end, we\npresent an explorative study on 'Post-Abstention', a task that allows\nre-attempting the abstained instances with the aim of increasing 'coverage' of\nthe system without significantly sacrificing its 'accuracy'. We first provide\nmathematical formulation of this task and then explore several methods to solve\nit. Comprehensive experiments on 11 QA datasets show that these methods lead to\nconsiderable risk improvements -- performance metric of the Post-Abstention\ntask -- both in the in-domain and the out-of-domain settings. We also conduct a\nthorough analysis of these results which further leads to several interesting\nfindings. Finally, we believe that our work will encourage and facilitate\nfurther research in this important area of addressing the reliability of NLP\nsystems.", "published": "2023-05-02 22:40:41", "link": "http://arxiv.org/abs/2305.01812v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Cancer Hallmark Classification with BERT-based Deep Learning\n  Approach", "abstract": "This paper presents a novel approach to accurately classify the hallmarks of\ncancer, which is a crucial task in cancer research. Our proposed method\nutilizes the Bidirectional Encoder Representations from Transformers (BERT)\narchitecture, which has shown exceptional performance in various downstream\napplications. By applying transfer learning, we fine-tuned the pre-trained BERT\nmodel on a small corpus of biomedical text documents related to cancer. The\noutcomes of our experimental investigations demonstrate that our approach\nattains a noteworthy accuracy of 94.45%, surpassing almost all prior findings\nwith a substantial increase of at least 8.04% as reported in the literature.\nThese findings highlight the effectiveness of our proposed model in accurately\nclassifying and comprehending text documents for cancer research, thus\ncontributing significantly to the field. As cancer remains one of the top ten\nleading causes of death globally, our approach holds great promise in advancing\ncancer research and improving patient outcomes.", "published": "2023-05-02 09:57:54", "link": "http://arxiv.org/abs/2305.03501v2", "categories": ["cs.CL", "I.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion\n  Synthesis", "abstract": "In this paper, we present TMR, a simple yet effective approach for text to 3D\nhuman motion retrieval. While previous work has only treated retrieval as a\nproxy evaluation metric, we tackle it as a standalone task. Our method extends\nthe state-of-the-art text-to-motion synthesis model TEMOS, and incorporates a\ncontrastive loss to better structure the cross-modal latent space. We show that\nmaintaining the motion generation loss, along with the contrastive training, is\ncrucial to obtain good performance. We introduce a benchmark for evaluation and\nprovide an in-depth analysis by reporting results on several protocols. Our\nextensive experiments on the KIT-ML and HumanML3D datasets show that TMR\noutperforms the prior work by a significant margin, for example reducing the\nmedian rank from 54 to 19. Finally, we showcase the potential of our approach\non moment retrieval. Our code and models are publicly available at\nhttps://mathis.petrovich.fr/tmr.", "published": "2023-05-02 17:52:41", "link": "http://arxiv.org/abs/2305.00976v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Topic Shift Detection in Chinese Dialogues: Corpus and Benchmark", "abstract": "Dialogue topic shift detection is to detect whether an ongoing topic has\nshifted or should shift in a dialogue, which can be divided into two\ncategories, i.e., response-known task and response-unknown task. Currently,\nonly a few investigated the latter, because it is still a challenge to predict\nthe topic shift without the response information. In this paper, we first\nannotate a Chinese Natural Topic Dialogue (CNTD) corpus consisting of 1308\ndialogues to fill the gap in the Chinese natural conversation topic corpus. And\nthen we focus on the response-unknown task and propose a teacher-student\nframework based on hierarchical contrastive learning to predict the topic shift\nwithout the response. Specifically, the response at high-level teacher-student\nis introduced to build the contrastive learning between the response and the\ncontext, while the label contrastive learning is constructed at low-level\nstudent. The experimental results on our Chinese CNTD and English TIAGE show\nthe effectiveness of our proposed model.", "published": "2023-05-02 04:03:50", "link": "http://arxiv.org/abs/2305.01195v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in\n  Language Models", "abstract": "The prompt-based learning paradigm, which bridges the gap between\npre-training and fine-tuning, achieves state-of-the-art performance on several\nNLP tasks, particularly in few-shot settings. Despite being widely applied,\nprompt-based learning is vulnerable to backdoor attacks. Textual backdoor\nattacks are designed to introduce targeted vulnerabilities into models by\npoisoning a subset of training samples through trigger injection and label\nmodification. However, they suffer from flaws such as abnormal natural language\nexpressions resulting from the trigger and incorrect labeling of poisoned\nsamples. In this study, we propose ProAttack, a novel and efficient method for\nperforming clean-label backdoor attacks based on the prompt, which uses the\nprompt itself as a trigger. Our method does not require external triggers and\nensures correct labeling of poisoned samples, improving the stealthy nature of\nthe backdoor attack. With extensive experiments on rich-resource and few-shot\ntext classification tasks, we empirically validate ProAttack's competitive\nperformance in textual backdoor attacks. Notably, in the rich-resource setting,\nProAttack achieves state-of-the-art attack success rates in the clean-label\nbackdoor attack benchmark without external triggers.", "published": "2023-05-02 06:19:36", "link": "http://arxiv.org/abs/2305.01219v6", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Class based Influence Functions for Error Detection", "abstract": "Influence functions (IFs) are a powerful tool for detecting anomalous\nexamples in large scale datasets. However, they are unstable when applied to\ndeep networks. In this paper, we provide an explanation for the instability of\nIFs and develop a solution to this problem. We show that IFs are unreliable\nwhen the two data points belong to two different classes. Our solution\nleverages class information to improve the stability of IFs. Extensive\nexperiments show that our modification significantly improves the performance\nand stability of IFs while incurring no additional computational cost.", "published": "2023-05-02 13:01:39", "link": "http://arxiv.org/abs/2305.01384v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "From Local to Global: Navigating Linguistic Diversity in the African\n  Context", "abstract": "The focus is on critical problems in NLP related to linguistic diversity and\nvariation across the African continent, specifically with regards to African\nlocal dialects and Arabic dialects that have received little attention. We\nevaluated our various approaches, demonstrating their effectiveness while\nhighlighting the potential impact of the proposed approach on businesses\nseeking to improve customer experience and product development in African local\ndialects. The idea of using the model as a teaching tool for product-based\ninstruction is interesting, as it could potentially stimulate interest in\nlearners and trigger techno entrepreneurship. Overall, our modified approach\noffers a promising analysis of the challenges of dealing with African local\ndialects. Particularly Arabic dialects, which could have a significant impact\non businesses seeking to improve customer experience and product development.", "published": "2023-05-02 13:57:44", "link": "http://arxiv.org/abs/2305.01427v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sentiment Perception Adversarial Attacks on Neural Machine Translation\n  Systems", "abstract": "With the advent of deep learning methods, Neural Machine Translation (NMT)\nsystems have become increasingly powerful. However, deep learning based systems\nare susceptible to adversarial attacks, where imperceptible changes to the\ninput can cause undesirable changes at the output of the system. To date there\nhas been little work investigating adversarial attacks on sequence-to-sequence\nsystems, such as NMT models. Previous work in NMT has examined attacks with the\naim of introducing target phrases in the output sequence. In this work,\nadversarial attacks for NMT systems are explored from an output perception\nperspective. Thus the aim of an attack is to change the perception of the\noutput sequence, without altering the perception of the input sequence. For\nexample, an adversary may distort the sentiment of translated reviews to have\nan exaggerated positive sentiment. In practice it is challenging to run\nextensive human perception experiments, so a proxy deep-learning classifier\napplied to the NMT output is used to measure perception changes. Experiments\ndemonstrate that the sentiment perception of NMT systems' output sequences can\nbe changed significantly with small imperceptible changes to input sequences.", "published": "2023-05-02 14:06:47", "link": "http://arxiv.org/abs/2305.01437v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Summarizing Multiple Documents with Conversational Structure for\n  Meta-Review Generation", "abstract": "We present PeerSum, a novel dataset for generating meta-reviews of scientific\npapers. The meta-reviews can be interpreted as abstractive summaries of\nreviews, multi-turn discussions and the paper abstract. These source documents\nhave rich inter-document relationships with an explicit hierarchical\nconversational structure, cross-references and (occasionally) conflicting\ninformation. To introduce the structural inductive bias into pre-trained\nlanguage models, we introduce Rammer ( Relationship-aware Multi-task\nMeta-review Generator), a model that uses sparse attention based on the\nconversational structure and a multi-task training objective that predicts\nmetadata features (e.g., review ratings). Our experimental results show that\nRammer outperforms other strong baseline models in terms of a suite of\nautomatic evaluation metrics. Further analyses, however, reveal that RAMMER and\nother models struggle to handle conflicts in source documents of PeerSum,\nsuggesting meta-review generation is a challenging task and a promising avenue\nfor further research.", "published": "2023-05-02 15:18:18", "link": "http://arxiv.org/abs/2305.01498v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Huatuo-26M, a Large-scale Chinese Medical QA Dataset", "abstract": "In this paper, we release a largest ever medical Question Answering (QA)\ndataset with 26 million QA pairs. We benchmark many existing approaches in our\ndataset in terms of both retrieval and generation. Experimental results show\nthat the existing models perform far lower than expected and the released\ndataset is still challenging in the pre-trained language model era. Moreover,\nwe also experimentally show the benefit of the proposed dataset in many\naspects: (i) trained models for other QA datasets in a zero-shot fashion; and\n(ii) as external knowledge for retrieval-augmented generation (RAG); and (iii)\nimproving existing pre-trained language models by using the QA pairs as a\npre-training corpus in continued training manner. We believe that this dataset\nwill not only contribute to medical research but also facilitate both the\npatients and clinical doctors. See\n\\url{https://github.com/FreedomIntelligence/Huatuo-26M}.", "published": "2023-05-02 15:33:01", "link": "http://arxiv.org/abs/2305.01526v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured\n  Game State Information", "abstract": "Dungeons & Dragons (D&D) is a tabletop roleplaying game with complex natural\nlanguage interactions between players and hidden state information. Recent work\nhas shown that large language models (LLMs) that have access to state\ninformation can generate higher quality game turns than LLMs that use dialog\nhistory alone. However, previous work used game state information that was\nheuristically created and was not a true gold standard game state. We present\nFIREBALL, a large dataset containing nearly 25,000 unique sessions from real\nD&D gameplay on Discord with true game state info. We recorded game play\nsessions of players who used the Avrae bot, which was developed to aid people\nin playing D&D online, capturing language, game commands and underlying game\nstate information. We demonstrate that FIREBALL can improve natural language\ngeneration (NLG) by using Avrae state information, improving both automated\nmetrics and human judgments of quality. Additionally, we show that LLMs can\ngenerate executable Avrae commands, particularly after finetuning.", "published": "2023-05-02 15:36:10", "link": "http://arxiv.org/abs/2305.01528v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Type-enhanced Ensemble Triple Representation via Triple-aware Attention\n  for Cross-lingual Entity Alignment", "abstract": "Entity alignment(EA) is a crucial task for integrating cross-lingual and\ncross-domain knowledge graphs(KGs), which aims to discover entities referring\nto the same real-world object from different KGs. Most existing methods\ngenerate aligning entity representation by mining the relevance of triple\nelements via embedding-based methods, paying little attention to triple\nindivisibility and entity role diversity. In this paper, a novel framework\nnamed TTEA -- Type-enhanced Ensemble Triple Representation via Triple-aware\nAttention for Cross-lingual Entity Alignment is proposed to overcome the above\nissues considering ensemble triple specificity and entity role features.\nSpecifically, the ensemble triple representation is derived by regarding\nrelation as information carrier between semantic space and type space, and\nhence the noise influence during spatial transformation and information\npropagation can be smoothly controlled via specificity-aware triple attention.\nMoreover, our framework uses triple-ware entity enhancement to model the role\ndiversity of triple elements. Extensive experiments on three real-world\ncross-lingual datasets demonstrate that our framework outperforms\nstate-of-the-art methods.", "published": "2023-05-02 15:56:11", "link": "http://arxiv.org/abs/2305.01556v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Why So Gullible? Enhancing the Robustness of Retrieval-Augmented Models\n  against Counterfactual Noise", "abstract": "Most existing retrieval-augmented language models (LMs) assume a naive\ndichotomy within a retrieved document set: query-relevance and irrelevance. Our\nwork investigates a more challenging scenario in which even the \"relevant\"\ndocuments may contain misleading or incorrect information, causing conflict\namong the retrieved documents and thereby negatively influencing model\ndecisions as noise. We observe that existing LMs are highly brittle to the\npresence of conflicting information in both the fine-tuning and in-context\nfew-shot learning scenarios. We propose approaches for handling knowledge\nconflicts among retrieved documents by explicitly fine-tuning a discriminator\nor prompting GPT-3.5 to elicit its discriminative capability. Our empirical\nresults on open-domain QA show that these approaches significantly enhance\nmodel robustness. We also provide our findings on incorporating the fine-tuned\ndiscriminator's decision into the in-context learning process, proposing a way\nto exploit the benefits of two disparate learning schemes. Alongside our\nfindings, we provide MacNoise, a machine-generated, conflict-induced dataset to\nfurther encourage research in this direction.", "published": "2023-05-02 16:28:10", "link": "http://arxiv.org/abs/2305.01579v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FreeLM: Fine-Tuning-Free Language Model", "abstract": "Pre-trained language models (PLMs) have achieved remarkable success in NLP\ntasks. Despite the great success, mainstream solutions largely follow the\npre-training then finetuning paradigm, which brings in both high deployment\ncosts and low training efficiency. Nevertheless, fine-tuning on a specific task\nis essential because PLMs are only pre-trained with language signal from large\nraw data. In this paper, we propose a novel fine-tuning-free strategy for\nlanguage models, to consider both language signal and teacher signal. Teacher\nsignal is an abstraction of a battery of downstream tasks, provided in a\nunified proposition format. Trained with both language and strong task-aware\nteacher signals in an interactive manner, our FreeLM model demonstrates strong\ngeneralization and robustness. FreeLM outperforms large models e.g., GPT-3 and\nInstructGPT, on a range of language understanding tasks in experiments. FreeLM\nis much smaller with 0.3B parameters, compared to 175B in these models.", "published": "2023-05-02 17:17:56", "link": "http://arxiv.org/abs/2305.01616v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Benefits of Bad Advice: Autocontrastive Decoding across Model Layers", "abstract": "Applying language models to natural language processing tasks typically\nrelies on the representations in the final model layer, as intermediate hidden\nlayer representations are presumed to be less informative. In this work, we\nargue that due to the gradual improvement across model layers, additional\ninformation can be gleaned from the contrast between higher and lower layers\nduring inference. Specifically, in choosing between the probable next token\npredictions of a generative model, the predictions of lower layers can be used\nto highlight which candidates are best avoided. We propose a novel approach\nthat utilizes the contrast between layers to improve text generation outputs,\nand show that it mitigates degenerative behaviors of the model in open-ended\ngeneration, significantly improving the quality of generated texts.\nFurthermore, our results indicate that contrasting between model layers at\ninference time can yield substantial benefits to certain aspects of general\nlanguage model capabilities, more effectively extracting knowledge during\ninference from a given set of model parameters.", "published": "2023-05-02 17:42:37", "link": "http://arxiv.org/abs/2305.01628v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Disentangled Semantic Spaces of Explanations via Invertible\n  Neural Networks", "abstract": "Disentangled latent spaces usually have better semantic separability and\ngeometrical properties, which leads to better interpretability and more\ncontrollable data generation. While this has been well investigated in Computer\nVision, in tasks such as image disentanglement, in the NLP domain sentence\ndisentanglement is still comparatively under-investigated. Most previous work\nhave concentrated on disentangling task-specific generative factors, such as\nsentiment, within the context of style transfer. In this work, we focus on a\nmore general form of sentence disentanglement, targeting the localised\nmodification and control of more general sentence semantic features. To achieve\nthis, we contribute to a novel notion of sentence semantic disentanglement and\nintroduce a flow-based invertible neural network (INN) mechanism integrated\nwith a transformer-based language Autoencoder (AE) in order to deliver latent\nspaces with better separability properties. Experimental results demonstrate\nthat the model can conform the distributed latent space into a better\nsemantically disentangled sentence space, leading to improved language\ninterpretability and controlled generation when compared to the recent\nstate-of-the-art language VAE models.", "published": "2023-05-02 18:27:13", "link": "http://arxiv.org/abs/2305.01713v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Few-shot In-context Learning for Knowledge Base Question Answering", "abstract": "Question answering over knowledge bases is considered a difficult problem due\nto the challenge of generalizing to a wide variety of possible natural language\nquestions. Additionally, the heterogeneity of knowledge base schema items\nbetween different knowledge bases often necessitates specialized training for\ndifferent knowledge base question-answering (KBQA) datasets. To handle\nquestions over diverse KBQA datasets with a unified training-free framework, we\npropose KB-BINDER, which for the first time enables few-shot in-context\nlearning over KBQA tasks. Firstly, KB-BINDER leverages large language models\nlike Codex to generate logical forms as the draft for a specific question by\nimitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledge\nbase to bind the generated draft to an executable one with BM25 score matching.\nThe experimental results on four public heterogeneous KBQA datasets show that\nKB-BINDER can achieve a strong performance with only a few in-context\ndemonstrations. Especially on GraphQA and 3-hop MetaQA, KB-BINDER can even\noutperform the state-of-the-art trained models. On GrailQA and WebQSP, our\nmodel is also on par with other fully-trained models. We believe KB-BINDER can\nserve as an important baseline for future research. Our code is available at\nhttps://github.com/ltl3A87/KB-BINDER.", "published": "2023-05-02 19:31:55", "link": "http://arxiv.org/abs/2305.01750v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SLTUNET: A Simple Unified Model for Sign Language Translation", "abstract": "Despite recent successes with neural models for sign language translation\n(SLT), translation quality still lags behind spoken languages because of the\ndata scarcity and modality gap between sign video and text. To address both\nproblems, we investigate strategies for cross-modality representation sharing\nfor SLT. We propose SLTUNET, a simple unified neural model designed to support\nmultiple SLTrelated tasks jointly, such as sign-to-gloss, gloss-to-text and\nsign-to-text translation. Jointly modeling different tasks endows SLTUNET with\nthe capability to explore the cross-task relatedness that could help narrow the\nmodality gap. In addition, this allows us to leverage the knowledge from\nexternal resources, such as abundant parallel data used for spoken-language\nmachine translation (MT). We show in experiments that SLTUNET achieves\ncompetitive and even state-of-the-art performance on PHOENIX-2014T and\nCSL-Daily when augmented with MT data and equipped with a set of optimization\ntechniques. We further use the DGS Corpus for end-to-end SLT for the first\ntime. It covers broader domains with a significantly larger vocabulary, which\nis more challenging and which we consider to allow for a more realistic\nassessment of the current state of SLT than the former two. Still, SLTUNET\nobtains improved results on the DGS Corpus. Code is available at\nhttps://github.com/bzhangGo/sltunet.", "published": "2023-05-02 20:41:59", "link": "http://arxiv.org/abs/2305.01778v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "KEPLET: Knowledge-Enhanced Pretrained Language Model with Topic Entity\n  Awareness", "abstract": "In recent years, Pre-trained Language Models (PLMs) have shown their\nsuperiority by pre-training on unstructured text corpus and then fine-tuning on\ndownstream tasks. On entity-rich textual resources like Wikipedia,\nKnowledge-Enhanced PLMs (KEPLMs) incorporate the interactions between tokens\nand mentioned entities in pre-training, and are thus more effective on\nentity-centric tasks such as entity linking and relation classification.\nAlthough exploiting Wikipedia's rich structures to some extent, conventional\nKEPLMs still neglect a unique layout of the corpus where each Wikipedia page is\naround a topic entity (identified by the page URL and shown in the page title).\nIn this paper, we demonstrate that KEPLMs without incorporating the topic\nentities will lead to insufficient entity interaction and biased (relation)\nword semantics. We thus propose KEPLET, a novel Knowledge-Enhanced Pre-trained\nLanguagE model with Topic entity awareness. In an end-to-end manner, KEPLET\nidentifies where to add the topic entity's information in a Wikipedia sentence,\nfuses such information into token and mentioned entities representations, and\nsupervises the network learning, through which it takes topic entities back\ninto consideration. Experiments demonstrated the generality and superiority of\nKEPLET which was applied to two representative KEPLMs, achieving significant\nimprovements on four entity-centric tasks.", "published": "2023-05-02 22:28:26", "link": "http://arxiv.org/abs/2305.01810v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Think Rationally about What You See: Continuous Rationale Extraction for\n  Relation Extraction", "abstract": "Relation extraction (RE) aims to extract potential relations according to the\ncontext of two entities, thus, deriving rational contexts from sentences plays\nan important role. Previous works either focus on how to leverage the entity\ninformation (e.g., entity types, entity verbalization) to inference relations,\nbut ignore context-focused content, or use counterfactual thinking to remove\nthe model's bias of potential relations in entities, but the relation reasoning\nprocess will still be hindered by irrelevant content. Therefore, how to\npreserve relevant content and remove noisy segments from sentences is a crucial\ntask. In addition, retained content needs to be fluent enough to maintain\nsemantic coherence and interpretability. In this work, we propose a novel\nrationale extraction framework named RE2, which leverages two continuity and\nsparsity factors to obtain relevant and coherent rationales from sentences. To\nsolve the problem that the gold rationales are not labeled, RE2 applies an\noptimizable binary mask to each token in the sentence, and adjust the\nrationales that need to be selected according to the relation label.\nExperiments on four datasets show that RE2 surpasses baselines.", "published": "2023-05-02 03:52:34", "link": "http://arxiv.org/abs/2305.03503v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Read it Twice: Towards Faithfully Interpretable Fact Verification by\n  Revisiting Evidence", "abstract": "Real-world fact verification task aims to verify the factuality of a claim by\nretrieving evidence from the source document. The quality of the retrieved\nevidence plays an important role in claim verification. Ideally, the retrieved\nevidence should be faithful (reflecting the model's decision-making process in\nclaim verification) and plausible (convincing to humans), and can improve the\naccuracy of verification task. Although existing approaches leverage the\nsimilarity measure of semantic or surface form between claims and documents to\nretrieve evidence, they all rely on certain heuristics that prevent them from\nsatisfying all three requirements. In light of this, we propose a fact\nverification model named ReRead to retrieve evidence and verify claim that: (1)\nTrain the evidence retriever to obtain interpretable evidence (i.e.,\nfaithfulness and plausibility criteria); (2) Train the claim verifier to\nrevisit the evidence retrieved by the optimized evidence retriever to improve\nthe accuracy. The proposed system is able to achieve significant improvements\nupon best-reported models under different settings.", "published": "2023-05-02 03:23:14", "link": "http://arxiv.org/abs/2305.03507v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Cross-lingual Transfer of Vision and Language Models\n  via Translation-based Alignment", "abstract": "Pre-trained vision and language models such as CLIP have witnessed remarkable\nsuccess in connecting images and texts with a primary focus on English texts.\nDespite recent efforts to extend CLIP to support other languages, disparities\nin performance among different languages have been observed due to uneven\nresource availability. Additionally, current cross-lingual transfer methods of\nthose pre-trained models would consume excessive resources for a large number\nof languages. Therefore, we propose a new parameter-efficient cross-lingual\ntransfer learning framework that utilizes a translation-based alignment method\nto mitigate multilingual disparities and explores parameter-efficient\nfine-tuning methods for parameter-efficient cross-lingual transfer. Extensive\nexperiments on XTD and Multi30K datasets, covering 11 languages under\nzero-shot, few-shot, and full-dataset learning scenarios, show that our\nframework significantly reduces the multilingual disparities among languages\nand improves cross-lingual transfer results, especially in low-resource\nscenarios, while only keeping and fine-tuning an extremely small number of\nparameters compared to the full model (e.g., Our framework only requires 0.16\\%\nadditional parameters of a full-model for each language in the few-shot\nlearning scenario). The codes are available at\n\\url{https://github.com/eric-ai-lab/PECTVLM}. The codes are available at\n\\url{https://github.com/eric-ai-lab/PECTVLM}.", "published": "2023-05-02 14:09:02", "link": "http://arxiv.org/abs/2305.03510v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Shared Latent Space by Both Languages in Non-Autoregressive Neural\n  Machine Translation", "abstract": "Non-autoregressive neural machine translation (NAT) offers substantial\ntranslation speed up compared to autoregressive neural machine translation (AT)\nat the cost of translation quality. Latent variable modeling has emerged as a\npromising approach to bridge this quality gap, particularly for addressing the\nchronic multimodality problem in NAT. In the previous works that used latent\nvariable modeling, they added an auxiliary model to estimate the posterior\ndistribution of the latent variable conditioned on the source and target\nsentences. However, it causes several disadvantages, such as redundant\ninformation extraction in the latent variable, increasing the number of\nparameters, and a tendency to ignore some information from the inputs. In this\npaper, we propose a novel latent variable modeling that integrates a dual\nreconstruction perspective and an advanced hierarchical latent modeling with a\nshared intermediate latent space across languages. This latent variable\nmodeling hypothetically alleviates or prevents the above disadvantages. In our\nexperiment results, we present comprehensive demonstrations that our proposed\napproach infers superior latent variables which lead better translation\nquality. Finally, in the benchmark translation tasks, such as WMT, we\ndemonstrate that our proposed method significantly improves translation quality\ncompared to previous NAT baselines including the state-of-the-art NAT model.", "published": "2023-05-02 15:33:09", "link": "http://arxiv.org/abs/2305.03511v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lessons Learned in ATCO2: 5000 hours of Air Traffic Control\n  Communications for Robust Automatic Speech Recognition and Understanding", "abstract": "Voice communication between air traffic controllers (ATCos) and pilots is\ncritical for ensuring safe and efficient air traffic control (ATC). This task\nrequires high levels of awareness from ATCos and can be tedious and\nerror-prone. Recent attempts have been made to integrate artificial\nintelligence (AI) into ATC in order to reduce the workload of ATCos. However,\nthe development of data-driven AI systems for ATC demands large-scale annotated\ndatasets, which are currently lacking in the field. This paper explores the\nlessons learned from the ATCO2 project, a project that aimed to develop a\nunique platform to collect and preprocess large amounts of ATC data from\nairspace in real time. Audio and surveillance data were collected from publicly\naccessible radio frequency channels with VHF receivers owned by a community of\nvolunteers and later uploaded to Opensky Network servers, which can be\nconsidered an \"unlimited source\" of data. In addition, this paper reviews\nprevious work from ATCO2 partners, including (i) robust automatic speech\nrecognition, (ii) natural language processing, (iii) English language\nidentification of ATC communications, and (iv) the integration of surveillance\ndata such as ADS-B. We believe that the pipeline developed during the ATCO2\nproject, along with the open-sourcing of its data, will encourage research in\nthe ATC field. A sample of the ATCO2 corpus is available on the following\nwebsite: https://www.atco2.org/data, while the full corpus can be purchased\nthrough ELDA at http://catalog.elra.info/en-us/repository/browse/ELRA-S0484. We\ndemonstrated that ATCO2 is an appropriate dataset to develop ASR engines when\nlittle or near to no ATC in-domain data is available. For instance, with the\nCNN-TDNNf kaldi model, we reached the performance of as low as 17.9% and 24.9%\nWER on public ATC datasets which is 6.6/7.6% better than \"out-of-domain\" but\nsupervised CNN-TDNNf model.", "published": "2023-05-02 02:04:33", "link": "http://arxiv.org/abs/2305.01155v1", "categories": ["eess.AS", "cs.CL", "cs.HC", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Pipeline System of ASR and NLU with MLM-based Data Augmentation\n  toward STOP Low-resource Challenge", "abstract": "This paper describes our system for the low-resource domain adaptation track\n(Track 3) in Spoken Language Understanding Grand Challenge, which is a part of\nICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a\npipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain\nwith upsampling. For NLU, we fine-tune BART on all the Track3 data and then on\nlow-resource domain data. We apply masked LM (MLM) -based data augmentation,\nwhere some of input tokens and corresponding target labels are replaced using\nMLM. We also apply a retrieval-based approach, where model input is augmented\nwith similar training samples. As a result, we achieved exact match (EM)\naccuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the\n1st place at the challenge.", "published": "2023-05-02 04:03:30", "link": "http://arxiv.org/abs/2305.01194v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of\n  Large Language Models for Code Generation", "abstract": "Program synthesis has been long studied with recent approaches focused on\ndirectly using the power of Large Language Models (LLMs) to generate code.\nProgramming benchmarks, with curated synthesis problems and test-cases, are\nused to measure the performance of various LLMs on code synthesis. However,\nthese test-cases can be limited in both quantity and quality for fully\nassessing the functional correctness of the generated code. Such limitation in\nthe existing benchmarks begs the following question: In the era of LLMs, is the\ncode generated really correct? To answer this, we propose EvalPlus -- a code\nsynthesis evaluation framework to rigorously benchmark the functional\ncorrectness of LLM-synthesized code. EvalPlus augments a given evaluation\ndataset with large amounts of test-cases newly produced by an automatic test\ninput generator, powered by both LLM- and mutation-based strategies. While\nEvalPlus is general, we extend the test-cases of the popular HumanEval\nbenchmark by 80x to build HumanEval+. Our extensive evaluation across 26\npopular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to\ncatch significant amounts of previously undetected wrong code synthesized by\nLLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that\ntest insufficiency can lead to mis-ranking. For example, both\nWizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+,\nwhile none of them could on HumanEval. Our work not only indicates that prior\npopular code synthesis evaluation results do not accurately reflect the true\nperformance of LLMs for code synthesis, but also opens up a new direction to\nimprove such programming benchmarks through automated testing. We have\nopen-sourced our tools, enhanced datasets as well as all LLM-generated code at\nhttps://github.com/evalplus/evalplus to facilitate and accelerate future\nLLM-for-code research.", "published": "2023-05-02 05:46:48", "link": "http://arxiv.org/abs/2305.01210v3", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "MultiLegalSBD: A Multilingual Legal Sentence Boundary Detection Dataset", "abstract": "Sentence Boundary Detection (SBD) is one of the foundational building blocks\nof Natural Language Processing (NLP), with incorrectly split sentences heavily\ninfluencing the output quality of downstream tasks. It is a challenging task\nfor algorithms, especially in the legal domain, considering the complex and\ndifferent sentence structures used. In this work, we curated a diverse\nmultilingual legal dataset consisting of over 130'000 annotated sentences in 6\nlanguages. Our experimental results indicate that the performance of existing\nSBD models is subpar on multilingual legal data. We trained and tested\nmonolingual and multilingual models based on CRF, BiLSTM-CRF, and transformers,\ndemonstrating state-of-the-art performance. We also show that our multilingual\nmodels outperform all baselines in the zero-shot setting on a Portuguese test\nset. To encourage further research and development by the community, we have\nmade our dataset, models, and code publicly available.", "published": "2023-05-02 05:52:03", "link": "http://arxiv.org/abs/2305.01211v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2; I.7"], "primary_category": "cs.CL"}
{"title": "VPGTrans: Transfer Visual Prompt Generator across LLMs", "abstract": "While developing a new multimodal LLM (MLLM) by pre-training on tremendous\nimage-text pairs from scratch can be exceedingly resource-consuming, connecting\nan existing LLM with a comparatively lightweight visual prompt generator (VPG)\nbecomes a feasible paradigm. However, further tuning the VPG part of the MLLM\nstill suffers from indispensable computational costs, i.e., requiring thousands\nof GPU hours and millions of training data. One alternative solution is to\ntransfer an existing VPG from any existing MLLMs for the target MLLM.\n  In this work, we for the first time investigate the VPG transferability\nacross LLMs, and explore a solution to reduce the cost of VPG transfer. We\nfirst study the VPG transfer across different LLM sizes (e.g., small-to-large),\nand across different LLM types, through which we diagnose the key factors to\nmaximize the transfer efficiency. Based on our observation, we design a\ntwo-stage transfer framework named VPGTrans, which is simple yet highly\neffective. Through extensive experiments, we demonstrate that VPGTrans helps\nsignificantly speed up the transfer learning process without compromising\nperformance. Remarkably, it helps achieve the VPG transfer from BLIP-2\nOPT$_\\text{2.7B}$ to BLIP-2 OPT$_\\text{6.7B}$ with over 10 times speed-up and\n10.7% training data compared with connecting a VPG to OPT$_\\text{6.7B}$ from\nscratch. Further, a series of intriguing findings and potential rationales\nbehind them are provided and discussed. Finally, we showcase the practical\nvalue of our VPGTrans approach, by customizing two novel MLLMs, including\nVL-LLaMA and VL-Vicuna, with recently released LLaMA and Vicuna LLMs.", "published": "2023-05-02 09:28:39", "link": "http://arxiv.org/abs/2305.01278v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Mitigating Approximate Memorization in Language Models via Dissimilarity\n  Learned Policy", "abstract": "Large Language models (LLMs) are trained on large amounts of data, which can\ninclude sensitive information that may compromise personal privacy. LLMs showed\nto memorize parts of the training data and emit those data verbatim when an\nadversary prompts appropriately. Previous research has primarily focused on\ndata preprocessing and differential privacy techniques to address memorization\nor prevent verbatim memorization exclusively, which can give a false sense of\nprivacy. However, these methods rely on explicit and implicit assumptions about\nthe structure of the data to be protected, which often results in an incomplete\nsolution to the problem. To address this, we propose a novel framework that\nutilizes a reinforcement learning approach (PPO) to fine-tune LLMs to mitigate\napproximate memorization. Our approach utilizes a negative similarity score,\nsuch as BERTScore or SacreBLEU, as a reward signal to learn a dissimilarity\npolicy. Our results demonstrate that this framework effectively mitigates\napproximate memorization while maintaining high levels of coherence and fluency\nin the generated samples. Furthermore, our framework is robust in mitigating\napproximate memorization across various circumstances, including longer\ncontext, which is known to increase memorization in LLMs.", "published": "2023-05-02 15:53:28", "link": "http://arxiv.org/abs/2305.01550v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-supervised learning for infant cry analysis", "abstract": "In this paper, we explore self-supervised learning (SSL) for analyzing a\nfirst-of-its-kind database of cry recordings containing clinical indications of\nmore than a thousand newborns. Specifically, we target cry-based detection of\nneurological injury as well as identification of cry triggers such as pain,\nhunger, and discomfort. Annotating a large database in the medical setting is\nexpensive and time-consuming, typically requiring the collaboration of several\nexperts over years. Leveraging large amounts of unlabeled audio data to learn\nuseful representations can lower the cost of building robust models and,\nultimately, clinical solutions. In this work, we experiment with\nself-supervised pre-training of a convolutional neural network on large audio\ndatasets. We show that pre-training with SSL contrastive loss (SimCLR) performs\nsignificantly better than supervised pre-training for both neuro injury and cry\ntriggers. In addition, we demonstrate further performance gains through\nSSL-based domain adaptation using unlabeled infant cries. We also show that\nusing such SSL-based pre-training for adaptation to cry sounds decreases the\nneed for labeled data of the overall system.", "published": "2023-05-02 16:27:18", "link": "http://arxiv.org/abs/2305.01578v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Study on the Integration of Pipeline and E2E SLU systems for Spoken\n  Semantic Parsing toward STOP Quality Challenge", "abstract": "Recently there have been efforts to introduce new benchmark tasks for spoken\nlanguage understanding (SLU), like semantic parsing. In this paper, we describe\nour proposed spoken semantic parsing system for the quality track (Track 1) in\nSpoken Language Understanding Grand Challenge which is part of ICASSP Signal\nProcessing Grand Challenge 2023. We experiment with both end-to-end and\npipeline systems for this task. Strong automatic speech recognition (ASR)\nmodels like Whisper and pretrained Language models (LM) like BART are utilized\ninside our SLU framework to boost performance. We also investigate the output\nlevel combination of various models to get an exact match accuracy of 80.8,\nwhich won the 1st place at the challenge.", "published": "2023-05-02 17:25:19", "link": "http://arxiv.org/abs/2305.01620v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Basic syntax from speech: Spontaneous concatenation in unsupervised deep\n  neural networks", "abstract": "Computational models of syntax are predominantly text-based. Here we propose\nthat the most basic first step in the evolution of syntax can be modeled\ndirectly from raw speech in a fully unsupervised way. We focus on one of the\nmost ubiquitous and elementary suboperation of syntax -- concatenation. We\nintroduce spontaneous concatenation: a phenomenon where convolutional neural\nnetworks (CNNs) trained on acoustic recordings of individual words start\ngenerating outputs with two or even three words concatenated without ever\naccessing data with multiple words in the input. We replicate this finding in\nseveral independently trained models with different hyperparameters and\ntraining data. Additionally, networks trained on two words learn to embed words\ninto novel unobserved word combinations. We also show that the concatenated\noutputs contain precursors to compositionality. To our knowledge, this is a\npreviously unreported property of CNNs trained in the ciwGAN/fiwGAN setting on\nraw speech and has implications both for our understanding of how these\narchitectures learn as well as for modeling syntax and its evolution in the\nbrain from raw acoustic inputs. We also propose a potential neural mechanism\ncalled disinhibition that outlines a possible neural pathway towards\nconcatenation and compositionality and suggests our modeling is useful for\ngenerating testable prediction for biological and artificial neural processing\nof speech.", "published": "2023-05-02 17:38:21", "link": "http://arxiv.org/abs/2305.01626v3", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Psychologically-Inspired Causal Prompts", "abstract": "NLP datasets are richer than just input-output pairs; rather, they carry\ncausal relations between the input and output variables. In this work, we take\nsentiment classification as an example and look into the causal relations\nbetween the review (X) and sentiment (Y). As psychology studies show that\nlanguage can affect emotion, different psychological processes are evoked when\na person first makes a rating and then self-rationalizes their feeling in a\nreview (where the sentiment causes the review, i.e., Y -> X), versus first\ndescribes their experience, and weighs the pros and cons to give a final rating\n(where the review causes the sentiment, i.e., X -> Y ). Furthermore, it is also\na completely different psychological process if an annotator infers the\noriginal rating of the user by theory of mind (ToM) (where the review causes\nthe rating, i.e., X -ToM-> Y ). In this paper, we verbalize these three causal\nmechanisms of human psychological processes of sentiment classification into\nthree different causal prompts, and study (1) how differently they perform, and\n(2) what nature of sentiment classification data leads to agreement or\ndiversity in the model responses elicited by the prompts. We suggest future\nwork raise awareness of different causal structures in NLP tasks. Our code and\ndata are at https://github.com/cogito233/psych-causal-prompt", "published": "2023-05-02 20:06:00", "link": "http://arxiv.org/abs/2305.01764v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ME"], "primary_category": "cs.CL"}
{"title": "Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation\n  Incorporating Gloss Information", "abstract": "Visual Word Sense Disambiguation (VWSD) is a task to find the image that most\naccurately depicts the correct sense of the target word for the given context.\nPreviously, image-text matching models often suffered from recognizing\npolysemous words. This paper introduces an unsupervised VWSD approach that uses\ngloss information of an external lexical knowledge-base, especially the sense\ndefinitions. Specifically, we suggest employing Bayesian inference to\nincorporate the sense definitions when sense information of the answer is not\nprovided. In addition, to ameliorate the out-of-dictionary (OOD) issue, we\npropose a context-aware definition generation with GPT-3. Experimental results\nshow that the VWSD performance significantly increased with our Bayesian\ninference-based approach. In addition, our context-aware definition generation\nachieved prominent performance improvement in OOD examples exhibiting better\nperformance than the existing definition generation method.", "published": "2023-05-02 21:33:10", "link": "http://arxiv.org/abs/2305.01788v3", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Automated Code generation for Information Technology Tasks in YAML\n  through Large Language Models", "abstract": "The recent improvement in code generation capabilities due to the use of\nlarge language models has mainly benefited general purpose programming\nlanguages. Domain specific languages, such as the ones used for IT Automation,\nhave received far less attention, despite involving many active developers and\nbeing an essential component of modern cloud platforms. This work focuses on\nthe generation of Ansible-YAML, a widely used markup language for IT\nAutomation. We present Ansible Wisdom, a natural-language to Ansible-YAML code\ngeneration tool, aimed at improving IT automation productivity. Ansible Wisdom\nis a transformer-based model, extended by training with a new dataset\ncontaining Ansible-YAML. We also develop two novel performance metrics for YAML\nand Ansible to capture the specific characteristics of this domain. Results\nshow that Ansible Wisdom can accurately generate Ansible script from natural\nlanguage prompts with performance comparable or better than existing state of\nthe art code generation models. In few-shot settings we asses the impact of\ntraining with Ansible, YAML data and compare with different baselines including\nCodex-Davinci-002. We also show that after finetuning, our Ansible specific\nmodel (BLEU: 66.67) can outperform a much larger Codex-Davinci-002 (BLEU: 50.4)\nmodel, which was evaluated in few shot settings.", "published": "2023-05-02 21:01:01", "link": "http://arxiv.org/abs/2305.02783v4", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL"], "primary_category": "cs.SE"}
{"title": "Multimodal Neural Databases", "abstract": "The rise in loosely-structured data available through text, images, and other\nmodalities has called for new ways of querying them. Multimedia Information\nRetrieval has filled this gap and has witnessed exciting progress in recent\nyears. Tasks such as search and retrieval of extensive multimedia archives have\nundergone massive performance improvements, driven to a large extent by recent\ndevelopments in multimodal deep learning. However, methods in this field remain\nlimited in the kinds of queries they support and, in particular, their\ninability to answer database-like queries. For this reason, inspired by recent\nwork on neural databases, we propose a new framework, which we name Multimodal\nNeural Databases (MMNDBs). MMNDBs can answer complex database-like queries that\ninvolve reasoning over different input modalities, such as text and images, at\nscale. In this paper, we present the first architecture able to fulfill this\nset of requirements and test it with several baselines, showing the limitations\nof currently available models. The results show the potential of these new\ntechniques to process unstructured data coming from different modalities,\npaving the way for future research in the area. Code to replicate the\nexperiments will be released at\nhttps://github.com/GiovanniTRA/MultimodalNeuralDatabases", "published": "2023-05-02 14:27:56", "link": "http://arxiv.org/abs/2305.01447v1", "categories": ["cs.MM", "cs.CL", "cs.CV", "cs.DB", "cs.IR"], "primary_category": "cs.MM"}
{"title": "How to Unleash the Power of Large Language Models for Few-shot Relation\n  Extraction?", "abstract": "Scaling language models have revolutionized widespread NLP tasks, yet little\ncomprehensively explored few-shot relation extraction with large language\nmodels. In this paper, we investigate principal methodologies, in-context\nlearning and data generation, for few-shot relation extraction via GPT-3.5\nthrough exhaustive experiments. To enhance few-shot performance, we further\npropose task-related instructions and schema-constrained data generation. We\nobserve that in-context learning can achieve performance on par with previous\nprompt learning approaches, and data generation with the large language model\ncan boost previous solutions to obtain new state-of-the-art few-shot results on\nfour widely-studied relation extraction datasets. We hope our work can inspire\nfuture research for the capabilities of large language models in few-shot\nrelation extraction. Code is available in\nhttps://github.com/zjunlp/DeepKE/tree/main/example/llm.", "published": "2023-05-02 15:55:41", "link": "http://arxiv.org/abs/2305.01555v4", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Contrastive Speech Mixup for Low-resource Keyword Spotting", "abstract": "Most of the existing neural-based models for keyword spotting (KWS) in smart\ndevices require thousands of training samples to learn a decent audio\nrepresentation. However, with the rising demand for smart devices to become\nmore personalized, KWS models need to adapt quickly to smaller user samples. To\ntackle this challenge, we propose a contrastive speech mixup (CosMix) learning\nalgorithm for low-resource KWS. CosMix introduces an auxiliary contrastive loss\nto the existing mixup augmentation technique to maximize the relative\nsimilarity between the original pre-mixed samples and the augmented samples.\nThe goal is to inject enhancing constraints to guide the model towards simpler\nbut richer content-based speech representations from two augmented views (i.e.\nnoisy mixed and clean pre-mixed utterances). We conduct our experiments on the\nGoogle Speech Command dataset, where we trim the size of the training set to as\nsmall as 2.5 mins per keyword to simulate a low-resource condition. Our\nexperimental results show a consistent improvement in the performance of\nmultiple models, which exhibits the effectiveness of our method.", "published": "2023-05-02 03:07:44", "link": "http://arxiv.org/abs/2305.01170v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multitask learning in Audio Captioning: a sentence embedding regression\n  loss acts as a regularizer", "abstract": "In this work, we propose to study the performance of a model trained with a\nsentence embedding regression loss component for the Automated Audio Captioning\ntask. This task aims to build systems that can describe audio content with a\nsingle sentence written in natural language. Most systems are trained with the\nstandard Cross-Entropy loss, which does not take into account the semantic\ncloseness of the sentence. We found that adding a sentence embedding loss term\nreduces overfitting, but also increased SPIDEr from 0.397 to 0.418 in our first\nsetting on the AudioCaps corpus. When we increased the weight decay value, we\nfound our model to be much closer to the current state-of-the-art methods, with\na SPIDEr score up to 0.444 compared to a 0.475 score. Moreover, this model uses\neight times less trainable parameters. In this training setting, the sentence\nembedding loss has no more impact on the model performance.", "published": "2023-05-02 15:03:20", "link": "http://arxiv.org/abs/2305.01482v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Learning for Joint Acoustic Echo and Acoustic Howling Suppression\n  in Hybrid Meetings", "abstract": "Hybrid meetings have become increasingly necessary during the post-COVID\nperiod and also brought new challenges for solving audio-related problems. In\nparticular, the interplay between acoustic echo and acoustic howling in a\nhybrid meeting makes the joint suppression of them difficult. This paper\nproposes a deep learning approach to tackle this problem by formulating a\nrecurrent feedback suppression process as an instantaneous speech separation\ntask using the teacher-forced training strategy. Specifically, a self-attentive\nrecurrent neural network is utilized to extract the target speech from\nmicrophone recordings with accessible and learned reference signals, thus\nsuppressing acoustic echo and acoustic howling simultaneously. Different\ncombinations of input signals and loss functions have been investigated for\nperformance improvement. Experimental results demonstrate the effectiveness of\nthe proposed method for suppressing echo and howling jointly in hybrid\nmeetings.", "published": "2023-05-02 17:48:49", "link": "http://arxiv.org/abs/2305.01637v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Long-Term Rhythmic Video Soundtracker", "abstract": "We consider the problem of generating musical soundtracks in sync with\nrhythmic visual cues. Most existing works rely on pre-defined music\nrepresentations, leading to the incompetence of generative flexibility and\ncomplexity. Other methods directly generating video-conditioned waveforms\nsuffer from limited scenarios, short lengths, and unstable generation quality.\nTo this end, we present Long-Term Rhythmic Video Soundtracker (LORIS), a novel\nframework to synthesize long-term conditional waveforms. Specifically, our\nframework consists of a latent conditional diffusion probabilistic model to\nperform waveform synthesis. Furthermore, a series of context-aware conditioning\nencoders are proposed to take temporal information into consideration for a\nlong-term generation. Notably, we extend our model's applicability from dances\nto multiple sports scenarios such as floor exercise and figure skating. To\nperform comprehensive evaluations, we establish a benchmark for rhythmic video\nsoundtracks including the pre-processed dataset, improved evaluation metrics,\nand robust generative baselines. Extensive experiments show that our model\ngenerates long-term soundtracks with state-of-the-art musical quality and\nrhythmic correspondence. Codes are available at\n\\url{https://github.com/OpenGVLab/LORIS}.", "published": "2023-05-02 10:58:29", "link": "http://arxiv.org/abs/2305.01319v2", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Integrating spoken instructions into flight trajectory prediction to\n  optimize automation in air traffic control", "abstract": "The booming air transportation industry inevitably burdens air traffic\ncontrollers' workload, causing unexpected human factor-related incidents.\nCurrent air traffic control systems fail to consider spoken instructions for\ntraffic prediction, bringing significant challenges in detecting human errors\nduring real-time traffic operations. Here, we present an automation paradigm\nintegrating controlling intent into the information processing loop through the\nspoken instruction-aware flight trajectory prediction framework. A 3-stage\nprogressive multi-modal learning paradigm is proposed to address the modality\ngap between the trajectory and spoken instructions, as well as minimize the\ndata requirements. Experiments on a real-world dataset show the proposed\nframework achieves flight trajectory prediction with high predictability and\ntimeliness, obtaining over 20% relative reduction in mean deviation error.\nMoreover, the generalizability of the proposed framework is also confirmed by\nvarious model architectures. The proposed framework can formulate\nfull-automated information processing in real-world air traffic applications,\nsupporting human error detection and enhancing aviation safety.", "published": "2023-05-02 08:28:55", "link": "http://arxiv.org/abs/2305.01661v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AQ-GT: a Temporally Aligned and Quantized GRU-Transformer for Co-Speech\n  Gesture Synthesis", "abstract": "The generation of realistic and contextually relevant co-speech gestures is a\nchallenging yet increasingly important task in the creation of multimodal\nartificial agents. Prior methods focused on learning a direct correspondence\nbetween co-speech gesture representations and produced motions, which created\nseemingly natural but often unconvincing gestures during human assessment. We\npresent an approach to pre-train partial gesture sequences using a generative\nadversarial network with a quantization pipeline. The resulting codebook\nvectors serve as both input and output in our framework, forming the basis for\nthe generation and reconstruction of gestures. By learning the mapping of a\nlatent space representation as opposed to directly mapping it to a vector\nrepresentation, this framework facilitates the generation of highly realistic\nand expressive gestures that closely replicate human movement and behavior,\nwhile simultaneously avoiding artifacts in the generation process. We evaluate\nour approach by comparing it with established methods for generating co-speech\ngestures as well as with existing datasets of human behavior. We also perform\nan ablation study to assess our findings. The results show that our approach\noutperforms the current state of the art by a clear margin and is partially\nindistinguishable from human gesturing. We make our data pipeline and the\ngeneration framework publicly available.", "published": "2023-05-02 07:59:38", "link": "http://arxiv.org/abs/2305.01241v2", "categories": ["cs.HC", "cs.GR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
