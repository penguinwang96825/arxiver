{"title": "Geometric Deep Learning for Realized Covariance Matrix Forecasting", "abstract": "Traditional methods employed in matrix volatility forecasting often overlook\nthe inherent Riemannian manifold structure of symmetric positive definite\nmatrices, treating them as elements of Euclidean space, which can lead to\nsuboptimal predictive performance. Moreover, they often struggle to handle\nhigh-dimensional matrices. In this paper, we propose a novel approach for\nforecasting realized covariance matrices of asset returns using a\nRiemannian-geometry-aware deep learning framework. In this way, we account for\nthe geometric properties of the covariance matrices, including possible\nnon-linear dynamics and efficient handling of high-dimensionality. Moreover,\nbuilding upon a Fr\\'echet sample mean of realized covariance matrices, we are\nable to extend the HAR model to the matrix-variate. We demonstrate the efficacy\nof our approach using daily realized covariance matrices for the 50 most\ncapitalized companies in the S&P 500 index, showing that our method outperforms\ntraditional approaches in terms of predictive accuracy.", "published": "2024-12-12 18:01:48", "link": "http://arxiv.org/abs/2412.09517v1", "categories": ["q-fin.CP", "econ.EM", "q-fin.PM"], "primary_category": "q-fin.CP"}
{"title": "Isogeometric Analysis for the Pricing of Financial Derivatives with Nonlinear Models: Convertible Bonds and Options", "abstract": "Computational efficiency is essential for enhancing the accuracy and\npracticality of pricing complex financial derivatives. In this paper, we\ndiscuss Isogeometric Analysis (IGA) for valuing financial derivatives, modeled\nby two nonlinear Black-Scholes PDEs: the Leland model for European call with\ntransaction costs and the AFV model for convertible bonds with default options.\nWe compare the solutions of IGA with finite difference methods (FDM) and finite\nelement methods (FEM). In particular, very accurate solutions can be\nnumerically calculated on far less mesh (knots) than FDM or FEM, by using\nnon-uniform knots and weighted cubic NURBS, which in turn reduces the\ncomputational time significantly.", "published": "2024-12-12 06:39:05", "link": "http://arxiv.org/abs/2412.08987v1", "categories": ["q-fin.CP", "cs.NA", "math.NA", "q-fin.PR"], "primary_category": "q-fin.CP"}
{"title": "Replica del valor de un pool (CPM) y hedging de perdidas impermanentes", "abstract": "This article analytically characterizes the impermanent loss for automatic\nmarket makers in decentralized exchanges such as Uniswap or Balancer (CPMM). We\npresent a theoretical static replication formula for the pool value using a\ncombination of European calls and puts. We will formulate a result to guarantee\ncoverage for any final price that falls within a predefined range.", "published": "2024-12-12 12:11:22", "link": "http://arxiv.org/abs/2412.09662v1", "categories": ["q-fin.MF", "q-fin.RM", "91G15"], "primary_category": "q-fin.MF"}
{"title": "Robust mean-variance stochastic differential reinsurance and investment games under volatility risk and model uncertainty", "abstract": "This paper investigates robust stochastic differential games among insurers\nunder model uncertainty and stochastic volatility. The surplus processes of\nambiguity-averse insurers (AAIs) are characterized by drifted Brownian motion\nwith both common and idiosyncratic insurance risks. To mitigate these risks,\nAAIs can purchase proportional reinsurance. Besides, AAIs allocate their wealth\nin a financial market consisting of cash, and a stock characterized by the 4/2\nstochastic volatility model. AAIs compete with each other based on relative\nperformance with the mean-variance criterion under the worst-case scenario.\nThis paper formulates a robust time-consistent mean-field game in a non-linear\nsystem. The AAIs seek robust, time-consistent response strategies to achieve\nNash equilibrium strategies in the game. We introduce $n$-dimensional extended\nHamilton-Jacobi-Bellman-Isaacs (HJBI) equations and corresponding verification\ntheorems under compatible conditions. Semi-closed forms of the robust\n$n$-insurer equilibrium and mean-field equilibrium are derived, relying on\ncoupled Riccati equations. Suitable conditions are presented to ensure the\nexistence and uniqueness of the coupled Riccati equation as well as the\nintegrability in the verification theorem. As the number of AAIs increases, the\nresults in the $n$-insurer game converge to those in the mean-field game.\nNumerical examples are provided to illustrate economic behaviors in the games,\nhighlighting the herd effect of competition on the AAIs.", "published": "2024-12-12 11:02:24", "link": "http://arxiv.org/abs/2412.09171v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Many-insurer robust games of reinsurance and investment under model uncertainty in incomplete markets", "abstract": "This paper studies the robust reinsurance and investment games for\ncompetitive insurers. Model uncertainty is characterized by a class of\nequivalent probability measures. Each insurer is concerned with relative\nperformance under the worst-case scenario. Insurers' surplus processes are\napproximated by drifted Brownian motion with common and idiosyncratic insurance\nrisks. The insurers can purchase proportional reinsurance to divide the\ninsurance risk with the reinsurance premium calculated by the variance\nprinciple. We consider an incomplete market driven by the 4/2 stochastic\nvolatility mode. This paper formulates the robust mean-field game for a\nnon-linear system originating from the variance principle and the 4/2 model.\nFor the case of an exponential utility function, we derive closed-form\nsolutions for the $n$-insurer game and the corresponding mean-field game. We\nshow that relative concerns lead to new hedging terms in the investment and\nreinsurance strategies. Model uncertainty can significantly change the\ninsurers' hedging demands. The hedging demands in the investment-reinsurance\nstrategies exhibit highly non-linear dependence with the insurers' competitive\ncoefficients, risk aversion and ambiguity aversion coefficients. Finally,\nnumerical results demonstrate the herd effect of competition.", "published": "2024-12-12 10:44:03", "link": "http://arxiv.org/abs/2412.09157v1", "categories": ["q-fin.MF", "math.OC"], "primary_category": "q-fin.MF"}
{"title": "A Graph-Based Synthetic Data Pipeline for Scaling High-Quality Reasoning\n  Instructions", "abstract": "Synthesizing high-quality reasoning data for continual training has been\nproven to be effective in enhancing the performance of Large Language Models\n(LLMs). However, previous synthetic approaches struggle to easily scale up data\nand incur high costs in the pursuit of high quality. In this paper, we propose\nthe Graph-based Synthetic Data Pipeline (GSDP), an economical and scalable\nframework for high-quality reasoning data synthesis. Inspired by knowledge\ngraphs, we extracted knowledge points from seed data and constructed a\nknowledge point relationships graph to explore their interconnections. By\nexploring the implicit relationships among knowledge, our method achieves\n$\\times$255 data expansion. Furthermore, GSDP led by open-source models,\nachieves synthesis quality comparable to GPT-4-0613 while maintaining\n$\\times$100 lower costs. To tackle the most challenging mathematical reasoning\ntask, we present the GSDP-MATH dataset comprising over 1.91 million pairs of\nmath problems and answers. After fine-tuning on GSDP-MATH, GSDP-7B based on\nMistral-7B achieves 37.7% accuracy on MATH and 78.4% on GSM8K, demonstrating\nthe effectiveness of our method. The dataset and models will be released in\nhttps://github.com/Jayce1kk/GSDP.", "published": "2024-12-12 01:52:25", "link": "http://arxiv.org/abs/2412.08864v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Align, Generate, Learn: A Novel Closed-Loop Framework for Cross-Lingual\n  In-Context Learning", "abstract": "Cross-lingual in-context learning (XICL) has emerged as a transformative\nparadigm for leveraging large language models (LLMs) to tackle multilingual\ntasks, especially for low-resource languages. However, existing approaches\noften rely on external retrievers or task-specific fine-tuning, limiting their\nscalability and generalizability. In this paper, we propose a novel\nself-supervised framework that harnesses the generative capabilities of LLMs to\ninternally select and utilize task-relevant examples. Our method introduces two\nkey objectives: a retrieval-generation alignment loss to optimize the quality\nof selected examples and a semantic coherence loss to ensure cross-lingual\nconsistency. Through extensive experiments on multilingual benchmarks, our\napproach achieves state-of-the-art performance, significantly outperforming\nexisting baselines. Further analysis highlights its robustness across diverse\nlanguage families and its ability to generalize to unseen tasks. Human\nevaluations confirm the superior fluency, relevance, and semantic correctness\nof outputs generated by our method. This work provides a scalable, effective,\nand generalizable solution for cross-lingual in-context learning.", "published": "2024-12-12 05:36:51", "link": "http://arxiv.org/abs/2412.08955v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reasoning-Aware Query-Focused Summarization over Multi-Table Data", "abstract": "Query-focused summarization over multi-table data is a challenging yet\ncritical task for extracting precise and relevant information from structured\ndata. Existing methods often rely on complex preprocessing steps and struggle\nto generalize across domains or handle the logical reasoning required for\nmulti-table queries. In this paper, we propose QueryTableSummarizer++, an\nend-to-end generative framework leveraging large language models (LLMs)\nenhanced with table-aware pre-training, query-aligned fine-tuning, and\nreinforcement learning with feedback. Our method eliminates the need for\nintermediate serialization steps and directly generates query-relevant\nsummaries. Experiments on a benchmark dataset demonstrate that\nQueryTableSummarizer++ significantly outperforms state-of-the-art baselines in\nterms of BLEU, ROUGE, and F1-score. Additional analyses highlight its\nscalability, generalization across domains, and robust handling of complex\nqueries. Human evaluation further validates the superior quality and practical\napplicability of the generated summaries, establishing QueryTableSummarizer++\nas a highly effective solution for multi-table summarization tasks.", "published": "2024-12-12 06:04:31", "link": "http://arxiv.org/abs/2412.08970v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the Robustness of Retrieval-Augmented Generation Systems in\n  K-12 Educational Question Answering with Knowledge Discrepancies", "abstract": "Retrieval-Augmented Generation (RAG) systems have demonstrated remarkable\npotential as question answering systems in the K-12 Education domain, where\nknowledge is typically queried within the restricted scope of authoritative\ntextbooks. However, the discrepancy between textbooks and the parametric\nknowledge in Large Language Models (LLMs) could undermine the effectiveness of\nRAG systems. To systematically investigate the robustness of RAG systems under\nsuch knowledge discrepancies, we present EduKDQA, a question answering dataset\nthat simulates knowledge discrepancies in real applications by applying\nhypothetical knowledge updates in answers and source documents. EduKDQA\nincludes 3,005 questions covering five subjects, under a comprehensive question\ntypology from the perspective of context utilization and knowledge integration.\nWe conducted extensive experiments on retrieval and question answering\nperformance. We find that most RAG systems suffer from a substantial\nperformance drop in question answering with knowledge discrepancies, while\nquestions that require integration of contextual knowledge and parametric\nknowledge pose a challenge to LLMs.", "published": "2024-12-12 06:38:40", "link": "http://arxiv.org/abs/2412.08985v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improvement in Sign Language Translation Using Text CTC Alignment", "abstract": "Current sign language translation (SLT) approaches often rely on gloss-based\nsupervision with Connectionist Temporal Classification (CTC), limiting their\nability to handle non-monotonic alignments between sign language video and\nspoken text. In this work, we propose a novel method combining joint\nCTC/Attention and transfer learning. The joint CTC/Attention introduces\nhierarchical encoding and integrates CTC with the attention mechanism during\ndecoding, effectively managing both monotonic and non-monotonic alignments.\nMeanwhile, transfer learning helps bridge the modality gap between vision and\nlanguage in SLT. Experimental results on two widely adopted benchmarks,\nRWTH-PHOENIX-Weather 2014 T and CSL-Daily, show that our method achieves\nresults comparable to state-of-the-art and outperforms the pure-attention\nbaseline. Additionally, this work opens a new door for future research into\ngloss-free SLT using text-based CTC alignment.", "published": "2024-12-12 07:24:16", "link": "http://arxiv.org/abs/2412.09014v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty", "abstract": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance.", "published": "2024-12-12 07:52:56", "link": "http://arxiv.org/abs/2412.09036v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mining Word Boundaries from Speech-Text Parallel Data for Cross-domain\n  Chinese Word Segmentation", "abstract": "Inspired by early research on exploring naturally annotated data for Chinese\nWord Segmentation (CWS), and also by recent research on integration of speech\nand text processing, this work for the first time proposes to explicitly mine\nword boundaries from speech-text parallel data. We employ the Montreal Forced\nAligner (MFA) toolkit to perform character-level alignment on speech-text data,\ngiving pauses as candidate word boundaries. Based on detailed analysis of\ncollected pauses, we propose an effective probability-based strategy for\nfiltering unreliable word boundaries. To more effectively utilize word\nboundaries as extra training data, we also propose a robust complete-then-train\n(CTT) strategy. We conduct cross-domain CWS experiments on two target domains,\ni.e., ZX and AISHELL2. We have annotated about 1,000 sentences as the\nevaluation data of AISHELL2. Experiments demonstrate the effectiveness of our\nproposed approach.", "published": "2024-12-12 08:13:32", "link": "http://arxiv.org/abs/2412.09045v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Pixel Language Models on Non-Standardized Languages", "abstract": "We explore the potential of pixel-based models for transfer learning from\nstandard languages to dialects. These models convert text into images that are\ndivided into patches, enabling a continuous vocabulary representation that\nproves especially useful for out-of-vocabulary words common in dialectal data.\nUsing German as a case study, we compare the performance of pixel-based models\nto token-based models across various syntactic and semantic tasks. Our results\nshow that pixel-based models outperform token-based models in part-of-speech\ntagging, dependency parsing and intent detection for zero-shot dialect\nevaluation by up to 26 percentage points in some scenarios, though not in\nStandard German. However, pixel-based models fall short in topic\nclassification. These findings emphasize the potential of pixel-based models\nfor handling dialectal data, though further research should be conducted to\nassess their effectiveness in various linguistic contexts.", "published": "2024-12-12 09:11:45", "link": "http://arxiv.org/abs/2412.09084v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "ReFF: Reinforcing Format Faithfulness in Language Models across Varied\n  Tasks", "abstract": "Following formatting instructions to generate well-structured content is a\nfundamental yet often unmet capability for large language models (LLMs). To\nstudy this capability, which we refer to as format faithfulness, we present\nFormatBench, a comprehensive format-related benchmark. Compared to previous\nformat-related benchmarks, FormatBench involves a greater variety of tasks in\nterms of application scenes (traditional NLP tasks, creative works, autonomous\nagency tasks), human-LLM interaction styles (single-turn instruction,\nmulti-turn chat), and format types (inclusion, wrapping, length, coding).\nMoreover, each task in FormatBench is attached with a format checker program.\nExtensive experiments on the benchmark reveal that state-of-the-art open- and\nclosed-source LLMs still suffer from severe deficiency in format faithfulness.\nBy virtue of the decidable nature of formats, we propose to Reinforce Format\nFaithfulness (ReFF) to help LLMs generate formatted output as instructed\nwithout compromising general quality. Without any annotated data, ReFF can\nsubstantially improve the format faithfulness rate (e.g., from 21.6% in\noriginal LLaMA3 to 95.0% on caption segmentation task), while keep the general\nquality comparable (e.g., from 47.3 to 46.4 in F1 scores). Combined with\nlabeled training data, ReFF can simultaneously improve both format faithfulness\n(e.g., from 21.6% in original LLaMA3 to 75.5%) and general quality (e.g., from\n47.3 to 61.6 in F1 scores). We further offer an interpretability analysis to\nexplain how ReFF improves both format faithfulness and general quality.", "published": "2024-12-12 11:03:25", "link": "http://arxiv.org/abs/2412.09173v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CleanComedy: Creating Friendly Humor through Generative Techniques", "abstract": "Humor generation is a challenging task in natural language processing due to\nlimited resources and the quality of existing datasets. Available humor\nlanguage resources often suffer from toxicity and duplication, limiting their\neffectiveness for training robust models. This paper proposes CleanComedy, a\nspecialized, partially annotated toxicity-filtered corpus of English and\nRussian jokes collected from various sources. We study the effectiveness of our\ndata filtering approach through a survey on humor and toxicity levels in\nvarious joke groups. In addition, we study advances in computer humor\ngeneration by comparing jokes written by humans with various groups of\ngenerative jokes, including our baseline models trained on the CleanComedy\ndatasets.", "published": "2024-12-12 11:57:59", "link": "http://arxiv.org/abs/2412.09203v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Make Satire Boring Again: Reducing Stylistic Bias of Satirical Corpus by\n  Utilizing Generative LLMs", "abstract": "Satire detection is essential for accurately extracting opinions from textual\ndata and combating misinformation online. However, the lack of diverse corpora\nfor satire leads to the problem of stylistic bias which impacts the models'\ndetection performances. This study proposes a debiasing approach for satire\ndetection, focusing on reducing biases in training data by utilizing generative\nlarge language models. The approach is evaluated in both cross-domain (irony\ndetection) and cross-lingual (English) settings. Results show that the\ndebiasing method enhances the robustness and generalizability of the models for\nsatire and irony detection tasks in Turkish and English. However, its impact on\ncausal language models, such as Llama-3.1, is limited. Additionally, this work\ncurates and presents the Turkish Satirical News Dataset with detailed human\nannotations, with case studies on classification, debiasing, and\nexplainability.", "published": "2024-12-12 12:57:55", "link": "http://arxiv.org/abs/2412.09247v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Solve Domain-Specific Calculation Problems with\n  Knowledge-Intensive Programs Generator", "abstract": "Domain Large Language Models (LLMs) are developed for domain-specific tasks\nbased on general LLMs. But it still requires professional knowledge to\nfacilitate the expertise for some domain-specific tasks. In this paper, we\ninvestigate into knowledge-intensive calculation problems. We find that the\nmath problems to be challenging for LLMs, when involving complex\ndomain-specific rules and knowledge documents, rather than simple formulations\nof terminologies. Therefore, we propose a pipeline to solve the domain-specific\ncalculation problems with Knowledge-Intensive Programs Generator more\neffectively, named as KIPG. It generates knowledge-intensive programs according\nto the domain-specific documents. For each query, key variables are extracted,\nthen outcomes which are dependent on domain knowledge are calculated with the\nprograms. By iterative preference alignment, the code generator learns to\nimprove the logic consistency with the domain knowledge. Taking legal domain as\nan example, we have conducted experiments to prove the effectiveness of our\npipeline, and extensive analysis on the modules. We also find that the code\ngenerator is also adaptable to other domains, without training on the new\nknowledge.", "published": "2024-12-12 13:42:58", "link": "http://arxiv.org/abs/2412.09280v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training LayoutLM from Scratch for Efficient Named-Entity Recognition in\n  the Insurance Domain", "abstract": "Generic pre-trained neural networks may struggle to produce good results in\nspecialized domains like finance and insurance. This is due to a domain\nmismatch between training data and downstream tasks, as in-domain data are\noften scarce due to privacy constraints. In this work, we compare different\npre-training strategies for LayoutLM. We show that using domain-relevant\ndocuments improves results on a named-entity recognition (NER) problem using a\nnovel dataset of anonymized insurance-related financial documents called\nPayslips. Moreover, we show that we can achieve competitive results using a\nsmaller and faster model.", "published": "2024-12-12 15:09:44", "link": "http://arxiv.org/abs/2412.09341v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Falcon-UI: Understanding GUI Before Following User Instructions", "abstract": "Pursuing human-like interaction for Graphical User Interface (GUI) agents\nrequires understanding the GUI context and following user instructions.\nHowever, existing works typically couple these two aspects and focus more on\ninstruct-following abilities, while ignoring the importance of understanding\nthe GUI context. In this paper, we introduce an instruction-free GUI navigation\ndataset, termed Insight-UI Dataset, to enhance model comprehension of GUI\nenvironments. Insight-UI Dataset is automatically generated from the Common\nCrawl corpus, simulating various platforms -- including iOS, Android, Windows,\nand Linux -- across multiple resolutions on 312K domains. Although GUI\ninteractions vary by context, diverse interfaces share common internal\npatterns, such as clicking an item to view its details. It implies the\nfeasibility of independent GUI operation learning, followed by joint\noptimization with instruction tuning. Thereby, we develop the GUI agent model\nFalcon-UI, which is initially pretrained on Insight-UI Dataset and subsequently\nfine-tuned on Android and Web GUI datasets, including AITW, AITZ, Android\nControl, and Mind2Web. With 7 billion parameters, Falcon-UI achieves accuracy\ncomparable to the 72 billion-parameter Qwen2VL on AITZ, validating the\nalignment between GUI context comprehension and agent performance. Our code and\ndataset will be open-sourced.", "published": "2024-12-12 15:29:36", "link": "http://arxiv.org/abs/2412.09362v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Text Normalization for Luxembourgish using Real-Life Variation\n  Data", "abstract": "Orthographic variation is very common in Luxembourgish texts due to the\nabsence of a fully-fledged standard variety. Additionally, developing NLP tools\nfor Luxembourgish is a difficult task given the lack of annotated and parallel\ndata, which is exacerbated by ongoing standardization. In this paper, we\npropose the first sequence-to-sequence normalization models using the ByT5 and\nmT5 architectures with training data obtained from word-level real-life\nvariation data. We perform a fine-grained, linguistically-motivated evaluation\nto test byte-based, word-based and pipeline-based models for their strengths\nand weaknesses in text normalization. We show that our sequence model using\nreal-life variation data is an effective approach for tailor-made normalization\nin Luxembourgish.", "published": "2024-12-12 15:50:55", "link": "http://arxiv.org/abs/2412.09383v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Generation Models for Luxembourgish with Limited Data: A Balanced\n  Multilingual Strategy", "abstract": "This paper addresses the challenges in developing language models for\nless-represented languages, with a focus on Luxembourgish. Despite its active\ndevelopment, Luxembourgish faces a digital data scarcity, exacerbated by\nLuxembourg's multilingual context. We propose a novel text generation model\nbased on the T5 architecture, combining limited Luxembourgish data with equal\namounts, in terms of size and type, of German and French data. We hypothesise\nthat a model trained on Luxembourgish, German, and French will improve the\nmodel's cross-lingual transfer learning capabilities and outperform monolingual\nand large multilingual models. To verify this, the study at hand explores\nwhether multilingual or monolingual training is more beneficial for\nLuxembourgish language generation. For the evaluation, we introduce LuxGen, a\ntext generation benchmark that is the first of its kind for Luxembourgish.", "published": "2024-12-12 16:23:12", "link": "http://arxiv.org/abs/2412.09415v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors", "abstract": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusions in the mathematical\ndomain. We release MRBench - a new evaluation benchmark containing 192\nconversations and 1,596 responses from seven state-of-the-art LLM-based and\nhuman tutors, providing gold annotations for eight pedagogical dimensions. We\nassess reliability of the popular Prometheus2 and Llama-3.1-8B LLMs as\nevaluators and analyze each tutor's pedagogical abilities, highlighting which\nLLMs are good tutors and which ones are more suitable as question-answering\nsystems. We believe that the presented taxonomy, benchmark, and human-annotated\nlabels will streamline the evaluation process and help track the progress in AI\ntutors' development.", "published": "2024-12-12 16:24:35", "link": "http://arxiv.org/abs/2412.09416v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Impact of Copyrighted Material on Large Language Models: A Norwegian\n  Perspective", "abstract": "The use of copyrighted materials in training language models raises critical\nlegal and ethical questions. This paper presents a framework for and the\nresults of empirically assessing the impact of publisher-controlled copyrighted\ncorpora on the performance of generative large language models (LLMs) for\nNorwegian. When evaluated on a diverse set of tasks, we found that adding both\nbooks and newspapers to the data mixture of LLMs tend to improve their\nperformance, while the addition of fiction works seems to be detrimental. Our\nexperiments could inform the creation of a compensation scheme for authors\nwhose works contribute to AI development.", "published": "2024-12-12 17:11:22", "link": "http://arxiv.org/abs/2412.09460v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DiverseAgentEntropy: Quantifying Black-Box LLM Uncertainty through\n  Diverse Perspectives and Multi-Agent Interaction", "abstract": "Quantifying the uncertainty in the factual parametric knowledge of Large\nLanguage Models (LLMs), especially in a black-box setting, poses a significant\nchallenge. Existing methods, which gauge a model's uncertainty through\nevaluating self-consistency in responses to the original query, do not always\ncapture true uncertainty. Models might respond consistently to the origin query\nwith a wrong answer, yet respond correctly to varied questions from different\nperspectives about the same query, and vice versa. In this paper, we propose a\nnovel method, DiverseAgentEntropy, for evaluating a model's uncertainty using\nmulti-agent interaction under the assumption that if a model is certain, it\nshould consistently recall the answer to the original query across a diverse\ncollection of questions about the same original query. We further implement an\nabstention policy to withhold responses when uncertainty is high. Our method\noffers a more accurate prediction of the model's reliability and further\ndetects hallucinations, outperforming other self-consistency-based methods.\nAdditionally, it demonstrates that existing models often fail to consistently\nretrieve the correct answer to the same query under diverse varied questions\neven when knowing the correct answer.", "published": "2024-12-12 18:52:40", "link": "http://arxiv.org/abs/2412.09572v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OpenNER 1.0: Standardized Open-Access Named Entity Recognition Datasets\n  in 50+ Languages", "abstract": "We present OpenNER 1.0, a standardized collection of openly available named\nentity recognition (NER) datasets. OpenNER contains 34 datasets spanning 51\nlanguages, annotated in varying named entity ontologies. We correct annotation\nformat issues, standardize the original datasets into a uniform representation,\nmap entity type names to be more consistent across corpora, and provide the\ncollection in a structure that enables research in multilingual and\nmulti-ontology NER. We provide baseline models using three pretrained\nmultilingual language models to compare the performance of recent models and\nfacilitate future research in NER.", "published": "2024-12-12 18:55:53", "link": "http://arxiv.org/abs/2412.09587v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web\n  Tutorials", "abstract": "Graphical User Interface (GUI) agents can automate complex tasks across\ndigital environments, but their development is hindered by the scarcity of\nhigh-quality trajectory data for training. Existing approaches rely on\nexpensive human annotation, making them unsustainable at scale. We propose\nAgentTrek, a scalable data synthesis pipeline that generates web agent\ntrajectories by leveraging publicly available tutorials. Our three-stage\nmethod: (1) automatically harvests and filters tutorial-like texts from the\ninternet using a specialized classification model, (2) transforms these texts\ninto structured task specifications with step-by-step instructions, and (3)\nemploys a visual-language model (VLM) agent to execute these instructions in\nreal environments, while a VLM-based evaluator verifies trajectory correctness.\nThe synthesized trajectories encompass multiple modalities, including\ntext-based HTML observations with function-calling API actions, and\nvision-based screenshot observations with pixel-level actions. This multimodal\ndata, enriched with chain-of-thought reasoning, enables agents to achieve\nstate-of-the-art performance on both textual web browsing benchmarks (e.g.,\nWebArena) and visual web grounding and browsing benchmarks (e.g., ScreenSpot\nWeb and Multimodal Mind2Web). Furthermore, our fully automated approach\nsignificantly reduces data collection costs, achieving a cost of just $0.55 per\nhigh-quality trajectory without human annotators. Our work demonstrates that\nguided replay using web tutorials is a practical and scalable strategy for\ntraining advanced GUI agents, paving the way for more capable and autonomous\ndigital assistants.", "published": "2024-12-12 18:59:27", "link": "http://arxiv.org/abs/2412.09605v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GReaTer: Gradients over Reasoning Makes Smaller Language Models Strong\n  Prompt Optimizers", "abstract": "The effectiveness of large language models (LLMs) is closely tied to the\ndesign of prompts, making prompt optimization essential for enhancing their\nperformance across a wide range of tasks. Many existing approaches to\nautomating prompt engineering rely exclusively on textual feedback, refining\nprompts based solely on inference errors identified by large, computationally\nexpensive LLMs. Unfortunately, smaller models struggle to generate high-quality\nfeedback, resulting in complete dependence on large LLM judgment. Moreover,\nthese methods fail to leverage more direct and finer-grained information, such\nas gradients, due to operating purely in text space. To this end, we introduce\nGReaTer, a novel prompt optimization technique that directly incorporates\ngradient information over task-specific reasoning. By utilizing task loss\ngradients, GReaTer enables self-optimization of prompts for open-source,\nlightweight language models without the need for costly closed-source LLMs.\nThis allows high-performance prompt optimization without dependence on massive\nLLMs, closing the gap between smaller models and the sophisticated reasoning\noften needed for prompt refinement. Extensive evaluations across diverse\nreasoning tasks including BBH, GSM8k, and FOLIO demonstrate that GReaTer\nconsistently outperforms previous state-of-the-art prompt optimization methods,\neven those reliant on powerful LLMs. Additionally, GReaTer-optimized prompts\nfrequently exhibit better transferability and, in some cases, boost task\nperformance to levels comparable to or surpassing those achieved by larger\nlanguage models, highlighting the effectiveness of prompt optimization guided\nby gradients over reasoning. Code of GReaTer is available at\nhttps://github.com/psunlpgroup/GreaTer.", "published": "2024-12-12 20:59:43", "link": "http://arxiv.org/abs/2412.09722v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Large Language Models on Cross-Cultural Values in Connection\n  with Training Methodology", "abstract": "Large language models (LLMs) closely interact with humans, and thus need an\nintimate understanding of the cultural values of human society. In this paper,\nwe explore how open-source LLMs make judgments on diverse categories of\ncultural values across countries, and its relation to training methodology such\nas model sizes, training corpus, alignment, etc. Our analysis shows that LLMs\ncan judge socio-cultural norms similar to humans but less so on social systems\nand progress. In addition, LLMs tend to judge cultural values biased toward\nWestern culture, which can be improved with training on the multilingual\ncorpus. We also find that increasing model size helps a better understanding of\nsocial values, but smaller models can be enhanced by using synthetic data. Our\nanalysis reveals valuable insights into the design methodology of LLMs in\nconnection with their understanding of cultural values.", "published": "2024-12-12 00:52:11", "link": "http://arxiv.org/abs/2412.08846v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AI-assisted Knowledge Discovery in Biomedical Literature to Support\n  Decision-making in Precision Oncology", "abstract": "The delivery of appropriate targeted therapies to cancer patients requires\nthe complete analysis of the molecular profiling of tumors and the patient's\nclinical characteristics in the context of existing knowledge and recent\nfindings described in biomedical literature and several other sources. We\nevaluated the potential contributions of specific natural language processing\nsolutions to support knowledge discovery from biomedical literature. Two models\nfrom the Bidirectional Encoder Representations from Transformers (BERT) family,\ntwo Large Language Models, and PubTator 3.0 were tested for their ability to\nsupport the named entity recognition (NER) and the relation extraction (RE)\ntasks. PubTator 3.0 and the BioBERT model performed best in the NER task (best\nF1-score equal to 0.93 and 0.89, respectively), while BioBERT outperformed all\nother solutions in the RE task (best F1-score 0.79) and a specific use case it\nwas applied to by recognizing nearly all entity mentions and most of the\nrelations.", "published": "2024-12-12 03:24:49", "link": "http://arxiv.org/abs/2412.08900v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Phi-4 Technical Report", "abstract": "We present phi-4, a 14-billion parameter language model developed with a\ntraining recipe that is centrally focused on data quality. Unlike most language\nmodels, where pre-training is based primarily on organic data sources such as\nweb content or code, phi-4 strategically incorporates synthetic data throughout\nthe training process. While previous models in the Phi family largely distill\nthe capabilities of a teacher model (specifically GPT-4), phi-4 substantially\nsurpasses its teacher model on STEM-focused QA capabilities, giving evidence\nthat our data-generation and post-training techniques go beyond distillation.\nDespite minimal changes to the phi-3 architecture, phi-4 achieves strong\nperformance relative to its size -- especially on reasoning-focused benchmarks\n-- due to improved data, training curriculum, and innovations in the\npost-training scheme.", "published": "2024-12-12 03:37:41", "link": "http://arxiv.org/abs/2412.08905v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Text to Trajectory: Exploring Complex Constraint Representation and\n  Decomposition in Safe Reinforcement Learning", "abstract": "Safe reinforcement learning (RL) requires the agent to finish a given task\nwhile obeying specific constraints. Giving constraints in natural language form\nhas great potential for practical scenarios due to its flexible transfer\ncapability and accessibility. Previous safe RL methods with natural language\nconstraints typically need to design cost functions manually for each\nconstraint, which requires domain expertise and lacks flexibility. In this\npaper, we harness the dual role of text in this task, using it not only to\nprovide constraint but also as a training signal. We introduce the\nTrajectory-level Textual Constraints Translator (TTCT) to replace the manually\ndesigned cost function. Our empirical results demonstrate that TTCT effectively\ncomprehends textual constraint and trajectory, and the policies trained by TTCT\ncan achieve a lower violation rate than the standard cost function. Extra\nstudies are conducted to demonstrate that the TTCT has zero-shot transfer\ncapability to adapt to constraint-shift environments.", "published": "2024-12-12 04:06:54", "link": "http://arxiv.org/abs/2412.08920v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Scale Heterogeneous Text-Attributed Graph Datasets From Diverse\n  Domains", "abstract": "Heterogeneous Text-Attributed Graphs (HTAGs), where different types of\nentities are not only associated with texts but also connected by diverse\nrelationships, have gained widespread popularity and application across various\ndomains. However, current research on text-attributed graph learning\npredominantly focuses on homogeneous graphs, which feature a single node and\nedge type, thus leaving a gap in understanding how methods perform on HTAGs.\nOne crucial reason is the lack of comprehensive HTAG datasets that offer\noriginal textual content and span multiple domains of varying sizes. To this\nend, we introduce a collection of challenging and diverse benchmark datasets\nfor realistic and reproducible evaluation of machine learning models on HTAGs.\nOur HTAG datasets are multi-scale, span years in duration, and cover a wide\nrange of domains, including movie, community question answering, academic,\nliterature, and patent networks. We further conduct benchmark experiments on\nthese datasets with various graph neural networks. All source data, dataset\nconstruction codes, processed HTAGs, data loaders, benchmark codes, and\nevaluation setup are publicly available at GitHub and Hugging Face.", "published": "2024-12-12 04:58:32", "link": "http://arxiv.org/abs/2412.08937v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Mojito: Motion Trajectory and Intensity Control for Video Generation", "abstract": "Recent advancements in diffusion models have shown great promise in producing\nhigh-quality video content. However, efficiently training video diffusion\nmodels capable of integrating directional guidance and controllable motion\nintensity remains a challenging and under-explored area. To tackle these\nchallenges, this paper introduces Mojito, a diffusion model that incorporates\nboth motion trajectory and intensity control for text-to-video generation.\nSpecifically, Mojito features a Directional Motion Control (DMC) module that\nleverages cross-attention to efficiently direct the generated object's motion\nwithout training, alongside a Motion Intensity Modulator (MIM) that uses\noptical flow maps generated from videos to guide varying levels of motion\nintensity. Extensive experiments demonstrate Mojito's effectiveness in\nachieving precise trajectory and intensity control with high computational\nefficiency, generating motion patterns that closely match specified directions\nand intensities, providing realistic dynamics that align well with natural\nmotion in real-world scenarios.", "published": "2024-12-12 05:26:43", "link": "http://arxiv.org/abs/2412.08948v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World\n  Scenarios", "abstract": "This paper introduces RuleArena, a novel and challenging benchmark designed\nto evaluate the ability of large language models (LLMs) to follow complex,\nreal-world rules in reasoning. Covering three practical domains -- airline\nbaggage fees, NBA transactions, and tax regulations -- RuleArena assesses LLMs'\nproficiency in handling intricate natural language instructions that demand\nlong-context understanding, logical reasoning, and accurate mathematical\ncomputation. Two key attributes distinguish RuleArena from traditional\nrule-based reasoning benchmarks: (1) it extends beyond standard first-order\nlogic representations, and (2) it is grounded in authentic, practical\nscenarios, providing insights into the suitability and reliability of LLMs for\nreal-world applications. Our findings reveal several notable limitations in\nLLMs: (1) they struggle to identify and apply the appropriate rules, frequently\nbecoming confused by similar but distinct regulations, (2) they cannot\nconsistently perform accurate mathematical computations, even when they\ncorrectly identify the relevant rules, and (3) in general, they perform poorly\nin the benchmark. These results highlight significant challenges in advancing\nLLMs' rule-guided reasoning capabilities in real-life applications.", "published": "2024-12-12 06:08:46", "link": "http://arxiv.org/abs/2412.08972v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What Makes Cryptic Crosswords Challenging for LLMs?", "abstract": "Cryptic crosswords are puzzles that rely on general knowledge and the\nsolver's ability to manipulate language on different levels, dealing with\nvarious types of wordplay. Previous research suggests that solving such puzzles\nis challenging even for modern NLP models, including Large Language Models\n(LLMs). However, there is little to no research on the reasons for their poor\nperformance on this task. In this paper, we establish the benchmark results for\nthree popular LLMs: Gemma2, LLaMA3 and ChatGPT, showing that their performance\non this task is still significantly below that of humans. We also investigate\nwhy these models struggle to achieve superior performance. We release our code\nand introduced datasets at\nhttps://github.com/bodasadallah/decrypting-crosswords.", "published": "2024-12-12 07:23:52", "link": "http://arxiv.org/abs/2412.09012v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Shiksha: A Technical Domain focused Translation Dataset and Model for\n  Indian Languages", "abstract": "Neural Machine Translation (NMT) models are typically trained on datasets\nwith limited exposure to Scientific, Technical and Educational domains.\nTranslation models thus, in general, struggle with tasks that involve\nscientific understanding or technical jargon. Their performance is found to be\neven worse for low-resource Indian languages. Finding a translation dataset\nthat tends to these domains in particular, poses a difficult challenge. In this\npaper, we address this by creating a multilingual parallel corpus containing\nmore than 2.8 million rows of English-to-Indic and Indic-to-Indic high-quality\ntranslation pairs across 8 Indian languages. We achieve this by bitext mining\nhuman-translated transcriptions of NPTEL video lectures. We also finetune and\nevaluate NMT models using this corpus and surpass all other publicly available\nmodels at in-domain tasks. We also demonstrate the potential for generalizing\nto out-of-domain translation tasks by improving the baseline by over 2 BLEU on\naverage for these Indian languages on the Flores+ benchmark. We are pleased to\nrelease our model and dataset via this link: https://huggingface.co/SPRINGLab.", "published": "2024-12-12 07:40:55", "link": "http://arxiv.org/abs/2412.09025v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dialogue Language Model with Large-Scale Persona Data Engineering", "abstract": "Maintaining persona consistency is paramount in the application of\nopen-domain dialogue systems, as exemplified by models like ChatGPT. Despite\nsignificant advancements, the limited scale and diversity of current persona\ndialogue datasets remain challenges to achieving robust persona-consistent\ndialogue models. In this study, drawing inspiration from the success of\nlarge-scale pre-training, we introduce PPDS, an open-domain persona dialogue\nsystem that employs extensive generative pre-training on a persona dialogue\ndataset to enhance persona consistency. Specifically, we present a persona\nextraction model designed to autonomously and precisely generate vast persona\ndialogue datasets. Additionally, we unveil a pioneering persona augmentation\ntechnique to address the invalid persona bias inherent in the constructed\ndataset. Both quantitative and human evaluations consistently highlight the\nsuperior response quality and persona consistency of our proposed model,\nunderscoring its effectiveness.", "published": "2024-12-12 07:49:06", "link": "http://arxiv.org/abs/2412.09034v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Multi-Task Learning with LLMs for Implicit Sentiment Analysis:\n  Data-level and Task-level Automatic Weight Learning", "abstract": "Implicit sentiment analysis (ISA) presents significant challenges due to the\nabsence of salient cue words. Previous methods have struggled with insufficient\ndata and limited reasoning capabilities to infer underlying opinions.\nIntegrating multi-task learning (MTL) with large language models (LLMs) offers\nthe potential to enable models of varying sizes to reliably perceive and\nrecognize genuine opinions in ISA. However, existing MTL approaches are\nconstrained by two sources of uncertainty: data-level uncertainty, arising from\nhallucination problems in LLM-generated contextual information, and task-level\nuncertainty, stemming from the varying capacities of models to process\ncontextual information. To handle these uncertainties, we introduce MT-ISA, a\nnovel MTL framework that enhances ISA by leveraging the generation and\nreasoning capabilities of LLMs through automatic MTL. Specifically, MT-ISA\nconstructs auxiliary tasks using generative LLMs to supplement sentiment\nelements and incorporates automatic MTL to fully exploit auxiliary data. We\nintroduce data-level and task-level automatic weight learning (AWL), which\ndynamically identifies relationships and prioritizes more reliable data and\ncritical tasks, enabling models of varying sizes to adaptively learn\nfine-grained weights based on their reasoning capabilities. We investigate\nthree strategies for data-level AWL, while also introducing homoscedastic\nuncertainty for task-level AWL. Extensive experiments reveal that models of\nvarying sizes achieve an optimal balance between primary prediction and\nauxiliary tasks in MT-ISA. This underscores the effectiveness and adaptability\nof our approach.", "published": "2024-12-12 08:15:16", "link": "http://arxiv.org/abs/2412.09046v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dial-In LLM: Human-Aligned LLM-in-the-loop Intent Clustering for\n  Customer Service Dialogues", "abstract": "Discovering customer intentions in dialogue conversations is crucial for\nautomated service agents. Yet, existing intent clustering methods often fail to\nalign with human perceptions due to the heavy reliance on embedding distance\nmetrics and sentence embeddings. To address these limitations, we propose\nintegrating the semantic understanding capabilities of LLMs into an\n$\\textbf{LLM-in-the-loop (LLM-ITL)}$ intent clustering framework. Specifically,\nthis paper (1) investigates the effectiveness of fine-tuned LLMs in semantic\ncoherence evaluation and intent cluster naming, achieving over 95% accuracy;\n(2) designs an LLM-ITL clustering algorithm that facilitates the iterative\ndiscovery of coherent intent clusters; and (3) proposes task-specific\ntechniques tailored for customer service dialogue intent clustering. Since\nexisting English benchmarks pose limited semantic diversity and intent labels,\nwe introduced a comprehensive Chinese dialogue intent dataset, comprising over\n100,000 real customer service calls and 1,507 human-annotated intent clusters.\nThe proposed approaches significantly outperformed LLM-guided baselines,\nachieving notable improvements in clustering quality and a 12% boost in the\ndownstream intent classification task. Combined with several best practices,\nour findings highlight the potential of LLM-in-the-loop techniques for scalable\nand human-aligned problem-solving. Sample code and datasets are available at:\nhttps://anonymous.4open.science/r/Dial-in-LLM-0410.", "published": "2024-12-12 08:19:01", "link": "http://arxiv.org/abs/2412.09049v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning", "abstract": "Large Language Models (LLMs) have demonstrated remarkable abilities across\nvarious language tasks, but solving complex reasoning problems remains a\nsignificant challenge. While existing methods, such as Chain-of-Thought (CoT)\nand Tree-of-Thought (ToT), enhance reasoning by decomposing problems or\nstructuring prompts, they typically perform a single pass of reasoning and may\nfail to revisit flawed paths, compromising accuracy. To address this\nlimitation, we propose a novel reasoning framework called Forest-of-Thought\n(FoT), which integrates multiple reasoning trees to leverage collective\ndecision-making for solving complex logical problems. FoT employs sparse\nactivation strategies to select the most relevant reasoning paths, improving\nboth efficiency and accuracy. Additionally, we introduce a dynamic\nself-correction strategy that enables real-time error correction, along with\nconsensus-guided decision-making strategies to optimize both correctness and\ncomputational resources. Experimental results demonstrate that the FoT\nframework, combined with these strategies, significantly enhances the reasoning\ncapabilities of LLMs, enabling them to solve complex tasks with greater\nprecision and efficiency. Code will be available at\nhttps://github.com/iamhankai/Forest-of-Thought.", "published": "2024-12-12 09:01:18", "link": "http://arxiv.org/abs/2412.09078v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PolyIPA -- Multilingual Phoneme-to-Grapheme Conversion Model", "abstract": "This paper presents PolyIPA, a novel multilingual phoneme-to-grapheme\nconversion model designed for multilingual name transliteration, onomastic\nresearch, and information retrieval. The model leverages two helper models\ndeveloped for data augmentation: IPA2vec for finding soundalikes across\nlanguages, and similarIPA for handling phonetic notation variations. Evaluated\non a test set that spans multiple languages and writing systems, the model\nachieves a mean Character Error Rate of 0.055 and a character-level BLEU score\nof 0.914, with particularly strong performance on languages with shallow\northographies. The implementation of beam search further improves practical\nutility, with top-3 candidates reducing the effective error rate by 52.7\\% (to\nCER: 0.026), demonstrating the model's effectiveness for cross-linguistic\napplications.", "published": "2024-12-12 09:29:59", "link": "http://arxiv.org/abs/2412.09102v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "First Train to Generate, then Generate to Train: UnitedSynT5 for\n  Few-Shot NLI", "abstract": "Natural Language Inference (NLI) tasks require identifying the relationship\nbetween sentence pairs, typically classified as entailment, contradiction, or\nneutrality. While the current state-of-the-art (SOTA) model, Entailment\nFew-Shot Learning (EFL), achieves a 93.1% accuracy on the Stanford Natural\nLanguage Inference (SNLI) dataset, further advancements are constrained by the\ndataset's limitations. To address this, we propose a novel approach leveraging\nsynthetic data augmentation to enhance dataset diversity and complexity. We\npresent UnitedSynT5, an advanced extension of EFL that leverages a T5-based\ngenerator to synthesize additional premise-hypothesis pairs, which are\nrigorously cleaned and integrated into the training data. These augmented\nexamples are processed within the EFL framework, embedding labels directly into\nhypotheses for consistency. We train a GTR-T5-XL model on this expanded\ndataset, achieving a new benchmark of 94.7% accuracy on the SNLI dataset, 94.0%\naccuracy on the E-SNLI dataset, and 92.6% accuracy on the MultiNLI dataset,\nsurpassing the previous SOTA models. This research demonstrates the potential\nof synthetic data augmentation in improving NLI models, offering a path forward\nfor further advancements in natural language understanding tasks.", "published": "2024-12-12 13:21:09", "link": "http://arxiv.org/abs/2412.09263v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Understanding the Robustness of LLM-based Evaluations under\n  Perturbations", "abstract": "Traditional evaluation metrics like BLEU and ROUGE fall short when capturing\nthe nuanced qualities of generated text, particularly when there is no single\nground truth. In this paper, we explore the potential of Large Language Models\n(LLMs), specifically Google Gemini 1, to serve as automatic evaluators for\nnon-standardized metrics in summarization and dialog-based tasks. We conduct\nexperiments across multiple prompting strategies to examine how LLMs fare as\nquality evaluators when compared with human judgments on the SummEval and USR\ndatasets, asking the model to generate both a score as well as a justification\nfor the score. Furthermore, we explore the robustness of the LLM evaluator by\nusing perturbed inputs. Our findings suggest that while LLMs show promise,\ntheir alignment with human evaluators is limited, they are not robust against\nperturbations and significant improvements are required for their standalone\nuse as reliable evaluators for subjective metrics.", "published": "2024-12-12 13:31:58", "link": "http://arxiv.org/abs/2412.09269v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CRVQ: Channel-Relaxed Vector Quantization for Extreme Compression of\n  LLMs", "abstract": "Powerful large language models (LLMs) are increasingly expected to be\ndeployed with lower computational costs, enabling their capabilities on\nresource-constrained devices. Post-training quantization (PTQ) has emerged as a\nstar approach to achieve this ambition, with best methods compressing weights\nto less than 2 bit on average. In this paper, we propose Channel-Relaxed Vector\nQuantization (CRVQ), a novel technique that significantly improves the\nperformance of PTQ baselines at the cost of only minimal additional bits. This\nstate-of-the-art extreme compression method achieves its results through two\nkey innovations: (1) carefully selecting and reordering a very small subset of\ncritical weight channels, and (2) leveraging extended codebooks to relax the\nconstraint of critical channels. With our method, we demonstrate a 38.9\\%\nimprovement over the current strongest sub-2-bit PTQ baseline, enabling nearer\nlossless 1-bit compression. Furthermore, our approach offers flexible\ncustomization of quantization bit-width and performance, providing a wider\nrange of deployment options for diverse hardware platforms.", "published": "2024-12-12 13:45:11", "link": "http://arxiv.org/abs/2412.09282v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Benchmarking LLMs for Mimicking Child-Caregiver Language in Interaction", "abstract": "LLMs can generate human-like dialogues, yet their ability to simulate early\nchild-adult interactions remains largely unexplored. In this paper, we examined\nhow effectively LLMs can capture the distinctive features of child-caregiver\nlanguage in interaction, using both static and interactive benchmarking\nmethods. We found that state-of-the-art LLMs like Llama 3 and GPT-4o can\napproximate child-caregiver dialogues at the word and utterance level, but they\nstruggle to reproduce the child and caregiver's discursive patterns, exaggerate\nalignment, and fail to reach the level of diversity shown by humans. The\nbroader goal of this work is to initiate the development of a comprehensive\nbenchmark for LLMs in child-oriented applications.", "published": "2024-12-12 14:43:03", "link": "http://arxiv.org/abs/2412.09318v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Word Sense Linking: Disambiguating Outside the Sandbox", "abstract": "Word Sense Disambiguation (WSD) is the task of associating a word in a given\ncontext with its most suitable meaning among a set of possible candidates.\nWhile the task has recently witnessed renewed interest, with systems achieving\nperformances above the estimated inter-annotator agreement, at the time of\nwriting it still struggles to find downstream applications. We argue that one\nof the reasons behind this is the difficulty of applying WSD to plain text.\nIndeed, in the standard formulation, models work under the assumptions that a)\nall the spans to disambiguate have already been identified, and b) all the\npossible candidate senses of each span are provided, both of which are\nrequirements that are far from trivial. In this work, we present a new task\ncalled Word Sense Linking (WSL) where, given an input text and a reference\nsense inventory, systems have to both identify which spans to disambiguate and\nthen link them to their most suitable meaning.We put forward a\ntransformer-based architecture for the task and thoroughly evaluate both its\nperformance and those of state-of-the-art WSD systems scaled to WSL,\niteratively relaxing the assumptions of WSD. We hope that our work will foster\neasier integration of lexical semantics into downstream applications.", "published": "2024-12-12 15:38:34", "link": "http://arxiv.org/abs/2412.09370v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Bench to Bedside: A Review of Clinical Trials in Drug Discovery and\n  Development", "abstract": "Clinical trials are an indispensable part of the drug development process,\nbridging the gap between basic research and clinical application. During the\ndevelopment of new drugs, clinical trials are used not only to evaluate the\nsafety and efficacy of the drug but also to explore its dosage, treatment\nregimens, and potential side effects. This review discusses the various stages\nof clinical trials, including Phase I (safety assessment), Phase II\n(preliminary efficacy evaluation), Phase III (large-scale validation), and\nPhase IV (post-marketing surveillance), highlighting the characteristics of\neach phase and their interrelationships. Additionally, the paper addresses the\nmajor challenges encountered in clinical trials, such as ethical issues,\nsubject recruitment difficulties, diversity and representativeness concerns,\nand proposes strategies for overcoming these challenges. With the advancement\nof technology, innovative technologies such as artificial intelligence, big\ndata, and digitalization are gradually transforming clinical trial design and\nimplementation, improving trial efficiency and data quality. The article also\nlooks forward to the future of clinical trials, particularly the impact of\nemerging therapies such as gene therapy and immunotherapy on trial design, as\nwell as the importance of regulatory reforms and global collaboration. In\nconclusion, the core role of clinical trials in drug development will continue\nto drive the progress of innovative drug development and clinical treatment.", "published": "2024-12-12 15:46:43", "link": "http://arxiv.org/abs/2412.09378v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Imitate, Explore, and Self-Improve: A Reproduction Report on\n  Slow-thinking Reasoning Systems", "abstract": "Recently, slow-thinking reasoning systems, such as o1, have demonstrated\nremarkable capabilities in solving complex reasoning tasks. These systems\ntypically engage in an extended thinking process before responding to a query,\nallowing them to generate more thorough, accurate, and well-reasoned solutions.\nThese systems are primarily developed and maintained by industry, with their\ncore techniques not publicly disclosed. In response, an increasing number of\nstudies from the research community aim to explore the technical foundations\nunderlying these powerful reasoning systems. Building on these prior efforts,\nthis paper presents a reproduction report on implementing o1-like reasoning\nsystems. We introduce an ``imitate, explore, and self-improve'' framework,\ndenoted as \\textbf{STILL-2}, as our primary technical approach to train the\nreasoning model. In the initial phase, we use distilled long-form thought data\nto fine-tune the reasoning model, enabling it to invoke a slow-thinking mode.\nThe model is then encouraged to explore challenging problems by generating\nmultiple rollouts, which can result in increasingly more high-quality\ntrajectories that lead to correct answers. Furthermore, the model undergoes\nself-improvement by iteratively refining its training dataset. To verify the\neffectiveness of this approach, we conduct extensive experiments on three\nchallenging benchmarks. The experimental results demonstrate that our approach\nachieves competitive performance compared to industry-level reasoning systems\non these benchmarks.", "published": "2024-12-12 16:20:36", "link": "http://arxiv.org/abs/2412.09413v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Does Representation Matter? Exploring Intermediate Layers in Large\n  Language Models", "abstract": "Understanding what defines a good representation in large language models\n(LLMs) is fundamental to both theoretical understanding and practical\napplications. In this paper, we investigate the quality of intermediate\nrepresentations in various LLM architectures, including Transformers and State\nSpace Models (SSMs). We find that intermediate layers often yield more\ninformative representations for downstream tasks than the final layers. To\nmeasure the representation quality, we adapt and apply a suite of metrics -\nsuch as prompt entropy, curvature, and augmentation-invariance - originally\nproposed in other contexts. Our empirical study reveals significant\narchitectural differences, how representations evolve throughout training, and\nhow factors like input randomness and prompt length affect each layer. Notably,\nwe observe a bimodal pattern in the entropy of some intermediate layers and\nconsider potential explanations tied to training data. Overall, our results\nilluminate the internal mechanics of LLMs and guide strategies for\narchitectural optimization and training.", "published": "2024-12-12 18:48:51", "link": "http://arxiv.org/abs/2412.09563v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Context Canvas: Enhancing Text-to-Image Diffusion Models with Knowledge\n  Graph-Based RAG", "abstract": "We introduce a novel approach to enhance the capabilities of text-to-image\nmodels by incorporating a graph-based RAG. Our system dynamically retrieves\ndetailed character information and relational data from the knowledge graph,\nenabling the generation of visually accurate and contextually rich images. This\ncapability significantly improves upon the limitations of existing T2I models,\nwhich often struggle with the accurate depiction of complex or culturally\nspecific subjects due to dataset constraints. Furthermore, we propose a novel\nself-correcting mechanism for text-to-image models to ensure consistency and\nfidelity in visual outputs, leveraging the rich context from the graph to guide\ncorrections. Our qualitative and quantitative experiments demonstrate that\nContext Canvas significantly enhances the capabilities of popular models such\nas Flux, Stable Diffusion, and DALL-E, and improves the functionality of\nControlNet for fine-grained image editing tasks. To our knowledge, Context\nCanvas represents the first application of graph-based RAG in enhancing T2I\nmodels, representing a significant advancement for producing high-fidelity,\ncontext-aware multi-faceted images.", "published": "2024-12-12 18:59:41", "link": "http://arxiv.org/abs/2412.09614v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Systematic Analysis of LLM Contributions to Planning: Solver, Verifier,\n  Heuristic", "abstract": "In this work, we provide a systematic analysis of how large language models\n(LLMs) contribute to solving planning problems. In particular, we examine how\nLLMs perform when they are used as problem solver, solution verifier, and\nheuristic guidance to improve intermediate solutions. Our analysis reveals that\nalthough it is difficult for LLMs to generate correct plans out-of-the-box,\nLLMs are much better at providing feedback signals to intermediate/incomplete\nsolutions in the form of comparative heuristic functions. This evaluation\nframework provides insights into how future work may design better LLM-based\ntree-search algorithms to solve diverse planning and reasoning problems. We\nalso propose a novel benchmark to evaluate LLM's ability to learn user\npreferences on the fly, which has wide applications in practical settings.", "published": "2024-12-12 18:16:46", "link": "http://arxiv.org/abs/2412.09666v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Human vs. AI: A Novel Benchmark and a Comparative Study on the Detection\n  of Generated Images and the Impact of Prompts", "abstract": "With the advent of publicly available AI-based text-to-image systems, the\nprocess of creating photorealistic but fully synthetic images has been largely\ndemocratized. This can pose a threat to the public through a simplified spread\nof disinformation. Machine detectors and human media expertise can help to\ndifferentiate between AI-generated (fake) and real images and counteract this\ndanger. Although AI generation models are highly prompt-dependent, the impact\nof the prompt on the fake detection performance has rarely been investigated\nyet. This work therefore examines the influence of the prompt's level of detail\non the detectability of fake images, both with an AI detector and in a user\nstudy. For this purpose, we create a novel dataset, COCOXGEN, which consists of\nreal photos from the COCO dataset as well as images generated with SDXL and\nFooocus using prompts of two standardized lengths. Our user study with 200\nparticipants shows that images generated with longer, more detailed prompts are\ndetected significantly more easily than those generated with short prompts.\nSimilarly, an AI-based detection model achieves better performance on images\ngenerated with longer prompts. However, humans and AI models seem to pay\nattention to different details, as we show in a heat map analysis.", "published": "2024-12-12 20:37:52", "link": "http://arxiv.org/abs/2412.09715v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Memory Layers at Scale", "abstract": "Memory layers use a trainable key-value lookup mechanism to add extra\nparameters to a model without increasing FLOPs. Conceptually, sparsely\nactivated memory layers complement compute-heavy dense feed-forward layers,\nproviding dedicated capacity to store and retrieve information cheaply. This\nwork takes memory layers beyond proof-of-concept, proving their utility at\ncontemporary scale. On downstream tasks, language models augmented with our\nimproved memory layer outperform dense models with more than twice the\ncomputation budget, as well as mixture-of-expert models when matched for both\ncompute and parameters. We find gains are especially pronounced for factual\ntasks. We provide a fully parallelizable memory layer implementation,\ndemonstrating scaling laws with up to 128B memory parameters, pretrained to 1\ntrillion tokens, comparing to base models with up to 8B parameters.", "published": "2024-12-12 23:56:57", "link": "http://arxiv.org/abs/2412.09764v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AI Adoption to Combat Financial Crime: Study on Natural Language\n  Processing in Adverse Media Screening of Financial Services in English and\n  Bangla multilingual interpretation", "abstract": "This document explores the potential of employing Artificial Intelligence\n(AI), specifically Natural Language Processing (NLP), to strengthen the\ndetection and prevention of financial crimes within the Mobile Financial\nServices(MFS) of Bangladesh with multilingual scenario. The analysis focuses on\nthe utilization of NLP for adverse media screening, a vital aspect of\ncompliance with anti-money laundering (AML) and combating financial terrorism\n(CFT) regulations. Additionally, it investigates the overall reception and\nobstacles related to the integration of AI in Bangladeshi banks. This report\nmeasures the effectiveness of NLP is promising with an accuracy around 94\\%.\nNLP algorithms display substantial promise in accurately identifying adverse\nmedia content linked to financial crimes. The lack of progress in this aspect\nis visible in Bangladesh, whereas globally the technology is already being used\nto increase effectiveness and efficiency. Hence, it is clear there is an issue\nwith the acceptance of AI in Bangladesh. Some AML \\& CFT concerns are already\nbeing addressed by AI technology. For example, Image Recognition OCR technology\nare being used in KYC procedures. Primary hindrances to AI integration involve\na lack of technical expertise, high expenses, and uncertainties surrounding\nregulations. This investigation underscores the potential of AI-driven NLP\nsolutions in fortifying efforts to prevent financial crimes in Bangladesh.", "published": "2024-12-12 07:17:05", "link": "http://arxiv.org/abs/2412.12171v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A NotSo Simple Way to Beat Simple Bench", "abstract": "This paper presents a novel framework for enhancing reasoning capabilities in\nlarge language models (LLMs) by leveraging iterative reasoning and\nfeedback-driven methodologies. Building on the limitations identified in the\nSimpleBench benchmark, a dataset designed to evaluate logical coherence and\nreal-world reasoning, we propose a multi-step prompting strategy coupled with\nglobal consistency checks to improve model accuracy and robustness. Through\ncomparative analysis of state-of-the-art models, including Claude 3 Opus,\nClaude 3.5, GPT- 4o, and o1-preview, we demonstrate that iterative reasoning\nsignificantly enhances model performance, with improvements observed in both\nstandard accuracy metrics (AVG@5) and a newly introduced metric, Extreme\nAveraging (EAG@5). Our results reveal model-specific strengths: Claude excels\nin maintaining logical consistency, while GPT-4o exhibits exploratory\ncreativity but struggles with ambiguous prompts. By analyzing case studies and\nidentifying gaps in spatial and temporal reasoning, we highlight areas for\nfurther refinement. The findings underscore the potential of structured\nreasoning frameworks to address inherent model limitations, irrespective of\npretraining methodologies. This study lays the groundwork for integrating\ndynamic feedback mechanisms, adaptive restart strategies, and diverse\nevaluation metrics to advance LLM reasoning capabilities across complex and\nmulti-domain problem spaces.", "published": "2024-12-12 16:04:31", "link": "http://arxiv.org/abs/2412.12173v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OG-RAG: Ontology-Grounded Retrieval-Augmented Generation For Large\n  Language Models", "abstract": "This paper presents OG-RAG, an Ontology-Grounded Retrieval Augmented\nGeneration method designed to enhance LLM-generated responses by anchoring\nretrieval processes in domain-specific ontologies. While LLMs are widely used\nfor tasks like question answering and search, they struggle to adapt to\nspecialized knowledge, such as industrial workflows or knowledge work, without\nexpensive fine-tuning or sub-optimal retrieval methods. Existing\nretrieval-augmented models, such as RAG, offer improvements but fail to account\nfor structured domain knowledge, leading to suboptimal context generation.\nOntologies, which conceptually organize domain knowledge by defining entities\nand their interrelationships, offer a structured representation to address this\ngap. OG-RAG constructs a hypergraph representation of domain documents, where\neach hyperedge encapsulates clusters of factual knowledge grounded using\ndomain-specific ontology. An optimization algorithm then retrieves the minimal\nset of hyperedges that constructs a precise, conceptually grounded context for\nthe LLM. This method enables efficient retrieval while preserving the complex\nrelationships between entities. OG-RAG applies to domains where fact-based\nreasoning is essential, particularly in tasks that require workflows or\ndecision-making steps to follow predefined rules and procedures. These include\nindustrial workflows in healthcare, legal, and agricultural sectors, as well as\nknowledge-driven tasks such as news journalism, investigative research,\nconsulting and more. Our evaluations demonstrate that OG-RAG increases the\nrecall of accurate facts by 55% and improves response correctness by 40% across\nfour different LLMs. Additionally, OG-RAG enables 30% faster attribution of\nresponses to context and boosts fact-based reasoning accuracy by 27% compared\nto baseline methods.", "published": "2024-12-12 01:21:03", "link": "http://arxiv.org/abs/2412.15235v1", "categories": ["cs.CL", "cs.AI", "I.2.0; I.2.4"], "primary_category": "cs.CL"}
{"title": "CareBot: A Pioneering Full-Process Open-Source Medical Language Model", "abstract": "Recently, both closed-source LLMs and open-source communities have made\nsignificant strides, outperforming humans in various general domains. However,\ntheir performance in specific professional domains such as medicine, especially\nwithin the open-source community, remains suboptimal due to the complexity of\nmedical knowledge. In this paper, we propose CareBot, a bilingual medical LLM,\nwhich leverages a comprehensive approach integrating continuous pre-training\n(CPT), supervised fine-tuning (SFT), and reinforcement learning with human\nfeedback (RLHF). Our novel two-stage CPT method, comprising Stable CPT and\nBoost CPT, effectively bridges the gap between general and domain-specific\ndata, facilitating a smooth transition from pre-training to fine-tuning and\nenhancing domain knowledge progressively. We also introduce DataRater, a model\ndesigned to assess data quality during CPT, ensuring that the training data is\nboth accurate and relevant. For SFT, we develope a large and diverse bilingual\ndataset, along with ConFilter, a metric to enhance multi-turn dialogue quality,\nwhich is crucial to improving the model's ability to handle more complex\ndialogues. The combination of high-quality data sources and innovative\ntechniques significantly improves CareBot's performance across a range of\nmedical applications. Our rigorous evaluations on Chinese and English\nbenchmarks confirm CareBot's effectiveness in medical consultation and\neducation. These advancements not only address current limitations in medical\nLLMs but also set a new standard for developing effective and reliable\nopen-source models in the medical domain. We will open-source the datasets and\nmodels later, contributing valuable resources to the research community.", "published": "2024-12-12 05:27:43", "link": "http://arxiv.org/abs/2412.15236v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MoSLD: An Extremely Parameter-Efficient Mixture-of-Shared LoRAs for\n  Multi-Task Learning", "abstract": "Recently, LoRA has emerged as a crucial technique for fine-tuning large\npre-trained models, yet its performance in multi-task learning scenarios often\nfalls short. In contrast, the MoE architecture presents a natural solution to\nthis issue. However, it introduces challenges such as mutual interference of\ndata across multiple domains and knowledge forgetting of various tasks.\nAdditionally, MoE significantly increases the number of parameters, posing a\ncomputational cost challenge. Therefore, in this paper, we propose MoSLD, a\nmixture-of-shared-LoRAs model with a dropout strategy. MoSLD addresses these\nchallenges by sharing the upper projection matrix in LoRA among different\nexperts, encouraging the model to learn general knowledge across tasks, while\nstill allowing the lower projection matrix to focus on the unique features of\neach task. The application of dropout alleviates the imbalanced update of\nparameter matrix and mitigates parameter overfitting in LoRA. Extensive\nexperiments demonstrate that our model exhibits excellent performance in both\nsingle-task and multi-task scenarios, with robust out-of-domain generalization\ncapabilities.", "published": "2024-12-12 05:22:49", "link": "http://arxiv.org/abs/2412.08946v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Filter-then-Generate: Large Language Models with Structure-Text Adapter\n  for Knowledge Graph Completion", "abstract": "Large Language Models (LLMs) present massive inherent knowledge and superior\nsemantic comprehension capability, which have revolutionized various tasks in\nnatural language processing. Despite their success, a critical gap remains in\nenabling LLMs to perform knowledge graph completion (KGC). Empirical evidence\nsuggests that LLMs consistently perform worse than conventional KGC approaches,\neven through sophisticated prompt design or tailored instruction-tuning.\nFundamentally, applying LLMs on KGC introduces several critical challenges,\nincluding a vast set of entity candidates, hallucination issue of LLMs, and\nunder-exploitation of the graph structure. To address these challenges, we\npropose a novel instruction-tuning-based method, namely FtG. Specifically, we\npresent a filter-then-generate paradigm and formulate the KGC task into a\nmultiple-choice question format. In this way, we can harness the capability of\nLLMs while mitigating the issue casused by hallucinations. Moreover, we devise\na flexible ego-graph serialization prompt and employ a structure-text adapter\nto couple structure and text information in a contextualized manner.\nExperimental results demonstrate that FtG achieves substantial performance gain\ncompared to existing state-of-the-art methods. The instruction dataset and code\nare available at https://github.com/LB0828/FtG.", "published": "2024-12-12 09:22:04", "link": "http://arxiv.org/abs/2412.09094v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "When Text Embedding Meets Large Language Model: A Comprehensive Survey", "abstract": "Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications - such as semantic matching,\nclustering, and information retrieval - continue to rely on text embeddings for\ntheir efficiency and effectiveness. Therefore, integrating LLMs with text\nembeddings has become a major research focus in recent years. In this survey,\nwe categorize the interplay between LLMs and text embeddings into three\noverarching themes: (1) LLM-augmented text embedding, enhancing traditional\nembedding methods with LLMs; (2) LLMs as text embedders, adapting their innate\ncapabilities for high-quality embedding; and (3) Text embedding understanding\nwith LLMs, leveraging LLMs to analyze and interpret embeddings. By organizing\nrecent works based on interaction patterns rather than specific downstream\napplications, we offer a novel and systematic overview of contributions from\nvarious research and application domains in the era of LLMs. Furthermore, we\nhighlight the unresolved challenges that persisted in the pre-LLM era with\npre-trained language models (PLMs) and explore the emerging obstacles brought\nforth by LLMs. Building on this analysis, we outline prospective directions for\nthe evolution of text embedding, addressing both theoretical and practical\nopportunities in the rapidly advancing landscape of NLP.", "published": "2024-12-12 10:50:26", "link": "http://arxiv.org/abs/2412.09165v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Causal Graphical Models for Vision-Language Compositional Understanding", "abstract": "Recent work has empirically shown that Vision-Language Models (VLMs) struggle\nto fully understand the compositional properties of the human language, usually\nmodeling an image caption as a \"bag of words\". As a result, they perform poorly\non compositional tasks, which require a deeper understanding of the different\nentities of a sentence (subject, verb, etc.) jointly with their mutual\nrelationships in order to be solved. In this paper, we model the dependency\nrelations among textual and visual tokens using a Causal Graphical Model (CGM),\nbuilt using a dependency parser, and we train a decoder conditioned by the VLM\nvisual encoder. Differently from standard autoregressive or parallel\npredictions, our decoder's generative process is partially-ordered following\nthe CGM structure. This structure encourages the decoder to learn only the main\ncausal dependencies in a sentence discarding spurious correlations. Using\nextensive experiments on five compositional benchmarks, we show that our method\nsignificantly outperforms all the state-of-the-art compositional approaches by\na large margin, and it also improves over methods trained using much larger\ndatasets.", "published": "2024-12-12 15:22:03", "link": "http://arxiv.org/abs/2412.09353v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "From Intention To Implementation: Automating Biomedical Research via\n  LLMs", "abstract": "Conventional biomedical research is increasingly labor-intensive due to the\nexponential growth of scientific literature and datasets. Artificial\nintelligence (AI), particularly Large Language Models (LLMs), has the potential\nto revolutionize this process by automating various steps. Still, significant\nchallenges remain, including the need for multidisciplinary expertise,\nlogicality of experimental design, and performance measurements. This paper\nintroduces BioResearcher, the first end-to-end automated system designed to\nstreamline the entire biomedical research process involving dry lab\nexperiments. BioResearcher employs a modular multi-agent architecture,\nintegrating specialized agents for search, literature processing, experimental\ndesign, and programming. By decomposing complex tasks into logically related\nsub-tasks and utilizing a hierarchical learning approach, BioResearcher\neffectively addresses the challenges of multidisciplinary requirements and\nlogical complexity. Furthermore, BioResearcher incorporates an LLM-based\nreviewer for in-process quality control and introduces novel evaluation metrics\nto assess the quality and automation of experimental protocols. BioResearcher\nsuccessfully achieves an average execution success rate of 63.07% across eight\npreviously unmet research objectives. The generated protocols averagely\noutperform typical agent systems by 22.0% on five quality metrics. The system\ndemonstrates significant potential to reduce researchers' workloads and\naccelerate biomedical discoveries, paving the way for future innovations in\nautomated research systems.", "published": "2024-12-12 16:35:05", "link": "http://arxiv.org/abs/2412.09429v2", "categories": ["cs.MA", "cs.AI", "cs.CL"], "primary_category": "cs.MA"}
{"title": "Audios Don't Lie: Multi-Frequency Channel Attention Mechanism for Audio\n  Deepfake Detection", "abstract": "With the rapid development of artificial intelligence technology, the\napplication of deepfake technology in the audio field has gradually increased,\nresulting in a wide range of security risks. Especially in the financial and\nsocial security fields, the misuse of deepfake audios has raised serious\nconcerns. To address this challenge, this study proposes an audio deepfake\ndetection method based on multi-frequency channel attention mechanism (MFCA)\nand 2D discrete cosine transform (DCT). By processing the audio signal into a\nmelspectrogram, using MobileNet V2 to extract deep features, and combining it\nwith the MFCA module to weight different frequency channels in the audio\nsignal, this method can effectively capture the fine-grained frequency domain\nfeatures in the audio signal and enhance the Classification capability of fake\naudios. Experimental results show that compared with traditional methods, the\nmodel proposed in this study shows significant advantages in accuracy,\nprecision,recall, F1 score and other indicators. Especially in complex audio\nscenarios, this method shows stronger robustness and generalization\ncapabilities and provides a new idea for audio deepfake detection and has\nimportant practical application value. In the future, more advanced audio\ndetection technologies and optimization strategies will be explored to further\nimprove the accuracy and generalization capabilities of audio deepfake\ndetection.", "published": "2024-12-12 17:15:49", "link": "http://arxiv.org/abs/2412.09467v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Foundational Large Language Models for Materials Research", "abstract": "Materials discovery and development are critical for addressing global\nchallenges. Yet, the exponential growth in materials science literature\ncomprising vast amounts of textual data has created significant bottlenecks in\nknowledge extraction, synthesis, and scientific reasoning. Large Language\nModels (LLMs) offer unprecedented opportunities to accelerate materials\nresearch through automated analysis and prediction. Still, their effective\ndeployment requires domain-specific adaptation for understanding and solving\ndomain-relevant tasks. Here, we present LLaMat, a family of foundational models\nfor materials science developed through continued pretraining of LLaMA models\non an extensive corpus of materials literature and crystallographic data.\nThrough systematic evaluation, we demonstrate that LLaMat excels in\nmaterials-specific NLP and structured information extraction while maintaining\ngeneral linguistic capabilities. The specialized LLaMat-CIF variant\ndemonstrates unprecedented capabilities in crystal structure generation,\npredicting stable crystals with high coverage across the periodic table.\nIntriguingly, despite LLaMA-3's superior performance in comparison to LLaMA-2,\nwe observe that LLaMat-2 demonstrates unexpectedly enhanced domain-specific\nperformance across diverse materials science tasks, including structured\ninformation extraction from text and tables, more particularly in crystal\nstructure generation, a potential adaptation rigidity in overtrained LLMs.\nAltogether, the present work demonstrates the effectiveness of domain\nadaptation towards developing practically deployable LLM copilots for materials\nresearch. Beyond materials science, our findings reveal important\nconsiderations for domain adaptation of LLMs, such as model selection, training\nmethodology, and domain-specific performance, which may influence the\ndevelopment of specialized scientific AI systems.", "published": "2024-12-12 18:46:38", "link": "http://arxiv.org/abs/2412.09560v2", "categories": ["cond-mat.mtrl-sci", "cs.CL", "cs.IR"], "primary_category": "cond-mat.mtrl-sci"}
{"title": "JuStRank: Benchmarking LLM Judges for System Ranking", "abstract": "Given the rapid progress of generative AI, there is a pressing need to\nsystematically compare and choose between the numerous models and\nconfigurations available. The scale and versatility of such evaluations make\nthe use of LLM-based judges a compelling solution for this challenge.\nCrucially, this approach requires first to validate the quality of the LLM\njudge itself. Previous work has focused on instance-based assessment of LLM\njudges, where a judge is evaluated over a set of responses, or response pairs,\nwhile being agnostic to their source systems. We argue that this setting\noverlooks critical factors affecting system-level ranking, such as a judge's\npositive or negative bias towards certain systems. To address this gap, we\nconduct the first large-scale study of LLM judges as system rankers. System\nscores are generated by aggregating judgment scores over multiple system\noutputs, and the judge's quality is assessed by comparing the resulting system\nranking to a human-based ranking. Beyond overall judge assessment, our analysis\nprovides a fine-grained characterization of judge behavior, including their\ndecisiveness and bias.", "published": "2024-12-12 18:51:13", "link": "http://arxiv.org/abs/2412.09569v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DISHONEST: Dissecting misInformation Spread using Homogeneous sOcial\n  NEtworks and Semantic Topic classification", "abstract": "The emergence of the COVID-19 pandemic resulted in a significant rise in the\nspread of misinformation on online platforms such as Twitter. Oftentimes this\ngrowth is blamed on the idea of the \"echo chamber.\" However, the behavior said\nto characterize these echo chambers exists in two dimensions. The first is in a\nuser's social interactions, where they are said to stick with the same clique\nof like-minded users. The second is in the content of their posts, where they\nare said to repeatedly espouse homogeneous ideas. In this study, we link the\ntwo by using Twitter's network of retweets to study social interactions and\ntopic modeling to study tweet content. In order to measure the diversity of a\nuser's interactions over time, we develop a novel metric to track the speed at\nwhich they travel through the social network. The application of these analysis\nmethods to misinformation-focused data from the pandemic demonstrates\ncorrelation between social behavior and tweet content. We believe this\ncorrelation supports the common intuition about how antisocial users behave,\nand further suggests that it holds even in subcommunities already rife with\nmisinformation.", "published": "2024-12-12 18:53:46", "link": "http://arxiv.org/abs/2412.09578v1", "categories": ["cs.SI", "cs.AI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for\n  Long-term Streaming Video and Audio Interactions", "abstract": "Creating AI systems that can interact with environments over long periods,\nsimilar to human cognition, has been a longstanding research goal. Recent\nadvancements in multimodal large language models (MLLMs) have made significant\nstrides in open-world understanding. However, the challenge of continuous and\nsimultaneous streaming perception, memory, and reasoning remains largely\nunexplored. Current MLLMs are constrained by their sequence-to-sequence\narchitecture, which limits their ability to process inputs and generate\nresponses simultaneously, akin to being unable to think while perceiving.\nFurthermore, relying on long contexts to store historical data is impractical\nfor long-term interactions, as retaining all information becomes costly and\ninefficient. Therefore, rather than relying on a single foundation model to\nperform all functions, this project draws inspiration from the concept of the\nSpecialized Generalist AI and introduces disentangled streaming perception,\nreasoning, and memory mechanisms, enabling real-time interaction with streaming\nvideo and audio input. The proposed framework InternLM-XComposer2.5-OmniLive\n(IXC2.5-OL) consists of three key modules: (1) Streaming Perception Module:\nProcesses multimodal information in real-time, storing key details in memory\nand triggering reasoning in response to user queries. (2) Multi-modal Long\nMemory Module: Integrates short-term and long-term memory, compressing\nshort-term memories into long-term ones for efficient retrieval and improved\naccuracy. (3) Reasoning Module: Responds to queries and executes reasoning\ntasks, coordinating with the perception and memory modules. This project\nsimulates human-like cognition, enabling multimodal large language models to\nprovide continuous and adaptive service over time.", "published": "2024-12-12 18:58:30", "link": "http://arxiv.org/abs/2412.09596v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "TimeRefine: Temporal Grounding with Time Refining Video LLM", "abstract": "Video temporal grounding aims to localize relevant temporal boundaries in a\nvideo given a textual prompt. Recent work has focused on enabling Video LLMs to\nperform video temporal grounding via next-token prediction of temporal\ntimestamps. However, accurately localizing timestamps in videos remains\nchallenging for Video LLMs when relying solely on temporal token prediction.\nOur proposed TimeRefine addresses this challenge in two ways. First, instead of\ndirectly predicting the start and end timestamps, we reformulate the temporal\ngrounding task as a temporal refining task: the model first makes rough\npredictions and then refines them by predicting offsets to the target segment.\nThis refining process is repeated multiple times, through which the model\nprogressively self-improves its temporal localization accuracy. Second, to\nenhance the model's temporal perception capabilities, we incorporate an\nauxiliary prediction head that penalizes the model more if a predicted segment\ndeviates further from the ground truth, thus encouraging the model to make\ncloser and more accurate predictions. Our plug-and-play method can be\nintegrated into most LLM-based temporal grounding approaches. The experimental\nresults demonstrate that TimeRefine achieves 3.6% and 5.0% mIoU improvements on\nthe ActivityNet and Charades-STA datasets, respectively. Code and pretrained\nmodels will be released.", "published": "2024-12-12 18:59:11", "link": "http://arxiv.org/abs/2412.09601v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Olympus: A Universal Task Router for Computer Vision Tasks", "abstract": "We introduce Olympus, a new approach that transforms Multimodal Large\nLanguage Models (MLLMs) into a unified framework capable of handling a wide\narray of computer vision tasks. Utilizing a controller MLLM, Olympus delegates\nover 20 specialized tasks across images, videos, and 3D objects to dedicated\nmodules. This instruction-based routing enables complex workflows through\nchained actions without the need for training heavy generative models. Olympus\neasily integrates with existing MLLMs, expanding their capabilities with\ncomparable performance. Experimental results demonstrate that Olympus achieves\nan average routing accuracy of 94.75% across 20 tasks and precision of 91.82%\nin chained action scenarios, showcasing its effectiveness as a universal task\nrouter that can solve a diverse range of computer vision tasks. Project page:\nhttp://yuanze-lin.me/Olympus_page/", "published": "2024-12-12 18:59:40", "link": "http://arxiv.org/abs/2412.09612v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Formal Languages and TQFTs with Defects", "abstract": "A construction that assigns a Boolean 1D TQFT with defects to a finite state\nautomaton was recently developed by Gustafson, Im, Kaldawy, Khovanov, and Lihn.\nWe show that the construction is functorial with respect to the category of\nfinite state automata with transducers as morphisms. Certain classes of\nsubregular languages correspond to additional cohomological structures on the\nassociated TQFTs. We also show that the construction generalizes to\ncontext-free grammars through a categorical version of the\nChomsky-Sch\\\"utzenberger representation theorem, due to Melli\\`es and\nZeilberger. The corresponding TQFTs are then described as morphisms of colored\noperads on an operad of cobordisms with defects.", "published": "2024-12-12 19:07:31", "link": "http://arxiv.org/abs/2412.09688v1", "categories": ["math-ph", "cs.CL", "math.MP", "math.QA", "68Q45, 81T45"], "primary_category": "math-ph"}
{"title": "MGM: Global Understanding of Audience Overlap Graphs for Predicting the\n  Factuality and the Bias of News Media", "abstract": "In the current era of rapidly growing digital data, evaluating the political\nbias and factuality of news outlets has become more important for seeking\nreliable information online. In this work, we study the classification problem\nof profiling news media from the lens of political bias and factuality.\nTraditional profiling methods, such as Pre-trained Language Models (PLMs) and\nGraph Neural Networks (GNNs) have shown promising results, but they face\nnotable challenges. PLMs focus solely on textual features, causing them to\noverlook the complex relationships between entities, while GNNs often struggle\nwith media graphs containing disconnected components and insufficient labels.\nTo address these limitations, we propose MediaGraphMind (MGM), an effective\nsolution within a variational Expectation-Maximization (EM) framework. Instead\nof relying on limited neighboring nodes, MGM leverages features, structural\npatterns, and label information from globally similar nodes. Such a framework\nnot only enables GNNs to capture long-range dependencies for learning\nexpressive node representations but also enhances PLMs by integrating\nstructural information and therefore improving the performance of both models.\nThe extensive experiments demonstrate the effectiveness of the proposed\nframework and achieve new state-of-the-art results. Further, we share our\nrepository1 which contains the dataset, code, and documentation", "published": "2024-12-12 18:37:32", "link": "http://arxiv.org/abs/2412.10467v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Explore Theory of Mind: Program-guided adversarial data generation for\n  theory of mind reasoning", "abstract": "Do large language models (LLMs) have theory of mind? A plethora of papers and\nbenchmarks have been introduced to evaluate if current models have been able to\ndevelop this key ability of social intelligence. However, all rely on limited\ndatasets with simple patterns that can potentially lead to problematic blind\nspots in evaluation and an overestimation of model capabilities. We introduce\nExploreToM, the first framework to allow large-scale generation of diverse and\nchallenging theory of mind data for robust training and evaluation. Our\napproach leverages an A* search over a custom domain-specific language to\nproduce complex story structures and novel, diverse, yet plausible scenarios to\nstress test the limits of LLMs. Our evaluation reveals that state-of-the-art\nLLMs, such as Llama-3.1-70B and GPT-4o, show accuracies as low as 0% and 9% on\nExploreToM-generated data, highlighting the need for more robust theory of mind\nevaluation. As our generations are a conceptual superset of prior work,\nfine-tuning on our data yields a 27-point accuracy improvement on the classic\nToMi benchmark (Le et al., 2019). ExploreToM also enables uncovering underlying\nskills and factors missing for models to show theory of mind, such as\nunreliable state tracking or data imbalances, which may contribute to models'\npoor performance on benchmarks.", "published": "2024-12-12 21:29:00", "link": "http://arxiv.org/abs/2412.12175v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Dipper: Diversity in Prompts for Producing Large Language Model\n  Ensembles in Reasoning tasks", "abstract": "Large Language Models still encounter substantial challenges in reasoning\ntasks, especially for smaller models, which many users may be restricted to due\nto resource constraints (e.g. GPU memory restrictions). Inference-time methods\nto boost LLM performance, such as prompting methods to invoke certain reasoning\npathways in responses, have been shown effective in past works, though they\nlargely rely on sequential queries. The ensemble method, which consists of\nmultiple constituent models running in parallel, is a promising approach to\nachieving better inference-time performance, especially given recent\ndevelopments that enabled significant speed-ups in LLM batch inference. In this\nwork, we propose a novel, training-free LLM ensemble framework where a single\nLLM model is fed an optimized, diverse set of prompts in parallel, effectively\nproducing an ensemble at inference time to achieve performance improvement in\nreasoning tasks. We empirically demonstrate that our method leads to\nsignificant gains on math reasoning tasks, e.g., on MATH, where our ensemble\nconsisting of a few small models (e.g., three Qwen2-MATH-1.5B-it models) can\noutperform a larger model (e.g., Qwen2-MATH-7B-it).", "published": "2024-12-12 17:49:05", "link": "http://arxiv.org/abs/2412.15238v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CL"}
{"title": "CSSinger: End-to-End Chunkwise Streaming Singing Voice Synthesis System\n  Based on Conditional Variational Autoencoder", "abstract": "Singing Voice Synthesis (SVS) aims to generate singing voices of high\nfidelity and expressiveness. Conventional SVS systems usually utilize an\nacoustic model to transform a music score into acoustic features, followed by a\nvocoder to reconstruct the singing voice. It was recently shown that end-to-end\nmodeling is effective in the fields of SVS and Text to Speech (TTS). In this\nwork, we thus present a fully end-to-end SVS method together with a chunkwise\nstreaming inference to address the latency issue for practical usages. Note\nthat this is the first attempt to fully implement end-to-end streaming audio\nsynthesis using latent representations in VAE. We have made specific\nimprovements to enhance the performance of streaming SVS using latent\nrepresentations. Experimental results demonstrate that the proposed method\nachieves synthesized audio with high expressiveness and pitch accuracy in both\nstreaming SVS and TTS tasks.", "published": "2024-12-12 04:01:28", "link": "http://arxiv.org/abs/2412.08918v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Complex-Cycle-Consistent Diffusion Model for Monaural Speech Enhancement", "abstract": "In this paper, we present a novel diffusion model-based monaural speech\nenhancement method. Our approach incorporates the separate estimation of speech\nspectra's magnitude and phase in two diffusion networks. Throughout the\ndiffusion process, noise clips from real-world noise interferences are added\ngradually to the clean speech spectra and a noise-aware reverse process is\nproposed to learn how to generate both clean speech spectra and noise spectra.\nFurthermore, to fully leverage the intrinsic relationship between magnitude and\nphase, we introduce a complex-cycle-consistent (CCC) mechanism that uses the\nestimated magnitude to map the phase, and vice versa. We implement this\nalgorithm within a phase-aware speech enhancement diffusion model (SEDM). We\nconduct extensive experiments on public datasets to demonstrate the\neffectiveness of our method, highlighting the significant benefits of\nexploiting the intrinsic relationship between phase and magnitude information\nto enhance speech. The comparison to conventional diffusion models demonstrates\nthe superiority of SEDM.", "published": "2024-12-12 01:31:17", "link": "http://arxiv.org/abs/2412.08856v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Interpreting Graphic Notation with MusicLDM: An AI Improvisation of\n  Cornelius Cardew's Treatise", "abstract": "This work presents a novel method for composing and improvising music\ninspired by Cornelius Cardew's Treatise, using AI to bridge graphic notation\nand musical expression. By leveraging OpenAI's ChatGPT to interpret the\nabstract visual elements of Treatise, we convert these graphical images into\ndescriptive textual prompts. These prompts are then input into MusicLDM, a\npre-trained latent diffusion model designed for music generation. We introduce\na technique called \"outpainting,\" which overlaps sections of AI-generated music\nto create a seamless and cohesive composition. We demostrate a new perspective\non performing and interpreting graphic scores, showing how AI can transform\nvisual stimuli into sound and expand the creative possibilities in\ncontemporary/experimental music composition. Musical pieces are available at\nhttps://bit.ly/TreatiseAI", "published": "2024-12-12 05:08:36", "link": "http://arxiv.org/abs/2412.08944v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "EmoDubber: Towards High Quality and Emotion Controllable Movie Dubbing", "abstract": "Given a piece of text, a video clip, and a reference audio, the movie dubbing\ntask aims to generate speech that aligns with the video while cloning the\ndesired voice. The existing methods have two primary deficiencies: (1) They\nstruggle to simultaneously hold audio-visual sync and achieve clear\npronunciation; (2) They lack the capacity to express user-defined emotions. To\naddress these problems, we propose EmoDubber, an emotion-controllable dubbing\narchitecture that allows users to specify emotion type and emotional intensity\nwhile satisfying high-quality lip sync and pronunciation. Specifically, we\nfirst design Lip-related Prosody Aligning (LPA), which focuses on learning the\ninherent consistency between lip motion and prosody variation by duration level\ncontrastive learning to incorporate reasonable alignment. Then, we design\nPronunciation Enhancing (PE) strategy to fuse the video-level phoneme sequences\nby efficient conformer to improve speech intelligibility. Next, the speaker\nidentity adapting module aims to decode acoustics prior and inject the speaker\nstyle embedding. After that, the proposed Flow-based User Emotion Controlling\n(FUEC) is used to synthesize waveform by flow matching prediction network\nconditioned on acoustics prior. In this process, the FUEC determines the\ngradient direction and guidance scale based on the user's emotion instructions\nby the positive and negative guidance mechanism, which focuses on amplifying\nthe desired emotion while suppressing others. Extensive experimental results on\nthree benchmark datasets demonstrate favorable performance compared to several\nstate-of-the-art methods.", "published": "2024-12-12 06:39:49", "link": "http://arxiv.org/abs/2412.08988v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech-Forensics: Towards Comprehensive Synthetic Speech Dataset\n  Establishment and Analysis", "abstract": "Detecting synthetic from real speech is increasingly crucial due to the risks\nof misinformation and identity impersonation. While various datasets for\nsynthetic speech analysis have been developed, they often focus on specific\nareas, limiting their utility for comprehensive research. To fill this gap, we\npropose the Speech-Forensics dataset by extensively covering authentic,\nsynthetic, and partially forged speech samples that include multiple segments\nsynthesized by different high-quality algorithms. Moreover, we propose a\nTEmporal Speech LocalizaTion network, called TEST, aiming at simultaneously\nperforming authenticity detection, multiple fake segments localization, and\nsynthesis algorithms recognition, without any complex post-processing. TEST\neffectively integrates LSTM and Transformer to extract more powerful temporal\nspeech representations and utilizes dense prediction on multi-scale pyramid\nfeatures to estimate the synthetic spans. Our model achieves an average mAP of\n83.55% and an EER of 5.25% at the utterance level. At the segment level, it\nattains an EER of 1.07% and a 92.19% F1 score. These results highlight the\nmodel's robust capability for a comprehensive analysis of synthetic speech,\noffering a promising avenue for future research and practical applications in\nthis field.", "published": "2024-12-12 07:48:17", "link": "http://arxiv.org/abs/2412.09032v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "YingSound: Video-Guided Sound Effects Generation with Multi-modal\n  Chain-of-Thought Controls", "abstract": "Generating sound effects for product-level videos, where only a small amount\nof labeled data is available for diverse scenes, requires the production of\nhigh-quality sounds in few-shot settings. To tackle the challenge of limited\nlabeled data in real-world scenes, we introduce YingSound, a foundation model\ndesigned for video-guided sound generation that supports high-quality audio\ngeneration in few-shot settings. Specifically, YingSound consists of two major\nmodules. The first module uses a conditional flow matching transformer to\nachieve effective semantic alignment in sound generation across audio and\nvisual modalities. This module aims to build a learnable audio-visual\naggregator (AVA) that integrates high-resolution visual features with\ncorresponding audio features at multiple stages. The second module is developed\nwith a proposed multi-modal visual-audio chain-of-thought (CoT) approach to\ngenerate finer sound effects in few-shot settings. Finally, an\nindustry-standard video-to-audio (V2A) dataset that encompasses various\nreal-world scenarios is presented. We show that YingSound effectively generates\nhigh-quality synchronized sounds across diverse conditional inputs through\nautomated evaluations and human studies. Project Page:\n\\url{https://giantailab.github.io/yingsound/}", "published": "2024-12-12 10:55:57", "link": "http://arxiv.org/abs/2412.09168v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On the Generation and Removal of Speaker Adversarial Perturbation for\n  Voice-Privacy Protection", "abstract": "Neural networks are commonly known to be vulnerable to adversarial attacks\nmounted through subtle perturbation on the input data. Recent development in\nvoice-privacy protection has shown the positive use cases of the same technique\nto conceal speaker's voice attribute with additive perturbation signal\ngenerated by an adversarial network. This paper examines the reversibility\nproperty where an entity generating the adversarial perturbations is authorized\nto remove them and restore original speech (e.g., the speaker him/herself). A\nsimilar technique could also be used by an investigator to deanonymize a\nvoice-protected speech to restore criminals' identities in security and\nforensic analysis. In this setting, the perturbation generative module is\nassumed to be known in the removal process. To this end, a joint training of\nperturbation generation and removal modules is proposed. Experimental results\non the LibriSpeech dataset demonstrated that the subtle perturbations added to\nthe original speech can be predicted from the anonymized speech while achieving\nthe goal of privacy protection. By removing these perturbations from the\nanonymized sample, the original speech can be restored. Audio samples can be\nfound in \\url{https://voiceprivacy.github.io/Perturbation-Generation-Removal/}.", "published": "2024-12-12 11:46:07", "link": "http://arxiv.org/abs/2412.09195v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multimodal Music Generation with Explicit Bridges and Retrieval\n  Augmentation", "abstract": "Multimodal music generation aims to produce music from diverse input\nmodalities, including text, videos, and images. Existing methods use a common\nembedding space for multimodal fusion. Despite their effectiveness in other\nmodalities, their application in multimodal music generation faces challenges\nof data scarcity, weak cross-modal alignment, and limited controllability. This\npaper addresses these issues by using explicit bridges of text and music for\nmultimodal alignment. We introduce a novel method named Visuals Music Bridge\n(VMB). Specifically, a Multimodal Music Description Model converts visual\ninputs into detailed textual descriptions to provide the text bridge; a\nDual-track Music Retrieval module that combines broad and targeted retrieval\nstrategies to provide the music bridge and enable user control. Finally, we\ndesign an Explicitly Conditioned Music Generation framework to generate music\nbased on the two bridges. We conduct experiments on video-to-music,\nimage-to-music, text-to-music, and controllable music generation tasks, along\nwith experiments on controllability. The results demonstrate that VMB\nsignificantly enhances music quality, modality, and customization alignment\ncompared to previous methods. VMB sets a new standard for interpretable and\nexpressive multimodal music generation with applications in various multimedia\nfields. Demos and code are available at https://github.com/wbs2788/VMB.", "published": "2024-12-12 16:33:21", "link": "http://arxiv.org/abs/2412.09428v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Enriching Multimodal Sentiment Analysis through Textual Emotional\n  Descriptions of Visual-Audio Content", "abstract": "Multimodal Sentiment Analysis (MSA) stands as a critical research frontier,\nseeking to comprehensively unravel human emotions by amalgamating text, audio,\nand visual data. Yet, discerning subtle emotional nuances within audio and\nvideo expressions poses a formidable challenge, particularly when emotional\npolarities across various segments appear similar. In this paper, our objective\nis to spotlight emotion-relevant attributes of audio and visual modalities to\nfacilitate multimodal fusion in the context of nuanced emotional shifts in\nvisual-audio scenarios. To this end, we introduce DEVA, a progressive fusion\nframework founded on textual sentiment descriptions aimed at accentuating\nemotional features of visual-audio content. DEVA employs an Emotional\nDescription Generator (EDG) to transmute raw audio and visual data into\ntextualized sentiment descriptions, thereby amplifying their emotional\ncharacteristics. These descriptions are then integrated with the source data to\nyield richer, enhanced features. Furthermore, DEVA incorporates the Text-guided\nProgressive Fusion Module (TPF), leveraging varying levels of text as a core\nmodality guide. This module progressively fuses visual-audio minor modalities\nto alleviate disparities between text and visual-audio modalities. Experimental\nresults on widely used sentiment analysis benchmark datasets, including MOSI,\nMOSEI, and CH-SIMS, underscore significant enhancements compared to\nstate-of-the-art models. Moreover, fine-grained emotion experiments corroborate\nthe robust sensitivity of DEVA to subtle emotional variations.", "published": "2024-12-12 11:30:41", "link": "http://arxiv.org/abs/2412.10460v1", "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Comparative Analysis of Mel-Frequency Cepstral Coefficients and Wavelet\n  Based Audio Signal Processing for Emotion Detection and Mental Health\n  Assessment in Spoken Speech", "abstract": "The intersection of technology and mental health has spurred innovative\napproaches to assessing emotional well-being, particularly through\ncomputational techniques applied to audio data analysis. This study explores\nthe application of Convolutional Neural Network (CNN) and Long Short-Term\nMemory (LSTM) models on wavelet extracted features and Mel-frequency Cepstral\nCoefficients (MFCCs) for emotion detection from spoken speech. Data\naugmentation techniques, feature extraction, normalization, and model training\nwere conducted to evaluate the models' performance in classifying emotional\nstates. Results indicate that the CNN model achieved a higher accuracy of 61%\ncompared to the LSTM model's accuracy of 56%. Both models demonstrated better\nperformance in predicting specific emotions such as surprise and anger,\nleveraging distinct audio features like pitch and speed variations.\nRecommendations include further exploration of advanced data augmentation\ntechniques, combined feature extraction methods, and the integration of\nlinguistic analysis with speech characteristics for improved accuracy in mental\nhealth diagnostics. Collaboration for standardized dataset collection and\nsharing is recommended to foster advancements in affective computing and mental\nhealth care interventions.", "published": "2024-12-12 22:55:11", "link": "http://arxiv.org/abs/2412.10469v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multimodal Sentiment Analysis based on Video and Audio Inputs", "abstract": "Despite the abundance of current researches working on the sentiment analysis\nfrom videos and audios, finding the best model that gives the highest accuracy\nrate is still considered a challenge for researchers in this field. The main\nobjective of this paper is to prove the usability of emotion recognition models\nthat take video and audio inputs. The datasets used to train the models are the\nCREMA-D dataset for audio and the RAVDESS dataset for video. The fine-tuned\nmodels that been used are: Facebook/wav2vec2-large for audio and the\nGoogle/vivit-b-16x2-kinetics400 for video. The avarage of the probabilities for\neach emotion generated by the two previous models is utilized in the decision\nmaking framework. After disparity in the results, if one of the models gets\nmuch higher accuracy, another test framework is created. The methods used are\nthe Weighted Average method, the Confidence Level Threshold method, the Dynamic\nWeighting Based on Confidence method, and the Rule-Based Logic method. This\nlimited approach gives encouraging results that make future research into these\nmethods viable.", "published": "2024-12-12 14:42:10", "link": "http://arxiv.org/abs/2412.09317v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learned Compression for Compressed Learning", "abstract": "Modern sensors produce increasingly rich streams of high-resolution data. Due\nto resource constraints, machine learning systems discard the vast majority of\nthis information via resolution reduction. Compressed-domain learning allows\nmodels to operate on compact latent representations, allowing higher effective\nresolution for the same budget. However, existing compression systems are not\nideal for compressed learning. Linear transform coding and end-to-end learned\ncompression systems reduce bitrate, but do not uniformly reduce dimensionality;\nthus, they do not meaningfully increase efficiency. Generative autoencoders\nreduce dimensionality, but their adversarial or perceptual objectives lead to\nsignificant information loss. To address these limitations, we introduce WaLLoC\n(Wavelet Learned Lossy Compression), a neural codec architecture that combines\nlinear transform coding with nonlinear dimensionality-reducing autoencoders.\nWaLLoC sandwiches a shallow, asymmetric autoencoder and entropy bottleneck\nbetween an invertible wavelet packet transform. Across several key metrics,\nWaLLoC outperforms the autoencoders used in state-of-the-art latent diffusion\nmodels. WaLLoC does not require perceptual or adversarial losses to represent\nhigh-frequency detail, providing compatibility with modalities beyond RGB\nimages and stereo audio. WaLLoC's encoder consists almost entirely of linear\noperations, making it exceptionally efficient and suitable for mobile\ncomputing, remote sensing, and learning directly from compressed data. We\ndemonstrate WaLLoC's capability for compressed-domain learning across several\ntasks, including image classification, colorization, document understanding,\nand music source separation. Our code, experiments, and pre-trained audio and\nimage codecs are available at https://ut-sysml.org/walloc", "published": "2024-12-12 16:09:57", "link": "http://arxiv.org/abs/2412.09405v1", "categories": ["eess.IV", "cs.CV", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "eess.IV"}
