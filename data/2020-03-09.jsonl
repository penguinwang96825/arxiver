{"title": "Shallow Discourse Annotation for Chinese TED Talks", "abstract": "Text corpora annotated with language-related properties are an important\nresource for the development of Language Technology. The current work\ncontributes a new resource for Chinese Language Technology and for\nChinese-English translation, in the form of a set of TED talks (some originally\ngiven in English, some in Chinese) that have been annotated with discourse\nrelations in the style of the Penn Discourse TreeBank, adapted to properties of\nChinese text that are not present in English. The resource is currently unique\nin annotating discourse-level properties of planned spoken monologues rather\nthan of written text. An inter-annotator agreement study demonstrates that the\nannotation scheme is able to achieve highly reliable results.", "published": "2020-03-09 10:50:16", "link": "http://arxiv.org/abs/2003.04032v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence Analogies: Exploring Linguistic Relationships and Regularities\n  in Sentence Embeddings", "abstract": "While important properties of word vector representations have been studied\nextensively, far less is known about the properties of sentence vector\nrepresentations. Word vectors are often evaluated by assessing to what degree\nthey exhibit regularities with regard to relationships of the sort considered\nin word analogies. In this paper, we investigate to what extent commonly used\nsentence vector representation spaces as well reflect certain kinds of\nregularities. We propose a number of schemes to induce evaluation data, based\non lexical analogy data as well as semantic relationships between sentences.\nOur experiments consider a wide range of sentence embedding methods, including\nones based on BERT-style contextual embeddings. We find that different models\ndiffer substantially in their ability to reflect such regularities.", "published": "2020-03-09 10:58:38", "link": "http://arxiv.org/abs/2003.04036v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multi-Source Entity-Level Sentiment Corpus for the Financial Domain:\n  The FinLin Corpus", "abstract": "We introduce FinLin, a novel corpus containing investor reports, company\nreports, news articles, and microblogs from StockTwits, targeting multiple\nentities stemming from the automobile industry and covering a 3-month period.\nFinLin was annotated with a sentiment score and a relevance score in the range\n[-1.0, 1.0] and [0.0, 1.0], respectively. The annotations also include the text\nspans selected for the sentiment, thus, providing additional insight into the\nannotators' reasoning. Overall, FinLin aims to complement the current knowledge\nby providing a novel and publicly available financial sentiment corpus and to\nfoster research on the topic of financial sentiment analysis and potential\napplications in behavioural science.", "published": "2020-03-09 12:34:48", "link": "http://arxiv.org/abs/2003.04073v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Matching Text with Deep Mutual Information Estimation", "abstract": "Text matching is a core natural language processing research problem. How to\nretain sufficient information on both content and structure information is one\nimportant challenge. In this paper, we present a neural approach for\ngeneral-purpose text matching with deep mutual information estimation\nincorporated. Our approach, Text matching with Deep Info Max (TIM), is\nintegrated with a procedure of unsupervised learning of representations by\nmaximizing the mutual information between text matching neural network's input\nand output. We use both global and local mutual information to learn text\nrepresentations. We evaluate our text matching approach on several tasks\nincluding natural language inference, paraphrase identification, and answer\nselection. Compared to the state-of-the-art approaches, the experiments show\nthat our method integrated with mutual information estimation learns better\ntext representation and achieves better experimental results of text matching\ntasks without exploiting pretraining on external data.", "published": "2020-03-09 15:25:37", "link": "http://arxiv.org/abs/2003.11521v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tigrinya Neural Machine Translation with Transfer Learning for\n  Humanitarian Response", "abstract": "We report our experiments in building a domain-specific Tigrinya-to-English\nneural machine translation system. We use transfer learning from other Ge'ez\nscript languages and report an improvement of 1.3 BLEU points over a classic\nneural baseline. We publish our development pipeline as an open-source library\nand also provide a demonstration application.", "published": "2020-03-09 10:34:25", "link": "http://arxiv.org/abs/2003.11523v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overview of the CCKS 2019 Knowledge Graph Evaluation Track: Entity,\n  Relation, Event and QA", "abstract": "Knowledge graph models world knowledge as concepts, entities, and the\nrelationships between them, which has been widely used in many real-world\ntasks. CCKS 2019 held an evaluation track with 6 tasks and attracted more than\n1,600 teams. In this paper, we give an overview of the knowledge graph\nevaluation tract at CCKS 2019. By reviewing the task definition, successful\nmethods, useful resources, good strategies and research challenges associated\nwith each task in CCKS 2019, this paper can provide a helpful reference for\ndeveloping knowledge graph applications and conducting future knowledge graph\nresearches.", "published": "2020-03-09 00:32:13", "link": "http://arxiv.org/abs/2003.03875v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "An Empirical Investigation of Pre-Trained Transformer Language Models\n  for Open-Domain Dialogue Generation", "abstract": "We present an empirical investigation of pre-trained Transformer-based\nauto-regressive language models for the task of open-domain dialogue\ngeneration. Training paradigm of pre-training and fine-tuning is employed to\nconduct the parameter learning. Corpora of News and Wikipedia in Chinese and\nEnglish are collected for the pre-training stage respectively. Dialogue context\nand response are concatenated into a single sequence utilized as the input of\nthe models during the fine-tuning stage. A weighted joint prediction paradigm\nfor both context and response is designed to evaluate the performance of models\nwith or without the loss term for context prediction. Various of decoding\nstrategies such as greedy search, beam search, top-k sampling, etc. are\nemployed to conduct the response text generation. Extensive experiments are\nconducted on the typical single-turn and multi-turn dialogue corpora such as\nWeibo, Douban, Reddit, DailyDialog, and Persona-Chat. Detailed numbers of\nautomatic evaluation metrics on relevance and diversity of the generated\nresults for the languages models as well as the baseline approaches are\nreported.", "published": "2020-03-09 15:20:21", "link": "http://arxiv.org/abs/2003.04195v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Combining Pretrained High-Resource Embeddings and Subword\n  Representations for Low-Resource Languages", "abstract": "The contrast between the need for large amounts of data for current Natural\nLanguage Processing (NLP) techniques, and the lack thereof, is accentuated in\nthe case of African languages, most of which are considered low-resource. To\nhelp circumvent this issue, we explore techniques exploiting the qualities of\nmorphologically rich languages (MRLs), while leveraging pretrained word vectors\nin well-resourced languages. In our exploration, we show that a meta-embedding\napproach combining both pretrained and morphologically-informed word embeddings\nperforms best in the downstream task of Xhosa-English translation.", "published": "2020-03-09 21:30:55", "link": "http://arxiv.org/abs/2003.04419v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Vector logic allows counterfactual virtualization by The Square Root of\n  NOT", "abstract": "In this work we investigate the representation of counterfactual conditionals\nusing the vector logic, a matrix-vectors formalism for logical functions and\ntruth values. Inside this formalism, the counterfactuals can be transformed in\ncomplex matrices preprocessing an implication matrix with one of the square\nroots of NOT, a complex matrix. This mathematical approach puts in evidence the\nvirtual character of the counterfactuals. This happens because this\nrepresentation produces a valuation of a counterfactual that is the\nsuperposition of the two opposite truth values weighted, respectively, by two\ncomplex conjugated coefficients. This result shows that this procedure gives an\nuncertain evaluation projected on the complex domain. After this basic\nrepresentation, the judgment of the plausibility of a given counterfactual\nallows us to shift the decision towards an acceptance or a refusal. This shift\nis the result of applying for a second time one of the two square roots of NOT.", "published": "2020-03-09 20:56:36", "link": "http://arxiv.org/abs/2003.11519v3", "categories": ["cs.CL", "cs.LO", "03B65", "A.m"], "primary_category": "cs.CL"}
{"title": "TEDL: A Text Encryption Method Based on Deep Learning", "abstract": "Recent years have seen an increasing emphasis on information security, and\nvarious encryption methods have been proposed. However, for symmetric\nencryption methods, the well-known encryption techniques still rely on the key\nspace to guarantee security and suffer from frequent key updating. Aiming to\nsolve those problems, this paper proposes a novel text encryption method based\non deep learning called TEDL, where the secret key includes hyperparameters in\ndeep learning model and the core step of encryption is transforming input data\ninto weights trained under hyperparameters. Firstly, both communication parties\nestablish a word vector table by training a deep learning model according to\nspecified hyperparameters. Then, a self-update codebook is constructed on the\nword vector table with the SHA-256 function and other tricks. When\ncommunication starts, encryption and decryption are equivalent to indexing and\ninverted indexing on the codebook, respectively, thus achieving the\ntransformation between plaintext and ciphertext. Results of experiments and\nrelevant analyses show that TEDL performs well for security, efficiency,\ngenerality, and has a lower demand for the frequency of key redistribution.\nEspecially, as a supplement to current encryption methods, the time-consuming\nprocess of constructing a codebook increases the difficulty of brute-force\nattacks while not degrade the communication efficiency.", "published": "2020-03-09 11:04:36", "link": "http://arxiv.org/abs/2003.04038v2", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Neuro-symbolic Architectures for Context Understanding", "abstract": "Computational context understanding refers to an agent's ability to fuse\ndisparate sources of information for decision-making and is, therefore,\ngenerally regarded as a prerequisite for sophisticated machine reasoning\ncapabilities, such as in artificial intelligence (AI). Data-driven and\nknowledge-driven methods are two classical techniques in the pursuit of such\nmachine sense-making capability. However, while data-driven methods seek to\nmodel the statistical regularities of events by making observations in the\nreal-world, they remain difficult to interpret and they lack mechanisms for\nnaturally incorporating external knowledge. Conversely, knowledge-driven\nmethods, combine structured knowledge bases, perform symbolic reasoning based\non axiomatic principles, and are more interpretable in their inferential\nprocessing; however, they often lack the ability to estimate the statistical\nsalience of an inference. To combat these issues, we propose the use of hybrid\nAI methodology as a general framework for combining the strengths of both\napproaches. Specifically, we inherit the concept of neuro-symbolism as a way of\nusing knowledge-bases to guide the learning progress of deep neural networks.\nWe further ground our discussion in two applications of neuro-symbolism and, in\nboth cases, show that our systems maintain interpretability while achieving\ncomparable performance, relative to the state-of-the-art.", "published": "2020-03-09 15:04:07", "link": "http://arxiv.org/abs/2003.04707v1", "categories": ["cs.AI", "cs.CL", "cs.SC"], "primary_category": "cs.AI"}
{"title": "KGvec2go -- Knowledge Graph Embeddings as a Service", "abstract": "In this paper, we present KGvec2go, a Web API for accessing and consuming\ngraph embeddings in a light-weight fashion in downstream applications.\nCurrently, we serve pre-trained embeddings for four knowledge graphs. We\nintroduce the service and its usage, and we show further that the trained\nmodels have semantic value by evaluating them on multiple semantic benchmarks.\nThe evaluation also reveals that the combination of multiple models can lead to\na better outcome than the best individual model.", "published": "2020-03-09 12:57:10", "link": "http://arxiv.org/abs/2003.05809v1", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Joint Multiclass Debiasing of Word Embeddings", "abstract": "Bias in Word Embeddings has been a subject of recent interest, along with\nefforts for its reduction. Current approaches show promising progress towards\ndebiasing single bias dimensions such as gender or race. In this paper, we\npresent a joint multiclass debiasing approach that is capable of debiasing\nmultiple bias dimensions simultaneously. In that direction, we present two\napproaches, HardWEAT and SoftWEAT, that aim to reduce biases by minimizing the\nscores of the Word Embeddings Association Test (WEAT). We demonstrate the\nviability of our methods by debiasing Word Embeddings on three classes of\nbiases (religion, gender and race) in three different publicly available word\nembeddings and show that our concepts can both reduce or even completely\neliminate bias, while maintaining meaningful relationships between vectors in\nword embeddings. Our work strengthens the foundation for more unbiased neural\nrepresentations of textual data.", "published": "2020-03-09 22:06:37", "link": "http://arxiv.org/abs/2003.11520v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Enhancing End-to-End Multi-channel Speech Separation via Spatial Feature\n  Learning", "abstract": "Hand-crafted spatial features (e.g., inter-channel phase difference, IPD)\nplay a fundamental role in recent deep learning based multi-channel speech\nseparation (MCSS) methods. However, these manually designed spatial features\nare hard to incorporate into the end-to-end optimized MCSS framework. In this\nwork, we propose an integrated architecture for learning spatial features\ndirectly from the multi-channel speech waveforms within an end-to-end speech\nseparation framework. In this architecture, time-domain filters spanning signal\nchannels are trained to perform adaptive spatial filtering. These filters are\nimplemented by a 2d convolution (conv2d) layer and their parameters are\noptimized using a speech separation objective function in a purely data-driven\nfashion. Furthermore, inspired by the IPD formulation, we design a conv2d\nkernel to compute the inter-channel convolution differences (ICDs), which are\nexpected to provide the spatial cues that help to distinguish the directional\nsources. Evaluation results on simulated multi-channel reverberant WSJ0 2-mix\ndataset demonstrate that our proposed ICD based MCSS model improves the overall\nsignal-to-distortion ratio by 10.4% over the IPD based MCSS model.", "published": "2020-03-09 05:28:20", "link": "http://arxiv.org/abs/2003.03927v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Tackling real noisy reverberant meetings with all-neural source\n  separation, counting, and diarization system", "abstract": "Automatic meeting analysis is an essential fundamental technology required to\nlet, e.g. smart devices follow and respond to our conversations. To achieve an\noptimal automatic meeting analysis, we previously proposed an all-neural\napproach that jointly solves source separation, speaker diarization and source\ncounting problems in an optimal way (in a sense that all the 3 tasks can be\njointly optimized through error back-propagation). It was shown that the method\ncould well handle simulated clean (noiseless and anechoic) dialog-like data,\nand achieved very good performance in comparison with several conventional\nmethods. However, it was not clear whether such all-neural approach would be\nsuccessfully generalized to more complicated real meeting data containing more\nspontaneously-speaking speakers, severe noise and reverberation, and how it\nperforms in comparison with the state-of-the-art systems in such scenarios. In\nthis paper, we first consider practical issues required for improving the\nrobustness of the all-neural approach, and then experimentally show that, even\nin real meeting scenarios, the all-neural approach can perform effective speech\nenhancement, and simultaneously outperform state-of-the-art systems.", "published": "2020-03-09 09:25:38", "link": "http://arxiv.org/abs/2003.03987v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving noise robust automatic speech recognition with single-channel\n  time-domain enhancement network", "abstract": "With the advent of deep learning, research on noise-robust automatic speech\nrecognition (ASR) has progressed rapidly. However, ASR performance in noisy\nconditions of single-channel systems remains unsatisfactory. Indeed, most\nsingle-channel speech enhancement (SE) methods (denoising) have brought only\nlimited performance gains over state-of-the-art ASR back-end trained on\nmulti-condition training data. Recently, there has been much research on neural\nnetwork-based SE methods working in the time-domain showing levels of\nperformance never attained before. However, it has not been established whether\nthe high enhancement performance achieved by such time-domain approaches could\nbe translated into ASR. In this paper, we show that a single-channel\ntime-domain denoising approach can significantly improve ASR performance,\nproviding more than 30 % relative word error reduction over a strong ASR\nback-end on the real evaluation data of the single-channel track of the CHiME-4\ndataset. These positive results demonstrate that single-channel noise reduction\ncan still improve ASR performance, which should open the door to more research\nin that direction.", "published": "2020-03-09 09:36:31", "link": "http://arxiv.org/abs/2003.03998v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Toward Cross-Domain Speech Recognition with End-to-End Models", "abstract": "In the area of multi-domain speech recognition, research in the past focused\non hybrid acoustic models to build cross-domain and domain-invariant speech\nrecognition systems. In this paper, we empirically examine the difference in\nbehavior between hybrid acoustic models and neural end-to-end systems when\nmixing acoustic training data from several domains. For these experiments we\ncomposed a multi-domain dataset from public sources, with the different domains\nin the corpus covering a wide variety of topics and acoustic conditions such as\ntelephone conversations, lectures, read speech and broadcast news. We show that\nfor the hybrid models, supplying additional training data from other domains\nwith mismatched acoustic conditions does not increase the performance on\nspecific domains. However, our end-to-end models optimized with sequence-based\ncriterion generalize better than the hybrid models on diverse domains. In term\nof word-error-rate performance, our experimental acoustic-to-word and\nattention-based models trained on multi-domain dataset reach the performance of\ndomain-specific long short-term memory (LSTM) hybrid models, thus resulting in\nmulti-domain speech recognition systems that do not suffer in performance over\ndomain specific ones. Moreover, the use of neural end-to-end models eliminates\nthe need of domain-adapted language models during recognition, which is a great\nadvantage when the input domain is unknown.", "published": "2020-03-09 15:19:53", "link": "http://arxiv.org/abs/2003.04194v1", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Semantic Object Prediction and Spatial Sound Super-Resolution with\n  Binaural Sounds", "abstract": "Humans can robustly recognize and localize objects by integrating visual and\nauditory cues. While machines are able to do the same now with images, less\nwork has been done with sounds. This work develops an approach for dense\nsemantic labelling of sound-making objects, purely based on binaural sounds. We\npropose a novel sensor setup and record a new audio-visual dataset of street\nscenes with eight professional binaural microphones and a 360 degree camera.\nThe co-existence of visual and audio cues is leveraged for supervision\ntransfer. In particular, we employ a cross-modal distillation framework that\nconsists of a vision `teacher' method and a sound `student' method -- the\nstudent method is trained to generate the same results as the teacher method.\nThis way, the auditory system can be trained without using human annotations.\nWe also propose two auxiliary tasks namely, a) a novel task on Spatial Sound\nSuper-resolution to increase the spatial resolution of sounds, and b) dense\ndepth prediction of the scene. We then formulate the three tasks into one\nend-to-end trainable multi-tasking network aiming to boost the overall\nperformance. Experimental results on the dataset show that 1) our method\nachieves promising results for semantic prediction and the two auxiliary tasks;\nand 2) the three tasks are mutually beneficial -- training them together\nachieves the best performance and 3) the number and orientations of microphones\nare both important. The data and code will be released to facilitate the\nresearch in this new direction.", "published": "2020-03-09 15:49:01", "link": "http://arxiv.org/abs/2003.04210v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Deep Neural Networks for Automatic Speech Processing: A Survey from\n  Large Corpora to Limited Data", "abstract": "Most state-of-the-art speech systems are using Deep Neural Networks (DNNs).\nThose systems require a large amount of data to be learned. Hence, learning\nstate-of-the-art frameworks on under-resourced speech languages/problems is a\ndifficult task. Problems could be the limited amount of data for impaired\nspeech. Furthermore, acquiring more data and/or expertise is time-consuming and\nexpensive. In this paper we position ourselves for the following speech\nprocessing tasks: Automatic Speech Recognition, speaker identification and\nemotion recognition. To assess the problem of limited data, we firstly\ninvestigate state-of-the-art Automatic Speech Recognition systems as it\nrepresents the hardest tasks (due to the large variability in each language).\nNext, we provide an overview of techniques and tasks requiring fewer data. In\nthe last section we investigate few-shot techniques as we interpret\nunder-resourced speech as a few-shot problem. In that sense we propose an\noverview of few-shot techniques and perspectives of using such techniques for\nthe focused speech problems in this survey. It occurs that the reviewed\ntechniques are not well adapted for large datasets. Nevertheless, some\npromising results from the literature encourage the usage of such techniques\nfor speech processing.", "published": "2020-03-09 16:26:30", "link": "http://arxiv.org/abs/2003.04241v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Unsupervised Style and Content Separation by Minimizing Mutual\n  Information for Speech Synthesis", "abstract": "We present a method to generate speech from input text and a style vector\nthat is extracted from a reference speech signal in an unsupervised manner,\ni.e., no style annotation, such as speaker information, is required. Existing\nunsupervised methods, during training, generate speech by computing style from\nthe corresponding ground truth sample and use a decoder to combine the style\nvector with the input text. Training the model in such a way leaks content\ninformation into the style vector. The decoder can use the leaked content and\nignore some of the input text to minimize the reconstruction loss. At inference\ntime, when the reference speech does not match the content input, the output\nmay not contain all of the content of the input text. We refer to this problem\nas \"content leakage\", which we address by explicitly estimating and minimizing\nthe mutual information between the style and the content through an adversarial\ntraining formulation. We call our method MIST - Mutual Information based Style\nContent Separation. The main goal of the method is to preserve the input\ncontent in the synthesized speech signal, which we measure by the word error\nrate (WER) and show substantial improvements over state-of-the-art unsupervised\nspeech synthesis methods.", "published": "2020-03-09 23:47:41", "link": "http://arxiv.org/abs/2003.06227v1", "categories": ["eess.AS", "cs.CV", "cs.IT", "cs.LG", "cs.SD", "math.IT"], "primary_category": "eess.AS"}
