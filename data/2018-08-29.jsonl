{"title": "Mapping Language to Code in Programmatic Context", "abstract": "Source code is rarely written in isolation. It depends significantly on the\nprogrammatic context, such as the class that the code would reside in. To study\nthis phenomenon, we introduce the task of generating class member functions\ngiven English documentation and the programmatic context provided by the rest\nof the class. This task is challenging because the desired code can vary\ngreatly depending on the functionality the class provides (e.g., a sort\nfunction may or may not be available when we are asked to \"return the smallest\nelement\" in a particular member variable list). We introduce CONCODE, a new\nlarge dataset with over 100,000 examples consisting of Java classes from online\ncode repositories, and develop a new encoder-decoder architecture that models\nthe interaction between the method documentation and the class environment. We\nalso present a detailed error analysis suggesting that there is significant\nroom for future work on this task.", "published": "2018-08-29 00:08:25", "link": "http://arxiv.org/abs/1808.09588v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Task Identification of Entities, Relations, and Coreference for\n  Scientific Knowledge Graph Construction", "abstract": "We introduce a multi-task setup of identifying and classifying entities,\nrelations, and coreference clusters in scientific articles. We create SciERC, a\ndataset that includes annotations for all three tasks and develop a unified\nframework called Scientific Information Extractor (SciIE) for with shared span\nrepresentations. The multi-task setup reduces cascading errors between tasks\nand leverages cross-sentence relations through coreference links. Experiments\nshow that our multi-task model outperforms previous models in scientific\ninformation extraction without using any domain-specific features. We further\nshow that the framework supports construction of a scientific knowledge graph,\nwhich we use to analyze information in scientific literature.", "published": "2018-08-29 01:53:12", "link": "http://arxiv.org/abs/1808.09602v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decoupling Strategy and Generation in Negotiation Dialogues", "abstract": "We consider negotiation settings in which two agents use natural language to\nbargain on goods. Agents need to decide on both high-level strategy (e.g.,\nproposing \\$50) and the execution of that strategy (e.g., generating \"The bike\nis brand new. Selling for just \\$50.\"). Recent work on negotiation trains\nneural models, but their end-to-end nature makes it hard to control their\nstrategy, and reinforcement learning tends to lead to degenerate solutions. In\nthis paper, we propose a modular approach based on coarse di- alogue acts\n(e.g., propose(price=50)) that decouples strategy and generation. We show that\nwe can flexibly set the strategy using supervised learning, reinforcement\nlearning, or domain-specific knowledge without degeneracy, while our\nretrieval-based generation can maintain context-awareness and produce diverse\nutterances. We test our approach on the recently proposed DEALORNODEAL game,\nand we also collect a richer dataset based on real items on Craigslist. Human\nevaluation shows that our systems achieve higher task success rate and more\nhuman-like negotiation behavior than previous approaches.", "published": "2018-08-29 04:59:15", "link": "http://arxiv.org/abs/1808.09637v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Tree-Based Neural Sentence Modeling", "abstract": "Neural networks with tree-based sentence encoders have shown better results\non many downstream tasks. Most of existing tree-based encoders adopt syntactic\nparsing trees as the explicit structure prior. To study the effectiveness of\ndifferent tree structures, we replace the parsing trees with trivial trees\n(i.e., binary balanced tree, left-branching tree and right-branching tree) in\nthe encoders. Though trivial trees contain no syntactic information, those\nencoders get competitive or even better results on all of the ten downstream\ntasks we investigated. This surprising result indicates that explicit syntax\nguidance may not be the main contributor to the superior performances of\ntree-based neural sentence modeling. Further analysis show that tree modeling\ngives better results when crucial words are closer to the final representation.\nAdditional experiments give more clues on how to design an effective tree-based\nencoder. Our code is open-source and available at\nhttps://github.com/ExplorerFreda/TreeEnc.", "published": "2018-08-29 05:32:17", "link": "http://arxiv.org/abs/1808.09644v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Metaphor Detection in Context", "abstract": "We present end-to-end neural models for detecting metaphorical word use in\ncontext. We show that relatively standard BiLSTM models which operate on\ncomplete sentences work well in this setting, in comparison to previous work\nthat used more restricted forms of linguistic context. These models establish a\nnew state-of-the-art on existing verb metaphor detection benchmarks, and show\nstrong performance on jointly predicting the metaphoricity of all words in a\nrunning text.", "published": "2018-08-29 06:32:47", "link": "http://arxiv.org/abs/1808.09653v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Operation Sequence Model for Explainable Neural Machine Translation", "abstract": "We propose to achieve explainable neural machine translation (NMT) by\nchanging the output representation to explain itself. We present a novel\napproach to NMT which generates the target sentence by monotonically walking\nthrough the source sentence. Word reordering is modeled by operations which\nallow setting markers in the target sentence and move a target-side write head\nbetween those markers. In contrast to many modern neural models, our system\nemits explicit word alignment information which is often crucial to practical\nmachine translation as it improves explainability. Our technique can outperform\na plain text system in terms of BLEU score under the recent Transformer\narchitecture on Japanese-English and Portuguese-English, and is within 0.5 BLEU\ndifference on Spanish-English.", "published": "2018-08-29 08:56:42", "link": "http://arxiv.org/abs/1808.09688v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What can we learn from Semantic Tagging?", "abstract": "We investigate the effects of multi-task learning using the recently\nintroduced task of semantic tagging. We employ semantic tagging as an auxiliary\ntask for three different NLP tasks: part-of-speech tagging, Universal\nDependency parsing, and Natural Language Inference. We compare full neural\nnetwork sharing, partial neural network sharing, and what we term the learning\nwhat to share setting where negative transfer between tasks is less likely. Our\nfindings show considerable improvements for all tasks, particularly in the\nlearning what to share setting, which shows consistent gains across all tasks.", "published": "2018-08-29 10:30:03", "link": "http://arxiv.org/abs/1808.09716v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Characterizing the Influence of Features on Reading Difficulty\n  Estimation for Non-native Readers", "abstract": "In recent years, the number of people studying English as a second language\n(ESL) has surpassed the number of native speakers. Recent work have\ndemonstrated the success of providing personalized content based on reading\ndifficulty, such as information retrieval and summarization. However, almost\nall prior studies of reading difficulty are designed for native speakers,\nrather than non-native readers. In this study, we investigate various features\nfor ESL readers, by conducting a linear regression to estimate the reading\nlevel of English language sources. This estimation is based not only on the\ncomplexity of lexical and syntactic features, but also several novel concepts,\nincluding the age of word and grammar acquisition from several sources, word\nsense from WordNet, and the implicit relation between sentences. By employing\nBayesian Information Criterion (BIC) to select the optimal model, we find that\nthe combination of the number of words, the age of word acquisition and the\nheight of the parsing tree generate better results than alternative competing\nmodels. Thus, our results show that proposed second language reading difficulty\nestimation outperforms other first language reading difficulty estimations.", "published": "2018-08-29 10:32:24", "link": "http://arxiv.org/abs/1808.09718v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying the sentiment styles of YouTube's vloggers", "abstract": "Vlogs provide a rich public source of data in a novel setting. This paper\nexamined the continuous sentiment styles employed in 27,333 vlogs using a\ndynamic intra-textual approach to sentiment analysis. Using unsupervised\nclustering, we identified seven distinct continuous sentiment trajectories\ncharacterized by fluctuations of sentiment throughout a vlog's narrative time.\nWe provide a taxonomy of these seven continuous sentiment styles and found that\nvlogs whose sentiment builds up towards a positive ending are the most\nprevalent in our sample. Gender was associated with preferences for different\ncontinuous sentiment trajectories. This paper discusses the findings with\nrespect to previous work and concludes with an outlook towards possible uses of\nthe corpus, method and findings of this paper for related areas of research.", "published": "2018-08-29 10:45:45", "link": "http://arxiv.org/abs/1808.09722v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distant Supervision from Disparate Sources for Low-Resource\n  Part-of-Speech Tagging", "abstract": "We introduce DsDs: a cross-lingual neural part-of-speech tagger that learns\nfrom disparate sources of distant supervision, and realistically scales to\nhundreds of low-resource languages. The model exploits annotation projection,\ninstance selection, tag dictionaries, morphological lexicons, and distributed\nrepresentations, all in a uniform framework. The approach is simple, yet\nsurprisingly effective, resulting in a new state of the art without access to\nany gold annotated data.", "published": "2018-08-29 11:30:22", "link": "http://arxiv.org/abs/1808.09733v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Notes on Deep Learning for NLP", "abstract": "My notes on Deep Learning for NLP.", "published": "2018-08-29 12:58:45", "link": "http://arxiv.org/abs/1808.09772v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Cross-Lingual Named Entity Recognition with Minimal Resources", "abstract": "For languages with no annotated resources, unsupervised transfer of natural\nlanguage processing models such as named-entity recognition (NER) from\nresource-rich languages would be an appealing capability. However, differences\nin words and word order across languages make it a challenging problem. To\nimprove mapping of lexical items across languages, we propose a method that\nfinds translations based on bilingual word embeddings. To improve robustness to\nword order differences, we propose to use self-attention, which allows for a\ndegree of flexibility with respect to word order. We demonstrate that these\nmethods achieve state-of-the-art or competitive NER performance on commonly\ntested languages under a cross-lingual setting, with much lower resource\nrequirements than past approaches. We also evaluate the challenges of applying\nthese methods to Uyghur, a low-resource language.", "published": "2018-08-29 14:55:31", "link": "http://arxiv.org/abs/1808.09861v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Review Helpfulness Prediction with Embedding-Gated CNN", "abstract": "Product reviews, in the form of texts dominantly, significantly help\nconsumers finalize their purchasing decisions. Thus, it is important for\ne-commerce companies to predict review helpfulness to present and recommend\nreviews in a more informative manner. In this work, we introduce a\nconvolutional neural network model that is able to extract abstract features\nfrom multi-granularity representations. Inspired by the fact that different\nwords contribute to the meaning of a sentence differently, we consider to learn\nword-level embedding-gates for all the representations. Furthermore, as it is\ncommon that some product domains/categories have rich user reviews, other\ndomains not. To help domains with less sufficient data, we integrate our model\ninto a cross-domain relationship learning framework for effectively\ntransferring knowledge from other domains. Extensive experiments show that our\nmodel yields better performance than the existing methods.", "published": "2018-08-29 15:52:06", "link": "http://arxiv.org/abs/1808.09896v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Neural Model of Adaptation in Reading", "abstract": "It has been argued that humans rapidly adapt their lexical and syntactic\nexpectations to match the statistics of the current linguistic context. We\nprovide further support to this claim by showing that the addition of a simple\nadaptation mechanism to a neural language model improves our predictions of\nhuman reading times compared to a non-adaptive model. We analyze the\nperformance of the model on controlled materials from psycholinguistic\nexperiments and show that it adapts not only to lexical items but also to\nabstract syntactic structures.", "published": "2018-08-29 17:10:47", "link": "http://arxiv.org/abs/1808.09930v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Character-Based Neural Machine Translation with Capacity and\n  Compression", "abstract": "Translating characters instead of words or word-fragments has the potential\nto simplify the processing pipeline for neural machine translation (NMT), and\nimprove results by eliminating hyper-parameters and manual feature engineering.\nHowever, it results in longer sequences in which each symbol contains less\ninformation, creating both modeling and computational challenges. In this\npaper, we show that the modeling problem can be solved by standard\nsequence-to-sequence architectures of sufficient depth, and that deep models\noperating at the character level outperform identical models operating over\nword fragments. This result implies that alternative architectures for handling\ncharacter input are better viewed as methods for reducing computation time than\nas improved ways of modeling longer sequences. From this perspective, we\nevaluate several techniques for character-level NMT, verify that they do not\nmatch the performance of our deep character baseline model, and evaluate the\nperformance versus computation time tradeoffs they offer. Within this\nframework, we also perform the first evaluation for NMT of conditional\ncomputation over time, in which the model learns which timesteps can be\nskipped, rather than having them be dictated by a fixed schedule specified\nbefore training begins.", "published": "2018-08-29 17:46:50", "link": "http://arxiv.org/abs/1808.09943v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grammar Induction with Neural Language Models: An Unusual Replication", "abstract": "A substantial thread of recent work on latent tree learning has attempted to\ndevelop neural network models with parse-valued latent variables and train them\non non-parsing tasks, in the hope of having them discover interpretable tree\nstructure. In a recent paper, Shen et al. (2018) introduce such a model and\nreport near-state-of-the-art results on the target task of language modeling,\nand the first strong latent tree learning result on constituency parsing. In an\nattempt to reproduce these results, we discover issues that make the original\nresults hard to trust, including tuning and even training on what is\neffectively the test set. Here, we attempt to reproduce these results in a fair\nexperiment and to extend them to two new datasets. We find that the results of\nthis work are robust: All variants of the model under study outperform all\nlatent tree learning baselines, and perform competitively with symbolic grammar\ninduction systems. We find that this model represents the first empirical\nsuccess for latent tree learning, and that neural network language modeling\nwarrants further study as a setting for grammar induction.", "published": "2018-08-29 18:21:50", "link": "http://arxiv.org/abs/1808.10000v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Correcting Length Bias in Neural Machine Translation", "abstract": "We study two problems in neural machine translation (NMT). First, in beam\nsearch, whereas a wider beam should in principle help translation, it often\nhurts NMT. Second, NMT has a tendency to produce translations that are too\nshort. Here, we argue that these problems are closely related and both rooted\nin label bias. We show that correcting the brevity problem almost eliminates\nthe beam problem; we compare some commonly-used methods for doing this, finding\nthat a simple per-word reward works well; and we introduce a simple and quick\nway to tune this reward using the perceptron algorithm.", "published": "2018-08-29 18:33:11", "link": "http://arxiv.org/abs/1808.10006v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hard Non-Monotonic Attention for Character-Level Transduction", "abstract": "Character-level string-to-string transduction is an important component of\nvarious NLP tasks. The goal is to map an input string to an output string,\nwhere the strings may be of different lengths and have characters taken from\ndifferent alphabets. Recent approaches have used sequence-to-sequence models\nwith an attention mechanism to learn which parts of the input string the model\nshould focus on during the generation of the output string. Both soft attention\nand hard monotonic attention have been used, but hard non-monotonic attention\nhas only been used in other sequence modeling tasks such as image captioning\n(Xu et al., 2015), and has required a stochastic approximation to compute the\ngradient. In this work, we introduce an exact, polynomial-time algorithm for\nmarginalizing over the exponential number of non-monotonic alignments between\ntwo strings, showing that hard attention models can be viewed as neural\nreparameterizations of the classical IBM Model 1. We compare soft and hard\nnon-monotonic attention experimentally and find that the exact algorithm\nsignificantly improves performance over the stochastic approximation and\noutperforms soft attention. Code is available at https://github.\ncom/shijie-wu/neural-transducer.", "published": "2018-08-29 20:00:20", "link": "http://arxiv.org/abs/1808.10024v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieval-Based Neural Code Generation", "abstract": "In models to generate program source code from natural language, representing\nthis code in a tree structure has been a common approach. However, existing\nmethods often fail to generate complex code correctly due to a lack of ability\nto memorize large and complex structures. We introduce ReCode, a method based\non subtree retrieval that makes it possible to explicitly reference existing\ncode examples within a neural code generation model. First, we retrieve\nsentences that are similar to input sentences using a dynamic-programming-based\nsentence similarity scoring method. Next, we extract n-grams of action\nsequences that build the associated abstract syntax tree. Finally, we increase\nthe probability of actions that cause the retrieved n-gram action subtree to be\nin the predicted code. We show that our approach improves the performance on\ntwo code generation tasks by up to +2.6 BLEU.", "published": "2018-08-29 20:01:21", "link": "http://arxiv.org/abs/1808.10025v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Adaptive Transfer for Conversational Language Understanding", "abstract": "Conversational agents such as Alexa and Google Assistant constantly need to\nincrease their language understanding capabilities by adding new domains. A\nmassive amount of labeled data is required for training each new domain. While\ndomain adaptation approaches alleviate the annotation cost, prior approaches\nsuffer from increased training time and suboptimal concept alignments. To\ntackle this, we introduce a novel Zero-Shot Adaptive Transfer method for slot\ntagging that utilizes the slot description for transferring reusable concepts\nacross domains, and enjoys efficient training without any explicit concept\nalignments. Extensive experimentation over a dataset of 10 domains relevant to\nour commercial personal digital assistant shows that our model outperforms\nprevious state-of-the-art systems by a large margin, and achieves an even\nhigher improvement in the low data regime.", "published": "2018-08-29 22:57:55", "link": "http://arxiv.org/abs/1808.10059v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Development and Evaluation of a Personalized Computer-aided Question\n  Generation for English Learners to Improve Proficiency and Correct Mistakes", "abstract": "In the last several years, the field of computer assisted language learning\nhas increasingly focused on computer aided question generation. However, this\napproach often provides test takers with an exhaustive amount of questions that\nare not designed for any specific testing purpose. In this work, we present a\npersonalized computer aided question generation that generates multiple choice\nquestions at various difficulty levels and types, including vocabulary, grammar\nand reading comprehension. In order to improve the weaknesses of test takers,\nit selects questions depending on an estimated proficiency level and unclear\nconcepts behind incorrect responses. This results show that the students with\nthe personalized automatic quiz generation corrected their mistakes more\nfrequently than ones only with computer aided question generation. Moreover,\nstudents demonstrated the most progress between the pretest and post test and\ncorrectly answered more difficult questions. Finally, we investigated the\npersonalizing strategy and found that a student could make a significant\nprogress if the proposed system offered the vocabulary questions at the same\nlevel of his or her proficiency level, and if the grammar and reading\ncomprehension questions were at a level lower than his or her proficiency\nlevel.", "published": "2018-08-29 11:26:07", "link": "http://arxiv.org/abs/1808.09732v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Question Answering by Reasoning Across Documents with Graph\n  Convolutional Networks", "abstract": "Most research in reading comprehension has focused on answering questions\nbased on individual documents or even single paragraphs. We introduce a neural\nmodel which integrates and reasons relying on information spread within\ndocuments and across multiple documents. We frame it as an inference problem on\na graph. Mentions of entities are nodes of this graph while edges encode\nrelations between different mentions (e.g., within- and cross-document\nco-reference). Graph convolutional networks (GCNs) are applied to these graphs\nand trained to perform multi-step reasoning. Our Entity-GCN method is scalable\nand compact, and it achieves state-of-the-art results on a multi-document\nquestion answering dataset, WikiHop (Welbl et al., 2018).", "published": "2018-08-29 16:44:51", "link": "http://arxiv.org/abs/1808.09920v4", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Reinforcement Learning-driven Translation Model for Search-Oriented\n  Conversational Systems", "abstract": "Search-oriented conversational systems rely on information needs expressed in\nnatural language (NL). We focus here on the understanding of NL expressions for\nbuilding keyword-based queries. We propose a reinforcement-learning-driven\ntranslation model framework able to 1) learn the translation from NL\nexpressions to queries in a supervised way, and, 2) to overcome the lack of\nlarge-scale dataset by framing the translation model as a word selection\napproach and injecting relevance feedback in the learning process. Experiments\nare carried out on two TREC datasets and outline the effectiveness of our\napproach.", "published": "2018-08-29 15:11:49", "link": "http://arxiv.org/abs/1809.01495v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Learning Gender-Neutral Word Embeddings", "abstract": "Word embedding models have become a fundamental component in a wide range of\nNatural Language Processing (NLP) applications. However, embeddings trained on\nhuman-generated corpora have been demonstrated to inherit strong gender\nstereotypes that reflect social constructs. To address this concern, in this\npaper, we propose a novel training procedure for learning gender-neutral word\nembeddings. Our approach aims to preserve gender information in certain\ndimensions of word vectors while compelling other dimensions to be free of\ngender influence. Based on the proposed method, we generate a Gender-Neutral\nvariant of GloVe (GN-GloVe). Quantitative and qualitative experiments\ndemonstrate that GN-GloVe successfully isolates gender information without\nsacrificing the functionality of the embedding model.", "published": "2018-08-29 21:11:09", "link": "http://arxiv.org/abs/1809.01496v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Complexity and mission computability of adaptive computing systems", "abstract": "There is a subset of computational problems that are computable in polynomial\ntime for which an existing algorithm may not complete due to a lack of high\nperformance technology on a mission field. We define a subclass of\ndeterministic polynomial time complexity class called mission class, as many\npolynomial problems are not computable in mission time. By focusing on such\nsubclass of languages in the context for successful military applications, we\nalso discuss their computational and communicational constraints. We\ninvestigate feasible (non)linear models that will minimize energy and maximize\nmemory, efficiency, and computational power, and also provide an approximate\nsolution obtained within a pre-determined length of computation time using\nlimited resources so that an optimal solution to a language could be\ndetermined.", "published": "2018-08-29 00:03:04", "link": "http://arxiv.org/abs/1808.09586v1", "categories": ["math.OC", "cs.CC", "cs.CL", "cs.NI"], "primary_category": "math.OC"}
{"title": "Improved Semantic-Aware Network Embedding with Fine-Grained Word\n  Alignment", "abstract": "Network embeddings, which learn low-dimensional representations for each\nvertex in a large-scale network, have received considerable attention in recent\nyears. For a wide range of applications, vertices in a network are typically\naccompanied by rich textual information such as user profiles, paper abstracts,\netc. We propose to incorporate semantic features into network embeddings by\nmatching important words between text sequences for all pairs of vertices. We\nintroduce a word-by-word alignment framework that measures the compatibility of\nembeddings between word pairs, and then adaptively accumulates these alignment\nfeatures with a simple yet effective aggregation function. In experiments, we\nevaluate the proposed framework on three real-world benchmarks for downstream\ntasks, including link prediction and multi-label vertex classification. Results\ndemonstrate that our model outperforms state-of-the-art network embedding\nmethods by a large margin.", "published": "2018-08-29 04:23:02", "link": "http://arxiv.org/abs/1808.09633v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Voice Conversion Based on Cross-Domain Features Using Variational Auto\n  Encoders", "abstract": "An effective approach to non-parallel voice conversion (VC) is to utilize\ndeep neural networks (DNNs), specifically variational auto encoders (VAEs), to\nmodel the latent structure of speech in an unsupervised manner. A previous\nstudy has confirmed the ef- fectiveness of VAE using the STRAIGHT spectra for\nVC. How- ever, VAE using other types of spectral features such as mel- cepstral\ncoefficients (MCCs), which are related to human per- ception and have been\nwidely used in VC, have not been prop- erly investigated. Instead of using one\nspecific type of spectral feature, it is expected that VAE may benefit from\nusing multi- ple types of spectral features simultaneously, thereby improving\nthe capability of VAE for VC. To this end, we propose a novel VAE framework\n(called cross-domain VAE, CDVAE) for VC. Specifically, the proposed framework\nutilizes both STRAIGHT spectra and MCCs by explicitly regularizing multiple\nobjectives in order to constrain the behavior of the learned encoder and de-\ncoder. Experimental results demonstrate that the proposed CD- VAE framework\noutperforms the conventional VAE framework in terms of subjective tests.", "published": "2018-08-29 04:32:42", "link": "http://arxiv.org/abs/1808.09634v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Adapting Visual Question Answering Models for Enhancing Multimodal\n  Community Q&A Platforms", "abstract": "Question categorization and expert retrieval methods have been crucial for\ninformation organization and accessibility in community question & answering\n(CQA) platforms. Research in this area, however, has dealt with only the text\nmodality. With the increasing multimodal nature of web content, we focus on\nextending these methods for CQA questions accompanied by images. Specifically,\nwe leverage the success of representation learning for text and images in the\nvisual question answering (VQA) domain, and adapt the underlying concept and\narchitecture for automated category classification and expert retrieval on\nimage-based questions posted on Yahoo! Chiebukuro, the Japanese counterpart of\nYahoo! Answers.\n  To the best of our knowledge, this is the first work to tackle the\nmultimodality challenge in CQA, and to adapt VQA models for tasks on a more\necologically valid source of visual questions. Our analysis of the differences\nbetween visual QA and community QA data drives our proposal of novel\naugmentations of an attention method tailored for CQA, and use of auxiliary\ntasks for learning better grounding features. Our final model markedly\noutperforms the text-only and VQA model baselines for both tasks of\nclassification and expert retrieval on real-world multimodal CQA data.", "published": "2018-08-29 05:53:17", "link": "http://arxiv.org/abs/1808.09648v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "APRIL: Interactively Learning to Summarise by Combining Active\n  Preference Learning and Reinforcement Learning", "abstract": "We propose a method to perform automatic document summarisation without using\nreference summaries. Instead, our method interactively learns from users'\npreferences. The merit of preference-based interactive summarisation is that\npreferences are easier for users to provide than reference summaries. Existing\npreference-based interactive learning methods suffer from high sample\ncomplexity, i.e. they need to interact with the oracle for many rounds in order\nto converge. In this work, we propose a new objective function, which enables\nus to leverage active learning, preference learning and reinforcement learning\ntechniques in order to reduce the sample complexity. Both simulation and\nreal-user experiments suggest that our method significantly advances the state\nof the art. Our source code is freely available at\nhttps://github.com/UKPLab/emnlp2018-april.", "published": "2018-08-29 06:49:49", "link": "http://arxiv.org/abs/1808.09658v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Context Mover's Distance & Barycenters: Optimal Transport of Contexts\n  for Building Representations", "abstract": "We present a framework for building unsupervised representations of entities\nand their compositions, where each entity is viewed as a probability\ndistribution rather than a vector embedding. In particular, this distribution\nis supported over the contexts which co-occur with the entity and are embedded\nin a suitable low-dimensional space. This enables us to consider representation\nlearning from the perspective of Optimal Transport and take advantage of its\ntools such as Wasserstein distance and barycenters. We elaborate how the method\ncan be applied for obtaining unsupervised representations of text and\nillustrate the performance (quantitatively as well as qualitatively) on tasks\nsuch as measuring sentence similarity, word entailment and similarity, where we\nempirically observe significant gains (e.g., 4.1% relative improvement over\nSent2vec, GenSen).\n  The key benefits of the proposed approach include: (a) capturing uncertainty\nand polysemy via modeling the entities as distributions, (b) utilizing the\nunderlying geometry of the particular task (with the ground cost), (c)\nsimultaneously providing interpretability with the notion of optimal transport\nbetween contexts and (d) easy applicability on top of existing point embedding\nmethods. The code, as well as prebuilt histograms, are available under\nhttps://github.com/context-mover/.", "published": "2018-08-29 07:18:29", "link": "http://arxiv.org/abs/1808.09663v6", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Rule induction for global explanation of trained models", "abstract": "Understanding the behavior of a trained network and finding explanations for\nits outputs is important for improving the network's performance and\ngeneralization ability, and for ensuring trust in automated systems. Several\napproaches have previously been proposed to identify and visualize the most\nimportant features by analyzing a trained network. However, the relations\nbetween different features and classes are lost in most cases. We propose a\ntechnique to induce sets of if-then-else rules that capture these relations to\nglobally explain the predictions of a network. We first calculate the\nimportance of the features in the trained network. We then weigh the original\ninputs with these feature importance scores, simplify the transformed input\nspace, and finally fit a rule induction model to explain the model predictions.\nWe find that the output rule-sets can explain the predictions of a neural\nnetwork trained for 4-class text classification from the 20 newsgroups dataset\nto a macro-averaged F-score of 0.80. We make the code available at\nhttps://github.com/clips/interpret_with_rules.", "published": "2018-08-29 12:02:11", "link": "http://arxiv.org/abs/1808.09744v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Neural Compositional Denotational Semantics for Question Answering", "abstract": "Answering compositional questions requiring multi-step reasoning is\nchallenging. We introduce an end-to-end differentiable model for interpreting\nquestions about a knowledge graph (KG), which is inspired by formal approaches\nto semantics. Each span of text is represented by a denotation in a KG and a\nvector that captures ungrounded aspects of meaning. Learned composition modules\nrecursively combine constituent spans, culminating in a grounding for the\ncomplete sentence which answers the question. For example, to interpret \"not\ngreen\", the model represents \"green\" as a set of KG entities and \"not\" as a\ntrainable ungrounded vector---and then uses this vector to parameterize a\ncomposition function that performs a complement operation. For each sentence,\nwe build a parse chart subsuming all possible parses, allowing the model to\njointly learn both the composition operators and output structure by gradient\ndescent from end-task supervision. The model learns a variety of challenging\nsemantic operators, such as quantifiers, disjunctions and composed relations,\nand infers latent syntactic structure. It also generalizes well to longer\nquestions than seen in its training data, in contrast to RNN, its tree-based\nvariants, and semantic parsing baselines.", "published": "2018-08-29 17:43:11", "link": "http://arxiv.org/abs/1808.09942v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning a Policy for Opportunistic Active Learning", "abstract": "Active learning identifies data points to label that are expected to be the\nmost useful in improving a supervised model. Opportunistic active learning\nincorporates active learning into interactive tasks that constrain possible\nqueries during interactions. Prior work has shown that opportunistic active\nlearning can be used to improve grounding of natural language descriptions in\nan interactive object retrieval task. In this work, we use reinforcement\nlearning for such an object retrieval task, to learn a policy that effectively\ntrades off task completion with model improvement that would benefit future\ntasks.", "published": "2018-08-29 18:40:26", "link": "http://arxiv.org/abs/1808.10009v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Extended playing techniques: The next milestone in musical instrument\n  recognition", "abstract": "The expressive variability in producing a musical note conveys information\nessential to the modeling of orchestration and style. As such, it plays a\ncrucial role in computer-assisted browsing of massive digital music corpora.\nYet, although the automatic recognition of a musical instrument from the\nrecording of a single \"ordinary\" note is considered a solved problem, automatic\nidentification of instrumental playing technique (IPT) remains largely\nunderdeveloped. We benchmark machine listening systems for query-by-example\nbrowsing among 143 extended IPTs for 16 instruments, amounting to 469 triplets\nof instrument, mute, and technique. We identify and discuss three necessary\nconditions for significantly outperforming the traditional mel-frequency\ncepstral coefficient (MFCC) baseline: the addition of second-order scattering\ncoefficients to account for amplitude modulation, the incorporation of\nlong-range temporal dependencies, and metric learning using large-margin\nnearest neighbors (LMNN) to reduce intra-class variability. Evaluating on the\nStudio On Line (SOL) dataset, we obtain a precision at rank 5 of 99.7% for\ninstrument recognition (baseline at 89.0%) and of 61.0% for IPT recognition\n(baseline at 44.5%). We interpret this gain through a qualitative assessment of\npractical usability and visualization using nonlinear dimensionality reduction.", "published": "2018-08-29 11:16:30", "link": "http://arxiv.org/abs/1808.09730v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Replay spoofing detection system for automatic speaker verification\n  using multi-task learning of noise classes", "abstract": "In this paper, we propose a replay attack spoofing detection system for\nautomatic speaker verification using multitask learning of noise classes. We\ndefine the noise that is caused by the replay attack as replay noise. We\nexplore the effectiveness of training a deep neural network simultaneously for\nreplay attack spoofing detection and replay noise classification. The\nmulti-task learning includes classifying the noise of playback devices,\nrecording environments, and recording devices as well as the spoofing\ndetection. Each of the three types of the noise classes also includes a genuine\nclass. The experiment results on the ASVspoof2017 datasets demonstrate that the\nperformance of our proposed system is improved by 30% relatively on the\nevaluation set.", "published": "2018-08-29 05:01:00", "link": "http://arxiv.org/abs/1808.09638v4", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP", "stat.ML"], "primary_category": "eess.AS"}
