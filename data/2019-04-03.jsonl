{"title": "Multi-Modal Generative Adversarial Network for Short Product Title\n  Generation in Mobile E-Commerce", "abstract": "Nowadays, more and more customers browse and purchase products in favor of\nusing mobile E-Commerce Apps such as Taobao and Amazon. Since merchants are\nusually inclined to describe redundant and over-informative product titles to\nattract attentions from customers, it is important to concisely display short\nproduct titles on limited screen of mobile phones. To address this discrepancy,\nprevious studies mainly consider textual information of long product titles and\nlacks of human-like view during training and evaluation process. In this paper,\nwe propose a Multi-Modal Generative Adversarial Network (MM-GAN) for short\nproduct title generation in E-Commerce, which innovatively incorporates image\ninformation and attribute tags from product, as well as textual information\nfrom original long titles. MM-GAN poses short title generation as a\nreinforcement learning process, where the generated titles are evaluated by the\ndiscriminator in a human-like view. Extensive experiments on a large-scale\nE-Commerce dataset demonstrate that our algorithm outperforms other\nstate-of-the-art methods. Moreover, we deploy our model into a real-world\nonline E-Commerce environment and effectively boost the performance of click\nthrough rate and click conversion rate by 1.66% and 1.87%, respectively.", "published": "2019-04-03 01:29:48", "link": "http://arxiv.org/abs/1904.01735v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-task Learning for Chinese Word Usage Errors Detection", "abstract": "Chinese word usage errors often occur in non-native Chinese learners'\nwriting. It is very helpful for non-native Chinese learners to detect them\nautomatically when learning writing. In this paper, we propose a novel\napproach, which takes advantages of different auxiliary tasks, such as\nPOS-tagging prediction and word log frequency prediction, to help the task of\nChinese word usage error detection. With the help of these auxiliary tasks, we\nachieve the state-of-the-art results on the performances on the HSK corpus\ndata, without any other extra data.", "published": "2019-04-03 05:58:07", "link": "http://arxiv.org/abs/1904.01783v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual transfer learning for spoken language understanding", "abstract": "Typically, spoken language understanding (SLU) models are trained on\nannotated data which are costly to gather. Aiming to reduce data needs for\nbootstrapping a SLU system for a new language, we present a simple but\neffective weight transfer approach using data from another language. The\napproach is evaluated with our promising multi-task SLU framework developed\ntowards different languages. We evaluate our approach on the ATIS and a\nreal-world SLU dataset, showing that i) our monolingual models outperform the\nstate-of-the-art, ii) we can reduce data amounts needed for bootstrapping a SLU\nsystem for a new language greatly, and iii) while multitask training improves\nover separate training, different weight transfer settings may work best for\ndifferent SLU modules.", "published": "2019-04-03 08:11:57", "link": "http://arxiv.org/abs/1904.01825v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Deep Structured Semantic Models for Commonsense Reasoning", "abstract": "Commonsense reasoning is fundamental to natural language understanding. While\ntraditional methods rely heavily on human-crafted features and knowledge bases,\nwe explore learning commonsense knowledge from a large amount of raw text via\nunsupervised learning. We propose two neural network models based on the Deep\nStructured Semantic Models (DSSM) framework to tackle two classic commonsense\nreasoning tasks, Winograd Schema challenges (WSC) and Pronoun Disambiguation\n(PDP). Evaluation shows that the proposed models effectively capture contextual\ninformation in the sentence and co-reference information between pronouns and\nnouns, and achieve significant improvement over previous state-of-the-art\napproaches.", "published": "2019-04-03 11:57:25", "link": "http://arxiv.org/abs/1904.01938v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Large-Scale Comparison of Historical Text Normalization Systems", "abstract": "There is no consensus on the state-of-the-art approach to historical text\nnormalization. Many techniques have been proposed, including rule-based\nmethods, distance metrics, character-based statistical machine translation, and\nneural encoder--decoder models, but studies have used different datasets,\ndifferent evaluation methods, and have come to different conclusions. This\npaper presents the largest study of historical text normalization done so far.\nWe critically survey the existing literature and report experiments on eight\nlanguages, comparing systems spanning all categories of proposed normalization\ntechniques, analysing the effect of training data quantity, and using different\nevaluation methods. The datasets and scripts are made publicly available.", "published": "2019-04-03 14:46:38", "link": "http://arxiv.org/abs/1904.02036v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CAN-NER: Convolutional Attention Network for Chinese Named Entity\n  Recognition", "abstract": "Named entity recognition (NER) in Chinese is essential but difficult because\nof the lack of natural delimiters. Therefore, Chinese Word Segmentation (CWS)\nis usually considered as the first step for Chinese NER. However, models based\non word-level embeddings and lexicon features often suffer from segmentation\nerrors and out-of-vocabulary (OOV) words. In this paper, we investigate a\nConvolutional Attention Network called CAN for Chinese NER, which consists of a\ncharacter-based convolutional neural network (CNN) with local-attention layer\nand a gated recurrent unit (GRU) with global self-attention layer to capture\nthe information from adjacent characters and sentence contexts. Also, compared\nto other models, not depending on any external resources like lexicons and\nemploying small size of char embeddings make our model more practical.\nExtensive experimental results show that our approach outperforms\nstate-of-the-art methods without word embedding and external lexicon resources\non different domain datasets including Weibo, MSRA and Chinese Resume NER\ndataset.", "published": "2019-04-03 17:56:38", "link": "http://arxiv.org/abs/1904.02141v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive\n  Autoencoders", "abstract": "We introduce deep inside-outside recursive autoencoders (DIORA), a\nfully-unsupervised method for discovering syntax that simultaneously learns\nrepresentations for constituents within the induced tree. Our approach predicts\neach word in an input sentence conditioned on the rest of the sentence and uses\ninside-outside dynamic programming to consider all possible binary trees over\nthe sentence. At test time the CKY algorithm extracts the highest scoring\nparse. DIORA achieves a new state-of-the-art F1 in unsupervised binary\nconstituency parsing (unlabeled) in two benchmark datasets, WSJ and MultiNLI.", "published": "2019-04-03 17:56:48", "link": "http://arxiv.org/abs/1904.02142v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing Biomedical Embeddings from Language Models", "abstract": "Contextualized word embeddings derived from pre-trained language models (LMs)\nshow significant improvements on downstream NLP tasks. Pre-training on\ndomain-specific corpora, such as biomedical articles, further improves their\nperformance. In this paper, we conduct probing experiments to determine what\nadditional information is carried intrinsically by the in-domain trained\ncontextualized embeddings. For this we use the pre-trained LMs as fixed feature\nextractors and restrict the downstream task models to not have additional\nsequence modeling layers. We compare BERT, ELMo, BioBERT and BioELMo, a\nbiomedical version of ELMo trained on 10M PubMed abstracts. Surprisingly, while\nfine-tuned BioBERT is better than BioELMo in biomedical NER and NLI tasks, as a\nfixed feature extractor BioELMo outperforms BioBERT in our probing tasks. We\nuse visualization and nearest neighbor analysis to show that better encoding of\nentity-type and relational information leads to this superiority.", "published": "2019-04-03 18:05:02", "link": "http://arxiv.org/abs/1904.02181v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Effect of Downstream Classification Tasks for Evaluating Sentence\n  Embeddings", "abstract": "One popular method for quantitatively evaluating the utility of sentence\nembeddings involves using them in downstream language processing tasks that\nrequire sentence representations as input. One simple such task is\nclassification, where the sentence representations are used to train and test\nmodels on several classification datasets. We argue that by evaluating sentence\nrepresentations in such a manner, the goal of the representations becomes\nlearning a low-dimensional factorization of a sentence-task label matrix. We\nshow how characteristics of this matrix can affect the ability for a\nlow-dimensional factorization to perform as sentence representations in a suite\nof classification tasks. Primarily, sentences that have more labels across all\npossible classification tasks have a higher reconstruction loss, however the\ngeneral nature of this effect is ultimately dependent on the overall\ndistribution of labels across all possible sentences.", "published": "2019-04-03 20:12:10", "link": "http://arxiv.org/abs/1904.02228v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT Post-Training for Review Reading Comprehension and Aspect-based\n  Sentiment Analysis", "abstract": "Question-answering plays an important role in e-commerce as it allows\npotential customers to actively seek crucial information about products or\nservices to help their purchase decision making. Inspired by the recent success\nof machine reading comprehension (MRC) on formal documents, this paper explores\nthe potential of turning customer reviews into a large source of knowledge that\ncan be exploited to answer user questions.~We call this problem Review Reading\nComprehension (RRC). To the best of our knowledge, no existing work has been\ndone on RRC. In this work, we first build an RRC dataset called ReviewRC based\non a popular benchmark for aspect-based sentiment analysis. Since ReviewRC has\nlimited training examples for RRC (and also for aspect-based sentiment\nanalysis), we then explore a novel post-training approach on the popular\nlanguage model BERT to enhance the performance of fine-tuning of BERT for RRC.\nTo show the generality of the approach, the proposed post-training is also\napplied to some other review-based tasks such as aspect extraction and aspect\nsentiment classification in aspect-based sentiment analysis. Experimental\nresults demonstrate that the proposed post-training is highly effective. The\ndatasets and code are available at https://www.cs.uic.edu/~hxu/.", "published": "2019-04-03 20:29:10", "link": "http://arxiv.org/abs/1904.02232v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-task Learning for Japanese Predicate Argument Structure Analysis", "abstract": "An event-noun is a noun that has an argument structure similar to a\npredicate. Recent works, including those considered state-of-the-art, ignore\nevent-nouns or build a single model for solving both Japanese predicate\nargument structure analysis (PASA) and event-noun argument structure analysis\n(ENASA). However, because there are interactions between predicates and\nevent-nouns, it is not sufficient to target only predicates. To address this\nproblem, we present a multi-task learning method for PASA and ENASA. Our\nmulti-task models improved the performance of both tasks compared to a\nsingle-task model by sharing knowledge from each task. Moreover, in PASA, our\nmodels achieved state-of-the-art results in overall F1 scores on the NAIST Text\nCorpus. In addition, this is the first work to employ neural networks in ENASA.", "published": "2019-04-03 21:29:14", "link": "http://arxiv.org/abs/1904.02244v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Outside the Box: Discourse-level Features Improve Metaphor\n  Identification", "abstract": "Most current approaches to metaphor identification use restricted linguistic\ncontexts, e.g. by considering only a verb's arguments or the sentence\ncontaining a phrase. Inspired by pragmatic accounts of metaphor, we argue that\nbroader discourse features are crucial for better metaphor identification. We\ntrain simple gradient boosting classifiers on representations of an utterance\nand its surrounding discourse learned with a variety of document embedding\nmethods, obtaining near state-of-the-art results on the 2018 VU Amsterdam\nmetaphor identification task without the complex metaphor-specific features or\ndeep neural architectures employed by other systems. A qualitative analysis\nfurther confirms the need for broader context in metaphor processing.", "published": "2019-04-03 21:38:25", "link": "http://arxiv.org/abs/1904.02246v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Vocabulary for Big Code Machine Learning", "abstract": "When building machine learning models that operate on source code, several\ndecisions have to be made to model source-code vocabulary. These decisions can\nhave a large impact: some can lead to not being able to train models at all,\nothers significantly affect performance, particularly for Neural Language\nModels. Yet, these decisions are not often fully described. This paper lists\nimportant modeling choices for source code vocabulary, and explores their\nimpact on the resulting vocabulary on a large-scale corpus of 14,436 projects.\nWe show that a subset of decisions have decisive characteristics, allowing to\ntrain accurate Neural Language Models quickly on a large corpus of 10,106\nprojects.", "published": "2019-04-03 09:27:57", "link": "http://arxiv.org/abs/1904.01873v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Subword-Level Language Identification for Intra-Word Code-Switching", "abstract": "Language identification for code-switching (CS), the phenomenon of\nalternating between two or more languages in conversations, has traditionally\nbeen approached under the assumption of a single language per token. However,\nif at least one language is morphologically rich, a large number of words can\nbe composed of morphemes from more than one language (intra-word CS). In this\npaper, we extend the language identification task to the subword-level, such\nthat it includes splitting mixed words while tagging each part with a language\nID. We further propose a model for this task, which is based on a segmental\nrecurrent neural network. In experiments on a new Spanish--Wixarika dataset and\non an adapted German--Turkish dataset, our proposed model performs slightly\nbetter than or roughly on par with our best baseline, respectively. Considering\nonly mixed words, however, it strongly outperforms all baselines.", "published": "2019-04-03 13:08:12", "link": "http://arxiv.org/abs/1904.01989v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OpBerg: Discovering causal sentences using optimal alignments", "abstract": "The biological literature is rich with sentences that describe causal\nrelations. Methods that automatically extract such sentences can help\nbiologists to synthesize the literature and even discover latent relations that\nhad not been articulated explicitly. Current methods for extracting causal\nsentences are based on either machine learning or a predefined database of\ncausal terms. Machine learning approaches require a large set of labeled\ntraining data and can be susceptible to noise. Methods based on predefined\ndatabases are limited by the quality of their curation and are unable to\ncapture new concepts or mistakes in the input. We address these challenges by\nadapting and improving a method designed for a seemingly unrelated problem:\nfinding alignments between genomic sequences. This paper presents a novel and\noutperforming method for extracting causal relations from text by aligning the\npart-of-speech representations of an input set with that of known causal\nsentences. Our experiments show that when applied to the task of finding causal\nsentences in biological literature, our method improves on the accuracy of\nother methods in a computationally efficient manner.", "published": "2019-04-03 14:36:49", "link": "http://arxiv.org/abs/1904.02032v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Automated Fact Checking in the News Room", "abstract": "Fact checking is an essential task in journalism; its importance has been\nhighlighted due to recently increased concerns and efforts in combating\nmisinformation. In this paper, we present an automated fact-checking platform\nwhich given a claim, it retrieves relevant textual evidence from a document\ncollection, predicts whether each piece of evidence supports or refutes the\nclaim, and returns a final verdict. We describe the architecture of the system\nand the user interface, focusing on the choices made to improve its\nuser-friendliness and transparency. We conduct a user study of the\nfact-checking platform in a journalistic setting: we integrated it with a\ncollection of news articles and provide an evaluation of the platform using\nfeedback from journalists in their workflow. We found that the predictions of\nour platform were correct 58\\% of the time, and 59\\% of the returned evidence\nwas relevant.", "published": "2019-04-03 14:46:44", "link": "http://arxiv.org/abs/1904.02037v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "75 Languages, 1 Model: Parsing Universal Dependencies Universally", "abstract": "We present UDify, a multilingual multi-task model capable of accurately\npredicting universal part-of-speech, morphological features, lemmas, and\ndependency trees simultaneously for all 124 Universal Dependencies treebanks\nacross 75 languages. By leveraging a multilingual BERT self-attention model\npretrained on 104 languages, we found that fine-tuning it on all datasets\nconcatenated together with simple softmax classifiers for each UD task can\nresult in state-of-the-art UPOS, UFeats, Lemmas, UAS, and LAS scores, without\nrequiring any recurrent or language-specific components. We evaluate UDify for\nmultilingual learning, showing that low-resource languages benefit the most\nfrom cross-linguistic annotations. We also evaluate for zero-shot learning,\nwith results suggesting that multilingual training provides strong UD\npredictions even for languages that neither UDify nor BERT have ever been\ntrained on. Code for UDify is available at\nhttps://github.com/hyperparticle/udify.", "published": "2019-04-03 16:52:55", "link": "http://arxiv.org/abs/1904.02099v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Massively Multilingual Adversarial Speech Recognition", "abstract": "We report on adaptation of multilingual end-to-end speech recognition models\ntrained on as many as 100 languages. Our findings shed light on the relative\nimportance of similarity between the target and pretraining languages along the\ndimensions of phonetics, phonology, language family, geographical location, and\northography. In this context, experiments demonstrate the effectiveness of two\nadditional pretraining objectives in encouraging language-independent encoder\nrepresentations: a context-independent phoneme objective paired with a\nlanguage-adversarial classification objective.", "published": "2019-04-03 19:28:53", "link": "http://arxiv.org/abs/1904.02210v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Jointly Extracting and Compressing Documents with Summary State\n  Representations", "abstract": "We present a new neural model for text summarization that first extracts\nsentences from a document and then compresses them. The proposed model offers a\nbalance that sidesteps the difficulties in abstractive methods while generating\nmore concise summaries than extractive methods. In addition, our model\ndynamically determines the length of the output summary based on the gold\nsummaries it observes during training and does not require length constraints\ntypical to extractive summarization. The model achieves state-of-the-art\nresults on the CNN/DailyMail and Newsroom datasets, improving over current\nextractive and abstractive methods. Human evaluations demonstrate that our\nmodel generates concise and informative summaries. We also make available a new\ndataset of oracle compressive summaries derived automatically from the\nCNN/DailyMail reference summaries.", "published": "2019-04-03 14:24:04", "link": "http://arxiv.org/abs/1904.02020v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Black is to Criminal as Caucasian is to Police: Detecting and Removing\n  Multiclass Bias in Word Embeddings", "abstract": "Online texts -- across genres, registers, domains, and styles -- are riddled\nwith human stereotypes, expressed in overt or subtle ways. Word embeddings,\ntrained on these texts, perpetuate and amplify these stereotypes, and propagate\nbiases to machine learning models that use word embeddings as features. In this\nwork, we propose a method to debias word embeddings in multiclass settings such\nas race and religion, extending the work of (Bolukbasi et al., 2016) from the\nbinary setting, such as binary gender. Next, we propose a novel methodology for\nthe evaluation of multiclass debiasing. We demonstrate that our multiclass\ndebiasing is robust and maintains the efficacy in standard NLP tasks.", "published": "2019-04-03 22:17:27", "link": "http://arxiv.org/abs/1904.04047v3", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Evaluating KGR10 Polish word embeddings in the recognition of temporal\n  expressions using BiLSTM-CRF", "abstract": "The article introduces a new set of Polish word embeddings, built using KGR10\ncorpus, which contains more than 4 billion words. These embeddings are\nevaluated in the problem of recognition of temporal expressions (timexes) for\nthe Polish language. We described the process of KGR10 corpus creation and a\nnew approach to the recognition problem using Bidirectional Long-Short Term\nMemory (BiLSTM) network with additional CRF layer, where specific embeddings\nare essential. We presented experiments and conclusions drawn from them.", "published": "2019-04-03 14:47:30", "link": "http://arxiv.org/abs/1904.04055v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "End-to-end Binaural Sound Localisation from the Raw Waveform", "abstract": "A novel end-to-end binaural sound localisation approach is proposed which\nestimates the azimuth of a sound source directly from the waveform. Instead of\nemploying hand-crafted features commonly employed for binaural sound\nlocalisation, such as the interaural time and level difference, our end-to-end\nsystem approach uses a convolutional neural network (CNN) to extract specific\nfeatures from the waveform that are suitable for localisation. Two systems are\nproposed which differ in the initial frequency analysis stage. The first system\nis auditory-inspired and makes use of a gammatone filtering layer, while the\nsecond system is fully data-driven and exploits a trainable convolutional layer\nto perform frequency analysis. In both systems, a set of dedicated\nconvolutional kernels are then employed to search for specific localisation\ncues, which are coupled with a localisation stage using fully connected layers.\nLocalisation experiments using binaural simulation in both anechoic and\nreverberant environments show that the proposed systems outperform a\nstate-of-the-art deep neural network system. Furthermore, our investigation of\nthe frequency analysis stage in the second system suggests that the CNN is able\nto exploit different frequency bands for localisation according to the\ncharacteristics of the reverberant environment.", "published": "2019-04-03 11:07:46", "link": "http://arxiv.org/abs/1904.01916v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GEDI: Gammachirp Envelope Distortion Index for Predicting\n  Intelligibility of Enhanced Speech", "abstract": "In this study, we propose a new concept, the gammachirp envelope distortion\nindex (GEDI), based on the signal-to-distortion ratio in the auditory envelope,\nSDRenv to predict the intelligibility of speech enhanced by nonlinear\nalgorithms. The objective of GEDI is to calculate the distortion between\nenhanced and clean-speech representations in the domain of a temporal envelope\nextracted by the gammachirp auditory filterbank and modulation filterbank. We\nalso extend GEDI with multi-resolution analysis (mr-GEDI) to predict the speech\nintelligibility of sounds under non-stationary noise conditions. We evaluate\nGEDI in terms of speech intelligibility predictions of speech sounds enhanced\nby a classic spectral subtraction and a Wiener filtering method. The\npredictions are compared with human results for various signal-to-noise ratio\nconditions with additive pink and babble noises. The results showed that\nmr-GEDI predicted the intelligibility curves better than short-time objective\nintelligibility (STOI) measure, extended-STOI (ESTOI) measure, and hearing-aid\nspeech perception index (HASPI) under pink-noise conditions, and better than\nHASPI under babble-noise conditions. The mr-GEDI method does not present an\noverestimation tendency and is considered a more conservative approach than\nSTOI and ESTOI. Therefore, the evaluation with mr-GEDI may provide additional\ninformation in the development of speech enhancement algorithms.", "published": "2019-04-03 16:42:44", "link": "http://arxiv.org/abs/1904.02096v6", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
