{"title": "KGQuiz: Evaluating the Generalization of Encoded Knowledge in Large\n  Language Models", "abstract": "Large language models (LLMs) demonstrate remarkable performance on\nknowledge-intensive tasks, suggesting that real-world knowledge is encoded in\ntheir model parameters. However, besides explorations on a few probing tasks in\nlimited knowledge domains, it is not well understood how to evaluate LLMs'\nknowledge systematically and how well their knowledge abilities generalize,\nacross a spectrum of knowledge domains and progressively complex task formats.\nTo this end, we propose KGQuiz, a knowledge-intensive benchmark to\ncomprehensively investigate the knowledge generalization abilities of LLMs.\nKGQuiz is a scalable framework constructed from triplet-based knowledge, which\ncovers three knowledge domains and consists of five tasks with increasing\ncomplexity: true-or-false, multiple-choice QA, blank filling, factual editing,\nand open-ended knowledge generation. To gain a better understanding of LLMs'\nknowledge abilities and their generalization, we evaluate 10 open-source and\nblack-box LLMs on the KGQuiz benchmark across the five knowledge-intensive\ntasks and knowledge domains. Extensive experiments demonstrate that LLMs\nachieve impressive performance in straightforward knowledge QA tasks, while\nsettings and contexts requiring more complex reasoning or employing\ndomain-specific facts still present significant challenges. We envision KGQuiz\nas a testbed to analyze such nuanced variations in performance across domains\nand task formats, and ultimately to understand, evaluate, and improve LLMs'\nknowledge abilities across a wide spectrum of knowledge domains and tasks.", "published": "2023-10-15 04:00:36", "link": "http://arxiv.org/abs/2310.09725v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Relation Classification with Graph Meaning Representations", "abstract": "In the field of natural language understanding, the intersection of neural\nmodels and graph meaning representations (GMRs) remains a compelling area of\nresearch. Despite the growing interest, a critical gap persists in\nunderstanding the exact influence of GMRs, particularly concerning relation\nextraction tasks. Addressing this, we introduce DAGNN-plus, a simple and\nparameter-efficient neural architecture designed to decouple contextual\nrepresentation learning from structural information propagation. Coupled with\nvarious sequence encoders and GMRs, this architecture provides a foundation for\nsystematic experimentation on two English and two Chinese datasets. Our\nempirical analysis utilizes four different graph formalisms and nine parsers.\nThe results yield a nuanced understanding of GMRs, showing improvements in\nthree out of the four datasets, particularly favoring English over Chinese due\nto highly accurate parsers. Interestingly, GMRs appear less effective in\nliterary-domain datasets compared to general-domain datasets. These findings\nlay the groundwork for better-informed design of GMRs and parsers to improve\nrelation classification, which is expected to tangibly impact the future\ntrajectory of natural language understanding research.", "published": "2023-10-15 08:07:56", "link": "http://arxiv.org/abs/2310.09772v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RSVP: Customer Intent Detection via Agent Response Contrastive and\n  Generative Pre-Training", "abstract": "The dialogue systems in customer services have been developed with neural\nmodels to provide users with precise answers and round-the-clock support in\ntask-oriented conversations by detecting customer intents based on their\nutterances. Existing intent detection approaches have highly relied on\nadaptively pre-training language models with large-scale datasets, yet the\npredominant cost of data collection may hinder their superiority. In addition,\nthey neglect the information within the conversational responses of the agents,\nwhich have a lower collection cost, but are significant to customer intent as\nagents must tailor their replies based on the customers' intent. In this paper,\nwe propose RSVP, a self-supervised framework dedicated to task-oriented\ndialogues, which utilizes agent responses for pre-training in a two-stage\nmanner. Specifically, we introduce two pre-training tasks to incorporate the\nrelations of utterance-response pairs: 1) Response Retrieval by selecting a\ncorrect response from a batch of candidates, and 2) Response Generation by\nmimicking agents to generate the response to a given utterance. Our benchmark\nresults for two real-world customer service datasets show that RSVP\nsignificantly outperforms the state-of-the-art baselines by 4.95% for accuracy,\n3.4% for MRR@3, and 2.75% for MRR@5 on average. Extensive case studies are\ninvestigated to show the validity of incorporating agent responses into the\npre-training stage.", "published": "2023-10-15 08:21:38", "link": "http://arxiv.org/abs/2310.09773v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the Reliability of Large Language Model Knowledge", "abstract": "Large language models (LLMs) have been treated as knowledge bases due to\ntheir strong performance in knowledge probing tasks. LLMs are typically\nevaluated using accuracy, yet this metric does not capture the vulnerability of\nLLMs to hallucination-inducing factors like prompt and context variability. How\ndo we evaluate the capabilities of LLMs to consistently produce factually\ncorrect answers? In this paper, we propose MOdel kNowledge relIabiliTy scORe\n(MONITOR), a novel metric designed to directly measure LLMs' factual\nreliability. MONITOR computes the distance between the probability\ndistributions of a valid output and its counterparts produced by the same LLM\nprobing the same fact using different styles of prompts and\ncontexts.Experiments on a comprehensive range of 12 LLMs demonstrate the\neffectiveness of MONITOR in evaluating the factual reliability of LLMs while\nmaintaining a low computational overhead. In addition, we release the FKTC\n(Factual Knowledge Test Corpus) test set, containing 210,158 prompts in total\nto foster research along this line (https://github.com/Vicky-Wil/MONITOR).", "published": "2023-10-15 12:40:30", "link": "http://arxiv.org/abs/2310.09820v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Merging Experts into One: Improving Computational Efficiency of Mixture\n  of Experts", "abstract": "Scaling the size of language models usually leads to remarkable advancements\nin NLP tasks. But it often comes with a price of growing computational cost.\nAlthough a sparse Mixture of Experts (MoE) can reduce the cost by activating a\nsmall subset of parameters (e.g., one expert) for each input, its computation\nescalates significantly if increasing the number of activated experts, limiting\nits practical utility. Can we retain the advantages of adding more experts\nwithout substantially increasing the computational costs? In this paper, we\nfirst demonstrate the superiority of selecting multiple experts and then\npropose a computation-efficient approach called \\textbf{\\texttt{Merging Experts\ninto One}} (MEO), which reduces the computation cost to that of a single\nexpert. Extensive experiments show that MEO significantly improves\ncomputational efficiency, e.g., FLOPS drops from 72.0G of vanilla MoE to 28.6G\n(MEO). Moreover, we propose a token-level attention block that further enhances\nthe efficiency and performance of token-level MEO, e.g., 83.3\\% (MEO) vs.\n82.6\\% (vanilla MoE) average score on the GLUE benchmark. Our code will be\nreleased upon acceptance. Code will be released at:\n\\url{https://github.com/Shwai-He/MEO}.", "published": "2023-10-15 13:28:42", "link": "http://arxiv.org/abs/2310.09832v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Stance Classification on Social Media Using Quantified Moral\n  Foundations", "abstract": "This study enhances stance detection on social media by incorporating deeper\npsychological attributes, specifically individuals' moral foundations. These\ntheoretically-derived dimensions aim to provide a comprehensive profile of an\nindividual's moral concerns which, in recent work, has been linked to behaviour\nin a range of domains, including society, politics, health, and the\nenvironment. In this paper, we investigate how moral foundation dimensions can\ncontribute to predicting an individual's stance on a given target. Specifically\nwe incorporate moral foundation features extracted from text, along with\nmessage semantic features, to classify stances at both message- and user-levels\nusing both traditional machine learning models and large language models. Our\npreliminary results suggest that encoding moral foundations can enhance the\nperformance of stance detection tasks and help illuminate the associations\nbetween specific moral foundations and online stances on target topics. The\nresults highlight the importance of considering deeper psychological attributes\nin stance analysis and underscores the role of moral foundations in guiding\nonline social behavior.", "published": "2023-10-15 14:40:57", "link": "http://arxiv.org/abs/2310.09848v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reformulating NLP tasks to Capture Longitudinal Manifestation of\n  Language Disorders in People with Dementia", "abstract": "Dementia is associated with language disorders which impede communication.\nHere, we automatically learn linguistic disorder patterns by making use of a\nmoderately-sized pre-trained language model and forcing it to focus on\nreformulated natural language processing (NLP) tasks and associated linguistic\npatterns. Our experiments show that NLP tasks that encapsulate contextual\ninformation and enhance the gradient signal with linguistic patterns benefit\nperformance. We then use the probability estimates from the best model to\nconstruct digital linguistic markers measuring the overall quality in\ncommunication and the intensity of a variety of language disorders. We\ninvestigate how the digital markers characterize dementia speech from a\nlongitudinal perspective. We find that our proposed communication marker is\nable to robustly and reliably characterize the language of people with\ndementia, outperforming existing linguistic approaches; and shows external\nvalidity via significant correlation with clinical markers of behaviour.\nFinally, our proposed linguistic disorder markers provide useful insights into\ngradual language impairment associated with disease progression.", "published": "2023-10-15 17:58:47", "link": "http://arxiv.org/abs/2310.09897v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empirical study of pretrained multilingual language models for zero-shot\n  cross-lingual knowledge transfer in generation", "abstract": "Zero-shot cross-lingual knowledge transfer enables the multilingual\npretrained language model (mPLM), finetuned on a task in one language, make\npredictions for this task in other languages. While being broadly studied for\nnatural language understanding tasks, the described setting is understudied for\ngeneration. Previous works notice a frequent problem of generation in a wrong\nlanguage and propose approaches to address it, usually using mT5 as a backbone\nmodel. In this work, we test alternative mPLMs, such as mBART and NLLB-200,\nconsidering full finetuning and parameter-efficient finetuning with adapters.\nWe find that mBART with adapters performs similarly to mT5 of the same size,\nand NLLB-200 can be competitive in some cases. We also underline the importance\nof tuning learning rate used for finetuning, which helps to alleviate the\nproblem of generation in the wrong language.", "published": "2023-10-15 18:58:53", "link": "http://arxiv.org/abs/2310.09917v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FiLM: Fill-in Language Models for Any-Order Generation", "abstract": "Language models have become the backbone of today's AI systems. However,\ntheir predominant left-to-right generation limits the use of bidirectional\ncontext, which is essential for tasks that involve filling text in the middle.\nWe propose the Fill-in Language Model (FiLM), a new language modeling approach\nthat allows for flexible generation at any position without adhering to a\nspecific generation order. Its training extends the masked language modeling\nobjective by adopting varying mask probabilities sampled from the Beta\ndistribution to enhance the generative capabilities of FiLM. During inference,\nFiLM can seamlessly insert missing phrases, sentences, or paragraphs, ensuring\nthat the outputs are fluent and are coherent with the surrounding context. In\nboth automatic and human evaluations, FiLM outperforms existing infilling\nmethods that rely on left-to-right language models trained on rearranged text\nsegments. FiLM is easy to implement and can be either trained from scratch or\nfine-tuned from a left-to-right language model. Notably, as the model size\ngrows, FiLM's perplexity approaches that of strong left-to-right language\nmodels of similar sizes, indicating FiLM's scalability and potential as a large\nlanguage model.", "published": "2023-10-15 19:37:39", "link": "http://arxiv.org/abs/2310.09930v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overview of ImageArg-2023: The First Shared Task in Multimodal Argument\n  Mining", "abstract": "This paper presents an overview of the ImageArg shared task, the first\nmultimodal Argument Mining shared task co-located with the 10th Workshop on\nArgument Mining at EMNLP 2023. The shared task comprises two classification\nsubtasks - (1) Subtask-A: Argument Stance Classification; (2) Subtask-B: Image\nPersuasiveness Classification. The former determines the stance of a tweet\ncontaining an image and a piece of text toward a controversial topic (e.g., gun\ncontrol and abortion). The latter determines whether the image makes the tweet\ntext more persuasive. The shared task received 31 submissions for Subtask-A and\n21 submissions for Subtask-B from 9 different teams across 6 countries. The top\nsubmission in Subtask-A achieved an F1-score of 0.8647 while the best\nsubmission in Subtask-B achieved an F1-score of 0.5561.", "published": "2023-10-15 04:52:04", "link": "http://arxiv.org/abs/2310.12172v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HiCL: Hierarchical Contrastive Learning of Unsupervised Sentence\n  Embeddings", "abstract": "In this paper, we propose a hierarchical contrastive learning framework,\nHiCL, which considers local segment-level and global sequence-level\nrelationships to improve training efficiency and effectiveness. Traditional\nmethods typically encode a sequence in its entirety for contrast with others,\noften neglecting local representation learning, leading to challenges in\ngeneralizing to shorter texts. Conversely, HiCL improves its effectiveness by\ndividing the sequence into several segments and employing both local and global\ncontrastive learning to model segment-level and sequence-level relationships.\nFurther, considering the quadratic time complexity of transformers over input\ntokens, HiCL boosts training efficiency by first encoding short segments and\nthen aggregating them to obtain the sequence representation. Extensive\nexperiments show that HiCL enhances the prior top-performing SNCSE model across\nseven extensively evaluated STS tasks, with an average increase of +0.2%\nobserved on BERT-large and +0.44% on RoBERTa-large.", "published": "2023-10-15 03:14:33", "link": "http://arxiv.org/abs/2310.09720v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Domain-Specific Language Model Post-Training for Indonesian Financial\n  NLP", "abstract": "BERT and IndoBERT have achieved impressive performance in several NLP tasks.\nThere has been several investigation on its adaption in specialized domains\nespecially for English language. We focus on financial domain and Indonesian\nlanguage, where we perform post-training on pre-trained IndoBERT for financial\ndomain using a small scale of Indonesian financial corpus. In this paper, we\nconstruct an Indonesian self-supervised financial corpus, Indonesian financial\nsentiment analysis dataset, Indonesian financial topic classification dataset,\nand release a family of BERT models for financial NLP. We also evaluate the\neffectiveness of domain-specific post-training on sentiment analysis and topic\nclassification tasks. Our findings indicate that the post-training increases\nthe effectiveness of a language model when it is fine-tuned to domain-specific\ndownstream tasks.", "published": "2023-10-15 05:07:08", "link": "http://arxiv.org/abs/2310.09736v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Model-Aware In-Context Learning for Code Generation", "abstract": "Large language models (LLMs) have shown impressive in-context learning (ICL)\nability in code generation. LLMs take a prompt consisting of requirement-code\nexamples and a new requirement as input, and output new programs. Existing\nstudies have found that ICL is highly dominated by the examples and thus arises\nresearch on example selection. However, existing approaches randomly select\nexamples or only consider the textual similarity of requirements to retrieve,\nleading to sub-optimal performance. In this paper, we propose a novel\nlearning-based selection approach named LAIL (LLM-Aware In-context Learning)\nfor code generation. Given a candidate example, we exploit LLMs themselves to\nestimate it by considering the generation probabilities of ground-truth\nprograms given a requirement and the example. We then label candidate examples\nas positive or negative through the probability feedback. Based on the labeled\ndata, we import a contrastive learning objective to train an effective\nretriever that acquires the preference of LLMs in code generation. We apply\nLAIL to three LLMs and evaluate it on three representative datasets (e.g.,\nMBJP, MBPP, and MBCPP). LATA outperforms the state-of-the-art baselines by\n11.58%, 6.89%, and 5.07% on CodeGen, and 4.38%, 2.85%, and 2.74% on GPT-3.5 in\nterms of Pass@1, respectively.", "published": "2023-10-15 06:12:58", "link": "http://arxiv.org/abs/2310.09748v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Diversifying the Mixture-of-Experts Representation for Language Models\n  with Orthogonal Optimizer", "abstract": "The Mixture of Experts (MoE) has emerged as a highly successful technique in\ndeep learning, based on the principle of divide-and-conquer to maximize model\ncapacity without significant additional computational cost. Even in the era of\nlarge-scale language models (LLMs), MoE continues to play a crucial role, as\nsome researchers have indicated that GPT-4 adopts the MoE structure to ensure\ndiverse inference results. However, MoE is susceptible to performance\ndegeneracy, particularly evident in the issues of imbalance and homogeneous\nrepresentation among experts. While previous studies have extensively addressed\nthe problem of imbalance, the challenge of homogeneous representation remains\nunresolved. In this study, we shed light on the homogeneous representation\nproblem, wherein experts in the MoE fail to specialize and lack diversity,\nleading to frustratingly high similarities in their representations (up to 99\\%\nin a well-performed MoE model). This problem restricts the expressive power of\nthe MoE and, we argue, contradicts its original intention. To tackle this\nissue, we propose a straightforward yet highly effective solution: OMoE, an\northogonal expert optimizer. Additionally, we introduce an alternating training\nstrategy that encourages each expert to update in a direction orthogonal to the\nsubspace spanned by other experts. Our algorithm facilitates MoE training in\ntwo key ways: firstly, it explicitly enhances representation diversity, and\nsecondly, it implicitly fosters interaction between experts during orthogonal\nweights computation. Through extensive experiments, we demonstrate that our\nproposed optimization algorithm significantly improves the performance of\nfine-tuning the MoE model on the GLUE benchmark, SuperGLUE benchmark,\nquestion-answering task, and name entity recognition tasks.", "published": "2023-10-15 07:20:28", "link": "http://arxiv.org/abs/2310.09762v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MILPaC: A Novel Benchmark for Evaluating Translation of Legal Text to\n  Indian Languages", "abstract": "Most legal text in the Indian judiciary is written in complex English due to\nhistorical reasons. However, only a small fraction of the Indian population is\ncomfortable in reading English. Hence legal text needs to be made available in\nvarious Indian languages, possibly by translating the available legal text from\nEnglish. Though there has been a lot of research on translation to and between\nIndian languages, to our knowledge, there has not been much prior work on such\ntranslation in the legal domain. In this work, we construct the first\nhigh-quality legal parallel corpus containing aligned text units in English and\nnine Indian languages, that includes several low-resource languages. We also\nbenchmark the performance of a wide variety of Machine Translation (MT) systems\nover this corpus, including commercial MT systems, open-source MT systems and\nLarge Language Models. Through a comprehensive survey by Law practitioners, we\ncheck how satisfied they are with the translations by some of these MT systems,\nand how well automatic MT evaluation metrics agree with the opinions of Law\npractitioners.", "published": "2023-10-15 07:49:56", "link": "http://arxiv.org/abs/2310.09765v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VLIS: Unimodal Language Models Guide Multimodal Language Generation", "abstract": "Multimodal language generation, which leverages the synergy of language and\nvision, is a rapidly expanding field. However, existing vision-language models\nface challenges in tasks that require complex linguistic understanding. To\naddress this issue, we introduce Visual-Language models as Importance Sampling\nweights (VLIS), a novel framework that combines the visual conditioning\ncapability of vision-language models with the language understanding of\nunimodal text-only language models without further training. It extracts\npointwise mutual information of each image and text from a visual-language\nmodel and uses the value as an importance sampling weight to adjust the token\nlikelihood from a text-only model. VLIS improves vision-language models on\ndiverse tasks, including commonsense understanding (WHOOPS, OK-VQA, and\nScienceQA) and complex text generation (Concadia, Image Paragraph Captioning,\nand ROCStories). Our results suggest that VLIS represents a promising new\ndirection for multimodal language generation.", "published": "2023-10-15 07:58:52", "link": "http://arxiv.org/abs/2310.09767v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bounding and Filling: A Fast and Flexible Framework for Image Captioning", "abstract": "Most image captioning models following an autoregressive manner suffer from\nsignificant inference latency. Several models adopted a non-autoregressive\nmanner to speed up the process. However, the vanilla non-autoregressive manner\nresults in subpar performance, since it generates all words simultaneously,\nwhich fails to capture the relationships between words in a description. The\nsemi-autoregressive manner employs a partially parallel method to preserve\nperformance, but it sacrifices inference speed. In this paper, we introduce a\nfast and flexible framework for image captioning called BoFiCap based on\nbounding and filling techniques. The BoFiCap model leverages the inherent\ncharacteristics of image captioning tasks to pre-define bounding boxes for\nimage regions and their relationships. Subsequently, the BoFiCap model fills\ncorresponding words in each box using two-generation manners. Leveraging the\nbox hints, our filling process allows each word to better perceive other words.\nAdditionally, our model offers flexible image description generation: 1) by\nemploying different generation manners based on speed or performance\nrequirements, 2) producing varied sentences based on user-specified boxes.\nExperimental evaluations on the MS-COCO benchmark dataset demonstrate that our\nframework in a non-autoregressive manner achieves the state-of-the-art on\ntask-specific metric CIDEr (125.6) while speeding up 9.22x than the baseline\nmodel with an autoregressive manner; in a semi-autoregressive manner, our\nmethod reaches 128.4 on CIDEr while a 3.69x speedup. Our code and data is\navailable at https://github.com/ChangxinWang/BoFiCap.", "published": "2023-10-15 16:17:20", "link": "http://arxiv.org/abs/2310.09876v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "In-Context Learning with Iterative Demonstration Selection", "abstract": "Spurred by advancements in scale, large language models (LLMs) have\ndemonstrated strong few-shot learning ability via in-context learning (ICL).\nHowever, the performance of ICL has been shown to be highly sensitive to the\nselection of few-shot demonstrations. Selecting the most suitable examples as\ncontext remains an ongoing challenge and an open problem. Existing literature\nhas highlighted the importance of selecting examples that are diverse or\nsemantically similar to the test sample while ignoring the fact that the\noptimal selection dimension, i.e., diversity or similarity, is task-specific.\nBased on how the test sample is answered, we propose Iterative Demonstration\nSelection (IDS) to leverage the merits of both dimensions. Using zero-shot\nchain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples\nthat are diverse but still strongly correlated with the test sample as ICL\ndemonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample\nbefore demonstration selection. The output reasoning path is then used to\nchoose demonstrations that are prepended to the test sample for inference. The\ngenerated answer is followed by its corresponding reasoning path for extracting\na new set of demonstrations in the next iteration. After several iterations,\nIDS adopts majority voting to obtain the final result. Through extensive\nexperiments on tasks including reasoning, question answering, and topic\nclassification, we demonstrate that IDS can consistently outperform existing\nICL demonstration selection methods.", "published": "2023-10-15 16:40:19", "link": "http://arxiv.org/abs/2310.09881v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Lifelong Sequence Generation with Dynamic Module Expansion and\n  Adaptation", "abstract": "Lifelong sequence generation (LSG), a problem in continual learning, aims to\ncontinually train a model on a sequence of generation tasks to learn constantly\nemerging new generation patterns while avoiding the forgetting of previous\nknowledge. Existing LSG methods mainly focus on maintaining old knowledge while\npaying little attention to knowledge transfer across tasks. In contrast, humans\ncan better learn new tasks by leveraging previously acquired knowledge from\nsimilar tasks. Inspired by the learning paradigm of humans, we propose Dynamic\nModule Expansion and Adaptation (DMEA), which enables the model to dynamically\ndetermine the architecture for acquiring new knowledge based on task\ncorrelation and select the most similar previous tasks to facilitate adaptation\nto new tasks. In addition, as the learning process can easily be biased towards\nthe current task which might cause more severe forgetting of previously learned\nknowledge, we propose dynamic gradient scaling to balance the learning of the\ncurrent task and replayed tasks. With extensive experiments, we demonstrate\nthat DMEA can consistently outperform existing methods in different LSG\nsettings.", "published": "2023-10-15 16:51:11", "link": "http://arxiv.org/abs/2310.09886v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for\n  Multimodal Medical Diagnosis", "abstract": "Driven by the large foundation models, the development of artificial\nintelligence has witnessed tremendous progress lately, leading to a surge of\ngeneral interest from the public. In this study, we aim to assess the\nperformance of OpenAI's newest model, GPT-4V(ision), specifically in the realm\nof multimodal medical diagnosis. Our evaluation encompasses 17 human body\nsystems, including Central Nervous System, Head and Neck, Cardiac, Chest,\nHematology, Hepatobiliary, Gastrointestinal, Urogenital, Gynecology,\nObstetrics, Breast, Musculoskeletal, Spine, Vascular, Oncology, Trauma,\nPediatrics, with images taken from 8 modalities used in daily clinic routine,\ne.g., X-ray, Computed Tomography (CT), Magnetic Resonance Imaging (MRI),\nPositron Emission Tomography (PET), Digital Subtraction Angiography (DSA),\nMammography, Ultrasound, and Pathology. We probe the GPT-4V's ability on\nmultiple clinical tasks with or without patent history provided, including\nimaging modality and anatomy recognition, disease diagnosis, report generation,\ndisease localisation.\n  Our observation shows that, while GPT-4V demonstrates proficiency in\ndistinguishing between medical image modalities and anatomy, it faces\nsignificant challenges in disease diagnosis and generating comprehensive\nreports. These findings underscore that while large multimodal models have made\nsignificant advancements in computer vision and natural language processing, it\nremains far from being used to effectively support real-world medical\napplications and clinical decision-making.\n  All images used in this report can be found in\nhttps://github.com/chaoyi-wu/GPT-4V_Medical_Evaluation.", "published": "2023-10-15 18:32:27", "link": "http://arxiv.org/abs/2310.09909v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Prompting Scientific Names for Zero-Shot Species Recognition", "abstract": "Trained on web-scale image-text pairs, Vision-Language Models (VLMs) such as\nCLIP can recognize images of common objects in a zero-shot fashion. However, it\nis underexplored how to use CLIP for zero-shot recognition of highly\nspecialized concepts, e.g., species of birds, plants, and animals, for which\ntheir scientific names are written in Latin or Greek. Indeed, CLIP performs\npoorly for zero-shot species recognition with prompts that use scientific\nnames, e.g., \"a photo of Lepus Timidus\" (which is a scientific name in Latin).\nBecause these names are usually not included in CLIP's training set. To improve\nperformance, prior works propose to use large-language models (LLMs) to\ngenerate descriptions (e.g., of species color and shape) and additionally use\nthem in prompts. We find that they bring only marginal gains. Differently, we\nare motivated to translate scientific names (e.g., Lepus Timidus) to common\nEnglish names (e.g., mountain hare) and use such in the prompts. We find that\ncommon names are more likely to be included in CLIP's training set, and\nprompting them achieves 2$\\sim$5 times higher accuracy on benchmarking datasets\nof fine-grained species recognition.", "published": "2023-10-15 19:36:43", "link": "http://arxiv.org/abs/2310.09929v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "UvA-MT's Participation in the WMT23 General Translation Shared Task", "abstract": "This paper describes the UvA-MT's submission to the WMT 2023 shared task on\ngeneral machine translation. We participate in the constrained track in two\ndirections: English <-> Hebrew. In this competition, we show that by using one\nmodel to handle bidirectional tasks, as a minimal setting of Multilingual\nMachine Translation (MMT), it is possible to achieve comparable results with\nthat of traditional bilingual translation for both directions. By including\neffective strategies, like back-translation, re-parameterized embedding table,\nand task-oriented fine-tuning, we obtained competitive final results in the\nautomatic evaluation for both English -> Hebrew and Hebrew -> English\ndirections.", "published": "2023-10-15 20:49:31", "link": "http://arxiv.org/abs/2310.09946v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models for In-Context Student Modeling: Synthesizing\n  Student's Behavior in Visual Programming", "abstract": "Student modeling is central to many educational technologies as it enables\npredicting future learning outcomes and designing targeted instructional\nstrategies. However, open-ended learning domains pose challenges for accurately\nmodeling students due to the diverse behaviors and a large space of possible\nmisconceptions. To approach these challenges, we explore the application of\nlarge language models (LLMs) for in-context student modeling in open-ended\nlearning domains. More concretely, given a particular student's attempt on a\nreference task as observation, the objective is to synthesize the student's\nattempt on a target task. We introduce a novel framework, LLM for Student\nSynthesis (LLM-SS), that leverages LLMs for synthesizing a student's behavior.\nOur framework can be combined with different LLMs; moreover, we fine-tune LLMs\nto boost their student modeling capabilities. We instantiate several methods\nbased on LLM-SS framework and evaluate them using an existing benchmark,\nStudentSyn, for student attempt synthesis in a visual programming domain.\nExperimental results show that our methods perform significantly better than\nthe baseline method NeurSS provided in the StudentSyn benchmark. Furthermore,\nour method using a fine-tuned version of the GPT-3.5 model is significantly\nbetter than using the base GPT-3.5 model and gets close to human tutors'\nperformance.", "published": "2023-10-15 12:56:13", "link": "http://arxiv.org/abs/2310.10690v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Conversational Search: Large Language Model-Aided Informative\n  Query Rewriting", "abstract": "Query rewriting plays a vital role in enhancing conversational search by\ntransforming context-dependent user queries into standalone forms. Existing\napproaches primarily leverage human-rewritten queries as labels to train query\nrewriting models. However, human rewrites may lack sufficient information for\noptimal retrieval performance. To overcome this limitation, we propose\nutilizing large language models (LLMs) as query rewriters, enabling the\ngeneration of informative query rewrites through well-designed instructions. We\ndefine four essential properties for well-formed rewrites and incorporate all\nof them into the instruction. In addition, we introduce the role of rewrite\neditors for LLMs when initial query rewrites are available, forming a\n\"rewrite-then-edit\" process. Furthermore, we propose distilling the rewriting\ncapabilities of LLMs into smaller models to reduce rewriting latency. Our\nexperimental evaluation on the QReCC dataset demonstrates that informative\nquery rewrites can yield substantially improved retrieval performance compared\nto human rewrites, especially with sparse retrievers.", "published": "2023-10-15 03:04:17", "link": "http://arxiv.org/abs/2310.09716v2", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.HC"}
{"title": "When can transformers reason with abstract symbols?", "abstract": "We investigate the capabilities of transformer models on relational reasoning\ntasks. In these tasks, models are trained on a set of strings encoding abstract\nrelations, and are then tested out-of-distribution on data that contains\nsymbols that did not appear in the training dataset. We prove that for any\nrelational reasoning task in a large family of tasks, transformers learn the\nabstract relations and generalize to the test set when trained by gradient\ndescent on sufficiently large quantities of training data. This is in contrast\nto classical fully-connected networks, which we prove fail to learn to reason.\nOur results inspire modifications of the transformer architecture that add only\ntwo trainable parameters per head, and that we empirically demonstrate improve\ndata efficiency for learning to reason.", "published": "2023-10-15 06:45:38", "link": "http://arxiv.org/abs/2310.09753v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Homophone Disambiguation Reveals Patterns of Context Mixing in Speech\n  Transformers", "abstract": "Transformers have become a key architecture in speech processing, but our\nunderstanding of how they build up representations of acoustic and linguistic\nstructure is limited. In this study, we address this gap by investigating how\nmeasures of 'context-mixing' developed for text models can be adapted and\napplied to models of spoken language. We identify a linguistic phenomenon that\nis ideal for such a case study: homophony in French (e.g. livre vs livres),\nwhere a speech recognition model has to attend to syntactic cues such as\ndeterminers and pronouns in order to disambiguate spoken words with identical\npronunciations and transcribe them while respecting grammatical agreement. We\nperform a series of controlled experiments and probing analyses on\nTransformer-based speech models. Our findings reveal that representations in\nencoder-only models effectively incorporate these cues to identify the correct\ntranscription, whereas encoders in encoder-decoder models mainly relegate the\ntask of capturing contextual dependencies to decoder modules.", "published": "2023-10-15 19:24:13", "link": "http://arxiv.org/abs/2310.09925v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Chameleon: a Heterogeneous and Disaggregated Accelerator System for\n  Retrieval-Augmented Language Models", "abstract": "A Retrieval-Augmented Language Model (RALM) combines a large language model\n(LLM) with a vector database to retrieve context-specific knowledge during text\ngeneration. This strategy facilitates impressive generation quality even with\nsmaller models, thus reducing computational demands by orders of magnitude. To\nserve RALMs efficiently and flexibly, we propose Chameleon, a heterogeneous\naccelerator system integrating both LLM and vector search accelerators in a\ndisaggregated architecture. The heterogeneity ensures efficient serving for\nboth inference and retrieval, while the disaggregation allows independent\nscaling of LLM and vector search accelerators to fulfill diverse RALM\nrequirements. Our Chameleon prototype implements vector search accelerators on\nFPGAs and assigns LLM inference to GPUs, with CPUs as cluster coordinators.\nEvaluated on various RALMs, Chameleon exhibits up to 2.16$\\times$ reduction in\nlatency and 3.18x speedup in throughput compared to the hybrid CPU-GPU\narchitecture. The promising results pave the way for adopting heterogeneous\naccelerators for not only LLM inference but also vector search in future RALM\nsystems.", "published": "2023-10-15 20:57:25", "link": "http://arxiv.org/abs/2310.09949v4", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Farzi Data: Autoregressive Data Distillation", "abstract": "We study data distillation for auto-regressive machine learning tasks, where\nthe input and output have a strict left-to-right causal structure. More\nspecifically, we propose Farzi, which summarizes an event sequence dataset into\na small number of synthetic sequences -- Farzi Data -- which are optimized to\nmaintain (if not improve) model performance compared to training on the full\ndataset. Under the hood, Farzi conducts memory-efficient data distillation by\n(i) deriving efficient reverse-mode differentiation of the Adam optimizer by\nleveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional\ndiscrete event-space into a latent-space which provably promotes implicit\nregularization. Empirically, for sequential recommendation and language\nmodeling tasks, we are able to achieve 98-120% of downstream full-data\nperformance when training state-of-the-art models on Farzi Data of size as\nlittle as 0.1% of the original dataset. Notably, being able to train better\nmodels with significantly less data sheds light on the design of future large\nauto-regressive models, and opens up new opportunities to further scale up\nmodel and data sizes.", "published": "2023-10-15 23:23:27", "link": "http://arxiv.org/abs/2310.09983v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "CoCoFormer: A controllable feature-rich polyphonic music generation\n  method", "abstract": "This paper explores the modeling method of polyphonic music sequence. Due to\nthe great potential of Transformer models in music generation, controllable\nmusic generation is receiving more attention. In the task of polyphonic music,\ncurrent controllable generation research focuses on controlling the generation\nof chords, but lacks precise adjustment for the controllable generation of\nchoral music textures. This paper proposed Condition Choir Transformer\n(CoCoFormer) which controls the output of the model by controlling the chord\nand rhythm inputs at a fine-grained level. In this paper, the self-supervised\nmethod improves the loss function and performs joint training through\nconditional control input and unconditional input training. In order to\nalleviate the lack of diversity on generated samples caused by the teacher\nforcing training, this paper added an adversarial training method. CoCoFormer\nenhances model performance with explicit and implicit inputs to chords and\nrhythms. In this paper, the experiments proves that CoCoFormer has reached the\ncurrent better level than current models. On the premise of specifying the\npolyphonic music texture, the same melody can also be generated in a variety of\nways.", "published": "2023-10-15 14:04:48", "link": "http://arxiv.org/abs/2310.09843v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Large Vocabulary Spontaneous Speech Recognition for Tigrigna", "abstract": "This thesis proposes and describes a research attempt at designing and\ndeveloping a speaker independent spontaneous automatic speech recognition\nsystem for Tigrigna The acoustic model of the Speech Recognition System is\ndeveloped using Carnegie Mellon University Automatic Speech Recognition\ndevelopment tool (Sphinx) while the SRIM tool is used for the development of\nthe language model.\n  Keywords Automatic Speech Recognition Tigrigna language", "published": "2023-10-15 13:07:41", "link": "http://arxiv.org/abs/2402.04254v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "68T50 (Primary)", "H.1.2"], "primary_category": "eess.AS"}
{"title": "MERTech: Instrument Playing Technique Detection Using Self-Supervised\n  Pretrained Model With Multi-Task Finetuning", "abstract": "Instrument playing techniques (IPTs) constitute a pivotal component of\nmusical expression. However, the development of automatic IPT detection methods\nsuffers from limited labeled data and inherent class imbalance issues. In this\npaper, we propose to apply a self-supervised learning model pre-trained on\nlarge-scale unlabeled music data and finetune it on IPT detection tasks. This\napproach addresses data scarcity and class imbalance challenges. Recognizing\nthe significance of pitch in capturing the nuances of IPTs and the importance\nof onset in locating IPT events, we investigate multi-task finetuning with\npitch and onset detection as auxiliary tasks. Additionally, we apply a\npost-processing approach for event-level prediction, where an IPT activation\ninitiates an event only if the onset output confirms an onset in that frame.\nOur method outperforms prior approaches in both frame-level and event-level\nmetrics across multiple IPT benchmark datasets. Further experiments demonstrate\nthe efficacy of multi-task finetuning on each IPT class.", "published": "2023-10-15 15:00:00", "link": "http://arxiv.org/abs/2310.09853v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
