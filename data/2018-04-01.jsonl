{"title": "Training Tips for the Transformer Model", "abstract": "This article describes our experiments in neural machine translation using\nthe recent Tensor2Tensor framework and the Transformer sequence-to-sequence\nmodel (Vaswani et al., 2017). We examine some of the critical parameters that\naffect the final translation quality, memory usage, training stability and\ntraining time, concluding each experiment with a set of recommendations for\nfellow researchers. In addition to confirming the general mantra \"more data and\nlarger models\", we address scaling to multiple GPUs and provide practical tips\nfor improved training regarding batch size, learning rate, warmup steps,\nmaximum sentence length and checkpoint averaging. We hope that our observations\nwill allow others to get better results given their particular hardware and\ndata constraints.", "published": "2018-04-01 01:59:52", "link": "http://arxiv.org/abs/1804.00247v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Completely Unsupervised Phoneme Recognition by Adversarially Learning\n  Mapping Relationships from Audio Embeddings", "abstract": "Unsupervised discovery of acoustic tokens from audio corpora without\nannotation and learning vector representations for these tokens have been\nwidely studied. Although these techniques have been shown successful in some\napplications such as query-by-example Spoken Term Detection (STD), the lack of\nmapping relationships between these discovered tokens and real phonemes have\nlimited the down-stream applications. This paper represents probably the first\nattempt towards the goal of completely unsupervised phoneme recognition, or\nmapping audio signals to phoneme sequences without phoneme-labeled audio data.\nThe basic idea is to cluster the embedded acoustic tokens and learn the mapping\nbetween the cluster sequences and the unknown phoneme sequences with a\nGenerative Adversarial Network (GAN). An unsupervised phoneme recognition\naccuracy of 36% was achieved in the preliminary experiments.", "published": "2018-04-01 16:35:21", "link": "http://arxiv.org/abs/1804.00316v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Learning of Interactive Spoken Content Retrieval and Trainable\n  User Simulator", "abstract": "User-machine interaction is crucial for information retrieval, especially for\nspoken content retrieval, because spoken content is difficult to browse, and\nspeech recognition has a high degree of uncertainty. In interactive retrieval,\nthe machine takes different actions to interact with the user to obtain better\nretrieval results; here it is critical to select the most efficient action. In\nprevious work, deep Q-learning techniques were proposed to train an interactive\nretrieval system but rely on a hand-crafted user simulator; building a reliable\nuser simulator is difficult. In this paper, we further improve the interactive\nspoken content retrieval framework by proposing a learnable user simulator\nwhich is jointly trained with interactive retrieval system, making the\nhand-crafted user simulator unnecessary. The experimental results show that the\nlearned simulated users not only achieve larger rewards than the hand-crafted\nones but act more like real users.", "published": "2018-04-01 16:46:53", "link": "http://arxiv.org/abs/1804.00318v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Spoken SQuAD: A Study of Mitigating the Impact of Speech Recognition\n  Errors on Listening Comprehension", "abstract": "Reading comprehension has been widely studied. One of the most representative\nreading comprehension tasks is Stanford Question Answering Dataset (SQuAD), on\nwhich machine is already comparable with human. On the other hand, accessing\nlarge collections of multimedia or spoken content is much more difficult and\ntime-consuming than plain text content for humans. It's therefore highly\nattractive to develop machines which can automatically understand spoken\ncontent. In this paper, we propose a new listening comprehension task - Spoken\nSQuAD. On the new task, we found that speech recognition errors have\ncatastrophic impact on machine comprehension, and several approaches are\nproposed to mitigate the impact.", "published": "2018-04-01 17:12:47", "link": "http://arxiv.org/abs/1804.00320v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Marian: Fast Neural Machine Translation in C++", "abstract": "We present Marian, an efficient and self-contained Neural Machine Translation\nframework with an integrated automatic differentiation engine based on dynamic\ncomputation graphs. Marian is written entirely in C++. We describe the design\nof the encoder-decoder framework and demonstrate that a research-friendly\ntoolkit can achieve high training and translation speed.", "published": "2018-04-01 20:50:57", "link": "http://arxiv.org/abs/1804.00344v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CIKM AnalytiCup 2017 Lazada Product Title Quality Challenge An Ensemble\n  of Deep and Shallow Learning to predict the Quality of Product Titles", "abstract": "We present an approach where two different models (Deep and Shallow) are\ntrained separately on the data and a weighted average of the outputs is taken\nas the final result. For the Deep approach, we use different combinations of\nmodels like Convolution Neural Network, pretrained word2vec embeddings and\nLSTMs to get representations which are then used to train a Deep Neural\nNetwork. For Clarity prediction, we also use an Attentive Pooling approach for\nthe pooling operation so as to be aware of the Title-Category pair. For the\nshallow approach, we use boosting technique LightGBM on features generated\nusing title and categories. We find that an ensemble of these approaches does a\nbetter job than using them alone suggesting that the results of the deep and\nshallow approach are highly complementary", "published": "2018-04-01 13:02:57", "link": "http://arxiv.org/abs/1804.01000v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Revisiting Skip-Gram Negative Sampling Model with Rectification", "abstract": "We revisit skip-gram negative sampling (SGNS), one of the most popular\nneural-network based approaches to learning distributed word representation. We\nfirst point out the ambiguity issue undermining the SGNS model, in the sense\nthat the word vectors can be entirely distorted without changing the objective\nvalue. To resolve the issue, we investigate the intrinsic structures in\nsolution that a good word embedding model should deliver. Motivated by this, we\nrectify the SGNS model with quadratic regularization, and show that this simple\nmodification suffices to structure the solution in the desired manner. A\ntheoretical justification is presented, which provides novel insights into\nquadratic regularization . Preliminary experiments are also conducted on\nGoogle's analytical reasoning task to support the modified SGNS model.", "published": "2018-04-01 15:41:01", "link": "http://arxiv.org/abs/1804.00306v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "I-vector Transformation Using Conditional Generative Adversarial\n  Networks for Short Utterance Speaker Verification", "abstract": "I-vector based text-independent speaker verification (SV) systems often have\npoor performance with short utterances, as the biased phonetic distribution in\na short utterance makes the extracted i-vector unreliable. This paper proposes\nan i-vector compensation method using a generative adversarial network (GAN),\nwhere its generator network is trained to generate a compensated i-vector from\na short-utterance i-vector and its discriminator network is trained to\ndetermine whether an i-vector is generated by the generator or the one\nextracted from a long utterance. Additionally, we assign two other learning\ntasks to the GAN to stabilize its training and to make the generated ivector\nmore speaker-specific. Speaker verification experiments on the NIST SRE 2008\n\"10sec-10sec\" condition show that our method reduced the equal error rate by\n11.3% from the conventional i-vector and PLDA system.", "published": "2018-04-01 12:36:03", "link": "http://arxiv.org/abs/1804.00290v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
