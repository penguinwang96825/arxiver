{"title": "DialBERT: A Hierarchical Pre-Trained Model for Conversation\n  Disentanglement", "abstract": "Disentanglement is a problem in which multiple conversations occur in the\nsame channel simultaneously, and the listener should decide which utterance is\npart of the conversation he will respond to. We propose a new model, named\nDialogue BERT (DialBERT), which integrates local and global semantics in a\nsingle stream of messages to disentangle the conversations that mixed together.\nWe employ BERT to capture the matching information in each utterance pair at\nthe utterance-level, and use a BiLSTM to aggregate and incorporate the\ncontext-level information. With only a 3% increase in parameters, a 12%\nimprovement has been attained in comparison to BERT, based on the F1-Score. The\nmodel achieves a state-of-the-art result on the a new dataset proposed by IBM\nand surpasses previous work by a substantial margin.", "published": "2020-04-08 00:54:01", "link": "http://arxiv.org/abs/2004.03760v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CALM: Continuous Adaptive Learning for Language Modeling", "abstract": "Training large language representation models has become a standard in the\nnatural language processing community. This allows for fine tuning on any\nnumber of specific tasks, however, these large high capacity models can\ncontinue to train on domain specific unlabeled data to make initialization even\nmore robust for supervised tasks. We demonstrate that in practice these\npre-trained models present performance deterioration in the form of\ncatastrophic forgetting when evaluated on tasks from a general domain such as\nGLUE. In this work we propose CALM, Continuous Adaptive Learning for Language\nModeling: techniques to render models which retain knowledge across multiple\ndomains. With these methods, we are able to reduce the performance gap across\nsupervised tasks introduced by task specific models which we demonstrate using\na continual learning setting in biomedical and clinical domains.", "published": "2020-04-08 03:51:17", "link": "http://arxiv.org/abs/2004.03794v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Versatile Generative Language Model Via Parameter-Efficient\n  Transfer Learning", "abstract": "Fine-tuning pre-trained generative language models to down-stream language\ngeneration tasks has shown promising results. However, this comes with the cost\nof having a single, large model for each task, which is not ideal in\nlow-memory/power scenarios (e.g., mobile). In this paper, we propose an\neffective way to fine-tune multiple down-stream generation tasks simultaneously\nusing a single, large pre-trained model. The experiments on five diverse\nlanguage generation tasks show that by just using an additional 2-3% parameters\nfor each task, our model can maintain or even improve the performance of\nfine-tuning the whole model.", "published": "2020-04-08 06:18:44", "link": "http://arxiv.org/abs/2004.03829v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ShanghaiTech at MRP 2019: Sequence-to-Graph Transduction with\n  Second-Order Edge Inference for Cross-Framework Meaning Representation\n  Parsing", "abstract": "This paper presents the system used in our submission to the \\textit{CoNLL\n2019 shared task: Cross-Framework Meaning Representation Parsing}. Our system\nis a graph-based parser which combines an extended pointer-generator network\nthat generates nodes and a second-order mean field variational inference module\nthat predicts edges. Our system achieved \\nth{1} and \\nth{2} place for the DM\nand PSD frameworks respectively on the in-framework ranks and achieved \\nth{3}\nplace for the DM framework on the cross-framework ranks.", "published": "2020-04-08 07:19:18", "link": "http://arxiv.org/abs/2004.03849v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diverse, Controllable, and Keyphrase-Aware: A Corpus and Method for News\n  Multi-Headline Generation", "abstract": "News headline generation aims to produce a short sentence to attract readers\nto read the news. One news article often contains multiple keyphrases that are\nof interest to different users, which can naturally have multiple reasonable\nheadlines. However, most existing methods focus on the single headline\ngeneration. In this paper, we propose generating multiple headlines with\nkeyphrases of user interests, whose main idea is to generate multiple\nkeyphrases of interest to users for the news first, and then generate multiple\nkeyphrase-relevant headlines. We propose a multi-source Transformer decoder,\nwhich takes three sources as inputs: (a) keyphrase, (b) keyphrase-filtered\narticle, and (c) original article to generate keyphrase-relevant, high-quality,\nand diverse headlines. Furthermore, we propose a simple and effective method to\nmine the keyphrases of interest in the news article and build a first\nlarge-scale keyphrase-aware news headline corpus, which contains over 180K\naligned triples of $<$news article, headline, keyphrase$>$. Extensive\nexperimental comparisons on the real-world dataset show that the proposed\nmethod achieves state-of-the-art results in terms of quality and diversity", "published": "2020-04-08 08:30:05", "link": "http://arxiv.org/abs/2004.03875v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep daxes: Mutual exclusivity arises through both learning biases and\n  pragmatic strategies in neural networks", "abstract": "Children's tendency to associate novel words with novel referents has been\ntaken to reflect a bias toward mutual exclusivity. This tendency may be\nadvantageous both as (1) an ad-hoc referent selection heuristic to single out\nreferents lacking a label and as (2) an organizing principle of lexical\nacquisition. This paper investigates under which circumstances\ncross-situational neural models can come to exhibit analogous behavior to\nchildren, focusing on these two possibilities and their interaction. To this\nend, we evaluate neural networks' on both symbolic data and, as a first, on\nlarge-scale image data. We find that constraints in both learning and selection\ncan foster mutual exclusivity, as long as they put words in competition for\nlexical meaning. For computational models, these findings clarify the role of\navailable options for better performance in tasks where mutual exclusivity is\nadvantageous. For cognitive research, they highlight latent interactions\nbetween word learning, referent selection mechanisms, and the structure of\nstimuli of varying complexity: symbolic and visual.", "published": "2020-04-08 09:34:24", "link": "http://arxiv.org/abs/2004.03902v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rapformer: Conditional Rap Lyrics Generation with Denoising Autoencoders", "abstract": "The ability to combine symbols to generate language is a defining\ncharacteristic of human intelligence, particularly in the context of artistic\nstory-telling through lyrics. We develop a method for synthesizing a rap verse\nbased on the content of any text (e.g., a news article), or for augmenting\npre-existing rap lyrics. Our method, called Rapformer, is based on training a\nTransformer-based denoising autoencoder to reconstruct rap lyrics from content\nwords extracted from the lyrics, trying to preserve the essential meaning,\nwhile matching the target style. Rapformer features a novel BERT-based\nparaphrasing scheme for rhyme enhancement which increases the average rhyme\ndensity of output lyrics by 10%. Experimental results on three diverse input\ndomains show that Rapformer is capable of generating technically fluent verses\nthat offer a good trade-off between content preservation and style transfer.\nFurthermore, a Turing-test-like experiment reveals that Rapformer fools human\nlyrics experts 25% of the time.", "published": "2020-04-08 12:24:10", "link": "http://arxiv.org/abs/2004.03965v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-training is a Hot Topic: Contextualized Document Embeddings Improve\n  Topic Coherence", "abstract": "Topic models extract groups of words from documents, whose interpretation as\na topic hopefully allows for a better understanding of the data. However, the\nresulting word groups are often not coherent, making them harder to interpret.\nRecently, neural topic models have shown improvements in overall coherence.\nConcurrently, contextual embeddings have advanced the state of the art of\nneural models in general. In this paper, we combine contextualized\nrepresentations with neural topic models. We find that our approach produces\nmore meaningful and coherent topics than traditional bag-of-words topic models\nand recent neural models. Our results indicate that future improvements in\nlanguage models will translate into better topic models.", "published": "2020-04-08 12:37:51", "link": "http://arxiv.org/abs/2004.03974v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transfer learning and subword sampling for asymmetric-resource\n  one-to-many neural translation", "abstract": "There are several approaches for improving neural machine translation for\nlow-resource languages: Monolingual data can be exploited via pretraining or\ndata augmentation; Parallel corpora on related language pairs can be used via\nparameter sharing or transfer learning in multilingual models; Subword\nsegmentation and regularization techniques can be applied to ensure high\ncoverage of the vocabulary. We review these approaches in the context of an\nasymmetric-resource one-to-many translation task, in which the pair of target\nlanguages are related, with one being a very low-resource and the other a\nhigher-resource language. We test various methods on three artificially\nrestricted translation tasks -- English to Estonian (low-resource) and Finnish\n(high-resource), English to Slovak and Czech, English to Danish and Swedish --\nand one real-world task, Norwegian to North S\\'ami and Finnish. The experiments\nshow positive effects especially for scheduled multi-task learning, denoising\nautoencoder, and subword sampling.", "published": "2020-04-08 14:19:05", "link": "http://arxiv.org/abs/2004.04002v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Attention Gazetteer Embeddings for Named-Entity Recognition", "abstract": "Recent attempts to ingest external knowledge into neural models for\nnamed-entity recognition (NER) have exhibited mixed results. In this work, we\npresent GazSelfAttn, a novel gazetteer embedding approach that uses\nself-attention and match span encoding to build enhanced gazetteer embeddings.\nIn addition, we demonstrate how to build gazetteer resources from the open\nsource Wikidata knowledge base. Evaluations on CoNLL-03 and Ontonotes 5\ndatasets, show F1 improvements over baseline model from 92.34 to 92.86 and\n89.11 to 89.32 respectively, achieving performance comparable to large\nstate-of-the-art models.", "published": "2020-04-08 15:31:26", "link": "http://arxiv.org/abs/2004.04060v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are All Good Word Vector Spaces Isomorphic?", "abstract": "Existing algorithms for aligning cross-lingual word vector spaces assume that\nvector spaces are approximately isomorphic. As a result, they perform poorly or\nfail completely on non-isomorphic spaces. Such non-isomorphism has been\nhypothesised to result from typological differences between languages. In this\nwork, we ask whether non-isomorphism is also crucially a sign of degenerate\nword vector spaces. We present a series of experiments across diverse languages\nwhich show that variance in performance across language pairs is not only due\nto typological differences, but can mostly be attributed to the size of the\nmonolingual resources available, and to the properties and duration of\nmonolingual training (e.g. \"under-training\").", "published": "2020-04-08 15:49:19", "link": "http://arxiv.org/abs/2004.04070v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Emotion Intensity Prediction", "abstract": "Emotion intensity prediction determines the degree or intensity of an emotion\nthat the author expresses in a text, extending previous categorical approaches\nto emotion detection. While most previous work on this topic has concentrated\non English texts, other languages would also benefit from fine-grained emotion\nclassification, preferably without having to recreate the amount of annotated\ndata available in English in each new language. Consequently, we explore\ncross-lingual transfer approaches for fine-grained emotion detection in Spanish\nand Catalan tweets. To this end we annotate a test set of Spanish and Catalan\ntweets using Best-Worst scaling. We compare six cross-lingual approaches, e.g.,\nmachine translation and cross-lingual embeddings, which have varying\nrequirements for parallel data -- from millions of parallel sentences to\ncompletely unsupervised. The results show that on this data, methods with low\nparallel-data requirements perform surprisingly better than methods that use\nmore parallel data, which we explain through an in-depth error analysis. We\nmake the dataset and the code available at\n\\url{https://github.com/jerbarnes/fine-grained_cross-lingual_emotion}", "published": "2020-04-08 16:28:16", "link": "http://arxiv.org/abs/2004.04103v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Frequency, Acceptability, and Selection: A case study of\n  clause-embedding", "abstract": "We investigate the relationship between the frequency with which verbs are\nfound in particular subcategorization frames and the acceptability of those\nverbs in those frames, focusing in particular on subordinate clause-taking\nverbs, such as \"think\", \"want\", and \"tell\". We show that verbs'\nsubcategorization frame frequency distributions are poor predictors of their\nacceptability in those frames---explaining, at best, less than 1/3 of the total\ninformation about acceptability across the lexicon---and, further, that common\nmatrix factorization techniques used to model the acquisition of verbs'\nacceptability in subcategorization frames fare only marginally better. All data\nand code are available at http://megaattitude.io.", "published": "2020-04-08 16:34:19", "link": "http://arxiv.org/abs/2004.04106v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity-Switched Datasets: An Approach to Auditing the In-Domain\n  Robustness of Named Entity Recognition Models", "abstract": "Named entity recognition systems perform well on standard datasets comprising\nEnglish news. But given the paucity of data, it is difficult to draw\nconclusions about the robustness of systems with respect to recognizing a\ndiverse set of entities. We propose a method for auditing the in-domain\nrobustness of systems, focusing specifically on differences in performance due\nto the national origin of entities. We create entity-switched datasets, in\nwhich named entities in the original texts are replaced by plausible named\nentities of the same type but of different national origin. We find that\nstate-of-the-art systems' performance vary widely even in-domain: In the same\ncontext, entities from certain origins are more reliably recognized than\nentities from elsewhere. Systems perform best on American and Indian entities,\nand worst on Vietnamese and Indonesian entities. This auditing approach can\nfacilitate the development of more robust named entity recognition systems, and\nwill allow research in this area to consider fairness criteria that have\nreceived heightened attention in other predictive technology work.", "published": "2020-04-08 17:11:31", "link": "http://arxiv.org/abs/2004.04123v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Asking and Answering Questions to Evaluate the Factual Consistency of\n  Summaries", "abstract": "Practical applications of abstractive summarization models are limited by\nfrequent factual inconsistencies with respect to their input. Existing\nautomatic evaluation metrics for summarization are largely insensitive to such\nerrors. We propose an automatic evaluation protocol called QAGS (pronounced\n\"kags\") that is designed to identify factual inconsistencies in a generated\nsummary. QAGS is based on the intuition that if we ask questions about a\nsummary and its source, we will receive similar answers if the summary is\nfactually consistent with the source. To evaluate QAGS, we collect human\njudgments of factual consistency on model-generated summaries for the\nCNN/DailyMail (Hermann et al., 2015) and XSUM (Narayan et al., 2018)\nsummarization datasets. QAGS has substantially higher correlations with these\njudgments than other automatic evaluation metrics. Also, QAGS offers a natural\nform of interpretability: The answers and questions generated while computing\nQAGS indicate which tokens of a summary are inconsistent and why. We believe\nQAGS is a promising tool in automatically generating usable and factually\nconsistent text.", "published": "2020-04-08 20:01:09", "link": "http://arxiv.org/abs/2004.04228v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Error correction and extraction in request dialogs", "abstract": "We propose a dialog system utility component that gets the last two\nutterances of a user and can detect whether the last utterance is an error\ncorrection of the second last utterance. If yes, it corrects the second last\nutterance according to the error correction in the last utterance and outputs\nthe extracted pairs of reparandum and repair entity. This component offers two\nadvantages, learning the concept of corrections to avoid collecting corrections\nfor every new domain and extracting reparandum and repair pairs, which offers\nthe possibility to learn out of it.\n  For the error correction one sequence labeling and two sequence to sequence\napproaches are presented. For the error correction detection these three error\ncorrection approaches can also be used and in addition, we present a sequence\nclassification approach. One error correction detection and one error\ncorrection approach can be combined to a pipeline or the error correction\napproaches can be trained and used end-to-end to avoid two components. We\nmodified the EPIC-KITCHENS-100 dataset to evaluate the approaches for\ncorrecting entity phrases in request dialogs. For error correction detection\nand correction, we got an accuracy of 96.40 % on synthetic validation data and\nan accuracy of 77.81 % on human-created real-world test data.", "published": "2020-04-08 20:49:10", "link": "http://arxiv.org/abs/2004.04243v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Spotify Podcast Dataset", "abstract": "Podcasts are a relatively new form of audio media. Episodes appear on a\nregular cadence, and come in many different formats and levels of formality.\nThey can be formal news journalism or conversational chat; fiction or\nnon-fiction. They are rapidly growing in popularity and yet have been\nrelatively little studied. As an audio format, podcasts are more varied in\nstyle and production types than, say, broadcast news, and contain many more\ngenres than typically studied in video research. The medium is therefore a rich\ndomain with many research avenues for the IR and NLP communities. We present\nthe Spotify Podcast Dataset, a set of approximately 100K podcast episodes\ncomprised of raw audio files along with accompanying ASR transcripts. This\nrepresents over 47,000 hours of transcribed audio, and is an order of magnitude\nlarger than previous speech-to-text corpora.", "published": "2020-04-08 21:25:00", "link": "http://arxiv.org/abs/2004.04270v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Severing the Edge Between Before and After: Neural Architectures for\n  Temporal Ordering of Events", "abstract": "In this paper, we propose a neural architecture and a set of training methods\nfor ordering events by predicting temporal relations. Our proposed models\nreceive a pair of events within a span of text as input and they identify\ntemporal relations (Before, After, Equal, Vague) between them. Given that a key\nchallenge with this task is the scarcity of annotated data, our models rely on\neither pretrained representations (i.e. RoBERTa, BERT or ELMo), transfer and\nmulti-task learning (by leveraging complementary datasets), and self-training\ntechniques. Experiments on the MATRES dataset of English documents establish a\nnew state-of-the-art on this task.", "published": "2020-04-08 23:17:10", "link": "http://arxiv.org/abs/2004.04295v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation\n  with Semantic Fidelity", "abstract": "End-to-end neural data-to-text (D2T) generation has recently emerged as an\nalternative to pipeline-based architectures. However, it has faced challenges\nin generalizing to new domains and generating semantically consistent text. In\nthis work, we present DataTuner, a neural, end-to-end data-to-text generation\nsystem that makes minimal assumptions about the data representation and the\ntarget domain. We take a two-stage generation-reranking approach, combining a\nfine-tuned language model with a semantic fidelity classifier. Each of our\ncomponents is learnt end-to-end without the need for dataset-specific\nheuristics, entity delexicalization, or post-processing. We show that DataTuner\nachieves state of the art results on the automated metrics across four major\nD2T datasets (LDC2017T10, WebNLG, ViGGO, and Cleaned E2E), with a fluency\nassessed by human annotators nearing or exceeding the human-written reference\ntexts. We further demonstrate that the model-based semantic fidelity scorer in\nDataTuner is a better assessment tool compared to traditional, heuristic-based\nmeasures. Our generated text has a significantly better semantic fidelity than\nthe state of the art across all four datasets", "published": "2020-04-08 11:16:53", "link": "http://arxiv.org/abs/2004.06577v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Narrative Text in a Switching Dynamical System", "abstract": "Early work on narrative modeling used explicit plans and goals to generate\nstories, but the language generation itself was restricted and inflexible.\nModern methods use language models for more robust generation, but often lack\nan explicit representation of the scaffolding and dynamics that guide a\ncoherent narrative. This paper introduces a new model that integrates explicit\nnarrative structure with neural language models, formalizing narrative modeling\nas a Switching Linear Dynamical System (SLDS). A SLDS is a dynamical system in\nwhich the latent dynamics of the system (i.e. how the state vector transforms\nover time) is controlled by top-level discrete switching variables. The\nswitching variables represent narrative structure (e.g., sentiment or discourse\nstates), while the latent state vector encodes information on the current state\nof the narrative. This probabilistic formulation allows us to control\ngeneration, and can be learned in a semi-supervised fashion using both labeled\nand unlabeled data. Additionally, we derive a Gibbs sampler for our model that\ncan fill in arbitrary parts of the narrative, guided by the switching\nvariables. Our filled-in (English language) narratives outperform several\nbaselines on both automatic and human evaluations.", "published": "2020-04-08 01:05:19", "link": "http://arxiv.org/abs/2004.03762v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Downstream Model Design of Pre-trained Language Model for Relation\n  Extraction Task", "abstract": "Supervised relation extraction methods based on deep neural network play an\nimportant role in the recent information extraction field. However, at present,\ntheir performance still fails to reach a good level due to the existence of\ncomplicated relations. On the other hand, recently proposed pre-trained\nlanguage models (PLMs) have achieved great success in multiple tasks of natural\nlanguage processing through fine-tuning when combined with the model of\ndownstream tasks. However, original standard tasks of PLM do not include the\nrelation extraction task yet. We believe that PLMs can also be used to solve\nthe relation extraction problem, but it is necessary to establish a specially\ndesigned downstream task model or even loss function for dealing with\ncomplicated relations. In this paper, a new network architecture with a special\nloss function is designed to serve as a downstream model of PLMs for supervised\nrelation extraction. Experiments have shown that our method significantly\nexceeded the current optimal baseline models across multiple public datasets of\nrelation extraction.", "published": "2020-04-08 03:16:06", "link": "http://arxiv.org/abs/2004.03786v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SciWING -- A Software Toolkit for Scientific Document Processing", "abstract": "We introduce SciWING, an open-source software toolkit which provides access\nto pre-trained models for scientific document processing tasks, inclusive of\ncitation string parsing and logical structure recovery. SciWING enables\nresearchers to rapidly experiment with different models by swapping and\nstacking different modules. It also enables them declare and run models from a\nconfiguration file. It enables researchers to perform production-ready transfer\nlearning from general, pre-trained transformers (i.e., BERT, SciBERT etc), and\naids development of end-user applications. It includes ready-to-use web and\nterminal-based applications and demonstrations (Available from\nhttp://sciwing.io).", "published": "2020-04-08 04:43:37", "link": "http://arxiv.org/abs/2004.03807v2", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Improving BERT with Self-Supervised Attention", "abstract": "One of the most popular paradigms of applying large pre-trained NLP models\nsuch as BERT is to fine-tune it on a smaller dataset. However, one challenge\nremains as the fine-tuned model often overfits on smaller datasets. A symptom\nof this phenomenon is that irrelevant or misleading words in the sentence,\nwhich are easy to understand for human beings, can substantially degrade the\nperformance of these finetuned BERT models. In this paper, we propose a novel\ntechnique, called Self-Supervised Attention (SSA) to help facilitate this\ngeneralization challenge. Specifically, SSA automatically generates weak,\ntoken-level attention labels iteratively by probing the fine-tuned model from\nthe previous iteration. We investigate two different ways of integrating SSA\ninto BERT and propose a hybrid approach to combine their benefits. Empirically,\nthrough a variety of public datasets, we illustrate significant performance\nimprovement using our SSA-enhanced BERT model.", "published": "2020-04-08 04:48:44", "link": "http://arxiv.org/abs/2004.03808v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-Agent Task-Oriented Dialog Policy Learning with Role-Aware Reward\n  Decomposition", "abstract": "Many studies have applied reinforcement learning to train a dialog policy and\nshow great promise these years. One common approach is to employ a user\nsimulator to obtain a large number of simulated user experiences for\nreinforcement learning algorithms. However, modeling a realistic user simulator\nis challenging. A rule-based simulator requires heavy domain expertise for\ncomplex tasks, and a data-driven simulator requires considerable data and it is\neven unclear how to evaluate a simulator. To avoid explicitly building a user\nsimulator beforehand, we propose Multi-Agent Dialog Policy Learning, which\nregards both the system and the user as the dialog agents. Two agents interact\nwith each other and are jointly learned simultaneously. The method uses the\nactor-critic framework to facilitate pretraining and improve scalability. We\nalso propose Hybrid Value Network for the role-aware reward decomposition to\nintegrate role-specific domain knowledge of each agent in the task-oriented\ndialog. Results show that our method can successfully build a system policy and\na user policy simultaneously, and two agents can achieve a high task success\nrate through conversational interaction.", "published": "2020-04-08 04:51:40", "link": "http://arxiv.org/abs/2004.03809v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Explicit Reordering for Neural Machine Translation", "abstract": "In Transformer-based neural machine translation (NMT), the positional\nencoding mechanism helps the self-attention networks to learn the source\nrepresentation with order dependency, which makes the Transformer-based NMT\nachieve state-of-the-art results for various translation tasks. However,\nTransformer-based NMT only adds representations of positions sequentially to\nword vectors in the input sentence and does not explicitly consider reordering\ninformation in this sentence. In this paper, we first empirically investigate\nthe relationship between source reordering information and translation\nperformance. The empirical findings show that the source input with the target\norder learned from the bilingual parallel dataset can substantially improve\ntranslation performance. Thus, we propose a novel reordering method to\nexplicitly model this reordering information for the Transformer-based NMT. The\nempirical results on the WMT14 English-to-German, WAT ASPEC\nJapanese-to-English, and WMT17 Chinese-to-English translation tasks show the\neffectiveness of the proposed approach.", "published": "2020-04-08 05:28:46", "link": "http://arxiv.org/abs/2004.03818v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SIA: A Scalable Interoperable Annotation Server for Biomedical Named\n  Entities", "abstract": "Recent years showed a strong increase in biomedical sciences and an inherent\nincrease in publication volume. Extraction of specific information from these\nsources requires highly sophisticated text mining and information extraction\ntools. However, the integration of freely available tools into customized\nworkflows is often cumbersome and difficult. We describe SIA (Scalable\nInteroperable Annotation Server), our contribution to the BeCalm-Technical\ninteroperability and performance of annotation servers (BeCalm-TIPS) task, a\nscalable, extensible, and robust annotation service. The system currently\ncovers six named entity types (i.e., Chemicals, Diseases, Genes, miRNA,\nMutations, and Organisms) and is freely available under Apache 2.0 license at\nhttps://github.com/Erechtheus/sia.", "published": "2020-04-08 05:44:55", "link": "http://arxiv.org/abs/2004.03822v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "On the Effect of Dropping Layers of Pre-trained Transformer Models", "abstract": "Transformer-based NLP models are trained using hundreds of millions or even\nbillions of parameters, limiting their applicability in computationally\nconstrained environments. While the number of parameters generally correlates\nwith performance, it is not clear whether the entire network is required for a\ndownstream task. Motivated by the recent work on pruning and distilling\npre-trained models, we explore strategies to drop layers in pre-trained models,\nand observe the effect of pruning on downstream GLUE tasks. We were able to\nprune BERT, RoBERTa and XLNet models up to 40%, while maintaining up to 98% of\ntheir original performance. Additionally we show that our pruned models are on\npar with those built using knowledge distillation, both in terms of size and\nperformance. Our experiments yield interesting observations such as, (i) the\nlower layers are most critical to maintain downstream task performance, (ii)\nsome tasks such as paraphrase detection and sentence similarity are more robust\nto the dropping of layers, and (iii) models trained using a different objective\nfunction exhibit different learning patterns and w.r.t the layer dropping.", "published": "2020-04-08 07:09:59", "link": "http://arxiv.org/abs/2004.03844v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Internal and external pressures on language emergence: least effort,\n  object constancy and frequency", "abstract": "In previous work, artificial agents were shown to achieve almost perfect\naccuracy in referential games where they have to communicate to identify\nimages. Nevertheless, the resulting communication protocols rarely display\nsalient features of natural languages, such as compositionality. In this paper,\nwe propose some realistic sources of pressure on communication that avert this\noutcome. More specifically, we formalise the principle of least effort through\nan auxiliary objective. Moreover, we explore several game variants, inspired by\nthe principle of object constancy, in which we alter the frequency, position,\nand luminosity of the objects in the images. We perform an extensive analysis\non their effect through compositionality metrics, diagnostic classifiers, and\nzero-shot evaluation. Our findings reveal that the proposed sources of pressure\nresult in emerging languages with less redundancy, more focus on high-level\nconceptual information, and better abilities of generalisation. Overall, our\ncontributions reduce the gap between emergent and natural languages.", "published": "2020-04-08 08:12:41", "link": "http://arxiv.org/abs/2004.03868v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Analyzing Redundancy in Pretrained Transformer Models", "abstract": "Transformer-based deep NLP models are trained using hundreds of millions of\nparameters, limiting their applicability in computationally constrained\nenvironments. In this paper, we study the cause of these limitations by\ndefining a notion of Redundancy, which we categorize into two classes: General\nRedundancy and Task-specific Redundancy. We dissect two popular pretrained\nmodels, BERT and XLNet, studying how much redundancy they exhibit at a\nrepresentation-level and at a more fine-grained neuron-level. Our analysis\nreveals interesting insights, such as: i) 85% of the neurons across the network\nare redundant and ii) at least 92% of them can be removed when optimizing\ntowards a downstream task. Based on our analysis, we present an efficient\nfeature-based transfer learning procedure, which maintains 97% performance\nwhile using at-most 10% of the original neurons.", "published": "2020-04-08 14:29:23", "link": "http://arxiv.org/abs/2004.04010v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DynaBERT: Dynamic BERT with Adaptive Width and Depth", "abstract": "The pre-trained language models like BERT, though powerful in many natural\nlanguage processing tasks, are both computation and memory expensive. To\nalleviate this problem, one approach is to compress them for specific tasks\nbefore deployment. However, recent works on BERT compression usually compress\nthe large BERT model to a fixed smaller size. They can not fully satisfy the\nrequirements of different edge devices with various hardware performances. In\nthis paper, we propose a novel dynamic BERT model (abbreviated as DynaBERT),\nwhich can flexibly adjust the size and latency by selecting adaptive width and\ndepth. The training process of DynaBERT includes first training a\nwidth-adaptive BERT and then allowing both adaptive width and depth, by\ndistilling knowledge from the full-sized model to small sub-networks. Network\nrewiring is also used to keep the more important attention heads and neurons\nshared by more sub-networks. Comprehensive experiments under various efficiency\nconstraints demonstrate that our proposed dynamic BERT (or RoBERTa) at its\nlargest size has comparable performance as BERT-base (or RoBERTa-base), while\nat smaller widths and depths consistently outperforms existing BERT compression\nmethods. Code is available at\nhttps://github.com/huawei-noah/Pretrained-Language-Model/tree/master/DynaBERT.", "published": "2020-04-08 15:06:28", "link": "http://arxiv.org/abs/2004.04037v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KdConv: A Chinese Multi-domain Dialogue Dataset Towards Multi-turn\n  Knowledge-driven Conversation", "abstract": "The research of knowledge-driven conversational systems is largely limited\ndue to the lack of dialog data which consist of multi-turn conversations on\nmultiple topics and with knowledge annotations. In this paper, we propose a\nChinese multi-domain knowledge-driven conversation dataset, KdConv, which\ngrounds the topics in multi-turn conversations to knowledge graphs. Our corpus\ncontains 4.5K conversations from three domains (film, music, and travel), and\n86K utterances with an average turn number of 19.0. These conversations contain\nin-depth discussions on related topics and natural transition between multiple\ntopics. To facilitate the following research on this corpus, we provide several\nbenchmark models. Comparative results show that the models can be enhanced by\nintroducing background knowledge, yet there is still a large space for\nleveraging knowledge to model multi-turn conversations for further research.\nResults also show that there are obvious performance differences between\ndifferent domains, indicating that it is worth to further explore transfer\nlearning and domain adaptation. The corpus and benchmark models are publicly\navailable.", "published": "2020-04-08 16:25:39", "link": "http://arxiv.org/abs/2004.04100v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LadaBERT: Lightweight Adaptation of BERT through Hybrid Model\n  Compression", "abstract": "BERT is a cutting-edge language representation model pre-trained by a large\ncorpus, which achieves superior performances on various natural language\nunderstanding tasks. However, a major blocking issue of applying BERT to online\nservices is that it is memory-intensive and leads to unsatisfactory latency of\nuser requests, raising the necessity of model compression. Existing solutions\nleverage the knowledge distillation framework to learn a smaller model that\nimitates the behaviors of BERT. However, the training procedure of knowledge\ndistillation is expensive itself as it requires sufficient training data to\nimitate the teacher model. In this paper, we address this issue by proposing a\nhybrid solution named LadaBERT (Lightweight adaptation of BERT through hybrid\nmodel compression), which combines the advantages of different model\ncompression methods, including weight pruning, matrix factorization and\nknowledge distillation. LadaBERT achieves state-of-the-art accuracy on various\npublic datasets while the training overheads can be reduced by an order of\nmagnitude.", "published": "2020-04-08 17:18:56", "link": "http://arxiv.org/abs/2004.04124v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pruning and Sparsemax Methods for Hierarchical Attention Networks", "abstract": "This paper introduces and evaluates two novel Hierarchical Attention Network\nmodels [Yang et al., 2016] - i) Hierarchical Pruned Attention Networks, which\nremove the irrelevant words and sentences from the classification process in\norder to reduce potential noise in the document classification accuracy and ii)\nHierarchical Sparsemax Attention Networks, which replace the Softmax function\nused in the attention mechanism with the Sparsemax [Martins and Astudillo,\n2016], capable of better handling importance distributions where a lot of words\nor sentences have very low probabilities. Our empirical evaluation on the IMDB\nReview for sentiment analysis datasets shows both approaches to be able to\nmatch the results obtained by the current state-of-the-art (without, however,\nany significant benefits). All our source code is made available\nathttps://github.com/jmribeiro/dsl-project.", "published": "2020-04-08 17:56:58", "link": "http://arxiv.org/abs/2004.04343v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multilingual Chart-based Constituency Parse Extraction from Pre-trained\n  Language Models", "abstract": "As it has been unveiled that pre-trained language models (PLMs) are to some\nextent capable of recognizing syntactic concepts in natural language, much\neffort has been made to develop a method for extracting complete (binary)\nparses from PLMs without training separate parsers. We improve upon this\nparadigm by proposing a novel chart-based method and an effective top-K\nensemble technique. Moreover, we demonstrate that we can broaden the scope of\napplication of the approach into multilingual settings. Specifically, we show\nthat by applying our method on multilingual PLMs, it becomes possible to induce\nnon-trivial parses for sentences from nine languages in an integrated and\nlanguage-agnostic manner, attaining performance superior or comparable to that\nof unsupervised PCFGs. We also verify that our approach is robust to\ncross-lingual transfer. Finally, we provide analyses on the inner workings of\nour method. For instance, we discover universal attention heads which are\nconsistently sensitive to syntactic information irrespective of the input\nlanguage.", "published": "2020-04-08 05:42:26", "link": "http://arxiv.org/abs/2004.13805v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Understanding Knowledge Gaps in Visual Question Answering: Implications\n  for Gap Identification and Testing", "abstract": "Visual Question Answering (VQA) systems are tasked with answering natural\nlanguage questions corresponding to a presented image. Traditional VQA datasets\ntypically contain questions related to the spatial information of objects,\nobject attributes, or general scene questions. Recently, researchers have\nrecognized the need to improve the balance of such datasets to reduce the\nsystem's dependency on memorized linguistic features and statistical biases,\nwhile aiming for enhanced visual understanding. However, it is unclear whether\nany latent patterns exist to quantify and explain these failures. As an initial\nstep towards better quantifying our understanding of the performance of VQA\nmodels, we use a taxonomy of Knowledge Gaps (KGs) to tag questions with one or\nmore types of KGs. Each Knowledge Gap (KG) describes the reasoning abilities\nneeded to arrive at a resolution. After identifying KGs for each question, we\nexamine the skew in the distribution of questions for each KG. We then\nintroduce a targeted question generation model to reduce this skew, which\nallows us to generate new types of questions for an image. These new questions\ncan be added to existing VQA datasets to increase the diversity of questions\nand reduce the skew.", "published": "2020-04-08 00:27:43", "link": "http://arxiv.org/abs/2004.03755v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Satirical News Detection with Semantic Feature Extraction and\n  Game-theoretic Rough Sets", "abstract": "Satirical news detection is an important yet challenging task to prevent\nspread of misinformation. Many feature based and end-to-end neural nets based\nsatirical news detection systems have been proposed and delivered promising\nresults. Existing approaches explore comprehensive word features from satirical\nnews articles, but lack semantic metrics using word vectors for tweet form\nsatirical news. Moreover, the vagueness of satire and news parody determines\nthat a news tweet can hardly be classified with a binary decision, that is,\nsatirical or legitimate. To address these issues, we collect satirical and\nlegitimate news tweets, and propose a semantic feature based approach. Features\nare extracted by exploring inconsistencies in phrases, entities, and between\nmain and relative clauses. We apply game-theoretic rough set model to detect\nsatirical news, in which probabilistic thresholds are derived by game\nequilibrium and repetition learning mechanism. Experimental results on the\ncollected dataset show the robustness and improvement of the proposed approach\ncompared with Pawlak rough set model and SVM.", "published": "2020-04-08 03:22:21", "link": "http://arxiv.org/abs/2004.03788v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Structure-Level Knowledge Distillation For Multilingual Sequence\n  Labeling", "abstract": "Multilingual sequence labeling is a task of predicting label sequences using\na single unified model for multiple languages. Compared with relying on\nmultiple monolingual models, using a multilingual model has the benefit of a\nsmaller model size, easier in online serving, and generalizability to\nlow-resource languages. However, current multilingual models still underperform\nindividual monolingual models significantly due to model capacity limitations.\nIn this paper, we propose to reduce the gap between monolingual models and the\nunified multilingual model by distilling the structural knowledge of several\nmonolingual models (teachers) to the unified multilingual model (student). We\npropose two novel KD methods based on structure-level information: (1)\napproximately minimizes the distance between the student's and the teachers'\nstructure level probability distributions, (2) aggregates the structure-level\nknowledge to local distributions and minimizes the distance between two local\nprobability distributions. Our experiments on 4 multilingual tasks with 25\ndatasets show that our approaches outperform several strong baselines and have\nstronger zero-shot generalizability than both the baseline model and teacher\nmodels.", "published": "2020-04-08 07:14:01", "link": "http://arxiv.org/abs/2004.03846v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Word frequency and sentiment analysis of twitter messages during\n  Coronavirus pandemic", "abstract": "The COVID-19 epidemic has had a great impact on social media conversation,\nespecially on sites like Twitter, which has emerged as a hub for public\nreaction and information sharing. This paper deals by analyzing a vast dataset\nof Twitter messages related to this disease, starting from January 2020. Two\napproaches were used: a statistical analysis of word frequencies and a\nsentiment analysis to gauge user attitudes. Word frequencies are modeled using\nunigrams, bigrams, and trigrams, with power law distribution as the fitting\nmodel. The validity of the model is confirmed through metrics like Sum of\nSquared Errors (SSE), R-squared ($R^2$), and Root Mean Squared Error (RMSE).\nHigh $R^2$ and low SSE/RMSE values indicate a good fit for the model. Sentiment\nanalysis is conducted to understand the general emotional tone of Twitter users\nmessages. The results reveal that a majority of tweets exhibit neutral\nsentiment polarity, with only 2.57\\% expressing negative polarity.", "published": "2020-04-08 10:45:08", "link": "http://arxiv.org/abs/2004.03925v2", "categories": ["cs.IR", "cs.CL", "cs.SI"], "primary_category": "cs.IR"}
{"title": "Putting a Spin on Language: A Quantum Interpretation of Unary\n  Connectives for Linguistic Applications", "abstract": "Extended versions of the Lambek Calculus currently used in computational\nlinguistics rely on unary modalities to allow for the controlled application of\nstructural rules affecting word order and phrase structure. These controlled\nstructural operations give rise to derivational ambiguities that are missed by\nthe original Lambek Calculus or its pregroup simplification. Proposals for\ncompositional interpretation of extended Lambek Calculus in the compact closed\ncategory of FVect and linear maps have been made, but in these proposals the\nsyntax-semantics mapping ignores the control modalities, effectively\nrestricting their role to the syntax. Our aim is to turn the modalities into\nfirst-class citizens of the vectorial interpretation. Building on the\ndirectional density matrix semantics, we extend the interpretation of the type\nsystem with an extra spin density matrix space. The interpretation of proofs\nthen results in ambiguous derivations being tensored with orthogonal spin\nstates. Our method introduces a way of simultaneously representing co-existing\ninterpretations of ambiguous utterances, and provides a uniform framework for\nthe integration of lexical and derivational ambiguity.", "published": "2020-04-08 17:25:11", "link": "http://arxiv.org/abs/2004.04128v3", "categories": ["cs.CL", "cs.LO", "quant-ph"], "primary_category": "cs.CL"}
{"title": "Generating Counter Narratives against Online Hate Speech: Data and\n  Strategies", "abstract": "Recently research has started focusing on avoiding undesired effects that\ncome with content moderation, such as censorship and overblocking, when dealing\nwith hatred online. The core idea is to directly intervene in the discussion\nwith textual responses that are meant to counter the hate content and prevent\nit from further spreading. Accordingly, automation strategies, such as natural\nlanguage generation, are beginning to be investigated. Still, they suffer from\nthe lack of sufficient amount of quality data and tend to produce\ngeneric/repetitive responses. Being aware of the aforementioned limitations, we\npresent a study on how to collect responses to hate effectively, employing\nlarge scale unsupervised language models such as GPT-2 for the generation of\nsilver data, and the best annotation strategies/neural architectures that can\nbe used for data filtering before expert validation/post-editing.", "published": "2020-04-08 19:35:00", "link": "http://arxiv.org/abs/2004.04216v1", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Measuring Emotions in the COVID-19 Real World Worry Dataset", "abstract": "The COVID-19 pandemic is having a dramatic impact on societies and economies\naround the world. With various measures of lockdowns and social distancing in\nplace, it becomes important to understand emotional responses on a large scale.\nIn this paper, we present the first ground truth dataset of emotional responses\nto COVID-19. We asked participants to indicate their emotions and express these\nin text. This resulted in the Real World Worry Dataset of 5,000 texts (2,500\nshort + 2,500 long texts). Our analyses suggest that emotional responses\ncorrelated with linguistic measures. Topic modeling further revealed that\npeople in the UK worry about their family and the economic situation.\nTweet-sized texts functioned as a call for solidarity, while longer texts shed\nlight on worries and concerns. Using predictive modeling approaches, we were\nable to approximate the emotional responses of participants from text within\n14% of their actual value. We encourage others to use the dataset and improve\nhow we can use automated methods to learn about emotional responses and worries\nabout an urgent problem.", "published": "2020-04-08 19:52:14", "link": "http://arxiv.org/abs/2004.04225v2", "categories": ["cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Architecture for a multilingual Wikipedia", "abstract": "Wikipedia's vision is a world in which everyone can share in the sum of all\nknowledge. In its first two decades, this vision has been very unevenly\nachieved. One of the largest hindrances is the sheer number of languages\nWikipedia needs to cover in order to achieve that goal. We argue that we need a\nnew approach to tackle this problem more effectively, a multilingual Wikipedia\nwhere content can be shared between language editions. This paper proposes an\narchitecture for a system that fulfills this goal. It separates the goal in two\nparts: creating and maintaining content in an abstract notation within a\nproject called Abstract Wikipedia, and creating an infrastructure called\nWikilambda that can translate this notation to natural language. Both parts are\nfully owned and maintained by the community, as is the integration of the\nresults in the existing Wikipedia editions. This architecture will make more\nencyclopedic content available to more people in their own language, and at the\nsame time allow more people to contribute knowledge and reach more people with\ntheir contributions, no matter what their respective language backgrounds.\nAdditionally, Wikilambda will unlock a new type of knowledge asset people can\nshare in through the Wikimedia projects, functions, which will vastly expand\nwhat people can do with knowledge from Wikimedia, and provide a new venue to\ncollaborate and to engage the creativity of contributors from all around the\nworld. These two projects will considerably expand the capabilities of the\nWikimedia platform to enable every single human being to freely share in the\nsum of all knowledge.", "published": "2020-04-08 22:25:10", "link": "http://arxiv.org/abs/2004.04733v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Word Equations: Inherently Interpretable Sparse Word Embeddingsthrough\n  Sparse Coding", "abstract": "Word embeddings are a powerful natural language processing technique, but\nthey are extremely difficult to interpret. To enable interpretable NLP models,\nwe create vectors where each dimension is inherently interpretable. By\ninherently interpretable, we mean a system where each dimension is associated\nwith some human understandable hint that can describe the meaning of that\ndimension. In order to create more interpretable word embeddings, we transform\npretrained dense word embeddings into sparse embeddings. These new embeddings\nare inherently interpretable: each of their dimensions is created from and\nrepresents a natural language word or specific grammatical concept. We\nconstruct these embeddings through sparse coding, where each vector in the\nbasis set is itself a word embedding. Therefore, each dimension of our sparse\nvectors corresponds to a natural language word. We also show that models\ntrained using these sparse embeddings can achieve good performance and are more\ninterpretable in practice, including through human evaluations.", "published": "2020-04-08 19:49:49", "link": "http://arxiv.org/abs/2004.13847v3", "categories": ["cs.CL", "cs.LG", "stat.ML", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Emotional Voice Conversion With Cycle-consistent Adversarial Network", "abstract": "Emotional Voice Conversion, or emotional VC, is a technique of converting\nspeech from one emotion state into another one, keeping the basic linguistic\ninformation and speaker identity. Previous approaches for emotional VC need\nparallel data and use dynamic time warping (DTW) method to temporally align the\nsource-target speech parameters. These approaches often define a minimum\ngeneration loss as the objective function, such as L1 or L2 loss, to learn\nmodel parameters. Recently, cycle-consistent generative adversarial networks\n(CycleGAN) have been used successfully for non-parallel VC. This paper\ninvestigates the efficacy of using CycleGAN for emotional VC tasks. Rather than\nattempting to learn a mapping between parallel training data using a\nframe-to-frame minimum generation loss, the CycleGAN uses two discriminators\nand one classifier to guide the learning process, where the discriminators aim\nto differentiate between the natural and converted speech and the classifier\naims to classify the underlying emotion from the natural and converted speech.\nThe training process of the CycleGAN models randomly pairs source-target speech\nparameters, without any temporal alignment operation. The objective and\nsubjective evaluation results confirm the effectiveness of using CycleGAN\nmodels for emotional VC. The non-parallel training for a CycleGAN indicates its\npotential for non-parallel emotional VC.", "published": "2020-04-08 02:50:46", "link": "http://arxiv.org/abs/2004.03781v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Multi-Target Emotional Voice Conversion With Neural Vocoders", "abstract": "Emotional voice conversion (EVC) is one way to generate expressive synthetic\nspeech. Previous approaches mainly focused on modeling one-to-one mapping,\ni.e., conversion from one emotional state to another emotional state, with\nMel-cepstral vocoders. In this paper, we investigate building a multi-target\nEVC (MTEVC) architecture, which combines a deep bidirectional long-short term\nmemory (DBLSTM)-based conversion model and a neural vocoder. Phonetic\nposteriorgrams (PPGs) containing rich linguistic information are incorporated\ninto the conversion model as auxiliary input features, which boost the\nconversion performance. To leverage the advantages of the newly emerged neural\nvocoders, we investigate the conditional WaveNet and flow-based WaveNet\n(FloWaveNet) as speech generators. The vocoders take in additional speaker\ninformation and emotion information as auxiliary features and are trained with\na multi-speaker and multi-emotion speech corpus. Objective metrics and\nsubjective evaluation of the experimental results verify the efficacy of the\nproposed MTEVC architecture for EVC.", "published": "2020-04-08 03:00:27", "link": "http://arxiv.org/abs/2004.03782v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Noise Tokens: Learning Neural Noise Templates for Environment-Aware\n  Speech Enhancement", "abstract": "In recent years, speech enhancement (SE) has achieved impressive progress\nwith the success of deep neural networks (DNNs). However, the DNN approach\nusually fails to generalize well to unseen environmental noise that is not\nincluded in the training. To address this problem, we propose \"noise tokens\"\n(NTs), which are a set of neural noise templates that are jointly trained with\nthe SE system. NTs dynamically capture the environment variability and thus\nenable the DNN model to handle various environments to produce STFT magnitude\nwith higher quality. Experimental results show that using NTs is an effective\nstrategy that consistently improves the generalization ability of SE systems\nacross different DNN architectures. Furthermore, we investigate applying a\nstate-of-the-art neural vocoder to generate waveform instead of traditional\ninverse STFT (ISTFT). Subjective listening tests show the residual noise can be\nsignificantly suppressed through mel-spectrogram correction and vocoder-based\nwaveform synthesis.", "published": "2020-04-08 14:17:31", "link": "http://arxiv.org/abs/2004.04001v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "An investigation of phone-based subword units for end-to-end speech\n  recognition", "abstract": "Phones and their context-dependent variants have been the standard modeling\nunits for conventional speech recognition systems, while characters and\nsubwords have demonstrated their effectiveness for end-to-end recognition\nsystems. We investigate the use of phone-based subwords, in particular, byte\npair encoder (BPE), as modeling units for end-to-end speech recognition. In\naddition, we also developed multi-level language model-based decoding\nalgorithms based on a pronunciation dictionary. Besides the use of the lexicon,\nwhich is easily available, our system avoids the need of additional expert\nknowledge or processing steps from conventional systems. Experimental results\nshow that phone-based BPEs tend to yield more accurate recognition systems than\nthe character-based counterpart. In addition, further improvement can be\nobtained with a novel one-pass joint beam search decoder, which efficiently\ncombines phone- and character-based BPE systems. For Switchboard, our\nphone-based BPE system achieves 6.8\\%/14.4\\% word error rate (WER) on the\nSwitchboard/CallHome portion of the test set while joint decoding achieves\n6.3\\%/13.3\\% WER. On Fisher + Switchboard, joint decoding leads to 4.9\\%/9.5\\%\nWER, setting new milestones for telephony speech recognition.", "published": "2020-04-08 22:50:17", "link": "http://arxiv.org/abs/2004.04290v6", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Conditioned Source Separation for Music Instrument Performances", "abstract": "In music source separation, the number of sources may vary for each piece and\nsome of the sources may belong to the same family of instruments, thus sharing\ntimbral characteristics and making the sources more correlated. This leads to\nadditional challenges in the source separation problem. This paper proposes a\nsource separation method for multiple musical instruments sounding\nsimultaneously and explores how much additional information apart from the\naudio stream can lift the quality of source separation. We explore conditioning\ntechniques at different levels of a primary source separation network and\nutilize two extra modalities of data, namely presence or absence of instruments\nin the mixture, and the corresponding video stream data.", "published": "2020-04-08 08:24:15", "link": "http://arxiv.org/abs/2004.03873v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Investigation of Singing Voice Separation for Singing Voice Detection in\n  Polyphonic Music", "abstract": "Singing voice detection (SVD), to recognize vocal parts in the song, is an\nessential task in music information retrieval (MIR). The task remains\nchallenging since singing voice varies and intertwines with the accompaniment\nmusic, especially for some complicated polyphonic music such as choral music\nrecordings. To address this problem, we investigate singing voice detection\nwhile discarding the interference from the accompaniment. The proposed SVD has\ntwo steps: i. The singing voice separation (SVS) technique is first utilized to\nfilter out the singing voice's potential part coarsely. ii. Upon the continuity\nof vocal in the time domain, Long-term Recurrent Convolutional Networks (LRCN)\nis used to learn compositional features. Moreover, to eliminate the outliers,\nwe choose to use a median filter for time-domain smoothing. Experimental\nresults show that the proposed method outperforms the existing state-of-the-art\nworks on two public datasets, the Jamendo Corpus and the RWC pop dataset.", "published": "2020-04-08 15:12:01", "link": "http://arxiv.org/abs/2004.04040v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MM Algorithms for Joint Independent Subspace Analysis with Application\n  to Blind Single and Multi-Source Extraction", "abstract": "In this work, we propose efficient algorithms for joint independent subspace\nanalysis (JISA), an extension of independent component analysis that deals with\nparallel mixtures, where not all the components are independent. We derive an\nalgorithmic framework for JISA based on the majorization-minimization (MM)\noptimization technique (JISA-MM). We use a well-known inequality for\nsuper-Gaussian sources to derive a surrogate function of the negative\nlog-likelihood of the observed data. The minimization of this surrogate\nfunction leads to a variant of the hybrid exact-approximate diagonalization\nproblem, but where multiple demixing vectors are grouped together. In the\nspirit of auxiliary function based independent vector analysis (AuxIVA), we\npropose several updates that can be applied alternately to one, or jointly to\ntwo, groups of demixing vectors.\n  Recently, blind extraction of one or more sources has gained interest as a\nreasonable way of exploiting larger microphone arrays to achieve better\nseparation. In particular, several MM algorithms have been proposed for\noverdetermined IVA (OverIVA). By applying JISA-MM, we are not only able to\nrederive these in a general manner, but also find several new algorithms. We\nrun extensive numerical experiments to evaluate their performance, and compare\nit to that of full separation with AuxIVA. We find that algorithms using\npairwise updates of two sources, or of one source and the background have the\nfastest convergence, and are able to separate target sources quickly and\nprecisely from the background. In addition, we characterize the performance of\nall algorithms under a large number of noise, reverberation, and background\nmismatch conditions.", "published": "2020-04-08 10:45:40", "link": "http://arxiv.org/abs/2004.03926v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Bayesian x-vector: Bayesian Neural Network based x-vector System for\n  Speaker Verification", "abstract": "Speaker verification systems usually suffer from the mismatch problem between\ntraining and evaluation data, such as speaker population mismatch, the channel\nand environment variations. In order to address this issue, it requires the\nsystem to have good generalization ability on unseen data. In this work, we\nincorporate Bayesian neural networks (BNNs) into the deep neural network (DNN)\nx-vector speaker verification system to improve the system's generalization\nability. With the weight uncertainty modeling provided by BNNs, we expect the\nsystem could generalize better on the evaluation data and make verification\ndecisions more accurately. Our experiment results indicate that the DNN\nx-vector system could benefit from BNNs especially when the mismatch problem is\nsevere for evaluations using out-of-domain data. Specifically, results show\nthat the system could benefit from BNNs by a relative EER decrease of 2.66% and\n2.32% respectively for short- and long-utterance in-domain evaluations.\nAdditionally, the fusion of DNN x-vector and Bayesian x-vector systems could\nachieve further improvement. Moreover, experiments conducted by out-of-domain\nevaluations, e.g. models trained on Voxceleb1 while evaluated on NIST SRE10\ncore test, suggest that BNNs could bring a larger relative EER decrease of\naround 4.69%.", "published": "2020-04-08 14:35:12", "link": "http://arxiv.org/abs/2004.04014v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Semi-supervised acoustic modelling for five-lingual code-switched ASR\n  using automatically-segmented soap opera speech", "abstract": "This paper considers the impact of automatic segmentation on the\nfully-automatic, semi-supervised training of automatic speech recognition (ASR)\nsystems for five-lingual code-switched (CS) speech. Four automatic segmentation\ntechniques were evaluated in terms of the recognition performance of an ASR\nsystem trained on the resulting segments in a semi-supervised manner. The\nsystem's output was compared with the recognition rates achieved by a\nsemi-supervised system trained on manually assigned segments. Three of the\nautomatic techniques use a newly proposed convolutional neural network (CNN)\nmodel for framewise classification, and include a novel form of HMM smoothing\nof the CNN outputs. Automatic segmentation was applied in combination with\nautomatic speaker diarization. The best-performing segmentation technique was\nalso tested without speaker diarization. An evaluation based on 248 unsegmented\nsoap opera episodes indicated that voice activity detection (VAD) based on a\nCNN followed by Gaussian mixture modelhidden Markov model smoothing\n(CNN-GMM-HMM) yields the best ASR performance. The semi-supervised system\ntrained with the resulting segments achieved an overall WER improvement of 1.1%\nabsolute over the system trained with manually created segments. Furthermore,\nwe found that system performance improved even further when the automatic\nsegmentation was used in conjunction with speaker diarization.", "published": "2020-04-08 04:36:25", "link": "http://arxiv.org/abs/2004.06480v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
