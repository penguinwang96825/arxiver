{"title": "Structure-aware Fine-tuning of Sequence-to-sequence Transformers for\n  Transition-based AMR Parsing", "abstract": "Predicting linearized Abstract Meaning Representation (AMR) graphs using\npre-trained sequence-to-sequence Transformer models has recently led to large\nimprovements on AMR parsing benchmarks. These parsers are simple and avoid\nexplicit modeling of structure but lack desirable properties such as graph\nwell-formedness guarantees or built-in graph-sentence alignments. In this work\nwe explore the integration of general pre-trained sequence-to-sequence language\nmodels and a structure-aware transition-based approach. We depart from a\npointer-based transition system and propose a simplified transition set,\ndesigned to better exploit pre-trained language models for structured\nfine-tuning. We also explore modeling the parser state within the pre-trained\nencoder-decoder architecture and different vocabulary strategies for the same\npurpose. We provide a detailed comparison with recent progress in AMR parsing\nand show that the proposed parser retains the desirable properties of previous\ntransition-based approaches, while being simpler and reaching the new parsing\nstate of the art for AMR 2.0, without the need for graph re-categorization.", "published": "2021-10-29 04:36:31", "link": "http://arxiv.org/abs/2110.15534v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MentalBERT: Publicly Available Pretrained Language Models for Mental\n  Healthcare", "abstract": "Mental health is a critical issue in modern society, and mental disorders\ncould sometimes turn to suicidal ideation without adequate treatment. Early\ndetection of mental disorders and suicidal ideation from social content\nprovides a potential way for effective social intervention. Recent advances in\npretrained contextualized language representations have promoted the\ndevelopment of several domain-specific pretrained models and facilitated\nseveral downstream applications. However, there are no existing pretrained\nlanguage models for mental healthcare. This paper trains and release two\npretrained masked language models, i.e., MentalBERT and MentalRoBERTa, to\nbenefit machine learning for the mental healthcare research community. Besides,\nwe evaluate our trained domain-specific models and several variants of\npretrained language models on several mental disorder detection benchmarks and\ndemonstrate that language representations pretrained in the target domain\nimprove the performance of mental health detection tasks.", "published": "2021-10-29 08:36:47", "link": "http://arxiv.org/abs/2110.15621v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Amendable Generation for Dialogue State Tracking", "abstract": "In task-oriented dialogue systems, recent dialogue state tracking methods\ntend to perform one-pass generation of the dialogue state based on the previous\ndialogue state. The mistakes of these models made at the current turn are prone\nto be carried over to the next turn, causing error propagation. In this paper,\nwe propose a novel Amendable Generation for Dialogue State Tracking (AG-DST),\nwhich contains a two-pass generation process: (1) generating a primitive\ndialogue state based on the dialogue of the current turn and the previous\ndialogue state, and (2) amending the primitive dialogue state from the first\npass. With the additional amending generation pass, our model is tasked to\nlearn more robust dialogue state tracking by amending the errors that still\nexist in the primitive dialogue state, which plays the role of reviser in the\ndouble-checking process and alleviates unnecessary error propagation.\nExperimental results show that AG-DST significantly outperforms previous works\nin two active DST datasets (MultiWOZ 2.2 and WOZ 2.0), achieving new\nstate-of-the-art performances.", "published": "2021-10-29 10:02:43", "link": "http://arxiv.org/abs/2110.15659v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overview of ADoBo 2021: Automatic Detection of Unassimilated Borrowings\n  in the Spanish Press", "abstract": "This paper summarizes the main findings of the ADoBo 2021 shared task,\nproposed in the context of IberLef 2021. In this task, we invited participants\nto detect lexical borrowings (coming mostly from English) in Spanish newswire\ntexts. This task was framed as a sequence classification problem using BIO\nencoding. We provided participants with an annotated corpus of lexical\nborrowings which we split into training, development and test splits. We\nreceived submissions from 4 teams with 9 different system runs overall. The\nresults, which range from F1 scores of 37 to 85, suggest that this is a\nchallenging task, especially when out-of-domain or OOV words are considered,\nand that traditional methods informed with lexicographic information would\nbenefit from taking advantage of current NLP trends.", "published": "2021-10-29 11:07:59", "link": "http://arxiv.org/abs/2110.15682v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Leverage Multimodal EHR Data for Better Medical Predictions?", "abstract": "Healthcare is becoming a more and more important research topic recently.\nWith the growing data in the healthcare domain, it offers a great opportunity\nfor deep learning to improve the quality of medical service. However, the\ncomplexity of electronic health records (EHR) data is a challenge for the\napplication of deep learning. Specifically, the data produced in the hospital\nadmissions are monitored by the EHR system, which includes structured data like\ndaily body temperature, and unstructured data like free text and laboratory\nmeasurements. Although there are some preprocessing frameworks proposed for\nspecific EHR data, the clinical notes that contain significant clinical value\nare beyond the realm of their consideration. Besides, whether these different\ndata from various views are all beneficial to the medical tasks and how to best\nutilize these data remain unclear. Therefore, in this paper, we first extract\nthe accompanying clinical notes from EHR and propose a method to integrate\nthese data, we also comprehensively study the different models and the data\nleverage methods for better medical task prediction. The results on two medical\nprediction tasks show that our fused model with different data outperforms the\nstate-of-the-art method that without clinical notes, which illustrates the\nimportance of our fusion method and the value of clinical note features. Our\ncode is available at https: //github.com/emnlp-mimic/mimic.", "published": "2021-10-29 13:26:05", "link": "http://arxiv.org/abs/2110.15763v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Theories on Styles to their Transfer in Text: Bridging the Gap with\n  a Hierarchical Survey", "abstract": "Humans are naturally endowed with the ability to write in a particular style.\nThey can, for instance, re-phrase a formal letter in an informal way, convey a\nliteral message with the use of figures of speech or edit a novel by mimicking\nthe style of some well-known authors. Automating this form of creativity\nconstitutes the goal of style transfer. As a natural language generation task,\nstyle transfer aims at rewriting existing texts, and specifically, it creates\nparaphrases that exhibit some desired stylistic attributes. From a practical\nperspective, it envisions beneficial applications, like chatbots that modulate\ntheir communicative style to appear empathetic, or systems that automatically\nsimplify technical articles for a non-expert audience. Several style-aware\nparaphrasing methods have attempted to tackle style transfer. A handful of\nsurveys give a methodological overview of the field, but they do not support\nresearchers to focus on specific styles. With this paper, we aim at providing a\ncomprehensive discussion of the styles that have received attention in the\ntransfer task. We organize them in a hierarchy, highlighting the challenges for\nthe definition of each of them, and pointing out gaps in the current research\nlandscape. The hierarchy comprises two main groups. One encompasses styles that\npeople modulate arbitrarily, along the lines of registers and genres. The other\ngroup corresponds to unintentionally expressed styles, due to an author's\npersonal characteristics. Hence, our review shows how these groups relate to\none another, and where specific styles, including some that have not yet been\nexplored, belong in the hierarchy. Moreover, we summarize the methods employed\nfor different stylistic families, hinting researchers towards those that would\nbe the most fitting for future research.", "published": "2021-10-29 15:53:06", "link": "http://arxiv.org/abs/2110.15871v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer Ensembles for Sexism Detection", "abstract": "This document presents in detail the work done for the sexism detection task\nat EXIST2021 workshop. Our methodology is built on ensembles of\nTransformer-based models which are trained on different background and corpora\nand fine-tuned on the provided dataset from the EXIST2021 workshop. We report\naccuracy of 0.767 for the binary classification task (task1), and f1 score\n0.766, and for the multi-class task (task2) accuracy 0.623 and f1-score 0.535.", "published": "2021-10-29 16:51:50", "link": "http://arxiv.org/abs/2110.15905v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Full Constituency Parsing with Neighboring Distribution\n  Divergence", "abstract": "Unsupervised constituency parsing has been explored much but is still far\nfrom being solved. Conventional unsupervised constituency parser is only able\nto capture the unlabeled structure of sentences. Towards unsupervised full\nconstituency parsing, we propose an unsupervised and training-free labeling\nprocedure by exploiting the property of a recently introduced metric,\nNeighboring Distribution Divergence (NDD), which evaluates semantic similarity\nbetween sentences before and after editions. For implementation, we develop NDD\ninto Dual POS-NDD (DP-NDD) and build \"molds\" to detect constituents and their\nlabels in sentences. We show that DP-NDD not only labels constituents precisely\nbut also inducts more accurate unlabeled constituency trees than all previous\nunsupervised methods with simpler rules. With two frameworks for labeled\nconstituency trees inference, we set both the new state-of-the-art for\nunlabeled F1 and strong baselines for labeled F1. In contrast with the\nconventional predicting-and-evaluating scenario, our method acts as an\nplausible example to inversely apply evaluating metrics for prediction.", "published": "2021-10-29 17:27:34", "link": "http://arxiv.org/abs/2110.15931v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Personal Food Preferences via Food Logs Embedding", "abstract": "Diet management is key to managing chronic diseases such as diabetes.\nAutomated food recommender systems may be able to assist by providing meal\nrecommendations that conform to a user's nutrition goals and food preferences.\nCurrent recommendation systems suffer from a lack of accuracy that is in part\ndue to a lack of knowledge of food preferences, namely foods users like to and\nare able to eat frequently. In this work, we propose a method for learning food\npreferences from food logs, a comprehensive but noisy source of information\nabout users' dietary habits. We also introduce accompanying metrics. The method\ngenerates and compares word embeddings to identify the parent food category of\neach food entry and then calculates the most popular. Our proposed approach\nidentifies 82% of a user's ten most frequently eaten foods. Our method is\npublicly available on (https://github.com/aametwally/LearningFoodPreferences)", "published": "2021-10-29 02:36:24", "link": "http://arxiv.org/abs/2110.15498v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pre-training Co-evolutionary Protein Representation via A Pairwise\n  Masked Language Model", "abstract": "Understanding protein sequences is vital and urgent for biology, healthcare,\nand medicine. Labeling approaches are expensive yet time-consuming, while the\namount of unlabeled data is increasing quite faster than that of the labeled\ndata due to low-cost, high-throughput sequencing methods. In order to extract\nknowledge from these unlabeled data, representation learning is of significant\nvalue for protein-related tasks and has great potential for helping us learn\nmore about protein functions and structures. The key problem in the protein\nsequence representation learning is to capture the co-evolutionary information\nreflected by the inter-residue co-variation in the sequences. Instead of\nleveraging multiple sequence alignment as is usually done, we propose a novel\nmethod to capture this information directly by pre-training via a dedicated\nlanguage model, i.e., Pairwise Masked Language Model (PMLM). In a conventional\nmasked language model, the masked tokens are modeled by conditioning on the\nunmasked tokens only, but processed independently to each other. However, our\nproposed PMLM takes the dependency among masked tokens into consideration,\ni.e., the probability of a token pair is not equal to the product of the\nprobability of the two tokens. By applying this model, the pre-trained encoder\nis able to generate a better representation for protein sequences. Our result\nshows that the proposed method can effectively capture the inter-residue\ncorrelations and improves the performance of contact prediction by up to 9%\ncompared to the MLM baseline under the same setting. The proposed model also\nsignificantly outperforms the MSA baseline by more than 7% on the TAPE contact\nprediction benchmark when pre-trained on a subset of the sequence database\nwhich the MSA is generated from, revealing the potential of the sequence\npre-training method to surpass MSA based methods in general.", "published": "2021-10-29 04:01:32", "link": "http://arxiv.org/abs/2110.15527v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Hand Sign Recognition: Identify Unusuality through Latent\n  Cognizance", "abstract": "Sign language is a main communication channel among hearing disability\ncommunity. Automatic sign language transcription could facilitate better\ncommunication and understanding between hearing disability community and\nhearing majority. As a recent work in automatic sign language transcription has\ndiscussed, effectively handling or identifying a non-sign posture is one of the\nkey issues. A non-sign posture is a posture unintended for sign reading and\ndoes not belong to any valid sign. A non-sign posture may arise during sign\ntransition or simply from an unaware posture. Confidence ratio has been\nproposed to mitigate the issue. Confidence ratio is simple to compute and\nreadily available without extra training. However, confidence ratio is reported\nto only partially address the problem. In addition, confidence ratio\nformulation is susceptible to computational instability. This article proposes\nalternative formulations to confidence ratio, investigates an issue of non-sign\nidentification for Thai Finger Spelling recognition, explores potential\nsolutions and has found a promising direction. Not only does this finding\naddress the issue of non-sign identification, it also provide some insight\nbehind a well-learned inference machine, revealing hidden meaning and new\ninterpretation of the underlying mechanism. Our proposed methods are evaluated\nand shown to be effective for non-sign detection.", "published": "2021-10-29 05:15:17", "link": "http://arxiv.org/abs/2110.15542v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Handshakes AI Research at CASE 2021 Task 1: Exploring different\n  approaches for multilingual tasks", "abstract": "The aim of the CASE 2021 Shared Task 1 (H\\\"urriyeto\\u{g}lu et al., 2021) was\nto detect and classify socio-political and crisis event information at\ndocument, sentence, cross-sentence, and token levels in a multilingual setting,\nwith each of these subtasks being evaluated separately in each test language.\nOur submission contained entries in all of the subtasks, and the scores\nobtained validated our research finding: That the multilingual aspect of the\ntasks should be embraced, so that modeling and training regimes use the\nmultilingual nature of the tasks to their mutual benefit, rather than trying to\ntackle the different languages separately. Our code is available at\nhttps://github.com/HandshakesByDC/case2021/", "published": "2021-10-29 07:58:49", "link": "http://arxiv.org/abs/2110.15599v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Comparing Machine Learning-Centered Approaches for Forecasting Language\n  Patterns During Frustration in Early Childhood", "abstract": "When faced with self-regulation challenges, children have been known the use\ntheir language to inhibit their emotions and behaviors. Yet, to date, there has\nbeen a critical lack of evidence regarding what patterns in their speech\nchildren use during these moments of frustration. In this paper, eXtreme\nGradient Boosting, Random Forest, Long Short-Term Memory Recurrent Neural\nNetworks, and Elastic Net Regression, have all been used to forecast these\nlanguage patterns in children. Based on the results of a comparative analysis\nbetween these methods, the study reveals that when dealing with\nhigh-dimensional and dense data, with very irregular and abnormal\ndistributions, as is the case with self-regulation patterns in children,\ndecision tree-based algorithms are able to outperform traditional regression\nand neural network methods in their shortcomings.", "published": "2021-10-29 13:45:38", "link": "http://arxiv.org/abs/2110.15778v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MetaICL: Learning to Learn In Context", "abstract": "We introduce MetaICL (Meta-training for In-Context Learning), a new\nmeta-training framework for few-shot learning where a pretrained language model\nis tuned to do in-context learning on a large set of training tasks. This\nmeta-training enables the model to more effectively learn a new task in context\nat test time, by simply conditioning on a few training examples with no\nparameter updates or task-specific templates. We experiment on a large, diverse\ncollection of tasks consisting of 142 NLP datasets including classification,\nquestion answering, natural language inference, paraphrase detection and more,\nacross seven different meta-training/target splits. MetaICL outperforms a range\nof baselines including in-context learning without meta-training and multi-task\nlearning followed by zero-shot transfer. We find that the gains are\nparticularly significant for target tasks that have domain shifts from the\nmeta-training tasks, and that using a diverse set of the meta-training tasks is\nkey to improvements. We also show that MetaICL approaches (and sometimes beats)\nthe performance of models fully finetuned on the target task, and outperforms\nmuch bigger models with nearly 8x parameters. Finally, we show that MetaICL is\ncomplementary to human-written instructions, and the best performance can be\nachieved by combining both approaches.", "published": "2021-10-29 17:42:08", "link": "http://arxiv.org/abs/2110.15943v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Visual Keyword Spotting with Attention", "abstract": "In this paper, we consider the task of spotting spoken keywords in silent\nvideo sequences -- also known as visual keyword spotting. To this end, we\ninvestigate Transformer-based models that ingest two streams, a visual encoding\nof the video and a phonetic encoding of the keyword, and output the temporal\nlocation of the keyword if present. Our contributions are as follows: (1) We\npropose a novel architecture, the Transpotter, that uses full cross-modal\nattention between the visual and phonetic streams; (2) We show through\nextensive evaluations that our model outperforms the prior state-of-the-art\nvisual keyword spotting and lip reading methods on the challenging LRW, LRS2,\nLRS3 datasets by a large margin; (3) We demonstrate the ability of our model to\nspot words under the extreme conditions of isolated mouthings in sign language\nvideos.", "published": "2021-10-29 17:59:04", "link": "http://arxiv.org/abs/2110.15957v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "The Golden Rule as a Heuristic to Measure the Fairness of Texts Using\n  Machine Learning", "abstract": "In this paper we present a natural language programming framework to consider\nhow the fairness of acts can be measured. For the purposes of the paper, a fair\nact is defined as one that one would be accepting of if it were done to\noneself. The approach is based on an implementation of the golden rule (GR) in\nthe digital domain. Despite the GRs prevalence as an axiom throughout history,\nno transfer of this moral philosophy into computational systems exists. In this\npaper we consider how to algorithmically operationalise this rule so that it\nmay be used to measure sentences such as: the boy harmed the girl, and\ncategorise them as fair or unfair. A review and reply to criticisms of the GR\nis made. A suggestion of how the technology may be implemented to avoid unfair\nbiases in word embeddings is made - given that individuals would typically not\nwish to be on the receiving end of an unfair act, such as racism, irrespective\nof whether the corpus being used deems such discrimination as praiseworthy.", "published": "2021-10-29 22:33:45", "link": "http://arxiv.org/abs/2111.00107v4", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Deep Keyphrase Completion", "abstract": "Keyphrase provides accurate information of document content that is highly\ncompact, concise, full of meanings, and widely used for discourse\ncomprehension, organization, and text retrieval. Though previous studies have\nmade substantial efforts for automated keyphrase extraction and generation,\nsurprisingly, few studies have been made for \\textit{keyphrase completion}\n(KPC). KPC aims to generate more keyphrases for document (e.g. scientific\npublication) taking advantage of document content along with a very limited\nnumber of known keyphrases, which can be applied to improve text indexing\nsystem, etc. In this paper, we propose a novel KPC method with an\nencoder-decoder framework. We name it \\textit{deep keyphrase completion} (DKPC)\nsince it attempts to capture the deep semantic meaning of the document content\ntogether with known keyphrases via a deep learning framework. Specifically, the\nencoder and the decoder in DKPC play different roles to make full use of the\nknown keyphrases. The former considers the keyphrase-guiding factors, which\naggregates information of known keyphrases into context. On the contrary, the\nlatter considers the keyphrase-inhibited factor to inhibit semantically\nrepeated keyphrase generation. Extensive experiments on benchmark datasets\ndemonstrate the efficacy of our proposed model.", "published": "2021-10-29 07:15:35", "link": "http://arxiv.org/abs/2111.01910v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Towards Tractable Mathematical Reasoning: Challenges, Strategies, and\n  Opportunities for Solving Math Word Problems", "abstract": "Mathematical reasoning would be one of the next frontiers for artificial\nintelligence to make significant progress. The ongoing surge to solve math word\nproblems (MWPs) and hence achieve better mathematical reasoning ability would\ncontinue to be a key line of research in the coming time. We inspect non-neural\nand neural methods to solve math word problems narrated in a natural language.\nWe also highlight the ability of these methods to be generalizable,\nmathematically reasonable, interpretable, and explainable. Neural approaches\ndominate the current state of the art, and we survey them highlighting three\nstrategies to MWP solving: (1) direct answer generation, (2) expression tree\ngeneration for inferring answers, and (3) template retrieval for answer\ncomputation. Moreover, we discuss technological approaches, review the\nevolution of intuitive design choices to solve MWPs, and examine them for\nmathematical reasoning ability. We finally identify several gaps that warrant\nthe need for external knowledge and knowledge-infused learning, among several\nother opportunities in solving MWPs.", "published": "2021-10-29 05:20:31", "link": "http://arxiv.org/abs/2111.05364v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Path-Enhanced Multi-Relational Question Answering with Knowledge Graph\n  Embeddings", "abstract": "The multi-relational Knowledge Base Question Answering (KBQA) system performs\nmulti-hop reasoning over the knowledge graph (KG) to achieve the answer. Recent\napproaches attempt to introduce the knowledge graph embedding (KGE) technique\nto handle the KG incompleteness but only consider the triple facts and neglect\nthe significant semantic correlation between paths and multi-relational\nquestions. In this paper, we propose a Path and Knowledge Embedding-Enhanced\nmulti-relational Question Answering model (PKEEQA), which leverages multi-hop\npaths between entities in the KG to evaluate the ambipolar correlation between\na path embedding and a multi-relational question embedding via a customizable\npath representation mechanism, benefiting for achieving more accurate answers\nfrom the perspective of both the triple facts and the extra paths. Experimental\nresults illustrate that PKEEQA improves KBQA models' performance for\nmulti-relational question answering with explainability to some extent derived\nfrom paths.", "published": "2021-10-29 08:37:46", "link": "http://arxiv.org/abs/2110.15622v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fusing ASR Outputs in Joint Training for Speech Emotion Recognition", "abstract": "Alongside acoustic information, linguistic features based on speech\ntranscripts have been proven useful in Speech Emotion Recognition (SER).\nHowever, due to the scarcity of emotion labelled data and the difficulty of\nrecognizing emotional speech, it is hard to obtain reliable linguistic features\nand models in this research area. In this paper, we propose to fuse Automatic\nSpeech Recognition (ASR) outputs into the pipeline for joint training SER. The\nrelationship between ASR and SER is understudied, and it is unclear what and\nhow ASR features benefit SER. By examining various ASR outputs and fusion\nmethods, our experiments show that in joint ASR-SER training, incorporating\nboth ASR hidden and text output using a hierarchical co-attention fusion\napproach improves the SER performance the most. On the IEMOCAP corpus, our\napproach achieves 63.4% weighted accuracy, which is close to the baseline\nresults achieved by combining ground-truth transcripts. In addition, we also\npresent novel word error rate analysis on IEMOCAP and layer-difference analysis\nof the Wav2vec 2.0 model to better understand the relationship between ASR and\nSER.", "published": "2021-10-29 11:21:17", "link": "http://arxiv.org/abs/2110.15684v2", "categories": ["eess.AS", "cs.CL", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On the Feasibility of Predicting Questions being Forgotten in Stack\n  Overflow", "abstract": "For their attractiveness, comprehensiveness and dynamic coverage of relevant\ntopics, community-based question answering sites such as Stack Overflow heavily\nrely on the engagement of their communities: Questions on new technologies,\ntechnology features as well as technology versions come up and have to be\nanswered as technology evolves (and as community members gather experience with\nit). At the same time, other questions cease in importance over time, finally\nbecoming irrelevant to users. Beyond filtering low-quality questions,\n\"forgetting\" questions, which have become redundant, is an important step for\nkeeping the Stack Overflow content concise and useful. In this work, we study\nthis managed forgetting task for Stack Overflow. Our work is based on data from\nmore than a decade (2008 - 2019) - covering 18.1M questions, that are made\npublicly available by the site itself. For establishing a deeper understanding,\nwe first analyze and characterize the set of questions about to be forgotten,\ni.e., questions that get a considerable number of views in the current period\nbut become unattractive in the near future. Subsequently, we examine the\ncapability of a wide range of features in predicting such forgotten questions\nin different categories. We find some categories in which those questions are\nmore predictable. We also discover that the text-based features are\nsurprisingly not helpful in this prediction task, while the meta information is\nmuch more predictive.", "published": "2021-10-29 15:59:11", "link": "http://arxiv.org/abs/2110.15789v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Combining Unsupervised and Text Augmented Semi-Supervised Learning for\n  Low Resourced Autoregressive Speech Recognition", "abstract": "Recent advances in unsupervised representation learning have demonstrated the\nimpact of pretraining on large amounts of read speech. We adapt these\ntechniques for domain adaptation in low-resource -- both in terms of data and\ncompute -- conversational and broadcast domains. Moving beyond CTC, we pretrain\nstate-of-the-art Conformer models in an unsupervised manner. While the\nunsupervised approach outperforms traditional semi-supervised training, the\ntechniques are complementary. Combining the techniques is a 5% absolute\nimprovement in WER, averaged over all conditions, compared to semi-supervised\ntraining alone. Additional text data is incorporated through external language\nmodels. By using CTC-based decoding, we are better able to take advantage of\nthe additional text data. When used as a transcription model, it allows the\nConformer model to better incorporate the knowledge from the language model\nthrough semi-supervised training than shallow fusion. Final performance is an\nadditional 2% better absolute when using CTC-based decoding for semi-supervised\ntraining compared to shallow fusion.", "published": "2021-10-29 14:59:18", "link": "http://arxiv.org/abs/2110.15836v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Skyformer: Remodel Self-Attention with Gaussian Kernel and Nystr\u00f6m\n  Method", "abstract": "Transformers are expensive to train due to the quadratic time and space\ncomplexity in the self-attention mechanism. On the other hand, although kernel\nmachines suffer from the same computation bottleneck in pairwise dot products,\nseveral approximation schemes have been successfully incorporated to\nconsiderably reduce their computational cost without sacrificing too much\naccuracy. In this work, we leverage the computation methods for kernel machines\nto alleviate the high computational cost and introduce Skyformer, which\nreplaces the softmax structure with a Gaussian kernel to stabilize the model\ntraining and adapts the Nystr\\\"om method to a non-positive semidefinite matrix\nto accelerate the computation. We further conduct theoretical analysis by\nshowing that the matrix approximation error of our proposed method is small in\nthe spectral norm. Experiments on Long Range Arena benchmark show that the\nproposed method is sufficient in getting comparable or even better performance\nthan the full self-attention while requiring fewer computation resources.", "published": "2021-10-29 18:28:49", "link": "http://arxiv.org/abs/2111.00035v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Measuring a Texts Fairness Dimensions Using Machine Learning Based on\n  Social Psychological Factors", "abstract": "Fairness is a principal social value that can be observed in civilisations\naround the world. A manifestation of this is in social agreements, often\ndescribed in texts, such as contracts. Yet, despite the prevalence of such, a\nfairness metric for texts describing a social act remains wanting. To address\nthis, we take a step back to consider the problem based on first principals.\nInstead of using rules or templates, we utilise social psychology literature to\ndetermine the principal factors that humans use when making a fairness\nassessment. We then attempt to digitise these using word embeddings into a\nmulti-dimensioned sentence level fairness perceptions vector to serve as an\napproximation for these fairness perceptions. The method leverages a pro-social\nbias within word embeddings, for which we obtain an F1= 81.0. A second\napproach, using PCA and ML based on the said fairness approximation vector\nproduces an F1 score of 86.2. We detail improvements that can be made in the\nmethodology to incorporate the projection of sentence embedding on to a\nsubspace representation of fairness.", "published": "2021-10-29 21:09:17", "link": "http://arxiv.org/abs/2111.00086v4", "categories": ["cs.AI", "cs.CL", "cs.CY", "I.2.7"], "primary_category": "cs.AI"}
{"title": "SA-SDR: A novel loss function for separation of meeting style data", "abstract": "Many state-of-the-art neural network-based source separation systems use the\naveraged Signal-to-Distortion Ratio (SDR) as a training objective function. The\nbasic SDR is, however, undefined if the network reconstructs the reference\nsignal perfectly or if the reference signal contains silence, e.g., when a\ntwo-output separator processes a single-speaker recording. Many modifications\nto the plain SDR have been proposed that trade-off between making the loss more\nrobust and distorting its value. We propose to switch from a mean over the SDRs\nof each individual output channel to a global SDR over all output channels at\nthe same time, which we call source-aggregated SDR (SA-SDR). This makes the\nloss robust against silence and perfect reconstruction as long as at least one\nreference signal is not silent. We experimentally show that our proposed SA-SDR\nis more stable and preferable over other well-known modifications when\nprocessing meeting-style data that typically contains many silent or\nsingle-speaker regions.", "published": "2021-10-29 07:14:47", "link": "http://arxiv.org/abs/2110.15581v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "VRAIN-UPV MLLP's system for the Blizzard Challenge 2021", "abstract": "This paper presents the VRAIN-UPV MLLP's speech synthesis system for the SH1\ntask of the Blizzard Challenge 2021. The SH1 task consisted in building a\nSpanish text-to-speech system trained on (but not limited to) the corpus\nreleased by the Blizzard Challenge 2021 organization. It included 5 hours of\nstudio-quality recordings from a native Spanish female speaker. In our case,\nthis dataset was solely used to build a two-stage neural text-to-speech\npipeline composed of a non-autoregressive acoustic model with explicit duration\nmodeling and a HiFi-GAN neural vocoder. Our team is identified as J in the\nevaluation results. Our system obtained very good results in the subjective\nevaluation tests. Only one system among other 11 participants achieved better\nnaturalness than ours. Concretely, it achieved a naturalness MOS of 3.61\ncompared to 4.21 for real samples.", "published": "2021-10-29 13:58:41", "link": "http://arxiv.org/abs/2110.15792v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Differentiable Tracking-Based Training of Deep Learning Sound Source\n  Localizers", "abstract": "Data-based and learning-based sound source localization (SSL) has shown\npromising results in challenging conditions, and is commonly set as a\nclassification or a regression problem. Regression-based approaches have\ncertain advantages over classification-based, such as continuous\ndirection-of-arrival estimation of static and moving sources. However,\nmulti-source scenarios require multiple regressors without a clear training\nstrategy up-to-date, that does not rely on auxiliary information such as\nsimultaneous sound classification. We investigate end-to-end training of such\nmethods with a technique recently proposed for video object detectors, adapted\nto the SSL setting. A differentiable network is constructed that can be plugged\nto the output of the localizer to solve the optimal assignment between\npredictions and references, optimizing directly the popular CLEAR-MOT tracking\nmetrics. Results indicate large improvements over directly optimizing mean\nsquared errors, in terms of localization error, detection metrics, and tracking\ncapabilities.", "published": "2021-10-29 18:04:00", "link": "http://arxiv.org/abs/2111.00030v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards automatic detection and classification of orca (Orcinus orca)\n  calls using cross-correlation methods", "abstract": "Orca (Orcinus orca) is known for complex vocalisation. Their social structure\nconsists of pods and clans sharing unique dialects due to geographic isolation.\nSound type repertoires are fundamental for monitoring orca populations and are\ntypically created visually and aurally. An orca pod occurring in the Ligurian\nSea (Pelagos Sanctuary) in December 2019 provided a unique occasion for\nlong-term recordings. The numerous data collected with the bottom recorder were\nanalysed with a traditional human-driven inspection to create a repertoire of\nthis pod and to compare it to catalogues from different orca populations\n(Icelandic and Antarctic) investigating its origins. Automatic signal detection\nand cross-correlation methods (R package warbleR) were used for the first time\nin orca studies. We found the Pearson cross-correlation method to be efficient\nfor most pairwise calculations (> 85%) but with false positives. One sound type\nfrom our repertoire presented a high positive match (range 0.62-0.67) with one\nfrom the Icelandic catalogue, which was confirmed visually and aurally. Our\nfirst attempt to automatically classify orca sound types presented limitations\ndue to background noise and sound complexity of orca communication. We show\ncross-correlation methods can be a powerful tool for sound type classification\nin combination with conventional methods.", "published": "2021-10-29 07:50:17", "link": "http://arxiv.org/abs/2110.15593v2", "categories": ["q-bio.QM", "cs.SD", "eess.AS"], "primary_category": "q-bio.QM"}
{"title": "Contrastive prediction strategies for unsupervised segmentation and\n  categorization of phonemes and words", "abstract": "We investigate the performance on phoneme categorization and phoneme and word\nsegmentation of several self-supervised learning (SSL) methods based on\nContrastive Predictive Coding (CPC). Our experiments show that with the\nexisting algorithms there is a trade off between categorization and\nsegmentation performance. We investigate the source of this conflict and\nconclude that the use of context building networks, albeit necessary for\nsuperior performance on categorization tasks, harms segmentation performance by\ncausing a temporal shift on the learned representations. Aiming to bridge this\ngap, we take inspiration from the leading approach on segmentation, which\nsimultaneously models the speech signal at the frame and phoneme level, and\nincorporate multi-level modelling into Aligned CPC (ACPC), a variation of CPC\nwhich exhibits the best performance on categorization tasks. Our multi-level\nACPC (mACPC) improves in all categorization metrics and achieves\nstate-of-the-art performance in word segmentation.", "published": "2021-10-29 16:55:13", "link": "http://arxiv.org/abs/2110.15909v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Personalized breath based biometric authentication with wearable\n  multimodality", "abstract": "Breath with nose sound features has been shown as a potential biometric in\npersonal identification and verification. In this paper, we show that\ninformation that comes from other modalities captured by motion sensors on the\nchest in addition to audio features could further improve the performance. Our\nwork is composed of three main contributions: hardware creation, dataset\npublication, and proposed multimodal models. To be more specific, we design new\nhardware which consists of an acoustic sensor to collect audio features from\nthe nose, as well as an accelerometer and gyroscope to collect movement on the\nchest as a result of an individual's breathing. Using this hardware, we publish\na collected dataset from a number of sessions from different volunteers, each\nsession includes three common gestures: normal, deep, and strong breathing.\nFinally, we experiment with two multimodal models based on Convolutional Long\nShort Term Memory (CNN-LSTM) and Temporal Convolutional Networks (TCN)\narchitectures. The results demonstrate the suitability of our new hardware for\nboth verification and identification tasks.", "published": "2021-10-29 17:41:43", "link": "http://arxiv.org/abs/2110.15941v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
