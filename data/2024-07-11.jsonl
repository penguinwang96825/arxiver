{"title": "How does Burrows' Delta work on medieval Chinese poetic texts?", "abstract": "Burrows' Delta was introduced in 2002 and has proven to be an effective tool\nfor author attribution. Despite the fact that these are different languages,\nthey mostly belong to the same grammatical type and use the same graphic\nprinciple to convey speech in writing: a phonemic alphabet with word separation\nusing spaces. The question I want to address in this article is how well this\nattribution method works with texts in a language with a different grammatical\nstructure and a script based on different principles. There are fewer studies\nanalyzing the effectiveness of the Delta method on Chinese texts than on texts\nin European languages. I believe that such a low level of attention to Delta\nfrom sinologists is due to the structure of the scientific field dedicated to\nmedieval Chinese poetry. Clustering based on intertextual distances worked\nflawlessly. Delta produced results where clustering showed that the samples of\none author were most similar to each other, and Delta never confused different\npoets. Despite the fact that I used an unconventional approach and applied the\nDelta method to a language poorly suited for it, the method demonstrated its\neffectiveness. Tang dynasty poets are correctly identified using Delta, and the\nempirical pattern observed for authors writing in European standard languages\nhas been confirmed once again.", "published": "2024-07-11 00:07:14", "link": "http://arxiv.org/abs/2407.08099v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Text: Leveraging Multi-Task Learning and Cognitive Appraisal\n  Theory for Post-Purchase Intention Analysis", "abstract": "Supervised machine-learning models for predicting user behavior offer a\nchallenging classification problem with lower average prediction performance\nscores than other text classification tasks. This study evaluates multi-task\nlearning frameworks grounded in Cognitive Appraisal Theory to predict user\nbehavior as a function of users' self-expression and psychological attributes.\nOur experiments show that users' language and traits improve predictions above\nand beyond models predicting only from text. Our findings highlight the\nimportance of integrating psychological constructs into NLP to enhance the\nunderstanding and prediction of user actions. We close with a discussion of the\nimplications for future applications of large language models for computational\npsychology.", "published": "2024-07-11 04:57:52", "link": "http://arxiv.org/abs/2407.08182v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "System Report for CCL24-Eval Task 7: Multi-Error Modeling and\n  Fluency-Targeted Pre-training for Chinese Essay Evaluation", "abstract": "This system report presents our approaches and results for the Chinese Essay\nFluency Evaluation (CEFE) task at CCL-2024. For Track 1, we optimized\npredictions for challenging fine-grained error types using binary\nclassification models and trained coarse-grained models on the Chinese Learner\n4W corpus. In Track 2, we enhanced performance by constructing a pseudo-dataset\nwith multiple error types per sentence. For Track 3, where we achieved first\nplace, we generated fluency-rated pseudo-data via back-translation for\npre-training and used an NSP-based strategy with Symmetric Cross Entropy loss\nto capture context and mitigate long dependencies. Our methods effectively\naddress key challenges in Chinese Essay Fluency Evaluation.", "published": "2024-07-11 06:17:08", "link": "http://arxiv.org/abs/2407.08206v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs' morphological analyses of complex FST-generated Finnish words", "abstract": "Rule-based language processing systems have been overshadowed by neural\nsystems in terms of utility, but it remains unclear whether neural NLP systems,\nin practice, learn the grammar rules that humans use. This work aims to shed\nlight on the issue by evaluating state-of-the-art LLMs in a task of\nmorphological analysis of complex Finnish noun forms. We generate the forms\nusing an FST tool, and they are unlikely to have occurred in the training sets\nof the LLMs, therefore requiring morphological generalisation capacity. We find\nthat GPT-4-turbo has some difficulties in the task while GPT-3.5-turbo\nstruggles and smaller models Llama2-70B and Poro-34B fail nearly completely.", "published": "2024-07-11 08:12:28", "link": "http://arxiv.org/abs/2407.08269v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RB-SQL: A Retrieval-based LLM Framework for Text-to-SQL", "abstract": "Large language models (LLMs) with in-context learning have significantly\nimproved the performance of text-to-SQL task. Previous works generally focus on\nusing exclusive SQL generation prompt to improve the LLMs' reasoning ability.\nHowever, they are mostly hard to handle large databases with numerous tables\nand columns, and usually ignore the significance of pre-processing database and\nextracting valuable information for more efficient prompt engineering. Based on\nabove analysis, we propose RB-SQL, a novel retrieval-based LLM framework for\nin-context prompt engineering, which consists of three modules that retrieve\nconcise tables and columns as schema, and targeted examples for in-context\nlearning. Experiment results demonstrate that our model achieves better\nperformance than several competitive baselines on public datasets BIRD and\nSpider.", "published": "2024-07-11 08:19:58", "link": "http://arxiv.org/abs/2407.08273v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks", "abstract": "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets.", "published": "2024-07-11 12:50:42", "link": "http://arxiv.org/abs/2407.08454v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Public Fine-Tuning Datasets: A Complex Review of Current\n  Practices from a Construction Perspective", "abstract": "With the rapid development of the large model domain, research related to\nfine-tuning has concurrently seen significant advancement, given that\nfine-tuning is a constituent part of the training process for large-scale\nmodels. Data engineering plays a fundamental role in the training process of\nmodels, which includes data infrastructure, data processing, etc. Data during\nfine-tuning likewise forms the base for large models. In order to embrace the\npower and explore new possibilities of fine-tuning datasets, this paper reviews\ncurrent public fine-tuning datasets from the perspective of data construction.\nAn overview of public fine-tuning datasets from two sides: evolution and\ntaxonomy, is provided in this review, aiming to chart the development\ntrajectory. Construction techniques and methods for public fine-tuning datasets\nof Large Language Models (LLMs), including data generation and data\naugmentation among others, are detailed. This elaboration follows the\naforementioned taxonomy, specifically across demonstration, comparison, and\ngeneralist categories. Additionally, a category tree of data generation\ntechniques has been abstracted in our review to assist researchers in gaining a\ndeeper understanding of fine-tuning datasets from the construction dimension.\nOur review also summarizes the construction features in different data\npreparation phases of current practices in this field, aiming to provide a\ncomprehensive overview and inform future research. Fine-tuning dataset\npractices, encompassing various data modalities, are also discussed from a\nconstruction perspective in our review. Towards the end of the article, we\noffer insights and considerations regarding the future construction and\ndevelopments of fine-tuning datasets.", "published": "2024-07-11 13:11:16", "link": "http://arxiv.org/abs/2407.08475v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating LLMs as Voting Assistants via Contextual Augmentation: A\n  Case Study on the European Parliament Elections 2024", "abstract": "In light of the recent 2024 European Parliament elections, we are\ninvestigating if LLMs can be used as Voting Advice Applications (VAAs). We\naudit MISTRAL and MIXTRAL models and evaluate their accuracy in predicting the\nstance of political parties based on the latest \"EU and I\" voting assistance\nquestionnaire. Furthermore, we explore alternatives to improve models'\nperformance by augmenting the input context via Retrieval-Augmented Generation\n(RAG) relying on web search, and Self-Reflection using staged conversations\nthat aim to re-collect relevant content from the model's internal memory. We\nfind that MIXTRAL is highly accurate with an 82% accuracy on average with a\nsignificant performance disparity across different political groups (50-95%).\nAugmenting the input context with expert-curated information can lead to a\nsignificant boost of approx. 9%, which remains an open challenge for automated\nRAG approaches, even considering curated content.", "published": "2024-07-11 13:29:28", "link": "http://arxiv.org/abs/2407.08495v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Universal Truthfulness Hyperplane Inside LLMs", "abstract": "While large language models (LLMs) have demonstrated remarkable abilities\nacross various fields, hallucination remains a significant challenge. Recent\nstudies have explored hallucinations through the lens of internal\nrepresentations, proposing mechanisms to decipher LLMs' adherence to facts.\nHowever, these approaches often fail to generalize to out-of-distribution data,\nleading to concerns about whether internal representation patterns reflect\nfundamental factual awareness, or only overfit spurious correlations on the\nspecific datasets. In this work, we investigate whether a universal\ntruthfulness hyperplane that distinguishes the model's factually correct and\nincorrect outputs exists within the model. To this end, we scale up the number\nof training datasets and conduct an extensive evaluation -- we train the\ntruthfulness hyperplane on a diverse collection of over 40 datasets and examine\nits cross-task, cross-domain, and in-domain generalization. Our results\nindicate that increasing the diversity of the training datasets significantly\nenhances the performance in all scenarios, while the volume of data samples\nplays a less critical role. This finding supports the optimistic hypothesis\nthat a universal truthfulness hyperplane may indeed exist within the model,\noffering promising directions for future research.", "published": "2024-07-11 15:07:26", "link": "http://arxiv.org/abs/2407.08582v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Turn-Level Empathy Prediction Using Psychological Indicators", "abstract": "For the WASSA 2024 Empathy and Personality Prediction Shared Task, we propose\na novel turn-level empathy detection method that decomposes empathy into six\npsychological indicators: Emotional Language, Perspective-Taking, Sympathy and\nCompassion, Extroversion, Openness, and Agreeableness. A pipeline of text\nenrichment using a Large Language Model (LLM) followed by DeBERTA fine-tuning\ndemonstrates a significant improvement in the Pearson Correlation Coefficient\nand F1 scores for empathy detection, highlighting the effectiveness of our\napproach. Our system officially ranked 7th at the CONV-turn track.", "published": "2024-07-11 15:43:27", "link": "http://arxiv.org/abs/2407.08607v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tamil Language Computing: the Present and the Future", "abstract": "This paper delves into the text processing aspects of Language Computing,\nwhich enables computers to understand, interpret, and generate human language.\nFocusing on tasks such as speech recognition, machine translation, sentiment\nanalysis, text summarization, and language modelling, language computing\nintegrates disciplines including linguistics, computer science, and cognitive\npsychology to create meaningful human-computer interactions. Recent\nadvancements in deep learning have made computers more accessible and capable\nof independent learning and adaptation. In examining the landscape of language\ncomputing, the paper emphasises foundational work like encoding, where Tamil\ntransitioned from ASCII to Unicode, enhancing digital communication. It\ndiscusses the development of computational resources, including raw data,\ndictionaries, glossaries, annotated data, and computational grammars, necessary\nfor effective language processing. The challenges of linguistic annotation, the\ncreation of treebanks, and the training of large language models are also\ncovered, emphasising the need for high-quality, annotated data and advanced\nlanguage models. The paper underscores the importance of building practical\napplications for languages like Tamil to address everyday communication needs,\nhighlighting gaps in current technology. It calls for increased research\ncollaboration, digitization of historical texts, and fostering digital usage to\nensure the comprehensive development of Tamil language processing, ultimately\nenhancing global communication and access to digital services.", "published": "2024-07-11 15:56:02", "link": "http://arxiv.org/abs/2407.08618v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Building Specialized Generalist AI with System 1 and System 2\n  Fusion", "abstract": "In this perspective paper, we introduce the concept of Specialized Generalist\nArtificial Intelligence (SGAI or simply SGI) as a crucial milestone toward\nArtificial General Intelligence (AGI). Compared to directly scaling general\nabilities, SGI is defined as AI that specializes in at least one task,\nsurpassing human experts, while also retaining general abilities. This fusion\npath enables SGI to rapidly achieve high-value areas. We categorize SGI into\nthree stages based on the level of mastery over professional skills and\ngenerality performance. Additionally, we discuss the necessity of SGI in\naddressing issues associated with large language models, such as their\ninsufficient generality, specialized capabilities, uncertainty in innovation,\nand practical applications. Furthermore, we propose a conceptual framework for\ndeveloping SGI that integrates the strengths of Systems 1 and 2 cognitive\nprocessing. This framework comprises three layers and four key components,\nwhich focus on enhancing individual abilities and facilitating collaborative\nevolution. We conclude by summarizing the potential challenges and suggesting\nfuture directions. We hope that the proposed SGI will provide insights into\nfurther research and applications towards achieving AGI.", "published": "2024-07-11 16:23:16", "link": "http://arxiv.org/abs/2407.08642v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Taxonomy for Data Contamination in Large Language Models", "abstract": "Large language models pretrained on extensive web corpora demonstrate\nremarkable performance across a wide range of downstream tasks. However, a\ngrowing concern is data contamination, where evaluation datasets may be\ncontained in the pretraining corpus, inflating model performance.\nDecontamination, the process of detecting and removing such data, is a\npotential solution; yet these contaminants may originate from altered versions\nof the test set, evading detection during decontamination. How different types\nof contamination impact the performance of language models on downstream tasks\nis not fully understood. We present a taxonomy that categorizes the various\ntypes of contamination encountered by LLMs during the pretraining phase and\nidentify which types pose the highest risk. We analyze the impact of\ncontamination on two key NLP tasks -- summarization and question answering --\nrevealing how different types of contamination influence task performance\nduring evaluation.", "published": "2024-07-11 17:50:34", "link": "http://arxiv.org/abs/2407.08716v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is Your Model Really A Good Math Reasoner? Evaluating Mathematical\n  Reasoning with Checklist", "abstract": "Exceptional mathematical reasoning ability is one of the key features that\ndemonstrate the power of large language models (LLMs). How to comprehensively\ndefine and evaluate the mathematical abilities of LLMs, and even reflect the\nuser experience in real-world scenarios, has emerged as a critical issue.\nCurrent benchmarks predominantly concentrate on problem-solving capabilities,\npresenting a substantial risk of model overfitting and fails to accurately\nmeasure the genuine mathematical reasoning abilities. In this paper, we argue\nthat if a model really understands a problem, it should be robustly applied\nacross a diverse array of tasks. To this end, we introduce MathCheck, a\nwell-designed checklist for testing task generalization and reasoning\nrobustness, as well as an automatic tool to generate checklists efficiently.\nMathCheck includes multiple mathematical reasoning tasks and robustness tests\nto facilitate a comprehensive evaluation of both mathematical reasoning ability\nand behavior testing. Utilizing MathCheck, we develop MathCheck-GSM and\nMathCheck-GEO to assess math textual reasoning and multi-modal reasoning\nabilities, respectively, serving as upgraded versions of benchmarks including\nGSM8k, GeoQA, UniGeo, and Geometry3K. We adopt MathCheck-GSM and MathCheck-GEO\nto evaluate 26 LLMs and 17 MLLMs. Our results demonstrate that while frontier\nLLMs like GPT-4o continue to excel in various abilities on the checklist, many\nother model families exhibit a significant decline. Further experiments\nindicate that, compared to traditional math benchmarks, MathCheck better\nreflects true mathematical abilities and represents mathematical intelligence\nmore linearly, thereby supporting our design. Using MathCheck, we can\nefficiently conduct informative behavior analysis to deeply investigate models.\nFinally, we show that our checklist paradigm can easily extend to other\nreasoning tasks.", "published": "2024-07-11 17:58:58", "link": "http://arxiv.org/abs/2407.08733v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MAGNET: Improving the Multilingual Fairness of Language Models with\n  Adaptive Gradient-Based Tokenization", "abstract": "In multilingual settings, non-Latin scripts and low-resource languages are\nusually disadvantaged in terms of language models' utility, efficiency, and\ncost. Specifically, previous studies have reported multiple modeling biases\nthat the current tokenization algorithms introduce to non-Latin script\nlanguages, the main one being over-segmentation. In this work, we propose\nMAGNET; multilingual adaptive gradient-based tokenization to reduce\nover-segmentation via adaptive gradient-based subword tokenization. MAGNET\nlearns to predict segment boundaries between byte tokens in a sequence via\nsub-modules within the model, which act as internal boundary predictors\n(tokenizers). Previous gradient-based tokenization methods aimed for uniform\ncompression across sequences by integrating a single boundary predictor during\ntraining and optimizing it end-to-end through stochastic reparameterization\nalongside the next token prediction objective. However, this approach still\nresults in over-segmentation for non-Latin script languages in multilingual\nsettings. In contrast, MAGNET offers a customizable architecture where\nbyte-level sequences are routed through language-script-specific predictors,\neach optimized for its respective language script. This modularity enforces\nequitable segmentation granularity across different language scripts compared\nto previous methods. Through extensive experiments, we demonstrate that in\naddition to reducing segmentation disparities, MAGNET also enables faster\nlanguage modelling and improves downstream utility.", "published": "2024-07-11 18:59:21", "link": "http://arxiv.org/abs/2407.08818v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rule-Based, Neural and LLM Back-Translation: Comparative Insights from a\n  Variant of Ladin", "abstract": "This paper explores the impact of different back-translation approaches on\nmachine translation for Ladin, specifically the Val Badia variant. Given the\nlimited amount of parallel data available for this language (only 18k\nLadin-Italian sentence pairs), we investigate the performance of a multilingual\nneural machine translation model fine-tuned for Ladin-Italian. In addition to\nthe available authentic data, we synthesise further translations by using three\ndifferent models: a fine-tuned neural model, a rule-based system developed\nspecifically for this language pair, and a large language model. Our\nexperiments show that all approaches achieve comparable translation quality in\nthis low-resource scenario, yet round-trip translations highlight differences\nin model performance.", "published": "2024-07-11 19:05:43", "link": "http://arxiv.org/abs/2407.08819v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Nuanced Bias in Large Language Model Free Response Answers", "abstract": "Pre-trained large language models (LLMs) can now be easily adapted for\nspecific business purposes using custom prompts or fine tuning. These\ncustomizations are often iteratively re-engineered to improve some aspect of\nperformance, but after each change businesses want to ensure that there has\nbeen no negative impact on the system's behavior around such critical issues as\nbias. Prior methods of benchmarking bias use techniques such as word masking\nand multiple choice questions to assess bias at scale, but these do not capture\nall of the nuanced types of bias that can occur in free response answers, the\ntypes of answers typically generated by LLM systems. In this paper, we identify\nseveral kinds of nuanced bias in free text that cannot be similarly identified\nby multiple choice tests. We describe these as: confidence bias, implied bias,\ninclusion bias and erasure bias. We present a semi-automated pipeline for\ndetecting these types of bias by first eliminating answers that can be\nautomatically classified as unbiased and then co-evaluating name reversed pairs\nusing crowd workers. We believe that the nuanced classifications our method\ngenerates can be used to give better feedback to LLMs, especially as LLM\nreasoning capabilities become more advanced.", "published": "2024-07-11 19:58:13", "link": "http://arxiv.org/abs/2407.08842v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automata-based constraints for language model decoding", "abstract": "Language models (LMs) are often expected to generate strings in some formal\nlanguage; for example, structured data, API calls, or code snippets. Although\nLMs can be tuned to improve their adherence to formal syntax, this does not\nguarantee conformance, especially with smaller LMs suitable for large-scale\ndeployment. In addition, tuning requires significant resources, making it\nimpractical for uncommon or task-specific formats. To prevent downstream\nparsing errors we would ideally constrain the LM to only produce valid output,\nbut this is severely complicated by tokenization, which is typically both\nambiguous and misaligned with the formal grammar. We solve these issues through\nthe application of automata theory, deriving an efficient closed-form solution\nfor the regular languages, a broad class of formal languages with many\npractical applications, including API calls or schema-guided JSON and YAML. We\nalso discuss pragmatic extensions for coping with the issue of high branching\nfactor, and extend our techniques to deterministic context-free languages,\nwhich similarly admit an efficient closed-form solution. Previous work on this\ntopic (Willard and Louf, 2023) layers bespoke solutions onto automata, leading\nto problems with speed, correctness, and extensibility. Instead, we reformulate\nthe entire task in terms of automata so we can leverage well-studied and\nwell-optimized algorithms. Our system compiles constraints ~7,000x faster, is\nprovably correct, and can be extended in a modular fashion.", "published": "2024-07-11 00:25:01", "link": "http://arxiv.org/abs/2407.08103v3", "categories": ["cs.CL", "cs.FL"], "primary_category": "cs.CL"}
{"title": "Looks can be Deceptive: Distinguishing Repetition Disfluency from\n  Reduplication", "abstract": "Reduplication and repetition, though similar in form, serve distinct\nlinguistic purposes. Reduplication is a deliberate morphological process used\nto express grammatical, semantic, or pragmatic nuances, while repetition is\noften unintentional and indicative of disfluency. This paper presents the first\nlarge-scale study of reduplication and repetition in speech using computational\nlinguistics. We introduce IndicRedRep, a new publicly available dataset\ncontaining Hindi, Telugu, and Marathi text annotated with reduplication and\nrepetition at the word level. We evaluate transformer-based models for\nmulti-class reduplication and repetition token classification, utilizing the\nReparandum-Interregnum-Repair structure to distinguish between the two\nphenomena. Our models achieve macro F1 scores of up to 85.62% in Hindi, 83.95%\nin Telugu, and 84.82% in Marathi for reduplication-repetition classification.", "published": "2024-07-11 03:00:14", "link": "http://arxiv.org/abs/2407.08147v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "fairBERTs: Erasing Sensitive Information Through Semantic and\n  Fairness-aware Perturbations", "abstract": "Pre-trained language models (PLMs) have revolutionized both the natural\nlanguage processing research and applications. However, stereotypical biases\n(e.g., gender and racial discrimination) encoded in PLMs have raised negative\nethical implications for PLMs, which critically limits their broader\napplications. To address the aforementioned unfairness issues, we present\nfairBERTs, a general framework for learning fair fine-tuned BERT series models\nby erasing the protected sensitive information via semantic and fairness-aware\nperturbations generated by a generative adversarial network. Through extensive\nqualitative and quantitative experiments on two real-world tasks, we\ndemonstrate the great superiority of fairBERTs in mitigating unfairness while\nmaintaining the model utility. We also verify the feasibility of transferring\nadversarial components in fairBERTs to other conventionally trained BERT-like\nmodels for yielding fairness improvements. Our findings may shed light on\nfurther research on building fairer fine-tuned PLMs.", "published": "2024-07-11 05:13:38", "link": "http://arxiv.org/abs/2407.08189v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generating Contextually-Relevant Navigation Instructions for Blind and\n  Low Vision People", "abstract": "Navigating unfamiliar environments presents significant challenges for blind\nand low-vision (BLV) individuals. In this work, we construct a dataset of\nimages and goals across different scenarios such as searching through kitchens\nor navigating outdoors. We then investigate how grounded instruction generation\nmethods can provide contextually-relevant navigational guidance to users in\nthese instances. Through a sighted user study, we demonstrate that large\npretrained language models can produce correct and useful instructions\nperceived as beneficial for BLV users. We also conduct a survey and interview\nwith 4 BLV users and observe useful insights on preferences for different\ninstructions based on the scenario.", "published": "2024-07-11 06:40:36", "link": "http://arxiv.org/abs/2407.08219v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Speculative RAG: Enhancing Retrieval Augmented Generation through\n  Drafting", "abstract": "Retrieval augmented generation (RAG) combines the generative abilities of\nlarge language models (LLMs) with external knowledge sources to provide more\naccurate and up-to-date responses. Recent RAG advancements focus on improving\nretrieval outcomes through iterative LLM refinement or self-critique\ncapabilities acquired through additional instruction tuning of LLMs. In this\nwork, we introduce Speculative RAG - a framework that leverages a larger\ngeneralist LM to efficiently verify multiple RAG drafts produced in parallel by\na smaller, distilled specialist LM. Each draft is generated from a distinct\nsubset of retrieved documents, offering diverse perspectives on the evidence\nwhile reducing input token counts per draft. This approach enhances\ncomprehension of each subset and mitigates potential position bias over long\ncontext. Our method accelerates RAG by delegating drafting to the smaller\nspecialist LM, with the larger generalist LM performing a single verification\npass over the drafts. Extensive experiments demonstrate that Speculative RAG\nachieves state-of-the-art performance with reduced latency on TriviaQA,\nMuSiQue, PopQA, PubHealth, and ARC-Challenge benchmarks. It notably enhances\naccuracy by up to 12.97% while reducing latency by 50.83% compared to\nconventional RAG systems on PubHealth.", "published": "2024-07-11 06:50:19", "link": "http://arxiv.org/abs/2407.08223v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AutoBencher: Towards Declarative Benchmark Construction", "abstract": "We present AutoBencher, a declarative framework for automatic benchmark\nconstruction, and use it to scalably discover novel insights and\nvulnerabilities of existing language models. Concretely, given a few desiderata\nof benchmarks (e.g., question difficulty, topic salience), we operationalize\neach desideratum and cast benchmark creation as an optimization problem.\nSpecifically, we experiment with two settings with different optimization\nobjectives: (i) for capability evaluation, we declare the goal of finding a\nsalient, difficult dataset that induces novel performance patterns; (ii) for\nsafety evaluation, we declare the goal of finding a dataset of unsafe prompts\nthat existing LMs fail to decline. To tackle this optimization problem, we use\na language model to iteratively propose and refine dataset descriptions, which\nare then used to generate topic-specific questions and answers. These\ndescriptions are optimized to improve the declared desiderata. We use\nAutoBencher (powered by GPT-4) to create datasets for math, multilinguality,\nknowledge, and safety. The scalability of AutoBencher allows it to test\nfine-grained categories and tail knowledge, creating datasets that elicit 22%\nmore model errors (i.e., difficulty) than existing benchmarks. On the novelty\nends, AutoBencher also helps identify specific gaps not captured by existing\nbenchmarks: e.g., Gemini-Pro has knowledge gaps on Permian Extinction and\nFordism while GPT-4o fails to decline harmful requests about cryptocurrency\nscams.", "published": "2024-07-11 10:03:47", "link": "http://arxiv.org/abs/2407.08351v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the attribution of confidence to large language models", "abstract": "Credences are mental states corresponding to degrees of confidence in\npropositions. Attribution of credences to Large Language Models (LLMs) is\ncommonplace in the empirical literature on LLM evaluation. Yet the theoretical\nbasis for LLM credence attribution is unclear. We defend three claims. First,\nour semantic claim is that LLM credence attributions are (at least in general)\ncorrectly interpreted literally, as expressing truth-apt beliefs on the part of\nscientists that purport to describe facts about LLM credences. Second, our\nmetaphysical claim is that the existence of LLM credences is at least\nplausible, although current evidence is inconclusive. Third, our epistemic\nclaim is that LLM credence attributions made in the empirical literature on LLM\nevaluation are subject to non-trivial sceptical concerns. It is a distinct\npossibility that even if LLMs have credences, LLM credence attributions are\ngenerally false because the experimental techniques used to assess LLM\ncredences are not truth-tracking.", "published": "2024-07-11 10:51:06", "link": "http://arxiv.org/abs/2407.08388v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Self-training Language Models for Arithmetic Reasoning", "abstract": "Recent language models achieve impressive results in tasks involving complex\nmultistep reasoning, but scaling these capabilities further traditionally\nrequires expensive collection of more annotated data. In this work, we explore\nthe potential of improving models' reasoning capabilities without new data,\nmerely using automated feedback to the validity of their predictions in\narithmetic reasoning (self-training).\n  In systematic experimentation across six different arithmetic reasoning\ndatasets, we find that models can substantially improve in both single-round\n(offline) and online self-training, reaching a correct result in +13.9% and\n+25.9% more cases, respectively, underlining the importance of actuality of\nself-training feedback. We further find that in the single-round, offline\nself-training, traditional supervised training can deliver gains comparable to\npreference optimization, but in online self-training, preference optimization\nmethods largely outperform supervised training thanks to their superior\nstability and robustness on unseen types of problems.", "published": "2024-07-11 11:06:05", "link": "http://arxiv.org/abs/2407.08400v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Instruction Following: Evaluating Inferential Rule Following of\n  Large Language Models", "abstract": "Although Large Language Models (LLMs) have demonstrated strong ability, they\nare further supposed to be controlled and guided by in real-world scenarios to\nbe safe, accurate, and intelligent. This demands the possession of capability\nof LLMs. However, no prior work has made a clear evaluation of the inferential\nrule-following capability of LLMs. Previous studies that try to evaluate the\ninferential rule-following capability of LLMs fail to distinguish the\ninferential rule-following scenarios from the instruction-following scenarios.\nTherefore, this paper first clarifies the concept of inferential rule-following\nand proposes a comprehensive benchmark, RuleBench, to evaluate a diversified\nrange of inferential rule-following abilities. Our experimental results on a\nvariety of LLMs show that they are still limited in following rules. Our\nanalysis based on the evaluation results provides insights into the\nimprovements for LLMs toward a better inferential rule-following intelligent\nagent. We further propose Inferential Rule-Following Tuning (IRFT). The\nexperimental results show that through IRFT, LLMs can learn abstract\nrule-following abilities from purely synthetic data and then generalize to\nRuleBench. The data and code can be found at:\nhttps://anonymous.4open.science/r/llm-rule-following-B3E3/", "published": "2024-07-11 12:26:55", "link": "http://arxiv.org/abs/2407.08440v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Are Large Language Models Really Bias-Free? Jailbreak Prompts for\n  Assessing Adversarial Robustness to Bias Elicitation", "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence,\ndemonstrating remarkable computational power and linguistic capabilities.\nHowever, these models are inherently prone to various biases stemming from\ntheir training data. These include selection, linguistic, and confirmation\nbiases, along with common stereotypes related to gender, ethnicity, sexual\norientation, religion, socioeconomic status, disability, and age. This study\nexplores the presence of these biases within the responses given by the most\nrecent LLMs, analyzing the impact on their fairness and reliability. We also\ninvestigate how known prompt engineering techniques can be exploited to\neffectively reveal hidden biases of LLMs, testing their adversarial robustness\nagainst jailbreak prompts specially crafted for bias elicitation. Extensive\nexperiments are conducted using the most widespread LLMs at different scales,\nconfirming that LLMs can still be manipulated to produce biased or\ninappropriate responses, despite their advanced capabilities and sophisticated\nalignment processes. Our findings underscore the importance of enhancing\nmitigation techniques to address these safety issues, toward a more sustainable\nand inclusive artificial intelligence.", "published": "2024-07-11 12:30:19", "link": "http://arxiv.org/abs/2407.08441v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Lynx: An Open Source Hallucination Evaluation Model", "abstract": "Retrieval Augmented Generation (RAG) techniques aim to mitigate\nhallucinations in Large Language Models (LLMs). However, LLMs can still produce\ninformation that is unsupported or contradictory to the retrieved contexts. We\nintroduce LYNX, a SOTA hallucination detection LLM that is capable of advanced\nreasoning on challenging real-world hallucination scenarios. To evaluate LYNX,\nwe present HaluBench, a comprehensive hallucination evaluation benchmark,\nconsisting of 15k samples sourced from various real-world domains. Our\nexperiment results show that LYNX outperforms GPT-4o, Claude-3-Sonnet, and\nclosed and open-source LLM-as-a-judge models on HaluBench. We release LYNX,\nHaluBench and our evaluation code for public access.", "published": "2024-07-11 13:22:17", "link": "http://arxiv.org/abs/2407.08488v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Emergent Visual-Semantic Hierarchies in Image-Text Representations", "abstract": "While recent vision-and-language models (VLMs) like CLIP are a powerful tool\nfor analyzing text and images in a shared semantic space, they do not\nexplicitly model the hierarchical nature of the set of texts which may describe\nan image. Conversely, existing multimodal hierarchical representation learning\nmethods require costly training from scratch, failing to leverage the knowledge\nencoded by state-of-the-art multimodal foundation models. In this work, we\nstudy the knowledge of existing foundation models, finding that they exhibit\nemergent understanding of visual-semantic hierarchies despite not being\ndirectly trained for this purpose. We propose the Radial Embedding (RE)\nframework for probing and optimizing hierarchical understanding, and contribute\nthe HierarCaps dataset, a benchmark facilitating the study of hierarchical\nknowledge in image--text representations, constructed automatically via large\nlanguage models. Our results show that foundation VLMs exhibit zero-shot\nhierarchical understanding, surpassing the performance of prior models\nexplicitly designed for this purpose. Furthermore, we show that foundation\nmodels may be better aligned to hierarchical reasoning via a text-only\nfine-tuning phase, while retaining pretraining knowledge.", "published": "2024-07-11 14:09:42", "link": "http://arxiv.org/abs/2407.08521v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Uncertainty Estimation of Large Language Models in Medical Question\n  Answering", "abstract": "Large Language Models (LLMs) show promise for natural language generation in\nhealthcare, but risk hallucinating factually incorrect information. Deploying\nLLMs for medical question answering necessitates reliable uncertainty\nestimation (UE) methods to detect hallucinations. In this work, we benchmark\npopular UE methods with different model sizes on medical question-answering\ndatasets. Our results show that current approaches generally perform poorly in\nthis domain, highlighting the challenge of UE for medical applications. We also\nobserve that larger models tend to yield better results, suggesting a\ncorrelation between model size and the reliability of UE. To address these\nchallenges, we propose Two-phase Verification, a probability-free Uncertainty\nEstimation approach. First, an LLM generates a step-by-step explanation\nalongside its initial answer, followed by formulating verification questions to\ncheck the factual claims in the explanation. The model then answers these\nquestions twice: first independently, and then referencing the explanation.\nInconsistencies between the two sets of answers measure the uncertainty in the\noriginal response. We evaluate our approach on three biomedical\nquestion-answering datasets using Llama 2 Chat models and compare it against\nthe benchmarked baseline methods. The results show that our Two-phase\nVerification method achieves the best overall accuracy and stability across\nvarious datasets and model sizes, and its performance scales as the model size\nincreases.", "published": "2024-07-11 16:51:33", "link": "http://arxiv.org/abs/2407.08662v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GTA: A Benchmark for General Tool Agents", "abstract": "Significant focus has been placed on integrating large language models (LLMs)\nwith various tools in developing general-purpose agents. This poses a challenge\nto LLMs' tool-use capabilities. However, there are evident gaps between\nexisting tool-use evaluations and real-world scenarios. Current evaluations\noften use AI-generated queries, single-step tasks, dummy tools, and text-only\ninteractions, failing to reveal the agents' real-world problem-solving\nabilities effectively. To address this, we propose GTA, a benchmark for General\nTool Agents, featuring three main aspects: (i) Real user queries: human-written\nqueries with simple real-world objectives but implicit tool-use, requiring the\nLLM to reason the suitable tools and plan the solution steps. (ii) Real\ndeployed tools: an evaluation platform equipped with tools across perception,\noperation, logic, and creativity categories to evaluate the agents' actual task\nexecution performance. (iii) Real multimodal inputs: authentic image files,\nsuch as spatial scenes, web page screenshots, tables, code snippets, and\nprinted/handwritten materials, used as the query contexts to align with\nreal-world scenarios closely. We design 229 real-world tasks and executable\ntool chains to evaluate mainstream LLMs. Our findings show that real-world user\nqueries are challenging for existing LLMs, with GPT-4 completing less than 50%\nof the tasks and most LLMs achieving below 25%. This evaluation reveals the\nbottlenecks in the tool-use capabilities of current LLMs in real-world\nscenarios, which provides future direction for advancing general-purpose tool\nagents. The code and dataset are available at\nhttps://github.com/open-compass/GTA.", "published": "2024-07-11 17:50:09", "link": "http://arxiv.org/abs/2407.08713v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Models of What? Mistaking Engineering Achievements for Human\n  Linguistic Agency", "abstract": "In this paper we argue that key, often sensational and misleading, claims\nregarding linguistic capabilities of Large Language Models (LLMs) are based on\nat least two unfounded assumptions; the assumption of language completeness and\nthe assumption of data completeness. Language completeness assumes that a\ndistinct and complete thing such as `a natural language' exists, the essential\ncharacteristics of which can be effectively and comprehensively modelled by an\nLLM. The assumption of data completeness relies on the belief that a language\ncan be quantified and wholly captured by data. Work within the enactive\napproach to cognitive science makes clear that, rather than a distinct and\ncomplete thing, language is a means or way of acting. Languaging is not the\nkind of thing that can admit of a complete or comprehensive modelling. From an\nenactive perspective we identify three key characteristics of enacted language;\nembodiment, participation, and precariousness, that are absent in LLMs, and\nlikely incompatible in principle with current architectures. We argue that\nthese absences imply that LLMs are not now and cannot in their present form be\nlinguistic agents the way humans are. We illustrate the point in particular\nthrough the phenomenon of `algospeak', a recently described pattern of high\nstakes human language activity in heavily controlled online environments. On\nthe basis of these points, we conclude that sensational and misleading claims\nabout LLM agency and capabilities emerge from a deep misconception of both what\nhuman language is and what LLMs are.", "published": "2024-07-11 18:06:01", "link": "http://arxiv.org/abs/2407.08790v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Fault Diagnosis in Power Grids with Large Language Model", "abstract": "Power grid fault diagnosis is a critical task for ensuring the reliability\nand stability of electrical infrastructure. Traditional diagnostic systems\noften struggle with the complexity and variability of power grid data. This\npaper proposes a novel approach that leverages Large Language Models (LLMs),\nspecifically ChatGPT and GPT-4, combined with advanced prompt engineering to\nenhance fault diagnosis accuracy and explainability. We designed comprehensive,\ncontext-aware prompts to guide the LLMs in interpreting complex data and\nproviding detailed, actionable insights. Our method was evaluated against\nbaseline techniques, including standard prompting, Chain-of-Thought (CoT), and\nTree-of-Thought (ToT) methods, using a newly constructed dataset comprising\nreal-time sensor data, historical fault records, and component descriptions.\nExperimental results demonstrate significant improvements in diagnostic\naccuracy, explainability quality, response coherence, and contextual\nunderstanding, underscoring the effectiveness of our approach. These findings\nsuggest that prompt-engineered LLMs offer a promising solution for robust and\nreliable power grid fault diagnosis.", "published": "2024-07-11 19:44:18", "link": "http://arxiv.org/abs/2407.08836v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GPT-4 is judged more human than humans in displaced and inverted Turing\n  tests", "abstract": "Everyday AI detection requires differentiating between people and AI in\ninformal, online conversations. In many cases, people will not interact\ndirectly with AI systems but instead read conversations between AI systems and\nother people. We measured how well people and large language models can\ndiscriminate using two modified versions of the Turing test: inverted and\ndisplaced. GPT-3.5, GPT-4, and displaced human adjudicators judged whether an\nagent was human or AI on the basis of a Turing test transcript. We found that\nboth AI and displaced human judges were less accurate than interactive\ninterrogators, with below chance accuracy overall. Moreover, all three judged\nthe best-performing GPT-4 witness to be human more often than human witnesses.\nThis suggests that both humans and current LLMs struggle to distinguish between\nthe two when they are not actively interrogating the person, underscoring an\nurgent need for more accurate tools to detect AI in conversations.", "published": "2024-07-11 20:28:24", "link": "http://arxiv.org/abs/2407.08853v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Automatic Pruning of Fine-tuning Datasets for Transformer-based Language\n  Models", "abstract": "Transformer-based language models have shown state-of-the-art performance on\na variety of natural language understanding tasks. To achieve this performance,\nthese models are first pre-trained on general corpus and then fine-tuned on\ndownstream tasks. Previous work studied the effect of pruning the training set\nof the downstream tasks on the performance of the model on its evaluation set.\nIn this work, we propose an automatic dataset pruning method for the training\nset of fine-tuning tasks. Our method is based on the model's success rate in\ncorrectly classifying each training data point. Unlike previous work which\nrelies on user feedback to determine subset size, our method automatically\nextracts training subsets that are adapted for each pair of model and\nfine-tuning task. Our method provides multiple subsets for use in dataset\npruning that navigate the trade-off between subset size and evaluation\naccuracy. Our largest subset, which we also refer to as the winning ticket\nsubset, is on average $3 \\times$ smaller than the original training set of the\nfine-tuning task. Our experiments on 5 downstream tasks and 2 language models\nshow that, on average, fine-tuning on the winning ticket subsets results in a\n$0.1 \\%$ increase in the evaluation performance of the model.", "published": "2024-07-11 22:46:18", "link": "http://arxiv.org/abs/2407.08887v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Characterizing Prompt Compression Methods for Long Context Inference", "abstract": "Long context inference presents challenges at the system level with increased\ncompute and memory requirements, as well as from an accuracy perspective in\nbeing able to reason over long contexts. Recently, several methods have been\nproposed to compress the prompt to reduce the context length. However, there\nhas been little work on comparing the different proposed methods across\ndifferent tasks through a standardized analysis. This has led to conflicting\nresults. To address this, here we perform a comprehensive characterization and\nevaluation of different prompt compression methods. In particular, we analyze\nextractive compression, summarization-based abstractive compression, and token\npruning methods. Surprisingly, we find that extractive compression often\noutperforms all the other approaches, and enables up to 10x compression with\nminimal accuracy degradation. Interestingly, we also find that despite several\nrecent claims, token pruning methods often lag behind extractive compression.\nWe only found marginal improvements on summarization tasks.", "published": "2024-07-11 23:34:32", "link": "http://arxiv.org/abs/2407.08892v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NinjaLLM: Fast, Scalable and Cost-effective RAG using Amazon SageMaker\n  and AWS Trainium and Inferentia2", "abstract": "Retrieval-augmented generation (RAG) techniques are widely used today to\nretrieve and present information in a conversational format. This paper\npresents a set of enhancements to traditional RAG techniques, focusing on large\nlanguage models (LLMs) fine-tuned and hosted on AWS Trainium and Inferentia2 AI\nchips via SageMaker. These chips are characterized by their elasticity,\naffordability, and efficient performance for AI compute tasks. Besides enabling\ndeployment on these chips, this work aims to improve tool usage, add citation\ncapabilities, and mitigate the risks of hallucinations and unsafe responses due\nto context bias. We benchmark our RAG system's performance on the Natural\nQuestions and HotPotQA datasets, achieving an accuracy of 62% and 59%\nrespectively, exceeding other models such as DBRX and Mixtral Instruct.", "published": "2024-07-11 05:04:44", "link": "http://arxiv.org/abs/2407.12057v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "How Well Can a Long Sequence Model Model Long Sequences? Comparing\n  Architechtural Inductive Biases on Long-Context Abilities", "abstract": "Long sequences occur in abundance within real-world scenarios, hence properly\nmodelling them opens numerous down-stream use-cases. Deep neural networks,\nhowever, have often struggled with these for a variety of reasons. Recent\nadvances, both in system engineering as well as model design, have enabled the\nscaling up of model that are purported to support extended context length. In\nparticular, the state-space and linear recurrent neural network families of\nmodels hypothetically can entend to infinite sequence lenth. However, is this\ntoo good to be true? We conduct an evaluation to show that while such claims\nmay be sound theoretically, there remain large practical gaps that are\nempirically observed. In particular, recurrent models still suffer in the same\nsettings as long-context LLMs with attention. We further show that different\ninductive biases have inconsistent extrapolation capabilities, highlighting the\nneed to further study such paradigms and investigate why long-context models\nseemingly fail to behave as one might expect.", "published": "2024-07-11 01:08:39", "link": "http://arxiv.org/abs/2407.08112v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Privacy-Preserving Data Deduplication for Enhancing Federated Learning\n  of Language Models (Extended Version)", "abstract": "Deduplication is a vital preprocessing step that enhances machine learning\nmodel performance and saves training time and energy. However, enhancing\nfederated learning through deduplication poses challenges, especially regarding\nscalability and potential privacy violations if deduplication involves sharing\nall clients' data. In this paper, we address the problem of deduplication in a\nfederated setup by introducing a pioneering protocol, Efficient\nPrivacy-Preserving Multi-Party Deduplication (EP-MPD). It efficiently removes\nduplicates from multiple clients' datasets without compromising data privacy.\nEP-MPD is constructed in a modular fashion, utilizing two novel variants of the\nPrivate Set Intersection protocol. Our extensive experiments demonstrate the\nsignificant benefits of deduplication in federated learning of large language\nmodels. For instance, we observe up to 19.62\\% improvement in perplexity and up\nto 27.95\\% reduction in running time while varying the duplication level\nbetween 10\\% and 30\\%. EP-MPD effectively balances privacy and performance in\nfederated learning, making it a valuable solution for large-scale applications.", "published": "2024-07-11 03:10:27", "link": "http://arxiv.org/abs/2407.08152v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Automatic Generation of Web Censorship Probe Lists", "abstract": "Domain probe lists--used to determine which URLs to probe for Web\ncensorship--play a critical role in Internet censorship measurement studies.\nIndeed, the size and accuracy of the domain probe list limits the set of\ncensored pages that can be detected; inaccurate lists can lead to an incomplete\nview of the censorship landscape or biased results. Previous efforts to\ngenerate domain probe lists have been mostly manual or crowdsourced. This\napproach is time-consuming, prone to errors, and does not scale well to the\never-changing censorship landscape.\n  In this paper, we explore methods for automatically generating probe lists\nthat are both comprehensive and up-to-date for Web censorship measurement. We\nstart from an initial set of 139,957 unique URLs from various existing test\nlists consisting of pages from a variety of languages to generate new candidate\npages. By analyzing content from these URLs (i.e., performing topic and keyword\nextraction), expanding these topics, and using them as a feed to search\nengines, our method produces 119,255 new URLs across 35,147 domains. We then\ntest the new candidate pages by attempting to access each URL from servers in\neleven different global locations over a span of four months to check for their\nconnectivity and potential signs of censorship. Our measurements reveal that\nour method discovered over 1,400 domains--not present in the original\ndataset--we suspect to be blocked. In short, automatically updating probe lists\nis possible, and can help further automate censorship measurements at scale.", "published": "2024-07-11 05:04:52", "link": "http://arxiv.org/abs/2407.08185v1", "categories": ["cs.CR", "cs.CL", "cs.CY", "cs.NI"], "primary_category": "cs.CR"}
{"title": "A Text-to-Game Engine for UGC-Based Role-Playing Games", "abstract": "The transition from professionally generated content (PGC) to user-generated\ncontent (UGC) has reshaped various media formats, encompassing formats such as\ntext and video. With rapid advancements in generative AI, a similar\ntransformation is set to redefine the gaming industry, particularly within the\ndomain of role-playing games (RPGs). This paper introduces a novel framework\nfor a text-to-game engine that leverages foundation models to transform simple\ntextual inputs into intricate, multi-modal RPG experiences. The engine\ndynamically generates game narratives, integrating text, visuals, and\nmechanics, while adapting characters, environments, and gameplay in realtime\nbased on player interactions. To evaluate and demonstrate the feasibility and\nversatility of this framework, we developed the 'Zagii' game engine. Zagii has\nsuccessfully powered hundreds of RPG games across diverse genres and\nfacilitated tens of thousands of online gameplay sessions, showcasing its\nscalability and adaptability. These results highlight the framework's\neffectiveness and its potential to foster a more open and democratized approach\nto game development. Our work underscores the transformative role of generative\nAI in reshaping the gaming lifecycle and advancing the boundaries of\ninteractive entertainment.", "published": "2024-07-11 05:33:19", "link": "http://arxiv.org/abs/2407.08195v2", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Towards Explainable Evolution Strategies with Large Language Models", "abstract": "This paper introduces an approach that integrates self-adaptive Evolution\nStrategies (ES) with Large Language Models (LLMs) to enhance the explainability\nof complex optimization processes. By employing a self-adaptive ES equipped\nwith a restart mechanism, we effectively navigate the challenging landscapes of\nbenchmark functions, capturing detailed logs of the optimization journey. The\nlogs include fitness evolution, step-size adjustments and restart events due to\nstagnation. An LLM is then utilized to process these logs, generating concise,\nuser-friendly summaries that highlight key aspects such as convergence\nbehavior, optimal fitness achievements, and encounters with local optima. Our\ncase study on the Rastrigin function demonstrates how our approach makes the\ncomplexities of ES optimization transparent. Our findings highlight the\npotential of using LLMs to bridge the gap between advanced optimization\nalgorithms and their interpretability.", "published": "2024-07-11 09:28:27", "link": "http://arxiv.org/abs/2407.08331v2", "categories": ["cs.NE", "cs.AI", "cs.CL"], "primary_category": "cs.NE"}
{"title": "Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large\n  Language Models -- The Story Goes On", "abstract": "In this paper, we investigate the underlying factors that potentially enhance\nthe mathematical reasoning capabilities of large language models (LLMs). We\nargue that the data scaling law for math reasoning capabilities in modern LLMs\nis far from being saturated, highlighting how the model's quality improves with\nincreases in data quantity. To support this claim, we introduce the\nSkywork-Math model series, supervised fine-tuned (SFT) on common 7B LLMs using\nour proposed 2.5M-instance Skywork-MathQA dataset. Skywork-Math 7B has achieved\nimpressive accuracies of 51.2% on the competition-level MATH benchmark and\n83.9% on the GSM8K benchmark using only SFT data, outperforming an early\nversion of GPT-4 on MATH. The superior performance of Skywork-Math models\ncontributes to our novel two-stage data synthesis and model SFT pipelines,\nwhich include three different augmentation methods and a diverse seed problem\nset, ensuring both the quantity and quality of Skywork-MathQA dataset across\nvarying difficulty levels. Most importantly, we provide several practical\ntakeaways to enhance math reasoning abilities in LLMs for both research and\nindustry applications.", "published": "2024-07-11 09:56:51", "link": "http://arxiv.org/abs/2407.08348v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Autoregressive Speech Synthesis without Vector Quantization", "abstract": "We present MELLE, a novel continuous-valued tokens based language modeling\napproach for text to speech synthesis (TTS). MELLE autoregressively generates\ncontinuous mel-spectrogram frames directly from text condition, bypassing the\nneed for vector quantization, which are originally designed for audio\ncompression and sacrifice fidelity compared to mel-spectrograms. Specifically,\n(i) instead of cross-entropy loss, we apply regression loss with a proposed\nspectrogram flux loss function to model the probability distribution of the\ncontinuous-valued tokens. (ii) we have incorporated variational inference into\nMELLE to facilitate sampling mechanisms, thereby enhancing the output diversity\nand model robustness. Experiments demonstrate that, compared to the two-stage\ncodec language models VALL-E and its variants, the single-stage MELLE mitigates\nrobustness issues by avoiding the inherent flaws of sampling discrete codes,\nachieves superior performance across multiple metrics, and, most importantly,\noffers a more streamlined paradigm. See https://aka.ms/melle for demos of our\nwork.", "published": "2024-07-11 14:36:53", "link": "http://arxiv.org/abs/2407.08551v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Transformer Circuit Faithfulness Metrics are not Robust", "abstract": "Mechanistic interpretability work attempts to reverse engineer the learned\nalgorithms present inside neural networks. One focus of this work has been to\ndiscover 'circuits' -- subgraphs of the full model that explain behaviour on\nspecific tasks. But how do we measure the performance of such circuits? Prior\nwork has attempted to measure circuit 'faithfulness' -- the degree to which the\ncircuit replicates the performance of the full model. In this work, we survey\nmany considerations for designing experiments that measure circuit faithfulness\nby ablating portions of the model's computation. Concerningly, we find existing\nmethods are highly sensitive to seemingly insignificant changes in the ablation\nmethodology. We conclude that existing circuit faithfulness scores reflect both\nthe methodological choices of researchers as well as the actual components of\nthe circuit - the task a circuit is required to perform depends on the ablation\nused to test it. The ultimate goal of mechanistic interpretability work is to\nunderstand neural networks, so we emphasize the need for more clarity in the\nprecise claims being made about circuits. We open source a library at\nhttps://github.com/UFO-101/auto-circuit that includes highly efficient\nimplementations of a wide range of ablation methodologies and circuit discovery\nalgorithms.", "published": "2024-07-11 17:59:00", "link": "http://arxiv.org/abs/2407.08734v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Proving that Cryptic Crossword Clue Answers are Correct", "abstract": "Cryptic crossword clues are challenging cognitive tasks, for which new test\nsets are released on a daily basis by multiple international newspapers. Each\ncryptic clue contains both the definition of the answer to be placed in the\ncrossword grid (in common with regular crosswords), and `wordplay' that proves\nthat the answer is correct (i.e. a human solver can be confident that an answer\nis correct without needing crossing words to confirm it). Using an existing\ncryptic wordplay proving framework (operating on Python proofs created by an\nLLM), we show that it is possible to distinguish between correct answers and\nalmost-correct ones based upon whether the wordplay `works'.", "published": "2024-07-11 19:13:16", "link": "http://arxiv.org/abs/2407.08824v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Brief state of the art in social information mining: Practical\n  application in analysis of trends in French legislative 2024", "abstract": "The analysis of social media information has undergone significant evolution\nin the last decade due to advancements in artificial intelligence (AI) and\nmachine learning (ML). This paper provides an overview of the state-of-the-art\ntechniques in social media mining, with a practical application in analyzing\ntrends in the 2024 French legislative elections. We leverage natural language\nprocessing (NLP) tools to gauge public opinion by extracting and analyzing\ncomments and reactions from the AgoraVox platform. The study reveals that the\nNational Rally party, led by Marine Le Pen, maintains a high level of\nengagement on social media, outperforming traditional parties. This trend is\ncorroborated by user interactions, indicating a strong digital presence. The\nresults highlight the utility of advanced AI models, such as transformers and\nlarge language models (LLMs), in capturing nuanced public sentiments and\npredicting political leanings, demonstrating their potential in real-time\nreputation management and crisis response.", "published": "2024-07-11 18:22:58", "link": "http://arxiv.org/abs/2408.01911v1", "categories": ["cs.CY", "cs.CL", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Diff-MST: Differentiable Mixing Style Transfer", "abstract": "Mixing style transfer automates the generation of a multitrack mix for a\ngiven set of tracks by inferring production attributes from a reference song.\nHowever, existing systems for mixing style transfer are limited in that they\noften operate only on a fixed number of tracks, introduce artifacts, and\nproduce mixes in an end-to-end fashion, without grounding in traditional audio\neffects, prohibiting interpretability and controllability. To overcome these\nchallenges, we introduce Diff-MST, a framework comprising a differentiable\nmixing console, a transformer controller, and an audio production style loss\nfunction. By inputting raw tracks and a reference song, our model estimates\ncontrol parameters for audio effects within a differentiable mixing console,\nproducing high-quality mixes and enabling post-hoc adjustments. Moreover, our\narchitecture supports an arbitrary number of input tracks without source\nlabelling, enabling real-world applications. We evaluate our model's\nperformance against robust baselines and showcase the effectiveness of our\napproach, architectural design, tailored audio production style loss, and\ninnovative training methodology for the given task.", "published": "2024-07-11 23:06:47", "link": "http://arxiv.org/abs/2407.08889v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Spiking Tucker Fusion Transformer for Audio-Visual Zero-Shot Learning", "abstract": "The spiking neural networks (SNNs) that efficiently encode temporal sequences\nhave shown great potential in extracting audio-visual joint feature\nrepresentations. However, coupling SNNs (binary spike sequences) with\ntransformers (float-point sequences) to jointly explore the temporal-semantic\ninformation still facing challenges. In this paper, we introduce a novel\nSpiking Tucker Fusion Transformer (STFT) for audio-visual zero-shot learning\n(ZSL). The STFT leverage the temporal and semantic information from different\ntime steps to generate robust representations. The time-step factor (TSF) is\nintroduced to dynamically synthesis the subsequent inference information. To\nguide the formation of input membrane potentials and reduce the spike noise, we\npropose a global-local pooling (GLP) which combines the max and average pooling\noperations. Furthermore, the thresholds of the spiking neurons are dynamically\nadjusted based on semantic and temporal cues. Integrating the temporal and\nsemantic information extracted by SNNs and Transformers are difficult due to\nthe increased number of parameters in a straightforward bilinear model. To\naddress this, we introduce a temporal-semantic Tucker fusion module, which\nachieves multi-scale fusion of SNN and Transformer outputs while maintaining\nfull second-order interactions. Our experimental results demonstrate the\neffectiveness of the proposed approach in achieving state-of-the-art\nperformance in three benchmark datasets. The harmonic mean (HM) improvement of\nVGGSound, UCF101 and ActivityNet are around 15.4\\%, 3.9\\%, and 14.9\\%,\nrespectively.", "published": "2024-07-11 02:01:26", "link": "http://arxiv.org/abs/2407.08130v1", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "An Unsupervised Domain Adaptation Method for Locating Manipulated Region\n  in partially fake Audio", "abstract": "When the task of locating manipulation regions in partially-fake audio (PFA)\ninvolves cross-domain datasets, the performance of deep learning models drops\nsignificantly due to the shift between the source and target domains. To\naddress this issue, existing approaches often employ data augmentation before\ntraining. However, they overlook the characteristics in target domain that are\nabsent in source domain. Inspired by the mixture-of-experts model, we propose\nan unsupervised method named Samples mining with Diversity and Entropy (SDE).\nOur method first learns from a collection of diverse experts that achieve great\nperformance from different perspectives in the source domain, but with\nambiguity on target samples. We leverage these diverse experts to select the\nmost informative samples by calculating their entropy. Furthermore, we\nintroduced a label generation method tailored for these selected samples that\nare incorporated in the training process in source domain integrating the\ntarget domain information. We applied our method to a cross-domain partially\nfake audio detection dataset, ADD2023Track2. By introducing 10% of unknown\nsamples from the target domain, we achieved an F1 score of 43.84%, which\nrepresents a relative increase of 77.2% compared to the second-best method.", "published": "2024-07-11 07:32:16", "link": "http://arxiv.org/abs/2407.08239v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Let Network Decide What to Learn: Symbolic Music Understanding Model\n  Based on Large-scale Adversarial Pre-training", "abstract": "As a crucial aspect of Music Information Retrieval (MIR), Symbolic Music\nUnderstanding (SMU) has garnered significant attention for its potential to\nassist both musicians and enthusiasts in learning and creating music. Recently,\npre-trained language models have been widely adopted in SMU due to the\nsubstantial similarities between symbolic music and natural language, as well\nas the ability of these models to leverage limited music data effectively.\nHowever, some studies have shown the common pre-trained methods like Mask\nLanguage Model (MLM) may introduce bias issues like racism discrimination in\nNatural Language Process (NLP) and affects the performance of downstream tasks,\nwhich also happens in SMU. This bias often arises when masked tokens cannot be\ninferred from their context, forcing the model to overfit the training set\ninstead of generalizing. To address this challenge, we propose\nAdversarial-MidiBERT for SMU, which adaptively determines what to mask during\nMLM via a masker network, rather than employing random masking. By avoiding the\nmasking of tokens that are difficult to infer from context, our model is better\nequipped to capture contextual structures and relationships, rather than merely\nconforming to the training data distribution. We evaluate our method across\nfour SMU tasks, and our approach demonstrates excellent performance in all\ncases. The code for our model is publicly available at\nhttps://github.com/RS2002/Adversarial-MidiBERT.", "published": "2024-07-11 08:54:38", "link": "http://arxiv.org/abs/2407.08306v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "From Real to Cloned Singer Identification", "abstract": "Cloned voices of popular singers sound increasingly realistic and have gained\npopularity over the past few years. They however pose a threat to the industry\ndue to personality rights concerns. As such, methods to identify the original\nsinger in synthetic voices are needed. In this paper, we investigate how singer\nidentification methods could be used for such a task. We present three\nembedding models that are trained using a singer-level contrastive learning\nscheme, where positive pairs consist of segments with vocals from the same\nsingers. These segments can be mixtures for the first model, vocals for the\nsecond, and both for the third. We demonstrate that all three models are highly\ncapable of identifying real singers. However, their performance deteriorates\nwhen classifying cloned versions of singers in our evaluation set. This is\nespecially true for models that use mixtures as an input. These findings\nhighlight the need to understand the biases that exist within singer\nidentification systems, and how they can influence the identification of voice\ndeepfakes in music.", "published": "2024-07-11 16:25:21", "link": "http://arxiv.org/abs/2407.08647v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ElasticAST: An Audio Spectrogram Transformer for All Length and\n  Resolutions", "abstract": "Transformers have rapidly overtaken CNN-based architectures as the new\nstandard in audio classification. Transformer-based models, such as the Audio\nSpectrogram Transformers (AST), also inherit the fixed-size input paradigm from\nCNNs. However, this leads to performance degradation for ASTs in the inference\nwhen input lengths vary from the training. This paper introduces an approach\nthat enables the use of variable-length audio inputs with AST models during\nboth training and inference. By employing sequence packing, our method\nElasticAST, accommodates any audio length during training, thereby offering\nflexibility across all lengths and resolutions at the inference. This\nflexibility allows ElasticAST to maintain evaluation capabilities at various\nlengths or resolutions and achieve similar performance to standard ASTs trained\nat specific lengths or resolutions. Moreover, experiments demonstrate\nElasticAST's better performance when trained and evaluated on native-length\naudio datasets.", "published": "2024-07-11 17:29:56", "link": "http://arxiv.org/abs/2407.08691v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
