{"title": "ParaSCI: A Large Scientific Paraphrase Dataset for Longer Paraphrase\n  Generation", "abstract": "We propose ParaSCI, the first large-scale paraphrase dataset in the\nscientific field, including 33,981 paraphrase pairs from ACL (ParaSCI-ACL) and\n316,063 pairs from arXiv (ParaSCI-arXiv). Digging into characteristics and\ncommon patterns of scientific papers, we construct this dataset though\nintra-paper and inter-paper methods, such as collecting citations to the same\npaper or aggregating definitions by scientific terms. To take advantage of\nsentences paraphrased partially, we put up PDBERT as a general paraphrase\ndiscovering method. The major advantages of paraphrases in ParaSCI lie in the\nprominent length and textual diversity, which is complementary to existing\nparaphrase datasets. ParaSCI obtains satisfactory results on human evaluation\nand downstream tasks, especially long paraphrase generation.", "published": "2021-01-21 01:10:06", "link": "http://arxiv.org/abs/2101.08382v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Content Selection Network for Document-grounded Retrieval-based Chatbots", "abstract": "Grounding human-machine conversation in a document is an effective way to\nimprove the performance of retrieval-based chatbots. However, only a part of\nthe document content may be relevant to help select the appropriate response at\na round. It is thus crucial to select the part of document content relevant to\nthe current conversation context. In this paper, we propose a document content\nselection network (CSN) to perform explicit selection of relevant document\ncontents, and filter out the irrelevant parts. We show in experiments on two\npublic document-grounded conversation datasets that CSN can effectively help\nselect the relevant document contents to the conversation context, and it\nproduces better results than the state-of-the-art approaches. Our code and\ndatasets are available at https://github.com/DaoD/CSN.", "published": "2021-01-21 03:47:06", "link": "http://arxiv.org/abs/2101.08426v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-sense embeddings through a word sense disambiguation process", "abstract": "Natural Language Understanding has seen an increasing number of publications\nin the last few years, especially after robust word embeddings models became\nprominent, when they proved themselves able to capture and represent semantic\nrelationships from massive amounts of data. Nevertheless, traditional models\noften fall short in intrinsic issues of linguistics, such as polysemy and\nhomonymy. Any expert system that makes use of natural language in its core, can\nbe affected by a weak semantic representation of text, resulting in inaccurate\noutcomes based on poor decisions. To mitigate such issues, we propose a novel\napproach called Most Suitable Sense Annotation (MSSA), that disambiguates and\nannotates each word by its specific sense, considering the semantic effects of\nits context. Our approach brings three main contributions to the semantic\nrepresentation scenario: (i) an unsupervised technique that disambiguates and\nannotates words by their senses, (ii) a multi-sense embeddings model that can\nbe extended to any traditional word embeddings algorithm, and (iii) a recurrent\nmethodology that allows our models to be re-used and their representations\nrefined. We test our approach on six different benchmarks for the word\nsimilarity task, showing that our approach can produce state-of-the-art results\nand outperforms several more complex state-of-the-art systems.", "published": "2021-01-21 16:22:34", "link": "http://arxiv.org/abs/2101.08700v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Multilingual Text Encoders for Unsupervised Cross-Lingual\n  Retrieval", "abstract": "Pretrained multilingual text encoders based on neural Transformer\narchitectures, such as multilingual BERT (mBERT) and XLM, have achieved strong\nperformance on a myriad of language understanding tasks. Consequently, they\nhave been adopted as a go-to paradigm for multilingual and cross-lingual\nrepresentation learning and transfer, rendering cross-lingual word embeddings\n(CLWEs) effectively obsolete. However, questions remain to which extent this\nfinding generalizes 1) to unsupervised settings and 2) for ad-hoc cross-lingual\nIR (CLIR) tasks. Therefore, in this work we present a systematic empirical\nstudy focused on the suitability of the state-of-the-art multilingual encoders\nfor cross-lingual document and sentence retrieval tasks across a large number\nof language pairs. In contrast to supervised language understanding, our\nresults indicate that for unsupervised document-level CLIR -- a setup with no\nrelevance judgments for IR-specific fine-tuning -- pretrained encoders fail to\nsignificantly outperform models based on CLWEs. For sentence-level CLIR, we\ndemonstrate that state-of-the-art performance can be achieved. However, the\npeak performance is not met using the general-purpose multilingual text\nencoders `off-the-shelf', but rather relying on their variants that have been\nfurther specialized for sentence understanding tasks.", "published": "2021-01-21 00:15:38", "link": "http://arxiv.org/abs/2101.08370v1", "categories": ["cs.CL", "cs.IR", "H.3.3; I.2.7"], "primary_category": "cs.CL"}
{"title": "Invariance, encodings, and generalization: learning identity effects\n  with neural networks", "abstract": "Often in language and other areas of cognition, whether two components of an\nobject are identical or not determines if it is well formed. We call such\nconstraints identity effects. When developing a system to learn well-formedness\nfrom examples, it is easy enough to build in an identify effect. But can\nidentity effects be learned from the data without explicit guidance? We provide\na framework in which we can rigorously prove that algorithms satisfying simple\ncriteria cannot make the correct inference. We then show that a broad class of\nlearning algorithms including deep feedforward neural networks trained via\ngradient-based algorithms (such as stochastic gradient descent or the Adam\nmethod) satisfy our criteria, dependent on the encoding of inputs. In some\nbroader circumstances we are able to provide adversarial examples that the\nnetwork necessarily classifies incorrectly. Finally, we demonstrate our theory\nwith computational experiments in which we explore the effect of different\ninput encodings on the ability of algorithms to generalize to novel inputs.\nThis allows us to show similar effects to those predicted by theory for more\nrealistic methods that violate some of the conditions of our theoretical\nresults.", "published": "2021-01-21 01:28:15", "link": "http://arxiv.org/abs/2101.08386v5", "categories": ["cs.LG", "cs.CL", "68Q25, 68R10, 68U05"], "primary_category": "cs.LG"}
{"title": "Validating Label Consistency in NER Data Annotation", "abstract": "Data annotation plays a crucial role in ensuring your named entity\nrecognition (NER) projects are trained with the right information to learn\nfrom. Producing the most accurate labels is a challenge due to the complexity\ninvolved with annotation. Label inconsistency between multiple subsets of data\nannotation (e.g., training set and test set, or multiple training subsets) is\nan indicator of label mistakes. In this work, we present an empirical method to\nexplore the relationship between label (in-)consistency and NER model\nperformance. It can be used to validate the label consistency (or catches the\ninconsistency) in multiple sets of NER data annotation. In experiments, our\nmethod identified the label inconsistency of test data in SCIERC and CoNLL03\ndatasets (with 26.7% and 5.4% label mistakes). It validated the consistency in\nthe corrected version of both datasets.", "published": "2021-01-21 16:19:00", "link": "http://arxiv.org/abs/2101.08698v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Distilling Large Language Models into Tiny and Effective Students using\n  pQRNN", "abstract": "Large pre-trained multilingual models like mBERT, XLM-R achieve state of the\nart results on language understanding tasks. However, they are not well suited\nfor latency critical applications on both servers and edge devices. It's\nimportant to reduce the memory and compute resources required by these models.\nTo this end, we propose pQRNN, a projection-based embedding-free neural encoder\nthat is tiny and effective for natural language processing tasks. Without\npre-training, pQRNNs significantly outperform LSTM models with pre-trained\nembeddings despite being 140x smaller. With the same number of parameters, they\noutperform transformer baselines thereby showcasing their parameter efficiency.\nAdditionally, we show that pQRNNs are effective student architectures for\ndistilling large pre-trained language models. We perform careful ablations\nwhich study the effect of pQRNN parameters, data augmentation, and distillation\nsettings. On MTOP, a challenging multilingual semantic parsing dataset, pQRNN\nstudents achieve 95.9\\% of the performance of an mBERT teacher while being 350x\nsmaller. On mATIS, a popular parsing task, pQRNN students on average are able\nto get to 97.1\\% of the teacher while again being 350x smaller. Our strong\nresults suggest that our approach is great for latency-sensitive applications\nwhile being able to leverage large mBERT-like models.", "published": "2021-01-21 23:45:50", "link": "http://arxiv.org/abs/2101.08890v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Analysis of Basic Emotions in Texts Based on BERT Vector Representation", "abstract": "In the following paper the authors present a GAN-type model and the most\nimportant stages of its development for the task of emotion recognition in\ntext. In particular, we propose an approach for generating a synthetic dataset\nof all possible emotions combinations based on manually labelled incomplete\ndata.", "published": "2021-01-21 07:11:21", "link": "http://arxiv.org/abs/2101.11433v2", "categories": ["cs.CL", "cs.LG", "I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Challenges Encountered in Turkish Natural Language Processing Studies", "abstract": "Natural language processing is a branch of computer science that combines\nartificial intelligence with linguistics. It aims to analyze a language element\nsuch as writing or speaking with software and convert it into information.\nConsidering that each language has its own grammatical rules and vocabulary\ndiversity, the complexity of the studies in this field is somewhat\nunderstandable. For instance, Turkish is a very interesting language in many\nways. Examples of this are agglutinative word structure, consonant/vowel\nharmony, a large number of productive derivational morphemes (practically\ninfinite vocabulary), derivation and syntactic relations, a complex emphasis on\nvocabulary and phonological rules. In this study, the interesting features of\nTurkish in terms of natural language processing are mentioned. In addition,\nsummary info about natural language processing techniques, systems and various\nsources developed for Turkish are given.", "published": "2021-01-21 08:30:33", "link": "http://arxiv.org/abs/2101.11436v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Arabic Speech Recognition by End-to-End, Modular Systems and Human", "abstract": "Recent advances in automatic speech recognition (ASR) have achieved accuracy\nlevels comparable to human transcribers, which led researchers to debate if the\nmachine has reached human performance. Previous work focused on the English\nlanguage and modular hidden Markov model-deep neural network (HMM-DNN) systems.\nIn this paper, we perform a comprehensive benchmarking for end-to-end\ntransformer ASR, modular HMM-DNN ASR, and human speech recognition (HSR) on the\nArabic language and its dialects. For the HSR, we evaluate linguist performance\nand lay-native speaker performance on a new dataset collected as a part of this\nstudy. For ASR the end-to-end work led to 12.5%, 27.5%, 33.8% WER; a new\nperformance milestone for the MGB2, MGB3, and MGB5 challenges respectively. Our\nresults suggest that human performance in the Arabic language is still\nconsiderably better than the machine with an absolute WER gap of 3.5% on\naverage.", "published": "2021-01-21 05:55:29", "link": "http://arxiv.org/abs/2101.08454v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Adv-OLM: Generating Textual Adversaries via OLM", "abstract": "Deep learning models are susceptible to adversarial examples that have\nimperceptible perturbations in the original input, resulting in adversarial\nattacks against these models. Analysis of these attacks on the state of the art\ntransformers in NLP can help improve the robustness of these models against\nsuch adversarial inputs. In this paper, we present Adv-OLM, a black-box attack\nmethod that adapts the idea of Occlusion and Language Models (OLM) to the\ncurrent state of the art attack methods. OLM is used to rank words of a\nsentence, which are later substituted using word replacement strategies. We\nexperimentally show that our approach outperforms other attack methods for\nseveral text classification tasks.", "published": "2021-01-21 10:04:56", "link": "http://arxiv.org/abs/2101.08523v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Noisy-target Training: A Training Strategy for DNN-based Speech\n  Enhancement without Clean Speech", "abstract": "Deep neural network (DNN)-based speech enhancement ordinarily requires clean\nspeech signals as the training target. However, collecting clean signals is\nvery costly because they must be recorded in a studio. This requirement\ncurrently restricts the amount of training data for speech enhancement to less\nthan 1/1000 of that of speech recognition which does not need clean signals.\nIncreasing the amount of training data is important for improving the\nperformance, and hence the requirement of clean signals should be relaxed. In\nthis paper, we propose a training strategy that does not require clean signals.\nThe proposed method only utilizes noisy signals for training, which enables us\nto use a variety of speech signals in the wild. Our experimental results showed\nthat the proposed method can achieve the performance similar to that of a DNN\ntrained with clean signals.", "published": "2021-01-21 14:14:21", "link": "http://arxiv.org/abs/2101.08625v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Effect of Deep Learning Feature Inference Techniques on Respiratory\n  Sounds", "abstract": "Analysis of respiratory sounds increases its importance every day. Many\ndifferent methods are available in the analysis, and new techniques are\ncontinuing to be developed to further improve these methods. Features are\nextracted from audio signals and trained using different machine learning\ntechniques. The use of deep learning, which is a different method and has\nincreased in recent years, also shows its influence in this field. Deep\nlearning techniques applied to the image of audio signals give good results and\ncontinue to be developed. In this study, image filters were applied to the\nvalues obtained from audio signals and the results of the features formed from\nthis were examined in machine learning and deep learning techniques. Their\nresults were compared with the results of methods that had previously achieved\ngood results.", "published": "2021-01-21 04:52:23", "link": "http://arxiv.org/abs/2101.08438v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Turkish Voice Commands based Chess Game using Gammatone Cepstral\n  Coefficients", "abstract": "This study was carried out to enable individuals with limited mobility skills\nto play chess in real time and to play games with the individuals around them\nwithout being under any social distress or stress. Voice recordings were taken\nfrom 50 people (23 men and 27 women). While recording the sound, 29 words from\neach person were used which are determined as necessary for playing the game.\nMel Frequency Coefficients (MFCC) and Gammatone Cepstral Coefficients (GTCC)\nqualification methods were used. In addition, k-NN, Naive Bayes and Neural\nNetwork classification methods were used for classification. Two different\nclassification procedures were applied, namely, person-based and general. While\nthe performance rate in person-based classification ranged from 75% to 98%, a\nperformance over 84% was achieved in general classification.", "published": "2021-01-21 04:58:17", "link": "http://arxiv.org/abs/2101.08441v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Online Streaming End-to-End Neural Diarization Handling Overlapping\n  Speech and Flexible Numbers of Speakers", "abstract": "We propose a streaming diarization method based on an end-to-end neural\ndiarization (EEND) model, which handles flexible numbers of speakers and\noverlapping speech. In our previous study, the speaker-tracing buffer (STB)\nmechanism was proposed to achieve a chunk-wise streaming diarization using a\npre-trained EEND model. STB traces the speaker information in previous chunks\nto map the speakers in a new chunk. However, it only worked with two-speaker\nrecordings. In this paper, we propose an extended STB for flexible numbers of\nspeakers, FLEX-STB. The proposed method uses a zero-padding followed by\nspeaker-tracing, which alleviates the difference in the number of speakers\nbetween a buffer and a current chunk. We also examine buffer update strategies\nto select important frames for tracing multiple speakers. Experiments on\nCALLHOME and DIHARD II datasets show that the proposed method achieves\ncomparable performance to the offline EEND method with 1-second latency. The\nresults also show that our proposed method outperforms recently proposed\nchunk-wise diarization methods based on EEND (BW-EDA-EEND).", "published": "2021-01-21 07:24:21", "link": "http://arxiv.org/abs/2101.08473v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Effect of Window Size for Detection of Abnormalities in Respiratory\n  Sounds", "abstract": "The recording of respiratory sounds was of significant benefit in the\ndiagnosis of abnormalities in respiratory sounds. The duration of the sounds\nused in the diagnosis affects the speed of the diagnosis. In this study, the\neffect of window size on diagnosis of abnormalities in respiratory sounds and\nthe most efficient recording time for diagnosis were analyzed. First, window\nsize was applied to each sound in the data set consisting of normal and\nabnormal breathing sounds, 0.5 second and from 1 to 20 seconds Increased by 1\nsecond. Then, the data applied to window size was inferred feature extraction\nwith Mel Frequency Cepstral Coefficient (MFCC) and the performance of each\nwindow was calculated using the leave one-out classifier and the k-nearest\nneighbor (KNN) algorithm. As a result, it was determined that the data was\nsignificant with an average performance of 92.06% in the records between 2 and\n10 seconds.", "published": "2021-01-21 08:28:22", "link": "http://arxiv.org/abs/2101.08495v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Joint Diagonalization Based Efficient Approach to Underdetermined\n  Blind Audio Source Separation Using the Multichannel Wiener Filter", "abstract": "This paper presents a computationally efficient approach to blind source\nseparation (BSS) of audio signals, applicable even when there are more sources\nthan microphones (i.e., the underdetermined case). When there are as many\nsources as microphones (i.e., the determined case), BSS can be performed\ncomputationally efficiently by independent component analysis (ICA).\nUnfortunately, however, ICA is basically inapplicable to the underdetermined\ncase. Another BSS approach using the multichannel Wiener filter (MWF) is\napplicable even to this case, and encompasses full-rank spatial covariance\nanalysis (FCA) and multichannel non-negative matrix factorization (MNMF).\nHowever, these methods require massive numbers of matrix inversions to design\nthe MWF, and are thus computationally inefficient. To overcome this drawback,\nwe exploit the well-known property of diagonal matrices that matrix inversion\namounts to mere inversion of the diagonal elements and can thus be performed\ncomputationally efficiently. This makes it possible to drastically reduce the\ncomputational cost of the above matrix inversions based on a joint\ndiagonalization (JD) idea, leading to computationally efficient BSS.\nSpecifically, we restrict the N spatial covariance matrices (SCMs) of all N\nsources to a class of (exactly) jointly diagonalizable matrices. Based on this\napproach, we present FastFCA, a computationally efficient extension of FCA. We\nalso present a unified framework for underdetermined and determined audio BSS,\nwhich highlights a theoretical connection between FastFCA and other methods.\nMoreover, we reveal that FastFCA can be regarded as a regularized version of\napproximate joint diagonalization (AJD).", "published": "2021-01-21 11:42:02", "link": "http://arxiv.org/abs/2101.08563v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Study of F0 Modification for X-Vector Based Speech Pseudonymization\n  Across Gender", "abstract": "Speech pseudonymization aims at altering a speech signal to map the\nidentifiable personal characteristics of a given speaker to another identity.\nIn other words, it aims to hide the source speaker identity while preserving\nthe intelligibility of the spoken content. This study takes place in the\nVoicePrivacy 2020 challenge framework, where the baseline system performs\npseudonymization by modifying x-vector information to match a target speaker\nwhile keeping the fundamental frequency (F0) unchanged. We propose to alter\nother paralin-guistic features, here F0, and analyze the impact of this\nmodification across gender. We found that the proposed F0 modification always\nimproves pseudonymization We observed that both source and target speaker\ngenders affect the performance gain when modifying the F0.", "published": "2021-01-21 07:33:07", "link": "http://arxiv.org/abs/2101.08478v1", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "LEAF: A Learnable Frontend for Audio Classification", "abstract": "Mel-filterbanks are fixed, engineered audio features which emulate human\nperception and have been used through the history of audio understanding up to\ntoday. However, their undeniable qualities are counterbalanced by the\nfundamental limitations of handmade representations. In this work we show that\nwe can train a single learnable frontend that outperforms mel-filterbanks on a\nwide range of audio signals, including speech, music, audio events and animal\nsounds, providing a general-purpose learned frontend for audio classification.\nTo do so, we introduce a new principled, lightweight, fully learnable\narchitecture that can be used as a drop-in replacement of mel-filterbanks. Our\nsystem learns all operations of audio features extraction, from filtering to\npooling, compression and normalization, and can be integrated into any neural\nnetwork at a negligible parameter cost. We perform multi-task training on eight\ndiverse audio classification tasks, and show consistent improvements of our\nmodel over mel-filterbanks and previous learnable alternatives. Moreover, our\nsystem outperforms the current state-of-the-art learnable frontend on Audioset,\nwith orders of magnitude fewer parameters.", "published": "2021-01-21 13:25:58", "link": "http://arxiv.org/abs/2101.08596v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
