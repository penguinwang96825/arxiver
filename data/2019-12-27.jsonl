{"title": "Language Independent Sentiment Analysis", "abstract": "Social media platforms and online forums generate rapid and increasing amount\nof textual data. Businesses, government agencies, and media organizations seek\nto perform sentiment analysis on this rich text data. The results of these\nanalytics are used for adapting marketing strategies, customizing products,\nsecurity and various other decision makings. Sentiment analysis has been\nextensively studied and various methods have been developed for it with great\nsuccess. These methods, however apply to texts written in a specific language.\nThis limits applicability to a limited demographic and a specific geographic\nregion. In this paper we propose a general approach for sentiment analysis on\ndata containing texts from multiple languages. This enables all the\napplications to utilize the results of sentiment analysis in a language\noblivious or language-independent fashion.", "published": "2019-12-27 03:20:48", "link": "http://arxiv.org/abs/1912.11973v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explicit Sentence Compression for Neural Machine Translation", "abstract": "State-of-the-art Transformer-based neural machine translation (NMT) systems\nstill follow a standard encoder-decoder framework, in which source sentence\nrepresentation can be well done by an encoder with self-attention mechanism.\nThough Transformer-based encoder may effectively capture general information in\nits resulting source sentence representation, the backbone information, which\nstands for the gist of a sentence, is not specifically focused on. In this\npaper, we propose an explicit sentence compression method to enhance the source\nsentence representation for NMT. In practice, an explicit sentence compression\ngoal used to learn the backbone information in a sentence. We propose three\nways, including backbone source-side fusion, target-side fusion, and both-side\nfusion, to integrate the compressed sentence into NMT. Our empirical tests on\nthe WMT English-to-French and English-to-German translation tasks show that the\nproposed sentence compression method significantly improves the translation\nperformances over strong baselines.", "published": "2019-12-27 04:14:06", "link": "http://arxiv.org/abs/1912.11980v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Synthesising Expressiveness in Peking Opera via Duration Informed\n  Attention Network", "abstract": "This paper presents a method that generates expressive singing voice of\nPeking opera. The synthesis of expressive opera singing usually requires pitch\ncontours to be extracted as the training data, which relies on techniques and\nis not able to be manually labeled. With the Duration Informed Attention\nNetwork (DurIAN), this paper makes use of musical note instead of pitch\ncontours for expressive opera singing synthesis. The proposed method enables\nhuman annotation being combined with automatic extracted features to be used as\ntraining data thus the proposed method gives extra flexibility in data\ncollection for Peking opera singing synthesis. Comparing with the expressive\nsinging voice of Peking opera synthesised by pitch contour based system, the\nproposed musical note based system produces comparable singing voice in Peking\nopera with expressiveness in various aspects.", "published": "2019-12-27 07:28:11", "link": "http://arxiv.org/abs/1912.12010v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multi-cascaded Model with Data Augmentation for Enhanced Paraphrase\n  Detection in Short Texts", "abstract": "Paraphrase detection is an important task in text analytics with numerous\napplications such as plagiarism detection, duplicate question identification,\nand enhanced customer support helpdesks. Deep models have been proposed for\nrepresenting and classifying paraphrases. These models, however, require large\nquantities of human-labeled data, which is expensive to obtain. In this work,\nwe present a data augmentation strategy and a multi-cascaded model for improved\nparaphrase detection in short texts. Our data augmentation strategy considers\nthe notions of paraphrases and non-paraphrases as binary relations over the set\nof texts. Subsequently, it uses graph theoretic concepts to efficiently\ngenerate additional paraphrase and non-paraphrase pairs in a sound manner. Our\nmulti-cascaded model employs three supervised feature learners (cascades) based\non CNN and LSTM networks with and without soft-attention. The learned features,\ntogether with hand-crafted linguistic features, are then forwarded to a\ndiscriminator network for final classification. Our model is both wide and deep\nand provides greater robustness across clean and noisy short texts. We evaluate\nour approach on three benchmark datasets and show that it produces a comparable\nor state-of-the-art performance on all three.", "published": "2019-12-27 12:10:10", "link": "http://arxiv.org/abs/1912.12068v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Job Prediction: From Deep Neural Network Models to Applications", "abstract": "Determining the job is suitable for a student or a person looking for work\nbased on their job's descriptions such as knowledge and skills that are\ndifficult, as well as how employers must find ways to choose the candidates\nthat match the job they require. In this paper, we focus on studying the job\nprediction using different deep neural network models including TextCNN,\nBi-GRU-LSTM-CNN, and Bi-GRU-CNN with various pre-trained word embeddings on the\nIT Job dataset. In addition, we also proposed a simple and effective ensemble\nmodel combining different deep neural network models. The experimental results\nillustrated that our proposed ensemble model achieved the highest result with\nan F1 score of 72.71%. Moreover, we analyze these experimental results to have\ninsights about this problem to find better solutions in the future.", "published": "2019-12-27 16:13:43", "link": "http://arxiv.org/abs/1912.12214v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clinical XLNet: Modeling Sequential Clinical Notes and Predicting\n  Prolonged Mechanical Ventilation", "abstract": "Clinical notes contain rich data, which is unexploited in predictive modeling\ncompared to structured data. In this work, we developed a new text\nrepresentation Clinical XLNet for clinical notes which also leverages the\ntemporal information of the sequence of the notes. We evaluated our models on\nprolonged mechanical ventilation prediction problem and our experiments\ndemonstrated that Clinical XLNet outperforms the best baselines consistently.", "published": "2019-12-27 03:40:31", "link": "http://arxiv.org/abs/1912.11975v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Visual Agreement Regularized Training for Multi-Modal Machine\n  Translation", "abstract": "Multi-modal machine translation aims at translating the source sentence into\na different language in the presence of the paired image. Previous work\nsuggests that additional visual information only provides dispensable help to\ntranslation, which is needed in several very special cases such as translating\nambiguous words. To make better use of visual information, this work presents\nvisual agreement regularized training. The proposed approach jointly trains the\nsource-to-target and target-to-source translation models and encourages them to\nshare the same focus on the visual information when generating semantically\nequivalent visual words (e.g. \"ball\" in English and \"ballon\" in French).\nBesides, a simple yet effective multi-head co-attention model is also\nintroduced to capture interactions between visual and textual features. The\nresults show that our approaches can outperform competitive baselines by a\nlarge margin on the Multi30k dataset. Further analysis demonstrates that the\nproposed regularized training can effectively improve the agreement of\nattention on the image, leading to better use of visual information.", "published": "2019-12-27 07:46:29", "link": "http://arxiv.org/abs/1912.12014v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Encoding word order in complex embeddings", "abstract": "Sequential word order is important when processing text. Currently, neural\nnetworks (NNs) address this by modeling word position using position\nembeddings. The problem is that position embeddings capture the position of\nindividual words, but not the ordered relationship (e.g., adjacency or\nprecedence) between individual word positions. We present a novel and\nprincipled solution for modeling both the global absolute positions of words\nand their order relationships. Our solution generalizes word embeddings,\npreviously defined as independent vectors, to continuous word functions over a\nvariable (position). The benefit of continuous functions over variable\npositions is that word representations shift smoothly with increasing\npositions. Hence, word representations in different positions can correlate\nwith each other in a continuous function. The general solution of these\nfunctions is extended to complex-valued domain due to richer representations.\nWe extend CNN, RNN and Transformer NNs to complex-valued versions to\nincorporate our complex embedding (we make all code available). Experiments on\ntext classification, machine translation and language modeling show gains over\nboth classical word embeddings and position-enriched word embeddings. To our\nknowledge, this is the first work in NLP to link imaginary numbers in\ncomplex-valued representations to concrete meanings (i.e., word order).", "published": "2019-12-27 20:59:38", "link": "http://arxiv.org/abs/1912.12333v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Is Attention All What You Need? -- An Empirical Investigation on\n  Convolution-Based Active Memory and Self-Attention", "abstract": "The key to a Transformer model is the self-attention mechanism, which allows\nthe model to analyze an entire sequence in a computationally efficient manner.\nRecent work has suggested the possibility that general attention mechanisms\nused by RNNs could be replaced by active-memory mechanisms. In this work, we\nevaluate whether various active-memory mechanisms could replace self-attention\nin a Transformer. Our experiments suggest that active-memory alone achieves\ncomparable results to the self-attention mechanism for language modelling, but\noptimal results are mostly achieved by using both active-memory and\nself-attention mechanisms together. We also note that, for some specific\nalgorithmic tasks, active-memory mechanisms alone outperform both\nself-attention and a combination of the two.", "published": "2019-12-27 02:01:13", "link": "http://arxiv.org/abs/1912.11959v2", "categories": ["cs.LG", "cs.CL", "stat.ML", "I.2.7", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Structural characterization of musical harmonies", "abstract": "Understanding the structural characteristics of harmony is essential for an\neffective use of music as a communication medium. Of the three expressive axes\nof music (melody, rhythm, harmony), harmony is the foundation on which the\nemotional content is built, and its understanding is important in areas such as\nmultimedia and affective computing. The common tool for studying this kind of\nstructure in computing science is the formal grammar but, in the case of music,\ngrammars run into problems due to the ambiguous nature of some of the concepts\ndefined in music theory. In this paper, we consider one of such constructs:\nmodulation, that is, the change of key in the middle of a musical piece, an\nimportant tool used by many authors to enhance the capacity of music to express\nemotions. We develop a hybrid method in which an evidence-gathering numerical\nmethod detects modulation and then, based on the detected tonalities, a\nnon-ambiguous grammar can be used for analyzing the structure of each tonal\ncomponent. Experiments with music from the XVII and XVIII centuries show that\nwe can detect the precise point of modulation with an error of at most two\nchords in almost 97% of the cases. Finally, we show examples of complete\nmodulation and structural analysis of musical harmonies.", "published": "2019-12-27 23:15:49", "link": "http://arxiv.org/abs/1912.12362v1", "categories": ["cs.MM", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "MoEVC: A Mixture-of-experts Voice Conversion System with Sparse Gating\n  Mechanism for Accelerating Online Computation", "abstract": "With the recent advancements of deep learning technologies, the performance\nof voice conversion (VC) in terms of quality and similarity has been\nsignificantly improved. However, heavy computations are generally required for\ndeep-learning-based VC systems, which can cause notable latency and thus\nconfine their deployments in real-world applications. Therefore, increasing\nonline computation efficiency has become an important task. In this study, we\npropose a novel mixture-of-experts (MoE) based VC system. The MoE model uses a\ngating mechanism to specify optimal weights to feature maps to increase VC\nperformance. In addition, assigning sparse constraints on the gating mechanism\ncan accelerate online computation by skipping the convolution process by\nzeroing out redundant feature maps. Experimental results show that by\nspecifying suitable sparse constraints, we can effectively increase the online\ncomputation efficiency with a notable 70% FLOPs (floating-point operations per\nsecond) reduction while improving the VC performance in both objective\nevaluations and human listening tests.", "published": "2019-12-27 04:50:20", "link": "http://arxiv.org/abs/1912.11984v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Monaural Speech Enhancement Using a Multi-Branch Temporal Convolutional\n  Network", "abstract": "Deep learning has achieved substantial improvement on single-channel speech\nenhancement tasks. However, the performance of multi-layer perceptions\n(MLPs)-based methods is limited by the ability to capture the long-term\neffective history information. The recurrent neural networks (RNNs), e.g., long\nshort-term memory (LSTM) model, are able to capture the long-term temporal\ndependencies, but come with the issues of the high latency and the complexity\nof training.To address these issues, the temporal convolutional network (TCN)\nwas proposed to replace the RNNs in various sequence modeling tasks. In this\npaper we propose a novel TCN model that employs multi-branch structure, called\nmulti-branch TCN (MB-TCN), for monaural speech enhancement.The MB-TCN exploits\nsplit-transform-aggregate design, which is expected to obtain strong\nrepresentational power at a low computational complexity.Inspired by the TCN,\nthe MB-TCN model incorporates one dimensional causal dilated CNN and residual\nlearning to expand receptive fields for capturing long-term temporal contextual\ninformation.Our extensive experimental investigation suggests that the MB-TCNs\noutperform the residual long short-term memory networks (ResLSTMs), temporal\nconvolutional networks (TCNs), and the CNN networks that employ dense\naggregations in terms of speech intelligibility and quality, while providing\nsuperior parameter efficiency. Furthermore, our experimental results\ndemonstrate that our proposed MB-TCN model is able to outperform multiple\nstate-of-the-art deep learning-based speech enhancement methods in terms of\nfive widely used objective metrics.", "published": "2019-12-27 08:34:05", "link": "http://arxiv.org/abs/1912.12023v5", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Cross-scale Attention Model for Acoustic Event Classification", "abstract": "A major advantage of a deep convolutional neural network (CNN) is that the\nfocused receptive field size is increased by stacking multiple convolutional\nlayers. Accordingly, the model can explore the long-range dependency of\nfeatures from the top layers. However, a potential limitation of the network is\nthat the discriminative features from the bottom layers (which can model the\nshort-range dependency) are smoothed out in the final representation. This\nlimitation is especially evident in the acoustic event classification (AEC)\ntask, where both short- and long-duration events are involved in an audio clip\nand needed to be classified. In this paper, we propose a cross-scale attention\n(CSA) model, which explicitly integrates features from different scales to form\nthe final representation. Moreover, we propose the adoption of the attention\nmechanism to specify the weights of local and global features based on the\nspatial and temporal characteristics of acoustic events. Using mathematic\nformulations, we further reveal that the proposed CSA model can be regarded as\na weighted residual CNN (ResCNN) model when the ResCNN is used as a backbone\nmodel. We tested the proposed model on two AEC datasets: one is an urban AEC\ntask, and the other is an AEC task in smart car environments. Experimental\nresults show that the proposed CSA model can effectively improve the\nperformance of current state-of-the-art deep learning algorithms.", "published": "2019-12-27 07:28:57", "link": "http://arxiv.org/abs/1912.12011v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "nnAudio: An on-the-fly GPU Audio to Spectrogram Conversion Toolbox Using\n  1D Convolution Neural Networks", "abstract": "Converting time domain waveforms to frequency domain spectrograms is\ntypically considered to be a prepossessing step done before model training.\nThis approach, however, has several drawbacks. First, it takes a lot of hard\ndisk space to store different frequency domain representations. This is\nespecially true during the model development and tuning process, when exploring\nvarious types of spectrograms for optimal performance. Second, if another\ndataset is used, one must process all the audio clips again before the network\ncan be retrained. In this paper, we integrate the time domain to frequency\ndomain conversion as part of the model structure, and propose a neural network\nbased toolbox, nnAudio, which leverages 1D convolutional neural networks to\nperform time domain to frequency domain conversion during feed-forward. It\nallows on-the-fly spectrogram generation without the need to store any\nspectrograms on the disk. This approach also allows back-propagation on the\nwaveforms-to-spectrograms transformation layer, which implies that this\ntransformation process can be made trainable, and hence further optimized by\ngradient descent. nnAudio reduces the waveforms-to-spectrograms conversion time\nfor 1,770 waveforms (from the MAPS dataset) from $10.64$ seconds with librosa\nto only $0.001$ seconds for Short-Time Fourier Transform (STFT), $18.3$ seconds\nto $0.015$ seconds for Mel spectrogram, $103.4$ seconds to $0.258$ for\nconstant-Q transform (CQT), when using GPU on our DGX work station with CPU:\nIntel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz Tesla v100 32Gb GPUs. (Only 1 GPU is\nbeing used for all the experiments.) We also further optimize the existing CQT\nalgorithm, so that the CQT spectrogram can be obtained without aliasing in a\nmuch faster computation time (from $0.258$ seconds to only $0.001$ seconds).", "published": "2019-12-27 10:50:31", "link": "http://arxiv.org/abs/1912.12055v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
