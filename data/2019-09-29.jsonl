{"title": "Semi-Supervised Neural Text Generation by Joint Learning of Natural\n  Language Generation and Natural Language Understanding Models", "abstract": "In Natural Language Generation (NLG), End-to-End (E2E) systems trained\nthrough deep learning have recently gained a strong interest. Such deep models\nneed a large amount of carefully annotated data to reach satisfactory\nperformance. However, acquiring such datasets for every new NLG application is\na tedious and time-consuming task. In this paper, we propose a semi-supervised\ndeep learning scheme that can learn from non-annotated data and annotated data\nwhen available. It uses an NLG and a Natural Language Understanding (NLU)\nsequence-to-sequence models which are learned jointly to compensate for the\nlack of annotation. Experiments on two benchmark datasets show that, with\nlimited amount of annotated data, the method can achieve very competitive\nresults while not using any pre-processing or re-scoring tricks. These findings\nopen the way to the exploitation of non-annotated datasets which is the current\nbottleneck for the E2E NLG system development to new applications.", "published": "2019-09-29 11:37:18", "link": "http://arxiv.org/abs/1910.03484v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Zero-resource Cross-lingual Entity Linking", "abstract": "Cross-lingual entity linking (XEL) grounds named entities in a source\nlanguage to an English Knowledge Base (KB), such as Wikipedia. XEL is\nchallenging for most languages because of limited availability of requisite\nresources. However, much previous work on XEL has been on simulated settings\nthat actually use significant resources (e.g. source language Wikipedia,\nbilingual entity maps, multilingual embeddings) that are unavailable in truly\nlow-resource languages. In this work, we first examine the effect of these\nresource assumptions and quantify how much the availability of these resource\naffects overall quality of existing XEL systems. Next, we propose three\nimprovements to both entity candidate generation and disambiguation that make\nbetter use of the limited data we do have in resource-scarce scenarios. With\nexperiments on four extremely low-resource languages, we show that our model\nresults in gains of 6-23% in end-to-end linking accuracy.", "published": "2019-09-29 01:36:19", "link": "http://arxiv.org/abs/1909.13180v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Pilot Study for Chinese SQL Semantic Parsing", "abstract": "The task of semantic parsing is highly useful for dialogue and question\nanswering systems. Many datasets have been proposed to map natural language\ntext into SQL, among which the recent Spider dataset provides cross-domain\nsamples with multiple tables and complex queries. We build a Spider dataset for\nChinese, which is currently a low-resource language in this task area.\nInteresting research questions arise from the uniqueness of the language, which\nrequires word segmentation, and also from the fact that SQL keywords and\ncolumns of DB tables are typically written in English. We compare character-\nand word-based encoders for a semantic parser, and different embedding schemes.\nResults show that word-based semantic parser is subject to segmentation errors\nand cross-lingual word embeddings are useful for text-to-SQL.", "published": "2019-09-29 14:41:47", "link": "http://arxiv.org/abs/1909.13293v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controllable Data Synthesis Method for Grammatical Error Correction", "abstract": "Due to the lack of parallel data in current Grammatical Error Correction\n(GEC) task, models based on Sequence to Sequence framework cannot be adequately\ntrained to obtain higher performance. We propose two data synthesis methods\nwhich can control the error rate and the ratio of error types on synthetic\ndata. The first approach is to corrupt each word in the monolingual corpus with\na fixed probability, including replacement, insertion and deletion. Another\napproach is to train error generation models and further filtering the decoding\nresults of the models. The experiments on different synthetic data show that\nthe error rate is 40% and the ratio of error types is the same can improve the\nmodel performance better. Finally, we synthesize about 100 million data and\nachieve comparable performance as the state of the art, which uses twice as\nmuch data as we use.", "published": "2019-09-29 15:35:08", "link": "http://arxiv.org/abs/1909.13302v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language-Agnostic Syllabification with Neural Sequence Labeling", "abstract": "The identification of syllables within phonetic sequences is known as\nsyllabification. This task is thought to play an important role in natural\nlanguage understanding, speech production, and the development of speech\nrecognition systems. The concept of the syllable is cross-linguistic, though\nformal definitions are rarely agreed upon, even within a language. In response,\ndata-driven syllabification methods have been developed to learn from\nsyllabified examples. These methods often employ classical machine learning\nsequence labeling models. In recent years, recurrence-based neural networks\nhave been shown to perform increasingly well for sequence labeling tasks such\nas named entity recognition (NER), part of speech (POS) tagging, and chunking.\nWe present a novel approach to the syllabification problem which leverages\nmodern neural network techniques. Our network is constructed with long\nshort-term memory (LSTM) cells, a convolutional component, and a conditional\nrandom field (CRF) output layer. Existing syllabification approaches are rarely\nevaluated across multiple language families. To demonstrate cross-linguistic\ngeneralizability, we show that the network is competitive with state of the art\nsystems in syllabifying English, Dutch, Italian, French, Manipuri, and Basque\ndatasets.", "published": "2019-09-29 20:32:27", "link": "http://arxiv.org/abs/1909.13362v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Simple and Effective Model for Answering Multi-span Questions", "abstract": "Models for reading comprehension (RC) commonly restrict their output space to\nthe set of all single contiguous spans from the input, in order to alleviate\nthe learning problem and avoid the need for a model that generates text\nexplicitly. However, forcing an answer to be a single span can be restrictive,\nand some recent datasets also include multi-span questions, i.e., questions\nwhose answer is a set of non-contiguous spans in the text. Naturally, models\nthat return single spans cannot answer these questions. In this work, we\npropose a simple architecture for answering multi-span questions by casting the\ntask as a sequence tagging problem, namely, predicting for each input token\nwhether it should be part of the output or not. Our model substantially\nimproves performance on span extraction questions from DROP and Quoref by 9.9\nand 5.5 EM points respectively.", "published": "2019-09-29 21:49:36", "link": "http://arxiv.org/abs/1909.13375v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fake news detection using Deep Learning", "abstract": "The evolution of the information and communication technologies has\ndramatically increased the number of people with access to the Internet, which\nhas changed the way the information is consumed. As a consequence of the above,\nfake news have become one of the major concerns because its potential to\ndestabilize governments, which makes them a potential danger to modern society.\nAn example of this can be found in the US. electoral campaign, where the term\n\"fake news\" gained great notoriety due to the influence of the hoaxes in the\nfinal result of these. In this work the feasibility of applying deep learning\ntechniques to discriminate fake news on the Internet using only their text is\nstudied. In order to accomplish that, three different neural network\narchitectures are proposed, one of them based on BERT, a modern language model\ncreated by Google which achieves state-of-the-art results.", "published": "2019-09-29 17:45:48", "link": "http://arxiv.org/abs/1910.03496v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Towards Automatic Bot Detection in Twitter for Health-related Tasks", "abstract": "With the increasing use of social media data for health-related research, the\ncredibility of the information from this source has been questioned as the\nposts may originate from automated accounts or \"bots\". While automatic bot\ndetection approaches have been proposed, there are none that have been\nevaluated on users posting health-related information. In this paper, we extend\nan existing bot detection system and customize it for health-related research.\nUsing a dataset of Twitter users, we first show that the system, which was\ndesigned for political bot detection, underperforms when applied to\nhealth-related Twitter users. We then incorporate additional features and a\nstatistical machine learning classifier to significantly improve bot detection\nperformance. Our approach obtains F_1 scores of 0.7 for the \"bot\" class,\nrepresenting improvements of 0.339. Our approach is customizable and\ngeneralizable for bot detection in other health-related social media cohorts.", "published": "2019-09-29 01:58:15", "link": "http://arxiv.org/abs/1909.13184v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Recent Advances in End-to-End Spoken Language Understanding", "abstract": "This work investigates spoken language understanding (SLU) systems in the\nscenario when the semantic information is extracted directly from the speech\nsignal by means of a single end-to-end neural network model. Two SLU tasks are\nconsidered: named entity recognition (NER) and semantic slot filling (SF). For\nthese tasks, in order to improve the model performance, we explore various\ntechniques including speaker adaptation, a modification of the connectionist\ntemporal classification (CTC) training criterion, and sequential pretraining.", "published": "2019-09-29 18:01:00", "link": "http://arxiv.org/abs/1909.13332v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Unfolding the Structure of a Document using Deep Learning", "abstract": "Understanding and extracting of information from large documents, such as\nbusiness opportunities, academic articles, medical documents and technical\nreports, poses challenges not present in short documents. Such large documents\nmay be multi-themed, complex, noisy and cover diverse topics. We describe a\nframework that can analyze large documents and help people and computer systems\nlocate desired information in them. We aim to automatically identify and\nclassify different sections of documents and understand their purpose within\nthe document. A key contribution of our research is modeling and extracting the\nlogical and semantic structure of electronic documents using deep learning\ntechniques. We evaluate the effectiveness and robustness of our framework\nthrough extensive experiments on two collections: more than one million\nscholarly articles from arXiv and a collection of requests for proposal\ndocuments from government sources.", "published": "2019-09-29 21:33:46", "link": "http://arxiv.org/abs/1910.03678v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Gated Task Interaction Framework for Multi-task Sequence Tagging", "abstract": "Recent studies have shown that neural models can achieve high performance on\nseveral sequence labelling/tagging problems without the explicit use of\nlinguistic features such as part-of-speech (POS) tags. These models are trained\nonly using the character-level and the word embedding vectors as inputs. Others\nhave shown that linguistic features can improve the performance of neural\nmodels on tasks such as chunking and named entity recognition (NER). However,\nthe change in performance depends on the degree of semantic relatedness between\nthe linguistic features and the target task; in some instances, linguistic\nfeatures can have a negative impact on performance. This paper presents an\napproach to jointly learn these linguistic features along with the target\nsequence labelling tasks with a new multi-task learning (MTL) framework called\nGated Tasks Interaction (GTI) network for solving multiple sequence tagging\ntasks. The GTI network exploits the relations between the multiple tasks via\nneural gate modules. These gate modules control the flow of information between\nthe different tasks. Experiments on benchmark datasets for chunking and NER\nshow that our framework outperforms other competitive baselines trained with\nand without external training resources.", "published": "2019-09-29 02:56:24", "link": "http://arxiv.org/abs/1909.13193v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Lifelong Neural Topic Learning in Contextualized Autoregressive Topic\n  Models of Language via Informative Transfers", "abstract": "Topic models such as LDA, DocNADE, iDocNADEe have been popular in document\nanalysis. However, the traditional topic models have several limitations\nincluding: (1) Bag-of-words (BoW) assumption, where they ignore word ordering,\n(2) Data sparsity, where the application of topic models is challenging due to\nlimited word co-occurrences, leading to incoherent topics and (3) No Continuous\nLearning framework for topic learning in lifelong fashion, exploiting\nhistorical knowledge (or latent topics) and minimizing catastrophic forgetting.\nThis thesis focuses on addressing the above challenges within neural topic\nmodeling framework. We propose: (1) Contextualized topic model that combines a\ntopic and a language model and introduces linguistic structures (such as word\nordering, syntactic and semantic features, etc.) in topic modeling, (2) A novel\nlifelong learning mechanism into neural topic modeling framework to demonstrate\ncontinuous learning in sequential document collections and minimizing\ncatastrophic forgetting. Additionally, we perform a selective data augmentation\nto alleviate the need for complete historical corpora during data hallucination\nor replay.", "published": "2019-09-29 16:43:30", "link": "http://arxiv.org/abs/1909.13315v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Speaker Verification in Emotional Talking Environments based on\n  Third-Order Circular Suprasegmental Hidden Markov Model", "abstract": "Speaker verification accuracy in emotional talking environments is not high\nas it is in neutral ones. This work aims at accepting or rejecting the claimed\nspeaker using his/her voice in emotional environments based on the Third-Order\nCircular Suprasegmental Hidden Markov Model (CSPHMM3) as a classifier. An\nEmirati-accented (Arabic) speech database with Mel-Frequency Cepstral\nCoefficients as the extracted features has been used to evaluate our work. Our\nresults demonstrate that speaker verification accuracy based on CSPHMM3 is\ngreater than that based on the state-of-the-art classifiers and models such as\nGaussian Mixture Model (GMM), Support Vector Machine (SVM), and Vector\nQuantization (VQ).", "published": "2019-09-29 09:43:38", "link": "http://arxiv.org/abs/1909.13244v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Identifying Mood Episodes Using Dialogue Features from Clinical\n  Interviews", "abstract": "Bipolar disorder, a severe chronic mental illness characterized by\npathological mood swings from depression to mania, requires ongoing symptom\nseverity tracking to both guide and measure treatments that are critical for\nmaintaining long-term health. Mental health professionals assess symptom\nseverity through semi-structured clinical interviews. During these interviews,\nthey observe their patients' spoken behaviors, including both what the patients\nsay and how they say it. In this work, we move beyond acoustic and lexical\ninformation, investigating how higher-level interactive patterns also change\nduring mood episodes. We then perform a secondary analysis, asking if these\ninteractive patterns, measured through dialogue features, can be used in\nconjunction with acoustic features to automatically recognize mood episodes.\nOur results show that it is beneficial to consider dialogue features when\nanalyzing and building automated systems for predicting and monitoring mood.", "published": "2019-09-29 01:11:53", "link": "http://arxiv.org/abs/1910.05115v2", "categories": ["eess.AS", "cs.SD", "q-bio.NC"], "primary_category": "eess.AS"}
{"title": "MG-VAE: Deep Chinese Folk Songs Generation with Specific Regional Style", "abstract": "Regional style in Chinese folk songs is a rich treasure that can be used for\nethnic music creation and folk culture research. In this paper, we propose\nMG-VAE, a music generative model based on VAE (Variational Auto-Encoder) that\nis capable of capturing specific music style and generating novel tunes for\nChinese folk songs (Min Ge) in a manipulatable way. Specifically, we\ndisentangle the latent space of VAE into four parts in an adversarial training\nway to control the information of pitch and rhythm sequence, as well as of\nmusic style and content. In detail, two classifiers are used to separate style\nand content latent space, and temporal supervision is utilized to disentangle\nthe pitch and rhythm sequence. The experimental results show that the\ndisentanglement is successful and our model is able to create novel folk songs\nwith controllable regional styles. To our best knowledge, this is the first\nstudy on applying deep generative model and adversarial training for Chinese\nmusic generation.", "published": "2019-09-29 14:01:36", "link": "http://arxiv.org/abs/1909.13287v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "FaSNet: Low-latency Adaptive Beamforming for Multi-microphone Audio\n  Processing", "abstract": "Beamforming has been extensively investigated for multi-channel audio\nprocessing tasks. Recently, learning-based beamforming methods, sometimes\ncalled \\textit{neural beamformers}, have achieved significant improvements in\nboth signal quality (e.g. signal-to-noise ratio (SNR)) and speech recognition\n(e.g. word error rate (WER)). Such systems are generally non-causal and require\na large context for robust estimation of inter-channel features, which is\nimpractical in applications requiring low-latency responses. In this paper, we\npropose filter-and-sum network (FaSNet), a time-domain, filter-based\nbeamforming approach suitable for low-latency scenarios. FaSNet has a two-stage\nsystem design that first learns frame-level time-domain adaptive beamforming\nfilters for a selected reference channel, and then calculate the filters for\nall remaining channels. The filtered outputs at all channels are summed to\ngenerate the final output. Experiments show that despite its small model size,\nFaSNet is able to outperform several traditional oracle beamformers with\nrespect to scale-invariant signal-to-noise ratio (SI-SNR) in reverberant speech\nenhancement and separation tasks. Moreover, when trained with a\nfrequency-domain objective function on the CHiME-3 dataset, FaSNet achieves\n14.3\\% relative word error rate reduction (RWERR) compared with the baseline\nmodel. These results show the efficacy of FaSNet particularly in reverberant\nand noisy signal conditions.", "published": "2019-09-29 22:26:06", "link": "http://arxiv.org/abs/1909.13387v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
