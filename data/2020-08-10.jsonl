{"title": "On Commonsense Cues in BERT for Solving Commonsense Tasks", "abstract": "BERT has been used for solving commonsense tasks such as CommonsenseQA. While\nprior research has found that BERT does contain commonsense information to some\nextent, there has been work showing that pre-trained models can rely on\nspurious associations (e.g., data bias) rather than key cues in solving\nsentiment classification and other problems. We quantitatively investigate the\npresence of structural commonsense cues in BERT when solving commonsense tasks,\nand the importance of such cues for the model prediction. Using two different\nmeasures, we find that BERT does use relevant knowledge for solving the task,\nand the presence of commonsense knowledge is positively correlated to the model\naccuracy.", "published": "2020-08-10 08:12:34", "link": "http://arxiv.org/abs/2008.03945v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Large-Scale Chinese Short-Text Conversation Dataset", "abstract": "The advancements of neural dialogue generation models show promising results\non modeling short-text conversations. However, training such models usually\nneeds a large-scale high-quality dialogue corpus, which is hard to access. In\nthis paper, we present a large-scale cleaned Chinese conversation dataset,\nLCCC, which contains a base version (6.8million dialogues) and a large version\n(12.0 million dialogues). The quality of our dataset is ensured by a rigorous\ndata cleaning pipeline, which is built based on a set of rules and a classifier\nthat is trained on manually annotated 110K dialogue pairs. We also release\npre-training dialogue models which are trained on LCCC-base and LCCC-large\nrespectively. The cleaned dataset and the pre-training models will facilitate\nthe research of short-text conversation modeling. All the models and datasets\nare available at https://github.com/thu-coai/CDial-GPT.", "published": "2020-08-10 08:12:49", "link": "http://arxiv.org/abs/2008.03946v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KR-BERT: A Small-Scale Korean-Specific Language Model", "abstract": "Since the appearance of BERT, recent works including XLNet and RoBERTa\nutilize sentence embedding models pre-trained by large corpora and a large\nnumber of parameters. Because such models have large hardware and a huge amount\nof data, they take a long time to pre-train. Therefore it is important to\nattempt to make smaller models that perform comparatively. In this paper, we\ntrained a Korean-specific model KR-BERT, utilizing a smaller vocabulary and\ndataset. Since Korean is one of the morphologically rich languages with poor\nresources using non-Latin alphabets, it is also important to capture\nlanguage-specific linguistic phenomena that the Multilingual BERT model missed.\nWe tested several tokenizers including our BidirectionalWordPiece Tokenizer and\nadjusted the minimal span of tokens for tokenization ranging from sub-character\nlevel to character-level to construct a better vocabulary for our model. With\nthose adjustments, our KR-BERT model performed comparably and even better than\nother existing pre-trained models using a corpus about 1/10 of the size.", "published": "2020-08-10 09:26:00", "link": "http://arxiv.org/abs/2008.03979v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Bootstrapped Model to Detect Abuse and Intent in White Supremacist\n  Corpora", "abstract": "Intelligence analysts face a difficult problem: distinguishing extremist\nrhetoric from potential extremist violence. Many are content to express abuse\nagainst some target group, but only a few indicate a willingness to engage in\nviolence. We address this problem by building a predictive model for intent,\nbootstrapping from a seed set of intent words, and language templates\nexpressing intent. We design both an n-gram and attention-based deep learner\nfor intent and use them as colearners to improve both the basis for prediction\nand the predictions themselves. They converge to stable predictions in a few\nrounds. We merge predictions of intent with predictions of abusive language to\ndetect posts that indicate a desire for violent action. We validate the\npredictions by comparing them to crowd-sourced labelling. The methodology can\nbe applied to other linguistic properties for which a plausible starting point\ncan be defined.", "published": "2020-08-10 17:17:21", "link": "http://arxiv.org/abs/2008.04276v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SemEval-2020 Task 9: Overview of Sentiment Analysis of Code-Mixed Tweets", "abstract": "In this paper, we present the results of the SemEval-2020 Task 9 on Sentiment\nAnalysis of Code-Mixed Tweets (SentiMix 2020). We also release and describe our\nHinglish (Hindi-English) and Spanglish (Spanish-English) corpora annotated with\nword-level language identification and sentence-level sentiment labels. These\ncorpora are comprised of 20K and 19K examples, respectively. The sentiment\nlabels are - Positive, Negative, and Neutral. SentiMix attracted 89 submissions\nin total including 61 teams that participated in the Hinglish contest and 28\nsubmitted systems to the Spanglish competition. The best performance achieved\nwas 75.0% F1 score for Hinglish and 80.6% F1 for Spanglish. We observe that\nBERT-like models and ensemble methods are the most common and successful\napproaches among the participants.", "published": "2020-08-10 17:17:52", "link": "http://arxiv.org/abs/2008.04277v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question Identification in Arabic Language Using Emotional Based\n  Features", "abstract": "With the growth of content on social media networks, enterprises and services\nproviders have become interested in identifying the questions of their\ncustomers. Tracking these questions become very challenging with the growth of\ntext that grows directly proportional to the increase of Arabic users thus\nmaking it very difficult to be tracked manually. By automatic identifying the\nquestions seeking answers on the social media networks and defining their\ncategory, we can automatically answer them by finding an existing answer or\neven routing them to those responsible for answering those questions in the\ncustomer service. This will result in saving the time and the effort and\nenhancing the customer feedback and improving the business. In this paper, we\nhave implemented a binary classifier to classify Arabic text to either question\nseeking answer or not. We have added emotional based features to the state of\nthe art features. Experimental evaluation has done and showed that these\nemotional features have improved the accuracy of the classifier.", "published": "2020-08-10 00:18:32", "link": "http://arxiv.org/abs/2008.03843v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge Distillation and Data Selection for Semi-Supervised Learning\n  in CTC Acoustic Models", "abstract": "Semi-supervised learning (SSL) is an active area of research which aims to\nutilize unlabelled data in order to improve the accuracy of speech recognition\nsystems. The current study proposes a methodology for integration of two key\nideas: 1) SSL using connectionist temporal classification (CTC) objective and\nteacher-student based learning 2) Designing effective data-selection mechanisms\nfor leveraging unlabelled data to boost performance of student models. Our aim\nis to establish the importance of good criteria in selecting samples from a\nlarge pool of unlabelled data based on attributes like confidence measure,\nspeaker and content variability. The question we try to answer is: Is it\npossible to design a data selection mechanism which reduces dependence on a\nlarge set of randomly selected unlabelled samples without compromising on Word\nError Rate (WER)? We perform empirical investigations of different data\nselection methods to answer this question and quantify the effect of different\nsampling strategies. On a semi-supervised ASR setting with 40000 hours of\ncarefully selected unlabelled data, our CTC-SSL approach gives 17% relative WER\nimprovement over a baseline CTC system trained with labelled data. It also\nachieves on-par performance with CTC-SSL system trained on order of magnitude\nlarger unlabeled data based on random sampling.", "published": "2020-08-10 07:00:08", "link": "http://arxiv.org/abs/2008.03923v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Describe What to Change: A Text-guided Unsupervised Image-to-Image\n  Translation Approach", "abstract": "Manipulating visual attributes of images through human-written text is a very\nchallenging task. On the one hand, models have to learn the manipulation\nwithout the ground truth of the desired output. On the other hand, models have\nto deal with the inherent ambiguity of natural language. Previous research\nusually requires either the user to describe all the characteristics of the\ndesired image or to use richly-annotated image captioning datasets. In this\nwork, we propose a novel unsupervised approach, based on image-to-image\ntranslation, that alters the attributes of a given image through a command-like\nsentence such as \"change the hair color to black\". Contrarily to\nstate-of-the-art approaches, our model does not require a human-annotated\ndataset nor a textual description of all the attributes of the desired image,\nbut only those that have to be modified. Our proposed model disentangles the\nimage content from the visual attributes, and it learns to modify the latter\nusing the textual description, before generating a new image from the content\nand the modified attribute representation. Because text might be inherently\nambiguous (blond hair may refer to different shadows of blond, e.g. golden,\nicy, sandy), our method generates multiple stochastic versions of the same\ntranslation. Experiments show that the proposed model achieves promising\nperformances on two large-scale public datasets: CelebA and CUB. We believe our\napproach will pave the way to new avenues of research combining textual and\nspeech commands with visual attributes.", "published": "2020-08-10 15:40:05", "link": "http://arxiv.org/abs/2008.04200v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "FireBERT: Hardening BERT-based classifiers against adversarial attack", "abstract": "We present FireBERT, a set of three proof-of-concept NLP classifiers hardened\nagainst TextFooler-style word-perturbation by producing diverse alternatives to\noriginal samples. In one approach, we co-tune BERT against the training data\nand synthetic adversarial samples. In a second approach, we generate the\nsynthetic samples at evaluation time through substitution of words and\nperturbation of embedding vectors. The diversified evaluation results are then\ncombined by voting. A third approach replaces evaluation-time word substitution\nwith perturbation of embedding vectors. We evaluate FireBERT for MNLI and IMDB\nMovie Review datasets, in the original and on adversarial examples generated by\nTextFooler. We also test whether TextFooler is less successful in creating new\nadversarial samples when manipulating FireBERT, compared to working on\nunhardened classifiers. We show that it is possible to improve the accuracy of\nBERT-based models in the face of adversarial attacks without significantly\nreducing the accuracy for regular benchmark samples. We present co-tuning with\na synthetic data generator as a highly effective method to protect against 95%\nof pre-manufactured adversarial samples while maintaining 98% of original\nbenchmark performance. We also demonstrate evaluation-time perturbation as a\npromising direction for further research, restoring accuracy up to 75% of\nbenchmark performance for pre-made adversarials, and up to 65% (from a baseline\nof 75% orig. / 12% attack) under active attack by TextFooler.", "published": "2020-08-10 15:43:28", "link": "http://arxiv.org/abs/2008.04203v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VAW-GAN for Singing Voice Conversion with Non-parallel Training Data", "abstract": "Singing voice conversion aims to convert singer's voice from source to target\nwithout changing singing content. Parallel training data is typically required\nfor the training of singing voice conversion system, that is however not\npractical in real-life applications. Recent encoder-decoder structures, such as\nvariational autoencoding Wasserstein generative adversarial network (VAW-GAN),\nprovide an effective way to learn a mapping through non-parallel training data.\nIn this paper, we propose a singing voice conversion framework that is based on\nVAW-GAN. We train an encoder to disentangle singer identity and singing prosody\n(F0 contour) from phonetic content. By conditioning on singer identity and F0,\nthe decoder generates output spectral features with unseen target singer\nidentity, and improves the F0 rendering. Experimental results show that the\nproposed framework achieves better performance than the baseline frameworks.", "published": "2020-08-10 09:44:10", "link": "http://arxiv.org/abs/2008.03992v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Navigating Human Language Models with Synthetic Agents", "abstract": "Modern natural language models such as the GPT-2/GPT-3 contain tremendous\namounts of information about human belief in a consistently testable form. If\nthese models could be shown to accurately reflect the underlying beliefs of the\nhuman beings that produced the data used to train these models, then such\nmodels become a powerful sociological tool in ways that are distinct from\ntraditional methods, such as interviews and surveys. In this study, We train a\nversion of the GPT-2 on a corpora of historical chess games, and then \"launch\"\nclusters of synthetic agents into the model, using text strings to create\ncontext and orientation. We compare the trajectories contained in the text\ngenerated by the agents/model and compare that to the known ground truth of the\nchess board, move legality, and historical patterns of play. We find that the\npercentages of moves by piece using the model are substantially similar from\nhuman patterns. We further find that the model creates an accurate latent\nrepresentation of the chessboard, and that it is possible to plot trajectories\nof legal moves across the board using this knowledge.", "published": "2020-08-10 14:39:53", "link": "http://arxiv.org/abs/2008.04162v7", "categories": ["cs.AI", "cs.CL", "cs.MA", "I.2; I.6; J.4"], "primary_category": "cs.AI"}
{"title": "Can We Spot the \"Fake News\" Before It Was Even Written?", "abstract": "Given the recent proliferation of disinformation online, there has been also\ngrowing research interest in automatically debunking rumors, false claims, and\n\"fake news.\" A number of fact-checking initiatives have been launched so far,\nboth manual and automatic, but the whole enterprise remains in a state of\ncrisis: by the time a claim is finally fact-checked, it could have reached\nmillions of users, and the harm caused could hardly be undone. An arguably more\npromising direction is to focus on fact-checking entire news outlets, which can\nbe done in advance. Then, we could fact-check the news before it was even\nwritten: by checking how trustworthy the outlets that published it is. We\ndescribe how we do this in the Tanbih news aggregator, which makes readers\naware of what they are reading. In particular, we develop media profiles that\nshow the general factuality of reporting, the degree of propagandistic content,\nhyper-partisanship, leading political ideology, general frame of reporting, and\nstance with respect to various claims and topics.", "published": "2020-08-10 19:21:06", "link": "http://arxiv.org/abs/2008.04374v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "DQI: A Guide to Benchmark Evaluation", "abstract": "A `state of the art' model A surpasses humans in a benchmark B, but fails on\nsimilar benchmarks C, D, and E. What does B have that the other benchmarks do\nnot? Recent research provides the answer: spurious bias. However, developing A\nto solve benchmarks B through E does not guarantee that it will solve future\nbenchmarks. To progress towards a model that `truly learns' an underlying task,\nwe need to quantify the differences between successive benchmarks, as opposed\nto existing binary and black-box approaches. We propose a novel approach to\nsolve this underexplored task of quantifying benchmark quality by debuting a\ndata quality metric: DQI.", "published": "2020-08-10 08:38:55", "link": "http://arxiv.org/abs/2008.03964v1", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "cs.CL"}
{"title": "Audio-visual Speaker Recognition with a Cross-modal Discriminative\n  Network", "abstract": "Audio-visual speaker recognition is one of the tasks in the recent 2019 NIST\nspeaker recognition evaluation (SRE). Studies in neuroscience and computer\nscience all point to the fact that vision and auditory neural signals interact\nin the cognitive process. This motivated us to study a cross-modal network,\nnamely voice-face discriminative network (VFNet) that establishes the general\nrelation between human voice and face. Experiments show that VFNet provides\nadditional speaker discriminative information. With VFNet, we achieve 16.54%\nequal error rate relative reduction over the score level fusion audio-visual\nbaseline on evaluation set of 2019 NIST SRE.", "published": "2020-08-10 04:16:20", "link": "http://arxiv.org/abs/2008.03894v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Deep Self-Supervised Hierarchical Clustering for Speaker Diarization", "abstract": "The state-of-the-art speaker diarization systems use agglomerative\nhierarchical clustering (AHC) which performs the clustering of previously\nlearned neural embeddings. While the clustering approach attempts to identify\nspeaker clusters, the AHC algorithm does not involve any further learning. In\nthis paper, we propose a novel algorithm for hierarchical clustering which\ncombines the speaker clustering along with a representation learning framework.\nThe proposed approach is based on principles of self-supervised learning where\nthe self-supervision is derived from the clustering algorithm. The\nrepresentation learning network is trained with a regularized triplet loss\nusing the clustering solution at the current step while the clustering\nalgorithm uses the deep embeddings from the representation learning step. By\ncombining the self-supervision based representation learning along with the\nclustering algorithm, we show that the proposed algorithm improves\nsignificantly 29% relative improvement) over the AHC algorithm with cosine\nsimilarity for a speaker diarization task on CALLHOME dataset. In addition, the\nproposed approach also improves over the state-of-the-art system with PLDA\naffinity matrix with 10% relative improvement in DER.", "published": "2020-08-10 08:32:35", "link": "http://arxiv.org/abs/2008.03960v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Perceptually-Motivated Approach for Low-Complexity, Real-Time\n  Enhancement of Fullband Speech", "abstract": "Over the past few years, speech enhancement methods based on deep learning\nhave greatly surpassed traditional methods based on spectral subtraction and\nspectral estimation. Many of these new techniques operate directly in the the\nshort-time Fourier transform (STFT) domain, resulting in a high computational\ncomplexity. In this work, we propose PercepNet, an efficient approach that\nrelies on human perception of speech by focusing on the spectral envelope and\non the periodicity of the speech. We demonstrate high-quality, real-time\nenhancement of fullband (48 kHz) speech with less than 5% of a CPU core.", "published": "2020-08-10 17:03:16", "link": "http://arxiv.org/abs/2008.04259v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "improving partition-block-based acoustic echo canceler in under-modeling\n  scenarios", "abstract": "Recently, a partitioned-block-based frequency-domain Kalman filter (PFKF) has\nbeen proposed for acoustic echo cancellation. Compared with the normal\nfrequency-domain Kalman filter, the PFKF utilizes the partitioned-block\nstructure, resulting in both fast convergence and low time-latency. We present\nan analysis of the steady-state behavior of the PFKF and found that it suffers\nfrom a biased steady-state solution when the filter is of deficient length.\nAccordingly, we propose an effective modification that has the benefit of the\nguaranteed optimal steady-state behavior. Simulations are conducted to validate\nthe improved performance of the proposed method.", "published": "2020-08-10 08:11:03", "link": "http://arxiv.org/abs/2008.03944v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Subword Regularization: An Analysis of Scalability and Generalization\n  for End-to-End Automatic Speech Recognition", "abstract": "Subwords are the most widely used output units in end-to-end speech\nrecognition. They combine the best of two worlds by modeling the majority of\nfrequent words directly and at the same time allow open vocabulary speech\nrecognition by backing off to shorter units or characters to construct words\nunseen during training. However, mapping text to subwords is ambiguous and\noften multiple segmentation variants are possible. Yet, many systems are\ntrained using only the most likely segmentation. Recent research suggests that\nsampling subword segmentations during training acts as a regularizer for neural\nmachine translation and speech recognition models, leading to performance\nimprovements. In this work, we conduct a principled investigation on the\nregularizing effect of the subword segmentation sampling method for a streaming\nend-to-end speech recognition task. In particular, we evaluate the subword\nregularization contribution depending on the size of the training dataset. Our\nresults suggest that subword regularization provides a consistent improvement\nof (2-8%) relative word-error-rate reduction, even in a large-scale setting\nwith datasets up to a size of 20k hours. Further, we analyze the effect of\nsubword regularization on recognition of unseen words and its implications on\nbeam diversity.", "published": "2020-08-10 11:42:43", "link": "http://arxiv.org/abs/2008.04034v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Data Efficient Voice Cloning from Noisy Samples with Domain Adversarial\n  Training", "abstract": "Data efficient voice cloning aims at synthesizing target speaker's voice with\nonly a few enrollment samples at hand. To this end, speaker adaptation and\nspeaker encoding are two typical methods based on base model trained from\nmultiple speakers. The former uses a small set of target speaker data to\ntransfer the multi-speaker model to target speaker's voice through direct model\nupdate, while in the latter, only a few seconds of target speaker's audio\ndirectly goes through an extra speaker encoding model along with the\nmulti-speaker model to synthesize target speaker's voice without model update.\nNevertheless, the two methods need clean target speaker data. However, the\nsamples provided by user may inevitably contain acoustic noise in real\napplications. It's still challenging to generating target voice with noisy\ndata. In this paper, we study the data efficient voice cloning problem from\nnoisy samples under the sequence-to-sequence based TTS paradigm. Specifically,\nwe introduce domain adversarial training (DAT) to speaker adaptation and\nspeaker encoding, which aims to disentangle noise from speech-noise mixture.\nExperiments show that for both speaker adaptation and encoding, the proposed\napproaches can consistently synthesize clean speech from noisy speaker samples,\napparently outperforming the method adopting state-of-the-art speech\nenhancement module.", "published": "2020-08-10 17:08:45", "link": "http://arxiv.org/abs/2008.04265v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-Supervised Learning of Audio-Visual Objects from Video", "abstract": "Our objective is to transform a video into a set of discrete audio-visual\nobjects using self-supervised learning. To this end, we introduce a model that\nuses attention to localize and group sound sources, and optical flow to\naggregate information over time. We demonstrate the effectiveness of the\naudio-visual object embeddings that our model learns by using them for four\ndownstream speech-oriented tasks: (a) multi-speaker sound source separation,\n(b) localizing and tracking speakers, (c) correcting misaligned audio-visual\ndata, and (d) active speaker detection. Using our representation, these tasks\ncan be solved entirely by training on unlabeled video, without the aid of\nobject detectors. We also demonstrate the generality of our method by applying\nit to non-human speakers, including cartoons and puppets.Our model\nsignificantly outperforms other self-supervised approaches, and obtains\nperformance competitive with methods that use supervised face detection.", "published": "2020-08-10 16:18:01", "link": "http://arxiv.org/abs/2008.04237v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "TinySpeech: Attention Condensers for Deep Speech Recognition Neural\n  Networks on Edge Devices", "abstract": "Advances in deep learning have led to state-of-the-art performance across a\nmultitude of speech recognition tasks. Nevertheless, the widespread deployment\nof deep neural networks for on-device speech recognition remains a challenge,\nparticularly in edge scenarios where the memory and computing resources are\nhighly constrained (e.g., low-power embedded devices) or where the memory and\ncomputing budget dedicated to speech recognition is low (e.g., mobile devices\nperforming numerous tasks besides speech recognition). In this study, we\nintroduce the concept of attention condensers for building low-footprint,\nhighly-efficient deep neural networks for on-device speech recognition on the\nedge. An attention condenser is a self-attention mechanism that learns and\nproduces a condensed embedding characterizing joint local and cross-channel\nactivation relationships, and performs selective attention accordingly. To\nillustrate its efficacy, we introduce TinySpeech, low-precision deep neural\nnetworks comprising largely of attention condensers tailored for on-device\nspeech recognition using a machine-driven design exploration strategy, with one\ntailored specifically with microcontroller operation constraints. Experimental\nresults on the Google Speech Commands benchmark dataset for limited-vocabulary\nspeech recognition showed that TinySpeech networks achieved significantly lower\narchitectural complexity (as much as $507\\times$ fewer parameters), lower\ncomputational complexity (as much as $48\\times$ fewer multiply-add operations),\nand lower storage requirements (as much as $2028\\times$ lower weight memory\nrequirements) when compared to previous work. These results not only\ndemonstrate the efficacy of attention condensers for building highly efficient\nnetworks for on-device speech recognition, but also illuminate its potential\nfor accelerating deep learning on the edge and empowering TinyML applications.", "published": "2020-08-10 16:34:52", "link": "http://arxiv.org/abs/2008.04245v6", "categories": ["eess.AS", "cs.LG", "cs.NE"], "primary_category": "eess.AS"}
