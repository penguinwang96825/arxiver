{"title": "Enabling Medical Translation for Low-Resource Languages", "abstract": "We present research towards bridging the language gap between migrant workers\nin Qatar and medical staff. In particular, we present the first steps towards\nthe development of a real-world Hindi-English machine translation system for\ndoctor-patient communication. As this is a low-resource language pair,\nespecially for speech and for the medical domain, our initial focus has been on\ngathering suitable training data from various sources. We applied a variety of\nmethods ranging from fully automatic extraction from the Web to manual\nannotation of test data. Moreover, we developed a method for automatically\naugmenting the training data with synthetically generated variants, which\nyielded a very sizable improvement of more than 3 BLEU points absolute.", "published": "2016-10-09 06:42:02", "link": "http://arxiv.org/abs/1610.02633v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Interpreting Neural Networks to Improve Politeness Comprehension", "abstract": "We present an interpretable neural network approach to predicting and\nunderstanding politeness in natural language requests. Our models are based on\nsimple convolutional neural networks directly on raw text, avoiding any manual\nidentification of complex sentiment or syntactic features, while performing\nbetter than such feature-based models from previous work. More importantly, we\nuse the challenging task of politeness prediction as a testbed to next present\na much-needed understanding of what these successful networks are actually\nlearning. For this, we present several network visualizations based on\nactivation clusters, first derivative saliency, and embedding space\ntransformations, helping us automatically identify several subtle linguistics\nmarkers of politeness theories. Further, this analysis reveals multiple novel,\nhigh-scoring politeness strategies which, when added back as new features,\nreduce the accuracy gap between the original featurized system and the neural\nmodel, thus providing a clear quantitative interpretation of the success of\nthese neural networks.", "published": "2016-10-09 14:42:58", "link": "http://arxiv.org/abs/1610.02683v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Emergence of linguistic laws in human voice", "abstract": "Linguistic laws constitute one of the quantitative cornerstones of modern\ncognitive sciences and have been routinely investigated in written corpora, or\nin the equivalent transcription of oral corpora. This means that inferences of\nstatistical patterns of language in acoustics are biased by the arbitrary,\nlanguage-dependent segmentation of the signal, and virtually precludes the\npossibility of making comparative studies between human voice and other animal\ncommunication systems. Here we bridge this gap by proposing a method that\nallows to measure such patterns in acoustic signals of arbitrary origin,\nwithout needs to have access to the language corpus underneath. The method has\nbeen applied to six different human languages, recovering successfully some\nwell-known laws of human communication at timescales even below the phoneme and\nfinding yet another link between complexity and criticality in a biological\nsystem. These methods further pave the way for new comparative studies in\nanimal communication or the analysis of signals of unknown code.", "published": "2016-10-09 22:41:19", "link": "http://arxiv.org/abs/1610.02736v1", "categories": ["physics.soc-ph", "cs.CL"], "primary_category": "physics.soc-ph"}
{"title": "Open-Ended Visual Question-Answering", "abstract": "This thesis report studies methods to solve Visual Question-Answering (VQA)\ntasks with a Deep Learning framework. As a preliminary step, we explore Long\nShort-Term Memory (LSTM) networks used in Natural Language Processing (NLP) to\ntackle Question-Answering (text based). We then modify the previous model to\naccept an image as an input in addition to the question. For this purpose, we\nexplore the VGG-16 and K-CNN convolutional neural networks to extract visual\nfeatures from the image. These are merged with the word embedding or with a\nsentence embedding of the question to predict the answer. This work was\nsuccessfully submitted to the Visual Question Answering Challenge 2016, where\nit achieved a 53,62% of accuracy in the test dataset. The developed software\nhas followed the best programming practices and Python code style, providing a\nconsistent baseline in Keras for different configurations.", "published": "2016-10-09 16:38:31", "link": "http://arxiv.org/abs/1610.02692v1", "categories": ["cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.CL"}
