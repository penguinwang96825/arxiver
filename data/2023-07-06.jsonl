{"title": "Text Alignment Is An Efficient Unified Model for Massive NLP Tasks", "abstract": "Large language models (LLMs), typically designed as a function of next-word\nprediction, have excelled across extensive NLP tasks. Despite the generality,\nnext-word prediction is often not an efficient formulation for many of the\ntasks, demanding an extreme scale of model parameters (10s or 100s of billions)\nand sometimes yielding suboptimal performance. In practice, it is often\ndesirable to build more efficient models -- despite being less versatile, they\nstill apply to a substantial subset of problems, delivering on par or even\nsuperior performance with much smaller model sizes. In this paper, we propose\ntext alignment as an efficient unified model for a wide range of crucial tasks\ninvolving text entailment, similarity, question answering (and answerability),\nfactual consistency, and so forth. Given a pair of texts, the model measures\nthe degree of alignment between their information. We instantiate an alignment\nmodel (Align) through lightweight finetuning of RoBERTa (355M parameters) using\n5.9M examples from 28 datasets. Despite its compact size, extensive experiments\nshow the model's efficiency and strong performance: (1) On over 20 datasets of\naforementioned diverse tasks, the model matches or surpasses FLAN-T5 models\nthat have around 2x or 10x more parameters; the single unified model also\noutperforms task-specific models finetuned on individual datasets; (2) When\napplied to evaluate factual consistency of language generation on 23 datasets,\nour model improves over various baselines, including the much larger GPT-3.5\n(ChatGPT) and sometimes even GPT-4; (3) The lightweight model can also serve as\nan add-on component for LLMs such as GPT-3.5 in question answering tasks,\nimproving the average exact match (EM) score by 17.94 and F1 score by 15.05\nthrough identifying unanswerable questions.", "published": "2023-07-06 02:28:31", "link": "http://arxiv.org/abs/2307.02729v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Linguistic Style Matching in Online Communities: The Role of\n  Social Context and Conversation Dynamics", "abstract": "Linguistic style matching (LSM) in conversations can be reflective of several\naspects of social influence such as power or persuasion. However, how LSM\nrelates to the outcomes of online communication on platforms such as Reddit is\nan unknown question. In this study, we analyze a large corpus of two-party\nconversation threads in Reddit where we identify all occurrences of LSM using\ntwo types of style: the use of function words and formality. Using this\nframework, we examine how levels of LSM differ in conversations depending on\nseveral social factors within Reddit: post and subreddit features, conversation\ndepth, user tenure, and the controversiality of a comment. Finally, we measure\nthe change of LSM following loss of status after community banning. Our\nfindings reveal the interplay of LSM in Reddit conversations with several\ncommunity metrics, suggesting the importance of understanding conversation\nengagement when understanding community dynamics.", "published": "2023-07-06 03:43:45", "link": "http://arxiv.org/abs/2307.02758v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Models to Generate, Recognize, and Reframe Unhelpful Thoughts", "abstract": "Many cognitive approaches to well-being, such as recognizing and reframing\nunhelpful thoughts, have received considerable empirical support over the past\ndecades, yet still lack truly widespread adoption in self-help format. A\nbarrier to that adoption is a lack of adequately specific and diverse dedicated\npractice material. This work examines whether current language models can be\nleveraged to both produce a virtually unlimited quantity of practice material\nillustrating standard unhelpful thought patterns matching specific given\ncontexts, and generate suitable positive reframing proposals. We propose\nPATTERNREFRAME, a novel dataset of about 10k examples of thoughts containing\nunhelpful thought patterns conditioned on a given persona, accompanied by about\n27k positive reframes. By using this dataset to train and/or evaluate current\nmodels, we show that existing models can already be powerful tools to help\ngenerate an abundance of tailored practice material and hypotheses, with no or\nminimal additional model training required.", "published": "2023-07-06 04:40:52", "link": "http://arxiv.org/abs/2307.02768v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generative Zero-Shot Prompt Learning for Cross-Domain Slot Filling with\n  Inverse Prompting", "abstract": "Zero-shot cross-domain slot filling aims to transfer knowledge from the\nlabeled source domain to the unlabeled target domain. Existing models either\nencode slot descriptions and examples or design handcrafted question templates\nusing heuristic rules, suffering from poor generalization capability or\nrobustness. In this paper, we propose a generative zero-shot prompt learning\nframework for cross-domain slot filling, both improving generalization and\nrobustness than previous work. Besides, we introduce a novel inverse prompting\nstrategy to distinguish different slot types to avoid the multiple prediction\nproblem, and an efficient prompt-tuning strategy to boost higher performance by\nonly training fewer prompt parameters. Experiments and analysis demonstrate the\neffectiveness of our proposed framework, especially huge improvements (+13.44%\nF1) on the unseen slots.", "published": "2023-07-06 07:53:46", "link": "http://arxiv.org/abs/2307.02830v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NatLogAttack: A Framework for Attacking Natural Language Inference\n  Models with Natural Logic", "abstract": "Reasoning has been a central topic in artificial intelligence from the\nbeginning. The recent progress made on distributed representation and neural\nnetworks continues to improve the state-of-the-art performance of natural\nlanguage inference. However, it remains an open question whether the models\nperform real reasoning to reach their conclusions or rely on spurious\ncorrelations. Adversarial attacks have proven to be an important tool to help\nevaluate the Achilles' heel of the victim models. In this study, we explore the\nfundamental problem of developing attack models based on logic formalism. We\npropose NatLogAttack to perform systematic attacks centring around natural\nlogic, a classical logic formalism that is traceable back to Aristotle's\nsyllogism and has been closely developed for natural language inference. The\nproposed framework renders both label-preserving and label-flipping attacks. We\nshow that compared to the existing attack models, NatLogAttack generates better\nadversarial examples with fewer visits to the victim models. The victim models\nare found to be more vulnerable under the label-flipping setting. NatLogAttack\nprovides a tool to probe the existing and future NLI models' capacity from a\nkey viewpoint and we hope more logic-based attacks will be further explored for\nunderstanding the desired property of reasoning.", "published": "2023-07-06 08:32:14", "link": "http://arxiv.org/abs/2307.02849v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ValiText -- a unified validation framework for computational text-based\n  measures of social constructs", "abstract": "Guidance on how to validate computational text-based measures of social\nconstructs is fragmented. While researchers generally acknowledge the\nimportance of validating text-based measures, they often lack a shared\nvocabulary and a unified framework to do so. This paper introduces ValiText, a\nnew validation framework designed to assist scholars in validly measuring\nsocial constructs in textual data. The framework is built on a conceptual\nfoundation of validity in the social sciences, strengthened by an empirical\nreview of validation practices in the social sciences and consultations with\nexperts. Ultimately, ValiText prescribes researchers to demonstrate three types\nof validation evidence: substantive evidence (outlining the theoretical\nunderpinning of the measure), structural evidence (examining the properties of\nthe text model and its output) and external evidence (testing for how the\nmeasure relates to independent information). The framework is further\nsupplemented by a checklist of validation steps, offering practical guidance in\nthe form of documentation sheets that guide researchers in the validation\nprocess.", "published": "2023-07-06 09:03:10", "link": "http://arxiv.org/abs/2307.02863v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Agentivit\u00e0 e telicit\u00e0 in GilBERTo: implicazioni cognitive", "abstract": "The goal of this study is to investigate whether a Transformer-based neural\nlanguage model infers lexical semantics and use this information for the\ncompletion of morphosyntactic patterns. The semantic properties considered are\ntelicity (also combined with definiteness) and agentivity. Both act at the\ninterface between semantics and morphosyntax: they are semantically determined\nand syntactically encoded. The tasks were submitted to both the computational\nmodel and a group of Italian native speakers. The comparison between the two\ngroups of data allows us to investigate to what extent neural language models\ncapture significant aspects of human semantic competence.", "published": "2023-07-06 10:52:22", "link": "http://arxiv.org/abs/2307.02910v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Style Over Substance: Evaluation Biases for Large Language Models", "abstract": "As large language models (LLMs) continue to advance, accurately and\ncomprehensively evaluating their performance becomes increasingly challenging.\nRanking the relative performance of LLMs based on Elo ratings, according to\nhuman judgment, is gaining more popularity. However, the extent to which humans\nand LLMs are capable evaluators remains uncertain. This study investigates the\nbehavior of crowd-sourced and expert annotators, as well as LLMs, when\ncomparing outputs from different models. To achieve this, we curate a dataset\nof intentionally flawed machine-generated answers. Our findings reveal a\nconcerning bias in the evaluation process, as answers with factual errors are\nrated more favorably than answers that are too short or contained grammatical\nerrors. To address this issue, we propose independently evaluating\nmachine-generated text across multiple dimensions, rather than merging all the\nevaluation aspects into a single score. We instantiate this idea with the Elo\nrating system, resulting in the Multi-Elo Rating System (MERS). Empirical\nresults from our study reveal that this proposed approach significantly\nenhances the quality of LLM-based evaluations, particularly in terms of factual\naccuracy. However, there is no significant improvement in crowd-sourced-based\nevaluations, indicating the need for further investigation.", "published": "2023-07-06 14:42:01", "link": "http://arxiv.org/abs/2307.03025v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KoRC: Knowledge oriented Reading Comprehension Benchmark for Deep Text\n  Understanding", "abstract": "Deep text understanding, which requires the connections between a given\ndocument and prior knowledge beyond its text, has been highlighted by many\nbenchmarks in recent years. However, these benchmarks have encountered two\nmajor limitations. On the one hand, most of them require human annotation of\nknowledge, which leads to limited knowledge coverage. On the other hand, they\nusually use choices or spans in the texts as the answers, which results in\nnarrow answer space. To overcome these limitations, we build a new challenging\nbenchmark named KoRc in this paper. Compared with previous benchmarks, KoRC has\ntwo advantages, i.e., broad knowledge coverage and flexible answer format.\nSpecifically, we utilize massive knowledge bases to guide annotators or large\nlanguage models (LLMs) to construct knowledgable questions. Moreover, we use\nlabels in knowledge bases rather than spans or choices as the final answers. We\ntest state-of-the-art models on KoRC and the experimental results show that the\nstrongest baseline only achieves 68.3% and 30.0% F1 measure in the\nin-distribution and out-of-distribution test set, respectively. These results\nindicate that deep text understanding is still an unsolved challenge. The\nbenchmark dataset, leaderboard, and baseline methods are released in\nhttps://github.com/THU-KEG/KoRC.", "published": "2023-07-06 16:35:25", "link": "http://arxiv.org/abs/2307.03115v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting Multi-valued Relations from Language Models", "abstract": "The widespread usage of latent language representations via pre-trained\nlanguage models (LMs) suggests that they are a promising source of structured\nknowledge. However, existing methods focus only on a single object per\nsubject-relation pair, even though often multiple objects are correct. To\novercome this limitation, we analyze these representations for their potential\nto yield materialized multi-object relational knowledge. We formulate the\nproblem as a rank-then-select task. For ranking candidate objects, we evaluate\nexisting prompting techniques and propose new ones incorporating domain\nknowledge. Among the selection methods, we find that choosing objects with a\nlikelihood above a learned relation-specific threshold gives a 49.5% F1 score.\nOur results highlight the difficulty of employing LMs for the multi-valued\nslot-filling task and pave the way for further research on extracting\nrelational knowledge from latent language representations.", "published": "2023-07-06 16:48:32", "link": "http://arxiv.org/abs/2307.03122v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BLEURT Has Universal Translations: An Analysis of Automatic Metrics by\n  Minimum Risk Training", "abstract": "Automatic metrics play a crucial role in machine translation. Despite the\nwidespread use of n-gram-based metrics, there has been a recent surge in the\ndevelopment of pre-trained model-based metrics that focus on measuring sentence\nsemantics. However, these neural metrics, while achieving higher correlations\nwith human evaluations, are often considered to be black boxes with potential\nbiases that are difficult to detect. In this study, we systematically analyze\nand compare various mainstream and cutting-edge automatic metrics from the\nperspective of their guidance for training machine translation systems. Through\nMinimum Risk Training (MRT), we find that certain metrics exhibit robustness\ndefects, such as the presence of universal adversarial translations in BLEURT\nand BARTScore. In-depth analysis suggests two main causes of these robustness\ndeficits: distribution biases in the training datasets, and the tendency of the\nmetric paradigm. By incorporating token-level constraints, we enhance the\nrobustness of evaluation metrics, which in turn leads to an improvement in the\nperformance of machine translation systems. Codes are available at\n\\url{https://github.com/powerpuffpomelo/fairseq_mrt}.", "published": "2023-07-06 16:59:30", "link": "http://arxiv.org/abs/2307.03131v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lost in the Middle: How Language Models Use Long Contexts", "abstract": "While recent language models have the ability to take long contexts as input,\nrelatively little is known about how well they use longer context. We analyze\nthe performance of language models on two tasks that require identifying\nrelevant information in their input contexts: multi-document question answering\nand key-value retrieval. We find that performance can degrade significantly\nwhen changing the position of relevant information, indicating that current\nlanguage models do not robustly make use of information in long input contexts.\nIn particular, we observe that performance is often highest when relevant\ninformation occurs at the beginning or end of the input context, and\nsignificantly degrades when models must access relevant information in the\nmiddle of long contexts, even for explicitly long-context models. Our analysis\nprovides a better understanding of how language models use their input context\nand provides new evaluation protocols for future long-context language models.", "published": "2023-07-06 17:54:11", "link": "http://arxiv.org/abs/2307.03172v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PREADD: Prefix-Adaptive Decoding for Controlled Text Generation", "abstract": "We propose Prefix-Adaptive Decoding (PREADD), a flexible method for\ncontrolled text generation. Unlike existing methods that use auxiliary expert\nmodels to control for attributes, PREADD does not require an external model,\ninstead relying on linearly combining output logits from multiple prompts.\nSpecifically, PREADD contrasts the output logits generated using a raw prompt\nagainst those generated using a prefix-prepended prompt, enabling both positive\nand negative control with respect to any attribute encapsulated by the prefix.\nWe evaluate PREADD on three tasks -- toxic output mitigation, gender bias\nreduction, and sentiment control -- and find that PREADD outperforms not only\nprompting baselines, but also an auxiliary-expert control method, by 12% or\nmore in relative gain on our main metrics for each task.", "published": "2023-07-06 16:56:46", "link": "http://arxiv.org/abs/2307.03214v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Covering Uncommon Ground: Gap-Focused Question Generation for Answer\n  Assessment", "abstract": "Human communication often involves information gaps between the\ninterlocutors. For example, in an educational dialogue, a student often\nprovides an answer that is incomplete, and there is a gap between this answer\nand the perfect one expected by the teacher. Successful dialogue then hinges on\nthe teacher asking about this gap in an effective manner, thus creating a rich\nand interactive educational experience. We focus on the problem of generating\nsuch gap-focused questions (GFQs) automatically. We define the task, highlight\nkey desired aspects of a good GFQ, and propose a model that satisfies these.\nFinally, we provide an evaluation by human annotators of our generated\nquestions compared against human generated ones, demonstrating competitive\nperformance.", "published": "2023-07-06 22:21:42", "link": "http://arxiv.org/abs/2307.03319v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BiPhone: Modeling Inter Language Phonetic Influences in Text", "abstract": "A large number of people are forced to use the Web in a language they have\nlow literacy in due to technology asymmetries. Written text in the second\nlanguage (L2) from such users often contains a large number of errors that are\ninfluenced by their native language (L1). We propose a method to mine phoneme\nconfusions (sounds in L2 that an L1 speaker is likely to conflate) for pairs of\nL1 and L2. These confusions are then plugged into a generative model (Bi-Phone)\nfor synthetically producing corrupted L2 text. Through human evaluations, we\nshow that Bi-Phone generates plausible corruptions that differ across L1s and\nalso have widespread coverage on the Web. We also corrupt the popular language\nunderstanding benchmark SuperGLUE with our technique (FunGLUE for Phonetically\nNoised GLUE) and show that SoTA language understating models perform poorly. We\nalso introduce a new phoneme prediction pre-training task which helps byte\nmodels to recover performance close to SuperGLUE. Finally, we also release the\nFunGLUE benchmark to promote further research in phonetically robust language\nmodels. To the best of our knowledge, FunGLUE is the first benchmark to\nintroduce L1-L2 interactions in text.", "published": "2023-07-06 22:31:55", "link": "http://arxiv.org/abs/2307.03322v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Strahler Number of Natural Language Sentences in Comparison with Random\n  Trees", "abstract": "The Strahler number was originally proposed to characterize the complexity of\nriver bifurcation and has found various applications. This article proposes\ncomputation of the Strahler number's upper and lower limits for natural\nlanguage sentence tree structures. Through empirical measurements across\ngrammatically annotated data, the Strahler number of natural language sentences\nis shown to be almost 3 or 4, similarly to the case of river bifurcation as\nreported by Strahler (1957). From the theory behind the number, we show that it\nis one kind of lower limit on the amount of memory required to process\nsentences. We consider the Strahler number to provide reasoning that explains\nreports showing that the number of required memory areas to process sentences\nis 3 to 4 for parsing (Schuler et al., 2010), and reports indicating a\npsychological \"magical number\" of 3 to 5 (Cowan, 2001). An analytical and\nempirical analysis shows that the Strahler number is not constant but grows\nlogarithmically; therefore, the Strahler number of sentences derives from the\nrange of sentence lengths. Furthermore, the Strahler number is not different\nfor random trees, which could suggest that its origin is not specific to\nnatural language.", "published": "2023-07-06 00:06:14", "link": "http://arxiv.org/abs/2307.02697v4", "categories": ["cs.CL", "physics.data-an"], "primary_category": "cs.CL"}
{"title": "CFSum: A Coarse-to-Fine Contribution Network for Multimodal\n  Summarization", "abstract": "Multimodal summarization usually suffers from the problem that the\ncontribution of the visual modality is unclear. Existing multimodal\nsummarization approaches focus on designing the fusion methods of different\nmodalities, while ignoring the adaptive conditions under which visual\nmodalities are useful. Therefore, we propose a novel Coarse-to-Fine\ncontribution network for multimodal Summarization (CFSum) to consider different\ncontributions of images for summarization. First, to eliminate the interference\nof useless images, we propose a pre-filter module to abandon useless images.\nSecond, to make accurate use of useful images, we propose two levels of visual\ncomplement modules, word level and phrase level. Specifically, image\ncontributions are calculated and are adopted to guide the attention of both\ntextual and visual modalities. Experimental results have shown that CFSum\nsignificantly outperforms multiple strong baselines on the standard benchmark.\nFurthermore, the analysis verifies that useful images can even help generate\nnon-visual words which are implicitly represented in the image.", "published": "2023-07-06 01:46:00", "link": "http://arxiv.org/abs/2307.02716v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Dense Retrieval Adaptation using Target Domain Description", "abstract": "In information retrieval (IR), domain adaptation is the process of adapting a\nretrieval model to a new domain whose data distribution is different from the\nsource domain. Existing methods in this area focus on unsupervised domain\nadaptation where they have access to the target document collection or\nsupervised (often few-shot) domain adaptation where they additionally have\naccess to (limited) labeled data in the target domain. There also exists\nresearch on improving zero-shot performance of retrieval models with no\nadaptation. This paper introduces a new category of domain adaptation in IR\nthat is as-yet unexplored. Here, similar to the zero-shot setting, we assume\nthe retrieval model does not have access to the target document collection. In\ncontrast, it does have access to a brief textual description that explains the\ntarget domain. We define a taxonomy of domain attributes in retrieval tasks to\nunderstand different properties of a source domain that can be adapted to a\ntarget domain. We introduce a novel automatic data construction pipeline that\nproduces a synthetic document collection, query set, and pseudo relevance\nlabels, given a textual domain description. Extensive experiments on five\ndiverse target domains show that adapting dense retrieval models using the\nconstructed synthetic data leads to effective retrieval performance on the\ntarget domain.", "published": "2023-07-06 02:59:47", "link": "http://arxiv.org/abs/2307.02740v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "PRD: Peer Rank and Discussion Improve Large Language Model based\n  Evaluations", "abstract": "Nowadays, the quality of responses generated by different modern large\nlanguage models (LLMs) is hard to evaluate and compare automatically. Recent\nstudies suggest and predominantly use LLMs for reference-free evaluation of\nopen-ended question answering. More specifically, they use the recognized\n\"strongest\" LLM as the evaluator, which conducts pairwise comparisons of\ncandidate models' answers and provides a ranking score. However, this intuitive\nmethod has multiple problems, such as bringing in self-enhancement (favoring\nits own answers) and positional bias. We draw insights and lessons from the\neducational domain (Cho & MacArthur, 2011; Walsh, 2014) to improve LLM-based\nevaluations. Specifically, we propose (1) the peer rank (PR) algorithm that\ntakes into account each peer LLM's pairwise preferences of all answer pairs,\nand outputs a final ranking of models; and (2) peer discussion (PD), where we\nprompt two LLMs to discuss and try to reach a mutual agreement on the\npreferences of two answers. We conduct experiments on two benchmark datasets.\nWe find that our approaches achieve higher accuracy and align better with human\njudgments. Interestingly, PR can induce a relatively accurate self-ranking of\nmodels under the anonymous setting, where each model's name is unrevealed. Our\nwork provides space to explore evaluating models that are hard to compare for\nhumans.", "published": "2023-07-06 04:05:44", "link": "http://arxiv.org/abs/2307.02762v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Your spouse needs professional help: Determining the Contextual\n  Appropriateness of Messages through Modeling Social Relationships", "abstract": "Understanding interpersonal communication requires, in part, understanding\nthe social context and norms in which a message is said. However, current\nmethods for identifying offensive content in such communication largely operate\nindependent of context, with only a few approaches considering community norms\nor prior conversation as context. Here, we introduce a new approach to\nidentifying inappropriate communication by explicitly modeling the social\nrelationship between the individuals. We introduce a new dataset of\ncontextually-situated judgments of appropriateness and show that large language\nmodels can readily incorporate relationship information to accurately identify\nappropriateness in a given context. Using data from online conversations and\nmovie dialogues, we provide insight into how the relationships themselves\nfunction as implicit norms and quantify the degree to which context-sensitivity\nis needed in different conversation settings. Further, we also demonstrate that\ncontextual-appropriateness judgments are predictive of other social factors\nexpressed in language such as condescension and politeness.", "published": "2023-07-06 04:06:05", "link": "http://arxiv.org/abs/2307.02763v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Enhancing LLM with Evolutionary Fine Tuning for News Summary Generation", "abstract": "News summary generation is an important task in the field of intelligence\nanalysis, which can provide accurate and comprehensive information to help\npeople better understand and respond to complex real-world events. However,\ntraditional news summary generation methods face some challenges, which are\nlimited by the model itself and the amount of training data, as well as the\ninfluence of text noise, making it difficult to generate reliable information\naccurately. In this paper, we propose a new paradigm for news summary\ngeneration using LLM with powerful natural language understanding and\ngenerative capabilities. We use LLM to extract multiple structured event\npatterns from the events contained in news paragraphs, evolve the event pattern\npopulation with genetic algorithm, and select the most adaptive event pattern\nto input into the LLM to generate news summaries. A News Summary Generator\n(NSG) is designed to select and evolve the event pattern populations and\ngenerate news summaries. The experimental results show that the news summary\ngenerator is able to generate accurate and reliable news summaries with some\ngeneralization ability.", "published": "2023-07-06 08:13:53", "link": "http://arxiv.org/abs/2307.02839v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Contrast Is All You Need", "abstract": "In this study, we analyze data-scarce classification scenarios, where\navailable labeled legal data is small and imbalanced, potentially hurting the\nquality of the results. We focused on two finetuning objectives; SetFit\n(Sentence Transformer Finetuning), a contrastive learning setup, and a vanilla\nfinetuning setup on a legal provision classification task. Additionally, we\ncompare the features that are extracted with LIME (Local Interpretable\nModel-agnostic Explanations) to see which particular features contributed to\nthe model's classification decisions. The results show that a contrastive setup\nwith SetFit performed better than vanilla finetuning while using a fraction of\nthe training samples. LIME results show that the contrastive learning approach\nhelps boost both positive and negative features which are legally informative\nand contribute to the classification results. Thus a model finetuned with a\ncontrastive objective seems to base its decisions more confidently on legally\ninformative features.", "published": "2023-07-06 09:36:54", "link": "http://arxiv.org/abs/2307.02882v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LEA: Improving Sentence Similarity Robustness to Typos Using Lexical\n  Attention Bias", "abstract": "Textual noise, such as typos or abbreviations, is a well-known issue that\npenalizes vanilla Transformers for most downstream tasks. We show that this is\nalso the case for sentence similarity, a fundamental task in multiple domains,\ne.g. matching, retrieval or paraphrasing. Sentence similarity can be approached\nusing cross-encoders, where the two sentences are concatenated in the input\nallowing the model to exploit the inter-relations between them. Previous works\naddressing the noise issue mainly rely on data augmentation strategies, showing\nimproved robustness when dealing with corrupted samples that are similar to the\nones used for training. However, all these methods still suffer from the token\ndistribution shift induced by typos. In this work, we propose to tackle textual\nnoise by equipping cross-encoders with a novel LExical-aware Attention module\n(LEA) that incorporates lexical similarities between words in both sentences.\nBy using raw text similarities, our approach avoids the tokenization shift\nproblem obtaining improved robustness. We demonstrate that the attention bias\nintroduced by LEA helps cross-encoders to tackle complex scenarios with textual\nnoise, specially in domains with short-text descriptions and limited context.\nExperiments using three popular Transformer encoders in five e-commerce\ndatasets for product matching show that LEA consistently boosts performance\nunder the presence of noise, while remaining competitive on the original\n(clean) splits. We also evaluate our approach in two datasets for textual\nentailment and paraphrasing showing that LEA is robust to typos in domains with\nlonger sentences and more natural context. Additionally, we thoroughly analyze\nseveral design choices in our approach, providing insights about the impact of\nthe decisions made and fostering future research in cross-encoders dealing with\ntypos.", "published": "2023-07-06 10:53:50", "link": "http://arxiv.org/abs/2307.02912v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain", "abstract": "Adapting pretrained language models to novel domains, such as clinical\napplications, traditionally involves retraining their entire set of parameters.\nParameter-Efficient Fine-Tuning (PEFT) techniques for fine-tuning language\nmodels significantly reduce computational requirements by selectively\nfine-tuning small subsets of parameters. In this study, we propose a two-step\nPEFT framework and evaluate it in the clinical domain. Our approach combines a\nspecialised PEFT adapter layer designed for clinical domain adaptation with\nanother adapter specialised for downstream tasks. We evaluate the framework on\nmultiple clinical outcome prediction datasets, comparing it to clinically\ntrained language models. Our framework achieves a better AUROC score averaged\nacross all clinical downstream tasks compared to clinical language models. In\nparticular, we observe large improvements of 4-5% AUROC in large-scale\nmultilabel classification tasks, such as diagnoses and procedures\nclassification. To our knowledge, this study is the first to provide an\nextensive empirical analysis of the interplay between PEFT techniques and\ndomain adaptation in an important real-world domain of clinical applications.", "published": "2023-07-06 15:06:41", "link": "http://arxiv.org/abs/2307.03042v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Domain Adaptation of Sentence Embeddings Using Adapters", "abstract": "Sentence embeddings enable us to capture the semantic similarity of short\ntexts. Most sentence embedding models are trained for general semantic textual\nsimilarity tasks. Therefore, to use sentence embeddings in a particular domain,\nthe model must be adapted to it in order to achieve good results. Usually, this\nis done by fine-tuning the entire sentence embedding model for the domain of\ninterest. While this approach yields state-of-the-art results, all of the\nmodel's weights are updated during fine-tuning, making this method\nresource-intensive. Therefore, instead of fine-tuning entire sentence embedding\nmodels for each target domain individually, we propose to train lightweight\nadapters. These domain-specific adapters do not require fine-tuning all\nunderlying sentence embedding model parameters. Instead, we only train a small\nnumber of additional parameters while keeping the weights of the underlying\nsentence embedding model fixed. Training domain-specific adapters allows always\nusing the same base model and only exchanging the domain-specific adapters to\nadapt sentence embeddings to a specific domain. We show that using adapters for\nparameter-efficient domain adaptation of sentence embeddings yields competitive\nperformance within 1% of a domain-adapted, entirely fine-tuned sentence\nembedding model while only training approximately 3.6% of the parameters.", "published": "2023-07-06 16:26:34", "link": "http://arxiv.org/abs/2307.03104v6", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Survey on Evaluation of Large Language Models", "abstract": "Large language models (LLMs) are gaining increasing popularity in both\nacademia and industry, owing to their unprecedented performance in various\napplications. As LLMs continue to play a vital role in both research and daily\nuse, their evaluation becomes increasingly critical, not only at the task\nlevel, but also at the society level for better understanding of their\npotential risks. Over the past years, significant efforts have been made to\nexamine LLMs from various perspectives. This paper presents a comprehensive\nreview of these evaluation methods for LLMs, focusing on three key dimensions:\nwhat to evaluate, where to evaluate, and how to evaluate. Firstly, we provide\nan overview from the perspective of evaluation tasks, encompassing general\nnatural language processing tasks, reasoning, medical usage, ethics,\neducations, natural and social sciences, agent applications, and other areas.\nSecondly, we answer the `where' and `how' questions by diving into the\nevaluation methods and benchmarks, which serve as crucial components in\nassessing performance of LLMs. Then, we summarize the success and failure cases\nof LLMs in different tasks. Finally, we shed light on several future challenges\nthat lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to\nresearchers in the realm of LLMs evaluation, thereby aiding the development of\nmore proficient LLMs. Our key point is that evaluation should be treated as an\nessential discipline to better assist the development of LLMs. We consistently\nmaintain the related open-source materials at:\nhttps://github.com/MLGroupJLU/LLM-eval-survey.", "published": "2023-07-06 16:28:35", "link": "http://arxiv.org/abs/2307.03109v9", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VisKoP: Visual Knowledge oriented Programming for Interactive Knowledge\n  Base Question Answering", "abstract": "We present Visual Knowledge oriented Programming platform (VisKoP), a\nknowledge base question answering (KBQA) system that integrates human into the\nloop to edit and debug the knowledge base (KB) queries. VisKoP not only\nprovides a neural program induction module, which converts natural language\nquestions into knowledge oriented program language (KoPL), but also maps KoPL\nprograms into graphical elements. KoPL programs can be edited with simple\ngraphical operators, such as dragging to add knowledge operators and slot\nfilling to designate operator arguments. Moreover, VisKoP provides\nauto-completion for its knowledge base schema and users can easily debug the\nKoPL program by checking its intermediate results. To facilitate the practical\nKBQA on a million-entity-level KB, we design a highly efficient KoPL execution\nengine for the back-end. Experiment results show that VisKoP is highly\nefficient and user interaction can fix a large portion of wrong KoPL programs\nto acquire the correct answer. The VisKoP online demo\nhttps://demoviskop.xlore.cn (Stable release of this paper) and\nhttps://viskop.xlore.cn (Beta release with new features), highly efficient KoPL\nengine https://pypi.org/project/kopl-engine, and screencast video\nhttps://youtu.be/zAbJtxFPTXo are now publicly available.", "published": "2023-07-06 16:58:27", "link": "http://arxiv.org/abs/2307.03130v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Can ChatGPT's Responses Boost Traditional Natural Language Processing?", "abstract": "The employment of foundation models is steadily expanding, especially with\nthe launch of ChatGPT and the release of other foundation models. These models\nhave shown the potential of emerging capabilities to solve problems, without\nbeing particularly trained to solve. A previous work demonstrated these\nemerging capabilities in affective computing tasks; the performance quality was\nsimilar to traditional Natural Language Processing (NLP) techniques, but\nfalling short of specialised trained models, like fine-tuning of the RoBERTa\nlanguage model. In this work, we extend this by exploring if ChatGPT has novel\nknowledge that would enhance existing specialised models when they are fused\ntogether. We achieve this by investigating the utility of verbose responses\nfrom ChatGPT about solving a downstream task, in addition to studying the\nutility of fusing that with existing NLP methods. The study is conducted on\nthree affective computing problems, namely sentiment analysis, suicide tendency\ndetection, and big-five personality assessment. The results conclude that\nChatGPT has indeed novel knowledge that can improve existing NLP techniques by\nway of fusion, be it early or late fusion.", "published": "2023-07-06 15:42:05", "link": "http://arxiv.org/abs/2307.04648v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CORE-GPT: Combining Open Access research and large language models for\n  credible, trustworthy question answering", "abstract": "In this paper, we present CORE-GPT, a novel question-answering platform that\ncombines GPT-based language models and more than 32 million full-text open\naccess scientific articles from CORE. We first demonstrate that GPT3.5 and GPT4\ncannot be relied upon to provide references or citations for generated text. We\nthen introduce CORE-GPT which delivers evidence-based answers to questions,\nalong with citations and links to the cited papers, greatly increasing the\ntrustworthiness of the answers and reducing the risk of hallucinations.\nCORE-GPT's performance was evaluated on a dataset of 100 questions covering the\ntop 20 scientific domains in CORE, resulting in 100 answers and links to 500\nrelevant articles. The quality of the provided answers and and relevance of the\nlinks were assessed by two annotators. Our results demonstrate that CORE-GPT\ncan produce comprehensive and trustworthy answers across the majority of\nscientific domains, complete with links to genuine, relevant scientific\narticles.", "published": "2023-07-06 13:41:36", "link": "http://arxiv.org/abs/2307.04683v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How word semantics and phonology affect handwriting of Alzheimer's\n  patients: a machine learning based analysis", "abstract": "Using kinematic properties of handwriting to support the diagnosis of\nneurodegenerative disease is a real challenge: non-invasive detection\ntechniques combined with machine learning approaches promise big steps forward\nin this research field. In literature, the tasks proposed focused on different\ncognitive skills to elicitate handwriting movements. In particular, the meaning\nand phonology of words to copy can compromise writing fluency. In this paper,\nwe investigated how word semantics and phonology affect the handwriting of\npeople affected by Alzheimer's disease. To this aim, we used the data from six\nhandwriting tasks, each requiring copying a word belonging to one of the\nfollowing categories: regular (have a predictable phoneme-grapheme\ncorrespondence, e.g., cat), non-regular (have atypical phoneme-grapheme\ncorrespondence, e.g., laugh), and non-word (non-meaningful pronounceable letter\nstrings that conform to phoneme-grapheme conversion rules). We analyzed the\ndata using a machine learning approach by implementing four well-known and\nwidely-used classifiers and feature selection. The experimental results showed\nthat the feature selection allowed us to derive a different set of highly\ndistinctive features for each word type. Furthermore, non-regular words needed,\non average, more features but achieved excellent classification performance:\nthe best result was obtained on a non-regular, reaching an accuracy close to\n90%.", "published": "2023-07-06 13:35:06", "link": "http://arxiv.org/abs/2307.04762v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "S2vNTM: Semi-supervised vMF Neural Topic Modeling", "abstract": "Language model based methods are powerful techniques for text classification.\nHowever, the models have several shortcomings. (1) It is difficult to integrate\nhuman knowledge such as keywords. (2) It needs a lot of resources to train the\nmodels. (3) It relied on large text data to pretrain. In this paper, we propose\nSemi-Supervised vMF Neural Topic Modeling (S2vNTM) to overcome these\ndifficulties. S2vNTM takes a few seed keywords as input for topics. S2vNTM\nleverages the pattern of keywords to identify potential topics, as well as\noptimize the quality of topics' keywords sets. Across a variety of datasets,\nS2vNTM outperforms existing semi-supervised topic modeling methods in\nclassification accuracy with limited keywords provided. S2vNTM is at least\ntwice as fast as baselines.", "published": "2023-07-06 21:44:31", "link": "http://arxiv.org/abs/2307.04804v2", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "UniCoRN: Unified Cognitive Signal ReconstructioN bridging cognitive\n  signals and human language", "abstract": "Decoding text stimuli from cognitive signals (e.g. fMRI) enhances our\nunderstanding of the human language system, paving the way for building\nversatile Brain-Computer Interface. However, existing studies largely focus on\ndecoding individual word-level fMRI volumes from a restricted vocabulary, which\nis far too idealized for real-world application. In this paper, we propose\nfMRI2text, the first openvocabulary task aiming to bridge fMRI time series and\nhuman language. Furthermore, to explore the potential of this new task, we\npresent a baseline solution, UniCoRN: the Unified Cognitive Signal\nReconstructioN for Brain Decoding. By reconstructing both individual time\npoints and time series, UniCoRN establishes a robust encoder for cognitive\nsignals (fMRI & EEG). Leveraging a pre-trained language model as decoder,\nUniCoRN proves its efficacy in decoding coherent text from fMRI series across\nvarious split settings. Our model achieves a 34.77% BLEU score on fMRI2text,\nand a 37.04% BLEU when generalized to EEGto-text decoding, thereby surpassing\nthe former baseline. Experimental results indicate the feasibility of decoding\nconsecutive fMRI volumes, and the effectiveness of decoding different cognitive\nsignals using a unified structure.", "published": "2023-07-06 05:26:49", "link": "http://arxiv.org/abs/2307.05355v1", "categories": ["eess.SP", "cs.CL"], "primary_category": "eess.SP"}
{"title": "On-Device Constrained Self-Supervised Speech Representation Learning for\n  Keyword Spotting via Knowledge Distillation", "abstract": "Large self-supervised models are effective feature extractors, but their\napplication is challenging under on-device budget constraints and biased\ndataset collection, especially in keyword spotting. To address this, we\nproposed a knowledge distillation-based self-supervised speech representation\nlearning (S3RL) architecture for on-device keyword spotting. Our approach used\na teacher-student framework to transfer knowledge from a larger, more complex\nmodel to a smaller, light-weight model using dual-view cross-correlation\ndistillation and the teacher's codebook as learning objectives. We evaluated\nour model's performance on an Alexa keyword spotting detection task using a\n16.6k-hour in-house dataset. Our technique showed exceptional performance in\nnormal and noisy conditions, demonstrating the efficacy of knowledge\ndistillation methods in constructing self-supervised models for keyword\nspotting tasks while working within on-device resource constraints.", "published": "2023-07-06 02:03:31", "link": "http://arxiv.org/abs/2307.02720v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "RecallM: An Adaptable Memory Mechanism with Temporal Understanding for\n  Large Language Models", "abstract": "Large Language Models (LLMs) have made extraordinary progress in the field of\nArtificial Intelligence and have demonstrated remarkable capabilities across a\nlarge variety of tasks and domains. However, as we venture closer to creating\nArtificial General Intelligence (AGI) systems, we recognize the need to\nsupplement LLMs with long-term memory to overcome the context window limitation\nand more importantly, to create a foundation for sustained reasoning,\ncumulative learning and long-term user interaction. In this paper we propose\nRecallM, a novel architecture for providing LLMs with an adaptable and\nupdatable long-term memory mechanism. Unlike previous methods, the RecallM\narchitecture is particularly effective at belief updating and maintaining a\ntemporal understanding of the knowledge provided to it. We demonstrate through\nvarious experiments the effectiveness of this architecture. Furthermore,\nthrough our own temporal understanding and belief updating experiments, we show\nthat RecallM is four times more effective than using a vector database for\nupdating knowledge previously stored in long-term memory. We also demonstrate\nthat RecallM shows competitive performance on general question-answering and\nin-context learning tasks.", "published": "2023-07-06 02:51:54", "link": "http://arxiv.org/abs/2307.02738v3", "categories": ["cs.AI", "cs.CL", "cs.SC"], "primary_category": "cs.AI"}
{"title": "What Should Data Science Education Do with Large Language Models?", "abstract": "The rapid advances of large language models (LLMs), such as ChatGPT, are\nrevolutionizing data science and statistics. These state-of-the-art tools can\nstreamline complex processes. As a result, it reshapes the role of data\nscientists. We argue that LLMs are transforming the responsibilities of data\nscientists, shifting their focus from hands-on coding, data-wrangling and\nconducting standard analyses to assessing and managing analyses performed by\nthese automated AIs. This evolution of roles is reminiscent of the transition\nfrom a software engineer to a product manager. We illustrate this transition\nwith concrete data science case studies using LLMs in this paper. These\ndevelopments necessitate a meaningful evolution in data science education.\nPedagogy must now place greater emphasis on cultivating diverse skillsets among\nstudents, such as LLM-informed creativity, critical thinking, AI-guided\nprogramming. LLMs can also play a significant role in the classroom as\ninteractive teaching and learning tools, contributing to personalized\neducation. This paper discusses the opportunities, resources and open\nchallenges for each of these directions. As with any transformative technology,\nintegrating LLMs into education calls for careful consideration. While LLMs can\nperform repetitive tasks efficiently, it's crucial to remember that their role\nis to supplement human intelligence and creativity, not to replace it.\nTherefore, the new era of data science education should balance the benefits of\nLLMs while fostering complementary human expertise and innovations. In\nconclusion, the rise of LLMs heralds a transformative period for data science\nand its education. This paper seeks to shed light on the emerging trends,\npotential opportunities, and challenges accompanying this paradigm shift,\nhoping to spark further discourse and investigation into this exciting,\nuncharted territory.", "published": "2023-07-06 06:07:29", "link": "http://arxiv.org/abs/2307.02792v2", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "VerifAI: Verified Generative AI", "abstract": "Generative AI has made significant strides, yet concerns about the accuracy\nand reliability of its outputs continue to grow. Such inaccuracies can have\nserious consequences such as inaccurate decision-making, the spread of false\ninformation, privacy violations, legal liabilities, and more. Although efforts\nto address these risks are underway, including explainable AI and responsible\nAI practices such as transparency, privacy protection, bias mitigation, and\nsocial and environmental responsibility, misinformation caused by generative AI\nwill remain a significant challenge. We propose that verifying the outputs of\ngenerative AI from a data management perspective is an emerging issue for\ngenerative AI. This involves analyzing the underlying data from multi-modal\ndata lakes, including text files, tables, and knowledge graphs, and assessing\nits quality and consistency. By doing so, we can establish a stronger\nfoundation for evaluating the outputs of generative AI models. Such an approach\ncan ensure the correctness of generative AI, promote transparency, and enable\ndecision-making with greater confidence. Our vision is to promote the\ndevelopment of verifiable generative AI and contribute to a more trustworthy\nand responsible use of AI.", "published": "2023-07-06 06:11:51", "link": "http://arxiv.org/abs/2307.02796v2", "categories": ["cs.DB", "cs.CL", "cs.LG"], "primary_category": "cs.DB"}
{"title": "The Relationship Between Speech Features Changes When You Get Depressed:\n  Feature Correlations for Improving Speed and Performance of Depression\n  Detection", "abstract": "This work shows that depression changes the correlation between features\nextracted from speech. Furthermore, it shows that using such an insight can\nimprove the training speed and performance of depression detectors based on\nSVMs and LSTMs. The experiments were performed over the Androids Corpus, a\npublicly available dataset involving 112 speakers, including 58 people\ndiagnosed with depression by professional psychiatrists. The results show that\nthe models used in the experiments improve in terms of training speed and\nperformance when fed with feature correlation matrices rather than with feature\nvectors. The relative reduction of the error rate ranges between 23.1% and\n26.6% depending on the model. The probable explanation is that feature\ncorrelation matrices appear to be more variable in the case of depressed\nspeakers. Correspondingly, such a phenomenon can be thought of as a depression\nmarker.", "published": "2023-07-06 09:54:35", "link": "http://arxiv.org/abs/2307.02892v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "On the Cultural Gap in Text-to-Image Generation", "abstract": "One challenge in text-to-image (T2I) generation is the inadvertent reflection\nof culture gaps present in the training data, which signifies the disparity in\ngenerated image quality when the cultural elements of the input text are rarely\ncollected in the training set. Although various T2I models have shown\nimpressive but arbitrary examples, there is no benchmark to systematically\nevaluate a T2I model's ability to generate cross-cultural images. To bridge the\ngap, we propose a Challenging Cross-Cultural (C3) benchmark with comprehensive\nevaluation criteria, which can assess how well-suited a model is to a target\nculture. By analyzing the flawed images generated by the Stable Diffusion model\non the C3 benchmark, we find that the model often fails to generate certain\ncultural objects. Accordingly, we propose a novel multi-modal metric that\nconsiders object-text alignment to filter the fine-tuning data in the target\nculture, which is used to fine-tune a T2I model to improve cross-cultural\ngeneration. Experimental results show that our multi-modal metric provides\nstronger data selection performance on the C3 benchmark than existing metrics,\nin which the object-text alignment is crucial. We release the benchmark, data,\ncode, and generated images to facilitate future research on culturally diverse\nT2I generation (https://github.com/longyuewangdcu/C3-Bench).", "published": "2023-07-06 13:17:55", "link": "http://arxiv.org/abs/2307.02971v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Efficient Semiring-Weighted Earley Parsing", "abstract": "This paper provides a reference description, in the form of a deduction\nsystem, of Earley's (1970) context-free parsing algorithm with various\nspeed-ups. Our presentation includes a known worst-case runtime improvement\nfrom Earley's $O (N^3|G||R|)$, which is unworkable for the large grammars that\narise in natural language processing, to $O (N^3|G|)$, which matches the\nruntime of CKY on a binarized version of the grammar $G$. Here $N$ is the\nlength of the sentence, $|R|$ is the number of productions in $G$, and $|G|$ is\nthe total length of those productions. We also provide a version that achieves\nruntime of $O (N^3|M|)$ with $|M| \\leq |G|$ when the grammar is represented\ncompactly as a single finite-state automaton $M$ (this is partly novel). We\ncarefully treat the generalization to semiring-weighted deduction,\npreprocessing the grammar like Stolcke (1995) to eliminate deduction cycles,\nand further generalize Stolcke's method to compute the weights of sentence\nprefixes. We also provide implementation details for efficient execution,\nensuring that on a preprocessed grammar, the semiring-weighted versions of our\nmethods have the same asymptotic runtime and space requirements as the\nunweighted methods, including sub-cubic runtime on some grammars.", "published": "2023-07-06 13:33:59", "link": "http://arxiv.org/abs/2307.02982v1", "categories": ["cs.CL", "cs.DS", "cs.FL"], "primary_category": "cs.CL"}
{"title": "Improving Retrieval-Augmented Large Language Models via Data Importance\n  Learning", "abstract": "Retrieval augmentation enables large language models to take advantage of\nexternal knowledge, for example on tasks like question answering and data\nimputation. However, the performance of such retrieval-augmented models is\nlimited by the data quality of their underlying retrieval corpus. In this\npaper, we propose an algorithm based on multilinear extension for evaluating\nthe data importance of retrieved data points. There are exponentially many\nterms in the multilinear extension, and one key contribution of this paper is a\npolynomial time algorithm that computes exactly, given a retrieval-augmented\nmodel with an additive utility function and a validation set, the data\nimportance of data points in the retrieval corpus using the multilinear\nextension of the model's utility function. We further proposed an even more\nefficient ({\\epsilon}, {\\delta})-approximation algorithm. Our experimental\nresults illustrate that we can enhance the performance of large language models\nby only pruning or reweighting the retrieval corpus, without requiring further\ntraining. For some tasks, this even allows a small model (e.g., GPT-JT),\naugmented with a search engine API, to outperform GPT-3.5 (without retrieval\naugmentation). Moreover, we show that weights based on multilinear extension\ncan be computed efficiently in practice (e.g., in less than ten minutes for a\ncorpus with 100 million elements).", "published": "2023-07-06 14:44:07", "link": "http://arxiv.org/abs/2307.03027v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Generalizing Backpropagation for Gradient-Based Interpretability", "abstract": "Many popular feature-attribution methods for interpreting deep neural\nnetworks rely on computing the gradients of a model's output with respect to\nits inputs. While these methods can indicate which input features may be\nimportant for the model's prediction, they reveal little about the inner\nworkings of the model itself. In this paper, we observe that the gradient\ncomputation of a model is a special case of a more general formulation using\nsemirings. This observation allows us to generalize the backpropagation\nalgorithm to efficiently compute other interpretable statistics about the\ngradient graph of a neural network, such as the highest-weighted path and\nentropy. We implement this generalized algorithm, evaluate it on synthetic\ndatasets to better understand the statistics it computes, and apply it to study\nBERT's behavior on the subject-verb number agreement task (SVA). With this\nmethod, we (a) validate that the amount of gradient flow through a component of\na model reflects its importance to a prediction and (b) for SVA, identify which\npathways of the self-attention mechanism are most important.", "published": "2023-07-06 15:19:53", "link": "http://arxiv.org/abs/2307.03056v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "DeepOnto: A Python Package for Ontology Engineering with Deep Learning", "abstract": "Integrating deep learning techniques, particularly language models (LMs),\nwith knowledge representation techniques like ontologies has raised widespread\nattention, urging the need of a platform that supports both paradigms. Although\npackages such as OWL API and Jena offer robust support for basic ontology\nprocessing features, they lack the capability to transform various types of\ninformation within ontologies into formats suitable for downstream deep\nlearning-based applications. Moreover, widely-used ontology APIs are primarily\nJava-based while deep learning frameworks like PyTorch and Tensorflow are\nmainly for Python programming. To address the needs, we present DeepOnto, a\nPython package designed for ontology engineering with deep learning. The\npackage encompasses a core ontology processing module founded on the\nwidely-recognised and reliable OWL API, encapsulating its fundamental features\nin a more \"Pythonic\" manner and extending its capabilities to incorporate other\nessential components including reasoning, verbalisation, normalisation,\ntaxonomy, projection, and more. Building on this module, DeepOnto offers a\nsuite of tools, resources, and algorithms that support various ontology\nengineering tasks, such as ontology alignment and completion, by harnessing\ndeep learning methods, primarily pre-trained LMs. In this paper, we also\ndemonstrate the practical utility of DeepOnto through two use-cases: the\nDigital Health Coaching in Samsung Research UK and the Bio-ML track of the\nOntology Alignment Evaluation Initiative (OAEI).", "published": "2023-07-06 15:35:02", "link": "http://arxiv.org/abs/2307.03067v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "primary_category": "cs.AI"}
{"title": "T-MARS: Improving Visual Representations by Circumventing Text Feature\n  Learning", "abstract": "Large web-sourced multimodal datasets have powered a slew of new methods for\nlearning general-purpose visual representations, advancing the state of the art\nin computer vision and revolutionizing zero- and few-shot recognition. One\ncrucial decision facing practitioners is how, if at all, to curate these\never-larger datasets. For example, the creators of the LAION-5B dataset chose\nto retain only image-caption pairs whose CLIP similarity score exceeded a\ndesignated threshold. In this paper, we propose a new state-of-the-art data\nfiltering approach motivated by our observation that nearly 40% of LAION's\nimages contain text that overlaps significantly with the caption. Intuitively,\nsuch data could be wasteful as it incentivizes models to perform optical\ncharacter recognition rather than learning visual features. However, naively\nremoving all such data could also be wasteful, as it throws away images that\ncontain visual features (in addition to overlapping text). Our simple and\nscalable approach, T-MARS (Text Masking and Re-Scoring), filters out only those\npairs where the text dominates the remaining visual features -- by first\nmasking out the text and then filtering out those with a low CLIP similarity\nscore of the masked image. Experimentally, T-MARS outperforms the top-ranked\nmethod on the \"medium scale\" of DataComp (a data filtering benchmark) by a\nmargin of 6.5% on ImageNet and 4.7% on VTAB. Additionally, our systematic\nevaluation on various data pool sizes from 2M to 64M shows that the accuracy\ngains enjoyed by T-MARS linearly increase as data and compute are scaled\nexponentially. Code is available at https://github.com/locuslab/T-MARS.", "published": "2023-07-06 16:59:52", "link": "http://arxiv.org/abs/2307.03132v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Distilling Large Vision-Language Model with Out-of-Distribution\n  Generalizability", "abstract": "Large vision-language models have achieved outstanding performance, but their\nsize and computational requirements make their deployment on\nresource-constrained devices and time-sensitive tasks impractical. Model\ndistillation, the process of creating smaller, faster models that maintain the\nperformance of larger models, is a promising direction towards the solution.\nThis paper investigates the distillation of visual representations in large\nteacher vision-language models into lightweight student models using a small-\nor mid-scale dataset. Notably, this study focuses on open-vocabulary\nout-of-distribution (OOD) generalization, a challenging problem that has been\noverlooked in previous model distillation literature. We propose two principles\nfrom vision and language modality perspectives to enhance student's OOD\ngeneralization: (1) by better imitating teacher's visual representation space,\nand carefully promoting better coherence in vision-language alignment with the\nteacher; (2) by enriching the teacher's language representations with\ninformative and finegrained semantic attributes to effectively distinguish\nbetween different labels. We propose several metrics and conduct extensive\nexperiments to investigate their techniques. The results demonstrate\nsignificant improvements in zero-shot and few-shot student performance on\nopen-vocabulary out-of-distribution classification, highlighting the\neffectiveness of our proposed approaches. Poster:\nhttps://xuanlinli17.github.io/pdfs/iccv23_large_vlm_distillation_poster.pdf\nCode: https://github.com/xuanlinli17/large_vlm_distillation_ood", "published": "2023-07-06 17:05:26", "link": "http://arxiv.org/abs/2307.03135v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Focused Transformer: Contrastive Training for Context Scaling", "abstract": "Large language models have an exceptional capability to incorporate new\ninformation in a contextual manner. However, the full potential of such an\napproach is often restrained due to a limitation in the effective context\nlength. One solution to this issue is to endow an attention layer with access\nto an external memory, which comprises of (key, value) pairs. Yet, as the\nnumber of documents increases, the proportion of relevant keys to irrelevant\nones decreases, leading the model to focus more on the irrelevant keys. We\nidentify a significant challenge, dubbed the distraction issue, where keys\nlinked to different semantic values might overlap, making them hard to\ndistinguish. To tackle this problem, we introduce the Focused Transformer\n(FoT), a technique that employs a training process inspired by contrastive\nlearning. This novel approach enhances the structure of the (key, value) space,\nenabling an extension of the context length. Our method allows for fine-tuning\npre-existing, large-scale models to lengthen their effective context. This is\ndemonstrated by our fine-tuning of $3B$ and $7B$ OpenLLaMA checkpoints. The\nresulting models, which we name LongLLaMA, exhibit advancements in tasks\nrequiring a long context. We further illustrate that our LongLLaMA models\nadeptly manage a $256 k$ context length for passkey retrieval.", "published": "2023-07-06 17:52:10", "link": "http://arxiv.org/abs/2307.03170v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Vision Language Transformers: A Survey", "abstract": "Vision language tasks, such as answering questions about or generating\ncaptions that describe an image, are difficult tasks for computers to perform.\nA relatively recent body of research has adapted the pretrained transformer\narchitecture introduced in \\citet{vaswani2017attention} to vision language\nmodeling. Transformer models have greatly improved performance and versatility\nover previous vision language models. They do so by pretraining models on a\nlarge generic datasets and transferring their learning to new tasks with minor\nchanges in architecture and parameter values. This type of transfer learning\nhas become the standard modeling practice in both natural language processing\nand computer vision. Vision language transformers offer the promise of\nproducing similar advancements in tasks which require both vision and language.\nIn this paper, we provide a broad synthesis of the currently available research\non vision language transformer models and offer some analysis of their\nstrengths, limitations and some open questions that remain.", "published": "2023-07-06 19:08:56", "link": "http://arxiv.org/abs/2307.03254v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "It is not Sexually Suggestive, It is Educative. Separating Sex Education\n  from Suggestive Content on TikTok Videos", "abstract": "We introduce SexTok, a multi-modal dataset composed of TikTok videos labeled\nas sexually suggestive (from the annotator's point of view), sex-educational\ncontent, or neither. Such a dataset is necessary to address the challenge of\ndistinguishing between sexually suggestive content and virtual sex education\nvideos on TikTok. Children's exposure to sexually suggestive videos has been\nshown to have adversarial effects on their development. Meanwhile, virtual sex\neducation, especially on subjects that are more relevant to the LGBTQIA+\ncommunity, is very valuable. The platform's current system removes or penalizes\nsome of both types of videos, even though they serve different purposes. Our\ndataset contains video URLs, and it is also audio transcribed. To validate its\nimportance, we explore two transformer-based models for classifying the videos.\nOur preliminary results suggest that the task of distinguishing between these\ntypes of videos is learnable but challenging. These experiments suggest that\nthis dataset is meaningful and invites further study on the subject.", "published": "2023-07-06 20:23:17", "link": "http://arxiv.org/abs/2307.03274v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "I.2.10; I.4.9; I.2.7; I.5.4"], "primary_category": "cs.CV"}
{"title": "Gammatonegram Representation for End-to-End Dysarthric Speech Processing\n  Tasks: Speech Recognition, Speaker Identification, and Intelligibility\n  Assessment", "abstract": "Dysarthria is a disability that causes a disturbance in the human speech\nsystem and reduces the quality and intelligibility of a person's speech.\nBecause of this effect, the normal speech processing systems can not work\nproperly on impaired speech. This disability is usually associated with\nphysical disabilities. Therefore, designing a system that can perform some\ntasks by receiving voice commands in the smart home can be a significant\nachievement. In this work, we introduce gammatonegram as an effective method to\nrepresent audio files with discriminative details, which is used as input for\nthe convolutional neural network. On the other word, we convert each speech\nfile into an image and propose image recognition system to classify speech in\ndifferent scenarios. Proposed CNN is based on the transfer learning method on\nthe pre-trained Alexnet. In this research, the efficiency of the proposed\nsystem for speech recognition, speaker identification, and intelligibility\nassessment is evaluated. According to the results on the UA dataset, the\nproposed speech recognition system achieved 91.29% accuracy in\nspeaker-dependent mode, the speaker identification system acquired 87.74%\naccuracy in text-dependent mode, and the intelligibility assessment system\nachieved 96.47% accuracy in two-class mode. Finally, we propose a multi-network\nspeech recognition system that works fully automatically. This system is\nlocated in a cascade arrangement with the two-class intelligibility assessment\nsystem, and the output of this system activates each one of the speech\nrecognition networks. This architecture achieves an accuracy of 92.3% WRR. The\nsource code of this paper is available.", "published": "2023-07-06 21:10:50", "link": "http://arxiv.org/abs/2307.03296v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "InfoSync: Information Synchronization across Multilingual\n  Semi-structured Tables", "abstract": "Information Synchronization of semi-structured data across languages is\nchallenging. For instance, Wikipedia tables in one language should be\nsynchronized across languages. To address this problem, we introduce a new\ndataset InfoSyncC and a two-step method for tabular synchronization. InfoSync\ncontains 100K entity-centric tables (Wikipedia Infoboxes) across 14 languages,\nof which a subset (3.5K pairs) are manually annotated. The proposed method\nincludes 1) Information Alignment to map rows and 2) Information Update for\nupdating missing/outdated information for aligned tables across multilingual\ntables. When evaluated on InfoSync, information alignment achieves an F1 score\nof 87.91 (en <-> non-en). To evaluate information updation, we perform\nhuman-assisted Wikipedia edits on Infoboxes for 603 table pairs. Our approach\nobtains an acceptance rate of 77.28% on Wikipedia, showing the effectiveness of\nthe proposed method.", "published": "2023-07-06 21:55:15", "link": "http://arxiv.org/abs/2307.03313v1", "categories": ["cs.CL", "cs.CY", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Undecimated Wavelet Transform for Word Embedded Semantic Marginal\n  Autoencoder in Security improvement and Denoising different Languages", "abstract": "By combining the undecimated wavelet transform within a Word Embedded\nSemantic Marginal Autoencoder (WESMA), this research study provides a novel\nstrategy for improving security measures and denoising multiple languages. The\nincorporation of these strategies is intended to address the issues of\nrobustness, privacy, and multilingualism in data processing applications. The\nundecimated wavelet transform is used as a feature extraction tool to identify\nprominent language patterns and structural qualities in the input data. The\nproposed system may successfully capture significant information while\npreserving the temporal and geographical links within the data by employing\nthis transform. This improves security measures by increasing the system's\nability to detect abnormalities, discover hidden patterns, and distinguish\nbetween legitimate content and dangerous threats. The Word Embedded Semantic\nMarginal Autoencoder also functions as an intelligent framework for\ndimensionality and noise reduction. The autoencoder effectively learns the\nunderlying semantics of the data and reduces noise components by exploiting\nword embeddings and semantic context. As a result, data quality and accuracy\nare increased in following processing stages. The suggested methodology is\ntested using a diversified dataset that includes several languages and security\nscenarios. The experimental results show that the proposed approach is\neffective in attaining security enhancement and denoising capabilities across\nmultiple languages. The system is strong in dealing with linguistic variances,\nproducing consistent outcomes regardless of the language used. Furthermore,\nincorporating the undecimated wavelet transform considerably improves the\nsystem's ability to efficiently address complex security concerns", "published": "2023-07-06 04:10:40", "link": "http://arxiv.org/abs/2307.03679v1", "categories": ["cs.CL", "cs.CR", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Read, Look or Listen? What's Needed for Solving a Multimodal Dataset", "abstract": "The prevalence of large-scale multimodal datasets presents unique challenges\nin assessing dataset quality. We propose a two-step method to analyze\nmultimodal datasets, which leverages a small seed of human annotation to map\neach multimodal instance to the modalities required to process it. Our method\nsheds light on the importance of different modalities in datasets, as well as\nthe relationship between them. We apply our approach to TVQA, a video\nquestion-answering dataset, and discover that most questions can be answered\nusing a single modality, without a substantial bias towards any specific\nmodality. Moreover, we find that more than 70% of the questions are solvable\nusing several different single-modality strategies, e.g., by either looking at\nthe video or listening to the audio, highlighting the limited integration of\nmultiple modalities in TVQA. We leverage our annotation and analyze the MERLOT\nReserve, finding that it struggles with image-based questions compared to text\nand audio, but also with auditory speaker identification. Based on our\nobservations, we introduce a new test set that necessitates multiple\nmodalities, observing a dramatic drop in model performance. Our methodology\nprovides valuable insights into multimodal datasets and highlights the need for\nthe development of more robust models.", "published": "2023-07-06 08:02:45", "link": "http://arxiv.org/abs/2307.04532v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Performance Comparison of Pre-trained Models for Speech-to-Text in\n  Turkish: Whisper-Small and Wav2Vec2-XLS-R-300M", "abstract": "In this study, the performances of the Whisper-Small and Wav2Vec2-XLS-R-300M\nmodels which are two pre-trained multilingual models for speech to text were\nexamined for the Turkish language. Mozilla Common Voice version 11.0 which is\nprepared in Turkish language and is an open-source data set, was used in the\nstudy. The multilingual models, Whisper- Small and Wav2Vec2-XLS-R-300M were\nfine-tuned with this data set which contains a small amount of data. The speech\nto text performance of the two models was compared. WER values are calculated\nas 0.28 and 0.16 for the Wav2Vec2-XLS- R-300M and the Whisper-Small models\nrespectively. In addition, the performances of the models were examined with\nthe test data prepared with call center records that were not included in the\ntraining and validation dataset.", "published": "2023-07-06 21:01:18", "link": "http://arxiv.org/abs/2307.04765v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Amplifying Limitations, Harms and Risks of Large Language Models", "abstract": "We present this article as a small gesture in an attempt to counter what\nappears to be exponentially growing hype around Artificial Intelligence (AI)\nand its capabilities, and the distraction provided by the associated talk of\nscience-fiction scenarios that might arise if AI should become sentient and\nsuper-intelligent. It may also help those outside of the field to become more\ninformed about some of the limitations of AI technology. In the current context\nof popular discourse AI defaults to mean foundation and large language models\n(LLMs) such as those used to create ChatGPT. This in itself is a\nmisrepresentation of the diversity, depth and volume of research, researchers,\nand technology that truly represents the field of AI. AI being a field of\nresearch that has existed in software artefacts since at least the 1950's. We\nset out to highlight a number of limitations of LLMs, and in so doing highlight\nthat harms have already arisen and will continue to arise due to these\nlimitations. Along the way we also highlight some of the associated risks for\nindividuals and organisations in using this technology.", "published": "2023-07-06 11:53:45", "link": "http://arxiv.org/abs/2307.04821v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "A Novel Site-Agnostic Multimodal Deep Learning Model to Identify\n  Pro-Eating Disorder Content on Social Media", "abstract": "Over the last decade, there has been a vast increase in eating disorder\ndiagnoses and eating disorder-attributed deaths, reaching their zenith during\nthe Covid-19 pandemic. This immense growth derived in part from the stressors\nof the pandemic but also from increased exposure to social media, which is rife\nwith content that promotes eating disorders. This study aimed to create a\nmultimodal deep learning model that can determine if a given social media post\npromotes eating disorders based on a combination of visual and textual data. A\nlabeled dataset of Tweets was collected from Twitter, recently rebranded as X,\nupon which twelve deep learning models were trained and evaluated. Based on\nmodel performance, the most effective deep learning model was the multimodal\nfusion of the RoBERTa natural language processing model and the MaxViT image\nclassification model, attaining accuracy and F1 scores of 95.9% and 0.959,\nrespectively. The RoBERTa and MaxViT fusion model, deployed to classify an\nunlabeled dataset of posts from the social media sites Tumblr and Reddit,\ngenerated results akin to those of previous research studies that did not\nemploy artificial intelligence-based techniques, indicating that deep learning\nmodels can develop insights congruent to those of researchers. Additionally,\nthe model was used to conduct a time-series analysis of yet unseen Tweets from\neight Twitter hashtags, uncovering that, since 2014, the relative abundance of\ncontent that promotes eating disorders has decreased drastically within those\ncommunities. Despite this reduction, by 2018, content that promotes eating\ndisorders had either stopped declining or increased in ampleness anew on those\nhashtags.", "published": "2023-07-06 16:04:46", "link": "http://arxiv.org/abs/2307.06775v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Label-Synchronous Neural Transducer for End-to-End ASR", "abstract": "Neural transducers provide a natural way of streaming ASR. However, they\naugment output sequences with blank tokens which leads to challenges for domain\nadaptation using text data. This paper proposes a label-synchronous neural\ntransducer (LS-Transducer), which extracts a label-level encoder representation\nbefore combining it with the prediction network output. Hence blank tokens are\nno longer needed and the prediction network can be easily adapted using text\ndata. An Auto-regressive Integrate-and-Fire (AIF) mechanism is proposed to\ngenerate the label-level encoder representation while retaining the streaming\nproperty. In addition, a streaming joint decoding method is designed to improve\nASR accuracy. Experiments show that compared to standard neural transducers,\nthe proposed LS-Transducer gave a 10% relative WER reduction (WERR) for\nintra-domain Librispeech-100h data, as well as 17% and 19% relative WERRs on\ncross-domain TED-LIUM2 and AESRC2020 data with an adapted prediction network.", "published": "2023-07-06 16:01:10", "link": "http://arxiv.org/abs/2307.03088v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Recovering implicit pitch contours from formants in whispered speech", "abstract": "Whispered speech is characterised by a noise-like excitation that results in\nthe lack of fundamental frequency. Considering that prosodic phenomena such as\nintonation are perceived through f0 variation, the perception of whispered\nprosody is relatively difficult. At the same time, studies have shown that\nspeakers do attempt to produce intonation when whispering and that prosodic\nvariability is being transmitted, suggesting that intonation \"survives\" in\nwhispered formant structure. In this paper, we aim to estimate the way in which\nformant contours correlate with an \"implicit\" pitch contour in whisper, using a\nmachine learning model. We propose a two-step method: using a parallel corpus,\nwe first transform the whispered formants into their phonated equivalents using\na denoising autoencoder. We then analyse the formant contours to predict\nphonated pitch contour variation. We observe that our method is effective in\nestablishing a relationship between whispered and phonated formants and in\nuncovering implicit pitch contours in whisper.", "published": "2023-07-06 17:49:08", "link": "http://arxiv.org/abs/2307.03168v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong\n  General Audio Event Taggers", "abstract": "In this paper, we focus on Whisper, a recent automatic speech recognition\nmodel trained with a massive 680k hour labeled speech corpus recorded in\ndiverse conditions. We first show an interesting finding that while Whisper is\nvery robust against real-world background sounds (e.g., music), its audio\nrepresentation is actually not noise-invariant, but is instead highly\ncorrelated to non-speech sounds, indicating that Whisper recognizes speech\nconditioned on the noise type. With this finding, we build a unified audio\ntagging and speech recognition model Whisper-AT by freezing the backbone of\nWhisper, and training a lightweight audio tagging model on top of it. With <1%\nextra computational cost, Whisper-AT can recognize audio events, in addition to\nspoken text, in a single forward pass.", "published": "2023-07-06 17:58:28", "link": "http://arxiv.org/abs/2307.03183v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DSARSR: Deep Stacked Auto-encoders Enhanced Robust Speaker Recognition", "abstract": "Speaker recognition is a biometric modality that utilizes the speaker's\nspeech segments to recognize the identity, determining whether the test speaker\nbelongs to one of the enrolled speakers. In order to improve the robustness of\nthe i-vector framework on cross-channel conditions and explore the nova method\nfor applying deep learning to speaker recognition, the Stacked Auto-encoders\nare used to get the abstract extraction of the i-vector instead of applying\nPLDA. After pre-processing and feature extraction, the speaker and\nchannel-independent speeches are employed for UBM training. The UBM is then\nused to extract the i-vector of the enrollment and test speech. Unlike the\ntraditional i-vector framework, which uses linear discriminant analysis (LDA)\nto reduce dimension and increase the discrimination between speaker subspaces,\nthis research use stacked auto-encoders to reconstruct the i-vector with lower\ndimension and different classifiers can be chosen to achieve final\nclassification. The experimental results show that the proposed method achieves\nbetter performance than the state-of-the-art method.", "published": "2023-07-06 03:19:40", "link": "http://arxiv.org/abs/2307.02751v1", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Evaluating raw waveforms with deep learning frameworks for speech\n  emotion recognition", "abstract": "Speech emotion recognition is a challenging task in speech processing field.\nFor this reason, feature extraction process has a crucial importance to\ndemonstrate and process the speech signals. In this work, we represent a model,\nwhich feeds raw audio files directly into the deep neural networks without any\nfeature extraction stage for the recognition of emotions utilizing six\ndifferent data sets, EMO-DB, RAVDESS, TESS, CREMA, SAVEE, and TESS+RAVDESS. To\ndemonstrate the contribution of proposed model, the performance of traditional\nfeature extraction techniques namely, mel-scale spectogram, mel-frequency\ncepstral coefficients, are blended with machine learning algorithms, ensemble\nlearning methods, deep and hybrid deep learning techniques. Support vector\nmachine, decision tree, naive Bayes, random forests models are evaluated as\nmachine learning algorithms while majority voting and stacking methods are\nassessed as ensemble learning techniques. Moreover, convolutional neural\nnetworks, long short-term memory networks, and hybrid CNN- LSTM model are\nevaluated as deep learning techniques and compared with machine learning and\nensemble learning methods. To demonstrate the effectiveness of proposed model,\nthe comparison with state-of-the-art studies are carried out. Based on the\nexperiment results, CNN model excels existent approaches with 95.86% of\naccuracy for TESS+RAVDESS data set using raw audio files, thence determining\nthe new state-of-the-art. The proposed model performs 90.34% of accuracy for\nEMO-DB with CNN model, 90.42% of accuracy for RAVDESS with CNN model, 99.48% of\naccuracy for TESS with LSTM model, 69.72% of accuracy for CREMA with CNN model,\n85.76% of accuracy for SAVEE with CNN model in speaker-independent audio\ncategorization problems.", "published": "2023-07-06 07:27:59", "link": "http://arxiv.org/abs/2307.02820v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-visual End-to-end Multi-channel Speech Separation, Dereverberation\n  and Recognition", "abstract": "Accurate recognition of cocktail party speech containing overlapping\nspeakers, noise and reverberation remains a highly challenging task to date.\nMotivated by the invariance of visual modality to acoustic signal corruption,\nan audio-visual multi-channel speech separation, dereverberation and\nrecognition approach featuring a full incorporation of visual information into\nall system components is proposed in this paper. The efficacy of the video\ninput is consistently demonstrated in mask-based MVDR speech separation,\nDNN-WPE or spectral mapping (SpecM) based speech dereverberation front-end and\nConformer ASR back-end. Audio-visual integrated front-end architectures\nperforming speech separation and dereverberation in a pipelined or joint\nfashion via mask-based WPD are investigated. The error cost mismatch between\nthe speech enhancement front-end and ASR back-end components is minimized by\nend-to-end jointly fine-tuning using either the ASR cost function alone, or its\ninterpolation with the speech enhancement loss. Experiments were conducted on\nthe mixture overlapped and reverberant speech data constructed using simulation\nor replay of the Oxford LRS2 dataset. The proposed audio-visual multi-channel\nspeech separation, dereverberation and recognition systems consistently\noutperformed the comparable audio-only baseline by 9.1% and 6.2% absolute\n(41.7% and 36.0% relative) word error rate (WER) reductions. Consistent speech\nenhancement improvements were also obtained on PESQ, STOI and SRMR scores.", "published": "2023-07-06 10:50:46", "link": "http://arxiv.org/abs/2307.02909v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Transfer Learning for the Efficient Detection of COVID-19 from\n  Smartphone Audio Data", "abstract": "Disease detection from smartphone data represents an open research challenge\nin mobile health (m-health) systems. COVID-19 and its respiratory symptoms are\nan important case study in this area and their early detection is a potential\nreal instrument to counteract the pandemic situation. The efficacy of this\nsolution mainly depends on the performances of AI algorithms applied to the\ncollected data and their possible implementation directly on the users' mobile\ndevices. Considering these issues, and the limited amount of available data, in\nthis paper we present the experimental evaluation of 3 different deep learning\nmodels, compared also with hand-crafted features, and of two main approaches of\ntransfer learning in the considered scenario: both feature extraction and\nfine-tuning. Specifically, we considered VGGish, YAMNET, and\nL\\textsuperscript{3}-Net (including 12 different configurations) evaluated\nthrough user-independent experiments on 4 different datasets (13,447 samples in\ntotal). Results clearly show the advantages of L\\textsuperscript{3}-Net in all\nthe experimental settings as it overcomes the other solutions by 12.3\\% in\nterms of Precision-Recall AUC as features extractor, and by 10\\% when the model\nis fine-tuned. Moreover, we note that to fine-tune only the fully-connected\nlayers of the pre-trained models generally leads to worse performances, with an\naverage drop of 6.6\\% with respect to feature extraction. %highlighting the\nneed for further investigations. Finally, we evaluate the memory footprints of\nthe different models for their possible applications on commercial mobile\ndevices.", "published": "2023-07-06 13:19:27", "link": "http://arxiv.org/abs/2307.02975v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Track Mix Generation on Music Streaming Services using Transformers", "abstract": "This paper introduces Track Mix, a personalized playlist generation system\nreleased in 2022 on the music streaming service Deezer. Track Mix automatically\ngenerates \"mix\" playlists inspired by initial music tracks, allowing users to\ndiscover music similar to their favorite content. To generate these mixes, we\nconsider a Transformer model trained on millions of track sequences from user\nplaylists. In light of the growing popularity of Transformers in recent years,\nwe analyze the advantages, drawbacks, and technical challenges of using such a\nmodel for mix generation on the service, compared to a more traditional\ncollaborative filtering approach. Since its release, Track Mix has been\ngenerating playlists for millions of users daily, enhancing their music\ndiscovery experience on Deezer.", "published": "2023-07-06 15:10:29", "link": "http://arxiv.org/abs/2307.03045v1", "categories": ["cs.IR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
