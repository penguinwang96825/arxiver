{"title": "Conversational Machine Comprehension: a Literature Review", "abstract": "Conversational Machine Comprehension (CMC), a research track in\nconversational AI, expects the machine to understand an open-domain natural\nlanguage text and thereafter engage in a multi-turn conversation to answer\nquestions related to the text. While most of the research in Machine Reading\nComprehension (MRC) revolves around single-turn question answering (QA),\nmulti-turn CMC has recently gained prominence, thanks to the advancement in\nnatural language understanding via neural language models such as BERT and the\nintroduction of large-scale conversational datasets such as CoQA and QuAC. The\nrise in interest has, however, led to a flurry of concurrent publications, each\nwith a different yet structurally similar modeling approach and an inconsistent\nview of the surrounding literature. With the volume of model submissions to\nconversational datasets increasing every year, there exists a need to\nconsolidate the scattered knowledge in this domain to streamline future\nresearch. This literature review attempts at providing a holistic overview of\nCMC with an emphasis on the common trends across recently published models,\nspecifically in their approach to tackling conversational history. The review\nsynthesizes a generic framework for CMC models while highlighting the\ndifferences in recent approaches and intends to serve as a compendium of CMC\nfor future researchers.", "published": "2020-06-01 02:20:08", "link": "http://arxiv.org/abs/2006.00671v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Online Versus Offline NMT Quality: An In-depth Analysis on\n  English-German and German-English", "abstract": "We conduct in this work an evaluation study comparing offline and online\nneural machine translation architectures. Two sequence-to-sequence models:\nconvolutional Pervasive Attention (Elbayad et al. 2018) and attention-based\nTransformer (Vaswani et al. 2017) are considered. We investigate, for both\narchitectures, the impact of online decoding constraints on the translation\nquality through a carefully designed human evaluation on English-German and\nGerman-English language pairs, the latter being particularly sensitive to\nlatency constraints. The evaluation results allow us to identify the strengths\nand shortcomings of each model when we shift to the online setup.", "published": "2020-06-01 09:43:54", "link": "http://arxiv.org/abs/2006.00814v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient EUD Parsing", "abstract": "We present the system submission from the FASTPARSE team for the EUD Shared\nTask at IWPT 2020. We engaged with the task by focusing on efficiency. For this\nwe considered training costs and inference efficiency. Our models are a\ncombination of distilled neural dependency parsers and a rule-based system that\nprojects UD trees into EUD graphs. We obtained an average ELAS of 74.04 for our\nofficial submission, ranking 4th overall.", "published": "2020-06-01 10:31:56", "link": "http://arxiv.org/abs/2006.00838v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rhetoric, Logic, and Dialectic: Advancing Theory-based Argument Quality\n  Assessment in Natural Language Processing", "abstract": "Though preceding work in computational argument quality (AQ) mostly focuses\non assessing overall AQ, researchers agree that writers would benefit from\nfeedback targeting individual dimensions of argumentation theory. However, a\nlarge-scale theory-based corpus and corresponding computational models are\nmissing. We fill this gap by conducting an extensive analysis covering three\ndiverse domains of online argumentative writing and presenting GAQCorpus: the\nfirst large-scale English multi-domain (community Q&A forums, debate forums,\nreview forums) corpus annotated with theory-based AQ scores. We then propose\nthe first computational approaches to theory-based assessment, which can serve\nas strong baselines for future work. We demonstrate the feasibility of\nlarge-scale AQ annotation, show that exploiting relations between dimensions\nyields performance improvements, and explore the synergies between theory-based\nprediction and practical AQ assessment.", "published": "2020-06-01 10:39:50", "link": "http://arxiv.org/abs/2006.00843v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distilling Neural Networks for Greener and Faster Dependency Parsing", "abstract": "The carbon footprint of natural language processing research has been\nincreasing in recent years due to its reliance on large and inefficient neural\nnetwork implementations. Distillation is a network compression technique which\nattempts to impart knowledge from a large model to a smaller one. We use\nteacher-student distillation to improve the efficiency of the Biaffine\ndependency parser which obtains state-of-the-art performance with respect to\naccuracy and parsing speed (Dozat and Manning, 2017). When distilling to 20\\%\nof the original model's trainable parameters, we only observe an average\ndecrease of $\\sim$1 point for both UAS and LAS across a number of diverse\nUniversal Dependency treebanks while being 2.30x (1.19x) faster than the\nbaseline model on CPU (GPU) at inference time. We also observe a small increase\nin performance when compressing to 80\\% for some treebanks. Finally, through\ndistillation we attain a parser which is not only faster but also more accurate\nthan the fastest modern parser on the Penn Treebank.", "published": "2020-06-01 10:43:53", "link": "http://arxiv.org/abs/2006.00844v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals", "abstract": "A growing body of work makes use of probing to investigate the working of\nneural models, often considered black boxes. Recently, an ongoing debate\nemerged surrounding the limitations of the probing paradigm. In this work, we\npoint out the inability to infer behavioral conclusions from probing results\nand offer an alternative method that focuses on how the information is being\nused, rather than on what information is encoded. Our method, Amnesic Probing,\nfollows the intuition that the utility of a property for a given task can be\nassessed by measuring the influence of a causal intervention that removes it\nfrom the representation. Equipped with this new analysis tool, we can ask\nquestions that were not possible before, e.g. is part-of-speech information\nimportant for word prediction? We perform a series of analyses on BERT to\nanswer these types of questions. Our findings demonstrate that conventional\nprobing performance is not correlated to task importance, and we call for\nincreased scrutiny of claims that draw behavioral or causal conclusions from\nprobing results.", "published": "2020-06-01 15:00:11", "link": "http://arxiv.org/abs/2006.00995v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toxicity Detection: Does Context Really Matter?", "abstract": "Moderation is crucial to promoting healthy on-line discussions. Although\nseveral `toxicity' detection datasets and models have been published, most of\nthem ignore the context of the posts, implicitly assuming that comments maybe\njudged independently. We investigate this assumption by focusing on two\nquestions: (a) does context affect the human judgement, and (b) does\nconditioning on context improve performance of toxicity detection systems? We\nexperiment with Wikipedia conversations, limiting the notion of context to the\nprevious post in the thread and the discussion title. We find that context can\nboth amplify or mitigate the perceived toxicity of posts. Moreover, a small but\nsignificant subset of manually labeled posts (5% in one of our experiments) end\nup having the opposite toxicity labels if the annotators are not provided with\ncontext. Surprisingly, we also find no evidence that context actually improves\nthe performance of toxicity classifiers, having tried a range of classifiers\nand mechanisms to make them context aware. This points to the need for larger\ndatasets of comments annotated in context. We make our code and data publicly\navailable.", "published": "2020-06-01 15:03:48", "link": "http://arxiv.org/abs/2006.00998v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DocBank: A Benchmark Dataset for Document Layout Analysis", "abstract": "Document layout analysis usually relies on computer vision models to\nunderstand documents while ignoring textual information that is vital to\ncapture. Meanwhile, high quality labeled datasets with both visual and textual\ninformation are still insufficient. In this paper, we present \\textbf{DocBank},\na benchmark dataset that contains 500K document pages with fine-grained\ntoken-level annotations for document layout analysis. DocBank is constructed\nusing a simple yet effective way with weak supervision from the \\LaTeX{}\ndocuments available on the arXiv.com. With DocBank, models from different\nmodalities can be compared fairly and multi-modal approaches will be further\ninvestigated and boost the performance of document layout analysis. We build\nseveral strong baselines and manually split train/dev/test sets for evaluation.\nExperiment results show that models trained on DocBank accurately recognize the\nlayout information for a variety of documents. The DocBank dataset is publicly\navailable at \\url{https://github.com/doc-analysis/DocBank}.", "published": "2020-06-01 16:04:30", "link": "http://arxiv.org/abs/2006.01038v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is 42 the Answer to Everything in Subtitling-oriented Speech\n  Translation?", "abstract": "Subtitling is becoming increasingly important for disseminating information,\ngiven the enormous amounts of audiovisual content becoming available daily.\nAlthough Neural Machine Translation (NMT) can speed up the process of\ntranslating audiovisual content, large manual effort is still required for\ntranscribing the source language, and for spotting and segmenting the text into\nproper subtitles. Creating proper subtitles in terms of timing and segmentation\nhighly depends on information present in the audio (utterance duration, natural\npauses). In this work, we explore two methods for applying Speech Translation\n(ST) to subtitling: a) a direct end-to-end and b) a classical cascade approach.\nWe discuss the benefit of having access to the source language speech for\nimproving the conformity of the generated subtitles to the spatial and temporal\nsubtitling constraints and show that length is not the answer to everything in\nthe case of subtitling-oriented ST.", "published": "2020-06-01 17:02:28", "link": "http://arxiv.org/abs/2006.01080v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NSTM: Real-Time Query-Driven News Overview Composition at Bloomberg", "abstract": "Millions of news articles from hundreds of thousands of sources around the\nglobe appear in news aggregators every day. Consuming such a volume of news\npresents an almost insurmountable challenge. For example, a reader searching on\nBloomberg's system for news about the U.K. would find 10,000 articles on a\ntypical day. Apple Inc., the world's most journalistically covered company,\ngarners around 1,800 news articles a day.\n  We realized that a new kind of summarization engine was needed, one that\nwould condense large volumes of news into short, easy to absorb points. The\nsystem would filter out noise and duplicates to identify and summarize key news\nabout companies, countries or markets.\n  When given a user query, Bloomberg's solution, Key News Themes (or NSTM),\nleverages state-of-the-art semantic clustering techniques and novel\nsummarization methods to produce comprehensive, yet concise, digests to\ndramatically simplify the news consumption process.\n  NSTM is available to hundreds of thousands of readers around the world and\nserves thousands of requests daily with sub-second latency. At ACL 2020, we\nwill present a demo of NSTM.", "published": "2020-06-01 17:58:57", "link": "http://arxiv.org/abs/2006.01117v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lexical Normalization for Code-switched Data and its Effect on\n  POS-tagging", "abstract": "Lexical normalization, the translation of non-canonical data to standard\nlanguage, has shown to improve the performance of manynatural language\nprocessing tasks on social media. Yet, using multiple languages in one\nutterance, also called code-switching (CS), is frequently overlooked by these\nnormalization systems, despite its common use in social media. In this paper,\nwe propose three normalization models specifically designed to handle\ncode-switched data which we evaluate for two language pairs: Indonesian-English\n(Id-En) and Turkish-German (Tr-De). For the latter, we introduce novel\nnormalization layers and their corresponding language ID and POS tags for the\ndataset, and evaluate the downstream effect of normalization on POS tagging.\nResults show that our CS-tailored normalization models outperform Id-En state\nof the art and Tr-De monolingual models, and lead to 5.4% relative performance\nincrease for POS tagging as compared to unnormalized input.", "published": "2020-06-01 18:07:15", "link": "http://arxiv.org/abs/2006.01175v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Effective Contextual Language Modeling Framework for Speech\n  Summarization with Augmented Features", "abstract": "Tremendous amounts of multimedia associated with speech information are\ndriving an urgent need to develop efficient and effective automatic\nsummarization methods. To this end, we have seen rapid progress in applying\nsupervised deep neural network-based methods to extractive speech\nsummarization. More recently, the Bidirectional Encoder Representations from\nTransformers (BERT) model was proposed and has achieved record-breaking success\non many natural language processing (NLP) tasks such as question answering and\nlanguage understanding. In view of this, we in this paper contextualize and\nenhance the state-of-the-art BERT-based model for speech summarization, while\nits contributions are at least three-fold. First, we explore the incorporation\nof confidence scores into sentence representations to see if such an attempt\ncould help alleviate the negative effects caused by imperfect automatic speech\nrecognition (ASR). Secondly, we also augment the sentence embeddings obtained\nfrom BERT with extra structural and linguistic features, such as sentence\nposition and inverse document frequency (IDF) statistics. Finally, we validate\nthe effectiveness of our proposed method on a benchmark dataset, in comparison\nto several classic and celebrated speech summarization methods.", "published": "2020-06-01 18:27:48", "link": "http://arxiv.org/abs/2006.01189v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stance in Replies and Quotes (SRQ): A New Dataset For Learning Stance in\n  Twitter Conversations", "abstract": "Automated ways to extract stance (denying vs. supporting opinions) from\nconversations on social media are essential to advance opinion mining research.\nRecently, there is a renewed excitement in the field as we see new models\nattempting to improve the state-of-the-art. However, for training and\nevaluating the models, the datasets used are often small. Additionally, these\nsmall datasets have uneven class distributions, i.e., only a tiny fraction of\nthe examples in the dataset have favoring or denying stances, and most other\nexamples have no clear stance. Moreover, the existing datasets do not\ndistinguish between the different types of conversations on social media (e.g.,\nreplying vs. quoting on Twitter). Because of this, models trained on one event\ndo not generalize to other events.\n  In the presented work, we create a new dataset by labeling stance in\nresponses to posts on Twitter (both replies and quotes) on controversial\nissues. To the best of our knowledge, this is currently the largest\nhuman-labeled stance dataset for Twitter conversations with over 5200 stance\nlabels. More importantly, we designed a tweet collection methodology that\nfavors the selection of denial-type responses. This class is expected to be\nmore useful in the identification of rumors and determining antagonistic\nrelationships between users. Moreover, we include many baseline models for\nlearning the stance in conversations and compare the performance of various\nmodels. We show that combining data from replies and quotes decreases the\naccuracy of models indicating that the two modalities behave differently when\nit comes to stance learning.", "published": "2020-06-01 03:30:08", "link": "http://arxiv.org/abs/2006.00691v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sarcasm Detection using Context Separators in Online Discourse", "abstract": "Sarcasm is an intricate form of speech, where meaning is conveyed implicitly.\nBeing a convoluted form of expression, detecting sarcasm is an assiduous\nproblem. The difficulty in recognition of sarcasm has many pitfalls, including\nmisunderstandings in everyday communications, which leads us to an increasing\nfocus on automated sarcasm detection. In the second edition of the Figurative\nLanguage Processing (FigLang 2020) workshop, the shared task of sarcasm\ndetection released two datasets, containing responses along with their context\nsampled from Twitter and Reddit.\n  In this work, we use RoBERTa_large to detect sarcasm in both the datasets. We\nfurther assert the importance of context in improving the performance of\ncontextual word embedding based models by using three different types of inputs\n- Response-only, Context-Response, and Context-Response (Separated). We show\nthat our proposed architecture performs competitively for both the datasets. We\nalso show that the addition of a separation token between context and target\nresponse results in an improvement of 5.13% in the F1-score in the Reddit\ndataset.", "published": "2020-06-01 10:52:35", "link": "http://arxiv.org/abs/2006.00850v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Attention Word Embedding", "abstract": "Word embedding models learn semantically rich vector representations of words\nand are widely used to initialize natural processing language (NLP) models. The\npopular continuous bag-of-words (CBOW) model of word2vec learns a vector\nembedding by masking a given word in a sentence and then using the other words\nas a context to predict it. A limitation of CBOW is that it equally weights the\ncontext words when making a prediction, which is inefficient, since some words\nhave higher predictive value than others. We tackle this inefficiency by\nintroducing the Attention Word Embedding (AWE) model, which integrates the\nattention mechanism into the CBOW model. We also propose AWE-S, which\nincorporates subword information. We demonstrate that AWE and AWE-S outperform\nthe state-of-the-art word embedding models both on a variety of word similarity\ndatasets and when used for initialization of NLP models.", "published": "2020-06-01 14:47:48", "link": "http://arxiv.org/abs/2006.00988v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Aligning Faithful Interpretations with their Social Attribution", "abstract": "We find that the requirement of model interpretations to be faithful is vague\nand incomplete. With interpretation by textual highlights as a case-study, we\npresent several failure cases. Borrowing concepts from social science, we\nidentify that the problem is a misalignment between the causal chain of\ndecisions (causal attribution) and the attribution of human behavior to the\ninterpretation (social attribution). We re-formulate faithfulness as an\naccurate attribution of causality to the model, and introduce the concept of\naligned faithfulness: faithful causal chains that are aligned with their\nexpected social behavior. The two steps of causal attribution and social\nattribution together complete the process of explaining behavior. With this\nformalization, we characterize various failures of misaligned faithful\nhighlight interpretations, and propose an alternative causal chain to remedy\nthe issues. Finally, we implement highlight explanations of the proposed causal\nformat using contrastive explanations.", "published": "2020-06-01 16:45:38", "link": "http://arxiv.org/abs/2006.01067v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Emergence of Separable Manifolds in Deep Language Representations", "abstract": "Deep neural networks (DNNs) have shown much empirical success in solving\nperceptual tasks across various cognitive modalities. While they are only\nloosely inspired by the biological brain, recent studies report considerable\nsimilarities between representations extracted from task-optimized DNNs and\nneural populations in the brain. DNNs have subsequently become a popular model\nclass to infer computational principles underlying complex cognitive functions,\nand in turn, they have also emerged as a natural testbed for applying methods\noriginally developed to probe information in neural populations. In this work,\nwe utilize mean-field theoretic manifold analysis, a recent technique from\ncomputational neuroscience that connects geometry of feature representations\nwith linear separability of classes, to analyze language representations from\nlarge-scale contextual embedding models. We explore representations from\ndifferent model families (BERT, RoBERTa, GPT, etc.) and find evidence for\nemergence of linguistic manifolds across layer depth (e.g., manifolds for\npart-of-speech tags), especially in ambiguous data (i.e, words with multiple\npart-of-speech tags, or part-of-speech classes including many words). In\naddition, we find that the emergence of linear separability in these manifolds\nis driven by a combined reduction of manifolds' radius, dimensionality and\ninter-manifold correlations.", "published": "2020-06-01 17:23:44", "link": "http://arxiv.org/abs/2006.01095v4", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Encoding formulas as deep networks: Reinforcement learning for zero-shot\n  execution of LTL formulas", "abstract": "We demonstrate a reinforcement learning agent which uses a compositional\nrecurrent neural network that takes as input an LTL formula and determines\nsatisfying actions. The input LTL formulas have never been seen before, yet the\nnetwork performs zero-shot generalization to satisfy them. This is a novel form\nof multi-task learning for RL agents where agents learn from one diverse set of\ntasks and generalize to a new set of diverse tasks. The formulation of the\nnetwork enables this capacity to generalize. We demonstrate this ability in two\ndomains. In a symbolic domain, the agent finds a sequence of letters that is\naccepted. In a Minecraft-like environment, the agent finds a sequence of\nactions that conform to the formula. While prior work could learn to execute\none formula reliably given examples of that formula, we demonstrate how to\nencode all formulas reliably. This could form the basis of new multitask agents\nthat discover sub-tasks and execute them without any additional training, as\nwell as the agents which follow more complex linguistic commands. The\nstructures required for this generalization are specific to LTL formulas, which\nopens up an interesting theoretical question: what structures are required in\nneural networks for zero-shot generalization to different logics?", "published": "2020-06-01 17:50:20", "link": "http://arxiv.org/abs/2006.01110v2", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Hybrid Improved Document-level Embedding (HIDE)", "abstract": "In recent times, word embeddings are taking a significant role in sentiment\nanalysis. As the generation of word embeddings needs huge corpora, many\napplications use pretrained embeddings. In spite of the success, word\nembeddings suffers from certain drawbacks such as it does not capture sentiment\ninformation of a word, contextual information in terms of parts of speech tags\nand domain-specific information. In this work we propose HIDE a Hybrid Improved\nDocument level Embedding which incorporates domain information, parts of speech\ninformation and sentiment information into existing word embeddings such as\nGloVe and Word2Vec. It combine improved word embeddings into document level\nembeddings. Further, Latent Semantic Analysis (LSA) has been used to represent\ndocuments as a vectors. HIDE is generated, combining LSA and document level\nembeddings, which is computed from improved word embeddings. We test HIDE with\nsix different datasets and shown considerable improvement over the accuracy of\nexisting pretrained word vectors such as GloVe and Word2Vec. We further compare\nour work with two existing document level sentiment analysis approaches. HIDE\nperforms better than existing systems.", "published": "2020-06-01 19:09:13", "link": "http://arxiv.org/abs/2006.01203v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BERT-based Ensembles for Modeling Disclosure and Support in\n  Conversational Social Media Text", "abstract": "There is a growing interest in understanding how humans initiate and hold\nconversations. The affective understanding of conversations focuses on the\nproblem of how speakers use emotions to react to a situation and to each other.\nIn the CL-Aff Shared Task, the organizers released Get it #OffMyChest dataset,\nwhich contains Reddit comments from casual and confessional conversations,\nlabeled for their disclosure and supportiveness characteristics. In this paper,\nwe introduce a predictive ensemble model exploiting the finetuned\ncontextualized word embeddings, RoBERTa and ALBERT. We show that our model\noutperforms the base models in all considered metrics, achieving an improvement\nof $3\\%$ in the F1 score. We further conduct statistical analysis and outline\ndeeper insights into the given dataset while providing a new characterization\nof impact for the dataset.", "published": "2020-06-01 19:52:01", "link": "http://arxiv.org/abs/2006.01222v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "An Effectiveness Metric for Ordinal Classification: Formal Properties\n  and Experimental Results", "abstract": "In Ordinal Classification tasks, items have to be assigned to classes that\nhave a relative ordering, such as positive, neutral, negative in sentiment\nanalysis. Remarkably, the most popular evaluation metrics for ordinal\nclassification tasks either ignore relevant information (for instance,\nprecision/recall on each of the classes ignores their relative ordering) or\nassume additional information (for instance, Mean Average Error assumes\nabsolute distances between classes). In this paper we propose a new metric for\nOrdinal Classification, Closeness Evaluation Measure, that is rooted on\nMeasurement Theory and Information Theory. Our theoretical analysis and\nexperimental results over both synthetic data and data from NLP shared tasks\nindicate that the proposed metric captures quality aspects from different\ntraditional tasks simultaneously. In addition, it generalizes some popular\nclassification (nominal scale) and error minimization (interval scale) metrics,\ndepending on the measurement scale in which it is instantiated.", "published": "2020-06-01 20:35:46", "link": "http://arxiv.org/abs/2006.01245v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Context-based Transformer Models for Answer Sentence Selection", "abstract": "An important task for the design of Question Answering systems is the\nselection of the sentence containing (or constituting) the answer from\ndocuments relevant to the asked question. Most previous work has only used the\ntarget sentence to compute its score with the question as the models were not\npowerful enough to also effectively encode additional contextual information.\nIn this paper, we analyze the role of the contextual information in the\nsentence selection task, proposing a Transformer based architecture that\nleverages two types of contexts, local and global. The former describes the\nparagraph containing the sentence, aiming at solving implicit references,\nwhereas the latter describes the entire document containing the candidate\nsentence, providing content-based information. The results on three different\nbenchmarks show that the combination of local and global contexts in a\nTransformer model significantly improves the accuracy in Answer Sentence\nSelection.", "published": "2020-06-01 21:52:19", "link": "http://arxiv.org/abs/2006.01285v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Translating Natural Language Instructions for Behavioral Robot\n  Navigation with a Multi-Head Attention Mechanism", "abstract": "We propose a multi-head attention mechanism as a blending layer in a neural\nnetwork model that translates natural language to a high level behavioral\nlanguage for indoor robot navigation. We follow the framework established by\n(Zang et al., 2018a) that proposes the use of a navigation graph as a knowledge\nbase for the task. Our results show significant performance gains when\ntranslating instructions on previously unseen environments, therefore,\nimproving the generalization capabilities of the model.", "published": "2020-06-01 03:49:43", "link": "http://arxiv.org/abs/2006.00697v3", "categories": ["cs.LG", "cs.CL", "cs.RO", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Streaming Language Identification using Combination of Acoustic\n  Representations and ASR Hypotheses", "abstract": "This paper presents our modeling and architecture approaches for building a\nhighly accurate low-latency language identification system to support\nmultilingual spoken queries for voice assistants. A common approach to solve\nmultilingual speech recognition is to run multiple monolingual ASR systems in\nparallel and rely on a language identification (LID) component that detects the\ninput language. Conventionally, LID relies on acoustic only information to\ndetect input language. We propose an approach that learns and combines acoustic\nlevel representations with embeddings estimated on ASR hypotheses resulting in\nup to 50% relative reduction of identification error rate, compared to a model\nthat uses acoustic only features. Furthermore, to reduce the processing cost\nand latency, we exploit a streaming architecture to identify the spoken\nlanguage early when the system reaches a predetermined confidence level,\nalleviating the need to run multiple ASR systems until the end of input query.\nThe combined acoustic and text LID, coupled with our proposed streaming runtime\narchitecture, results in an average of 1500ms early identification for more\nthan 50% of utterances, with almost no degradation in accuracy. We also show\nimproved results by adopting a semi-supervised learning (SSL) technique using\nthe newly proposed model architecture as a teacher model.", "published": "2020-06-01 04:08:55", "link": "http://arxiv.org/abs/2006.00703v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Influence via Ethos: On the Persuasive Power of Reputation in\n  Deliberation Online", "abstract": "Deliberation among individuals online plays a key role in shaping the\nopinions that drive votes, purchases, donations and other critical offline\nbehavior. Yet, the determinants of opinion-change via persuasion in\ndeliberation online remain largely unexplored. Our research examines the\npersuasive power of $\\textit{ethos}$ -- an individual's \"reputation\" -- using a\n7-year panel of over a million debates from an argumentation platform\ncontaining explicit indicators of successful persuasion. We identify the causal\neffect of reputation on persuasion by constructing an instrument for reputation\nfrom a measure of past debate competition, and by controlling for unstructured\nargument text using neural models of language in the double machine-learning\nframework. We find that an individual's reputation significantly impacts their\npersuasion rate above and beyond the validity, strength and presentation of\ntheir arguments. In our setting, we find that having 10 additional reputation\npoints causes a 31% increase in the probability of successful persuasion over\nthe platform average. We also find that the impact of reputation is moderated\nby characteristics of the argument content, in a manner consistent with a\ntheoretical model that attributes the persuasive power of reputation to\nheuristic information-processing under cognitive overload. We discuss\nmanagerial implications for platforms that facilitate deliberative\ndecision-making for public and private organizations online.", "published": "2020-06-01 04:25:40", "link": "http://arxiv.org/abs/2006.00707v1", "categories": ["econ.EM", "cs.CL", "stat.AP"], "primary_category": "econ.EM"}
{"title": "Learning to Recognize Code-switched Speech Without Forgetting\n  Monolingual Speech Recognition", "abstract": "Recently, there has been significant progress made in Automatic Speech\nRecognition (ASR) of code-switched speech, leading to gains in accuracy on\ncode-switched datasets in many language pairs. Code-switched speech co-occurs\nwith monolingual speech in one or both languages being mixed. In this work, we\nshow that fine-tuning ASR models on code-switched speech harms performance on\nmonolingual speech. We point out the need to optimize models for code-switching\nwhile also ensuring that monolingual performance is not sacrificed. Monolingual\nmodels may be trained on thousands of hours of speech which may not be\navailable for re-training a new model. We propose using the Learning Without\nForgetting (LWF) framework for code-switched ASR when we only have access to a\nmonolingual model and do not have the data it was trained on. We show that it\nis possible to train models using this framework that perform well on both\ncode-switched and monolingual test sets. In cases where we have access to\nmonolingual training data as well, we propose regularization strategies for\nfine-tuning models for code-switching without sacrificing monolingual accuracy.\nWe report improvements in Word Error Rate (WER) in monolingual and\ncode-switched test sets compared to baselines that use pooled data and simple\nfine-tuning.", "published": "2020-06-01 08:16:24", "link": "http://arxiv.org/abs/2006.00782v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Transcription-Enriched Joint Embeddings for Spoken Descriptions of\n  Images and Videos", "abstract": "In this work, we propose an effective approach for training unique embedding\nrepresentations by combining three simultaneous modalities: image and spoken\nand textual narratives. The proposed methodology departs from a baseline system\nthat spawns a embedding space trained with only spoken narratives and image\ncues. Our experiments on the EPIC-Kitchen and Places Audio Caption datasets\nshow that introducing the human-generated textual transcriptions of the spoken\nnarratives helps to the training procedure yielding to get better embedding\nrepresentations. The triad speech, image and words allows for a better estimate\nof the point embedding and show an improving of the performance within tasks\nlike image and speech retrieval, even when text third modality, text, is not\npresent in the task.", "published": "2020-06-01 08:18:15", "link": "http://arxiv.org/abs/2006.00785v1", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.MM"], "primary_category": "cs.CV"}
{"title": "COVID-19: Social Media Sentiment Analysis on Reopening", "abstract": "The novel coronavirus (COVID-19) pandemic is the most talked topic in social\nmedia platforms in 2020. People are using social media such as Twitter to\nexpress their opinion and share information on a number of issues related to\nthe COVID-19 in this stay at home order. In this paper, we investigate the\nsentiment and emotion of peoples in the United States on the subject of\nreopening. We choose the social media platform Twitter for our analysis and\nstudy the Tweets to discover the sentimental perspective, emotional\nperspective, and triggering words towards the reopening. During this COVID-19\npandemic, researchers have made some analysis on various social media dataset\nregarding lockdown and stay at home. However, in our analysis, we are\nparticularly interested to analyse public sentiment on reopening. Our major\nfinding is that when all states resorted to lockdown in March, people showed\ndominant emotion of fear, but as reopening starts people have less fear. While\nthis may be true, due to this reopening phase daily positive cases are rising\ncompared to the lockdown situation. Overall, people have a less negative\nsentiment towards the situation of reopening.", "published": "2020-06-01 09:15:02", "link": "http://arxiv.org/abs/2006.00804v1", "categories": ["cs.SI", "cs.CL", "cs.IR"], "primary_category": "cs.SI"}
{"title": "A Neural Network Model of Lexical Competition during Infant Spoken Word\n  Recognition", "abstract": "Visual world studies show that upon hearing a word in a target-absent visual\ncontext containing related and unrelated items, toddlers and adults briefly\ndirect their gaze towards phonologically related items, before shifting towards\nsemantically and visually related ones. We present a neural network model that\nprocesses dynamic unfolding phonological representations and maps them to\nstatic internal semantic and visual representations. The model, trained on\nrepresentations derived from real corpora, simulates this early phonological\nover semantic/visual preference. Our results support the hypothesis that\nincremental unfolding of a spoken word is in itself sufficient to account for\nthe transient preference for phonological competitors over both unrelated and\nsemantically and visually related ones. Phonological representations mapped\ndynamically in a bottom-up fashion to semantic-visual representations capture\nthe early phonological preference effects reported in a visual world task. The\nsemantic-visual preference observed later in such a trial does not require\ntop-down feedback from a semantic or visual system.", "published": "2020-06-01 15:04:11", "link": "http://arxiv.org/abs/2006.00999v1", "categories": ["cs.CL", "cs.LG", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Probing Emergent Semantics in Predictive Agents via Question Answering", "abstract": "Recent work has shown how predictive modeling can endow agents with rich\nknowledge of their surroundings, improving their ability to act in complex\nenvironments. We propose question-answering as a general paradigm to decode and\nunderstand the representations that such agents develop, applying our method to\ntwo recent approaches to predictive modeling -action-conditional CPC (Guo et\nal., 2018) and SimCore (Gregor et al., 2019). After training agents with these\npredictive objectives in a visually-rich, 3D environment with an assortment of\nobjects, colors, shapes, and spatial configurations, we probe their internal\nstate representations with synthetic (English) questions, without\nbackpropagating gradients from the question-answering decoder into the agent.\nThe performance of different agents when probed this way reveals that they\nlearn to encode factual, and seemingly compositional, information about\nobjects, properties and spatial relations from their physical environment. Our\napproach is intuitive, i.e. humans can easily interpret responses of the model\nas opposed to inspecting continuous vectors, and model-agnostic, i.e.\napplicable to any modeling approach. By revealing the implicit knowledge of\nobjects, quantities, properties and relations acquired by agents as they learn,\nquestion-conditional agent probing can stimulate the design and development of\nstronger predictive learning objectives.", "published": "2020-06-01 15:27:36", "link": "http://arxiv.org/abs/2006.01016v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Cascaded Text Generation with Markov Transformers", "abstract": "The two dominant approaches to neural text generation are fully\nautoregressive models, using serial beam search decoding, and\nnon-autoregressive models, using parallel decoding with no output dependencies.\nThis work proposes an autoregressive model with sub-linear parallel time\ngeneration. Noting that conditional random fields with bounded context can be\ndecoded in parallel, we propose an efficient cascaded decoding approach for\ngenerating high-quality output. To parameterize this cascade, we introduce a\nMarkov transformer, a variant of the popular fully autoregressive model that\nallows us to simultaneously decode with specific autoregressive context\ncutoffs. This approach requires only a small modification from standard\nautoregressive training, while showing competitive accuracy/speed tradeoff\ncompared to existing methods on five machine translation datasets.", "published": "2020-06-01 17:52:15", "link": "http://arxiv.org/abs/2006.01112v2", "categories": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Quantum circuit design for universal distribution using a superposition\n  of classical automata", "abstract": "In this research, we present a quantum circuit design and implementation for\na parallel universal linear bounded automata. This circuit is able to\naccelerate the inference of algorithmic structures in data for discovering\ncausal generative models. The computation model is practically restricted in\ntime and space resources. A classical exhaustive enumeration of all possible\nprograms on the automata is shown for a couple of example cases. The precise\nquantum circuit design that allows executing a superposition of programs, along\nwith a superposition of inputs as in the standard quantum Turing machine\nformulation, is presented. This is the first time, a superposition of classical\nautomata is implemented on the circuit model of quantum computation, having the\ncorresponding mechanistic parts of a classical Turing machine. The\nsuperposition of programs allows our model to be used for experimenting with\nthe space of program-output behaviors in algorithmic information theory. Our\nimplementations on OpenQL and Qiskit quantum programming language is copy-left\nand is publicly available on GitHub.", "published": "2020-06-01 14:47:28", "link": "http://arxiv.org/abs/2006.00987v2", "categories": ["quant-ph", "cs.CL", "cs.ET", "cs.IT", "math.IT"], "primary_category": "quant-ph"}
{"title": "Phase-aware Single-stage Speech Denoising and Dereverberation with U-Net", "abstract": "In this work, we tackle a denoising and dereverberation problem with a\nsingle-stage framework. Although denoising and dereverberation may be\nconsidered two separate challenging tasks, and thus, two modules are typically\nrequired for each task, we show that a single deep network can be shared to\nsolve the two problems. To this end, we propose a new masking method called\nphase-aware beta-sigmoid mask (PHM), which reuses the estimated magnitude\nvalues to estimate the clean phase by respecting the triangle inequality in the\ncomplex domain between three signal components such as mixture, source and the\nrest. Two PHMs are used to deal with direct and reverberant source, which\nallows controlling the proportion of reverberation in the enhanced speech at\ninference time. In addition, to improve the speech enhancement performance, we\npropose a new time-domain loss function and show a reasonable performance gain\ncompared to MSE loss in the complex domain. Finally, to achieve a real-time\ninference, an optimization strategy for U-Net is proposed which significantly\nreduces the computational overhead up to 88.9% compared to the na\\\"ive version.", "published": "2020-06-01 03:23:51", "link": "http://arxiv.org/abs/2006.00687v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Evaluation of CNN-based Automatic Music Tagging Models", "abstract": "Recent advances in deep learning accelerated the development of content-based\nautomatic music tagging systems. Music information retrieval (MIR) researchers\nproposed various architecture designs, mainly based on convolutional neural\nnetworks (CNNs), that achieve state-of-the-art results in this multi-label\nbinary classification task. However, due to the differences in experimental\nsetups followed by researchers, such as using different dataset splits and\nsoftware versions for evaluation, it is difficult to compare the proposed\narchitectures directly with each other. To facilitate further research, in this\npaper we conduct a consistent evaluation of different music tagging models on\nthree datasets (MagnaTagATune, Million Song Dataset, and MTG-Jamendo) and\nprovide reference results using common evaluation metrics (ROC-AUC and PR-AUC).\nFurthermore, all the models are evaluated with perturbed inputs to investigate\nthe generalization capabilities concerning time stretch, pitch shift, dynamic\nrange compression, and addition of white noise. For reproducibility, we provide\nthe PyTorch implementations with the pre-trained models.", "published": "2020-06-01 07:03:26", "link": "http://arxiv.org/abs/2006.00751v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A time-scale modification dataset with subjective quality labels", "abstract": "Time Scale Modification (TSM) is a well-researched field; however, no\neffective objective measure of quality exists. This paper details the creation,\nsubjective evaluation, and analysis of a dataset for use in the development of\nan objective measure of quality for TSM. Comprised of two parts, the training\ncomponent contains 88 source files processed using six TSM methods at 10 time\nscales, while the testing component contains 20 source files processed using\nthree additional methods at four time scales. The source material contains\nspeech, solo harmonic and percussive instruments, sound effects, and a range of\nmusic genres. Ratings (42 529) were collected from 633 sessions using\nlaboratory and remote collection methods. Analysis of results shows no\ncorrelation between age and quality of rating; expert and non-expert listeners\nto be equivalent; minor differences between participants with and without\nhearing issues; and minimal differences between testing modalities. A\ncomparison of published objective measures and subjective scores shows the\nobjective measures to be poor indicators of subjective quality. Initial results\nfor a retrained objective measure of quality are presented with results\napproaching average root mean squared error loss and Pearson correlation values\nof subjective sessions. The labeled dataset is available at\nhttp://ieee-dataport.org/1987.", "published": "2020-06-01 10:48:38", "link": "http://arxiv.org/abs/2006.00848v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Similarity-and-Independence-Aware Beamformer: Method for Target Source\n  Extraction using Magnitude Spectrogram as Reference", "abstract": "This study presents a novel method for source extraction, referred to as the\nsimilarity-and-independence-aware beamformer (SIBF). The SIBF extracts the\ntarget signal using a rough magnitude spectrogram as the reference signal. The\nadvantage of the SIBF is that it can obtain an accurate target signal, compared\nto the spectrogram generated by target-enhancing methods such as the speech\nenhancement based on deep neural networks (DNNs). For the extraction, we extend\nthe framework of the deflationary independent component analysis, by\nconsidering the similarity between the reference and extracted target, as well\nas the mutual independence of all potential sources. To solve the extraction\nproblem by maximum-likelihood estimation, we introduce two source model types\nthat can reflect the similarity. The experimental results from the CHiME3\ndataset show that the target signal extracted by the SIBF is more accurate than\nthe reference signal generated by the DNN.\n  Index Terms: semiblind source separation, similarity-and-independence-aware\nbeamformer, deflationary independent component analysis, source model", "published": "2020-06-01 08:03:09", "link": "http://arxiv.org/abs/2006.00772v3", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "High-Fidelity Audio Generation and Representation Learning with Guided\n  Adversarial Autoencoder", "abstract": "Unsupervised disentangled representation learning from the unlabelled audio\ndata, and high fidelity audio generation have become two linchpins in the\nmachine learning research fields. However, the representation learned from an\nunsupervised setting does not guarantee its' usability for any downstream task\nat hand, which can be a wastage of the resources, if the training was conducted\nfor that particular posterior job. Also, during the representation learning, if\nthe model is highly biased towards the downstream task, it losses its\ngeneralisation capability which directly benefits the downstream job but the\nability to scale it to other related task is lost. Therefore, to fill this gap,\nwe propose a new autoencoder based model named \"Guided Adversarial Autoencoder\n(GAAE)\", which can learn both post-task-specific representations and the\ngeneral representation capturing the factors of variation in the training data\nleveraging a small percentage of labelled samples; thus, makes it suitable for\nfuture related tasks. Furthermore, our proposed model can generate audio with\nsuperior quality, which is indistinguishable from the real audio samples.\nHence, with the extensive experimental results, we have demonstrated that by\nharnessing the power of the high-fidelity audio generation, the proposed GAAE\nmodel can learn powerful representation from unlabelled dataset leveraging a\nfewer percentage of labelled data as supervision/guidance.", "published": "2020-06-01 12:19:32", "link": "http://arxiv.org/abs/2006.00877v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Tonal harmony and the topology of dynamical score networks", "abstract": "We introduce the concept of dynamical score networks for the representation\nand analysis of tonal compositions: a score is interpreted as a dynamical\nnetwork where every chord is a node and each progression links successive\nchords. This network can be viewed as a time series of a non-stationary signal,\nand as such, it can be partitioned for the automatic identification of tonal\nregions using time series analysis and change point detection without relying\non comparisons with pre-determined reference sets or extensive corpora. We\ndemonstrate that the essential features of tonal harmony, centricity,\nreferentiality, directedness and hierarchy, emerge naturally from the network\ntopology and its scale-free properties. Finally, solving for the minimal length\npath through a route optimization algorithm on these graphs provides an\nabstraction of harmonic sequences that can be generalized for the conception of\ngenerative models of tonal compositional design.", "published": "2020-06-01 15:59:56", "link": "http://arxiv.org/abs/2006.01033v2", "categories": ["cs.SD", "cs.SI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Constrained Variational Autoencoder for improving EEG based Speech\n  Recognition Systems", "abstract": "In this paper we introduce a recurrent neural network (RNN) based variational\nautoencoder (VAE) model with a new constrained loss function that can generate\nmore meaningful electroencephalography (EEG) features from raw EEG features to\nimprove the performance of EEG based speech recognition systems. We demonstrate\nthat both continuous and isolated speech recognition systems trained and tested\nusing EEG features generated from raw EEG features using our VAE model results\nin improved performance and we demonstrate our results for a limited English\nvocabulary consisting of 30 unique sentences for continuous speech recognition\nand for an English vocabulary consisting of 2 unique sentences for isolated\nspeech recognition. We compare our method with another recently introduced\nmethod described by authors in [1] to improve the performance of EEG based\ncontinuous speech recognition systems and we demonstrate that our method\noutperforms their method as vocabulary size increases when trained and tested\nusing the same data set. Even though we demonstrate results only for automatic\nspeech recognition (ASR) experiments in this paper, the proposed VAE model with\nconstrained loss function can be extended to a variety of other EEG based brain\ncomputer interface (BCI) applications.", "published": "2020-06-01 06:03:50", "link": "http://arxiv.org/abs/2006.02902v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
