{"title": "Neural Network based Deep Transfer Learning for Cross-domain Dependency\n  Parsing", "abstract": "In this paper, we describe the details of the neural dependency parser\nsub-mitted by our team to the NLPCC 2019 Shared Task of Semi-supervised do-main\nadaptation subtask on Cross-domain Dependency Parsing. Our system is based on\nthe stack-pointer networks(STACKPTR). Considering the im-portance of context,\nwe utilize self-attention mechanism for the representa-tion vectors to capture\nthe meaning of words. In addition, to adapt three dif-ferent domains, we\nutilize neural network based deep transfer learning which transfers the\npre-trained partial network in the source domain to be a part of deep neural\nnetwork in the three target domains (product comments, product blogs and web\nfiction) respectively. Results on the three target domains demonstrate that our\nmodel performs competitively.", "published": "2019-08-08 01:16:34", "link": "http://arxiv.org/abs/1908.02895v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Neural Language Representations Learn Physical Commonsense?", "abstract": "Humans understand language based on the rich background knowledge about how\nthe physical world works, which in turn allows us to reason about the physical\nworld through language. In addition to the properties of objects (e.g., boats\nrequire fuel) and their affordances, i.e., the actions that are applicable to\nthem (e.g., boats can be driven), we can also reason about if-then inferences\nbetween what properties of objects imply the kind of actions that are\napplicable to them (e.g., that if we can drive something then it likely\nrequires fuel).\n  In this paper, we investigate the extent to which state-of-the-art neural\nlanguage representations, trained on a vast amount of natural language text,\ndemonstrate physical commonsense reasoning. While recent advancements of neural\nlanguage models have demonstrated strong performance on various types of\nnatural language inference tasks, our study based on a dataset of over 200k\nnewly collected annotations suggests that neural language representations still\nonly learn associations that are explicitly written down.", "published": "2019-08-08 01:41:16", "link": "http://arxiv.org/abs/1908.02899v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating Noisy Inputs for Question Answering", "abstract": "Natural language processing systems are often downstream of unreliable\ninputs: machine translation, optical character recognition, or speech\nrecognition. For instance, virtual assistants can only answer your questions\nafter understanding your speech. We investigate and mitigate the effects of\nnoise from Automatic Speech Recognition systems on two factoid Question\nAnswering (QA) tasks. Integrating confidences into the model and forced\ndecoding of unknown words are empirically shown to improve the accuracy of\ndownstream neural QA systems. We create and train models on a synthetic corpus\nof over 500,000 noisy sentences and evaluate on two human corpora from Quizbowl\nand Jeopardy! competitions.", "published": "2019-08-08 03:31:11", "link": "http://arxiv.org/abs/1908.02914v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Test Suite and Manual Evaluation of Document-Level NMT at WMT19", "abstract": "As the quality of machine translation rises and neural machine translation\n(NMT) is moving from sentence to document level translations, it is becoming\nincreasingly difficult to evaluate the output of translation systems.\n  We provide a test suite for WMT19 aimed at assessing discourse phenomena of\nMT systems participating in the News Translation Task. We have manually checked\nthe outputs and identified types of translation errors that are relevant to\ndocument-level translation.", "published": "2019-08-08 12:50:23", "link": "http://arxiv.org/abs/1908.03043v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Key Fact as Pivot: A Two-Stage Model for Low Resource Table-to-Text\n  Generation", "abstract": "Table-to-text generation aims to translate the structured data into the\nunstructured text. Most existing methods adopt the encoder-decoder framework to\nlearn the transformation, which requires large-scale training samples. However,\nthe lack of large parallel data is a major practical problem for many domains.\nIn this work, we consider the scenario of low resource table-to-text\ngeneration, where only limited parallel data is available. We propose a novel\nmodel to separate the generation into two stages: key fact prediction and\nsurface realization. It first predicts the key facts from the tables, and then\ngenerates the text with the key facts. The training of key fact prediction\nneeds much fewer annotated data, while surface realization can be trained with\npseudo parallel corpus. We evaluate our model on a biography generation\ndataset. Our model can achieve $27.34$ BLEU score with only $1,000$ parallel\ndata, while the baseline model only obtain the performance of $9.71$ BLEU\nscore.", "published": "2019-08-08 13:41:31", "link": "http://arxiv.org/abs/1908.03067v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Image Captioning using Facial Expression and Attention", "abstract": "Benefiting from advances in machine vision and natural language processing\ntechniques, current image captioning systems are able to generate detailed\nvisual descriptions. For the most part, these descriptions represent an\nobjective characterisation of the image, although some models do incorporate\nsubjective aspects related to the observer's view of the image, such as\nsentiment; current models, however, usually do not consider the emotional\ncontent of images during the caption generation process. This paper addresses\nthis issue by proposing novel image captioning models which use facial\nexpression features to generate image captions. The models generate image\ncaptions using long short-term memory networks applying facial features in\naddition to other visual features at different time steps. We compare a\ncomprehensive collection of image captioning models with and without facial\nfeatures using all standard evaluation metrics. The evaluation metrics indicate\nthat applying facial features with an attention mechanism achieves the best\nperformance, showing more expressive and more correlated image captions, on an\nimage caption dataset extracted from the standard Flickr 30K dataset,\nconsisting of around 11K images containing faces. An analysis of the generated\ncaptions finds that, perhaps unexpectedly, the improvement in caption quality\nappears to come not from the addition of adjectives linked to emotional aspects\nof the images, but from more variety in the actions described in the captions.", "published": "2019-08-08 04:07:39", "link": "http://arxiv.org/abs/1908.02923v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Towards Generating Stylized Image Captions via Adversarial Training", "abstract": "While most image captioning aims to generate objective descriptions of\nimages, the last few years have seen work on generating visually grounded image\ncaptions which have a specific style (e.g., incorporating positive or negative\nsentiment). However, because the stylistic component is typically the last part\nof training, current models usually pay more attention to the style at the\nexpense of accurate content description. In addition, there is a lack of\nvariability in terms of the stylistic aspects. To address these issues, we\npropose an image captioning model called ATTEND-GAN which has two core\ncomponents: first, an attention-based caption generator to strongly correlate\ndifferent parts of an image with different parts of a caption; and second, an\nadversarial training mechanism to assist the caption generator to add diverse\nstylistic components to the generated captions. Because of these components,\nATTEND-GAN can generate correlated captions as well as more human-like\nvariability of stylistic patterns. Our system outperforms the state-of-the-art\nas well as a collection of our baseline models. A linguistic analysis of the\ngenerated captions demonstrates that captions generated using ATTEND-GAN have a\nwider range of stylistic adjectives and adjective-noun pairs.", "published": "2019-08-08 06:25:38", "link": "http://arxiv.org/abs/1908.02943v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Your Stance is Exposed! Analysing Possible Factors for Stance Detection\n  on Social Media", "abstract": "To what extent user's stance towards a given topic could be inferred? Most of\nthe studies on stance detection have focused on analysing user's posts on a\ngiven topic to predict the stance. However, the stance in social media can be\ninferred from a mixture of signals that might reflect user's beliefs including\nposts and online interactions. This paper examines various online features of\nusers to detect their stance towards different topics. We compare multiple set\nof features, including on-topic content, network interactions, user's\npreferences, and online network connections. Our objective is to understand the\nonline signals that can reveal the users' stance. Experimentation is applied on\ntweets dataset from the SemEval stance detection task, which covers five\ntopics. Results show that stance of a user can be detected with multiple\nsignals of user's online activity, including their posts on the topic, the\nnetwork they interact with or follow, the websites they visit, and the content\nthey like. The performance of the stance modelling using different network\nfeatures are comparable with the state-of-the-art reported model that used\ntextual content only. In addition, combining network and content features leads\nto the highest reported performance to date on the SemEval dataset with\nF-measure of 72.49%. We further present an extensive analysis to show how these\ndifferent set of features can reveal stance. Our findings have distinct privacy\nimplications, where they highlight that stance is strongly embedded in user's\nonline social network that, in principle, individuals can be profiled from\ntheir interactions and connections even when they do not post about the topic.", "published": "2019-08-08 16:18:30", "link": "http://arxiv.org/abs/1908.03146v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Assessing Sentiment of the Expressed Stance on Social Media", "abstract": "Stance detection is the task of inferring viewpoint towards a given topic or\nentity either being supportive or opposing. One may express a viewpoint towards\na topic by using positive or negative language. This paper examines how the\nstance is being expressed in social media according to the sentiment polarity.\nThere has been a noticeable misconception of the similarity between the stance\nand sentiment when it comes to viewpoint discovery, where negative sentiment is\nassumed to mean against stance, and positive sentiment means in-favour stance.\nTo analyze the relation between stance and sentiment, we construct a new\ndataset with four topics and examine how people express their viewpoint with\nregards these topics. We validate our results by carrying a further analysis of\nthe popular stance benchmark SemEval stance dataset. Our analyses reveal that\nsentiment and stance are not highly aligned, and hence the simple sentiment\npolarity cannot be used solely to denote a stance toward a given topic.", "published": "2019-08-08 17:21:16", "link": "http://arxiv.org/abs/1908.03181v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "On the Variance of the Adaptive Learning Rate and Beyond", "abstract": "The learning rate warmup heuristic achieves remarkable success in stabilizing\ntraining, accelerating convergence and improving generalization for adaptive\nstochastic optimization algorithms like RMSprop and Adam. Here, we study its\nmechanism in details. Pursuing the theory behind warmup, we identify a problem\nof the adaptive learning rate (i.e., it has problematically large variance in\nthe early stage), suggest warmup works as a variance reduction technique, and\nprovide both empirical and theoretical evidence to verify our hypothesis. We\nfurther propose RAdam, a new variant of Adam, by introducing a term to rectify\nthe variance of the adaptive learning rate. Extensive experimental results on\nimage classification, language modeling, and neural machine translation verify\nour intuition and demonstrate the effectiveness and robustness of our proposed\nmethod. All implementations are available at:\nhttps://github.com/LiyuanLucasLiu/RAdam.", "published": "2019-08-08 20:51:17", "link": "http://arxiv.org/abs/1908.03265v4", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Universal Adversarial Audio Perturbations", "abstract": "We demonstrate the existence of universal adversarial perturbations, which\ncan fool a family of audio classification architectures, for both targeted and\nuntargeted attack scenarios. We propose two methods for finding such\nperturbations. The first method is based on an iterative, greedy approach that\nis well-known in computer vision: it aggregates small perturbations to the\ninput so as to push it to the decision boundary. The second method, which is\nthe main contribution of this work, is a novel penalty formulation, which finds\ntargeted and untargeted universal adversarial perturbations. Differently from\nthe greedy approach, the penalty method minimizes an appropriate objective\nfunction on a batch of samples. Therefore, it produces more successful attacks\nwhen the number of training samples is limited. Moreover, we provide a proof\nthat the proposed penalty method theoretically converges to a solution that\ncorresponds to universal adversarial perturbations. We also demonstrate that it\nis possible to provide successful attacks using the penalty method when only\none sample from the target dataset is available for the attacker. Experimental\nresults on attacking various 1D CNN architectures have shown attack success\nrates higher than 85.0% and 83.1% for targeted and untargeted attacks,\nrespectively using the proposed penalty method.", "published": "2019-08-08 17:07:30", "link": "http://arxiv.org/abs/1908.03173v5", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Exploiting semi-supervised training through a dropout regularization in\n  end-to-end speech recognition", "abstract": "In this paper, we explore various approaches for semi supervised learning in\nan end to end automatic speech recognition (ASR) framework. The first step in\nour approach involves training a seed model on the limited amount of labelled\ndata. Additional unlabelled speech data is employed through a data selection\nmechanism to obtain the best hypothesized output, further used to retrain the\nseed model. However, uncertainties of the model may not be well captured with a\nsingle hypothesis. As opposed to this technique, we apply a dropout mechanism\nto capture the uncertainty by obtaining multiple hypothesized text transcripts\nof an speech recording. We assume that the diversity of automatically generated\ntranscripts for an utterance will implicitly increase the reliability of the\nmodel. Finally, the data selection process is also applied on these\nhypothesized transcripts to reduce the uncertainty. Experiments on freely\navailable TEDLIUM corpus and proprietary Adobe's internal dataset show that the\nproposed approach significantly reduces ASR errors, compared to the baseline\nmodel.", "published": "2019-08-08 19:21:49", "link": "http://arxiv.org/abs/1908.05227v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML", "62H30"], "primary_category": "eess.AS"}
