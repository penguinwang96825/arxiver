{"title": "Lexically Aware Semi-Supervised Learning for OCR Post-Correction", "abstract": "Much of the existing linguistic data in many languages of the world is locked\naway in non-digitized books and documents. Optical character recognition (OCR)\ncan be used to produce digitized text, and previous work has demonstrated the\nutility of neural post-correction methods that improve the results of\ngeneral-purpose OCR systems on recognition of less-well-resourced languages.\nHowever, these methods rely on manually curated post-correction data, which are\nrelatively scarce compared to the non-annotated raw images that need to be\ndigitized.\n  In this paper, we present a semi-supervised learning method that makes it\npossible to utilize these raw images to improve performance, specifically\nthrough the use of self-training, a technique where a model is iteratively\ntrained on its own outputs. In addition, to enforce consistency in the\nrecognized vocabulary, we introduce a lexically-aware decoding method that\naugments the neural post-correction model with a count-based language model\nconstructed from the recognized texts, implemented using weighted finite-state\nautomata (WFSA) for efficient and effective decoding.\n  Results on four endangered languages demonstrate the utility of the proposed\nmethod, with relative error reductions of 15-29%, where we find the combination\nof self-training and lexically-aware decoding essential for achieving\nconsistent improvements. Data and code are available at\nhttps://shrutirij.github.io/ocr-el/.", "published": "2021-11-04 04:39:02", "link": "http://arxiv.org/abs/2111.02622v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Response Generation with Context-Aware Prompt Learning", "abstract": "Pre-trained language models (PLM) have marked a huge leap in neural dialogue\nmodeling. While PLMs are pre-trained on large-scale text corpora, they are\nusually fine-tuned on scarce dialogue data with specific domain knowledge and\ndialogue styles. However, tailoring the language models while fully utilizing\nprior knowledge in large pre-trained models remains a challenge. In this paper,\nwe present a novel approach for pre-trained dialogue modeling that casts the\ndialogue generation problem as a prompt-learning task. Instead of fine-tuning\non limited dialogue data, our approach, DialogPrompt, learns continuous prompt\nembeddings optimized for dialogue contexts, which appropriately elicit\nknowledge from the large pre-trained model. To encourage the model to better\nutilize the prompt embeddings, the prompt encoders are designed to be\ndynamically generated based on the dialogue context. Experiments on popular\nconversation datasets show that our approach significantly outperforms the\nfine-tuning baseline and the generic prompt-learning methods. Furthermore,\nhuman evaluations strongly support the superiority of DialogPrompt in regard to\nresponse generation quality.", "published": "2021-11-04 05:40:13", "link": "http://arxiv.org/abs/2111.02643v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Medicines Question Answering System, MeQA", "abstract": "In this paper we present the first system in Spanish capable of answering\nquestions about medicines for human use, called MeQA (Medicines Question\nAnswering), a project created by the Spanish Agency for Medicines and Health\nProducts (AEMPS, for its acronym in Spanish). Online services that offer\nmedical help have proliferated considerably, mainly due to the current pandemic\nsituation due to COVID-19. For example, websites such as Doctoralia, Savia, or\nSaludOnNet, offer Doctor Answers type consultations, in which patients or users\ncan send questions to doctors and specialists, and receive an answer in less\nthan 24 hours. Many of the questions received are related to medicines for\nhuman use, and most can be answered through the leaflets. Therefore, a system\nsuch as MeQA capable of answering these types of questions automatically could\nalleviate the burden on these websites, and it would be of great use to such\npatients.", "published": "2021-11-04 11:20:54", "link": "http://arxiv.org/abs/2111.02760v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Do Neural Sequence Models Generalize? Local and Global Context Cues\n  for Out-of-Distribution Prediction", "abstract": "After a neural sequence model encounters an unexpected token, can its\nbehavior be predicted? We show that RNN and transformer language models exhibit\nstructured, consistent generalization in out-of-distribution contexts. We begin\nby introducing two idealized models of generalization in next-word prediction:\na local context model in which generalization is consistent with the last word\nobserved, and a global context model in which generalization is consistent with\nthe global structure of the input. In experiments in English, Finnish,\nMandarin, and random regular languages, we demonstrate that neural language\nmodels interpolate between these two forms of generalization: their predictions\nare well-approximated by a log-linear combination of local and global\npredictive distributions. We then show that, in some languages, noise mediates\nthe two forms of generalization: noise applied to input tokens encourages\nglobal generalization, while noise in history representations encourages local\ngeneralization. Finally, we offer a preliminary theoretical explanation of\nthese results by proving that the observed interpolation behavior is expected\nin log-linear models with a particular feature correlation structure. These\nresults help explain the effectiveness of two popular regularization schemes\nand show that aspects of sequence model generalization can be understood and\ncontrolled.", "published": "2021-11-04 19:08:14", "link": "http://arxiv.org/abs/2111.03108v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CLUES: Few-Shot Learning Evaluation in Natural Language Understanding", "abstract": "Most recent progress in natural language understanding (NLU) has been driven,\nin part, by benchmarks such as GLUE, SuperGLUE, SQuAD, etc. In fact, many NLU\nmodels have now matched or exceeded \"human-level\" performance on many tasks in\nthese benchmarks. Most of these benchmarks, however, give models access to\nrelatively large amounts of labeled data for training. As such, the models are\nprovided far more data than required by humans to achieve strong performance.\nThat has motivated a line of work that focuses on improving few-shot learning\nperformance of NLU models. However, there is a lack of standardized evaluation\nbenchmarks for few-shot NLU resulting in different experimental settings in\ndifferent papers. To help accelerate this line of work, we introduce CLUES\n(Constrained Language Understanding Evaluation Standard), a benchmark for\nevaluating the few-shot learning capabilities of NLU models. We demonstrate\nthat while recent models reach human performance when they have access to large\namounts of labeled data, there is a huge gap in performance in the few-shot\nsetting for most tasks. We also demonstrate differences between alternative\nmodel families and adaptation techniques in the few shot setting. Finally, we\ndiscuss several principles and choices in designing the experimental settings\nfor evaluating the true few-shot learning performance and suggest a unified\nstandardized approach to few-shot learning evaluation. We aim to encourage\nresearch on NLU models that can generalize to new tasks with a small number of\nexamples. Code and data for CLUES are available at\nhttps://github.com/microsoft/CLUES.", "published": "2021-11-04 00:43:15", "link": "http://arxiv.org/abs/2111.02570v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Contextual Semantic Parsing for Multilingual Task-Oriented Dialogues", "abstract": "Robust state tracking for task-oriented dialogue systems currently remains\nrestricted to a few popular languages. This paper shows that given a\nlarge-scale dialogue data set in one language, we can automatically produce an\neffective semantic parser for other languages using machine translation. We\npropose automatic translation of dialogue datasets with alignment to ensure\nfaithful translation of slot values and eliminate costly human supervision used\nin previous benchmarks. We also propose a new contextual semantic parsing\nmodel, which encodes the formal slots and values, and only the last agent and\nuser utterances. We show that the succinct representation reduces the\ncompounding effect of translation errors, without harming the accuracy in\npractice.\n  We evaluate our approach on several dialogue state tracking benchmarks. On\nRiSAWOZ, CrossWOZ, CrossWOZ-EN, and MultiWOZ-ZH datasets we improve the state\nof the art by 11%, 17%, 20%, and 0.3% in joint goal accuracy. We present a\ncomprehensive error analysis for all three datasets showing erroneous\nannotations can lead to misguided judgments on the quality of the model.\n  Finally, we present RiSAWOZ English and German datasets, created using our\ntranslation methodology. On these datasets, accuracy is within 11% of the\noriginal showing that high-accuracy multilingual dialogue datasets are possible\nwithout relying on expensive human annotations. We release our datasets and\nsoftware open source.", "published": "2021-11-04 01:08:14", "link": "http://arxiv.org/abs/2111.02574v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Semantic Cognition, Inductive Generalization, and Language Models", "abstract": "My doctoral research focuses on understanding semantic knowledge in neural\nnetwork models trained solely to predict natural language (referred to as\nlanguage models, or LMs), by drawing on insights from the study of concepts and\ncategories grounded in cognitive science. I propose a framework inspired by\n'inductive reasoning,' a phenomenon that sheds light on how humans utilize\nbackground knowledge to make inductive leaps and generalize from new pieces of\ninformation about concepts and their properties. Drawing from experiments that\nstudy inductive reasoning, I propose to analyze semantic inductive\ngeneralization in LMs using phenomena observed in human-induction literature,\ninvestigate inductive behavior on tasks such as implicit reasoning and emergent\nfeature recognition, and analyze and relate induction dynamics to the learned\nconceptual representation space.", "published": "2021-11-04 03:19:52", "link": "http://arxiv.org/abs/2111.02603v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Learning to Speak and Hear Through Multi-Agent Communication\n  over a Continuous Acoustic Channel", "abstract": "Multi-agent reinforcement learning has been used as an effective means to\nstudy emergent communication between agents, yet little focus has been given to\ncontinuous acoustic communication. This would be more akin to human language\nacquisition; human infants acquire language in large part through continuous\nsignalling with their caregivers. We therefore ask: Are we able to observe\nemergent language between agents with a continuous communication channel? Our\ngoal is to provide a platform to begin bridging the gap between human and agent\ncommunication, allowing us to analyse continuous signals, how they emerge,\ntheir characteristics, and how they relate to human language acquisition. We\npropose a messaging environment where a Speaker agent needs to convey a set of\nattributes to a Listener over a noisy acoustic channel. Using DQN to train our\nagents, we show that: (1) unlike the discrete case, the acoustic Speaker learns\nredundancy to improve Listener coherency, (2) the acoustic Speaker develops\nmore compositional communication protocols which implicitly compensates for\ntransmission errors over a noisy channel, and (3) DQN has significant\nperformance gains and increased compositionality when compared to previous\nmethods optimised using REINFORCE.", "published": "2021-11-04 12:44:18", "link": "http://arxiv.org/abs/2111.02827v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A text autoencoder from transformer for fast encoding language\n  representation", "abstract": "In recent years BERT shows apparent advantages and great potential in natural\nlanguage processing tasks. However, both training and applying BERT requires\nintensive time and resources for computing contextual language representations,\nwhich hinders its universality and applicability. To overcome this bottleneck,\nwe propose a deep bidirectional language model by using window masking\nmechanism at attention layer. This work computes contextual language\nrepresentations without random masking as does in BERT and maintains the deep\nbidirectional architecture like BERT. To compute the same sentence\nrepresentation, our method shows O(n) complexity less compared to other\ntransformer-based models with O($n^2$). To further demonstrate its superiority,\ncomputing context language representations on CPU environments is conducted, by\nusing the embeddings from the proposed method, logistic regression shows much\nhigher accuracy in terms of SMS classification. Moverover, the proposed method\nalso achieves significant higher performance in semantic similarity tasks.", "published": "2021-11-04 13:09:10", "link": "http://arxiv.org/abs/2111.02844v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised and Distributional Detection of Machine-Generated Text", "abstract": "The power of natural language generation models has provoked a flurry of\ninterest in automatic methods to detect if a piece of text is human or\nmachine-authored. The problem so far has been framed in a standard supervised\nway and consists in training a classifier on annotated data to predict the\norigin of one given new document. In this paper, we frame the problem in an\nunsupervised and distributional way: we assume that we have access to a large\ncollection of unannotated documents, a big fraction of which is\nmachine-generated. We propose a method to detect those machine-generated\ndocuments leveraging repeated higher-order n-grams, which we show over-appear\nin machine-generated text as compared to human ones. That weak signal is the\nstarting point of a self-training setting where pseudo-labelled documents are\nused to train an ensemble of classifiers. Our experiments show that leveraging\nthat signal allows us to rank suspicious documents accurately. Precision at\n5000 is over 90% for top-k sampling strategies, and over 80% for nucleus\nsampling for the largest model we used (GPT2-large). The drop with increased\nsize of model is small, which could indicate that the results hold for other\ncurrent and future large language models.", "published": "2021-11-04 14:07:46", "link": "http://arxiv.org/abs/2111.02878v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "StyleCLIPDraw: Coupling Content and Style in Text-to-Drawing Synthesis", "abstract": "Generating images that fit a given text description using machine learning\nhas improved greatly with the release of technologies such as the CLIP\nimage-text encoder model; however, current methods lack artistic control of the\nstyle of image to be generated. We introduce StyleCLIPDraw which adds a style\nloss to the CLIPDraw text-to-drawing synthesis model to allow artistic control\nof the synthesized drawings in addition to control of the content via text.\nWhereas performing decoupled style transfer on a generated image only affects\nthe texture, our proposed coupled approach is able to capture a style in both\ntexture and shape, suggesting that the style of the drawing is coupled with the\ndrawing process itself. More results and our code are available at\nhttps://github.com/pschaldenbrand/StyleCLIPDraw", "published": "2021-11-04 19:57:17", "link": "http://arxiv.org/abs/2111.03133v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Speech recognition for air traffic control via feature learning and\n  end-to-end training", "abstract": "In this work, we propose a new automatic speech recognition (ASR) system\nbased on feature learning and an end-to-end training procedure for air traffic\ncontrol (ATC) systems. The proposed model integrates the feature learning\nblock, recurrent neural network (RNN), and connectionist temporal\nclassification loss to build an end-to-end ASR model. Facing the complex\nenvironments of ATC speech, instead of the handcrafted features, a learning\nblock is designed to extract informative features from raw waveforms for\nacoustic modeling. Both the SincNet and 1D convolution blocks are applied to\nprocess the raw waveforms, whose outputs are concatenated to the RNN layers for\nthe temporal modeling. Thanks to the ability to learn representations from raw\nwaveforms, the proposed model can be optimized in a complete end-to-end manner,\ni.e., from waveform to text. Finally, the multilingual issue in the ATC domain\nis also considered to achieve the ASR task by constructing a combined\nvocabulary of Chinese characters and English letters. The proposed approach is\nvalidated on a multilingual real-world corpus (ATCSpeech), and the experimental\nresults demonstrate that the proposed approach outperforms other baselines,\nachieving a 6.9\\% character error rate.", "published": "2021-11-04 06:38:21", "link": "http://arxiv.org/abs/2111.02654v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Voice Conversion Can Improve ASR in Very Low-Resource Settings", "abstract": "Voice conversion (VC) could be used to improve speech recognition systems in\nlow-resource languages by using it to augment limited training data. However,\nVC has not been widely used for this purpose because of practical issues such\nas compute speed and limitations when converting to and from unseen speakers.\nMoreover, it is still unclear whether a VC model trained on one well-resourced\nlanguage can be applied to speech from another low-resource language for the\naim of data augmentation. In this work we assess whether a VC system can be\nused cross-lingually to improve low-resource speech recognition. We combine\nseveral recent techniques to design and train a practical VC system in English,\nand then use this system to augment data for training speech recognition models\nin several low-resource languages. When using a sensible amount of VC augmented\ndata, speech recognition performance is improved in all four low-resource\nlanguages considered. We also show that VC-based augmentation is superior to\nSpecAugment (a widely used signal processing augmentation method) in the\nlow-resource languages considered.", "published": "2021-11-04 07:57:00", "link": "http://arxiv.org/abs/2111.02674v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CoreLM: Coreference-aware Language Model Fine-Tuning", "abstract": "Language Models are the underpin of all modern Natural Language Processing\n(NLP) tasks. The introduction of the Transformers architecture has contributed\nsignificantly into making Language Modeling very effective across many NLP\ntask, leading to significant advancements in the field. However, Transformers\ncome with a big computational cost, which grows quadratically with respect to\nthe input length. This presents a challenge as to understand long texts\nrequires a lot of context. In this paper, we propose a Fine-Tuning framework,\nnamed CoreLM, that extends the architecture of current Pretrained Language\nModels so that they incorporate explicit entity information. By introducing\nentity representations, we make available information outside the contextual\nspace of the model, which results in a better Language Model for a fraction of\nthe computational cost. We implement our approach using GPT2 and compare the\nfine-tuned model to the original. Our proposed model achieves a lower\nPerplexity in GUMBY and LAMBDADA datasets when compared to GPT2 and a\nfine-tuned version of GPT2 without any changes. We also compare the models'\nperformance in terms of Accuracy in LAMBADA and Children's Book Test, with and\nwithout the use of model-created coreference annotations.", "published": "2021-11-04 08:44:31", "link": "http://arxiv.org/abs/2111.02687v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Benchmarking Multimodal AutoML for Tabular Data with Text Fields", "abstract": "We consider the use of automated supervised learning systems for data tables\nthat not only contain numeric/categorical columns, but one or more text fields\nas well. Here we assemble 18 multimodal data tables that each contain some text\nfields and stem from a real business application. Our publicly-available\nbenchmark enables researchers to comprehensively evaluate their own methods for\nsupervised learning with numeric, categorical, and text features. To ensure\nthat any single modeling strategy which performs well over all 18 datasets will\nserve as a practical foundation for multimodal text/tabular AutoML, the diverse\ndatasets in our benchmark vary greatly in: sample size, problem types (a mix of\nclassification and regression tasks), number of features (with the number of\ntext columns ranging from 1 to 28 between datasets), as well as how the\npredictive signal is decomposed between text vs. numeric/categorical features\n(and predictive interactions thereof). Over this benchmark, we evaluate various\nstraightforward pipelines to model such data, including standard two-stage\napproaches where NLP is used to featurize the text such that AutoML for tabular\ndata can then be applied. Compared with human data science teams, the fully\nautomated methodology that performed best on our benchmark (stack ensembling a\nmultimodal Transformer with various tree models) also manages to rank 1st place\nwhen fit to the raw text/tabular data in two MachineHack prediction\ncompetitions and 2nd place (out of 2380 teams) in Kaggle's Mercari Price\nSuggestion Challenge.", "published": "2021-11-04 09:29:16", "link": "http://arxiv.org/abs/2111.02705v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Fine-tuned Wav2vec 2.0/HuBERT Benchmark For Speech Emotion\n  Recognition, Speaker Verification and Spoken Language Understanding", "abstract": "Speech self-supervised models such as wav2vec 2.0 and HuBERT are making\nrevolutionary progress in Automatic Speech Recognition (ASR). However, they\nhave not been totally proven to produce better performance on tasks other than\nASR. In this work, we explored partial fine-tuning and entire fine-tuning on\nwav2vec 2.0 and HuBERT pre-trained models for three non-ASR speech tasks:\nSpeech Emotion Recognition, Speaker Verification and Spoken Language\nUnderstanding. With simple proposed downstream frameworks, the best scores\nreached 79.58% weighted accuracy on speaker-dependent setting and 73.01%\nweighted accuracy on speaker-independent setting for Speech Emotion Recognition\non IEMOCAP, 2.36% equal error rate for Speaker Verification on VoxCeleb1,\n89.38% accuracy for Intent Classification and 78.92% F1 for Slot Filling on\nSLURP, showing the strength of fine-tuned wav2vec 2.0 and HuBERT on learning\nprosodic, voice-print and semantic representations.", "published": "2021-11-04 10:39:06", "link": "http://arxiv.org/abs/2111.02735v3", "categories": ["cs.CL", "cs.NE", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of\n  Language Models", "abstract": "Large-scale pre-trained language models have achieved tremendous success\nacross a wide range of natural language understanding (NLU) tasks, even\nsurpassing human performance. However, recent studies reveal that the\nrobustness of these models can be challenged by carefully crafted textual\nadversarial examples. While several individual datasets have been proposed to\nevaluate model robustness, a principled and comprehensive benchmark is still\nmissing. In this paper, we present Adversarial GLUE (AdvGLUE), a new multi-task\nbenchmark to quantitatively and thoroughly explore and evaluate the\nvulnerabilities of modern large-scale language models under various types of\nadversarial attacks. In particular, we systematically apply 14 textual\nadversarial attack methods to GLUE tasks to construct AdvGLUE, which is further\nvalidated by humans for reliable annotations. Our findings are summarized as\nfollows. (i) Most existing adversarial attack algorithms are prone to\ngenerating invalid or ambiguous adversarial examples, with around 90% of them\neither changing the original semantic meanings or misleading human annotators\nas well. Therefore, we perform a careful filtering process to curate a\nhigh-quality benchmark. (ii) All the language models and robust training\nmethods we tested perform poorly on AdvGLUE, with scores lagging far behind the\nbenign accuracy. We hope our work will motivate the development of new\nadversarial attacks that are more stealthy and semantic-preserving, as well as\nnew robust language models against sophisticated adversarial attacks. AdvGLUE\nis available at https://adversarialglue.github.io.", "published": "2021-11-04 12:59:55", "link": "http://arxiv.org/abs/2111.02840v2", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reducing the impact of out of vocabulary words in the translation of\n  natural language questions into SPARQL queries", "abstract": "Accessing the large volumes of information available in public knowledge\nbases might be complicated for those users unfamiliar with the SPARQL query\nlanguage. Automatic translation of questions posed in natural language in\nSPARQL has the potential of overcoming this problem. Existing systems based on\nneural-machine translation are very effective but easily fail in recognizing\nwords that are Out Of the Vocabulary (OOV) of the training set. This is a\nserious issue while querying large ontologies. In this paper, we combine Named\nEntity Linking, Named Entity Recognition, and Neural Machine Translation to\nperform automatic translation of natural language questions into SPARQL\nqueries. We demonstrate empirically that our approach is more effective and\nresilient to OOV words than existing approaches by running the experiments on\nMonument, QALD-9, and LC-QuAD v1, which are well-known datasets for Question\nAnswering over DBpedia.", "published": "2021-11-04 16:53:59", "link": "http://arxiv.org/abs/2111.03000v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adversarial Attacks on Knowledge Graph Embeddings via Instance\n  Attribution Methods", "abstract": "Despite the widespread use of Knowledge Graph Embeddings (KGE), little is\nknown about the security vulnerabilities that might disrupt their intended\nbehaviour. We study data poisoning attacks against KGE models for link\nprediction. These attacks craft adversarial additions or deletions at training\ntime to cause model failure at test time. To select adversarial deletions, we\npropose to use the model-agnostic instance attribution methods from\nInterpretable Machine Learning, which identify the training instances that are\nmost influential to a neural model's predictions on test instances. We use\nthese influential triples as adversarial deletions. We further propose a\nheuristic method to replace one of the two entities in each influential triple\nto generate adversarial additions. Our experiments show that the proposed\nstrategies outperform the state-of-art data poisoning attacks on KGE models and\nimprove the MRR degradation due to the attacks by up to 62% over the baselines.", "published": "2021-11-04 19:38:48", "link": "http://arxiv.org/abs/2111.03120v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "InQSS: a speech intelligibility and quality assessment model using a\n  multi-task learning network", "abstract": "Speech intelligibility and quality assessment models are essential tools for\nresearchers to evaluate and improve speech processing models. However, only a\nfew studies have investigated multi-task models for intelligibility and quality\nassessment due to the limitations of available data. In this study, we released\nTMHINT-QI, the first Chinese speech dataset that records the quality and\nintelligibility scores of clean, noisy, and enhanced utterances. Then, we\npropose InQSS, a non-intrusive multi-task learning framework for\nintelligibility and quality assessment. We evaluated the InQSS on both the\ntraining-from-scratch and the pretrained models. The experimental results\nconfirm the effectiveness of the InQSS framework. In addition, the resulting\nmodel can predict not only the intelligibility scores but also the quality\nscores of a speech signal.", "published": "2021-11-04 02:01:27", "link": "http://arxiv.org/abs/2111.02585v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "WaveFake: A Data Set to Facilitate Audio Deepfake Detection", "abstract": "Deep generative modeling has the potential to cause significant harm to\nsociety. Recognizing this threat, a magnitude of research into detecting\nso-called \"Deepfakes\" has emerged. This research most often focuses on the\nimage domain, while studies exploring generated audio signals have, so-far,\nbeen neglected. In this paper we make three key contributions to narrow this\ngap. First, we provide researchers with an introduction to common signal\nprocessing techniques used for analyzing audio signals. Second, we present a\nnovel data set, for which we collected nine sample sets from five different\nnetwork architectures, spanning two languages. Finally, we supply practitioners\nwith two baseline models, adopted from the signal processing community, to\nfacilitate further research in this area.", "published": "2021-11-04 12:26:34", "link": "http://arxiv.org/abs/2111.02813v1", "categories": ["cs.LG", "cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "MT3: Multi-Task Multitrack Music Transcription", "abstract": "Automatic Music Transcription (AMT), inferring musical notes from raw audio,\nis a challenging task at the core of music understanding. Unlike Automatic\nSpeech Recognition (ASR), which typically focuses on the words of a single\nspeaker, AMT often requires transcribing multiple instruments simultaneously,\nall while preserving fine-scale pitch and timing information. Further, many AMT\ndatasets are \"low-resource\", as even expert musicians find music transcription\ndifficult and time-consuming. Thus, prior work has focused on task-specific\narchitectures, tailored to the individual instruments of each task. In this\nwork, motivated by the promising results of sequence-to-sequence transfer\nlearning for low-resource Natural Language Processing (NLP), we demonstrate\nthat a general-purpose Transformer model can perform multi-task AMT, jointly\ntranscribing arbitrary combinations of musical instruments across several\ntranscription datasets. We show this unified training framework achieves\nhigh-quality transcription results across a range of datasets, dramatically\nimproving performance for low-resource instruments (such as guitar), while\npreserving strong performance for abundant instruments (such as piano).\nFinally, by expanding the scope of AMT, we expose the need for more consistent\nevaluation metrics and better dataset alignment, and provide a strong baseline\nfor this new direction of multi-task AMT.", "published": "2021-11-04 17:19:39", "link": "http://arxiv.org/abs/2111.03017v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generating Diverse Realistic Laughter for Interactive Art", "abstract": "We propose an interactive art project to make those rendered invisible by the\nCOVID-19 crisis and its concomitant solitude reappear through the welcome\nmelody of laughter, and connections created and explored through advanced\nlaughter synthesis approaches. However, the unconditional generation of the\ndiversity of human emotional responses in high-quality auditory synthesis\nremains an open problem, with important implications for the application of\nthese approaches in artistic settings. We developed LaughGANter, an approach to\nreproduce the diversity of human laughter using generative adversarial networks\n(GANs). When trained on a dataset of diverse laughter samples, LaughGANter\ngenerates diverse, high quality laughter samples, and learns a latent space\nsuitable for emotional analysis and novel artistic applications such as latent\nmixing/interpolation and emotional transfer.", "published": "2021-11-04 20:31:15", "link": "http://arxiv.org/abs/2111.03146v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
