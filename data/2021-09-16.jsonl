{"title": "Sister Help: Data Augmentation for Frame-Semantic Role Labeling", "abstract": "While FrameNet is widely regarded as a rich resource of semantics in natural\nlanguage processing, a major criticism concerns its lack of coverage and the\nrelative paucity of its labeled data compared to other commonly used lexical\nresources such as PropBank and VerbNet. This paper reports on a pilot study to\naddress these gaps. We propose a data augmentation approach, which uses\nexisting frame-specific annotation to automatically annotate other lexical\nunits of the same frame which are unannotated. Our rule-based approach defines\nthe notion of a sister lexical unit and generates frame-specific augmented data\nfor training. We present experiments on frame-semantic role labeling which\ndemonstrate the importance of this data augmentation: we obtain a large\nimprovement to prior results on frame identification and argument\nidentification for FrameNet, utilizing both full-text and lexicographic\nannotations under FrameNet. Our findings on data augmentation highlight the\nvalue of automatic resource creation for improved models in frame-semantic\nparsing.", "published": "2021-09-16 05:15:29", "link": "http://arxiv.org/abs/2109.07725v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MOVER: Mask, Over-generate and Rank for Hyperbole Generation", "abstract": "Despite being a common figure of speech, hyperbole is under-researched in\nFigurative Language Processing. In this paper, we tackle the challenging task\nof hyperbole generation to transfer a literal sentence into its hyperbolic\nparaphrase. To address the lack of available hyperbolic sentences, we construct\nHYPO-XL, the first large-scale English hyperbole corpus containing 17,862\nhyperbolic sentences in a non-trivial way. Based on our corpus, we propose an\nunsupervised method for hyperbole generation that does not require parallel\nliteral-hyperbole pairs. During training, we fine-tune BART to infill masked\nhyperbolic spans of sentences from HYPO-XL. During inference, we mask part of\nan input literal sentence and over-generate multiple possible hyperbolic\nversions. Then a BERT-based ranker selects the best candidate by hyperbolicity\nand paraphrase quality. Automatic and human evaluation results show that our\nmodel is effective at generating hyperbolic paraphrase sentences and\noutperforms several baseline systems.", "published": "2021-09-16 05:25:13", "link": "http://arxiv.org/abs/2109.07726v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Spanish Biomedical Crawled Corpus: A Large, Diverse Dataset for Spanish\n  Biomedical Language Models", "abstract": "We introduce CoWeSe (the Corpus Web Salud Espa\\~nol), the largest Spanish\nbiomedical corpus to date, consisting of 4.5GB (about 750M tokens) of clean\nplain text. CoWeSe is the result of a massive crawler on 3000 Spanish domains\nexecuted in 2020. The corpus is openly available and already preprocessed.\nCoWeSe is an important resource for biomedical and health NLP in Spanish and\nhas already been employed to train domain-specific language models and to\nproduce word embbedings. We released the CoWeSe corpus under a Creative Commons\nAttribution 4.0 International license, both in Zenodo\n(\\url{https://zenodo.org/record/4561971\\#.YTI5SnVKiEA}).", "published": "2021-09-16 07:22:28", "link": "http://arxiv.org/abs/2109.07765v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constructing Emotion Consensus and Utilizing Unpaired Data for\n  Empathetic Dialogue Generation", "abstract": "Researches on dialogue empathy aim to endow an agent with the capacity of\naccurate understanding and proper responding for emotions. Existing models for\nempathetic dialogue generation focus on the emotion flow in one direction, that\nis, from the context to response. We argue that conducting an empathetic\nconversation is a bidirectional process, where empathy occurs when the emotions\nof two interlocutors could converge on the same point, i.e., reaching an\nemotion consensus. Besides, we also find that the empathetic dialogue corpus is\nextremely limited, which further restricts the model performance. To address\nthe above issues, we propose a dual-generative model, Dual-Emp, to\nsimultaneously construct the emotion consensus and utilize some external\nunpaired data. Specifically, our model integrates a forward dialogue model, a\nbackward dialogue model, and a discrete latent variable representing the\nemotion consensus into a unified architecture. Then, to alleviate the\nconstraint of paired data, we extract unpaired emotional data from open-domain\nconversations and employ Dual-Emp to produce pseudo paired empathetic samples,\nwhich is more efficient and low-cost than the human annotation. Automatic and\nhuman evaluations demonstrate that our method outperforms competitive baselines\nin producing coherent and empathetic responses.", "published": "2021-09-16 07:57:01", "link": "http://arxiv.org/abs/2109.07779v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Neural Machine Translation by Bidirectional Training", "abstract": "We present a simple and effective pretraining strategy -- bidirectional\ntraining (BiT) for neural machine translation. Specifically, we bidirectionally\nupdate the model parameters at the early stage and then tune the model\nnormally. To achieve bidirectional updating, we simply reconstruct the training\nsamples from \"src$\\rightarrow$tgt\" to \"src+tgt$\\rightarrow$tgt+src\" without any\ncomplicated model modifications. Notably, our approach does not increase any\nparameters or training steps, requiring the parallel data merely. Experimental\nresults show that BiT pushes the SOTA neural machine translation performance\nacross 15 translation tasks on 8 language pairs (data sizes range from 160K to\n38M) significantly higher. Encouragingly, our proposed model can complement\nexisting data manipulation strategies, i.e. back translation, data\ndistillation, and data diversification. Extensive analyses show that our\napproach functions as a novel bilingual code-switcher, obtaining better\nbilingual alignment.", "published": "2021-09-16 07:58:33", "link": "http://arxiv.org/abs/2109.07780v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transductive Learning for Unsupervised Text Style Transfer", "abstract": "Unsupervised style transfer models are mainly based on an inductive learning\napproach, which represents the style as embeddings, decoder parameters, or\ndiscriminator parameters and directly applies these general rules to the test\ncases. However, the lacking of parallel corpus hinders the ability of these\ninductive learning methods on this task. As a result, it is likely to cause\nsevere inconsistent style expressions, like `the salad is rude`. To tackle this\nproblem, we propose a novel transductive learning approach in this paper, based\non a retrieval-based context-aware style representation. Specifically, an\nattentional encoder-decoder with a retriever framework is utilized. It involves\ntop-K relevant sentences in the target style in the transfer process. In this\nway, we can learn a context-aware style embedding to alleviate the above\ninconsistency problem. In this paper, both sparse (BM25) and dense retrieval\nfunctions (MIPS) are used, and two objective functions are designed to\nfacilitate joint learning. Experimental results show that our method\noutperforms several strong baselines. The proposed transductive learning\napproach is general and effective to the task of unsupervised style transfer,\nand we will apply it to the other two typical methods in the future.", "published": "2021-09-16 08:57:20", "link": "http://arxiv.org/abs/2109.07812v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Language Model Understood the Prompt was Ambiguous: Probing\n  Syntactic Uncertainty Through Generation", "abstract": "Temporary syntactic ambiguities arise when the beginning of a sentence is\ncompatible with multiple syntactic analyses. We inspect to which extent neural\nlanguage models (LMs) exhibit uncertainty over such analyses when processing\ntemporarily ambiguous inputs, and how that uncertainty is modulated by\ndisambiguating cues. We probe the LM's expectations by generating from it: we\nuse stochastic decoding to derive a set of sentence completions, and estimate\nthe probability that the LM assigns to each interpretation based on the\ndistribution of parses across completions. Unlike scoring-based methods for\ntargeted syntactic evaluation, this technique makes it possible to explore\ncompletions that are not hypothesized in advance by the researcher. We apply\nthis method to study the behavior of two LMs (GPT2 and an LSTM) on three types\nof temporary ambiguity, using materials from human sentence processing\nexperiments. We find that LMs can track multiple analyses simultaneously; the\ndegree of uncertainty varies across constructions and contexts. As a response\nto disambiguating cues, the LMs often select the correct interpretation, but\noccasional errors point to potential areas of improvement.", "published": "2021-09-16 10:27:05", "link": "http://arxiv.org/abs/2109.07848v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Translation Transformers Rediscover Inherent Data Domains", "abstract": "Many works proposed methods to improve the performance of Neural Machine\nTranslation (NMT) models in a domain/multi-domain adaptation scenario. However,\nan understanding of how NMT baselines represent text domain information\ninternally is still lacking. Here we analyze the sentence representations\nlearned by NMT Transformers and show that these explicitly include the\ninformation on text domains, even after only seeing the input sentences without\ndomains labels. Furthermore, we show that this internal information is enough\nto cluster sentences by their underlying domains without supervision. We show\nthat NMT models produce clusters better aligned to the actual domains compared\nto pre-trained language models (LMs). Notably, when computed on document-level,\nNMT cluster-to-domain correspondence nears 100%. We use these findings together\nwith an approach to NMT domain adaptation using automatically extracted\ndomains. Whereas previous work relied on external LMs for text clustering, we\npropose re-using the NMT model as a source of unsupervised clusters. We perform\nan extensive experimental study comparing two approaches across two data\nscenarios, three language pairs, and both sentence-level and document-level\nclustering, showing equal or significantly superior performance compared to\nLMs.", "published": "2021-09-16 10:58:13", "link": "http://arxiv.org/abs/2109.07864v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MFE-NER: Multi-feature Fusion Embedding for Chinese Named Entity\n  Recognition", "abstract": "In Chinese Named Entity Recognition, character substitution is a complicated\nlinguistic phenomenon. Some Chinese characters are quite similar as they share\nthe same components or have similar pronunciations. People replace characters\nin a named entity with similar characters to generate a new collocation but\nreferring to the same object. As a result, it always leads to unrecognizable or\nmislabeling errors in the NER task. In this paper, we propose a lightweight\nmethod, MFE-NER, which fuses glyph and phonetic features, to help pre-trained\nlanguage models handle the character substitution problem in the NER task with\nlimited extra cost. Basically, in the glyph domain, we disassemble Chinese\ncharacters into Five-Stroke components to represent structure features. In the\nphonetic domain, an improved phonetic system is proposed in our work, making it\nreasonable to describe phonetic similarity among Chinese characters.\nExperiments demonstrate that our method performs especially well in detecting\ncharacter substitutions while slightly improving the overall performance of\nChinese NER.", "published": "2021-09-16 11:16:43", "link": "http://arxiv.org/abs/2109.07877v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Search for a Search Method -- Simple Heuristics Suffice for\n  Adversarial Text Attacks", "abstract": "Recently more attention has been given to adversarial attacks on neural\nnetworks for natural language processing (NLP). A central research topic has\nbeen the investigation of search algorithms and search constraints, accompanied\nby benchmark algorithms and tasks. We implement an algorithm inspired by zeroth\norder optimization-based attacks and compare with the benchmark results in the\nTextAttack framework. Surprisingly, we find that optimization-based methods do\nnot yield any improvement in a constrained setup and slightly benefit from\napproximate gradient information only in unconstrained setups where search\nspaces are larger. In contrast, simple heuristics exploiting nearest neighbors\nwithout querying the target function yield substantial success rates in\nconstrained setups, and nearly full success rate in unconstrained setups, at an\norder of magnitude fewer queries. We conclude from these results that current\nTextAttack benchmark tasks are too easy and constraints are too strict,\npreventing meaningful research on black-box adversarial text attacks.", "published": "2021-09-16 12:22:17", "link": "http://arxiv.org/abs/2109.07926v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RetrievalSum: A Retrieval Enhanced Framework for Abstractive\n  Summarization", "abstract": "Existing summarization systems mostly generate summaries purely relying on\nthe content of the source document. However, even for humans, we usually need\nsome references or exemplars to help us fully understand the source document\nand write summaries in a particular format. But how to find the high-quality\nexemplars and incorporate them into summarization systems is still challenging\nand worth exploring. In this paper, we propose RetrievalSum, a novel retrieval\nenhanced abstractive summarization framework consisting of a dense Retriever\nand a Summarizer. At first, several closely related exemplars are retrieved as\nsupplementary input to help the generation model understand the text more\ncomprehensively. Furthermore, retrieved exemplars can also play a role in\nguiding the model to capture the writing style of a specific corpus. We\nvalidate our method on a wide range of summarization datasets across multiple\ndomains and two backbone models: BERT and BART. Results show that our framework\nobtains significant improvement by 1.38~4.66 in ROUGE-1 score when compared\nwith the powerful pre-trained models, and achieve new state-of-the-art on\nBillSum. Human evaluation demonstrates that our retrieval enhanced model can\nbetter capture the domain-specific writing style.", "published": "2021-09-16 12:52:48", "link": "http://arxiv.org/abs/2109.07943v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Attribute Injection for Pretrained Language Models", "abstract": "Metadata attributes (e.g., user and product IDs from reviews) can be\nincorporated as additional inputs to neural-based NLP models, by modifying the\narchitecture of the models, in order to improve their performance. Recent\nmodels however rely on pretrained language models (PLMs), where previously used\ntechniques for attribute injection are either nontrivial or ineffective. In\nthis paper, we propose a lightweight and memory-efficient method to inject\nattributes to PLMs. We extend adapters, i.e. tiny plug-in feed-forward modules,\nto include attributes both independently of or jointly with the text. To limit\nthe increase of parameters especially when the attribute vocabulary is large,\nwe use low-rank approximations and hypercomplex multiplications, significantly\ndecreasing the total parameters. We also introduce training mechanisms to\nhandle domains in which attributes can be multi-labeled or sparse. Extensive\nexperiments and analyses on eight datasets from different domains show that our\nmethod outperforms previous attribute injection methods and achieves\nstate-of-the-art performance on various datasets.", "published": "2021-09-16 13:08:24", "link": "http://arxiv.org/abs/2109.07953v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Unsupervised Question Answering via Summarization-Informed\n  Question Generation", "abstract": "Question Generation (QG) is the task of generating a plausible question for a\ngiven <passage, answer> pair. Template-based QG uses linguistically-informed\nheuristics to transform declarative sentences into interrogatives, whereas\nsupervised QG uses existing Question Answering (QA) datasets to train a system\nto generate a question given a passage and an answer. A disadvantage of the\nheuristic approach is that the generated questions are heavily tied to their\ndeclarative counterparts. A disadvantage of the supervised approach is that\nthey are heavily tied to the domain/language of the QA dataset used as training\ndata. In order to overcome these shortcomings, we propose an unsupervised QG\nmethod which uses questions generated heuristically from summaries as a source\nof training data for a QG system. We make use of freely available news summary\ndata, transforming declarative summary sentences into appropriate questions\nusing heuristics informed by dependency parsing, named entity recognition and\nsemantic role labeling. The resulting questions are then combined with the\noriginal news articles to train an end-to-end neural QG model. We extrinsically\nevaluate our approach using unsupervised QA: our QG model is used to generate\nsynthetic QA pairs for training a QA model. Experimental results show that,\ntrained with only 20k English Wikipedia-based synthetic QA pairs, the QA model\nsubstantially outperforms previous unsupervised models on three in-domain\ndatasets (SQuAD1.1, Natural Questions, TriviaQA) and three out-of-domain\ndatasets (NewsQA, BioASQ, DuoRC), demonstrating the transferability of the\napproach.", "published": "2021-09-16 13:08:43", "link": "http://arxiv.org/abs/2109.07954v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Language Models Know the Way to Rome?", "abstract": "The global geometry of language models is important for a range of\napplications, but language model probes tend to evaluate rather local\nrelations, for which ground truths are easily obtained. In this paper we\nexploit the fact that in geography, ground truths are available beyond local\nrelations. In a series of experiments, we evaluate the extent to which language\nmodel representations of city and country names are isomorphic to real-world\ngeography, e.g., if you tell a language model where Paris and Berlin are, does\nit know the way to Rome? We find that language models generally encode limited\ngeographic information, but with larger models performing the best, suggesting\nthat geographic knowledge can be induced from higher-order co-occurrence\nstatistics.", "published": "2021-09-16 13:28:16", "link": "http://arxiv.org/abs/2109.07971v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-aware Entity Typing in Knowledge Graphs", "abstract": "Knowledge graph entity typing aims to infer entities' missing types in\nknowledge graphs which is an important but under-explored issue. This paper\nproposes a novel method for this task by utilizing entities' contextual\ninformation. Specifically, we design two inference mechanisms: i) N2T:\nindependently use each neighbor of an entity to infer its type; ii) Agg2T:\naggregate the neighbors of an entity to infer its type. Those mechanisms will\nproduce multiple inference results, and an exponentially weighted pooling\nmethod is used to generate the final inference result. Furthermore, we propose\na novel loss function to alleviate the false-negative problem during training.\nExperiments on two real-world KGs demonstrate the effectiveness of our method.\nThe source code and data of this paper can be obtained from\nhttps://github.com/CCIIPLab/CET.", "published": "2021-09-16 13:59:27", "link": "http://arxiv.org/abs/2109.07990v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The NiuTrans System for the WMT21 Efficiency Task", "abstract": "This paper describes the NiuTrans system for the WMT21 translation efficiency\ntask (http://statmt.org/wmt21/efficiency-task.html). Following last year's\nwork, we explore various techniques to improve efficiency while maintaining\ntranslation quality. We investigate the combinations of lightweight Transformer\narchitectures and knowledge distillation strategies. Also, we improve the\ntranslation efficiency with graph optimization, low precision, dynamic\nbatching, and parallel pre/post-processing. Our system can translate 247,000\nwords per second on an NVIDIA A100, being 3$\\times$ faster than last year's\nsystem. Our system is the fastest and has the lowest memory consumption on the\nGPU-throughput track. The code, model, and pipeline will be available at\nNiuTrans.NMT (https://github.com/NiuTrans/NiuTrans.NMT).", "published": "2021-09-16 14:21:52", "link": "http://arxiv.org/abs/2109.08003v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The NiuTrans System for WNGT 2020 Efficiency Task", "abstract": "This paper describes the submissions of the NiuTrans Team to the WNGT 2020\nEfficiency Shared Task. We focus on the efficient implementation of deep\nTransformer models \\cite{wang-etal-2019-learning, li-etal-2019-niutrans} using\nNiuTensor (https://github.com/NiuTrans/NiuTensor), a flexible toolkit for NLP\ntasks. We explored the combination of deep encoder and shallow decoder in\nTransformer models via model compression and knowledge distillation. The neural\nmachine translation decoding also benefits from FP16 inference, attention\ncaching, dynamic batching, and batch pruning. Our systems achieve promising\nresults in both translation quality and efficiency, e.g., our fastest system\ncan translate more than 40,000 tokens per second with an RTX 2080 Ti while\nmaintaining 42.9 BLEU on \\textit{newstest2018}. The code, models, and docker\nimages are available at NiuTrans.NMT\n(https://github.com/NiuTrans/NiuTrans.NMT).", "published": "2021-09-16 14:32:01", "link": "http://arxiv.org/abs/2109.08008v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Locating Language-Specific Information in Contextualized Embeddings", "abstract": "Multilingual pretrained language models (MPLMs) exhibit multilinguality and\nare well suited for transfer across languages. Most MPLMs are trained in an\nunsupervised fashion and the relationship between their objective and\nmultilinguality is unclear. More specifically, the question whether MPLM\nrepresentations are language-agnostic or they simply interleave well with\nlearned task prediction heads arises. In this work, we locate language-specific\ninformation in MPLMs and identify its dimensionality and the layers where this\ninformation occurs. We show that language-specific information is scattered\nacross many dimensions, which can be projected into a linear subspace. Our\nstudy contributes to a better understanding of MPLM representations, going\nbeyond treating them as unanalyzable blobs of information.", "published": "2021-09-16 15:11:55", "link": "http://arxiv.org/abs/2109.08040v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Error Type Annotation for Arabic", "abstract": "We present ARETA, an automatic error type annotation system for Modern\nStandard Arabic. We design ARETA to address Arabic's morphological richness and\northographic ambiguity. We base our error taxonomy on the Arabic Learner Corpus\n(ALC) Error Tagset with some modifications. ARETA achieves a performance of\n85.8% (micro average F1 score) on a manually annotated blind test portion of\nALC. We also demonstrate ARETA's usability by applying it to a number of\nsubmissions from the QALB 2014 shared task for Arabic grammatical error\ncorrection. The resulting analyses give helpful insights on the strengths and\nweaknesses of different submissions, which is more useful than the opaque M2\nscoring metrics used in the shared task. ARETA employs a large Arabic\nmorphological analyzer, but is completely unsupervised otherwise. We make ARETA\npublicly available.", "published": "2021-09-16 15:50:11", "link": "http://arxiv.org/abs/2109.08068v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MeLT: Message-Level Transformer with Masked Document Representations as\n  Pre-Training for Stance Detection", "abstract": "Much of natural language processing is focused on leveraging large capacity\nlanguage models, typically trained over single messages with a task of\npredicting one or more tokens. However, modeling human language at\nhigher-levels of context (i.e., sequences of messages) is under-explored. In\nstance detection and other social media tasks where the goal is to predict an\nattribute of a message, we have contextual data that is loosely semantically\nconnected by authorship. Here, we introduce Message-Level Transformer (MeLT) --\na hierarchical message-encoder pre-trained over Twitter and applied to the task\nof stance prediction. We focus on stance prediction as a task benefiting from\nknowing the context of the message (i.e., the sequence of previous messages).\nThe model is trained using a variant of masked-language modeling; where instead\nof predicting tokens, it seeks to generate an entire masked (aggregated)\nmessage vector via reconstruction loss. We find that applying this pre-trained\nmasked message-level transformer to the downstream task of stance detection\nachieves F1 performance of 67%.", "published": "2021-09-16 17:07:45", "link": "http://arxiv.org/abs/2109.08113v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Online Hate Speech through the Causal Lens", "abstract": "The societal issue of digital hostility has previously attracted a lot of\nattention. The topic counts an ample body of literature, yet remains prominent\nand challenging as ever due to its subjective nature. We posit that a better\nunderstanding of this problem will require the use of causal inference\nframeworks. This survey summarises the relevant research that revolves around\nestimations of causal effects related to online hate speech. Initially, we\nprovide an argumentation as to why re-establishing the exploration of hate\nspeech in causal terms is of the essence. Following that, we give an overview\nof the leading studies classified with respect to the direction of their\noutcomes, as well as an outline of all related research, and a summary of open\nresearch problems that can influence future work on the topic.", "published": "2021-09-16 17:13:55", "link": "http://arxiv.org/abs/2109.08120v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Tri-training of Dependency Parsers", "abstract": "We compare two orthogonal semi-supervised learning techniques, namely\ntri-training and pretrained word embeddings, in the task of dependency parsing.\nWe explore language-specific FastText and ELMo embeddings and multilingual BERT\nembeddings. We focus on a low resource scenario as semi-supervised learning can\nbe expected to have the most impact here. Based on treebank size and available\nELMo models, we select Hungarian, Uyghur (a zero-shot language for mBERT) and\nVietnamese. Furthermore, we include English in a simulated low-resource\nsetting. We find that pretrained word embeddings make more effective use of\nunlabelled data than tri-training but that the two approaches can be\nsuccessfully combined.", "published": "2021-09-16 17:19:05", "link": "http://arxiv.org/abs/2109.08122v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does Summary Evaluation Survive Translation to Other Languages?", "abstract": "The creation of a quality summarization dataset is an expensive,\ntime-consuming effort, requiring the production and evaluation of summaries by\nboth trained humans and machines. If such effort is made in one language, it\nwould be beneficial to be able to use it in other languages without repeating\nhuman annotations. To investigate how much we can trust machine translation of\nsuch a dataset, we translate the English dataset SummEval to seven languages\nand compare performance across automatic evaluation measures. We explore\nequivalence testing as the appropriate statistical paradigm for evaluating\ncorrelations between human and automated scoring of summaries. While we find\nsome potential for dataset reuse in languages similar to the source, most\nsummary evaluation methods are not found to be statistically equivalent across\ntranslations.", "published": "2021-09-16 17:35:01", "link": "http://arxiv.org/abs/2109.08129v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Bag of Tricks for Dialogue Summarization", "abstract": "Dialogue summarization comes with its own peculiar challenges as opposed to\nnews or scientific articles summarization. In this work, we explore four\ndifferent challenges of the task: handling and differentiating parts of the\ndialogue belonging to multiple speakers, negation understanding, reasoning\nabout the situation, and informal language understanding. Using a pretrained\nsequence-to-sequence language model, we explore speaker name substitution,\nnegation scope highlighting, multi-task learning with relevant tasks, and\npretraining on in-domain data. Our experiments show that our proposed\ntechniques indeed improve summarization performance, outperforming strong\nbaselines.", "published": "2021-09-16 21:32:02", "link": "http://arxiv.org/abs/2109.08232v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Regularized Training of Nearest Neighbor Language Models", "abstract": "Including memory banks in a natural language processing architecture\nincreases model capacity by equipping it with additional data at inference\ntime. In this paper, we build upon $k$NN-LM \\citep{khandelwal20generalization},\nwhich uses a pre-trained language model together with an exhaustive $k$NN\nsearch through the training data (memory bank) to achieve state-of-the-art\nresults. We investigate whether we can improve the $k$NN-LM performance by\ninstead training a LM with the knowledge that we will be using a $k$NN\npost-hoc. We achieved significant improvement using our method on language\nmodeling tasks on \\texttt{WIKI-2} and \\texttt{WIKI-103}. The main phenomenon\nthat we encounter is that adding a simple L2 regularization on the activations\n(not weights) of the model, a transformer, improves the post-hoc $k$NN\nclassification performance. We explore some possible reasons for this\nimprovement. In particular, we find that the added L2 regularization seems to\nimprove the performance for high-frequency words without deteriorating the\nperformance for low frequency ones.", "published": "2021-09-16 23:20:24", "link": "http://arxiv.org/abs/2109.08249v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Balancing out Bias: Achieving Fairness Through Balanced Training", "abstract": "Group bias in natural language processing tasks manifests as disparities in\nsystem error rates across texts authorized by different demographic groups,\ntypically disadvantaging minority groups. Dataset balancing has been shown to\nbe effective at mitigating bias, however existing approaches do not directly\naccount for correlations between author demographics and linguistic variables,\nlimiting their effectiveness. To achieve Equal Opportunity fairness, such as\nequal job opportunity without regard to demographics, this paper introduces a\nsimple, but highly effective, objective for countering bias using balanced\ntraining. We extend the method in the form of a gated model, which incorporates\nprotected attributes as input, and show that it is effective at reducing bias\nin predictions through demographic input perturbation, outperforming all other\nbias mitigation techniques when combined with balanced training.", "published": "2021-09-16 23:40:28", "link": "http://arxiv.org/abs/2109.08253v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Ontology-Based Information Extraction System for Residential Land Use\n  Suitability Analysis", "abstract": "We propose an Ontology-Based Information Extraction (OBIE) system to automate\nthe extraction of the criteria and values applied in Land Use Suitability\nAnalysis (LUSA) from bylaw and regulation documents related to the geographic\narea of interest. The results obtained by our proposed LUSA OBIE system (land\nuse suitability criteria and their values) are presented as an ontology\npopulated with instances of the extracted criteria and property values. This\nlatter output ontology is incorporated into a Multi-Criteria Decision Making\n(MCDM) model applied for constructing suitability maps for different kinds of\nland uses. The resulting maps may be the final desired product or can be\nincorporated into the cellular automata urban modeling and simulation for\npredicting future urban growth. A case study has been conducted where the\noutput from LUSA OBIE is applied to help produce a suitability map for the City\nof Regina, Saskatchewan, to assist in the identification of suitable areas for\nresidential development. A set of Saskatchewan bylaw and regulation documents\nwere downloaded and input to the LUSA OBIE system. We accessed the extracted\ninformation using both the populated LUSA ontology and the set of annotated\ndocuments. In this regard, the LUSA OBIE system was effective in producing a\nfinal suitability map.", "published": "2021-09-16 02:18:30", "link": "http://arxiv.org/abs/2109.07672v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Benchmarking Commonsense Knowledge Base Population with an Effective\n  Evaluation Dataset", "abstract": "Reasoning over commonsense knowledge bases (CSKB) whose elements are in the\nform of free-text is an important yet hard task in NLP. While CSKB completion\nonly fills the missing links within the domain of the CSKB, CSKB population is\nalternatively proposed with the goal of reasoning unseen assertions from\nexternal resources. In this task, CSKBs are grounded to a large-scale\neventuality (activity, state, and event) graph to discriminate whether novel\ntriples from the eventuality graph are plausible or not. However, existing\nevaluations on the population task are either not accurate (automatic\nevaluation with randomly sampled negative examples) or of small scale (human\nannotation). In this paper, we benchmark the CSKB population task with a new\nlarge-scale dataset by first aligning four popular CSKBs, and then presenting a\nhigh-quality human-annotated evaluation set to probe neural models' commonsense\nreasoning ability. We also propose a novel inductive commonsense reasoning\nmodel that reasons over graphs. Experimental results show that generalizing\ncommonsense reasoning on unseen assertions is inherently a hard task. Models\nachieving high accuracy during training perform poorly on the evaluation set,\nwith a large gap between human performance. We will make the data publicly\navailable for future contributions. Codes and data are available at\nhttps://github.com/HKUST-KnowComp/CSKB-Population.", "published": "2021-09-16 02:50:01", "link": "http://arxiv.org/abs/2109.07679v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Jointly Modeling Aspect and Polarity for Aspect-based Sentiment Analysis\n  in Persian Reviews", "abstract": "Identification of user's opinions from natural language text has become an\nexciting field of research due to its growing applications in the real world.\nThe research field is known as sentiment analysis and classification, where\naspect category detection (ACD) and aspect category polarity (ACP) are two\nimportant sub-tasks of aspect-based sentiment analysis. The goal in ACD is to\nspecify which aspect of the entity comes up in opinion while ACP aims to\nspecify the polarity of each aspect category from the ACD task. The previous\nworks mostly propose separate solutions for these two sub-tasks. This paper\nfocuses on the ACD and ACP sub-tasks to solve both problems simultaneously. The\nproposed method carries out multi-label classification where four different\ndeep models were employed and comparatively evaluated to examine their\nperformance. A dataset of Persian reviews was collected from CinemaTicket\nwebsite including 2200 samples from 14 categories. The developed models were\nevaluated using the collected dataset in terms of example-based and label-based\nmetrics. The results indicate the high applicability and preference of the CNN\nand GRU models in comparison to LSTM and Bi-LSTM.", "published": "2021-09-16 02:55:12", "link": "http://arxiv.org/abs/2109.07680v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Models are Few-shot Multilingual Learners", "abstract": "General-purpose language models have demonstrated impressive capabilities,\nperforming on par with state-of-the-art approaches on a range of downstream\nnatural language processing (NLP) tasks and benchmarks when inferring\ninstructions from very few examples. Here, we evaluate the multilingual skills\nof the GPT and T5 models in conducting multi-class classification on\nnon-English languages without any parameter updates. We show that, given a few\nEnglish examples as context, pre-trained language models can predict not only\nEnglish test samples but also non-English ones. Finally, we find the in-context\nfew-shot cross-lingual prediction results of language models are significantly\nbetter than random prediction, and they are competitive compared to the\nexisting state-of-the-art cross-lingual models.", "published": "2021-09-16 03:08:22", "link": "http://arxiv.org/abs/2109.07684v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Does External Knowledge Help Explainable Natural Language Inference?\n  Automatic Evaluation vs. Human Ratings", "abstract": "Natural language inference (NLI) requires models to learn and apply\ncommonsense knowledge. These reasoning abilities are particularly important for\nexplainable NLI systems that generate a natural language explanation in\naddition to their label prediction. The integration of external knowledge has\nbeen shown to improve NLI systems, here we investigate whether it can also\nimprove their explanation capabilities. For this, we investigate different\nsources of external knowledge and evaluate the performance of our models on\nin-domain data as well as on special transfer datasets that are designed to\nassess fine-grained reasoning capabilities. We find that different sources of\nknowledge have a different effect on reasoning abilities, for example, implicit\nknowledge stored in language models can hinder reasoning on numbers and\nnegations. Finally, we conduct the largest and most fine-grained explainable\nNLI crowdsourcing study to date. It reveals that even large differences in\nautomatic performance scores do neither reflect in human ratings of label,\nexplanation, commonsense nor grammar correctness.", "published": "2021-09-16 09:56:20", "link": "http://arxiv.org/abs/2109.07833v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Alquist 4.0: Towards Social Intelligence Using Generative Models and\n  Dialogue Personalization", "abstract": "The open domain-dialogue system Alquist has a goal to conduct a coherent and\nengaging conversation that can be considered as one of the benchmarks of social\nintelligence. The fourth version of the system, developed within the Alexa\nPrize Socialbot Grand Challenge 4, brings two main innovations. The first\naddresses coherence, and the second addresses the engagingness of the\nconversation. For innovations regarding coherence, we propose a novel hybrid\napproach combining hand-designed responses and a generative model. The proposed\napproach utilizes hand-designed dialogues, out-of-domain detection, and a\nneural response generator. Hand-designed dialogues walk the user through\nhigh-quality conversational flows. The out-of-domain detection recognizes that\nthe user diverges from the predefined flow and prevents the system from\nproducing a scripted response that might not make sense for unexpected user\ninput. Finally, the neural response generator generates a response based on the\ncontext of the dialogue that correctly reacts to the unexpected user input and\nreturns the dialogue to the boundaries of hand-designed dialogues. The\ninnovations for engagement that we propose are mostly inspired by the famous\nexploration-exploitation dilemma. To conduct an engaging conversation with the\ndialogue partners, one has to learn their preferences and interests --\nexploration. Moreover, to engage the partner, we have to utilize the knowledge\nwe have already learned -- exploitation. In this work, we present the\nprinciples and inner workings of individual components of the open-domain\ndialogue system Alquist developed within the Alexa Prize Socialbot Grand\nChallenge 4 and the experiments we have conducted to evaluate them.", "published": "2021-09-16 13:24:34", "link": "http://arxiv.org/abs/2109.07968v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Let the CAT out of the bag: Contrastive Attributed explanations for Text", "abstract": "Contrastive explanations for understanding the behavior of black box models\nhas gained a lot of attention recently as they provide potential for recourse.\nIn this paper, we propose a method Contrastive Attributed explanations for Text\n(CAT) which provides contrastive explanations for natural language text data\nwith a novel twist as we build and exploit attribute classifiers leading to\nmore semantically meaningful explanations. To ensure that our contrastive\ngenerated text has the fewest possible edits with respect to the original text,\nwhile also being fluent and close to a human generated contrastive, we resort\nto a minimal perturbation approach regularized using a BERT language model and\nattribute classifiers trained on available attributes. We show through\nqualitative examples and a user study that our method not only conveys more\ninsight because of these attributes, but also leads to better quality\n(contrastive) text. Quantitatively, we show that our method outperforms other\nstate-of-the-art methods across four data sets on four benchmark metrics.", "published": "2021-09-16 13:44:55", "link": "http://arxiv.org/abs/2109.07983v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KnowMAN: Weakly Supervised Multinomial Adversarial Networks", "abstract": "The absence of labeled data for training neural models is often addressed by\nleveraging knowledge about the specific task, resulting in heuristic but noisy\nlabels. The knowledge is captured in labeling functions, which detect certain\nregularities or patterns in the training samples and annotate corresponding\nlabels for training. This process of weakly supervised training may result in\nan over-reliance on the signals captured by the labeling functions and hinder\nmodels to exploit other signals or to generalize well. We propose KnowMAN, an\nadversarial scheme that enables to control influence of signals associated with\nspecific labeling functions. KnowMAN forces the network to learn\nrepresentations that are invariant to those signals and to pick up other\nsignals that are more generally associated with an output label. KnowMAN\nstrongly improves results compared to direct weakly supervised learning with a\npre-trained transformer language model and a feature-based baseline.", "published": "2021-09-16 14:01:30", "link": "http://arxiv.org/abs/2109.07994v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Phrase Retrieval Learns Passage Retrieval, Too", "abstract": "Dense retrieval methods have shown great promise over sparse retrieval\nmethods in a range of NLP problems. Among them, dense phrase retrieval-the most\nfine-grained retrieval unit-is appealing because phrases can be directly used\nas the output for question answering and slot filling tasks. In this work, we\nfollow the intuition that retrieving phrases naturally entails retrieving\nlarger text blocks and study whether phrase retrieval can serve as the basis\nfor coarse-level retrieval including passages and documents. We first observe\nthat a dense phrase-retrieval system, without any retraining, already achieves\nbetter passage retrieval accuracy (+3-5% in top-5 accuracy) compared to passage\nretrievers, which also helps achieve superior end-to-end QA performance with\nfewer passages. Then, we provide an interpretation for why phrase-level\nsupervision helps learn better fine-grained entailment compared to\npassage-level supervision, and also show that phrase retrieval can be improved\nto achieve competitive performance in document-retrieval tasks such as entity\nlinking and knowledge-grounded dialogue. Finally, we demonstrate how phrase\nfiltering and vector quantization can reduce the size of our index by 4-10x,\nmaking dense phrase retrieval a practical and versatile solution in\nmulti-granularity retrieval.", "published": "2021-09-16 17:42:45", "link": "http://arxiv.org/abs/2109.08133v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Numerical reasoning in machine reading comprehension tasks: are we there\n  yet?", "abstract": "Numerical reasoning based machine reading comprehension is a task that\ninvolves reading comprehension along with using arithmetic operations such as\naddition, subtraction, sorting, and counting. The DROP benchmark (Dua et al.,\n2019) is a recent dataset that has inspired the design of NLP models aimed at\nsolving this task. The current standings of these models in the DROP\nleaderboard, over standard metrics, suggest that the models have achieved\nnear-human performance. However, does this mean that these models have learned\nto reason? In this paper, we present a controlled study on some of the\ntop-performing model architectures for the task of numerical reasoning. Our\nobservations suggest that the standard metrics are incapable of measuring\nprogress towards such tasks.", "published": "2021-09-16 20:13:56", "link": "http://arxiv.org/abs/2109.08207v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Procedures as Programs: Hierarchical Control of Situated Agents through\n  Natural Language", "abstract": "When humans conceive how to perform a particular task, they do so\nhierarchically: splitting higher-level tasks into smaller sub-tasks. However,\nin the literature on natural language (NL) command of situated agents, most\nworks have treated the procedures to be executed as flat sequences of simple\nactions, or any hierarchies of procedures have been shallow at best. In this\npaper, we propose a formalism of procedures as programs, a powerful yet\nintuitive method of representing hierarchical procedural knowledge for agent\ncommand and control. We further propose a modeling paradigm of hierarchical\nmodular networks, which consist of a planner and reactors that convert NL\nintents to predictions of executable programs and probe the environment for\ninformation necessary to complete the program execution. We instantiate this\nframework on the IQA and ALFRED datasets for NL instruction following. Our\nmodel outperforms reactive baselines by a large margin on both datasets. We\nalso demonstrate that our framework is more data-efficient, and that it allows\nfor fast iterative development.", "published": "2021-09-16 20:36:21", "link": "http://arxiv.org/abs/2109.08214v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SenTag: a Web-based Tool for Semantic Annotation of Textual Documents", "abstract": "In this work, we present SenTag, a lightweight web-based tool focused on\nsemantic annotation of textual documents. The platform allows multiple users to\nwork on a corpus of documents. The tool enables to tag a corpus of documents\nthrough an intuitive and easy-to-use user interface that adopts the Extensible\nMarkup Language (XML) as output format. The main goal of the application is\ntwo-fold: facilitating the tagging process and reducing or avoiding for errors\nin the output documents. Moreover, it allows to identify arguments and other\nentities that are used to build an arguments graph. It is also possible to\nassess the level of agreement of annotators working on a corpus of text.", "published": "2021-09-16 08:39:33", "link": "http://arxiv.org/abs/2110.15062v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Transferable Persona-Grounded Dialogues via Grounded Minimal Edits", "abstract": "Grounded dialogue models generate responses that are grounded on certain\nconcepts. Limited by the distribution of grounded dialogue data, models trained\non such data face the transferability challenges in terms of the data\ndistribution and the type of grounded concepts. To address the challenges, we\npropose the grounded minimal editing framework, which minimally edits existing\nresponses to be grounded on the given concept. Focusing on personas, we propose\nGrounded Minimal Editor (GME), which learns to edit by disentangling and\nrecombining persona-related and persona-agnostic parts of the response. To\nevaluate persona-grounded minimal editing, we present the PersonaMinEdit\ndataset, and experimental results show that GME outperforms competitive\nbaselines by a large margin. To evaluate the transferability, we experiment on\nthe test set of BlendedSkillTalk and show that GME can edit dialogue models'\nresponses to largely improve their persona consistency while preserving the use\nof knowledge and empathy.", "published": "2021-09-16 04:15:42", "link": "http://arxiv.org/abs/2109.07713v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scaling Laws for Neural Machine Translation", "abstract": "We present an empirical study of scaling properties of encoder-decoder\nTransformer models used in neural machine translation (NMT). We show that\ncross-entropy loss as a function of model size follows a certain scaling law.\nSpecifically (i) We propose a formula which describes the scaling behavior of\ncross-entropy loss as a bivariate function of encoder and decoder size, and\nshow that it gives accurate predictions under a variety of scaling approaches\nand languages; we show that the total number of parameters alone is not\nsufficient for such purposes. (ii) We observe different power law exponents\nwhen scaling the decoder vs scaling the encoder, and provide recommendations\nfor optimal allocation of encoder/decoder capacity based on this observation.\n(iii) We also report that the scaling behavior of the model is acutely\ninfluenced by composition bias of the train/test sets, which we define as any\ndeviation from naturally generated text (either via machine generated or human\ntranslated text). We observe that natural text on the target side enjoys\nscaling, which manifests as successful reduction of the cross-entropy loss.\n(iv) Finally, we investigate the relationship between the cross-entropy loss\nand the quality of the generated translations. We find two different behaviors,\ndepending on the nature of the test data. For test sets which were originally\ntranslated from target language to source language, both loss and BLEU score\nimprove as model size increases. In contrast, for test sets originally\ntranslated from source language to target language, the loss improves, but the\nBLEU score stops improving after a certain threshold. We release generated text\nfrom all models used in this study.", "published": "2021-09-16 06:15:20", "link": "http://arxiv.org/abs/2109.07740v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Reframing Instructional Prompts to GPTk's Language", "abstract": "What kinds of instructional prompts are easier to follow for Language Models\n(LMs)? We study this question by conducting extensive empirical analysis that\nshed light on important features of successful instructional prompts.\nSpecifically, we study several classes of reframing techniques for manual\nreformulation of prompts into more effective ones. Some examples include\ndecomposing a complex task instruction into multiple simpler tasks or itemizing\ninstructions into sequential steps. Our experiments compare the zero-shot and\nfew-shot performance of LMs prompted with reframed instructions on 12 NLP tasks\nacross 6 categories. Compared with original instructions, our reframed\ninstructions lead to significant improvements across LMs with different sizes.\nFor example, the same reframed prompts boost few-shot performance of\nGPT3-series and GPT2-series by 12.5% and 6.7% respectively averaged over all\ntasks. Furthermore, reframed instructions reduce the number of examples\nrequired to prompt LMs in the few-shot setting. We hope these\nempirically-driven techniques will pave the way towards more effective future\nprompting algorithms.", "published": "2021-09-16 09:44:43", "link": "http://arxiv.org/abs/2109.07830v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Humanly Certifying Superhuman Classifiers", "abstract": "Estimating the performance of a machine learning system is a longstanding\nchallenge in artificial intelligence research. Today, this challenge is\nespecially relevant given the emergence of systems which appear to increasingly\noutperform human beings. In some cases, this \"superhuman\" performance is\nreadily demonstrated; for example by defeating legendary human players in\ntraditional two player games. On the other hand, it can be challenging to\nevaluate classification models that potentially surpass human performance.\nIndeed, human annotations are often treated as a ground truth, which implicitly\nassumes the superiority of the human over any models trained on human\nannotations. In reality, human annotators can make mistakes and be subjective.\nEvaluating the performance with respect to a genuine oracle may be more\nobjective and reliable, even when querying the oracle is expensive or\nimpossible. In this paper, we first raise the challenge of evaluating the\nperformance of both humans and models with respect to an oracle which is\nunobserved. We develop a theory for estimating the accuracy compared to the\noracle, using only imperfect human annotations for reference. Our analysis\nprovides a simple recipe for detecting and certifying superhuman performance in\nthis setting, which we believe will assist in understanding the stage of\ncurrent research on classification. We validate the convergence of the bounds\nand the assumptions of our theory on carefully designed toy experiments with\nknown oracles. Moreover, we demonstrate the utility of our theory by\nmeta-analyzing large-scale natural language processing tasks, for which an\noracle does not exist, and show that under our assumptions a number of models\nfrom recent years are with high probability superhuman.", "published": "2021-09-16 11:00:05", "link": "http://arxiv.org/abs/2109.07867v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Context-NER : Contextual Phrase Generation at Scale", "abstract": "Named Entity Recognition (NER) has seen significant progress in recent years,\nwith numerous state-of-the-art (SOTA) models achieving high performance.\nHowever, very few studies have focused on the generation of entities' context.\nIn this paper, we introduce CONTEXT-NER, a task that aims to generate the\nrelevant context for entities in a sentence, where the context is a phrase\ndescribing the entity but not necessarily present in the sentence. To\nfacilitate research in this task, we also present the EDGAR10-Q dataset, which\nconsists of annual and quarterly reports from the top 1500 publicly traded\ncompanies. The dataset is the largest of its kind, containing 1M sentences,\n2.8M entities, and an average of 35 tokens per sentence, making it a\nchallenging dataset. We propose a baseline approach that combines a phrase\ngeneration algorithm with inferencing using a 220M language model, achieving a\nROUGE-L score of 27% on the test split. Additionally, we perform a one-shot\ninference with ChatGPT, which obtains a 30% ROUGE-L, highlighting the\ndifficulty of the dataset. We also evaluate models such as T5 and BART, which\nachieve a maximum ROUGE-L of 49% after supervised finetuning on EDGAR10-Q. We\nalso find that T5-large, when pre-finetuned on EDGAR10-Q, achieve SOTA results\non downstream finance tasks such as Headline, FPB, and FiQA SA, outperforming\nvanilla version by 10.81 points. To our surprise, this 66x smaller\npre-finetuned model also surpasses the finance-specific LLM BloombergGPT-50B by\n15 points. We hope that our dataset and generated artifacts will encourage\nfurther research in this direction, leading to the development of more\nsophisticated language models for financial text analysis", "published": "2021-09-16 16:10:05", "link": "http://arxiv.org/abs/2109.08079v4", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Fast-Slow Transformer for Visually Grounding Speech", "abstract": "We present Fast-Slow Transformer for Visually Grounding Speech, or FaST-VGS.\nFaST-VGS is a Transformer-based model for learning the associations between raw\nspeech waveforms and visual images. The model unifies dual-encoder and\ncross-attention architectures into a single model, reaping the superior\nretrieval speed of the former along with the accuracy of the latter. FaST-VGS\nachieves state-of-the-art speech-image retrieval accuracy on benchmark\ndatasets, and its learned representations exhibit strong performance on the\nZeroSpeech 2021 phonetic and semantic tasks.", "published": "2021-09-16 18:45:45", "link": "http://arxiv.org/abs/2109.08186v4", "categories": ["eess.AS", "cs.CL", "cs.IR"], "primary_category": "eess.AS"}
{"title": "Utterance-level neural confidence measure for end-to-end children speech\n  recognition", "abstract": "Confidence measure is a performance index of particular importance for\nautomatic speech recognition (ASR) systems deployed in real-world scenarios. In\nthe present study, utterance-level neural confidence measure (NCM) in\nend-to-end automatic speech recognition (E2E ASR) is investigated. The E2E\nsystem adopts the joint CTC-attention Transformer architecture. The prediction\nof NCM is formulated as a task of binary classification, i.e., accept/reject\nthe input utterance, based on a set of predictor features acquired during the\nASR decoding process. The investigation is focused on evaluating and comparing\nthe efficacies of predictor features that are derived from different internal\nand external modules of the E2E system. Experiments are carried out on children\nspeech, for which state-of-the-art ASR systems show less than satisfactory\nperformance and robust confidence measure is particularly useful. It is noted\nthat predictor features related to acoustic information of speech play a more\nimportant role in estimating confidence measure than those related to\nlinguistic information. N-best score features show significantly better\nperformance than single-best ones. It has also been shown that the metrics of\nEER and AUC are not appropriate to evaluate the NCM of a mismatched ASR with\nsignificant performance gap.", "published": "2021-09-16 06:49:20", "link": "http://arxiv.org/abs/2109.07750v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DDS: A new device-degraded speech dataset for speech enhancement", "abstract": "A large and growing amount of speech content in real-life scenarios is being\nrecorded on consumer-grade devices in uncontrolled environments, resulting in\ndegraded speech quality. Transforming such low-quality device-degraded speech\ninto high-quality speech is a goal of speech enhancement (SE). This paper\nintroduces a new speech dataset, DDS, to facilitate the research on SE. DDS\nprovides aligned parallel recordings of high-quality speech (recorded in\nprofessional studios) and a number of versions of low-quality speech, producing\napproximately 2,000 hours speech data. The DDS dataset covers 27 realistic\nrecording conditions by combining diverse acoustic environments and microphone\ndevices, and each version of a condition consists of multiple recordings from\nsix microphone positions to simulate different noise and reverberation levels.\nWe also test several SE baseline systems on the DDS dataset and show the impact\nof recording diversity on performance.", "published": "2021-09-16 12:27:15", "link": "http://arxiv.org/abs/2109.07931v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "PDAugment: Data Augmentation by Pitch and Duration Adjustments for\n  Automatic Lyrics Transcription", "abstract": "Automatic lyrics transcription (ALT), which can be regarded as automatic\nspeech recognition (ASR) on singing voice, is an interesting and practical\ntopic in academia and industry. ALT has not been well developed mainly due to\nthe dearth of paired singing voice and lyrics datasets for model training.\nConsidering that there is a large amount of ASR training data, a\nstraightforward method is to leverage ASR data to enhance ALT training.\nHowever, the improvement is marginal when training the ALT system directly with\nASR data, because of the gap between the singing voice and standard speech data\nwhich is rooted in music-specific acoustic characteristics in singing voice. In\nthis paper, we propose PDAugment, a data augmentation method that adjusts pitch\nand duration of speech at syllable level under the guidance of music scores to\nhelp ALT training. Specifically, we adjust the pitch and duration of each\nsyllable in natural speech to those of the corresponding note extracted from\nmusic scores, so as to narrow the gap between natural speech and singing voice.\nExperiments on DSing30 and Dali corpus show that the ALT system equipped with\nour PDAugment outperforms previous state-of-the-art systems by 5.9% and 18.1%\nWERs respectively, demonstrating the effectiveness of PDAugment for ALT.", "published": "2021-09-16 12:48:25", "link": "http://arxiv.org/abs/2109.07940v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "NORESQA: A Framework for Speech Quality Assessment using Non-Matching\n  References", "abstract": "The perceptual task of speech quality assessment (SQA) is a challenging task\nfor machines to do. Objective SQA methods that rely on the availability of the\ncorresponding clean reference have been the primary go-to approaches for SQA.\nClearly, these methods fail in real-world scenarios where the ground truth\nclean references are not available. In recent years, non-intrusive methods that\ntrain neural networks to predict ratings or scores have attracted much\nattention, but they suffer from several shortcomings such as lack of\nrobustness, reliance on labeled data for training and so on. In this work, we\npropose a new direction for speech quality assessment. Inspired by human's\ninnate ability to compare and assess the quality of speech signals even when\nthey have non-matching contents, we propose a novel framework that predicts a\nsubjective relative quality score for the given speech signal with respect to\nany provided reference without using any subjective data. We show that neural\nnetworks trained using our framework produce scores that correlate well with\nsubjective mean opinion scores (MOS) and are also competitive to methods such\nas DNSMOS, which explicitly relies on MOS from humans for training networks.\nMoreover, our method also provides a natural way to embed quality-related\ninformation in neural networks, which we show is helpful for downstream tasks\nsuch as speech enhancement.", "published": "2021-09-16 17:27:12", "link": "http://arxiv.org/abs/2109.08125v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Telehealthcare and Telepathology in Pandemic: A Noninvasive, Low-Cost\n  Micro-Invasive and Multimodal Real-Time Online Application for Early\n  Diagnosis of COVID-19 Infection", "abstract": "To contain the spread of the virus and stop the overcrowding of hospitalized\npatients, the coronavirus pandemic crippled healthcare facilities, mandating\nlockdowns and promoting remote work. As a result, telehealth has become\nincreasingly popular for offering low-risk care to patients. However, the\ndifficulty of preventing the next potential waves of infection has increased by\nconstant virus mutation into new forms and a general lack of test kits,\nparticularly in developing nations. In this research, a unique cloud-based\napplication for the early identification of individuals who may have COVID-19\ninfection is proposed. The application provides five modes of diagnosis from\npossible symptoms (f1), cough sound (f2), specific blood biomarkers (f3), Raman\nspectral data of blood specimens (f4), and ECG signal paper-based image (f5).\nWhen a user selects an option and enters the information, the data is sent to\nthe cloud server. The deployed machine learning (ML) and deep learning (DL)\nmodels classify the data in real time and inform the user of the likelihood of\nCOVID-19 infection. Our deployed models can classify with an accuracy of 100%,\n99.80%, 99.55%, 95.65%, and 77.59% from f3, f4, f5, f2, and f1 respectively.\nMoreover, the sensitivity for f2, f3, and f4 is 100%, which indicates the\ncorrect identification of COVID positive patients. This is significant in\nlimiting the spread of the virus. Additionally, another ML model, as seen to\noffer 92% accuracy serves to identify patients who, out of a large group of\npatients admitted to the hospital cohort, need immediate critical care support\nby estimating the mortality risk of patients from blood parameters. The\ninstantaneous multimodal nature of our technique offers multiplex and accurate\ndiagnostic methods, highlighting the effectiveness of telehealth as a simple,\nwidely available, and low-cost diagnostic solution, even for future pandemics.", "published": "2021-09-16 10:22:31", "link": "http://arxiv.org/abs/2109.07846v2", "categories": ["cs.LG", "cs.SD", "eess.AS", "q-bio.BM"], "primary_category": "cs.LG"}
{"title": "Graph Fourier Transform based Audio Zero-watermarking", "abstract": "The frequent exchange of multimedia information in the present era projects\nan increasing demand for copyright protection. In this work, we propose a novel\naudio zero-watermarking technology based on graph Fourier transform for\nenhancing the robustness with respect to copyright protection. In this\napproach, the combined shift operator is used to construct the graph signal,\nupon which the graph Fourier analysis is performed. The selected maximum\nabsolute graph Fourier coefficients representing the characteristics of the\naudio segment are then encoded into a feature binary sequence using K-means\nalgorithm. Finally, the resultant feature binary sequence is XOR-ed with the\nwatermark binary sequence to realize the embedding of the zero-watermarking.\nThe experimental studies show that the proposed approach performs more\neffectively in resisting common or synchronization attacks than the existing\nstate-of-the-art methods.", "published": "2021-09-16 14:31:32", "link": "http://arxiv.org/abs/2109.08007v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
