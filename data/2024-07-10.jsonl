{"title": "Financial market geometry: The tube oscillator", "abstract": "Based on geometrical considerations, we propose a new oscillator for\ntechnical market analysis, the tube oscillator. This oscillator measures the\ntrending behavior of a fixed market instrument based on its past history. It is\nshown in an empirical analysis of the German DAX and the Forex EUR/USD exchange\nrate that a simple trading strategy based on this oscillator and fixed\nthreshold leads to consistent positive monthly returns of average magnitude of\n2% or more. The oscillator is derived from a broader understanding of the\ngeometric behavior of prices throughout a fixed period, which we term financial\nmarket geometry. The remarkable profit results of the presented technique show\nthat 1) prices of financial market instruments have a strong underlying\ndeterministic component which can be detected and quantified with a matching\napproach and 2) financial market geometry is capable of providing such\ndetectors.", "published": "2024-07-10 20:33:16", "link": "http://arxiv.org/abs/2407.08036v1", "categories": ["q-fin.TR", "91-10"], "primary_category": "q-fin.TR"}
{"title": "ETM: Modern Insights into Perspective on Text-to-SQL Evaluation in the\n  Age of Large Language Models", "abstract": "The task of Text-to-SQL enables anyone to retrieve information from SQL\ndatabases using natural language. While this task has made substantial\nprogress, the two primary evaluation metrics -- Execution Accuracy (EXE) and\nExact Set Matching Accuracy (ESM) -- suffer from inherent limitations that can\nmisrepresent performance. Specifically, ESM's rigid matching overlooks\nsemantically correct but stylistically different queries, whereas EXE can\noverestimate correctness by ignoring structural errors that yield correct\noutputs. These shortcomings become especially problematic when assessing\noutputs from large language model (LLM)-based approaches without fine-tuning,\nwhich vary more in style and structure compared to their fine-tuned\ncounterparts. Thus, we introduce a new metric, Enhanced Tree Matching (ETM),\nwhich mitigates these issues by comparing queries using both syntactic and\nsemantic elements. Through evaluating nine LLM-based models, we show that EXE\nand ESM can produce false positive and negative rates as high as 23.0% and\n28.9%, while ETM reduces these rates to 0.3% and 2.7%, respectively. We release\nour ETM script as open source, offering the community a more robust and\nreliable approach to evaluating Text-to-SQL.", "published": "2024-07-10 02:20:19", "link": "http://arxiv.org/abs/2407.07313v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Examining Long-Context Large Language Models for Environmental Review\n  Document Comprehension", "abstract": "As LLMs become increasingly ubiquitous, researchers have tried various\ntechniques to augment the knowledge provided to these models. Long context and\nretrieval-augmented generation (RAG) are two such methods that have recently\ngained popularity. In this work, we examine the benefits of both of these\ntechniques by utilizing question answering (QA) task in a niche domain. While\nthe effectiveness of LLM-based QA systems has already been established at an\nacceptable level in popular domains such as trivia and literature, it has not\noften been established in niche domains that traditionally require specialized\nexpertise. We construct the NEPAQuAD1.0 benchmark to evaluate the performance\nof five long-context LLMs -- Claude Sonnet, Gemini, GPT-4, Llama 3.1, and\nMistral -- when answering questions originating from Environmental Impact\nStatements prepared by U.S. federal government agencies in accordance with the\nNational Environmental Environmental Act (NEPA). We specifically measure the\nability of LLMs to understand the nuances of legal, technical, and\ncompliance-related information present in NEPA documents in different\ncontextual scenarios. We test the LLMs' internal prior NEPA knowledge by\nproviding questions without any context, as well as assess how LLMs synthesize\nthe contextual information present in long NEPA documents to facilitate the\nquestion/answering task. We compare the performance of the models in handling\ndifferent types of questions (e.g., problem-solving, divergent, etc.). Our\nresults suggest that RAG powered models significantly outperform those provided\nwith only the PDF context in terms of answer accuracy, regardless of the choice\nof the LLM. Our further analysis reveals that many models perform better\nanswering closed type questions (Yes/No) than divergent and problem-solving\nquestions.", "published": "2024-07-10 02:33:09", "link": "http://arxiv.org/abs/2407.07321v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probability of Differentiation Reveals Brittleness of Homogeneity Bias\n  in GPT-4", "abstract": "Homogeneity bias in Large Language Models (LLMs) refers to their tendency to\nhomogenize the representations of some groups compared to others. Previous\nstudies documenting this bias have predominantly used encoder models, which may\nhave inadvertently introduced biases. To address this limitation, we prompted\nGPT-4 to generate single word/expression completions associated with 18\nsituation cues-specific, measurable elements of environments that influence how\nindividuals perceive situations and compared the variability of these\ncompletions using probability of differentiation. This approach directly\nassessed homogeneity bias from the model's outputs, bypassing encoder models.\nAcross five studies, we find that homogeneity bias is highly volatile across\nsituation cues and writing prompts, suggesting that the bias observed in past\nwork may reflect those within encoder models rather than LLMs. Furthermore, we\nfind that homogeneity bias in LLMs is brittle, as even minor and arbitrary\nchanges in prompts can significantly alter the expression of biases. Future\nwork should further explore how variations in syntactic features and topic\nchoices in longer text generations influence homogeneity bias in LLMs.", "published": "2024-07-10 02:56:55", "link": "http://arxiv.org/abs/2407.07329v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Blending: LLM Safety Alignment Evaluation with Language\n  Mixture", "abstract": "As safety remains a crucial concern throughout the development lifecycle of\nLarge Language Models (LLMs), researchers and industrial practitioners have\nincreasingly focused on safeguarding and aligning LLM behaviors with human\npreferences and ethical standards. LLMs, trained on extensive multilingual\ncorpora, exhibit powerful generalization abilities across diverse languages and\ndomains. However, current safety alignment practices predominantly focus on\nsingle-language scenarios, which leaves their effectiveness in complex\nmultilingual contexts, especially for those complex mixed-language formats,\nlargely unexplored. In this study, we introduce Multilingual Blending, a\nmixed-language query-response scheme designed to evaluate the safety alignment\nof various state-of-the-art LLMs (e.g., GPT-4o, GPT-3.5, Llama3) under\nsophisticated, multilingual conditions. We further investigate language\npatterns such as language availability, morphology, and language family that\ncould impact the effectiveness of Multilingual Blending in compromising the\nsafeguards of LLMs. Our experimental results show that, without meticulously\ncrafted prompt templates, Multilingual Blending significantly amplifies the\ndetriment of malicious queries, leading to dramatically increased bypass rates\nin LLM safety alignment (67.23% on GPT-3.5 and 40.34% on GPT-4o), far exceeding\nthose of single-language baselines. Moreover, the performance of Multilingual\nBlending varies notably based on intrinsic linguistic properties, with\nlanguages of different morphology and from diverse families being more prone to\nevading safety alignments. These findings underscore the necessity of\nevaluating LLMs and developing corresponding safety alignment strategies in a\ncomplex, multilingual context to align with their superior cross-language\ngeneralization capabilities.", "published": "2024-07-10 03:26:15", "link": "http://arxiv.org/abs/2407.07342v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LokiLM: Technical Report", "abstract": "In this work, we introduce LokiLM, a 1.4B parameter large language model\ntrained on 500B tokens. Our model performs strongly in natural language\nreasoning tasks and achieves state-of-the-art performance among models with\n1.5B parameters or less. LokiLM is trained using multi-teacher knowledge\ndistillation and high-quality training data to achieve benchmark results\ncompetitive with larger models trained on significantly more tokens. We support\nthese findings by introducing steps to avoid benchmark contamination and\noverfitting throughout our development process. Despite its promising\nperformance, LokiLM exhibits a concerning amount of hallucinations and scores\npoorly on the TruthfulQA benchmark, so we do not release the model publicly.", "published": "2024-07-10 05:05:47", "link": "http://arxiv.org/abs/2407.07370v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KpopMT: Translation Dataset with Terminology for Kpop Fandom", "abstract": "While machines learn from existing corpora, humans have the unique capability\nto establish and accept new language systems. This makes human form unique\nlanguage systems within social groups. Aligning with this, we focus on a gap\nremaining in addressing translation challenges within social groups, where\nin-group members utilize unique terminologies. We propose KpopMT dataset, which\naims to fill this gap by enabling precise terminology translation, choosing\nKpop fandom as an initiative for social groups given its global popularity.\nExpert translators provide 1k English translations for Korean posts and\ncomments, each annotated with specific terminology within social groups'\nlanguage systems. We evaluate existing translation systems including GPT models\non KpopMT to identify their failure cases. Results show overall low scores,\nunderscoring the challenges of reflecting group-specific terminologies and\nstyles in translation. We make KpopMT publicly available.", "published": "2024-07-10 07:14:51", "link": "http://arxiv.org/abs/2407.07413v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Review-LLM: Harnessing Large Language Models for Personalized Review\n  Generation", "abstract": "Product review generation is an important task in recommender systems, which\ncould provide explanation and persuasiveness for the recommendation. Recently,\nLarge Language Models (LLMs, e.g., ChatGPT) have shown superior text modeling\nand generating ability, which could be applied in review generation. However,\ndirectly applying the LLMs for generating reviews might be troubled by the\n``polite'' phenomenon of the LLMs and could not generate personalized reviews\n(e.g., negative reviews). In this paper, we propose Review-LLM that customizes\nLLMs for personalized review generation. Firstly, we construct the prompt input\nby aggregating user historical behaviors, which include corresponding item\ntitles and reviews. This enables the LLMs to capture user interest features and\nreview writing style. Secondly, we incorporate ratings as indicators of\nsatisfaction into the prompt, which could further improve the model's\nunderstanding of user preferences and the sentiment tendency control of\ngenerated reviews. Finally, we feed the prompt text into LLMs, and use\nSupervised Fine-Tuning (SFT) to make the model generate personalized reviews\nfor the given user and target item. Experimental results on the real-world\ndataset show that our fine-tuned model could achieve better review generation\nperformance than existing close-source LLMs.", "published": "2024-07-10 09:22:19", "link": "http://arxiv.org/abs/2407.07487v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bucket Pre-training is All You Need", "abstract": "Large language models (LLMs) have demonstrated exceptional performance across\nvarious natural language processing tasks. However, the conventional\nfixed-length data composition strategy for pretraining, which involves\nconcatenating and splitting documents, can introduce noise and limit the\nmodel's ability to capture long-range dependencies. To address this, we first\nintroduce three metrics for evaluating data composition quality: padding ratio,\ntruncation ratio, and concatenation ratio. We further propose a multi-bucket\ndata composition method that moves beyond the fixed-length paradigm, offering a\nmore flexible and efficient approach to pretraining. Extensive experiments\ndemonstrate that our proposed method could significantly improving both the\nefficiency and efficacy of LLMs pretraining. Our approach not only reduces\nnoise and preserves context but also accelerates training, making it a\npromising solution for LLMs pretraining.", "published": "2024-07-10 09:27:23", "link": "http://arxiv.org/abs/2407.07495v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Benchmarking: A New Paradigm for Evaluation and Assessment of\n  Large Language Models", "abstract": "In current benchmarks for evaluating large language models (LLMs), there are\nissues such as evaluation content restriction, untimely updates, and lack of\noptimization guidance. In this paper, we propose a new paradigm for the\nmeasurement of LLMs: Benchmarking-Evaluation-Assessment. Our paradigm shifts\nthe \"location\" of LLM evaluation from the \"examination room\" to the \"hospital\".\nThrough conducting a \"physical examination\" on LLMs, it utilizes specific\ntask-solving as the evaluation content, performs deep attribution of existing\nproblems within LLMs, and provides recommendation for optimization.", "published": "2024-07-10 10:42:02", "link": "http://arxiv.org/abs/2407.07531v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Leakage of Code Generation Evaluation Datasets", "abstract": "In this paper, we consider contamination by code generation test sets, in\nparticular in their use in modern large language models. We discuss three\npossible sources of such contamination and show findings supporting each of\nthem: (i) direct data leakage, (ii) indirect data leakage through the use of\nsynthetic data and (iii) overfitting to evaluation sets during model selection.\nTo address this, we release Less Basic Python Problems (LBPP): an\nuncontaminated new benchmark of 161 prompts with their associated Python\nsolutions. LBPP is released at https://huggingface.co/datasets/CohereForAI/lbpp .", "published": "2024-07-10 11:50:20", "link": "http://arxiv.org/abs/2407.07565v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Psycho-linguistic Experiment on Universal Semantic Components of Verbal\n  Humor: System Description and Annotation", "abstract": "Objective criteria for universal semantic components that distinguish a\nhumorous utterance from a non-humorous one are presently under debate. In this\narticle, we give an in-depth observation of our system of self-paced reading\nfor annotation of humor, that collects readers' annotations while they open a\ntext word by word. The system registers keys that readers press to open the\nnext word, choose a class (humorous versus non-humorous texts), change their\nchoice. We also touch upon our psycho-linguistic experiment conducted with the\nsystem and the data collected during it.", "published": "2024-07-10 12:56:17", "link": "http://arxiv.org/abs/2407.07617v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Review of the Challenges with Massive Web-mined Corpora Used in Large\n  Language Models Pre-Training", "abstract": "This article presents a comprehensive review of the challenges associated\nwith using massive web-mined corpora for the pre-training of large language\nmodels (LLMs). This review identifies key challenges in this domain, including\nchallenges such as noise (irrelevant or misleading information), duplication of\ncontent, the presence of low-quality or incorrect information, biases, and the\ninclusion of sensitive or personal information in web-mined corpora. Addressing\nthese issues is crucial for the development of accurate, reliable, and\nethically responsible language models. Through an examination of current\nmethodologies for data cleaning, pre-processing, bias detection and mitigation,\nwe highlight the gaps in existing approaches and suggest directions for future\nresearch. Our discussion aims to catalyze advancements in developing more\nsophisticated and ethically responsible LLMs.", "published": "2024-07-10 13:09:23", "link": "http://arxiv.org/abs/2407.07630v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WorldAPIs: The World Is Worth How Many APIs? A Thought Experiment", "abstract": "AI systems make decisions in physical environments through primitive actions\nor affordances that are accessed via API calls. While deploying AI agents in\nthe real world involves numerous high-level actions, existing embodied\nsimulators offer a limited set of domain-salient APIs. This naturally brings up\nthe questions: how many primitive actions (APIs) are needed for a versatile\nembodied agent, and what should they look like? We explore this via a thought\nexperiment: assuming that wikiHow tutorials cover a wide variety of\nhuman-written tasks, what is the space of APIs needed to cover these\ninstructions? We propose a framework to iteratively induce new APIs by\ngrounding wikiHow instruction to situated agent policies. Inspired by recent\nsuccesses in large language models (LLMs) for embodied planning, we propose a\nfew-shot prompting to steer GPT-4 to generate Pythonic programs as agent\npolicies and bootstrap a universe of APIs by 1) reusing a seed set of APIs; and\nthen 2) fabricate new API calls when necessary. The focus of this thought\nexperiment is on defining these APIs rather than their executability. We apply\nthe proposed pipeline on instructions from wikiHow tutorials. On a small\nfraction (0.5%) of tutorials, we induce an action space of 300+ APIs necessary\nfor capturing the rich variety of tasks in the physical world. A detailed\nautomatic and human analysis of the induction output reveals that the proposed\npipeline enables effective reuse and creation of APIs. Moreover, a manual\nreview revealed that existing simulators support only a small subset of the\ninduced APIs (9 of the top 50 frequent APIs), motivating the development of\naction-rich embodied environments.", "published": "2024-07-10 15:52:44", "link": "http://arxiv.org/abs/2407.07778v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent\n  Communities", "abstract": "The rapid adoption of large language models (LLMs) in multi-agent systems has\nhighlighted their impressive capabilities in various applications, such as\ncollaborative problem-solving and autonomous negotiation. However, the security\nimplications of these LLM-based multi-agent systems have not been thoroughly\ninvestigated, particularly concerning the spread of manipulated knowledge. In\nthis paper, we investigate this critical issue by constructing a detailed\nthreat model and a comprehensive simulation environment that mirrors real-world\nmulti-agent deployments in a trusted platform. Subsequently, we propose a novel\ntwo-stage attack method involving Persuasiveness Injection and Manipulated\nKnowledge Injection to systematically explore the potential for manipulated\nknowledge (i.e., counterfactual and toxic knowledge) spread without explicit\nprompt manipulation.\n  Our method leverages the inherent vulnerabilities of LLMs in handling world\nknowledge, which can be exploited by attackers to unconsciously spread\nfabricated information. Through extensive experiments, we demonstrate that our\nattack method can successfully induce LLM-based agents to spread both\ncounterfactual and toxic knowledge without degrading their foundational\ncapabilities during agent communication. Furthermore, we show that these\nmanipulations can persist through popular retrieval-augmented generation\nframeworks, where several benign agents store and retrieve manipulated chat\nhistories for future interactions. This persistence indicates that even after\nthe interaction has ended, the benign agents may continue to be influenced by\nmanipulated knowledge. Our findings reveal significant security risks in\nLLM-based multi-agent systems, emphasizing the imperative need for robust\ndefenses against manipulated knowledge spread, such as introducing ``guardian''\nagents and advanced fact-checking tools.", "published": "2024-07-10 16:08:46", "link": "http://arxiv.org/abs/2407.07791v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attribute or Abstain: Large Language Models as Long Document Assistants", "abstract": "LLMs can help humans working with long documents, but are known to\nhallucinate. Attribution can increase trust in LLM responses: The LLM provides\nevidence that supports its response, which enhances verifiability. Existing\napproaches to attribution have only been evaluated in RAG settings, where the\ninitial retrieval confounds LLM performance. This is crucially different from\nthe long document setting, where retrieval is not needed, but could help. Thus,\na long document specific evaluation of attribution is missing. To fill this\ngap, we present LAB, a benchmark of 6 diverse long document tasks with\nattribution, and experiments with different approaches to attribution on 5 LLMs\nof different sizes.\n  We find that citation, i.e. response generation and evidence extraction in\none step, performs best for large and fine-tuned models, while additional\nretrieval can help for small, prompted models. We investigate whether the \"Lost\nin the Middle'' phenomenon exists for attribution, but do not find this. We\nalso find that evidence quality can predict response quality on datasets with\nsimple responses, but not so for complex responses, as models struggle with\nproviding evidence for complex claims.", "published": "2024-07-10 16:16:02", "link": "http://arxiv.org/abs/2407.07799v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Overshadowing Causes Amalgamated Hallucination in Large\n  Language Models", "abstract": "Hallucination is often regarded as a major impediment for using large\nlanguage models (LLMs), especially for knowledge-intensive tasks. Even when the\ntraining corpus consists solely of true statements, language models still\ngenerate hallucinations in the form of amalgamations of multiple facts. We coin\nthis phenomenon as ``knowledge overshadowing'': when we query knowledge from a\nlanguage model with multiple conditions, some conditions overshadow others,\nleading to hallucinated outputs. This phenomenon partially stems from training\ndata imbalance, which we verify on both pretrained models and fine-tuned\nmodels, over a wide range of LM model families and sizes.From a theoretical\npoint of view, knowledge overshadowing can be interpreted as\nover-generalization of the dominant conditions (patterns). We show that the\nhallucination rate grows with both the imbalance ratio (between the popular and\nunpopular condition) and the length of dominant condition description,\nconsistent with our derived generalization bound. Finally, we propose to\nutilize overshadowing conditions as a signal to catch hallucination before it\nis produced, along with a training-free self-contrastive decoding method to\nalleviate hallucination during inference. Our proposed approach showcases up to\n82% F1 for hallucination anticipation and 11.2% to 39.4% hallucination control,\nwith different models and datasets.", "published": "2024-07-10 20:37:42", "link": "http://arxiv.org/abs/2407.08039v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interpretable Differential Diagnosis with Dual-Inference Large Language\n  Models", "abstract": "Automatic differential diagnosis (DDx) is an essential medical task that\ngenerates a list of potential diseases as differentials based on patient\nsymptom descriptions. In practice, interpreting these differential diagnoses\nyields significant value but remains under-explored. Given the powerful\ncapabilities of large language models (LLMs), we investigated using LLMs for\ninterpretable DDx. Specifically, we curated the first DDx dataset with\nexpert-derived interpretation on 570 clinical notes. Besides, we proposed\nDual-Inf, a novel framework that enabled LLMs to conduct bidirectional\ninference (i.e., from symptoms to diagnoses and vice versa) for DDx\ninterpretation. Both human and automated evaluation validated its efficacy in\npredicting and elucidating differentials across four base LLMs. In addition,\nDual-Inf could reduce interpretation errors and hold promise for rare disease\nexplanations. To the best of our knowledge, it is the first work that\ncustomizes LLMs for DDx explanation and comprehensively evaluates their\ninterpretation performance. Overall, our study bridges a critical gap in DDx\ninterpretation and enhances clinical decision-making.", "published": "2024-07-10 02:58:37", "link": "http://arxiv.org/abs/2407.07330v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Guide To Effectively Leveraging LLMs for Low-Resource Text\n  Summarization: Data Augmentation and Semi-supervised Approaches", "abstract": "Existing approaches for low-resource text summarization primarily employ\nlarge language models (LLMs) like GPT-3 or GPT-4 at inference time to generate\nsummaries directly; however, such approaches often suffer from inconsistent LLM\noutputs and are difficult to adapt to domain-specific data in low-resource\nscenarios. In this work, we propose two novel methods to effectively utilize\nLLMs for low-resource text summarization: 1) MixSumm, an LLM-based data\naugmentation regime that synthesizes high-quality documents (short and long)\nfor few-shot text summarization, and 2) PPSL, a prompt-based pseudolabeling\nstrategy for sample-efficient semi-supervised text summarization. Specifically,\nMixSumm leverages the open-source LLaMA-3-70b-Instruct model to generate new\ndocuments by mixing topical information derived from a small seed set, and PPSL\nleverages the LLaMA-3-70b-Instruct model to generate high-quality pseudo-labels\nin a semi-supervised learning setup. We evaluate our methods on the TweetSumm,\nWikiHow, and ArXiv/PubMed datasets and use L-Eval, a LLaMA-3-based evaluation\nmetric, and ROUGE scores to measure the quality of generated summaries. Our\nexperiments on extractive and abstractive summarization show that MixSumm and\nPPSL achieve competitive ROUGE scores as a fully supervised method with 5% of\nthe labeled data.", "published": "2024-07-10 03:25:47", "link": "http://arxiv.org/abs/2407.07341v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Extraction of Disease Risk Factors from Medical Publications", "abstract": "We present a novel approach to automating the identification of risk factors\nfor diseases from medical literature, leveraging pre-trained models in the\nbio-medical domain, while tuning them for the specific task. Faced with the\nchallenges of the diverse and unstructured nature of medical articles, our\nstudy introduces a multi-step system to first identify relevant articles, then\nclassify them based on the presence of risk factor discussions and, finally,\nextract specific risk factor information for a disease through a\nquestion-answering model.\n  Our contributions include the development of a comprehensive pipeline for the\nautomated extraction of risk factors and the compilation of several datasets,\nwhich can serve as valuable resources for further research in this area. These\ndatasets encompass a wide range of diseases, as well as their associated risk\nfactors, meticulously identified and validated through a fine-grained\nevaluation scheme. We conducted both automatic and thorough manual evaluation,\ndemonstrating encouraging results. We also highlight the importance of\nimproving models and expanding dataset comprehensiveness to keep pace with the\nrapidly evolving field of medical research.", "published": "2024-07-10 05:17:55", "link": "http://arxiv.org/abs/2407.07373v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GLBench: A Comprehensive Benchmark for Graph with Large Language Models", "abstract": "The emergence of large language models (LLMs) has revolutionized the way we\ninteract with graphs, leading to a new paradigm called GraphLLM. Despite the\nrapid development of GraphLLM methods in recent years, the progress and\nunderstanding of this field remain unclear due to the lack of a benchmark with\nconsistent experimental protocols. To bridge this gap, we introduce GLBench,\nthe first comprehensive benchmark for evaluating GraphLLM methods in both\nsupervised and zero-shot scenarios. GLBench provides a fair and thorough\nevaluation of different categories of GraphLLM methods, along with traditional\nbaselines such as graph neural networks. Through extensive experiments on a\ncollection of real-world datasets with consistent data processing and splitting\nstrategies, we have uncovered several key findings. Firstly, GraphLLM methods\noutperform traditional baselines in supervised settings, with LLM-as-enhancers\nshowing the most robust performance. However, using LLMs as predictors is less\neffective and often leads to uncontrollable output issues. We also notice that\nno clear scaling laws exist for current GraphLLM methods. In addition, both\nstructures and semantics are crucial for effective zero-shot transfer, and our\nproposed simple baseline can even outperform several models tailored for\nzero-shot scenarios. The data and code of the benchmark can be found at\nhttps://github.com/NineAbyss/GLBench.", "published": "2024-07-10 08:20:47", "link": "http://arxiv.org/abs/2407.07457v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Arabic Automatic Story Generation with Large Language Models", "abstract": "Large language models (LLMs) have recently emerged as a powerful tool for a\nwide range of language generation tasks. Nevertheless, this progress has been\nslower in Arabic. In this work, we focus on the task of generating stories from\nLLMs. For our training, we use stories acquired through machine translation\n(MT) as well as GPT-4. For the MT data, we develop a careful pipeline that\nensures we acquire high-quality stories. For our GPT-41 data, we introduce\ncrafted prompts that allow us to generate data well-suited to the Arabic\ncontext in both Modern Standard Arabic (MSA) and two Arabic dialects (Egyptian\nand Moroccan). For example, we generate stories tailored to various Arab\ncountries on a wide host of topics. Our manual evaluation shows that our model\nfine-tuned on these training datasets can generate coherent stories that adhere\nto our instructions. We also conduct an extensive automatic and human\nevaluation comparing our models against state-of-the-art proprietary and\nopen-source models. Our datasets and models will be made publicly available at\nhttps: //github.com/UBC-NLP/arastories.", "published": "2024-07-10 11:26:10", "link": "http://arxiv.org/abs/2407.07551v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Computational Learning of Construction Grammars: State of the Art\n  and Prospective Roadmap", "abstract": "This paper documents and reviews the state of the art concerning\ncomputational models of construction grammar learning. It brings together prior\nwork on the computational learning of form-meaning pairings, which has so far\nbeen studied in several distinct areas of research. The goal of this paper is\nthreefold. First of all, it aims to synthesise the variety of methodologies\nthat have been proposed to date and the results that have been obtained.\nSecond, it aims to identify those parts of the challenge that have been\nsuccessfully tackled and reveal those that require further research. Finally,\nit aims to provide a roadmap which can help to boost and streamline future\nresearch efforts on the computational learning of large-scale, usage-based\nconstruction grammars.", "published": "2024-07-10 12:45:02", "link": "http://arxiv.org/abs/2407.07606v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Proposed S.C.O.R.E. Evaluation Framework for Large Language Models :\n  Safety, Consensus, Objectivity, Reproducibility and Explainability", "abstract": "A comprehensive qualitative evaluation framework for large language models\n(LLM) in healthcare that expands beyond traditional accuracy and quantitative\nmetrics needed. We propose 5 key aspects for evaluation of LLMs: Safety,\nConsensus, Objectivity, Reproducibility and Explainability (S.C.O.R.E.). We\nsuggest that S.C.O.R.E. may form the basis for an evaluation framework for\nfuture LLM-based models that are safe, reliable, trustworthy, and ethical for\nhealthcare and clinical applications.", "published": "2024-07-10 13:45:16", "link": "http://arxiv.org/abs/2407.07666v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Language of Weather: Social Media Reactions to Weather Accounting\n  for Climatic and Linguistic Baselines", "abstract": "This study explores how different weather conditions influence public\nsentiment on social media, focusing on Twitter data from the UK. By considering\nclimate and linguistic baselines, we improve the accuracy of weather-related\nsentiment analysis. Our findings show that emotional responses to weather are\ncomplex, influenced by combinations of weather variables and regional language\ndifferences. The results highlight the importance of context-sensitive methods\nfor better understanding public mood in response to weather, which can enhance\nimpact-based forecasting and risk communication in the context of climate\nchange.", "published": "2024-07-10 14:08:24", "link": "http://arxiv.org/abs/2407.07683v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Decompose and Compare Consistency: Measuring VLMs' Answer Reliability\n  via Task-Decomposition Consistency Comparison", "abstract": "Despite tremendous advancements, current state-of-the-art Vision-Language\nModels (VLMs) are still far from perfect. They tend to hallucinate and may\ngenerate biased responses. In such circumstances, having a way to assess the\nreliability of a given response generated by a VLM is quite useful. Existing\nmethods, such as estimating uncertainty using answer likelihoods or\nprompt-based confidence generation, often suffer from overconfidence. Other\nmethods use self-consistency comparison but are affected by confirmation\nbiases. To alleviate these, we propose Decompose and Compare Consistency (DeCC)\nfor reliability measurement. By comparing the consistency between the direct\nanswer generated using the VLM's internal reasoning process, and the indirect\nanswers obtained by decomposing the question into sub-questions and reasoning\nover the sub-answers produced by the VLM, DeCC measures the reliability of\nVLM's direct answer. Experiments across six vision-language tasks with three\nVLMs show DeCC's reliability estimation achieves better correlation with task\naccuracy compared to the existing methods.", "published": "2024-07-10 17:00:29", "link": "http://arxiv.org/abs/2407.07840v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "FACTS About Building Retrieval Augmented Generation-based Chatbots", "abstract": "Enterprise chatbots, powered by generative AI, are emerging as key\napplications to enhance employee productivity. Retrieval Augmented Generation\n(RAG), Large Language Models (LLMs), and orchestration frameworks like\nLangchain and Llamaindex are crucial for building these chatbots. However,\ncreating effective enterprise chatbots is challenging and requires meticulous\nRAG pipeline engineering. This includes fine-tuning embeddings and LLMs,\nextracting documents from vector databases, rephrasing queries, reranking\nresults, designing prompts, honoring document access controls, providing\nconcise responses, including references, safeguarding personal information, and\nbuilding orchestration agents. We present a framework for building RAG-based\nchatbots based on our experience with three NVIDIA chatbots: for IT/HR\nbenefits, financial earnings, and general content. Our contributions are\nthree-fold: introducing the FACTS framework (Freshness, Architectures, Cost,\nTesting, Security), presenting fifteen RAG pipeline control points, and\nproviding empirical results on accuracy-latency tradeoffs between large and\nsmall LLMs. To the best of our knowledge, this is the first paper of its kind\nthat provides a holistic view of the factors as well as solutions for building\nsecure enterprise-grade chatbots.\"", "published": "2024-07-10 17:20:59", "link": "http://arxiv.org/abs/2407.07858v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "DS@GT eRisk 2024: Sentence Transformers for Social Media Risk Assessment", "abstract": "We present working notes for DS@GT team in the eRisk 2024 for Tasks 1 and 3.\nWe propose a ranking system for Task 1 that predicts symptoms of depression\nbased on the Beck Depression Inventory (BDI-II) questionnaire using binary\nclassifiers trained on question relevancy as a proxy for ranking. We find that\nbinary classifiers are not well calibrated for ranking, and perform poorly\nduring evaluation. For Task 3, we use embeddings from BERT to predict the\nseverity of eating disorder symptoms based on user post history. We find that\nclassical machine learning models perform well on the task, and end up\ncompetitive with the baseline models. Representation of text data is crucial in\nboth tasks, and we find that sentence transformers are a powerful tool for\ndownstream modeling. Source code and models are available at\n\\url{https://github.com/dsgt-kaggle-clef/erisk-2024}.", "published": "2024-07-10 19:30:16", "link": "http://arxiv.org/abs/2407.08008v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Critical Review of Causal Reasoning Benchmarks for Large Language\n  Models", "abstract": "Numerous benchmarks aim to evaluate the capabilities of Large Language Models\n(LLMs) for causal inference and reasoning. However, many of them can likely be\nsolved through the retrieval of domain knowledge, questioning whether they\nachieve their purpose. In this review, we present a comprehensive overview of\nLLM benchmarks for causality. We highlight how recent benchmarks move towards a\nmore thorough definition of causal reasoning by incorporating interventional or\ncounterfactual reasoning. We derive a set of criteria that a useful benchmark\nor set of benchmarks should aim to satisfy. We hope this work will pave the way\ntowards a general framework for the assessment of causal understanding in LLMs\nand the design of novel benchmarks.", "published": "2024-07-10 20:11:51", "link": "http://arxiv.org/abs/2407.08029v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "FsPONER: Few-shot Prompt Optimization for Named Entity Recognition in\n  Domain-specific Scenarios", "abstract": "Large Language Models (LLMs) have provided a new pathway for Named Entity\nRecognition (NER) tasks. Compared with fine-tuning, LLM-powered prompting\nmethods avoid the need for training, conserve substantial computational\nresources, and rely on minimal annotated data. Previous studies have achieved\ncomparable performance to fully supervised BERT-based fine-tuning approaches on\ngeneral NER benchmarks. However, none of the previous approaches has\ninvestigated the efficiency of LLM-based few-shot learning in domain-specific\nscenarios. To address this gap, we introduce FsPONER, a novel approach for\noptimizing few-shot prompts, and evaluate its performance on domain-specific\nNER datasets, with a focus on industrial manufacturing and maintenance, while\nusing multiple LLMs -- GPT-4-32K, GPT-3.5-Turbo, LLaMA 2-chat, and Vicuna.\nFsPONER consists of three few-shot selection methods based on random sampling,\nTF-IDF vectors, and a combination of both. We compare these methods with a\ngeneral-purpose GPT-NER method as the number of few-shot examples increases and\nevaluate their optimal NER performance against fine-tuned BERT and LLaMA\n2-chat. In the considered real-world scenarios with data scarcity, FsPONER with\nTF-IDF surpasses fine-tuned models by approximately 10% in F1 score.", "published": "2024-07-10 20:32:50", "link": "http://arxiv.org/abs/2407.08035v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Virtual Agents for Alcohol Use Counseling: Exploring LLM-Powered\n  Motivational Interviewing", "abstract": "We introduce a novel application of large language models (LLMs) in\ndeveloping a virtual counselor capable of conducting motivational interviewing\n(MI) for alcohol use counseling. Access to effective counseling remains\nlimited, particularly for substance abuse, and virtual agents offer a promising\nsolution by leveraging LLM capabilities to simulate nuanced communication\ntechniques inherent in MI. Our approach combines prompt engineering and\nintegration into a user-friendly virtual platform to facilitate realistic,\nempathetic interactions. We evaluate the effectiveness of our virtual agent\nthrough a series of studies focusing on replicating MI techniques and human\ncounselor dialog. Initial findings suggest that our LLM-powered virtual agent\nmatches human counselors' empathetic and adaptive conversational skills,\npresenting a significant step forward in virtual health counseling and\nproviding insights into the design and implementation of LLM-based therapeutic\ninteractions.", "published": "2024-07-10 23:50:08", "link": "http://arxiv.org/abs/2407.08095v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Automated Question Generation on Tabular Data for Conversational Data\n  Exploration", "abstract": "Exploratory data analysis (EDA) is an essential step for analyzing a dataset\nto derive insights. Several EDA techniques have been explored in the\nliterature. Many of them leverage visualizations through various plots. But it\nis not easy to interpret them for a non-technical user, and producing\nappropriate visualizations is also tough when there are a large number of\ncolumns. Few other works provide a view of some interesting slices of data but\nit is still difficult for the user to draw relevant insights from them. Of\nlate, conversational data exploration is gaining a lot of traction among\nnon-technical users. It helps the user to explore the dataset without having\ndeep technical knowledge about the data. Towards this, we propose a system that\nrecommends interesting questions in natural language based on relevant slices\nof a dataset in a conversational setting. Specifically, given a dataset, we\npick a select set of interesting columns and identify interesting slices of\nsuch columns and column combinations based on few interestingness measures. We\nuse our own fine-tuned variation of a pre-trained language model(T5) to\ngenerate natural language questions in a specific manner. We then slot-fill\nvalues in the generated questions and rank them for recommendations. We show\nthe utility of our proposed system in a coversational setting with a collection\nof real datasets.", "published": "2024-07-10 08:07:05", "link": "http://arxiv.org/abs/2407.12859v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "STAGE: Simplified Text-Attributed Graph Embeddings Using Pre-trained\n  LLMs", "abstract": "We present Simplified Text-Attributed Graph Embeddings (STAGE), a\nstraightforward yet effective method for enhancing node features in Graph\nNeural Network (GNN) models that encode Text-Attributed Graphs (TAGs). Our\napproach leverages Large-Language Models (LLMs) to generate embeddings for\ntextual attributes. STAGE achieves competitive results on various node\nclassification benchmarks while also maintaining a simplicity in implementation\nrelative to current state-of-the-art (SoTA) techniques. We show that utilizing\npre-trained LLMs as embedding generators provides robust features for ensemble\nGNN training, enabling pipelines that are simpler than current SoTA approaches\nwhich require multiple expensive training and prompting stages. We also\nimplement diffusion-pattern GNNs in an effort to make this pipeline scalable to\ngraphs beyond academic benchmarks.", "published": "2024-07-10 08:50:25", "link": "http://arxiv.org/abs/2407.12860v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Analyzing Large language models chatbots: An experimental approach using\n  a probability test", "abstract": "This study consists of qualitative empirical research, conducted through\nexploratory tests with two different Large Language Models (LLMs) chatbots:\nChatGPT and Gemini. The methodological procedure involved exploratory tests\nbased on prompts designed with a probability question. The \"Linda Problem\",\nwidely recognized in cognitive psychology, was used as a basis to create the\ntests, along with the development of a new problem specifically for this\nexperiment, the \"Mary Problem\". The object of analysis is the dataset with the\noutputs provided by each chatbot interaction. The purpose of the analysis is to\nverify whether the chatbots mainly employ logical reasoning that aligns with\nprobability theory or if they are more frequently affected by the stereotypical\ntextual descriptions in the prompts. The findings provide insights about the\napproach each chatbot employs in handling logic and textual constructions,\nsuggesting that, while the analyzed chatbots perform satisfactorily on a\nwell-known probabilistic problem, they exhibit significantly lower performance\non new tests that require direct application of probabilistic logic.", "published": "2024-07-10 15:49:40", "link": "http://arxiv.org/abs/2407.12862v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "HiLight: Technical Report on the Motern AI Video Language Model", "abstract": "This technical report presents the implementation of a state-of-the-art video\nencoder for video-text modal alignment and a video conversation framework\ncalled HiLight, which features dual visual towers. The work is divided into two\nmain parts: 1.alignment of video and text modalities; 2.convenient and\nefficient way to interact with users. Our goal is to address the task of video\ncomprehension in the context of billiards. The report includes a discussion of\nthe concepts and the final solution developed during the task's implementation.", "published": "2024-07-10 02:43:18", "link": "http://arxiv.org/abs/2407.07325v2", "categories": ["cs.CV", "cs.CL", "cs.MM", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Out-of-distribution generalisation in spoken language understanding", "abstract": "Test data is said to be out-of-distribution (OOD) when it unexpectedly\ndiffers from the training data, a common challenge in real-world use cases of\nmachine learning. Although OOD generalisation has gained interest in recent\nyears, few works have focused on OOD generalisation in spoken language\nunderstanding (SLU) tasks. To facilitate research on this topic, we introduce a\nmodified version of the popular SLU dataset SLURP, featuring data splits for\ntesting OOD generalisation in the SLU task. We call our modified dataset SLURP\nFor OOD generalisation, or SLURPFOOD. Utilising our OOD data splits, we find\nend-to-end SLU models to have limited capacity for generalisation. Furthermore,\nby employing model interpretability techniques, we shed light on the factors\ncontributing to the generalisation difficulties of the models. To improve the\ngeneralisation, we experiment with two techniques, which improve the results on\nsome, but not all the splits, emphasising the need for new techniques.", "published": "2024-07-10 07:27:38", "link": "http://arxiv.org/abs/2407.07425v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "HebDB: a Weakly Supervised Dataset for Hebrew Speech Processing", "abstract": "We present HebDB, a weakly supervised dataset for spoken language processing\nin the Hebrew language. HebDB offers roughly 2500 hours of natural and\nspontaneous speech recordings in the Hebrew language, consisting of a large\nvariety of speakers and topics. We provide raw recordings together with a\npre-processed, weakly supervised, and filtered version. The goal of HebDB is to\nfurther enhance research and development of spoken language processing tools\nfor the Hebrew language. Hence, we additionally provide two baseline systems\nfor Automatic Speech Recognition (ASR): (i) a self-supervised model; and (ii) a\nfully supervised model. We present the performance of these two methods\noptimized on HebDB and compare them to current multi-lingual ASR alternatives.\nResults suggest the proposed method reaches better results than the evaluated\nbaselines considering similar model sizes. Dataset, code, and models are\npublicly available under https://pages.cs.huji.ac.il/adiyoss-lab/HebDB/.", "published": "2024-07-10 11:51:26", "link": "http://arxiv.org/abs/2407.07566v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Teaching Transformers Causal Reasoning through Axiomatic Training", "abstract": "For text-based AI systems to interact in the real world, causal reasoning is\nan essential skill. Since interventional data is costly to generate, we study\nto what extent an agent can learn causal reasoning from passive data.\nSpecifically, we consider an axiomatic training setup where an agent learns\nfrom multiple demonstrations of a causal axiom (or rule), rather than\nincorporating the axiom as an inductive bias or inferring it from data values.\nA key question is whether the agent would learn to generalize from the axiom\ndemonstrations to new scenarios. For example, if a transformer model is trained\non demonstrations of the causal transitivity axiom over small graphs, would it\ngeneralize to applying the transitivity axiom over large graphs? Our results,\nbased on a novel axiomatic training scheme, indicate that such generalization\nis possible. We consider the task of inferring whether a variable causes\nanother variable, given a causal graph structure. We find that a 67 million\nparameter transformer model, when trained on linear causal chains (along with\nsome noisy variations) can generalize well to new kinds of graphs, including\nlonger causal chains, causal chains with reversed order, and graphs with\nbranching; even when it is not explicitly trained for such settings. Our model\nperforms at par (or even better) than many larger language models such as\nGPT-4, Gemini Pro, and Phi-3. Overall, our axiomatic training framework\nprovides a new paradigm of learning causal reasoning from passive data that can\nbe used to learn arbitrary axioms, as long as sufficient demonstrations can be\ngenerated.", "published": "2024-07-10 12:50:44", "link": "http://arxiv.org/abs/2407.07612v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "PaliGemma: A versatile 3B VLM for transfer", "abstract": "PaliGemma is an open Vision-Language Model (VLM) that is based on the\nSigLIP-So400m vision encoder and the Gemma-2B language model. It is trained to\nbe a versatile and broadly knowledgeable base model that is effective to\ntransfer. It achieves strong performance on a wide variety of open-world tasks.\nWe evaluate PaliGemma on almost 40 diverse tasks including standard VLM\nbenchmarks, but also more specialized tasks such as remote-sensing and\nsegmentation.", "published": "2024-07-10 14:57:46", "link": "http://arxiv.org/abs/2407.07726v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Fine-Tuning Large Language Models with User-Level Differential Privacy", "abstract": "We investigate practical and scalable algorithms for training large language\nmodels (LLMs) with user-level differential privacy (DP) in order to provably\nsafeguard all the examples contributed by each user. We study two variants of\nDP-SGD with: (1) example-level sampling (ELS) and per-example gradient\nclipping, and (2) user-level sampling (ULS) and per-user gradient clipping. We\nderive a novel user-level DP accountant that allows us to compute provably\ntight privacy guarantees for ELS. Using this, we show that while ELS can\noutperform ULS in specific settings, ULS generally yields better results when\neach user has a diverse collection of examples. We validate our findings\nthrough experiments in synthetic mean estimation and LLM fine-tuning tasks\nunder fixed compute budgets. We find that ULS is significantly better in\nsettings where either (1) strong privacy guarantees are required, or (2) the\ncompute budget is large. Notably, our focus on LLM-compatible training\nalgorithms allows us to scale to models with hundreds of millions of parameters\nand datasets with hundreds of thousands of users.", "published": "2024-07-10 15:07:58", "link": "http://arxiv.org/abs/2407.07737v1", "categories": ["cs.LG", "cs.CL", "cs.CR", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Multi-task Prompt Words Learning for Social Media Content Generation", "abstract": "The rapid development of the Internet has profoundly changed human life.\nHumans are increasingly expressing themselves and interacting with others on\nsocial media platforms. However, although artificial intelligence technology\nhas been widely used in many aspects of life, its application in social media\ncontent creation is still blank. To solve this problem, we propose a new prompt\nword generation framework based on multi-modal information fusion, which\ncombines multiple tasks including topic classification, sentiment analysis,\nscene recognition and keyword extraction to generate more comprehensive prompt\nwords. Subsequently, we use a template containing a set of prompt words to\nguide ChatGPT to generate high-quality tweets. Furthermore, in the absence of\neffective and objective evaluation criteria in the field of content generation,\nwe use the ChatGPT tool to evaluate the results generated by the algorithm,\nmaking large-scale evaluation of content generation algorithms possible.\nEvaluation results on extensive content generation demonstrate that our cue\nword generation framework generates higher quality content compared to manual\nmethods and other cueing techniques, while topic classification, sentiment\nanalysis, and scene recognition significantly enhance content clarity and its\nconsistency with the image.", "published": "2024-07-10 15:46:32", "link": "http://arxiv.org/abs/2407.07771v1", "categories": ["cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Evaluating Large Language Models with Grid-Based Game Competitions: An\n  Extensible LLM Benchmark and Leaderboard", "abstract": "We introduce a novel and extensible benchmark for large language models\n(LLMs) through grid-based games such as Tic-Tac-Toe, Connect Four, and Gomoku.\nThe open-source game simulation code, available on GitHub, allows LLMs to\ncompete and generates detailed data files in JSON, CSV, TXT, and PNG formats\nfor leaderboard rankings and further analysis. We present the results of games\namong leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet by\nAnthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo and\nGPT-4o by OpenAI, and Llama3-70B by Meta. We also encourage submissions of\nresults from other LLMs. In total, we simulated 2,310 matches (5 sessions for\neach pair among 7 LLMs and a random player) across three types of games, using\nthree distinct prompt types: list, illustration, and image. The results\nrevealed significant variations in LLM performance across different games and\nprompt types, with analysis covering win and disqualification rates, missed\nopportunity analysis, and invalid move analysis. The details of the leaderboard\nand result matrix data are available as open-access data on GitHub. This study\nenhances our understanding of LLMs' capabilities in playing games they were not\nspecifically trained for, helping to assess their rule comprehension and\nstrategic thinking. On the path to Artificial General Intelligence (AGI), this\nstudy lays the groundwork for future exploration into their utility in complex\ndecision-making scenarios, illuminating their strategic thinking abilities and\noffering directions for further inquiry into the limits of LLMs within\ngame-based frameworks.", "published": "2024-07-10 16:14:34", "link": "http://arxiv.org/abs/2407.07796v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.AI"}
{"title": "AVCap: Leveraging Audio-Visual Features as Text Tokens for Captioning", "abstract": "In recent years, advancements in representation learning and language models\nhave propelled Automated Captioning (AC) to new heights, enabling the\ngeneration of human-level descriptions. Leveraging these advancements, we\npropose AVCap, an Audio-Visual Captioning framework, a simple yet powerful\nbaseline approach applicable to audio-visual captioning. AVCap utilizes\naudio-visual features as text tokens, which has many advantages not only in\nperformance but also in the extensibility and scalability of the model. AVCap\nis designed around three pivotal dimensions: the exploration of optimal\naudio-visual encoder architectures, the adaptation of pre-trained models\naccording to the characteristics of generated text, and the investigation into\nthe efficacy of modality fusion in captioning. Our method outperforms existing\naudio-visual captioning methods across all metrics and the code is available on\nhttps://github.com/JongSuk1/AVCap", "published": "2024-07-10 16:17:49", "link": "http://arxiv.org/abs/2407.07801v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ROSA: Random Subspace Adaptation for Efficient Fine-Tuning", "abstract": "Model training requires significantly more memory, compared with inference.\nParameter efficient fine-tuning (PEFT) methods provide a means of adapting\nlarge models to downstream tasks using less memory. However, existing methods\nsuch as adapters, prompt tuning or low-rank adaptation (LoRA) either introduce\nlatency overhead at inference time or achieve subpar downstream performance\ncompared with full fine-tuning. In this work we propose Random Subspace\nAdaptation (ROSA), a method that outperforms previous PEFT methods by a\nsignificant margin, while maintaining a zero latency overhead during inference\ntime. In contrast to previous methods, ROSA is able to adapt subspaces of\narbitrarily large dimension, better approximating full-finetuning. We\ndemonstrate both theoretically and experimentally that this makes ROSA strictly\nmore expressive than LoRA, without consuming additional memory during runtime.\nAs PEFT methods are especially useful in the natural language processing\ndomain, where models operate on scales that make full fine-tuning very\nexpensive, we evaluate ROSA in two common NLP scenarios: natural language\ngeneration (NLG) and natural language understanding (NLU) with GPT-2 and\nRoBERTa, respectively. We show that on almost every GLUE task ROSA outperforms\nLoRA by a significant margin, while also outperforming LoRA on NLG tasks. Our\ncode is available at https://github.com/rosa-paper/rosa", "published": "2024-07-10 16:20:53", "link": "http://arxiv.org/abs/2407.07802v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Transformer Block Coupling and its Correlation with Generalization in\n  LLMs", "abstract": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing, and a precise understanding of the internal mechanisms\ndriving their success is essential. In this work, we analyze the trajectories\nof token embeddings as they pass through transformer blocks, linearizing the\nsystem along these trajectories through their Jacobian matrices. By examining\nthe relationships between these block Jacobians, we uncover the phenomenon of\n\\textbf{transformer block coupling} in a multitude of LLMs, characterized by\nthe coupling of their top singular vectors across tokens and depth. Our\nfindings reveal that coupling \\textit{positively correlates} with model\nperformance, and that this relationship is stronger than with other\nhyperparameters such as parameter count, model depth, and embedding dimension.\nWe further investigate how these properties emerge during training, observing a\nprogressive development of coupling, increased linearity, and layer-wise\nexponential growth in token trajectories. Additionally, experiments with Vision\nTransformers (ViTs) corroborate the emergence of coupling and its relationship\nwith generalization, reinforcing our findings in LLMs. Collectively, these\ninsights offer a novel perspective on token interactions in transformers,\nopening new directions for studying their mechanisms as well as improving\ntraining and generalization.", "published": "2024-07-10 16:30:27", "link": "http://arxiv.org/abs/2407.07810v5", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards Robust Alignment of Language Models: Distributionally\n  Robustifying Direct Preference Optimization", "abstract": "This study addresses the challenge of noise in training datasets for Direct\nPreference Optimization (DPO), a method for aligning Large Language Models\n(LLMs) with human preferences. We categorize noise into pointwise noise, which\nincludes low-quality data points, and pairwise noise, which encompasses\nerroneous data pair associations that affect preference rankings. Utilizing\nDistributionally Robust Optimization (DRO), we enhance DPO's resilience to\nthese types of noise. Our theoretical insights reveal that DPO inherently\nembeds DRO principles, conferring robustness to pointwise noise, with the\nregularization coefficient $\\beta$ playing a critical role in its noise\nresistance. Extending this framework, we introduce Distributionally\nRobustifying DPO (Dr. DPO), which integrates pairwise robustness by optimizing\nagainst worst-case pairwise scenarios. The novel hyperparameter $\\beta'$ in Dr.\nDPO allows for fine-tuned control over data pair reliability, providing a\nstrategic balance between exploration and exploitation in noisy training\nenvironments. Empirical evaluations demonstrate that Dr. DPO substantially\nimproves the quality of generated text and response accuracy in preference\ndatasets, showcasing enhanced performance in both noisy and noise-free\nsettings. The code is available at https://github.com/junkangwu/Dr_DPO.", "published": "2024-07-10 17:48:25", "link": "http://arxiv.org/abs/2407.07880v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Training on the Test Task Confounds Evaluation and Emergence", "abstract": "We study a fundamental problem in the evaluation of large language models\nthat we call training on the test task. Unlike wrongful practices like training\non the test data, leakage, or data contamination, training on the test task is\nnot a malpractice. Rather, the term describes a growing set of practices that\nutilize knowledge about evaluation tasks at training time. We demonstrate that\ntraining on the test task confounds both relative model evaluations and claims\nabout emergent capabilities. We argue that the seeming superiority of one model\nfamily over another may be explained by a different degree of training on the\ntest task. To this end, we propose an effective method to adjust for the effect\nof training on the test task on benchmark evaluations. Put simply, to fine-tune\neach model under comparison on the same task-relevant data before evaluation.\nWe then show that instances of emergent behavior disappear gradually as models\ntrain on the test task. Our work promotes a new perspective on the evaluation\nof large language models with broad implications for benchmarking and the study\nof emergent capabilities", "published": "2024-07-10 17:57:58", "link": "http://arxiv.org/abs/2407.07890v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large\n  Multimodal Models", "abstract": "Visual instruction tuning has made considerable strides in enhancing the\ncapabilities of Large Multimodal Models (LMMs). However, existing open LMMs\nlargely focus on single-image tasks, their applications to multi-image\nscenarios remains less explored. Additionally, prior LMM research separately\ntackles different scenarios, leaving it impossible to generalize cross\nscenarios with new emerging capabilities. To this end, we introduce\nLLaVA-NeXT-Interleave, which simultaneously tackles Multi-image, Multi-frame\n(video), Multi-view (3D), and Multi-patch (single-image) scenarios in LMMs. To\nenable these capabilities, we regard the interleaved data format as a general\ntemplate and compile the M4-Instruct dataset with 1,177.6k samples, spanning 4\nprimary domains with 14 tasks and 41 datasets. We also curate the\nLLaVA-Interleave Bench to comprehensively evaluate the multi-image performance\nof LMMs. Through extensive experiments, LLaVA-NeXT-Interleave achieves leading\nresults in multi-image, video, and 3D benchmarks, while maintaining the\nperformance of single-image tasks. Besides, our model also exhibits several\nemerging capabilities, e.g., transferring tasks across different settings and\nmodalities. Code is available at https://github.com/LLaVA-VL/LLaVA-NeXT", "published": "2024-07-10 17:59:43", "link": "http://arxiv.org/abs/2407.07895v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Search, Examine and Early-Termination: Fake News Detection with\n  Annotation-Free Evidences", "abstract": "Pioneer researches recognize evidences as crucial elements in fake news\ndetection apart from patterns. Existing evidence-aware methods either require\nlaborious pre-processing procedures to assure relevant and high-quality\nevidence data, or incorporate the entire spectrum of available evidences in all\nnews cases, regardless of the quality and quantity of the retrieved data. In\nthis paper, we propose an approach named \\textbf{SEE} that retrieves useful\ninformation from web-searched annotation-free evidences with an\nearly-termination mechanism. The proposed SEE is constructed by three main\nphases: \\textbf{S}earching online materials using the news as a query and\ndirectly using their titles as evidences without any annotating or filtering\nprocedure, sequentially \\textbf{E}xamining the news alongside with each piece\nof evidence via attention mechanisms to produce new hidden states with\nretrieved information, and allowing \\textbf{E}arly-termination within the\nexamining loop by assessing whether there is adequate confidence for producing\na correct prediction. We have conducted extensive experiments on datasets with\nunprocessed evidences, i.e., Weibo21, GossipCop, and pre-processed evidences,\nnamely Snopes and PolitiFact. The experimental results demonstrate that the\nproposed method outperforms state-of-the-art approaches.", "published": "2024-07-10 07:22:30", "link": "http://arxiv.org/abs/2407.07931v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Rel-A.I.: An Interaction-Centered Approach To Measuring Human-LM\n  Reliance", "abstract": "The ability to communicate uncertainty, risk, and limitation is crucial for\nthe safety of large language models. However, current evaluations of these\nabilities rely on simple calibration, asking whether the language generated by\nthe model matches appropriate probabilities. Instead, evaluation of this aspect\nof LLM communication should focus on the behaviors of their human\ninterlocutors: how much do they rely on what the LLM says? Here we introduce an\ninteraction-centered evaluation framework called Rel-A.I. (pronounced \"rely\"})\nthat measures whether humans rely on LLM generations. We use this framework to\nstudy how reliance is affected by contextual features of the interaction (e.g,\nthe knowledge domain that is being discussed), or the use of greetings\ncommunicating warmth or competence (e.g., \"I'm happy to help!\"). We find that\ncontextual characteristics significantly affect human reliance behavior. For\nexample, people rely 10% more on LMs when responding to questions involving\ncalculations and rely 30% more on LMs that are perceived as more competent. Our\nresults show that calibration and language quality alone are insufficient in\nevaluating the risks of human-LM interactions, and illustrate the need to\nconsider features of the interactional context.", "published": "2024-07-10 18:00:05", "link": "http://arxiv.org/abs/2407.07950v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Automated Neural Patent Landscaping in the Small Data Regime", "abstract": "Patent landscaping is the process of identifying all patents related to a\nparticular technological area, and is important for assessing various aspects\nof the intellectual property context. Traditionally, constructing patent\nlandscapes is intensely laborious and expensive, and the rapid expansion of\npatenting activity in recent decades has driven an increasing need for\nefficient and effective automated patent landscaping approaches. In particular,\nit is critical that we be able to construct patent landscapes using a minimal\nnumber of labeled examples, as labeling patents for a narrow technology area\nrequires highly specialized (and hence expensive) technical knowledge. We\npresent an automated neural patent landscaping system that demonstrates\nsignificantly improved performance on difficult examples (0.69 $F_1$ on 'hard'\nexamples, versus 0.6 for previously reported systems), and also significant\nimprovements with much less training data (overall 0.75 $F_1$ on as few as 24\nexamples). Furthermore, in evaluating such automated landscaping systems,\nacquiring good data is challenge; we demonstrate a higher-quality training data\ngeneration procedure by merging Abood and Feltenberger's (2018)\n\"seed/anti-seed\" approach with active learning to collect difficult labeled\nexamples near the decision boundary. Using this procedure we created a new\ndataset of labeled AI patents for training and testing. As in prior work we\ncompare our approach with a number of baseline systems, and we release our code\nand data for others to build upon.", "published": "2024-07-10 19:13:37", "link": "http://arxiv.org/abs/2407.08001v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective\n  Weight-Activation Quantization", "abstract": "Low-Rank Adaptation (LoRA), as a representative Parameter-Efficient\nFine-Tuning (PEFT)method, significantly enhances the training efficiency by\nupdating only a small portion of the weights in Large Language Models (LLMs).\nRecently, weight-only quantization techniques have also been applied to LoRA\nmethods to reduce the memory footprint of fine-tuning. However, applying\nweight-activation quantization to the LoRA pipeline is under-explored, and we\nobserve substantial performance degradation primarily due to the presence of\nactivation outliers. In this work, we propose RoLoRA, the first LoRA-based\nscheme for effective weight-activation quantization. RoLoRA utilizes rotation\nfor outlier elimination and proposes rotation-aware fine-tuning to preserve the\noutlier-free characteristics in rotated LLMs. Experimental results show RoLoRA\nconsistently improves low-bit LoRA convergence and post-training quantization\nrobustness in weight-activation settings. We evaluate RoLoRA across\nLLaMA2-7B/13B, LLaMA3-8B models, achieving up to 29.5% absolute accuracy gain\nof 4-bit weight-activation quantized LLaMA2- 13B on commonsense reasoning tasks\ncompared to LoRA baseline. We further demonstrate its effectiveness on Large\nMultimodal Models (LLaVA-1.5-7B). Codes are available at\nhttps://github.com/HuangOwen/RoLoRA", "published": "2024-07-10 20:52:18", "link": "http://arxiv.org/abs/2407.08044v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Was it Slander? Towards Exact Inversion of Generative Language Models", "abstract": "Training large language models (LLMs) requires a substantial investment of\ntime and money. To get a good return on investment, the developers spend\nconsiderable effort ensuring that the model never produces harmful and\noffensive outputs. However, bad-faith actors may still try to slander the\nreputation of an LLM by publicly reporting a forged output. In this paper, we\nshow that defending against such slander attacks requires reconstructing the\ninput of the forged output or proving that it does not exist. To do so, we\npropose and evaluate a search based approach for targeted adversarial attacks\nfor LLMs. Our experiments show that we are rarely able to reconstruct the exact\ninput of an arbitrary output, thus demonstrating that LLMs are still vulnerable\nto slander attacks.", "published": "2024-07-10 11:08:06", "link": "http://arxiv.org/abs/2407.11059v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "EfficientQAT: Efficient Quantization-Aware Training for Large Language\n  Models", "abstract": "Large language models (LLMs) are crucial in modern natural language\nprocessing and artificial intelligence. However, they face challenges in\nmanaging their significant memory requirements. Although quantization-aware\ntraining (QAT) offers a solution by reducing memory consumption through low-bit\nrepresentations with minimal accuracy loss, it is impractical due to\nsubstantial training resources. To address this, we propose Efficient\nQuantization-Aware Training (EfficientQAT), a more feasible QAT algorithm.\nEfficientQAT involves two consecutive phases: Block-wise training of all\nparameters (Block-AP) and end-to-end training of quantization parameters\n(E2E-QP). To the best of our knowledge, Block-AP is the first method to enable\ndirect training of all parameters in a block-wise manner, reducing accuracy\nloss in low-bit scenarios by enhancing the solution space during optimization.\nE2E-QP then trains only the quantization parameters (step sizes) end-to-end,\nfurther improving the performance of quantized models by considering\ninteractions among all sub-modules. Extensive experiments demonstrate that\nEfficientQAT outperforms previous quantization methods across a range of\nmodels, including base LLMs, instruction-tuned LLMs, and multimodal LLMs, with\nscales from 7B to 70B parameters at various quantization bits. For instance,\nEfficientQAT obtains a 2-bit Llama-2-70B model on a single A100-80GB GPU in 41\nhours, with less than 3 points accuracy degradation compared to the full\nprecision (69.48 vs. 72.41). Code is available at\nhttps://github.com/OpenGVLab/EfficientQAT.", "published": "2024-07-10 17:53:30", "link": "http://arxiv.org/abs/2407.11062v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Grounding and Evaluation for Large Language Models: Practical Challenges\n  and Lessons Learned (Survey)", "abstract": "With the ongoing rapid adoption of Artificial Intelligence (AI)-based systems\nin high-stakes domains, ensuring the trustworthiness, safety, and observability\nof these systems has become crucial. It is essential to evaluate and monitor AI\nsystems not only for accuracy and quality-related metrics but also for\nrobustness, bias, security, interpretability, and other responsible AI\ndimensions. We focus on large language models (LLMs) and other generative AI\nmodels, which present additional challenges such as hallucinations, harmful and\nmanipulative content, and copyright infringement. In this survey article\naccompanying our KDD 2024 tutorial, we highlight a wide range of harms\nassociated with generative AI systems, and survey state of the art approaches\n(along with open challenges) to address these harms.", "published": "2024-07-10 01:23:10", "link": "http://arxiv.org/abs/2407.12858v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CiteME: Can Language Models Accurately Cite Scientific Claims?", "abstract": "Thousands of new scientific papers are published each month. Such information\noverload complicates researcher efforts to stay current with the\nstate-of-the-art as well as to verify and correctly attribute claims. We pose\nthe following research question: Given a text excerpt referencing a paper,\ncould an LM act as a research assistant to correctly identify the referenced\npaper? We advance efforts to answer this question by building a benchmark that\nevaluates the abilities of LMs in citation attribution. Our benchmark, CiteME,\nconsists of text excerpts from recent machine learning papers, each referencing\na single other paper. CiteME use reveals a large gap between frontier LMs and\nhuman performance, with LMs achieving only 4.2-18.5% accuracy and humans 69.7%.\nWe close this gap by introducing CiteAgent, an autonomous system built on the\nGPT-4o LM that can also search and read papers, which achieves an accuracy of\n35.3\\% on CiteME. Overall, CiteME serves as a challenging testbed for\nopen-ended claim attribution, driving the research community towards a future\nwhere any claim made by an LM can be automatically verified and discarded if\nfound to be incorrect.", "published": "2024-07-10 11:31:20", "link": "http://arxiv.org/abs/2407.12861v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Explaining Spectrograms in Machine Learning: A Study on Neural Networks\n  for Speech Classification", "abstract": "This study investigates discriminative patterns learned by neural networks\nfor accurate speech classification, with a specific focus on vowel\nclassification tasks. By examining the activations and features of neural\nnetworks for vowel classification, we gain insights into what the networks\n\"see\" in spectrograms. Through the use of class activation mapping, we identify\nthe frequencies that contribute to vowel classification and compare these\nfindings with linguistic knowledge. Experiments on a American English dataset\nof vowels showcases the explainability of neural networks and provides valuable\ninsights into the causes of misclassifications and their characteristics when\ndifferentiating them from unvoiced speech. This study not only enhances our\nunderstanding of the underlying acoustic cues in vowel classification but also\noffers opportunities for improving speech recognition by bridging the gap\nbetween abstract representations in neural networks and established linguistic\nknowledge", "published": "2024-07-10 07:37:18", "link": "http://arxiv.org/abs/2407.17416v1", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Dynamic Encoder Size Based on Data-Driven Layer-wise Pruning for Speech\n  Recognition", "abstract": "Varying-size models are often required to deploy ASR systems under different\nhardware and/or application constraints such as memory and latency. To avoid\nredundant training and optimization efforts for individual models of different\nsizes, we present the dynamic encoder size approach, which jointly trains\nmultiple performant models within one supernet from scratch. These subnets of\nvarious sizes are layer-wise pruned from the supernet, and thus, enjoy full\nparameter sharing. By combining score-based pruning with supernet training, we\npropose two novel methods, Simple-Top-k and Iterative-Zero-Out, to\nautomatically select the best-performing subnets in a data-driven manner,\navoiding resource-intensive search efforts. Our experiments using CTC on both\nLibrispeech and TED-LIUM-v2 corpora show that our methods can achieve on-par\nperformance as individually trained models of each size category. Also, our\napproach consistently brings small performance improvements for the full-size\nsupernet.", "published": "2024-07-10 08:35:21", "link": "http://arxiv.org/abs/2407.18930v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Generative Image as Action Models", "abstract": "Image-generation diffusion models have been fine-tuned to unlock new\ncapabilities such as image-editing and novel view synthesis. Can we similarly\nunlock image-generation models for visuomotor control? We present GENIMA, a\nbehavior-cloning agent that fine-tunes Stable Diffusion to 'draw joint-actions'\nas targets on RGB images. These images are fed into a controller that maps the\nvisual targets into a sequence of joint-positions. We study GENIMA on 25\nRLBench and 9 real-world manipulation tasks. We find that, by lifting actions\ninto image-space, internet pre-trained diffusion models can generate policies\nthat outperform state-of-the-art visuomotor approaches, especially in\nrobustness to scene perturbations and generalizing to novel objects. Our method\nis also competitive with 3D agents, despite lacking priors such as depth,\nkeypoints, or motion-planners.", "published": "2024-07-10 17:41:10", "link": "http://arxiv.org/abs/2407.07875v2", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "LitSearch: A Retrieval Benchmark for Scientific Literature Search", "abstract": "Literature search questions, such as \"Where can I find research on the\nevaluation of consistency in generated summaries?\" pose significant challenges\nfor modern search engines and retrieval systems. These questions often require\na deep understanding of research concepts and the ability to reason across\nentire articles. In this work, we introduce LitSearch, a retrieval benchmark\ncomprising 597 realistic literature search queries about recent ML and NLP\npapers. LitSearch is constructed using a combination of (1) questions generated\nby GPT-4 based on paragraphs containing inline citations from research papers\nand (2) questions manually written by authors about their recently published\npapers. All LitSearch questions were manually examined or edited by experts to\nensure high quality. We extensively benchmark state-of-the-art retrieval models\nand also evaluate two LLM-based reranking pipelines. We find a significant\nperformance gap between BM25 and state-of-the-art dense retrievers, with a\n24.8% absolute difference in recall@5. The LLM-based reranking strategies\nfurther improve the best-performing dense retriever by 4.4%. Additionally,\ncommercial search engines and research tools like Google Search perform poorly\non LitSearch, lagging behind the best dense retriever by up to 32 recall\npoints. Taken together, these results show that LitSearch is an informative new\ntestbed for retrieval systems while catering to a real-world use case.", "published": "2024-07-10 18:00:03", "link": "http://arxiv.org/abs/2407.18940v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "SimuSOE: A Simulated Snoring Dataset for Obstructive Sleep\n  Apnea-Hypopnea Syndrome Evaluation during Wakefulness", "abstract": "Obstructive Sleep Apnea-Hypopnea Syndrome (OSAHS) is a prevalent chronic\nbreathing disorder caused by upper airway obstruction. Previous studies\nadvanced OSAHS evaluation through machine learning-based systems trained on\nsleep snoring or speech signal datasets. However, constructing datasets for\ntraining a precise and rapid OSAHS evaluation system poses a challenge, since\n1) it is time-consuming to collect sleep snores and 2) the speech signal is\nlimited in reflecting upper airway obstruction. In this paper, we propose a new\nsnoring dataset for OSAHS evaluation, named SimuSOE, in which a novel and\ntime-effective snoring collection method is introduced for tackling the above\nproblems. In particular, we adopt simulated snoring which is a type of snore\nintentionally emitted by patients to replace natural snoring. Experimental\nresults indicate that the simulated snoring signal during wakefulness can serve\nas an effective feature in OSAHS preliminary screening.", "published": "2024-07-10 06:36:55", "link": "http://arxiv.org/abs/2407.07397v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "STONE: Self-supervised Tonality Estimator", "abstract": "Although deep neural networks can estimate the key of a musical piece, their\nsupervision incurs a massive annotation effort. Against this shortcoming, we\npresent STONE, the first self-supervised tonality estimator. The architecture\nbehind STONE, named ChromaNet, is a convnet with octave equivalence which\noutputs a key signature profile (KSP) of 12 structured logits. First, we train\nChromaNet to regress artificial pitch transpositions between any two unlabeled\nmusical excerpts from the same audio track, as measured as cross-power spectral\ndensity (CPSD) within the circle of fifths (CoF). We observe that this\nself-supervised pretext task leads KSP to correlate with tonal key signature.\nBased on this observation, we extend STONE to output a structured KSP of 24\nlogits, and introduce supervision so as to disambiguate major versus minor keys\nsharing the same key signature. Applying different amounts of supervision\nyields semi-supervised and fully supervised tonality estimators: i.e.,\nSemi-TONEs and Sup-TONEs. We evaluate these estimators on FMAK, a new dataset\nof 5489 real-world musical recordings with expert annotation of 24 major and\nminor keys. We find that Semi-TONE matches the classification accuracy of\nSup-TONE with reduced supervision and outperforms it with equal supervision.", "published": "2024-07-10 07:09:56", "link": "http://arxiv.org/abs/2407.07408v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Source Tracing of Audio Deepfake Systems", "abstract": "Recent progress in generative AI technology has made audio deepfakes\nremarkably more realistic. While current research on anti-spoofing systems\nprimarily focuses on assessing whether a given audio sample is fake or genuine,\nthere has been limited attention on discerning the specific techniques to\ncreate the audio deepfakes. Algorithms commonly used in audio deepfake\ngeneration, like text-to-speech (TTS) and voice conversion (VC), undergo\ndistinct stages including input processing, acoustic modeling, and waveform\ngeneration. In this work, we introduce a system designed to classify various\nspoofing attributes, capturing the distinctive features of individual modules\nthroughout the entire generation pipeline. We evaluate our system on two\ndatasets: the ASVspoof 2019 Logical Access and the Multi-Language Audio\nAnti-Spoofing Dataset (MLAAD). Results from both experiments demonstrate the\nrobustness of the system to identify the different spoofing attributes of\ndeepfake generation systems.", "published": "2024-07-10 19:49:10", "link": "http://arxiv.org/abs/2407.08016v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Phonetic Richness for Improved Automatic Speaker Verification", "abstract": "When it comes to authentication in speaker verification systems, not all\nutterances are created equal. It is essential to estimate the quality of test\nutterances in order to account for varying acoustic conditions. In addition to\nthe net-speech duration of an utterance, it is observed in this paper that\nphonetic richness is also a key indicator of utterance quality, playing a\nsignificant role in accurate speaker verification. Several phonetic histogram\nbased formulations of phonetic richness are explored using transcripts obtained\nfrom an automatic speaker recognition system. The proposed phonetic richness\nmeasure is found to be positively correlated with voice authentication scores\nacross evaluation benchmarks. Additionally, the proposed measure in combination\nwith net speech helps in calibrating the speaker verification scores, obtaining\na relative EER improvement of 5.8% on the Voxceleb1 evaluation protocol. The\nproposed phonetic richness based calibration provides higher benefit for short\nutterances with repeated words.", "published": "2024-07-10 19:51:41", "link": "http://arxiv.org/abs/2407.08017v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "VoxMed: One-Step Respiratory Disease Classifier using Digital\n  Stethoscope Sounds", "abstract": "As respiratory illnesses become more common, it is crucial to quickly and\naccurately detect them to improve patient care. There is a need for improved\ndiagnostic methods for immediate medical assessments for optimal patient\noutcomes. This paper introduces VoxMed, a UI-assisted one-step classifier that\nuses digital stethoscope recordings to diagnose respiratory diseases. It\nemploys an Audio Spectrogram Transformer(AST) for feature extraction and a 1-D\nCNN-based architecture to classify respiratory diseases, offering professionals\ninformation regarding their patients respiratory health in seconds. We use the\nICBHI dataset, which includes stethoscope recordings collected from patients in\nGreece and Portugal, to classify respiratory diseases. GitHub repository:\nhttps://github.com/Sample-User131001/VoxMed", "published": "2024-07-10 04:26:22", "link": "http://arxiv.org/abs/2407.18926v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ASGIR: Audio Spectrogram Transformer Guided Classification And\n  Information Retrieval For Birds", "abstract": "Recognition and interpretation of bird vocalizations are pivotal in\nornithological research and ecological conservation efforts due to their\nsignificance in understanding avian behaviour, performing habitat assessment\nand judging ecological health. This paper presents an audio spectrogram-guided\nclassification framework called ASGIR for improved bird sound recognition and\ninformation retrieval. Our work is accompanied by a simple-to-use, two-step\ninformation retrieval system that uses geographical location and bird sounds to\nlocalize and retrieve relevant bird information by scraping Wikipedia page\ninformation of recognized birds. ASGIR offers a substantial performance on a\nrandom subset of 51 classes of Xeno-Canto dataset Bird sounds from European\ncountries with a median of 100\\% performance on F1, Precision and Sensitivity\nmetrics. Our code is available as follows:\nhttps://github.com/MainSample1234/AS-GIR .", "published": "2024-07-10 04:32:19", "link": "http://arxiv.org/abs/2407.18927v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Video-to-Audio Generation with Hidden Alignment", "abstract": "Generating semantically and temporally aligned audio content in accordance\nwith video input has become a focal point for researchers, particularly\nfollowing the remarkable breakthrough in text-to-video generation. In this\nwork, we aim to offer insights into the video-to-audio generation paradigm,\nfocusing on three crucial aspects: vision encoders, auxiliary embeddings, and\ndata augmentation techniques. Beginning with a foundational model built on a\nsimple yet surprisingly effective intuition, we explore various vision encoders\nand auxiliary embeddings through ablation studies. Employing a comprehensive\nevaluation pipeline that emphasizes generation quality and video-audio\nsynchronization alignment, we demonstrate that our model exhibits\nstate-of-the-art video-to-audio generation capabilities. Furthermore, we\nprovide critical insights into the impact of different data augmentation\nmethods on enhancing the generation framework's overall capacity. We showcase\npossibilities to advance the challenge of generating synchronized audio from\nsemantic and temporal perspectives. We hope these insights will serve as a\nstepping stone toward developing more realistic and accurate audio-visual\ngeneration models.", "published": "2024-07-10 08:40:39", "link": "http://arxiv.org/abs/2407.07464v3", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Beat-It: Beat-Synchronized Multi-Condition 3D Dance Generation", "abstract": "Dance, as an art form, fundamentally hinges on the precise synchronization\nwith musical beats. However, achieving aesthetically pleasing dance sequences\nfrom music is challenging, with existing methods often falling short in\ncontrollability and beat alignment. To address these shortcomings, this paper\nintroduces Beat-It, a novel framework for beat-specific, key pose-guided dance\ngeneration. Unlike prior approaches, Beat-It uniquely integrates explicit beat\nawareness and key pose guidance, effectively resolving two main issues: the\nmisalignment of generated dance motions with musical beats, and the inability\nto map key poses to specific beats, critical for practical choreography. Our\napproach disentangles beat conditions from music using a nearest beat distance\nrepresentation and employs a hierarchical multi-condition fusion mechanism.\nThis mechanism seamlessly integrates key poses, beats, and music features,\nmitigating condition conflicts and offering rich, multi-conditioned guidance\nfor dance generation. Additionally, a specially designed beat alignment loss\nensures the generated dance movements remain in sync with the designated beats.\nExtensive experiments confirm Beat-It's superiority over existing\nstate-of-the-art methods in terms of beat alignment and motion controllability.", "published": "2024-07-10 11:29:25", "link": "http://arxiv.org/abs/2407.07554v1", "categories": ["cs.GR", "cs.SD", "eess.AS"], "primary_category": "cs.GR"}
{"title": "Scaling Law in Neural Data: Non-Invasive Speech Decoding with 175 Hours\n  of EEG Data", "abstract": "Brain-computer interfaces (BCIs) hold great potential for aiding individuals\nwith speech impairments. Utilizing electroencephalography (EEG) to decode\nspeech is particularly promising due to its non-invasive nature. However,\nrecordings are typically short, and the high variability in EEG data has led\nresearchers to focus on classification tasks with a few dozen classes. To\nassess its practical applicability for speech neuroprostheses, we investigate\nthe relationship between the size of EEG data and decoding accuracy in the open\nvocabulary setting. We collected extensive EEG data from a single participant\n(175 hours) and conducted zero-shot speech segment classification using\nself-supervised representation learning. The model trained on the entire\ndataset achieved a top-1 accuracy of 48\\% and a top-10 accuracy of 76\\%, while\nmitigating the effects of myopotential artifacts. Conversely, when the data was\nlimited to the typical amount used in practice ($\\sim$10 hours), the top-1\naccuracy dropped to 2.5\\%, revealing a significant scaling effect.\nAdditionally, as the amount of training data increased, the EEG latent\nrepresentation progressively exhibited clearer temporal structures of spoken\nphrases. This indicates that the decoder can recognize speech segments in a\ndata-driven manner without explicit measurements of word recognition. This\nresearch marks a significant step towards the practical realization of\nEEG-based speech BCIs.", "published": "2024-07-10 12:29:01", "link": "http://arxiv.org/abs/2407.07595v1", "categories": ["q-bio.NC", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "q-bio.NC"}
{"title": "Targeted Augmented Data for Audio Deepfake Detection", "abstract": "The availability of highly convincing audio deepfake generators highlights\nthe need for designing robust audio deepfake detectors. Existing works often\nrely solely on real and fake data available in the training set, which may lead\nto overfitting, thereby reducing the robustness to unseen manipulations. To\nenhance the generalization capabilities of audio deepfake detectors, we propose\na novel augmentation method for generating audio pseudo-fakes targeting the\ndecision boundary of the model. Inspired by adversarial attacks, we perturb\noriginal real data to synthesize pseudo-fakes with ambiguous prediction\nprobabilities. Comprehensive experiments on two well-known architectures\ndemonstrate that the proposed augmentation contributes to improving the\ngeneralization capabilities of these architectures.", "published": "2024-07-10 12:31:53", "link": "http://arxiv.org/abs/2407.07598v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SaMoye: Zero-shot Singing Voice Conversion Model Based on Feature\n  Disentanglement and Enhancement", "abstract": "Singing voice conversion (SVC) aims to convert a singer's voice to another\nsinger's from a reference audio while keeping the original semantics. However,\nexisting SVC methods can hardly perform zero-shot due to incomplete feature\ndisentanglement or dependence on the speaker look-up table. We propose the\nfirst open-source high-quality zero-shot SVC model SaMoye that can convert\nsinging to human and non-human timbre. SaMoye disentangles the singing voice's\nfeatures into content, timbre, and pitch features, where we combine multiple\nASR models and compress the content features to reduce timbre leaks. Besides,\nwe enhance the timbre features by unfreezing the speaker encoder and mixing the\nspeaker embedding with top-3 similar speakers. We also establish an\nunparalleled large-scale dataset to guarantee zero-shot performance, which\ncomprises more than 1,815 hours of pure singing voice and 6,367 speakers. We\nconduct objective and subjective experiments to find that SaMoye outperforms\nother models in zero-shot SVC tasks even under extreme conditions like\nconverting singing to animals' timbre. The code and weight of SaMoye are\navailable on https://github.com/CarlWangChina/SaMoye-SVC. The weights, code,\ndataset, and documents of SaMoye are publicly available on\n\\url{https://github.com/CarlWangChina/SaMoye-SVC}.", "published": "2024-07-10 15:00:08", "link": "http://arxiv.org/abs/2407.07728v5", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS", "68Txx(Primary)14F05, 91Fxx(Secondary)", "I.2.7; J.5"], "primary_category": "cs.SD"}
{"title": "RT-LA-VocE: Real-Time Low-SNR Audio-Visual Speech Enhancement", "abstract": "In this paper, we aim to generate clean speech frame by frame from a live\nvideo stream and a noisy audio stream without relying on future inputs. To this\nend, we propose RT-LA-VocE, which completely re-designs every component of\nLA-VocE, a state-of-the-art non-causal audio-visual speech enhancement model,\nto perform causal real-time inference with a 40ms input frame. We do so by\ndevising new visual and audio encoders that rely solely on past frames,\nreplacing the Transformer encoder with the Emformer, and designing a new causal\nneural vocoder C-HiFi-GAN. On the popular AVSpeech dataset, we show that our\nalgorithm achieves state-of-the-art results in all real-time scenarios. More\nimportantly, each component is carefully tuned to minimize the algorithm\nlatency to the theoretical minimum (40ms) while maintaining a low end-to-end\nprocessing latency of 28.15ms per frame, enabling real-time frame-by-frame\nenhancement with minimal delay.", "published": "2024-07-10 16:49:23", "link": "http://arxiv.org/abs/2407.07825v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech dereverberation constrained on room impulse response\n  characteristics", "abstract": "Single-channel speech dereverberation aims at extracting a dry speech signal\nfrom a recording affected by the acoustic reflections in a room. However, most\ncurrent deep learning-based approaches for speech dereverberation are not\ninterpretable for room acoustics, and can be considered as black-box systems in\nthat regard. In this work, we address this problem by regularizing the training\nloss using a novel physical coherence loss which encourages the room impulse\nresponse (RIR) induced by the dereverberated output of the model to match the\nacoustic properties of the room in which the signal was recorded. Our\ninvestigation demonstrates the preservation of the original dereverberated\nsignal alongside the provision of a more physically coherent RIR.", "published": "2024-07-10 07:31:49", "link": "http://arxiv.org/abs/2407.08657v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
