{"title": "Personalized Dialogue Generation with Persona-Adaptive Attention", "abstract": "Persona-based dialogue systems aim to generate consistent responses based on\nhistorical context and predefined persona. Unlike conventional dialogue\ngeneration, the persona-based dialogue needs to consider both dialogue context\nand persona, posing a challenge for coherent training. Specifically, this\nrequires a delicate weight balance between context and persona. To achieve\nthat, in this paper, we propose an effective framework with Persona-Adaptive\nAttention (PAA), which adaptively integrates the weights from the persona and\ncontext information via our designed attention. In addition, a dynamic masking\nmechanism is applied to the PAA to not only drop redundant information in\ncontext and persona but also serve as a regularization mechanism to avoid\noverfitting. Experimental results demonstrate the superiority of the proposed\nPAA framework compared to the strong baselines in both automatic and human\nevaluation. Moreover, the proposed PAA approach can perform equivalently well\nin a low-resource regime compared to models trained in a full-data setting,\nwhich achieve a similar result with only 20% to 30% of data compared to the\nlarger models trained in the full-data setting. To fully exploit the\neffectiveness of our design, we designed several variants for handling the\nweighted information in different ways, showing the necessity and sufficiency\nof our weighting and masking designs.", "published": "2022-10-27 00:22:16", "link": "http://arxiv.org/abs/2210.15088v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Syntax Complies with the Free-Energy Principle", "abstract": "Natural language syntax yields an unbounded array of hierarchically\nstructured expressions. We claim that these are used in the service of active\ninference in accord with the free-energy principle (FEP). While conceptual\nadvances alongside modelling and simulation work have attempted to connect\nspeech segmentation and linguistic communication with the FEP, we extend this\nprogram to the underlying computations responsible for generating syntactic\nobjects. We argue that recently proposed principles of economy in language\ndesign - such as \"minimal search\" criteria from theoretical syntax - adhere to\nthe FEP. This affords a greater degree of explanatory power to the FEP - with\nrespect to higher language functions - and offers linguistics a grounding in\nfirst principles with respect to computability. We show how both tree-geometric\ndepth and a Kolmogorov complexity estimate (recruiting a Lempel-Ziv compression\nalgorithm) can be used to accurately predict legal operations on syntactic\nworkspaces, directly in line with formulations of variational free energy\nminimization. This is used to motivate a general principle of language design\nthat we term Turing-Chomsky Compression (TCC). We use TCC to align concerns of\nlinguists with the normative account of self-organization furnished by the FEP,\nby marshalling evidence from theoretical linguistics and psycholinguistics to\nground core principles of efficient syntactic computation within active\ninference.", "published": "2022-10-27 00:59:21", "link": "http://arxiv.org/abs/2210.15098v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TRScore: A Novel GPT-based Readability Scorer for ASR Segmentation and\n  Punctuation model evaluation and selection", "abstract": "Punctuation and Segmentation are key to readability in Automatic Speech\nRecognition (ASR), often evaluated using F1 scores that require high-quality\nhuman transcripts and do not reflect readability well. Human evaluation is\nexpensive, time-consuming, and suffers from large inter-observer variability,\nespecially in conversational speech devoid of strict grammatical structures.\nLarge pre-trained models capture a notion of grammatical structure. We present\nTRScore, a novel readability measure using the GPT model to evaluate different\nsegmentation and punctuation systems. We validate our approach with human\nexperts. Additionally, our approach enables quantitative assessment of text\npost-processing techniques such as capitalization, inverse text normalization\n(ITN), and disfluency on overall readability, which traditional word error rate\n(WER) and slot error rate (SER) metrics fail to capture. TRScore is strongly\ncorrelated to traditional F1 and human readability scores, with Pearson's\ncorrelation coefficients of 0.67 and 0.98, respectively. It also eliminates the\nneed for human transcriptions for model selection.", "published": "2022-10-27 01:11:32", "link": "http://arxiv.org/abs/2210.15104v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Curriculum Learning Approach for Multi-domain Text Classification\n  Using Keyword weight Ranking", "abstract": "Text classification is a very classic NLP task, but it has two prominent\nshortcomings: On the one hand, text classification is deeply domain-dependent.\nThat is, a classifier trained on the corpus of one domain may not perform so\nwell in another domain. On the other hand, text classification models require a\nlot of annotated data for training. However, for some domains, there may not\nexist enough annotated data. Therefore, it is valuable to investigate how to\nefficiently utilize text data from different domains to improve the performance\nof models in various domains. Some multi-domain text classification models are\ntrained by adversarial training to extract shared features among all domains\nand the specific features of each domain. We noted that the distinctness of the\ndomain-specific features is different, so in this paper, we propose to use a\ncurriculum learning strategy based on keyword weight ranking to improve the\nperformance of multi-domain text classification models. The experimental\nresults on the Amazon review and FDU-MTL datasets show that our curriculum\nlearning strategy effectively improves the performance of multi-domain text\nclassification models based on adversarial learning and outperforms\nstate-of-the-art methods.", "published": "2022-10-27 03:15:26", "link": "http://arxiv.org/abs/2210.15147v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disentangled and Robust Representation Learning for Bragging\n  Classification in Social Media", "abstract": "Researching bragging behavior on social media arouses interest of\ncomputational (socio) linguists. However, existing bragging classification\ndatasets suffer from a serious data imbalance issue. Because labeling a\ndata-balance dataset is expensive, most methods introduce external knowledge to\nimprove model learning. Nevertheless, such methods inevitably introduce noise\nand non-relevance information from external knowledge. To overcome the\ndrawback, we propose a novel bragging classification method with\ndisentangle-based representation augmentation and domain-aware adversarial\nstrategy. Specifically, model learns to disentangle and reconstruct\nrepresentation and generate augmented features via disentangle-based\nrepresentation augmentation. Moreover, domain-aware adversarial strategy aims\nto constrain domain of augmented features to improve their robustness.\nExperimental results demonstrate that our method achieves state-of-the-art\nperformance compared to other methods.", "published": "2022-10-27 05:15:16", "link": "http://arxiv.org/abs/2210.15180v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Too Brittle To Touch: Comparing the Stability of Quantization and\n  Distillation Towards Developing Lightweight Low-Resource MT Models", "abstract": "Leveraging shared learning through Massively Multilingual Models,\nstate-of-the-art machine translation models are often able to adapt to the\npaucity of data for low-resource languages. However, this performance comes at\nthe cost of significantly bloated models which are not practically deployable.\nKnowledge Distillation is one popular technique to develop competitive,\nlightweight models: In this work, we first evaluate its use to compress MT\nmodels focusing on languages with extremely limited training data. Through our\nanalysis across 8 languages, we find that the variance in the performance of\nthe distilled models due to their dependence on priors including the amount of\nsynthetic data used for distillation, the student architecture, training\nhyperparameters and confidence of the teacher models, makes distillation a\nbrittle compression mechanism. To mitigate this, we explore the use of\npost-training quantization for the compression of these models. Here, we find\nthat while distillation provides gains across some low-resource languages,\nquantization provides more consistent performance trends for the entire range\nof languages, especially the lowest-resource languages in our target set.", "published": "2022-10-27 05:30:13", "link": "http://arxiv.org/abs/2210.15184v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Truncation Sampling as Language Model Desmoothing", "abstract": "Long samples of text from neural language models can be of poor quality.\nTruncation sampling algorithms--like top-$p$ or top-$k$ -- address this by\nsetting some words' probabilities to zero at each step. This work provides\nframing for the aim of truncation, and an improved algorithm for that aim. We\npropose thinking of a neural language model as a mixture of a true distribution\nand a smoothing distribution that avoids infinite perplexity. In this light,\ntruncation algorithms aim to perform desmoothing, estimating a subset of the\nsupport of the true distribution. Finding a good subset is crucial: we show\nthat top-$p$ unnecessarily truncates high-probability words, for example\ncausing it to truncate all words but Trump for a document that starts with\nDonald. We introduce $\\eta$-sampling, which truncates words below an\nentropy-dependent probability threshold. Compared to previous algorithms,\n$\\eta$-sampling generates more plausible long English documents according to\nhumans, is better at breaking out of repetition, and behaves more reasonably on\na battery of test distributions.", "published": "2022-10-27 05:52:35", "link": "http://arxiv.org/abs/2210.15191v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parsing linearizations appreciate PoS tags - but some are fussy about\n  errors", "abstract": "PoS tags, once taken for granted as a useful resource for syntactic parsing,\nhave become more situational with the popularization of deep learning. Recent\nwork on the impact of PoS tags on graph- and transition-based parsers suggests\nthat they are only useful when tagging accuracy is prohibitively high, or in\nlow-resource scenarios. However, such an analysis is lacking for the emerging\nsequence labeling parsing paradigm, where it is especially relevant as some\nmodels explicitly use PoS tags for encoding and decoding. We undertake a study\nand uncover some trends. Among them, PoS tags are generally more useful for\nsequence labeling parsers than for other paradigms, but the impact of their\naccuracy is highly encoding-dependent, with the PoS-based head-selection\nencoding being best only when both tagging accuracy and resource availability\nare high.", "published": "2022-10-27 07:15:36", "link": "http://arxiv.org/abs/2210.15219v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Effect of Normalization for Bi-directional Amharic-English Neural\n  Machine Translation", "abstract": "Machine translation (MT) is one of the main tasks in natural language\nprocessing whose objective is to translate texts automatically from one natural\nlanguage to another. Nowadays, using deep neural networks for MT tasks has\nreceived great attention. These networks require lots of data to learn abstract\nrepresentations of the input and store it in continuous vectors. This paper\npresents the first relatively large-scale Amharic-English parallel sentence\ndataset. Using these compiled data, we build bi-directional Amharic-English\ntranslation models by fine-tuning the existing Facebook M2M100 pre-trained\nmodel achieving a BLEU score of 37.79 in Amharic-English 32.74 in\nEnglish-Amharic translation. Additionally, we explore the effects of Amharic\nhomophone normalization on the machine translation task. The results show that\nthe normalization of Amharic homophone characters increases the performance of\nAmharic-English machine translation in both directions.", "published": "2022-10-27 07:18:53", "link": "http://arxiv.org/abs/2210.15224v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT-Flow-VAE: A Weakly-supervised Model for Multi-Label Text\n  Classification", "abstract": "Multi-label Text Classification (MLTC) is the task of categorizing documents\ninto one or more topics. Considering the large volumes of data and varying\ndomains of such tasks, fully supervised learning requires manually fully\nannotated datasets which is costly and time-consuming. In this paper, we\npropose BERT-Flow-VAE (BFV), a Weakly-Supervised Multi-Label Text\nClassification (WSMLTC) model that reduces the need for full supervision. This\nnew model (1) produces BERT sentence embeddings and calibrates them using a\nflow model, (2) generates an initial topic-document matrix by averaging results\nof a seeded sparse topic model and a textual entailment model which only\nrequire surface name of topics and 4-6 seed words per topic, and (3) adopts a\nVAE framework to reconstruct the embeddings under the guidance of the\ntopic-document matrix. Finally, (4) it uses the means produced by the encoder\nmodel in the VAE architecture as predictions for MLTC. Experimental results on\n6 multi-label datasets show that BFV can substantially outperform other\nbaseline WSMLTC models in key metrics and achieve approximately 84% performance\nof a fully-supervised model.", "published": "2022-10-27 07:18:56", "link": "http://arxiv.org/abs/2210.15225v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Creating a morphological and syntactic tagged corpus for the Uzbek\n  language", "abstract": "Nowadays, creation of the tagged corpora is becoming one of the most\nimportant tasks of Natural Language Processing (NLP). There are not enough\ntagged corpora to build machine learning models for the low-resource Uzbek\nlanguage. In this paper, we tried to fill that gap by developing a novel Part\nOf Speech (POS) and syntactic tagset for creating the syntactic and\nmorphologically tagged corpus of the Uzbek language. This work also includes\ndetailed description and presentation of a web-based application to work on a\ntagging as well. Based on the developed annotation tool and the software, we\nshare our experience results of the first stage of the tagged corpus creation", "published": "2022-10-27 07:44:12", "link": "http://arxiv.org/abs/2210.15234v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Knowledge Graph Construction and Event-centric Knowledge\n  Infusion for Scientific NLI", "abstract": "With the advance of natural language inference (NLI), a rising demand for NLI\nis to handle scientific texts. Existing methods depend on pre-trained models\n(PTM) which lack domain-specific knowledge. To tackle this drawback, we\nintroduce a scientific knowledge graph to generalize PTM to scientific domain.\nHowever, existing knowledge graph construction approaches suffer from some\ndrawbacks, i.e., expensive labeled data, failure to apply in other domains,\nlong inference time and difficulty extending to large corpora. Therefore, we\npropose an unsupervised knowledge graph construction method to build a\nscientific knowledge graph (SKG) without any labeled data. Moreover, to\nalleviate noise effect from SKG and complement knowledge in sentences better,\nwe propose an event-centric knowledge infusion method to integrate external\nknowledge into each event that is a fine-grained semantic unit in sentences.\nExperimental results show that our method achieves state-of-the-art performance\nand the effectiveness and reliability of SKG.", "published": "2022-10-27 08:09:16", "link": "http://arxiv.org/abs/2210.15248v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conversation Disentanglement with Bi-Level Contrastive Learning", "abstract": "Conversation disentanglement aims to group utterances into detached sessions,\nwhich is a fundamental task in processing multi-party conversations. Existing\nmethods have two main drawbacks. First, they overemphasize pairwise utterance\nrelations but pay inadequate attention to the utterance-to-context relation\nmodeling. Second, huge amount of human annotated data is required for training,\nwhich is expensive to obtain in practice. To address these issues, we propose a\ngeneral disentangle model based on bi-level contrastive learning. It brings\ncloser utterances in the same session while encourages each utterance to be\nnear its clustered session prototypes in the representation space. Unlike\nexisting approaches, our disentangle model works in both supervised setting\nwith labeled data and unsupervised setting when no such data is available. The\nproposed method achieves new state-of-the-art performance on both settings\nacross several public datasets.", "published": "2022-10-27 08:41:46", "link": "http://arxiv.org/abs/2210.15265v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging knowledge graphs to update scientific word embeddings using\n  latent semantic imputation", "abstract": "The most interesting words in scientific texts will often be novel or rare.\nThis presents a challenge for scientific word embedding models to determine\nquality embedding vectors for useful terms that are infrequent or newly\nemerging. We demonstrate how \\gls{lsi} can address this problem by imputing\nembeddings for domain-specific words from up-to-date knowledge graphs while\notherwise preserving the original word embedding model. We use the MeSH\nknowledge graph to impute embedding vectors for biomedical terminology without\nretraining and evaluate the resulting embedding model on a domain-specific\nword-pair similarity task. We show that LSI can produce reliable embedding\nvectors for rare and OOV terms in the biomedical domain.", "published": "2022-10-27 12:15:26", "link": "http://arxiv.org/abs/2210.15358v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-consistent Reasoning For Solving Math Word Problems", "abstract": "Math word problems (MWPs) is a task that automatically derives solution\nexpression from a giving math problems in text. The previous studies suffer\nfrom spurious correlations between input text and output expression. To\nmitigate this issue, we propose a self-consistent reasoning framework called\nSCR, which attempts to adopt a pruning strategy to correct the output\ndistribution shift so as to implicitly fix those spurious correlative samples.\nSpecifically, we firstly obtain a sub-network by pruning a roberta2tree model,\nfor the sake to use the gap on output distribution between the original\nroberta2tree model and the pruned sub-network to expose spurious correlative\nsamples. Then, we calibrate the output distribution shift by applying symmetric\nKullback-Leibler divergence to alleviate spurious correlations. In addition,\nSCR generates equivalent expressions, thereby, capturing the original text's\nlogic rather than relying on hints from original text. Extensive experiments on\ntwo large-scale benchmarks demonstrate that our model substantially outperforms\nthe strong baseline methods.", "published": "2022-10-27 12:34:31", "link": "http://arxiv.org/abs/2210.15373v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "He Said, She Said: Style Transfer for Shifting the Perspective of\n  Dialogues", "abstract": "In this work, we define a new style transfer task: perspective shift, which\nreframes a dialogue from informal first person to a formal third person\nrephrasing of the text. This task requires challenging coreference resolution,\nemotion attribution, and interpretation of informal text. We explore several\nbaseline approaches and discuss further directions on this task when applied to\nshort dialogues. As a sample application, we demonstrate that applying\nperspective shifting to a dialogue summarization dataset (SAMSum) substantially\nimproves the zero-shot performance of extractive news summarization models on\nthis data. Additionally, supervised extractive models perform better when\ntrained on perspective shifted data than on the original dialogues. We release\nour code publicly.", "published": "2022-10-27 14:16:07", "link": "http://arxiv.org/abs/2210.15462v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COST-EFF: Collaborative Optimization of Spatial and Temporal Efficiency\n  with Slenderized Multi-exit Language Models", "abstract": "Transformer-based pre-trained language models (PLMs) mostly suffer from\nexcessive overhead despite their advanced capacity. For resource-constrained\ndevices, there is an urgent need for a spatially and temporally efficient model\nwhich retains the major capacity of PLMs. However, existing statically\ncompressed models are unaware of the diverse complexities between input\ninstances, potentially resulting in redundancy and inadequacy for simple and\ncomplex inputs. Also, miniature models with early exiting encounter challenges\nin the trade-off between making predictions and serving the deeper layers.\nMotivated by such considerations, we propose a collaborative optimization for\nPLMs that integrates static model compression and dynamic inference\nacceleration. Specifically, the PLM is slenderized in width while the depth\nremains intact, complementing layer-wise early exiting to speed up inference\ndynamically. To address the trade-off of early exiting, we propose a joint\ntraining approach that calibrates slenderization and preserves contributive\nstructures to each exit instead of only the final layer. Experiments are\nconducted on GLUE benchmark and the results verify the Pareto optimality of our\napproach at high compression and acceleration rate with 1/8 parameters and 1/19\nFLOPs of BERT.", "published": "2022-10-27 15:06:40", "link": "http://arxiv.org/abs/2210.15523v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Terminology-aware Medical Dialogue Generation", "abstract": "Medical dialogue generation aims to generate responses according to a history\nof dialogue turns between doctors and patients. Unlike open-domain dialogue\ngeneration, this requires background knowledge specific to the medical domain.\nExisting generative frameworks for medical dialogue generation fall short of\nincorporating domain-specific knowledge, especially with regard to medical\nterminology. In this paper, we propose a novel framework to improve medical\ndialogue generation by considering features centered on domain-specific\nterminology. We leverage an attention mechanism to incorporate terminologically\ncentred features, and fill in the semantic gap between medical background\nknowledge and common utterances by enforcing language models to learn\nterminology representations with an auxiliary terminology recognition task.\nExperimental results demonstrate the effectiveness of our approach, in which\nour proposed framework outperforms SOTA language models. Additionally, we\nprovide a new dataset with medical terminology annotations to support the\nresearch on medical dialogue generation. Our dataset and code are available at\nhttps://github.com/tangg555/meddialog.", "published": "2022-10-27 15:41:46", "link": "http://arxiv.org/abs/2210.15551v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ACES: Translation Accuracy Challenge Sets for Evaluating Machine\n  Translation Metrics", "abstract": "As machine translation (MT) metrics improve their correlation with human\njudgement every year, it is crucial to understand the limitations of such\nmetrics at the segment level. Specifically, it is important to investigate\nmetric behaviour when facing accuracy errors in MT because these can have\ndangerous consequences in certain contexts (e.g., legal, medical). We curate\nACES, a translation accuracy challenge set, consisting of 68 phenomena ranging\nfrom simple perturbations at the word/character level to more complex errors\nbased on discourse and real-world knowledge. We use ACES to evaluate a wide\nrange of MT metrics including the submissions to the WMT 2022 metrics shared\ntask and perform several analyses leading to general recommendations for metric\ndevelopers. We recommend: a) combining metrics with different strengths, b)\ndeveloping metrics that give more weight to the source and less to\nsurface-level overlap with the reference and c) explicitly modelling additional\nlanguage-specific information beyond what is available via multilingual\nembeddings.", "published": "2022-10-27 16:59:02", "link": "http://arxiv.org/abs/2210.15615v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "COMET-QE and Active Learning for Low-Resource Machine Translation", "abstract": "Active learning aims to deliver maximum benefit when resources are scarce. We\nuse COMET-QE, a reference-free evaluation metric, to select sentences for\nlow-resource neural machine translation. Using Swahili, Kinyarwanda and Spanish\nfor our experiments, we show that COMET-QE significantly outperforms two\nvariants of Round Trip Translation Likelihood (RTTL) and random sentence\nselection by up to 5 BLEU points for 20k sentences selected by Active Learning\non a 30k baseline. This suggests that COMET-QE is a powerful tool for sentence\nselection in the very low-resource limit.", "published": "2022-10-27 18:00:41", "link": "http://arxiv.org/abs/2210.15696v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Nearest Neighbor Language Models for Stylistic Controllable Generation", "abstract": "Recent language modeling performance has been greatly improved by the use of\nexternal memory. This memory encodes the context so that similar contexts can\nbe recalled during decoding. This similarity depends on how the model learns to\nencode context, which can be altered to include other attributes, such as\nstyle. We construct and evaluate an architecture for this purpose, using\ncorpora annotated for politeness, formality, and toxicity. Through extensive\nexperiments and human evaluation we demonstrate the potential of our method to\ngenerate text while controlling style. We find that style-specific datastores\nimprove generation performance, though results vary greatly across styles, and\nthe effect of pretraining data and specific styles should be explored in future\nwork.", "published": "2022-10-27 20:46:12", "link": "http://arxiv.org/abs/2210.15762v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Masked Vision-Language Transformer in Fashion", "abstract": "We present a masked vision-language transformer (MVLT) for fashion-specific\nmulti-modal representation. Technically, we simply utilize vision transformer\narchitecture for replacing the BERT in the pre-training model, making MVLT the\nfirst end-to-end framework for the fashion domain. Besides, we designed masked\nimage reconstruction (MIR) for a fine-grained understanding of fashion. MVLT is\nan extensible and convenient architecture that admits raw multi-modal inputs\nwithout extra pre-processing models (e.g., ResNet), implicitly modeling the\nvision-language alignments. More importantly, MVLT can easily generalize to\nvarious matching and generative tasks. Experimental results show obvious\nimprovements in retrieval (rank@5: 17%) and recognition (accuracy: 3%) tasks\nover the Fashion-Gen 2018 winner Kaleido-BERT. Code is made available at\nhttps://github.com/GewelsJI/MVLT.", "published": "2022-10-27 01:44:08", "link": "http://arxiv.org/abs/2210.15110v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage\n  Retrieval", "abstract": "Pre-trained language model (PTM) has been shown to yield powerful text\nrepresentations for dense passage retrieval task. The Masked Language Modeling\n(MLM) is a major sub-task of the pre-training process. However, we found that\nthe conventional random masking strategy tend to select a large number of\ntokens that have limited effect on the passage retrieval task (e,g. stop-words\nand punctuation). By noticing the term importance weight can provide valuable\ninformation for passage retrieval, we hereby propose alternative retrieval\noriented masking (dubbed as ROM) strategy where more important tokens will have\na higher probability of being masked out, to capture this straightforward yet\nessential information to facilitate the language model pre-training process.\nNotably, the proposed new token masking method will not change the architecture\nand learning objective of original PTM. Our experiments verify that the\nproposed ROM enables term importance information to help language model\npre-training thus achieving better performance on multiple passage retrieval\nbenchmarks.", "published": "2022-10-27 02:43:48", "link": "http://arxiv.org/abs/2210.15133v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Gendered Mental Health Stigma in Masked Language Models", "abstract": "Mental health stigma prevents many individuals from receiving the appropriate\ncare, and social psychology studies have shown that mental health tends to be\noverlooked in men. In this work, we investigate gendered mental health stigma\nin masked language models. In doing so, we operationalize mental health stigma\nby developing a framework grounded in psychology research: we use clinical\npsychology literature to curate prompts, then evaluate the models' propensity\nto generate gendered words. We find that masked language models capture\nsocietal stigma about gender in mental health: models are consistently more\nlikely to predict female subjects than male in sentences about having a mental\nhealth condition (32% vs. 19%), and this disparity is exacerbated for sentences\nthat indicate treatment-seeking behavior. Furthermore, we find that different\nmodels capture dimensions of stigma differently for men and women, associating\nstereotypes like anger, blame, and pity more with women with mental health\nconditions than with men. In showing the complex nuances of models' gendered\nmental health stigma, we demonstrate that context and overlapping dimensions of\nidentity are important considerations when assessing computational models'\nsocial biases.", "published": "2022-10-27 03:09:46", "link": "http://arxiv.org/abs/2210.15144v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Dictionary-Assisted Supervised Contrastive Learning", "abstract": "Text analysis in the social sciences often involves using specialized\ndictionaries to reason with abstract concepts, such as perceptions about the\neconomy or abuse on social media. These dictionaries allow researchers to\nimpart domain knowledge and note subtle usages of words relating to a\nconcept(s) of interest. We introduce the dictionary-assisted supervised\ncontrastive learning (DASCL) objective, allowing researchers to leverage\nspecialized dictionaries when fine-tuning pretrained language models. The text\nis first keyword simplified: a common, fixed token replaces any word in the\ncorpus that appears in the dictionary(ies) relevant to the concept of interest.\nDuring fine-tuning, a supervised contrastive objective draws closer the\nembeddings of the original and keyword-simplified texts of the same class while\npushing further apart the embeddings of different classes. The\nkeyword-simplified texts of the same class are more textually similar than\ntheir original text counterparts, which additionally draws the embeddings of\nthe same class closer together. Combining DASCL and cross-entropy improves\nclassification performance metrics in few-shot learning settings and social\nscience applications compared to using cross-entropy alone and alternative\ncontrastive and data augmentation methods.", "published": "2022-10-27 04:57:43", "link": "http://arxiv.org/abs/2210.15172v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TASA: Deceiving Question Answering Models by Twin Answer Sentences\n  Attack", "abstract": "We present Twin Answer Sentences Attack (TASA), an adversarial attack method\nfor question answering (QA) models that produces fluent and grammatical\nadversarial contexts while maintaining gold answers. Despite phenomenal\nprogress on general adversarial attacks, few works have investigated the\nvulnerability and attack specifically for QA models. In this work, we first\nexplore the biases in the existing models and discover that they mainly rely on\nkeyword matching between the question and context, and ignore the relevant\ncontextual relations for answer prediction. Based on two biases above, TASA\nattacks the target model in two folds: (1) lowering the model's confidence on\nthe gold answer with a perturbed answer sentence; (2) misguiding the model\ntowards a wrong answer with a distracting answer sentence. Equipped with\ndesigned beam search and filtering methods, TASA can generate more effective\nattacks than existing textual attack methods while sustaining the quality of\ncontexts, in extensive experiments on five QA datasets and human evaluations.", "published": "2022-10-27 07:16:30", "link": "http://arxiv.org/abs/2210.15221v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Boundary-Aware Language Model Pretraining for Chinese\n  Sequence Labeling", "abstract": "Boundary information is critical for various Chinese language processing\ntasks, such as word segmentation, part-of-speech tagging, and named entity\nrecognition. Previous studies usually resorted to the use of a high-quality\nexternal lexicon, where lexicon items can offer explicit boundary information.\nHowever, to ensure the quality of the lexicon, great human effort is always\nnecessary, which has been generally ignored. In this work, we suggest\nunsupervised statistical boundary information instead, and propose an\narchitecture to encode the information directly into pre-trained language\nmodels, resulting in Boundary-Aware BERT (BABERT). We apply BABERT for feature\ninduction of Chinese sequence labeling tasks. Experimental results on ten\nbenchmarks of Chinese sequence labeling demonstrate that BABERT can provide\nconsistent improvements on all datasets. In addition, our method can complement\nprevious supervised lexicon exploration, where further improvements can be\nachieved when integrated with external lexicon information.", "published": "2022-10-27 07:38:50", "link": "http://arxiv.org/abs/2210.15231v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Seq2Seq-SC: End-to-End Semantic Communication Systems with Pre-trained\n  Language Model", "abstract": "In this work, we propose a realistic semantic network called seq2seq-SC,\ndesigned to be compatible with 5G NR and capable of working with generalized\ntext datasets using a pre-trained language model. The goal is to achieve\nunprecedented communication efficiency by focusing on the meaning of messages\nin semantic communication. We employ a performance metric called semantic\nsimilarity, measured by BLEU for lexical similarity and SBERT for semantic\nsimilarity. Our findings demonstrate that seq2seq-SC outperforms previous\nmodels in extracting semantically meaningful information while maintaining\nsuperior performance. This study paves the way for continued advancements in\nsemantic communication and its prospective incorporation with future wireless\nsystems in 6G networks.", "published": "2022-10-27 07:48:18", "link": "http://arxiv.org/abs/2210.15237v2", "categories": ["eess.SP", "cs.CL"], "primary_category": "eess.SP"}
{"title": "Towards Language-driven Scientific AI", "abstract": "Inspired by recent and revolutionary developments in AI, particularly in\nlanguage understanding and generation, we set about designing AI systems that\nare able to address complex scientific tasks that challenge human capabilities\nto make new discoveries. Central to our approach is the notion of natural\nlanguage as core representation, reasoning, and exchange format between\nscientific AI and human scientists. In this paper, we identify and discuss some\nof the main research challenges to accomplish such vision.", "published": "2022-10-27 11:11:08", "link": "http://arxiv.org/abs/2210.15327v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dial2vec: Self-Guided Contrastive Learning of Unsupervised Dialogue\n  Embeddings", "abstract": "In this paper, we introduce the task of learning unsupervised dialogue\nembeddings. Trivial approaches such as combining pre-trained word or sentence\nembeddings and encoding through pre-trained language models (PLMs) have been\nshown to be feasible for this task. However, these approaches typically ignore\nthe conversational interactions between interlocutors, resulting in poor\nperformance. To address this issue, we proposed a self-guided contrastive\nlearning approach named dial2vec. Dial2vec considers a dialogue as an\ninformation exchange process. It captures the conversational interaction\npatterns between interlocutors and leverages them to guide the learning of the\nembeddings corresponding to each interlocutor. The dialogue embedding is\nobtained by an aggregation of the embeddings from all interlocutors. To verify\nour approach, we establish a comprehensive benchmark consisting of six\nwidely-used dialogue datasets. We consider three evaluation tasks: domain\ncategorization, semantic relatedness, and dialogue retrieval. Dial2vec achieves\non average 8.7, 9.0, and 13.8 points absolute improvements in terms of purity,\nSpearman's correlation, and mean average precision (MAP) over the strongest\nbaseline on the three tasks respectively. Further analysis shows that dial2vec\nobtains informative and discriminative embeddings for both interlocutors under\nthe guidance of the conversational interactions and achieves the best\nperformance when aggregating them through the interlocutor-level pooling\nstrategy. All codes and data are publicly available at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/dial2vec.", "published": "2022-10-27 11:14:06", "link": "http://arxiv.org/abs/2210.15332v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MorphTE: Injecting Morphology in Tensorized Embeddings", "abstract": "In the era of deep learning, word embeddings are essential when dealing with\ntext tasks. However, storing and accessing these embeddings requires a large\namount of space. This is not conducive to the deployment of these models on\nresource-limited devices. Combining the powerful compression capability of\ntensor products, we propose a word embedding compression method with\nmorphological augmentation, Morphologically-enhanced Tensorized Embeddings\n(MorphTE). A word consists of one or more morphemes, the smallest units that\nbear meaning or have a grammatical function. MorphTE represents a word\nembedding as an entangled form of its morpheme vectors via the tensor product,\nwhich injects prior semantic and grammatical knowledge into the learning of\nembeddings. Furthermore, the dimensionality of the morpheme vector and the\nnumber of morphemes are much smaller than those of words, which greatly reduces\nthe parameters of the word embeddings. We conduct experiments on tasks such as\nmachine translation and question answering. Experimental results on four\ntranslation datasets of different languages show that MorphTE can compress word\nembedding parameters by about 20 times without performance loss and\nsignificantly outperforms related embedding compression methods.", "published": "2022-10-27 12:43:28", "link": "http://arxiv.org/abs/2210.15379v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "QUILL: Query Intent with Large Language Models using Retrieval\n  Augmentation and Multi-stage Distillation", "abstract": "Large Language Models (LLMs) have shown impressive results on a variety of\ntext understanding tasks. Search queries though pose a unique challenge, given\ntheir short-length and lack of nuance or context. Complicated feature\nengineering efforts do not always lead to downstream improvements as their\nperformance benefits may be offset by increased complexity of knowledge\ndistillation. Thus, in this paper we make the following contributions: (1) We\ndemonstrate that Retrieval Augmentation of queries provides LLMs with valuable\nadditional context enabling improved understanding. While Retrieval\nAugmentation typically increases latency of LMs (thus hurting distillation\nefficacy), (2) we provide a practical and effective way of distilling Retrieval\nAugmentation LLMs. Specifically, we use a novel two-stage distillation approach\nthat allows us to carry over the gains of retrieval augmentation, without\nsuffering the increased compute typically associated with it. (3) We\ndemonstrate the benefits of the proposed approach (QUILL) on a billion-scale,\nreal-world query understanding system resulting in huge gains. Via extensive\nexperiments, including on public benchmarks, we believe this work offers a\nrecipe for practical use of retrieval-augmented query understanding.", "published": "2022-10-27 18:44:58", "link": "http://arxiv.org/abs/2210.15718v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Reinforced Question Rewriting for Conversational Question Answering", "abstract": "Conversational Question Answering (CQA) aims to answer questions contained\nwithin dialogues, which are not easily interpretable without context.\nDeveloping a model to rewrite conversational questions into self-contained ones\nis an emerging solution in industry settings as it allows using existing\nsingle-turn QA systems to avoid training a CQA model from scratch. Previous\nwork trains rewriting models using human rewrites as supervision. However, such\nobjectives are disconnected with QA models and therefore more human-like\nrewrites do not guarantee better QA performance. In this paper we propose using\nQA feedback to supervise the rewriting model with reinforcement learning.\nExperiments show that our approach can effectively improve QA performance over\nbaselines for both extractive and retrieval QA. Furthermore, human evaluation\nshows that our method can generate more accurate and detailed rewrites when\ncompared to human annotations.", "published": "2022-10-27 21:23:36", "link": "http://arxiv.org/abs/2210.15777v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Contrastive Decoding: Open-ended Text Generation as Optimization", "abstract": "Given a language model (LM), maximum probability is a poor decoding objective\nfor open-ended generation, because it produces short and repetitive text. On\nthe other hand, sampling can often produce incoherent text that drifts from the\noriginal topics. We propose contrastive decoding (CD), a reliable decoding\napproach that optimizes a contrastive objective subject to a plausibility\nconstraint. The contrastive objective returns the difference between the\nlikelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM\n(called the amateur, e.g. OPT-125M), and the constraint ensures that the\noutputs are plausible. CD is inspired by the fact that the failures of larger\nLMs (e.g., repetition, incoherence) are even more prevalent in smaller LMs, and\nthat this difference signals which texts should be preferred. CD requires zero\nadditional training, and produces higher quality text than decoding from the\nlarger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and\nsignificantly outperforms four strong decoding algorithms (e.g., nucleus,\ntop-k) in automatic and human evaluations across wikipedia, news and story\ndomains.", "published": "2022-10-27 00:58:21", "link": "http://arxiv.org/abs/2210.15097v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards High-Quality Neural TTS for Low-Resource Languages by Learning\n  Compact Speech Representations", "abstract": "This paper aims to enhance low-resource TTS by reducing training data\nrequirements using compact speech representations. A Multi-Stage Multi-Codebook\n(MSMC) VQ-GAN is trained to learn the representation, MSMCR, and decode it to\nwaveforms. Subsequently, we train the multi-stage predictor to predict MSMCRs\nfrom the text for TTS synthesis. Moreover, we optimize the training strategy by\nleveraging more audio to learn MSMCRs better for low-resource languages. It\nselects audio from other languages using speaker similarity metric to augment\nthe training set, and applies transfer learning to improve training quality. In\nMOS tests, the proposed system significantly outperforms FastSpeech and VITS in\nstandard and low-resource scenarios, showing lower data requirements. The\nproposed training strategy effectively enhances MSMCRs on waveform\nreconstruction. It improves TTS performance further, which wins 77% votes in\nthe preference test for the low-resource TTS with only 15 minutes of paired\ndata.", "published": "2022-10-27 02:32:00", "link": "http://arxiv.org/abs/2210.15131v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Training Autoregressive Speech Recognition Models with Limited in-domain\n  Supervision", "abstract": "Advances in self-supervised learning have significantly reduced the amount of\ntranscribed audio required for training. However, the majority of work in this\narea is focused on read speech. We explore limited supervision in the domain of\nconversational speech. While we assume the amount of in-domain data is limited,\nwe augment the model with open source read speech data. The XLS-R model has\nbeen shown to perform well with limited adaptation data and serves as a strong\nbaseline. We use untranscribed data for self-supervised learning and\nsemi-supervised training in an autoregressive encoder-decoder model. We\ndemonstrate that by using the XLS-R model for pseudotranscription, a much\nsmaller autoregressive model can outperform a finetuned XLS-R model when\ntranscribed in-domain data is limited, reducing WER by as much as 8% absolute.", "published": "2022-10-27 02:49:23", "link": "http://arxiv.org/abs/2210.15135v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Articulation GAN: Unsupervised modeling of articulatory learning", "abstract": "Generative deep neural networks are widely used for speech synthesis, but\nmost existing models directly generate waveforms or spectral outputs. Humans,\nhowever, produce speech by controlling articulators, which results in the\nproduction of speech sounds through physical properties of sound propagation.\nWe introduce the Articulatory Generator to the Generative Adversarial Network\nparadigm, a new unsupervised generative model of speech production/synthesis.\nThe Articulatory Generator more closely mimics human speech production by\nlearning to generate articulatory representations (electromagnetic\narticulography or EMA) in a fully unsupervised manner. A separate pre-trained\nphysical model (ema2wav) then transforms the generated EMA representations to\nspeech waveforms, which get sent to the Discriminator for evaluation.\nArticulatory analysis suggests that the network learns to control articulators\nin a similar manner to humans during speech production. Acoustic analysis of\nthe outputs suggests that the network learns to generate words that are both\npresent and absent in the training distribution. We additionally discuss\nimplications of articulatory representations for cognitive models of human\nlanguage and speech technology in general.", "published": "2022-10-27 05:07:04", "link": "http://arxiv.org/abs/2210.15173v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Outlier-Aware Training for Improving Group Accuracy Disparities", "abstract": "Methods addressing spurious correlations such as Just Train Twice (JTT,\narXiv:2107.09044v2) involve reweighting a subset of the training set to\nmaximize the worst-group accuracy. However, the reweighted set of examples may\npotentially contain unlearnable examples that hamper the model's learning. We\npropose mitigating this by detecting outliers to the training set and removing\nthem before reweighting. Our experiments show that our method achieves\ncompetitive or better accuracy compared with JTT and can detect and remove\nannotation errors in the subset being reweighted in JTT.", "published": "2022-10-27 05:27:23", "link": "http://arxiv.org/abs/2210.15183v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "COCO-DR: Combating Distribution Shifts in Zero-Shot Dense Retrieval with\n  Contrastive and Distributionally Robust Learning", "abstract": "We present a new zero-shot dense retrieval (ZeroDR) method, COCO-DR, to\nimprove the generalization ability of dense retrieval by combating the\ndistribution shifts between source training tasks and target scenarios. To\nmitigate the impact of document differences, COCO-DR continues pretraining the\nlanguage model on the target corpora to adapt the model to target distributions\nvia COtinuous COtrastive learning. To prepare for unseen target queries,\nCOCO-DR leverages implicit Distributionally Robust Optimization (iDRO) to\nreweight samples from different source query clusters for improving model\nrobustness over rare queries during fine-tuning. COCO-DR achieves superior\naverage performance on BEIR, the zero-shot retrieval benchmark. At BERT Base\nscale, COCO-DR Base outperforms other ZeroDR models with 60x larger size. At\nBERT Large scale, COCO-DR Large outperforms the giant GPT-3 embedding model\nwhich has 500x more parameters. Our analysis show the correlation between\nCOCO-DR's effectiveness in combating distribution shifts and improving\nzero-shot accuracy. Our code and model can be found at\n\\url{https://github.com/OpenMatch/COCO-DR}.", "published": "2022-10-27 06:51:39", "link": "http://arxiv.org/abs/2210.15212v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Iterative pseudo-forced alignment by acoustic CTC loss for\n  self-supervised ASR domain adaptation", "abstract": "High-quality data labeling from specific domains is costly and human\ntime-consuming. In this work, we propose a self-supervised domain adaptation\nmethod, based upon an iterative pseudo-forced alignment algorithm. The produced\nalignments are employed to customize an end-to-end Automatic Speech Recognition\n(ASR) and iteratively refined. The algorithm is fed with frame-wise character\nposteriors produced by a seed ASR, trained with out-of-domain data, and\noptimized throughout a Connectionist Temporal Classification (CTC) loss. The\nalignments are computed iteratively upon a corpus of broadcast TV. The process\nis repeated by reducing the quantity of text to be aligned or expanding the\nalignment window until finding the best possible audio-text alignment. The\nstarting timestamps, or temporal anchors, are produced uniquely based on the\nconfidence score of the last aligned utterance. This score is computed with the\npaths of the CTC-alignment matrix. With this methodology, no human-revised text\nreferences are required. Alignments from long audio files with low-quality\ntranscriptions, like TV captions, are filtered out by confidence score and\nready for further ASR adaptation. The obtained results, on both the Spanish\nRTVE2022 and CommonVoice databases, underpin the feasibility of using CTC-based\nsystems to perform: highly accurate audio-text alignments, domain adaptation\nand semi-supervised training of end-to-end ASR.", "published": "2022-10-27 07:23:08", "link": "http://arxiv.org/abs/2210.15226v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "How well can Text-to-Image Generative Models understand Ethical Natural\n  Language Interventions?", "abstract": "Text-to-image generative models have achieved unprecedented success in\ngenerating high-quality images based on natural language descriptions. However,\nit is shown that these models tend to favor specific social groups when\nprompted with neutral text descriptions (e.g., 'a photo of a lawyer').\nFollowing Zhao et al. (2021), we study the effect on the diversity of the\ngenerated images when adding ethical intervention that supports equitable\njudgment (e.g., 'if all individuals can be a lawyer irrespective of their\ngender') in the input prompts. To this end, we introduce an Ethical NaTural\nLanguage Interventions in Text-to-Image GENeration (ENTIGEN) benchmark dataset\nto evaluate the change in image generations conditional on ethical\ninterventions across three social axes -- gender, skin color, and culture.\nThrough ENTIGEN framework, we find that the generations from minDALL.E,\nDALL.E-mini and Stable Diffusion cover diverse social groups while preserving\nthe image quality. Preliminary studies indicate that a large change in the\nmodel predictions is triggered by certain phrases such as 'irrespective of\ngender' in the context of gender bias in the ethical interventions. We release\ncode and annotated data at https://github.com/Hritikbansal/entigen_emnlp.", "published": "2022-10-27 07:32:39", "link": "http://arxiv.org/abs/2210.15230v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Weight Averaging: A Simple Yet Effective Method to Overcome Catastrophic\n  Forgetting in Automatic Speech Recognition", "abstract": "Adapting a trained Automatic Speech Recognition (ASR) model to new tasks\nresults in catastrophic forgetting of old tasks, limiting the model's ability\nto learn continually and to be extended to new speakers, dialects, languages,\netc. Focusing on End-to-End ASR, in this paper, we propose a simple yet\neffective method to overcome catastrophic forgetting: weight averaging. By\nsimply taking the average of the previous and the adapted model, our method\nachieves high performance on both the old and new tasks. It can be further\nimproved by introducing a knowledge distillation loss during the adaptation. We\nillustrate the effectiveness of our method on both monolingual and multilingual\nASR. In both cases, our method strongly outperforms all baselines, even in its\nsimplest form.", "published": "2022-10-27 09:31:37", "link": "http://arxiv.org/abs/2210.15282v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SAN: a robust end-to-end ASR model architecture", "abstract": "In this paper, we propose a novel Siamese Adversarial Network (SAN)\narchitecture for automatic speech recognition, which aims at solving the\ndifficulty of fuzzy audio recognition. Specifically, SAN constructs two\nsub-networks to differentiate the audio feature input and then introduces a\nloss to unify the output distribution of these sub-networks. Adversarial\nlearning enables the network to capture more essential acoustic features and\nhelps the models achieve better performance when encountering fuzzy audio\ninput. We conduct numerical experiments with the SAN model on several datasets\nfor the automatic speech recognition task. All experimental results show that\nthe siamese adversarial nets significantly reduce the character error rate\n(CER). Specifically, we achieve a new state of art 4.37 CER without language\nmodel on the AISHELL-1 dataset, which leads to around 5% relative CER\nreduction. To reveal the generality of the siamese adversarial net, we also\nconduct experiments on the phoneme recognition task, which also shows the\nsuperiority of the siamese adversarial network.", "published": "2022-10-27 09:36:25", "link": "http://arxiv.org/abs/2210.15285v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Can language models handle recursively nested grammatical structures? A\n  case study on comparing models and humans", "abstract": "How should we compare the capabilities of language models (LMs) and humans? I\ndraw inspiration from comparative psychology to highlight some challenges. In\nparticular, I consider a case study: processing of recursively nested\ngrammatical structures. Prior work suggests that LMs cannot handle these\nstructures as reliably as humans can. However, the humans were provided with\ninstructions and training, while the LMs were evaluated zero-shot. I therefore\nmatch the evaluation more closely. Providing large LMs with a simple prompt --\nsubstantially less content than the human training -- allows the LMs to\nconsistently outperform the human results, and even to extrapolate to more\ndeeply nested conditions than were tested with humans. Further, reanalyzing the\nprior human data suggests that the humans may not perform above chance at the\ndifficult structures initially. Thus, large LMs may indeed process recursively\nnested grammatical structures as reliably as humans. This case study highlights\nhow discrepancies in the evaluation can confound comparisons of language models\nand humans. I therefore reflect on the broader challenge of comparing human and\nmodel capabilities, and highlight an important difference between evaluating\ncognitive models and foundation models.", "published": "2022-10-27 10:25:12", "link": "http://arxiv.org/abs/2210.15303v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FCTalker: Fine and Coarse Grained Context Modeling for Expressive\n  Conversational Speech Synthesis", "abstract": "Conversational Text-to-Speech (TTS) aims to synthesis an utterance with the\nright linguistic and affective prosody in a conversational context. The\ncorrelation between the current utterance and the dialogue history at the\nutterance level was used to improve the expressiveness of synthesized speech.\nHowever, the fine-grained information in the dialogue history at the word level\nalso has an important impact on the prosodic expression of an utterance, which\nhas not been well studied in the prior work. Therefore, we propose a novel\nexpressive conversational TTS model, termed as FCTalker, that learn the fine\nand coarse grained context dependency at the same time during speech\ngeneration. Specifically, the FCTalker includes fine and coarse grained\nencoders to exploit the word and utterance-level context dependency. To model\nthe word-level dependencies between an utterance and its dialogue history, the\nfine-grained dialogue encoder is built on top of a dialogue BERT model. The\nexperimental results show that the proposed method outperforms all baselines\nand generates more expressive speech that is contextually appropriate. We\nrelease the source code at: https://github.com/walker-hyf/FCTalker.", "published": "2022-10-27 12:20:20", "link": "http://arxiv.org/abs/2210.15360v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Opening the Black Box of wav2vec Feature Encoder", "abstract": "Self-supervised models, namely, wav2vec and its variants, have shown\npromising results in various downstream tasks in the speech domain. However,\ntheir inner workings are poorly understood, calling for in-depth analyses on\nwhat the model learns. In this paper, we concentrate on the convolutional\nfeature encoder where its latent space is often speculated to represent\ndiscrete acoustic units. To analyze the embedding space in a reductive manner,\nwe feed the synthesized audio signals, which is the summation of simple sine\nwaves. Through extensive experiments, we conclude that various information is\nembedded inside the feature encoder representations: (1) fundamental frequency,\n(2) formants, and (3) amplitude, packed with (4) sufficient temporal detail.\nFurther, the information incorporated inside the latent representations is\nanalogous to spectrograms but with a fundamental difference: latent\nrepresentations construct a metric space so that closer representations imply\nacoustic similarity.", "published": "2022-10-27 12:47:35", "link": "http://arxiv.org/abs/2210.15386v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automatic Severity Classification of Dysarthric speech by using\n  Self-supervised Model with Multi-task Learning", "abstract": "Automatic assessment of dysarthric speech is essential for sustained\ntreatments and rehabilitation. However, obtaining atypical speech is\nchallenging, often leading to data scarcity issues. To tackle the problem, we\npropose a novel automatic severity assessment method for dysarthric speech,\nusing the self-supervised model in conjunction with multi-task learning.\nWav2vec 2.0 XLS-R is jointly trained for two different tasks: severity\nclassification and auxiliary automatic speech recognition (ASR). For the\nbaseline experiments, we employ hand-crafted acoustic features and machine\nlearning classifiers such as SVM, MLP, and XGBoost. Explored on the Korean\ndysarthric speech QoLT database, our model outperforms the traditional baseline\nmethods, with a relative percentage increase of 1.25% for F1-score. In\naddition, the proposed model surpasses the model trained without ASR head,\nachieving 10.61% relative percentage improvements. Furthermore, we present how\nmulti-task learning affects the severity classification performance by\nanalyzing the latent representations and regularization effect.", "published": "2022-10-27 12:48:10", "link": "http://arxiv.org/abs/2210.15387v3", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Make More of Your Data: Minimal Effort Data Augmentation for Automatic\n  Speech Recognition and Translation", "abstract": "Data augmentation is a technique to generate new training data based on\nexisting data. We evaluate the simple and cost-effective method of\nconcatenating the original data examples to build new training instances.\nContinued training with such augmented data is able to improve off-the-shelf\nTransformer and Conformer models that were optimized on the original data only.\nWe demonstrate considerable improvements on the LibriSpeech-960h test sets (WER\n2.83 and 6.87 for test-clean and test-other), which carry over to models\ncombined with shallow fusion (WER 2.55 and 6.27). Our method of continued\ntraining also leads to improvements of up to 0.9 WER on the ASR part of\nCoVoST-2 for four non English languages, and we observe that the gains are\nhighly dependent on the size of the original training data. We compare\ndifferent concatenation strategies and found that our method does not need\nspeaker information to achieve its improvements. Finally, we demonstrate on two\ndatasets that our methods also works for speech translation tasks.", "published": "2022-10-27 13:01:01", "link": "http://arxiv.org/abs/2210.15398v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "What Language Model to Train if You Have One Million GPU Hours?", "abstract": "The crystallization of modeling methods around the Transformer architecture\nhas been a boon for practitioners. Simple, well-motivated architectural\nvariations can transfer across tasks and scale, increasing the impact of\nmodeling research. However, with the emergence of state-of-the-art 100B+\nparameters models, large language models are increasingly expensive to\naccurately design and train. Notably, it can be difficult to evaluate how\nmodeling decisions may impact emergent capabilities, given that these\ncapabilities arise mainly from sheer scale alone. In the process of building\nBLOOM--the Big Science Large Open-science Open-access Multilingual language\nmodel--our goal is to identify an architecture and training setup that makes\nthe best use of our 1,000,000 A100-GPU-hours budget. Specifically, we perform\nan ablation study at the billion-parameter scale comparing different modeling\npractices and their impact on zero-shot generalization. In addition, we study\nthe impact of various popular pre-training corpora on zero-shot generalization.\nWe also study the performance of a multilingual model and how it compares to\nthe English-only one. Finally, we consider the scaling behaviour of\nTransformers to choose the target model size, shape, and training setup. All\nour models and code are open-sourced at https://huggingface.co/bigscience .", "published": "2022-10-27 13:43:27", "link": "http://arxiv.org/abs/2210.15424v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Virtuoso: Massive Multilingual Speech-Text Joint Semi-Supervised\n  Learning for Text-To-Speech", "abstract": "This paper proposes Virtuoso, a massively multilingual speech-text joint\nsemi-supervised learning framework for text-to-speech synthesis (TTS) models.\nExisting multilingual TTS typically supports tens of languages, which are a\nsmall fraction of the thousands of languages in the world. One difficulty to\nscale multilingual TTS to hundreds of languages is collecting high-quality\nspeech-text paired data in low-resource languages. This study extends Maestro,\na speech-text joint pretraining framework for automatic speech recognition\n(ASR), to speech generation tasks. To train a TTS model from various types of\nspeech and text data, different training schemes are designed to handle\nsupervised (paired TTS and ASR data) and unsupervised (untranscribed speech and\nunspoken text) datasets. Experimental evaluation shows that 1) multilingual TTS\nmodels trained on Virtuoso can achieve significantly better naturalness and\nintelligibility than baseline ones in seen languages, and 2) they can\nsynthesize reasonably intelligible and naturally sounding speech for unseen\nlanguages where no high-quality paired TTS data is available.", "published": "2022-10-27 14:09:48", "link": "http://arxiv.org/abs/2210.15447v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving abstractive summarization with energy-based re-ranking", "abstract": "Current abstractive summarization systems present important weaknesses which\nprevent their deployment in real-world applications, such as the omission of\nrelevant information and the generation of factual inconsistencies (also known\nas hallucinations). At the same time, automatic evaluation metrics such as CTC\nscores have been recently proposed that exhibit a higher correlation with human\njudgments than traditional lexical-overlap metrics such as ROUGE. In this work,\nwe intend to close the loop by leveraging the recent advances in summarization\nmetrics to create quality-aware abstractive summarizers. Namely, we propose an\nenergy-based model that learns to re-rank summaries according to one or a\ncombination of these metrics. We experiment using several metrics to train our\nenergy-based re-ranker and show that it consistently improves the scores\nachieved by the predicted summaries. Nonetheless, human evaluation results show\nthat the re-ranking approach should be used with care for highly abstractive\nsummaries, as the available metrics are not yet sufficiently reliable for this\npurpose.", "published": "2022-10-27 15:43:36", "link": "http://arxiv.org/abs/2210.15553v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Control Diffusion: Efficiently Scaling through Space, Time, and\n  Tasks", "abstract": "Training generalist agents is difficult across several axes, requiring us to\ndeal with high-dimensional inputs (space), long horizons (time), and\ngeneralization to novel tasks. Recent advances with architectures have allowed\nfor improved scaling along one or two of these axes, but are still\ncomputationally prohibitive to use. In this paper, we propose to address all\nthree axes by leveraging \\textbf{L}anguage to \\textbf{C}ontrol\n\\textbf{D}iffusion models as a hierarchical planner conditioned on language\n(LCD). We effectively and efficiently scale diffusion models for planning in\nextended temporal, state, and task dimensions to tackle long horizon control\nproblems conditioned on natural language instructions, as a step towards\ngeneralist agents. Comparing LCD with other state-of-the-art models on the\nCALVIN language robotics benchmark finds that LCD outperforms other SOTA\nmethods in multi-task success rates, whilst improving inference speed over\nother comparable diffusion models by 3.3x~15x. We show that LCD can\nsuccessfully leverage the unique strength of diffusion models to produce\ncoherent long range plans while addressing their weakness in generating\nlow-level details and control.", "published": "2022-10-27 17:20:50", "link": "http://arxiv.org/abs/2210.15629v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Exploring Effective Distillation of Self-Supervised Speech Models for\n  Automatic Speech Recognition", "abstract": "Recent years have witnessed great strides in self-supervised learning (SSL)\non the speech processing. The SSL model is normally pre-trained on a great\nvariety of unlabelled data and a large model size is preferred to increase the\nmodeling capacity. However, this might limit its potential applications due to\nthe expensive computation and memory costs introduced by the oversize model.\nMiniaturization for SSL models has become an important research direction of\npractical value. To this end, we explore the effective distillation of\nHuBERT-based SSL models for automatic speech recognition (ASR). First, in order\nto establish a strong baseline, a comprehensive study on different student\nmodel structures is conducted. On top of this, as a supplement to the\nregression loss widely adopted in previous works, a discriminative loss is\nintroduced for HuBERT to enhance the distillation performance, especially in\nlow-resource scenarios. In addition, we design a simple and effective algorithm\nto distill the front-end input from waveform to Fbank feature, resulting in 17%\nparameter reduction and doubling inference speed, at marginal performance\ndegradation.", "published": "2022-10-27 17:21:14", "link": "http://arxiv.org/abs/2210.15631v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Simulating realistic speech overlaps improves multi-talker ASR", "abstract": "Multi-talker automatic speech recognition (ASR) has been studied to generate\ntranscriptions of natural conversation including overlapping speech of multiple\nspeakers. Due to the difficulty in acquiring real conversation data with\nhigh-quality human transcriptions, a na\\\"ive simulation of multi-talker speech\nby randomly mixing multiple utterances was conventionally used for model\ntraining. In this work, we propose an improved technique to simulate\nmulti-talker overlapping speech with realistic speech overlaps, where an\narbitrary pattern of speech overlaps is represented by a sequence of discrete\ntokens. With this representation, speech overlapping patterns can be learned\nfrom real conversations based on a statistical language model, such as N-gram,\nwhich can be then used to generate multi-talker speech for training. In our\nexperiments, multi-talker ASR models trained with the proposed method show\nconsistent improvement on the word error rates across multiple datasets.", "published": "2022-10-27 18:29:39", "link": "http://arxiv.org/abs/2210.15715v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Token-level Sequence Labeling for Spoken Language Understanding using\n  Compositional End-to-End Models", "abstract": "End-to-end spoken language understanding (SLU) systems are gaining popularity\nover cascaded approaches due to their simplicity and ability to avoid error\npropagation. However, these systems model sequence labeling as a sequence\nprediction task causing a divergence from its well-established token-level\ntagging formulation. We build compositional end-to-end SLU systems that\nexplicitly separate the added complexity of recognizing spoken mentions in SLU\nfrom the NLU task of sequence labeling. By relying on intermediate decoders\ntrained for ASR, our end-to-end systems transform the input modality from\nspeech to token-level representations that can be used in the traditional\nsequence labeling framework. This composition of ASR and NLU formulations in\nour end-to-end SLU system offers direct compatibility with pre-trained ASR and\nNLU systems, allows performance monitoring of individual components and enables\nthe use of globally normalized losses like CRF, making them attractive in\npractical scenarios. Our models outperform both cascaded and direct end-to-end\nmodels on a labeling task of named entity recognition across SLU benchmarks.", "published": "2022-10-27 19:33:18", "link": "http://arxiv.org/abs/2210.15734v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Self-supervised language learning from raw audio: Lessons from the Zero\n  Resource Speech Challenge", "abstract": "Recent progress in self-supervised or unsupervised machine learning has\nopened the possibility of building a full speech processing system from raw\naudio without using any textual representations or expert labels such as\nphonemes, dictionaries or parse trees. The contribution of the Zero Resource\nSpeech Challenge series since 2015 has been to break down this long-term\nobjective into four well-defined tasks -- Acoustic Unit Discovery, Spoken Term\nDiscovery, Discrete Resynthesis, and Spoken Language Modeling -- and introduce\nassociated metrics and benchmarks enabling model comparison and cumulative\nprogress. We present an overview of the six editions of this challenge series\nsince 2015, discuss the lessons learned, and outline the areas which need more\nwork or give puzzling results.", "published": "2022-10-27 20:32:41", "link": "http://arxiv.org/abs/2210.15759v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Evaluating context-invariance in unsupervised speech representations", "abstract": "Unsupervised speech representations have taken off, with benchmarks (SUPERB,\nZeroSpeech) demonstrating major progress on semi-supervised speech recognition,\nspeech synthesis, and speech-only language modelling. Inspiration comes from\nthe promise of ``discovering the phonemes'' of a language or a similar\nlow-bitrate encoding. However, one of the critical properties of phoneme\ntranscriptions is context-invariance: the phonetic context of a speech sound\ncan have massive influence on the way it is pronounced, while the text remains\nstable. This is what allows tokens of the same word to have the same\ntranscriptions -- key to language understanding. Current benchmarks do not\nmeasure context-invariance. We develop a new version of the ZeroSpeech ABX\nbenchmark that measures context-invariance, and apply it to recent\nself-supervised representations. We demonstrate that the context-independence\nof representations is predictive of the stability of word-level\nrepresentations. We suggest research concentrate on improving\ncontext-independence of self-supervised and unsupervised representations.", "published": "2022-10-27 21:15:49", "link": "http://arxiv.org/abs/2210.15775v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Compact End-to-End Model with Local and Global Context for Spoken\n  Language Identification", "abstract": "We introduce TitaNet-LID, a compact end-to-end neural network for Spoken\nLanguage Identification (LID) that is based on the ContextNet architecture.\nTitaNet-LID employs 1D depth-wise separable convolutions and\nSqueeze-and-Excitation layers to effectively capture local and global context\nwithin an utterance. Despite its small size, TitaNet-LID achieves performance\nsimilar to state-of-the-art models on the VoxLingua107 dataset while being 10\ntimes smaller. Furthermore, it can be easily adapted to new acoustic conditions\nand unseen languages through simple fine-tuning, achieving a state-of-the-art\naccuracy of 88.2% on the FLEURS benchmark. Our model is scalable and can\nachieve a better trade-off between accuracy and speed. TitaNet-LID performs\nwell even on short utterances less than 5s in length, indicating its robustness\nto input length.", "published": "2022-10-27 21:47:30", "link": "http://arxiv.org/abs/2210.15781v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fast DistilBERT on CPUs", "abstract": "Transformer-based language models have become the standard approach to\nsolving natural language processing tasks. However, industry adoption usually\nrequires the maximum throughput to comply with certain latency constraints that\nprevents Transformer models from being used in production. To address this gap,\nmodel compression techniques such as quantization and pruning may be used to\nimprove inference efficiency. However, these compression techniques require\nspecialized software to apply and deploy at scale. In this work, we propose a\nnew pipeline for creating and running Fast Transformer models on CPUs,\nutilizing hardware-aware pruning, knowledge distillation, quantization, and our\nown Transformer inference runtime engine with optimized kernels for sparse and\nquantized operators. We demonstrate the efficiency of our pipeline by creating\na Fast DistilBERT model showing minimal accuracy loss on the question-answering\nSQuADv1.1 benchmark, and throughput results under typical production\nconstraints and environments. Our results outperform existing state-of-the-art\nNeural Magic's DeepSparse runtime performance by up to 50% and up to 4.1x\nperformance speedup over ONNX Runtime. Source code is publicly available at\nhttps://github.com/intel/intel-extension-for-transformers.", "published": "2022-10-27 07:22:50", "link": "http://arxiv.org/abs/2211.07715v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Retrieving Users' Opinions on Social Media with Multimodal Aspect-Based\n  Sentiment Analysis", "abstract": "People post their opinions and experiences on social media, yielding rich\ndatabases of end-users' sentiments. This paper shows to what extent machine\nlearning can analyze and structure these databases. An automated data analysis\npipeline is deployed to provide insights into user-generated content for\nresearchers in other domains. First, the domain expert can select an image and\na term of interest. Then, the pipeline uses image retrieval to find all images\nshowing similar content and applies aspect-based sentiment analysis to outline\nusers' opinions about the selected term. As part of an interdisciplinary\nproject between architecture and computer science researchers, an empirical\nstudy of Hamburg's Elbphilharmonie was conveyed. Therefore, we selected 300\nthousand posts with the hashtag \\enquote{\\texttt{hamburg}} from the platform\nFlickr. Image retrieval methods generated a subset of slightly more than 1.5\nthousand images displaying the Elbphilharmonie. We found that these posts\nmainly convey a neutral or positive sentiment towards it. With this pipeline,\nwe suggest a new semantic computing method that offers novel insights into\nend-users opinions, e.g., for architecture domain experts.", "published": "2022-10-27 12:38:10", "link": "http://arxiv.org/abs/2210.15377v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Working Alliance Transformer for Psychotherapy Dialogue Classification", "abstract": "As a predictive measure of the treatment outcome in psychotherapy, the\nworking alliance measures the agreement of the patient and the therapist in\nterms of their bond, task and goal. Long been a clinical quantity estimated by\nthe patients' and therapists' self-evaluative reports, we believe that the\nworking alliance can be better characterized using natural language processing\ntechnique directly in the dialogue transcribed in each therapy session. In this\nwork, we propose the Working Alliance Transformer (WAT), a Transformer-based\nclassification model that has a psychological state encoder which infers the\nworking alliance scores by projecting the embedding of the dialogues turns onto\nthe embedding space of the clinical inventory for working alliance. We evaluate\nour method in a real-world dataset with over 950 therapy sessions with anxiety,\ndepression, schizophrenia and suicidal patients and demonstrate an empirical\nadvantage of using information about the therapeutic states in this sequence\nclassification task of psychotherapy dialogues.", "published": "2022-10-27 16:43:05", "link": "http://arxiv.org/abs/2210.15603v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "A Training and Inference Strategy Using Noisy and Enhanced Speech as\n  Target for Speech Enhancement without Clean Speech", "abstract": "The lack of clean speech is a practical challenge to the development of\nspeech enhancement systems, which means that there is an inevitable mismatch\nbetween their training criterion and evaluation metric. In response to this\nunfavorable situation, we propose a training and inference strategy that\nadditionally uses enhanced speech as a target by improving the previously\nproposed noisy-target training (NyTT). Because homogeneity between in-domain\nnoise and extraneous noise is the key to the effectiveness of NyTT, we train\nvarious student models by remixing 1) the teacher model's estimated speech and\nnoise for enhanced-target training or 2) raw noisy speech and the teacher\nmodel's estimated noise for noisy-target training. Experimental results show\nthat our proposed method outperforms several baselines, especially with the\nteacher/student inference, where predicted clean speech is derived successively\nthrough the teacher and final student models.", "published": "2022-10-27 12:26:24", "link": "http://arxiv.org/abs/2210.15368v3", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CasNet: Investigating Channel Robustness for Speech Separation", "abstract": "Recording channel mismatch between training and testing conditions has been\nshown to be a serious problem for speech separation. This situation greatly\nreduces the separation performance, and cannot meet the requirement of daily\nuse. In this study, inheriting the use of our previously constructed TAT-2mix\ncorpus, we address the channel mismatch problem by proposing a channel-aware\naudio separation network (CasNet), a deep learning framework for end-to-end\ntime-domain speech separation. CasNet is implemented on top of TasNet. Channel\nembedding (characterizing channel information in a mixture of multiple\nutterances) generated by Channel Encoder is introduced into the separation\nmodule by the FiLM technique. Through two training strategies, we explore two\nroles that channel embedding may play: 1) a real-life noise disturbance, making\nthe model more robust, or 2) a guide, instructing the separation model to\nretain the desired channel information. Experimental results on TAT-2mix show\nthat CasNet trained with both training strategies outperforms the TasNet\nbaseline, which does not use channel embeddings.", "published": "2022-10-27 12:28:45", "link": "http://arxiv.org/abs/2210.15370v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LyricJam Sonic: A Generative System for Real-Time Composition and\n  Musical Improvisation", "abstract": "Electronic music artists and sound designers have unique workflow practices\nthat necessitate specialized approaches for developing music information\nretrieval and creativity support tools. Furthermore, electronic music\ninstruments, such as modular synthesizers, have near-infinite possibilities for\nsound creation and can be combined to create unique and complex audio paths.\nThe process of discovering interesting sounds is often serendipitous and\nimpossible to replicate. For this reason, many musicians in electronic genres\nrecord audio output at all times while they work in the studio. Subsequently,\nit is difficult for artists to rediscover audio segments that might be suitable\nfor use in their compositions from thousands of hours of recordings. In this\npaper, we describe LyricJam Sonic -- a novel creative tool for musicians to\nrediscover their previous recordings, re-contextualize them with other\nrecordings, and create original live music compositions in real-time. A\nbi-modal AI-driven approach uses generated lyric lines to find matching audio\nclips from the artist's past studio recordings, and uses them to generate new\nlyric lines, which in turn are used to find other clips, thus creating a\ncontinuous and evolving stream of music and lyrics. The intent is to keep the\nartists in a state of creative flow conducive to music creation rather than\ntaking them into an analytical/critical state of deliberately searching for\npast audio segments. The system can run in either a fully autonomous mode\nwithout user input, or in a live performance mode, where the artist plays live\nmusic, while the system \"listens\" and creates a continuous stream of music and\nlyrics in response.", "published": "2022-10-27 17:27:58", "link": "http://arxiv.org/abs/2210.15638v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Signal Enhancement with Learning from Positive and Unlabelled Data", "abstract": "Supervised learning is a mainstream approach to audio signal enhancement (SE)\nand requires parallel training data consisting of both noisy signals and the\ncorresponding clean signals. Such data can only be synthesised and are\nmismatched with real data, which can result in poor performance on real data.\nMoreover, clean signals may be inaccessible in certain scenarios, which renders\nthis conventional approach infeasible. Here we explore SE using non-parallel\ntraining data consisting of noisy signals and noise, which can be easily\nrecorded. We define the positive (P) and the negative (N) classes as signal\ninactivity and activity, respectively. We observe that the spectrogram patches\nof noise clips can be used as P data and those of noisy signal clips as\nunlabelled data. Thus, learning from positive and unlabelled data enables a\nconvolutional neural network to learn to classify each spectrogram patch as P\nor N to enable SE.", "published": "2022-10-27 03:07:47", "link": "http://arxiv.org/abs/2210.15143v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Streaming Voice Conversion Via Intermediate Bottleneck Features And\n  Non-streaming Teacher Guidance", "abstract": "Streaming voice conversion (VC) is the task of converting the voice of one\nperson to another in real-time. Previous streaming VC methods use phonetic\nposteriorgrams (PPGs) extracted from automatic speech recognition (ASR) systems\nto represent speaker-independent information. However, PPGs lack the prosody\nand vocalization information of the source speaker, and streaming PPGs contain\nundesired leaked timbre of the source speaker. In this paper, we propose to use\nintermediate bottleneck features (IBFs) to replace PPGs. VC systems trained\nwith IBFs retain more prosody and vocalization information of the source\nspeaker. Furthermore, we propose a non-streaming teacher guidance (TG)\nframework that addresses the timbre leakage problem. Experiments show that our\nproposed IBFs and the TG framework achieve a state-of-the-art streaming VC\nnaturalness of 3.85, a content consistency of 3.77, and a timbre similarity of\n3.77 under a future receptive field of 160 ms which significantly outperform\nprevious streaming VC systems.", "published": "2022-10-27 03:53:21", "link": "http://arxiv.org/abs/2210.15158v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Solving Audio Inverse Problems with a Diffusion Model", "abstract": "This paper presents CQT-Diff, a data-driven generative audio model that can,\nonce trained, be used for solving various different audio inverse problems in a\nproblem-agnostic setting. CQT-Diff is a neural diffusion model with an\narchitecture that is carefully constructed to exploit pitch-equivariant\nsymmetries in music. This is achieved by preconditioning the model with an\ninvertible Constant-Q Transform (CQT), whose logarithmically-spaced frequency\naxis represents pitch equivariance as translation equivariance. The proposed\nmethod is evaluated with objective and subjective metrics in three different\nand varied tasks: audio bandwidth extension, inpainting, and declipping. The\nresults show that CQT-Diff outperforms the compared baselines and ablations in\naudio bandwidth extension and, without retraining, delivers competitive\nperformance against modern baselines in audio inpainting and declipping. This\nwork represents the first diffusion-based general framework for solving inverse\nproblems in audio processing.", "published": "2022-10-27 07:29:51", "link": "http://arxiv.org/abs/2210.15228v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning Music Representations with wav2vec 2.0", "abstract": "Learning music representations that are general-purpose offers the\nflexibility to finetune several downstream tasks using smaller datasets. The\nwav2vec 2.0 speech representation model showed promising results in many\ndownstream speech tasks, but has been less effective when adapted to music. In\nthis paper, we evaluate whether pre-training wav2vec 2.0 directly on music data\ncan be a better solution instead of finetuning the speech model. We illustrate\nthat when pre-training on music data, the discrete latent representations are\nable to encode the semantic meaning of musical concepts such as pitch and\ninstrument. Our results show that finetuning wav2vec 2.0 pre-trained on music\ndata allows us to achieve promising results on music classification tasks that\nare competitive with prior work on audio representations. In addition, the\nresults are superior to the pre-trained model on speech embeddings,\ndemonstrating that wav2vec 2.0 pre-trained on music data can be a promising\nmusic representation model.", "published": "2022-10-27 10:45:29", "link": "http://arxiv.org/abs/2210.15310v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Robust Data2vec: Noise-robust Speech Representation Learning for ASR by\n  Combining Regression and Improved Contrastive Learning", "abstract": "Self-supervised pre-training methods based on contrastive learning or\nregression tasks can utilize more unlabeled data to improve the performance of\nautomatic speech recognition (ASR). However, the robustness impact of combining\nthe two pre-training tasks and constructing different negative samples for\ncontrastive learning still remains unclear. In this paper, we propose a\nnoise-robust data2vec for self-supervised speech representation learning by\njointly optimizing the contrastive learning and regression tasks in the\npre-training stage. Furthermore, we present two improved methods to facilitate\ncontrastive learning. More specifically, we first propose to construct\npatch-based non-semantic negative samples to boost the noise robustness of the\npre-training model, which is achieved by dividing the features into patches at\ndifferent sizes (i.e., so-called negative samples). Second, by analyzing the\ndistribution of positive and negative samples, we propose to remove the easily\ndistinguishable negative samples to improve the discriminative capacity for\npre-training models. Experimental results on the CHiME-4 dataset show that our\nmethod is able to improve the performance of the pre-trained model in noisy\nscenarios. We find that joint training of the contrastive learning and\nregression tasks can avoid the model collapse to some extent compared to only\ntraining the regression task.", "published": "2022-10-27 11:04:02", "link": "http://arxiv.org/abs/2210.15324v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-class Detection of Pathological Speech with Latent Features: How\n  does it perform on unseen data?", "abstract": "The detection of pathologies from speech features is usually defined as a\nbinary classification task with one class representing a specific pathology and\nthe other class representing healthy speech. In this work, we train neural\nnetworks, large margin classifiers, and tree boosting machines to distinguish\nbetween four pathologies: Parkinson's disease, laryngeal cancer, cleft lip and\npalate, and oral squamous cell carcinoma. We show that latent representations\nextracted at different layers of a pre-trained wav2vec 2.0 system can be\neffectively used to classify these types of pathological voices. We evaluate\nthe robustness of our classifiers by adding room impulse responses to the test\ndata and by applying them to unseen speech corpora. Our approach achieves\nunweighted average F1-Scores between 74.1% and 97.0%, depending on the model\nand the noise conditions used. The systems generalize and perform well on\nunseen data of healthy speakers sampled from a variety of different sources.", "published": "2022-10-27 11:19:37", "link": "http://arxiv.org/abs/2210.15336v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-dimensional Edge-based Audio Event Relational Graph Representation\n  Learning for Acoustic Scene Classification", "abstract": "Most existing deep learning-based acoustic scene classification (ASC)\napproaches directly utilize representations extracted from spectrograms to\nidentify target scenes. However, these approaches pay little attention to the\naudio events occurring in the scene despite they provide crucial semantic\ninformation. This paper conducts the first study that investigates whether\nreal-life acoustic scenes can be reliably recognized based only on the features\nthat describe a limited number of audio events. To model the task-specific\nrelationships between coarse-grained acoustic scenes and fine-grained audio\nevents, we propose an event relational graph representation learning (ERGL)\nframework for ASC. Specifically, ERGL learns a graph representation of an\nacoustic scene from the input audio, where the embedding of each event is\ntreated as a node, while the relationship cues derived from each pair of event\nembeddings are described by a learned multidimensional edge feature.\nExperiments on a polyphonic acoustic scene dataset show that the proposed ERGL\nachieves competitive performance on ASC by using only a limited number of\nembeddings of audio events without any data augmentations. The validity of the\nproposed ERGL framework proves the feasibility of recognizing diverse acoustic\nscenes based on the event relational graph. Our code is available on our\nhomepage (https://github.com/Yuanbo2020/ERGL).", "published": "2022-10-27 12:25:06", "link": "http://arxiv.org/abs/2210.15366v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Time-Domain Based Embeddings for Spoofed Audio Representation", "abstract": "Anti-spoofing is the task of speech authentication. That is, identifying\ngenuine human speech compared to spoofed speech. The main focus of this paper\nis to suggest new representations for genuine and spoofed speech, based on the\nprobability mass function (PMF) estimation of the audio waveforms' amplitude.\n  We introduce a new feature extraction method for speech audio signals: unlike\ntraditional methods, our method is based on direct processing of time-domain\naudio samples. The PMF is utilized by designing a feature extractor based on\ndifferent PMF distances and similarity measures. As an additional step, we used\nfilter-bank preprocessing, which significantly affects the discriminative\ncharacteristics of the features and facilitates convenient visualization of\npossible clustering of spoofing attacks. Furthermore, we use diffusion maps to\nreveal the underlying manifold on which the data lies.\n  The suggested embeddings allow the use of simple linear separators to achieve\ndecent performance. In addition, we present a convenient way to visualize the\ndata, which helps to assess the efficiency of different spoofing techniques.\n  The experimental results show the potential of using multi-channel PMF based\nfeatures for the anti-spoofing task, in addition to the benefits of using\ndiffusion maps both as an analysis tool and as an embedding tool.", "published": "2022-10-27 13:45:43", "link": "http://arxiv.org/abs/2210.15428v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploiting spatial information with the informed complex-valued spatial\n  autoencoder for target speaker extraction", "abstract": "In conventional multichannel audio signal enhancement, spatial and spectral\nfiltering are often performed sequentially. In contrast, it has been shown that\nfor neural spatial filtering a joint approach of spectro-spatial filtering is\nmore beneficial. In this contribution, we investigate the spatial filtering\nperformed by such a time-varying spectro-spatial filter. We extend the recently\nproposed complex-valued spatial autoencoder (COSPA) for the task of target\nspeaker extraction by leveraging its interpretable structure and purposefully\ninforming the network of the target speaker's position. We show that the\nresulting informed COSPA (iCOSPA) effectively and flexibly extracts a target\nspeaker from a mixture of speakers. We also find that the proposed architecture\nis well capable of learning pronounced spatial selectivity patterns and show\nthat the results depend significantly on the training target and the reference\nsignal when computing various evaluation metrics.", "published": "2022-10-27 14:47:51", "link": "http://arxiv.org/abs/2210.15512v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Proceedings of the ACII Affective Vocal Bursts Workshop and Competition\n  2022 (A-VB): Understanding a critically understudied modality of emotional\n  expression", "abstract": "This is the Proceedings of the ACII Affective Vocal Bursts Workshop and\nCompetition (A-VB). A-VB was a workshop-based challenge that introduces the\nproblem of understanding emotional expression in vocal bursts -- a wide range\nof non-verbal vocalizations that includes laughs, grunts, gasps, and much more.\nWith affective states informing both mental and physical wellbeing, the core\nfocus of the A-VB workshop was the broader discussion of current strategies in\naffective computing for modeling vocal emotional expression. Within this first\niteration of the A-VB challenge, the participants were presented with four\nemotion-focused sub-challenges that utilize the large-scale and `in-the-wild'\nHume-VB dataset. The dataset and the four sub-challenges draw attention to new\ninnovations in emotion science as it pertains to vocal expression, addressing\nlow- and high-dimensional theories of emotional expression, cultural variation,\nand `call types' (laugh, cry, sigh, etc.).", "published": "2022-10-27 20:19:20", "link": "http://arxiv.org/abs/2210.15754v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "V-Cloak: Intelligibility-, Naturalness- & Timbre-Preserving Real-Time\n  Voice Anonymization", "abstract": "Voice data generated on instant messaging or social media applications\ncontains unique user voiceprints that may be abused by malicious adversaries\nfor identity inference or identity theft. Existing voice anonymization\ntechniques, e.g., signal processing and voice conversion/synthesis, suffer from\ndegradation of perceptual quality. In this paper, we develop a voice\nanonymization system, named V-Cloak, which attains real-time voice\nanonymization while preserving the intelligibility, naturalness and timbre of\nthe audio. Our designed anonymizer features a one-shot generative model that\nmodulates the features of the original audio at different frequency levels. We\ntrain the anonymizer with a carefully-designed loss function. Apart from the\nanonymity loss, we further incorporate the intelligibility loss and the\npsychoacoustics-based naturalness loss. The anonymizer can realize untargeted\nand targeted anonymization to achieve the anonymity goals of unidentifiability\nand unlinkability.\n  We have conducted extensive experiments on four datasets, i.e., LibriSpeech\n(English), AISHELL (Chinese), CommonVoice (French) and CommonVoice (Italian),\nfive Automatic Speaker Verification (ASV) systems (including two DNN-based, two\nstatistical and one commercial ASV), and eleven Automatic Speech Recognition\n(ASR) systems (for different languages). Experiment results confirm that\nV-Cloak outperforms five baselines in terms of anonymity performance. We also\ndemonstrate that V-Cloak trained only on the VoxCeleb1 dataset against\nECAPA-TDNN ASV and DeepSpeech2 ASR has transferable anonymity against other\nASVs and cross-language intelligibility for other ASRs. Furthermore, we verify\nthe robustness of V-Cloak against various de-noising techniques and adaptive\nattacks. Hopefully, V-Cloak may provide a cloak for us in a prism world.", "published": "2022-10-27 02:58:57", "link": "http://arxiv.org/abs/2210.15140v1", "categories": ["cs.SD", "cs.AI", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Masked Autoencoders Are Articulatory Learners", "abstract": "Articulatory recordings track the positions and motion of different\narticulators along the vocal tract and are widely used to study speech\nproduction and to develop speech technologies such as articulatory based speech\nsynthesizers and speech inversion systems. The University of Wisconsin X-Ray\nmicrobeam (XRMB) dataset is one of various datasets that provide articulatory\nrecordings synced with audio recordings. The XRMB articulatory recordings\nemploy pellets placed on a number of articulators which can be tracked by the\nmicrobeam. However, a significant portion of the articulatory recordings are\nmistracked, and have been so far unsuable. In this work, we present a deep\nlearning based approach using Masked Autoencoders to accurately reconstruct the\nmistracked articulatory recordings for 41 out of 47 speakers of the XRMB\ndataset. Our model is able to reconstruct articulatory trajectories that\nclosely match ground truth, even when three out of eight articulators are\nmistracked, and retrieve 3.28 out of 3.4 hours of previously unusable\nrecordings.", "published": "2022-10-27 06:03:47", "link": "http://arxiv.org/abs/2210.15195v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "HRTF Field: Unifying Measured HRTF Magnitude Representation with Neural\n  Fields", "abstract": "Head-related transfer functions (HRTFs) are a set of functions describing the\nspatial filtering effect of the outer ear (i.e., torso, head, and pinnae) onto\nsound sources at different azimuth and elevation angles. They are widely used\nin spatial audio rendering. While the azimuth and elevation angles are\nintrinsically continuous, measured HRTFs in existing datasets employ different\nspatial sampling schemes, making it difficult to model HRTFs across datasets.\nIn this work, we propose to use neural fields, a differentiable representation\nof functions through neural networks, to model HRTFs with arbitrary spatial\nsampling schemes. Such representation is unified across datasets with different\nspatial sampling schemes. HRTFs for arbitrary azimuth and elevation angles can\nbe derived from this representation. We further introduce a generative model\nnamed HRTF field to learn the latent space of the HRTF neural fields across\nsubjects. We demonstrate promising performance on HRTF interpolation and\ngeneration tasks and point out potential future work.", "published": "2022-10-27 06:05:20", "link": "http://arxiv.org/abs/2210.15196v3", "categories": ["eess.AS", "cs.GR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A knowledge-driven vowel-based approach of depression classification\n  from speech using data augmentation", "abstract": "We propose a novel explainable machine learning (ML) model that identifies\ndepression from speech, by modeling the temporal dependencies across utterances\nand utilizing the spectrotemporal information at the vowel level. Our method\nfirst models the variable-length utterances at the local-level into a\nfixed-size vowel-based embedding using a convolutional neural network with a\nspatial pyramid pooling layer (\"vowel CNN\"). Following that, the depression is\nclassified at the global-level from a group of vowel CNN embeddings that serve\nas the input of another 1D CNN (\"depression CNN\"). Different data augmentation\nmethods are designed for both the training of vowel CNN and depression CNN. We\ninvestigate the performance of the proposed system at various temporal\ngranularities when modeling short, medium, and long analysis windows,\ncorresponding to 10, 21, and 42 utterances, respectively. The proposed method\nreaches comparable performance with previous state-of-the-art approaches and\ndepicts explainable properties with respect to the depression outcome. The\nfindings from this work may benefit clinicians by providing additional\nintuitions during joint human-ML decision-making tasks.", "published": "2022-10-27 08:34:08", "link": "http://arxiv.org/abs/2210.15261v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Fast and Accurate Pitch Estimation Algorithm Based on the Pseudo\n  Wigner-Ville Distribution", "abstract": "Estimation of fundamental frequency (F0) in voiced segments of speech\nsignals, also known as pitch tracking, plays a crucial role in pitch\nsynchronous speech analysis, speech synthesis, and speech manipulation. In this\npaper, we capitalize on the high time and frequency resolution of the pseudo\nWigner-Ville distribution (PWVD) and propose a new PWVD-based pitch estimation\nmethod. We devise an efficient algorithm to compute PWVD faster and use\ncepstrum-based pre-filtering to avoid cross-term interference. Evaluating our\napproach on a database with speech and electroglottograph (EGG) recordings\nyields a state-of-the-art mean absolute error (MAE) of around 4Hz. Our approach\nis also effective at voiced/unvoiced classification and handling sudden\nfrequency changes.", "published": "2022-10-27 09:05:04", "link": "http://arxiv.org/abs/2210.15272v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "On Out-of-Distribution Detection for Audio with Deep Nearest Neighbors", "abstract": "Out-of-distribution (OOD) detection is concerned with identifying data points\nthat do not belong to the same distribution as the model's training data. For\nthe safe deployment of predictive models in a real-world environment, it is\ncritical to avoid making confident predictions on OOD inputs as it can lead to\npotentially dangerous consequences. However, OOD detection largely remains an\nunder-explored area in the audio (and speech) domain. This is despite the fact\nthat audio is a central modality for many tasks, such as speaker diarization,\nautomatic speech recognition, and sound event detection. To address this, we\npropose to leverage feature-space of the model with deep k-nearest neighbors to\ndetect OOD samples. We show that this simple and flexible method effectively\ndetects OOD inputs across a broad category of audio (and speech) datasets.\nSpecifically, it improves the false positive rate (FPR@TPR95) by 17% and the\nAUROC score by 7% than other prior techniques.", "published": "2022-10-27 09:35:33", "link": "http://arxiv.org/abs/2210.15283v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deformable Temporal Convolutional Networks for Monaural Noisy\n  Reverberant Speech Separation", "abstract": "Speech separation models are used for isolating individual speakers in many\nspeech processing applications. Deep learning models have been shown to lead to\nstate-of-the-art (SOTA) results on a number of speech separation benchmarks.\nOne such class of models known as temporal convolutional networks (TCNs) has\nshown promising results for speech separation tasks. A limitation of these\nmodels is that they have a fixed receptive field (RF). Recent research in\nspeech dereverberation has shown that the optimal RF of a TCN varies with the\nreverberation characteristics of the speech signal. In this work deformable\nconvolution is proposed as a solution to allow TCN models to have dynamic RFs\nthat can adapt to various reverberation times for reverberant speech\nseparation. The proposed models are capable of achieving an 11.1 dB average\nscale-invariant signalto-distortion ratio (SISDR) improvement over the input\nsignal on the WHAMR benchmark. A relatively small deformable TCN model of 1.3M\nparameters is proposed which gives comparable separation performance to larger\nand more computationally complex models.", "published": "2022-10-27 10:29:19", "link": "http://arxiv.org/abs/2210.15305v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Rigid-Body Sound Synthesis with Differentiable Modal Resonators", "abstract": "Physical models of rigid bodies are used for sound synthesis in applications\nfrom virtual environments to music production. Traditional methods such as\nmodal synthesis often rely on computationally expensive numerical solvers,\nwhile recent deep learning approaches are limited by post-processing of their\nresults. In this work we present a novel end-to-end framework for training a\ndeep neural network to generate modal resonators for a given 2D shape and\nmaterial, using a bank of differentiable IIR filters. We demonstrate our method\non a dataset of synthetic objects, but train our model using an audio-domain\nobjective, paving the way for physically-informed synthesisers to be learned\ndirectly from recordings of real-world objects.", "published": "2022-10-27 10:34:38", "link": "http://arxiv.org/abs/2210.15306v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Convolutive Block-Matching Segmentation Algorithm with Application to\n  Music Structure Analysis", "abstract": "Music Structure Analysis (MSA) consists of representing a song in sections\n(such as ``chorus'', ``verse'', ``solo'' etc), and can be seen as the retrieval\nof a simplified organization of the song. This work presents a new algorithm,\ncalled Convolutive Block-Matching (CBM) algorithm, devoted to MSA. In\nparticular, the CBM algorithm is a dynamic programming algorithm, applying on\nautosimilarity matrices, a standard tool in MSA. In this work, autosimilarity\nmatrices are computed from the feature representation of an audio signal, and\ntime is sampled on the barscale. We study three different similarity functions\nfor the computation of autosimilarity matrices. We report that the proposed\nalgorithm achieves a level of performance competitive to that of supervised\nState-of-the-Art methods on 3 among 4 metrics, while being unsupervised.", "published": "2022-10-27 12:11:01", "link": "http://arxiv.org/abs/2210.15356v3", "categories": ["cs.SD", "cs.IR", "eess.AS", "H.5.5"], "primary_category": "cs.SD"}
{"title": "Explicit Intensity Control for Accented Text-to-speech", "abstract": "Accented text-to-speech (TTS) synthesis seeks to generate speech with an\naccent (L2) as a variant of the standard version (L1). How to control the\nintensity of accent in the process of TTS is a very interesting research\ndirection, and has attracted more and more attention. Recent work design a\nspeaker-adversarial loss to disentangle the speaker and accent information, and\nthen adjust the loss weight to control the accent intensity. However, such a\ncontrol method lacks interpretability, and there is no direct correlation\nbetween the controlling factor and natural accent intensity. To this end, this\npaper propose a new intuitive and explicit accent intensity control scheme for\naccented TTS. Specifically, we first extract the posterior probability, called\nas ``goodness of pronunciation (GoP)'' from the L1 speech recognition model to\nquantify the phoneme accent intensity for accented speech, then design a\nFastSpeech2 based TTS model, named Ai-TTS, to take the accent intensity\nexpression into account during speech generation. Experiments show that the our\nmethod outperforms the baseline model in terms of accent rendering and\nintensity control.", "published": "2022-10-27 12:23:41", "link": "http://arxiv.org/abs/2210.15364v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-Supervised Training of Speaker Encoder with Multi-Modal Diverse\n  Positive Pairs", "abstract": "We study a novel neural architecture and its training strategies of speaker\nencoder for speaker recognition without using any identity labels. The speaker\nencoder is trained to extract a fixed-size speaker embedding from a spoken\nutterance of various length. Contrastive learning is a typical self-supervised\nlearning technique. However, the quality of the speaker encoder depends very\nmuch on the sampling strategy of positive and negative pairs. It is common that\nwe sample a positive pair of segments from the same utterance. Unfortunately,\nsuch poor-man's positive pairs (PPP) lack necessary diversity for the training\nof a robust encoder. In this work, we propose a multi-modal contrastive\nlearning technique with novel sampling strategies. By cross-referencing between\nspeech and face data, we study a method that finds diverse positive pairs (DPP)\nfor contrastive learning, thus improving the robustness of the speaker encoder.\nWe train the speaker encoder on the VoxCeleb2 dataset without any speaker\nlabels, and achieve an equal error rate (EER) of 2.89\\%, 3.17\\% and 6.27\\%\nunder the proposed progressive clustering strategy, and an EER of 1.44\\%,\n1.77\\% and 3.27\\% under the two-stage learning strategy with pseudo labels, on\nthe three test sets of VoxCeleb1. This novel solution outperforms the\nstate-of-the-art self-supervised learning methods by a large margin, at the\nsame time, achieves comparable results with the supervised learning\ncounterpart. We also evaluate our self-supervised learning technique on LRS2\nand LRW datasets, where the speaker information is unknown. All experiments\nsuggest that the proposed neural architecture and sampling strategies are\nrobust across datasets.", "published": "2022-10-27 12:47:06", "link": "http://arxiv.org/abs/2210.15385v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "FreeVC: Towards High-Quality Text-Free One-Shot Voice Conversion", "abstract": "Voice conversion (VC) can be achieved by first extracting source content\ninformation and target speaker information, and then reconstructing waveform\nwith these information. However, current approaches normally either extract\ndirty content information with speaker information leaked in, or demand a large\namount of annotated data for training. Besides, the quality of reconstructed\nwaveform can be degraded by the mismatch between conversion model and vocoder.\nIn this paper, we adopt the end-to-end framework of VITS for high-quality\nwaveform reconstruction, and propose strategies for clean content information\nextraction without text annotation. We disentangle content information by\nimposing an information bottleneck to WavLM features, and propose the\nspectrogram-resize based data augmentation to improve the purity of extracted\ncontent information. Experimental results show that the proposed method\noutperforms the latest VC models trained with annotated data and has greater\nrobustness.", "published": "2022-10-27 13:32:38", "link": "http://arxiv.org/abs/2210.15418v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Toroidal Probabilistic Spherical Discriminant Analysis", "abstract": "In speaker recognition, where speech segments are mapped to embeddings on the\nunit hypersphere, two scoring back-ends are commonly used, namely cosine\nscoring and PLDA. We have recently proposed PSDA, an analog to PLDA that uses\nVon Mises-Fisher distributions instead of Gaussians. In this paper, we present\ntoroidal PSDA (T-PSDA). It extends PSDA with the ability to model within and\nbetween-speaker variabilities in toroidal submanifolds of the hypersphere. Like\nPLDA and PSDA, the model allows closed-form scoring and closed-form EM updates\nfor training. On VoxCeleb, we find T-PSDA accuracy on par with cosine scoring,\nwhile PLDA accuracy is inferior. On NIST SRE'21 we find that T-PSDA gives large\naccuracy gains compared to both cosine scoring and PLDA.", "published": "2022-10-27 14:05:39", "link": "http://arxiv.org/abs/2210.15441v1", "categories": ["cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Source-Filter HiFi-GAN: Fast and Pitch Controllable High-Fidelity Neural\n  Vocoder", "abstract": "Our previous work, the unified source-filter GAN (uSFGAN) vocoder, introduced\na novel architecture based on the source-filter theory into the parallel\nwaveform generative adversarial network to achieve high voice quality and pitch\ncontrollability. However, the high temporal resolution inputs result in high\ncomputation costs. Although the HiFi-GAN vocoder achieves fast high-fidelity\nvoice generation thanks to the efficient upsampling-based generator\narchitecture, the pitch controllability is severely limited. To realize a fast\nand pitch-controllable high-fidelity neural vocoder, we introduce the\nsource-filter theory into HiFi-GAN by hierarchically conditioning the resonance\nfiltering network on a well-estimated source excitation information. According\nto the experimental results, our proposed method outperforms HiFi-GAN and\nuSFGAN on a singing voice generation in voice quality and synthesis speed on a\nsingle CPU. Furthermore, unlike the uSFGAN vocoder, the proposed method can be\neasily adopted/integrated in real-time applications and end-to-end systems.", "published": "2022-10-27 15:19:09", "link": "http://arxiv.org/abs/2210.15533v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multimodal Transformer Distillation for Audio-Visual Synchronization", "abstract": "Audio-visual synchronization aims to determine whether the mouth movements\nand speech in the video are synchronized. VocaLiST reaches state-of-the-art\nperformance by incorporating multimodal Transformers to model audio-visual\ninteract information. However, it requires high computing resources, making it\nimpractical for real-world applications. This paper proposed an MTDVocaLiST\nmodel, which is trained by our proposed multimodal Transformer distillation\n(MTD) loss. MTD loss enables MTDVocaLiST model to deeply mimic the\ncross-attention distribution and value-relation in the Transformer of VocaLiST.\nAdditionally, we harness uncertainty weighting to fully exploit the interaction\ninformation across all layers. Our proposed method is effective in two aspects:\nFrom the distillation method perspective, MTD loss outperforms other strong\ndistillation baselines. From the distilled model's performance perspective: 1)\nMTDVocaLiST outperforms similar-size SOTA models, SyncNet, and Perfect Match\nmodels by 15.65% and 3.35%; 2) MTDVocaLiST reduces the model size of VocaLiST\nby 83.52%, yet still maintaining similar performance.", "published": "2022-10-27 15:53:38", "link": "http://arxiv.org/abs/2210.15563v3", "categories": ["cs.CV", "cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "FedAudio: A Federated Learning Benchmark for Audio Tasks", "abstract": "Federated learning (FL) has gained substantial attention in recent years due\nto the data privacy concerns related to the pervasiveness of consumer devices\nthat continuously collect data from users. While a number of FL benchmarks have\nbeen developed to facilitate FL research, none of them include audio data and\naudio-related tasks. In this paper, we fill this critical gap by introducing a\nnew FL benchmark for audio tasks which we refer to as FedAudio. FedAudio\nincludes four representative and commonly used audio datasets from three\nimportant audio tasks that are well aligned with FL use cases. In particular, a\nunique contribution of FedAudio is the introduction of data noises and label\nerrors to the datasets to emulate challenges when deploying FL systems in\nreal-world settings. FedAudio also includes the benchmark results of the\ndatasets and a PyTorch library with the objective of facilitating researchers\nto fairly compare their algorithms. We hope FedAudio could act as a catalyst to\ninspire new FL research for audio tasks and thus benefit the acoustic and\nspeech research community. The datasets and benchmark results can be accessed\nat https://github.com/zhang-tuo-pdf/FedAudio.", "published": "2022-10-27 18:14:37", "link": "http://arxiv.org/abs/2210.15707v4", "categories": ["cs.SD", "cs.DC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "One-Shot Acoustic Matching Of Audio Signals -- Learning to Hear Music In\n  Any Room/ Concert Hall", "abstract": "The acoustic space in which a sound is created and heard plays an essential\nrole in how that sound is perceived by affording a unique sense of\n\\textit{presence}. Every sound we hear results from successive convolution\noperations intrinsic to the sound source and external factors such as\nmicrophone characteristics and room impulse responses. Typically, researchers\nuse an excitation such as a pistol shot or balloon pop as an impulse signal\nwith which an auralization can be created. The room \"impulse\" responses\nconvolved with the signal of interest can transform the input sound into the\nsound played in the acoustic space of interest. Here we propose a novel\narchitecture that can transform a sound of interest into any other acoustic\nspace(room or hall) of interest by using arbitrary audio recorded as a proxy\nfor a balloon pop. The architecture is grounded in simple signal processing\nideas to learn residual signals from a learned acoustic signature and the input\nsignal. Our framework allows a neural network to adjust gains of every point in\nthe time-frequency representation, giving sound qualitative and quantitative\nresults.", "published": "2022-10-27 19:54:05", "link": "http://arxiv.org/abs/2210.15750v2", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Conditioning and Sampling in Variational Diffusion Models for Speech\n  Super-Resolution", "abstract": "Recently, diffusion models (DMs) have been increasingly used in audio\nprocessing tasks, including speech super-resolution (SR), which aims to restore\nhigh-frequency content given low-resolution speech utterances. This is commonly\nachieved by conditioning the network of noise predictor with low-resolution\naudio. In this paper, we propose a novel sampling algorithm that communicates\nthe information of the low-resolution audio via the reverse sampling process of\nDMs. The proposed method can be a drop-in replacement for the vanilla sampling\nprocess and can significantly improve the performance of the existing works.\nMoreover, by coupling the proposed sampling method with an unconditional DM,\ni.e., a DM with no auxiliary inputs to its noise predictor, we can generalize\nit to a wide range of SR setups. We also attain state-of-the-art results on the\nVCTK Multi-Speaker benchmark with this novel formulation.", "published": "2022-10-27 22:31:20", "link": "http://arxiv.org/abs/2210.15793v3", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Deep Learning Object Detection Approaches to Signal Identification", "abstract": "Traditionally source identification is solved using threshold based energy\ndetection algorithms. These algorithms frequently sum up the activity in\nregions, and consider regions above a specific activity threshold to be\nsources. While these algorithms work for the majority of cases, they often fail\nto detect signals that occupy small frequency bands, fail to distinguish\nsources with overlapping frequency bands, and cannot detect any signals under a\nspecified signal to noise ratio. Through the conversion of raw signal data to\nspectrogram, source identification can be framed as an object detection\nproblem. By leveraging modern advancements in deep learning based object\ndetection, we propose a system that manages to alleviate the failure cases\nencountered when using traditional source identification algorithms. Our\ncontributions include framing source identification as an object detection\nproblem, the publication of a spectrogram object detection dataset, and\nevaluation of the RetinaNet and YOLOv5 object detection models trained on the\ndataset. Our final models achieve Mean Average Precisions of up to 0.906. With\nsuch a high Mean Average Precision, these models are sufficiently robust for\nuse in real world applications.", "published": "2022-10-27 02:08:46", "link": "http://arxiv.org/abs/2210.16173v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Pretraining Respiratory Sound Representations using Metadata and\n  Contrastive Learning", "abstract": "Methods based on supervised learning using annotations in an end-to-end\nfashion have been the state-of-the-art for classification problems. However,\nthey may be limited in their generalization capability, especially in the low\ndata regime. In this study, we address this issue using supervised contrastive\nlearning combined with available metadata to solve multiple pretext tasks that\nlearn a good representation of data. We apply our approach on respiratory sound\nclassification. This task is suited for this setting as demographic information\nsuch as sex and age are correlated with presence of lung diseases, and learning\na system that implicitly encode this information may better detect anomalies.\nSupervised contrastive learning is a paradigm that learns similar\nrepresentations to samples sharing the same class labels and dissimilar\nrepresentations to samples with different class labels. The feature extractor\nlearned using this paradigm extract useful features from the data, and we show\nthat it outperforms cross-entropy in classifying respiratory anomalies in two\ndifferent datasets. We also show that learning representations using only\nmetadata, without class labels, obtains similar performance as using cross\nentropy with those labels only. In addition, when combining class labels with\nmetadata using multiple supervised contrastive learning, an extension of\nsupervised contrastive learning solving an additional task of grouping patients\nwithin the same sex and age group, more informative features are learned. This\nwork suggests the potential of using multiple metadata sources in supervised\ncontrastive settings, in particular in settings with class imbalance and few\ndata. Our code is released at https://github.com/ilyassmoummad/scl_icbhi2017", "published": "2022-10-27 12:59:00", "link": "http://arxiv.org/abs/2210.16192v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Contextual-Utterance Training for Automatic Speech Recognition", "abstract": "Recent studies of streaming automatic speech recognition (ASR) recurrent\nneural network transducer (RNN-T)-based systems have fed the encoder with past\ncontextual information in order to improve its word error rate (WER)\nperformance. In this paper, we first propose a contextual-utterance training\ntechnique which makes use of the previous and future contextual utterances in\norder to do an implicit adaptation to the speaker, topic and acoustic\nenvironment. Also, we propose a dual-mode contextual-utterance training\ntechnique for streaming automatic speech recognition (ASR) systems. This\nproposed approach allows to make a better use of the available acoustic context\nin streaming models by distilling \"in-place\" the knowledge of a teacher, which\nis able to see both past and future contextual utterances, to the student which\ncan only see the current and past contextual utterances. The experimental\nresults show that a conformer-transducer system trained with the proposed\ntechniques outperforms the same system trained with the classical RNN-T loss.\nSpecifically, the proposed technique is able to reduce both the WER and the\naverage last token emission latency by more than 6% and 40ms relative,\nrespectively.", "published": "2022-10-27 08:10:44", "link": "http://arxiv.org/abs/2210.16238v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Synthesizer Preset Interpolation using Transformer Auto-Encoders", "abstract": "Sound synthesizers are widespread in modern music production but they\nincreasingly require expert skills to be mastered. This work focuses on\ninterpolation between presets, i.e., sets of values of all sound synthesis\nparameters, to enable the intuitive creation of new sounds from existing ones.\n  We introduce a bimodal auto-encoder neural network, which simultaneously\nprocesses presets using multi-head attention blocks, and audio using\nconvolutions. This model has been tested on a popular frequency modulation\nsynthesizer with more than one hundred parameters. Experiments have compared\nthe model to related architectures and methods, and have demonstrated that it\nperforms smoother interpolations. After training, the proposed model can be\nintegrated into commercial synthesizers for live interpolation or sound design\ntasks.", "published": "2022-10-27 15:20:18", "link": "http://arxiv.org/abs/2210.16984v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Diffiner: A Versatile Diffusion-based Generative Refiner for Speech\n  Enhancement", "abstract": "Although deep neural network (DNN)-based speech enhancement (SE) methods\noutperform the previous non-DNN-based ones, they often degrade the perceptual\nquality of generated outputs. To tackle this problem, we introduce a DNN-based\ngenerative refiner, Diffiner, aiming to improve perceptual speech quality\npre-processed by an SE method. We train a diffusion-based generative model by\nutilizing a dataset consisting of clean speech only. Then, our refiner\neffectively mixes clean parts newly generated via denoising diffusion\nrestoration into the degraded and distorted parts caused by a preceding SE\nmethod, resulting in refined speech. Once our refiner is trained on a set of\nclean speech, it can be applied to various SE methods without additional\ntraining specialized for each SE module. Therefore, our refiner can be a\nversatile post-processing module w.r.t. SE methods and has high potential in\nterms of modularity. Experimental results show that our method improved\nperceptual speech quality regardless of the preceding SE methods used.", "published": "2022-10-27 10:46:32", "link": "http://arxiv.org/abs/2210.17287v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Efficient Similarity-based Passive Filter Pruning for Compressing CNNs", "abstract": "Convolution neural networks (CNNs) have shown great success in various\napplications. However, the computational complexity and memory storage of CNNs\nis a bottleneck for their deployment on resource-constrained devices. Recent\nefforts towards reducing the computation cost and the memory overhead of CNNs\ninvolve similarity-based passive filter pruning methods. Similarity-based\npassive filter pruning methods compute a pairwise similarity matrix for the\nfilters and eliminate a few similar filters to obtain a small pruned CNN.\nHowever, the computational complexity of computing the pairwise similarity\nmatrix is high, particularly when a convolutional layer has many filters. To\nreduce the computational complexity in obtaining the pairwise similarity\nmatrix, we propose to use an efficient method where the complete pairwise\nsimilarity matrix is approximated from only a few of its columns by using a\nNystr\\\"om approximation method. The proposed efficient similarity-based passive\nfilter pruning method is 3 times faster and gives same accuracy at the same\nreduction in computations for CNNs compared to that of the similarity-based\npruning method that computes a complete pairwise similarity matrix. Apart from\nthis, the proposed efficient similarity-based pruning method performs similarly\nor better than the existing norm-based pruning methods. The efficacy of the\nproposed pruning method is evaluated on CNNs such as DCASE 2021 Task 1A\nbaseline network and a VGGish network designed for acoustic scene\nclassification.", "published": "2022-10-27 09:57:47", "link": "http://arxiv.org/abs/2210.17416v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
