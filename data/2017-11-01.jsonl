{"title": "Neural Wikipedian: Generating Textual Summaries from Knowledge Base\n  Triples", "abstract": "Most people do not interact with Semantic Web data directly. Unless they have\nthe expertise to understand the underlying technology, they need textual or\nvisual interfaces to help them make sense of it. We explore the problem of\ngenerating natural language summaries for Semantic Web data. This is\nnon-trivial, especially in an open-domain context. To address this problem, we\nexplore the use of neural networks. Our system encodes the information from a\nset of triples into a vector of fixed dimensionality and generates a textual\nsummary by conditioning the output on the encoded vector. We train and evaluate\nour models on two corpora of loosely aligned Wikipedia snippets and DBpedia and\nWikidata triples with promising results.", "published": "2017-11-01 01:08:49", "link": "http://arxiv.org/abs/1711.00155v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Keyword-based Query Comprehending via Multiple Optimized-Demand\n  Augmentation", "abstract": "In this paper, we consider the problem of machine reading task when the\nquestions are in the form of keywords, rather than natural language. In recent\nyears, researchers have achieved significant success on machine reading\ncomprehension tasks, such as SQuAD and TriviaQA. These datasets provide a\nnatural language question sentence and a pre-selected passage, and the goal is\nto answer the question according to the passage. However, in the situation of\ninteracting with machines by means of text, people are more likely to raise a\nquery in form of several keywords rather than a complete sentence. The\nkeyword-based query comprehension is a new challenge, because small variations\nto a question may completely change its semantical information, thus yield\ndifferent answers. In this paper, we propose a novel neural network system that\nconsists a Demand Optimization Model based on a passage-attention neural\nmachine translation and a Reader Model that can find the answer given the\noptimized question. The Demand Optimization Model optimizes the original query\nand output multiple reconstructed questions, then the Reader Model takes the\nnew questions as input and locate the answers from the passage. To make\npredictions robust, an evaluation mechanism will score the reconstructed\nquestions so the final answer strike a good balance between the quality of both\nthe Demand Optimization Model and the Reader Model. Experimental results on\nseveral datasets show that our framework significantly improves multiple strong\nbaselines on this challenging task.", "published": "2017-11-01 02:56:27", "link": "http://arxiv.org/abs/1711.00179v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Text Language Identification for the South African Languages", "abstract": "Virtual assistants and text chatbots have recently been gaining popularity.\nGiven the short message nature of text-based chat interactions, the language\nidentification systems of these bots might only have 15 or 20 characters to\nmake a prediction. However, accurate text language identification is important,\nespecially in the early stages of many multilingual natural language processing\npipelines.\n  This paper investigates the use of a naive Bayes classifier, to accurately\npredict the language family that a piece of text belongs to, combined with a\nlexicon based classifier to distinguish the specific South African language\nthat the text is written in. This approach leads to a 31% reduction in the\nlanguage detection error.\n  In the spirit of reproducible research the training and testing datasets as\nwell as the code are published on github. Hopefully it will be useful to create\na text language identification shared task for South African languages.", "published": "2017-11-01 08:27:46", "link": "http://arxiv.org/abs/1711.00247v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Paraphrase Generation with Deep Reinforcement Learning", "abstract": "Automatic generation of paraphrases from a given sentence is an important yet\nchallenging task in natural language processing (NLP), and plays a key role in\na number of applications such as question answering, search, and dialogue. In\nthis paper, we present a deep reinforcement learning approach to paraphrase\ngeneration. Specifically, we propose a new framework for the task, which\nconsists of a \\textit{generator} and an \\textit{evaluator}, both of which are\nlearned from data. The generator, built as a sequence-to-sequence learning\nmodel, can produce paraphrases given a sentence. The evaluator, constructed as\na deep matching model, can judge whether two sentences are paraphrases of each\nother. The generator is first trained by deep learning and then further\nfine-tuned by reinforcement learning in which the reward is given by the\nevaluator. For the learning of the evaluator, we propose two methods based on\nsupervised learning and inverse reinforcement learning respectively, depending\non the type of available training data. Empirical study shows that the learned\nevaluator can guide the generator to produce more accurate paraphrases.\nExperimental results demonstrate the proposed models (the generators)\noutperform the state-of-the-art methods in paraphrase generation in both\nautomatic evaluation and human evaluation.", "published": "2017-11-01 10:40:42", "link": "http://arxiv.org/abs/1711.00279v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Automatic Generation of Entertaining Dialogues in Chinese\n  Crosstalks", "abstract": "Crosstalk, also known by its Chinese name xiangsheng, is a traditional\nChinese comedic performing art featuring jokes and funny dialogues, and one of\nChina's most popular cultural elements. It is typically in the form of a\ndialogue between two performers for the purpose of bringing laughter to the\naudience, with one person acting as the leading comedian and the other as the\nsupporting role. Though general dialogue generation has been widely explored in\nprevious studies, it is unknown whether such entertaining dialogues can be\nautomatically generated or not. In this paper, we for the first time\ninvestigate the possibility of automatic generation of entertaining dialogues\nin Chinese crosstalks. Given the utterance of the leading comedian in each\ndialogue, our task aims to generate the replying utterance of the supporting\nrole. We propose a humor-enhanced translation model to address this task and\nhuman evaluation results demonstrate the efficacy of our proposed model. The\nfeasibility of automatic entertaining dialogue generation is also verified.", "published": "2017-11-01 11:29:27", "link": "http://arxiv.org/abs/1711.00294v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Neural Machine Translation through Phrase-based Forced\n  Decoding", "abstract": "Compared to traditional statistical machine translation (SMT), neural machine\ntranslation (NMT) often sacrifices adequacy for the sake of fluency. We propose\na method to combine the advantages of traditional SMT and NMT by exploiting an\nexisting phrase-based SMT model to compute the phrase-based decoding cost for\nan NMT output and then using this cost to rerank the n-best NMT outputs. The\nmain challenge in implementing this approach is that NMT outputs may not be in\nthe search space of the standard phrase-based decoding algorithm, because the\nsearch space of phrase-based SMT is limited by the phrase-based translation\nrule table. We propose a soft forced decoding algorithm, which can always\nsuccessfully find a decoding path for any NMT output. We show that using the\nforced decoding cost to rerank the NMT outputs can successfully improve\ntranslation quality on four different language pairs.", "published": "2017-11-01 12:25:20", "link": "http://arxiv.org/abs/1711.00309v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Structure and Interpretability of Word Embeddings", "abstract": "Dense word embeddings, which encode semantic meanings of words to low\ndimensional vector spaces have become very popular in natural language\nprocessing (NLP) research due to their state-of-the-art performances in many\nNLP tasks. Word embeddings are substantially successful in capturing semantic\nrelations among words, so a meaningful semantic structure must be present in\nthe respective vector spaces. However, in many cases, this semantic structure\nis broadly and heterogeneously distributed across the embedding dimensions,\nwhich makes interpretation a big challenge. In this study, we propose a\nstatistical method to uncover the latent semantic structure in the dense word\nembeddings. To perform our analysis we introduce a new dataset (SEMCAT) that\ncontains more than 6500 words semantically grouped under 110 categories. We\nfurther propose a method to quantify the interpretability of the word\nembeddings; the proposed method is a practical alternative to the classical\nword intrusion test that requires human intervention.", "published": "2017-11-01 13:22:02", "link": "http://arxiv.org/abs/1711.00331v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Discourse Phenomena in Neural Machine Translation", "abstract": "For machine translation to tackle discourse phenomena, models must have\naccess to extra-sentential linguistic context. There has been recent interest\nin modelling context in neural machine translation (NMT), but models have been\nprincipally evaluated with standard automatic metrics, poorly adapted to\nevaluating discourse phenomena. In this article, we present hand-crafted,\ndiscourse test sets, designed to test the models' ability to exploit previous\nsource and target sentences. We investigate the performance of recently\nproposed multi-encoder NMT models trained on subtitles for English to French.\nWe also explore a novel way of exploiting context from the previous sentence.\nDespite gains using BLEU, multi-encoder models give limited improvement in the\nhandling of discourse phenomena: 50% accuracy on our coreference test set and\n53.5% for coherence/cohesion (compared to a non-contextual baseline of 50%). A\nsimple strategy of decoding the concatenation of the previous and current\nsentence leads to good performance, and our novel strategy of multi-encoding\nand decoding of two sentences leads to the best performance (72.5% for\ncoreference and 57% for coherence/cohesion), highlighting the importance of\ntarget-side context.", "published": "2017-11-01 19:00:22", "link": "http://arxiv.org/abs/1711.00513v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Annotation Graphs: Annotating Complex Natural Language Phenomena", "abstract": "This paper introduces a new web-based software tool for annotating text, Text\nAnnotation Graphs, or TAG. It provides functionality for representing complex\nrelationships between words and word phrases that are not available in other\nsoftware tools, including the ability to define and visualize relationships\nbetween the relationships themselves (semantic hypergraphs). Additionally, we\ninclude an approach to representing text annotations in which annotation\nsubgraphs, or semantic summaries, are used to show relationships outside of the\nsequential context of the text itself. Users can use these subgraphs to quickly\nfind similar structures within the current document or external annotated\ndocuments. Initially, TAG was developed to support information extraction tasks\non a large database of biomedical articles. However, our software is flexible\nenough to support a wide range of annotation tasks for any domain. Examples are\nprovided that showcase TAG's capabilities on morphological parsing and event\nextraction tasks. The TAG software is available at: https://github.com/\nCreativeCodingLab/TextAnnotationGraphs.", "published": "2017-11-01 20:24:39", "link": "http://arxiv.org/abs/1711.00529v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning with Latent Language", "abstract": "The named concepts and compositional operators present in natural language\nprovide a rich source of information about the kinds of abstractions humans use\nto navigate the world. Can this linguistic background knowledge improve the\ngenerality and efficiency of learned classifiers and control policies? This\npaper aims to show that using the space of natural language strings as a\nparameter space is an effective way to capture natural task structure. In a\npretraining phase, we learn a language interpretation model that transforms\ninputs (e.g. images) into outputs (e.g. labels) given natural language\ndescriptions. To learn a new concept (e.g. a classifier), we search directly in\nthe space of descriptions to minimize the interpreter's loss on training\nexamples. Crucially, our models do not require language data to learn these\nconcepts: language is used only in pretraining to impose structure on\nsubsequent learning. Results on image classification, text editing, and\nreinforcement learning show that, in all settings, models with a linguistic\nparameterization outperform those without.", "published": "2017-11-01 18:00:22", "link": "http://arxiv.org/abs/1711.00482v1", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Uncovering Latent Style Factors for Expressive Speech Synthesis", "abstract": "Prosodic modeling is a core problem in speech synthesis. The key challenge is\nproducing desirable prosody from textual input containing only phonetic\ninformation. In this preliminary study, we introduce the concept of \"style\ntokens\" in Tacotron, a recently proposed end-to-end neural speech synthesis\nmodel. Using style tokens, we aim to extract independent prosodic styles from\ntraining data. We show that without annotation data or an explicit supervision\nsignal, our approach can automatically learn a variety of prosodic variations\nin a purely data-driven way. Importantly, each style token corresponds to a\nfixed style factor regardless of the given text sequence. As a result, we can\ncontrol the prosodic style of synthetic speech in a somewhat predictable and\nglobally consistent way.", "published": "2017-11-01 19:40:00", "link": "http://arxiv.org/abs/1711.00520v1", "categories": ["cs.CL", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Avoiding Your Teacher's Mistakes: Training Neural Networks with\n  Controlled Weak Supervision", "abstract": "Training deep neural networks requires massive amounts of training data, but\nfor many tasks only limited labeled data is available. This makes weak\nsupervision attractive, using weak or noisy signals like the output of\nheuristic methods or user click-through data for training. In a semi-supervised\nsetting, we can use a large set of data with weak labels to pretrain a neural\nnetwork and then fine-tune the parameters with a small amount of data with true\nlabels. This feels intuitively sub-optimal as these two independent stages\nleave the model unaware about the varying label quality. What if we could\nsomehow inform the model about the label quality? In this paper, we propose a\nsemi-supervised learning method where we train two neural networks in a\nmulti-task fashion: a \"target network\" and a \"confidence network\". The target\nnetwork is optimized to perform a given task and is trained using a large set\nof unlabeled data that are weakly annotated. We propose to weight the gradient\nupdates to the target network using the scores provided by the second\nconfidence network, which is trained on a small amount of supervised data. Thus\nwe avoid that the weight updates computed from noisy labels harm the quality of\nthe target network model. We evaluate our learning strategy on two different\ntasks: document ranking and sentiment classification. The results demonstrate\nthat our approach not only enhances the performance compared to the baselines\nbut also speeds up the learning process from weak labels.", "published": "2017-11-01 12:38:59", "link": "http://arxiv.org/abs/1711.00313v2", "categories": ["cs.LG", "cs.CL", "cs.NE", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Just ASK: Building an Architecture for Extensible Self-Service Spoken\n  Language Understanding", "abstract": "This paper presents the design of the machine learning architecture that\nunderlies the Alexa Skills Kit (ASK) a large scale Spoken Language\nUnderstanding (SLU) Software Development Kit (SDK) that enables developers to\nextend the capabilities of Amazon's virtual assistant, Alexa. At Amazon, the\ninfrastructure powers over 25,000 skills deployed through the ASK, as well as\nAWS's Amazon Lex SLU Service. The ASK emphasizes flexibility, predictability\nand a rapid iteration cycle for third party developers. It imposes inductive\nbiases that allow it to learn robust SLU models from extremely small and sparse\ndatasets and, in doing so, removes significant barriers to entry for software\ndevelopers and dialogue systems researchers.", "published": "2017-11-01 22:10:11", "link": "http://arxiv.org/abs/1711.00549v4", "categories": ["cs.CL", "cs.AI", "cs.NE", "cs.SE", "68T50"], "primary_category": "cs.CL"}
{"title": "Reducing Model Complexity for DNN Based Large-Scale Audio Classification", "abstract": "Audio classification is the task of identifying the sound categories that are\nassociated with a given audio signal. This paper presents an investigation on\nlarge-scale audio classification based on the recently released AudioSet\ndatabase. AudioSet comprises 2 millions of audio samples from YouTube, which\nare human-annotated with 527 sound category labels. Audio classification\nexperiments with the balanced training set and the evaluation set of AudioSet\nare carried out by applying different types of neural network models. The\nclassification performance and the model complexity of these models are\ncompared and analyzed. While the CNN models show better performance than MLP\nand RNN, its model complexity is relatively high and undesirable for practical\nuse. We propose two different strategies that aim at constructing\nlow-dimensional embedding feature extractors and hence reducing the number of\nmodel parameters. It is shown that the simplified CNN model has only 1/22 model\nparameters of the original model, with only a slight degradation of\nperformance.", "published": "2017-11-01 07:19:07", "link": "http://arxiv.org/abs/1711.00229v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Shift-Invariant Kernel Additive Modelling for Audio Source Separation", "abstract": "A major goal in blind source separation to identify and separate sources is\nto model their inherent characteristics. While most state-of-the-art approaches\nare supervised methods trained on large datasets, interest in non-data-driven\napproaches such as Kernel Additive Modelling (KAM) remains high due to their\ninterpretability and adaptability. KAM performs the separation of a given\nsource applying robust statistics on the time-frequency bins selected by a\nsource-specific kernel function, commonly the K-NN function. This choice\nassumes that the source of interest repeats in both time and frequency. In\npractice, this assumption does not always hold. Therefore, we introduce a\nshift-invariant kernel function capable of identifying similar spectral content\neven under frequency shifts. This way, we can considerably increase the amount\nof suitable sound material available to the robust statistics. While this leads\nto an increase in separation performance, a basic formulation, however, is\ncomputationally expensive. Therefore, we additionally present acceleration\ntechniques that lower the overall computational complexity.", "published": "2017-11-01 13:59:23", "link": "http://arxiv.org/abs/1711.00351v2", "categories": ["cs.SD", "eess.AS", "H.5.5; I.5.1; I.5.4"], "primary_category": "cs.SD"}
{"title": "TasNet: time-domain audio separation network for real-time,\n  single-channel speech separation", "abstract": "Robust speech processing in multi-talker environments requires effective\nspeech separation. Recent deep learning systems have made significant progress\ntoward solving this problem, yet it remains challenging particularly in\nreal-time, short latency applications. Most methods attempt to construct a mask\nfor each source in time-frequency representation of the mixture signal which is\nnot necessarily an optimal representation for speech separation. In addition,\ntime-frequency decomposition results in inherent problems such as\nphase/magnitude decoupling and long time window which is required to achieve\nsufficient frequency resolution. We propose Time-domain Audio Separation\nNetwork (TasNet) to overcome these limitations. We directly model the signal in\nthe time-domain using an encoder-decoder framework and perform the source\nseparation on nonnegative encoder outputs. This method removes the frequency\ndecomposition step and reduces the separation problem to estimation of source\nmasks on encoder outputs which is then synthesized by the decoder. Our system\noutperforms the current state-of-the-art causal and noncausal speech separation\nalgorithms, reduces the computational cost of speech separation, and\nsignificantly reduces the minimum required latency of the output. This makes\nTasNet suitable for applications where low-power, real-time implementation is\ndesirable such as in hearable and telecommunication devices.", "published": "2017-11-01 21:19:22", "link": "http://arxiv.org/abs/1711.00541v2", "categories": ["cs.SD", "cs.LG", "cs.MM", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
