{"title": "Vision-Dialog Navigation by Exploring Cross-modal Memory", "abstract": "Vision-dialog navigation posed as a new holy-grail task in vision-language\ndisciplinary targets at learning an agent endowed with the capability of\nconstant conversation for help with natural language and navigating according\nto human responses. Besides the common challenges faced in visual language\nnavigation, vision-dialog navigation also requires to handle well with the\nlanguage intentions of a series of questions about the temporal context from\ndialogue history and co-reasoning both dialogs and visual scenes. In this\npaper, we propose the Cross-modal Memory Network (CMN) for remembering and\nunderstanding the rich information relevant to historical navigation actions.\nOur CMN consists of two memory modules, the language memory module (L-mem) and\nthe visual memory module (V-mem). Specifically, L-mem learns latent\nrelationships between the current language interaction and a dialog history by\nemploying a multi-head attention mechanism. V-mem learns to associate the\ncurrent visual views and the cross-modal memory about the previous navigation\nactions. The cross-modal memory is generated via a vision-to-language attention\nand a language-to-vision attention. Benefiting from the collaborative learning\nof the L-mem and the V-mem, our CMN is able to explore the memory about the\ndecision making of historical navigation actions which is for the current step.\nExperiments on the CVDN dataset show that our CMN outperforms the previous\nstate-of-the-art model by a significant margin on both seen and unseen\nenvironments.", "published": "2020-03-15 03:08:06", "link": "http://arxiv.org/abs/2003.06745v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Leveraging Foreign Language Labeled Data for Aspect-Based Opinion Mining", "abstract": "Aspect-based opinion mining is the task of identifying sentiment at the\naspect level in opinionated text, which consists of two subtasks: aspect\ncategory extraction and sentiment polarity classification. While aspect\ncategory extraction aims to detect and categorize opinion targets such as\nproduct features, sentiment polarity classification assigns a sentiment label,\ni.e. positive, negative, or neutral, to each identified aspect. Supervised\nlearning methods have been shown to deliver better accuracy for this task but\nthey require labeled data, which is costly to obtain, especially for\nresource-poor languages like Vietnamese. To address this problem, we present a\nsupervised aspect-based opinion mining method that utilizes labeled data from a\nforeign language (English in this case), which is translated to Vietnamese by\nan automated translation tool (Google Translate). Because aspects and opinions\nin different languages may be expressed by different words, we propose using\nword embeddings, in addition to other features, to reduce the vocabulary\ndifference between the original and translated texts, thus improving the\neffectiveness of aspect category extraction and sentiment polarity\nclassification processes. We also introduce an annotated corpus of aspect\ncategories and sentiment polarities extracted from restaurant reviews in\nVietnamese, and conduct a series of experiments on the corpus. Experimental\nresults demonstrate the effectiveness of the proposed approach.", "published": "2020-03-15 15:53:53", "link": "http://arxiv.org/abs/2003.06858v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Exploring Gaussian mixture model framework for speaker adaptation of\n  deep neural network acoustic models", "abstract": "In this paper we investigate the GMM-derived (GMMD) features for adaptation\nof deep neural network (DNN) acoustic models. The adaptation of the DNN trained\non GMMD features is done through the maximum a posteriori (MAP) adaptation of\nthe auxiliary GMM model used for GMMD feature extraction. We explore fusion of\nthe adapted GMMD features with conventional features, such as bottleneck and\nMFCC features, in two different neural network architectures: DNN and\ntime-delay neural network (TDNN). We analyze and compare different types of\nadaptation techniques such as i-vectors and feature-space adaptation techniques\nbased on maximum likelihood linear regression (fMLLR) with the proposed\nadaptation approach, and explore their complementarity using various types of\nfusion such as feature level, posterior level, lattice level and others in\norder to discover the best possible way of combination. Experimental results on\nthe TED-LIUM corpus show that the proposed adaptation technique can be\neffectively integrated into DNN and TDNN setups at different levels and provide\nadditional gain in recognition performance: up to 6% of relative word error\nrate reduction (WERR) over the strong feature-space adaptation techniques based\non maximum likelihood linear regression (fMLLR) speaker adapted DNN baseline,\nand up to 18% of relative WERR in comparison with a speaker independent (SI)\nDNN baseline model, trained on conventional features. For TDNN models the\nproposed approach achieves up to 26% of relative WERR in comparison with a SI\nbaseline, and up 13% in comparison with the model adapted by using i-vectors.\nThe analysis of the adapted GMMD features from various points of view\ndemonstrates their effectiveness at different levels.", "published": "2020-03-15 18:56:19", "link": "http://arxiv.org/abs/2003.06894v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A proto-object based audiovisual saliency map", "abstract": "Natural environment and our interaction with it is essentially multisensory,\nwhere we may deploy visual, tactile and/or auditory senses to perceive, learn\nand interact with our environment. Our objective in this study is to develop a\nscene analysis algorithm using multisensory information, specifically vision\nand audio. We develop a proto-object based audiovisual saliency map (AVSM) for\nthe analysis of dynamic natural scenes. A specialized audiovisual camera with\n$360 \\degree$ Field of View, capable of locating sound direction, is used to\ncollect spatiotemporally aligned audiovisual data. We demonstrate that the\nperformance of proto-object based audiovisual saliency map in detecting and\nlocalizing salient objects/events is in agreement with human judgment. In\naddition, the proto-object based AVSM that we compute as a linear combination\nof visual and auditory feature conspicuity maps captures a higher number of\nvalid salient events compared to unisensory saliency maps. Such an algorithm\ncan be useful in surveillance, robotic navigation, video compression and\nrelated applications.", "published": "2020-03-15 08:34:35", "link": "http://arxiv.org/abs/2003.06779v1", "categories": ["eess.AS", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
