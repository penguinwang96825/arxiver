{"title": "Integrated Node Encoder for Labelled Textual Networks", "abstract": "Voluminous works have been implemented to exploit content-enhanced network\nembedding models, with little focus on the labelled information of nodes.\nAlthough TriDNR leverages node labels by treating them as node attributes, it\nfails to enrich unlabelled node vectors with the labelled information, which\nleads to the weaker classification result on the test set in comparison to\nexisting unsupervised textual network embedding models. In this study, we\ndesign an integrated node encoder (INE) for textual networks which is jointly\ntrained on the structure-based and label-based objectives. As a result, the\nnode encoder preserves the integrated knowledge of not only the network text\nand structure, but also the labelled information. Furthermore, INE allows the\ncreation of label-enhanced vectors for unlabelled nodes by entering their node\ncontents. Our node embedding achieves state-of-the-art performances in the\nclassification task on two public citation networks, namely Cora and DBLP,\npushing benchmarks up by 10.0\\% and 12.1\\%, respectively, with the 70\\%\ntraining ratio. Additionally, a feasible solution that generalizes our model\nfrom textual networks to a broader range of networks is proposed.", "published": "2020-05-24 09:20:34", "link": "http://arxiv.org/abs/2005.11694v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When does MAML Work the Best? An Empirical Study on Model-Agnostic\n  Meta-Learning in NLP Applications", "abstract": "Model-Agnostic Meta-Learning (MAML), a model-agnostic meta-learning method,\nis successfully employed in NLP applications including few-shot text\nclassification and multi-domain low-resource language generation. Many\nimpacting factors, including data quantity, similarity among tasks, and the\nbalance between general language model and task-specific adaptation, can affect\nthe performance of MAML in NLP, but few works have thoroughly studied them. In\nthis paper, we conduct an empirical study to investigate these impacting\nfactors and conclude when MAML works the best based on the experimental\nresults.", "published": "2020-05-24 09:29:36", "link": "http://arxiv.org/abs/2005.11700v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel Distributed Representation of News (DRNews) for Stock Market\n  Predictions", "abstract": "In this study, a novel Distributed Representation of News (DRNews) model is\ndeveloped and applied in deep learning-based stock market predictions. With the\nmerit of integrating contextual information and cross-documental knowledge, the\nDRNews model creates news vectors that describe both the semantic information\nand potential linkages among news events through an attributed news network.\nTwo stock market prediction tasks, namely the short-term stock movement\nprediction and stock crises early warning, are implemented in the framework of\nthe attention-based Long Short Term-Memory (LSTM) network. It is suggested that\nDRNews substantially enhances the results of both tasks comparing with five\nbaselines of news embedding models. Further, the attention mechanism suggests\nthat short-term stock trend and stock market crises both receive influences\nfrom daily news with the former demonstrates more critical responses on the\ninformation related to the stock market {\\em per se}, whilst the latter draws\nmore concerns on the banking sector and economic policies.", "published": "2020-05-24 10:01:27", "link": "http://arxiv.org/abs/2005.11706v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KaLM at SemEval-2020 Task 4: Knowledge-aware Language Models for\n  Comprehension And Generation", "abstract": "This paper presents our strategies in SemEval 2020 Task 4: Commonsense\nValidation and Explanation. We propose a novel way to search for evidence and\nchoose the different large-scale pre-trained models as the backbone for three\nsubtasks. The results show that our evidence-searching approach improves model\nperformance on commonsense explanation task. Our team ranks 2nd in subtask C\naccording to human evaluation score.", "published": "2020-05-24 15:09:21", "link": "http://arxiv.org/abs/2005.11768v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Common Sense or World Knowledge? Investigating Adapter-Based Knowledge\n  Injection into Pretrained Transformers", "abstract": "Following the major success of neural language models (LMs) such as BERT or\nGPT-2 on a variety of language understanding tasks, recent work focused on\ninjecting (structured) knowledge from external resources into these models.\nWhile on the one hand, joint pretraining (i.e., training from scratch, adding\nobjectives based on external knowledge to the primary LM objective) may be\nprohibitively computationally expensive, post-hoc fine-tuning on external\nknowledge, on the other hand, may lead to the catastrophic forgetting of\ndistributional knowledge. In this work, we investigate models for complementing\nthe distributional knowledge of BERT with conceptual knowledge from ConceptNet\nand its corresponding Open Mind Common Sense (OMCS) corpus, respectively, using\nadapter training. While overall results on the GLUE benchmark paint an\ninconclusive picture, a deeper analysis reveals that our adapter-based models\nsubstantially outperform BERT (up to 15-20 performance points) on inference\ntasks that require the type of conceptual knowledge explicitly present in\nConceptNet and OMCS. All code and experiments are open sourced under:\nhttps://github.com/wluper/retrograph .", "published": "2020-05-24 15:49:57", "link": "http://arxiv.org/abs/2005.11787v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Does That Sound? Multi-Language SpokenName2Vec Algorithm Using\n  Speech Generation and Deep Learning", "abstract": "Searching for information about a specific person is an online activity\nfrequently performed by many users. In most cases, users are aided by queries\ncontaining a name and sending back to the web search engines for finding their\nwill. Typically, Web search engines provide just a few accurate results\nassociated with a name-containing query. Currently, most solutions for\nsuggesting synonyms in online search are based on pattern matching and phonetic\nencoding, however very often, the performance of such solutions is less than\noptimal. In this paper, we propose SpokenName2Vec, a novel and generic approach\nwhich addresses the similar name suggestion problem by utilizing automated\nspeech generation, and deep learning to produce spoken name embeddings. This\nsophisticated and innovative embeddings captures the way people pronounce names\nin any language and accent. Utilizing the name pronunciation can be helpful for\nboth differentiating and detecting names that sound alike, but are written\ndifferently. The proposed approach was demonstrated on a large-scale dataset\nconsisting of 250,000 forenames and evaluated using a machine learning\nclassifier and 7,399 names with their verified synonyms. The performance of the\nproposed approach was found to be superior to 10 other algorithms evaluated in\nthis study, including well used phonetic and string similarity algorithms, and\ntwo recently proposed algorithms. The results obtained suggest that the\nproposed approach could serve as a useful and valuable tool for solving the\nsimilar name suggestion problem.", "published": "2020-05-24 20:39:00", "link": "http://arxiv.org/abs/2005.11838v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stronger Baselines for Grammatical Error Correction Using Pretrained\n  Encoder-Decoder Model", "abstract": "Studies on grammatical error correction (GEC) have reported the effectiveness\nof pretraining a Seq2Seq model with a large amount of pseudodata. However, this\napproach requires time-consuming pretraining for GEC because of the size of the\npseudodata. In this study, we explore the utility of bidirectional and\nauto-regressive transformers (BART) as a generic pretrained encoder-decoder\nmodel for GEC. With the use of this generic pretrained model for GEC, the\ntime-consuming pretraining can be eliminated. We find that monolingual and\nmultilingual BART models achieve high performance in GEC, with one of the\nresults being comparable to the current strong results in English GEC. Our\nimplementations are publicly available at GitHub\n(https://github.com/Katsumata420/generic-pretrained-GEC).", "published": "2020-05-24 22:13:24", "link": "http://arxiv.org/abs/2005.11849v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Jointly Encoding Word Confusion Network and Dialogue Context with BERT\n  for Spoken Language Understanding", "abstract": "Spoken Language Understanding (SLU) converts hypotheses from automatic speech\nrecognizer (ASR) into structured semantic representations. ASR recognition\nerrors can severely degenerate the performance of the subsequent SLU module. To\naddress this issue, word confusion networks (WCNs) have been used to encode the\ninput for SLU, which contain richer information than 1-best or n-best\nhypotheses list. To further eliminate ambiguity, the last system act of\ndialogue context is also utilized as additional input. In this paper, a novel\nBERT based SLU model (WCN-BERT SLU) is proposed to encode WCNs and the dialogue\ncontext jointly. It can integrate both structural information and ASR posterior\nprobabilities of WCNs in the BERT architecture. Experiments on DSTC2, a\nbenchmark of SLU, show that the proposed method is effective and can outperform\nprevious state-of-the-art models significantly.", "published": "2020-05-24 02:26:13", "link": "http://arxiv.org/abs/2005.11640v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Question Type Driven and Copy Loss Enhanced Frameworkfor\n  Answer-Agnostic Neural Question Generation", "abstract": "The answer-agnostic question generation is a significant and challenging\ntask, which aims to automatically generate questions for a given sentence but\nwithout an answer. In this paper, we propose two new strategies to deal with\nthis task: question type prediction and copy loss mechanism. The question type\nmodule is to predict the types of questions that should be asked, which allows\nour model to generate multiple types of questions for the same source sentence.\nThe new copy loss enhances the original copy mechanism to make sure that every\nimportant word in the source sentence has been copied when generating\nquestions. Our integrated model outperforms the state-of-the-art approach in\nanswer-agnostic question generation, achieving a BLEU-4 score of 13.9 on SQuAD.\nHuman evaluation further validates the high quality of our generated questions.\nWe will make our code public available for further research.", "published": "2020-05-24 07:09:04", "link": "http://arxiv.org/abs/2005.11665v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Query Resolution for Conversational Search with Limited Supervision", "abstract": "In this work we focus on multi-turn passage retrieval as a crucial component\nof conversational search. One of the key challenges in multi-turn passage\nretrieval comes from the fact that the current turn query is often\nunderspecified due to zero anaphora, topic change, or topic return. Context\nfrom the conversational history can be used to arrive at a better expression of\nthe current turn query, defined as the task of query resolution. In this paper,\nwe model the query resolution task as a binary term classification problem: for\neach term appearing in the previous turns of the conversation decide whether to\nadd it to the current turn query or not. We propose QuReTeC (Query Resolution\nby Term Classification), a neural query resolution model based on bidirectional\ntransformers. We propose a distant supervision method to automatically generate\ntraining data by using query-passage relevance labels. Such labels are often\nreadily available in a collection either as human annotations or inferred from\nuser interactions. We show that QuReTeC outperforms state-of-the-art models,\nand furthermore, that our distant supervision method can be used to\nsubstantially reduce the amount of human-curated data required to train\nQuReTeC. We incorporate QuReTeC in a multi-turn, multi-stage passage retrieval\narchitecture and demonstrate its effectiveness on the TREC CAsT dataset.", "published": "2020-05-24 11:37:22", "link": "http://arxiv.org/abs/2005.11723v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "GoChat: Goal-oriented Chatbots with Hierarchical Reinforcement Learning", "abstract": "A chatbot that converses like a human should be goal-oriented (i.e., be\npurposeful in conversation), which is beyond language generation. However,\nexisting dialogue systems often heavily rely on cumbersome hand-crafted rules\nor costly labelled datasets to reach the goals. In this paper, we propose\nGoal-oriented Chatbots (GoChat), a framework for end-to-end training chatbots\nto maximize the longterm return from offline multi-turn dialogue datasets. Our\nframework utilizes hierarchical reinforcement learning (HRL), where the\nhigh-level policy guides the conversation towards the final goal by determining\nsome sub-goals, and the low-level policy fulfills the sub-goals by generating\nthe corresponding utterance for response. In our experiments on a real-world\ndialogue dataset for anti-fraud in financial, our approach outperforms previous\nmethods on both the quality of response generation as well as the success rate\nof accomplishing the goal.", "published": "2020-05-24 12:14:19", "link": "http://arxiv.org/abs/2005.11729v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adversarial NLI for Factual Correctness in Text Summarisation Models", "abstract": "We apply the Adversarial NLI dataset to train the NLI model and show that the\nmodel has the potential to enhance factual correctness in abstract\nsummarization. We follow the work of Falke et al. (2019), which rank multiple\ngenerated summaries based on the entailment probabilities between an source\ndocument and summaries and select the summary that has the highest entailment\nprobability. The authors' earlier study concluded that current NLI models are\nnot sufficiently accurate for the ranking task. We show that the Transformer\nmodels fine-tuned on the new dataset achieve significantly higher accuracy and\nhave the potential of selecting a coherent summary.", "published": "2020-05-24 13:02:57", "link": "http://arxiv.org/abs/2005.11739v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ON-TRAC Consortium for End-to-End and Simultaneous Speech Translation\n  Challenge Tasks at IWSLT 2020", "abstract": "This paper describes the ON-TRAC Consortium translation systems developed for\ntwo challenge tracks featured in the Evaluation Campaign of IWSLT 2020, offline\nspeech translation and simultaneous speech translation. ON-TRAC Consortium is\ncomposed of researchers from three French academic laboratories: LIA (Avignon\nUniversit\\'e), LIG (Universit\\'e Grenoble Alpes), and LIUM (Le Mans\nUniversit\\'e). Attention-based encoder-decoder models, trained end-to-end, were\nused for our submissions to the offline speech translation track. Our\ncontributions focused on data augmentation and ensembling of multiple models.\nIn the simultaneous speech translation track, we build on Transformer-based\nwait-k models for the text-to-text subtask. For speech-to-text simultaneous\ntranslation, we attach a wait-k MT system to a hybrid ASR system. We propose an\nalgorithm to control the latency of the ASR+MT cascade and achieve a good\nlatency-quality trade-off on both subtasks.", "published": "2020-05-24 23:44:45", "link": "http://arxiv.org/abs/2005.11861v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Transformer VQ-VAE for Unsupervised Unit Discovery and Speech Synthesis:\n  ZeroSpeech 2020 Challenge", "abstract": "In this paper, we report our submitted system for the ZeroSpeech 2020\nchallenge on Track 2019. The main theme in this challenge is to build a speech\nsynthesizer without any textual information or phonetic labels. In order to\ntackle those challenges, we build a system that must address two major\ncomponents such as 1) given speech audio, extract subword units in an\nunsupervised way and 2) re-synthesize the audio from novel speakers. The system\nalso needs to balance the codebook performance between the ABX error rate and\nthe bitrate compression rate. Our main contribution here is we proposed\nTransformer-based VQ-VAE for unsupervised unit discovery and Transformer-based\ninverter for the speech synthesis given the extracted codebook. Additionally,\nwe also explored several regularization methods to improve performance even\nfurther.", "published": "2020-05-24 07:42:43", "link": "http://arxiv.org/abs/2005.11676v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Glottal source estimation robustness: A comparison of sensitivity of\n  voice source estimation techniques", "abstract": "This paper addresses the problem of estimating the voice source directly from\nspeech waveforms. A novel principle based on Anticausality Dominated Regions\n(ACDR) is used to estimate the glottal open phase. This technique is compared\nto two other state-of-the-art well-known methods, namely the Zeros of the\nZ-Transform (ZZT) and the Iterative Adaptive Inverse Filtering (IAIF)\nalgorithms. Decomposition quality is assessed on synthetic signals through two\nobjective measures: the spectral distortion and a glottal formant determination\nrate. Technique robustness is tested by analyzing the influence of noise and\nGlottal Closure Instant (GCI) location errors. Besides impacts of the\nfundamental frequency and the first formant on the performance are evaluated.\nOur proposed approach shows significant improvement in robustness, which could\nbe of a great interest when decomposing real speech.", "published": "2020-05-24 08:13:47", "link": "http://arxiv.org/abs/2005.11682v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MASK: A flexible framework to facilitate de-identification of clinical\n  texts", "abstract": "Medical health records and clinical summaries contain a vast amount of\nimportant information in textual form that can help advancing research on\ntreatments, drugs and public health. However, the majority of these information\nis not shared because they contain private information about patients, their\nfamilies, or medical staff treating them. Regulations such as HIPPA in the US,\nPHIPPA in Canada and GDPR regulate the protection, processing and distribution\nof this information. In case this information is de-identified and personal\ninformation are replaced or redacted, they could be distributed to the research\ncommunity. In this paper, we present MASK, a software package that is designed\nto perform the de-identification task. The software is able to perform named\nentity recognition using some of the state-of-the-art techniques and then mask\nor redact recognized entities. The user is able to select named entity\nrecognition algorithm (currently implemented are two versions of CRF-based\ntechniques and BiLSTM-based neural network with pre-trained GLoVe and ELMo\nembedding) and masking algorithm (e.g. shift dates, replace names/locations,\ntotally redact entity).", "published": "2020-05-24 08:53:00", "link": "http://arxiv.org/abs/2005.11687v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lite Audio-Visual Speech Enhancement", "abstract": "Previous studies have confirmed the effectiveness of incorporating visual\ninformation into speech enhancement (SE) systems. Despite improved denoising\nperformance, two problems may be encountered when implementing an audio-visual\nSE (AVSE) system: (1) additional processing costs are incurred to incorporate\nvisual input and (2) the use of face or lip images may cause privacy problems.\nIn this study, we propose a Lite AVSE (LAVSE) system to address these problems.\nThe system includes two visual data compression techniques and removes the\nvisual feature extraction network from the training model, yielding better\nonline computation efficiency. Our experimental results indicate that the\nproposed LAVSE system can provide notably better performance than an audio-only\nSE system with a similar number of model parameters. In addition, the\nexperimental results confirm the effectiveness of the two techniques for visual\ndata compression.", "published": "2020-05-24 15:09:42", "link": "http://arxiv.org/abs/2005.11769v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Acoustic Word Embedding System for Code-Switching Query-by-example\n  Spoken Term Detection", "abstract": "In this paper, we propose a deep convolutional neural network-based acoustic\nword embedding system on code-switching query by example spoken term detection.\nDifferent from previous configurations, we combine audio data in two languages\nfor training instead of only using one single language. We transform the\nacoustic features of keyword templates and searching content to\nfixed-dimensional vectors and calculate the distances between keyword segments\nand searching content segments obtained in a sliding manner. An auxiliary\nvariability-invariant loss is also applied to training data within the same\nword but different speakers. This strategy is used to prevent the extractor\nfrom encoding undesired speaker- or accent-related information into the\nacoustic word embeddings. Experimental results show that our proposed system\nproduces promising searching results in the code-switching test scenario. With\nthe increased number of templates and the employment of variability-invariant\nloss, the searching performance is further enhanced.", "published": "2020-05-24 15:27:56", "link": "http://arxiv.org/abs/2005.11777v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "MIMO Speech Compression and Enhancement Based on Convolutional Denoising\n  Autoencoder", "abstract": "For speech-related applications in IoT environments, identifying effective\nmethods to handle interference noises and compress the amount of data in\ntransmissions is essential to achieve high-quality services. In this study, we\npropose a novel multi-input multi-output speech compression and enhancement\n(MIMO-SCE) system based on a convolutional denoising autoencoder (CDAE) model\nto simultaneously improve speech quality and reduce the dimensions of\ntransmission data. Compared with conventional single-channel and multi-input\nsingle-output systems, MIMO systems can be employed in applications that handle\nmultiple acoustic signals need to be handled. We investigated two CDAE models,\na fully convolutional network (FCN) and a Sinc FCN, as the core models in MIMO\nsystems. The experimental results confirm that the proposed MIMO-SCE framework\neffectively improves speech quality and intelligibility while reducing the\namount of recording data by a factor of 7 for transmission.", "published": "2020-05-24 09:58:11", "link": "http://arxiv.org/abs/2005.11704v2", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "SERIL: Noise Adaptive Speech Enhancement using Regularization-based\n  Incremental Learning", "abstract": "Numerous noise adaptation techniques have been proposed to fine-tune\ndeep-learning models in speech enhancement (SE) for mismatched noise\nenvironments. Nevertheless, adaptation to a new environment may lead to\ncatastrophic forgetting of the previously learned environments. The\ncatastrophic forgetting issue degrades the performance of SE in real-world\nembedded devices, which often revisit previous noise environments. The nature\nof embedded devices does not allow solving the issue with additional storage of\nall pre-trained models or earlier training data. In this paper, we propose a\nregularization-based incremental learning SE (SERIL) strategy, complementing\nexisting noise adaptation strategies without using additional storage. With a\nregularization constraint, the parameters are updated to the new noise\nenvironment while retaining the knowledge of the previous noise environments.\nThe experimental results show that, when faced with a new noise domain, the\nSERIL model outperforms the unadapted SE model. Meanwhile, compared with the\ncurrent adaptive technique based on fine-tuning, the SERIL model can reduce the\nforgetting of previous noise environments by 52%. The results verify that the\nSERIL model can effectively adjust itself to new noise environments while\novercoming the catastrophic forgetting issue. The results make SERIL a\nfavorable choice for real-world SE applications, where the noise environment\nchanges frequently.", "published": "2020-05-24 14:49:10", "link": "http://arxiv.org/abs/2005.11760v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Detecting Adversarial Examples for Speech Recognition via Uncertainty\n  Quantification", "abstract": "Machine learning systems and also, specifically, automatic speech recognition\n(ASR) systems are vulnerable against adversarial attacks, where an attacker\nmaliciously changes the input. In the case of ASR systems, the most interesting\ncases are targeted attacks, in which an attacker aims to force the system into\nrecognizing given target transcriptions in an arbitrary audio sample. The\nincreasing number of sophisticated, quasi imperceptible attacks raises the\nquestion of countermeasures. In this paper, we focus on hybrid ASR systems and\ncompare four acoustic models regarding their ability to indicate uncertainty\nunder attack: a feed-forward neural network and three neural networks\nspecifically designed for uncertainty quantification, namely a Bayesian neural\nnetwork, Monte Carlo dropout, and a deep ensemble. We employ uncertainty\nmeasures of the acoustic model to construct a simple one-class classification\nmodel for assessing whether inputs are benign or adversarial. Based on this\napproach, we are able to detect adversarial examples with an area under the\nreceiving operator curve score of more than 0.99. The neural networks for\nuncertainty quantification simultaneously diminish the vulnerability to the\nattack, which is reflected in a lower recognition accuracy of the malicious\ntarget text in comparison to a standard hybrid ASR system.", "published": "2020-05-24 19:31:02", "link": "http://arxiv.org/abs/2005.14611v2", "categories": ["eess.AS", "cs.CR", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
