{"title": "Language models are not naysayers: An analysis of language models on\n  negation benchmarks", "abstract": "Negation has been shown to be a major bottleneck for masked language models,\nsuch as BERT. However, whether this finding still holds for larger-sized\nauto-regressive language models (``LLMs'') has not been studied\ncomprehensively. With the ever-increasing volume of research and applications\nof LLMs, we take a step back to evaluate the ability of current-generation LLMs\nto handle negation, a fundamental linguistic phenomenon that is central to\nlanguage understanding. We evaluate different LLMs -- including the open-source\nGPT-neo, GPT-3, and InstructGPT -- against a wide range of negation benchmarks.\nThrough systematic experimentation with varying model sizes and prompts, we\nshow that LLMs have several limitations including insensitivity to the presence\nof negation, an inability to capture the lexical semantics of negation, and a\nfailure to reason under negation.", "published": "2023-06-14 01:16:37", "link": "http://arxiv.org/abs/2306.08189v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the Effectiveness of GPT-3 in Detecting False Political\n  Statements: A Case Study on the LIAR Dataset", "abstract": "The detection of political fake statements is crucial for maintaining\ninformation integrity and preventing the spread of misinformation in society.\nHistorically, state-of-the-art machine learning models employed various methods\nfor detecting deceptive statements. These methods include the use of metadata\n(W. Wang et al., 2018), n-grams analysis (Singh et al., 2021), and linguistic\n(Wu et al., 2022) and stylometric (Islam et al., 2020) features. Recent\nadvancements in large language models, such as GPT-3 (Brown et al., 2020) have\nachieved state-of-the-art performance on a wide range of tasks. In this study,\nwe conducted experiments with GPT-3 on the LIAR dataset (W. Wang et al., 2018)\nand achieved higher accuracy than state-of-the-art models without using any\nadditional meta or linguistic features. Additionally, we experimented with\nzero-shot learning using a carefully designed prompt and achieved near\nstate-of-the-art performance. An advantage of this approach is that the model\nprovided evidence for its decision, which adds transparency to the model's\ndecision-making and offers a chance for users to verify the validity of the\nevidence provided.", "published": "2023-06-14 01:16:49", "link": "http://arxiv.org/abs/2306.08190v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Contrastive Loss is All You Need to Recover Analogies as Parallel Lines", "abstract": "While static word embedding models are known to represent linguistic\nanalogies as parallel lines in high-dimensional space, the underlying mechanism\nas to why they result in such geometric structures remains obscure. We find\nthat an elementary contrastive-style method employed over distributional\ninformation performs competitively with popular word embedding models on\nanalogy recovery tasks, while achieving dramatic speedups in training time.\nFurther, we demonstrate that a contrastive loss is sufficient to create these\nparallel structures in word embeddings, and establish a precise relationship\nbetween the co-occurrence statistics and the geometric structure of the\nresulting word embeddings.", "published": "2023-06-14 03:22:10", "link": "http://arxiv.org/abs/2306.08221v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating the dynamics of hand and lips in French Cued Speech using\n  attention mechanisms and CTC-based decoding", "abstract": "Hard of hearing or profoundly deaf people make use of cued speech (CS) as a\ncommunication tool to understand spoken language. By delivering cues that are\nrelevant to the phonetic information, CS offers a way to enhance lipreading. In\nliterature, there have been several studies on the dynamics between the hand\nand the lips in the context of human production. This article proposes a way to\ninvestigate how a neural network learns this relation for a single speaker\nwhile performing a recognition task using attention mechanisms. Further, an\nanalysis of the learnt dynamics is utilized to establish the relationship\nbetween the two modalities and extract automatic segments. For the purpose of\nthis study, a new dataset has been recorded for French CS. Along with the\nrelease of this dataset, a benchmark will be reported for word-level\nrecognition, a novelty in the automatic recognition of French CS.", "published": "2023-06-14 06:58:07", "link": "http://arxiv.org/abs/2306.08290v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically\n  Constructed from Live Streaming", "abstract": "Open-domain dialogue systems have made promising progress in recent years.\nWhile the state-of-the-art dialogue agents are built upon large-scale\ntext-based social media data and large pre-trained models, there is no\nguarantee these agents could also perform well in fast-growing scenarios, such\nas live streaming, due to the bounded transferability of pre-trained models and\nbiased distributions of public datasets from Reddit and Weibo, etc. To improve\nthe essential capability of responding and establish a benchmark in the live\nopen-domain scenario, we introduce the LiveChat dataset, composed of 1.33\nmillion real-life Chinese dialogues with almost 3800 average sessions across\n351 personas and fine-grained profiles for each persona. LiveChat is\nautomatically constructed by processing numerous live videos on the Internet\nand naturally falls within the scope of multi-party conversations, where the\nissues of Who says What to Whom should be considered. Therefore, we target two\ncritical tasks of response modeling and addressee recognition and propose\nretrieval-based baselines grounded on advanced techniques. Experimental results\nhave validated the positive effects of leveraging persona profiles and larger\naverage sessions per persona. In addition, we also benchmark the\ntransferability of advanced generation-based models on LiveChat and pose some\nfuture directions for current challenges.", "published": "2023-06-14 09:50:06", "link": "http://arxiv.org/abs/2306.08401v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building a Corpus for Biomedical Relation Extraction of Species Mentions", "abstract": "We present a manually annotated corpus, Species-Species Interaction, for\nextracting meaningful binary relations between species, in biomedical texts, at\nsentence level, with a focus on the gut microbiota. The corpus leverages\nPubTator to annotate species in full-text articles after evaluating different\nNamed Entity Recognition species taggers. Our first results are promising for\nextracting relations between species using BERT and its biomedical variants.", "published": "2023-06-14 09:56:32", "link": "http://arxiv.org/abs/2306.08403v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Definition Modeling: To model definitions.\" Generating Definitions With\n  Little to No Semantics", "abstract": "Definition Modeling, the task of generating definitions, was first proposed\nas a means to evaluate the semantic quality of word embeddings-a coherent\nlexical semantic representations of a word in context should contain all the\ninformation necessary to generate its definition. The relative novelty of this\ntask entails that we do not know which factors are actually relied upon by a\nDefinition Modeling system. In this paper, we present evidence that the task\nmay not involve as much semantics as one might expect: we show how an earlier\nmodel from the literature is both rather insensitive to semantic aspects such\nas explicit polysemy, as well as reliant on formal similarities between\nheadwords and words occurring in its glosses, casting doubt on the validity of\nthe task as a means to evaluate embeddings.", "published": "2023-06-14 11:08:38", "link": "http://arxiv.org/abs/2306.08433v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PoetryDiffusion: Towards Joint Semantic and Metrical Manipulation in\n  Poetry Generation", "abstract": "Controllable text generation is a challenging and meaningful field in natural\nlanguage generation (NLG). Especially, poetry generation is a typical one with\nwell-defined and strict conditions for text generation which is an ideal\nplayground for the assessment of current methodologies. While prior works\nsucceeded in controlling either semantic or metrical aspects of poetry\ngeneration, simultaneously addressing both remains a challenge. In this paper,\nwe pioneer the use of the Diffusion model for generating sonnets and Chinese\nSongCi poetry to tackle such challenges. In terms of semantics, our\nPoetryDiffusion model, built upon the Diffusion model, generates entire\nsentences or poetry by comprehensively considering the entirety of sentence\ninformation. This approach enhances semantic expression, distinguishing it from\nautoregressive and large language models (LLMs). For metrical control, the\nseparation feature of diffusion generation and its constraint control module\nenable us to flexibly incorporate a novel metrical controller to manipulate and\nevaluate metrics (format and rhythm). The denoising process in PoetryDiffusion\nallows for gradual enhancement of semantics and flexible integration of the\nmetrical controller which can calculate and impose penalties on states that\nstray significantly from the target control distribution. Experimental results\non two datasets demonstrate that our model outperforms existing models in\nautomatic evaluation of semantic, metrical, and overall performance as well as\nhuman evaluation.", "published": "2023-06-14 11:57:31", "link": "http://arxiv.org/abs/2306.08456v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Relaxed Optimization Approach for Adversarial Attacks against Neural\n  Machine Translation Models", "abstract": "In this paper, we propose an optimization-based adversarial attack against\nNeural Machine Translation (NMT) models. First, we propose an optimization\nproblem to generate adversarial examples that are semantically similar to the\noriginal sentences but destroy the translation generated by the target NMT\nmodel. This optimization problem is discrete, and we propose a continuous\nrelaxation to solve it. With this relaxation, we find a probability\ndistribution for each token in the adversarial example, and then we can\ngenerate multiple adversarial examples by sampling from these distributions.\nExperimental results show that our attack significantly degrades the\ntranslation quality of multiple NMT models while maintaining the semantic\nsimilarity between the original and adversarial sentences. Furthermore, our\nattack outperforms the baselines in terms of success rate, similarity\npreservation, effect on translation quality, and token error rate. Finally, we\npropose a black-box extension of our attack by sampling from an optimized\nprobability distribution for a reference model whose gradients are accessible.", "published": "2023-06-14 13:13:34", "link": "http://arxiv.org/abs/2306.08492v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does mBERT understand Romansh? Evaluating word embeddings using word\n  alignment", "abstract": "We test similarity-based word alignment models (SimAlign and awesome-align)\nin combination with word embeddings from mBERT and XLM-R on parallel sentences\nin German and Romansh. Since Romansh is an unseen language, we are dealing with\na zero-shot setting. Using embeddings from mBERT, both models reach an\nalignment error rate of 0.22, which outperforms fast_align, a statistical\nmodel, and is on par with similarity-based word alignment for seen languages.\nWe interpret these results as evidence that mBERT contains information that can\nbe meaningful and applicable to Romansh.\n  To evaluate performance, we also present a new trilingual corpus, which we\ncall the DERMIT (DE-RM-IT) corpus, containing press releases made by the Canton\nof Grisons in German, Romansh and Italian in the past 25 years. The corpus\ncontains 4 547 parallel documents and approximately 100 000 sentence pairs in\neach language combination. We additionally present a gold standard for\nGerman-Romansh word alignment. The data is available at\nhttps://github.com/eyldlv/DERMIT-Corpus.", "published": "2023-06-14 19:00:12", "link": "http://arxiv.org/abs/2306.08702v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap", "abstract": "Large language models (LLMs), such as ChatGPT and GPT4, are making new waves\nin the field of natural language processing and artificial intelligence, due to\ntheir emergent ability and generalizability. However, LLMs are black-box\nmodels, which often fall short of capturing and accessing factual knowledge. In\ncontrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are\nstructured knowledge models that explicitly store rich factual knowledge. KGs\ncan enhance LLMs by providing external knowledge for inference and\ninterpretability. Meanwhile, KGs are difficult to construct and evolving by\nnature, which challenges the existing methods in KGs to generate new facts and\nrepresent unseen knowledge. Therefore, it is complementary to unify LLMs and\nKGs together and simultaneously leverage their advantages. In this article, we\npresent a forward-looking roadmap for the unification of LLMs and KGs. Our\nroadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs,\nwhich incorporate KGs during the pre-training and inference phases of LLMs, or\nfor the purpose of enhancing understanding of the knowledge learned by LLMs; 2)\nLLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding,\ncompletion, construction, graph-to-text generation, and question answering; and\n3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a\nmutually beneficial way to enhance both LLMs and KGs for bidirectional\nreasoning driven by both data and knowledge. We review and summarize existing\nefforts within these three frameworks in our roadmap and pinpoint their future\nresearch directions.", "published": "2023-06-14 07:15:26", "link": "http://arxiv.org/abs/2306.08302v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Research on Named Entity Recognition in Improved transformer with R-Drop\n  structure", "abstract": "To enhance the generalization ability of the model and improve the\neffectiveness of the transformer for named entity recognition tasks, the\nXLNet-Transformer-R model is proposed in this paper. The XLNet pre-trained\nmodel and the Transformer encoder with relative positional encodings are\ncombined to enhance the model's ability to process long text and learn\ncontextual information to improve robustness. To prevent overfitting, the\nR-Drop structure is used to improve the generalization capability and enhance\nthe accuracy of the model in named entity recognition tasks. The model in this\npaper performs ablation experiments on the MSRA dataset and comparison\nexperiments with other models on four datasets with excellent performance,\ndemonstrating the strategic effectiveness of the XLNet-Transformer-R model.", "published": "2023-06-14 07:34:27", "link": "http://arxiv.org/abs/2306.08315v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "T5-SR: A Unified Seq-to-Seq Decoding Strategy for Semantic Parsing", "abstract": "Translating natural language queries into SQLs in a seq2seq manner has\nattracted much attention recently. However, compared with\nabstract-syntactic-tree-based SQL generation, seq2seq semantic parsers face\nmuch more challenges, including poor quality on schematical information\nprediction and poor semantic coherence between natural language queries and\nSQLs. This paper analyses the above difficulties and proposes a\nseq2seq-oriented decoding strategy called SR, which includes a new intermediate\nrepresentation SSQL and a reranking method with score re-estimator to solve the\nabove obstacles respectively. Experimental results demonstrate the\neffectiveness of our proposed techniques and T5-SR-3b achieves new\nstate-of-the-art results on the Spider dataset.", "published": "2023-06-14 08:57:13", "link": "http://arxiv.org/abs/2306.08368v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A semantically enhanced dual encoder for aspect sentiment triplet\n  extraction", "abstract": "Aspect sentiment triplet extraction (ASTE) is a crucial subtask of\naspect-based sentiment analysis (ABSA) that aims to comprehensively identify\nsentiment triplets. Previous research has focused on enhancing ASTE through\ninnovative table-filling strategies. However, these approaches often overlook\nthe multi-perspective nature of language expressions, resulting in a loss of\nvaluable interaction information between aspects and opinions. To address this\nlimitation, we propose a framework that leverages both a basic encoder,\nprimarily based on BERT, and a particular encoder comprising a Bi-LSTM network\nand graph convolutional network (GCN ). The basic encoder captures the\nsurface-level semantics of linguistic expressions, while the particular encoder\nextracts deeper semantics, including syntactic and lexical information. By\nmodeling the dependency tree of comments and considering the part-of-speech and\npositional information of words, we aim to capture semantics that are more\nrelevant to the underlying intentions of the sentences. An interaction strategy\ncombines the semantics learned by the two encoders, enabling the fusion of\nmultiple perspectives and facilitating a more comprehensive understanding of\naspect--opinion relationships. Experiments conducted on benchmark datasets\ndemonstrate the state-of-the-art performance of our proposed framework.", "published": "2023-06-14 09:04:14", "link": "http://arxiv.org/abs/2306.08373v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DiffuDetox: A Mixed Diffusion Model for Text Detoxification", "abstract": "Text detoxification is a conditional text generation task aiming to remove\noffensive content from toxic text. It is highly useful for online forums and\nsocial media, where offensive content is frequently encountered. Intuitively,\nthere are diverse ways to detoxify sentences while preserving their meanings,\nand we can select from detoxified sentences before displaying text to users.\nConditional diffusion models are particularly suitable for this task given\ntheir demonstrated higher generative diversity than existing conditional text\ngeneration models based on language models. Nonetheless, text fluency declines\nwhen they are trained with insufficient data, which is the case for this task.\nIn this work, we propose DiffuDetox, a mixed conditional and unconditional\ndiffusion model for text detoxification. The conditional model takes toxic text\nas the condition and reduces its toxicity, yielding a diverse set of detoxified\nsentences. The unconditional model is trained to recover the input text, which\nallows the introduction of additional fluent text for training and thus ensures\ntext fluency. Extensive experimental results and in-depth analysis demonstrate\nthe effectiveness of our proposed DiffuDetox.", "published": "2023-06-14 13:41:23", "link": "http://arxiv.org/abs/2306.08505v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MiniLLM: Knowledge Distillation of Large Language Models", "abstract": "Knowledge Distillation (KD) is a promising technique for reducing the high\ncomputational demand of large language models (LLMs). However, previous KD\nmethods are primarily applied to white-box classification models or training\nsmall models to imitate black-box model APIs like ChatGPT. How to effectively\ndistill the knowledge of white-box LLMs into small models is still\nunder-explored, which becomes more important with the prosperity of open-source\nLLMs. In this work, we propose a KD approach that distills LLMs into smaller\nlanguage models. We first replace the forward Kullback-Leibler divergence (KLD)\nobjective in the standard KD approaches with reverse KLD, which is more\nsuitable for KD on generative language models, to prevent the student model\nfrom overestimating the low-probability regions of the teacher distribution.\nThen, we derive an effective optimization approach to learn this objective. The\nstudent models are named MiniLLM. Extensive experiments in the\ninstruction-following setting show that MiniLLM generates more precise\nresponses with higher overall quality, lower exposure bias, better calibration,\nand higher long-text generation performance than the baselines. Our method is\nscalable for different model families with 120M to 13B parameters. Our code,\ndata, and model checkpoints can be found in\nhttps://github.com/microsoft/LMOps/tree/main/minillm.", "published": "2023-06-14 14:44:03", "link": "http://arxiv.org/abs/2306.08543v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct", "abstract": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated\nexceptional performance in code-related tasks. However, most existing models\nare solely pre-trained on extensive raw code data without instruction\nfine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs\nwith complex instruction fine-tuning, by adapting the Evol-Instruct method to\nthe domain of code. Through comprehensive experiments on four prominent code\ngeneration benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we\nunveil the exceptional capabilities of our model. It surpasses all other\nopen-source Code LLMs by a substantial margin. Moreover, our model even\noutperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on\nHumanEval and HumanEval+. Our code, model weights, and data are public at\nhttps://github.com/nlpxucan/WizardLM", "published": "2023-06-14 15:18:48", "link": "http://arxiv.org/abs/2306.08568v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Babel-ImageNet: Massively Multilingual Evaluation of Vision-and-Language\n  Representations", "abstract": "Vision-and-language (VL) models with separate encoders for each modality\n(e.g., CLIP) have become the go-to models for zero-shot image classification\nand image-text retrieval. They are, however, mostly evaluated in English as\nmultilingual benchmarks are limited in availability. We introduce\nBabel-ImageNet, a massively multilingual benchmark that offers (partial)\ntranslations of ImageNet labels to 100 languages, built without machine\ntranslation or manual annotation. We instead automatically obtain reliable\ntranslations by linking them -- via shared WordNet synsets -- to BabelNet, a\nmassively multilingual lexico-semantic network. We evaluate 11 public\nmultilingual CLIP models on zero-shot image classification (ZS-IC) on our\nbenchmark, demonstrating a significant gap between English ImageNet performance\nand that of high-resource languages (e.g., German or Chinese), and an even\nbigger gap for low-resource languages (e.g., Sinhala or Lao). Crucially, we\nshow that the models' ZS-IC performance highly correlates with their\nperformance in image-text retrieval, validating the use of Babel-ImageNet to\nevaluate multilingual models for the vast majority of languages without gold\nimage-text data. Finally, we show that the performance of multilingual CLIP can\nbe drastically improved for low-resource languages with parameter-efficient\nlanguage-specific training. We make our code and data publicly available:\n\\url{https://github.com/gregor-ge/Babel-ImageNet}", "published": "2023-06-14 17:53:06", "link": "http://arxiv.org/abs/2306.08658v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Radiology-GPT: A Large Language Model for Radiology", "abstract": "We introduce Radiology-GPT, a large language model for radiology. Using an\ninstruction tuning approach on an extensive dataset of radiology domain\nknowledge, Radiology-GPT demonstrates superior performance compared to general\nlanguage models such as StableLM, Dolly and LLaMA. It exhibits significant\nversatility in radiological diagnosis, research, and communication. This work\nserves as a catalyst for future developments in clinical NLP. The successful\nimplementation of Radiology-GPT is indicative of the potential of localizing\ngenerative large language models, specifically tailored for distinctive medical\nspecialties, while ensuring adherence to privacy standards such as HIPAA. The\nprospect of developing individualized, large-scale language models that cater\nto specific needs of various hospitals presents a promising direction. The\nfusion of conversational competence and domain-specific knowledge in these\nmodels is set to foster future development in healthcare AI. A demo of\nRadiology-GPT is available at\nhttps://huggingface.co/spaces/allen-eric/radiology-gpt.", "published": "2023-06-14 17:57:24", "link": "http://arxiv.org/abs/2306.08666v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Utilizing Longitudinal Chest X-Rays and Reports to Pre-Fill Radiology\n  Reports", "abstract": "Despite the reduction in turn-around times in radiology reports with the use\nof speech recognition software, persistent communication errors can\nsignificantly impact the interpretation of the radiology report. Pre-filling a\nradiology report holds promise in mitigating reporting errors, and despite\nefforts in the literature to generate medical reports, there exists a lack of\napproaches that exploit the longitudinal nature of patient visit records in the\nMIMIC-CXR dataset. To address this gap, we propose to use longitudinal\nmulti-modal data, i.e., previous patient visit CXR, current visit CXR, and\nprevious visit report, to pre-fill the 'findings' section of a current patient\nvisit report. We first gathered the longitudinal visit information for 26,625\npatients from the MIMIC-CXR dataset and created a new dataset called\nLongitudinal-MIMIC. With this new dataset, a transformer-based model was\ntrained to capture the information from longitudinal patient visit records\ncontaining multi-modal data (CXR images + reports) via a cross-attention-based\nmulti-modal fusion module and a hierarchical memory-driven decoder. In contrast\nto previous work that only uses current visit data as input to train a model,\nour work exploits the longitudinal information available to pre-fill the\n'findings' section of radiology reports. Experiments show that our approach\noutperforms several recent approaches. Code will be published at\nhttps://github.com/CelestialShine/Longitudinal-Chest-X-Ray.", "published": "2023-06-14 21:17:31", "link": "http://arxiv.org/abs/2306.08749v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generate to Understand for Representation", "abstract": "In recent years, a significant number of high-quality pretrained models have\nemerged, greatly impacting Natural Language Understanding (NLU), Natural\nLanguage Generation (NLG), and Text Representation tasks. Traditionally, these\nmodels are pretrained on custom domain corpora and finetuned for specific\ntasks, resulting in high costs related to GPU usage and labor. Unfortunately,\nrecent trends in language modeling have shifted towards enhancing performance\nthrough scaling, further exacerbating the associated costs.\n  Introducing GUR: a pretraining framework that combines language modeling and\ncontrastive learning objectives in a single training step. We select similar\ntext pairs based on their Longest Common Substring (LCS) from raw unlabeled\ndocuments and train the model using masked language modeling and unsupervised\ncontrastive learning. The resulting model, GUR, achieves impressive results\nwithout any labeled training data, outperforming all other pretrained baselines\nas a retriever at the recall benchmark in a zero-shot setting. Additionally,\nGUR maintains its language modeling ability, as demonstrated in our ablation\nexperiment. Our code is available at \\url{https://github.com/laohur/GUR}.", "published": "2023-06-14 06:00:18", "link": "http://arxiv.org/abs/2306.10056v1", "categories": ["cs.CL", "cs.IR", "68T50 (Primary) 03B65, 91F20(Secondary)", "I.7"], "primary_category": "cs.CL"}
{"title": "Operationalising Representation in Natural Language Processing", "abstract": "Despite its centrality in the philosophy of cognitive science, there has been\nlittle prior philosophical work engaging with the notion of representation in\ncontemporary NLP practice. This paper attempts to fill that lacuna: drawing on\nideas from cognitive science, I introduce a framework for evaluating the\nrepresentational claims made about components of neural NLP models, proposing\nthree criteria with which to evaluate whether a component of a model represents\na property and operationalising these criteria using probing classifiers, a\npopular analysis technique in NLP (and deep learning more broadly).\n  The project of operationalising a philosophically-informed notion of\nrepresentation should be of interest to both philosophers of science and NLP\npractitioners. It affords philosophers a novel testing-ground for claims about\nthe nature of representation, and helps NLPers organise the large literature on\nprobing experiments, suggesting novel avenues for empirical research.", "published": "2023-06-14 01:34:16", "link": "http://arxiv.org/abs/2306.08193v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Research on an improved Conformer end-to-end Speech Recognition Model\n  with R-Drop Structure", "abstract": "To address the issue of poor generalization ability in end-to-end speech\nrecognition models within deep learning, this study proposes a new\nConformer-based speech recognition model called \"Conformer-R\" that incorporates\nthe R-drop structure. This model combines the Conformer model, which has shown\npromising results in speech recognition, with the R-drop structure. By doing\nso, the model is able to effectively model both local and global speech\ninformation while also reducing overfitting through the use of the R-drop\nstructure. This enhances the model's ability to generalize and improves overall\nrecognition efficiency. The model was first pre-trained on the Aishell1 and\nWenetspeech datasets for general domain adaptation, and subsequently fine-tuned\non computer-related audio data. Comparison tests with classic models such as\nLAS and Wenet were performed on the same test set, demonstrating the\nConformer-R model's ability to effectively improve generalization.", "published": "2023-06-14 08:01:23", "link": "http://arxiv.org/abs/2306.08329v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-target Backdoor Attacks for Code Pre-trained Models", "abstract": "Backdoor attacks for neural code models have gained considerable attention\ndue to the advancement of code intelligence. However, most existing works\ninsert triggers into task-specific data for code-related downstream tasks,\nthereby limiting the scope of attacks. Moreover, the majority of attacks for\npre-trained models are designed for understanding tasks. In this paper, we\npropose task-agnostic backdoor attacks for code pre-trained models. Our\nbackdoored model is pre-trained with two learning strategies (i.e., Poisoned\nSeq2Seq learning and token representation learning) to support the multi-target\nattack of downstream code understanding and generation tasks. During the\ndeployment phase, the implanted backdoors in the victim models can be activated\nby the designed triggers to achieve the targeted attack. We evaluate our\napproach on two code understanding tasks and three code generation tasks over\nseven datasets. Extensive experiments demonstrate that our approach can\neffectively and stealthily attack code-related downstream tasks.", "published": "2023-06-14 08:38:51", "link": "http://arxiv.org/abs/2306.08350v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "SpeechGLUE: How Well Can Self-Supervised Speech Models Capture\n  Linguistic Knowledge?", "abstract": "Self-supervised learning (SSL) for speech representation has been\nsuccessfully applied in various downstream tasks, such as speech and speaker\nrecognition. More recently, speech SSL models have also been shown to be\nbeneficial in advancing spoken language understanding tasks, implying that the\nSSL models have the potential to learn not only acoustic but also linguistic\ninformation. In this paper, we aim to clarify if speech SSL techniques can well\ncapture linguistic knowledge. For this purpose, we introduce SpeechGLUE, a\nspeech version of the General Language Understanding Evaluation (GLUE)\nbenchmark. Since GLUE comprises a variety of natural language understanding\ntasks, SpeechGLUE can elucidate the degree of linguistic ability of speech SSL\nmodels. Experiments demonstrate that speech SSL models, although inferior to\ntext-based SSL models, perform better than baselines, suggesting that they can\nacquire a certain amount of general linguistic knowledge from just unlabeled\nspeech data.", "published": "2023-06-14 09:04:29", "link": "http://arxiv.org/abs/2306.08374v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Simple Embodied Language Learning as a Byproduct of Meta-Reinforcement\n  Learning", "abstract": "Whereas machine learning models typically learn language by directly training\non language tasks (e.g., next-word prediction), language emerges in human\nchildren as a byproduct of solving non-language tasks (e.g., acquiring food).\nMotivated by this observation, we ask: can embodied reinforcement learning (RL)\nagents also indirectly learn language from non-language tasks? Learning to\nassociate language with its meaning requires a dynamic environment with varied\nlanguage. Therefore, we investigate this question in a multi-task environment\nwith language that varies across the different tasks. Specifically, we design\nan office navigation environment, where the agent's goal is to find a\nparticular office, and office locations differ in different buildings (i.e.,\ntasks). Each building includes a floor plan with a simple language description\nof the goal office's location, which can be visually read as an RGB image when\nvisited. We find RL agents indeed are able to indirectly learn language. Agents\ntrained with current meta-RL algorithms successfully generalize to reading\nfloor plans with held-out layouts and language phrases, and quickly navigate to\nthe correct office, despite receiving no direct language supervision.", "published": "2023-06-14 09:48:48", "link": "http://arxiv.org/abs/2306.08400v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ITALIC: An Italian Intent Classification Dataset", "abstract": "Recent large-scale Spoken Language Understanding datasets focus predominantly\non English and do not account for language-specific phenomena such as\nparticular phonemes or words in different lects. We introduce ITALIC, the first\nlarge-scale speech dataset designed for intent classification in Italian. The\ndataset comprises 16,521 crowdsourced audio samples recorded by 70 speakers\nfrom various Italian regions and annotated with intent labels and additional\nmetadata. We explore the versatility of ITALIC by evaluating current\nstate-of-the-art speech and text models. Results on intent classification\nsuggest that increasing scale and running language adaptation yield better\nspeech models, monolingual text models outscore multilingual ones, and that\nspeech recognition on ITALIC is more challenging than on existing Italian\nbenchmarks. We release both the dataset and the annotation scheme to streamline\nthe development of new Italian SLU models and language-specific datasets.", "published": "2023-06-14 13:36:24", "link": "http://arxiv.org/abs/2306.08502v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "AlbMoRe: A Corpus of Movie Reviews for Sentiment Analysis in Albanian", "abstract": "Lack of available resources such as text corpora for low-resource languages\nseriously hinders research on natural language processing and computational\nlinguistics. This paper presents AlbMoRe, a corpus of 800 sentiment annotated\nmovie reviews in Albanian. Each text is labeled as positive or negative and can\nbe used for sentiment analysis research. Preliminary results based on\ntraditional machine learning classifiers trained with the AlbMoRe samples are\nalso reported. They can serve as comparison baselines for future research\nexperiments.", "published": "2023-06-14 14:21:55", "link": "http://arxiv.org/abs/2306.08526v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Cross-lingual Mappings for Data Augmentation to Improve\n  Low-Resource Speech Recognition", "abstract": "Exploiting cross-lingual resources is an effective way to compensate for data\nscarcity of low resource languages. Recently, a novel multilingual model fusion\ntechnique has been proposed where a model is trained to learn cross-lingual\nacoustic-phonetic similarities as a mapping function. However, handcrafted\nlexicons have been used to train hybrid DNN-HMM ASR systems. To remove this\ndependency, we extend the concept of learnable cross-lingual mappings for\nend-to-end speech recognition. Furthermore, mapping models are employed to\ntransliterate the source languages to the target language without using\nparallel data. Finally, the source audio and its transliteration is used for\ndata augmentation to retrain the target language ASR. The results show that any\nsource language ASR model can be used for a low-resource target language\nrecognition followed by proposed mapping model. Furthermore, data augmentation\nresults in a relative gain up to 5% over baseline monolingual model.", "published": "2023-06-14 15:24:31", "link": "http://arxiv.org/abs/2306.08577v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Tagged End-to-End Simultaneous Speech Translation Training using\n  Simultaneous Interpretation Data", "abstract": "Simultaneous speech translation (SimulST) translates partial speech inputs\nincrementally. Although the monotonic correspondence between input and output\nis preferable for smaller latency, it is not the case for distant language\npairs such as English and Japanese. A prospective approach to this problem is\nto mimic simultaneous interpretation (SI) using SI data to train a SimulST\nmodel. However, the size of such SI data is limited, so the SI data should be\nused together with ordinary bilingual data whose translations are given in\noffline. In this paper, we propose an effective way to train a SimulST model\nusing mixed data of SI and offline. The proposed method trains a single model\nusing the mixed data with style tags that tell the model to generate SI- or\noffline-style outputs. Experiment results show improvements of BLEURT in\ndifferent latency ranges, and our analyses revealed the proposed model\ngenerates SI-style outputs more than the baseline.", "published": "2023-06-14 15:42:06", "link": "http://arxiv.org/abs/2306.08582v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Improving Code-Switching and Named Entity Recognition in ASR with Speech\n  Editing based Data Augmentation", "abstract": "Recently, end-to-end (E2E) automatic speech recognition (ASR) models have\nmade great strides and exhibit excellent performance in general speech\nrecognition. However, there remain several challenging scenarios that E2E\nmodels are not competent in, such as code-switching and named entity\nrecognition (NER). Data augmentation is a common and effective practice for\nthese two scenarios. However, the current data augmentation methods mainly rely\non audio splicing and text-to-speech (TTS) models, which might result in\ndiscontinuous, unrealistic, and less diversified speech. To mitigate these\npotential issues, we propose a novel data augmentation method by applying the\ntext-based speech editing model. The augmented speech from speech editing\nsystems is more coherent and diversified, also more akin to real speech. The\nexperimental results on code-switching and NER tasks show that our proposed\nmethod can significantly outperform the audio splicing and neural TTS based\ndata augmentation systems.", "published": "2023-06-14 15:50:13", "link": "http://arxiv.org/abs/2306.08588v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "When to Use Efficient Self Attention? Profiling Text, Speech and Image\n  Transformer Variants", "abstract": "We present the first unified study of the efficiency of self-attention-based\nTransformer variants spanning text, speech and vision. We identify input length\nthresholds (tipping points) at which efficient Transformer variants become more\nefficient than vanilla models, using a variety of efficiency metrics (latency,\nthroughput, and memory). To conduct this analysis for speech, we introduce\nL-HuBERT, a novel local-attention variant of a self-supervised speech model. We\nobserve that these thresholds are (a) much higher than typical dataset sequence\nlengths and (b) dependent on the metric and modality, showing that choosing the\nright model depends on modality, task type (long-form vs. typical context) and\nresource constraints (time vs. memory). By visualising the breakdown of the\ncomputational costs for transformer components, we also show that\nnon-self-attention components exhibit significant computational costs. We\nrelease our profiling toolkit at\nhttps://github.com/ajd12342/profiling-transformers .", "published": "2023-06-14 17:59:02", "link": "http://arxiv.org/abs/2306.08667v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "World-to-Words: Grounded Open Vocabulary Acquisition through Fast\n  Mapping in Vision-Language Models", "abstract": "The ability to connect language units to their referents in the physical\nworld, referred to as grounding, is crucial to learning and understanding\ngrounded meanings of words. While humans demonstrate fast mapping in new word\nlearning, it remains unclear whether modern vision-language models can truly\nrepresent language with their grounded meanings and how grounding may further\nbootstrap new word learning. To this end, we introduce Grounded Open Vocabulary\nAcquisition (GOVA) to examine grounding and bootstrapping in open-world\nlanguage learning. As an initial attempt, we propose object-oriented BERT\n(OctoBERT), a novel visually-grounded language model by pre-training on\nimage-text pairs highlighting grounding as an objective. Through extensive\nexperiments and analysis, we demonstrate that OctoBERT is a more coherent and\nfast grounded word learner, and that the grounding ability acquired during\npre-training helps the model to learn unseen words more rapidly and robustly.\nOur code is available at https://github.com/sled-group/world-to-words", "published": "2023-06-14 18:10:05", "link": "http://arxiv.org/abs/2306.08685v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Unified model for code-switching speech recognition and language\n  identification based on a concatenated tokenizer", "abstract": "Code-Switching (CS) multilingual Automatic Speech Recognition (ASR) models\ncan transcribe speech containing two or more alternating languages during a\nconversation. This paper proposes (1) a new method for creating code-switching\nASR datasets from purely monolingual data sources, and (2) a novel Concatenated\nTokenizer that enables ASR models to generate language ID for each emitted text\ntoken while reusing existing monolingual tokenizers. The efficacy of these\napproaches for building CS ASR models is demonstrated for two language pairs,\nEnglish-Hindi and English-Spanish, where we achieve new state-of-the-art\nresults on the Miami Bangor CS evaluation corpus. In addition to competitive\nASR performance, the proposed Concatenated Tokenizer models are highly\neffective for spoken language identification, achieving 98%+ accuracy on the\nout-of-distribution FLEURS dataset.", "published": "2023-06-14 21:24:11", "link": "http://arxiv.org/abs/2306.08753v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Recipes for Sequential Pre-training of Multilingual Encoder and Seq2Seq\n  Models", "abstract": "Pre-trained encoder-only and sequence-to-sequence (seq2seq) models each have\nadvantages, however training both model types from scratch is computationally\nexpensive. We explore recipes to improve pre-training efficiency by\ninitializing one model from the other. (1) Extracting the encoder from a\nseq2seq model, we show it under-performs a Masked Language Modeling (MLM)\nencoder, particularly on sequence labeling tasks. Variations of masking during\nseq2seq training, reducing the decoder size, and continuing with a small amount\nof MLM training do not close the gap. (2) Conversely, using an encoder to\nwarm-start seq2seq training, we show that by unfreezing the encoder partway\nthrough training, we can match task performance of a from-scratch seq2seq\nmodel. Overall, this two-stage approach is an efficient recipe to obtain both a\nmultilingual encoder and a seq2seq model, matching the performance of training\neach model from scratch while reducing the total compute cost by 27%.", "published": "2023-06-14 21:41:52", "link": "http://arxiv.org/abs/2306.08756v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EM-Network: Oracle Guided Self-distillation for Sequence Learning", "abstract": "We introduce EM-Network, a novel self-distillation approach that effectively\nleverages target information for supervised sequence-to-sequence (seq2seq)\nlearning. In contrast to conventional methods, it is trained with oracle\nguidance, which is derived from the target sequence. Since the oracle guidance\ncompactly represents the target-side context that can assist the sequence model\nin solving the task, the EM-Network achieves a better prediction compared to\nusing only the source input. To allow the sequence model to inherit the\npromising capability of the EM-Network, we propose a new self-distillation\nstrategy, where the original sequence model can benefit from the knowledge of\nthe EM-Network in a one-stage manner. We conduct comprehensive experiments on\ntwo types of seq2seq models: connectionist temporal classification (CTC) for\nspeech recognition and attention-based encoder-decoder (AED) for machine\ntranslation. Experimental results demonstrate that the EM-Network significantly\nadvances the current state-of-the-art approaches, improving over the best prior\nwork on speech recognition and establishing state-of-the-art performance on\nWMT'14 and IWSLT'14.", "published": "2023-06-14 12:24:55", "link": "http://arxiv.org/abs/2306.10058v1", "categories": ["cs.LG", "cs.CL", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Revealing the structure of language model capabilities", "abstract": "Building a theoretical understanding of the capabilities of large language\nmodels (LLMs) is vital for our ability to predict and explain the behavior of\nthese systems. Here, we investigate the structure of LLM capabilities by\nextracting latent capabilities from patterns of individual differences across a\nvaried population of LLMs. Using a combination of Bayesian and frequentist\nfactor analysis, we analyzed data from 29 different LLMs across 27 cognitive\ntasks. We found evidence that LLM capabilities are not monolithic. Instead,\nthey are better explained by three well-delineated factors that represent\nreasoning, comprehension and core language modeling. Moreover, we found that\nthese three factors can explain a high proportion of the variance in model\nperformance. These results reveal a consistent structure in the capabilities of\ndifferent LLMs and demonstrate the multifaceted nature of these capabilities.\nWe also found that the three abilities show different relationships to model\nproperties such as model size and instruction tuning. These patterns help\nrefine our understanding of scaling laws and indicate that changes to a model\nthat improve one ability might simultaneously impair others. Based on these\nfindings, we suggest that benchmarks could be streamlined by focusing on tasks\nthat tap into each broad model ability.", "published": "2023-06-14 15:43:25", "link": "http://arxiv.org/abs/2306.10062v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MCR-Data2vec 2.0: Improving Self-supervised Speech Pre-training via\n  Model-level Consistency Regularization", "abstract": "Self-supervised learning (SSL) has shown significant progress in speech\nprocessing tasks. However, despite the intrinsic randomness in the Transformer\nstructure, such as dropout variants and layer-drop, improving the model-level\nconsistency remains under-explored in the speech SSL literature. To address\nthis, we propose a new pre-training method that uses consistency regularization\nto improve Data2vec 2.0, the recent state-of-the-art (SOTA) SSL model.\nSpecifically, the proposed method involves sampling two different student\nsub-models within the Data2vec 2.0 framework, enabling two output variants\nderived from a single input without additional parameters. Subsequently, we\nregularize the outputs from the student sub-models to be consistent and require\nthem to predict the representation of the teacher model. Our experimental\nresults demonstrate that the proposed approach improves the SSL model's\nrobustness and generalization ability, resulting in SOTA results on the SUPERB\nbenchmark.", "published": "2023-06-14 12:17:43", "link": "http://arxiv.org/abs/2306.08463v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Gesper: A Restoration-Enhancement Framework for General Speech\n  Reconstruction", "abstract": "This paper describes a real-time General Speech Reconstruction (Gesper)\nsystem submitted to the ICASSP 2023 Speech Signal Improvement (SSI) Challenge.\nThis novel proposed system is a two-stage architecture, in which the speech\nrestoration is performed, and then cascaded by speech enhancement. We propose a\ncomplex spectral mapping-based generative adversarial network (CSM-GAN) as the\nspeech restoration module for the first time. For noise suppression and\ndereverberation, the enhancement module is performed with fullband-wideband\nparallel processing. On the blind test set of ICASSP 2023 SSI Challenge, the\nproposed Gesper system, which satisfies the real-time condition, achieves 3.27\nP.804 overall mean opinion score (MOS) and 3.35 P.835 overall MOS, ranked 1st\nin both track 1 and track 2.", "published": "2023-06-14 11:54:39", "link": "http://arxiv.org/abs/2306.08454v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Combining piano performance dimensions for score difficulty\n  classification", "abstract": "Predicting the difficulty of playing a musical score is essential for\nstructuring and exploring score collections. Despite its importance for music\neducation, the automatic difficulty classification of piano scores is not yet\nsolved, mainly due to the lack of annotated data and the subjectiveness of the\nannotations. This paper aims to advance the state-of-the-art in score\ndifficulty classification with two major contributions. To address the lack of\ndata, we present Can I Play It? (CIPI) dataset, a machine-readable piano score\ndataset with difficulty annotations obtained from the renowned classical music\npublisher Henle Verlag. The dataset is created by matching public domain scores\nwith difficulty labels from Henle Verlag, then reviewed and corrected by an\nexpert pianist. As a second contribution, we explore various input\nrepresentations from score information to pre-trained ML models for piano\nfingering and expressiveness inspired by the musicology definition of\nperformance. We show that combining the outputs of multiple classifiers\nperforms better than the classifiers on their own, pointing to the fact that\nthe representations capture different aspects of difficulty. In addition, we\nconduct numerous experiments that lay a foundation for score difficulty\nclassification and create a basis for future research. Our best-performing\nmodel reports a 39.47% balanced accuracy and 1.13 median square error across\nthe nine difficulty levels proposed in this study. Code, dataset, and models\nare made available for reproducibility.", "published": "2023-06-14 12:49:59", "link": "http://arxiv.org/abs/2306.08480v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Scalable-Complexity Steered Response Power Mapping based on Low-Rank and\n  Sparse Interpolation", "abstract": "The steered response power (SRP) is a popular approach to compute a map of\nthe acoustic scene, typically used for acoustic source localization. The SRP\nmap is obtained as the frequency-weighted output power of a beamformer steered\ntowards a grid of candidate locations. Due to the exhaustive search over a fine\ngrid at all frequency bins, conventional frequency domain-based SRP (conv.\nFD-SRP) results in a high computational complexity. Time domain-based SRP\n(conv. TD-SRP) implementations reduce computational complexity at the cost of\naccuracy using the inverse fast Fourier transform (iFFT). In this paper, to\nenable a more favourable complexity-performance trade-off as compared to conv.\nFD-SRP and conv. TD-SRP, we consider the problem of constructing a fine SRP map\nover the entire search space at scalable computational cost. We propose two\napproaches to this problem. Expressing the conv. FD-SRP map as a matrix\ntransform of frequency-domain GCCs, we decompose the SRP matrix into a sampling\nmatrix and an interpolation matrix. While sampling can be implemented by the\niFFT, we propose to use optimal low-rank or sparse approximations of the\ninterpolation matrix for complexity reduction. The proposed approaches, refered\nto as sampling + low-rank interpolation-based SRP (SLRI-SRP) and sampling +\nsparse interpolation-based SRP (SSPI-SRP), are evaluated in various\nlocalization scenarios with speech as source signals and compared to the\nstate-of-the-art. The results indicate that SSPI-SRP performs better if large\narray apertures are used, while SLRI-SRP performs better at small array\napertures or a large number of microphones. In comparison to conv. FD-SRP, two\nto three orders of magnitude of complexity reduction can achieved, often times\nenabling a more favourable complexity-performance trade-off as compared to\nconv. TD-SRP. A MATLAB implementation is available online.", "published": "2023-06-14 14:00:44", "link": "http://arxiv.org/abs/2306.08514v2", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Towards Building Voice-based Conversational Recommender Systems:\n  Datasets, Potential Solutions, and Prospects", "abstract": "Conversational recommender systems (CRSs) have become crucial emerging\nresearch topics in the field of RSs, thanks to their natural advantages of\nexplicitly acquiring user preferences via interactive conversations and\nrevealing the reasons behind recommendations. However, the majority of current\nCRSs are text-based, which is less user-friendly and may pose challenges for\ncertain users, such as those with visual impairments or limited writing and\nreading abilities. Therefore, for the first time, this paper investigates the\npotential of voice-based CRS (VCRSs) to revolutionize the way users interact\nwith RSs in a natural, intuitive, convenient, and accessible fashion. To\nsupport such studies, we create two VCRSs benchmark datasets in the e-commerce\nand movie domains, after realizing the lack of such datasets through an\nexhaustive literature review. Specifically, we first empirically verify the\nbenefits and necessity of creating such datasets. Thereafter, we convert the\nuser-item interactions to text-based conversations through the ChatGPT-driven\nprompts for generating diverse and natural templates, and then synthesize the\ncorresponding audios via the text-to-speech model. Meanwhile, a number of\nstrategies are delicately designed to ensure the naturalness and high quality\nof voice conversations. On this basis, we further explore the potential\nsolutions and point out possible directions to build end-to-end VCRSs by\nseamlessly extracting and integrating voice-based inputs, thus delivering\nperformance-enhanced, self-explainable, and user-friendly VCRSs. Our study aims\nto establish the foundation and motivate further pioneering research in the\nemerging field of VCRSs. This aligns with the principles of explainable AI and\nAI for social good, viz., utilizing technology's potential to create a fair,\nsustainable, and just world.", "published": "2023-06-14 03:17:02", "link": "http://arxiv.org/abs/2306.08219v1", "categories": ["cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "Feature Normalization for Fine-tuning Self-Supervised Models in Speech\n  Enhancement", "abstract": "Large, pre-trained representation models trained using self-supervised\nlearning have gained popularity in various fields of machine learning because\nthey are able to extract high-quality salient features from input data. As\nsuch, they have been frequently used as base networks for various pattern\nclassification tasks such as speech recognition. However, not much research has\nbeen conducted on applying these types of models to the field of speech signal\ngeneration. In this paper, we investigate the feasibility of using pre-trained\nspeech representation models for a downstream speech enhancement task. To\nalleviate mismatches between the input features of the pre-trained model and\nthe target enhancement model, we adopt a novel feature normalization technique\nto smoothly link these modules together. Our proposed method enables\nsignificant improvements in speech quality compared to baselines when combined\nwith various types of pre-trained speech models.", "published": "2023-06-14 10:03:33", "link": "http://arxiv.org/abs/2306.08406v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "BRUDEX Database: Binaural Room Impulse Responses with Uniformly\n  Distributed External Microphones", "abstract": "There is an emerging need for comparable data for multi-microphone\nprocessing, particularly in acoustic sensor networks. However, commonly\navailable databases are often limited in the spatial diversity of the\nmicrophones or only allow for particular signal processing tasks. In this\npaper, we present a database of acoustic impulse responses and recordings for a\nbinaural hearing aid setup, 36 spatially distributed microphones spanning a\nuniform grid of (5x5) m^2 and 12 source positions. This database can be used\nfor a variety of signal processing tasks, such as (multi-microphone) noise\nreduction, source localization, and dereverberation, as the measurements were\nperformed using the same setup for three different reverberation conditions\n(T_60\\approx{310, 510, 1300} ms). The usability of the database is demonstrated\nfor a noise reduction task using a minimum variance distortionless response\nbeamformer based on relative transfer functions, exploiting the availability of\nspatially distributed microphones.", "published": "2023-06-14 12:59:42", "link": "http://arxiv.org/abs/2306.08484v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Permutation Invariant Recurrent Neural Networks for Sound Source\n  Tracking Applications", "abstract": "Many multi-source localization and tracking models based on neural networks\nuse one or several recurrent layers at their final stages to track the movement\nof the sources. Conventional recurrent neural networks (RNNs), such as the long\nshort-term memories (LSTMs) or the gated recurrent units (GRUs), take a vector\nas their input and use another vector to store their state. However, this\napproach results in the information from all the sources being contained in a\nsingle ordered vector, which is not optimal for permutation-invariant problems\nsuch as multi-source tracking. In this paper, we present a new recurrent\narchitecture that uses unordered sets to represent both its input and its state\nand that is invariant to the permutations of the input set and equivariant to\nthe permutations of the state set. Hence, the information of every sound source\nis represented in an individual embedding and the new estimates are assigned to\nthe tracked trajectories regardless of their order.", "published": "2023-06-14 13:53:31", "link": "http://arxiv.org/abs/2306.08510v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Variance-Preserving-Based Interpolation Diffusion Models for Speech\n  Enhancement", "abstract": "The goal of this study is to implement diffusion models for speech\nenhancement (SE). The first step is to emphasize the theoretical foundation of\nvariance-preserving (VP)-based interpolation diffusion under continuous\nconditions. Subsequently, we present a more concise framework that encapsulates\nboth the VP- and variance-exploding (VE)-based interpolation diffusion methods.\nWe demonstrate that these two methods are special cases of the proposed\nframework. Additionally, we provide a practical example of VP-based\ninterpolation diffusion for the SE task. To improve performance and ease model\ntraining, we analyze the common difficulties encountered in diffusion models\nand suggest amenable hyper-parameters. Finally, we evaluate our model against\nseveral methods using a public benchmark to showcase the effectiveness of our\napproach", "published": "2023-06-14 14:22:22", "link": "http://arxiv.org/abs/2306.08527v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Anticipatory Music Transformer", "abstract": "We introduce anticipation: a method for constructing a controllable\ngenerative model of a temporal point process (the event process) conditioned\nasynchronously on realizations of a second, correlated process (the control\nprocess). We achieve this by interleaving sequences of events and controls,\nsuch that controls appear following stopping times in the event sequence. This\nwork is motivated by problems arising in the control of symbolic music\ngeneration. We focus on infilling control tasks, whereby the controls are a\nsubset of the events themselves, and conditional generation completes a\nsequence of events given the fixed control events. We train anticipatory\ninfilling models using the large and diverse Lakh MIDI music dataset. These\nmodels match the performance of autoregressive models for prompted music\ngeneration, with the additional capability to perform infilling control tasks,\nincluding accompaniment. Human evaluators report that an anticipatory model\nproduces accompaniments with similar musicality to even music composed by\nhumans over a 20-second clip.", "published": "2023-06-14 16:27:53", "link": "http://arxiv.org/abs/2306.08620v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
