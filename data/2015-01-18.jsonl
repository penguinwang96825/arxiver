{"title": "Phrase Based Language Model For Statistical Machine Translation", "abstract": "We consider phrase based Language Models (LM), which generalize the commonly\nused word level models. Similar concept on phrase based LMs appears in speech\nrecognition, which is rather specialized and thus less suitable for machine\ntranslation (MT). In contrast to the dependency LM, we first introduce the\nexhaustive phrase-based LMs tailored for MT use. Preliminary experimental\nresults show that our approach outperform word based LMs with the respect to\nperplexity and translation quality.", "published": "2015-01-18 16:37:53", "link": "http://arxiv.org/abs/1501.04324v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Belief Nets for Topic Modeling", "abstract": "Applying traditional collaborative filtering to digital publishing is\nchallenging because user data is very sparse due to the high volume of\ndocuments relative to the number of users. Content based approaches, on the\nother hand, is attractive because textual content is often very informative. In\nthis paper we describe large-scale content based collaborative filtering for\ndigital publishing. To solve the digital publishing recommender problem we\ncompare two approaches: latent Dirichlet allocation (LDA) and deep belief nets\n(DBN) that both find low-dimensional latent representations for documents.\nEfficient retrieval can be carried out in the latent representation. We work\nboth on public benchmarks and digital media content provided by Issuu, an\nonline publishing platform. This article also comes with a newly developed deep\nbelief nets toolbox for topic modeling tailored towards performance evaluation\nof the DBN model and comparisons to the LDA model.", "published": "2015-01-18 17:12:59", "link": "http://arxiv.org/abs/1501.04325v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Mathematical Language Processing: Automatic Grading and Feedback for\n  Open Response Mathematical Questions", "abstract": "While computer and communication technologies have provided effective means\nto scale up many aspects of education, the submission and grading of\nassessments such as homework assignments and tests remains a weak link. In this\npaper, we study the problem of automatically grading the kinds of open response\nmathematical questions that figure prominently in STEM (science, technology,\nengineering, and mathematics) courses. Our data-driven framework for\nmathematical language processing (MLP) leverages solution data from a large\nnumber of learners to evaluate the correctness of their solutions, assign\npartial-credit scores, and provide feedback to each learner on the likely\nlocations of any errors. MLP takes inspiration from the success of natural\nlanguage processing for text data and comprises three main steps. First, we\nconvert each solution to an open response mathematical question into a series\nof numerical features. Second, we cluster the features from several solutions\nto uncover the structures of correct, partially correct, and incorrect\nsolutions. We develop two different clustering approaches, one that leverages\ngeneric clustering algorithms and one based on Bayesian nonparametrics. Third,\nwe automatically grade the remaining (potentially large number of) solutions\nbased on their assigned cluster and one instructor-provided grade per cluster.\nAs a bonus, we can track the cluster assignment of each step of a multistep\nsolution and determine when it departs from a cluster of correct solutions,\nwhich enables us to indicate the likely locations of errors to learners. We\ntest and validate MLP on real-world MOOC data to demonstrate how it can\nsubstantially reduce the human effort required in large-scale educational\nplatforms.", "published": "2015-01-18 20:50:39", "link": "http://arxiv.org/abs/1501.04346v1", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
