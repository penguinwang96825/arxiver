{"title": "Phrase-based Image Captioning", "abstract": "Generating a novel textual description of an image is an interesting problem\nthat connects computer vision and natural language processing. In this paper,\nwe present a simple model that is able to generate descriptive sentences given\na sample image. This model has a strong focus on the syntax of the\ndescriptions. We train a purely bilinear model that learns a metric between an\nimage representation (generated from a previously trained Convolutional Neural\nNetwork) and phrases that are used to described them. The system is then able\nto infer phrases from a given image sample. Based on caption syntax statistics,\nwe propose a simple language model that can produce relevant descriptions for a\ngiven test image using the phrases inferred. Our approach, which is\nconsiderably simpler than state-of-the-art models, achieves comparable results\nin two popular datasets for the task: Flickr30k and the recently proposed\nMicrosoft COCO.", "published": "2015-02-12 14:17:15", "link": "http://arxiv.org/abs/1502.03671v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A new hybrid metric for verifying parallel corpora of Arabic-English", "abstract": "This paper discusses a new metric that has been applied to verify the quality\nin translation between sentence pairs in parallel corpora of Arabic-English.\nThis metric combines two techniques, one based on sentence length and the other\nbased on compression code length. Experiments on sample test parallel\nArabic-English corpora indicate the combination of these two techniques\nimproves accuracy of the identification of satisfactory and unsatisfactory\nsentence pairs compared to sentence length and compression code length alone.\nThe new method proposed in this research is effective at filtering noise and\nreducing mis-translations resulting in greatly improved quality.", "published": "2015-02-12 17:49:45", "link": "http://arxiv.org/abs/1502.03752v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Latent Variable Model Approach to PMI-based Word Embeddings", "abstract": "Semantic word embeddings represent the meaning of a word via a vector, and\nare created by diverse methods. Many use nonlinear operations on co-occurrence\nstatistics, and have hand-tuned hyperparameters and reweighting methods.\n  This paper proposes a new generative model, a dynamic version of the\nlog-linear topic model of~\\citet{mnih2007three}. The methodological novelty is\nto use the prior to compute closed form expressions for word statistics. This\nprovides a theoretical justification for nonlinear models like PMI, word2vec,\nand GloVe, as well as some hyperparameter choices. It also helps explain why\nlow-dimensional semantic embeddings contain linear algebraic structure that\nallows solution of word analogies, as shown by~\\citet{mikolov2013efficient} and\nmany subsequent papers.\n  Experimental support is provided for the generative model assumptions, the\nmost important of which is that latent word vectors are fairly uniformly\ndispersed in space.", "published": "2015-02-12 02:50:08", "link": "http://arxiv.org/abs/1502.03520v8", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Ordering-sensitive and Semantic-aware Topic Modeling", "abstract": "Topic modeling of textual corpora is an important and challenging problem. In\nmost previous work, the \"bag-of-words\" assumption is usually made which ignores\nthe ordering of words. This assumption simplifies the computation, but it\nunrealistically loses the ordering information and the semantic of words in the\ncontext. In this paper, we present a Gaussian Mixture Neural Topic Model\n(GMNTM) which incorporates both the ordering of words and the semantic meaning\nof sentences into topic modeling. Specifically, we represent each topic as a\ncluster of multi-dimensional vectors and embed the corpus into a collection of\nvectors generated by the Gaussian mixture model. Each word is affected not only\nby its topic, but also by the embedding vector of its surrounding words and the\ncontext. The Gaussian mixture components and the topic of documents, sentences\nand words can be learnt jointly. Extensive experiments show that our model can\nlearn better topics and more accurate word distributions for each topic.\nQuantitatively, comparing to state-of-the-art topic modeling approaches, GMNTM\nobtains significantly better performance in terms of perplexity, retrieval\naccuracy and classification accuracy.", "published": "2015-02-12 12:32:39", "link": "http://arxiv.org/abs/1502.03630v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Applying deep learning techniques on medical corpora from the World Wide\n  Web: a prototypical system and evaluation", "abstract": "BACKGROUND: The amount of biomedical literature is rapidly growing and it is\nbecoming increasingly difficult to keep manually curated knowledge bases and\nontologies up-to-date. In this study we applied the word2vec deep learning\ntoolkit to medical corpora to test its potential for identifying relationships\nfrom unstructured text. We evaluated the efficiency of word2vec in identifying\nproperties of pharmaceuticals based on mid-sized, unstructured medical text\ncorpora available on the web. Properties included relationships to diseases\n('may treat') or physiological processes ('has physiological effect'). We\ncompared the relationships identified by word2vec with manually curated\ninformation from the National Drug File - Reference Terminology (NDF-RT)\nontology as a gold standard. RESULTS: Our results revealed a maximum accuracy\nof 49.28% which suggests a limited ability of word2vec to capture linguistic\nregularities on the collected medical corpora compared with other published\nresults. We were able to document the influence of different parameter settings\non result accuracy and found and unexpected trade-off between ranking quality\nand accuracy. Pre-processing corpora to reduce syntactic variability proved to\nbe a good strategy for increasing the utility of the trained vector models.\nCONCLUSIONS: Word2vec is a very efficient implementation for computing vector\nrepresentations and for its ability to identify relationships in textual data\nwithout any prior domain knowledge. We found that the ranking and retrieved\nresults generated by word2vec were not of sufficient quality for automatic\npopulation of knowledge bases and ontologies, but could serve as a starting\npoint for further manual curation.", "published": "2015-02-12 14:44:15", "link": "http://arxiv.org/abs/1502.03682v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
