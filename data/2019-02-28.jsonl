{"title": "Reinforcement Learning based Curriculum Optimization for Neural Machine\n  Translation", "abstract": "We consider the problem of making efficient use of heterogeneous training\ndata in neural machine translation (NMT). Specifically, given a training\ndataset with a sentence-level feature such as noise, we seek an optimal\ncurriculum, or order for presenting examples to the system during training. Our\ncurriculum framework allows examples to appear an arbitrary number of times,\nand thus generalizes data weighting, filtering, and fine-tuning schemes. Rather\nthan relying on prior knowledge to design a curriculum, we use reinforcement\nlearning to learn one automatically, jointly with the NMT system, in the course\nof a single training run. We show that this approach can beat uniform and\nfiltering baselines on Paracrawl and WMT English-to-French datasets by up to\n+3.4 BLEU, and match the performance of a hand-designed, state-of-the-art\ncurriculum.", "published": "2019-02-28 19:35:09", "link": "http://arxiv.org/abs/1903.00041v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Massively Multilingual Neural Machine Translation", "abstract": "Multilingual neural machine translation (NMT) enables training a single model\nthat supports translation from multiple source languages into multiple target\nlanguages. In this paper, we push the limits of multilingual NMT in terms of\nnumber of languages being used. We perform extensive experiments in training\nmassively multilingual NMT models, translating up to 102 languages to and from\nEnglish within a single model. We explore different setups for training such\nmodels and analyze the trade-offs between translation quality and various\nmodeling decisions. We report results on the publicly available TED talks\nmultilingual corpus where we show that massively multilingual many-to-many\nmodels are effective in low resource settings, outperforming the previous\nstate-of-the-art while supporting up to 59 languages. Our experiments on a\nlarge-scale dataset with 102 languages to and from English and up to one\nmillion examples per direction also show promising results, surpassing strong\nbilingual baselines and encouraging future work on massively multilingual NMT.", "published": "2019-02-28 22:26:12", "link": "http://arxiv.org/abs/1903.00089v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT for Joint Intent Classification and Slot Filling", "abstract": "Intent classification and slot filling are two essential tasks for natural\nlanguage understanding. They often suffer from small-scale human-labeled\ntraining data, resulting in poor generalization capability, especially for rare\nwords. Recently a new language representation model, BERT (Bidirectional\nEncoder Representations from Transformers), facilitates pre-training deep\nbidirectional representations on large-scale unlabeled corpora, and has created\nstate-of-the-art models for a wide variety of natural language processing tasks\nafter simple fine-tuning. However, there has not been much effort on exploring\nBERT for natural language understanding. In this work, we propose a joint\nintent classification and slot filling model based on BERT. Experimental\nresults demonstrate that our proposed model achieves significant improvement on\nintent classification accuracy, slot filling F1, and sentence-level semantic\nframe accuracy on several public benchmark datasets, compared to the\nattention-based recurrent neural network models and slot-gated models.", "published": "2019-02-28 05:54:16", "link": "http://arxiv.org/abs/1902.10909v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Better, Faster, Stronger Sequence Tagging Constituent Parsers", "abstract": "Sequence tagging models for constituent parsing are faster, but less accurate\nthan other types of parsers. In this work, we address the following weaknesses\nof such constituent parsers: (a) high error rates around closing brackets of\nlong constituents, (b) large label sets, leading to sparsity, and (c) error\npropagation arising from greedy decoding. To effectively close brackets, we\ntrain a model that learns to switch between tagging schemes. To reduce\nsparsity, we decompose the label set and use multi-task learning to jointly\nlearn to predict sublabels. Finally, we mitigate issues from greedy decoding\nthrough auxiliary losses and sentence-level fine-tuning with policy gradient.\nCombining these techniques, we clearly surpass the performance of sequence\ntagging constituent parsers on the English and Chinese Penn Treebanks, and\nreduce their parsing time even further. On the SPMRL datasets, we observe even\ngreater improvements across the board, including a new state of the art on\nBasque, Hebrew, Polish and Swedish.", "published": "2019-02-28 10:06:20", "link": "http://arxiv.org/abs/1902.10985v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Rewards for Question Generation Models", "abstract": "Recent approaches to question generation have used modifications to a Seq2Seq\narchitecture inspired by advances in machine translation. Models are trained\nusing teacher forcing to optimise only the one-step-ahead prediction. However,\nat test time, the model is asked to generate a whole sequence, causing errors\nto propagate through the generation process (exposure bias). A number of\nauthors have proposed countering this bias by optimising for a reward that is\nless tightly coupled to the training data, using reinforcement learning. We\noptimise directly for quality metrics, including a novel approach using a\ndiscriminator learned directly from the training data. We confirm that policy\ngradient methods can be used to decouple training from the ground truth,\nleading to increases in the metrics used as rewards. We perform a human\nevaluation, and show that although these metrics have previously been assumed\nto be good proxies for question quality, they are poorly aligned with human\njudgement and the model simply learns to exploit the weaknesses of the reward\nsource.", "published": "2019-02-28 12:33:17", "link": "http://arxiv.org/abs/1902.11049v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-aware Neural-based Dialog Act Classification on Automatically\n  Generated Transcriptions", "abstract": "This paper presents our latest investigations on dialog act (DA)\nclassification on automatically generated transcriptions. We propose a novel\napproach that combines convolutional neural networks (CNNs) and conditional\nrandom fields (CRFs) for context modeling in DA classification. We explore the\nimpact of transcriptions generated from different automatic speech recognition\nsystems such as hybrid TDNN/HMM and End-to-End systems on the final\nperformance. Experimental results on two benchmark datasets (MRDA and SwDA)\nshow that the combination CNN and CRF improves consistently the accuracy.\nFurthermore, they show that although the word error rates are comparable,\nEnd-to-End ASR system seems to be more suitable for DA classification.", "published": "2019-02-28 12:55:31", "link": "http://arxiv.org/abs/1902.11060v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Training for Satire Detection: Controlling for Confounding\n  Variables", "abstract": "The automatic detection of satire vs. regular news is relevant for downstream\napplications (for instance, knowledge base population) and to improve the\nunderstanding of linguistic characteristics of satire. Recent approaches build\nupon corpora which have been labeled automatically based on article sources. We\nhypothesize that this encourages the models to learn characteristics for\ndifferent publication sources (e.g., \"The Onion\" vs. \"The Guardian\") rather\nthan characteristics of satire, leading to poor generalization performance to\nunseen publication sources. We therefore propose a novel model for satire\ndetection with an adversarial component to control for the confounding variable\nof publication source. On a large novel data set collected from German news\n(which we make available to the research community), we observe comparable\nsatire classification performance and, as desired, a considerable drop in\npublication classification performance with adversarial training. Our analysis\nshows that the adversarial component is crucial for the model to learn to pay\nattention to linguistic properties of satire.", "published": "2019-02-28 15:20:41", "link": "http://arxiv.org/abs/1902.11145v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FastFusionNet: New State-of-the-Art for DAWNBench SQuAD", "abstract": "In this technical report, we introduce FastFusionNet, an efficient variant of\nFusionNet [12]. FusionNet is a high performing reading comprehension\narchitecture, which was designed primarily for maximum retrieval accuracy with\nless regard towards computational requirements. For FastFusionNets we remove\nthe expensive CoVe layers [21] and substitute the BiLSTMs with far more\nefficient SRU layers [19]. The resulting architecture obtains state-of-the-art\nresults on DAWNBench [5] while achieving the lowest training and inference time\non SQuAD [25] to-date. The code is available at\nhttps://github.com/felixgwu/FastFusionNet.", "published": "2019-02-28 18:49:10", "link": "http://arxiv.org/abs/1902.11291v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Jointly Optimizing Diversity and Relevance in Neural Response Generation", "abstract": "Although recent neural conversation models have shown great potential, they\noften generate bland and generic responses. While various approaches have been\nexplored to diversify the output of the conversation model, the improvement\noften comes at the cost of decreased relevance. In this paper, we propose a\nSpaceFusion model to jointly optimize diversity and relevance that essentially\nfuses the latent space of a sequence-to-sequence model and that of an\nautoencoder model by leveraging novel regularization terms. As a result, our\napproach induces a latent space in which the distance and direction from the\npredicted response vector roughly match the relevance and diversity,\nrespectively. This property also lends itself well to an intuitive\nvisualization of the latent space. Both automatic and human evaluation results\ndemonstrate that the proposed approach brings significant improvement compared\nto strong baselines in both diversity and relevance.", "published": "2019-02-28 16:45:19", "link": "http://arxiv.org/abs/1902.11205v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Contextual Representation Learning Without Softmax Layer", "abstract": "Contextual representation models have achieved great success in improving\nvarious downstream tasks. However, these language-model-based encoders are\ndifficult to train due to the large parameter sizes and high computational\ncomplexity. By carefully examining the training procedure, we find that the\nsoftmax layer (the output layer) causes significant inefficiency due to the\nlarge vocabulary size. Therefore, we redesign the learning objective and\npropose an efficient framework for training contextual representation models.\nSpecifically, the proposed approach bypasses the softmax layer by performing\nlanguage modeling with dimension reduction, and allows the models to leverage\npre-trained word embeddings. Our framework reduces the time spent on the output\nlayer to a negligible level, eliminates almost all the trainable parameters of\nthe softmax layer and performs language modeling without truncating the\nvocabulary. When applied to ELMo, our method achieves a 4 times speedup and\neliminates 80% trainable parameters while achieving competitive performance on\ndownstream tasks.", "published": "2019-02-28 18:19:14", "link": "http://arxiv.org/abs/1902.11269v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Non-Parametric Adaptation for Neural Machine Translation", "abstract": "Neural Networks trained with gradient descent are known to be susceptible to\ncatastrophic forgetting caused by parameter shift during the training process.\nIn the context of Neural Machine Translation (NMT) this results in poor\nperformance on heterogeneous datasets and on sub-tasks like rare phrase\ntranslation. On the other hand, non-parametric approaches are immune to\nforgetting, perfectly complementing the generalization ability of NMT. However,\nattempts to combine non-parametric or retrieval based approaches with NMT have\nonly been successful on narrow domains, possibly due to over-reliance on\nsentence level retrieval. We propose a novel n-gram level retrieval approach\nthat relies on local phrase level similarities, allowing us to retrieve\nneighbors that are useful for translation even when overall sentence similarity\nis low. We complement this with an expressive neural network, allowing our\nmodel to extract information from the noisy retrieved context. We evaluate our\nsemi-parametric NMT approach on a heterogeneous dataset composed of WMT, IWSLT,\nJRC-Acquis and OpenSubtitles, and demonstrate gains on all 4 evaluation sets.\nThe semi-parametric nature of our approach opens the door for non-parametric\ndomain adaptation, demonstrating strong inference-time adaptation performance\non new domains without the need for any parameter updates.", "published": "2019-02-28 20:33:00", "link": "http://arxiv.org/abs/1903.00058v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Global Vectors for Node Representations", "abstract": "Most network embedding algorithms consist in measuring co-occurrences of\nnodes via random walks then learning the embeddings using Skip-Gram with\nNegative Sampling. While it has proven to be a relevant choice, there are\nalternatives, such as GloVe, which has not been investigated yet for network\nembedding. Even though SGNS better handles non co-occurrence than GloVe, it has\na worse time-complexity. In this paper, we propose a matrix factorization\napproach for network embedding, inspired by GloVe, that better handles non\nco-occurrence with a competitive time-complexity. We also show how to extend\nthis model to deal with networks where nodes are documents, by simultaneously\nlearning word, node and document representations. Quantitative evaluations show\nthat our model achieves state-of-the-art performance, while not being so\nsensitive to the choice of hyper-parameters. Qualitatively speaking, we show\nhow our model helps exploring a network of documents by generating\ncomplementary network-oriented and content-oriented keywords.", "published": "2019-02-28 10:46:54", "link": "http://arxiv.org/abs/1902.11004v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Link Prediction with Mutual Attention for Text-Attributed Networks", "abstract": "In this extended abstract, we present an algorithm that learns a similarity\nmeasure between documents from the network topology of a structured corpus. We\nleverage the Scaled Dot-Product Attention, a recently proposed attention\nmechanism, to design a mutual attention mechanism between pairs of documents.\nTo train its parameters, we use the network links as supervision. We provide\npreliminary experiment results with a citation dataset on two prediction tasks,\ndemonstrating the capacity of our model to learn a meaningful textual\nsimilarity.", "published": "2019-02-28 12:45:42", "link": "http://arxiv.org/abs/1902.11054v2", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Representation Learning for Recommender Systems with Application to the\n  Scientific Literature", "abstract": "The scientific literature is a large information network linking various\nactors (laboratories, companies, institutions, etc.). The vast amount of data\ngenerated by this network constitutes a dynamic heterogeneous attributed\nnetwork (HAN), in which new information is constantly produced and from which\nit is increasingly difficult to extract content of interest. In this article, I\npresent my first thesis works in partnership with an industrial company,\nDigital Scientific Research Technology. This later offers a scientific watch\ntool, Peerus, addressing various issues, such as the real time recommendation\nof newly published papers or the search for active experts to start new\ncollaborations. To tackle this diversity of applications, a common approach\nconsists in learning representations of the nodes and attributes of this HAN\nand use them as features for a variety of recommendation tasks. However, most\nworks on attributed network embedding pay too little attention to textual\nattributes and do not fully take advantage of recent natural language\nprocessing techniques. Moreover, proposed methods that jointly learn node and\ndocument representations do not provide a way to effectively infer\nrepresentations for new documents for which network information is missing,\nwhich happens to be crucial in real time recommender systems. Finally, the\ninterplay between textual and graph data in text-attributed heterogeneous\nnetworks remains an open research direction.", "published": "2019-02-28 12:53:38", "link": "http://arxiv.org/abs/1902.11058v1", "categories": ["cs.CL", "cs.DL", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Citation Needed: A Taxonomy and Algorithmic Assessment of Wikipedia's\n  Verifiability", "abstract": "Wikipedia is playing an increasingly central role on the web,and the policies\nits contributors follow when sourcing and fact-checking content affect million\nof readers. Among these core guiding principles, verifiability policies have a\nparticularly important role. Verifiability requires that information included\nin a Wikipedia article be corroborated against reliable secondary sources.\nBecause of the manual labor needed to curate and fact-check Wikipedia at scale,\nhowever, its contents do not always evenly comply with these policies.\nCitations (i.e. reference to external sources) may not conform to verifiability\nrequirements or may be missing altogether, potentially weakening the\nreliability of specific topic areas of the free encyclopedia. In this paper, we\naim to provide an empirical characterization of the reasons why and how\nWikipedia cites external sources to comply with its own verifiability\nguidelines. First, we construct a taxonomy of reasons why inline citations are\nrequired by collecting labeled data from editors of multiple Wikipedia language\neditions. We then collect a large-scale crowdsourced dataset of Wikipedia\nsentences annotated with categories derived from this taxonomy. Finally, we\ndesign and evaluate algorithmic models to determine if a statement requires a\ncitation, and to predict the citation reason based on our taxonomy. We evaluate\nthe robustness of such models across different classes of Wikipedia articles of\nvarying quality, as well as on an additional dataset of claims annotated for\nfact-checking purposes.", "published": "2019-02-28 14:50:59", "link": "http://arxiv.org/abs/1902.11116v1", "categories": ["cs.CY", "cs.CL", "cs.DL"], "primary_category": "cs.CY"}
{"title": "Incorporating End-to-End Speech Recognition Models for Sentiment\n  Analysis", "abstract": "Previous work on emotion recognition demonstrated a synergistic effect of\ncombining several modalities such as auditory, visual, and transcribed text to\nestimate the affective state of a speaker. Among these, the linguistic modality\nis crucial for the evaluation of an expressed emotion. However, manually\ntranscribed spoken text cannot be given as input to a system practically. We\nargue that using ground-truth transcriptions during training and evaluation\nphases leads to a significant discrepancy in performance compared to real-world\nconditions, as the spoken text has to be recognized on the fly and can contain\nspeech recognition mistakes. In this paper, we propose a method of integrating\nan automatic speech recognition (ASR) output with a character-level recurrent\nneural network for sentiment recognition. In addition, we conduct several\nexperiments investigating sentiment recognition for human-robot interaction in\na noise-realistic scenario which is challenging for the ASR systems. We\nquantify the improvement compared to using only the acoustic modality in\nsentiment recognition. We demonstrate the effectiveness of this approach on the\nMultimodal Corpus of Sentiment Intensity (MOSI) by achieving 73,6% accuracy in\na binary sentiment classification task, exceeding previously reported results\nthat use only acoustic input. In addition, we set a new state-of-the-art\nperformance on the MOSI dataset (80.4% accuracy, 2% absolute improvement).", "published": "2019-02-28 17:48:20", "link": "http://arxiv.org/abs/1902.11245v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "From Visual to Acoustic Question Answering", "abstract": "We introduce the new task of Acoustic Question Answering (AQA) to promote\nresearch in acoustic reasoning. The AQA task consists of analyzing an acoustic\nscene composed by a combination of elementary sounds and answering questions\nthat relate the position and properties of these sounds. The kind of relational\nquestions asked, require that the models perform non-trivial reasoning in order\nto answer correctly. Although similar problems have been extensively studied in\nthe domain of visual reasoning, we are not aware of any previous studies\naddressing the problem in the acoustic domain. We propose a method for\ngenerating the acoustic scenes from elementary sounds and a number of relevant\nquestions for each scene using templates. We also present preliminary results\nobtained with two models (FiLM and MAC) that have been shown to work for visual\nreasoning.", "published": "2019-02-28 18:35:45", "link": "http://arxiv.org/abs/1902.11280v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
