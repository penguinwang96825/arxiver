{"title": "InSRL: A Multi-view Learning Framework Fusing Multiple Information\n  Sources for Distantly-supervised Relation Extraction", "abstract": "Distant supervision makes it possible to automatically label bags of\nsentences for relation extraction by leveraging knowledge bases, but suffers\nfrom the sparse and noisy bag issues. Additional information sources are\nurgently needed to supplement the training data and overcome these issues. In\nthis paper, we introduce two widely-existing sources in knowledge bases, namely\nentity descriptions, and multi-grained entity types to enrich the distantly\nsupervised data. We see information sources as multiple views and fusing them\nto construct an intact space with sufficient information. An end-to-end\nmulti-view learning framework is proposed for relation extraction via Intact\nSpace Representation Learning (InSRL), and the representations of single views\nare jointly learned simultaneously. Moreover, inner-view and cross-view\nattention mechanisms are used to highlight important information on different\nlevels on an entity-pair basis. The experimental results on a popular benchmark\ndataset demonstrate the necessity of additional information sources and the\neffectiveness of our framework. We will release the implementation of our model\nand dataset with multiple information sources after the anonymized review\nphase.", "published": "2020-12-17 02:49:46", "link": "http://arxiv.org/abs/2012.09370v1", "categories": ["cs.CL", "68-06", "H.0"], "primary_category": "cs.CL"}
{"title": "Interactive Question Clarification in Dialogue via Reinforcement\n  Learning", "abstract": "Coping with ambiguous questions has been a perennial problem in real-world\ndialogue systems. Although clarification by asking questions is a common form\nof human interaction, it is hard to define appropriate questions to elicit more\nspecific intents from a user. In this work, we propose a reinforcement model to\nclarify ambiguous questions by suggesting refinements of the original query. We\nfirst formulate a collection partitioning problem to select a set of labels\nenabling us to distinguish potential unambiguous intents. We list the chosen\nlabels as intent phrases to the user for further confirmation. The selected\nlabel along with the original user query then serves as a refined query, for\nwhich a suitable response can more easily be identified. The model is trained\nusing reinforcement learning with a deep policy network. We evaluate our model\nbased on real-world user clicks and demonstrate significant improvements across\nseveral different experiments.", "published": "2020-12-17 06:38:04", "link": "http://arxiv.org/abs/2012.09411v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ultra-Fast, Low-Storage, Highly Effective Coarse-grained Selection in\n  Retrieval-based Chatbot by Using Deep Semantic Hashing", "abstract": "We study the coarse-grained selection module in retrieval-based chatbot.\nCoarse-grained selection is a basic module in a retrieval-based chatbot, which\nconstructs a rough candidate set from the whole database to speed up the\ninteraction with customers. So far, there are two kinds of approaches for\ncoarse-grained selection module: (1) sparse representation; (2) dense\nrepresentation. To the best of our knowledge, there is no systematic comparison\nbetween these two approaches in retrieval-based chatbots, and which kind of\nmethod is better in real scenarios is still an open question. In this paper, we\nfirst systematically compare these two methods from four aspects: (1)\neffectiveness; (2) index stoarge; (3) search time cost; (4) human evaluation.\nExtensive experiment results demonstrate that dense representation method\nsignificantly outperforms the sparse representation, but costs more time and\nstorage occupation. In order to overcome these fatal weaknesses of dense\nrepresentation method, we propose an ultra-fast, low-storage, and highly\neffective Deep Semantic Hashing Coarse-grained selection method, called DSHC\nmodel. Specifically, in our proposed DSHC model, a hashing optimizing module\nthat consists of two autoencoder models is stacked on a trained dense\nrepresentation model, and three loss functions are designed to optimize it. The\nhash codes provided by hashing optimizing module effectively preserve the rich\nsemantic and similarity information in dense vectors. Extensive experiment\nresults prove that, our proposed DSHC model can achieve much faster speed and\nlower storage than sparse representation, with limited performance loss\ncompared with dense representation. Besides, our source codes have been\npublicly released for future research.", "published": "2020-12-17 14:54:59", "link": "http://arxiv.org/abs/2012.09647v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hate Speech detection in the Bengali language: A dataset and its\n  baseline evaluation", "abstract": "Social media sites such as YouTube and Facebook have become an integral part\nof everyone's life and in the last few years, hate speech in the social media\ncomment section has increased rapidly. Detection of hate speech on social media\nwebsites faces a variety of challenges including small imbalanced data sets,\nthe findings of an appropriate model and also the choice of feature analysis\nmethod. further more, this problem is more severe for the Bengali speaking\ncommunity due to the lack of gold standard labelled datasets. This paper\npresents a new dataset of 30,000 user comments tagged by crowd sourcing and\nvarified by experts. All the comments are collected from YouTube and Facebook\ncomment section and classified into seven categories: sports, entertainment,\nreligion, politics, crime, celebrity and TikTok & meme. A total of 50\nannotators annotated each comment three times and the majority vote was taken\nas the final annotation. Nevertheless, we have conducted base line experiments\nand several deep learning models along with extensive pre-trained Bengali word\nembedding such as Word2Vec, FastText and BengFastText on this dataset to\nfacilitate future research opportunities. The experiment illustrated that\nalthough all deep learning models performed well, SVM achieved the best result\nwith 87.5% accuracy. Our core contribution is to make this benchmark dataset\navailable and accessible to facilitate further research in the field of in the\nfield of Bengali hate speech detection.", "published": "2020-12-17 15:53:54", "link": "http://arxiv.org/abs/2012.09686v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Five Psycholinguistic Characteristics for Better Interaction with Users", "abstract": "When two people pay attention to each other and are interested in what the\nother has to say or write, they almost instantly adapt their writing/speaking\nstyle to match the other. For a successful interaction with a user, chatbots\nand dialogue systems should be able to do the same. We propose a framework\nconsisting of five psycholinguistic textual characteristics for better\nhuman-computer interaction. We describe the annotation processes used for\ncollecting the data, and benchmark five binary classification tasks,\nexperimenting with different training sizes and model architectures. The best\narchitectures noticeably outperform several baselines and achieve\nmacro-averaged F$_1$-scores between 72\\% and 96\\% depending on the language and\nthe task. The proposed framework proved to be fairly easy to model for various\nlanguages even with small amount of manually annotated data if right\narchitectures are used.", "published": "2020-12-17 16:00:08", "link": "http://arxiv.org/abs/2012.09692v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MIX : a Multi-task Learning Approach to Solve Open-Domain Question\n  Answering", "abstract": "This paper introduces MIX, a multi-task deep learning approach to solve\nopen-ended question-answering. First, we design our system as a multi-stage\npipeline of 3 building blocks: a BM25-based Retriever to reduce the search\nspace, a RoBERTa-based Scorer, and an Extractor to rank retrieved paragraphs\nand extract relevant text spans, respectively. Eventually, we further improve\nthe computational efficiency of our system to deal with the scalability\nchallenge: thanks to multi-task learning, we parallelize the close tasks solved\nby the Scorer and the Extractor. Our system is on par with state-of-the-art\nperformances on the squad-open benchmark while being simpler conceptually.", "published": "2020-12-17 17:22:30", "link": "http://arxiv.org/abs/2012.09766v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Literature Retrieval for Precision Medicine with Neural Matching and\n  Faceted Summarization", "abstract": "Information retrieval (IR) for precision medicine (PM) often involves looking\nfor multiple pieces of evidence that characterize a patient case. This\ntypically includes at least the name of a condition and a genetic variation\nthat applies to the patient. Other factors such as demographic attributes,\ncomorbidities, and social determinants may also be pertinent. As such, the\nretrieval problem is often formulated as ad hoc search but with multiple facets\n(e.g., disease, mutation) that may need to be incorporated. In this paper, we\npresent a document reranking approach that combines neural query-document\nmatching and text summarization toward such retrieval scenarios. Our\narchitecture builds on the basic BERT model with three specific components for\nreranking: (a). document-query matching (b). keyword extraction and (c).\nfacet-conditioned abstractive summarization. The outcomes of (b) and (c) are\nused to essentially transform a candidate document into a concise summary that\ncan be compared with the query at hand to compute a relevance score. Component\n(a) directly generates a matching score of a candidate document for a query.\nThe full architecture benefits from the complementary potential of\ndocument-query matching and the novel document transformation approach based on\nsummarization along PM facets. Evaluations using NIST's TREC-PM track datasets\n(2017--2019) show that our model achieves state-of-the-art performance. To\nfoster reproducibility, our code is made available here:\nhttps://github.com/bionlproc/text-summ-for-doc-retrieval.", "published": "2020-12-17 02:01:32", "link": "http://arxiv.org/abs/2012.09355v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "MASKER: Masked Keyword Regularization for Reliable Text Classification", "abstract": "Pre-trained language models have achieved state-of-the-art accuracies on\nvarious text classification tasks, e.g., sentiment analysis, natural language\ninference, and semantic textual similarity. However, the reliability of the\nfine-tuned text classifiers is an often underlooked performance criterion. For\ninstance, one may desire a model that can detect out-of-distribution (OOD)\nsamples (drawn far from training distribution) or be robust against domain\nshifts. We claim that one central obstacle to the reliability is the\nover-reliance of the model on a limited number of keywords, instead of looking\nat the whole context. In particular, we find that (a) OOD samples often contain\nin-distribution keywords, while (b) cross-domain samples may not always contain\nkeywords; over-relying on the keywords can be problematic for both cases. In\nlight of this observation, we propose a simple yet effective fine-tuning\nmethod, coined masked keyword regularization (MASKER), that facilitates\ncontext-based prediction. MASKER regularizes the model to reconstruct the\nkeywords from the rest of the words and make low-confidence predictions without\nenough context. When applied to various pre-trained language models (e.g.,\nBERT, RoBERTa, and ALBERT), we demonstrate that MASKER improves OOD detection\nand cross-domain generalization without degrading classification accuracy. Code\nis available at https://github.com/alinlab/MASKER.", "published": "2020-12-17 04:54:16", "link": "http://arxiv.org/abs/2012.09392v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Unsupervised Learning of Discourse Structures using a Tree Autoencoder", "abstract": "Discourse information, as postulated by popular discourse theories, such as\nRST and PDTB, has been shown to improve an increasing number of downstream NLP\ntasks, showing positive effects and synergies of discourse with important\nreal-world applications. While methods for incorporating discourse become more\nand more sophisticated, the growing need for robust and general discourse\nstructures has not been sufficiently met by current discourse parsers, usually\ntrained on small scale datasets in a strictly limited number of domains. This\nmakes the prediction for arbitrary tasks noisy and unreliable. The overall\nresulting lack of high-quality, high-quantity discourse trees poses a severe\nlimitation to further progress. In order the alleviate this shortcoming, we\npropose a new strategy to generate tree structures in a task-agnostic,\nunsupervised fashion by extending a latent tree induction framework with an\nauto-encoding objective. The proposed approach can be applied to any\ntree-structured objective, such as syntactic parsing, discourse parsing and\nothers. However, due to the especially difficult annotation process to generate\ndiscourse trees, we initially develop a method to generate larger and more\ndiverse discourse treebanks. In this paper we are inferring general tree\nstructures of natural text in multiple domains, showing promising results on a\ndiverse set of tasks.", "published": "2020-12-17 08:40:34", "link": "http://arxiv.org/abs/2012.09446v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ReferentialGym: A Nomenclature and Framework for Language Emergence &\n  Grounding in (Visual) Referential Games", "abstract": "Natural languages are powerful tools wielded by human beings to communicate\ninformation and co-operate towards common goals. Their values lie in some main\nproperties like compositionality, hierarchy and recurrent syntax, which\ncomputational linguists have been researching the emergence of in artificial\nlanguages induced by language games. Only relatively recently, the AI community\nhas started to investigate language emergence and grounding working towards\nbetter human-machine interfaces. For instance, interactive/conversational AI\nassistants that are able to relate their vision to the ongoing conversation.\n  This paper provides two contributions to this research field. Firstly, a\nnomenclature is proposed to understand the main initiatives in studying\nlanguage emergence and grounding, accounting for the variations in assumptions\nand constraints. Secondly, a PyTorch based deep learning framework is\nintroduced, entitled ReferentialGym, which is dedicated to furthering the\nexploration of language emergence and grounding. By providing baseline\nimplementations of major algorithms and metrics, in addition to many different\nfeatures and approaches, ReferentialGym attempts to ease the entry barrier to\nthe field and provide the community with common implementations.", "published": "2020-12-17 10:22:15", "link": "http://arxiv.org/abs/2012.09486v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BERT Goes Shopping: Comparing Distributional Models for Product\n  Representations", "abstract": "Word embeddings (e.g., word2vec) have been applied successfully to eCommerce\nproducts through~\\textit{prod2vec}. Inspired by the recent performance\nimprovements on several NLP tasks brought by contextualized embeddings, we\npropose to transfer BERT-like architectures to eCommerce: our model --\n~\\textit{Prod2BERT} -- is trained to generate representations of products\nthrough masked session modeling. Through extensive experiments over multiple\nshops, different tasks, and a range of design choices, we systematically\ncompare the accuracy of~\\textit{Prod2BERT} and~\\textit{prod2vec} embeddings:\nwhile~\\textit{Prod2BERT} is found to be superior in several scenarios, we\nhighlight the importance of resources and hyperparameters in the best\nperforming models. Finally, we provide guidelines to practitioners for training\nembeddings under a variety of computational and data constraints.", "published": "2020-12-17 18:18:03", "link": "http://arxiv.org/abs/2012.09807v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Can Transformers Reason About Effects of Actions?", "abstract": "A recent work has shown that transformers are able to \"reason\" with facts and\nrules in a limited setting where the rules are natural language expressions of\nconjunctions of conditions implying a conclusion. Since this suggests that\ntransformers may be used for reasoning with knowledge given in natural\nlanguage, we do a rigorous evaluation of this with respect to a common form of\nknowledge and its corresponding reasoning -- the reasoning about effects of\nactions. Reasoning about action and change has been a top focus in the\nknowledge representation subfield of AI from the early days of AI and more\nrecently it has been a highlight aspect in common sense question answering. We\nconsider four action domains (Blocks World, Logistics, Dock-Worker-Robots and a\nGeneric Domain) in natural language and create QA datasets that involve\nreasoning about the effects of actions in these domains. We investigate the\nability of transformers to (a) learn to reason in these domains and (b)\ntransfer that learning from the generic domains to the other domains.", "published": "2020-12-17 21:12:58", "link": "http://arxiv.org/abs/2012.09938v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "COVID-19 Emotion Monitoring as a Tool to Increase Preparedness for\n  Disease Outbreaks in Developing Regions", "abstract": "The COVID-19 pandemic brought many challenges, from hospital-occupation\nmanagement to lock-down mental-health repercussions such as anxiety or\ndepression. In this work, we present a solution for the later problem by\ndeveloping a Twitter emotion-monitor system based on a state-of-the-art\nnatural-language processing model. The system monitors six different emotions\non accounts in cities, as well as politicians and health-authorities Twitter\naccounts. With an anonymous use of the emotion monitor, health authorities and\nprivate health-insurance companies can develop strategies to tackle problems\nsuch as suicide and clinical depression. The model chosen for such a task is a\nBidirectional-Encoder Representations from Transformers (BERT) pre-trained on a\nSpanish corpus (BETO). The model performed well on a validation dataset. The\nsystem is deployed online as part of a web application for simulation and data\nanalysis of COVID-19, in Colombia, available at\nhttps://epidemiologia-matematica.org.", "published": "2020-12-17 12:58:06", "link": "http://arxiv.org/abs/2012.12184v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Assessing COVID-19 Impacts on College Students via Automated Processing\n  of Free-form Text", "abstract": "In this paper, we report experimental results on assessing the impact of\nCOVID-19 on college students by processing free-form texts generated by them.\nBy free-form texts, we mean textual entries posted by college students\n(enrolled in a four year US college) via an app specifically designed to assess\nand improve their mental health. Using a dataset comprising of more than 9000\ntextual entries from 1451 students collected over four months (split between\npre and post COVID-19), and established NLP techniques, a) we assess how topics\nof most interest to student change between pre and post COVID-19, and b) we\nassess the sentiments that students exhibit in each topic between pre and post\nCOVID-19. Our analysis reveals that topics like Education became noticeably\nless important to students post COVID-19, while Health became much more\ntrending. We also found that across all topics, negative sentiment among\nstudents post COVID-19 was much higher compared to pre-COVID-19. We expect our\nstudy to have an impact on policy-makers in higher education across several\nspectra, including college administrators, teachers, parents, and mental health\ncounselors.", "published": "2020-12-17 02:46:48", "link": "http://arxiv.org/abs/2012.09369v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The voice of COVID-19: Acoustic correlates of infection", "abstract": "COVID-19 is a global health crisis that has been affecting many aspects of\nour daily lives throughout the past year. The symptomatology of COVID-19 is\nheterogeneous with a severity continuum. A considerable proportion of symptoms\nare related to pathological changes in the vocal system, leading to the\nassumption that COVID-19 may also affect voice production. For the very first\ntime, the present study aims to investigate voice acoustic correlates of an\ninfection with COVID-19 on the basis of a comprehensive acoustic parameter set.\nWe compare 88 acoustic features extracted from recordings of the vowels /i:/,\n/e:/, /o:/, /u:/, and /a:/ produced by 11 symptomatic COVID-19 positive and 11\nCOVID-19 negative German-speaking participants. We employ the Mann-Whitney U\ntest and calculate effect sizes to identify features with the most prominent\ngroup differences. The mean voiced segment length and the number of voiced\nsegments per second yield the most important differences across all vowels\nindicating discontinuities in the pulmonic airstream during phonation in\nCOVID-19 positive participants. Group differences in the front vowels /i:/ and\n/e:/ are additionally reflected in the variation of the fundamental frequency\nand the harmonics-to-noise ratio, group differences in back vowels /o:/ and\n/u:/ in statistics of the Mel-frequency cepstral coefficients and the spectral\nslope. Findings of this study can be considered an important proof-of-concept\ncontribution for a potential future voice-based identification of individuals\ninfected with COVID-19.", "published": "2020-12-17 10:12:41", "link": "http://arxiv.org/abs/2012.09478v1", "categories": ["cs.SD", "cs.CL", "eess.AS", "68T01", "J.3"], "primary_category": "cs.SD"}
{"title": "The effectiveness of unsupervised subword modeling with autoregressive\n  and cross-lingual phone-aware networks", "abstract": "This study addresses unsupervised subword modeling, i.e., learning acoustic\nfeature representations that can distinguish between subword units of a\nlanguage. We propose a two-stage learning framework that combines\nself-supervised learning and cross-lingual knowledge transfer. The framework\nconsists of autoregressive predictive coding (APC) as the front-end and a\ncross-lingual deep neural network (DNN) as the back-end. Experiments on the ABX\nsubword discriminability task conducted with the Libri-light and ZeroSpeech\n2017 databases showed that our approach is competitive or superior to\nstate-of-the-art studies. Comprehensive and systematic analyses at the phoneme-\nand articulatory feature (AF)-level showed that our approach was better at\ncapturing diphthong than monophthong vowel information, while also differences\nin the amount of information captured for different types of consonants were\nobserved. Moreover, a positive correlation was found between the effectiveness\nof the back-end in capturing a phoneme's information and the quality of the\ncross-lingual phone labels assigned to the phoneme. The AF-level analysis\ntogether with t-SNE visualization results showed that the proposed approach is\nbetter than MFCC and APC features in capturing manner and place of articulation\ninformation, vowel height, and backness information. Taken together, the\nanalyses showed that the two stages in our approach are both effective in\ncapturing phoneme and AF information. Nevertheless, monophthong vowel\ninformation is less well captured than consonant information, which suggests\nthat future research should focus on improving capturing monophthong vowel\ninformation.", "published": "2020-12-17 12:33:49", "link": "http://arxiv.org/abs/2012.09544v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Continual Lifelong Learning in Natural Language Processing: A Survey", "abstract": "Continual learning (CL) aims to enable information systems to learn from a\ncontinuous data stream across time. However, it is difficult for existing deep\nlearning architectures to learn a new task without largely forgetting\npreviously acquired knowledge. Furthermore, CL is particularly challenging for\nlanguage learning, as natural language is ambiguous: it is discrete,\ncompositional, and its meaning is context-dependent. In this work, we look at\nthe problem of CL through the lens of various NLP tasks. Our survey discusses\nmajor challenges in CL and current methods applied in neural network models. We\nalso provide a critical review of the existing CL evaluation methods and\ndatasets in NLP. Finally, we present our outlook on future research directions.", "published": "2020-12-17 18:44:36", "link": "http://arxiv.org/abs/2012.09823v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and\n  Head Pruning", "abstract": "The attention mechanism is becoming increasingly popular in Natural Language\nProcessing (NLP) applications, showing superior performance than convolutional\nand recurrent architectures. However, attention becomes the compution\nbottleneck because of its quadratic computational complexity to input length,\ncomplicated data movement and low arithmetic intensity. Moreover, existing NN\naccelerators mainly focus on optimizing convolutional or recurrent models, and\ncannot efficiently support attention. In this paper, we present SpAtten, an\nefficient algorithm-architecture co-design that leverages token sparsity, head\nsparsity, and quantization opportunities to reduce the attention computation\nand memory access. Inspired by the high redundancy of human languages, we\npropose the novel cascade token pruning to prune away unimportant tokens in the\nsentence. We also propose cascade head pruning to remove unessential heads.\nCascade pruning is fundamentally different from weight pruning since there is\nno trainable weight in the attention mechanism, and the pruned tokens and heads\nare selected on the fly. To efficiently support them on hardware, we design a\nnovel top-k engine to rank token and head importance scores with high\nthroughput. Furthermore, we propose progressive quantization that first fetches\nMSBs only and performs the computation; if the confidence is low, it fetches\nLSBs and recomputes the attention outputs, trading computation for memory\nreduction.\n  Extensive experiments on 30 benchmarks show that, on average, SpAtten reduces\nDRAM access by 10.0x with no accuracy loss, and achieves 1.6x, 3.0x, 162x, 347x\nspeedup, and 1,4x, 3.2x, 1193x, 4059x energy savings over A3 accelerator,\nMNNFast accelerator, TITAN Xp GPU, Xeon CPU, respectively.", "published": "2020-12-17 18:59:07", "link": "http://arxiv.org/abs/2012.09852v3", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AR"}
{"title": "Named Entity Recognition in the Legal Domain using a Pointer Generator\n  Network", "abstract": "Named Entity Recognition (NER) is the task of identifying and classifying\nnamed entities in unstructured text. In the legal domain, named entities of\ninterest may include the case parties, judges, names of courts, case numbers,\nreferences to laws etc. We study the problem of legal NER with noisy text\nextracted from PDF files of filed court cases from US courts. The \"gold\nstandard\" training data for NER systems provide annotation for each token of\nthe text with the corresponding entity or non-entity label. We work with only\npartially complete training data, which differ from the gold standard NER data\nin that the exact location of the entities in the text is unknown and the\nentities may contain typos and/or OCR mistakes. To overcome the challenges of\nour noisy training data, e.g. text extraction errors and/or typos and unknown\nlabel indices, we formulate the NER task as a text-to-text sequence generation\ntask and train a pointer generator network to generate the entities in the\ndocument rather than label them. We show that the pointer generator can be\neffective for NER in the absence of gold standard data and outperforms the\ncommon NER neural network architectures in long legal documents.", "published": "2020-12-17 21:10:34", "link": "http://arxiv.org/abs/2012.09936v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Predicting Decisions in Language Based Persuasion Games", "abstract": "Sender-receiver interactions, and specifically persuasion games, are widely\nresearched in economic modeling and artificial intelligence. However, in the\nclassic persuasion games setting, the messages sent from the expert to the\ndecision-maker (DM) are abstract or well-structured signals rather than natural\nlanguage messages. This paper addresses the use of natural language in\npersuasion games. For this purpose, we conduct an online repeated interaction\nexperiment. At each trial of the interaction, an informed expert aims to sell\nan uninformed decision-maker a vacation in a hotel, by sending her a review\nthat describes the hotel. While the expert is exposed to several scored\nreviews, the decision-maker observes only the single review sent by the expert,\nand her payoff in case she chooses to take the hotel is a random draw from the\nreview score distribution available to the expert only. We also compare the\nbehavioral patterns in this experiment to the equivalent patterns in similar\nexperiments where the communication is based on the numerical values of the\nreviews rather than the reviews' text, and observe substantial differences\nwhich can be explained through an equilibrium analysis of the game. We consider\na number of modeling approaches for our verbal communication setup, differing\nfrom each other in the model type (deep neural network vs. linear classifier),\nthe type of features used by the model (textual, behavioral or both) and the\nsource of the textual features (DNN-based vs. hand-crafted). Our results\ndemonstrate that given a prefix of the interaction sequence, our models can\npredict the future decisions of the decision-maker, particularly when a\nsequential modeling approach and hand-crafted textual features are applied.\nFurther analysis of the hand-crafted textual features allows us to make initial\nobservations about the aspects of text that drive decision making in our setup", "published": "2020-12-17 22:52:47", "link": "http://arxiv.org/abs/2012.09966v5", "categories": ["cs.AI", "cs.CL", "cs.GT"], "primary_category": "cs.AI"}
{"title": "Autoregressive Reasoning over Chains of Facts with Transformers", "abstract": "This paper proposes an iterative inference algorithm for multi-hop\nexplanation regeneration, that retrieves relevant factual evidence in the form\nof text snippets, given a natural language question and its answer. Combining\nmultiple sources of evidence or facts for multi-hop reasoning becomes\nincreasingly hard when the number of sources needed to make an inference grows.\nOur algorithm copes with this by decomposing the selection of facts from a\ncorpus autoregressively, conditioning the next iteration on previously selected\nfacts. This allows us to use a pairwise learning-to-rank loss. We validate our\nmethod on datasets of the TextGraphs 2019 and 2020 Shared Tasks for explanation\nregeneration. Existing work on this task either evaluates facts in isolation or\nartificially limits the possible chains of facts, thus limiting multi-hop\ninference. We demonstrate that our algorithm, when used with a pre-trained\ntransformer model, outperforms the previous state-of-the-art in terms of\nprecision, training time and inference efficiency.", "published": "2020-12-17 13:17:27", "link": "http://arxiv.org/abs/2012.11321v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Do You Do Yoga? Understanding Twitter Users' Types and Motivations using\n  Social and Textual Information", "abstract": "Leveraging social media data to understand people's lifestyle choices is an\nexciting domain to explore but requires a multiview formulation of the data. In\nthis paper, we propose a joint embedding model based on the fusion of neural\nnetworks with attention mechanism by incorporating social and textual\ninformation of users to understand their activities and motivations. We use\nwell-being related tweets from Twitter, focusing on 'Yoga'. We demonstrate our\nmodel on two downstream tasks: (i) finding user type such as either\npractitioner or promotional (promoting yoga studio/gym), other; (ii) finding\nuser motivation i.e. health benefit, spirituality, love to tweet/retweet about\nyoga but do not practice yoga.", "published": "2020-12-17 00:15:13", "link": "http://arxiv.org/abs/2012.09332v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "CIF-based Collaborative Decoding for End-to-end Contextual Speech\n  Recognition", "abstract": "End-to-end (E2E) models have achieved promising results on multiple speech\nrecognition benchmarks, and shown the potential to become the mainstream.\nHowever, the unified structure and the E2E training hamper injecting contextual\ninformation into them for contextual biasing. Though contextual LAS (CLAS)\ngives an excellent all-neural solution, the degree of biasing to given context\ninformation is not explicitly controllable. In this paper, we focus on\nincorporating context information into the continuous integrate-and-fire (CIF)\nbased model that supports contextual biasing in a more controllable fashion.\nSpecifically, an extra context processing network is introduced to extract\ncontextual embeddings, integrate acoustically relevant context information and\ndecode the contextual output distribution, thus forming a collaborative\ndecoding with the decoder of the CIF-based model. Evaluated on the named entity\nrich evaluation sets of HKUST/AISHELL-2, our method brings relative character\nerror rate (CER) reduction of 8.83%/21.13% and relative named entity character\nerror rate (NE-CER) reduction of 40.14%/51.50% when compared with a strong\nbaseline. Besides, it keeps the performance on original evaluation set without\ndegradation.", "published": "2020-12-17 09:40:11", "link": "http://arxiv.org/abs/2012.09466v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Low-Complexity Steered Response Power Mapping based on Nyquist-Shannon\n  Sampling", "abstract": "The steered response power (SRP) approach to acoustic source localization\ncomputes a map of the acoustic scene from the frequency-weighted output power\nof a beamformer steered towards a set of candidate locations. Equivalently, SRP\nmay be expressed in terms of time-domain generalized cross-correlations (GCCs)\nat lags equal to the candidate locations' time-differences of arrival (TDOAs).\nDue to the dense grid of candidate locations, each of which requires inverse\nFourier transform (IFT) evaluations, conventional SRP exhibits a high\ncomputational complexity. In this paper, we propose a low-complexity SRP\napproach based on Nyquist-Shannon sampling. Noting that on the one hand the\nrange of possible TDOAs is physically bounded, while on the other hand the GCCs\nare bandlimited, we critically sample the GCCs around their TDOA interval and\napproximate the SRP map by interpolation. In usual setups, the number of sample\npoints can be orders of magnitude less than the number of candidate locations\nand frequency bins, yielding a significant reduction of IFT computations at a\nlimited interpolation cost. Simulations comparing the proposed approximation\nwith conventional SRP indicate low approximation errors and equal localization\nperformance. MATLAB and Python implementations are available online.", "published": "2020-12-17 10:58:39", "link": "http://arxiv.org/abs/2012.09499v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DenoiSpeech: Denoising Text to Speech with Frame-Level Noise Modeling", "abstract": "While neural-based text to speech (TTS) models can synthesize natural and\nintelligible voice, they usually require high-quality speech data, which is\ncostly to collect. In many scenarios, only noisy speech of a target speaker is\navailable, which presents challenges for TTS model training for this speaker.\nPrevious works usually address the challenge using two methods: 1) training the\nTTS model using the speech denoised with an enhancement model; 2) taking a\nsingle noise embedding as input when training with noisy speech. However, they\nusually cannot handle speech with real-world complicated noise such as those\nwith high variations along time. In this paper, we develop DenoiSpeech, a TTS\nsystem that can synthesize clean speech for a speaker with noisy speech data.\nIn DenoiSpeech, we handle real-world noisy speech by modeling the fine-grained\nframe-level noise with a noise condition module, which is jointly trained with\nthe TTS model. Experimental results on real-world data show that DenoiSpeech\noutperforms the previous two methods by 0.31 and 0.66 MOS respectively.", "published": "2020-12-17 12:43:00", "link": "http://arxiv.org/abs/2012.09547v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Parallel WaveNet conditioned on VAE latent vectors", "abstract": "Recently the state-of-the-art text-to-speech synthesis systems have shifted\nto a two-model approach: a sequence-to-sequence model to predict a\nrepresentation of speech (typically mel-spectrograms), followed by a 'neural\nvocoder' model which produces the time-domain speech waveform from this\nintermediate speech representation. This approach is capable of synthesizing\nspeech that is confusable with natural speech recordings. However, the\ninference speed of neural vocoder approaches represents a major obstacle for\ndeploying this technology for commercial applications. Parallel WaveNet is one\napproach which has been developed to address this issue, trading off some\nsynthesis quality for significantly faster inference speed. In this paper we\ninvestigate the use of a sentence-level conditioning vector to improve the\nsignal quality of a Parallel WaveNet neural vocoder. We condition the neural\nvocoder with the latent vector from a pre-trained VAE component of a Tacotron\n2-style sequence-to-sequence model. With this, we are able to significantly\nimprove the quality of vocoded speech.", "published": "2020-12-17 16:14:32", "link": "http://arxiv.org/abs/2012.09703v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Enhancement with Zero-Shot Model Selection", "abstract": "Recent research on speech enhancement (SE) has seen the emergence of\ndeep-learning-based methods. It is still a challenging task to determine the\neffective ways to increase the generalizability of SE under diverse test\nconditions. In this study, we combine zero-shot learning and ensemble learning\nto propose a zero-shot model selection (ZMOS) approach to increase the\ngeneralization of SE performance. The proposed approach is realized in the\noffline and online phases. The offline phase clusters the entire set of\ntraining data into multiple subsets and trains a specialized SE model (termed\ncomponent SE model) with each subset. The online phase selects the most\nsuitable component SE model to perform the enhancement. Furthermore, two\nselection strategies were developed: selection based on the quality score (QS)\nand selection based on the quality embedding (QE). Both QS and QE were obtained\nusing a Quality-Net, a non-intrusive quality assessment network. Experimental\nresults confirmed that the proposed ZMOS approach can achieve better\nperformance in both seen and unseen noise types compared to the baseline\nsystems and other model selection systems, which indicates the effectiveness of\nthe proposed approach in providing robust SE performance.", "published": "2020-12-17 02:07:37", "link": "http://arxiv.org/abs/2012.09359v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Interactive Speech and Noise Modeling for Speech Enhancement", "abstract": "Speech enhancement is challenging because of the diversity of background\nnoise types. Most of the existing methods are focused on modelling the speech\nrather than the noise. In this paper, we propose a novel idea to model speech\nand noise simultaneously in a two-branch convolutional neural network, namely\nSN-Net. In SN-Net, the two branches predict speech and noise, respectively.\nInstead of information fusion only at the final output layer, interaction\nmodules are introduced at several intermediate feature domains between the two\nbranches to benefit each other. Such an interaction can leverage features\nlearned from one branch to counteract the undesired part and restore the\nmissing component of the other and thus enhance their discrimination\ncapabilities. We also design a feature extraction module, namely\nresidual-convolution-and-attention (RA), to capture the correlations along\ntemporal and frequency dimensions for both the speech and the noises.\nEvaluations on public datasets show that the interaction module plays a key\nrole in simultaneous modeling and the SN-Net outperforms the state-of-the-art\nby a large margin on various evaluation metrics. The proposed SN-Net also shows\nsuperior performance for speaker separation.", "published": "2020-12-17 06:18:31", "link": "http://arxiv.org/abs/2012.09408v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Continuous Speech Separation Using Speaker Inventory for Long\n  Multi-talker Recording", "abstract": "Leveraging additional speaker information to facilitate speech separation has\nreceived increasing attention in recent years. Recent research includes\nextracting target speech by using the target speaker's voice snippet and\njointly separating all participating speakers by using a pool of additional\nspeaker signals, which is known as speech separation using speaker inventory\n(SSUSI). However, all these systems ideally assume that the pre-enrolled\nspeaker signals are available and are only evaluated on simple data\nconfigurations. In realistic multi-talker conversations, the speech signal\ncontains a large proportion of non-overlapped regions, where we can derive\nrobust speaker embedding of individual talkers. In this work, we adopt the\nSSUSI model in long recordings and propose a self-informed, clustering-based\ninventory forming scheme for long recording, where the speaker inventory is\nfully built from the input signal without the need for external speaker\nsignals. Experiment results on simulated noisy reverberant long recording\ndatasets show that the proposed method can significantly improve the separation\nperformance across various conditions.", "published": "2020-12-17 16:44:21", "link": "http://arxiv.org/abs/2012.09727v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Deep embedded clustering of coral reef bioacoustics", "abstract": "Deep clustering was applied to unlabeled, automatically detected signals in a\ncoral reef soundscape to distinguish fish pulse calls from segments of whale\nsong. Deep embedded clustering (DEC) learned latent features and formed\nclassification clusters using fixed-length power spectrograms of the signals.\nHandpicked spectral and temporal features were also extracted and clustered\nwith Gaussian mixture models (GMM) and conventional clustering. DEC, GMM, and\nconventional clustering were tested on simulated datasets of fish pulse calls\n(fish) and whale song units (whale) with randomized bandwidth, duration, and\nSNR. Both GMM and DEC achieved high accuracy and identified clusters with fish,\nwhale, and overlapping fish and whale signals. Conventional clustering methods\nhad low accuracy in scenarios with unequal-sized clusters or overlapping\nsignals. Fish and whale signals recorded near Hawaii in February-March 2020\nwere clustered with DEC, GMM, and conventional clustering. DEC features\ndemonstrated the highest accuracy of 77.5% on a small, manually labeled dataset\nfor classifying signals into fish and whale clusters.", "published": "2020-12-17 23:54:44", "link": "http://arxiv.org/abs/2012.09982v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
