{"title": "CoVeR: Learning Covariate-Specific Vector Representations with Tensor\n  Decompositions", "abstract": "Word embedding is a useful approach to capture co-occurrence structures in\nlarge text corpora. However, in addition to the text data itself, we often have\nadditional covariates associated with individual corpus documents---e.g. the\ndemographic of the author, time and venue of publication---and we would like\nthe embedding to naturally capture this information. We propose CoVeR, a new\ntensor decomposition model for vector embeddings with covariates. CoVeR jointly\nlearns a \\emph{base} embedding for all the words as well as a weighted diagonal\nmatrix to model how each covariate affects the base embedding. To obtain author\nor venue-specific embedding, for example, we can then simply multiply the base\nembedding by the associated transformation matrix. The main advantages of our\napproach are data efficiency and interpretability of the covariate\ntransformation. Our experiments demonstrate that our joint model learns\nsubstantially better covariate-specific embeddings compared to the standard\napproach of learning a separate embedding for each covariate using only the\nrelevant subset of data, as well as other related methods. Furthermore, CoVeR\nencourages the embeddings to be \"topic-aligned\" in that the dimensions have\nspecific independent meanings. This allows our covariate-specific embeddings to\nbe compared by topic, enabling downstream differential analysis. We empirically\nevaluate the benefits of our algorithm on datasets, and demonstrate how it can\nbe used to address many natural questions about covariate effects.\n  Accompanying code to this paper can be found at\nhttp://github.com/kjtian/CoVeR.", "published": "2018-02-21 22:52:35", "link": "http://arxiv.org/abs/1802.07839v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Matching Article Pairs with Graphical Decomposition and Convolutions", "abstract": "Identifying the relationship between two articles, e.g., whether two articles\npublished from different sources describe the same breaking news, is critical\nto many document understanding tasks. Existing approaches for modeling and\nmatching sentence pairs do not perform well in matching longer documents, which\nembody more complex interactions between the enclosed entities than a sentence\ndoes. To model article pairs, we propose the Concept Interaction Graph to\nrepresent an article as a graph of concepts. We then match a pair of articles\nby comparing the sentences that enclose the same concept vertex through a\nseries of encoding techniques, and aggregate the matching signals through a\ngraph convolutional network. To facilitate the evaluation of long article\nmatching, we have created two datasets, each consisting of about 30K pairs of\nbreaking news articles covering diverse topics in the open domain. Extensive\nevaluations of the proposed methods on the two datasets demonstrate significant\nimprovements over a wide range of state-of-the-art methods for natural language\nmatching.", "published": "2018-02-21 08:01:41", "link": "http://arxiv.org/abs/1802.07459v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Sequence-based Multi-lingual Low Resource Speech Recognition", "abstract": "Techniques for multi-lingual and cross-lingual speech recognition can help in\nlow resource scenarios, to bootstrap systems and enable analysis of new\nlanguages and domains. End-to-end approaches, in particular sequence-based\ntechniques, are attractive because of their simplicity and elegance. While it\nis possible to integrate traditional multi-lingual bottleneck feature\nextractors as front-ends, we show that end-to-end multi-lingual training of\nsequence models is effective on context independent models trained using\nConnectionist Temporal Classification (CTC) loss. We show that our model\nimproves performance on Babel languages by over 6% absolute in terms of\nword/phoneme error rate when compared to mono-lingual systems built in the same\nsetting for these languages. We also show that the trained model can be adapted\ncross-lingually to an unseen language using just 25% of the target data. We\nshow that training on multiple languages is important for very low resource\ncross-lingual target scenarios, but not for multi-lingual testing scenarios.\nHere, it appears beneficial to include large well prepared datasets.", "published": "2018-02-21 04:09:26", "link": "http://arxiv.org/abs/1802.07420v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
