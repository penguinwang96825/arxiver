{"title": "A Tweet Dataset Annotated for Named Entity Recognition and Stance\n  Detection", "abstract": "Annotated datasets in different domains are critical for many supervised\nlearning-based solutions to related problems and for the evaluation of the\nproposed solutions. Topics in natural language processing (NLP) similarly\nrequire annotated datasets to be used for such purposes. In this paper, we\ntarget at two NLP problems, named entity recognition and stance detection, and\npresent the details of a tweet dataset in Turkish annotated for named entity\nand stance information. Within the course of the current study, both the named\nentity and stance annotations of the included tweets are made publicly\navailable, although previously the dataset has been publicly shared with stance\nannotations only. We believe that this dataset will be useful for uncovering\nthe possible relationships between named entity recognition and stance\ndetection in tweets.", "published": "2019-01-15 12:16:13", "link": "http://arxiv.org/abs/1901.04787v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Answering Comparative Questions: Better than Ten-Blue-Links?", "abstract": "We present CAM (comparative argumentative machine), a novel open-domain IR\nsystem to argumentatively compare objects with respect to information extracted\nfrom the Common Crawl. In a user study, the participants obtained 15% more\naccurate answers using CAM compared to a \"traditional\" keyword-based search and\nwere 20% faster in finding the answer to comparative questions.", "published": "2019-01-15 20:50:28", "link": "http://arxiv.org/abs/1901.05041v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Antigram Behaviour using Distributional Semantics", "abstract": "The field of computational linguistics constantly presents new challenges and\ntopics for research. Whether it be analyzing word usage changes over time or\nidentifying relationships between pairs of seemingly unrelated words. To this\npoint, we identify Anagrams and Antigrams as words possessing such unique\nproperties. The presented work is an exploration into generating anagrams from\na given word and determining whether there exists antigram (semantically\nopposite anagrams) relationships between the pairs of generated anagrams using\nGloVe embeddings. We propose a rudimentary, yet interpretable, rule-based\nalgorithm for detecting antigrams. On a small dataset of just 12 antigrams, our\napproach yielded an accuracy of 39\\% which shows that there is much work left\nto be done in this space.", "published": "2019-01-15 21:56:32", "link": "http://arxiv.org/abs/1901.05066v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Global-to-local Memory Pointer Networks for Task-Oriented Dialogue", "abstract": "End-to-end task-oriented dialogue is challenging since knowledge bases are\nusually large, dynamic and hard to incorporate into a learning framework. We\npropose the global-to-local memory pointer (GLMP) networks to address this\nissue. In our model, a global memory encoder and a local memory decoder are\nproposed to share external knowledge. The encoder encodes dialogue history,\nmodifies global contextual representation, and generates a global memory\npointer. The decoder first generates a sketch response with unfilled slots.\nNext, it passes the global memory pointer to filter the external knowledge for\nrelevant information, then instantiates the slots via the local memory\npointers. We empirically show that our model can improve copy accuracy and\nmitigate the common out-of-vocabulary problem. As a result, GLMP is able to\nimprove over the previous state-of-the-art models in both simulated bAbI\nDialogue dataset and human-human Stanford Multi-domain Dialogue dataset on\nautomatic and human evaluation.", "published": "2019-01-15 08:55:53", "link": "http://arxiv.org/abs/1901.04713v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploiting Synchronized Lyrics And Vocal Features For Music Emotion\n  Detection", "abstract": "One of the key points in music recommendation is authoring engaging playlists\naccording to sentiment and emotions. While previous works were mostly based on\naudio for music discovery and playlists generation, we take advantage of our\nsynchronized lyrics dataset to combine text representations and music features\nin a novel way; we therefore introduce the Synchronized Lyrics Emotion Dataset.\nUnlike other approaches that randomly exploited the audio samples and the whole\ntext, our data is split according to the temporal information provided by the\nsynchronization between lyrics and audio. This work shows a comparison between\ntext-based and audio-based deep learning classification models using different\ntechniques from Natural Language Processing and Music Information Retrieval\ndomains. From the experiments on audio we conclude that using vocals only,\ninstead of the whole audio data improves the overall performances of the audio\nclassifier. In the lyrics experiments we exploit the state-of-the-art word\nrepresentations applied to the main Deep Learning architectures available in\nliterature. In our benchmarks the results show how the Bilinear LSTM classifier\nwith Attention based on fastText word embedding performs better than the CNN\napplied on audio.", "published": "2019-01-15 14:10:25", "link": "http://arxiv.org/abs/1901.04831v1", "categories": ["cs.CL", "cs.AI", "68T50"], "primary_category": "cs.CL"}
{"title": "Incremental Reading for Question Answering", "abstract": "Any system which performs goal-directed continual learning must not only\nlearn incrementally but process and absorb information incrementally. Such a\nsystem also has to understand when its goals have been achieved. In this paper,\nwe consider these issues in the context of question answering. Current\nstate-of-the-art question answering models reason over an entire passage, not\nincrementally. As we will show, naive approaches to incremental reading, such\nas restriction to unidirectional language models in the model, perform poorly.\nWe present extensions to the DocQA [2] model to allow incremental reading\nwithout loss of accuracy. The model also jointly learns to provide the best\nanswer given the text that is seen so far and predict whether this best-so-far\nanswer is sufficient.", "published": "2019-01-15 17:03:32", "link": "http://arxiv.org/abs/1901.04936v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unconstrained Church-Turing thesis cannot possibly be true", "abstract": "The Church-Turing thesis asserts that if a partial strings-to-strings\nfunction is effectively computable then it is computable by a Turing machine.\n  In the 1930s, when Church and Turing worked on their versions of the thesis,\nthere was a robust notion of algorithm. These traditional algorithms are known\nalso as classical or sequential. In the original thesis, effectively computable\nmeant computable by an effective classical algorithm. Based on an earlier\naxiomatization of classical algorithms, the original thesis was proven in 2008.\n  Since the 1930s, the notion of algorithm has changed dramatically. New\nspecies of algorithms have been and are being introduced. We argue that the\ngeneralization of the original thesis, where effectively computable means\ncomputable by an effective algorithm of any species, cannot possibly be true.", "published": "2019-01-15 16:24:07", "link": "http://arxiv.org/abs/1901.04911v1", "categories": ["cs.LO", "cs.CL", "cs.DS", "math.LO", "quant-ph"], "primary_category": "cs.LO"}
{"title": "Orthonormal Embedding-based Deep Clustering for Single-channel Speech\n  Separation", "abstract": "Deep clustering is a deep neural network-based speech separation algorithm\nthat first trains the mixed component of signals with high-dimensional\nembeddings, and then uses a clustering algorithm to separate each mixture of\nsources. In this paper, we extend the baseline criterion of deep clustering\nwith an additional regularization term to further improve the overall\nperformance. This term plays a role in assigning a condition to the embeddings\nsuch that it gives less correlation to each embedding dimension, leading to\nbetter decomposition of the spectral bins. The regularization term helps to\nmitigate the unavoidable permutation problem in the conventional deep\nclustering method, which enables to bring better clustering through the\nformation of optimal embeddings. We evaluate the results by varying embedding\ndimension, signal-to-interference ratio (SIR), and gender dependency. The\nperformance comparison with the source separation measurement metric, i.e.\nsignal-to-distortion ratio (SDR), confirms that the proposed method outperforms\nthe conventional deep clustering method.", "published": "2019-01-15 07:31:54", "link": "http://arxiv.org/abs/1901.04690v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Classical Music Generation in Distinct Dastgahs with AlimNet ACGAN", "abstract": "In this paper AlimNet (With respect to great musician, Alim Qasimov) an\nauxiliary generative adversarial deep neural network (ACGAN) for generating\nmusic categorically, is used. This proposed network is a conditional ACGAN to\ncondition the generation process on music tracks which has a hybrid\narchitecture, composing of different kind of layers of neural networks. The\nemployed music dataset is MICM which contains 1137 music samples (506 violins\nand 631 straw) with seven types of classical music Dastgah labels. To extract\nboth temporal and spectral features, Short-Time Fourier Transform (STFT) is\napplied to convert input audio signals from time domain to time-frequency\ndomain. GANs are composed of a generator for generating new samples and a\ndiscriminator to help generator making better samples. Samples in\ntime-frequency domain are used to train discriminator in fourteen classes\n(seven Dastgahs and two instruments). The outputs of the conditional ACGAN are\nalso artificial music samples in those mentioned scales in time-frequency\ndomain. Then the output of the generator is transformed by Inverse STFT\n(ISTFT). Finally, randomly ten generated music samples (five violin and five\nstraw samples) are given to ten musicians to rate how exact the samples are and\nthe overall result was 76.5%.", "published": "2019-01-15 08:00:27", "link": "http://arxiv.org/abs/1901.04696v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Phoneme-Based Persian Speech Recognition", "abstract": "Undoubtedly, one of the most important issues in computer science is\nintelligent speech recognition. In these systems, computers try to detect and\nrespond to the speeches they are listening to, like humans. In this research,\npresenting of a suitable method for the diagnosis of Persian phonemes by AI\nusing the signal processing and classification algorithms have tried. For this\npurpose, the STFT algorithm has been used to process the audio signals, as well\nas to detect and classify the signals processed by the deep artificial neural\nnetwork. At first, educational samples were provided as two phonological\nphrases in Persian language and then signal processing operations were\nperformed on them. Then the results for the data training have been given to\nthe artificial deep neural network. At the final stage, the experiment was\nconducted on new sounds.", "published": "2019-01-15 08:07:48", "link": "http://arxiv.org/abs/1901.04699v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A linear programming approach to the tracking of partials", "abstract": "A new approach to the tracking of sinusoidal chirps using linear programming\nis proposed. It is demonstrated that the classical algorithm of McAulay and\nQuatieri is greedy and exhibits exponential complexity for long searches, while\napproaches based on the Viterbi algorithm exhibit factorial complexity. A\nlinear programming (LP) formulation to find the best $L$ paths in a lattice is\ndescribed and its complexity is shown to be less than previous approaches.\nFinally it is demonstrated that the new LP formulation outperforms the\nclassical algorithm in the tracking of sinusoidal chirps in high levels of\nnoise.", "published": "2019-01-15 21:05:34", "link": "http://arxiv.org/abs/1901.05044v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Spectrogram Feature Losses for Music Source Separation", "abstract": "In this paper we study deep learning-based music source separation, and\nexplore using an alternative loss to the standard spectrogram pixel-level L2\nloss for model training. Our main contribution is in demonstrating that adding\na high-level feature loss term, extracted from the spectrograms using a VGG\nnet, can improve separation quality vis-a-vis a pure pixel-level loss. We show\nthis improvement in the context of the MMDenseNet, a State-of-the-Art deep\nlearning model for this task, for the extraction of drums and vocal sounds from\nsongs in the musdb18 database, covering a broad range of western music genres.\nWe believe that this finding can be generalized and applied to broader machine\nlearning-based systems in the audio domain.", "published": "2019-01-15 21:51:12", "link": "http://arxiv.org/abs/1901.05061v3", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML", "62, 68", "I.2.6; H.5.5"], "primary_category": "cs.SD"}
{"title": "Bonseyes AI Pipeline -- bringing AI to you. End-to-end integration of\n  data, algorithms and deployment tools", "abstract": "Next generation of embedded Information and Communication Technology (ICT)\nsystems are collaborative systems able to perform autonomous tasks. The\nremarkable expansion of the embedded ICT market, together with the rise and\nbreakthroughs of Artificial Intelligence (AI), have put the focus on the Edge\nas it stands as one of the keys for the next technological revolution: the\nseamless integration of AI in our daily life. However, training and deployment\nof custom AI solutions on embedded devices require a fine-grained integration\nof data, algorithms, and tools to achieve high accuracy. Such integration\nrequires a high level of expertise that becomes a real bottleneck for small and\nmedium enterprises wanting to deploy AI solutions on the Edge which,\nultimately, slows down the adoption of AI on daily-life applications. In this\nwork, we present a modular AI pipeline as an integrating framework to bring\ndata, algorithms, and deployment tools together. By removing the integration\nbarriers and lowering the required expertise, we can interconnect the different\nstages of tools and provide a modular end-to-end development of AI products for\nembedded devices. Our AI pipeline consists of four modular main steps: i) data\ningestion, ii) model training, iii) deployment optimization and, iv) the IoT\nhub integration. To show the effectiveness of our pipeline, we provide examples\nof different AI applications during each of the steps. Besides, we integrate\nour deployment framework, LPDNN, into the AI pipeline and present its\nlightweight architecture and deployment capabilities for embedded devices.\nFinally, we demonstrate the results of the AI pipeline by showing the\ndeployment of several AI applications such as keyword spotting, image\nclassification and object detection on a set of well-known embedded platforms,\nwhere LPDNN consistently outperforms all other popular deployment frameworks.", "published": "2019-01-15 21:27:28", "link": "http://arxiv.org/abs/1901.05049v3", "categories": ["cs.LG", "cs.DC", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
