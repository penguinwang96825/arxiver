{"title": "MarketSenseAI 2.0: Enhancing Stock Analysis through LLM Agents", "abstract": "MarketSenseAI is a novel framework for holistic stock analysis which\nleverages Large Language Models (LLMs) to process financial news, historical\nprices, company fundamentals and the macroeconomic environment to support\ndecision making in stock analysis and selection. In this paper, we present the\nlatest advancements on MarketSenseAI, driven by rapid technological expansion\nin LLMs. Through a novel architecture combining Retrieval-Augmented Generation\nand LLM agents, the framework processes SEC filings and earnings calls, while\nenriching macroeconomic analysis through systematic processing of diverse\ninstitutional reports. We demonstrate a significant improvement in fundamental\nanalysis accuracy over the previous version. Empirical evaluation on S\\&P 100\nstocks over two years (2023-2024) shows MarketSenseAI achieving cumulative\nreturns of 125.9% compared to the index return of 73.5%, while maintaining\ncomparable risk profiles. Further validation on S\\&P 500 stocks during 2024\ndemonstrates the framework's scalability, delivering a 33.8% higher Sortino\nratio than the market. This work marks a significant advancement in applying\nLLM technology to financial analysis, offering insights into the robustness of\nLLM-driven investment strategies.", "published": "2025-02-01 12:33:23", "link": "http://arxiv.org/abs/2502.00415v1", "categories": ["q-fin.CP", "cs.AI", "cs.CL", "cs.MA", "q-fin.PM", "68T07, 68T50, 91G10, 91G15", "I.2.1; I.2.7; J.4"], "primary_category": "q-fin.CP"}
{"title": "Context-Preserving Tensorial Reconfiguration in Large Language Model\n  Training", "abstract": "Handling long-range dependencies in neural architectures has remained a\npersistent challenge due to computational limitations and inefficient\ncontextual retention mechanisms. Tensorial operations have provided a\nfoundation for restructuring model representations, yet conventional\narchitectures have struggled to incorporate such techniques without introducing\nexcessive complexity. A novel approach, Context-Preserving Tensorial\nReconfiguration (CPTR), enables dynamic reorganization of weight tensors\nthrough structured factorization and adaptive contraction, allowing for\nenhanced contextual integration without substantial computational overhead.\nEmpirical evaluations demonstrate that CPTR improves coherence retention across\nextended sequences, leading to measurable reductions in perplexity and improved\nrecall accuracy for long-context tasks. Performance comparisons reveal that\nCPTR-enhanced models exhibit greater computational efficiency and reduced\nmemory consumption while maintaining competitive language generation fluency\nand accuracy. Gradient stability metrics further validate the improved training\nefficiency, revealing more controlled variance in weight updates. Comparative\nstudies across baseline and CPTR-enhanced models confirm that tensorial\nreconfiguration contributes to more stable and computationally efficient\nlanguage modeling. The findings support the potential of CPTR in refining\ncontemporary neural architectures for tasks requiring long-range contextual\nunderstanding and efficient memory utilization.", "published": "2025-02-01 00:55:19", "link": "http://arxiv.org/abs/2502.00246v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scaling Flaws of Verifier-Guided Search in Mathematical Reasoning", "abstract": "Large language models (LLMs) struggle with multi-step reasoning, where\ninference-time scaling has emerged as a promising strategy for performance\nimprovement. Verifier-guided search outperforms repeated sampling when sample\nsize is limited by selecting and prioritizing valid reasoning paths. However,\nwe identify a critical limitation: scaling flaws, prevalent across different\nmodels (Mistral 7B and DeepSeekMath 7B), benchmarks (GSM8K and MATH), and\nverifiers (outcome value models and process reward models). As sample size\nincreases, verifier-guided search exhibits diminishing advantages and\neventually underperforms repeated sampling. Our analysis attributes this to\nverifier failures, where imperfect verifiers misrank candidates and erroneously\nprune all valid paths. These issues are further exacerbated in challenging and\nout-of-distribution problems, restricting search effectiveness. To mitigate\nverifier failures, we explore reducing reliance on verifiers and conduct\npreliminary investigations using two simple methods. Our findings reveal\nfundamental limitations in verifier-guided search and suggest future\ndirections.", "published": "2025-02-01 02:08:49", "link": "http://arxiv.org/abs/2502.00271v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference", "abstract": "To reduce memory costs in long-context inference with Large Language Models\n(LLMs), many recent works focus on compressing the key-value (KV) cache of\ndifferent tokens. However, we identify that the previous KV cache compression\nmethods measure token importance individually, neglecting the dependency\nbetween different tokens in the real-world language characterics. In light of\nthis, we introduce ChunkKV, grouping the tokens in a chunk as a basic\ncompressing unit, and retaining the most informative semantic chunks while\ndiscarding the less important ones. Furthermore, observing that ChunkKV\nexhibits higher similarity in the preserved indices across different layers, we\npropose layer-wise index reuse to further reduce computational overhead. We\nevaluated ChunkKV on cutting-edge long-context benchmarks including LongBench\nand Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context\nlearning benchmark. Our experiments with instruction tuning and multi-step\nreasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under\naggressive compression ratios compared to existing methods.", "published": "2025-02-01 03:49:47", "link": "http://arxiv.org/abs/2502.00299v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual Morphogenesis in Large Language Models: A Novel Approach to\n  Self-Organizing Token Representations", "abstract": "Token representations influence the efficiency and adaptability of language\nmodels, yet conventional tokenization strategies impose rigid segmentation\nboundaries that do not adjust dynamically to evolving contextual relationships.\nThe introduction of contextual morphogenesis establishes a self-organizing\nmechanism that restructures token boundaries based on learned contextual\ndependencies, allowing embeddings to evolve progressively across iterative\nprocessing steps. Empirical evaluations demonstrate that dynamically adjusted\ntokenization contributes to reductions in perplexity while maintaining\nrepresentational stability, particularly in linguistically complex domains\nwhere static segmentation fails to capture nuanced dependencies. Computational\ntrade-offs associated with self-organizing token structures indicate that\nadditional processing overhead remains within feasible limits, provided that\noptimization strategies account for segmentation update efficiency. Comparative\nassessments across different linguistic corpora suggest that adaptive\ntokenization preserves interpretability while improving alignment with\ncontextual cues, reinforcing the potential of morphogenetic segmentation\nmechanisms to refine predictive accuracy. Stability analyses confirm that\nevolving token structures maintain consistent segmentation behaviors across\nvaried text distributions, ensuring that representational adaptations remain\nlinguistically coherent. The effectiveness of contextual morphogenesis in\nrefining structural stability and predictive performance highlights its\nviability as an alternative to traditional tokenization methods. Further\nanalysis of computational efficiency considerations suggests that hybrid\nstrategies integrating both static and dynamic segmentation techniques may\noffer a balanced approach to optimizing representational flexibility while\nmaintaining inference efficiency.", "published": "2025-02-01 03:50:46", "link": "http://arxiv.org/abs/2502.00301v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FinchGPT: a Transformer based language model for birdsong analysis", "abstract": "The long-range dependencies among the tokens, which originate from\nhierarchical structures, are a defining hallmark of human language. However,\nwhether similar dependencies exist within the sequential vocalization of\nnon-human animals remains a topic of investigation. Transformer architectures,\nknown for their ability to model long-range dependencies among tokens, provide\na powerful tool for investigating this phenomenon. In this study, we employed\nthe Transformer architecture to analyze the songs of Bengalese finch (Lonchura\nstriata domestica), which are characterized by their highly variable and\ncomplex syllable sequences. To this end, we developed FinchGPT, a\nTransformer-based model trained on a textualized corpus of birdsongs, which\noutperformed other architecture models in this domain. Attention weight\nanalysis revealed that FinchGPT effectively captures long-range dependencies\nwithin syllables sequences. Furthermore, reverse engineering approaches\ndemonstrated the impact of computational and biological manipulations on its\nperformance: restricting FinchGPT's attention span and disrupting birdsong\nsyntax through the ablation of specific brain nuclei markedly influenced the\nmodel's outputs. Our study highlights the transformative potential of large\nlanguage models (LLMs) in deciphering the complexities of animal vocalizations,\noffering a novel framework for exploring the structural properties of non-human\ncommunication systems while shedding light on the computational distinctions\nbetween biological brains and artificial neural networks.", "published": "2025-02-01 07:06:34", "link": "http://arxiv.org/abs/2502.00344v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Social media polarization during conflict: Insights from an ideological\n  stance dataset on Israel-Palestine Reddit comments", "abstract": "In politically sensitive scenarios like wars, social media serves as a\nplatform for polarized discourse and expressions of strong ideological stances.\nWhile prior studies have explored ideological stance detection in general\ncontexts, limited attention has been given to conflict-specific settings. This\nstudy addresses this gap by analyzing 9,969 Reddit comments related to the\nIsrael-Palestine conflict, collected between October 2023 and August 2024. The\ncomments were categorized into three stance classes: Pro-Israel, Pro-Palestine,\nand Neutral. Various approaches, including machine learning, pre-trained\nlanguage models, neural networks, and prompt engineering strategies for open\nsource large language models (LLMs), were employed to classify these stances.\nPerformance was assessed using metrics such as accuracy, precision, recall, and\nF1-score. Among the tested methods, the Scoring and Reflective Re-read prompt\nin Mixtral 8x7B demonstrated the highest performance across all metrics. This\nstudy provides comparative insights into the effectiveness of different models\nfor detecting ideological stances in highly polarized social media contexts.\nThe dataset used in this research is publicly available for further exploration\nand validation.", "published": "2025-02-01 12:26:11", "link": "http://arxiv.org/abs/2502.00414v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniAttn: Reducing Inference Costs via Softmax Unification for\n  Post-Training LLMs", "abstract": "Post-training is essential for adapting Large Language Models (LLMs) to\nreal-world applications. Deploying post-trained models faces significant\nchallenges due to substantial memory overhead and noticeable inference latency.\nExisting work has identified significant redundancies in LLMs and proposed\nefficient architectures, namely intra-layer KV sharing and cross-layer KV\nsharing. However, intra-layer KV sharing still results in high inference costs,\nwhile cross-layer KV sharing leads to significant performance degradation. As a\nresult, both methods remain suboptimal for post-training pre-trained LLMs. In\nthis paper, we identify that the \\texttt{Softmax} operation is a primary\nbottleneck for LLM inference and discover that it is actually highly redundant\nduring post-training. We propose Softmax \\textbf{Uni}fication in\n\\textbf{Att}e\\textbf{n}tion (\\textbf{UniAttn}), a novel post-training method\nthat unifies Softmax activations across transformer blocks to reduce LLM\ninference costs. Additionally, UniAttn adopts a linear projection to compensate\nfor the errors induced by Softmax unification. Experiments show that UniAttn\nmatches the performance of standard post-training while significantly reducing\ninference costs, outperforming existing efficient architectures during\npost-training. Our code will be available at\n\\url{https://github.com/Bostoncake/UniAttn}.", "published": "2025-02-01 14:16:31", "link": "http://arxiv.org/abs/2502.00439v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HERA: Improving Long Document Summarization using Large Language Models\n  with Context Packaging and Reordering", "abstract": "Despite the rapid growth of context length of large language models (LLMs) ,\nLLMs still perform poorly in long document summarization. An important reason\nfor this is that relevant information about an event is scattered throughout\nlong documents, and the messy narrative order impairs the accurate\nunderstanding and utilization of LLMs for long documents. To address these\nissues, we propose a novel summary generation framework, called HERA.\nSpecifically, we first segment a long document by its semantic structure and\nretrieve text segments about the same event, and finally reorder them to form\nthe input context. We evaluate our approach on two long document summarization\ndatasets. The experimental results show that HERA outperforms foundation models\nin ROUGE, BERTScore and faithfulness metrics, while HERA does not require\nadditional fine-tuning and resources.", "published": "2025-02-01 14:55:06", "link": "http://arxiv.org/abs/2502.00448v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Ambiguities to Guide Query Rewrite for Robust Conversations in\n  Enterprise AI Assistants", "abstract": "Multi-turn conversations with an Enterprise AI Assistant can be challenging\ndue to conversational dependencies in questions, leading to ambiguities and\nerrors. To address this, we propose an NLU-NLG framework for ambiguity\ndetection and resolution through reformulating query automatically and\nintroduce a new task called \"Ambiguity-guided Query Rewrite.\" To detect\nambiguities, we develop a taxonomy based on real user conversational logs and\ndraw insights from it to design rules and extract features for a classifier\nwhich yields superior performance in detecting ambiguous queries, outperforming\nLLM-based baselines. Furthermore, coupling the query rewrite module with our\nambiguity detecting classifier shows that this end-to-end framework can\neffectively mitigate ambiguities without risking unnecessary insertions of\nunwanted phrases for clear queries, leading to an improvement in the overall\nperformance of the AI Assistant. Due to its significance, this has been\ndeployed in the real world application, namely Adobe Experience Platform AI\nAssistant.", "published": "2025-02-01 19:23:21", "link": "http://arxiv.org/abs/2502.00537v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "M+: Extending MemoryLLM with Scalable Long-Term Memory", "abstract": "Equipping large language models (LLMs) with latent-space memory has attracted\nincreasing attention as they can extend the context window of existing language\nmodels. However, retaining information from the distant past remains a\nchallenge. For example, MemoryLLM (Wang et al., 2024a), as a representative\nwork with latent-space memory, compresses past information into hidden states\nacross all layers, forming a memory pool of 1B parameters. While effective for\nsequence lengths up to 16k tokens, it struggles to retain knowledge beyond 20k\ntokens. In this work, we address this limitation by introducing M+, a\nmemory-augmented model based on MemoryLLM that significantly enhances long-term\ninformation retention. M+ integrates a long-term memory mechanism with a\nco-trained retriever, dynamically retrieving relevant information during text\ngeneration. We evaluate M+ on diverse benchmarks, including long-context\nunderstanding and knowledge retention tasks. Experimental results show that M+\nsignificantly outperforms MemoryLLM and recent strong baselines, extending\nknowledge retention from under 20k to over 160k tokens with similar GPU memory\noverhead.", "published": "2025-02-01 23:13:10", "link": "http://arxiv.org/abs/2502.00592v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explainable AI for Sentiment Analysis of Human Metapneumovirus (HMPV)\n  Using XLNet", "abstract": "In 2024, the outbreak of Human Metapneumovirus (HMPV) in China, which later\nspread to the UK and other countries, raised significant public concern. While\nHMPV typically causes mild symptoms, its effects on vulnerable individuals\nprompted health authorities to emphasize preventive measures. This paper\nexplores how sentiment analysis can enhance our understanding of public\nreactions to HMPV by analyzing social media data. We apply transformer models,\nparticularly XLNet, achieving 93.50% accuracy in sentiment classification.\nAdditionally, we use explainable AI (XAI) through SHAP to improve model\ntransparency.", "published": "2025-02-01 13:41:23", "link": "http://arxiv.org/abs/2502.01663v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ProxSparse: Regularized Learning of Semi-Structured Sparsity Masks for\n  Pretrained LLMs", "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance in\nnatural language processing tasks, yet their massive size makes serving them\ninefficient and costly. Semi-structured pruning has emerged as an effective\nmethod for model acceleration, but existing approaches are suboptimal because\nthey focus on local, layer-wise optimizations using heuristic rules, failing to\nleverage global feedback. We present ProxSparse, a learning-based framework for\nmask selection enabled by regularized optimization. ProxSparse transforms the\nrigid, non-differentiable mask selection process into a smoother optimization\nprocedure, allowing gradual mask exploration with flexibility. ProxSparse does\nnot involve additional weight updates once the mask is determined. Our\nextensive evaluations on 7 widely used models show that ProxSparse consistently\noutperforms previously proposed semi-structured mask selection methods with\nsignificant improvement, demonstrating the effectiveness of our learned\napproach towards semi-structured pruning.", "published": "2025-02-01 01:35:23", "link": "http://arxiv.org/abs/2502.00258v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Estimating LLM Uncertainty with Logits", "abstract": "Over the past few years, Large Language Models (LLMs) have developed rapidly\nand are widely applied in various domains. However, LLMs face the issue of\nhallucinations, generating responses that may be unreliable when the models\nlack relevant knowledge. To be aware of potential hallucinations, uncertainty\nestimation methods have been introduced, and most of them have confirmed that\nreliability lies in critical tokens. However, probability-based methods perform\npoorly in identifying token reliability, limiting their practical utility. In\nthis paper, we reveal that the probability-based method fails to estimate token\nreliability due to the loss of evidence strength information which is\naccumulated in the training stage. Therefore, we present Logits-induced token\nuncertainty (LogTokU), a framework for estimating decoupled token uncertainty\nin LLMs, enabling real-time uncertainty estimation without requiring multiple\nsampling processes. We employ evidence modeling to implement LogTokU and use\nthe estimated uncertainty to guide downstream tasks. The experimental results\ndemonstrate that LogTokU has significant effectiveness and promise.", "published": "2025-02-01 03:18:02", "link": "http://arxiv.org/abs/2502.00290v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MODS: Moderating a Mixture of Document Speakers to Summarize Debatable\n  Queries in Document Collections", "abstract": "Query-focused summarization (QFS) gives a summary of documents to answer a\nquery. Past QFS work assumes queries have one answer, ignoring debatable ones\n(Is law school worth it?). We introduce Debatable QFS (DQFS), a task to create\nsummaries that answer debatable queries via documents with opposing\nperspectives; summaries must comprehensively cover all sources and balance\nperspectives, favoring no side. These goals elude LLM QFS systems, which: 1)\nlack structured content plans, failing to guide LLMs to write balanced\nsummaries, and 2) use the same query to retrieve contexts across documents,\nfailing to cover all perspectives specific to each document's content. To\novercome this, we design MODS, a multi-LLM framework mirroring human panel\ndiscussions. MODS treats documents as individual Speaker LLMs and has a\nModerator LLM that picks speakers to respond to tailored queries for planned\ntopics. Speakers use tailored queries to retrieve relevant contexts from their\ndocuments and supply perspectives, which are tracked in a rich outline,\nyielding a content plan to guide the final summary. Experiments on\nConflictingQA with controversial web queries and DebateQFS, our new dataset of\ndebate queries from Debatepedia, show MODS beats SOTA by 38-59% in topic\nparagraph coverage and balance, based on new citation metrics. Users also find\nMODS's summaries to be readable and more balanced.", "published": "2025-02-01 05:08:14", "link": "http://arxiv.org/abs/2502.00322v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "UGPhysics: A Comprehensive Benchmark for Undergraduate Physics Reasoning\n  with Large Language Models", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nsolving complex reasoning tasks, particularly in mathematics. However, the\ndomain of physics reasoning presents unique challenges that have received\nsignificantly less attention. Existing benchmarks often fall short in\nevaluating LLMs' abilities on the breadth and depth of undergraduate-level\nphysics, underscoring the need for a comprehensive evaluation. To fill this\ngap, we introduce UGPhysics, a large-scale and comprehensive benchmark\nspecifically designed to evaluate UnderGraduate-level Physics (UGPhysics)\nreasoning with LLMs. UGPhysics includes 5,520 undergraduate-level physics\nproblems in both English and Chinese, covering 13 subjects with seven different\nanswer types and four distinct physics reasoning skills, all rigorously\nscreened for data leakage. Additionally, we develop a Model-Assistant\nRule-based Judgment (MARJ) pipeline specifically tailored for assessing answer\ncorrectness of physics problems, ensuring accurate evaluation. Our evaluation\nof 31 leading LLMs shows that the highest overall accuracy, 49.8% (achieved by\nOpenAI-o1-mini), emphasizes the necessity for models with stronger physics\nreasoning skills, beyond math abilities. We hope UGPhysics, along with MARJ,\nwill drive future advancements in AI for physics reasoning. Codes and data are\navailable at https://github.com/YangLabHKUST/UGPhysics .", "published": "2025-02-01 06:42:02", "link": "http://arxiv.org/abs/2502.00334v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Challenges and Innovations in LLM-Powered Fake News Detection: A\n  Synthesis of Approaches and Future Directions", "abstract": "The pervasiveness of the dissemination of fake news through social media\nplatforms poses critical risks to the trust of the general public, societal\nstability, and democratic institutions. This challenge calls for novel\nmethodologies in detection, which can keep pace with the dynamic and\nmulti-modal nature of misinformation. Recent works include powering the\ndetection using large language model advances in multimodal frameworks,\nmethodologies using graphs, and adversarial training in the literature of fake\nnews. Based on the different approaches which can bring success, some key\nhighlights will be underlined: enhanced LLM-improves accuracy through more\nadvanced semantics and cross-modality fusion for robust detections. The review\nfurther identifies critical gaps in adaptability to dynamic social media\ntrends, real-time, and cross-platform detection capabilities, as well as the\nethical challenges thrown up by the misuse of LLMs. Future directions underline\nthe development of style-agnostic models, cross-lingual detection frameworks,\nand robust policies with a view to mitigating LLM-driven misinformation. This\nsynthesis thus lays a concrete foundation for those researchers and\npractitioners committed to reinforcing fake news detection systems with\ncomplications that keep on growing in the digital landscape.", "published": "2025-02-01 06:56:17", "link": "http://arxiv.org/abs/2502.00339v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "The Impact of Persona-based Political Perspectives on Hateful Content\n  Detection", "abstract": "While pretraining language models with politically diverse content has been\nshown to improve downstream task fairness, such approaches require significant\ncomputational resources often inaccessible to many researchers and\norganizations. Recent work has established that persona-based prompting can\nintroduce political diversity in model outputs without additional training.\nHowever, it remains unclear whether such prompting strategies can achieve\nresults comparable to political pretraining for downstream tasks. We\ninvestigate this question using persona-based prompting strategies in\nmultimodal hate-speech detection tasks, specifically focusing on hate speech in\nmemes. Our analysis reveals that when mapping personas onto a political compass\nand measuring persona agreement, inherent political positioning has\nsurprisingly little correlation with classification decisions. Notably, this\nlack of correlation persists even when personas are explicitly injected with\nstronger ideological descriptors. Our findings suggest that while LLMs can\nexhibit political biases in their responses to direct political questions,\nthese biases may have less impact on practical classification tasks than\npreviously assumed. This raises important questions about the necessity of\ncomputationally expensive political pretraining for achieving fair performance\nin downstream tasks.", "published": "2025-02-01 09:53:17", "link": "http://arxiv.org/abs/2502.00385v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ALU: Agentic LLM Unlearning", "abstract": "Information removal or suppression in large language models (LLMs) is a\ndesired functionality, useful in AI regulation, legal compliance, safety, and\nprivacy. LLM unlearning methods aim to remove information on demand from LLMs.\nCurrent LLM unlearning methods struggle to balance the unlearning efficacy and\nutility due to the competing nature of these objectives. Keeping the unlearning\nprocess computationally feasible without assuming access to the model weights\nis an overlooked area. We present the first agentic LLM unlearning (ALU)\nmethod, a multi-agent, retrain-free, model-agnostic approach to LLM unlearning\nthat achieves effective unlearning while preserving the utility. Our ALU\nframework unlearns by involving multiple LLM agents, each designed for a\nspecific step in the unlearning process, without the need to update model\nweights for any of the agents in the framework. Users can easily request any\nset of unlearning instances in any sequence, and ALU seamlessly adapts in real\ntime. This is facilitated without requiring any changes in the underlying LLM\nmodel. Through extensive experiments on established benchmarks (TOFU, WMDP,\nWPU) and jailbreaking techniques (many shot, target masking, other languages),\nwe demonstrate that ALU consistently stands out as the most robust LLM\nunlearning framework among current state-of-the-art methods while incurring a\nlow constant-time cost. We further highlight ALU's superior performance\ncompared to existing methods when evaluated at scale. Specifically, ALU is\nassessed on up to 1000 unlearning targets, exceeding the evaluation scope of\nall previously proposed LLM unlearning methods.", "published": "2025-02-01 11:45:44", "link": "http://arxiv.org/abs/2502.00406v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Doing More with Less -- Implementing Routing Strategies in Large\n  Language Model-Based Systems: An Extended Survey", "abstract": "Large Language Models (LLM)-based systems, i.e. interconnected elements that\ninclude an LLM as a central component (e.g., conversational agents), are\ntypically monolithic static architectures that rely on a single LLM for all\nuser queries. However, they often require different preprocessing strategies,\nlevels of reasoning, or knowledge. Generalist LLMs (e.g. GPT-4) trained on very\nlarge multi-topic corpora can perform well in a variety of tasks. They require\nsignificant financial, energy, and hardware resources that may not be justified\nfor basic tasks. This implies potentially investing in unnecessary costs for a\ngiven query. To overcome this problem, a routing mechanism routes user queries\nto the most suitable components, such as smaller LLMs or experts in specific\ntopics. This approach may improve response quality while minimising costs.\nRouting can be expanded to other components of the conversational agent\narchitecture, such as the selection of optimal embedding strategies. This paper\nexplores key considerations for integrating routing into LLM-based systems,\nfocusing on resource management, cost definition, and strategy selection. Our\nmain contributions include a formalisation of the problem, a novel taxonomy of\nexisting approaches emphasising relevance and resource efficiency, and a\ncomparative analysis of these strategies in relation to industry practices.\nFinally, we identify critical challenges and directions for future research.", "published": "2025-02-01 12:08:38", "link": "http://arxiv.org/abs/2502.00409v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Towards Privacy-aware Mental Health AI Models: Advances, Challenges, and\n  Opportunities", "abstract": "Mental illness is a widespread and debilitating condition with substantial\nsocietal and personal costs. Traditional diagnostic and treatment approaches,\nsuch as self-reported questionnaires and psychotherapy sessions, often impose\nsignificant burdens on both patients and clinicians, limiting accessibility and\nefficiency. Recent advances in Artificial Intelligence (AI), particularly in\nNatural Language Processing and multimodal techniques, hold great potential for\nrecognizing and addressing conditions such as depression, anxiety, bipolar\ndisorder, schizophrenia, and post-traumatic stress disorder. However, privacy\nconcerns, including the risk of sensitive data leakage from datasets and\ntrained models, remain a critical barrier to deploying these AI systems in\nreal-world clinical settings. These challenges are amplified in multimodal\nmethods, where personal identifiers such as voice and facial data can be\nmisused. This paper presents a critical and comprehensive study of the privacy\nchallenges associated with developing and deploying AI models for mental\nhealth. We further prescribe potential solutions, including data anonymization,\nsynthetic data generation, and privacy-preserving model training, to strengthen\nprivacy safeguards in practical applications. Additionally, we discuss\nevaluation frameworks to assess the privacy-utility trade-offs in these\napproaches. By addressing these challenges, our work aims to advance the\ndevelopment of reliable, privacy-aware AI tools to support clinical\ndecision-making and improve mental health outcomes.", "published": "2025-02-01 15:10:02", "link": "http://arxiv.org/abs/2502.00451v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A statistically consistent measure of Semantic Variability using\n  Language Models", "abstract": "To address the issue of variability in the output generated by a language\nmodel, we present a measure of semantic variability that is statistically\nconsistent under mild assumptions. This measure, denoted as semantic spectral\nentropy, is a easy to implement algorithm that requires just off the shelf\nlanguage models. We put very few restrictions on the language models and we\nhave shown in a clear simulation studies that such method can generate accurate\nmetric despite randomness that arise from the language models.", "published": "2025-02-01 17:55:58", "link": "http://arxiv.org/abs/2502.00507v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Who's the MVP? A Game-Theoretic Evaluation Benchmark for Modular\n  Attribution in LLM Agents", "abstract": "Large Language Model (LLM) agents frameworks often employ modular\narchitectures, incorporating components such as planning, reasoning, action\nexecution, and reflection to tackle complex tasks. However, quantifying the\ncontribution of each module to overall system performance remains a significant\nchallenge, impeding optimization and interpretability. To address this, we\nintroduce CapaBench (Capability-level Assessment Benchmark), an evaluation\nframework grounded in cooperative game theory's Shapley Value, which\nsystematically measures the marginal impact of individual modules and their\ninteractions within an agent's architecture. By replacing default modules with\ntest variants across all possible combinations, CapaBench provides a principle\nmethod for attributing performance contributions. Key contributions include:\n(1) We are the first to propose a Shapley Value-based methodology for\nquantifying the contributions of capabilities in LLM agents; (2) Modules with\nhigh Shapley Values consistently lead to predictable performance gains when\ncombined, enabling targeted optimization; and (3) We build a multi-round\ndataset of over 1,500 entries spanning diverse domains and practical task\nscenarios, enabling comprehensive evaluation of agent capabilities. CapaBench\nbridges the gap between component-level evaluation and holistic system\nassessment, providing actionable insights for optimizing modular LLM agents and\nadvancing their deployment in complex, real-world scenarios.", "published": "2025-02-01 18:07:34", "link": "http://arxiv.org/abs/2502.00510v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "PolarQuant: Leveraging Polar Transformation for Efficient Key Cache\n  Quantization and Decoding Acceleration", "abstract": "The KV cache in large language models is a dominant factor in memory usage,\nlimiting their broader applicability. Quantizing the cache to lower bit widths\nis an effective way to reduce computational costs; however, previous methods\nstruggle with quantizing key vectors due to outliers, resulting in excessive\noverhead. We propose a novel quantization approach called PolarQuant, which\nefficiently addresses the outlier challenge. We observe that outliers typically\nappear in only one of two dimensions, which are rotated together by a specific\nangle when rotary position embeddings are applied. When represented as\ntwo-dimensional vectors, these dimensions exhibit well-structured patterns,\nwith radii and angles smoothly distributed in polar coordinates. This\nalleviates the challenge of outliers on per-channel quantization, making them\nwell-suited for quantization. Thus, PolarQuant divides key vectors into groups\nof two-dimensional sub-vectors, encoding them as the corresponding quantized\nradius and the polar angle, rather than quantizing original key vectors\ndirectly. PolarQuant achieves the superior efficiency in KV cache quantization\nand accelerates the decoding process by turning the query-key inner product\ninto a table lookup, all while maintaining the downstream performance of\nfull-precision models.", "published": "2025-02-01 18:59:03", "link": "http://arxiv.org/abs/2502.00527v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Vision-Language Modeling in PET/CT for Visual Grounding of Positive\n  Findings", "abstract": "Vision-language models can connect the text description of an object to its\nspecific location in an image through visual grounding. This has potential\napplications in enhanced radiology reporting. However, these models require\nlarge annotated image-text datasets, which are lacking for PET/CT. We developed\nan automated pipeline to generate weak labels linking PET/CT report\ndescriptions to their image locations and used it to train a 3D vision-language\nvisual grounding model. Our pipeline finds positive findings in PET/CT reports\nby identifying mentions of SUVmax and axial slice numbers. From 25,578 PET/CT\nexams, we extracted 11,356 sentence-label pairs. Using this data, we trained\nConTEXTual Net 3D, which integrates text embeddings from a large language model\nwith a 3D nnU-Net via token-level cross-attention. The model's performance was\ncompared against LLMSeg, a 2.5D version of ConTEXTual Net, and two nuclear\nmedicine physicians. The weak-labeling pipeline accurately identified lesion\nlocations in 98% of cases (246/251), with 7.5% requiring boundary adjustments.\nConTEXTual Net 3D achieved an F1 score of 0.80, outperforming LLMSeg (F1=0.22)\nand the 2.5D model (F1=0.53), though it underperformed both physicians (F1=0.94\nand 0.91). The model achieved better performance on FDG (F1=0.78) and DCFPyL\n(F1=0.75) exams, while performance dropped on DOTATE (F1=0.58) and Fluciclovine\n(F1=0.66). The model performed consistently across lesion sizes but showed\nreduced accuracy on lesions with low uptake. Our novel weak labeling pipeline\naccurately produced an annotated dataset of PET/CT image-text pairs,\nfacilitating the development of 3D visual grounding models. ConTEXTual Net 3D\nsignificantly outperformed other models but fell short of the performance of\nnuclear medicine physicians. Our study suggests that even larger datasets may\nbe needed to close this performance gap.", "published": "2025-02-01 18:59:31", "link": "http://arxiv.org/abs/2502.00528v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Converting Transformers into DGNNs Form", "abstract": "Recent advances in deep learning have established Transformer architectures\nas the predominant modeling paradigm. Central to the success of Transformers is\nthe self-attention mechanism, which scores the similarity between query and key\nmatrices to modulate a value matrix. This operation bears striking similarities\nto digraph convolution, prompting an investigation into whether digraph\nconvolution could serve as an alternative to self-attention. In this study, we\nformalize this concept by introducing a synthetic unitary digraph convolution\nbased on the digraph Fourier transform. The resulting model, which we term\nConverter, effectively converts a Transformer into a Directed Graph Neural\nNetwork (DGNN) form. We have tested Converter on Long-Range Arena benchmark,\nlong document classification, and DNA sequence-based taxonomy classification.\nOur experimental results demonstrate that Converter achieves superior\nperformance while maintaining computational efficiency and architectural\nsimplicity, which establishes it as a lightweight yet powerful Transformer\nvariant.", "published": "2025-02-01 22:44:46", "link": "http://arxiv.org/abs/2502.00585v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "RPGBENCH: Evaluating Large Language Models as Role-Playing Game Engines", "abstract": "We present RPGBench, the first benchmark designed to evaluate large language\nmodels (LLMs) as text-based role-playing game (RPG) engines. RPGBench comprises\ntwo core tasks: Game Creation (GC) and Game Simulation (GS). In GC, an LLM must\ncraft a valid and playable RPG world using a structured event-state\nrepresentation, ensuring logical coherence and proper termination conditions.\nIn GS, the LLM simulates interactive gameplay across multiple rounds while\nconsistently updating states and enforcing game rules. To comprehensively\nassess performance, RPGBench integrates objective and subjective evaluation\nmethodologies. Objective measures verify adherence to event mechanics and check\nvariable updates without requiring human intervention. Subjective measures,\nsuch as content interestingness, action quality, and role-playing capability,\nare evaluated via an LLM-as-a-judge framework, where a strong LLM grades each\ncandidate's outputs. Empirical results demonstrate that state-of-the-art LLMs\ncan produce engaging stories but often struggle to implement consistent,\nverifiable game mechanics, particularly in long or complex scenarios. By\ncombining structured, rule-based assessments with LLM-based judgments, RPGBench\nprovides a new standard for evaluating how well LLMs can balance creativity,\ncoherence, and complexity in text-based RPGs, opening avenues for more\nimmersive and controllable interactive storytelling.", "published": "2025-02-01 23:40:24", "link": "http://arxiv.org/abs/2502.00595v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multilingual State Space Models for Structured Question Answering in\n  Indic Languages", "abstract": "The diversity and complexity of Indic languages present unique challenges for\nnatural language processing (NLP) tasks, particularly in the domain of question\nanswering (QA).To address these challenges, this paper explores the application\nof State Space Models (SSMs),to build efficient and contextually aware QA\nsystems tailored for Indic languages. SSMs are particularly suited for this\ntask due to their ability to model long-term and short-term dependencies in\nsequential data, making them well-equipped to handle the rich morphology,\ncomplex syntax, and contextual intricacies characteristic of Indian languages.\nWe evaluated multiple SSM architectures across diverse datasets representing\nvarious Indic languages and conducted a comparative analysis of their\nperformance. Our results demonstrate that these models effectively capture\nlinguistic subtleties, leading to significant improvements in question\ninterpretation, context alignment, and answer generation. This work represents\nthe first application of SSMs to question answering tasks in Indic languages,\nestablishing a foundational benchmark for future research in this domain. We\npropose enhancements to existing SSM frameworks, optimizing their applicability\nto low-resource settings and multilingual scenarios prevalent in Indic\nlanguages.", "published": "2025-02-01 19:53:02", "link": "http://arxiv.org/abs/2502.01673v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Benchmark on Peer Review Toxic Detection: A Challenging Task with a New\n  Dataset", "abstract": "Peer review is crucial for advancing and improving science through\nconstructive criticism. However, toxic feedback can discourage authors and\nhinder scientific progress. This work explores an important but underexplored\narea: detecting toxicity in peer reviews. We first define toxicity in peer\nreviews across four distinct categories and curate a dataset of peer reviews\nfrom the OpenReview platform, annotated by human experts according to these\ndefinitions. Leveraging this dataset, we benchmark a variety of models,\nincluding a dedicated toxicity detection model, a sentiment analysis model,\nseveral open-source large language models (LLMs), and two closed-source LLMs.\nOur experiments explore the impact of different prompt granularities, from\ncoarse to fine-grained instructions, on model performance. Notably,\nstate-of-the-art LLMs like GPT-4 exhibit low alignment with human judgments\nunder simple prompts but achieve improved alignment with detailed instructions.\nMoreover, the model's confidence score is a good indicator of better alignment\nwith human judgments. For example, GPT-4 achieves a Cohen's Kappa score of 0.56\nwith human judgments, which increases to 0.63 when using only predictions with\na confidence score higher than 95%. Overall, our dataset and benchmarks\nunderscore the need for continued research to enhance toxicity detection\ncapabilities of LLMs. By addressing this issue, our work aims to contribute to\na healthy and responsible environment for constructive academic discourse and\nscientific collaboration.", "published": "2025-02-01 23:01:39", "link": "http://arxiv.org/abs/2502.01676v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Pause-Tuning for Long-Context Comprehension: A Lightweight Approach to\n  LLM Attention Recalibration", "abstract": "LLMs have demonstrated remarkable proficiency in understanding tasks but\ncontinue to struggle with long-context comprehension, particularly with content\nlocated in the middle of extensive inputs. This limitation, known as the\nLost-in-the-Middle (LITM) problem, hinders models from fully processing and\nutilizing information across lengthy contexts. To address this issue, we\nintroduce pause-tuning, a technique that redistributes attention to enhance\ncomprehension of long-context inputs. Our approach involves fine-tuning\nlanguage models on datasets with artificially inserted pause tokens, which\nserve to segment the input into smaller, more manageable parts. We evaluate\npause-tuning against alternative approaches using the Needle-in-a-Haystack\nbenchmark, where models must retrieve information embedded within contexts of\nup to 128K tokens. Experimental results demonstrate significant performance\ngains, with the LLaMA 3.2 3B Instruct model and the LLaMA 3.1 8B Instruct model\nimproving by 10.61% and 3.57% respectively on average, suggesting that\npause-tuning successfully enhances attention redistribution and improves\nlong-context retention. The code and data are available at\nhttps://anonymous.4open.science/r/LITM-PauseTokens-7357.", "published": "2025-02-01 21:47:15", "link": "http://arxiv.org/abs/2502.20405v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mordal: Automated Pretrained Model Selection for Vision Language Models", "abstract": "Incorporating multiple modalities into large language models (LLMs) is a\npowerful way to enhance their understanding of non-textual data, enabling them\nto perform multimodal tasks. Vision language models (VLMs) form the fastest\ngrowing category of multimodal models because of their many practical use\ncases, including in healthcare, robotics, and accessibility. Unfortunately,\neven though different VLMs in the literature demonstrate impressive visual\ncapabilities in different benchmarks, they are handcrafted by human experts;\nthere is no automated framework to create task-specific multimodal models.\n  We introduce Mordal, an automated multimodal model search framework that\nefficiently finds the best VLM for a user-defined task without manual\nintervention. Mordal achieves this both by reducing the number of candidates to\nconsider during the search process and by minimizing the time required to\nevaluate each remaining candidate. Our evaluation shows that Mordal can find\nthe best VLM for a given problem using up to $8.9\\times$--$11.6\\times$ lower\nGPU hours than grid search. In the process of our evaluation, we have also\ndiscovered new VLMs that outperform their state-of-the-art counterparts.", "published": "2025-02-01 00:41:29", "link": "http://arxiv.org/abs/2502.00241v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "DEUCE: Dual-diversity Enhancement and Uncertainty-awareness for\n  Cold-start Active Learning", "abstract": "Cold-start active learning (CSAL) selects valuable instances from an\nunlabeled dataset for manual annotation. It provides high-quality data at a low\nannotation cost for label-scarce text classification. However, existing CSAL\nmethods overlook weak classes and hard representative examples, resulting in\nbiased learning. To address these issues, this paper proposes a novel\ndual-diversity enhancing and uncertainty-aware (DEUCE) framework for CSAL.\nSpecifically, DEUCE leverages a pretrained language model (PLM) to efficiently\nextract textual representations, class predictions, and predictive uncertainty.\nThen, it constructs a Dual-Neighbor Graph (DNG) to combine information on both\ntextual diversity and class diversity, ensuring a balanced data distribution.\nIt further propagates uncertainty information via density-based clustering to\nselect hard representative instances. DEUCE performs well in selecting\nclass-balanced and hard representative data by dual-diversity and\ninformativeness. Experiments on six NLP datasets demonstrate the superiority\nand efficiency of DEUCE.", "published": "2025-02-01 04:00:03", "link": "http://arxiv.org/abs/2502.00305v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.6; I.2.7; I.5.1; H.3.1; H.3.3"], "primary_category": "cs.CL"}
{"title": "SigWavNet: Learning Multiresolution Signal Wavelet Network for Speech\n  Emotion Recognition", "abstract": "In the field of human-computer interaction and psychological assessment,\nspeech emotion recognition (SER) plays an important role in deciphering\nemotional states from speech signals. Despite advancements, challenges persist\ndue to system complexity, feature distinctiveness issues, and noise\ninterference. This paper introduces a new end-to-end (E2E) deep learning\nmulti-resolution framework for SER, addressing these limitations by extracting\nmeaningful representations directly from raw waveform speech signals. By\nleveraging the properties of the fast discrete wavelet transform (FDWT),\nincluding the cascade algorithm, conjugate quadrature filter, and coefficient\ndenoising, our approach introduces a learnable model for both wavelet bases and\ndenoising through deep learning techniques. The framework incorporates an\nactivation function for learnable asymmetric hard thresholding of wavelet\ncoefficients. Our approach exploits the capabilities of wavelets for effective\nlocalization in both time and frequency domains. We then combine\none-dimensional dilated convolutional neural networks (1D dilated CNN) with a\nspatial attention layer and bidirectional gated recurrent units (Bi-GRU) with a\ntemporal attention layer to efficiently capture the nuanced spatial and\ntemporal characteristics of emotional features. By handling variable-length\nspeech without segmentation and eliminating the need for pre or\npost-processing, the proposed model outperformed state-of-the-art methods on\nIEMOCAP and EMO-DB datasets. The source code of this paper is shared on the\nGithub repository:\nhttps://github.com/alaaNfissi/SigWavNet-Learning-Multiresolution-Signal-Wavelet-Network-for-Speech-Emotion-Recognition.", "published": "2025-02-01 04:18:06", "link": "http://arxiv.org/abs/2502.00310v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS", "I.2.7; I.2.6; I.2.1; I.2.0"], "primary_category": "cs.SD"}
{"title": "Distributive Fairness in Large Language Models: Evaluating Alignment\n  with Human Values", "abstract": "The growing interest in employing large language models (LLMs) for\ndecision-making in social and economic contexts has raised questions about\ntheir potential to function as agents in these domains. A significant number of\nsocietal problems involve the distribution of resources, where fairness, along\nwith economic efficiency, play a critical role in the desirability of outcomes.\nIn this paper, we examine whether LLM responses adhere to fundamental fairness\nconcepts such as equitability, envy-freeness, and Rawlsian maximin, and\ninvestigate their alignment with human preferences. We evaluate the performance\nof several LLMs, providing a comparative benchmark of their ability to reflect\nthese measures. Our results demonstrate a lack of alignment between current LLM\nresponses and human distributional preferences. Moreover, LLMs are unable to\nutilize money as a transferable resource to mitigate inequality. Nonetheless,\nwe demonstrate a stark contrast when (some) LLMs are tasked with selecting from\na predefined menu of options rather than generating one. In addition, we\nanalyze the robustness of LLM responses to variations in semantic factors (e.g.\nintentions or personas) or non-semantic prompting changes (e.g. templates or\norderings). Finally, we highlight potential strategies aimed at enhancing the\nalignment of LLM behavior with well-established fairness concepts.", "published": "2025-02-01 04:24:47", "link": "http://arxiv.org/abs/2502.00313v1", "categories": ["cs.GT", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.GT"}
{"title": "Enhancing Token Filtering Efficiency in Large Language Model Training\n  with Collider", "abstract": "Token filtering has been proposed to enhance utility of large language models\n(LLMs) by eliminating inconsequential tokens during training. While using fewer\ntokens should reduce computational workloads, existing studies have not\nsucceeded in achieving higher efficiency. This is primarily due to the\ninsufficient sparsity caused by filtering tokens only in the output layers, as\nwell as inefficient sparse GEMM (General Matrix Multiplication), even when\nhaving sufficient sparsity.\n  This paper presents Collider, a system unleashing the full efficiency of\ntoken filtering in LLM training. At its core, Collider filters activations of\ninconsequential tokens across all layers to maintain sparsity. Additionally, it\nfeatures an automatic workflow that transforms sparse GEMM into\ndimension-reduced dense GEMM for optimized efficiency. Evaluations on three\nLLMs-TinyLlama-1.1B, Qwen2.5-1.5B, and Phi1.5-1.4B-demonstrate that Collider\nreduces backpropagation time by up to 35.1% and end-to-end training time by up\nto 22.0% when filtering 40% of tokens. Utility assessments of training\nTinyLlama on 15B tokens indicate that Collider sustains the utility\nadvancements of token filtering by relatively improving model utility by 16.3%\ncomparing to regular training, and reduces training time from 4.7 days to 3.5\ndays using 8 GPUs. Collider is designed for easy integration into existing LLM\ntraining frameworks, allowing systems already using token filtering to\naccelerate training with just one line of code.", "published": "2025-02-01 06:57:01", "link": "http://arxiv.org/abs/2502.00340v1", "categories": ["cs.LG", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Sagalee: an Open Source Automatic Speech Recognition Dataset for Oromo\n  Language", "abstract": "We present a novel Automatic Speech Recognition (ASR) dataset for the Oromo\nlanguage, a widely spoken language in Ethiopia and neighboring regions. The\ndataset was collected through a crowd-sourcing initiative, encompassing a\ndiverse range of speakers and phonetic variations. It consists of 100 hours of\nreal-world audio recordings paired with transcriptions, covering read speech in\nboth clean and noisy environments. This dataset addresses the critical need for\nASR resources for the Oromo language which is underrepresented. To show its\napplicability for the ASR task, we conducted experiments using the Conformer\nmodel, achieving a Word Error Rate (WER) of 15.32% with hybrid CTC and AED loss\nand WER of 18.74% with pure CTC loss. Additionally, fine-tuning the Whisper\nmodel resulted in a significantly improved WER of 10.82%. These results\nestablish baselines for Oromo ASR, highlighting both the challenges and the\npotential for improving ASR performance in Oromo. The dataset is publicly\navailable at https://github.com/turinaf/sagalee and we encourage its use for\nfurther research and development in Oromo speech processing.", "published": "2025-02-01 12:47:36", "link": "http://arxiv.org/abs/2502.00421v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Bridging Internal Probability and Self-Consistency for Effective and\n  Efficient LLM Reasoning", "abstract": "Recent advancements in large language models (LLMs) have demonstrated\nremarkable reasoning capabilities. However, single-shot inference often yields\nunreliable results for complex reasoning tasks, leading researchers to explore\nmultiple reasoning paths through methods such as perplexity and\nself-consistency. In this paper, we present the first theoretical error\ndecomposition analysis of these techniques, breaking down their error into\nestimation error and model error. Our analysis reveals a fundamental trade-off:\nperplexity methods suffer from substantial model error due to the absence of a\nproper consistency function, while self-consistency exhibits high estimation\nerror due to a slow error convergence rate. To overcome these limitations, we\npropose Reasoning-Pruning Perplexity Consistency (RPC). This approach combines\nPerplexity Consistency, which seamlessly integrates LLM perplexity with\nself-consistency, and Reasoning Pruning, which eliminates low-probability\nreasoning paths to effectively prevent the degeneration of estimation error\nreduction. Theoretical analysis demonstrates that RPC not only accelerates the\nconvergence rate of estimation error to an exponential level but also holds\nstrong potential for further reducing model error. Extensive empirical\nevaluations on seven benchmark datasets confirm that RPC can significantly\nimprove reasoning performance, sample efficiency, and confidence reliability.", "published": "2025-02-01 18:09:49", "link": "http://arxiv.org/abs/2502.00511v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Understanding Multimodal LLMs Under Distribution Shifts: An\n  Information-Theoretic Approach", "abstract": "Multimodal large language models (MLLMs) have shown promising capabilities\nbut struggle under distribution shifts, where evaluation data differ from\ninstruction tuning distributions. Although previous works have provided\nempirical evaluations, we argue that establishing a formal framework that can\ncharacterize and quantify the risk of MLLMs is necessary to ensure the safe and\nreliable application of MLLMs in the real world. By taking an\ninformation-theoretic perspective, we propose the first theoretical framework\nthat enables the quantification of the maximum risk of MLLMs under distribution\nshifts. Central to our framework is the introduction of Effective Mutual\nInformation (EMI), a principled metric that quantifies the relevance between\ninput queries and model responses. We derive an upper bound for the EMI\ndifference between in-distribution (ID) and out-of-distribution (OOD) data,\nconnecting it to visual and textual distributional discrepancies. Extensive\nexperiments on real benchmark datasets, spanning 61 shift scenarios empirically\nvalidate our theoretical insights.", "published": "2025-02-01 22:06:56", "link": "http://arxiv.org/abs/2502.00577v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Defense Against the Dark Prompts: Mitigating Best-of-N Jailbreaking with\n  Prompt Evaluation", "abstract": "Recent work showed Best-of-N (BoN) jailbreaking using repeated use of random\naugmentations (such as capitalization, punctuation, etc) is effective against\nall major large language models (LLMs). We have found that $100\\%$ of the BoN\npaper's successful jailbreaks (confidence interval $[99.65\\%, 100.00\\%]$) and\n$99.8\\%$ of successful jailbreaks in our replication (confidence interval\n$[99.28\\%, 99.98\\%]$) were blocked with our Defense Against The Dark Prompts\n(DATDP) method. The DATDP algorithm works by repeatedly utilizing an evaluation\nLLM to evaluate a prompt for dangerous or manipulative behaviors--unlike some\nother approaches, DATDP also explicitly looks for jailbreaking attempts--until\na robust safety rating is generated. This success persisted even when utilizing\nsmaller LLMs to power the evaluation (Claude and LLaMa-3-8B-instruct proved\nalmost equally capable). These results show that, though language models are\nsensitive to seemingly innocuous changes to inputs, they seem also capable of\nsuccessfully evaluating the dangers of these inputs. Versions of DATDP can\ntherefore be added cheaply to generative AI systems to produce an immediate\nsignificant increase in safety.", "published": "2025-02-01 22:26:30", "link": "http://arxiv.org/abs/2502.00580v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY", "I.2.0"], "primary_category": "cs.CR"}
{"title": "Data-Driven Mispronunciation Pattern Discovery for Robust Speech\n  Recognition", "abstract": "Recent advancements in machine learning have significantly improved speech\nrecognition, but recognizing speech from non-fluent or accented speakers\nremains a challenge. Previous efforts, relying on rule-based pronunciation\npatterns, have struggled to fully capture non-native errors. We propose two\ndata-driven approaches using speech corpora to automatically detect\nmispronunciation patterns. By aligning non-native phones with their native\ncounterparts using attention maps, we achieved a 5.7% improvement in speech\nrecognition on native English datasets and a 12.8% improvement for non-native\nEnglish speakers, particularly Korean speakers. Our method offers practical\nadvancements for robust Automatic Speech Recognition (ASR) systems particularly\nfor situations where prior linguistic knowledge is not applicable.", "published": "2025-02-01 22:41:43", "link": "http://arxiv.org/abs/2502.00583v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Speculative Ensemble: Fast Large Language Model Ensemble via Speculation", "abstract": "Ensemble methods enhance Large Language Models (LLMs) by combining multiple\nmodels but suffer from high computational costs. In this paper, we introduce\nSpeculative Ensemble, a novel framework that accelerates LLM ensembles without\nsacrificing performance, inspired by Speculative Decoding-where a small\nproposal model generates tokens sequentially, and a larger target model\nverifies them in parallel. Our approach builds on two key insights: (1) the\nverification distribution can be the ensemble distribution of both the proposal\nand target models, and (2) alternating each model as the proposer and verifier\ncan further enhance efficiency. We generalize this method to ensembles with n\nmodels and theoretically prove that SE is never slower than a standard\nensemble, typically achieving faster speed. Extensive experiments demonstrate\nspeed improvements of 1.11x-2.23x over standard ensemble techniques without\ncompromising generation quality. Our code is available at\nhttps://github.com/Kamichanw/Speculative-Ensemble/", "published": "2025-02-01 05:22:11", "link": "http://arxiv.org/abs/2502.01662v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Solving the Content Gap in Roblox Game Recommendations: LLM-Based\n  Profile Generation and Reranking", "abstract": "With the vast and dynamic user-generated content on Roblox, creating\neffective game recommendations requires a deep understanding of game content.\nTraditional recommendation models struggle with the inconsistent and sparse\nnature of game text features such as titles and descriptions. Recent\nadvancements in large language models (LLMs) offer opportunities to enhance\nrecommendation systems by analyzing in-game text data. This paper addresses two\nchallenges: generating high-quality, structured text features for games without\nextensive human annotation, and validating these features to ensure they\nimprove recommendation relevance. We propose an approach that extracts in-game\ntext and uses LLMs to infer attributes such as genre and gameplay objectives\nfrom raw player interactions. Additionally, we introduce an LLM-based\nre-ranking mechanism to assess the effectiveness of the generated text\nfeatures, enhancing personalization and user satisfaction. Beyond\nrecommendations, our approach supports applications such as user\nengagement-based integrity detection, already deployed in production. This\nscalable framework demonstrates the potential of in-game text understanding to\nimprove recommendation quality on Roblox and adapt recommendations to its\nunique, user-generated ecosystem.", "published": "2025-02-01 06:30:56", "link": "http://arxiv.org/abs/2502.06802v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented\n  Generation", "abstract": "Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to\ngenerate grounded responses by leveraging external knowledge databases without\naltering model parameters. Although the absence of weight tuning prevents\nleakage via model parameters, it introduces the risk of inference adversaries\nexploiting retrieved documents in the model's context. Existing methods for\nmembership inference and data extraction often rely on jailbreaking or\ncarefully crafted unnatural queries, which can be easily detected or thwarted\nwith query rewriting techniques common in RAG systems. In this work, we present\nInterrogation Attack (IA), a membership inference technique targeting documents\nin the RAG datastore. By crafting natural-text queries that are answerable only\nwith the target document's presence, our approach demonstrates successful\ninference with just 30 queries while remaining stealthy; straightforward\ndetectors identify adversarial prompts from existing methods up to ~76x more\nfrequently than those generated by our attack. We observe a 2x improvement in\nTPR@1%FPR over prior inference attacks across diverse RAG configurations, all\nwhile costing less than $0.02 per document inference.", "published": "2025-02-01 04:01:18", "link": "http://arxiv.org/abs/2502.00306v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CR"}
{"title": "A Unit-based System and Dataset for Expressive Direct Speech-to-Speech\n  Translation", "abstract": "Current research in speech-to-speech translation (S2ST) primarily\nconcentrates on translation accuracy and speech naturalness, often overlooking\nkey elements like paralinguistic information, which is essential for conveying\nemotions and attitudes in communication. To address this, our research\nintroduces a novel, carefully curated multilingual dataset from various movie\naudio tracks. Each dataset pair is precisely matched for paralinguistic\ninformation and duration. We enhance this by integrating multiple prosody\ntransfer techniques, aiming for translations that are accurate,\nnatural-sounding, and rich in paralinguistic details. Our experimental results\nconfirm that our model retains more paralinguistic information from the source\nspeech while maintaining high standards of translation accuracy and\nnaturalness.", "published": "2025-02-01 09:24:32", "link": "http://arxiv.org/abs/2502.00374v1", "categories": ["cs.CL", "cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "When End-to-End is Overkill: Rethinking Cascaded Speech-to-Text\n  Translation", "abstract": "Though end-to-end speech-to-text translation has been a great success, we\nargue that the cascaded speech-to-text translation model still has its place,\nwhich is usually criticized for the error propagation between automatic speech\nrecognition (ASR) and machine translation (MT) models. In this paper, we\nexplore the benefits of incorporating multiple candidates from ASR and\nself-supervised speech features into MT. Our analysis reveals that the primary\ncause of cascading errors stems from the increased divergence between similar\nsamples in the speech domain when mapped to the text domain. By including\nmultiple candidates and self-supervised speech features, our approach allows\nthe machine translation model to choose the right words and ensure precise\ntranslation using various speech samples. This strategy minimizes error spread\nand takes advantage of large ASR and MT datasets, along with pre-trained ASR/MT\nmodels, while addressing associated issues.", "published": "2025-02-01 09:29:21", "link": "http://arxiv.org/abs/2502.00377v1", "categories": ["cs.CL", "cs.AI", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Toward noise-robust whisper keyword spotting on headphones with\n  in-earcup microphone and curriculum learning", "abstract": "The expanding feature set of modern headphones puts a challenge on the design\nof their control interface. Users may want to separately control each feature\nor quickly switch between modes that activate different features. Traditional\napproach of physical buttons may no longer be feasible when the feature set is\nlarge. Keyword spotting with voice commands is a promising solution to the\nissue. Most existing methods of keyword spotting only support commands spoken\nin a regular voice. However, regular voice may not be desirable in quiet places\nor public settings. In this paper, we investigate the problem of on-device\nkeyword spotting in whisper voice and explore approaches to improve noise\nrobustness. We leverage the inner microphone on noise-cancellation headphones\nas an additional source of voice input. We also design a curriculum learning\nstrategy that gradually increases the proportion of whisper keywords during\ntraining. We demonstrate through experiments that the combination of\nmulti-microphone processing and curriculum learning could improve F1 score of\nwhisper keyword spotting by up to 15% in noisy conditions.", "published": "2025-02-01 03:37:17", "link": "http://arxiv.org/abs/2502.00295v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Do neonates hear what we measure? Assessing neonatal ward soundscapes at\n  the neonates ears", "abstract": "Acoustic guidelines for neonatal intensive care units (NICUs) aim to protect\nvulnerable neonates from noise-induced physiological harm. However, the lack of\nrecognised international standards for measuring neonatal soundscapes has led\nto inconsistencies in instrumentation and microphone placement in existing\nliterature, raising concerns about the relevance and effectiveness of these\nguidelines. This study addresses these gaps through long-term acoustic\nmeasurements in an operational NICU and a high-dependency ward. We investigate\nthe influence of microphone positioning, bed placement, and ward layout on the\nassessment of NICU soundscapes. Beyond traditional A-weighted decibel metrics,\nthis study evaluates C-weighted metrics for low-frequency noise, the occurrence\nof tonal sounds (e.g., alarms), and transient loud events known to disrupt\nneonates' sleep. Using linear mixed-effects models with aligned ranks\ntransformation ANOVA (LME-ART-ANOVA), our results reveal significant\ndifferences in measured noise levels based on microphone placement,\nhighlighting the importance of capturing sound as perceived directly at the\nneonate's ears. Additionally, bed position and ward layout significantly impact\nnoise exposure, with a NICU bed position consistently exhibiting the highest\nsound levels across all (psycho)acoustic metrics. These findings support the\nadoption of binaural measurements along with the integration of additional\n(psycho)acoustic metrics, such as tonality and transient event occurrence\nrates, to reliably characterise the neonatal auditory experience.", "published": "2025-02-01 21:22:15", "link": "http://arxiv.org/abs/2502.00565v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AudioGenX: Explainability on Text-to-Audio Generative Models", "abstract": "Text-to-audio generation models (TAG) have achieved significant advances in\ngenerating audio conditioned on text descriptions. However, a critical\nchallenge lies in the lack of transparency regarding how each textual input\nimpacts the generated audio. To address this issue, we introduce AudioGenX, an\nExplainable AI (XAI) method that provides explanations for text-to-audio\ngeneration models by highlighting the importance of input tokens. AudioGenX\noptimizes an Explainer by leveraging factual and counterfactual objective\nfunctions to provide faithful explanations at the audio token level. This\nmethod offers a detailed and comprehensive understanding of the relationship\nbetween text inputs and audio outputs, enhancing both the explainability and\ntrustworthiness of TAG models. Extensive experiments demonstrate the\neffectiveness of AudioGenX in producing faithful explanations, benchmarked\nagainst existing methods using novel evaluation metrics specifically designed\nfor audio generation tasks.", "published": "2025-02-01 15:37:42", "link": "http://arxiv.org/abs/2502.00459v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Do Audio-Visual Segmentation Models Truly Segment Sounding Objects?", "abstract": "Unlike traditional visual segmentation, audio-visual segmentation (AVS)\nrequires the model not only to identify and segment objects but also to\ndetermine whether they are sound sources. Recent AVS approaches, leveraging\ntransformer architectures and powerful foundation models like SAM, have\nachieved impressive performance on standard benchmarks. Yet, an important\nquestion remains: Do these models genuinely integrate audio-visual cues to\nsegment sounding objects? In this paper, we systematically investigate this\nissue in the context of robust AVS. Our study reveals a fundamental bias in\ncurrent methods: they tend to generate segmentation masks based predominantly\non visual salience, irrespective of the audio context. This bias results in\nunreliable predictions when sounds are absent or irrelevant. To address this\nchallenge, we introduce AVSBench-Robust, a comprehensive benchmark\nincorporating diverse negative audio scenarios including silence, ambient\nnoise, and off-screen sounds. We also propose a simple yet effective approach\ncombining balanced training with negative samples and classifier-guided\nsimilarity learning. Our extensive experiments show that state-of-theart AVS\nmethods consistently fail under negative audio conditions, demonstrating the\nprevalence of visual bias. In contrast, our approach achieves remarkable\nimprovements in both standard metrics and robustness measures, maintaining\nnear-perfect false positive rates while preserving highquality segmentation\nperformance.", "published": "2025-02-01 07:40:29", "link": "http://arxiv.org/abs/2502.00358v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
