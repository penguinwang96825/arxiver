{"title": "Benchmarking Modern Named Entity Recognition Techniques for Free-text\n  Health Record De-identification", "abstract": "Electronic Health Records (EHRs) have become the primary form of medical\ndata-keeping across the United States. Federal law restricts the sharing of any\nEHR data that contains protected health information (PHI). De-identification,\nthe process of identifying and removing all PHI, is crucial for making EHR data\npublicly available for scientific research. This project explores several deep\nlearning-based named entity recognition (NER) methods to determine which\nmethod(s) perform better on the de-identification task. We trained and tested\nour models on the i2b2 training dataset, and qualitatively assessed their\nperformance using EHR data collected from a local hospital. We found that 1)\nBiLSTM-CRF represents the best-performing encoder/decoder combination, 2)\ncharacter-embeddings and CRFs tend to improve precision at the price of recall,\nand 3) transformers alone under-perform as context encoders. Future work\nfocused on structuring medical text may improve the extraction of semantic and\nsyntactic information for the purposes of EHR de-identification.", "published": "2021-03-25 01:26:58", "link": "http://arxiv.org/abs/2103.13546v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Term-community-based topic detection with variable resolution", "abstract": "Network-based procedures for topic detection in huge text collections offer\nan intuitive alternative to probabilistic topic models. We present in detail a\nmethod that is especially designed with the requirements of domain experts in\nmind. Like similar methods, it employs community detection in term\nco-occurrence graphs, but it is enhanced by including a resolution parameter\nthat can be used for changing the targeted topic granularity. We also establish\na term ranking and use semantic word-embedding for presenting term communities\nin a way that facilitates their interpretation. We demonstrate the application\nof our method with a widely used corpus of general news articles and show the\nresults of detailed social-sciences expert evaluations of detected topics at\nvarious resolutions. A comparison with topics detected by Latent Dirichlet\nAllocation is also included. Finally, we discuss factors that influence topic\ninterpretation.", "published": "2021-03-25 01:29:39", "link": "http://arxiv.org/abs/2103.13550v2", "categories": ["cs.CL", "I.2.7; I.5.3"], "primary_category": "cs.CL"}
{"title": "BERT4SO: Neural Sentence Ordering by Fine-tuning BERT", "abstract": "Sentence ordering aims to arrange the sentences of a given text in the\ncorrect order. Recent work frames it as a ranking problem and applies deep\nneural networks to it. In this work, we propose a new method, named BERT4SO, by\nfine-tuning BERT for sentence ordering. We concatenate all sentences and\ncompute their representations by using multiple special tokens and carefully\ndesigned segment (interval) embeddings. The tokens across multiple sentences\ncan attend to each other which greatly enhances their interactions. We also\npropose a margin-based listwise ranking loss based on ListMLE to facilitate the\noptimization process. Experimental results on five benchmark datasets\ndemonstrate the effectiveness of our proposed method.", "published": "2021-03-25 03:32:32", "link": "http://arxiv.org/abs/2103.13584v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mask Attention Networks: Rethinking and Strengthen Transformer", "abstract": "Transformer is an attention-based neural network, which consists of two\nsublayers, namely, Self-Attention Network (SAN) and Feed-Forward Network (FFN).\nExisting research explores to enhance the two sublayers separately to improve\nthe capability of Transformer for text representation. In this paper, we\npresent a novel understanding of SAN and FFN as Mask Attention Networks (MANs)\nand show that they are two special cases of MANs with static mask matrices.\nHowever, their static mask matrices limit the capability for localness modeling\nin text representation learning. We therefore introduce a new layer named\ndynamic mask attention network (DMAN) with a learnable mask matrix which is\nable to model localness adaptively. To incorporate advantages of DMAN, SAN, and\nFFN, we propose a sequential layered structure to combine the three types of\nlayers. Extensive experiments on various tasks, including neural machine\ntranslation and text summarization demonstrate that our model outperforms the\noriginal Transformer.", "published": "2021-03-25 04:07:44", "link": "http://arxiv.org/abs/2103.13597v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Approach to Improve Robustness of NLP Systems against ASR Errors", "abstract": "Speech-enabled systems typically first convert audio to text through an\nautomatic speech recognition (ASR) model and then feed the text to downstream\nnatural language processing (NLP) modules. The errors of the ASR system can\nseriously downgrade the performance of the NLP modules. Therefore, it is\nessential to make them robust to the ASR errors. Previous work has shown it is\neffective to employ data augmentation methods to solve this problem by\ninjecting ASR noise during the training process. In this paper, we utilize the\nprevalent pre-trained language model to generate training samples with\nASR-plausible noise. Compare to the previous methods, our approach generates\nASR noise that better fits the real-world error distribution. Experimental\nresults on spoken language translation(SLT) and spoken language understanding\n(SLU) show that our approach effectively improves the system robustness against\nthe ASR errors and achieves state-of-the-art results on both tasks.", "published": "2021-03-25 05:15:43", "link": "http://arxiv.org/abs/2103.13610v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pruning-then-Expanding Model for Domain Adaptation of Neural Machine\n  Translation", "abstract": "Domain Adaptation is widely used in practical applications of neural machine\ntranslation, which aims to achieve good performance on both the general-domain\nand in-domain. However, the existing methods for domain adaptation usually\nsuffer from catastrophic forgetting, domain divergence, and model explosion. To\naddress these three problems, we propose a method of \"divide and conquer\" which\nis based on the importance of neurons or parameters in the translation model.\nIn our method, we first prune the model and only keep the important neurons or\nparameters, making them responsible for both general-domain and in-domain\ntranslation. Then we further train the pruned model supervised by the original\nunpruned model with the knowledge distillation method. Last we expand the model\nto the original size and fine-tune the added parameters for the in-domain\ntranslation. We conduct experiments on different languages and domains and the\nresults show that our method can achieve significant improvements compared with\nseveral strong baselines.", "published": "2021-03-25 08:57:09", "link": "http://arxiv.org/abs/2103.13678v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bertinho: Galician BERT Representations", "abstract": "This paper presents a monolingual BERT model for Galician. We follow the\nrecent trend that shows that it is feasible to build robust monolingual BERT\nmodels even for relatively low-resource languages, while performing better than\nthe well-known official multilingual BERT (mBERT). More particularly, we\nrelease two monolingual Galician BERT models, built using 6 and 12 transformer\nlayers, respectively; trained with limited resources (~45 million tokens on a\nsingle GPU of 24GB). We then provide an exhaustive evaluation on a number of\ntasks such as POS-tagging, dependency parsing and named entity recognition. For\nthis purpose, all these tasks are cast in a pure sequence labeling setup in\norder to run BERT without the need to include any additional layers on top of\nit (we only use an output classification layer to map the contextualized\nrepresentations into the predicted label). The experiments show that our\nmodels, especially the 12-layer one, outperform the results of mBERT in most\ntasks.", "published": "2021-03-25 12:51:34", "link": "http://arxiv.org/abs/2103.13799v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Visual Grounding Strategies for Text-Only Natural Language Processing", "abstract": "Visual grounding is a promising path toward more robust and accurate Natural\nLanguage Processing (NLP) models. Many multimodal extensions of BERT (e.g.,\nVideoBERT, LXMERT, VL-BERT) allow a joint modeling of texts and images that\nlead to state-of-the-art results on multimodal tasks such as Visual Question\nAnswering. Here, we leverage multimodal modeling for purely textual tasks\n(language modeling and classification) with the expectation that the multimodal\npretraining provides a grounding that can improve text processing accuracy. We\npropose possible strategies in this respect. A first type of strategy, referred\nto as {\\it transferred grounding} consists in applying multimodal models to\ntext-only tasks using a placeholder to replace image input. The second one,\nwhich we call {\\it associative grounding}, harnesses image retrieval to match\ntexts with related images during both pretraining and text-only downstream\ntasks. We draw further distinctions into both strategies and then compare them\naccording to their impact on language modeling and commonsense-related\ndownstream tasks, showing improvement over text-only baselines.", "published": "2021-03-25 16:03:00", "link": "http://arxiv.org/abs/2103.13942v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "K-XLNet: A General Method for Combining Explicit Knowledge with Language\n  Model Pretraining", "abstract": "Though pre-trained language models such as Bert and XLNet, have rapidly\nadvanced the state-of-the-art on many NLP tasks, they implicit semantics only\nrelying on surface information between words in corpus. Intuitively, background\nknowledge influences the efficacy of understanding. Inspired by this common\nsense, we focus on improving model pretraining by leveraging explicit\nknowledge. Different from recent research that optimize pretraining model by\nknowledge masking strategies, we propose a simple but general method to combine\nexplicit knowledge with pretraining. To be specific, we first match knowledge\nfacts from knowledge graph (KG) and then add a knowledge injunction layer to\ntransformer directly without changing its architecture. The present study seeks\nto find the direct impact of explicit knowledge on transformer per-training. We\nconduct experiments on various datasets for different downstream tasks. The\nexperimental results show that solely by adding external knowledge to\ntransformer can improve the learning performance on many NLP tasks.", "published": "2021-03-25 06:14:18", "link": "http://arxiv.org/abs/2104.10649v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reading and Acting while Blindfolded: The Need for Semantics in Text\n  Game Agents", "abstract": "Text-based games simulate worlds and interact with players using natural\nlanguage. Recent work has used them as a testbed for autonomous\nlanguage-understanding agents, with the motivation being that understanding the\nmeanings of words or semantics is a key component of how humans understand,\nreason, and act in these worlds. However, it remains unclear to what extent\nartificial agents utilize semantic understanding of the text. To this end, we\nperform experiments to systematically reduce the amount of semantic information\navailable to a learning agent. Surprisingly, we find that an agent is capable\nof achieving high scores even in the complete absence of language semantics,\nindicating that the currently popular experimental setup and models may be\npoorly designed to understand and leverage game texts. To remedy this\ndeficiency, we propose an inverse dynamics decoder to regularize the\nrepresentation space and encourage exploration, which shows improved\nperformance on several games including Zork I. We discuss the implications of\nour findings for designing future agents with stronger semantic understanding.", "published": "2021-03-25 01:35:27", "link": "http://arxiv.org/abs/2103.13552v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Online Forums Summarization via Hierarchical Unified Deep\n  Neural Network", "abstract": "Online discussion forums are prevalent and easily accessible, thus allowing\npeople to share ideas and opinions by posting messages in the discussion\nthreads. Forum threads that significantly grow in length can become difficult\nfor participants, both newcomers and existing, to grasp main ideas. To mitigate\nthis problem, this study aims to create an automatic text summarizer for online\nforums. We present Hierarchical Unified Deep Neural Network to build sentence\nand thread representations for the forum summarization. In this scheme, Bi-LSTM\nderives a representation that comprises information of the whole sentence and\nwhole thread; whereas, CNN captures most informative features with respect to\ncontext from sentence and thread. Attention mechanism is applied on top of CNN\nto further highlight high-level representations that carry important\ninformation contributing to a desirable summary. Extensive performance\nevaluation has been conducted on three datasets, two of which are real-life\nonline forums and one is news dataset. The results reveal that the proposed\nmodel outperforms several competitive baselines.", "published": "2021-03-25 03:38:56", "link": "http://arxiv.org/abs/2103.13587v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Predicting Directionality in Causal Relations in Text", "abstract": "In this work, we test the performance of two bidirectional transformer-based\nlanguage models, BERT and SpanBERT, on predicting directionality in causal\npairs in the textual content. Our preliminary results show that predicting\ndirection for inter-sentence and implicit causal relations is more challenging.\nAnd, SpanBERT performs better than BERT on causal samples with longer span\nlength. We also introduce CREST which is a framework for unifying a collection\nof scattered datasets of causal relations.", "published": "2021-03-25 04:49:01", "link": "http://arxiv.org/abs/2103.13606v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Real-time low-resource phoneme recognition on edge devices", "abstract": "While speech recognition has seen a surge in interest and research over the\nlast decade, most machine learning models for speech recognition either require\nlarge training datasets or lots of storage and memory. Combined with the\nprominence of English as the number one language in which audio data is\navailable, this means most other languages currently lack good speech\nrecognition models.\n  The method presented in this paper shows how to create and train models for\nspeech recognition in any language which are not only highly accurate, but also\nrequire very little storage, memory and training data when compared with\ntraditional models. This allows training models to recognize any language and\ndeploying them on edge devices such as mobile phones or car displays for fast\nreal-time speech recognition.", "published": "2021-03-25 17:34:59", "link": "http://arxiv.org/abs/2103.13997v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Equality before the Law: Legal Judgment Consistency Analysis for\n  Fairness", "abstract": "In a legal system, judgment consistency is regarded as one of the most\nimportant manifestations of fairness. However, due to the complexity of factual\nelements that impact sentencing in real-world scenarios, few works have been\ndone on quantitatively measuring judgment consistency towards real-world data.\nIn this paper, we propose an evaluation metric for judgment inconsistency,\nLegal Inconsistency Coefficient (LInCo), which aims to evaluate inconsistency\nbetween data groups divided by specific features (e.g., gender, region, race).\nWe propose to simulate judges from different groups with legal judgment\nprediction (LJP) models and measure the judicial inconsistency with the\ndisagreement of the judgment results given by LJP models trained on different\ngroups. Experimental results on the synthetic data verify the effectiveness of\nLInCo. We further employ LInCo to explore the inconsistency in real cases and\ncome to the following observations: (1) Both regional and gender inconsistency\nexist in the legal system, but gender inconsistency is much less than regional\ninconsistency; (2) The level of regional inconsistency varies little across\ndifferent time periods; (3) In general, judicial inconsistency is negatively\ncorrelated with the severity of the criminal charges. Besides, we use LInCo to\nevaluate the performance of several de-bias methods, such as adversarial\nlearning, and find that these mechanisms can effectively help LJP models to\navoid suffering from data bias.", "published": "2021-03-25 14:28:00", "link": "http://arxiv.org/abs/2103.13868v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Residual Energy-Based Models for End-to-End Speech Recognition", "abstract": "End-to-end models with auto-regressive decoders have shown impressive results\nfor automatic speech recognition (ASR). These models formulate the\nsequence-level probability as a product of the conditional probabilities of all\nindividual tokens given their histories. However, the performance of locally\nnormalised models can be sub-optimal because of factors such as exposure bias.\nConsequently, the model distribution differs from the underlying data\ndistribution. In this paper, the residual energy-based model (R-EBM) is\nproposed to complement the auto-regressive ASR model to close the gap between\nthe two distributions. Meanwhile, R-EBMs can also be regarded as\nutterance-level confidence estimators, which may benefit many downstream tasks.\nExperiments on a 100hr LibriSpeech dataset show that R-EBMs can reduce the word\nerror rates (WERs) by 8.2%/6.7% while improving areas under precision-recall\ncurves of confidence scores by 12.6%/28.4% on test-clean/test-other sets.\nFurthermore, on a state-of-the-art model using self-supervised learning\n(wav2vec 2.0), R-EBMs still significantly improves both the WER and confidence\nestimation performance.", "published": "2021-03-25 22:08:00", "link": "http://arxiv.org/abs/2103.14152v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "EfficientTDNN: Efficient Architecture Search for Speaker Recognition", "abstract": "Convolutional neural networks (CNNs), such as the time-delay neural network\n(TDNN), have shown their remarkable capability in learning speaker embedding.\nHowever, they meanwhile bring a huge computational cost in storage size,\nprocessing, and memory. Discovering the specialized CNN that meets a specific\nconstraint requires a substantial effort of human experts. Compared with\nhand-designed approaches, neural architecture search (NAS) appears as a\npractical technique in automating the manual architecture design process and\nhas attracted increasing interest in spoken language processing tasks such as\nspeaker recognition. In this paper, we propose EfficientTDNN, an efficient\narchitecture search framework consisting of a TDNN-based supernet and a\nTDNN-NAS algorithm. The proposed supernet introduces temporal convolution of\ndifferent ranges of the receptive field and feature aggregation of various\nresolutions from different layers to TDNN. On top of it, the TDNN-NAS algorithm\nquickly searches for the desired TDNN architecture via weight-sharing subnets,\nwhich surprisingly reduces computation while handling the vast number of\ndevices with various resources requirements. Experimental results on the\nVoxCeleb dataset show the proposed EfficientTDNN enables approximate $10^{13}$\narchitectures concerning depth, kernel, and width. Considering different\ncomputation constraints, it achieves a 2.20% equal error rate (EER) with 204M\nmultiply-accumulate operations (MACs), 1.41% EER with 571M MACs as well as\n0.94% EER with 1.45G MACs. Comprehensive investigations suggest that the\ntrained supernet generalizes subnets not sampled during training and obtains a\nfavorable trade-off between accuracy and efficiency.", "published": "2021-03-25 03:28:07", "link": "http://arxiv.org/abs/2103.13581v5", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Radically Old Way of Computing Spectra: Applications in End-to-End ASR", "abstract": "We propose a technique to compute spectrograms using Frequency Domain Linear\nPrediction (FDLP) that uses all-pole models to fit the squared Hilbert envelope\nof speech in different frequency sub-bands. The spectrogram of a complete\nspeech utterance is computed by overlap-add of contiguous all-pole model\nresponses. A long context window of 1.5 seconds allows us to capture the low\nfrequency temporal modulations of speech in the spectrogram. For an end-to-end\nautomatic speech recognition task, the FDLP spectrogram performs on par with\nthe standard mel spectrogram features for clean read speech training and test\ndata. For more realistic speech data with train-test domain mismatches or\nreverberations, FDLP spectrogram shows up to 25% and 22% relative WER\nimprovements over mel spectrogram respectively.", "published": "2021-03-25 20:38:49", "link": "http://arxiv.org/abs/2103.14129v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Weakly-supervised Audio-visual Sound Source Detection and Separation", "abstract": "Learning how to localize and separate individual object sounds in the audio\nchannel of the video is a difficult task. Current state-of-the-art methods\npredict audio masks from artificially mixed spectrograms, known as\nMix-and-Separate framework. We propose an audio-visual co-segmentation, where\nthe network learns both what individual objects look and sound like, from\nvideos labeled with only object labels. Unlike other recent visually-guided\naudio source separation frameworks, our architecture can be learned in an\nend-to-end manner and requires no additional supervision or bounding box\nproposals. Specifically, we introduce weakly-supervised object segmentation in\nthe context of sound separation. We also formulate spectrogram mask prediction\nusing a set of learned mask bases, which combine using coefficients conditioned\non the output of object segmentation , a design that facilitates separation.\nExtensive experiments on the MUSIC dataset show that our proposed approach\noutperforms state-of-the-art methods on visually guided sound source separation\nand sound denoising.", "published": "2021-03-25 10:17:55", "link": "http://arxiv.org/abs/2104.02606v1", "categories": ["cs.CV", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Looking into Your Speech: Learning Cross-modal Affinity for Audio-visual\n  Speech Separation", "abstract": "In this paper, we address the problem of separating individual speech signals\nfrom videos using audio-visual neural processing. Most conventional approaches\nutilize frame-wise matching criteria to extract shared information between\nco-occurring audio and video. Thus, their performance heavily depends on the\naccuracy of audio-visual synchronization and the effectiveness of their\nrepresentations. To overcome the frame discontinuity problem between two\nmodalities due to transmission delay mismatch or jitter, we propose a\ncross-modal affinity network (CaffNet) that learns global correspondence as\nwell as locally-varying affinities between audio and visual streams. Given that\nthe global term provides stability over a temporal sequence at the\nutterance-level, this resolves the label permutation problem characterized by\ninconsistent assignments. By extending the proposed cross-modal affinity on the\ncomplex network, we further improve the separation performance in the complex\nspectral domain. Experimental results verify that the proposed methods\noutperform conventional ones on various datasets, demonstrating their\nadvantages in real-world scenarios.", "published": "2021-03-25 15:39:12", "link": "http://arxiv.org/abs/2104.02775v1", "categories": ["cs.CV", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
