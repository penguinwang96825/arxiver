{"title": "Code Comment Inconsistency Detection with BERT and Longformer", "abstract": "Comments, or natural language descriptions of source code, are standard\npractice among software developers. By communicating important aspects of the\ncode such as functionality and usage, comments help with software project\nmaintenance. However, when the code is modified without an accompanying\ncorrection to the comment, an inconsistency between the comment and code can\narise, which opens up the possibility for developer confusion and bugs. In this\npaper, we propose two models based on BERT (Devlin et al., 2019) and Longformer\n(Beltagy et al., 2020) to detect such inconsistencies in a natural language\ninference (NLI) context. Through an evaluation on a previously established\ncorpus of comment-method pairs both during and after code changes, we\ndemonstrate that our models outperform multiple baselines and yield comparable\nresults to the state-of-the-art models that exclude linguistic and lexical\nfeatures. We further discuss ideas for future research in using pretrained\nlanguage models for both inconsistency detection and automatic comment\nupdating.", "published": "2022-07-29 02:43:51", "link": "http://arxiv.org/abs/2207.14444v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking Azerbaijani Neural Machine Translation", "abstract": "Little research has been done on Neural Machine Translation (NMT) for\nAzerbaijani. In this paper, we benchmark the performance of Azerbaijani-English\nNMT systems on a range of techniques and datasets. We evaluate which\nsegmentation techniques work best on Azerbaijani translation and benchmark the\nperformance of Azerbaijani NMT models across several domains of text. Our\nresults show that while Unigram segmentation improves NMT performance and\nAzerbaijani translation models scale better with dataset quality than quantity,\ncross-domain generalization remains a challenge", "published": "2022-07-29 04:29:43", "link": "http://arxiv.org/abs/2207.14473v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Do you follow me?\": A Survey of Recent Approaches in Dialogue State\n  Tracking", "abstract": "While communicating with a user, a task-oriented dialogue system has to track\nthe user's needs at each turn according to the conversation history. This\nprocess called dialogue state tracking (DST) is crucial because it directly\ninforms the downstream dialogue policy. DST has received a lot of interest in\nrecent years with the text-to-text paradigm emerging as the favored approach.\nIn this review paper, we first present the task and its associated datasets.\nThen, considering a large number of recent publications, we identify highlights\nand advances of research in 2021-2022. Although neural approaches have enabled\nsignificant progress, we argue that some critical aspects of dialogue systems\nsuch as generalizability are still underexplored. To motivate future studies,\nwe propose several research avenues.", "published": "2022-07-29 11:53:22", "link": "http://arxiv.org/abs/2207.14627v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GTrans: Grouping and Fusing Transformer Layers for Neural Machine\n  Translation", "abstract": "Transformer structure, stacked by a sequence of encoder and decoder network\nlayers, achieves significant development in neural machine translation.\nHowever, vanilla Transformer mainly exploits the top-layer representation,\nassuming the lower layers provide trivial or redundant information and thus\nignoring the bottom-layer feature that is potentially valuable. In this work,\nwe propose the Group-Transformer model (GTrans) that flexibly divides\nmulti-layer representations of both encoder and decoder into different groups\nand then fuses these group features to generate target words. To corroborate\nthe effectiveness of the proposed method, extensive experiments and analytic\nexperiments are conducted on three bilingual translation benchmarks and two\nmultilingual translation tasks, including the IWLST-14, IWLST-17, LDC, WMT-14\nand OPUS-100 benchmark. Experimental and analytical results demonstrate that\nour model outperforms its Transformer counterparts by a consistent gain.\nFurthermore, it can be successfully scaled up to 60 encoder layers and 36\ndecoder layers.", "published": "2022-07-29 04:10:36", "link": "http://arxiv.org/abs/2207.14467v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Understanding the Relation of User and News Representations in\n  Content-Based Neural News Recommendation", "abstract": "A number of models for neural content-based news recommendation have been\nproposed. However, there is limited understanding of the relative importances\nof the three main components of such systems (news encoder, user encoder, and\nscoring function) and the trade-offs involved. In this paper, we assess the\nhypothesis that the most widely used means of matching user and candidate news\nrepresentations is not expressive enough. We allow our system to model more\ncomplex relations between the two by assessing more expressive scoring\nfunctions. Across a wide range of baseline and established systems this results\nin consistent improvements of around 6 points in AUC. Our results also indicate\na trade-off between the complexity of news encoder and scoring function: A\nfairly simple baseline model scores well above 68% AUC on the MIND dataset and\ncomes within 2 points of the published state-of-the-art, while requiring a\nfraction of the computational costs.", "published": "2022-07-29 14:24:25", "link": "http://arxiv.org/abs/2207.14704v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Thutmose Tagger: Single-pass neural model for Inverse Text Normalization", "abstract": "Inverse text normalization (ITN) is an essential post-processing step in\nautomatic speech recognition (ASR). It converts numbers, dates, abbreviations,\nand other semiotic classes from the spoken form generated by ASR to their\nwritten forms. One can consider ITN as a Machine Translation task and use\nneural sequence-to-sequence models to solve it. Unfortunately, such neural\nmodels are prone to hallucinations that could lead to unacceptable errors. To\nmitigate this issue, we propose a single-pass token classifier model that\nregards ITN as a tagging task. The model assigns a replacement fragment to\nevery input token or marks it for deletion or copying without changes. We\npresent a dataset preparation method based on the granular alignment of ITN\nexamples. The proposed model is less prone to hallucination errors. The model\nis trained on the Google Text Normalization dataset and achieves\nstate-of-the-art sentence accuracy on both English and Russian test sets.\nOne-to-one correspondence between tags and input words improves the\ninterpretability of the model's predictions, simplifies debugging, and allows\nfor post-processing corrections. The model is simpler than sequence-to-sequence\nmodels and easier to optimize in production settings. The model and the code to\nprepare the dataset is published as part of NeMo project.", "published": "2022-07-29 20:39:02", "link": "http://arxiv.org/abs/2208.00064v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Domain Specific Wav2vec 2.0 Fine-tuning For The SE&R 2022 Challenge", "abstract": "This paper presents our efforts to build a robust ASR model for the shared\ntask Automatic Speech Recognition for spontaneous and prepared speech & Speech\nEmotion Recognition in Portuguese (SE&R 2022). The goal of the challenge is to\nadvance the ASR research for the Portuguese language, considering prepared and\nspontaneous speech in different dialects. Our method consist on fine-tuning an\nASR model in a domain-specific approach, applying gain normalization and\nselective noise insertion. The proposed method improved over the strong\nbaseline provided on the test set in 3 of the 4 tracks available", "published": "2022-07-29 00:48:40", "link": "http://arxiv.org/abs/2207.14418v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Curriculum Learning for Data-Efficient Vision-Language Alignment", "abstract": "Aligning image and text encoders from scratch using contrastive learning\nrequires large amounts of paired image-text data. We alleviate this need by\naligning individually pre-trained language and vision representation models\nusing a much smaller amount of paired data, augmented with a curriculum\nlearning algorithm to learn fine-grained vision-language alignments. TOnICS\n(Training with Ontology-Informed Contrastive Sampling) initially samples\nminibatches whose image-text pairs contain a wide variety of objects to learn\nobject-level alignment, and progressively samples minibatches where all\nimage-text pairs contain the same object to learn finer-grained contextual\nalignment. Aligning pre-trained BERT and VinVL models to each other using\nTOnICS outperforms CLIP on downstream zero-shot image retrieval while using\nless than 1% as much training data.", "published": "2022-07-29 07:45:56", "link": "http://arxiv.org/abs/2207.14525v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "SERCNN: Stacked Embedding Recurrent Convolutional Neural Network in\n  Detecting Depression on Twitter", "abstract": "Conventional approaches to identify depression are not scalable, and the\npublic has limited awareness of mental health, especially in developing\ncountries. As evident by recent studies, social media has the potential to\ncomplement mental health screening on a greater scale. The vast amount of\nfirst-person narrative posts in chronological order can provide insights into\none's thoughts, feelings, behavior, or mood for some time, enabling a better\nunderstanding of depression symptoms reflected in the online space. In this\npaper, we propose SERCNN, which improves the user representation by (1)\nstacking two pretrained embeddings from different domains and (2) reintroducing\nthe embedding context to the MLP classifier. Our SERCNN shows great performance\nover state-of-the-art and other baselines, achieving 93.7% accuracy in a 5-fold\ncross-validation setting. Since not all users share the same level of online\nactivity, we introduced the concept of a fixed observation window that\nquantifies the observation period in a predefined number of posts. With as\nminimal as 10 posts per user, SERCNN performed exceptionally well with an 87%\naccuracy, which is on par with the BERT model, while having 98% less in the\nnumber of parameters. Our findings open up a promising direction for detecting\ndepression on social media with a smaller number of posts for inference,\ntowards creating solutions for a cost-effective and timely intervention. We\nhope that our work can bring this research area closer to real-world adoption\nin existing clinical practice.", "published": "2022-07-29 08:08:15", "link": "http://arxiv.org/abs/2207.14535v2", "categories": ["cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.AI"}
{"title": "Learning Phone Recognition from Unpaired Audio and Phone Sequences Based\n  on Generative Adversarial Network", "abstract": "ASR has been shown to achieve great performance recently. However, most of\nthem rely on massive paired data, which is not feasible for low-resource\nlanguages worldwide. This paper investigates how to learn directly from\nunpaired phone sequences and speech utterances. We design a two-stage iterative\nframework. GAN training is adopted in the first stage to find the mapping\nrelationship between unpaired speech and phone sequence. In the second stage,\nanother HMM model is introduced to train from the generator's output, which\nboosts the performance and provides a better segmentation for the next\niteration. In the experiment, we first investigate different choices of model\ndesigns. Then we compare the framework to different types of baselines: (i)\nsupervised methods (ii) acoustic unit discovery based methods (iii) methods\nlearning from unpaired data. Our framework performs consistently better than\nall acoustic unit discovery methods and previous methods learning from unpaired\ndata based on the TIMIT dataset.", "published": "2022-07-29 09:29:28", "link": "http://arxiv.org/abs/2207.14568v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Pronunciation-aware unique character encoding for RNN Transducer-based\n  Mandarin speech recognition", "abstract": "For Mandarin end-to-end (E2E) automatic speech recognition (ASR) tasks,\ncompared to character-based modeling units, pronunciation-based modeling units\ncould improve the sharing of modeling units in model training but meet\nhomophone problems. In this study, we propose to use a novel\npronunciation-aware unique character encoding for building E2E RNN-T-based\nMandarin ASR systems. The proposed encoding is a combination of\npronunciation-base syllable and character index (CI). By introducing the CI,\nthe RNN-T model can overcome the homophone problem while utilizing the\npronunciation information for extracting modeling units. With the proposed\nencoding, the model outputs can be converted into the final recognition result\nthrough a one-to-one mapping. We conducted experiments on Aishell and MagicData\ndatasets, and the experimental results showed the effectiveness of the proposed\nmethod.", "published": "2022-07-29 09:49:10", "link": "http://arxiv.org/abs/2207.14578v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "KG-NSF: Knowledge Graph Completion with a Negative-Sample-Free Approach", "abstract": "Knowledge Graph (KG) completion is an important task that greatly benefits\nknowledge discovery in many fields (e.g. biomedical research). In recent years,\nlearning KG embeddings to perform this task has received considerable\nattention. Despite the success of KG embedding methods, they predominantly use\nnegative sampling, resulting in increased computational complexity as well as\nbiased predictions due to the closed world assumption. To overcome these\nlimitations, we propose \\textbf{KG-NSF}, a negative sampling-free framework for\nlearning KG embeddings based on the cross-correlation matrices of embedding\nvectors. It is shown that the proposed method achieves comparable link\nprediction performance to negative sampling-based methods while converging much\nfaster.", "published": "2022-07-29 11:39:04", "link": "http://arxiv.org/abs/2207.14617v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Multiple-hypothesis RNN-T Loss for Unsupervised Fine-tuning and\n  Self-training of Neural Transducer", "abstract": "This paper proposes a new approach to perform unsupervised fine-tuning and\nself-training using unlabeled speech data for recurrent neural network\n(RNN)-Transducer (RNN-T) end-to-end (E2E) automatic speech recognition (ASR)\nsystems. Conventional systems perform fine-tuning/self-training using ASR\nhypothesis as the targets when using unlabeled audio data and are susceptible\nto the ASR performance of the base model. Here in order to alleviate the\ninfluence of ASR errors while using unlabeled data, we propose a\nmultiple-hypothesis RNN-T loss that incorporates multiple ASR 1-best hypotheses\ninto the loss function. For the fine-tuning task, ASR experiments on\nLibrispeech show that the multiple-hypothesis approach achieves a relative\nreduction of 14.2% word error rate (WER) when compared to the single-hypothesis\napproach, on the test_other set. For the self-training task, ASR models are\ntrained using supervised data from Wall Street Journal (WSJ), Aurora-4 along\nwith CHiME-4 real noisy data as unlabeled data. The multiple-hypothesis\napproach yields a relative reduction of 3.3% WER on the CHiME-4's\nsingle-channel real noisy evaluation set when compared with the\nsingle-hypothesis approach.", "published": "2022-07-29 15:14:03", "link": "http://arxiv.org/abs/2207.14736v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Rating the Crisis of Online Public Opinion Using a Multi-Level Index\n  System", "abstract": "Online public opinion usually spreads rapidly and widely, thus a small\nincident probably evolves into a large social crisis in a very short time, and\nresults in a heavy loss in credit or economic aspects. We propose a method to\nrate the crisis of online public opinion based on a multi-level index system to\nevaluate the impact of events objectively. Firstly, the dissemination mechanism\nof online public opinion is explained from the perspective of information\necology. According to the mechanism, some evaluation indexes are selected\nthrough correlation analysis and principal component analysis. Then, a\nclassification model of text emotion is created via the training by deep\nlearning to achieve the accurate quantification of the emotional indexes in the\nindex system. Finally, based on the multi-level evaluation index system and\ngrey correlation analysis, we propose a method to rate the crisis of online\npublic opinion. The experiment with the real-time incident show that this\nmethod can objectively evaluate the emotional tendency of Internet users and\nrate the crisis in different dissemination stages of online public opinion. It\nis helpful to realizing the crisis warning of online public opinion and timely\nblocking the further spread of the crisis.", "published": "2022-07-29 15:25:36", "link": "http://arxiv.org/abs/2207.14740v1", "categories": ["cs.SI", "cs.AI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "ALADIN: Distilling Fine-grained Alignment Scores for Efficient\n  Image-Text Matching and Retrieval", "abstract": "Image-text matching is gaining a leading role among tasks involving the joint\nunderstanding of vision and language. In literature, this task is often used as\na pre-training objective to forge architectures able to jointly deal with\nimages and texts. Nonetheless, it has a direct downstream application:\ncross-modal retrieval, which consists in finding images related to a given\nquery text or vice-versa. Solving this task is of critical importance in\ncross-modal search engines. Many recent methods proposed effective solutions to\nthe image-text matching problem, mostly using recent large vision-language (VL)\nTransformer networks. However, these models are often computationally\nexpensive, especially at inference time. This prevents their adoption in\nlarge-scale cross-modal retrieval scenarios, where results should be provided\nto the user almost instantaneously. In this paper, we propose to fill in the\ngap between effectiveness and efficiency by proposing an ALign And DIstill\nNetwork (ALADIN). ALADIN first produces high-effective scores by aligning at\nfine-grained level images and texts. Then, it learns a shared embedding space -\nwhere an efficient kNN search can be performed - by distilling the relevance\nscores obtained from the fine-grained alignments. We obtained remarkable\nresults on MS-COCO, showing that our method can compete with state-of-the-art\nVL Transformers while being almost 90 times faster. The code for reproducing\nour results is available at https://github.com/mesnico/ALADIN.", "published": "2022-07-29 16:01:48", "link": "http://arxiv.org/abs/2207.14757v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Low-data? No problem: low-resource, language-agnostic conversational\n  text-to-speech via F0-conditioned data augmentation", "abstract": "The availability of data in expressive styles across languages is limited,\nand recording sessions are costly and time consuming. To overcome these issues,\nwe demonstrate how to build low-resource, neural text-to-speech (TTS) voices\nwith only 1 hour of conversational speech, when no other conversational data\nare available in the same language. Assuming the availability of non-expressive\nspeech data in that language, we propose a 3-step technology: 1) we train an\nF0-conditioned voice conversion (VC) model as data augmentation technique; 2)\nwe train an F0 predictor to control the conversational flavour of the\nvoice-converted synthetic data; 3) we train a TTS system that consumes the\naugmented data. We prove that our technology enables F0 controllability, is\nscalable across speakers and languages and is competitive in terms of\nnaturalness over a state-of-the-art baseline model, another augmented method\nwhich does not make use of F0 information.", "published": "2022-07-29 11:00:46", "link": "http://arxiv.org/abs/2207.14607v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Unconstrained Audio Splicing Detection and Localization with\n  Neural Networks", "abstract": "Freely available and easy-to-use audio editing tools make it straightforward\nto perform audio splicing. Convincing forgeries can be created by combining\nvarious speech samples from the same person. Detection of such splices is\nimportant both in the public sector when considering misinformation, and in a\nlegal context to verify the integrity of evidence. Unfortunately, most existing\ndetection algorithms for audio splicing use handcrafted features and make\nspecific assumptions. However, criminal investigators are often faced with\naudio samples from unconstrained sources with unknown characteristics, which\nraises the need for more generally applicable methods.\n  With this work, we aim to take a first step towards unconstrained audio\nsplicing detection to address this need. We simulate various attack scenarios\nin the form of post-processing operations that may disguise splicing. We\npropose a Transformer sequence-to-sequence (seq2seq) network for splicing\ndetection and localization. Our extensive evaluation shows that the proposed\nmethod outperforms existing dedicated approaches for splicing detection [3, 10]\nas well as the general-purpose networks EfficientNet [28] and RegNet [25].", "published": "2022-07-29 13:57:16", "link": "http://arxiv.org/abs/2207.14682v4", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "UAVM: Towards Unifying Audio and Visual Models", "abstract": "Conventional audio-visual models have independent audio and video branches.\nIn this work, we unify the audio and visual branches by designing a Unified\nAudio-Visual Model (UAVM). The UAVM achieves a new state-of-the-art\naudio-visual event classification accuracy of 65.8% on VGGSound. More\ninterestingly, we also find a few intriguing properties of UAVM that the\nmodality-independent counterparts do not have.", "published": "2022-07-29 20:23:16", "link": "http://arxiv.org/abs/2208.00061v2", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
