{"title": "ModShift: Model Privacy via Designed Shifts", "abstract": "In this paper, shifts are introduced to preserve model privacy against an\neavesdropper in federated learning. Model learning is treated as a parameter\nestimation problem. This perspective allows us to derive the Fisher Information\nmatrix of the model updates from the shifted updates and drive them to\nsingularity, thus posing a hard estimation problem for Eve. The shifts are\nsecurely shared with the central server to maintain model accuracy at the\nserver and participating devices. A convergence test is proposed to detect if\nmodel updates have been tampered with and we show that our scheme passes this\ntest. Numerical results show that our scheme achieves a higher model shift when\ncompared to a noise injection scheme while requiring a lesser bandwidth secret\nchannel.", "published": "2025-07-26 21:00:56", "link": "http://arxiv.org/abs/2507.20060v1", "categories": ["cs.LG", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "Performance Analysis of Spatiotemporal 2-D Polar Codes for Massive MIMO with MMSE Receivers", "abstract": "With the evolution from 5G to 6G, ultra-reliable low-latency communication\n(URLLC) faces increasingly stringent performance requirements. Lower latency\nconstraints demand shorter channel coding lengths, which can severely degrade\ndecoding performance. The massive multiple-input multiple-output (MIMO) system\nis considered a crucial technology to address this challenge due to its\nabundant spatial degrees of freedom (DoF). While polar codes are theoretically\ncapacity-achieving in the limit of infinite code length, their practical\napplicability is limited by significant decoding latency. In this paper, we\nestablish a unified theoretical framework and propose a novel spatiotemporal\ntwo-dimensional (2-D) polar coding scheme for massive MIMO systems employing\nminimum mean square error (MMSE) receivers. The polar transform is jointly\napplied over both spatial and temporal dimensions to fully exploit the large\nspatial DoF. By leveraging the near-deterministic\nsignal-to-interference-plus-noise ratio (SINR) property of MMSE detection, the\nspatial domain is modeled as a set of parallel Gaussian sub-channels. Within\nthis framework, we perform a theoretical analysis of the 2-D polarization\nbehavior using the Gaussian approximation method, and the capacity-achieving\nproperty of the proposed scheme is proved under finite blocklength constraints\nand large spatial DoF. Simulation results further demonstrate that, compared to\ntraditional time-domain polar codes, the proposed 2-D scheme can significantly\nreduce latency while guaranteeing reliability, or alternatively improve\nreliability under the same latency constraint -- offering a capacity-achieving\nand latency-efficient channel coding solution for massive MIMO systems in\nfuture 6G URLLC scenarios.", "published": "2025-07-26 15:50:56", "link": "http://arxiv.org/abs/2507.19986v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Digital Twin Channel-Enabled Online Resource Allocation for 6G: Principle, Architecture and Application", "abstract": "Emerging applications such as holographic communication, autonomous driving,\nand the industrial Internet of Things impose stringent requirements on\nflexible, low-latency, and reliable resource allocation in 6G networks.\nConventional methods, which rely on statistical modeling, have proven effective\nin general contexts but may fail to achieve optimal performance in specific and\ndynamic environments. Furthermore, acquiring real-time channel state\ninformation (CSI) typically requires excessive pilot overhead. To address these\nchallenges, a digital twin channel (DTC)-enabled online optimization framework\nis proposed, in which DTC is employed to predict CSI based on environmental\nsensing. The predicted CSI is then utilized by lightweight game-theoretic\nalgorithms to perform online resource allocation in a timely and efficient\nmanner. Simulation results based on a digital replica of a realistic industrial\nworkshop demonstrate that the proposed method achieves throughput improvements\nof up to 11.5\\% compared with pilot-based ideal CSI schemes, validating its\neffectiveness for scalable, low-overhead, and environment-aware communication\nin future 6G networks.", "published": "2025-07-26 15:07:45", "link": "http://arxiv.org/abs/2507.19974v1", "categories": ["cs.AI", "cs.IT", "math.IT"], "primary_category": "cs.AI"}
{"title": "Spatial Language Likelihood Grounding Network for Bayesian Fusion of Human-Robot Observations", "abstract": "Fusing information from human observations can help robots overcome sensing\nlimitations in collaborative tasks. However, an uncertainty-aware fusion\nframework requires a grounded likelihood representing the uncertainty of human\ninputs. This paper presents a Feature Pyramid Likelihood Grounding Network\n(FP-LGN) that grounds spatial language by learning relevant map image features\nand their relationships with spatial relation semantics. The model is trained\nas a probability estimator to capture aleatoric uncertainty in human language\nusing three-stage curriculum learning. Results showed that FP-LGN matched\nexpert-designed rules in mean Negative Log-Likelihood (NLL) and demonstrated\ngreater robustness with lower standard deviation. Collaborative sensing results\ndemonstrated that the grounded likelihood successfully enabled\nuncertainty-aware fusion of heterogeneous human language observations and robot\nsensor measurements, achieving significant improvements in human-robot\ncollaborative task performance.", "published": "2025-07-26 13:24:02", "link": "http://arxiv.org/abs/2507.19947v1", "categories": ["cs.RO", "cs.CL", "cs.IT", "cs.LG", "cs.SY", "eess.SY", "math.IT"], "primary_category": "cs.RO"}
{"title": "Adaptive Learned Belief Propagation for Decoding Error-Correcting Codes", "abstract": "Weighted belief propagation (WBP) for the decoding of linear block codes is\nconsidered. In WBP, the Tanner graph of the code is unrolled with respect to\nthe iterations of the belief propagation decoder. Then, weights are assigned to\nthe edges of the resulting recurrent network and optimized offline using a\ntraining dataset. The main contribution of this paper is an adaptive WBP where\nthe weights of the decoder are determined for each received word. Two variants\nof this decoder are investigated. In the parallel WBP decoders, the weights\ntake values in a discrete set. A number of WBP decoders are run in parallel to\nsearch for the best sequence- of weights in real time. In the two-stage\ndecoder, a small neural network is used to dynamically determine the weights of\nthe WBP decoder for each received word. The proposed adaptive decoders\ndemonstrate significant improvements over the static counterparts in two\napplications. In the first application, Bose--Chaudhuri--Hocquenghem, polar and\nquasi-cyclic low-density parity-check (QC-LDPC) codes are used over an additive\nwhite Gaussian noise channel. The results indicate that the adaptive WBP\nachieves bit error rates (BERs) up to an order of magnitude less than the BERs\nof the static WBP at about the same decoding complexity, depending on the code,\nits rate, and the signal-to-noise ratio. The second application is a\nconcatenated code designed for a long-haul nonlinear optical fiber channel\nwhere the inner code is a QC-LDPC code and the outer code is a spatially\ncoupled LDPC code. In this case, the inner code is decoded using an adaptive\nWBP, while the outer code is decoded using the sliding window decoder and\nstatic belief propagation. The results show that the adaptive WBP provides a\ncoding gain of 0.8 dB compared to the neural normalized min-sum decoder, with\nabout the same computational complexity and decoding latency.", "published": "2025-07-26 13:12:30", "link": "http://arxiv.org/abs/2507.19941v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "An Efficient Alternating Minimization Algorithm for Computing Quantum Rate-Distortion Function", "abstract": "We consider the computation of the entanglement-assisted quantum\nrate-distortion function, which plays a central role in quantum information\ntheory. We propose an efficient alternating minimization algorithm based on the\nLagrangian analysis. Instead of fixing the multiplier corresponding to the\ndistortion constraint, we update the multiplier in each iteration. Hence the\nalgorithm solves the original problem itself, rather than the Lagrangian\nrelaxation of it. Moreover, all the other variables are iterated in closed form\nwithout solving multi-dimensional nonlinear equations or multivariate\noptimization problems. Numerical experiments show the accuracy of our proposed\nalgorithm and its improved efficiency over existing methods.", "published": "2025-07-26 11:55:31", "link": "http://arxiv.org/abs/2507.19920v1", "categories": ["cs.IT", "math.IT", "quant-ph"], "primary_category": "cs.IT"}
{"title": "Neural Estimation of the Information Bottleneck Based on a Mapping Approach", "abstract": "The information bottleneck (IB) method is a technique designed to extract\nmeaningful information related to one random variable from another random\nvariable, and has found extensive applications in machine learning problems. In\nthis paper, neural network based estimation of the IB problem solution is\nstudied, through the lens of a novel formulation of the IB problem. Via\nexploiting the inherent structure of the IB functional and leveraging the\nmapping approach, the proposed formulation of the IB problem involves only a\nsingle variable to be optimized, and subsequently is readily amenable to\ndata-driven estimators based on neural networks. A theoretical analysis is\nconducted to guarantee that the neural estimator asymptotically solves the IB\nproblem, and the numerical experiments on both synthetic and MNIST datasets\ndemonstrate the effectiveness of the neural estimator.", "published": "2025-07-26 07:04:40", "link": "http://arxiv.org/abs/2507.19832v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Efficient Computation of Marton's Error Exponent via Constraint Decoupling", "abstract": "The error exponent in lossy source coding characterizes the asymptotic decay\nrate of error probability with respect to blocklength. The Marton's error\nexponent provides the theoretically optimal bound on this rate. However,\ncomputation methods of the Marton's error exponent remain underdeveloped due to\nits formulation as a non-convex optimization problem with limited efficient\nsolvers. While a recent grid search algorithm can compute its inverse function,\nit incurs prohibitive computational costs from two-dimensional brute-force\nparameter grid searches. This paper proposes a composite maximization approach\nthat effectively handles both Marton's error exponent and its inverse function.\nThrough a constraint decoupling technique, the resulting problem formulations\nadmit efficient solvers driven by an alternating maximization algorithm. By\nfixing one parameter via a one-dimensional line search, the remaining\nsubproblem becomes convex and can be efficiently solved by alternating variable\nupdates, thereby significantly reducing search complexity. Therefore, the\nglobal convergence of the algorithm can be guaranteed. Numerical experiments\nfor simple sources and the Ahlswede's counterexample, demonstrates the superior\nefficiency of our algorithm in contrast to existing methods.", "published": "2025-07-26 06:17:03", "link": "http://arxiv.org/abs/2507.19816v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Homotopy-aware Multi-agent Navigation via Distributed Model Predictive Control", "abstract": "Multi-agent trajectory planning requires ensuring both safety and efficiency,\nyet deadlocks remain a significant challenge, especially in obstacle-dense\nenvironments. Such deadlocks frequently occur when multiple agents attempt to\ntraverse the same long and narrow corridor simultaneously. To address this, we\npropose a novel distributed trajectory planning framework that bridges the gap\nbetween global path and local trajectory cooperation. At the global level, a\nhomotopy-aware optimal path planning algorithm is proposed, which fully\nleverages the topological structure of the environment. A reference path is\nchosen from distinct homotopy classes by considering both its spatial and\ntemporal properties, leading to improved coordination among agents globally. At\nthe local level, a model predictive control-based trajectory optimization\nmethod is used to generate dynamically feasible and collision-free\ntrajectories. Additionally, an online replanning strategy ensures its\nadaptability to dynamic environments. Simulations and experiments validate the\neffectiveness of our approach in mitigating deadlocks. Ablation studies\ndemonstrate that by incorporating time-aware homotopic properties into the\nunderlying global paths, our method can significantly reduce deadlocks and\nimprove the average success rate from 4%-13% to over 90% in randomly generated\ndense scenarios.", "published": "2025-07-26 08:26:36", "link": "http://arxiv.org/abs/2507.19860v1", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY"], "primary_category": "cs.RO"}
{"title": "VAE-GAN Based Price Manipulation in Coordinated Local Energy Markets", "abstract": "This paper introduces a model for coordinating prosumers with heterogeneous\ndistributed energy resources (DERs), participating in the local energy market\n(LEM) that interacts with the market-clearing entity. The proposed LEM scheme\nutilizes a data-driven, model-free reinforcement learning approach based on the\nmulti-agent deep deterministic policy gradient (MADDPG) framework, enabling\nprosumers to make real-time decisions on whether to buy, sell, or refrain from\nany action while facilitating efficient coordination for optimal energy trading\nin a dynamic market. In addition, we investigate a price manipulation strategy\nusing a variational auto encoder-generative adversarial network (VAE-GAN)\nmodel, which allows utilities to adjust price signals in a way that induces\nfinancial losses for the prosumers. Our results show that under adversarial\npricing, heterogeneous prosumer groups, particularly those lacking generation\ncapabilities, incur financial losses. The same outcome holds across LEMs of\ndifferent sizes. As the market size increases, trading stabilizes and fairness\nimproves through emergent cooperation among agents.", "published": "2025-07-26 07:38:27", "link": "http://arxiv.org/abs/2507.19844v1", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.SY", "eess.SY"], "primary_category": "cs.LG"}
{"title": "Solving the MIT Inverse Problem by Considering Skin and Proximity Effects in Coils", "abstract": "This paper presents an improved technique for solving the inverse problem in\nmagnetic induction tomography (MIT) by considering skin and proximity effects\nin coils. MIT is a non-contact, noninvasive, and low-cost imaging modality for\nobtaining the distribution of conductivity inside an object. Reconstruction of\nlow conductivity distribution by MIT requires more accurate techniques since\nmeasured signals are inherently weak and the reconstruction problem is highly\nnonlinear and ill-posed. Previous MIT inverse problem studies have ignored skin\nand proximity effects inside coils in the forward method. In this article, the\nimproved technique incorporates these effects in the forward method.\nFurthermore, it employs the regularized Gauss-Newton algorithm to reconstruct\nthe conductivity distribution. The regularization parameter is obtained by an\nadaptive method using the two input parameters: a coefficient and an initial\nconductivity distribution. The new Jacobian matrix is computed based on a\nstandard technique. To compare the early and improved forward methods in\npossible medical and industrial applications with low conductivity regions, a\n2D 8-coil MIT system is modeled, and image reconstruction is performed for\nsynthetic phantoms. Results show that it is crucial to use the improved forward\nmethod for the reconstruction of the absolute conductivity values.", "published": "2025-07-26 16:51:49", "link": "http://arxiv.org/abs/2507.20004v1", "categories": ["physics.med-ph", "cs.NA", "math.NA", "physics.comp-ph"], "primary_category": "physics.med-ph"}
{"title": "Geometric-Perturbation-Robust Cut-Cell Scheme for Two-Material Flows: Exact Pressure-Equilibrium Preservation and Rigorous Analysis", "abstract": "Preserving pressure equilibrium across material interfaces is critical for\nthe stability of compressible multi-material flow simulations, yet most\ninterface-fitted sharp-interface schemes are notoriously sensitive to interface\ngeometry: even slight perturbations of the captured (or tracked) interface can\ntrigger large spurious pressure oscillations. We present a cut-cell method that\nis geometric-perturbation-robust (GPR) for the compressible two-material flows.\nBy construction, the scheme provably preserves exact interfacial pressure\nequilibrium in the presence of small interface-position errors. The key is a\nstrict consistency between the conserved variables and the geometric moments\n(i.e., the integrals of monomials) of every cut cell. We formulate auxiliary\ntransport equations, whose discrete solutions furnish evolved geometric moment,\nthese geometric moments remain perfectly synchronized with the conserved\nvariables -- even on a deforming mesh. Surpassing the classical geometric\nconservation law, our approach keeps all higher-order geometric moments\nconsistent, thereby eliminating accuracy loss due to geometric mismatches. To\nprevent the reconstruction step from destroying pressure equilibrium, we\nintroduce the notion of equilibrium-compatible (EC) reconstructions. A\ncarefully designed modification equips any conventional weighted essentially\nnon-oscillatory (WENO) reconstruction with the EC property; we detail a\nthird-order EC multi-resolution WENO (EC-MRWENO) variant. The tight coupling of\nEC-MRWENO with the evolved moments yields the first cut-cell solver that is\nsimultaneously provably GPR and genuinely high-order: it attains second-order\naccuracy precisely at material interfaces, while preserving third-order\naccuracy in smooth regions. Extensive two-dimensional tests confirm the\nframework's robustness, accuracy and stability under geometric perturbations\nand topology changes.", "published": "2025-07-26 14:46:20", "link": "http://arxiv.org/abs/2507.19966v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Time-continuous strongly conservative space-time finite element methods for the dynamic Biot model", "abstract": "We consider the dynamic Biot model (see [Biot, M. A. J. Appl. Phys. 33,\n1482--1498 (1962)]) describing the interaction between fluid flow and solid\ndeformation including wave propagation phenomena in both the liquid and solid\nphases of a saturated porous medium. This model couples a hyperbolic equation\nfor momentum balance to a second-order in time dynamic Darcy law and a\nparabolic equation for the balance of mass and is here considered in\nthree-field formulation with the displacement of the elastic matrix, the fluid\nvelocity, and the fluid pressure being the physical fields of interest.\n  A family of variational space-time finite element methods is proposed, which\ncombines a continuous-in-time Galerkin ansatz of arbitrary polynomial degree\nwith $H(\\mathrm{div})$-conforming approximations of the displacement field, its\ntime derivative, and the flux field--of discontinuous Galerkin (DG) type for\ndisplacements--with a piecewise polynomial pressure approximation, providing an\ninf-sup stable strongly conservative mixed method in each case. We prove error\nestimates in a combined energy norm in space for the maximum norm in time. The\ntheoretical results are confirmed by numerical experiments for different\npolynomial orders in space and time.", "published": "2025-07-26 13:52:41", "link": "http://arxiv.org/abs/2507.19955v1", "categories": ["math.NA", "cs.NA", "74H15 65M15 65M60 74F10"], "primary_category": "math.NA"}
{"title": "A Bi-fidelity numerical method for velocity discretization of Boltzmann equations", "abstract": "In this paper, we introduce a bi-fidelity algorithm for velocity\ndiscretization of Boltzmann-type kinetic equations under multiple scales. The\nproposed method employs a simpler and computationally cheaper low-fidelity\nmodel to capture a small set of significant velocity points through the greedy\napproach, then evaluates the high-fidelity model only at these few velocity\npoints and to reconstruct a bi-fidelity surrogate. This novel method integrates\na simpler collision term of relaxation type in the low-fidelity model and an\nasymptotic-preserving scheme in the high-fidelity update step. Both linear\nBoltzmann under diffusive scaling and the nonlinear full Boltzmann in\nhyperbolic scaling are discussed. We show the weak asymptotic-preserving\nproperty and empirical error bound estimates. Extensive numerical experiments\non linear semiconductor and nonlinear Boltzmann problems with smooth or\ndiscontinuous initial conditions and under various regimes have been carefully\nstudied, which demonstrates the effectiveness and robustness of our proposed\nscheme.", "published": "2025-07-26 13:21:11", "link": "http://arxiv.org/abs/2507.19945v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Deep Uzawa for Kinetic Transport with Lagrange-Enforced Boundaries", "abstract": "We propose a neural network framework for solving stationary linear transport\nequations with inflow boundary conditions. The method represents the solution\nusing a neural network and imposes the boundary condition via a Lagrange\nmultiplier, based on a saddle-point formulation inspired by the classical Uzawa\nalgorithm. The scheme is mesh-free, compatible with automatic differentiation\nand extends naturally to problems with scattering and heterogeneous media. We\nestablish convergence of the continuum formulation and analyse the effects of\nquadrature error, neural approximation and inexact optimisation in the discrete\nimplementation. Numerical experiments show that the method captures anisotropic\ntransport, enforces boundary conditions and resolves scattering dynamics\naccurately.", "published": "2025-07-26 10:36:24", "link": "http://arxiv.org/abs/2507.19907v1", "categories": ["math.NA", "cs.NA", "math.OC", "physics.comp-ph"], "primary_category": "math.NA"}
{"title": "A Modified Dielectric Contrast based Integral Equation for 2D TE Scattering by Inhomogeneous Domains", "abstract": "This work presents a modified domain integral equation approach for the\nforward problem of TE scattering, employing a modified definition of dielectric\ncontrast and discretizing the electric field density using Rao-Wilton-Glisson\n(RWG) basis functions. The proposed formulation mitigates the numerical\nchallenges introduced by the gradient-divergence operator in traditional\nelectric field-based vector formulations. The use of RWG basis functions over\ntriangular meshes enhances geometric conformity, ensures tangential continuity\nacross dielectric interfaces, and facilitates the application of well known\nsingularity extraction techniques for numerical accuracy. Validation through\nnumerical experiments on a two-layered dielectric cylinder demonstrates\nexcellent agreement between computed and analytical scattered fields.\nConvergence studies confirm improving solution accuracy with mesh refinement\nindicating robustness with respect to discretization without increasing the\niterations.", "published": "2025-07-26 04:01:11", "link": "http://arxiv.org/abs/2507.19777v1", "categories": ["math.NA", "cs.NA", "physics.comp-ph"], "primary_category": "math.NA"}
{"title": "An inverse random diffraction grating problem for the Helmholtz equation", "abstract": "This paper investigates the inverse scattering problem of time-harmonic plane\nwaves incident on a perfectly reflecting random periodic structure. To simulate\nrandom perturbations arising from manufacturing defects and surface wear in\nreal-world grating profiles, we propose a stochastic surface modeling framework\nmotivated by the discretization of the Wiener process. Our approach introduces\nrandomness at discrete nodes and then applies linear interpolation to construct\nthe surface, marking a novel attempt to incorporate the concepts of the Wiener\nprocess into random surface representation. Under this framework, each\nrealization of the random surface generates a Lipschitz-continuous diffraction\ngrating, mathematically represented as a sum of a baseline profile and a\nweighted linear combination of local `tent' basis functions, meanwhile\npreserving key statistics of the random surface. Building on this\nrepresentation, we introduce the Recursive Parametric Smoothing Strategy (RPSS)\nto invert the key statistics of our random surfaces. Combined with Monte Carlo\nsampling and a wavenumber continuation strategy, our reconstruction scheme\ndemonstrates effectiveness across multiple benchmark scenarios. Several\nnumerical results are presented along with some discussions in the end on\nreconstruction mechanisms and future extensions.", "published": "2025-07-26 02:29:19", "link": "http://arxiv.org/abs/2507.19744v1", "categories": ["math.NA", "cs.NA", "78A46, 65N21, 65C05"], "primary_category": "math.NA"}
{"title": "Implementation and Basis Construction for Smooth Finite Element Spaces", "abstract": "The construction of $C^m$ conforming finite elements on simplicial meshes has\nrecently advanced through the groundbreaking work of Hu, Lin, and Wu (Found.\nComput. Math. 24, 2024). Their framework characterizes smoothness via moments\nof normal derivatives over subsimplices, leading to explicit degrees of freedom\nand unisolvence, unifying earlier constructions. However, the absence of\nexplicit basis functions has left these spaces largely inaccessible for\npractical computation. In parallel, multivariate spline theory (Chui and Lai,\nJ. Approx. Theory 60, 1990) enforces $C^m$ smoothness through linear\nconstraints on Bernstein--B\\'{e}zier coefficients, but stable, locally\nsupported bases remain elusive beyond low dimensions. Building on the geometric\ndecomposition of the simplicial lattice proposed by Chen and Huang (Math. Comp.\n93, 2024), this work develops an explicit, computable framework for smooth\nfinite elements. The degrees of freedom defined by moments of normal\nderivatives are modified to align with the dual basis of the Bernstein\npolynomials, yielding structured local bases on each simplex. Explicit basis\nconstruction is essential not merely for completeness, but for enabling\nefficient matrix assembly, global continuity, and scalable solution of\nhigh-order elliptic partial differential equations. This development closes the\ngap between theoretical existence and practical realization, making smooth\nfinite element methods accessible to broad computational applications.", "published": "2025-07-26 01:19:14", "link": "http://arxiv.org/abs/2507.19732v1", "categories": ["math.NA", "cs.NA", "65N30, 65N12, 31B30"], "primary_category": "math.NA"}
{"title": "Optimal mean-variance portfolio selection under regime-switching-induced stock price shocks", "abstract": "In this paper, we investigate mean-variance (MV) portfolio selection problems\nwith jumps in a regime-switching financial model. The novelty of our approach\nlies in allowing not only the market parameters -- such as the interest rate,\nappreciation rate, volatility, and jump intensity -- to depend on the market\nregime, but also in permitting stock prices to experience jumps when the market\nregime switches, in addition to the usual micro-level jumps. This modeling\nchoice is motivated by empirical observations that stock prices often exhibit\nsharp declines when the market shifts from a ``bullish'' to a ``bearish''\nregime, and vice versa. By employing the completion-of-squares technique, we\nderive the optimal portfolio strategy and the efficient frontier, both of which\nare characterized by three systems of multi-dimensional ordinary differential\nequations (ODEs). Among these, two systems are linear, while the first one is\nan $\\ell$-dimensional, fully coupled, and highly nonlinear Riccati equation. In\nthe absence of regime-switching-induced stock price shocks, these systems\nreduce to simple linear ODEs. Thus, the introduction of\nregime-switching-induced stock price shocks adds significant complexity and\nchallenges to our model. Additionally, we explore the MV problem under a\nno-shorting constraint. In this case, the corresponding Riccati equation\nbecomes a $2\\ell$-dimensional, fully coupled, nonlinear ODE, for which we\nestablish solvability. The solution is then used to explicitly express the\noptimal portfolio and the efficient frontier.", "published": "2025-07-26 06:48:11", "link": "http://arxiv.org/abs/2507.19824v1", "categories": ["q-fin.PM", "math.OC", "math.PR", "q-fin.MF"], "primary_category": "q-fin.PM"}
{"title": "Lasso Penalization for High-Dimensional Beta Regression Models: Computation, Analysis, and Inference", "abstract": "Beta regression is commonly employed when the outcome variable is a\nproportion. Since its conception, the approach has been widely used in\napplications spanning various scientific fields. A series of extensions have\nbeen proposed over time, several of which address variable selection and\npenalized estimation, e.g., with an $\\ell_1$-penalty (LASSO). However, a\ntheoretical analysis of this popular approach in the context of Beta regression\nwith high-dimensional predictors is lacking. In this paper, we aim to close\nthis gap. A particular challenge arises from the non-convexity of the\nassociated negative log-likelihood, which we address by resorting to a\nframework for analyzing stationary points in a neighborhood of the target\nparameter. Leveraging this framework, we derive a non-asymptotic bound on the\n$\\ell_1$-error of such stationary points. In addition, we propose a debiasing\napproach to construct confidence intervals for the regression parameters. A\nproximal gradient algorithm is devised for optimizing the resulting penalized\nnegative log-likelihood function. Our theoretical analysis is corroborated via\nsimulation studies, and a real data example concerning the prediction of\ncounty-level proportions of incarceration is presented to showcase the\npractical utility of our methodology.", "published": "2025-07-26 23:19:17", "link": "http://arxiv.org/abs/2507.20079v1", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH"], "primary_category": "stat.ME"}
{"title": "PERRY: Policy Evaluation with Confidence Intervals using Auxiliary Data", "abstract": "Off-policy evaluation (OPE) methods aim to estimate the value of a new\nreinforcement learning (RL) policy prior to deployment. Recent advances have\nshown that leveraging auxiliary datasets, such as those synthesized by\ngenerative models, can improve the accuracy of these value estimates.\nUnfortunately, such auxiliary datasets may also be biased, and existing methods\nfor using data augmentation for OPE in RL lack principled uncertainty\nquantification. In high stakes settings like healthcare, reliable uncertainty\nestimates are important for comparing policy value estimates. In this work, we\npropose two approaches to construct valid confidence intervals for OPE when\nusing data augmentation. The first provides a confidence interval over the\npolicy performance conditioned on a particular initial state $V^{\\pi}(s_0)$--\nsuch intervals are particularly important for human-centered applications. To\ndo so we introduce a new conformal prediction method for high dimensional state\nMDPs. Second, we consider the more common task of estimating the average policy\nperformance over many initial states; to do so we draw on ideas from doubly\nrobust estimation and prediction powered inference. Across simulators spanning\nrobotics, healthcare and inventory management, and a real healthcare dataset\nfrom MIMIC-IV, we find that our methods can use augmented data and still\nconsistently produce intervals that cover the ground truth values, unlike\npreviously proposed methods.", "published": "2025-07-26 21:51:15", "link": "http://arxiv.org/abs/2507.20068v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Predicting Parkinson's Disease Progression Using Statistical and Neural Mixed Effects Models: A Comparative Study on Longitudinal Biomarkers", "abstract": "Predicting Parkinson's Disease (PD) progression is crucial, and voice\nbiomarkers offer a non-invasive method for tracking symptom severity (UPDRS\nscores) through telemonitoring. Analyzing this longitudinal data is challenging\ndue to within-subject correlations and complex, nonlinear patient-specific\nprogression patterns. This study benchmarks LMMs against two advanced hybrid\napproaches: the Generalized Neural Network Mixed Model (GNMM) (Mandel 2021),\nwhich embeds a neural network within a GLMM structure, and the Neural Mixed\nEffects (NME) model (Wortwein 2023), allowing nonlinear subject-specific\nparameters throughout the network. Using the Oxford Parkinson's telemonitoring\nvoice dataset, we evaluate these models' performance in predicting Total UPDRS\nto offer practical guidance for PD research and clinical applications.", "published": "2025-07-26 20:56:32", "link": "http://arxiv.org/abs/2507.20058v1", "categories": ["stat.ML", "cs.LG", "stat.AP"], "primary_category": "stat.ML"}
{"title": "Irredundant k-Fold Cross-Validation", "abstract": "In traditional k-fold cross-validation, each instance is used ($k\\!-\\!1$)\ntimes for training and once for testing, leading to redundancy that lets many\ninstances disproportionately influence the learning phase. We introduce\nIrredundant $k$--fold cross-validation, a novel method that guarantees each\ninstance is used exactly once for training and once for testing across the\nentire validation procedure. This approach ensures a more balanced utilization\nof the dataset, mitigates overfitting due to instance repetition, and enables\nsharper distinctions in comparative model analysis. The method preserves\nstratification and remains model-agnostic, i.e., compatible with any\nclassifier. Experimental results demonstrate that it delivers consistent\nperformance estimates across diverse datasets --comparable to $k$--fold\ncross-validation-- while providing less optimistic variance estimates because\ntraining partitions are non-overlapping, and significantly reducing the overall\ncomputational cost.", "published": "2025-07-26 19:59:37", "link": "http://arxiv.org/abs/2507.20048v1", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Dependency Network-Based Portfolio Design with Forecasting and VaR Constraints", "abstract": "This study proposes a novel portfolio optimization framework that integrates\nstatistical social network analysis with time series forecasting and risk\nmanagement. Using daily stock data from the S&P 500 (2020-2024), we construct\ndependency networks via Vector Autoregression (VAR) and Forecast Error Variance\nDecomposition (FEVD), transforming influence relationships into a cost-based\nnetwork. Specifically, FEVD breaks down the VAR's forecast error variance to\nquantify how much each stock's shocks contribute to another's uncertainty\ninformation we invert to form influence-based edge weights in our network. By\napplying the Minimum Spanning Tree (MST) algorithm, we extract the core\ninter-stock structure and identify central stocks through degree centrality. A\ndynamic portfolio is constructed using the top-ranked stocks, with capital\nallocated based on Value at Risk (VaR). To refine stock selection, we\nincorporate forecasts from ARIMA and Neural Network Autoregressive (NNAR)\nmodels. Trading simulations over a one-year period demonstrate that the\nMST-based strategies outperform a buy-and-hold benchmark, with the tuned\nNNAR-enhanced strategy achieving a 63.74% return versus 18.00% for the\nbenchmark. Our results highlight the potential of combining network structures,\npredictive modeling, and risk metrics to improve adaptive financial\ndecision-making.", "published": "2025-07-26 18:53:39", "link": "http://arxiv.org/abs/2507.20039v1", "categories": ["q-fin.PM", "econ.EM", "q-fin.ST", "stat.ML"], "primary_category": "q-fin.PM"}
{"title": "Discrete Gaussian Vector Fields On Meshes", "abstract": "Though the underlying fields associated with vector-valued environmental data\nare continuous, observations themselves are discrete. For example, climate\nmodels typically output grid-based representations of wind fields or ocean\ncurrents, and these are often downscaled to a discrete set of points. By\ntreating the area of interest as a two-dimensional manifold that can be\nrepresented as a triangular mesh and embedded in Euclidean space, this work\nshows that discrete intrinsic Gaussian processes for vector-valued data can be\ndeveloped from discrete differential operators defined with respect to a mesh.\nThese Gaussian processes account for the geometry and curvature of the manifold\nwhilst also providing a flexible and practical formulation that can be readily\napplied to any two-dimensional mesh. We show that these models can capture\nharmonic flows, incorporate boundary conditions, and model non-stationary data.\nFinally, we apply these models to downscaling stationary and non-stationary\ngridded wind data on the globe, and to inference of ocean currents from sparse\nobservations in bounded domains.", "published": "2025-07-26 17:43:31", "link": "http://arxiv.org/abs/2507.20024v1", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH"], "primary_category": "stat.ME"}
{"title": "Extreme value theory for singular subspace estimation in the matrix denoising model", "abstract": "This paper studies fine-grained singular subspace estimation in the matrix\ndenoising model where a deterministic low-rank signal matrix is additively\nperturbed by a stochastic matrix of Gaussian noise. We establish that the\nmaximum Euclidean row norm (i.e., the two-to-infinity norm) of the aligned\ndifference between the leading sample and population singular vectors\napproaches the Gumbel distribution in the large-matrix limit, under suitable\nsignal-to-noise conditions and after appropriate centering and scaling. We\napply our novel asymptotic distributional theory to test hypotheses of low-rank\nsignal structure encoded in the leading singular vectors and their\ncorresponding principal subspace. We provide de-biased estimators for the\ncorresponding nuisance signal singular values and show that our proposed\nplug-in test statistic has desirable properties. Notably, compared to using the\nFrobenius norm subspace distance, our test statistic based on the\ntwo-to-infinity norm has higher power to detect structured alternatives that\ndiffer from the null in only a few matrix entries or rows. Our main results are\nobtained by a novel synthesis of and technical analysis involving entrywise\nmatrix perturbation analysis, extreme value theory, saddle point approximation\nmethods, and random matrix theory. Our contributions complement the existing\nliterature for matrix denoising focused on minimaxity, mean squared error\nanalysis, unitarily invariant distances between subspaces, component-wise\nasymptotic distributional theory, and row-wise uniform error bounds. Numerical\nsimulations illustrate our main results and demonstrate the robustness\nproperties of our testing procedure to non-Gaussian noise distributions.", "published": "2025-07-26 15:28:36", "link": "http://arxiv.org/abs/2507.19978v1", "categories": ["math.ST", "cs.LG", "stat.ME", "stat.ML", "stat.TH", "62H25, 62H15 (Primary) 62E20 (Secondary)"], "primary_category": "math.ST"}
{"title": "Dimer-Enhanced Optimization: A First-Order Approach to Escaping Saddle Points in Neural Network Training", "abstract": "First-order optimization methods, such as SGD and Adam, are widely used for\ntraining large-scale deep neural networks due to their computational efficiency\nand robust performance. However, relying solely on gradient information, these\nmethods often struggle to navigate complex loss landscapes with flat regions,\nplateaus, and saddle points. Second-order methods, which use curvature\ninformation from the Hessian matrix, can address these challenges but are\ncomputationally infeasible for large models. The Dimer method, a first-order\ntechnique that constructs two closely spaced points to probe the local geometry\nof a potential energy surface, efficiently estimates curvature using only\ngradient information. Inspired by its use in molecular dynamics simulations for\nlocating saddle points, we propose Dimer-Enhanced Optimization (DEO), a novel\nframework to escape saddle points in neural network training. DEO adapts the\nDimer method to explore a broader region of the loss landscape, approximating\nthe Hessian's smallest eigenvector without computing the full matrix. By\nperiodically projecting the gradient onto the subspace orthogonal to the\nminimum curvature direction, DEO guides the optimizer away from saddle points\nand flat regions, enhancing training efficiency with non-stepwise updates.\nPreliminary experiments on a Transformer toy model show DEO achieves\ncompetitive performance compared to standard first-order methods, improving\nnavigation of complex loss landscapes. Our work repurposes physics-inspired,\nfirst-order curvature estimation to enhance neural network training in\nhigh-dimensional spaces.", "published": "2025-07-26 14:57:32", "link": "http://arxiv.org/abs/2507.19968v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "TS-Insight: Visualizing Thompson Sampling for Verification and XAI", "abstract": "Thompson Sampling (TS) and its variants are powerful Multi-Armed Bandit\nalgorithms used to balance exploration and exploitation strategies in active\nlearning. Yet, their probabilistic nature often turns them into a ``black\nbox'', hindering debugging and trust. We introduce TS-Insight, a visual\nanalytics tool explicitly designed to shed light on the internal decision\nmechanisms of Thompson Sampling-based algorithms, for model developers. It\ncomprises multiple plots, tracing for each arm the evolving posteriors,\nevidence counts, and sampling outcomes, enabling the verification, diagnosis,\nand explainability of exploration/exploitation dynamics. This tool aims at\nfostering trust and facilitating effective debugging and deployment in complex\nbinary decision-making scenarios especially in sensitive domains requiring\ninterpretable decision-making.", "published": "2025-07-26 09:58:26", "link": "http://arxiv.org/abs/2507.19898v1", "categories": ["cs.HC", "cs.AI", "cs.LG", "stat.ML", "I.2.6; H.5.2"], "primary_category": "cs.HC"}
{"title": "RestoreAI - Pattern-based Risk Estimation Of Remaining Explosives", "abstract": "Landmine removal is a slow, resource-intensive process affecting over 60\ncountries. While AI has been proposed to enhance explosive ordnance (EO)\ndetection, existing methods primarily focus on object recognition, with limited\nattention to prediction of landmine risk based on spatial pattern information.\nThis work aims to answer the following research question: How can AI be used to\npredict landmine risk from landmine patterns to improve clearance time\nefficiency? To that effect, we introduce RestoreAI, an AI system for\npattern-based risk estimation of remaining explosives. RestoreAI is the first\nAI system that leverages landmine patterns for risk prediction, improving the\naccuracy of estimating the residual risk of missing EO prior to land release.\nWe particularly focus on the implementation of three instances of RestoreAI,\nrespectively, linear, curved and Bayesian pattern deminers. First, the linear\npattern deminer uses linear landmine patterns from a principal component\nanalysis (PCA) for the landmine risk prediction. Second, the curved pattern\ndeminer uses curved landmine patterns from principal curves. Finally, the\nBayesian pattern deminer incorporates prior expert knowledge by using a\nBayesian pattern risk prediction. Evaluated on real-world landmine data,\nRestoreAI significantly boosts clearance efficiency. The top-performing\npattern-based deminers achieved a 14.37 percentage point increase in the\naverage share of cleared landmines per timestep and required 24.45% less time\nthan the best baseline deminer to locate all landmines. Interestingly, linear\nand curved pattern deminers showed no significant performance difference,\nsuggesting that more efficient linear patterns are a viable option for risk\nprediction.", "published": "2025-07-26 09:03:13", "link": "http://arxiv.org/abs/2507.19873v1", "categories": ["cs.LG", "cs.CY", "stat.AP", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Sparse-mode Dynamic Mode Decomposition for Disambiguating Local and Global Structures", "abstract": "The dynamic mode decomposition (DMD) is a data-driven approach that extracts\nthe dominant features from spatiotemporal data. In this work, we introduce\nsparse-mode DMD, a new variant of the optimized DMD framework that specifically\nleverages sparsity-promoting regularization in order to approximate DMD modes\nwhich have localized spatial structure. The algorithm maintains the\nnoise-robust properties of optimized DMD while disambiguating between modes\nwhich are spatially local versus global in nature. In many applications, such\nmodes are associated with discrete and continuous spectra respectively, thus\nallowing the algorithm to explicitly construct, in an unsupervised manner, the\ndistinct portions of the spectrum. We demonstrate this by analyzing synthetic\nand real-world systems, including examples from optical waveguides, quantum\nmechanics, and sea surface temperature data.", "published": "2025-07-26 04:24:40", "link": "http://arxiv.org/abs/2507.19787v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Bag of Coins: A Statistical Probe into Neural Confidence Structures", "abstract": "Modern neural networks, despite their high accuracy, often produce poorly\ncalibrated confidence scores, limiting their reliability in high-stakes\napplications. Existing calibration methods typically post-process model outputs\nwithout interrogating the internal consistency of the predictions themselves.\nIn this work, we introduce a novel, non-parametric statistical probe, the\nBag-of-Coins (BoC) test, that examines the internal consistency of a\nclassifier's logits. The BoC test reframes confidence estimation as a\nfrequentist hypothesis test: does the model's top-ranked class win 1-v-1\ncontests against random competitors at a rate consistent with its own stated\nsoftmax probability? When applied to modern deep learning architectures, this\nsimple probe reveals a fundamental dichotomy. On Vision Transformers (ViTs),\nthe BoC output serves as a state-of-the-art confidence score, achieving\nnear-perfect calibration with an ECE of 0.0212, an 88% improvement over a\ntemperature-scaled baseline. Conversely, on Convolutional Neural Networks\n(CNNs) like ResNet, the probe reveals a deep inconsistency between the model's\npredictions and its internal logit structure, a property missed by traditional\nmetrics. We posit that BoC is not merely a calibration method, but a new\ndiagnostic tool for understanding and exposing the differing ways that popular\narchitectures represent uncertainty.", "published": "2025-07-26 03:54:32", "link": "http://arxiv.org/abs/2507.19774v1", "categories": ["stat.ML", "cs.LG", "62M45, 62H30, 62P30"], "primary_category": "stat.ML"}
{"title": "Binaural Localization Model for Speech in Noise", "abstract": "Binaural acoustic source localization is important to human listeners for\nspatial awareness, communication and safety. In this paper, an end-to-end\nbinaural localization model for speech in noise is presented. A lightweight\nconvolutional recurrent network that localizes sound in the frontal azimuthal\nplane for noisy reverberant binaural signals is introduced. The model\nincorporates additive internal ear noise to represent the frequency-dependent\nhearing threshold of a typical listener. The localization performance of the\nmodel is compared with the steered response power algorithm, and the use of the\nmodel as a measure of interaural cue preservation for binaural speech\nenhancement methods is studied. A listening test was performed to compare the\nperformance of the model with human localization of speech in noisy conditions.", "published": "2025-07-26 18:01:45", "link": "http://arxiv.org/abs/2507.20027v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Binaural Speech Enhancement Using Complex Convolutional Recurrent Networks", "abstract": "From hearing aids to augmented and virtual reality devices, binaural speech\nenhancement algorithms have been established as state-of-the-art techniques to\nimprove speech intelligibility and listening comfort. In this paper, we present\nan end-to-end binaural speech enhancement method using a complex recurrent\nconvolutional network with an encoder-decoder architecture and a complex LSTM\nrecurrent block placed between the encoder and decoder. A loss function that\nfocuses on the preservation of spatial information in addition to speech\nintelligibility improvement and noise reduction is introduced. The network\nestimates individual complex ratio masks for the left and right-ear channels of\na binaural hearing device in the time-frequency domain. We show that, compared\nto other baseline algorithms, the proposed method significantly improves the\nestimated speech intelligibility and reduces the noise while preserving the\nspatial information of the binaural signals in acoustic situations with a\nsingle target speaker and isotropic noise of various types.", "published": "2025-07-26 17:42:53", "link": "http://arxiv.org/abs/2507.20023v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "DOA Estimation via Optimal Weighted Low-Rank Matrix Completion", "abstract": "This paper presents a novel method for estimating the direction of arrival\n(DOA) for a non-uniform and sparse linear sensor array using the weighted\nlifted structure low-rank matrix completion. The proposed method uses a single\nsnapshot sample in which a single array of data is observed. The method is\nrooted in a weighted lifted-structured low-rank matrix recovery framework. The\nmethod involves four key steps: (i) lifting the antenna samples to form a\nlow-rank stature, then (ii) designing left and right weight matrices to reflect\nthe sample informativeness, (iii) estimating a noise-free uniform array output\nthrough completion of the weighted lifted samples, and (iv) obtaining the DOAs\nfrom the restored uniform linear array samples.\n  We study the complexity of steps (i) to (iii) above, where we analyze the\nrequired sample for the array interpolation of step (iii) for DOA estimation.\nWe demonstrate that the proposed choice of weight matrices achieves a\nnear-optimal sample complexity. This complexity aligns with the problem's\ndegree of freedom, equivalent to the number of DOAs adjusted for logarithmic\nfactors. Numerical evaluations show the proposed method's superiority against\nthe non-weighted counterpart and atomic norm minimization-based methods.\nNotably, our proposed method significantly improves, with approximately a 10 dB\nreduction in normalized mean-squared error over the non-weighted method at\nlow-noise conditions.", "published": "2025-07-26 16:33:40", "link": "http://arxiv.org/abs/2507.19996v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Dependability Theory-based Statistical QoS Provisioning of Fluid Antenna Systems", "abstract": "Fluid antenna systems (FAS) have recently emerged as a promising technology\nfor next-generation wireless networks, offering real-time spatial\nreconfiguration to enhance reliability, throughput, and energy efficiency.\nNevertheless, existing studies often overlook the temporal dynamics of channel\nfading and their implications for mission-critical operations. In this paper,\nwe propose a dependability-theoretic framework for statistical\nquality-of-service (QoS) provisioning of FAS under finite blocklength (FBL)\nconstraints. Specifically, we derive new closed-form expressions for the\nlevel-crossing rate (LCR) and average fade duration (AFD) of an $N$-port FAS\nover Nakagami-$m$ fading channels. Leveraging these second-order statistics, we\ndefine two key dependability metrics such as mission reliability and mean\ntime-to-first-failure (MTTFF), to quantify the probability of uninterrupted\noperation over a defined mission duration. We further extend the classical\neffective capacity (EC) concept to incorporate mission reliability in the FBL\nregime, yielding a mission EC (mEC). To capture energy efficiency under bursty\ntraffic and latency constraints, we also develop the mission effective energy\nefficiency (mEEE) metric and formulate its maximization as a non-convex\nfractional optimization problem. This problem is then solved via a modified\nDinkelbach's method with an embedded line search. Extensive simulations uncover\ncritical trade-offs among port count, QoS exponent, signal-to-noise ratio, and\nmission duration, offering insights for the design of ultra-reliable,\nlow-latency, and energy-efficient industrial internet-of-things (IIoT) systems.", "published": "2025-07-26 15:43:31", "link": "http://arxiv.org/abs/2507.19984v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Deep Learning Based Joint Channel Estimation and Positioning for Sparse XL-MIMO OFDM Systems", "abstract": "This paper investigates joint channel estimation and positioning in\nnear-field sparse extra-large multiple-input multiple-output (XL-MIMO)\northogonal frequency division multiplexing (OFDM) systems. To achieve\ncooperative gains between channel estimation and positioning, we propose a deep\nlearning-based two-stage framework comprising positioning and channel\nestimation. In the positioning stage, the user's coordinates are predicted and\nutilized in the channel estimation stage, thereby enhancing the accuracy of\nchannel estimation. Within this framework, we propose a U-shaped Mamba\narchitecture for channel estimation and positioning, termed as CP-Mamba. This\nnetwork integrates the strengths of the Mamba model with the structural\nadvantages of U-shaped convolutional networks, enabling effective capture of\nlocal spatial features and long-range temporal dependencies of the channel.\nNumerical simulation results demonstrate that the proposed two-stage approach\nwith CP-Mamba architecture outperforms existing baseline methods. Moreover,\nsparse arrays (SA) exhibit significantly superior performance in both channel\nestimation and positioning accuracy compared to conventional compact arrays.", "published": "2025-07-26 12:47:39", "link": "http://arxiv.org/abs/2507.19936v1", "categories": ["eess.SP", "cs.AI", "cs.LG"], "primary_category": "eess.SP"}
{"title": "Toward Dual-Functional LAWN: Control-Aware System Design for Aerodynamics-Aided UAV Formations", "abstract": "Integrated sensing and communication (ISAC) has emerged as a pivotal\ntechnology for advancing low-altitude wireless networks (LAWNs), serving as a\ncritical enabler for next-generation communication systems. This paper\ninvestigates the system design for energy-saving unmanned aerial vehicle (UAV)\nformations in dual-functional LAWNs, where a ground base station (GBS)\nsimultaneously wirelessly controls multiple UAV formations and performs sensing\ntasks. To enhance flight endurance, we exploit the aerodynamic upwash effects\nand propose a distributed energy-saving formation framework based on the\nadapt-then-combine (ATC) diffusion least mean square (LMS) algorithm.\nSpecifically, each UAV updates the local position estimate by invoking the LMS\nalgorithm, followed by refining it through cooperative information exchange\nwith neighbors. This enables an optimized aerodynamic structure that minimizes\nthe formation's overall energy consumption. To ensure control stability and\nfairness, we formulate a maximum linear quadratic regulator (LQR) minimization\nproblem, which is subject to both the available power budget and the required\nsensing beam pattern gain. To address this non-convex problem, we develop a\ntwo-step approach by first deriving a closed-form expression of LQR as a\nfunction of arbitrary beamformers. Subsequently, an efficient iterative\nalgorithm that integrates successive convex approximation (SCA) and\nsemidefinite relaxation (SDR) techniques is proposed to obtain a sub-optimal\ndual-functional beamforming solution. Extensive simulation results confirm that\nthe 'V'-shaped formation is the most energy-efficient configuration and\ndemonstrate the superiority of our proposed design over benchmark schemes in\nimproving control performance.", "published": "2025-07-26 10:43:54", "link": "http://arxiv.org/abs/2507.19910v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Feature Engineering for Wireless Communications and Networking: Concepts, Methodologies, and Applications", "abstract": "AI-enabled wireless communications have attracted tremendous research\ninterest in recent years, particularly with the rise of novel paradigms such as\nlow-altitude integrated sensing and communication (ISAC) networks. Within these\nsystems, feature engineering plays a pivotal role by transforming raw wireless\ndata into structured representations suitable for AI models. Hence, this paper\noffers a comprehensive investigation of feature engineering techniques in\nAI-driven wireless communications. Specifically, we begin with a detailed\nanalysis of fundamental principles and methodologies of feature engineering.\nNext, we present its applications in wireless communication systems, with\nspecial emphasis on ISAC networks. Finally, we introduce a generative AI-based\nframework, which can reconstruct signal feature spectrum under malicious\nattacks in low-altitude ISAC networks. The case study shows that it can\neffectively reconstruct the signal spectrum, achieving an average structural\nsimilarity index improvement of 4%, thereby supporting downstream sensing and\ncommunication applications.", "published": "2025-07-26 07:21:27", "link": "http://arxiv.org/abs/2507.19837v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Debunking Optimization Myths in Federated Learning for Medical Image Classification", "abstract": "Federated Learning (FL) is a collaborative learning method that enables\ndecentralized model training while preserving data privacy. Despite its promise\nin medical imaging, recent FL methods are often sensitive to local factors such\nas optimizers and learning rates, limiting their robustness in practical\ndeployments. In this work, we revisit vanilla FL to clarify the impact of edge\ndevice configurations, benchmarking recent FL methods on colorectal pathology\nand blood cell classification task. We numerically show that the choice of\nlocal optimizer and learning rate has a greater effect on performance than the\nspecific FL method. Moreover, we find that increasing local training epochs can\neither enhance or impair convergence, depending on the FL method. These\nfindings indicate that appropriate edge-specific configuration is more crucial\nthan algorithmic complexity for achieving effective FL.", "published": "2025-07-26 06:41:17", "link": "http://arxiv.org/abs/2507.19822v1", "categories": ["cs.LG", "eess.IV", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Channel Estimation in Massive MIMO Systems with Orthogonal Delay-Doppler Division Multiplexing", "abstract": "Orthogonal delay-Doppler division multiplexing~(ODDM) modulation has recently\nbeen regarded as a promising technology to provide reliable communications in\nhigh-mobility situations. Accurate and low-complexity channel estimation is one\nof the most critical challenges for massive multiple input multiple\noutput~(MIMO) ODDM systems, mainly due to the extremely large antenna arrays\nand high-mobility environments. To overcome these challenges, this paper\naddresses the issue of channel estimation in downlink massive MIMO-ODDM systems\nand proposes a low-complexity algorithm based on memory approximate message\npassing~(MAMP) to estimate the channel state information~(CSI). Specifically,\nwe first establish the effective channel model of the massive MIMO-ODDM\nsystems, where the magnitudes of the elements in the equivalent channel vector\nfollow a Bernoulli-Gaussian distribution. Further, as the number of antennas\ngrows, the elements in the equivalent coefficient matrix tend to become\ncompletely random. Leveraging these characteristics, we utilize the MAMP method\nto determine the gains, delays, and Doppler effects of the multi-path channel,\nwhile the channel angles are estimated through the discrete Fourier transform\nmethod. Finally, numerical results show that the proposed channel estimation\nalgorithm approaches the Bayesian optimal results when the number of antennas\ntends to infinity and improves the channel estimation accuracy by about 30%\ncompared with the existing algorithms in terms of the normalized mean square\nerror.", "published": "2025-07-26 06:05:10", "link": "http://arxiv.org/abs/2507.19812v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Radar and Acoustic Sensor Fusion using a Transformer Encoder for Robust Drone Detection and Classification", "abstract": "The use of drones in a wide range of applications is steadily increasing.\nHowever, this has also raised critical security concerns such as unauthorized\ndrone intrusions into restricted zones. Therefore, robust and accurate drone\ndetection and classification mechanisms are required despite significant\nchallenges due to small size of drones, low-altitude flight, and environmental\nnoise. In this letter, we propose a multi-modal approach combining radar and\nacoustic sensing for detecting and classifying drones. We employ radar due to\nits long-range capabilities, and robustness to different weather conditions. We\nutilize raw acoustic signals without converting them to other domains such as\nspectrograms or Mel-frequency cepstral coefficients. This enables us to use\nfewer number of parameters compared to the stateof-the-art approaches.\nFurthermore, we explore the effectiveness of the transformer encoder\narchitecture in fusing these sensors. Experimental results obtained in outdoor\nsettings verify the superior performance of the proposed approach compared to\nthe state-of-the-art methods.", "published": "2025-07-26 04:21:08", "link": "http://arxiv.org/abs/2507.19785v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Coverage Probability and Average Rate Analysis of Hybrid Cellular and Cell-free Network", "abstract": "Cell-free wireless networks deploy distributed access points (APs) to\nsimultaneously serve user equipments (UEs) across the service region and are\nregarded as one of the most promising network architectural paradigms. Despite\nrecent advances in the performance analysis and optimization of cellfree\nwireless networks, it remains an open question whether large-scale deployment\nof APs in existing wireless networks can cost-effectively achieve communication\ncapacity growth. Besides, the realization of a cell-free network is considered\nto be a gradual long-term evolutionary process in which cell-free APs will be\nincrementally introduced into existing cellular networks, and form a hybrid\ncommunication network with the existing cellular base stations (BSs). Such a\ncollaboration will bridge the gap between the established cellular network and\nthe innovative cellfree network. Therefore, hybrid cellular and cell-free\nnetworks (HCCNs) emerge as a practical and feasible solution for advancing\ncell-free network development, and it is worthwhile to further explore its\nperformance limits. This paper presents a stochastic geometry-based hybrid\ncellular and cell-free network model to analyze the distributions of signal and\ninterference and reveal their mutual coupling. Specifically, in order to\nbenefit the UEs from both the cellular BSs and the cell-free APs, a conjugate\nbeamforming design is employed, and the aggregated signal is analyzed using\nmoment matching. Then, the coverage probability of the hybrid network is\ncharacterized by deriving the Laplace transforms and their higher-order\nderivatives of interference components. Furthermore, the average achievable\nrate of the hybrid network over channel fading is derived based on the\ninterference coupling analysis.", "published": "2025-07-26 03:34:30", "link": "http://arxiv.org/abs/2507.19763v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "LowKeyEMG: Electromyographic typing with a reduced keyset", "abstract": "We introduce LowKeyEMG, a real-time human-computer interface that enables\nefficient text entry using only 7 gesture classes decoded from surface\nelectromyography (sEMG). Prior work has attempted full-alphabet decoding from\nsEMG, but decoding large character sets remains unreliable, especially for\nindividuals with motor impairments. Instead, LowKeyEMG reduces the English\nalphabet to 4 gesture keys, with 3 more for space and system interaction, to\nreliably translate simple one-handed gestures into text, leveraging the\nrecurrent transformer-based language model RWKV for efficient computation. In\nreal-time experiments, participants achieved average one-handed keyboardless\ntyping speeds of 23.3 words per minute with LowKeyEMG, and improved gesture\nefficiency by 17% (relative to typed phrase length). When typing with only 7\nkeys, LowKeyEMG can achieve 98.2% top-3 word accuracy, demonstrating that this\nlow-key typing paradigm can maintain practical communication rates. Our results\nhave implications for assistive technologies and any interface where input\nbandwidth is constrained.", "published": "2025-07-26 01:41:58", "link": "http://arxiv.org/abs/2507.19736v1", "categories": ["cs.HC", "eess.SP"], "primary_category": "cs.HC"}
