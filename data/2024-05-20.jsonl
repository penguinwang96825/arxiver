{"title": "(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration\n  for Translating Ultra-Long Literary Texts", "abstract": "Recent advancements in machine translation (MT) have significantly enhanced\ntranslation quality across various domains. However, the translation of\nliterary texts remains a formidable challenge due to their complex language,\nfigurative expressions, and cultural nuances. In this work, we introduce a\nnovel multi-agent framework based on large language models (LLMs) for literary\ntranslation, implemented as a company called TransAgents, which mirrors\ntraditional translation publication process by leveraging the collective\ncapabilities of multiple agents, to address the intricate demands of\ntranslating literary works. To evaluate the effectiveness of our system, we\npropose two innovative evaluation strategies: Monolingual Human Preference\n(MHP) and Bilingual LLM Preference (BLP). MHP assesses translations from the\nperspective of monolingual readers of the target language, while BLP uses\nadvanced LLMs to compare translations directly with the original texts.\nEmpirical findings indicate that despite lower d-BLEU scores, translations from\nTransAgents are preferred by both human evaluators and LLMs over human-written\nreferences, particularly in genres requiring domain-specific knowledge. We also\nhighlight the strengths and limitations of TransAgents through case studies and\nsuggests directions for future research.", "published": "2024-05-20 05:55:08", "link": "http://arxiv.org/abs/2405.11804v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "xFinder: Large Language Models as Automated Evaluators for Reliable\n  Evaluation", "abstract": "The continuous advancement of large language models (LLMs) has brought\nincreasing attention to the critical issue of developing fair and reliable\nmethods for evaluating their performance. Particularly, the emergence of\ncheating phenomena, such as test set leakage and prompt format overfitting,\nposes significant challenges to the reliable evaluation of LLMs. As evaluation\nframeworks commonly use Regular Expression (RegEx) for answer extraction,\nmodels may adjust their responses to fit formats easily handled by RegEx.\nNevertheless, the key answer extraction module based on RegEx frequently\nsuffers from extraction errors. Furthermore, recent studies proposing\nfine-tuned LLMs as judge models for automated evaluation face challenges in\nterms of generalization ability and fairness. This paper comprehensively\nanalyzes the entire LLM evaluation chain and demonstrates that optimizing the\nkey answer extraction module improves extraction accuracy and enhances\nevaluation reliability. Our findings suggest that improving the key answer\nextraction module can lead to higher judgment accuracy and improved evaluation\nefficiency compared to the judge models. To address these issues, we propose\nxFinder, a novel evaluator for answer extraction and matching in LLM\nevaluation. As part of this process, we create a specialized dataset, the\n\\textbf{K}ey \\textbf{A}nswer \\textbf{F}inder (KAF) dataset, to ensure effective\nmodel training and evaluation. Generalization tests and real-world evaluations\nshow that the smallest xFinder model, with only 500 million parameters,\nachieves an average extraction accuracy of 93.42\\%. In contrast, RegEx accuracy\nin the best evaluation framework is 74.38\\%. The final judgment accuracy of\nxFinder reaches 97.61\\%, outperforming existing evaluation frameworks and judge\nmodels.", "published": "2024-05-20 08:30:13", "link": "http://arxiv.org/abs/2405.11874v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CReMa: Crisis Response through Computational Identification and Matching\n  of Cross-Lingual Requests and Offers Shared on Social Media", "abstract": "During times of crisis, social media platforms play a crucial role in\nfacilitating communication and coordinating resources. In the midst of chaos\nand uncertainty, communities often rely on these platforms to share urgent\npleas for help, extend support, and organize relief efforts. However, the\noverwhelming volume of conversations during such periods can escalate to\nunprecedented levels, necessitating the automated identification and matching\nof requests and offers to streamline relief operations. Additionally, there is\na notable absence of studies conducted in multi-lingual settings, despite the\nfact that any geographical area can have a diverse linguistic population.\nTherefore, we propose CReMa (Crisis Response Matcher), a systematic approach\nthat integrates textual, temporal, and spatial features to address the\nchallenges of effectively identifying and matching requests and offers on\nsocial media platforms during emergencies. Our approach utilizes a\ncrisis-specific pre-trained model and a multi-lingual embedding space. We\nemulate human decision-making to compute temporal and spatial features and\nnon-linearly weigh the textual features. The results from our experiments are\npromising, outperforming strong baselines. Additionally, we introduce a novel\nmulti-lingual dataset simulating help-seeking and offering assistance on social\nmedia in 16 languages and conduct comprehensive cross-lingual experiments.\nFurthermore, we analyze a million-scale geotagged global dataset to understand\npatterns in seeking help and offering assistance on social media. Overall,\nthese contributions advance the field of crisis informatics and provide\nbenchmarks for future research in the area.", "published": "2024-05-20 09:30:03", "link": "http://arxiv.org/abs/2405.11897v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Constraint-Enforcing Reward for Adversarial Attacks on Text\n  Classifiers", "abstract": "Text classifiers are vulnerable to adversarial examples --\ncorrectly-classified examples that are deliberately transformed to be\nmisclassified while satisfying acceptability constraints. The conventional\napproach to finding adversarial examples is to define and solve a combinatorial\noptimisation problem over a space of allowable transformations. While\neffective, this approach is slow and limited by the choice of transformations.\nAn alternate approach is to directly generate adversarial examples by\nfine-tuning a pre-trained language model, as is commonly done for other\ntext-to-text tasks. This approach promises to be much quicker and more\nexpressive, but is relatively unexplored. For this reason, in this work we\ntrain an encoder-decoder paraphrase model to generate a diverse range of\nadversarial examples. For training, we adopt a reinforcement learning algorithm\nand propose a constraint-enforcing reward that promotes the generation of valid\nadversarial examples. Experimental results over two text classification\ndatasets show that our model has achieved a higher success rate than the\noriginal paraphrase model, and overall has proved more effective than other\ncompetitive attacks. Finally, we show how key design choices impact the\ngenerated examples and discuss the strengths and weaknesses of the proposed\napproach.", "published": "2024-05-20 09:33:43", "link": "http://arxiv.org/abs/2405.11904v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Biomedical Entity Linking for Dutch: Fine-tuning a Self-alignment BERT\n  Model on an Automatically Generated Wikipedia Corpus", "abstract": "Biomedical entity linking, a main component in automatic information\nextraction from health-related texts, plays a pivotal role in connecting\ntextual entities (such as diseases, drugs and body parts mentioned by patients)\nto their corresponding concepts in a structured biomedical knowledge base. The\ntask remains challenging despite recent developments in natural language\nprocessing. This paper presents the first evaluated biomedical entity linking\nmodel for the Dutch language. We use MedRoBERTa.nl as base model and perform\nsecond-phase pretraining through self-alignment on a Dutch biomedical ontology\nextracted from the UMLS and Dutch SNOMED. We derive a corpus from Wikipedia of\nontology-linked Dutch biomedical entities in context and fine-tune our model on\nthis dataset. We evaluate our model on the Dutch portion of the Mantra\nGSC-corpus and achieve 54.7% classification accuracy and 69.8% 1-distance\naccuracy. We then perform a case study on a collection of unlabeled,\npatient-support forum data and show that our model is hampered by the limited\nquality of the preceding entity recognition step. Manual evaluation of small\nsample indicates that of the correctly extracted entities, around 65% is linked\nto the correct concept in the ontology. Our results indicate that biomedical\nentity linking in a language other than English remains challenging, but our\nDutch model can be used to for high-level analysis of patient-generated text.", "published": "2024-05-20 10:30:36", "link": "http://arxiv.org/abs/2405.11941v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FAME-MT Dataset: Formality Awareness Made Easy for Machine Translation\n  Purposes", "abstract": "People use language for various purposes. Apart from sharing information,\nindividuals may use it to express emotions or to show respect for another\nperson. In this paper, we focus on the formality level of machine-generated\ntranslations and present FAME-MT -- a dataset consisting of 11.2 million\ntranslations between 15 European source languages and 8 European target\nlanguages classified to formal and informal classes according to target\nsentence formality. This dataset can be used to fine-tune machine translation\nmodels to ensure a given formality level for each European target language\nconsidered. We describe the dataset creation procedure, the analysis of the\ndataset's quality showing that FAME-MT is a reliable source of language\nregister information, and we present a publicly available proof-of-concept\nmachine translation model that uses the dataset to steer the formality level of\nthe translation. Currently, it is the largest dataset of formality annotations,\nwith examples expressed in 112 European language pairs. The dataset is\npublished online: https://github.com/laniqo-public/fame-mt/ .", "published": "2024-05-20 10:35:30", "link": "http://arxiv.org/abs/2405.11942v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multiple-Choice Questions are Efficient and Robust LLM Evaluators", "abstract": "We present GSM-MC, a multiple-choice (MC) dataset constructed by collecting\nanswers and incorrect predictions on GSM8K from 60 open-source models. Through\nextensive experiments, we show that LLMs' performance on the MC version of this\npopular benchmark is strongly correlated with their performance on the original\nversion and is quite robust to distractor choices and option orders, while the\nevaluation time is reduced by a factor of up to 30. Following similar\nprocedures, we introduce MATH-MC, constructed from MATH, and PythonIO, a new\nprogram reasoning MC dataset constructed from HumanEval and MBPP. Experimental\nresults indicate that LLMs' performance on these MC benchmarks leaves much room\nfor improvement. Our data and code are available at\nhttps://github.com/Geralt-Targaryen/MC-Evaluation.", "published": "2024-05-20 11:47:13", "link": "http://arxiv.org/abs/2405.11966v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can AI Relate: Testing Large Language Model Response for Mental Health\n  Support", "abstract": "Large language models (LLMs) are already being piloted for clinical use in\nhospital systems like NYU Langone, Dana-Farber and the NHS. A proposed\ndeployment use case is psychotherapy, where a LLM-powered chatbot can treat a\npatient undergoing a mental health crisis. Deployment of LLMs for mental health\nresponse could hypothetically broaden access to psychotherapy and provide new\npossibilities for personalizing care. However, recent high-profile failures,\nlike damaging dieting advice offered by the Tessa chatbot to patients with\neating disorders, have led to doubt about their reliability in high-stakes and\nsafety-critical settings.\n  In this work, we develop an evaluation framework for determining whether LLM\nresponse is a viable and ethical path forward for the automation of mental\nhealth treatment. Our framework measures equity in empathy and adherence of LLM\nresponses to motivational interviewing theory. Using human evaluation with\ntrained clinicians and automatic quality-of-care metrics grounded in psychology\nresearch, we compare the responses provided by peer-to-peer responders to those\nprovided by a state-of-the-art LLM.\n  We show that LLMs like GPT-4 use implicit and explicit cues to infer patient\ndemographics like race. We then show that there are statistically significant\ndiscrepancies between patient subgroups: Responses to Black posters\nconsistently have lower empathy than for any other demographic group (2%-13%\nlower than the control group). Promisingly, we do find that the manner in which\nresponses are generated significantly impacts the quality of the response. We\nconclude by proposing safety guidelines for the potential deployment of LLMs\nfor mental health response.", "published": "2024-05-20 13:42:27", "link": "http://arxiv.org/abs/2405.12021v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling factors influencing judgment variation in Sentiment Analysis\n  with Natural Language Processing and Statistics", "abstract": "TripAdvisor reviews and comparable data sources play an important role in\nmany tasks in Natural Language Processing (NLP), providing a data basis for the\nidentification and classification of subjective judgments, such as hotel or\nrestaurant reviews, into positive or negative polarities. This study explores\nthree important factors influencing variation in crowdsourced polarity\njudgments, focusing on TripAdvisor reviews in Spanish. Three hypotheses are\ntested: the role of Part Of Speech (POS), the impact of sentiment words such as\n\"tasty\", and the influence of neutral words like \"ok\" on judgment variation.\nThe study's methodology employs one-word titles, demonstrating their efficacy\nin studying polarity variation of words. Statistical tests on mean equality are\nperformed on word groups of our interest. The results of this study reveal that\nadjectives in one-word titles tend to result in lower judgment variation\ncompared to other word types or POS. Sentiment words contribute to lower\njudgment variation as well, emphasizing the significance of sentiment words in\nresearch on polarity judgments, and neutral words are associated with higher\njudgment variation as expected. However, these effects cannot be always\nreproduced in longer titles, which suggests that longer titles do not represent\nthe best data source for testing the ambiguity of single words due to the\ninfluence on word polarity by other words like negation in longer titles. This\nempirical investigation contributes valuable insights into the factors\ninfluencing polarity variation of words, providing a foundation for NLP\npractitioners that aim to capture and predict polarity judgments in Spanish and\nfor researchers that aim to understand factors influencing judgment variation.", "published": "2024-05-20 14:24:18", "link": "http://arxiv.org/abs/2405.12055v1", "categories": ["cs.CL", "68T50, 91F20", "I.2.7"], "primary_category": "cs.CL"}
{"title": "STYLE: Improving Domain Transferability of Asking Clarification\n  Questions in Large Language Model Powered Conversational Agents", "abstract": "Equipping a conversational search engine with strategies regarding when to\nask clarification questions is becoming increasingly important across various\ndomains. Attributing to the context understanding capability of LLMs and their\naccess to domain-specific sources of knowledge, LLM-based clarification\nstrategies feature rapid transfer to various domains in a post-hoc manner.\nHowever, they still struggle to deliver promising performance on unseen\ndomains, struggling to achieve effective domain transferability. We take the\nfirst step to investigate this issue and existing methods tend to produce\none-size-fits-all strategies across diverse domains, limiting their search\neffectiveness. In response, we introduce a novel method, called Style, to\nachieve effective domain transferability. Our experimental results indicate\nthat Style bears strong domain transferability, resulting in an average search\nperformance improvement of ~10% on four unseen domains.", "published": "2024-05-20 14:28:25", "link": "http://arxiv.org/abs/2405.12059v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information\n  Needs in Large Language Models", "abstract": "Large language models (LLMs) are increasingly used to meet user information\nneeds, but their effectiveness in dealing with user queries that contain\nvarious types of ambiguity remains unknown, ultimately risking user trust and\nsatisfaction. To this end, we introduce CLAMBER, a benchmark for evaluating\nLLMs using a well-organized taxonomy. Building upon the taxonomy, we construct\n~12K high-quality data to assess the strengths, weaknesses, and potential risks\nof various off-the-shelf LLMs. Our findings indicate the limited practical\nutility of current LLMs in identifying and clarifying ambiguous user queries,\neven enhanced by chain-of-thought (CoT) and few-shot prompting. These\ntechniques may result in overconfidence in LLMs and yield only marginal\nenhancements in identifying ambiguity. Furthermore, current LLMs fall short in\ngenerating high-quality clarifying questions due to a lack of conflict\nresolution and inaccurate utilization of inherent knowledge. In this paper,\nCLAMBER presents a guidance and promotes further research on proactive and\ntrustworthy LLMs. Our dataset is available at\nhttps://github.com/zt991211/CLAMBER", "published": "2024-05-20 14:34:01", "link": "http://arxiv.org/abs/2405.12063v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Selective Annotation via Data Allocation: These Data Should Be Triaged\n  to Experts for Annotation Rather Than the Model", "abstract": "To obtain high-quality annotations under limited budget, semi-automatic\nannotation methods are commonly used, where a portion of the data is annotated\nby experts and a model is then trained to complete the annotations for the\nremaining data. However, these methods mainly focus on selecting informative\ndata for expert annotations to improve the model predictive ability (i.e.,\ntriage-to-human data), while the rest of the data is indiscriminately assigned\nto model annotation (i.e., triage-to-model data). This may lead to\ninefficiencies in budget allocation for annotations, as easy data that the\nmodel could accurately annotate may be unnecessarily assigned to the expert,\nand hard data may be misclassified by the model. As a result, the overall\nannotation quality may be compromised. To address this issue, we propose a\nselective annotation framework called SANT. It effectively takes advantage of\nboth the triage-to-human and triage-to-model data through the proposed\nerror-aware triage and bi-weighting mechanisms. As such, informative or hard\ndata is assigned to the expert for annotation, while easy data is handled by\nthe model. Experimental results show that SANT consistently outperforms other\nbaselines, leading to higher-quality annotation through its proper allocation\nof data to both expert and model workers. We provide pioneering work on data\nannotation within budget constraints, establishing a landmark for future\ntriage-based annotation studies.", "published": "2024-05-20 14:52:05", "link": "http://arxiv.org/abs/2405.12081v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distributional Semantics, Holism, and the Instability of Meaning", "abstract": "Large Language Models are built on the so-called distributional semantic\napproach to linguistic meaning that has the distributional hypothesis at its\ncore. The distributional hypothesis involves a holistic conception of word\nmeaning: the meaning of a word depends upon its relations to other words in the\nmodel. A standard objection to holism is the charge of instability: any change\nin the meaning properties of a linguistic system (a human speaker, for example)\nwould lead to many changes or a complete change in the entire system. We\nexamine whether the instability objection poses a problem for distributional\nmodels of meaning. First, we distinguish between distinct forms of instability\nthat these models could exhibit, and argue that only one such form is relevant\nfor understanding the relation between instability and communication: what we\ncall differential instability. Differential instability is variation in the\nrelative distances between points in a space, rather than variation in the\nabsolute position of those points. We distinguish differential and absolute\ninstability by constructing two of our own smaller language models. We\ndemonstrate the two forms of instability by showing these models change as the\ncorpora they are constructed from increase in size. We argue that the\ninstability that these models display is constrained by the structure and scale\nof relationships between words, such that the resistance to change for a word\nis roughly proportional to its frequent and consistent use within the language\nsystem. The differential instability that language models exhibit allows for\nproductive forms of meaning change while not leading to the problems raised by\nthe instability objection.", "published": "2024-05-20 14:53:25", "link": "http://arxiv.org/abs/2405.12084v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DOP: Diagnostic-Oriented Prompting for Large Language Models in\n  Mathematical Correction", "abstract": "Math world problems correction(MWPC) is a novel task dedicated to rectifying\nreasoning errors in the process of solving mathematical problems. In this\npaper, leveraging the advancements in large language models (LLMs), we address\ntwo key objectives:(1) Distinguishing between mathematical reasoning and error\ncorrection; (2) Exploring strategies to enhance the error correction\ncapabilities of LLMs in mathematics to solve MWPC task. We noticed that, in\nreal-time education,assisting students in recognizing their mistakes is more\ncrucial than simply providing correct answers. However, current research tends\nto prioritize obtaining accurate solutions to math problems rather than\ncorrecting potentially incorrect ones. Therefore, we modify the research\nparadigm, demonstrating that improving mathematical reasoning abilities does\nnot equate to mastery in error correction. Meanwhile, we propose a novel method\ncalled diagnostic-oriented promping(DOP) aimed at facilitating LLMs to excel in\nerror correction. In experiments, DOP has shown outstanding performance,\nhighlighting its significant impact. We argue that in mathematical education,\nthe demand for outstanding correctors surpasses that for proficient reasoners.\nCodes and data are available on\nhttps://github.com/ChenhaoEcnuCS/Reason-Correct.", "published": "2024-05-20 15:13:22", "link": "http://arxiv.org/abs/2405.12100v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CT-Eval: Benchmarking Chinese Text-to-Table Performance in Large\n  Language Models", "abstract": "Text-to-Table aims to generate structured tables to convey the key\ninformation from unstructured documents. Existing text-to-table datasets are\ntypically oriented English, limiting the research in non-English languages.\nMeanwhile, the emergence of large language models (LLMs) has shown great\nsuccess as general task solvers in multi-lingual settings (e.g., ChatGPT),\ntheoretically enabling text-to-table in other languages. In this paper, we\npropose a Chinese text-to-table dataset, CT-Eval, to benchmark LLMs on this\ntask. Our preliminary analysis of English text-to-table datasets highlights two\nkey factors for dataset construction: data diversity and data hallucination.\nInspired by this, the CT-Eval dataset selects a popular Chinese\nmultidisciplinary online encyclopedia as the source and covers 28 domains to\nensure data diversity. To minimize data hallucination, we first train an LLM to\njudge and filter out the task samples with hallucination, then employ human\nannotators to clean the hallucinations in the validation and testing sets.\nAfter this process, CT-Eval contains 88.6K task samples. Using CT-Eval, we\nevaluate the performance of open-source and closed-source LLMs. Our results\nreveal that zero-shot LLMs (including GPT-4) still have a significant\nperformance gap compared with human judgment. Furthermore, after fine-tuning,\nopen-source LLMs can significantly improve their text-to-table ability,\noutperforming GPT-4 by a large margin. In short, CT-Eval not only helps\nresearchers evaluate and quickly understand the Chinese text-to-table ability\nof existing LLMs but also serves as a valuable resource to significantly\nimprove the text-to-table performance of LLMs.", "published": "2024-05-20 16:58:02", "link": "http://arxiv.org/abs/2405.12174v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MathBench: Evaluating the Theory and Application Proficiency of LLMs\n  with a Hierarchical Mathematics Benchmark", "abstract": "Recent advancements in large language models (LLMs) have showcased\nsignificant improvements in mathematics. However, traditional math benchmarks\nlike GSM8k offer a unidimensional perspective, falling short in providing a\nholistic assessment of the LLMs' math capabilities. To address this gap, we\nintroduce MathBench, a new benchmark that rigorously assesses the mathematical\ncapabilities of large language models. MathBench spans a wide range of\nmathematical disciplines, offering a detailed evaluation of both theoretical\nunderstanding and practical problem-solving skills. The benchmark progresses\nthrough five distinct stages, from basic arithmetic to college mathematics, and\nis structured to evaluate models at various depths of knowledge. Each stage\nincludes theoretical questions and application problems, allowing us to measure\na model's mathematical proficiency and its ability to apply concepts in\npractical scenarios. MathBench aims to enhance the evaluation of LLMs'\nmathematical abilities, providing a nuanced view of their knowledge\nunderstanding levels and problem solving skills in a bilingual context. The\nproject is released at https://github.com/open-compass/MathBench .", "published": "2024-05-20 17:52:29", "link": "http://arxiv.org/abs/2405.12209v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question-Based Retrieval using Atomic Units for Enterprise RAG", "abstract": "Enterprise retrieval augmented generation (RAG) offers a highly flexible\nframework for combining powerful large language models (LLMs) with internal,\npossibly temporally changing, documents. In RAG, documents are first chunked.\nRelevant chunks are then retrieved for a user query, which are passed as\ncontext to a synthesizer LLM to generate the query response. However, the\nretrieval step can limit performance, as incorrect chunks can lead the\nsynthesizer LLM to generate a false response. This work applies a zero-shot\nadaptation of standard dense retrieval steps for more accurate chunk recall.\nSpecifically, a chunk is first decomposed into atomic statements. A set of\nsynthetic questions are then generated on these atoms (with the chunk as the\ncontext). Dense retrieval involves finding the closest set of synthetic\nquestions, and associated chunks, to the user query. It is found that retrieval\nwith the atoms leads to higher recall than retrieval with chunks. Further\nperformance gain is observed with retrieval using the synthetic questions\ngenerated over the atoms. Higher recall at the retrieval step enables higher\nperformance of the enterprise LLM using the RAG pipeline.", "published": "2024-05-20 20:27:00", "link": "http://arxiv.org/abs/2405.12363v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Targeted Multilingual Adaptation for Low-resource Language Families", "abstract": "The \"massively-multilingual\" training of multilingual models is known to\nlimit their utility in any one language, and they perform particularly poorly\non low-resource languages. However, there is evidence that low-resource\nlanguages can benefit from targeted multilinguality, where the model is trained\non closely related languages. To test this approach more rigorously, we\nsystematically study best practices for adapting a pre-trained model to a\nlanguage family. Focusing on the Uralic family as a test case, we adapt XLM-R\nunder various configurations to model 15 languages; we then evaluate the\nperformance of each experimental setting on two downstream tasks and 11\nevaluation languages. Our adapted models significantly outperform mono- and\nmultilingual baselines. Furthermore, a regression analysis of hyperparameter\neffects reveals that adapted vocabulary size is relatively unimportant for\nlow-resource languages, and that low-resource languages can be aggressively\nup-sampled during training at little detriment to performance in high-resource\nlanguages. These results introduce new best practices for performing language\nadaptation in a targeted setting.", "published": "2024-05-20 23:38:06", "link": "http://arxiv.org/abs/2405.12413v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Role of Dependency Distance in Text Simplification: A Human vs ChatGPT\n  Simplification Comparison", "abstract": "This study investigates human and ChatGPT text simplification and its\nrelationship to dependency distance. A set of 220 sentences, with increasing\ngrammatical difficulty as measured in a prior user study, were simplified by a\nhuman expert and using ChatGPT. We found that the three sentence sets all\ndiffered in mean dependency distances: the highest in the original sentence\nset, followed by ChatGPT simplified sentences, and the human simplified\nsentences showed the lowest mean dependency distance.", "published": "2024-05-20 17:43:17", "link": "http://arxiv.org/abs/2406.17787v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Ordinality in Text Classification: A Comparative Study of\n  Explicit and Implicit Techniques", "abstract": "Ordinal Classification (OC) is a widely encountered challenge in Natural\nLanguage Processing (NLP), with applications in various domains such as\nsentiment analysis, rating prediction, and more. Previous approaches to tackle\nOC have primarily focused on modifying existing or creating novel loss\nfunctions that \\textbf{explicitly} account for the ordinal nature of labels.\nHowever, with the advent of Pretrained Language Models (PLMs), it became\npossible to tackle ordinality through the \\textbf{implicit} semantics of the\nlabels as well. This paper provides a comprehensive theoretical and empirical\nexamination of both these approaches. Furthermore, we also offer strategic\nrecommendations regarding the most effective approach to adopt based on\nspecific settings.", "published": "2024-05-20 04:31:04", "link": "http://arxiv.org/abs/2405.11775v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Systematic Review on Healthcare Systems Engineering utilizing ChatGPT", "abstract": "This paper presents an analytical framework for conducting academic reviews\nin the field of Healthcare Systems Engineering, employing ChatGPT, a\nstate-of-the-art tool among recent language models. We utilized 9,809 abstract\nparagraphs from conference presentations to systematically review the field.\nThe framework comprises distinct analytical processes, each employing tailored\nprompts and the systematic use of the ChatGPT API. Through this framework, we\norganized the target field into 11 topic categories and conducted a\ncomprehensive analysis covering quantitative yearly trends and detailed\nsub-categories. This effort explores the potential for leveraging ChatGPT to\nalleviate the burden of academic reviews. Furthermore, it provides valuable\ninsights into the dynamic landscape of Healthcare Systems Engineering research.", "published": "2024-05-20 06:25:39", "link": "http://arxiv.org/abs/2405.11817v1", "categories": ["cs.ET", "cs.CL"], "primary_category": "cs.ET"}
{"title": "Beyond MLE: Investigating SEARNN for Low-Resourced Neural Machine\n  Translation", "abstract": "Structured prediction tasks, like machine translation, involve learning\nfunctions that map structured inputs to structured outputs. Recurrent Neural\nNetworks (RNNs) have historically been a popular choice for such tasks,\nincluding in natural language processing (NLP) applications. However, training\nRNNs using Maximum Likelihood Estimation (MLE) has its limitations, including\nexposure bias and a mismatch between training and testing metrics. SEARNN,\nbased on the learning to search (L2S) framework, has been proposed as an\nalternative to MLE for RNN training. This project explored the potential of\nSEARNN to improve machine translation for low-resourced African languages -- a\nchallenging task characterized by limited training data availability and the\nmorphological complexity of the languages. Through experiments conducted on\ntranslation for English to Igbo, French to \\ewe, and French to \\ghomala\ndirections, this project evaluated the efficacy of SEARNN over MLE in\naddressing the unique challenges posed by these languages. With an average BLEU\nscore improvement of $5.4$\\% over the MLE objective, we proved that SEARNN is\nindeed a viable algorithm to effectively train RNNs on machine translation for\nlow-resourced languages.", "published": "2024-05-20 06:28:43", "link": "http://arxiv.org/abs/2405.11819v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CoNLL#: Fine-grained Error Analysis and a Corrected Test Set for\n  CoNLL-03 English", "abstract": "Modern named entity recognition systems have steadily improved performance in\nthe age of larger and more powerful neural models. However, over the past\nseveral years, the state-of-the-art has seemingly hit another plateau on the\nbenchmark CoNLL-03 English dataset. In this paper, we perform a deep dive into\nthe test outputs of the highest-performing NER models, conducting a\nfine-grained evaluation of their performance by introducing new document-level\nannotations on the test set. We go beyond F1 scores by categorizing errors in\norder to interpret the true state of the art for NER and guide future work. We\nreview previous attempts at correcting the various flaws of the test set and\nintroduce CoNLL#, a new corrected version of the test set that addresses its\nsystematic and most prevalent errors, allowing for low-noise, interpretable\nerror analysis.", "published": "2024-05-20 08:16:34", "link": "http://arxiv.org/abs/2405.11865v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Intuitive Fine-Tuning: Towards Simplifying Alignment into a Single\n  Process", "abstract": "Supervised Fine-Tuning (SFT) and Preference Optimization (PO) are two\nfundamental processes for enhancing the capabilities of Language Models (LMs)\npost pre-training, aligning them better with human preferences. Although SFT\nadvances in training efficiency, PO delivers better alignment, thus they are\noften combined. However, common practices simply apply them sequentially\nwithout integrating their optimization objectives, ignoring the opportunities\nto bridge their paradigm gap and take the strengths from both. To obtain a\nunified understanding, we interpret SFT and PO with two sub-processes --\nPreference Estimation and Transition Optimization -- defined at token level\nwithin the Markov Decision Process (MDP) framework. This modeling shows that\nSFT is only a specialized case of PO with inferior estimation and optimization.\nPO evaluates the quality of model's entire generated answer, whereas SFT only\nscores predicted tokens based on preceding tokens from target answers.\nTherefore, SFT overestimates the ability of model, leading to inferior\noptimization. Building on this view, we introduce Intuitive Fine-Tuning (IFT)\nto integrate SFT and Preference Optimization into a single process. IFT\ncaptures LMs' intuitive sense of the entire answers through a temporal residual\nconnection, but it solely relies on a single policy and the same volume of\nnon-preference-labeled data as SFT. Our experiments show that IFT performs\ncomparably or even superiorly to sequential recipes of SFT and some typical\nPreference Optimization methods across several tasks, particularly those\nrequires generation, reasoning, and fact-following abilities. An explainable\nFrozen Lake game further validates the effectiveness of IFT for getting\ncompetitive policy.", "published": "2024-05-20 08:23:28", "link": "http://arxiv.org/abs/2405.11870v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unveiling and Manipulating Prompt Influence in Large Language Models", "abstract": "Prompts play a crucial role in guiding the responses of Large Language Models\n(LLMs). However, the intricate role of individual tokens in prompts, known as\ninput saliency, in shaping the responses remains largely underexplored.\nExisting saliency methods either misalign with LLM generation objectives or\nrely heavily on linearity assumptions, leading to potential inaccuracies. To\naddress this, we propose Token Distribution Dynamics (TDD), a\n\\textcolor{black}{simple yet effective} approach to unveil and manipulate the\nrole of prompts in generating LLM outputs. TDD leverages the robust\ninterpreting capabilities of the language model head (LM head) to assess input\nsaliency. It projects input tokens into the embedding space and then estimates\ntheir significance based on distribution dynamics over the vocabulary. We\nintroduce three TDD variants: forward, backward, and bidirectional, each\noffering unique insights into token relevance. Extensive experiments reveal\nthat the TDD surpasses state-of-the-art baselines with a big margin in\nelucidating the causal relationships between prompts and LLM outputs. Beyond\nmere interpretation, we apply TDD to two prompt manipulation tasks for\ncontrolled text generation: zero-shot toxic language suppression and sentiment\nsteering. Empirical results underscore TDD's proficiency in identifying both\ntoxic and sentimental cues in prompts, subsequently mitigating toxicity or\nmodulating sentiment in the generated content.", "published": "2024-05-20 09:15:36", "link": "http://arxiv.org/abs/2405.11891v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ARAIDA: Analogical Reasoning-Augmented Interactive Data Annotation", "abstract": "Human annotation is a time-consuming task that requires a significant amount\nof effort. To address this issue, interactive data annotation utilizes an\nannotation model to provide suggestions for humans to approve or correct.\nHowever, annotation models trained with limited labeled data are prone to\ngenerating incorrect suggestions, leading to extra human correction effort. To\ntackle this challenge, we propose Araida, an analogical reasoning-based\napproach that enhances automatic annotation accuracy in the interactive data\nannotation setting and reduces the need for human corrections. Araida involves\nan error-aware integration strategy that dynamically coordinates an annotation\nmodel and a k-nearest neighbors (KNN) model, giving more importance to KNN's\npredictions when predictions from the annotation model are deemed inaccurate.\nEmpirical studies demonstrate that Araida is adaptable to different annotation\ntasks and models. On average, it reduces human correction labor by 11.02%\ncompared to vanilla interactive data annotation methods.", "published": "2024-05-20 09:48:15", "link": "http://arxiv.org/abs/2405.11912v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "WisPerMed at BioLaySumm: Adapting Autoregressive Large Language Models\n  for Lay Summarization of Scientific Articles", "abstract": "This paper details the efforts of the WisPerMed team in the BioLaySumm2024\nShared Task on automatic lay summarization in the biomedical domain, aimed at\nmaking scientific publications accessible to non-specialists. Large language\nmodels (LLMs), specifically the BioMistral and Llama3 models, were fine-tuned\nand employed to create lay summaries from complex scientific texts. The\nsummarization performance was enhanced through various approaches, including\ninstruction tuning, few-shot learning, and prompt variations tailored to\nincorporate specific context information. The experiments demonstrated that\nfine-tuning generally led to the best performance across most evaluated\nmetrics. Few-shot learning notably improved the models' ability to generate\nrelevant and factually accurate texts, particularly when using a well-crafted\nprompt. Additionally, a Dynamic Expert Selection (DES) mechanism to optimize\nthe selection of text outputs based on readability and factuality metrics was\ndeveloped. Out of 54 participants, the WisPerMed team reached the 4th place,\nmeasured by readability, factuality, and relevance. Determined by the overall\nscore, our approach improved upon the baseline by approx. 5.5 percentage points\nand was only approx 1.5 percentage points behind the first place.", "published": "2024-05-20 10:54:47", "link": "http://arxiv.org/abs/2405.11950v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A review on the use of large language models as virtual tutors", "abstract": "Transformer architectures contribute to managing long-term dependencies for\nNatural Language Processing, representing one of the most recent changes in the\nfield. These architectures are the basis of the innovative, cutting-edge Large\nLanguage Models (LLMs) that have produced a huge buzz in several fields and\nindustrial sectors, among the ones education stands out. Accordingly, these\ngenerative Artificial Intelligence-based solutions have directed the change in\ntechniques and the evolution in educational methods and contents, along with\nnetwork infrastructure, towards high-quality learning. Given the popularity of\nLLMs, this review seeks to provide a comprehensive overview of those solutions\ndesigned specifically to generate and evaluate educational materials and which\ninvolve students and teachers in their design or experimental plan. To the best\nof our knowledge, this is the first review of educational applications (e.g.,\nstudent assessment) of LLMs. As expected, the most common role of these systems\nis as virtual tutors for automatic question generation. Moreover, the most\npopular models are GTP-3 and BERT. However, due to the continuous launch of new\ngenerative models, new works are expected to be published shortly.", "published": "2024-05-20 12:33:42", "link": "http://arxiv.org/abs/2405.11983v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Imp: Highly Capable Large Multimodal Models for Mobile Devices", "abstract": "By harnessing the capabilities of large language models (LLMs), recent large\nmultimodal models (LMMs) have shown remarkable versatility in open-world\nmultimodal understanding. Nevertheless, they are usually parameter-heavy and\ncomputation-intensive, thus hindering their applicability in\nresource-constrained scenarios. To this end, several lightweight LMMs have been\nproposed successively to maximize the capabilities under constrained scale\n(e.g., 3B). Despite the encouraging results achieved by these methods, most of\nthem only focus on one or two aspects of the design space, and the key design\nchoices that influence model capability have not yet been thoroughly\ninvestigated. In this paper, we conduct a systematic study for lightweight LMMs\nfrom the aspects of model architecture, training strategy, and training data.\nBased on our findings, we obtain Imp -- a family of highly capable LMMs at the\n2B-4B scales. Notably, our Imp-3B model steadily outperforms all the existing\nlightweight LMMs of similar size, and even surpasses the state-of-the-art LMMs\nat the 13B scale. With low-bit quantization and resolution reduction\ntechniques, our Imp model can be deployed on a Qualcomm Snapdragon 8Gen3 mobile\nchip with a high inference speed of about 13 tokens/s.", "published": "2024-05-20 15:23:19", "link": "http://arxiv.org/abs/2405.12107v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning", "abstract": "Low-rank adaptation is a popular parameter-efficient fine-tuning method for\nlarge language models. In this paper, we analyze the impact of low-rank\nupdating, as implemented in LoRA. Our findings suggest that the low-rank\nupdating mechanism may limit the ability of LLMs to effectively learn and\nmemorize new knowledge. Inspired by this observation, we propose a new method\ncalled MoRA, which employs a square matrix to achieve high-rank updating while\nmaintaining the same number of trainable parameters. To achieve it, we\nintroduce the corresponding non-parameter operators to reduce the input\ndimension and increase the output dimension for the square matrix. Furthermore,\nthese operators ensure that the weight can be merged back into LLMs, which\nmakes our method can be deployed like LoRA. We perform a comprehensive\nevaluation of our method across five tasks: instruction tuning, mathematical\nreasoning, continual pretraining, memory and pretraining. Our method\noutperforms LoRA on memory-intensive tasks and achieves comparable performance\non other tasks.", "published": "2024-05-20 15:48:32", "link": "http://arxiv.org/abs/2405.12130v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Eliciting Problem Specifications via Large Language Models", "abstract": "Cognitive systems generally require a human to translate a problem definition\ninto some specification that the cognitive system can use to attempt to solve\nthe problem or perform the task. In this paper, we illustrate that large\nlanguage models (LLMs) can be utilized to map a problem class, defined in\nnatural language, into a semi-formal specification that can then be utilized by\nan existing reasoning and learning system to solve instances from the problem\nclass. We present the design of LLM-enabled cognitive task analyst agent(s).\nImplemented with LLM agents, this system produces a definition of problem\nspaces for tasks specified in natural language. LLM prompts are derived from\nthe definition of problem spaces in the AI literature and general\nproblem-solving strategies (Polya's How to Solve It). A cognitive system can\nthen use the problem-space specification, applying domain-general problem\nsolving strategies (\"weak methods\" such as search), to solve multiple instances\nof problems from the problem class. This result, while preliminary, suggests\nthe potential for speeding cognitive systems research via disintermediation of\nproblem formulation while also retaining core capabilities of cognitive\nsystems, such as robust inference and online learning.", "published": "2024-05-20 16:19:02", "link": "http://arxiv.org/abs/2405.12147v2", "categories": ["cs.AI", "cs.CL", "I.2.11; I.2.7"], "primary_category": "cs.AI"}
{"title": "Fennec: Fine-grained Language Model Evaluation and Correction Extended\n  through Branching and Bridging", "abstract": "The rapid advancement of large language models has given rise to a plethora\nof applications across a myriad of real-world tasks, mainly centered on\naligning with human intent. However, the complexities inherent in human intent\nnecessitate a dependence on labor-intensive and time-consuming human\nevaluation. To alleviate this constraint, we delve into the paradigm of\nemploying open-source large language models as evaluators, aligning with the\nprevailing trend of utilizing GPT-4. Particularly, we present a step-by-step\nevaluation framework: \\textbf{Fennec}, capable of \\textbf{F}ine-grained\n\\textbf{E}valuatio\\textbf{N} and correctio\\textbf{N} \\textbf{E}xtended through\nbran\\textbf{C}hing and bridging. Specifically, the branching operation dissects\nthe evaluation task into various dimensions and granularities, thereby\nalleviating the challenges associated with evaluation. Concurrently, the\nbridging operation amalgamates diverse training datasets, augmenting the\nvariety of evaluation tasks. In experimental trials, our 7B model consistently\noutperforms open-source larger-scale evaluation models across various widely\nadopted benchmarks in terms of both \\textit{Agreement} and\n\\textit{Consistency}, closely approaching the capabilities of GPT-4. We employ\nthe fine-grained correction capabilities induced by the evaluation model to\nrefine multiple model responses, and the results show that the refinement\nelevates the quality of responses, leading to an improvement of 1-2 points on\nthe MT-Bench. Our code is available at\nGithub\\footnote{\\url{https://github.com/dropreg/Fennec}}.", "published": "2024-05-20 16:47:22", "link": "http://arxiv.org/abs/2405.12163v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Modeling citation worthiness by using attention-based bidirectional long\n  short-term memory networks and interpretable models", "abstract": "Scientist learn early on how to cite scientific sources to support their\nclaims. Sometimes, however, scientists have challenges determining where a\ncitation should be situated -- or, even worse, fail to cite a source\naltogether. Automatically detecting sentences that need a citation (i.e.,\ncitation worthiness) could solve both of these issues, leading to more robust\nand well-constructed scientific arguments. Previous researchers have applied\nmachine learning to this task but have used small datasets and models that do\nnot take advantage of recent algorithmic developments such as attention\nmechanisms in deep learning. We hypothesize that we can develop significantly\naccurate deep learning architectures that learn from large supervised datasets\nconstructed from open access publications. In this work, we propose a\nBidirectional Long Short-Term Memory (BiLSTM) network with attention mechanism\nand contextual information to detect sentences that need citations. We also\nproduce a new, large dataset (PMOA-CITE) based on PubMed Open Access Subset,\nwhich is orders of magnitude larger than previous datasets. Our experiments\nshow that our architecture achieves state of the art performance on the\nstandard ACL-ARC dataset ($F_{1}=0.507$) and exhibits high performance\n($F_{1}=0.856$) on the new PMOA-CITE. Moreover, we show that it can transfer\nlearning across these datasets. We further use interpretable models to\nilluminate how specific language is used to promote and inhibit citations. We\ndiscover that sections and surrounding sentences are crucial for our improved\npredictions. We further examined purported mispredictions of the model, and\nuncovered systematic human mistakes in citation behavior and source data. This\nopens the door for our model to check documents during pre-submission and\npre-archival procedures. We make this new dataset, the code, and a web-based\ntool available to the community.", "published": "2024-05-20 17:45:36", "link": "http://arxiv.org/abs/2405.12206v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large language models for sentiment analysis of newspaper articles\n  during COVID-19: The Guardian", "abstract": "During the COVID-19 pandemic, the news media coverage encompassed a wide\nrange of topics that includes viral transmission, allocation of medical\nresources, and government response measures. There have been studies on\nsentiment analysis of social media platforms during COVID-19 to understand the\npublic response given the rise of cases and government strategies implemented\nto control the spread of the virus. Sentiment analysis can provide a better\nunderstanding of changes in societal opinions and emotional trends during the\npandemic. Apart from social media, newspapers have played a vital role in the\ndissemination of information, including information from the government,\nexperts, and also the public about various topics. A study of sentiment\nanalysis of newspaper sources during COVID-19 for selected countries can give\nan overview of how the media covered the pandemic. In this study, we select The\nGuardian newspaper and provide a sentiment analysis during various stages of\nCOVID-19 that includes initial transmission, lockdowns and vaccination. We\nemploy novel large language models (LLMs) and refine them with expert-labelled\nsentiment analysis data. We also provide an analysis of sentiments experienced\npre-pandemic for comparison. The results indicate that during the early\npandemic stages, public sentiment prioritised urgent crisis response, later\nshifting focus to addressing the impact on health and the economy. In\ncomparison with related studies about social media sentiment analyses, we found\na discrepancy between The Guardian with dominance of negative sentiments (sad,\nannoyed, anxious and denial), suggesting that social media offers a more\ndiversified emotional reflection. We found a grim narrative in The Guardian\nwith overall dominance of negative sentiments, pre and during COVID-19 across\nnews sections including Australia, UK, World News, and Opinion", "published": "2024-05-20 07:10:52", "link": "http://arxiv.org/abs/2405.13056v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "RNG: Reducing Multi-level Noise and Multi-grained Semantic Gap for Joint\n  Multimodal Aspect-Sentiment Analysis", "abstract": "As an important multimodal sentiment analysis task, Joint Multimodal\nAspect-Sentiment Analysis (JMASA), aiming to jointly extract aspect terms and\ntheir associated sentiment polarities from the given text-image pairs, has\ngained increasing concerns. Existing works encounter two limitations: (1)\nmulti-level modality noise, i.e., instance- and feature-level noise; and (2)\nmulti-grained semantic gap, i.e., coarse- and fine-grained gap. Both issues may\ninterfere with accurate identification of aspect-sentiment pairs. To address\nthese limitations, we propose a novel framework named RNG for JMASA.\nSpecifically, to simultaneously reduce multi-level modality noise and\nmulti-grained semantic gap, we design three constraints: (1) Global Relevance\nConstraint (GR-Con) based on text-image similarity for instance-level noise\nreduction, (2) Information Bottleneck Constraint (IB-Con) based on the\nInformation Bottleneck (IB) principle for feature-level noise reduction, and\n(3) Semantic Consistency Constraint (SC-Con) based on mutual information\nmaximization in a contrastive learning way for multi-grained semantic gap\nreduction. Extensive experiments on two datasets validate our new\nstate-of-the-art performance.", "published": "2024-05-20 12:18:46", "link": "http://arxiv.org/abs/2405.13059v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework", "abstract": "As large language models (LLMs) continue to grow by scaling laws,\nreinforcement learning from human feedback (RLHF) has gained significant\nattention due to its outstanding performance. However, unlike pretraining or\nfine-tuning a single model, scaling reinforcement learning from human feedback\n(RLHF) for training large language models poses coordination challenges across\nfour models. We present OpenRLHF, an open-source framework enabling efficient\nRLHF scaling. Unlike existing RLHF frameworks that co-locate four models on the\nsame GPUs, OpenRLHF re-designs scheduling for the models beyond 70B parameters\nusing Ray, vLLM, and DeepSpeed, leveraging improved resource utilization and\ndiverse training approaches. Integrating seamlessly with Hugging Face, OpenRLHF\nprovides an out-of-the-box solution with optimized algorithms and launch\nscripts, which ensures user-friendliness. OpenRLHF implements RLHF, DPO,\nrejection sampling, and other alignment techniques. Empowering state-of-the-art\nLLM development, OpenRLHF's code is available at\n\\url{https://github.com/OpenRLHF/OpenRLHF}.", "published": "2024-05-20 01:04:40", "link": "http://arxiv.org/abs/2405.11143v4", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Token-wise Influential Training Data Retrieval for Large Language Models", "abstract": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn.", "published": "2024-05-20 01:57:34", "link": "http://arxiv.org/abs/2405.11724v2", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Inverse Design of Metal-Organic Frameworks Using Quantum Natural\n  Language Processing", "abstract": "In this study, we explore the potential of using quantum natural language\nprocessing (QNLP) to inverse design metal-organic frameworks (MOFs) with\ntargeted properties. Specifically, by analyzing 150 hypothetical MOF structures\nconsisting of 10 metal nodes and 15 organic ligands, we categorize these\nstructures into four distinct classes for pore volume and $H_{2}$ uptake\nvalues. We then compare various QNLP models (i.e. the bag-of-words, DisCoCat\n(Distributional Compositional Categorical), and sequence-based models) to\nidentify the most effective approach to process the MOF dataset. Using a\nclassical simulator provided by the IBM Qiskit, the bag-of-words model is\nidentified to be the optimum model, achieving validation accuracies of 85.7%\nand 86.7% for binary classification tasks on pore volume and $H_{2}$ uptake,\nrespectively. Further, we developed multi-class classification models tailored\nto the probabilistic nature of quantum circuits, with average test accuracies\nof 88.4% and 80.7% across different classes for pore volume and $H_{2}$ uptake\ndatasets. Finally, the performance of generating MOF with target properties\nshowed accuracies of 93.5% for pore volume and 89% for $H_{2}$ uptake,\nrespectively. Although our investigation covers only a fraction of the vast MOF\nsearch space, it marks a promising first step towards using quantum computing\nfor materials design, offering a new perspective through which to explore the\ncomplex landscape of MOFs.", "published": "2024-05-20 05:02:12", "link": "http://arxiv.org/abs/2405.11783v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "quant-ph"], "primary_category": "cs.LG"}
{"title": "A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI:\n  The First Romanian Natural Language Inference Corpus", "abstract": "Natural language inference (NLI), the task of recognizing the entailment\nrelationship in sentence pairs, is an actively studied topic serving as a proxy\nfor natural language understanding. Despite the relevance of the task in\nbuilding conversational agents and improving text classification, machine\ntranslation and other NLP tasks, to the best of our knowledge, there is no\npublicly available NLI corpus for the Romanian language. To this end, we\nintroduce the first Romanian NLI corpus (RoNLI) comprising 58K training\nsentence pairs, which are obtained via distant supervision, and 6K validation\nand test sentence pairs, which are manually annotated with the correct labels.\nWe conduct experiments with multiple machine learning methods based on distant\nlearning, ranging from shallow models based on word embeddings to\ntransformer-based neural networks, to establish a set of competitive baselines.\nFurthermore, we improve on the best model by employing a new curriculum\nlearning strategy based on data cartography. Our dataset and code to reproduce\nthe baselines are available at https://github.com/Eduard6421/RONLI.", "published": "2024-05-20 08:41:15", "link": "http://arxiv.org/abs/2405.11877v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Quantifying In-Context Reasoning Effects and Memorization Effects in\n  LLMs", "abstract": "In this study, we propose an axiomatic system to define and quantify the\nprecise memorization and in-context reasoning effects used by the large\nlanguage model (LLM) for language generation. These effects are formulated as\nnon-linear interactions between tokens/words encoded by the LLM. Specifically,\nthe axiomatic system enables us to categorize the memorization effects into\nfoundational memorization effects and chaotic memorization effects, and further\nclassify in-context reasoning effects into enhanced inference patterns,\neliminated inference patterns, and reversed inference patterns. Besides, the\ndecomposed effects satisfy the sparsity property and the universal matching\nproperty, which mathematically guarantee that the LLM's confidence score can be\nfaithfully decomposed into the memorization effects and in-context reasoning\neffects. Experiments show that the clear disentanglement of memorization\neffects and in-context reasoning effects enables a straightforward examination\nof detailed inference patterns encoded by LLMs.", "published": "2024-05-20 08:51:03", "link": "http://arxiv.org/abs/2405.11880v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "On Efficient and Statistical Quality Estimation for Data Annotation", "abstract": "Annotated datasets are an essential ingredient to train, evaluate, compare\nand productionalize supervised machine learning models. It is therefore\nimperative that annotations are of high quality. For their creation, good\nquality management and thereby reliable quality estimates are needed. Then, if\nquality is insufficient during the annotation process, rectifying measures can\nbe taken to improve it. Quality estimation is often performed by having experts\nmanually label instances as correct or incorrect. But checking all annotated\ninstances tends to be expensive. Therefore, in practice, usually only subsets\nare inspected; sizes are chosen mostly without justification or regard to\nstatistical power and more often than not, are relatively small. Basing\nestimates on small sample sizes, however, can lead to imprecise values for the\nerror rate. Using unnecessarily large sample sizes costs money that could be\nbetter spent, for instance on more annotations. Therefore, we first describe in\ndetail how to use confidence intervals for finding the minimal sample size\nneeded to estimate the annotation error rate. Then, we propose applying\nacceptance sampling as an alternative to error rate estimation We show that\nacceptance sampling can reduce the required sample sizes up to 50% while\nproviding the same statistical guarantees.", "published": "2024-05-20 09:57:29", "link": "http://arxiv.org/abs/2405.11919v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Chasing COMET: Leveraging Minimum Bayes Risk Decoding for Self-Improving\n  Machine Translation", "abstract": "This paper explores Minimum Bayes Risk (MBR) decoding for self-improvement in\nmachine translation (MT), particularly for domain adaptation and low-resource\nlanguages. We implement the self-improvement process by fine-tuning the model\non its MBR-decoded forward translations. By employing COMET as the MBR utility\nmetric, we aim to achieve the reranking of translations that better aligns with\nhuman preferences. The paper explores the iterative application of this\napproach and the potential need for language-specific MBR utility metrics. The\nresults demonstrate significant enhancements in translation quality for all\nexamined language pairs, including successful application to domain-adapted\nmodels and generalisation to low-resource settings. This highlights the\npotential of COMET-guided MBR for efficient MT self-improvement in various\nscenarios.", "published": "2024-05-20 10:25:03", "link": "http://arxiv.org/abs/2405.11937v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KG-RAG: Bridging the Gap Between Knowledge and Creativity", "abstract": "Ensuring factual accuracy while maintaining the creative capabilities of\nLarge Language Model Agents (LMAs) poses significant challenges in the\ndevelopment of intelligent agent systems. LMAs face prevalent issues such as\ninformation hallucinations, catastrophic forgetting, and limitations in\nprocessing long contexts when dealing with knowledge-intensive tasks. This\npaper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation)\npipeline, a novel framework designed to enhance the knowledge capabilities of\nLMAs by integrating structured Knowledge Graphs (KGs) with the functionalities\nof LLMs, thereby significantly reducing the reliance on the latent knowledge of\nLLMs. The KG-RAG pipeline constructs a KG from unstructured text and then\nperforms information retrieval over the newly created graph to perform KGQA\n(Knowledge Graph Question Answering). The retrieval methodology leverages a\nnovel algorithm called Chain of Explorations (CoE) which benefits from LLMs\nreasoning to explore nodes and relationships within the KG sequentially.\nPreliminary experiments on the ComplexWebQuestions dataset demonstrate notable\nimprovements in the reduction of hallucinated content and suggest a promising\npath toward developing intelligent systems adept at handling\nknowledge-intensive tasks.", "published": "2024-05-20 14:03:05", "link": "http://arxiv.org/abs/2405.12035v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Linguistic Structure from a Bottleneck on Sequential Information\n  Processing", "abstract": "Human language is a unique form of communication in the natural world,\ndistinguished by its structured nature. Most fundamentally, it is systematic,\nmeaning that signals can be broken down into component parts that are\nindividually meaningful -- roughly, words -- which are combined in a regular\nway to form sentences. Furthermore, the way in which these parts are combined\nmaintains a kind of locality: words are usually concatenated together, and they\nform contiguous phrases, keeping related parts of sentences close to each\nother. We address the challenge of understanding how these basic properties of\nlanguage arise from broader principles of efficient communication under\ninformation processing constraints. Here we show that natural-language-like\nsystematicity arises in codes that are constrained by predictive information, a\nmeasure of the amount of information that must be extracted from the past of a\nsequence in order to predict its future. In simulations, we show that such\ncodes approximately factorize their source distributions, and then express the\nresulting factors systematically and locally. Next, in a series of\ncross-linguistic corpus studies, we show that human languages are structured to\nhave low predictive information at the levels of phonology, morphology, syntax,\nand semantics. Our result suggests that human language performs a sequential,\ndiscrete form of Independent Components Analysis on the statistical\ndistribution over meanings that need to be expressed. It establishes a link\nbetween the statistical and algebraic structure of human language, and\nreinforces the idea that the structure of human language is shaped by\ncommunication under cognitive constraints.", "published": "2024-05-20 15:25:18", "link": "http://arxiv.org/abs/2405.12109v2", "categories": ["cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Reindex-Then-Adapt: Improving Large Language Models for Conversational\n  Recommendation", "abstract": "Large language models (LLMs) are revolutionizing conversational recommender\nsystems by adeptly indexing item content, understanding complex conversational\ncontexts, and generating relevant item titles. However, controlling the\ndistribution of recommended items remains a challenge. This leads to suboptimal\nperformance due to the failure to capture rapidly changing data distributions,\nsuch as item popularity, on targeted conversational recommendation platforms.\nIn conversational recommendation, LLMs recommend items by generating the titles\n(as multiple tokens) autoregressively, making it difficult to obtain and\ncontrol the recommendations over all items. Thus, we propose a\nReindex-Then-Adapt (RTA) framework, which converts multi-token item titles into\nsingle tokens within LLMs, and then adjusts the probability distributions over\nthese single-token item titles accordingly. The RTA framework marries the\nbenefits of both LLMs and traditional recommender systems (RecSys):\nunderstanding complex queries as LLMs do; while efficiently controlling the\nrecommended item distributions in conversational recommendations as traditional\nRecSys do. Our framework demonstrates improved accuracy metrics across three\ndifferent conversational recommendation datasets and two adaptation settings", "published": "2024-05-20 15:37:55", "link": "http://arxiv.org/abs/2405.12119v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Layout Agnostic Human Activity Recognition in Smart Homes through\n  Textual Descriptions Of Sensor Triggers (TDOST)", "abstract": "Human activity recognition (HAR) using ambient sensors in smart homes has\nnumerous applications for human healthcare and wellness. However, building\ngeneral-purpose HAR models that can be deployed to new smart home environments\nrequires a significant amount of annotated sensor data and training overhead.\nMost smart homes vary significantly in their layouts, i.e., floor plans and the\nspecifics of sensors embedded, resulting in low generalizability of HAR models\ntrained for specific homes. We address this limitation by introducing a novel,\nlayout-agnostic modeling approach for HAR systems in smart homes that utilizes\nthe transferrable representational capacity of natural language descriptions of\nraw sensor data. To this end, we generate Textual Descriptions Of Sensor\nTriggers (TDOST) that encapsulate the surrounding trigger conditions and\nprovide cues for underlying activities to the activity recognition models.\nLeveraging textual embeddings, rather than raw sensor data, we create activity\nrecognition systems that predict standard activities across homes without\neither (re-)training or adaptation on target homes. Through an extensive\nevaluation, we demonstrate the effectiveness of TDOST-based models in unseen\nsmart homes through experiments on benchmarked CASAS datasets. Furthermore, we\nconduct a detailed analysis of how the individual components of our approach\naffect downstream activity recognition performance.", "published": "2024-05-20 20:37:44", "link": "http://arxiv.org/abs/2405.12368v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Large Language Models for Medicine: A Survey", "abstract": "To address challenges in the digital economy's landscape of digital\nintelligence, large language models (LLMs) have been developed. Improvements in\ncomputational power and available resources have significantly advanced LLMs,\nallowing their integration into diverse domains for human life. Medical LLMs\nare essential application tools with potential across various medical\nscenarios. In this paper, we review LLM developments, focusing on the\nrequirements and applications of medical LLMs. We provide a concise overview of\nexisting models, aiming to explore advanced research directions and benefit\nresearchers for future medical applications. We emphasize the advantages of\nmedical LLMs in applications, as well as the challenges encountered during\ntheir development. Finally, we suggest directions for technical integration to\nmitigate challenges and potential research directions for the future of medical\nLLMs, aiming to meet the demands of the medical field better.", "published": "2024-05-20 02:32:26", "link": "http://arxiv.org/abs/2405.13055v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "A Novel Method for News Article Event-Based Embedding", "abstract": "Embedding news articles is a crucial tool for multiple fields, such as media\nbias detection, identifying fake news, and making news recommendations.\nHowever, existing news embedding methods are not optimized to capture the\nlatent context of news events. Most embedding methods rely on full-text\ninformation and neglect time-relevant embedding generation. In this paper, we\npropose a novel lightweight method that optimizes news embedding generation by\nfocusing on entities and themes mentioned in articles and their historical\nconnections to specific events. We suggest a method composed of three stages.\nFirst, we process and extract events, entities, and themes from the given news\narticles. Second, we generate periodic time embeddings for themes and entities\nby training time-separated GloVe models on current and historical data. Lastly,\nwe concatenate the news embeddings generated by two distinct approaches: Smooth\nInverse Frequency (SIF) for article-level vectors and Siamese Neural Networks\nfor embeddings with nuanced event-related information. We leveraged over\n850,000 news articles and 1,000,000 events from the GDELT project to test and\nevaluate our method. We conducted a comparative analysis of different news\nembedding generation methods for validation. Our experiments demonstrate that\nour approach can both improve and outperform state-of-the-art methods on shared\nevent detection tasks.", "published": "2024-05-20 20:55:07", "link": "http://arxiv.org/abs/2405.13071v2", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Thesis: Document Summarization with applications to Keyword extraction\n  and Image Retrieval", "abstract": "Automatic summarization is the process of reducing a text document in order\nto generate a summary that retains the most important points of the original\ndocument. In this work, we study two problems - i) summarizing a text document\nas set of keywords/caption, for image recommedation, ii) generating opinion\nsummary which good mix of relevancy and sentiment with the text document.\nIntially, we present our work on an recommending images for enhancing a\nsubstantial amount of existing plain text news articles. We use probabilistic\nmodels and word similarity heuristics to generate captions and extract\nKey-phrases which are re-ranked using a rank aggregation framework with\nrelevance feedback mechanism. We show that such rank aggregation and relevant\nfeedback which are typically used in Tagging Documents, Text Information\nRetrieval also helps in improving image retrieval. These queries are fed to the\nYahoo Search Engine to obtain relevant images 1. Our proposed method is\nobserved to perform better than all existing baselines. Additonally, We propose\na set of submodular functions for opinion summarization. Opinion summarization\nhas built in it the tasks of summarization and sentiment detection. However, it\nis not easy to detect sentiment and simultaneously extract summary. The two\ntasks conflict in the sense that the demand of compression may drop sentiment\nbearing sentences, and the demand of sentiment detection may bring in redundant\nsentences. However, using submodularity we show how to strike a balance between\nthe two requirements. Our functions generate summaries such that there is good\ncorrelation between document sentiment and summary sentiment along with good\nROUGE score. We also compare the performances of the proposed submodular\nfunctions.", "published": "2024-05-20 21:27:18", "link": "http://arxiv.org/abs/2406.00013v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "A Multi-Modal Explainability Approach for Human-Aware Robots in\n  Multi-Party Conversation", "abstract": "The addressee estimation (understanding to whom somebody is talking) is a\nfundamental task for human activity recognition in multi-party conversation\nscenarios. Specifically, in the field of human-robot interaction, it becomes\neven more crucial to enable social robots to participate in such interactive\ncontexts. However, it is usually implemented as a binary classification task,\nrestricting the robot's capability to estimate whether it was addressed\n\\review{or not, which} limits its interactive skills. For a social robot to\ngain the trust of humans, it is also important to manifest a certain level of\ntransparency and explainability. Explainable artificial intelligence thus plays\na significant role in the current machine learning applications and models, to\nprovide explanations for their decisions besides excellent performance. In our\nwork, we a) present an addressee estimation model with improved performance in\ncomparison with the previous state-of-the-art; b) further modify this model to\ninclude inherently explainable attention-based segments; c) implement the\nexplainable addressee estimation as part of a modular cognitive architecture\nfor multi-party conversation in an iCub robot; d) validate the real-time\nperformance of the explainable model in multi-party human-robot interaction; e)\npropose several ways to incorporate explainability and transparency in the\naforementioned architecture; and f) perform an online user study to analyze the\neffect of various explanations on how human participants perceive the robot.", "published": "2024-05-20 13:09:32", "link": "http://arxiv.org/abs/2407.03340v2", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG", "cs.RO", "eess.IV", "I.4.8; I.2.10; I.2.9; I.2.11; J.4"], "primary_category": "cs.AI"}
{"title": "Source Localization by Multidimensional Steered Response Power Mapping\n  with Sparse Bayesian Learning", "abstract": "We propose an advance Steered Response Power (SRP) method for localizing\nmultiple sources. While conventional SRP performs well in adverse conditions,\nit remains to struggle in scenarios with closely neighboring sources, resulting\nin ambiguous SRP maps. We address this issue by applying sparsity optimization\nin SRP to obtain high-resolution maps. Our approach represents SRP maps as\nmultidimensional matrices to preserve time-frequency information and further\nimprove performance in unfavorable conditions. We use multi-dictionary Sparse\nBayesian Learning to localize sources without needing prior knowledge of their\nquantity. We validate our method through practical experiments with a\n16-channel planar microphone array and compare against three other SRP and\nsparsity-based methods. Our multidimensional SRP approach outperforms\nconventional SRP and the current state-of-the-art sparse SRP methods for\nlocalizing closely spaced sources in a reverberant room.", "published": "2024-05-20 05:18:47", "link": "http://arxiv.org/abs/2405.11792v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "SSAMBA: Self-Supervised Audio Representation Learning with Mamba State\n  Space Model", "abstract": "Transformers have revolutionized deep learning across various tasks,\nincluding audio representation learning, due to their powerful modeling\ncapabilities. However, they often suffer from quadratic complexity in both GPU\nmemory usage and computational inference time, affecting their efficiency.\nRecently, state space models (SSMs) like Mamba have emerged as a promising\nalternative, offering a more efficient approach by avoiding these complexities.\nGiven these advantages, we explore the potential of SSM-based models in audio\ntasks. In this paper, we introduce Self-Supervised Audio Mamba (SSAMBA), the\nfirst self-supervised, attention-free, and SSM-based model for audio\nrepresentation learning. SSAMBA leverages the bidirectional Mamba to capture\ncomplex audio patterns effectively. We incorporate a self-supervised\npretraining framework that optimizes both discriminative and generative\nobjectives, enabling the model to learn robust audio representations from\nlarge-scale, unlabeled datasets. We evaluated SSAMBA on various tasks such as\naudio classification, keyword spotting, and speaker identification. Our results\ndemonstrate that SSAMBA outperforms the Self-Supervised Audio Spectrogram\nTransformer (SSAST) in most tasks. Notably, SSAMBA is approximately 92.7%\nfaster in batch inference speed and 95.4% more memory-efficient than SSAST for\nthe tiny model size with an input token size of 22k. These efficiency gains,\ncombined with superior performance, underscore the effectiveness of SSAMBA's\narchitectural innovation, making it a compelling choice for a wide range of\naudio processing applications.", "published": "2024-05-20 06:58:47", "link": "http://arxiv.org/abs/2405.11831v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Neighborhood Attention Transformer with Progressive Channel Fusion for\n  Speaker Verification", "abstract": "Transformer-based architectures for speaker verification typically require\nmore training data than ECAPA-TDNN. Therefore, recent work has generally been\ntrained on VoxCeleb1&2. We propose a backbone network based on self-attention,\nwhich can achieve competitive results when trained on VoxCeleb2 alone. The\nnetwork alternates between neighborhood attention and global attention to\ncapture local and global features, then aggregates features of different\nhierarchical levels, and finally performs attentive statistics pooling.\nAdditionally, we employ a progressive channel fusion strategy to expand the\nreceptive field in the channel dimension as the network deepens. We trained the\nproposed PCF-NAT model on VoxCeleb2 and evaluated it on VoxCeleb1 and the\nvalidation sets of VoxSRC. The EER and minDCF of the shallow PCF-NAT are on\naverage more than 20% lower than those of similarly sized ECAPA-TDNN. Deep\nPCF-NAT achieves an EER lower than 0.5% on VoxCeleb1-O. The code and models are\npublicly available at https://github.com/ChenNan1996/PCF-NAT.", "published": "2024-05-20 13:55:19", "link": "http://arxiv.org/abs/2405.12031v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-speaker Text-to-speech Training with Speaker Anonymized Data", "abstract": "The trend of scaling up speech generation models poses a threat of biometric\ninformation leakage of the identities of the voices in the training data,\nraising privacy and security concerns. In this paper, we investigate training\nmulti-speaker text-to-speech (TTS) models using data that underwent speaker\nanonymization (SA), a process that tends to hide the speaker identity of the\ninput speech while maintaining other attributes. Two signal processing-based\nand three deep neural network-based SA methods were used to anonymize VCTK, a\nmulti-speaker TTS dataset, which is further used to train an end-to-end TTS\nmodel, VITS, to perform unseen speaker TTS during the testing phase. We\nconducted extensive objective and subjective experiments to evaluate the\nanonymized training data, as well as the performance of the downstream TTS\nmodel trained using those data. Importantly, we found that UTMOS, a data-driven\nsubjective rating predictor model, and GVD, a metric that measures the gain of\nvoice distinctiveness, are good indicators of the downstream TTS performance.\nWe summarize insights in the hope of helping future researchers determine the\ngoodness of the SA system for multi-speaker TTS training.", "published": "2024-05-20 03:55:44", "link": "http://arxiv.org/abs/2405.11767v1", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ecVoice: Audio Text Extraction and Optimization of Video Based on Idioms\n  Similarity Replacement", "abstract": "The Text Extraction of the Audio from the Video plays an important role in\nmultimedia editing and processing. As a popular open source toolkit, Whisper\nperforms fast in human voice recognition. However, the recognition performance\nis dependent on the computing resource, which makes the low computing memory\nrunning Whisper become difficult. Our paper presents an available solution to\nextract the human voice from the video and gain the high quality text\ngeneration from the voice. The generated voice can be used in video language\ntranslation and translated voice simulation. To improve the extraction and\ntransform quality of human voice, we present ecVoice, a method using the idioms\nsimilarity computation and analysis to improve the quality of audio text\nextraction. Relative experiments are held to verify that the ecVoice can\nimprove the idiom grammar correction rate to 90\\% on average. The method is\nsimple but fast which means this method will cause less bad influence of\nconsuming computing resources when improving the voice recognition rate. Our\nmethod and solution can significantly enhance the Whisper recognition with low\ncomputing memory.", "published": "2024-05-20 20:27:17", "link": "http://arxiv.org/abs/2407.09489v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Images that Sound: Composing Images and Sounds on a Single Canvas", "abstract": "Spectrograms are 2D representations of sound that look very different from\nthe images found in our visual world. And natural images, when played as\nspectrograms, make unnatural sounds. In this paper, we show that it is possible\nto synthesize spectrograms that simultaneously look like natural images and\nsound like natural audio. We call these visual spectrograms images that sound.\nOur approach is simple and zero-shot, and it leverages pre-trained\ntext-to-image and text-to-spectrogram diffusion models that operate in a shared\nlatent space. During the reverse process, we denoise noisy latents with both\nthe audio and image diffusion models in parallel, resulting in a sample that is\nlikely under both models. Through quantitative evaluations and perceptual\nstudies, we find that our method successfully generates spectrograms that align\nwith a desired audio prompt while also taking the visual appearance of a\ndesired image prompt. Please see our project page for video results:\nhttps://ificl.github.io/images-that-sound/", "published": "2024-05-20 17:59:59", "link": "http://arxiv.org/abs/2405.12221v3", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
