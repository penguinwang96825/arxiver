{"title": "AdaPrompt: Adaptive Model Training for Prompt-based NLP", "abstract": "Prompt-based learning, with its capability to tackle zero-shot and few-shot\nNLP tasks, has gained much attention in community. The main idea is to bridge\nthe gap between NLP downstream tasks and language modeling (LM), by mapping\nthese tasks into natural language prompts, which are then filled by pre-trained\nlanguage models (PLMs). However, for prompt learning, there are still two\nsalient gaps between NLP tasks and pretraining. First, prompt information is\nnot necessarily sufficiently present during LM pretraining. Second,\ntask-specific data are not necessarily well represented during pretraining. We\naddress these two issues by proposing AdaPrompt, adaptively retrieving external\ndata for continual pretraining of PLMs by making use of both task and prompt\ncharacteristics. In addition, we make use of knowledge in Natural Language\nInference models for deriving adaptive verbalizers. Experimental results on\nfive NLP benchmarks show that AdaPrompt can improve over standard PLMs in\nfew-shot settings. In addition, in zero-shot settings, our method outperforms\nstandard prompt-based methods by up to 26.35\\% relative error reduction.", "published": "2022-02-10 04:04:57", "link": "http://arxiv.org/abs/2202.04824v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InterHT: Knowledge Graph Embeddings by Interaction between Head and Tail\n  Entities", "abstract": "Knowledge graph embedding (KGE) models learn the representation of entities\nand relations in knowledge graphs. Distance-based methods show promising\nperformance on link prediction task, which predicts the result by the distance\nbetween two entity representations. However, most of these methods represent\nthe head entity and tail entity separately, which limits the model capacity. We\npropose two novel distance-based methods named InterHT and InterHT+ that allow\nthe head and tail entities to interact better and get better entity\nrepresentation. Experimental results show that our proposed method achieves the\nbest results on ogbl-wikikg2 dataset.", "published": "2022-02-10 08:40:09", "link": "http://arxiv.org/abs/2202.04897v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Slovene SuperGLUE Benchmark: Translation and Evaluation", "abstract": "We present a Slovene combined machine-human translated SuperGLUE benchmark.\nWe describe the translation process and problems arising due to differences in\nmorphology and grammar. We evaluate the translated datasets in several modes:\nmonolingual, cross-lingual, and multilingual, taking into account differences\nbetween machine and human translated training sets. The results show that the\nmonolingual Slovene SloBERTa model is superior to massively multilingual and\ntrilingual BERT models, but these also show a good cross-lingual performance on\ncertain tasks. The performance of Slovene models still lags behind the best\nEnglish models.", "published": "2022-02-10 12:46:06", "link": "http://arxiv.org/abs/2202.04994v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language in Requirements Engineering for Structure Inference --\n  An Integrative Review", "abstract": "The automatic extraction of structure from text can be difficult for\nmachines. Yet, the elicitation of this information can provide many benefits\nand opportunities for various applications. Benefits have also been identified\nfor the area of Requirements Engineering. To evaluate what work has been done\nand is currently available, the paper at hand provides an integrative review\nregarding Natural Language Processing (NLP) tools for Requirements Engineering.\nThis assessment was conducted to provide a foundation for future work as well\nas deduce insights from the stats quo. To conduct the review, the history of\nRequirements Engineering and NLP are described as well as an evaluation of over\n136 NLP tools. To assess these tools, a set of criteria was defined. The\nresults are that currently no open source approach exists that allows for the\ndirect/primary extraction of information structure and even closed source\nsolutions show limitations such as supervision or input limitations, which\neliminates the possibility for fully automatic and universal application. As a\nresults, the authors deduce that the current approaches are not applicable and\na different methodology is necessary. An approach that allows for individual\nmanagement of the algorithm, knowledge base, and text corpus is a possibility\nbeing pursued.", "published": "2022-02-10 14:46:09", "link": "http://arxiv.org/abs/2202.05065v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InPars: Data Augmentation for Information Retrieval using Large Language\n  Models", "abstract": "The information retrieval community has recently witnessed a revolution due\nto large pretrained transformer models. Another key ingredient for this\nrevolution was the MS MARCO dataset, whose scale and diversity has enabled\nzero-shot transfer learning to various tasks. However, not all IR tasks and\ndomains can benefit from one single dataset equally. Extensive research in\nvarious NLP tasks has shown that using domain-specific training data, as\nopposed to a general-purpose one, improves the performance of neural models. In\nthis work, we harness the few-shot capabilities of large pretrained language\nmodels as synthetic data generators for IR tasks. We show that models finetuned\nsolely on our unsupervised dataset outperform strong baselines such as BM25 as\nwell as recently proposed self-supervised dense retrieval methods. Furthermore,\nretrievers finetuned on both supervised and our synthetic data achieve better\nzero-shot transfer than models finetuned only on supervised data. Code, models,\nand data are available at https://github.com/zetaalphavector/inpars .", "published": "2022-02-10 16:52:45", "link": "http://arxiv.org/abs/2202.05144v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Weaknesses in Machine Translation Metrics Through Minimum\n  Bayes Risk Decoding: A Case Study for COMET", "abstract": "Neural metrics have achieved impressive correlation with human judgements in\nthe evaluation of machine translation systems, but before we can safely\noptimise towards such metrics, we should be aware of (and ideally eliminate)\nbiases toward bad translations that receive high scores. Our experiments show\nthat sample-based Minimum Bayes Risk decoding can be used to explore and\nquantify such weaknesses. When applying this strategy to COMET for en-de and\nde-en, we find that COMET models are not sensitive enough to discrepancies in\nnumbers and named entities. We further show that these biases are hard to fully\nremove by simply training on additional synthetic data and release our code and\ndata for facilitating further experiments.", "published": "2022-02-10 17:07:32", "link": "http://arxiv.org/abs/2202.05148v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "The Abduction of Sherlock Holmes: A Dataset for Visual Abductive\n  Reasoning", "abstract": "Humans have remarkable capacity to reason abductively and hypothesize about\nwhat lies beyond the literal content of an image. By identifying concrete\nvisual clues scattered throughout a scene, we almost can't help but draw\nprobable inferences beyond the literal scene based on our everyday experience\nand knowledge about the world. For example, if we see a \"20 mph\" sign alongside\na road, we might assume the street sits in a residential area (rather than on a\nhighway), even if no houses are pictured. Can machines perform similar visual\nreasoning?\n  We present Sherlock, an annotated corpus of 103K images for testing machine\ncapacity for abductive reasoning beyond literal image contents. We adopt a\nfree-viewing paradigm: participants first observe and identify salient clues\nwithin images (e.g., objects, actions) and then provide a plausible inference\nabout the scene, given the clue. In total, we collect 363K (clue, inference)\npairs, which form a first-of-its-kind abductive visual reasoning dataset. Using\nour corpus, we test three complementary axes of abductive reasoning. We\nevaluate the capacity of models to: i) retrieve relevant inferences from a\nlarge candidate corpus; ii) localize evidence for inferences via bounding\nboxes, and iii) compare plausible inferences to match human judgments on a\nnewly-collected diagnostic corpus of 19K Likert-scale judgments. While we find\nthat fine-tuning CLIP-RN50x64 with a multitask objective outperforms strong\nbaselines, significant headroom exists between model performance and human\nagreement. Data, models, and leaderboard available at\nhttp://visualabduction.com/", "published": "2022-02-10 02:26:45", "link": "http://arxiv.org/abs/2202.04800v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Distilling Hypernymy Relations from Language Models: On the\n  Effectiveness of Zero-Shot Taxonomy Induction", "abstract": "In this paper, we analyze zero-shot taxonomy learning methods which are based\non distilling knowledge from language models via prompting and sentence\nscoring. We show that, despite their simplicity, these methods outperform some\nsupervised strategies and are competitive with the current state-of-the-art\nunder adequate conditions. We also show that statistical and linguistic\nproperties of prompts dictate downstream performance.", "published": "2022-02-10 07:29:05", "link": "http://arxiv.org/abs/2202.04876v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Locating and Editing Factual Associations in GPT", "abstract": "We analyze the storage and recall of factual associations in autoregressive\ntransformer language models, finding evidence that these associations\ncorrespond to localized, directly-editable computations. We first develop a\ncausal intervention for identifying neuron activations that are decisive in a\nmodel's factual predictions. This reveals a distinct set of steps in\nmiddle-layer feed-forward modules that mediate factual predictions while\nprocessing subject tokens. To test our hypothesis that these computations\ncorrespond to factual association recall, we modify feed-forward weights to\nupdate specific factual associations using Rank-One Model Editing (ROME). We\nfind that ROME is effective on a standard zero-shot relation extraction (zsRE)\nmodel-editing task, comparable to existing methods. To perform a more sensitive\nevaluation, we also evaluate ROME on a new dataset of counterfactual\nassertions, on which it simultaneously maintains both specificity and\ngeneralization, whereas other methods sacrifice one or another. Our results\nconfirm an important role for mid-layer feed-forward modules in storing factual\nassociations and suggest that direct manipulation of computational mechanisms\nmay be a feasible approach for model editing. The code, dataset,\nvisualizations, and an interactive demo notebook are available at\nhttps://rome.baulab.info/", "published": "2022-02-10 18:59:54", "link": "http://arxiv.org/abs/2202.05262v5", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Networks and Identity Drive Geographic Properties of the Diffusion of\n  Linguistic Innovation", "abstract": "Adoption of cultural innovation (e.g., music, beliefs, language) is often\ngeographically correlated, with adopters largely residing within the boundaries\nof relatively few well-studied, socially significant areas. These cultural\nregions are often hypothesized to be the result of either (i) identity\nperformance driving the adoption of cultural innovation, or (ii) homophily in\nthe networks underlying diffusion. In this study, we show that demographic\nidentity and network topology are both required to model the diffusion of\ninnovation, as they play complementary roles in producing its spatial\nproperties. We develop an agent-based model of cultural adoption, and validate\ngeographic patterns of transmission in our model against a novel dataset of\ninnovative words that we identify from a 10% sample of Twitter. Using our\nmodel, we are able to directly compare a combined network + identity model of\ndiffusion to simulated network-only and identity-only counterfactuals --\nallowing us to test the separate and combined roles of network and identity.\nWhile social scientists often treat either network or identity as the core\nsocial structure in modeling culture change, we show that key geographic\nproperties of diffusion actually depend on both factors as each one influences\ndifferent mechanisms of diffusion. Specifically, the network principally drives\nspread among urban counties via weak-tie diffusion, while identity plays a\ndisproportionate role in transmission among rural counties via strong-tie\ndiffusion. Diffusion between urban and rural areas, a key component in\ninnovation diffusing nationally, requires both network and identity. Our work\nsuggests that models must integrate both factors in order to understand and\nreproduce the adoption of innovation.", "published": "2022-02-10 05:17:15", "link": "http://arxiv.org/abs/2202.04842v1", "categories": ["cs.SI", "cs.CL", "cs.CY", "physics.soc-ph", "J.4; I.6.3; K.4"], "primary_category": "cs.SI"}
{"title": "A Survey on Artificial Intelligence for Source Code: A Dialogue Systems\n  Perspective", "abstract": "In this survey paper, we overview major deep learning methods used in Natural\nLanguage Processing (NLP) and source code over the last 35 years. Next, we\npresent a survey of the applications of Artificial Intelligence (AI) for source\ncode, also known as Code Intelligence (CI) and Programming Language Processing\n(PLP). We survey over 287 publications and present a software-engineering\ncentered taxonomy for CI placing each of the works into one category describing\nhow it best assists the software development cycle. Then, we overview the field\nof conversational assistants and their applications in software engineering and\neducation. Lastly, we highlight research opportunities at the intersection of\nAI for code and conversational assistants and provide future directions for\nresearching conversational assistants with CI capabilities.", "published": "2022-02-10 05:40:51", "link": "http://arxiv.org/abs/2202.04847v1", "categories": ["cs.CL", "cs.CY", "cs.LG", "cs.SE", "I.2.2; I.2.7; K.3.1"], "primary_category": "cs.CL"}
{"title": "The USTC-Ximalaya system for the ICASSP 2022 multi-channel multi-party\n  meeting transcription (M2MeT) challenge", "abstract": "We propose two improvements to target-speaker voice activity detection\n(TS-VAD), the core component in our proposed speaker diarization system that\nwas submitted to the 2022 Multi-Channel Multi-Party Meeting Transcription\n(M2MeT) challenge. These techniques are designed to handle multi-speaker\nconversations in real-world meeting scenarios with high speaker-overlap ratios\nand under heavy reverberant and noisy condition. First, for data preparation\nand augmentation in training TS-VAD models, speech data containing both real\nmeetings and simulated indoor conversations are used. Second, in refining\nresults obtained after TS-VAD based decoding, we perform a series of\npost-processing steps to improve the VAD results needed to reduce diarization\nerror rates (DERs). Tested on the ALIMEETING corpus, the newly released\nMandarin meeting dataset used in M2MeT, we demonstrate that our proposed system\ncan decrease the DER by up to 66.55/60.59% relatively when compared with\nclassical clustering based diarization on the Eval/Test set.", "published": "2022-02-10 06:06:48", "link": "http://arxiv.org/abs/2202.04855v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "TaxoEnrich: Self-Supervised Taxonomy Completion via Structure-Semantic\n  Representations", "abstract": "Taxonomies are fundamental to many real-world applications in various\ndomains, serving as structural representations of knowledge. To deal with the\nincreasing volume of new concepts needed to be organized as taxonomies,\nresearchers turn to automatically completion of an existing taxonomy with new\nconcepts. In this paper, we propose TaxoEnrich, a new taxonomy completion\nframework, which effectively leverages both semantic features and structural\ninformation in the existing taxonomy and offers a better representation of\ncandidate position to boost the performance of taxonomy completion.\nSpecifically, TaxoEnrich consists of four components: (1)\ntaxonomy-contextualized embedding which incorporates both semantic meanings of\nconcept and taxonomic relations based on powerful pretrained language models;\n(2) a taxonomy-aware sequential encoder which learns candidate position\nrepresentations by encoding the structural information of taxonomy; (3) a\nquery-aware sibling encoder which adaptively aggregates candidate siblings to\naugment candidate position representations based on their importance to the\nquery-position matching; (4) a query-position matching model which extends\nexisting work with our new candidate position representations. Extensive\nexperiments on four large real-world datasets from different domains show that\n\\TaxoEnrich achieves the best performance among all evaluation metrics and\noutperforms previous state-of-the-art methods by a large margin.", "published": "2022-02-10 08:10:43", "link": "http://arxiv.org/abs/2202.04887v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Cross-speaker style transfer for text-to-speech using data augmentation", "abstract": "We address the problem of cross-speaker style transfer for text-to-speech\n(TTS) using data augmentation via voice conversion. We assume to have a corpus\nof neutral non-expressive data from a target speaker and supporting\nconversational expressive data from different speakers. Our goal is to build a\nTTS system that is expressive, while retaining the target speaker's identity.\nThe proposed approach relies on voice conversion to first generate high-quality\ndata from the set of supporting expressive speakers. The voice converted data\nis then pooled with natural data from the target speaker and used to train a\nsingle-speaker multi-style TTS system. We provide evidence that this approach\nis efficient, flexible, and scalable. The method is evaluated using one or more\nsupporting speakers, as well as a variable amount of supporting data. We\nfurther provide evidence that this approach allows some controllability of\nspeaking style, when using multiple supporting speakers. We conclude by scaling\nour proposed technology to a set of 14 speakers across 7 languages. Results\nindicate that our technology consistently improves synthetic samples in terms\nof style similarity, while retaining the target speaker's identity.", "published": "2022-02-10 15:10:56", "link": "http://arxiv.org/abs/2202.05083v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Automatic Speech Recognition for Non-Native English with\n  Transfer Learning and Language Model Decoding", "abstract": "ASR systems designed for native English (L1) usually underperform on\nnon-native English (L2). To address this performance gap, \\textbf{(i)} we\nextend our previous work to investigate fine-tuning of a pre-trained wav2vec\n2.0 model \\cite{baevski2020wav2vec,xu2021self} under a rich set of L1 and L2\ntraining conditions. We further \\textbf{(ii)} incorporate language model\ndecoding in the ASR system, along with the fine-tuning method. Quantifying\ngains acquired from each of these two approaches separately and an error\nanalysis allows us to identify different sources of improvement within our\nmodels. We find that while the large self-trained wav2vec 2.0 may be\ninternalizing sufficient decoding knowledge for clean L1 speech\n\\cite{xu2021self}, this does not hold for L2 speech and accounts for the\nutility of employing language model decoding on L2 data.", "published": "2022-02-10 18:13:32", "link": "http://arxiv.org/abs/2202.05209v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Describing image focused in cognitive and visual details for visually\n  impaired people: An approach to generating inclusive paragraphs", "abstract": "Several services for people with visual disabilities have emerged recently\ndue to achievements in Assistive Technologies and Artificial Intelligence\nareas. Despite the growth in assistive systems availability, there is a lack of\nservices that support specific tasks, such as understanding the image context\npresented in online content, e.g., webinars. Image captioning techniques and\ntheir variants are limited as Assistive Technologies as they do not match the\nneeds of visually impaired people when generating specific descriptions. We\npropose an approach for generating context of webinar images combining a dense\ncaptioning technique with a set of filters, to fit the captions in our domain,\nand a language model for the abstractive summary task. The results demonstrated\nthat we can produce descriptions with higher interpretability and focused on\nthe relevant information for that group of people by combining image analysis\nmethods and neural language models.", "published": "2022-02-10 21:20:53", "link": "http://arxiv.org/abs/2202.05331v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Spatial active noise control based on individual kernel interpolation of\n  primary and secondary sound fields", "abstract": "A spatial active noise control (ANC) method based on the individual kernel\ninterpolation of primary and secondary sound fields is proposed. Spatial ANC is\naimed at cancelling unwanted primary noise within a continuous region by using\nmultiple secondary sources and microphones. A method based on the kernel\ninterpolation of a sound field makes it possible to attenuate noise over the\ntarget region with flexible array geometry. Furthermore, by using the kernel\nfunction with directional weighting, prior information on primary noise source\ndirections can be taken into consideration. However, whereas the sound field to\nbe interpolated is a superposition of primary and secondary sound fields, the\ndirectional weight for the primary noise source was applied to the total sound\nfield in previous work; therefore, the performance improvement was limited. We\npropose a method of individually interpolating the primary and secondary sound\nfields and formulate a normalized least-mean-square algorithm based on this\ninterpolation method. Experimental results indicate that the proposed method\noutperforms the method based on total kernel interpolation.", "published": "2022-02-10 02:54:17", "link": "http://arxiv.org/abs/2202.04807v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Auditory Model based Phase-Aware Bayesian Spectral Amplitude Estimator\n  for Single-Channel Speech Enhancement", "abstract": "Bayesian estimation of short-time spectral amplitude is one of the most\npredominant approaches for the enhancement of the noise corrupted speech. The\nperformance of these estimators are usually significantly improved when any\nperceptually relevant cost function is considered. On the other hand, the\nrecent progress in the phase-based speech signal processing have shown that the\nphase-only enhancement based on spectral phase estimation methods can also\nprovide joint improvement in the perceived speech quality and intelligibility,\neven in low SNR conditions. In this paper, to take advantage of both the\nperceptually motivated cost function involving STSAs of estimated and true\nclean speech and utilizing the prior spectral phase information, we have\nderived a phase-aware Bayesian STSA estimator. The parameters of the cost\nfunction are chosen based on the characteristics of the human auditory system,\nnamely, the dynamic compressive nonlinearity of the cochlea, the perceived\nloudness theory and the simultaneous masking properties of the ear. This type\nof parameter selection scheme results in more noise reduction while limiting\nthe speech distortion. The derived STSA estimator is optimal in the MMSE sense\nif the prior phase information is available. In practice, however, typically\nonly an estimate of the clean speech phase can be obtained via employing\ndifferent types of spectral phase estimation techniques which have been\ndeveloped throughout the last few years. In a blind setup, we have evaluated\nthe proposed Bayesian STSA estimator with different types of standard phase\nestimation methods available in the literature. Experimental results have shown\nthat the proposed estimator can achieve substantial improvement in performance\nthan the traditional phase-blind approaches.", "published": "2022-02-10 07:44:19", "link": "http://arxiv.org/abs/2202.04882v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sound masking degrades perception of self-location during stepping: A\n  case for sound-transparent spacesuits for Mars", "abstract": "Most efforts to improve spacesuits have been directed towards adding haptic\nfeedback. However, sound transparency can also improve situational awareness at\na relatively low cost. The extent of the improvement is unknown. We use the\nFukuda-Unterberger stepping test to measure the accuracy of one's perception of\nself-location. We compare accuracy outcomes in two scenarios: one where hearing\nis impaired with sound masking with white noise and one where it is not. These\nscenarios are acoustic proxies for a sound muffling space suit and a sound\ntransparent space suit respectively. The results show that when sound masking\nis applied, the error in self-location increases by 14.5cm, 95% CI [4.04\n28.22]. Suggestions to apply the findings to Mars spacesuit designs are\ndiscussed. A cost-benefit analysis is also provided.", "published": "2022-02-10 11:23:22", "link": "http://arxiv.org/abs/2202.04958v1", "categories": ["cs.SD", "eess.AS", "J.2"], "primary_category": "cs.SD"}
{"title": "A Probabilistic Fusion Framework for Spoofing Aware Speaker Verification", "abstract": "The performance of automatic speaker verification (ASV) systems could be\ndegraded by voice spoofing attacks. Most existing works aimed to develop\nstandalone spoofing countermeasure (CM) systems. Relatively little work\ntargeted at developing an integrated spoofing aware speaker verification (SASV)\nsystem. In the recent SASV challenge, the organizers encourage the development\nof such integration by releasing official protocols and baselines. In this\npaper, we build a probabilistic framework for fusing the ASV and CM subsystem\nscores. We further propose fusion strategies for direct inference and\nfine-tuning to predict the SASV score based on the framework. Surprisingly,\nthese strategies significantly improve the SASV equal error rate (EER) from\n19.31% of the baseline to 1.53% on the official evaluation trials of the SASV\nchallenge. We verify the effectiveness of our proposed components through\nablation studies and provide insights with score distribution analysis.", "published": "2022-02-10 18:57:08", "link": "http://arxiv.org/abs/2202.05253v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Single-channel speech enhancement by using psychoacoustical model\n  inspired fusion framework", "abstract": "When the parameters of Bayesian Short-time Spectral Amplitude (STSA)\nestimator for speech enhancement are selected based on the characteristics of\nthe human auditory system, the gain function of the estimator becomes more\nflexible. Although this type of estimator in acoustic domain is quite effective\nin reducing the back-ground noise at high frequencies, it produces more speech\ndistortions, which make the high-frequency contents of the speech such as\nfriciatives less perceptible in heavy noise conditions, resulting in\nintelligibility reduction. On the other hand, the speech enhancement scheme,\nwhich exploits the psychoacoustic evidence of frequency selectivity in the\nmodulation domain, is found to be able to increase the intelligibility of noisy\nspeech by a substantial amount, but also suffers from the temporal slurring\nproblem due to its essential design constraint. In order to achieve the joint\nimprovements in both the perceived speech quality and intelligibility, we\nproposed and investigated a fusion framework by combining the merits of\nacoustic and modulation domain approaches while avoiding their respective\nweaknesses. Objective measure evaluation shows that the proposed speech\nenhancement fusion framework can provide consistent improvements in the\nperceived speech quality and intelligibility across different SNR levels in\nvarious noise conditions, while compared to the other baseline techniques.", "published": "2022-02-10 13:13:54", "link": "http://arxiv.org/abs/2202.05272v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Initial Description of Capabilities and Constraints for a\n  Computational Auditory System (an Artificial Ear) for Cognitive Architectures", "abstract": "We present an initial set of factors, features, and constraints for\ndeveloping a Computational Auditory System (CAS, aka less formally an\nartificial ear, AE) for use by cognitive architectures. We start to define a\nCAS and what tasks it should be able to perform. We then outline the features\nof a CAS for use by a cognitive architecture and factors that influence its\nperformance. We conclude with an update on what has been created so far and\ninsights on how to create and use a CAS in a cognitive architecture and include\na set of functionalities for an artificial ear.", "published": "2022-02-10 21:23:17", "link": "http://arxiv.org/abs/2202.05332v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-Based Deep Learning Frameworks for Detecting COVID-19", "abstract": "This paper evaluates a wide range of audio-based deep learning frameworks\napplied to the breathing, cough, and speech sounds for detecting COVID-19. In\ngeneral, the audio recording inputs are transformed into low-level spectrogram\nfeatures, then they are fed into pre-trained deep learning models to extract\nhigh-level embedding features. Next, the dimension of these high-level\nembedding features are reduced before finetuning using Light Gradient Boosting\nMachine (LightGBM) as a back-end classification. Our experiments on the Second\nDiCOVA Challenge achieved the highest Area Under the Curve (AUC), F1 score,\nsensitivity score, and specificity score of 89.03%, 64.41%, 63.33%, and 95.13%,\nrespectively. Based on these scores, our method outperforms the\nstate-of-the-art systems, and improves the challenge baseline by 4.33%, 6.00%\nand 8.33% in terms of AUC, F1 score and sensitivity score, respectively.", "published": "2022-02-10 09:28:13", "link": "http://arxiv.org/abs/2202.05626v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Royalflush Speaker Diarization System for ICASSP 2022 Multi-channel\n  Multi-party Meeting Transcription Challenge", "abstract": "This paper describes the Royalflush speaker diarization system submitted to\nthe Multi-channel Multi-party Meeting Transcription Challenge(M2MeT). Our\nsystem comprises speech enhancement, overlapped speech detection, speaker\nembedding extraction, speaker clustering, speech separation and system fusion.\nIn this system, we made three contributions. First, we propose an architecture\nof combining the multi-channel and U-Net-based models, aiming at utilizing the\nbenefits of these two individual architectures, for far-field overlapped speech\ndetection. Second, in order to use overlapped speech detection model to help\nspeaker diarization, a speech separation based overlapped speech handling\napproach, in which the speaker verification technique is further applied, is\nproposed. Third, we explore three speaker embedding methods, and obtained the\nstate-of-the-art performance on the CNCeleb-E test set. With these proposals,\nour best individual system significantly reduces DER from 15.25% to 6.40%, and\nthe fusion of four systems finally achieves a DER of 6.30% on the far-field\nAlimeeting evaluation set.", "published": "2022-02-10 03:35:05", "link": "http://arxiv.org/abs/2202.04814v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "OWL (Observe, Watch, Listen): Audiovisual Temporal Context for\n  Localizing Actions in Egocentric Videos", "abstract": "Egocentric videos capture sequences of human activities from a first-person\nperspective and can provide rich multimodal signals. However, most current\nlocalization methods use third-person videos and only incorporate visual\ninformation. In this work, we take a deep look into the effectiveness of\naudiovisual context in detecting actions in egocentric videos and introduce a\nsimple-yet-effective approach via Observing, Watching, and Listening (OWL). OWL\nleverages audiovisual information and context for egocentric temporal action\nlocalization (TAL). We validate our approach in two large-scale datasets,\nEPIC-Kitchens, and HOMAGE. Extensive experiments demonstrate the relevance of\nthe audiovisual temporal context. Namely, we boost the localization performance\n(mAP) over visual-only models by +2.23% and +3.35% in the above datasets.", "published": "2022-02-10 10:50:52", "link": "http://arxiv.org/abs/2202.04947v3", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "ASRPU: A Programmable Accelerator for Low-Power Automatic Speech\n  Recognition", "abstract": "The outstanding accuracy achieved by modern Automatic Speech Recognition\n(ASR) systems is enabling them to quickly become a mainstream technology. ASR\nis essential for many applications, such as speech-based assistants, dictation\nsystems and real-time language translation. However, highly accurate ASR\nsystems are computationally expensive, requiring on the order of billions of\narithmetic operations to decode each second of audio, which conflicts with a\ngrowing interest in deploying ASR on edge devices. On these devices, hardware\nacceleration is key for achieving acceptable performance. However, ASR is a\nrich and fast-changing field, and thus, any overly specialized hardware\naccelerator may quickly become obsolete.\n  In this paper, we tackle those challenges by proposing ASRPU, a programmable\naccelerator for on-edge ASR. ASRPU contains a pool of general-purpose cores\nthat execute small pieces of parallel code. Each of these programs computes one\npart of the overall decoder (e.g. a layer in a neural network). The accelerator\nautomates some carefully chosen parts of the decoder to simplify the\nprogramming without sacrificing generality. We provide an analysis of a modern\nASR system implemented on ASRPU and show that this architecture can achieve\nreal-time decoding with a very low power budget.", "published": "2022-02-10 12:03:00", "link": "http://arxiv.org/abs/2202.04971v1", "categories": ["cs.AR", "cs.SD", "eess.AS"], "primary_category": "cs.AR"}
{"title": "Barwise Compression Schemes for Audio-Based Music Structure Analysis", "abstract": "Music Structure Analysis (MSA) consists in segmenting a music piece in\nseveral distinct sections. We approach MSA within a compression framework,\nunder the hypothesis that the structure is more easily revealed by a simplified\nrepresentation of the original content of the song. More specifically, under\nthe hypothesis that MSA is correlated with similarities occurring at the bar\nscale, this article introduces the use of linear and non-linear compression\nschemes on barwise audio signals. Compressed representations capture the most\nsalient components of the different bars in the song and are then used to infer\nthe song structure using a dynamic programming algorithm. This work explores\nboth low-rank approximation models such as Principal Component Analysis or\nNonnegative Matrix Factorization and \"piece-specific\" Auto-Encoding Neural\nNetworks, with the objective to learn latent representations specific to a\ngiven song. Such approaches do not rely on supervision nor annotations, which\nare well-known to be tedious to collect and possibly ambiguous in MSA\ndescription. In our experiments, several unsupervised compression schemes\nachieve a level of performance comparable to that of state-of-the-art\nsupervised methods (for 3s tolerance) on the RWC-Pop dataset, showcasing the\nimportance of the barwise compression processing for MSA.", "published": "2022-02-10 12:23:57", "link": "http://arxiv.org/abs/2202.04981v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "H.5.5"], "primary_category": "cs.SD"}
{"title": "Semi-Supervised Convolutive NMF for Automatic Piano Transcription", "abstract": "Automatic Music Transcription, which consists in transforming an audio\nrecording of a musical performance into symbolic format, remains a difficult\nMusic Information Retrieval task. In this work, which focuses on piano\ntranscription, we propose a semi-supervised approach using low-rank matrix\nfactorization techniques, in particular Convolutive Nonnegative Matrix\nFactorization. In the semi-supervised setting, only a single recording of each\nindividual notes is required. We show on the MAPS dataset that the proposed\nsemi-supervised CNMF method performs better than state-of-the-art low-rank\nfactorization techniques and a little worse than supervised deep learning\nstate-of-the-art methods, while however suffering from generalization issues.", "published": "2022-02-10 12:38:53", "link": "http://arxiv.org/abs/2202.04989v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "H.5.5"], "primary_category": "cs.SD"}
{"title": "Learnable Nonlinear Compression for Robust Speaker Verification", "abstract": "In this study, we focus on nonlinear compression methods in spectral features\nfor speaker verification based on deep neural network. We consider different\nkinds of channel-dependent (CD) nonlinear compression methods optimized in a\ndata-driven manner. Our methods are based on power nonlinearities and dynamic\nrange compression (DRC). We also propose multi-regime (MR) design on the\nnonlinearities, at improving robustness. Results on VoxCeleb1 and VoxMovies\ndata demonstrate improvements brought by proposed compression methods over both\nthe commonly-used logarithm and their static counterparts, especially for ones\nbased on power function. While CD generalization improves performance on\nVoxCeleb1, MR provides more robustness on VoxMovies, with a maximum relative\nequal error rate reduction of 21.6%.", "published": "2022-02-10 18:44:53", "link": "http://arxiv.org/abs/2202.05236v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Conditional Diffusion Probabilistic Model for Speech Enhancement", "abstract": "Speech enhancement is a critical component of many user-oriented audio\napplications, yet current systems still suffer from distorted and unnatural\noutputs. While generative models have shown strong potential in speech\nsynthesis, they are still lagging behind in speech enhancement. This work\nleverages recent advances in diffusion probabilistic models, and proposes a\nnovel speech enhancement algorithm that incorporates characteristics of the\nobserved noisy speech signal into the diffusion and reverse processes. More\nspecifically, we propose a generalized formulation of the diffusion\nprobabilistic model named conditional diffusion probabilistic model that, in\nits reverse process, can adapt to non-Gaussian real noises in the estimated\nspeech signal. In our experiments, we demonstrate strong performance of the\nproposed approach compared to representative generative models, and investigate\nthe generalization capability of our models to other datasets with noise\ncharacteristics unseen during training.", "published": "2022-02-10 18:58:01", "link": "http://arxiv.org/abs/2202.05256v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
