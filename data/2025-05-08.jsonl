{"title": "StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant", "abstract": "We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks.", "published": "2025-05-08 17:57:40", "link": "http://arxiv.org/abs/2505.05467v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ComPO: Preference Alignment via Comparison Oracles", "abstract": "Direct alignment methods are increasingly used for aligning large language\nmodels (LLMs) with human preferences. However, these methods suffer from the\nissues of verbosity and likelihood displacement, which can be driven by the\nnoisy preference pairs that induce similar likelihood for preferred and\ndispreferred responses. The contributions of this paper are two-fold. First, we\npropose a new preference alignment method based on comparison oracles and\nprovide the convergence guarantee for its basic scheme. Second, we improve our\nmethod using some heuristics and conduct the experiments to demonstrate the\nflexibility and compatibility of practical scheme in improving the performance\nof LLMs using noisy preference pairs. Evaluations are conducted across multiple\nbase and instruction-tuned models (Mistral-7B, Llama-3-8B and Gemma-2-9B) with\nbenchmarks (AlpacaEval 2, MT-Bench and Arena-Hard). Experimental results show\nthe effectiveness of our method as an alternative to addressing the limitations\nof existing direct alignment methods. A highlight of our work is that we\nevidence the importance of designing specialized methods for preference pairs\nwith distinct likelihood margin, which complements the recent findings in\n\\citet{Razin-2025-Unintentional}.", "published": "2025-05-08 17:56:57", "link": "http://arxiv.org/abs/2505.05465v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging", "abstract": "Vision-Language Models (VLMs) combine visual perception with the general\ncapabilities, such as reasoning, of Large Language Models (LLMs). However, the\nmechanisms by which these two abilities can be combined and contribute remain\npoorly understood. In this work, we explore to compose perception and reasoning\nthrough model merging that connects parameters of different models. Unlike\nprevious works that often focus on merging models of the same kind, we propose\nmerging models across modalities, enabling the incorporation of the reasoning\ncapabilities of LLMs into VLMs. Through extensive experiments, we demonstrate\nthat model merging offers a successful pathway to transfer reasoning abilities\nfrom LLMs to VLMs in a training-free manner. Moreover, we utilize the merged\nmodels to understand the internal mechanism of perception and reasoning and how\nmerging affects it. We find that perception capabilities are predominantly\nencoded in the early layers of the model, whereas reasoning is largely\nfacilitated by the middle-to-late layers. After merging, we observe that all\nlayers begin to contribute to reasoning, whereas the distribution of perception\nabilities across layers remains largely unchanged. These observations shed\nlight on the potential of model merging as a tool for multimodal integration\nand interpretation.", "published": "2025-05-08 17:56:23", "link": "http://arxiv.org/abs/2505.05464v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UKElectionNarratives: A Dataset of Misleading Narratives Surrounding Recent UK General Elections", "abstract": "Misleading narratives play a crucial role in shaping public opinion during\nelections, as they can influence how voters perceive candidates and political\nparties. This entails the need to detect these narratives accurately. To\naddress this, we introduce the first taxonomy of common misleading narratives\nthat circulated during recent elections in Europe. Based on this taxonomy, we\nconstruct and analyse UKElectionNarratives: the first dataset of\nhuman-annotated misleading narratives which circulated during the UK General\nElections in 2019 and 2024. We also benchmark Pre-trained and Large Language\nModels (focusing on GPT-4o), studying their effectiveness in detecting\nelection-related misleading narratives. Finally, we discuss potential use cases\nand make recommendations for future research directions using the proposed\ncodebook and dataset.", "published": "2025-05-08 17:51:20", "link": "http://arxiv.org/abs/2505.05459v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Adaptive Markup Language Generation for Contextually-Grounded Visual Document Understanding", "abstract": "Visual Document Understanding has become essential with the increase of\ntext-rich visual content. This field poses significant challenges due to the\nneed for effective integration of visual perception and textual comprehension,\nparticularly across diverse document types with complex layouts. Moreover,\nexisting fine-tuning datasets for this domain often fall short in providing the\ndetailed contextual information for robust understanding, leading to\nhallucinations and limited comprehension of spatial relationships among visual\nelements. To address these challenges, we propose an innovative pipeline that\nutilizes adaptive generation of markup languages, such as Markdown, JSON, HTML,\nand TiKZ, to build highly structured document representations and deliver\ncontextually-grounded responses. We introduce two fine-grained structured\ndatasets: DocMark-Pile, comprising approximately 3.8M pretraining data pairs\nfor document parsing, and DocMark-Instruct, featuring 624k fine-tuning data\nannotations for grounded instruction following. Extensive experiments\ndemonstrate that our proposed model significantly outperforms existing\nstate-of-theart MLLMs across a range of visual document understanding\nbenchmarks, facilitating advanced reasoning and comprehension capabilities in\ncomplex visual scenarios. Our code and models are released at https://github.\ncom/Euphoria16/DocMark.", "published": "2025-05-08 17:37:36", "link": "http://arxiv.org/abs/2505.05446v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "clem:todd: A Framework for the Systematic Benchmarking of LLM-Based Task-Oriented Dialogue System Realisations", "abstract": "The emergence of instruction-tuned large language models (LLMs) has advanced\nthe field of dialogue systems, enabling both realistic user simulations and\nrobust multi-turn conversational agents. However, existing research often\nevaluates these components in isolation-either focusing on a single user\nsimulator or a specific system design-limiting the generalisability of insights\nacross architectures and configurations. In this work, we propose clem todd\n(chat-optimized LLMs for task-oriented dialogue systems development), a\nflexible framework for systematically evaluating dialogue systems under\nconsistent conditions. clem todd enables detailed benchmarking across\ncombinations of user simulators and dialogue systems, whether existing models\nfrom literature or newly developed ones. It supports plug-and-play integration\nand ensures uniform datasets, evaluation metrics, and computational\nconstraints. We showcase clem todd's flexibility by re-evaluating existing\ntask-oriented dialogue systems within this unified setup and integrating three\nnewly proposed dialogue systems into the same evaluation pipeline. Our results\nprovide actionable insights into how architecture, scale, and prompting\nstrategies affect dialogue performance, offering practical guidance for\nbuilding efficient and effective conversational AI systems.", "published": "2025-05-08 17:36:36", "link": "http://arxiv.org/abs/2505.05445v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data", "abstract": "Data quality has become a key factor in enhancing model performance with the\nrapid development of large language models (LLMs). Model-driven data filtering\nhas increasingly become a primary approach for acquiring high-quality data.\nHowever, it still faces two main challenges: (1) the lack of an efficient data\nverification strategy makes it difficult to provide timely feedback on data\nquality; and (2) the selection of seed data for training classifiers lacks\nclear criteria and relies heavily on human expertise, introducing a degree of\nsubjectivity. To address the first challenge, we introduce an efficient\nverification strategy that enables rapid evaluation of the impact of data on\nLLM training with minimal computational cost. To tackle the second challenge,\nwe build upon the assumption that high-quality seed data is beneficial for LLM\ntraining, and by integrating the proposed verification strategy, we optimize\nthe selection of positive and negative samples and propose an efficient data\nfiltering pipeline. This pipeline not only improves filtering efficiency,\nclassifier quality, and robustness, but also significantly reduces experimental\nand inference costs. In addition, to efficiently filter high-quality data, we\nemploy a lightweight classifier based on fastText, and successfully apply the\nfiltering pipeline to two widely-used pre-training corpora, FineWeb and Chinese\nFineWeb datasets, resulting in the creation of the higher-quality Ultra-FineWeb\ndataset. Ultra-FineWeb contains approximately 1 trillion English tokens and 120\nbillion Chinese tokens. Empirical results demonstrate that the LLMs trained on\nUltra-FineWeb exhibit significant performance improvements across multiple\nbenchmark tasks, validating the effectiveness of our pipeline in enhancing both\ndata quality and training efficiency.", "published": "2025-05-08 17:15:20", "link": "http://arxiv.org/abs/2505.05427v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TransProQA: an LLM-based literary Translation evaluation metric with Professional Question Answering", "abstract": "The impact of Large Language Models (LLMs) has extended into literary\ndomains. However, existing evaluation metrics prioritize mechanical accuracy\nover artistic expression and tend to overrate machine translation (MT) as being\nsuperior to experienced professional human translation. In the long run, this\nbias could result in a permanent decline in translation quality and cultural\nauthenticity. In response to the urgent need for a specialized literary\nevaluation metric, we introduce TransProQA, a novel, reference-free, LLM-based\nquestion-answering (QA) framework designed specifically for literary\ntranslation evaluation. TransProQA uniquely integrates insights from\nprofessional literary translators and researchers, focusing on critical\nelements in literary quality assessment such as literary devices, cultural\nunderstanding, and authorial voice. Our extensive evaluation shows that while\nliterary-finetuned XCOMET-XL yields marginal gains, TransProQA substantially\noutperforms current metrics, achieving up to 0.07 gain in correlation (ACC-EQ\nand Kendall's tau) and surpassing the best state-of-the-art (SOTA) metrics by\nover 15 points in adequacy assessments. Incorporating professional translator\ninsights as weights further improves performance, highlighting the value of\ntranslator inputs. Notably, TransProQA approaches human-level evaluation\nperformance comparable to trained linguistic annotators. It demonstrates broad\napplicability to open-source models such as LLaMA3.3-70b and Qwen2.5-32b,\nindicating its potential as an accessible and training-free literary evaluation\nmetric and a valuable tool for evaluating texts that require local processing\ndue to copyright or ethical considerations.", "published": "2025-05-08 17:12:56", "link": "http://arxiv.org/abs/2505.05423v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation", "abstract": "Pioneering token-based works such as Chameleon and Emu3 have established a\nfoundation for multimodal unification but face challenges of high training\ncomputational overhead and limited comprehension performance due to a lack of\nhigh-level semantics. In this paper, we introduce TokLIP, a visual tokenizer\nthat enhances comprehension by semanticizing vector-quantized (VQ) tokens and\nincorporating CLIP-level semantics while enabling end-to-end multimodal\nautoregressive training with standard VQ tokens. TokLIP integrates a low-level\ndiscrete VQ tokenizer with a ViT-based token encoder to capture high-level\ncontinuous semantics. Unlike previous approaches (e.g., VILA-U) that discretize\nhigh-level features, TokLIP disentangles training objectives for comprehension\nand generation, allowing the direct application of advanced VQ tokenizers\nwithout the need for tailored quantization operations. Our empirical results\ndemonstrate that TokLIP achieves exceptional data efficiency, empowering visual\ntokens with high-level semantic understanding while enhancing low-level\ngenerative capacity, making it well-suited for autoregressive Transformers in\nboth comprehension and generation tasks. The code and models are available at\nhttps://github.com/TencentARC/TokLIP.", "published": "2025-05-08 17:12:19", "link": "http://arxiv.org/abs/2505.05422v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Reasoning Models Don't Always Say What They Think", "abstract": "Chain-of-thought (CoT) offers a potential boon for AI safety as it allows\nmonitoring a model's CoT to try to understand its intentions and reasoning\nprocesses. However, the effectiveness of such monitoring hinges on CoTs\nfaithfully representing models' actual reasoning processes. We evaluate CoT\nfaithfulness of state-of-the-art reasoning models across 6 reasoning hints\npresented in the prompts and find: (1) for most settings and models tested,\nCoTs reveal their usage of hints in at least 1% of examples where they use the\nhint, but the reveal rate is often below 20%, (2) outcome-based reinforcement\nlearning initially improves faithfulness but plateaus without saturating, and\n(3) when reinforcement learning increases how frequently hints are used (reward\nhacking), the propensity to verbalize them does not increase, even without\ntraining against a CoT monitor. These results suggest that CoT monitoring is a\npromising way of noticing undesired behaviors during training and evaluations,\nbut that it is not sufficient to rule them out. They also suggest that in\nsettings like ours where CoT reasoning is not necessary, test-time monitoring\nof CoTs is unlikely to reliably catch rare and catastrophic unexpected\nbehaviors.", "published": "2025-05-08 16:51:43", "link": "http://arxiv.org/abs/2505.05410v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Crosslingual Reasoning through Test-Time Scaling", "abstract": "Reasoning capabilities of large language models are primarily studied for\nEnglish, even when pretrained models are multilingual. In this work, we\ninvestigate to what extent English reasoning finetuning with long\nchain-of-thoughts (CoTs) can generalize across languages. First, we find that\nscaling up inference compute for English-centric reasoning language models\n(RLMs) improves multilingual mathematical reasoning across many languages\nincluding low-resource languages, to an extent where they outperform models\ntwice their size. Second, we reveal that while English-centric RLM's CoTs are\nnaturally predominantly English, they consistently follow a quote-and-think\npattern to reason about quoted non-English inputs. Third, we discover an\neffective strategy to control the language of long CoT reasoning, and we\nobserve that models reason better and more efficiently in high-resource\nlanguages. Finally, we observe poor out-of-domain reasoning generalization, in\nparticular from STEM to cultural commonsense knowledge, even for English.\nOverall, we demonstrate the potentials, study the mechanisms and outline the\nlimitations of crosslingual generalization of English reasoning test-time\nscaling. We conclude that practitioners should let English-centric RLMs reason\nin high-resource languages, while further work is needed to improve reasoning\nin low-resource languages and out-of-domain contexts.", "published": "2025-05-08 16:50:06", "link": "http://arxiv.org/abs/2505.05408v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Frame In, Frame Out: Do LLMs Generate More Biased News Headlines than Humans?", "abstract": "Framing in media critically shapes public perception by selectively\nemphasizing some details while downplaying others. With the rise of large\nlanguage models in automated news and content creation, there is growing\nconcern that these systems may introduce or even amplify framing biases\ncompared to human authors. In this paper, we explore how framing manifests in\nboth out-of-the-box and fine-tuned LLM-generated news content. Our analysis\nreveals that, particularly in politically and socially sensitive contexts, LLMs\ntend to exhibit more pronounced framing than their human counterparts. In\naddition, we observe significant variation in framing tendencies across\ndifferent model architectures, with some models displaying notably higher\nbiases. These findings point to the need for effective post-training mitigation\nstrategies and tighter evaluation frameworks to ensure that automated news\ncontent upholds the standards of balanced reporting.", "published": "2025-05-08 16:46:24", "link": "http://arxiv.org/abs/2505.05406v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ICon: In-Context Contribution for Automatic Data Selection", "abstract": "Data selection for instruction tuning is essential for improving the\nperformance of Large Language Models (LLMs) and reducing training cost.\nHowever, existing automated selection methods either depend on computationally\nexpensive gradient-based measures or manually designed heuristics, which may\nfail to fully exploit the intrinsic attributes of data. In this paper, we\npropose In-context Learning for Contribution Measurement (ICon), a novel\ngradient-free method that takes advantage of the implicit fine-tuning nature of\nin-context learning (ICL) to measure sample contribution without gradient\ncomputation or manual indicators engineering. ICon offers a computationally\nefficient alternative to gradient-based methods and reduces human inductive\nbias inherent in heuristic-based approaches. ICon comprises three components\nand identifies high-contribution data by assessing performance shifts under\nimplicit learning through ICL. Extensive experiments on three LLMs across 12\nbenchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of\nICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data\noutperform full datasets by 5.42% points and exceed the best performance of\nwidely used selection methods by 2.06% points. We further analyze\nhigh-contribution samples selected by ICon, which show both diverse tasks and\nappropriate difficulty levels, rather than just the hardest ones.", "published": "2025-05-08 15:17:37", "link": "http://arxiv.org/abs/2505.05327v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scalable Chain of Thoughts via Elastic Reasoning", "abstract": "Large reasoning models (LRMs) have achieved remarkable progress on complex\ntasks by generating extended chains of thought (CoT). However, their\nuncontrolled output lengths pose significant challenges for real-world\ndeployment, where inference-time budgets on tokens, latency, or compute are\nstrictly constrained. We propose Elastic Reasoning, a novel framework for\nscalable chain of thoughts that explicitly separates reasoning into two\nphases--thinking and solution--with independently allocated budgets. At test\ntime, Elastic Reasoning prioritize that completeness of solution segments,\nsignificantly improving reliability under tight resource constraints. To train\nmodels that are robust to truncated thinking, we introduce a lightweight\nbudget-constrained rollout strategy, integrated into GRPO, which teaches the\nmodel to reason adaptively when the thinking process is cut short and\ngeneralizes effectively to unseen budget constraints without additional\ntraining. Empirical results on mathematical (AIME, MATH500) and programming\n(LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning\nperforms robustly under strict budget constraints, while incurring\nsignificantly lower training cost than baseline methods. Remarkably, our\napproach also produces more concise and efficient reasoning even in\nunconstrained settings. Elastic Reasoning offers a principled and practical\nsolution to the pressing challenge of controllable reasoning at scale.", "published": "2025-05-08 15:01:06", "link": "http://arxiv.org/abs/2505.05315v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Toward Reasonable Parrots: Why Large Language Models Should Argue with Us by Design", "abstract": "In this position paper, we advocate for the development of conversational\ntechnology that is inherently designed to support and facilitate argumentative\nprocesses. We argue that, at present, large language models (LLMs) are\ninadequate for this purpose, and we propose an ideal technology design aimed at\nenhancing argumentative skills. This involves re-framing LLMs as tools to\nexercise our critical thinking rather than replacing them. We introduce the\nconcept of 'reasonable parrots' that embody the fundamental principles of\nrelevance, responsibility, and freedom, and that interact through argumentative\ndialogical moves. These principles and moves arise out of millennia of work in\nargumentation theory and should serve as the starting point for LLM-based\ntechnology that incorporates basic principles of argumentation.", "published": "2025-05-08 14:41:07", "link": "http://arxiv.org/abs/2505.05298v1", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL"}
{"title": "T-T: Table Transformer for Tagging-based Aspect Sentiment Triplet Extraction", "abstract": "Aspect sentiment triplet extraction (ASTE) aims to extract triplets composed\nof aspect terms, opinion terms, and sentiment polarities from given sentences.\nThe table tagging method is a popular approach to addressing this task, which\nencodes a sentence into a 2-dimensional table, allowing for the tagging of\nrelations between any two words. Previous efforts have focused on designing\nvarious downstream relation learning modules to better capture interactions\nbetween tokens in the table, revealing that a stronger capability to capture\nrelations can lead to greater improvements in the model. Motivated by this, we\nattempt to directly utilize transformer layers as downstream relation learning\nmodules. Due to the powerful semantic modeling capability of transformers, it\nis foreseeable that this will lead to excellent improvement. However, owing to\nthe quadratic relation between the length of the table and the length of the\ninput sentence sequence, using transformers directly faces two challenges:\noverly long table sequences and unfair local attention interaction. To address\nthese challenges, we propose a novel Table-Transformer (T-T) for the\ntagging-based ASTE method. Specifically, we introduce a stripe attention\nmechanism with a loop-shift strategy to tackle these challenges. The former\nmodifies the global attention mechanism to only attend to a 2-dimensional local\nattention window, while the latter facilitates interaction between different\nattention windows. Extensive and comprehensive experiments demonstrate that the\nT-T, as a downstream relation learning module, achieves state-of-the-art\nperformance with lower computational costs.", "published": "2025-05-08 14:17:27", "link": "http://arxiv.org/abs/2505.05271v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "QualBench: Benchmarking Chinese LLMs with Localized Professional Qualifications for Vertical Domain Evaluation", "abstract": "The rapid advancement of Chinese large language models (LLMs) underscores the\nneed for domain-specific evaluations to ensure reliable applications. However,\nexisting benchmarks often lack coverage in vertical domains and offer limited\ninsights into the Chinese working context. Leveraging qualification exams as a\nunified framework for human expertise evaluation, we introduce QualBench, the\nfirst multi-domain Chinese QA benchmark dedicated to localized assessment of\nChinese LLMs. The dataset includes over 17,000 questions across six vertical\ndomains, with data selections grounded in 24 Chinese qualifications to closely\nalign with national policies and working standards. Through comprehensive\nevaluation, the Qwen2.5 model outperformed the more advanced GPT-4o, with\nChinese LLMs consistently surpassing non-Chinese models, highlighting the\nimportance of localized domain knowledge in meeting qualification requirements.\nThe best performance of 75.26% reveals the current gaps in domain coverage\nwithin model capabilities. Furthermore, we present the failure of LLM\ncollaboration with crowdsourcing mechanisms and suggest the opportunities for\nmulti-domain RAG knowledge enhancement and vertical domain LLM training with\nFederated Learning.", "published": "2025-05-08 13:16:49", "link": "http://arxiv.org/abs/2505.05225v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revealing Weaknesses in Text Watermarking Through Self-Information Rewrite Attacks", "abstract": "Text watermarking aims to subtly embed statistical signals into text by\ncontrolling the Large Language Model (LLM)'s sampling process, enabling\nwatermark detectors to verify that the output was generated by the specified\nmodel. The robustness of these watermarking algorithms has become a key factor\nin evaluating their effectiveness. Current text watermarking algorithms embed\nwatermarks in high-entropy tokens to ensure text quality. In this paper, we\nreveal that this seemingly benign design can be exploited by attackers, posing\na significant risk to the robustness of the watermark. We introduce a generic\nefficient paraphrasing attack, the Self-Information Rewrite Attack (SIRA),\nwhich leverages the vulnerability by calculating the self-information of each\ntoken to identify potential pattern tokens and perform targeted attack. Our\nwork exposes a widely prevalent vulnerability in current watermarking\nalgorithms. The experimental results show SIRA achieves nearly 100% attack\nsuccess rates on seven recent watermarking methods with only 0.88 USD per\nmillion tokens cost. Our approach does not require any access to the watermark\nalgorithms or the watermarked LLM and can seamlessly transfer to any LLM as the\nattack model, even mobile-level models. Our findings highlight the urgent need\nfor more robust watermarking.", "published": "2025-05-08 12:39:00", "link": "http://arxiv.org/abs/2505.05190v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "A Benchmark Dataset and a Framework for Urdu Multimodal Named Entity Recognition", "abstract": "The emergence of multimodal content, particularly text and images on social\nmedia, has positioned Multimodal Named Entity Recognition (MNER) as an\nincreasingly important area of research within Natural Language Processing.\nDespite progress in high-resource languages such as English, MNER remains\nunderexplored for low-resource languages like Urdu. The primary challenges\ninclude the scarcity of annotated multimodal datasets and the lack of\nstandardized baselines. To address these challenges, we introduce the U-MNER\nframework and release the Twitter2015-Urdu dataset, a pioneering resource for\nUrdu MNER. Adapted from the widely used Twitter2015 dataset, it is annotated\nwith Urdu-specific grammar rules. We establish benchmark baselines by\nevaluating both text-based and multimodal models on this dataset, providing\ncomparative analyses to support future research on Urdu MNER. The U-MNER\nframework integrates textual and visual context using Urdu-BERT for text\nembeddings and ResNet for visual feature extraction, with a Cross-Modal Fusion\nModule to align and fuse information. Our model achieves state-of-the-art\nperformance on the Twitter2015-Urdu dataset, laying the groundwork for further\nMNER research in low-resource languages.", "published": "2025-05-08 11:38:20", "link": "http://arxiv.org/abs/2505.05148v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding In-context Learning of Addition via Activation Subspaces", "abstract": "To perform in-context learning, language models must extract signals from\nindividual few-shot examples, aggregate these into a learned prediction rule,\nand then apply this rule to new examples. How is this implemented in the\nforward pass of modern transformer models? To study this, we consider a\nstructured family of few-shot learning tasks for which the true prediction rule\nis to add an integer $k$ to the input. We find that Llama-3-8B attains high\naccuracy on this task for a range of $k$, and localize its few-shot ability to\njust three attention heads via a novel optimization approach. We further show\nthe extracted signals lie in a six-dimensional subspace, where four of the\ndimensions track the unit digit and the other two dimensions track overall\nmagnitude. We finally examine how these heads extract information from\nindividual few-shot examples, identifying a self-correction mechanism in which\nmistakes from earlier examples are suppressed by later examples. Our results\ndemonstrate how tracking low-dimensional subspaces across a forward pass can\nprovide insight into fine-grained computational structures.", "published": "2025-05-08 11:32:46", "link": "http://arxiv.org/abs/2505.05145v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Unveiling Language-Specific Features in Large Language Models via Sparse Autoencoders", "abstract": "The mechanisms behind multilingual capabilities in Large Language Models\n(LLMs) have been examined using neuron-based or internal-activation-based\nmethods. However, these methods often face challenges such as superposition and\nlayer-wise activation variance, which limit their reliability. Sparse\nAutoencoders (SAEs) offer a more nuanced analysis by decomposing the\nactivations of LLMs into sparse linear combination of SAE features. We\nintroduce a novel metric to assess the monolinguality of features obtained from\nSAEs, discovering that some features are strongly related to specific\nlanguages. Additionally, we show that ablating these SAE features only\nsignificantly reduces abilities in one language of LLMs, leaving others almost\nunaffected. Interestingly, we find some languages have multiple synergistic SAE\nfeatures, and ablating them together yields greater improvement than ablating\nindividually. Moreover, we leverage these SAE-derived language-specific\nfeatures to enhance steering vectors, achieving control over the language\ngenerated by LLMs.", "published": "2025-05-08 10:24:44", "link": "http://arxiv.org/abs/2505.05111v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "X-Driver: Explainable Autonomous Driving with Vision-Language Models", "abstract": "End-to-end autonomous driving has advanced significantly, offering benefits\nsuch as system simplicity and stronger driving performance in both open-loop\nand closed-loop settings than conventional pipelines. However, existing\nframeworks still suffer from low success rates in closed-loop evaluations,\nhighlighting their limitations in real-world deployment. In this paper, we\nintroduce X-Driver, a unified multi-modal large language models(MLLMs)\nframework designed for closed-loop autonomous driving, leveraging\nChain-of-Thought(CoT) and autoregressive modeling to enhance perception and\ndecision-making. We validate X-Driver across multiple autonomous driving tasks\nusing public benchmarks in CARLA simulation environment, including\nBench2Drive[6]. Our experimental results demonstrate superior closed-loop\nperformance, surpassing the current state-of-the-art(SOTA) while improving the\ninterpretability of driving decisions. These findings underscore the importance\nof structured reasoning in end-to-end driving and establish X-Driver as a\nstrong baseline for future research in closed-loop autonomous driving.", "published": "2025-05-08 09:52:55", "link": "http://arxiv.org/abs/2505.05098v1", "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.ET"], "primary_category": "cs.RO"}
{"title": "Reliably Bounding False Positives: A Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal Prediction", "abstract": "The rapid advancement of large language models has raised significant\nconcerns regarding their potential misuse by malicious actors. As a result,\ndeveloping effective detectors to mitigate these risks has become a critical\npriority. However, most existing detection methods focus excessively on\ndetection accuracy, often neglecting the societal risks posed by high false\npositive rates (FPRs). This paper addresses this issue by leveraging Conformal\nPrediction (CP), which effectively constrains the upper bound of FPRs. While\ndirectly applying CP constrains FPRs, it also leads to a significant reduction\nin detection performance. To overcome this trade-off, this paper proposes a\nZero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal\nPrediction (MCP), which both enforces the FPR constraint and improves detection\nperformance. This paper also introduces RealDet, a high-quality dataset that\nspans a wide range of domains, ensuring realistic calibration and enabling\nsuperior detection performance when combined with MCP. Empirical evaluations\ndemonstrate that MCP effectively constrains FPRs, significantly enhances\ndetection performance, and increases robustness against adversarial attacks\nacross multiple detectors and datasets.", "published": "2025-05-08 09:32:38", "link": "http://arxiv.org/abs/2505.05084v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Performance Evaluation of Large Language Models in Bangla Consumer Health Query Summarization", "abstract": "Consumer Health Queries (CHQs) in Bengali (Bangla), a low-resource language,\noften contain extraneous details, complicating efficient medical responses.\nThis study investigates the zero-shot performance of nine advanced large\nlanguage models (LLMs): GPT-3.5-Turbo, GPT-4, Claude-3.5-Sonnet,\nLlama3-70b-Instruct, Mixtral-8x22b-Instruct, Gemini-1.5-Pro,\nQwen2-72b-Instruct, Gemma-2-27b, and Athene-70B, in summarizing Bangla CHQs.\nUsing the BanglaCHQ-Summ dataset comprising 2,350 annotated query-summary\npairs, we benchmarked these LLMs using ROUGE metrics against Bangla T5, a\nfine-tuned state-of-the-art model. Mixtral-8x22b-Instruct emerged as the top\nperforming model in ROUGE-1 and ROUGE-L, while Bangla T5 excelled in ROUGE-2.\nThe results demonstrate that zero-shot LLMs can rival fine-tuned models,\nachieving high-quality summaries even without task-specific training. This work\nunderscores the potential of LLMs in addressing challenges in low-resource\nlanguages, providing scalable solutions for healthcare query summarization.", "published": "2025-05-08 09:06:28", "link": "http://arxiv.org/abs/2505.05070v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CodeMixBench: Evaluating Large Language Models on Code Generation with Code-Mixed Prompts", "abstract": "Large Language Models (LLMs) have achieved remarkable success in code\ngeneration tasks, powering various applications like code completion,\ndebugging, and programming assistance. However, existing benchmarks such as\nHumanEval, MBPP, and BigCodeBench primarily evaluate LLMs on English-only\nprompts, overlooking the real-world scenario where multilingual developers\noften use code-mixed language while interacting with LLMs. To address this gap,\nwe introduce CodeMixBench, a novel benchmark designed to evaluate the\nrobustness of LLMs on code generation from code-mixed prompts. Built upon\nBigCodeBench, CodeMixBench introduces controlled code-mixing (CMD) into the\nnatural language parts of prompts across three language pairs: Hinglish\n(Hindi-English), Spanish-English, and Chinese Pinyin-English. We\ncomprehensively evaluate a diverse set of open-source code generation models\nranging from 1.5B to 15B parameters. Our results show that code-mixed prompts\nconsistently degrade Pass@1 performance compared to their English-only\ncounterparts, with performance drops increasing under higher CMD levels for\nsmaller models. CodeMixBench provides a realistic evaluation framework for\nstudying multilingual code generation and highlights new challenges and\ndirections for building robust code generation models that generalize well\nacross diverse linguistic settings.", "published": "2025-05-08 08:55:32", "link": "http://arxiv.org/abs/2505.05063v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Teochew-Wild: The First In-the-wild Teochew Dataset with Orthographic Annotations", "abstract": "This paper reports the construction of the Teochew-Wild, a speech corpus of\nthe Teochew dialect. The corpus includes 18.9 hours of in-the-wild Teochew\nspeech data from multiple speakers, covering both formal and colloquial\nexpressions, with precise orthographic and pinyin annotations. Additionally, we\nprovide supplementary text processing tools and resources to propel research\nand applications in speech tasks for this low-resource language, such as\nautomatic speech recognition (ASR) and text-to-speech (TTS). To the best of our\nknowledge, this is the first publicly available Teochew dataset with accurate\northographic annotations. We conduct experiments on the corpus, and the results\nvalidate its effectiveness in ASR and TTS tasks.", "published": "2025-05-08 08:47:11", "link": "http://arxiv.org/abs/2505.05056v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Image-Text Relation Prediction for Multilingual Tweets", "abstract": "Various social networks have been allowing media uploads for over a decade\nnow. Still, it has not always been clear what is their relation with the posted\ntext or even if there is any at all. In this work, we explore how multilingual\nvision-language models tackle the task of image-text relation prediction in\ndifferent languages, and construct a dedicated balanced benchmark data set from\nTwitter posts in Latvian along with their manual translations into English. We\ncompare our results to previous work and show that the more recently released\nvision-language model checkpoints are becoming increasingly capable at this\ntask, but there is still much room for further improvement.", "published": "2025-05-08 08:23:20", "link": "http://arxiv.org/abs/2505.05040v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness", "abstract": "Evaluating user interface (UI) design effectiveness extends beyond aesthetics\nto influencing user behavior, a principle central to Design Persuasiveness. A/B\ntesting is the predominant method for determining which UI variations drive\nhigher user engagement, but it is costly and time-consuming. While recent\nVision-Language Models (VLMs) can process automated UI analysis, current\napproaches focus on isolated design attributes rather than comparative\npersuasiveness-the key factor in optimizing user interactions. To address this,\nwe introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design\nPersuasiveness Assessment task, featuring 300 real-world UI image pairs labeled\nwith A/B test results and expert rationales. Additionally, we propose G-FOCUS,\na novel inference-time reasoning strategy that enhances VLM-based\npersuasiveness assessment by reducing position bias and improving evaluation\naccuracy. Experimental results show that G-FOCUS surpasses existing inference\nstrategies in consistency and accuracy for pairwise UI evaluation. Through\npromoting VLM-driven evaluation of UI persuasiveness, our work offers an\napproach to complement A/B testing, propelling progress in scalable UI\npreference modeling and design optimization. Code and data will be released\npublicly.", "published": "2025-05-08 08:00:32", "link": "http://arxiv.org/abs/2505.05026v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scalable Multi-Stage Influence Function for Large Language Models via Eigenvalue-Corrected Kronecker-Factored Parameterization", "abstract": "Pre-trained large language models (LLMs) are commonly fine-tuned to adapt to\ndownstream tasks. Since the majority of knowledge is acquired during\npre-training, attributing the predictions of fine-tuned LLMs to their\npre-training data may provide valuable insights. Influence functions have been\nproposed as a means to explain model predictions based on training data.\nHowever, existing approaches fail to compute ``multi-stage'' influence and lack\nscalability to billion-scale LLMs.\n  In this paper, we propose the multi-stage influence function to attribute the\ndownstream predictions of fine-tuned LLMs to pre-training data under the\nfull-parameter fine-tuning paradigm. To enhance the efficiency and practicality\nof our multi-stage influence function, we leverage Eigenvalue-corrected\nKronecker-Factored (EK-FAC) parameterization for efficient approximation.\nEmpirical results validate the superior scalability of EK-FAC approximation and\nthe effectiveness of our multi-stage influence function. Additionally, case\nstudies on a real-world LLM, dolly-v2-3b, demonstrate its interpretive power,\nwith exemplars illustrating insights provided by multi-stage influence\nestimates. Our code is public at\nhttps://github.com/colored-dye/multi_stage_influence_function.", "published": "2025-05-08 07:43:44", "link": "http://arxiv.org/abs/2505.05017v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Pitfalls of Growing Group Complexity: LLMs and Social Choice-Based Aggregation for Group Recommendations", "abstract": "Large Language Models (LLMs) are increasingly applied in recommender systems\naimed at both individuals and groups. Previously, Group Recommender Systems\n(GRS) often used social choice-based aggregation strategies to derive a single\nrecommendation based on the preferences of multiple people. In this paper, we\ninvestigate under which conditions language models can perform these strategies\ncorrectly based on zero-shot learning and analyse whether the formatting of the\ngroup scenario in the prompt affects accuracy. We specifically focused on the\nimpact of group complexity (number of users and items), different LLMs,\ndifferent prompting conditions, including In-Context learning or generating\nexplanations, and the formatting of group preferences. Our results show that\nperformance starts to deteriorate when considering more than 100 ratings.\nHowever, not all language models were equally sensitive to growing group\ncomplexity. Additionally, we showed that In-Context Learning (ICL) can\nsignificantly increase the performance at higher degrees of group complexity,\nwhile adding other prompt modifications, specifying domain cues or prompting\nfor explanations, did not impact accuracy. We conclude that future research\nshould include group complexity as a factor in GRS evaluation due to its effect\non LLM performance. Furthermore, we showed that formatting the group scenarios\ndifferently, such as rating lists per user or per item, affected accuracy. All\nin all, our study implies that smaller LLMs are capable of generating group\nrecommendations under the right conditions, making the case for using smaller\nmodels that require less computing power and costs.", "published": "2025-05-08 07:43:01", "link": "http://arxiv.org/abs/2505.05016v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Rethinking Invariance in In-context Learning", "abstract": "In-Context Learning (ICL) has emerged as a pivotal capability of\nauto-regressive large language models, yet it is hindered by a notable\nsensitivity to the ordering of context examples regardless of their mutual\nindependence. To address this issue, recent studies have introduced several\nvariant algorithms of ICL that achieve permutation invariance. However, many of\nthese do not exhibit comparable performance with the standard auto-regressive\nICL algorithm. In this work, we identify two crucial elements in the design of\nan invariant ICL algorithm: information non-leakage and context\ninterdependence, which are not simultaneously achieved by any of the existing\nmethods. These investigations lead us to the proposed Invariant ICL (InvICL), a\nmethodology designed to achieve invariance in ICL while ensuring the two\nproperties. Empirically, our findings reveal that InvICL surpasses previous\nmodels, both invariant and non-invariant, in most benchmark datasets,\nshowcasing superior generalization capabilities across varying input lengths.\nCode is available at https://github.com/PKU-ML/InvICL.", "published": "2025-05-08 06:59:14", "link": "http://arxiv.org/abs/2505.04994v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes", "abstract": "Large language models (LLMs) have achieved remarkable success, yet aligning\ntheir generations with human preferences remains a critical challenge. Existing\napproaches to preference modeling often rely on an explicit or implicit reward\nfunction, overlooking the intricate and multifaceted nature of human\npreferences that may encompass conflicting factors across diverse tasks and\npopulations. To address this limitation, we introduce Latent Preference Coding\n(LPC), a novel framework that models the implicit factors as well as their\ncombinations behind holistic preferences using discrete latent codes. LPC\nseamlessly integrates with various offline alignment algorithms, automatically\ninferring the underlying factors and their importance from data without relying\non pre-defined reward functions and hand-crafted combination weights. Extensive\nexperiments on multiple benchmarks demonstrate that LPC consistently improves\nupon three alignment algorithms (DPO, SimPO, and IPO) using three base models\n(Mistral-7B, Llama3-8B, and Llama3-8B-Instruct). Furthermore, deeper analysis\nreveals that the learned latent codes effectively capture the differences in\nthe distribution of human preferences and significantly enhance the robustness\nof alignment against noise in data. By providing a unified representation for\nthe multifarious preference factors, LPC paves the way towards developing more\nrobust and versatile alignment techniques for the responsible deployment of\npowerful LLMs.", "published": "2025-05-08 06:59:06", "link": "http://arxiv.org/abs/2505.04993v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking the Relationship between the Power Law and Hierarchical Structures", "abstract": "Statistical analysis of corpora provides an approach to quantitatively\ninvestigate natural languages. This approach has revealed that several power\nlaws consistently emerge across different corpora and languages, suggesting the\nuniversal principles underlying languages. Particularly, the power-law decay of\ncorrelation has been interpreted as evidence for underlying hierarchical\nstructures in syntax, semantics, and discourse. This perspective has also been\nextended to child languages and animal signals. However, the argument\nsupporting this interpretation has not been empirically tested. To address this\nproblem, this study examines the validity of the argument for syntactic\nstructures. Specifically, we test whether the statistical properties of parse\ntrees align with the implicit assumptions in the argument. Using English\ncorpora, we analyze the mutual information, deviations from probabilistic\ncontext-free grammars (PCFGs), and other properties in parse trees, as well as\nin the PCFG that approximates these trees. Our results indicate that the\nassumptions do not hold for syntactic structures and that it is difficult to\napply the proposed argument to child languages and animal signals, highlighting\nthe need to reconsider the relationship between the power law and hierarchical\nstructures.", "published": "2025-05-08 06:41:46", "link": "http://arxiv.org/abs/2505.04984v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "General Transform: A Unified Framework for Adaptive Transform to Enhance Representations", "abstract": "Discrete transforms, such as the discrete Fourier transform, are widely used\nin machine learning to improve model performance by extracting meaningful\nfeatures. However, with numerous transforms available, selecting an appropriate\none often depends on understanding the dataset's properties, making the\napproach less effective when such knowledge is unavailable. In this work, we\npropose General Transform (GT), an adaptive transform-based representation\ndesigned for machine learning applications. Unlike conventional transforms, GT\nlearns data-driven mapping tailored to the dataset and task of interest. Here,\nwe demonstrate that models incorporating GT outperform conventional\ntransform-based approaches across computer vision and natural language\nprocessing tasks, highlighting its effectiveness in diverse learning scenarios.", "published": "2025-05-08 06:01:11", "link": "http://arxiv.org/abs/2505.04969v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Chain-of-Thought Tokens are Computer Program Variables", "abstract": "Chain-of-thoughts (CoT) requires large language models (LLMs) to generate\nintermediate steps before reaching the final answer, and has been proven\neffective to help LLMs solve complex reasoning tasks. However, the inner\nmechanism of CoT still remains largely unclear. In this paper, we empirically\nstudy the role of CoT tokens in LLMs on two compositional tasks: multi-digit\nmultiplication and dynamic programming. While CoT is essential for solving\nthese problems, we find that preserving only tokens that store intermediate\nresults would achieve comparable performance. Furthermore, we observe that\nstoring intermediate results in an alternative latent form will not affect\nmodel performance. We also randomly intervene some values in CoT, and notice\nthat subsequent CoT tokens and the final answer would change correspondingly.\nThese findings suggest that CoT tokens may function like variables in computer\nprograms but with potential drawbacks like unintended shortcuts and\ncomputational complexity limits between tokens. The code and data are available\nat https://github.com/solitaryzero/CoTs_are_Variables.", "published": "2025-05-08 05:32:36", "link": "http://arxiv.org/abs/2505.04955v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prompt-Based LLMs for Position Bias-Aware Reranking in Personalized Recommendations", "abstract": "Recommender systems are essential for delivering personalized content across\ndigital platforms by modeling user preferences and behaviors. Recently, large\nlanguage models (LLMs) have been adopted for prompt-based recommendation due to\ntheir ability to generate personalized outputs without task-specific training.\nHowever, LLM-based methods face limitations such as limited context window\nsize, inefficient pointwise and pairwise prompting, and difficulty handling\nlistwise ranking due to token constraints. LLMs can also be sensitive to\nposition bias, as they may overemphasize earlier items in the prompt regardless\nof their true relevance. To address and investigate these issues, we propose a\nhybrid framework that combines a traditional recommendation model with an LLM\nfor reranking top-k items using structured prompts. We evaluate the effects of\nuser history reordering and instructional prompts for mitigating position bias.\nExperiments on MovieLens-100K show that randomizing user history improves\nranking quality, but LLM-based reranking does not outperform the base model.\nExplicit instructions to reduce position bias are also ineffective. Our\nevaluations reveal limitations in LLMs' ability to model ranking context and\nmitigate bias. Our code is publicly available at\nhttps://github.com/aminul7506/LLMForReRanking.", "published": "2025-05-08 05:01:44", "link": "http://arxiv.org/abs/2505.04948v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation Models", "abstract": "Thanks to recent advancements in scalable deep architectures and large-scale\npretraining, text-to-video generation has achieved unprecedented capabilities\nin producing high-fidelity, instruction-following content across a wide range\nof styles, enabling applications in advertising, entertainment, and education.\nHowever, these models' ability to render precise on-screen text, such as\ncaptions or mathematical formulas, remains largely untested, posing significant\nchallenges for applications requiring exact textual accuracy. In this work, we\nintroduce T2VTextBench, the first human-evaluation benchmark dedicated to\nevaluating on-screen text fidelity and temporal consistency in text-to-video\nmodels. Our suite of prompts integrates complex text strings with dynamic scene\nchanges, testing each model's ability to maintain detailed instructions across\nframes. We evaluate ten state-of-the-art systems, ranging from open-source\nsolutions to commercial offerings, and find that most struggle to generate\nlegible, consistent text. These results highlight a critical gap in current\nvideo generators and provide a clear direction for future research aimed at\nenhancing textual manipulation in video synthesis.", "published": "2025-05-08 04:49:52", "link": "http://arxiv.org/abs/2505.04946v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models", "abstract": "Reasoning lies at the heart of intelligence, shaping the ability to make\ndecisions, draw conclusions, and generalize across domains. In artificial\nintelligence, as systems increasingly operate in open, uncertain, and\nmultimodal environments, reasoning becomes essential for enabling robust and\nadaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a\npromising paradigm, integrating modalities such as text, images, audio, and\nvideo to support complex reasoning capabilities and aiming to achieve\ncomprehensive perception, precise understanding, and deep reasoning. As\nresearch advances, multimodal reasoning has rapidly evolved from modular,\nperception-driven pipelines to unified, language-centric frameworks that offer\nmore coherent cross-modal understanding. While instruction tuning and\nreinforcement learning have improved model reasoning, significant challenges\nremain in omni-modal generalization, reasoning depth, and agentic behavior. To\naddress these issues, we present a comprehensive and structured survey of\nmultimodal reasoning research, organized around a four-stage developmental\nroadmap that reflects the field's shifting design philosophies and emerging\ncapabilities. First, we review early efforts based on task-specific modules,\nwhere reasoning was implicitly embedded across stages of representation,\nalignment, and fusion. Next, we examine recent approaches that unify reasoning\ninto multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT)\nand multimodal reinforcement learning enabling richer and more structured\nreasoning chains. Finally, drawing on empirical insights from challenging\nbenchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the\nconceptual direction of native large multimodal reasoning models (N-LMRMs),\nwhich aim to support scalable, agentic, and adaptive reasoning and planning in\ncomplex, real-world environments.", "published": "2025-05-08 03:35:23", "link": "http://arxiv.org/abs/2505.04921v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "An Open-Source Dual-Loss Embedding Model for Semantic Retrieval in Higher Education", "abstract": "Recent advances in AI have catalyzed the adoption of intelligent educational\ntools, yet many semantic retrieval systems remain ill-suited to the unique\nlinguistic and structural characteristics of academic content. This study\npresents two open-source embedding models fine-tuned for educational question\nanswering, particularly in the context of course syllabi. A synthetic dataset\nof 3,197 sentence pairs, spanning synonymous terminology, paraphrased\nquestions, and implicit-explicit mappings, was constructed through a\ncombination of manual curation and large language model (LLM)-assisted\ngeneration. Two training strategies were evaluated: (1) a baseline model\nfine-tuned using MultipleNegativesRankingLoss (MNRL), and (2) a dual-loss model\nthat combines MNRL with CosineSimilarityLoss to improve both semantic ranking\nand similarity calibration. Evaluations were conducted on 28 university course\nsyllabi using a fixed set of natural language questions categorized into\ncourse, faculty, and teaching assistant information. Results demonstrate that\nboth fine-tuned models outperform strong open-source baselines, including\nall-MiniLM-L6-v2 and multi-qa-MiniLM-L6-cos-v1, and that the dual-loss model\nnarrows the performance gap with high-performing proprietary embeddings such as\nOpenAI's text-embedding-3 series. This work contributes reusable,\ndomain-aligned embedding models and provides a replicable framework for\neducational semantic retrieval, supporting downstream applications such as\nacademic chatbots, retrieval-augmented generation (RAG) systems, and learning\nmanagement system (LMS) integrations.", "published": "2025-05-08 03:14:14", "link": "http://arxiv.org/abs/2505.04916v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Enigme: Generative Text Puzzles for Evaluating Reasoning in Language Models", "abstract": "Transformer-decoder language models are a core innovation in text based\ngenerative artificial intelligence. These models are being deployed as\ngeneral-purpose intelligence systems in many applications. Central to their\nutility is the capacity to understand natural language commands and exploit the\nreasoning embedded in human text corpora to apply some form of reasoning\nprocess to a wide variety of novel tasks. To understand the limitations of this\napproach to generating reasoning we argue that we need to consider the\narchitectural constraints of these systems. Consideration of the latent\nvariable structure of transformer-decoder models allows us to design reasoning\ntasks that should probe the boundary of their capacity to reason. We present\nenigme, an open-source library for generating text-based puzzles to be used in\ntraining and evaluating reasoning skills within transformer-decoder models and\nfuture AI architectures.", "published": "2025-05-08 03:09:57", "link": "http://arxiv.org/abs/2505.04914v1", "categories": ["cs.AI", "cs.CL", "I.2.7"], "primary_category": "cs.AI"}
{"title": "SpatialPrompting: Keyframe-driven Zero-Shot Spatial Reasoning with Off-the-Shelf Multimodal Large Language Models", "abstract": "This study introduces SpatialPrompting, a novel framework that harnesses the\nemergent reasoning capabilities of off-the-shelf multimodal large language\nmodels to achieve zero-shot spatial reasoning in three-dimensional (3D)\nenvironments. Unlike existing methods that rely on expensive 3D-specific\nfine-tuning with specialized 3D inputs such as point clouds or voxel-based\nfeatures, SpatialPrompting employs a keyframe-driven prompt generation\nstrategy. This framework uses metrics such as vision-language similarity,\nMahalanobis distance, field of view, and image sharpness to select a diverse\nand informative set of keyframes from image sequences and then integrates them\nwith corresponding camera pose data to effectively abstract spatial\nrelationships and infer complex 3D structures. The proposed framework not only\nestablishes a new paradigm for flexible spatial reasoning that utilizes\nintuitive visual and positional cues but also achieves state-of-the-art\nzero-shot performance on benchmark datasets, such as ScanQA and SQA3D, across\nseveral metrics. The proposed method effectively eliminates the need for\nspecialized 3D inputs and fine-tuning, offering a simpler and more scalable\nalternative to conventional approaches.", "published": "2025-05-08 02:59:01", "link": "http://arxiv.org/abs/2505.04911v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ConCISE: Confidence-guided Compression in Step-by-step Efficient Reasoning", "abstract": "Large Reasoning Models (LRMs) perform strongly in complex reasoning tasks via\nChain-of-Thought (CoT) prompting, but often suffer from verbose outputs caused\nby redundant content, increasing computational overhead, and degrading user\nexperience. Existing compression methods either operate post-hoc pruning,\nrisking disruption to reasoning coherence, or rely on sampling-based selection,\nwhich fails to intervene effectively during generation. In this work, we\nintroduce a confidence-guided perspective to explain the emergence of redundant\nreflection in LRMs, identifying two key patterns: Confidence Deficit, where the\nmodel reconsiders correct steps due to low internal confidence, and Termination\nDelay, where reasoning continues even after reaching a confident answer. Based\non this analysis, we propose ConCISE (Confidence-guided Compression In\nStep-by-step Efficient Reasoning), a framework that simplifies reasoning chains\nby reinforcing the model's confidence during inference, thus preventing the\ngeneration of redundant reflection steps. It integrates Confidence Injection to\nstabilize intermediate steps and Early Stopping to terminate reasoning when\nconfidence is sufficient. Extensive experiments demonstrate that fine-tuning\nLRMs on ConCISE-generated data yields significantly shorter outputs, reducing\nlength by up to approximately 50% under SimPO, while maintaining high task\naccuracy. ConCISE consistently outperforms existing baselines across multiple\nreasoning benchmarks.", "published": "2025-05-08 01:40:40", "link": "http://arxiv.org/abs/2505.04881v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Flow-GRPO: Training Flow Matching Models via Online RL", "abstract": "We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from $63\\%$ to $95\\%$. In visual text rendering, its accuracy\nimproves from $59\\%$ to $92\\%$, significantly enhancing text generation.\nFlow-GRPO also achieves substantial gains in human preference alignment.\nNotably, little to no reward hacking occurred, meaning rewards did not increase\nat the cost of image quality or diversity, and both remained stable in our\nexperiments.", "published": "2025-05-08 17:58:45", "link": "http://arxiv.org/abs/2505.05470v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Conversational Process Model Redesign", "abstract": "With the recent success of large language models (LLMs), the idea of\nAI-augmented Business Process Management systems is becoming more feasible. One\nof their essential characteristics is the ability to be conversationally\nactionable, allowing humans to interact with the LLM effectively to perform\ncrucial process life cycle tasks such as process model design and redesign.\nHowever, most current research focuses on single-prompt execution and\nevaluation of results, rather than on continuous interaction between the user\nand the LLM. In this work, we aim to explore the feasibility of using LLMs to\nempower domain experts in the creation and redesign of process models in an\niterative and effective way. The proposed conversational process model redesign\n(CPD) approach receives as input a process model and a redesign request by the\nuser in natural language. Instead of just letting the LLM make changes, the LLM\nis employed to (a) identify process change patterns from literature, (b)\nre-phrase the change request to be aligned with an expected wording for the\nidentified pattern (i.e., the meaning), and then to (c) apply the meaning of\nthe change to the process model. This multi-step approach allows for\nexplainable and reproducible changes. In order to ensure the feasibility of the\nCPD approach, and to find out how well the patterns from literature can be\nhandled by the LLM, we performed an extensive evaluation. The results show that\nsome patterns are hard to understand by LLMs and by users. Within the scope of\nthe study, we demonstrated that users need support to describe the changes\nclearly. Overall the evaluation shows that the LLMs can handle most changes\nwell according to a set of completeness and correctness criteria.", "published": "2025-05-08 17:44:45", "link": "http://arxiv.org/abs/2505.05453v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "EcoAgent: An Efficient Edge-Cloud Collaborative Multi-Agent Framework for Mobile Automation", "abstract": "Cloud-based mobile agents powered by (multimodal) large language models\n((M)LLMs) offer strong reasoning abilities but suffer from high latency and\ncost. While fine-tuned (M)SLMs enable edge deployment, they often lose general\ncapabilities and struggle with complex tasks. To address this, we propose\nEcoAgent, an Edge-Cloud cOllaborative multi-agent framework for mobile\nautomation. EcoAgent features a closed-loop collaboration among a cloud-based\nPlanning Agent and two edge-based agents: the Execution Agent for action\nexecution and the Observation Agent for verifying outcomes. The Observation\nAgent uses a Pre-Understanding Module to compress screen images into concise\ntext, reducing token usage. In case of failure, the Planning Agent retrieves\nscreen history and replans via a Reflection Module. Experiments on AndroidWorld\nshow that EcoAgent maintains high task success rates while significantly\nreducing MLLM token consumption, enabling efficient and practical mobile\nautomation.", "published": "2025-05-08 17:31:20", "link": "http://arxiv.org/abs/2505.05440v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "CART-ELC: Oblique Decision Tree Induction via Exhaustive Search", "abstract": "Oblique decision trees have attracted attention due to their potential for\nimproved classification performance over traditional axis-aligned decision\ntrees. However, methods that rely on exhaustive search to find oblique splits\nface computational challenges. As a result, they have not been widely explored.\nWe introduce a novel algorithm, Classification and Regression Tree - Exhaustive\nLinear Combinations (CART-ELC), for inducing oblique decision trees that\nperforms an exhaustive search on a restricted set of hyperplanes. We then\ninvestigate the algorithm's computational complexity and its predictive\ncapabilities. Our results demonstrate that CART-ELC consistently achieves\ncompetitive performance on small datasets, often yielding statistically\nsignificant improvements in classification accuracy relative to existing\ndecision tree induction algorithms, while frequently producing shallower,\nsimpler, and thus more interpretable trees.", "published": "2025-05-08 16:42:13", "link": "http://arxiv.org/abs/2505.05402v1", "categories": ["cs.LG", "cs.AI", "cs.DS", "I.2.6; I.5.2; F.2.2; G.3; G.2.1"], "primary_category": "cs.LG"}
{"title": "A Pain Assessment Framework based on multimodal data and Deep Machine Learning methods", "abstract": "From the original abstract:\n  This thesis initially aims to study the pain assessment process from a\nclinical-theoretical perspective while exploring and examining existing\nautomatic approaches. Building on this foundation, the primary objective of\nthis Ph.D. project is to develop innovative computational methods for automatic\npain assessment that achieve high performance and are applicable in real\nclinical settings. A primary goal is to thoroughly investigate and assess\nsignificant factors, including demographic elements that impact pain\nperception, as recognized in pain research, through a computational standpoint.\nWithin the limits of the available data in this research area, our goal was to\ndesign, develop, propose, and offer automatic pain assessment pipelines for\nunimodal and multimodal configurations that are applicable to the specific\nrequirements of different scenarios. The studies published in this Ph.D. thesis\nshowcased the effectiveness of the proposed methods, achieving state-of-the-art\nresults. Additionally, they paved the way for exploring new approaches in\nartificial intelligence, foundation models, and generative artificial\nintelligence.", "published": "2025-05-08 16:32:55", "link": "http://arxiv.org/abs/2505.05396v1", "categories": ["cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Threshold Modulation for Online Test-Time Adaptation of Spiking Neural Networks", "abstract": "Recently, spiking neural networks (SNNs), deployed on neuromorphic chips,\nprovide highly efficient solutions on edge devices in different scenarios.\nHowever, their ability to adapt to distribution shifts after deployment has\nbecome a crucial challenge. Online test-time adaptation (OTTA) offers a\npromising solution by enabling models to dynamically adjust to new data\ndistributions without requiring source data or labeled target samples.\nNevertheless, existing OTTA methods are largely designed for traditional\nartificial neural networks and are not well-suited for SNNs. To address this\ngap, we propose a low-power, neuromorphic chip-friendly online test-time\nadaptation framework, aiming to enhance model generalization under distribution\nshifts. The proposed approach is called Threshold Modulation (TM), which\ndynamically adjusts the firing threshold through neuronal dynamics-inspired\nnormalization, being more compatible with neuromorphic hardware. Experimental\nresults on benchmark datasets demonstrate the effectiveness of this method in\nimproving the robustness of SNNs against distribution shifts while maintaining\nlow computational cost. The proposed method offers a practical solution for\nonline test-time adaptation of SNNs, providing inspiration for the design of\nfuture neuromorphic chips. The demo code is available at\ngithub.com/NneurotransmitterR/TM-OTTA-SNN.", "published": "2025-05-08 16:09:40", "link": "http://arxiv.org/abs/2505.05375v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CV"}
{"title": "Time of the Flight of the Gaussians: Optimizing Depth Indirectly in Dynamic Radiance Fields", "abstract": "We present a method to reconstruct dynamic scenes from monocular\ncontinuous-wave time-of-flight (C-ToF) cameras using raw sensor samples that\nachieves similar or better accuracy than neural volumetric approaches and is\n100x faster. Quickly achieving high-fidelity dynamic 3D reconstruction from a\nsingle viewpoint is a significant challenge in computer vision. In C-ToF\nradiance field reconstruction, the property of interest-depth-is not directly\nmeasured, causing an additional challenge. This problem has a large and\nunderappreciated impact upon the optimization when using a fast primitive-based\nscene representation like 3D Gaussian splatting, which is commonly used with\nmulti-view data to produce satisfactory results and is brittle in its\noptimization otherwise. We incorporate two heuristics into the optimization to\nimprove the accuracy of scene geometry represented by Gaussians. Experimental\nresults show that our approach produces accurate reconstructions under\nconstrained C-ToF sensing conditions, including for fast motions like swinging\nbaseball bats. https://visual.cs.brown.edu/gftorf", "published": "2025-05-08 15:45:53", "link": "http://arxiv.org/abs/2505.05356v1", "categories": ["cs.GR", "cs.AI", "cs.CV"], "primary_category": "cs.GR"}
{"title": "High-fidelity Grain Growth Modeling: Leveraging Deep Learning for Fast Computations", "abstract": "Grain growth simulation is crucial for predicting metallic material\nmicrostructure evolution during annealing and resulting final mechanical\nproperties, but traditional partial differential equation-based methods are\ncomputationally expensive, creating bottlenecks in materials design and\nmanufacturing. In this work, we introduce a machine learning framework that\ncombines a Convolutional Long Short-Term Memory networks with an Autoencoder to\nefficiently predict grain growth evolution. Our approach captures both spatial\nand temporal aspects of grain evolution while encoding high-dimensional grain\nstructure data into a compact latent space for pattern learning, enhanced by a\nnovel composite loss function combining Mean Squared Error, Structural\nSimilarity Index Measurement, and Boundary Preservation to maintain structural\nintegrity of grain boundary topology of the prediction. Results demonstrated\nthat our machine learning approach accelerates grain growth prediction by up to\n\\SI{89}{\\times} faster, reducing computation time from \\SI{10}{\\minute} to\napproximately \\SI{10}{\\second} while maintaining high-fidelity predictions. The\nbest model (S-30-30) achieving a structural similarity score of\n\\SI{86.71}{\\percent} and mean grain size error of just \\SI{0.07}{\\percent}. All\nmodels accurately captured grain boundary topology, morphology, and size\ndistributions. This approach enables rapid microstructural prediction for\napplications where conventional simulations are prohibitively time-consuming,\npotentially accelerating innovation in materials science and manufacturing.", "published": "2025-05-08 15:43:40", "link": "http://arxiv.org/abs/2505.05354v1", "categories": ["cond-mat.mtrl-sci", "cs.AI"], "primary_category": "cond-mat.mtrl-sci"}
{"title": "Feature-Augmented Deep Networks for Multiscale Building Segmentation in High-Resolution UAV and Satellite Imagery", "abstract": "Accurate building segmentation from high-resolution RGB imagery remains\nchallenging due to spectral similarity with non-building features, shadows, and\nirregular building geometries. In this study, we present a comprehensive deep\nlearning framework for multiscale building segmentation using RGB aerial and\nsatellite imagery with spatial resolutions ranging from 0.4m to 2.7m. We curate\na diverse, multi-sensor dataset and introduce feature-augmented inputs by\nderiving secondary representations including Principal Component Analysis\n(PCA), Visible Difference Vegetation Index (VDVI), Morphological Building Index\n(MBI), and Sobel edge filters from RGB channels. These features guide a\nRes-U-Net architecture in learning complex spatial patterns more effectively.\nWe also propose training policies incorporating layer freezing, cyclical\nlearning rates, and SuperConvergence to reduce training time and resource\nusage. Evaluated on a held-out WorldView-3 image, our model achieves an overall\naccuracy of 96.5%, an F1-score of 0.86, and an Intersection over Union (IoU) of\n0.80, outperforming existing RGB-based benchmarks. This study demonstrates the\neffectiveness of combining multi-resolution imagery, feature augmentation, and\noptimized training strategies for robust building segmentation in remote\nsensing applications.", "published": "2025-05-08 15:08:36", "link": "http://arxiv.org/abs/2505.05321v1", "categories": ["cs.CV", "cs.AI", "I.4.6; I.4.10; I.5.1; I.2.10"], "primary_category": "cs.CV"}
{"title": "Mapping User Trust in Vision Language Models: Research Landscape, Challenges, and Prospects", "abstract": "The rapid adoption of Vision Language Models (VLMs), pre-trained on large\nimage-text and video-text datasets, calls for protecting and informing users\nabout when to trust these systems. This survey reviews studies on trust\ndynamics in user-VLM interactions, through a multi-disciplinary taxonomy\nencompassing different cognitive science capabilities, collaboration modes, and\nagent behaviours. Literature insights and findings from a workshop with\nprospective VLM users inform preliminary requirements for future VLM trust\nstudies.", "published": "2025-05-08 15:02:49", "link": "http://arxiv.org/abs/2505.05318v1", "categories": ["cs.CV", "cs.AI", "cs.CY", "cs.HC", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Benchmarking Ophthalmology Foundation Models for Clinically Significant Age Macular Degeneration Detection", "abstract": "Self-supervised learning (SSL) has enabled Vision Transformers (ViTs) to\nlearn robust representations from large-scale natural image datasets, enhancing\ntheir generalization across domains. In retinal imaging, foundation models\npretrained on either natural or ophthalmic data have shown promise, but the\nbenefits of in-domain pretraining remain uncertain. To investigate this, we\nbenchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasets\ntotaling 70,000 expert-annotated images for the task of moderate-to-late\nage-related macular degeneration (AMD) identification. Our results show that\niBOT pretrained on natural images achieves the highest out-of-distribution\ngeneralization, with AUROCs of 0.80-0.97, outperforming domain-specific models,\nwhich achieved AUROCs of 0.78-0.96 and a baseline ViT-L with no pretraining,\nwhich achieved AUROCs of 0.68-0.91. These findings highlight the value of\nfoundation models in improving AMD identification and challenge the assumption\nthat in-domain pretraining is necessary. Furthermore, we release BRAMD, an\nopen-access dataset (n=587) of DFIs with AMD labels from Brazil.", "published": "2025-05-08 14:31:02", "link": "http://arxiv.org/abs/2505.05291v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "q-bio.TO"], "primary_category": "eess.IV"}
{"title": "PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes", "abstract": "We introduce the novel task of Language-Guided Object Placement in Real 3D\nScenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual\nprompt broadly describing where the 3D asset should be placed. The task here is\nto find a valid placement for the 3D asset that respects the prompt. Compared\nwith other language-guided localization tasks in 3D scenes such as grounding,\nthis task has specific challenges: it is ambiguous because it has multiple\nvalid solutions, and it requires reasoning about 3D geometric relationships and\nfree space. We inaugurate this task by proposing a new benchmark and evaluation\nprotocol. We also introduce a new dataset for training 3D LLMs on this task, as\nwell as the first method to serve as a non-trivial baseline. We believe that\nthis challenging task and our new benchmark could become part of the suite of\nbenchmarks used to evaluate and compare generalist 3D LLM models.", "published": "2025-05-08 14:29:11", "link": "http://arxiv.org/abs/2505.05288v1", "categories": ["cs.CV", "cs.AI", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Software Development Life Cycle Perspective: A Survey of Benchmarks for CodeLLMs and Agents", "abstract": "Code large language models (CodeLLMs) and agents have shown great promise in\ntackling complex software engineering tasks.Compared to traditional software\nengineering methods, CodeLLMs and agents offer stronger abilities, and can\nflexibly process inputs and outputs in both natural and code. Benchmarking\nplays a crucial role in evaluating the capabilities of CodeLLMs and agents,\nguiding their development and deployment. However, despite their growing\nsignificance, there remains a lack of comprehensive reviews of benchmarks for\nCodeLLMs and agents. To bridge this gap, this paper provides a comprehensive\nreview of existing benchmarks for CodeLLMs and agents, studying and analyzing\n181 benchmarks from 461 relevant papers, covering the different phases of the\nsoftware development life cycle (SDLC). Our findings reveal a notable imbalance\nin the coverage of current benchmarks, with approximately 60% focused on the\nsoftware development phase in SDLC, while requirements engineering and software\ndesign phases receive minimal attention at only 5% and 3%, respectively.\nAdditionally, Python emerges as the dominant programming language across the\nreviewed benchmarks. Finally, this paper highlights the challenges of current\nresearch and proposes future directions, aiming to narrow the gap between the\ntheoretical capabilities of CodeLLMs and agents and their application in\nreal-world scenarios.", "published": "2025-05-08 14:27:45", "link": "http://arxiv.org/abs/2505.05283v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE"}
{"title": "Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration", "abstract": "Learning to cooperate in distributed partially observable environments with\nno communication abilities poses significant challenges for multi-agent deep\nreinforcement learning (MARL). This paper addresses key concerns in this\ndomain, focusing on inferring state representations from individual agent\nobservations and leveraging these representations to enhance agents'\nexploration and collaborative task execution policies. To this end, we propose\na novel state modelling framework for cooperative MARL, where agents infer\nmeaningful belief representations of the non-observable state, with respect to\noptimizing their own policies, while filtering redundant and less informative\njoint state information. Building upon this framework, we propose the MARL SMPE\nalgorithm. In SMPE, agents enhance their own policy's discriminative abilities\nunder partial observability, explicitly by incorporating their beliefs into the\npolicy network, and implicitly by adopting an adversarial type of exploration\npolicies which encourages agents to discover novel, high-value states while\nimproving the discriminative abilities of others. Experimentally, we show that\nSMPE outperforms state-of-the-art MARL algorithms in complex fully cooperative\ntasks from the MPE, LBF, and RWARE benchmarks.", "published": "2025-05-08 14:07:20", "link": "http://arxiv.org/abs/2505.05262v1", "categories": ["cs.LG", "cs.AI", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Advancing Neural Network Verification through Hierarchical Safety Abstract Interpretation", "abstract": "Traditional methods for formal verification (FV) of deep neural networks\n(DNNs) are constrained by a binary encoding of safety properties, where a model\nis classified as either safe or unsafe (robust or not robust). This binary\nencoding fails to capture the nuanced safety levels within a model, often\nresulting in either overly restrictive or too permissive requirements. In this\npaper, we introduce a novel problem formulation called Abstract\nDNN-Verification, which verifies a hierarchical structure of unsafe outputs,\nproviding a more granular analysis of the safety aspect for a given DNN.\nCrucially, by leveraging abstract interpretation and reasoning about output\nreachable sets, our approach enables assessing multiple safety levels during\nthe FV process, requiring the same (in the worst case) or even potentially less\ncomputational effort than the traditional binary verification approach.\nSpecifically, we demonstrate how this formulation allows rank adversarial\ninputs according to their abstract safety level violation, offering a more\ndetailed evaluation of the model's safety and robustness. Our contributions\ninclude a theoretical exploration of the relationship between our novel\nabstract safety formulation and existing approaches that employ abstract\ninterpretation for robustness verification, complexity analysis of the novel\nproblem introduced, and an empirical evaluation considering both a complex deep\nreinforcement learning task (based on Habitat 3.0) and standard\nDNN-Verification benchmarks.", "published": "2025-05-08 13:29:46", "link": "http://arxiv.org/abs/2505.05235v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "ChemRxivQuest: A Curated Chemistry Question-Answer Database Extracted from ChemRxiv Preprints", "abstract": "The rapid expansion of chemistry literature poses significant challenges for\nresearchers seeking to efficiently access domain-specific knowledge. To support\nadvancements in chemistry-focused natural language processing (NLP), we present\nChemRxivQuest, a curated dataset of 970 high-quality question-answer (QA) pairs\nderived from 155 ChemRxiv preprints across 17 subfields of chemistry. Each QA\npair is explicitly linked to its source text segment to ensure traceability and\ncontextual accuracy. ChemRxivQuest was constructed using an automated pipeline\nthat combines optical character recognition (OCR), GPT-4o-based QA generation,\nand a fuzzy matching technique for answer verification. The dataset emphasizes\nconceptual, mechanistic, applied, and experimental questions, enabling\napplications in retrieval-based QA systems, search engine development, and\nfine-tuning of domain-adapted large language models. We analyze the dataset's\nstructure, coverage, and limitations, and outline future directions for\nexpansion and expert validation. ChemRxivQuest provides a foundational resource\nfor chemistry NLP research, education, and tool development.", "published": "2025-05-08 13:26:33", "link": "http://arxiv.org/abs/2505.05232v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Put CASH on Bandits: A Max K-Armed Problem for Automated Machine Learning", "abstract": "The Combined Algorithm Selection and Hyperparameter optimization (CASH) is a\nchallenging resource allocation problem in the field of AutoML. We propose\nMaxUCB, a max $k$-armed bandit method to trade off exploring different model\nclasses and conducting hyperparameter optimization. MaxUCB is specifically\ndesigned for the light-tailed and bounded reward distributions arising in this\nsetting and, thus, provides an efficient alternative compared to classic max\n$k$-armed bandit methods assuming heavy-tailed reward distributions. We\ntheoretically and empirically evaluate our method on four standard AutoML\nbenchmarks, demonstrating superior performance over prior approaches.", "published": "2025-05-08 13:18:05", "link": "http://arxiv.org/abs/2505.05226v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Incentive-Aware Machine Learning; Robustness, Fairness, Improvement & Causality", "abstract": "The article explores the emerging domain of incentive-aware machine learning\n(ML), which focuses on algorithmic decision-making in contexts where\nindividuals can strategically modify their inputs to influence outcomes. It\ncategorizes the research into three perspectives: robustness, aiming to design\nmodels resilient to \"gaming\"; fairness, analyzing the societal impacts of such\nsystems; and improvement/causality, recognizing situations where strategic\nactions lead to genuine personal or societal improvement. The paper introduces\na unified framework encapsulating models for these perspectives, including\noffline, online, and causal settings, and highlights key challenges such as\ndifferentiating between gaming and improvement and addressing heterogeneity\namong agents. By synthesizing findings from diverse works, we outline\ntheoretical advancements and practical solutions for robust, fair, and\ncausally-informed incentive-aware ML systems.", "published": "2025-05-08 13:04:32", "link": "http://arxiv.org/abs/2505.05211v1", "categories": ["cs.GT", "cs.AI"], "primary_category": "cs.GT"}
{"title": "LAPSO: A Unified Optimization View for Learning-Augmented Power System Operations", "abstract": "With the high penetration of renewables, traditional model-based power system\noperation is challenged to deliver economic, stable, and robust decisions.\nMachine learning has emerged as a powerful modeling tool for capturing complex\ndynamics to address these challenges. However, its separate design often lacks\nsystematic integration with existing methods. To fill the gap, this paper\nproposes a holistic framework of Learning-Augmented Power System Operations\n(LAPSO, pronounced as Lap-So). Adopting a native optimization perspective,\nLAPSO is centered on the operation stage and aims to break the boundary between\ntemporally siloed power system tasks, such as forecast, operation and control,\nwhile unifying the objectives of machine learning and model-based optimizations\nat both training and inference stages. Systematic analysis and simulations\ndemonstrate the effectiveness of applying LAPSO in designing new integrated\nalgorithms, such as stability-constrained optimization (SCO) and\nobjective-based forecasting (OBF), while enabling end-to-end tracing of\ndifferent sources of uncertainties. In addition, a dedicated Python\npackage-lapso is introduced to automatically augment existing power system\noptimization models with learnable components. All code and data are available\nat https://github.com/xuwkk/lapso_exp.", "published": "2025-05-08 13:00:24", "link": "http://arxiv.org/abs/2505.05203v1", "categories": ["eess.SY", "cs.AI", "cs.SY"], "primary_category": "eess.SY"}
{"title": "Societal and technological progress as sewing an ever-growing, ever-changing, patchy, and polychrome quilt", "abstract": "Artificial Intelligence (AI) systems are increasingly placed in positions\nwhere their decisions have real consequences, e.g., moderating online spaces,\nconducting research, and advising on policy. Ensuring they operate in a safe\nand ethically acceptable fashion is thus critical. However, most solutions have\nbeen a form of one-size-fits-all \"alignment\". We are worried that such systems,\nwhich overlook enduring moral diversity, will spark resistance, erode trust,\nand destabilize our institutions. This paper traces the underlying problem to\nan often-unstated Axiom of Rational Convergence: the idea that under ideal\nconditions, rational agents will converge in the limit of conversation on a\nsingle ethics. Treating that premise as both optional and doubtful, we propose\nwhat we call the appropriateness framework: an alternative approach grounded in\nconflict theory, cultural evolution, multi-agent systems, and institutional\neconomics. The appropriateness framework treats persistent disagreement as the\nnormal case and designs for it by applying four principles: (1) contextual\ngrounding, (2) community customization, (3) continual adaptation, and (4)\npolycentric governance. We argue here that adopting these design principles is\na good way to shift the main alignment metaphor from moral unification to a\nmore productive metaphor of conflict management, and that taking this step is\nboth desirable and urgent.", "published": "2025-05-08 12:55:07", "link": "http://arxiv.org/abs/2505.05197v1", "categories": ["cs.AI", "cs.CY"], "primary_category": "cs.AI"}
{"title": "Concept-Based Unsupervised Domain Adaptation", "abstract": "Concept Bottleneck Models (CBMs) enhance interpretability by explaining\npredictions through human-understandable concepts but typically assume that\ntraining and test data share the same distribution. This assumption often fails\nunder domain shifts, leading to degraded performance and poor generalization.\nTo address these limitations and improve the robustness of CBMs, we propose the\nConcept-based Unsupervised Domain Adaptation (CUDA) framework. CUDA is designed\nto: (1) align concept representations across domains using adversarial\ntraining, (2) introduce a relaxation threshold to allow minor domain-specific\ndifferences in concept distributions, thereby preventing performance drop due\nto over-constraints of these distributions, (3) infer concepts directly in the\ntarget domain without requiring labeled concept data, enabling CBMs to adapt to\ndiverse domains, and (4) integrate concept learning into conventional domain\nadaptation (DA) with theoretical guarantees, improving interpretability and\nestablishing new benchmarks for DA. Experiments demonstrate that our approach\nsignificantly outperforms the state-of-the-art CBM and DA methods on real-world\ndatasets.", "published": "2025-05-08 12:52:02", "link": "http://arxiv.org/abs/2505.05195v1", "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Biomed-DPT: Dual Modality Prompt Tuning for Biomedical Vision-Language Models", "abstract": "Prompt learning is one of the most effective paradigms for adapting\npre-trained vision-language models (VLMs) to the biomedical image\nclassification tasks in few shot scenarios. However, most of the current prompt\nlearning methods only used the text prompts and ignored the particular\nstructures (such as the complex anatomical structures and subtle pathological\nfeatures) in the biomedical images. In this work, we propose Biomed-DPT, a\nknowledge-enhanced dual modality prompt tuning technique. In designing the text\nprompt, Biomed-DPT constructs a dual prompt including the template-driven\nclinical prompts and the large language model (LLM)-driven domain-adapted\nprompts, then extracts the clinical knowledge from the domain-adapted prompts\nthrough the knowledge distillation technique. In designing the vision prompt,\nBiomed-DPT introduces the zero vector as a soft prompt to leverage attention\nre-weighting so that the focus on non-diagnostic regions and the recognition of\nnon-critical pathological features are avoided. Biomed-DPT achieves an average\nclassification accuracy of 66.14\\% across 11 biomedical image datasets covering\n9 modalities and 10 organs, with performance reaching 78.06\\% in base classes\nand 75.97\\% in novel classes, surpassing the Context Optimization (CoOp) method\nby 6.20\\%, 3.78\\%, and 8.04\\%, respectively. Our code are available at\n\\underline{https://github.com/Kanyooo/Biomed-DPT}.", "published": "2025-05-08 12:37:51", "link": "http://arxiv.org/abs/2505.05189v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Stochastic Variational Propagation: Local, Scalable and Efficient Alternative to Backpropagation", "abstract": "Backpropagation (BP) is the cornerstone of deep learning, but its reliance on\nglobal gradient synchronization limits scalability and imposes significant\nmemory overhead. We propose Stochastic Variational Propagation (SVP), a\nscalable alternative that reframes training as hierarchical variational\ninference. SVP treats layer activations as latent variables and optimizes local\nEvidence Lower Bounds (ELBOs), enabling independent, local updates while\npreserving global coherence. However, directly applying KL divergence in\nlayer-wise ELBOs risks inter-layer's representation collapse due to excessive\ncompression. To prevent this, SVP projects activations into low-dimensional\nspaces via fixed random matrices, ensuring information preservation and\nrepresentational diversity. Combined with a feature alignment loss for\ninter-layer consistency, SVP achieves competitive accuracy with BP across\ndiverse architectures (MLPs, CNNs, Transformers) and datasets (MNIST to\nImageNet), reduces memory usage by up to 4x, and significantly improves\nscalability. More broadly, SVP introduces a probabilistic perspective to deep\nrepresentation learning, opening pathways toward more modular and interpretable\nneural network design.", "published": "2025-05-08 12:32:29", "link": "http://arxiv.org/abs/2505.05181v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "MARK: Memory Augmented Refinement of Knowledge", "abstract": "Large Language Models (LLMs) assist in specialized tasks but struggle to\nalign with evolving domain knowledge without costly fine-tuning. Domain\nknowledge consists of: Knowledge: Immutable facts (e.g., 'A stone is solid')\nand generally accepted principles (e.g., ethical standards); Refined Memory:\nEvolving insights shaped by business needs and real-world changes. However, a\nsignificant gap often exists between a domain expert's deep, nuanced\nunderstanding and the system's domain knowledge, which can hinder accurate\ninformation retrieval and application. Our Memory-Augmented Refinement of\nKnowledge (MARK) framework enables LLMs to continuously learn without\nretraining by leveraging structured refined memory, inspired by the Society of\nMind. MARK operates through specialized agents, each serving a distinct role:\nResidual Refined Memory Agent: Stores and retrieves domain-specific insights to\nmaintain context over time; User Question Refined Memory Agent: Captures\nuser-provided facts, abbreviations, and terminology for better comprehension;\nLLM Response Refined Memory Agent: Extracts key elements from responses for\nrefinement and personalization. These agents analyse stored refined memory,\ndetect patterns, resolve contradictions, and improve response accuracy.\nTemporal factors like recency and frequency prioritize relevant information\nwhile discarding outdated insights. MARK enhances LLMs in multiple ways: Ground\nTruth Strategy: Reduces hallucinations by establishing a structured reference;\nDomain-Specific Adaptation: Essential for fields like healthcare, law, and\nmanufacturing, where proprietary insights are absent from public datasets;\nPersonalized AI Assistants: Improves virtual assistants by remembering user\npreferences, ensuring coherent responses over time.", "published": "2025-05-08 12:28:00", "link": "http://arxiv.org/abs/2505.05177v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Dukawalla: Voice Interfaces for Small Businesses in Africa", "abstract": "Small and medium sized businesses often struggle with data driven decision\nmaking do to a lack of advanced analytics tools, especially in African\ncountries where they make up a majority of the workforce. Though many tools\nexist they are not designed to fit into the ways of working of SMB workers who\nare mobile first, have limited time to learn new workflows, and for whom social\nand business are tightly coupled. To address this, the Dukawalla prototype was\ncreated. This intelligent assistant bridges the gap between raw business data,\nand actionable insights by leveraging voice interaction and the power of\ngenerative AI. Dukawalla provides an intuitive way for business owners to\ninteract with their data, aiding in informed decision making. This paper\nexamines Dukawalla's deployment across SMBs in Nairobi, focusing on their\nexperiences using this voice based assistant to streamline data collection and\nprovide business insights", "published": "2025-05-08 12:13:16", "link": "http://arxiv.org/abs/2505.05170v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC"}
{"title": "Guiding Evolutionary AutoEncoder Training with Activation-Based Pruning Operators", "abstract": "This study explores a novel approach to neural network pruning using\nevolutionary computation, focusing on simultaneously pruning the encoder and\ndecoder of an autoencoder. We introduce two new mutation operators that use\nlayer activations to guide weight pruning. Our findings reveal that one of\nthese activation-informed operators outperforms random pruning, resulting in\nmore efficient autoencoders with comparable performance to canonically trained\nmodels. Prior work has established that autoencoder training is effective and\nscalable with a spatial coevolutionary algorithm that cooperatively coevolves a\npopulation of encoders with a population of decoders, rather than one\nautoencoder. We evaluate how the same activity-guided mutation operators\ntransfer to this context. We find that random pruning is better than guided\npruning, in the coevolutionary setting. This suggests activation-based guidance\nproves more effective in low-dimensional pruning environments, where\nconstrained sample spaces can lead to deviations from true uniformity in\nrandomization. Conversely, population-driven strategies enhance robustness by\nexpanding the total pruning dimensionality, achieving statistically uniform\nrandomness that better preserves system dynamics. We experiment with pruning\naccording to different schedules and present best combinations of operator and\nschedule for the canonical and coevolving populations cases.", "published": "2025-05-08 11:21:29", "link": "http://arxiv.org/abs/2505.05138v1", "categories": ["cs.NE", "cs.AI"], "primary_category": "cs.NE"}
{"title": "Is there a half-life for the success rates of AI agents?", "abstract": "Building on the recent empirical work of Kwa et al. (2025), I show that\nwithin their suite of research-engineering tasks the performance of AI agents\non longer-duration tasks can be explained by an extremely simple mathematical\nmodel -- a constant rate of failing during each minute a human would take to do\nthe task. This implies an exponentially declining success rate with the length\nof the task and that each agent could be characterised by its own half-life.\nThis empirical regularity allows us to estimate the success rate for an agent\nat different task lengths. And the fact that this model is a good fit for the\ndata is suggestive of the underlying causes of failure on longer tasks -- that\nthey involve increasingly large sets of subtasks where failing any one fails\nthe task. Whether this model applies more generally on other suites of tasks is\nunknown and an important subject for further work.", "published": "2025-05-08 10:31:03", "link": "http://arxiv.org/abs/2505.05115v1", "categories": ["cs.AI", "68T42", "I.2.8"], "primary_category": "cs.AI"}
{"title": "Multi-agent Embodied AI: Advances and Future Directions", "abstract": "Embodied artificial intelligence (Embodied AI) plays a pivotal role in the\napplication of advanced technologies in the intelligent era, where AI systems\nare integrated with physical bodies that enable them to perceive, reason, and\ninteract with their environments. Through the use of sensors for input and\nactuators for action, these systems can learn and adapt based on real-world\nfeedback, allowing them to perform tasks effectively in dynamic and\nunpredictable environments. As techniques such as deep learning (DL),\nreinforcement learning (RL), and large language models (LLMs) mature, embodied\nAI has become a leading field in both academia and industry, with applications\nspanning robotics, healthcare, transportation, and manufacturing. However, most\nresearch has focused on single-agent systems that often assume static, closed\nenvironments, whereas real-world embodied AI must navigate far more complex\nscenarios. In such settings, agents must not only interact with their\nsurroundings but also collaborate with other agents, necessitating\nsophisticated mechanisms for adaptation, real-time learning, and collaborative\nproblem-solving. Despite increasing interest in multi-agent systems, existing\nresearch remains narrow in scope, often relying on simplified models that fail\nto capture the full complexity of dynamic, open environments for multi-agent\nembodied AI. Moreover, no comprehensive survey has systematically reviewed the\nadvancements in this area. As embodied AI rapidly evolves, it is crucial to\ndeepen our understanding of multi-agent embodied AI to address the challenges\npresented by real-world applications. To fill this gap and foster further\ndevelopment in the field, this paper reviews the current state of research,\nanalyzes key contributions, and identifies challenges and future directions,\nproviding insights to guide innovation and progress in this field.", "published": "2025-05-08 10:13:53", "link": "http://arxiv.org/abs/2505.05108v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI"}
{"title": "A Neuro-Symbolic Framework for Sequence Classification with Relational and Temporal Knowledge", "abstract": "One of the goals of neuro-symbolic artificial intelligence is to exploit\nbackground knowledge to improve the performance of learning tasks. However,\nmost of the existing frameworks focus on the simplified scenario where\nknowledge does not change over time and does not cover the temporal dimension.\nIn this work we consider the much more challenging problem of knowledge-driven\nsequence classification where different portions of knowledge must be employed\nat different timesteps, and temporal relations are available. Our experimental\nevaluation compares multi-stage neuro-symbolic and neural-only architectures,\nand it is conducted on a newly-introduced benchmarking framework. Results\ndemonstrate the challenging nature of this novel setting, and also highlight\nunder-explored shortcomings of neuro-symbolic methods, representing a precious\nreference for future research.", "published": "2025-05-08 10:10:00", "link": "http://arxiv.org/abs/2505.05106v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Beyond Low-rank Decomposition: A Shortcut Approach for Efficient On-Device Learning", "abstract": "On-device learning has emerged as a promising direction for AI development,\nparticularly because of its potential to reduce latency issues and mitigate\nprivacy risks associated with device-server communication, while improving\nenergy efficiency. Despite these advantages, significant memory and\ncomputational constraints still represent major challenges for its deployment.\nDrawing on previous studies on low-rank decomposition methods that address\nactivation memory bottlenecks in backpropagation, we propose a novel shortcut\napproach as an alternative. Our analysis and experiments demonstrate that our\nmethod can reduce activation memory usage, even up to $120.09\\times$ compared\nto vanilla training, while also reducing overall training FLOPs up to\n$1.86\\times$ when evaluated on traditional benchmarks.", "published": "2025-05-08 09:34:15", "link": "http://arxiv.org/abs/2505.05086v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "FG-CLIP: Fine-Grained Visual and Textual Alignment", "abstract": "Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks\nsuch as image-text retrieval and zero-shot classification but struggles with\nfine-grained understanding due to its focus on coarse-grained short captions.\nTo address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances\nfine-grained understanding through three key innovations. First, we leverage\nlarge multimodal models to generate 1.6 billion long caption-image pairs for\ncapturing global-level semantic details. Second, a high-quality dataset is\nconstructed with 12 million images and 40 million region-specific bounding\nboxes aligned with detailed captions to ensure precise, context-rich\nrepresentations. Third, 10 million hard fine-grained negative samples are\nincorporated to improve the model's ability to distinguish subtle semantic\ndifferences. Corresponding training methods are meticulously designed for these\ndata. Extensive experiments demonstrate that FG-CLIP outperforms the original\nCLIP and other state-of-the-art methods across various downstream tasks,\nincluding fine-grained understanding, open-vocabulary object detection,\nimage-text retrieval, and general multimodal benchmarks. These results\nhighlight FG-CLIP's effectiveness in capturing fine-grained image details and\nimproving overall model performance. The related data, code, and models are\navailable at https://github.com/360CVGroup/FG-CLIP.", "published": "2025-05-08 09:06:53", "link": "http://arxiv.org/abs/2505.05071v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Enhancing Reinforcement Learning for the Floorplanning of Analog ICs with Beam Search", "abstract": "The layout of analog ICs requires making complex trade-offs, while addressing\ndevice physics and variability of the circuits. This makes full automation with\nlearning-based solutions hard to achieve. However, reinforcement learning (RL)\nhas recently reached significant results, particularly in solving the\nfloorplanning problem. This paper presents a hybrid method that combines RL\nwith a beam (BS) strategy. The BS algorithm enhances the agent's inference\nprocess, allowing for the generation of flexible floorplans by accomodating\nvarious objective weightings, and addressing congestion without without the\nneed for policy retraining or fine-tuning. Moreover, the RL agent's\ngeneralization ability stays intact, along with its efficient handling of\ncircuit features and constraints. Experimental results show approx. 5-85%\nimprovement in area, dead space and half-perimeter wire length compared to a\nstandard RL application, along with higher rewards for the agent. Moreover,\nperformance and efficiency align closely with those of existing\nstate-of-the-art techniques.", "published": "2025-05-08 08:50:32", "link": "http://arxiv.org/abs/2505.05059v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Direct Image Classification from Fourier Ptychographic Microscopy Measurements without Reconstruction", "abstract": "The computational imaging technique of Fourier Ptychographic Microscopy (FPM)\nenables high-resolution imaging with a wide field of view and can serve as an\nextremely valuable tool, e.g. in the classification of cells in medical\napplications. However, reconstructing a high-resolution image from tens or even\nhundreds of measurements is computationally expensive, particularly for a wide\nfield of view. Therefore, in this paper, we investigate the idea of classifying\nthe image content in the FPM measurements directly without performing a\nreconstruction step first. We show that Convolutional Neural Networks (CNN) can\nextract meaningful information from measurement sequences, significantly\noutperforming the classification on a single band-limited image (up to 12 %)\nwhile being significantly more efficient than a reconstruction of a\nhigh-resolution image. Furthermore, we demonstrate that a learned multiplexing\nof several raw measurements allows maintaining the classification accuracy\nwhile reducing the amount of data (and consequently also the acquisition time)\nsignificantly.", "published": "2025-05-08 08:46:28", "link": "http://arxiv.org/abs/2505.05054v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "eess.IV"}
{"title": "A Reputation System for Large Language Model-based Multi-agent Systems to Avoid the Tragedy of the Commons", "abstract": "The tragedy of the commons, where individual self-interest leads to\ncollectively disastrous outcomes, is a pervasive challenge in human society.\nRecent studies have demonstrated that similar phenomena can arise in generative\nmulti-agent systems (MASs). To address this challenge, this paper explores the\nuse of reputation systems as a remedy. We propose RepuNet, a dynamic,\ndual-level reputation framework that models both agent-level reputation\ndynamics and system-level network evolution. Specifically, driven by direct\ninteractions and indirect gossip, agents form reputations for both themselves\nand their peers, and decide whether to connect or disconnect other agents for\nfuture interactions. Through two distinct scenarios, we show that RepuNet\neffectively mitigates the 'tragedy of the commons', promoting and sustaining\ncooperation in generative MASs. Moreover, we find that reputation systems can\ngive rise to rich emergent behaviors in generative MASs, such as the formation\nof cooperative clusters, the social isolation of exploitative agents, and the\npreference for sharing positive gossip rather than negative ones.", "published": "2025-05-08 08:02:20", "link": "http://arxiv.org/abs/2505.05029v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Generating Reliable Synthetic Clinical Trial Data: The Role of Hyperparameter Optimization and Domain Constraints", "abstract": "The generation of synthetic clinical trial data offers a promising approach\nto mitigating privacy concerns and data accessibility limitations in medical\nresearch. However, ensuring that synthetic datasets maintain high fidelity,\nutility, and adherence to domain-specific constraints remains a key challenge.\nWhile hyperparameter optimization (HPO) has been shown to improve generative\nmodel performance, the effectiveness of different optimization strategies for\nsynthetic clinical data remains unclear. This study systematically evaluates\nfour HPO strategies across eight generative models, comparing single-metric\noptimization against compound metric optimization approaches. Our results\ndemonstrate that HPO consistently improves synthetic data quality, with TVAE,\nCTGAN, and CTAB-GAN+ achieving improvements of up to 60%, 39%, and 38%,\nrespectively. Compound metric optimization outperformed single-metric\nstrategies, producing more balanced and generalizable synthetic datasets.\nInterestingly, HPO alone is insufficient to ensure clinically valid synthetic\ndata, as all models exhibited violations of fundamental survival constraints.\nPreprocessing and postprocessing played a crucial role in reducing these\nviolations, as models lacking robust processing steps produced invalid data in\nup to 61% of cases. These findings underscore the necessity of integrating\nexplicit domain knowledge alongside HPO to create high quality synthetic\ndatasets. Our study provides actionable recommendations for improving synthetic\ndata generation, with future research needed to refine metric selection and\nvalidate these findings on larger datasets to enhance clinical applicability.", "published": "2025-05-08 07:51:36", "link": "http://arxiv.org/abs/2505.05019v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "An Agent-Based Modeling Approach to Free-Text Keyboard Dynamics for Continuous Authentication", "abstract": "Continuous authentication systems leveraging free-text keyboard dynamics\noffer a promising additional layer of security in a multifactor authentication\nsetup that can be used in a transparent way with no impact on user experience.\nThis study investigates the efficacy of behavioral biometrics by employing an\nAgent-Based Model (ABM) to simulate diverse typing profiles across mechanical\nand membrane keyboards. Specifically, we generated synthetic keystroke data\nfrom five unique agents, capturing features related to dwell time, flight time,\nand error rates within sliding 5-second windows updated every second. Two\nmachine learning approaches, One-Class Support Vector Machine (OC-SVM) and\nRandom Forest (RF), were evaluated for user verification. Results revealed a\nstark contrast in performance: while One-Class SVM failed to differentiate\nindividual users within each group, Random Forest achieved robust\nintra-keyboard user recognition (Accuracy > 0.7) but struggled to generalize\nacross keyboards for the same user, highlighting the significant impact of\nkeyboard hardware on typing behavior. These findings suggest that: (1)\nkeyboard-specific user profiles may be necessary for reliable authentication,\nand (2) ensemble methods like RF outperform One-Class SVM in capturing\nfine-grained user-specific patterns.", "published": "2025-05-08 07:42:05", "link": "http://arxiv.org/abs/2505.05015v1", "categories": ["cs.LG", "cs.AI", "cs.CR", "68T10, 62H30", "I.2.6; I.5.4; I.6.3"], "primary_category": "cs.LG"}
{"title": "StabStitch++: Unsupervised Online Video Stitching with Spatiotemporal Bidirectional Warps", "abstract": "We retarget video stitching to an emerging issue, named warping shake, which\nunveils the temporal content shakes induced by sequentially unsmooth warps when\nextending image stitching to video stitching. Even if the input videos are\nstable, the stitched video can inevitably cause undesired warping shakes and\naffect the visual experience. To address this issue, we propose StabStitch++, a\nnovel video stitching framework to realize spatial stitching and temporal\nstabilization with unsupervised learning simultaneously. First, different from\nexisting learning-based image stitching solutions that typically warp one image\nto align with another, we suppose a virtual midplane between original image\nplanes and project them onto it. Concretely, we design a differentiable\nbidirectional decomposition module to disentangle the homography transformation\nand incorporate it into our spatial warp, evenly spreading alignment burdens\nand projective distortions across two views. Then, inspired by camera paths in\nvideo stabilization, we derive the mathematical expression of stitching\ntrajectories in video stitching by elaborately integrating spatial and temporal\nwarps. Finally, a warp smoothing model is presented to produce stable stitched\nvideos with a hybrid loss to simultaneously encourage content alignment,\ntrajectory smoothness, and online collaboration. Compared with StabStitch that\nsacrifices alignment for stabilization, StabStitch++ makes no compromise and\noptimizes both of them simultaneously, especially in the online mode. To\nestablish an evaluation benchmark and train the learning framework, we build a\nvideo stitching dataset with a rich diversity in camera motions and scenes.\nExperiments exhibit that StabStitch++ surpasses current solutions in stitching\nperformance, robustness, and efficiency, offering compelling advancements in\nthis field by building a real-time online video stitching system.", "published": "2025-05-08 07:12:23", "link": "http://arxiv.org/abs/2505.05001v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Foam-Agent: Towards Automated Intelligent CFD Workflows", "abstract": "Computational Fluid Dynamics (CFD) is an essential simulation tool in various\nengineering disciplines, but it often requires substantial domain expertise and\nmanual configuration, creating barriers to entry. We present Foam-Agent, a\nmulti-agent framework that automates complex OpenFOAM-based CFD simulation\nworkflows from natural language inputs. Our innovation includes (1) a\nhierarchical multi-index retrieval system with specialized indices for\ndifferent simulation aspects, (2) a dependency-aware file generation system\nthat provides consistency management across configuration files, and (3) an\niterative error correction mechanism that diagnoses and resolves simulation\nfailures without human intervention. Through comprehensive evaluation on the\ndataset of 110 simulation tasks, Foam-Agent achieves an 83.6% success rate with\nClaude 3.5 Sonnet, significantly outperforming existing frameworks (55.5% for\nMetaOpenFOAM and 37.3% for OpenFOAM-GPT). Ablation studies demonstrate the\ncritical contribution of each system component, with the specialized error\ncorrection mechanism providing a 36.4% performance improvement. Foam-Agent\nsubstantially lowers the CFD expertise threshold while maintaining modeling\naccuracy, demonstrating the potential of specialized multi-agent systems to\ndemocratize access to complex scientific simulation tools. The code is public\nat https://github.com/csml-rpi/Foam-Agent", "published": "2025-05-08 07:05:51", "link": "http://arxiv.org/abs/2505.04997v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Decomposition of Probabilities of Causation with Two Mediators", "abstract": "Mediation analysis for probabilities of causation (PoC) provides a\nfundamental framework for evaluating the necessity and sufficiency of treatment\nin provoking an event through different causal pathways. One of the primary\nobjectives of causal mediation analysis is to decompose the total effect into\npath-specific components. In this study, we investigate the path-specific\nprobability of necessity and sufficiency (PNS) to decompose the total PNS into\npath-specific components along distinct causal pathways between treatment and\noutcome, incorporating two mediators. We define the path-specific PNS for\ndecomposition and provide an identification theorem. Furthermore, we conduct\nnumerical experiments to assess the properties of the proposed estimators from\nfinite samples and demonstrate their practical application using a real-world\neducational dataset.", "published": "2025-05-08 06:40:17", "link": "http://arxiv.org/abs/2505.04983v1", "categories": ["stat.ME", "cs.AI"], "primary_category": "stat.ME"}
{"title": "ChainMarks: Securing DNN Watermark with Cryptographic Chain", "abstract": "With the widespread deployment of deep neural network (DNN) models, dynamic\nwatermarking techniques are being used to protect the intellectual property of\nmodel owners. However, recent studies have shown that existing watermarking\nschemes are vulnerable to watermark removal and ambiguity attacks. Besides, the\nvague criteria for determining watermark presence further increase the\nlikelihood of such attacks. In this paper, we propose a secure DNN watermarking\nscheme named ChainMarks, which generates secure and robust watermarks by\nintroducing a cryptographic chain into the trigger inputs and utilizes a\ntwo-phase Monte Carlo method for determining watermark presence. First,\nChainMarks generates trigger inputs as a watermark dataset by repeatedly\napplying a hash function over a secret key, where the target labels associated\nwith trigger inputs are generated from the digital signature of model owner.\nThen, the watermarked model is produced by training a DNN over both the\noriginal and watermark datasets. To verify watermarks, we compare the predicted\nlabels of trigger inputs with the target labels and determine ownership with a\nmore accurate decision threshold that considers the classification probability\nof specific models. Experimental results show that ChainMarks exhibits higher\nlevels of robustness and security compared to state-of-the-art watermarking\nschemes. With a better marginal utility, ChainMarks provides a higher\nprobability guarantee of watermark presence in DNN models with the same level\nof watermark accuracy.", "published": "2025-05-08 06:30:46", "link": "http://arxiv.org/abs/2505.04977v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "AI and Vision based Autonomous Navigation of Nano-Drones in Partially-Known Environments", "abstract": "The miniaturisation of sensors and processors, the advancements in connected\nedge intelligence, and the exponential interest in Artificial Intelligence are\nboosting the affirmation of autonomous nano-size drones in the Internet of\nRobotic Things ecosystem. However, achieving safe autonomous navigation and\nhigh-level tasks such as exploration and surveillance with these tiny platforms\nis extremely challenging due to their limited resources. This work focuses on\nenabling the safe and autonomous flight of a pocket-size, 30-gram platform\ncalled Crazyflie 2.1 in a partially known environment. We propose a novel\nAI-aided, vision-based reactive planning method for obstacle avoidance under\nthe ambit of Integrated Sensing, Computing and Communication paradigm. We deal\nwith the constraints of the nano-drone by splitting the navigation task into\ntwo parts: a deep learning-based object detector runs on the edge (external\nhardware) while the planning algorithm is executed onboard. The results show\nthe ability to command the drone at $\\sim8$ frames-per-second and a model\nperformance reaching a COCO mean-average-precision of $60.8$. Field experiments\ndemonstrate the feasibility of the solution with the drone flying at a top\nspeed of $1$ m/s while steering away from an obstacle placed in an unknown\nposition and reaching the target destination. The outcome highlights the\ncompatibility of the communication delay and the model performance with the\nrequirements of the real-time navigation task. We provide a feasible\nalternative to a fully onboard implementation that can be extended to\nautonomous exploration with nano-drones.", "published": "2025-05-08 06:16:36", "link": "http://arxiv.org/abs/2505.04972v1", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.NI"], "primary_category": "cs.RO"}
{"title": "Moments of Causal Effects", "abstract": "The moments of random variables are fundamental statistical measures for\ncharacterizing the shape of a probability distribution, encompassing metrics\nsuch as mean, variance, skewness, and kurtosis. Additionally, the product\nmoments, including covariance and correlation, reveal the relationships between\nmultiple random variables. On the other hand, the primary focus of causal\ninference is the evaluation of causal effects, which are defined as the\ndifference between two potential outcomes. While traditional causal effect\nassessment focuses on the average causal effect, this work provides\ndefinitions, identification theorems, and bounds for moments and product\nmoments of causal effects to analyze their distribution and relationships. We\nconduct experiments to illustrate the estimation of the moments of causal\neffects from finite samples and demonstrate their practical application using a\nreal-world medical dataset.", "published": "2025-05-08 06:09:05", "link": "http://arxiv.org/abs/2505.04971v1", "categories": ["stat.ME", "cs.AI"], "primary_category": "stat.ME"}
{"title": "Position: The AI Conference Peer Review Crisis Demands Author Feedback and Reviewer Rewards", "abstract": "The peer review process in major artificial intelligence (AI) conferences\nfaces unprecedented challenges with the surge of paper submissions (exceeding\n10,000 submissions per venue), accompanied by growing concerns over review\nquality and reviewer responsibility. This position paper argues for the need to\ntransform the traditional one-way review system into a bi-directional feedback\nloop where authors evaluate review quality and reviewers earn formal\naccreditation, creating an accountability framework that promotes a\nsustainable, high-quality peer review system. The current review system can be\nviewed as an interaction between three parties: the authors, reviewers, and\nsystem (i.e., conference), where we posit that all three parties share\nresponsibility for the current problems. However, issues with authors can only\nbe addressed through policy enforcement and detection tools, and ethical\nconcerns can only be corrected through self-reflection. As such, this paper\nfocuses on reforming reviewer accountability with systematic rewards through\ntwo key mechanisms: (1) a two-stage bi-directional review system that allows\nauthors to evaluate reviews while minimizing retaliatory behavior, (2)a\nsystematic reviewer reward system that incentivizes quality reviewing. We ask\nfor the community's strong interest in these problems and the reforms that are\nneeded to enhance the peer review process.", "published": "2025-05-08 05:51:48", "link": "http://arxiv.org/abs/2505.04966v1", "categories": ["cs.AI", "cs.CY"], "primary_category": "cs.AI"}
{"title": "ADD: Physics-Based Motion Imitation with Adversarial Differential Discriminators", "abstract": "Multi-objective optimization problems, which require the simultaneous\noptimization of multiple terms, are prevalent across numerous applications.\nExisting multi-objective optimization methods often rely on manually tuned\naggregation functions to formulate a joint optimization target. The performance\nof such hand-tuned methods is heavily dependent on careful weight selection, a\ntime-consuming and laborious process. These limitations also arise in the\nsetting of reinforcement-learning-based motion tracking for physically\nsimulated characters, where intricately crafted reward functions are typically\nused to achieve high-fidelity results. Such solutions not only require domain\nexpertise and significant manual adjustment, but also limit the applicability\nof the resulting reward function across diverse skills. To bridge this gap, we\npresent a novel adversarial multi-objective optimization technique that is\nbroadly applicable to a range of multi-objective optimization problems,\nincluding motion tracking. The proposed adversarial differential discriminator\nreceives a single positive sample, yet is still effective at guiding the\noptimization process. We demonstrate that our technique can enable characters\nto closely replicate a variety of acrobatic and agile behaviors, achieving\ncomparable quality to state-of-the-art motion-tracking methods, without relying\non manually tuned reward functions. Results are best visualized through\nhttps://youtu.be/rz8BYCE9E2w.", "published": "2025-05-08 05:42:33", "link": "http://arxiv.org/abs/2505.04961v1", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.RO"], "primary_category": "cs.GR"}
{"title": "Graffe: Graph Representation Learning via Diffusion Probabilistic Models", "abstract": "Diffusion probabilistic models (DPMs), widely recognized for their potential\nto generate high-quality samples, tend to go unnoticed in representation\nlearning. While recent progress has highlighted their potential for capturing\nvisual semantics, adapting DPMs to graph representation learning remains in its\ninfancy. In this paper, we introduce Graffe, a self-supervised diffusion model\nproposed for graph representation learning. It features a graph encoder that\ndistills a source graph into a compact representation, which, in turn, serves\nas the condition to guide the denoising process of the diffusion decoder. To\nevaluate the effectiveness of our model, we first explore the theoretical\nfoundations of applying diffusion models to representation learning, proving\nthat the denoising objective implicitly maximizes the conditional mutual\ninformation between data and its representation. Specifically, we prove that\nthe negative logarithm of the denoising score matching loss is a tractable\nlower bound for the conditional mutual information. Empirically, we conduct a\nseries of case studies to validate our theoretical insights. In addition,\nGraffe delivers competitive results under the linear probing setting on node\nand graph classification tasks, achieving state-of-the-art performance on 9 of\nthe 11 real-world datasets. These findings indicate that powerful generative\nmodels, especially diffusion models, serve as an effective tool for graph\nrepresentation learning.", "published": "2025-05-08 05:38:19", "link": "http://arxiv.org/abs/2505.04956v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Position: Epistemic Artificial Intelligence is Essential for Machine Learning Models to Know When They Do Not Know", "abstract": "Despite the impressive achievements of AI, including advancements in\ngenerative models and large language models, there remains a significant gap in\nthe ability of AI to handle uncertainty and generalize beyond the training\ndata. We argue that AI models, especially in autonomous systems, fail to make\nrobust predictions when faced with unfamiliar or adversarial data, as evidenced\nby incidents with autonomous vehicles. Traditional machine learning approaches\nstruggle to address these issues due to an overemphasis on data fitting and\ndomain adaptation. This position paper posits a paradigm shift towards\nepistemic artificial intelligence, emphasizing the need for models to learn not\nonly from what they know but also from their ignorance. This approach, which\nfocuses on recognizing and managing uncertainty, offers a potential solution to\nimprove the resilience and robustness of AI systems, ensuring that they can\nbetter handle unpredictable real-world environments.", "published": "2025-05-08 05:10:38", "link": "http://arxiv.org/abs/2505.04950v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Structural Alignment in Link Prediction", "abstract": "While Knowledge Graphs (KGs) have become increasingly popular across various\nscientific disciplines for their ability to model and interlink huge quantities\nof data, essentially all real-world KGs are known to be incomplete. As such,\nwith the growth of KG use has been a concurrent development of machine learning\ntools designed to predict missing information in KGs, which is referred to as\nthe Link Prediction Task. The majority of state-of-the-art link predictors to\ndate have followed an embedding-based paradigm. In this paradigm, it is assumed\nthat the information content of a KG is best represented by the (individual)\nvector representations of its nodes and edges, and that therefore node and edge\nembeddings are particularly well-suited to performing link prediction.\n  This thesis proposes an alternative perspective on the field's approach to\nlink prediction and KG data modelling. Specifically, this work re-analyses KGs\nand state-of-the-art link predictors from a graph-structure-first perspective\nthat models the information content of a KG in terms of whole triples, rather\nthan individual nodes and edges.\n  Following a literature review and two core sets of experiments, this thesis\nconcludes that a structure-first perspective on KGs and link prediction is both\nviable and useful for understanding KG learning and for enabling cross-KG\ntransfer learning for the link prediction task. This observation is used to\ncreate and propose the Structural Alignment Hypothesis, which postulates that\nlink prediction can be understood and modelled as a structural task.\n  All code and data used for this thesis are open-sourced. This thesis was\nwritten bilingually, with the main document in English and an informal extended\nsummary in Irish. An Irish-language translation dictionary of machine learning\nterms (the Focl\\'oir Tr\\'achtais) created for this work is open-sourced as\nwell.", "published": "2025-05-08 04:27:15", "link": "http://arxiv.org/abs/2505.04939v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Fair Uncertainty Quantification for Depression Prediction", "abstract": "Trustworthy depression prediction based on deep learning, incorporating both\npredictive reliability and algorithmic fairness across diverse demographic\ngroups, is crucial for clinical application. Recently, achieving reliable\ndepression predictions through uncertainty quantification has attracted\nincreasing attention. However, few studies have focused on the fairness of\nuncertainty quantification (UQ) in depression prediction. In this work, we\ninvestigate the algorithmic fairness of UQ, namely Equal Opportunity Coverage\n(EOC) fairness, and propose Fair Uncertainty Quantification (FUQ) for\ndepression prediction. FUQ pursues reliable and fair depression predictions\nthrough group-based analysis. Specifically, we first group all the participants\nby different sensitive attributes and leverage conformal prediction to quantify\nuncertainty within each demographic group, which provides a theoretically\nguaranteed and valid way to quantify uncertainty for depression prediction and\nfacilitates the investigation of fairness across different demographic groups.\nFurthermore, we propose a fairness-aware optimization strategy that formulates\nfairness as a constrained optimization problem under EOC constraints. This\nenables the model to preserve predictive reliability while adapting to the\nheterogeneous uncertainty levels across demographic groups, thereby achieving\noptimal fairness. Through extensive evaluations on several visual and audio\ndepression datasets, our approach demonstrates its effectiveness.", "published": "2025-05-08 04:09:36", "link": "http://arxiv.org/abs/2505.04931v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Belief Filtering for Epistemic Control in Linguistic State Space", "abstract": "We examine belief filtering as a mechanism for the epistemic control of\nartificial agents, focusing on the regulation of internal cognitive states\nrepresented as linguistic expressions. This mechanism is developed within the\nSemantic Manifold framework, where belief states are dynamic, structured\nensembles of natural language fragments. Belief filters act as content-aware\noperations on these fragments across various cognitive transitions. This paper\nillustrates how the inherent interpretability and modularity of such a\nlinguistically-grounded cognitive architecture directly enable belief\nfiltering, offering a principled approach to agent regulation. The study\nhighlights the potential for enhancing AI safety and alignment through\nstructured interventions in an agent's internal semantic space and points to\nnew directions for architecturally embedded cognitive governance.", "published": "2025-05-08 03:52:43", "link": "http://arxiv.org/abs/2505.04927v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction", "abstract": "Although deep learning models have demonstrated remarkable potential in\nweather prediction, most of them overlook either the \\textbf{physics} of the\nunderlying weather evolution or the \\textbf{topology} of the Earth's surface.\nIn light of these disadvantages, we develop PASSAT, a novel Physics-ASSisted\nAnd Topology-informed deep learning model for weather prediction. PASSAT\nattributes the weather evolution to two key factors: (i) the advection process\nthat can be characterized by the advection equation and the Navier-Stokes\nequation; (ii) the Earth-atmosphere interaction that is difficult to both model\nand calculate. PASSAT also takes the topology of the Earth's surface into\nconsideration, other than simply treating it as a plane. With these\nconsiderations, PASSAT numerically solves the advection equation and the\nNavier-Stokes equation on the spherical manifold, utilizes a spherical graph\nneural network to capture the Earth-atmosphere interaction, and generates the\ninitial velocity fields that are critical to solving the advection equation\nfrom the same spherical graph neural network. In the $5.625^\\circ$-resolution\nERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-based\nweather prediction models and the operational numerical weather prediction\nmodel IFS T42. Code and checkpoint are available at\nhttps://github.com/Yumenomae/PASSAT_5p625.", "published": "2025-05-08 03:25:55", "link": "http://arxiv.org/abs/2505.04918v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Precise gradient descent training dynamics for finite-width multi-layer neural networks", "abstract": "In this paper, we provide the first precise distributional characterization\nof gradient descent iterates for general multi-layer neural networks under the\ncanonical single-index regression model, in the `finite-width proportional\nregime' where the sample size and feature dimension grow proportionally while\nthe network width and depth remain bounded. Our non-asymptotic state evolution\ntheory captures Gaussian fluctuations in first-layer weights and concentration\nin deeper-layer weights, and remains valid for non-Gaussian features.\n  Our theory differs from existing neural tangent kernel (NTK), mean-field (MF)\ntheories and tensor program (TP) in several key aspects. First, our theory\noperates in the finite-width regime whereas these existing theories are\nfundamentally infinite-width. Second, our theory allows weights to evolve from\nindividual initializations beyond the lazy training regime, whereas NTK and MF\nare either frozen at or only weakly sensitive to initialization, and TP relies\non special initialization schemes. Third, our theory characterizes both\ntraining and generalization errors for general multi-layer neural networks\nbeyond the uniform convergence regime, whereas existing theories study\ngeneralization almost exclusively in two-layer settings.\n  As a statistical application, we show that vanilla gradient descent can be\naugmented to yield consistent estimates of the generalization error at each\niteration, which can be used to guide early stopping and hyperparameter tuning.\nAs a further theoretical implication, we show that despite model\nmisspecification, the model learned by gradient descent retains the structure\nof a single-index function with an effective signal determined by a linear\ncombination of the true signal and the initialization.", "published": "2025-05-08 02:19:39", "link": "http://arxiv.org/abs/2505.04898v1", "categories": ["cs.LG", "cs.AI", "math.OC", "math.ST", "stat.ML", "stat.TH"], "primary_category": "cs.LG"}
{"title": "Clustering with Communication: A Variational Framework for Single Cell Representation Learning", "abstract": "Single-cell RNA sequencing (scRNA-seq) has revealed complex cellular\nheterogeneity, but recent studies emphasize that understanding biological\nfunction also requires modeling cell-cell communication (CCC), the signaling\ninteractions mediated by ligand-receptor pairs that coordinate cellular\nbehavior. Tools like CellChat have demonstrated that CCC plays a critical role\nin processes such as cell differentiation, tissue regeneration, and immune\nresponse, and that transcriptomic data inherently encodes rich information\nabout intercellular signaling. We propose CCCVAE, a novel variational\nautoencoder framework that incorporates CCC signals into single-cell\nrepresentation learning. By leveraging a communication-aware kernel derived\nfrom ligand-receptor interactions and a sparse Gaussian process, CCCVAE encodes\nbiologically informed priors into the latent space. Unlike conventional VAEs\nthat treat each cell independently, CCCVAE encourages latent embeddings to\nreflect both transcriptional similarity and intercellular signaling context.\nEmpirical results across four scRNA-seq datasets show that CCCVAE improves\nclustering performance, achieving higher evaluation scores than standard VAE\nbaselines. This work demonstrates the value of embedding biological priors into\ndeep generative models for unsupervised single-cell analysis.", "published": "2025-05-08 01:53:36", "link": "http://arxiv.org/abs/2505.04891v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Cross-Branch Orthogonality for Improved Generalization in Face Deepfake Detection", "abstract": "Remarkable advancements in generative AI technology have given rise to a\nspectrum of novel deepfake categories with unprecedented leaps in their\nrealism, and deepfakes are increasingly becoming a nuisance to law enforcement\nauthorities and the general public. In particular, we observe alarming levels\nof confusion, deception, and loss of faith regarding multimedia content within\nsociety caused by face deepfakes, and existing deepfake detectors are\nstruggling to keep up with the pace of improvements in deepfake generation.\nThis is primarily due to their reliance on specific forgery artifacts, which\nlimits their ability to generalise and detect novel deepfake types. To combat\nthe spread of malicious face deepfakes, this paper proposes a new strategy that\nleverages coarse-to-fine spatial information, semantic information, and their\ninteractions while ensuring feature distinctiveness and reducing the redundancy\nof the modelled features. A novel feature orthogonality-based disentanglement\nstrategy is introduced to ensure branch-level and cross-branch feature\ndisentanglement, which allows us to integrate multiple feature vectors without\nadding complexity to the feature space or compromising generalisation.\nComprehensive experiments on three public benchmarks: FaceForensics++,\nCeleb-DF, and the Deepfake Detection Challenge (DFDC) show that these design\nchoices enable the proposed approach to outperform current state-of-the-art\nmethods by 5% on the Celeb-DF dataset and 7% on the DFDC dataset in a\ncross-dataset evaluation setting.", "published": "2025-05-08 01:49:53", "link": "http://arxiv.org/abs/2505.04888v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "QBR: A Question-Bank-Based Approach to Fine-Grained Legal Knowledge Retrieval for the General Public", "abstract": "Retrieval of legal knowledge by the general public is a challenging problem\ndue to the technicality of the professional knowledge and the lack of\nfundamental understanding by laypersons on the subject. Traditional information\nretrieval techniques assume that users are capable of formulating succinct and\nprecise queries for effective document retrieval. In practice, however, the\nwide gap between the highly technical contents and untrained users makes legal\nknowledge retrieval very difficult. We propose a methodology, called QBR, which\nemploys a Questions Bank (QB) as an effective medium for bridging the knowledge\ngap. We show how the QB is used to derive training samples to enhance the\nembedding of knowledge units within documents, which leads to effective\nfine-grained knowledge retrieval. We discuss and evaluate through experiments\nvarious advantages of QBR over traditional methods. These include more\naccurate, efficient, and explainable document retrieval, better comprehension\nof retrieval results, and highly effective fine-grained knowledge retrieval. We\nalso present some case studies and show that QBR achieves social impact by\nassisting citizens to resolve everyday legal concerns.", "published": "2025-05-08 01:43:21", "link": "http://arxiv.org/abs/2505.04883v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "GroverGPT-2: Simulating Grover's Algorithm via Chain-of-Thought Reasoning and Quantum-Native Tokenization", "abstract": "Quantum computing offers theoretical advantages over classical computing for\nspecific tasks, yet the boundary of practical quantum advantage remains an open\nquestion. To investigate this boundary, it is crucial to understand whether,\nand how, classical machines can learn and simulate quantum algorithms. Recent\nprogress in large language models (LLMs) has demonstrated strong reasoning\nabilities, prompting exploration into their potential for this challenge. In\nthis work, we introduce GroverGPT-2, an LLM-based method for simulating\nGrover's algorithm using Chain-of-Thought (CoT) reasoning and quantum-native\ntokenization. Building on its predecessor, GroverGPT-2 performs simulation\ndirectly from quantum circuit representations while producing logically\nstructured and interpretable outputs. Our results show that GroverGPT-2 can\nlearn and internalize quantum circuit logic through efficient processing of\nquantum-native tokens, providing direct evidence that classical models like\nLLMs can capture the structure of quantum algorithms. Furthermore, GroverGPT-2\noutputs interleave circuit data with natural language, embedding explicit\nreasoning into the simulation. This dual capability positions GroverGPT-2 as a\nprototype for advancing machine understanding of quantum algorithms and\nmodeling quantum circuit logic. We also identify an empirical scaling law for\nGroverGPT-2 with increasing qubit numbers, suggesting a path toward scalable\nclassical simulation. These findings open new directions for exploring the\nlimits of classical simulatability, enhancing quantum education and research,\nand laying groundwork for future foundation models in quantum computing.", "published": "2025-05-08 01:38:12", "link": "http://arxiv.org/abs/2505.04880v1", "categories": ["quant-ph", "cs.AI", "cs.LG"], "primary_category": "quant-ph"}
{"title": "Learning from Loss Landscape: Generalizable Mixed-Precision Quantization via Adaptive Sharpness-Aware Gradient Aligning", "abstract": "Mixed Precision Quantization (MPQ) has become an essential technique for\noptimizing neural network by determining the optimal bitwidth per layer.\nExisting MPQ methods, however, face a major hurdle: they require a\ncomputationally expensive search for quantization policies on large-scale\ndatasets. To resolve this issue, we introduce a novel approach that first\nsearches for quantization policies on small datasets and then generalizes them\nto large-scale datasets. This approach simplifies the process, eliminating the\nneed for large-scale quantization fine-tuning and only necessitating model\nweight adjustment. Our method is characterized by three key techniques:\nsharpness-aware minimization for enhanced quantization generalization, implicit\ngradient direction alignment to handle gradient conflicts among different\noptimization objectives, and an adaptive perturbation radius to accelerate\noptimization. Both theoretical analysis and experimental results validate our\napproach. Using the CIFAR10 dataset (just 0.5\\% the size of ImageNet training\ndata) for MPQ policy search, we achieved equivalent accuracy on ImageNet with a\nsignificantly lower computational cost, while improving efficiency by up to\n150% over the baselines.", "published": "2025-05-08 01:20:24", "link": "http://arxiv.org/abs/2505.04877v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Federated Learning for Cyber Physical Systems: A Comprehensive Survey", "abstract": "The integration of machine learning (ML) in cyber physical systems (CPS) is a\ncomplex task due to the challenges that arise in terms of real-time decision\nmaking, safety, reliability, device heterogeneity, and data privacy. There are\nalso open research questions that must be addressed in order to fully realize\nthe potential of ML in CPS. Federated learning (FL), a distributed approach to\nML, has become increasingly popular in recent years. It allows models to be\ntrained using data from decentralized sources. This approach has been gaining\npopularity in the CPS field, as it integrates computer, communication, and\nphysical processes. Therefore, the purpose of this work is to provide a\ncomprehensive analysis of the most recent developments of FL-CPS, including the\nnumerous application areas, system topologies, and algorithms developed in\nrecent years. The paper starts by discussing recent advances in both FL and\nCPS, followed by their integration. Then, the paper compares the application of\nFL in CPS with its applications in the internet of things (IoT) in further\ndepth to show their connections and distinctions. Furthermore, the article\nscrutinizes how FL is utilized in critical CPS applications, e.g., intelligent\ntransportation systems, cybersecurity services, smart cities, and smart\nhealthcare solutions. The study also includes critical insights and lessons\nlearned from various FL-CPS implementations. The paper's concluding section\ndelves into significant concerns and suggests avenues for further research in\nthis fast-paced and dynamic era.", "published": "2025-05-08 01:17:15", "link": "http://arxiv.org/abs/2505.04873v1", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Auto-regressive transformation for image alignment", "abstract": "Existing methods for image alignment struggle in cases involving\nfeature-sparse regions, extreme scale and field-of-view differences, and large\ndeformations, often resulting in suboptimal accuracy. Robustness to these\nchallenges improves through iterative refinement of the transformation field\nwhile focusing on critical regions in multi-scale image representations. We\nthus propose Auto-Regressive Transformation (ART), a novel method that\niteratively estimates the coarse-to-fine transformations within an\nauto-regressive framework. Leveraging hierarchical multi-scale features, our\nnetwork refines the transformations using randomly sampled points at each\nscale. By incorporating guidance from the cross-attention layer, the model\nfocuses on critical regions, ensuring accurate alignment even in challenging,\nfeature-limited conditions. Extensive experiments across diverse datasets\ndemonstrate that ART significantly outperforms state-of-the-art methods,\nestablishing it as a powerful new method for precise image alignment with broad\napplicability.", "published": "2025-05-08 00:28:31", "link": "http://arxiv.org/abs/2505.04864v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "D-CODA: Diffusion for Coordinated Dual-Arm Data Augmentation", "abstract": "Learning bimanual manipulation is challenging due to its high dimensionality\nand tight coordination required between two arms. Eye-in-hand imitation\nlearning, which uses wrist-mounted cameras, simplifies perception by focusing\non task-relevant views. However, collecting diverse demonstrations remains\ncostly, motivating the need for scalable data augmentation. While prior work\nhas explored visual augmentation in single-arm settings, extending these\napproaches to bimanual manipulation requires generating viewpoint-consistent\nobservations across both arms and producing corresponding action labels that\nare both valid and feasible. In this work, we propose Diffusion for COordinated\nDual-arm Data Augmentation (D-CODA), a method for offline data augmentation\ntailored to eye-in-hand bimanual imitation learning that trains a diffusion\nmodel to synthesize novel, viewpoint-consistent wrist-camera images for both\narms while simultaneously generating joint-space action labels. It employs\nconstrained optimization to ensure that augmented states involving\ngripper-to-object contacts adhere to constraints suitable for bimanual\ncoordination. We evaluate D-CODA on 5 simulated and 3 real-world tasks. Our\nresults across 2250 simulation trials and 300 real-world trials demonstrate\nthat it outperforms baselines and ablations, showing its potential for scalable\ndata augmentation in eye-in-hand bimanual manipulation. Our project website is\nat: https://dcodaaug.github.io/D-CODA/.", "published": "2025-05-08 00:03:04", "link": "http://arxiv.org/abs/2505.04860v1", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with Video Diffusion and Data Augmentation", "abstract": "Creating high-quality animatable 3D human avatars from a single image remains\na significant challenge in computer vision due to the inherent difficulty of\nreconstructing complete 3D information from a single viewpoint. Current\napproaches face a clear limitation: 3D Gaussian Splatting (3DGS) methods\nproduce high-quality results but require multiple views or video sequences,\nwhile video diffusion models can generate animations from single images but\nstruggle with consistency and identity preservation. We present SVAD, a novel\napproach that addresses these limitations by leveraging complementary strengths\nof existing techniques. Our method generates synthetic training data through\nvideo diffusion, enhances it with identity preservation and image restoration\nmodules, and utilizes this refined data to train 3DGS avatars. Comprehensive\nevaluations demonstrate that SVAD outperforms state-of-the-art (SOTA)\nsingle-image methods in maintaining identity consistency and fine details\nacross novel poses and viewpoints, while enabling real-time rendering\ncapabilities. Through our data augmentation pipeline, we overcome the\ndependency on dense monocular or multi-view training data typically required by\ntraditional 3DGS approaches. Extensive quantitative, qualitative comparisons\nshow our method achieves superior performance across multiple metrics against\nbaseline models. By effectively combining the generative power of diffusion\nmodels with both the high-quality results and rendering efficiency of 3DGS, our\nwork establishes a new approach for high-fidelity avatar generation from a\nsingle image input.", "published": "2025-05-08 17:59:58", "link": "http://arxiv.org/abs/2505.05475v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "3D Scene Generation: A Survey", "abstract": "3D scene generation seeks to synthesize spatially structured, semantically\nmeaningful, and photorealistic environments for applications such as immersive\nmedia, robotics, autonomous driving, and embodied AI. Early methods based on\nprocedural rules offered scalability but limited diversity. Recent advances in\ndeep generative models (e.g., GANs, diffusion models) and 3D representations\n(e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene\ndistributions, improving fidelity, diversity, and view consistency. Recent\nadvances like diffusion models bridge 3D scene synthesis and photorealism by\nreframing generation as image or video synthesis problems. This survey provides\na systematic overview of state-of-the-art approaches, organizing them into four\nparadigms: procedural generation, neural 3D-based generation, image-based\ngeneration, and video-based generation. We analyze their technical foundations,\ntrade-offs, and representative results, and review commonly used datasets,\nevaluation protocols, and downstream applications. We conclude by discussing\nkey challenges in generation capacity, 3D representation, data and annotations,\nand evaluation, and outline promising directions including higher fidelity,\nphysics-aware and interactive generation, and unified perception-generation\nmodels. This review organizes recent advances in 3D scene generation and\nhighlights promising directions at the intersection of generative AI, 3D\nvision, and embodied intelligence. To track ongoing developments, we maintain\nan up-to-date project page:\nhttps://github.com/hzxie/Awesome-3D-Scene-Generation.", "published": "2025-05-08 17:59:54", "link": "http://arxiv.org/abs/2505.05474v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "DiffusionSfM: Predicting Structure and Motion via Ray Origin and Endpoint Diffusion", "abstract": "Current Structure-from-Motion (SfM) methods typically follow a two-stage\npipeline, combining learned or geometric pairwise reasoning with a subsequent\nglobal optimization step. In contrast, we propose a data-driven multi-view\nreasoning approach that directly infers 3D scene geometry and camera poses from\nmulti-view images. Our framework, DiffusionSfM, parameterizes scene geometry\nand cameras as pixel-wise ray origins and endpoints in a global frame and\nemploys a transformer-based denoising diffusion model to predict them from\nmulti-view inputs. To address practical challenges in training diffusion models\nwith missing data and unbounded scene coordinates, we introduce specialized\nmechanisms that ensure robust learning. We empirically validate DiffusionSfM on\nboth synthetic and real datasets, demonstrating that it outperforms classical\nand learning-based approaches while naturally modeling uncertainty.", "published": "2025-05-08 17:59:47", "link": "http://arxiv.org/abs/2505.05473v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation", "abstract": "Recent progress in unified models for image understanding and generation has\nbeen impressive, yet most approaches remain limited to single-modal generation\nconditioned on multiple modalities. In this paper, we present Mogao, a unified\nframework that advances this paradigm by enabling interleaved multi-modal\ngeneration through a causal approach. Mogao integrates a set of key technical\nimprovements in architecture design, including a deep-fusion design, dual\nvision encoders, interleaved rotary position embeddings, and multi-modal\nclassifier-free guidance, which allow it to harness the strengths of both\nautoregressive models for text generation and diffusion models for high-quality\nimage synthesis. These practical improvements also make Mogao particularly\neffective to process interleaved sequences of text and images arbitrarily. To\nfurther unlock the potential of unified models, we introduce an efficient\ntraining strategy on a large-scale, in-house dataset specifically curated for\njoint text and image generation. Extensive experiments show that Mogao not only\nachieves state-of-the-art performance in multi-modal understanding and\ntext-to-image generation, but also excels in producing high-quality, coherent\ninterleaved outputs. Its emergent capabilities in zero-shot image editing and\ncompositional generation highlight Mogao as a practical omni-modal foundation\nmodel, paving the way for future development and scaling the unified\nmulti-modal systems.", "published": "2025-05-08 17:58:57", "link": "http://arxiv.org/abs/2505.05472v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Generating Physically Stable and Buildable LEGO Designs from Text", "abstract": "We introduce LegoGPT, the first approach for generating physically stable\nLEGO brick models from text prompts. To achieve this, we construct a\nlarge-scale, physically stable dataset of LEGO designs, along with their\nassociated captions, and train an autoregressive large language model to\npredict the next brick to add via next-token prediction. To improve the\nstability of the resulting designs, we employ an efficient validity check and\nphysics-aware rollback during autoregressive inference, which prunes infeasible\ntoken predictions using physics laws and assembly constraints. Our experiments\nshow that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO\ndesigns that align closely with the input text prompts. We also develop a\ntext-based LEGO texturing method to generate colored and textured designs. We\nshow that our designs can be assembled manually by humans and automatically by\nrobotic arms. We also release our new dataset, StableText2Lego, containing over\n47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed\ncaptions, along with our code and models at the project website:\nhttps://avalovelace1.github.io/LegoGPT/.", "published": "2025-05-08 17:58:18", "link": "http://arxiv.org/abs/2505.05469v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SITE: towards Spatial Intelligence Thorough Evaluation", "abstract": "Spatial intelligence (SI) represents a cognitive ability encompassing the\nvisualization, manipulation, and reasoning about spatial relationships,\nunderpinning disciplines from neuroscience to robotics. We introduce SITE, a\nbenchmark dataset towards SI Thorough Evaluation in a standardized format of\nmulti-choice visual question-answering, designed to assess large\nvision-language models' spatial intelligence across diverse visual modalities\n(single-image, multi-image, and video) and SI factors (figural to environmental\nscales, spatial visualization and orientation, intrinsic and extrinsic, static\nand dynamic). Our approach to curating the benchmark combines a bottom-up\nsurvey about 31 existing datasets and a top-down strategy drawing upon three\nclassification systems in cognitive science, which prompt us to design two\nnovel types of tasks about view-taking and dynamic scenes. Extensive\nexperiments reveal that leading models fall behind human experts especially in\nspatial orientation, a fundamental SI factor. Moreover, we demonstrate a\npositive correlation between a model's spatial reasoning proficiency and its\nperformance on an embodied AI task.", "published": "2025-05-08 17:45:44", "link": "http://arxiv.org/abs/2505.05456v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "PillarMamba: Learning Local-Global Context for Roadside Point Cloud via Hybrid State Space Model", "abstract": "Serving the Intelligent Transport System (ITS) and Vehicle-to-Everything\n(V2X) tasks, roadside perception has received increasing attention in recent\nyears, as it can extend the perception range of connected vehicles and improve\ntraffic safety. However, roadside point cloud oriented 3D object detection has\nnot been effectively explored. To some extent, the key to the performance of a\npoint cloud detector lies in the receptive field of the network and the ability\nto effectively utilize the scene context. The recent emergence of Mamba, based\non State Space Model (SSM), has shaken up the traditional convolution and\ntransformers that have long been the foundational building blocks, due to its\nefficient global receptive field. In this work, we introduce Mamba to\npillar-based roadside point cloud perception and propose a framework based on\nCross-stage State-space Group (CSG), called PillarMamba. It enhances the\nexpressiveness of the network and achieves efficient computation through\ncross-stage feature fusion. However, due to the limitations of scan directions,\nstate space model faces local connection disrupted and historical relationship\nforgotten. To address this, we propose the Hybrid State-space Block (HSB) to\nobtain the local-global context of roadside point cloud. Specifically, it\nenhances neighborhood connections through local convolution and preserves\nhistorical memory through residual attention. The proposed method outperforms\nthe state-of-the-art methods on the popular large scale roadside benchmark:\nDAIR-V2X-I. The code will be released soon.", "published": "2025-05-08 16:33:04", "link": "http://arxiv.org/abs/2505.05397v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "EDmamba: A Simple yet Effective Event Denoising Method with State Space Model", "abstract": "Event cameras excel in high-speed vision due to their high temporal\nresolution, high dynamic range, and low power consumption. However, as dynamic\nvision sensors, their output is inherently noisy, making efficient denoising\nessential to preserve their ultra-low latency and real-time processing\ncapabilities. Existing event denoising methods struggle with a critical\ndilemma: computationally intensive approaches compromise the sensor's\nhigh-speed advantage, while lightweight methods often lack robustness across\nvarying noise levels. To address this, we propose a novel event denoising\nframework based on State Space Models (SSMs). Our approach represents events as\n4D event clouds and includes a Coarse Feature Extraction (CFE) module that\nextracts embedding features from both geometric and polarity-aware subspaces.\nThe model is further composed of two essential components: A Spatial Mamba\n(S-SSM) that models local geometric structures and a Temporal Mamba (T-SSM)\nthat captures global temporal dynamics, efficiently propagating spatiotemporal\nfeatures across events. Experiments demonstrate that our method achieves\nstate-of-the-art accuracy and efficiency, with 88.89K parameters, 0.0685s per\n100K events inference time, and a 0.982 accuracy score, outperforming\nTransformer-based methods by 2.08% in denoising accuracy and 36X faster.", "published": "2025-05-08 16:27:27", "link": "http://arxiv.org/abs/2505.05391v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "GeomHair: Reconstruction of Hair Strands from Colorless 3D Scans", "abstract": "We propose a novel method that reconstructs hair strands directly from\ncolorless 3D scans by leveraging multi-modal hair orientation extraction. Hair\nstrand reconstruction is a fundamental problem in computer vision and graphics\nthat can be used for high-fidelity digital avatar synthesis, animation, and\nAR/VR applications. However, accurately recovering hair strands from raw scan\ndata remains challenging due to human hair's complex and fine-grained\nstructure. Existing methods typically rely on RGB captures, which can be\nsensitive to the environment and can be a challenging domain for extracting the\norientation of guiding strands, especially in the case of challenging\nhairstyles. To reconstruct the hair purely from the observed geometry, our\nmethod finds sharp surface features directly on the scan and estimates strand\norientation through a neural 2D line detector applied to the renderings of scan\nshading. Additionally, we incorporate a diffusion prior trained on a diverse\nset of synthetic hair scans, refined with an improved noise schedule, and\nadapted to the reconstructed contents via a scan-specific text prompt. We\ndemonstrate that this combination of supervision signals enables accurate\nreconstruction of both simple and intricate hairstyles without relying on color\ninformation. To facilitate further research, we introduce Strands400, the\nlargest publicly available dataset of hair strands with detailed surface\ngeometry extracted from real-world data, which contains reconstructed hair\nstrands from the scans of 400 subjects.", "published": "2025-05-08 16:11:09", "link": "http://arxiv.org/abs/2505.05376v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "OcularAge: A Comparative Study of Iris and Periocular Images for Pediatric Age Estimation", "abstract": "Estimating a child's age from ocular biometric images is challenging due to\nsubtle physiological changes and the limited availability of longitudinal\ndatasets. Although most biometric age estimation studies have focused on facial\nfeatures and adult subjects, pediatric-specific analysis, particularly of the\niris and periocular regions, remains relatively unexplored. This study presents\na comparative evaluation of iris and periocular images for estimating the ages\nof children aged between 4 and 16 years. We utilized a longitudinal dataset\ncomprising more than 21,000 near-infrared (NIR) images, collected from 288\npediatric subjects over eight years using two different imaging sensors. A\nmulti-task deep learning framework was employed to jointly perform age\nprediction and age-group classification, enabling a systematic exploration of\nhow different convolutional neural network (CNN) architectures, particularly\nthose adapted for non-square ocular inputs, capture the complex variability\ninherent in pediatric eye images. The results show that periocular models\nconsistently outperform iris-based models, achieving a mean absolute error\n(MAE) of 1.33 years and an age-group classification accuracy of 83.82%. These\nresults mark the first demonstration that reliable age estimation is feasible\nfrom children's ocular images, enabling privacy-preserving age checks in\nchild-centric applications. This work establishes the first longitudinal\nbenchmark for pediatric ocular age estimation, providing a foundation for\ndesigning robust, child-focused biometric systems. The developed models proved\nresilient across different imaging sensors, confirming their potential for\nreal-world deployment. They also achieved inference speeds of less than 10\nmilliseconds per image on resource-constrained VR headsets, demonstrating their\nsuitability for real-time applications.", "published": "2025-05-08 16:09:08", "link": "http://arxiv.org/abs/2505.05374v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Joint Super-Resolution and Segmentation for 1-m Impervious Surface Area Mapping in China's Yangtze River Economic Belt", "abstract": "We propose a novel joint framework by integrating super-resolution and\nsegmentation, called JointSeg, which enables the generation of 1-meter ISA maps\ndirectly from freely available Sentinel-2 imagery. JointSeg was trained on\nmultimodal cross-resolution inputs, offering a scalable and affordable\nalternative to traditional approaches. This synergistic design enables gradual\nresolution enhancement from 10m to 1m while preserving fine-grained spatial\ntextures, and ensures high classification fidelity through effective\ncross-scale feature fusion. This method has been successfully applied to the\nYangtze River Economic Belt (YREB), a region characterized by complex\nurban-rural patterns and diverse topography. As a result, a comprehensive ISA\nmapping product for 2021, referred to as ISA-1, was generated, covering an area\nof over 2.2 million square kilometers. Quantitative comparisons against the 10m\nESA WorldCover and other benchmark products reveal that ISA-1 achieves an\nF1-score of 85.71%, outperforming bilinear-interpolation-based segmentation by\n9.5%, and surpassing other ISA datasets by 21.43%-61.07%. In densely urbanized\nareas (e.g., Suzhou, Nanjing), ISA-1 reduces ISA overestimation through\nimproved discrimination of green spaces and water bodies. Conversely, in\nmountainous regions (e.g., Ganzi, Zhaotong), it identifies significantly more\nISA due to its enhanced ability to detect fragmented anthropogenic features\nsuch as rural roads and sparse settlements, demonstrating its robustness across\ndiverse landscapes. Moreover, we present biennial ISA maps from 2017 to 2023,\ncapturing spatiotemporal urbanization dynamics across representative cities.\nThe results highlight distinct regional growth patterns: rapid expansion in\nupstream cities, moderate growth in midstream regions, and saturation in\ndownstream metropolitan areas.", "published": "2025-05-08 16:04:35", "link": "http://arxiv.org/abs/2505.05367v1", "categories": ["cs.CV", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Hearing and Seeing Through CLIP: A Framework for Self-Supervised Sound Source Localization", "abstract": "Large-scale vision-language models demonstrate strong multimodal alignment\nand generalization across diverse tasks. Among them, CLIP stands out as one of\nthe most successful approaches. In this work, we extend the application of CLIP\nto sound source localization, proposing a self-supervised method operates\nwithout explicit text input. We introduce a framework that maps audios into\ntokens compatible with CLIP's text encoder, producing audio-driven embeddings.\nThese embeddings are used to generate sounding region masks, from which visual\nfeatures are extracted and aligned with the audio embeddings through a\ncontrastive audio-visual correspondence objective. Our findings show that\nalignment knowledge of pre-trained multimodal foundation model enables our\nmethod to generate more complete and compact localization for sounding objects.\nWe further propose an LLM-guided extension that distills object-aware\naudio-visual scene understanding into the model during training to enhance\nalignment. Extensive experiments across five diverse tasks demonstrate that our\nmethod, in all variants, outperforms state-of-the-art approaches and achieves\nstrong generalization in zero-shot settings.", "published": "2025-05-08 15:32:04", "link": "http://arxiv.org/abs/2505.05343v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Progressive Inertial Poser: Progressive Real-Time Kinematic Chain Estimation for 3D Full-Body Pose from Three IMU Sensors", "abstract": "The motion capture system that supports full-body virtual representation is\nof key significance for virtual reality. Compared to vision-based systems,\nfull-body pose estimation from sparse tracking signals is not limited by\nenvironmental conditions or recording range. However, previous works either\nface the challenge of wearing additional sensors on the pelvis and lower-body\nor rely on external visual sensors to obtain global positions of key joints. To\nimprove the practicality of the technology for virtual reality applications, we\nestimate full-body poses using only inertial data obtained from three Inertial\nMeasurement Unit (IMU) sensors worn on the head and wrists, thereby reducing\nthe complexity of the hardware system. In this work, we propose a method called\nProgressive Inertial Poser (ProgIP) for human pose estimation, which combines\nneural network estimation with a human dynamics model, considers the\nhierarchical structure of the kinematic chain, and employs a multi-stage\nprogressive network estimation with increased depth to reconstruct full-body\nmotion in real time. The encoder combines Transformer Encoder and bidirectional\nLSTM (TE-biLSTM) to flexibly capture the temporal dependencies of the inertial\nsequence, while the decoder based on multi-layer perceptrons (MLPs) transforms\nhigh-dimensional features and accurately projects them onto Skinned\nMulti-Person Linear (SMPL) model parameters. Quantitative and qualitative\nexperimental results on multiple public datasets show that our method\noutperforms state-of-the-art methods with the same inputs, and is comparable to\nrecent works using six IMU sensors.", "published": "2025-05-08 15:28:09", "link": "http://arxiv.org/abs/2505.05336v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Aesthetics Without Semantics", "abstract": "While it is easy for human observers to judge an image as beautiful or ugly,\naesthetic decisions result from a combination of entangled perceptual and\ncognitive (semantic) factors, making the understanding of aesthetic judgements\nparticularly challenging from a scientific point of view. Furthermore, our\nresearch shows a prevailing bias in current databases, which include mostly\nbeautiful images, further complicating the study and prediction of aesthetic\nresponses. We address these limitations by creating a database of images with\nminimal semantic content and devising, and next exploiting, a method to\ngenerate images on the ugly side of aesthetic valuations. The resulting Minimum\nSemantic Content (MSC) database consists of a large and balanced collection of\n10,426 images, each evaluated by 100 observers. We next use established image\nmetrics to demonstrate how augmenting an image set biased towards beautiful\nimages with ugly images can modify, or even invert, an observed relationship\nbetween image features and aesthetics valuation. Taken together, our study\nreveals that works in empirical aesthetics attempting to link image content and\naesthetic judgements may magnify, underestimate, or simply miss interesting\neffects due to a limitation of the range of aesthetic values they consider.", "published": "2025-05-08 15:22:11", "link": "http://arxiv.org/abs/2505.05331v1", "categories": ["cs.CV", "q-bio.NC", "stat.CO"], "primary_category": "cs.CV"}
{"title": "Augmented Deep Contexts for Spatially Embedded Video Coding", "abstract": "Most Neural Video Codecs (NVCs) only employ temporal references to generate\ntemporal-only contexts and latent prior. These temporal-only NVCs fail to\nhandle large motions or emerging objects due to limited contexts and misaligned\nlatent prior. To relieve the limitations, we propose a Spatially Embedded Video\nCodec (SEVC), in which the low-resolution video is compressed for spatial\nreferences. Firstly, our SEVC leverages both spatial and temporal references to\ngenerate augmented motion vectors and hybrid spatial-temporal contexts.\nSecondly, to address the misalignment issue in latent prior and enrich the\nprior information, we introduce a spatial-guided latent prior augmented by\nmultiple temporal latent representations. At last, we design a joint\nspatial-temporal optimization to learn quality-adaptive bit allocation for\nspatial references, further boosting rate-distortion performance. Experimental\nresults show that our SEVC effectively alleviates the limitations in handling\nlarge motions or emerging objects, and also reduces 11.9% more bitrate than the\nprevious state-of-the-art NVC while providing an additional low-resolution\nbitstream. Our code and model are available at https://github.com/EsakaK/SEVC.", "published": "2025-05-08 14:57:52", "link": "http://arxiv.org/abs/2505.05309v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "PRE-Mamba: A 4D State Space Model for Ultra-High-Frequent Event Camera Deraining", "abstract": "Event cameras excel in high temporal resolution and dynamic range but suffer\nfrom dense noise in rainy conditions. Existing event deraining methods face\ntrade-offs between temporal precision, deraining effectiveness, and\ncomputational efficiency. In this paper, we propose PRE-Mamba, a novel\npoint-based event camera deraining framework that fully exploits the\nspatiotemporal characteristics of raw event and rain. Our framework introduces\na 4D event cloud representation that integrates dual temporal scales to\npreserve high temporal precision, a Spatio-Temporal Decoupling and Fusion\nmodule (STDF) that enhances deraining capability by enabling shallow decoupling\nand interaction of temporal and spatial information, and a Multi-Scale State\nSpace Model (MS3M) that captures deeper rain dynamics across dual-temporal and\nmulti-spatial scales with linear computational complexity. Enhanced by\nfrequency-domain regularization, PRE-Mamba achieves superior performance (0.95\nSR, 0.91 NR, and 0.4s/M events) with only 0.26M parameters on EventRain-27K, a\ncomprehensive dataset with labeled synthetic and real-world sequences.\nMoreover, our method generalizes well across varying rain intensities,\nviewpoints, and even snowy conditions.", "published": "2025-05-08 14:52:45", "link": "http://arxiv.org/abs/2505.05307v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "MTL-UE: Learning to Learn Nothing for Multi-Task Learning", "abstract": "Most existing unlearnable strategies focus on preventing unauthorized users\nfrom training single-task learning (STL) models with personal data.\nNevertheless, the paradigm has recently shifted towards multi-task data and\nmulti-task learning (MTL), targeting generalist and foundation models that can\nhandle multiple tasks simultaneously. Despite their growing importance, MTL\ndata and models have been largely neglected while pursuing unlearnable\nstrategies. This paper presents MTL-UE, the first unified framework for\ngenerating unlearnable examples for multi-task data and MTL models. Instead of\noptimizing perturbations for each sample, we design a generator-based structure\nthat introduces label priors and class-wise feature embeddings which leads to\nmuch better attacking performance. In addition, MTL-UE incorporates intra-task\nand inter-task embedding regularization to increase inter-class separation and\nsuppress intra-class variance which enhances the attack robustness greatly.\nFurthermore, MTL-UE is versatile with good supports for dense prediction tasks\nin MTL. It is also plug-and-play allowing integrating existing\nsurrogate-dependent unlearnable methods with little adaptation. Extensive\nexperiments show that MTL-UE achieves superior attacking performance\nconsistently across 4 MTL datasets, 3 base UE methods, 5 model backbones, and 5\nMTL task-weighting strategies.", "published": "2025-05-08 14:26:00", "link": "http://arxiv.org/abs/2505.05279v1", "categories": ["cs.LG", "cs.CR", "cs.CV"], "primary_category": "cs.LG"}
{"title": "White Light Specular Reflection Data Augmentation for Deep Learning Polyp Detection", "abstract": "Colorectal cancer is one of the deadliest cancers today, but it can be\nprevented through early detection of malignant polyps in the colon, primarily\nvia colonoscopies. While this method has saved many lives, human error remains\na significant challenge, as missing a polyp could have fatal consequences for\nthe patient. Deep learning (DL) polyp detectors offer a promising solution.\nHowever, existing DL polyp detectors often mistake white light reflections from\nthe endoscope for polyps, which can lead to false positives.To address this\nchallenge, in this paper, we propose a novel data augmentation approach that\nartificially adds more white light reflections to create harder training\nscenarios. Specifically, we first generate a bank of artificial lights using\nthe training dataset. Then we find the regions of the training images that we\nshould not add these artificial lights on. Finally, we propose a sliding window\nmethod to add the artificial light to the areas that fit of the training\nimages, resulting in augmented images. By providing the model with more\nopportunities to make mistakes, we hypothesize that it will also have more\nchances to learn from those mistakes, ultimately improving its performance in\npolyp detection. Experimental results demonstrate the effectiveness of our new\ndata augmentation method.", "published": "2025-05-08 13:51:00", "link": "http://arxiv.org/abs/2505.05248v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "PADriver: Towards Personalized Autonomous Driving", "abstract": "In this paper, we propose PADriver, a novel closed-loop framework for\npersonalized autonomous driving (PAD). Built upon Multi-modal Large Language\nModel (MLLM), PADriver takes streaming frames and personalized textual prompts\nas inputs. It autoaggressively performs scene understanding, danger level\nestimation and action decision. The predicted danger level reflects the risk of\nthe potential action and provides an explicit reference for the final action,\nwhich corresponds to the preset personalized prompt. Moreover, we construct a\nclosed-loop benchmark named PAD-Highway based on Highway-Env simulator to\ncomprehensively evaluate the decision performance under traffic rules. The\ndataset contains 250 hours videos with high-quality annotation to facilitate\nthe development of PAD behavior analysis. Experimental results on the\nconstructed benchmark show that PADriver outperforms state-of-the-art\napproaches on different evaluation metrics, and enables various driving modes.", "published": "2025-05-08 13:36:07", "link": "http://arxiv.org/abs/2505.05240v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Does CLIP perceive art the same way we do?", "abstract": "CLIP has emerged as a powerful multimodal model capable of connecting images\nand text through joint embeddings, but to what extent does it \"see\" the same\nway humans do - especially when interpreting artworks? In this paper, we\ninvestigate CLIP's ability to extract high-level semantic and stylistic\ninformation from paintings, including both human-created and AI-generated\nimagery. We evaluate its perception across multiple dimensions: content, scene\nunderstanding, artistic style, historical period, and the presence of visual\ndeformations or artifacts. By designing targeted probing tasks and comparing\nCLIP's responses to human annotations and expert benchmarks, we explore its\nalignment with human perceptual and contextual understanding. Our findings\nreveal both strengths and limitations in CLIP's visual representations,\nparticularly in relation to aesthetic cues and artistic intent. We further\ndiscuss the implications of these insights for using CLIP as a guidance\nmechanism during generative processes, such as style transfer or prompt-based\nimage synthesis. Our work highlights the need for deeper interpretability in\nmultimodal systems, especially when applied to creative domains where nuance\nand subjectivity play a central role.", "published": "2025-05-08 13:21:10", "link": "http://arxiv.org/abs/2505.05229v1", "categories": ["cs.CV", "cs.MM", "68T45, 68T07 (Primary) 68T50, 68U10 (Secondary)", "I.2.7; I.2.10"], "primary_category": "cs.CV"}
{"title": "Multi-Objective Reinforcement Learning for Adaptive Personalized Autonomous Driving", "abstract": "Human drivers exhibit individual preferences regarding driving style.\nAdapting autonomous vehicles to these preferences is essential for user trust\nand satisfaction. However, existing end-to-end driving approaches often rely on\npredefined driving styles or require continuous user feedback for adaptation,\nlimiting their ability to support dynamic, context-dependent preferences. We\npropose a novel approach using multi-objective reinforcement learning (MORL)\nwith preference-driven optimization for end-to-end autonomous driving that\nenables runtime adaptation to driving style preferences. Preferences are\nencoded as continuous weight vectors to modulate behavior along interpretable\nstyle objectives$\\unicode{x2013}$including efficiency, comfort, speed, and\naggressiveness$\\unicode{x2013}$without requiring policy retraining. Our\nsingle-policy agent integrates vision-based perception in complex mixed-traffic\nscenarios and is evaluated in diverse urban environments using the CARLA\nsimulator. Experimental results demonstrate that the agent dynamically adapts\nits driving behavior according to changing preferences while maintaining\nperformance in terms of collision avoidance and route completion.", "published": "2025-05-08 13:16:37", "link": "http://arxiv.org/abs/2505.05223v1", "categories": ["cs.RO", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Diffusion Model Quantization: A Review", "abstract": "Recent success of large text-to-image models has empirically underscored the\nexceptional performance of diffusion models in generative tasks. To facilitate\ntheir efficient deployment on resource-constrained edge devices, model\nquantization has emerged as a pivotal technique for both compression and\nacceleration. This survey offers a thorough review of the latest advancements\nin diffusion model quantization, encapsulating and analyzing the current state\nof the art in this rapidly advancing domain. First, we provide an overview of\nthe key challenges encountered in the quantization of diffusion models,\nincluding those based on U-Net architectures and Diffusion Transformers (DiT).\nWe then present a comprehensive taxonomy of prevalent quantization techniques,\nengaging in an in-depth discussion of their underlying principles.\nSubsequently, we perform a meticulous analysis of representative diffusion\nmodel quantization schemes from both qualitative and quantitative perspectives.\nFrom a quantitative standpoint, we rigorously benchmark a variety of methods\nusing widely recognized datasets, delivering an extensive evaluation of the\nmost recent and impactful research in the field. From a qualitative standpoint,\nwe categorize and synthesize the effects of quantization errors, elucidating\nthese impacts through both visual analysis and trajectory examination. In\nconclusion, we outline prospective avenues for future research, proposing novel\ndirections for the quantization of generative models in practical applications.\nThe list of related papers, corresponding codes, pre-trained models and\ncomparison results are publicly available at the survey project homepage\nhttps://github.com/TaylorJocelyn/Diffusion-Model-Quantization.", "published": "2025-05-08 13:09:34", "link": "http://arxiv.org/abs/2505.05215v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "HQC-NBV: A Hybrid Quantum-Classical View Planning Approach", "abstract": "Efficient view planning is a fundamental challenge in computer vision and\nrobotic perception, critical for tasks ranging from search and rescue\noperations to autonomous navigation. While classical approaches, including\nsampling-based and deterministic methods, have shown promise in planning camera\nviewpoints for scene exploration, they often struggle with computational\nscalability and solution optimality in complex settings. This study introduces\nHQC-NBV, a hybrid quantum-classical framework for view planning that leverages\nquantum properties to efficiently explore the parameter space while maintaining\nrobustness and scalability. We propose a specific Hamiltonian formulation with\nmulti-component cost terms and a parameter-centric variational ansatz with\nbidirectional alternating entanglement patterns that capture the hierarchical\ndependencies between viewpoint parameters. Comprehensive experiments\ndemonstrate that quantum-specific components provide measurable performance\nadvantages. Compared to the classical methods, our approach achieves up to\n49.2% higher exploration efficiency across diverse environments. Our analysis\nof entanglement architecture and coherence-preserving terms provides insights\ninto the mechanisms of quantum advantage in robotic exploration tasks. This\nwork represents a significant advancement in integrating quantum computing into\nrobotic perception systems, offering a paradigm-shifting solution for various\nrobot vision tasks.", "published": "2025-05-08 13:05:07", "link": "http://arxiv.org/abs/2505.05212v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "EAM: Enhancing Anything with Diffusion Transformers for Blind Super-Resolution", "abstract": "Utilizing pre-trained Text-to-Image (T2I) diffusion models to guide Blind\nSuper-Resolution (BSR) has become a predominant approach in the field. While\nT2I models have traditionally relied on U-Net architectures, recent\nadvancements have demonstrated that Diffusion Transformers (DiT) achieve\nsignificantly higher performance in this domain. In this work, we introduce\nEnhancing Anything Model (EAM), a novel BSR method that leverages DiT and\noutperforms previous U-Net-based approaches. We introduce a novel block,\n$\\Psi$-DiT, which effectively guides the DiT to enhance image restoration. This\nblock employs a low-resolution latent as a separable flow injection control,\nforming a triple-flow architecture that effectively leverages the prior\nknowledge embedded in the pre-trained DiT. To fully exploit the prior guidance\ncapabilities of T2I models and enhance their generalization in BSR, we\nintroduce a progressive Masked Image Modeling strategy, which also reduces\ntraining costs. Additionally, we propose a subject-aware prompt generation\nstrategy that employs a robust multi-modal model in an in-context learning\nframework. This strategy automatically identifies key image areas, provides\ndetailed descriptions, and optimizes the utilization of T2I diffusion priors.\nOur experiments demonstrate that EAM achieves state-of-the-art results across\nmultiple datasets, outperforming existing methods in both quantitative metrics\nand visual quality.", "published": "2025-05-08 13:03:07", "link": "http://arxiv.org/abs/2505.05209v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Improved Brain Tumor Detection in MRI: Fuzzy Sigmoid Convolution in Deep Learning", "abstract": "Early detection and accurate diagnosis are essential to improving patient\noutcomes. The use of convolutional neural networks (CNNs) for tumor detection\nhas shown promise, but existing models often suffer from overparameterization,\nwhich limits their performance gains. In this study, fuzzy sigmoid convolution\n(FSC) is introduced along with two additional modules: top-of-the-funnel and\nmiddle-of-the-funnel. The proposed methodology significantly reduces the number\nof trainable parameters without compromising classification accuracy. A novel\nconvolutional operator is central to this approach, effectively dilating the\nreceptive field while preserving input data integrity. This enables efficient\nfeature map reduction and enhances the model's tumor detection capability. In\nthe FSC-based model, fuzzy sigmoid activation functions are incorporated within\nconvolutional layers to improve feature extraction and classification. The\ninclusion of fuzzy logic into the architecture improves its adaptability and\nrobustness. Extensive experiments on three benchmark datasets demonstrate the\nsuperior performance and efficiency of the proposed model. The FSC-based\narchitecture achieved classification accuracies of 99.17%, 99.75%, and 99.89%\non three different datasets. The model employs 100 times fewer parameters than\nlarge-scale transfer learning architectures, highlighting its computational\nefficiency and suitability for detecting brain tumors early. This research\noffers lightweight, high-performance deep-learning models for medical imaging\napplications.", "published": "2025-05-08 13:02:44", "link": "http://arxiv.org/abs/2505.05208v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "PaniCar: Securing the Perception of Advanced Driving Assistance Systems Against Emergency Vehicle Lighting", "abstract": "The safety of autonomous cars has come under scrutiny in recent years,\nespecially after 16 documented incidents involving Teslas (with autopilot\nengaged) crashing into parked emergency vehicles (police cars, ambulances, and\nfiretrucks). While previous studies have revealed that strong light sources\noften introduce flare artifacts in the captured image, which degrade the image\nquality, the impact of flare on object detection performance remains unclear.\nIn this research, we unveil PaniCar, a digital phenomenon that causes an object\ndetector's confidence score to fluctuate below detection thresholds when\nexposed to activated emergency vehicle lighting. This vulnerability poses a\nsignificant safety risk, and can cause autonomous vehicles to fail to detect\nobjects near emergency vehicles. In addition, this vulnerability could be\nexploited by adversaries to compromise the security of advanced driving\nassistance systems (ADASs). We assess seven commercial ADASs (Tesla Model 3,\n\"manufacturer C\", HP, Pelsee, AZDOME, Imagebon, Rexing), four object detectors\n(YOLO, SSD, RetinaNet, Faster R-CNN), and 14 patterns of emergency vehicle\nlighting to understand the influence of various technical and environmental\nfactors. We also evaluate four SOTA flare removal methods and show that their\nperformance and latency are insufficient for real-time driving constraints. To\nmitigate this risk, we propose Caracetamol, a robust framework designed to\nenhance the resilience of object detectors against the effects of activated\nemergency vehicle lighting. Our evaluation shows that on YOLOv3 and Faster\nRCNN, Caracetamol improves the models' average confidence of car detection by\n0.20, the lower confidence bound by 0.33, and reduces the fluctuation range by\n0.33. In addition, Caracetamol is capable of processing frames at a rate of\nbetween 30-50 FPS, enabling real-time ADAS car detection.", "published": "2025-05-08 12:33:48", "link": "http://arxiv.org/abs/2505.05183v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Probabilistic Embeddings for Frozen Vision-Language Models: Uncertainty Quantification with Gaussian Process Latent Variable Models", "abstract": "Vision-Language Models (VLMs) learn joint representations by mapping images\nand text into a shared latent space. However, recent research highlights that\ndeterministic embeddings from standard VLMs often struggle to capture the\nuncertainties arising from the ambiguities in visual and textual descriptions\nand the multiple possible correspondences between images and texts. Existing\napproaches tackle this by learning probabilistic embeddings during VLM\ntraining, which demands large datasets and does not leverage the powerful\nrepresentations already learned by large-scale VLMs like CLIP. In this paper,\nwe propose GroVE, a post-hoc approach to obtaining probabilistic embeddings\nfrom frozen VLMs. GroVE builds on Gaussian Process Latent Variable Model\n(GPLVM) to learn a shared low-dimensional latent space where image and text\ninputs are mapped to a unified representation, optimized through single-modal\nembedding reconstruction and cross-modal alignment objectives. Once trained,\nthe Gaussian Process model generates uncertainty-aware probabilistic\nembeddings. Evaluation shows that GroVE achieves state-of-the-art uncertainty\ncalibration across multiple downstream tasks, including cross-modal retrieval,\nvisual question answering, and active learning.", "published": "2025-05-08 11:57:35", "link": "http://arxiv.org/abs/2505.05163v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Research on Anomaly Detection Methods Based on Diffusion Models", "abstract": "Anomaly detection is a fundamental task in machine learning and data mining,\nwith significant applications in cybersecurity, industrial fault diagnosis, and\nclinical disease monitoring. Traditional methods, such as statistical modeling\nand machine learning-based approaches, often face challenges in handling\ncomplex, high-dimensional data distributions. In this study, we explore the\npotential of diffusion models for anomaly detection, proposing a novel\nframework that leverages the strengths of diffusion probabilistic models (DPMs)\nto effectively identify anomalies in both image and audio data. The proposed\nmethod models the distribution of normal data through a diffusion process and\nreconstructs input data via reverse diffusion, using a combination of\nreconstruction errors and semantic discrepancies as anomaly indicators. To\nenhance the framework's performance, we introduce multi-scale feature\nextraction, attention mechanisms, and wavelet-domain representations, enabling\nthe model to capture fine-grained structures and global dependencies in the\ndata. Extensive experiments on benchmark datasets, including MVTec AD and\nUrbanSound8K, demonstrate that our method outperforms state-of-the-art anomaly\ndetection techniques, achieving superior accuracy and robustness across diverse\ndata modalities. This research highlights the effectiveness of diffusion models\nin anomaly detection and provides a robust and efficient solution for\nreal-world applications.", "published": "2025-05-08 11:19:08", "link": "http://arxiv.org/abs/2505.05137v1", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Automated vision-based assistance tools in bronchoscopy: stenosis severity estimation", "abstract": "Purpose: Subglottic stenosis refers to the narrowing of the subglottis, the\nairway between the vocal cords and the trachea. Its severity is typically\nevaluated by estimating the percentage of obstructed airway. This estimation\ncan be obtained from CT data or through visual inspection by experts exploring\nthe region. However, visual inspections are inherently subjective, leading to\nless consistent and robust diagnoses. No public methods or datasets are\ncurrently available for automated evaluation of this condition from\nbronchoscopy video.\n  Methods: We propose a pipeline for automated subglottic stenosis severity\nestimation during the bronchoscopy exploration, without requiring the physician\nto traverse the stenosed region. Our approach exploits the physical effect of\nillumination decline in endoscopy to segment and track the lumen and obtain a\n3D model of the airway. This 3D model is obtained from a single frame and is\nused to measure the airway narrowing.\n  Results: Our pipeline is the first to enable automated and robust subglottic\nstenosis severity measurement using bronchoscopy images. The results show\nconsistency with ground-truth estimations from CT scans and expert estimations,\nand reliable repeatability across multiple estimations on the same patient. Our\nevaluation is performed on our new Subglottic Stenosis Dataset of real\nbronchoscopy procedures data.\n  Conclusion: We demonstrate how to automate evaluation of subglottic stenosis\nseverity using only bronchoscopy. Our approach can assist with and shorten\ndiagnosis and monitoring procedures, with automated and repeatable estimations\nand less exploration time, and save radiation exposure to patients as no CT is\nrequired. Additionally, we release the first public benchmark for subglottic\nstenosis severity assessment.", "published": "2025-05-08 11:13:38", "link": "http://arxiv.org/abs/2505.05136v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "An Active Contour Model for Silhouette Vectorization using B\u00e9zier Curves", "abstract": "In this paper, we propose an active contour model for silhouette\nvectorization using cubic B\\'ezier curves. Among the end points of the B\\'ezier\ncurves, we distinguish between corner and regular points where the orientation\nof the tangent vector is prescribed. By minimizing the distance of the B\\'ezier\ncurves to the silhouette boundary, the active contour model optimizes the\nlocation of the B\\'ezier curves end points, the orientation of the tangent\nvectors in the regular points, and the estimation of the B\\'ezier curve\nparameters. This active contour model can use the silhouette vectorization\nobtained by any method as an initial guess. The proposed method significantly\nreduces the average distance between the silhouette boundary and its\nvectorization obtained by the world-class graphic software Inkscape, Adobe\nIllustrator, and a curvature-based vectorization method, which we introduce for\ncomparison. Our method also allows us to impose additional regularity on the\nB\\'ezier curves by reducing their lengths.", "published": "2025-05-08 11:09:49", "link": "http://arxiv.org/abs/2505.05132v1", "categories": ["cs.GR", "cs.CV", "math.FA"], "primary_category": "cs.GR"}
{"title": "MDAA-Diff: CT-Guided Multi-Dose Adaptive Attention Diffusion Model for PET Denoising", "abstract": "Acquiring high-quality Positron Emission Tomography (PET) images requires\nadministering high-dose radiotracers, which increases radiation exposure risks.\nGenerating standard-dose PET (SPET) from low-dose PET (LPET) has become a\npotential solution. However, previous studies have primarily focused on single\nlow-dose PET denoising, neglecting two critical factors: discrepancies in dose\nresponse caused by inter-patient variability, and complementary anatomical\nconstraints derived from CT images. In this work, we propose a novel CT-Guided\nMulti-dose Adaptive Attention Denoising Diffusion Model (MDAA-Diff) for\nmulti-dose PET denoising. Our approach integrates anatomical guidance and\ndose-level adaptation to achieve superior denoising performance under low-dose\nconditions. Specifically, this approach incorporates a CT-Guided High-frequency\nWavelet Attention (HWA) module, which uses wavelet transforms to separate\nhigh-frequency anatomical boundary features from CT images. These extracted\nfeatures are then incorporated into PET imaging through an adaptive weighted\nfusion mechanism to enhance edge details. Additionally, we propose the\nDose-Adaptive Attention (DAA) module, a dose-conditioned enhancement mechanism\nthat dynamically integrates dose levels into channel-spatial attention weight\ncalculation. Extensive experiments on 18F-FDG and 68Ga-FAPI datasets\ndemonstrate that MDAA-Diff outperforms state-of-the-art approaches in\npreserving diagnostic quality under reduced-dose conditions. Our code is\npublicly available.", "published": "2025-05-08 10:27:12", "link": "http://arxiv.org/abs/2505.05112v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "MDE-Edit: Masked Dual-Editing for Multi-Object Image Editing via Diffusion Models", "abstract": "Multi-object editing aims to modify multiple objects or regions in complex\nscenes while preserving structural coherence. This task faces significant\nchallenges in scenarios involving overlapping or interacting objects: (1)\nInaccurate localization of target objects due to attention misalignment,\nleading to incomplete or misplaced edits; (2) Attribute-object mismatch, where\ncolor or texture changes fail to align with intended regions due to\ncross-attention leakage, creating semantic conflicts (\\textit{e.g.}, color\nbleeding into non-target areas). Existing methods struggle with these\nchallenges: approaches relying on global cross-attention mechanisms suffer from\nattention dilution and spatial interference between objects, while mask-based\nmethods fail to bind attributes to geometrically accurate regions due to\nfeature entanglement in multi-object scenarios. To address these limitations,\nwe propose a training-free, inference-stage optimization approach that enables\nprecise localized image manipulation in complex multi-object scenes, named\nMDE-Edit. MDE-Edit optimizes the noise latent feature in diffusion models via\ntwo key losses: Object Alignment Loss (OAL) aligns multi-layer cross-attention\nwith segmentation masks for precise object positioning, and Color Consistency\nLoss (CCL) amplifies target attribute attention within masks while suppressing\nleakage to adjacent regions. This dual-loss design ensures localized and\ncoherent multi-object edits. Extensive experiments demonstrate that MDE-Edit\noutperforms state-of-the-art methods in editing accuracy and visual quality,\noffering a robust solution for complex multi-object image manipulation tasks.", "published": "2025-05-08 10:01:14", "link": "http://arxiv.org/abs/2505.05101v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "DispBench: Benchmarking Disparity Estimation to Synthetic Corruptions", "abstract": "Deep learning (DL) has surpassed human performance on standard benchmarks,\ndriving its widespread adoption in computer vision tasks. One such task is\ndisparity estimation, estimating the disparity between matching pixels in\nstereo image pairs, which is crucial for safety-critical applications like\nmedical surgeries and autonomous navigation. However, DL-based disparity\nestimation methods are highly susceptible to distribution shifts and\nadversarial attacks, raising concerns about their reliability and\ngeneralization. Despite these concerns, a standardized benchmark for evaluating\nthe robustness of disparity estimation methods remains absent, hindering\nprogress in the field.\n  To address this gap, we introduce DispBench, a comprehensive benchmarking\ntool for systematically assessing the reliability of disparity estimation\nmethods. DispBench evaluates robustness against synthetic image corruptions\nsuch as adversarial attacks and out-of-distribution shifts caused by 2D Common\nCorruptions across multiple datasets and diverse corruption scenarios. We\nconduct the most extensive performance and robustness analysis of disparity\nestimation methods to date, uncovering key correlations between accuracy,\nreliability, and generalization. Open-source code for DispBench:\nhttps://github.com/shashankskagnihotri/benchmarking_robustness/tree/disparity_estimation/final/disparity_estimation", "published": "2025-05-08 09:40:17", "link": "http://arxiv.org/abs/2505.05091v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Nonlinear Motion-Guided and Spatio-Temporal Aware Network for Unsupervised Event-Based Optical Flow", "abstract": "Event cameras have the potential to capture continuous motion information\nover time and space, making them well-suited for optical flow estimation.\nHowever, most existing learning-based methods for event-based optical flow\nadopt frame-based techniques, ignoring the spatio-temporal characteristics of\nevents. Additionally, these methods assume linear motion between consecutive\nevents within the loss time window, which increases optical flow errors in\nlong-time sequences. In this work, we observe that rich spatio-temporal\ninformation and accurate nonlinear motion between events are crucial for\nevent-based optical flow estimation. Therefore, we propose E-NMSTFlow, a novel\nunsupervised event-based optical flow network focusing on long-time sequences.\nWe propose a Spatio-Temporal Motion Feature Aware (STMFA) module and an\nAdaptive Motion Feature Enhancement (AMFE) module, both of which utilize rich\nspatio-temporal information to learn spatio-temporal data associations.\nMeanwhile, we propose a nonlinear motion compensation loss that utilizes the\naccurate nonlinear motion between events to improve the unsupervised learning\nof our network. Extensive experiments demonstrate the effectiveness and\nsuperiority of our method. Remarkably, our method ranks first among\nunsupervised learning methods on the MVSEC and DSEC-Flow datasets. Our project\npage is available at https://wynelio.github.io/E-NMSTFlow.", "published": "2025-05-08 09:39:19", "link": "http://arxiv.org/abs/2505.05089v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SSH-Net: A Self-Supervised and Hybrid Network for Noisy Image Watermark Removal", "abstract": "Visible watermark removal is challenging due to its inherent complexities and\nthe noise carried within images. Existing methods primarily rely on supervised\nlearning approaches that require paired datasets of watermarked and\nwatermark-free images, which are often impractical to obtain in real-world\nscenarios. To address this challenge, we propose SSH-Net, a Self-Supervised and\nHybrid Network specifically designed for noisy image watermark removal. SSH-Net\nsynthesizes reference watermark-free images using the watermark distribution in\na self-supervised manner and adopts a dual-network design to address the task.\nThe upper network, focused on the simpler task of noise removal, employs a\nlightweight CNN-based architecture, while the lower network, designed to handle\nthe more complex task of simultaneously removing watermarks and noise,\nincorporates Transformer blocks to model long-range dependencies and capture\nintricate image features. To enhance the model's effectiveness, a shared\nCNN-based feature encoder is introduced before dual networks to extract common\nfeatures that both networks can leverage. Our code will be available at\nhttps://github.com/wenyang001/SSH-Net.", "published": "2025-05-08 09:36:49", "link": "http://arxiv.org/abs/2505.05088v1", "categories": ["cs.MM", "cs.CV", "eess.IV"], "primary_category": "cs.MM"}
{"title": "PIDiff: Image Customization for Personalized Identities with Diffusion Models", "abstract": "Text-to-image generation for personalized identities aims at incorporating\nthe specific identity into images using a text prompt and an identity image.\nBased on the powerful generative capabilities of DDPMs, many previous works\nadopt additional prompts, such as text embeddings and CLIP image embeddings, to\nrepresent the identity information, while they fail to disentangle the identity\ninformation and background information. As a result, the generated images not\nonly lose key identity characteristics but also suffer from significantly\nreduced diversity. To address this issue, previous works have combined the W+\nspace from StyleGAN with diffusion models, leveraging this space to provide a\nmore accurate and comprehensive representation of identity features through\nmulti-level feature extraction. However, the entanglement of identity and\nbackground information in in-the-wild images during training prevents accurate\nidentity localization, resulting in severe semantic interference between\nidentity and background. In this paper, we propose a novel fine-tuning-based\ndiffusion model for personalized identities text-to-image generation, named\nPIDiff, which leverages the W+ space and an identity-tailored fine-tuning\nstrategy to avoid semantic entanglement and achieves accurate feature\nextraction and localization. Style editing can also be achieved by PIDiff\nthrough preserving the characteristics of identity features in the W+ space,\nwhich vary from coarse to fine. Through the combination of the proposed\ncross-attention block and parameter optimization strategy, PIDiff preserves the\nidentity information and maintains the generation capability for in-the-wild\nimages of the pre-trained model during inference. Our experimental results\nvalidate the effectiveness of our method in this task.", "published": "2025-05-08 09:26:28", "link": "http://arxiv.org/abs/2505.05081v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "The City that Never Settles: Simulation-based LiDAR Dataset for Long-Term Place Recognition Under Extreme Structural Changes", "abstract": "Large-scale construction and demolition significantly challenge long-term\nplace recognition (PR) by drastically reshaping urban and suburban\nenvironments. Existing datasets predominantly reflect limited or indoor-focused\nchanges, failing to adequately represent extensive outdoor transformations. To\nbridge this gap, we introduce the City that Never Settles (CNS) dataset, a\nsimulation-based dataset created using the CARLA simulator, capturing major\nstructural changes-such as building construction and demolition-across diverse\nmaps and sequences. Additionally, we propose TCR_sym, a symmetric version of\nthe original TCR metric, enabling consistent measurement of structural changes\nirrespective of source-target ordering. Quantitative comparisons demonstrate\nthat CNS encompasses more extensive transformations than current real-world\nbenchmarks. Evaluations of state-of-the-art LiDAR-based PR methods on CNS\nreveal substantial performance degradation, underscoring the need for robust\nalgorithms capable of handling significant environmental changes. Our dataset\nis available at https://github.com/Hyunho111/CNS_dataset.", "published": "2025-05-08 09:16:01", "link": "http://arxiv.org/abs/2505.05076v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Visual Affordances: Enabling Robots to Understand Object Functionality", "abstract": "Human-robot interaction for assistive technologies relies on the prediction\nof affordances, which are the potential actions a robot can perform on objects.\nPredicting object affordances from visual perception is formulated differently\nfor tasks such as grasping detection, affordance classification, affordance\nsegmentation, and hand-object interaction synthesis. In this work, we highlight\nthe reproducibility issue in these redefinitions, making comparative benchmarks\nunfair and unreliable. To address this problem, we propose a unified\nformulation for visual affordance prediction, provide a comprehensive and\nsystematic review of previous works highlighting strengths and limitations of\nmethods and datasets, and analyse what challenges reproducibility. To favour\ntransparency, we introduce the Affordance Sheet, a document to detail the\nproposed solution, the datasets, and the validation. As the physical properties\nof an object influence the interaction with the robot, we present a generic\nframework that links visual affordance prediction to the physical world. Using\nthe weight of an object as an example for this framework, we discuss how\nestimating object mass can affect the affordance prediction. Our approach\nbridges the gap between affordance perception and robot actuation, and accounts\nfor the complete information about objects of interest and how the robot\ninteracts with them to accomplish its task.", "published": "2025-05-08 09:10:05", "link": "http://arxiv.org/abs/2505.05074v1", "categories": ["cs.CV", "cs.RO"], "primary_category": "cs.CV"}
{"title": "RepSNet: A Nucleus Instance Segmentation model based on Boundary Regression and Structural Re-parameterization", "abstract": "Pathological diagnosis is the gold standard for tumor diagnosis, and nucleus\ninstance segmentation is a key step in digital pathology analysis and\npathological diagnosis. However, the computational efficiency of the model and\nthe treatment of overlapping targets are the major challenges in the studies of\nthis problem. To this end, a neural network model RepSNet was designed based on\na nucleus boundary regression and a structural re-parameterization scheme for\nsegmenting and classifying the nuclei in H\\&E-stained histopathological images.\nFirst, RepSNet estimates the boundary position information (BPI) of the parent\nnucleus for each pixel. The BPI estimation incorporates the local information\nof the pixel and the contextual information of the parent nucleus. Then, the\nnucleus boundary is estimated by aggregating the BPIs from a series of pixels\nusing a proposed boundary voting mechanism (BVM), and the instance segmentation\nresults are computed from the estimated nucleus boundary using a connected\ncomponent analysis procedure. The BVM intrinsically achieves a kind of\nsynergistic belief enhancement among the BPIs from various pixels. Therefore,\ndifferent from the methods available in literature that obtain nucleus\nboundaries based on a direct pixel recognition scheme, RepSNet computes its\nboundary decisions based on some guidances from macroscopic information using\nan integration mechanism. In addition, RepSNet employs a re-parametrizable\nencoder-decoder structure. This model can not only aggregate features from some\nreceptive fields with various scales which helps segmentation accuracy\nimprovement, but also reduce the parameter amount and computational burdens in\nthe model inference phase through the structural re-parameterization technique.\nExtensive experiments demonstrated the superiorities of RepSNet compared to\nseveral typical benchmark models.", "published": "2025-05-08 09:08:58", "link": "http://arxiv.org/abs/2505.05073v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "ULFine: Unbiased Lightweight Fine-tuning for Foundation-Model-Assisted Long-Tailed Semi-Supervised Learning", "abstract": "Based on the success of large-scale visual foundation models like CLIP in\nvarious downstream tasks, this paper initially attempts to explore their impact\non Long-Tailed Semi-Supervised Learning (LTSSL) by employing the foundation\nmodel with three strategies: Linear Probing (LP), Lightweight Fine-Tuning\n(LFT), and Full Fine-Tuning (FFT). Our analysis presents the following\ninsights: i) Compared to LTSSL algorithms trained from scratch, FFT results in\na decline in model performance, whereas LP and LFT, although boosting overall\nmodel performance, exhibit negligible benefits to tail classes. ii) LP produces\nnumerous false pseudo-labels due to \\textit{underlearned} training data, while\nLFT can reduce the number of these false labels but becomes overconfident about\nthem owing to \\textit{biased fitting} training data. This exacerbates the\npseudo-labeled and classifier biases inherent in LTSSL, limiting performance\nimprovement in the tail classes. With these insights, we propose a Unbiased\nLightweight Fine-tuning strategy, \\textbf{ULFine}, which mitigates the\noverconfidence via confidence-aware adaptive fitting of textual prototypes and\ncounteracts the pseudo-labeled and classifier biases via complementary fusion\nof dual logits. Extensive experiments demonstrate that ULFine markedly\ndecreases training costs by over ten times and substantially increases\nprediction accuracies compared to state-of-the-art methods.", "published": "2025-05-08 08:54:57", "link": "http://arxiv.org/abs/2505.05062v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "UncertainSAM: Fast and Efficient Uncertainty Quantification of the Segment Anything Model", "abstract": "The introduction of the Segment Anything Model (SAM) has paved the way for\nnumerous semantic segmentation applications. For several tasks, quantifying the\nuncertainty of SAM is of particular interest. However, the ambiguous nature of\nthe class-agnostic foundation model SAM challenges current uncertainty\nquantification (UQ) approaches. This paper presents a theoretically motivated\nuncertainty quantification model based on a Bayesian entropy formulation\njointly respecting aleatoric, epistemic, and the newly introduced task\nuncertainty. We use this formulation to train USAM, a lightweight post-hoc UQ\nmethod. Our model traces the root of uncertainty back to under-parameterised\nmodels, insufficient prompts or image ambiguities. Our proposed deterministic\nUSAM demonstrates superior predictive capabilities on the SA-V, MOSE, ADE20k,\nDAVIS, and COCO datasets, offering a computationally cheap and easy-to-use UQ\nalternative that can support user-prompting, enhance semi-supervised pipelines,\nor balance the tradeoff between accuracy and cost efficiency.", "published": "2025-05-08 08:36:23", "link": "http://arxiv.org/abs/2505.05049v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "xTrace: A Facial Expressive Behaviour Analysis Tool for Continuous Affect Recognition", "abstract": "Recognising expressive behaviours in face videos is a long-standing challenge\nin Affective Computing. Despite significant advancements in recent years, it\nstill remains a challenge to build a robust and reliable system for\nnaturalistic and in-the-wild facial expressive behaviour analysis in real time.\nThis paper addresses two key challenges in building such a system: (1). The\npaucity of large-scale labelled facial affect video datasets with extensive\ncoverage of the 2D emotion space, and (2). The difficulty of extracting facial\nvideo features that are discriminative, interpretable, robust, and\ncomputationally efficient. Toward addressing these challenges, we introduce\nxTrace, a robust tool for facial expressive behaviour analysis and predicting\ncontinuous values of dimensional emotions, namely valence and arousal, from\nin-the-wild face videos.\n  To address challenge (1), our affect recognition model is trained on the\nlargest facial affect video data set, containing ~450k videos that cover most\nemotion zones in the dimensional emotion space, making xTrace highly versatile\nin analysing a wide spectrum of naturalistic expressive behaviours. To address\nchallenge (2), xTrace uses facial affect descriptors that are not only\nexplainable, but can also achieve a high degree of accuracy and robustness with\nlow computational complexity. The key components of xTrace are benchmarked\nagainst three existing tools: MediaPipe, OpenFace, and Augsburg Affect Toolbox.\nOn an in-the-wild validation set composed of 50k videos, xTrace achieves 0.86\nmean CCC and 0.13 mean absolute error values. We present a detailed error\nanalysis of affect predictions from xTrace, illustrating (a). its ability to\nrecognise emotions with high accuracy across most bins in the 2D emotion space,\n(b). its robustness to non-frontal head pose angles, and (c). a strong\ncorrelation between its uncertainty estimates and its accuracy.", "published": "2025-05-08 08:27:37", "link": "http://arxiv.org/abs/2505.05043v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "ADNP-15: An Open-Source Histopathological Dataset for Neuritic Plaque Segmentation in Human Brain Whole Slide Images with Frequency Domain Image Enhancement for Stain Normalization", "abstract": "Alzheimer's Disease (AD) is a neurodegenerative disorder characterized by\namyloid-beta plaques and tau neurofibrillary tangles, which serve as key\nhistopathological features. The identification and segmentation of these\nlesions are crucial for understanding AD progression but remain challenging due\nto the lack of large-scale annotated datasets and the impact of staining\nvariations on automated image analysis. Deep learning has emerged as a powerful\ntool for pathology image segmentation; however, model performance is\nsignificantly influenced by variations in staining characteristics,\nnecessitating effective stain normalization and enhancement techniques. In this\nstudy, we address these challenges by introducing an open-source dataset\n(ADNP-15) of neuritic plaques (i.e., amyloid deposits combined with a crown of\ndystrophic tau-positive neurites) in human brain whole slide images. We\nestablish a comprehensive benchmark by evaluating five widely adopted deep\nlearning models across four stain normalization techniques, providing deeper\ninsights into their influence on neuritic plaque segmentation. Additionally, we\npropose a novel image enhancement method that improves segmentation accuracy,\nparticularly in complex tissue structures, by enhancing structural details and\nmitigating staining inconsistencies. Our experimental results demonstrate that\nthis enhancement strategy significantly boosts model generalization and\nsegmentation accuracy. All datasets and code are open-source, ensuring\ntransparency and reproducibility while enabling further advancements in the\nfield.", "published": "2025-05-08 08:25:44", "link": "http://arxiv.org/abs/2505.05041v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Split Matching for Inductive Zero-shot Semantic Segmentation", "abstract": "Zero-shot Semantic Segmentation (ZSS) aims to segment categories that are not\nannotated during training. While fine-tuning vision-language models has\nachieved promising results, these models often overfit to seen categories due\nto the lack of supervision for unseen classes. As an alternative to fully\nsupervised approaches, query-based segmentation has shown great latent in ZSS,\nas it enables object localization without relying on explicit labels. However,\nconventional Hungarian matching, a core component in query-based frameworks,\nneeds full supervision and often misclassifies unseen categories as background\nin the setting of ZSS. To address this issue, we propose Split Matching (SM), a\nnovel assignment strategy that decouples Hungarian matching into two\ncomponents: one for seen classes in annotated regions and another for latent\nclasses in unannotated regions (referred to as unseen candidates).\nSpecifically, we partition the queries into seen and candidate groups, enabling\neach to be optimized independently according to its available supervision. To\ndiscover unseen candidates, we cluster CLIP dense features to generate pseudo\nmasks and extract region-level embeddings using CLS tokens. Matching is then\nconducted separately for the two groups based on both class-level similarity\nand mask-level consistency. Additionally, we introduce a Multi-scale Feature\nEnhancement (MFE) module that refines decoder features through residual\nmulti-scale aggregation, improving the model's ability to capture spatial\ndetails across resolutions. SM is the first to introduce decoupled Hungarian\nmatching under the inductive ZSS setting, and achieves state-of-the-art\nperformance on two standard benchmarks.", "published": "2025-05-08 07:56:30", "link": "http://arxiv.org/abs/2505.05023v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SOAP: Style-Omniscient Animatable Portraits", "abstract": "Creating animatable 3D avatars from a single image remains challenging due to\nstyle limitations (realistic, cartoon, anime) and difficulties in handling\naccessories or hairstyles. While 3D diffusion models advance single-view\nreconstruction for general objects, outputs often lack animation controls or\nsuffer from artifacts because of the domain gap. We propose SOAP, a\nstyle-omniscient framework to generate rigged, topology-consistent avatars from\nany portrait. Our method leverages a multiview diffusion model trained on 24K\n3D heads with multiple styles and an adaptive optimization pipeline to deform\nthe FLAME mesh while maintaining topology and rigging via differentiable\nrendering. The resulting textured avatars support FACS-based animation,\nintegrate with eyeballs and teeth, and preserve details like braided hair or\naccessories. Extensive experiments demonstrate the superiority of our method\nover state-of-the-art techniques for both single-view head modeling and\ndiffusion-based generation of Image-to-3D. Our code and data are publicly\navailable for research purposes at https://github.com/TingtingLiao/soap.", "published": "2025-05-08 07:56:16", "link": "http://arxiv.org/abs/2505.05022v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Adaptive Contextual Embedding for Robust Far-View Borehole Detection", "abstract": "In controlled blasting operations, accurately detecting densely distributed\ntiny boreholes from far-view imagery is critical for operational safety and\nefficiency. However, existing detection methods often struggle due to small\nobject scales, highly dense arrangements, and limited distinctive visual\nfeatures of boreholes. To address these challenges, we propose an adaptive\ndetection approach that builds upon existing architectures (e.g., YOLO) by\nexplicitly leveraging consistent embedding representations derived through\nexponential moving average (EMA)-based statistical updates.\n  Our method introduces three synergistic components: (1) adaptive augmentation\nutilizing dynamically updated image statistics to robustly handle illumination\nand texture variations; (2) embedding stabilization to ensure consistent and\nreliable feature extraction; and (3) contextual refinement leveraging spatial\ncontext for improved detection accuracy. The pervasive use of EMA in our method\nis particularly advantageous given the limited visual complexity and small\nscale of boreholes, allowing stable and robust representation learning even\nunder challenging visual conditions. Experiments on a challenging proprietary\nquarry-site dataset demonstrate substantial improvements over baseline\nYOLO-based architectures, highlighting our method's effectiveness in realistic\nand complex industrial scenarios.", "published": "2025-05-08 07:25:42", "link": "http://arxiv.org/abs/2505.05008v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Driving with Context: Online Map Matching for Complex Roads Using Lane Markings and Scenario Recognition", "abstract": "Accurate online map matching is fundamental to vehicle navigation and the\nactivation of intelligent driving functions. Current online map matching\nmethods are prone to errors in complex road networks, especially in multilevel\nroad area. To address this challenge, we propose an online Standard Definition\n(SD) map matching method by constructing a Hidden Markov Model (HMM) with\nmultiple probability factors. Our proposed method can achieve accurate map\nmatching even in complex road networks by carefully leveraging lane markings\nand scenario recognition in the designing of the probability factors. First,\nthe lane markings are generated by a multi-lane tracking method and associated\nwith the SD map using HMM to build an enriched SD map. In areas covered by the\nenriched SD map, the vehicle can re-localize itself by performing Iterative\nClosest Point (ICP) registration for the lane markings. Then, the probability\nfactor accounting for the lane marking detection can be obtained using the\nassociation probability between adjacent lanes and roads. Second, the driving\nscenario recognition model is applied to generate the emission probability\nfactor of scenario recognition, which improves the performance of map matching\non elevated roads and ordinary urban roads underneath them. We validate our\nmethod through extensive road tests in Europe and China, and the experimental\nresults show that our proposed method effectively improves the online map\nmatching accuracy as compared to other existing methods, especially in\nmultilevel road area. Specifically, the experiments show that our proposed\nmethod achieves $F_1$ scores of 98.04% and 94.60% on the Zenseact Open Dataset\nand test data of multilevel road areas in Shanghai respectively, significantly\noutperforming benchmark methods. The implementation is available at\nhttps://github.com/TRV-Lab/LMSR-OMM.", "published": "2025-05-08 07:24:52", "link": "http://arxiv.org/abs/2505.05007v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Automated Thoracolumbar Stump Rib Detection and Analysis in a Large CT Cohort", "abstract": "Thoracolumbar stump ribs are one of the essential indicators of thoracolumbar\ntransitional vertebrae or enumeration anomalies. While some studies manually\nassess these anomalies and describe the ribs qualitatively, this study aims to\nautomate thoracolumbar stump rib detection and analyze their morphology\nquantitatively. To this end, we train a high-resolution deep-learning model for\nrib segmentation and show significant improvements compared to existing models\n(Dice score 0.997 vs. 0.779, p-value < 0.01). In addition, we use an iterative\nalgorithm and piece-wise linear interpolation to assess the length of the ribs,\nshowing a success rate of 98.2%. When analyzing morphological features, we show\nthat stump ribs articulate more posteriorly at the vertebrae (-19.2 +- 3.8 vs\n-13.8 +- 2.5, p-value < 0.01), are thinner (260.6 +- 103.4 vs. 563.6 +- 127.1,\np-value < 0.01), and are oriented more downwards and sideways within the first\ncentimeters in contrast to full-length ribs. We show that with partially\nvisible ribs, these features can achieve an F1-score of 0.84 in differentiating\nstump ribs from regular ones. We publish the model weights and masks for public\nuse.", "published": "2025-05-08 07:21:12", "link": "http://arxiv.org/abs/2505.05004v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Inter-Diffusion Generation Model of Speakers and Listeners for Effective Communication", "abstract": "Full-body gestures play a pivotal role in natural interactions and are\ncrucial for achieving effective communication. Nevertheless, most existing\nstudies primarily focus on the gesture generation of speakers, overlooking the\nvital role of listeners in the interaction process and failing to fully explore\nthe dynamic interaction between them. This paper innovatively proposes an\nInter-Diffusion Generation Model of Speakers and Listeners for Effective\nCommunication. For the first time, we integrate the full-body gestures of\nlisteners into the generation framework. By devising a novel inter-diffusion\nmechanism, this model can accurately capture the complex interaction patterns\nbetween speakers and listeners during communication. In the model construction\nprocess, based on the advanced diffusion model architecture, we innovatively\nintroduce interaction conditions and the GAN model to increase the denoising\nstep size. As a result, when generating gesture sequences, the model can not\nonly dynamically generate based on the speaker's speech information but also\nrespond in realtime to the listener's feedback, enabling synergistic\ninteraction between the two. Abundant experimental results demonstrate that\ncompared with the current state-of-the-art gesture generation methods, the\nmodel we proposed has achieved remarkable improvements in the naturalness,\ncoherence, and speech-gesture synchronization of the generated gestures. In the\nsubjective evaluation experiments, users highly praised the generated\ninteraction scenarios, believing that they are closer to real life human\ncommunication situations. Objective index evaluations also show that our model\noutperforms the baseline methods in multiple key indicators, providing more\npowerful support for effective communication.", "published": "2025-05-08 07:00:58", "link": "http://arxiv.org/abs/2505.04996v1", "categories": ["cs.GR", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.GR"}
{"title": "Federated Deconfounding and Debiasing Learning for Out-of-Distribution Generalization", "abstract": "Attribute bias in federated learning (FL) typically leads local models to\noptimize inconsistently due to the learning of non-causal associations,\nresulting degraded performance. Existing methods either use data augmentation\nfor increasing sample diversity or knowledge distillation for learning\ninvariant representations to address this problem. However, they lack a\ncomprehensive analysis of the inference paths, and the interference from\nconfounding factors limits their performance. To address these limitations, we\npropose the \\underline{Fed}erated \\underline{D}econfounding and\n\\underline{D}ebiasing \\underline{L}earning (FedDDL) method. It constructs a\nstructured causal graph to analyze the model inference process, and performs\nbackdoor adjustment to eliminate confounding paths. Specifically, we design an\nintra-client deconfounding learning module for computer vision tasks to\ndecouple background and objects, generating counterfactual samples that\nestablish a connection between the background and any label, which stops the\nmodel from using the background to infer the label. Moreover, we design an\ninter-client debiasing learning module to construct causal prototypes to reduce\nthe proportion of the background in prototype components. Notably, it bridges\nthe gap between heterogeneous representations via causal prototypical\nregularization. Extensive experiments on 2 benchmarking datasets demonstrate\nthat \\methodname{} significantly enhances the model capability to focus on main\nobjects in unseen data, leading to 4.5\\% higher Top-1 Accuracy on average over\n9 state-of-the-art existing methods.", "published": "2025-05-08 06:32:59", "link": "http://arxiv.org/abs/2505.04979v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "ReAlign: Bilingual Text-to-Motion Generation via Step-Aware Reward-Guided Alignment", "abstract": "Bilingual text-to-motion generation, which synthesizes 3D human motions from\nbilingual text inputs, holds immense potential for cross-linguistic\napplications in gaming, film, and robotics. However, this task faces critical\nchallenges: the absence of bilingual motion-language datasets and the\nmisalignment between text and motion distributions in diffusion models, leading\nto semantically inconsistent or low-quality motions. To address these\nchallenges, we propose BiHumanML3D, a novel bilingual human motion dataset,\nwhich establishes a crucial benchmark for bilingual text-to-motion generation\nmodels. Furthermore, we propose a Bilingual Motion Diffusion model (BiMD),\nwhich leverages cross-lingual aligned representations to capture semantics,\nthereby achieving a unified bilingual model. Building upon this, we propose\nReward-guided sampling Alignment (ReAlign) method, comprising a step-aware\nreward model to assess alignment quality during sampling and a reward-guided\nstrategy that directs the diffusion process toward an optimally aligned\ndistribution. This reward model integrates step-aware tokens and combines a\ntext-aligned module for semantic consistency and a motion-aligned module for\nrealism, refining noisy motions at each timestep to balance probability density\nand alignment. Experiments demonstrate that our approach significantly improves\ntext-motion alignment and motion quality compared to existing state-of-the-art\nmethods. Project page: https://wengwanjiang.github.io/ReAlign-page/.", "published": "2025-05-08 06:19:18", "link": "http://arxiv.org/abs/2505.04974v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "DenseGrounding: Improving Dense Language-Vision Semantics for Ego-Centric 3D Visual Grounding", "abstract": "Enabling intelligent agents to comprehend and interact with 3D environments\nthrough natural language is crucial for advancing robotics and human-computer\ninteraction. A fundamental task in this field is ego-centric 3D visual\ngrounding, where agents locate target objects in real-world 3D spaces based on\nverbal descriptions. However, this task faces two significant challenges: (1)\nloss of fine-grained visual semantics due to sparse fusion of point clouds with\nego-centric multi-view images, (2) limited textual semantic context due to\narbitrary language descriptions. We propose DenseGrounding, a novel approach\ndesigned to address these issues by enhancing both visual and textual\nsemantics. For visual features, we introduce the Hierarchical Scene Semantic\nEnhancer, which retains dense semantics by capturing fine-grained global scene\nfeatures and facilitating cross-modal alignment. For text descriptions, we\npropose a Language Semantic Enhancer that leverages large language models to\nprovide rich context and diverse language descriptions with additional context\nduring model training. Extensive experiments show that DenseGrounding\nsignificantly outperforms existing methods in overall accuracy, with\nimprovements of 5.81% and 7.56% when trained on the comprehensive full dataset\nand smaller mini subset, respectively, further advancing the SOTA in egocentric\n3D visual grounding. Our method also achieves 1st place and receives the\nInnovation Award in the CVPR 2024 Autonomous Grand Challenge Multi-view 3D\nVisual Grounding Track, validating its effectiveness and robustness.", "published": "2025-05-08 05:49:06", "link": "http://arxiv.org/abs/2505.04965v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "CAG-VLM: Fine-Tuning of a Large-Scale Model to Recognize Angiographic Images for Next-Generation Diagnostic Systems", "abstract": "Coronary angiography (CAG) is the gold-standard imaging modality for\nevaluating coronary artery disease, but its interpretation and subsequent\ntreatment planning rely heavily on expert cardiologists. To enable AI-based\ndecision support, we introduce a two-stage, physician-curated pipeline and a\nbilingual (Japanese/English) CAG image-report dataset. First, we sample 14,686\nframes from 539 exams and annotate them for key-frame detection and left/right\nlaterality; a ConvNeXt-Base CNN trained on this data achieves 0.96 F1 on\nlaterality classification, even on low-contrast frames. Second, we apply the\nCNN to 243 independent exams, extract 1,114 key frames, and pair each with its\npre-procedure report and expert-validated diagnostic and treatment summary,\nyielding a parallel corpus. We then fine-tune three open-source VLMs\n(PaliGemma2, Gemma3, and ConceptCLIP-enhanced Gemma3) via LoRA and evaluate\nthem using VLScore and cardiologist review. Although PaliGemma2 w/LoRA attains\nthe highest VLScore, Gemma3 w/LoRA achieves the top clinician rating (mean\n7.20/10); we designate this best-performing model as CAG-VLM. These results\ndemonstrate that specialized, fine-tuned VLMs can effectively assist\ncardiologists in generating clinical reports and treatment recommendations from\nCAG images.", "published": "2025-05-08 05:44:52", "link": "http://arxiv.org/abs/2505.04964v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "ViCTr: Vital Consistency Transfer for Pathology Aware Image Synthesis", "abstract": "Synthesizing medical images remains challenging due to limited annotated\npathological data, modality domain gaps, and the complexity of representing\ndiffuse pathologies such as liver cirrhosis. Existing methods often struggle to\nmaintain anatomical fidelity while accurately modeling pathological features,\nfrequently relying on priors derived from natural images or inefficient\nmulti-step sampling. In this work, we introduce ViCTr (Vital Consistency\nTransfer), a novel two-stage framework that combines a rectified flow\ntrajectory with a Tweedie-corrected diffusion process to achieve high-fidelity,\npathology-aware image synthesis. First, we pretrain ViCTr on the ATLAS-8k\ndataset using Elastic Weight Consolidation (EWC) to preserve critical\nanatomical structures. We then fine-tune the model adversarially with Low-Rank\nAdaptation (LoRA) modules for precise control over pathology severity. By\nreformulating Tweedie's formula within a linear trajectory framework, ViCTr\nsupports one-step sampling, reducing inference from 50 steps to just 4, without\nsacrificing anatomical realism. We evaluate ViCTr on BTCV (CT), AMOS (MRI), and\nCirrMRI600+ (cirrhosis) datasets. Results demonstrate state-of-the-art\nperformance, achieving a Medical Frechet Inception Distance (MFID) of 17.01 for\ncirrhosis synthesis 28% lower than existing approaches and improving nnUNet\nsegmentation by +3.8% mDSC when used for data augmentation. Radiologist reviews\nindicate that ViCTr-generated liver cirrhosis MRIs are clinically\nindistinguishable from real scans. To our knowledge, ViCTr is the first method\nto provide fine-grained, pathology-aware MRI synthesis with graded severity\ncontrol, closing a critical gap in AI-driven medical imaging research.", "published": "2025-05-08 05:44:16", "link": "http://arxiv.org/abs/2505.04963v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "An Efficient Method for Accurate Pose Estimation and Error Correction of Cuboidal Objects", "abstract": "The proposed system outlined in this paper is a solution to a use case that\nrequires the autonomous picking of cuboidal objects from an organized or\nunorganized pile with high precision. This paper presents an efficient method\nfor precise pose estimation of cuboid-shaped objects, which aims to reduce\nerrors in target pose in a time-efficient manner. Typical pose estimation\nmethods like global point cloud registrations are prone to minor pose errors\nfor which local registration algorithms are generally used to improve pose\naccuracy. However, due to the execution time overhead and uncertainty in the\nerror of the final achieved pose, an alternate, linear time approach is\nproposed for pose error estimation and correction. This paper presents an\noverview of the solution followed by a detailed description of individual\nmodules of the proposed algorithm.", "published": "2025-05-08 05:43:31", "link": "http://arxiv.org/abs/2505.04962v1", "categories": ["cs.CV", "cs.RO"], "primary_category": "cs.CV"}
{"title": "MoRe-3DGSMR: Motion-resolved reconstruction framework for free-breathing pulmonary MRI based on 3D Gaussian representation", "abstract": "This study presents an unsupervised, motion-resolved reconstruction framework\nfor high-resolution, free-breathing pulmonary magnetic resonance imaging (MRI),\nutilizing a three-dimensional Gaussian representation (3DGS). The proposed\nmethod leverages 3DGS to address the challenges of motion-resolved 3D isotropic\npulmonary MRI reconstruction by enabling data smoothing between voxels for\ncontinuous spatial representation. Pulmonary MRI data acquisition is performed\nusing a golden-angle radial sampling trajectory, with respiratory motion\nsignals extracted from the center of k-space in each radial spoke. Based on the\nestimated motion signal, the k-space data is sorted into multiple respiratory\nphases. A 3DGS framework is then applied to reconstruct a reference image\nvolume from the first motion state. Subsequently, a patient-specific\nconvolutional neural network is trained to estimate the deformation vector\nfields (DVFs), which are used to generate the remaining motion states through\nspatial transformation of the reference volume. The proposed reconstruction\npipeline is evaluated on six datasets from six subjects and bench-marked\nagainst three state-of-the-art reconstruction methods. The experimental\nfindings demonstrate that the proposed reconstruction framework effectively\nreconstructs high-resolution, motion-resolved pulmonary MR images. Compared\nwith existing approaches, it achieves superior image quality, reflected by\nhigher signal-to-noise ratio and contrast-to-noise ratio. The proposed\nunsupervised 3DGS-based reconstruction method enables accurate motion-resolved\npulmonary MRI with isotropic spatial resolution. Its superior performance in\nimage quality metrics over state-of-the-art methods highlights its potential as\na robust solution for clinical pulmonary MR imaging.", "published": "2025-05-08 05:41:46", "link": "http://arxiv.org/abs/2505.04959v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Building-Guided Pseudo-Label Learning for Cross-Modal Building Damage Mapping", "abstract": "Accurate building damage assessment using bi-temporal multi-modal remote\nsensing images is essential for effective disaster response and recovery\nplanning. This study proposes a novel Building-Guided Pseudo-Label Learning\nFramework to address the challenges of mapping building damage from\npre-disaster optical and post-disaster SAR images. First, we train a series of\nbuilding extraction models using pre-disaster optical images and building\nlabels. To enhance building segmentation, we employ multi-model fusion and\ntest-time augmentation strategies to generate pseudo-probabilities, followed by\na low-uncertainty pseudo-label training method for further refinement. Next, a\nchange detection model is trained on bi-temporal cross-modal images and damaged\nbuilding labels. To improve damage classification accuracy, we introduce a\nbuilding-guided low-uncertainty pseudo-label refinement strategy, which\nleverages building priors from the previous step to guide pseudo-label\ngeneration for damaged buildings, reducing uncertainty and enhancing\nreliability. Experimental results on the 2025 IEEE GRSS Data Fusion Contest\ndataset demonstrate the effectiveness of our approach, which achieved the\nhighest mIoU score (54.28%) and secured first place in the competition.", "published": "2025-05-08 04:37:12", "link": "http://arxiv.org/abs/2505.04941v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "FF-PNet: A Pyramid Network Based on Feature and Field for Brain Image Registration", "abstract": "In recent years, deformable medical image registration techniques have made\nsignificant progress. However, existing models still lack efficiency in\nparallel extraction of coarse and fine-grained features. To address this, we\nconstruct a new pyramid registration network based on feature and deformation\nfield (FF-PNet). For coarse-grained feature extraction, we design a Residual\nFeature Fusion Module (RFFM), for fine-grained image deformation, we propose a\nResidual Deformation Field Fusion Module (RDFFM). Through the parallel\noperation of these two modules, the model can effectively handle complex image\ndeformations. It is worth emphasizing that the encoding stage of FF-PNet only\nemploys traditional convolutional neural networks without any attention\nmechanisms or multilayer perceptrons, yet it still achieves remarkable\nimprovements in registration accuracy, fully demonstrating the superior feature\ndecoding capabilities of RFFM and RDFFM. We conducted extensive experiments on\nthe LPBA and OASIS datasets. The results show our network consistently\noutperforms popular methods in metrics like the Dice Similarity Coefficient.", "published": "2025-05-08 04:27:11", "link": "http://arxiv.org/abs/2505.04938v1", "categories": ["cs.CV", "cs.IR"], "primary_category": "cs.CV"}
{"title": "Canny2Palm: Realistic and Controllable Palmprint Generation for Large-scale Pre-training", "abstract": "Palmprint recognition is a secure and privacy-friendly method of biometric\nidentification. One of the major challenges to improve palmprint recognition\naccuracy is the scarcity of palmprint data. Recently, a popular line of\nresearch revolves around the synthesis of virtual palmprints for large-scale\npre-training purposes. In this paper, we propose a novel synthesis method named\nCanny2Palm that extracts palm textures with Canny edge detector and uses them\nto condition a Pix2Pix network for realistic palmprint generation. By\nre-assembling palmprint textures from different identities, we are able to\ncreate new identities by seeding the generator with new assemblies. Canny2Palm\nnot only synthesizes realistic data following the distribution of real\npalmprints but also enables controllable diversity to generate large-scale new\nidentities. On open-set palmprint recognition benchmarks, models pre-trained\nwith Canny2Palm synthetic data outperform the state-of-the-art with up to 7.2%\nhigher identification accuracy. Moreover, the performance of models pre-trained\nwith Canny2Palm continues to improve given 10,000 synthetic IDs while those\nwith existing methods already saturate, demonstrating the potential of our\nmethod for large-scale pre-training.", "published": "2025-05-08 03:37:07", "link": "http://arxiv.org/abs/2505.04922v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "A Simple Detector with Frame Dynamics is a Strong Tracker", "abstract": "Infrared object tracking plays a crucial role in Anti-Unmanned Aerial Vehicle\n(Anti-UAV) applications. Existing trackers often depend on cropped template\nregions and have limited motion modeling capabilities, which pose challenges\nwhen dealing with tiny targets. To address this, we propose a simple yet\neffective infrared tiny-object tracker that enhances tracking performance by\nintegrating global detection and motion-aware learning with temporal priors.\nOur method is based on object detection and achieves significant improvements\nthrough two key innovations. First, we introduce frame dynamics, leveraging\nframe difference and optical flow to encode both prior target features and\nmotion characteristics at the input level, enabling the model to better\ndistinguish the target from background clutter. Second, we propose a trajectory\nconstraint filtering strategy in the post-processing stage, utilizing\nspatio-temporal priors to suppress false positives and enhance tracking\nrobustness. Extensive experiments show that our method consistently outperforms\nexisting approaches across multiple metrics in challenging infrared UAV\ntracking scenarios. Notably, we achieve state-of-the-art performance in the 4th\nAnti-UAV Challenge, securing 1st place in Track 1 and 2nd place in Track 2.", "published": "2025-05-08 03:16:03", "link": "http://arxiv.org/abs/2505.04917v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "GlyphMastero: A Glyph Encoder for High-Fidelity Scene Text Editing", "abstract": "Scene text editing, a subfield of image editing, requires modifying texts in\nimages while preserving style consistency and visual coherence with the\nsurrounding environment. While diffusion-based methods have shown promise in\ntext generation, they still struggle to produce high-quality results. These\nmethods often generate distorted or unrecognizable characters, particularly\nwhen dealing with complex characters like Chinese. In such systems, characters\nare composed of intricate stroke patterns and spatial relationships that must\nbe precisely maintained. We present GlyphMastero, a specialized glyph encoder\ndesigned to guide the latent diffusion model for generating texts with\nstroke-level precision. Our key insight is that existing methods, despite using\npretrained OCR models for feature extraction, fail to capture the hierarchical\nnature of text structures - from individual strokes to stroke-level\ninteractions to overall character-level structure. To address this, our glyph\nencoder explicitly models and captures the cross-level interactions between\nlocal-level individual characters and global-level text lines through our novel\nglyph attention module. Meanwhile, our model implements a feature pyramid\nnetwork to fuse the multi-scale OCR backbone features at the global-level.\nThrough these cross-level and multi-scale fusions, we obtain more detailed\nglyph-aware guidance, enabling precise control over the scene text generation\nprocess. Our method achieves an 18.02\\% improvement in sentence accuracy over\nthe state-of-the-art multi-lingual scene text editing baseline, while\nsimultaneously reducing the text-region Fr\\'echet inception distance by\n53.28\\%.", "published": "2025-05-08 03:11:58", "link": "http://arxiv.org/abs/2505.04915v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Advanced 3D Imaging Approach to TSV/TGV Metrology and Inspection Using Only Optical Microscopy", "abstract": "This paper introduces an innovative approach to silicon and glass via\ninspection, which combines hybrid field microscopy with photometric stereo.\nConventional optical microscopy techniques are generally limited to superficial\ninspections and struggle to effectively visualize the internal structures of\nsilicon and glass vias. By utilizing various lighting conditions for 3D\nreconstruction, the proposed method surpasses these limitations. By integrating\nphotometric stereo to the traditional optical microscopy, the proposed method\nnot only enhances the capability to detect micro-scale defects but also\nprovides a detailed visualization of depth and edge abnormality, which are\ntypically not visible with conventional optical microscopy inspection. The\nexperimental results demonstrated that the proposed method effectively captures\nintricate surface details and internal structures. Quantitative comparisons\nbetween the reconstructed models and actual measurements present the capability\nof the proposed method to significantly improve silicon and glass via\ninspection process. As a result, the proposed method achieves enhanced\ncost-effectiveness while maintaining high accuracy and repeatability,\nsuggesting substantial advancements in silicon and glass via inspection\ntechniques", "published": "2025-05-08 03:09:27", "link": "http://arxiv.org/abs/2505.04913v1", "categories": ["eess.IV", "cs.CV", "physics.optics"], "primary_category": "eess.IV"}
{"title": "Pro2SAM: Mask Prompt to SAM with Grid Points for Weakly Supervised Object Localization", "abstract": "Weakly Supervised Object Localization (WSOL), which aims to localize objects\nby only using image-level labels, has attracted much attention because of its\nlow annotation cost in real applications. Current studies focus on the Class\nActivation Map (CAM) of CNN and the self-attention map of transformer to\nidentify the region of objects. However, both CAM and self-attention maps can\nnot learn pixel-level fine-grained information on the foreground objects, which\nhinders the further advance of WSOL. To address this problem, we initiatively\nleverage the capability of zero-shot generalization and fine-grained\nsegmentation in Segment Anything Model (SAM) to boost the activation of\nintegral object regions. Further, to alleviate the semantic ambiguity issue\naccrued in single point prompt-based SAM, we propose an innovative mask prompt\nto SAM (Pro2SAM) network with grid points for WSOL task. First, we devise a\nGlobal Token Transformer (GTFormer) to generate a coarse-grained foreground map\nas a flexible mask prompt, where the GTFormer jointly embeds patch tokens and\nnovel global tokens to learn foreground semantics. Secondly, we deliver grid\npoints as dense prompts into SAM to maximize the probability of foreground\nmask, which avoids the lack of objects caused by a single point/box prompt.\nFinally, we propose a pixel-level similarity metric to come true the mask\nmatching from mask prompt to SAM, where the mask with the highest score is\nviewed as the final localization map. Experiments show that the proposed\nPro2SAM achieves state-of-the-art performance on both CUB-200-2011 and ILSVRC,\nwith 84.03\\% and 66.85\\% Top-1 Loc, respectively.", "published": "2025-05-08 02:44:53", "link": "http://arxiv.org/abs/2505.04905v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "OWT: A Foundational Organ-Wise Tokenization Framework for Medical Imaging", "abstract": "Recent advances in representation learning often rely on holistic, black-box\nembeddings that entangle multiple semantic components, limiting\ninterpretability and generalization. These issues are especially critical in\nmedical imaging. To address these limitations, we propose an Organ-Wise\nTokenization (OWT) framework with a Token Group-based Reconstruction (TGR)\ntraining paradigm. Unlike conventional approaches that produce holistic\nfeatures, OWT explicitly disentangles an image into separable token groups,\neach corresponding to a distinct organ or semantic entity. Our design ensures\neach token group encapsulates organ-specific information, boosting\ninterpretability, generalization, and efficiency while allowing fine-grained\ncontrol in downstream tasks. Experiments on CT and MRI datasets demonstrate the\neffectiveness of OWT in not only achieving strong image reconstruction and\nsegmentation performance, but also enabling novel semantic-level generation and\nretrieval applications that are out of reach for standard holistic embedding\nmethods. These findings underscore the potential of OWT as a foundational\nframework for semantically disentangled representation learning, offering broad\nscalability and applicability to real-world medical imaging scenarios and\nbeyond.", "published": "2025-05-08 02:30:44", "link": "http://arxiv.org/abs/2505.04899v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Mix-QSAM: Mixed-Precision Quantization of the Segment Anything Model", "abstract": "The Segment Anything Model (SAM) is a popular vision foundation model;\nhowever, its high computational and memory demands make deployment on\nresource-constrained devices challenging. While Post-Training Quantization\n(PTQ) is a practical approach for reducing computational overhead, existing PTQ\nmethods rely on fixed bit-width quantization, leading to suboptimal accuracy\nand efficiency. To address this limitation, we propose Mix-QSAM, a\nmixed-precision PTQ framework for SAM. First, we introduce a layer-wise\nimportance score, derived using Kullback-Leibler (KL) divergence, to quantify\neach layer's contribution to the model's output. Second, we introduce\ncross-layer synergy, a novel metric based on causal mutual information, to\ncapture dependencies between adjacent layers. This ensures that highly\ninterdependent layers maintain similar bit-widths, preventing abrupt precision\nmismatches that degrade feature propagation and numerical stability. Using\nthese metrics, we formulate an Integer Quadratic Programming (IQP) problem to\ndetermine optimal bit-width allocation under model size and bit-operation\nconstraints, assigning higher precision to critical layers while minimizing\nbit-width in less influential layers. Experimental results demonstrate that\nMix-QSAM consistently outperforms existing PTQ methods on instance segmentation\nand object detection tasks, achieving up to 20% higher average precision under\n6-bit and 4-bit mixed-precision settings, while maintaining computational\nefficiency.", "published": "2025-05-08 00:08:31", "link": "http://arxiv.org/abs/2505.04861v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Sideways on the highways", "abstract": "We present two generalised ants (LLRRRL and LLRLRLL) which admit both highway\nbehaviours and other kinds of emergent behaviours from initially finite\nconfigurations. This limits the well known Highway conjecture on Langton's ant\nas it shows that a generalised version of this conjecture generically does not\nhold on generalised ants.", "published": "2025-05-08 17:14:05", "link": "http://arxiv.org/abs/2505.05426v1", "categories": ["cs.DM"], "primary_category": "cs.DM"}
{"title": "p-complete square-free Word-representation of Word-representable Graphs", "abstract": "A graph $G = (V,E)$ is word-representable, if there exists a word $w$ over\nthe alphabet $V$ such that for letters ${x,y} \\in V$ , $x$ and $y$ alternate in\n$w$ if and only if $xy$ is an edge in the graph $G$. In this paper, we\nintroduce the concept of $p$-complete square-free word-representable graph\n$G(V,E)$. A word $w$ defined over alphabet $V$ is called $p$-complete\nsquare-free word if there does not exist any subset $S\\subseteq \\Sigma$ such\nthat the word $w_{S}$ contains a square $XX$ where $|X| \\ge p$ and $1\\le p \\le\n|w|/2$. A word-representable graph is considered $p$-complete square-free\nword-representable if there exists a $p$-complete square-free word-representant\nof that graph. This pattern is significant as it proves the existence of\npatterns that do not depend on graph labelling and cannot be avoided by certain\nclasses of word-representable graphs. The class of word-representable graphs\nincludes both $p$-complete square-free word-representable graphs and\nnon-$p$-complete square-free word-representable graphs. Additionally, this\nconcept generalises the square pattern found in the words. A word-representable\ngraph is $p$-complete square-free uniform word-representable if its\n$p$-complete square-free word-representant is a uniform word. We analyse the\nproperties of $p$-complete square-free uniform words and find that the graphs\nrepresented by these words avoid having $K_p$ (the complete graph on $p$\nvertices) as an induced subgraph. We provide classifications for small values\nof $p$: for $p=1$, only complete graphs and for $p=2$, only complete and\nedgeless graphs satisfy the condition. We find that $K_3$-free circle graphs\nare 3-complete square-free uniform word-representable. Furthermore, we\nestablish that only graphs with representation number at most 3 can be\n3-complete square-free uniform word-representable and provide a constructive\nmethod to generate such graphs.", "published": "2025-05-08 10:21:12", "link": "http://arxiv.org/abs/2505.05110v1", "categories": ["cs.DM", "math.CO"], "primary_category": "cs.DM"}
{"title": "Artifact Sharing for Information Retrieval Research", "abstract": "Sharing artifacts -- such as trained models, pre-built indexes, and the code\nto use them -- aids in reproducibility efforts by allowing researchers to\nvalidate intermediate steps and improves the sustainability of research by\nallowing multiple groups to build off one another's prior computational work.\nAlthough there are de facto consensuses on how to share research code (through\na git repository linked to from publications) and trained models (via\nHuggingFace Hub), there is no consensus for other types of artifacts, such as\nbuilt indexes. Given the practical utility of using shared indexes, researchers\nhave resorted to self-hosting these resources or performing ad hoc file\ntransfers upon request, ultimately limiting the artifacts' discoverability and\nreuse. This demonstration introduces a flexible and interoperable way to share\nartifacts for Information Retrieval research, improving both their\naccessibility and usability.", "published": "2025-05-08 17:23:32", "link": "http://arxiv.org/abs/2505.05434v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Stealthy LLM-Driven Data Poisoning Attacks Against Embedding-Based Retrieval-Augmented Recommender Systems", "abstract": "We present a systematic study of provider-side data poisoning in\nretrieval-augmented recommender systems (RAG-based). By modifying only a small\nfraction of tokens within item descriptions -- for instance, adding emotional\nkeywords or borrowing phrases from semantically related items -- an attacker\ncan significantly promote or demote targeted items. We formalize these attacks\nunder token-edit and semantic-similarity constraints, and we examine their\neffectiveness in both promotion (long-tail items) and demotion (short-head\nitems) scenarios. Our experiments on MovieLens, using two large language model\n(LLM) retrieval modules, show that even subtle attacks shift final rankings and\nitem exposures while eluding naive detection. The results underscore the\nvulnerability of RAG-based pipelines to small-scale metadata rewrites and\nemphasize the need for robust textual consistency checks and provenance\ntracking to thwart stealthy provider-side poisoning.", "published": "2025-05-08 12:53:42", "link": "http://arxiv.org/abs/2505.05196v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Hybrid Personalization Using Declarative and Procedural Memory Modules of the Cognitive Architecture ACT-R", "abstract": "Recommender systems often rely on sub-symbolic machine learning approaches\nthat operate as opaque black boxes. These approaches typically fail to account\nfor the cognitive processes that shape user preferences and decision-making. In\nthis vision paper, we propose a hybrid user modeling framework based on the\ncognitive architecture ACT-R that integrates symbolic and sub-symbolic\nrepresentations of human memory. Our goal is to combine ACT-R's declarative\nmemory, which is responsible for storing symbolic chunks along sub-symbolic\nactivations, with its procedural memory, which contains symbolic production\nrules. This integration will help simulate how users retrieve past experiences\nand apply decision-making strategies. With this approach, we aim to provide\nmore transparent recommendations, enable rule-based explanations, and\nfacilitate the modeling of cognitive biases. We argue that our approach has the\npotential to inform the design of a new generation of human-centered,\npsychology-informed recommender systems.", "published": "2025-05-08 09:32:04", "link": "http://arxiv.org/abs/2505.05083v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Divide-and-Conquer: Cold-Start Bundle Recommendation via Mixture of Diffusion Experts", "abstract": "Cold-start bundle recommendation focuses on modeling new bundles with\ninsufficient information to provide recommendations. Advanced bundle\nrecommendation models usually learn bundle representations from multiple views\n(e.g., interaction view) at both the bundle and item levels. Consequently, the\ncold-start problem for bundles is more challenging than that for traditional\nitems due to the dual-level multi-view complexity. In this paper, we propose a\nnovel Mixture of Diffusion Experts (MoDiffE) framework, which employs a\ndivide-and-conquer strategy for cold-start bundle recommendation and follows\nthree steps:(1) Divide: The bundle cold-start problem is divided into\nindependent but similar sub-problems sequentially by level and view, which can\nbe summarized as the poor representation of feature-missing bundles in\nprior-embedding models. (2) Conquer: Beyond prior-embedding models that\nfundamentally provide the embedded representations, we introduce a\ndiffusion-based method to solve all sub-problems in a unified way, which\ndirectly generates diffusion representations using diffusion models without\ndepending on specific features. (3) Combine: A cold-aware hierarchical Mixture\nof Experts (MoE) is employed to combine results of the sub-problems for final\nrecommendations, where the two models for each view serve as experts and are\nadaptively fused for different bundles in a multi-layer manner. Additionally,\nMoDiffE adopts a multi-stage decoupled training pipeline and introduces a\ncold-start gating augmentation method to enable the training of gating for cold\nbundles. Through extensive experiments on three real-world datasets, we\ndemonstrate that MoDiffE significantly outperforms existing solutions in\nhandling cold-start bundle recommendation. It achieves up to a 0.1027 absolute\ngain in Recall@20 in cold-start scenarios and up to a 47.43\\% relative\nimprovement in all-bundle scenarios.", "published": "2025-05-08 08:13:44", "link": "http://arxiv.org/abs/2505.05035v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "LSRP: A Leader-Subordinate Retrieval Framework for Privacy-Preserving Cloud-Device Collaboration", "abstract": "Cloud-device collaboration leverages on-cloud Large Language Models (LLMs)\nfor handling public user queries and on-device Small Language Models (SLMs) for\nprocessing private user data, collectively forming a powerful and\nprivacy-preserving solution. However, existing approaches often fail to fully\nleverage the scalable problem-solving capabilities of on-cloud LLMs while\nunderutilizing the advantage of on-device SLMs in accessing and processing\npersonalized data. This leads to two interconnected issues: 1) Limited\nutilization of the problem-solving capabilities of on-cloud LLMs, which fail to\nalign with personalized user-task needs, and 2) Inadequate integration of user\ndata into on-device SLM responses, resulting in mismatches in contextual user\ninformation.\n  In this paper, we propose a Leader-Subordinate Retrieval framework for\nPrivacy-preserving cloud-device collaboration (LSRP), a novel solution that\nbridges these gaps by: 1) enhancing on-cloud LLM guidance to on-device SLM\nthrough a dynamic selection of task-specific leader strategies named as\nuser-to-user retrieval-augmented generation (U-U-RAG), and 2) integrating the\ndata advantages of on-device SLMs through small model feedback Direct\nPreference Optimization (SMFB-DPO) for aligning the on-cloud LLM with the\non-device SLM. Experiments on two datasets demonstrate that LSRP consistently\noutperforms state-of-the-art baselines, significantly improving question-answer\nrelevance and personalization, while preserving user privacy through efficient\non-device retrieval. Our code is available at:\nhttps://github.com/Zhang-Yingyi/LSRP.", "published": "2025-05-08 08:06:34", "link": "http://arxiv.org/abs/2505.05031v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Learning Item Representations Directly from Multimodal Features for Effective Recommendation", "abstract": "Conventional multimodal recommender systems predominantly leverage Bayesian\nPersonalized Ranking (BPR) optimization to learn item representations by\namalgamating item identity (ID) embeddings with multimodal features.\nNevertheless, our empirical and theoretical findings unequivocally demonstrate\na pronounced optimization gradient bias in favor of acquiring representations\nfrom multimodal features over item ID embeddings. As a consequence, item ID\nembeddings frequently exhibit suboptimal characteristics despite the\nconvergence of multimodal feature parameters. Given the rich informational\ncontent inherent in multimodal features, in this paper, we propose a novel\nmodel (i.e., LIRDRec) that learns item representations directly from these\nfeatures to augment recommendation performance. Recognizing that features\nderived from each modality may capture disparate yet correlated aspects of\nitems, we propose a multimodal transformation mechanism, integrated with\nmodality-specific encoders, to effectively fuse features from all modalities.\nMoreover, to differentiate the influence of diverse modality types, we devise a\nprogressive weight copying fusion module within LIRDRec. This module\nincrementally learns the weight assigned to each modality in synthesizing the\nfinal user or item representations. Finally, we utilize the powerful visual\nunderstanding of Multimodal Large Language Models (MLLMs) to convert the item\nimages into texts and extract semantics embeddings upon the texts via LLMs.\nEmpirical evaluations conducted on five real-world datasets validate the\nsuperiority of our approach relative to competing baselines. It is worth noting\nthe proposed model, equipped with embeddings extracted from MLLMs and LLMs, can\nfurther improve the recommendation accuracy of NDCG@20 by an average of 4.21%\ncompared to the original embeddings.", "published": "2025-05-08 05:42:22", "link": "http://arxiv.org/abs/2505.04960v1", "categories": ["cs.IR", "cs.MM"], "primary_category": "cs.IR"}
{"title": "A Connection Between Learning to Reject and Bhattacharyya Divergences", "abstract": "Learning to reject provide a learning paradigm which allows for our models to\nabstain from making predictions. One way to learn the rejector is to learn an\nideal marginal distribution (w.r.t. the input domain) - which characterizes a\nhypothetical best marginal distribution - and compares it to the true marginal\ndistribution via a density ratio. In this paper, we consider learning a joint\nideal distribution over both inputs and labels; and develop a link between\nrejection and thresholding different statistical divergences. We further find\nthat when one considers a variant of the log-loss, the rejector obtained by\nconsidering the joint ideal distribution corresponds to the thresholding of the\nskewed Bhattacharyya divergence between class-probabilities. This is in\ncontrast to the marginal case - that is equivalent to a typical\ncharacterization of optimal rejection, Chow's Rule - which corresponds to a\nthresholding of the Kullback-Leibler divergence. In general, we find that\nrejecting via a Bhattacharyya divergence is less aggressive than Chow's Rule.", "published": "2025-05-08 14:18:42", "link": "http://arxiv.org/abs/2505.05273v1", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT"], "primary_category": "stat.ML"}
{"title": "Bounds on $k$-hash distances and rates of linear codes", "abstract": "In this paper, we bound the rate of linear codes in $\\mathbb{F}_q^n$ with the\nproperty that any $k \\leq q$ codewords are all simultaneously distinct in at\nleast $d_k$ coordinates. For the particular case $d_k=1$, this leads to bounds\non the rate of linear $q$-ary $k$-hash codes which generalize, with a simpler\nproof, results recently obtained for the case $q=k=3$ by Pohoata and Zakharov\nand by Bishnoi D'haeseleeer and Gijswijt. We finally discuss some related open\nproblems on the list-decoding zero-error capacity of discrete memoryless\nchannels.", "published": "2025-05-08 13:34:02", "link": "http://arxiv.org/abs/2505.05239v1", "categories": ["cs.IT", "math.CO", "math.IT"], "primary_category": "cs.IT"}
{"title": "Smoothed analysis in compressed sensing", "abstract": "Arbitrary matrices $M \\in \\mathbb{R}^{m \\times n}$, randomly perturbed in an\nadditive manner using a random matrix $R \\in \\mathbb{R}^{m \\times n}$, are\nshown to asymptotically almost surely satisfy the so-called {\\sl robust null\nspace property} whilst asymptotically meeting the optimal number of\nmeasurements required for {\\sl unique reconstruction} via $\\ell_1$-minimisation\nalgorithms. A wide range of random perturbation matrices is considered; in\nthat, $R$ is allowed to be sub-gaussian, sub-exponential, as well as extremely\nheavy-tailed, where only the first $\\log n$ moments of each entry of $R$ are\nbounded. A key tool driving our proofs is {\\sl Mendelson's small-ball method}\n({\\em Learning without concentration}, J. ACM, Vol. $62$, $2015$).", "published": "2025-05-08 12:37:36", "link": "http://arxiv.org/abs/2505.05188v1", "categories": ["math.PR", "cs.IT", "math.IT"], "primary_category": "math.PR"}
{"title": "Testing Message-Passing Concurrency", "abstract": "A key computational question underpinning the automated testing and\nverification of concurrent programs is the \\emph{consistency question} --\n\\emph{given a partial execution history, can it be completed in a consistent\nmanner?} Due to its importance, consistency testing has been studied\nextensively for memory models, as well as for database isolation levels. A\ncommon theme in all these settings is the use of shared-memory as the primal\nmode of interthread communication. On the other hand, modern programming\nlanguages, such as Go, Rust and Kotlin, advocate a paradigm shift towards\nchannel-based (i.e., message-passing) communication. However, the consistency\nquestion for channel-based concurrency is currently poorly understood.\n  In this paper we lift the study of fundamental consistency problems to\nchannels, taking into account various input parameters, such as the number of\nthreads executing, the number of channels, and the channel capacities. We draw\na rich complexity landscape, including upper bounds that become polynomial when\ncertain input parameters are fixed, as well as hardness lower bounds. Our upper\nbounds are based on novel algorithms that can drive the verification of channel\nconsistency in automated verification tools. Our lower bounds characterize\nminimal input parameters that are sufficient for hardness to arise, and thus\nshed light on the intricacies of testing channel-based concurrency. In\ncombination, our upper and lower bounds characterize the boundary of\n\\emph{tractability/intractability} of verifying channel consistency, and imply\nthat our algorithms are often (nearly) optimal.", "published": "2025-05-08 11:57:28", "link": "http://arxiv.org/abs/2505.05162v1", "categories": ["cs.PL", "cs.IT", "math.IT"], "primary_category": "cs.PL"}
{"title": "Balancing Client Participation in Federated Learning Using AoI", "abstract": "Federated Learning (FL) offers a decentralized framework that preserves data\nprivacy while enabling collaborative model training across distributed clients.\nHowever, FL faces significant challenges due to limited communication\nresources, statistical heterogeneity, and the need for balanced client\nparticipation. This paper proposes an Age of Information (AoI)-based client\nselection policy that addresses these challenges by minimizing load imbalance\nthrough controlled selection intervals. Our method employs a decentralized\nMarkov scheduling policy, allowing clients to independently manage\nparticipation based on age-dependent selection probabilities, which balances\nclient updates across training rounds with minimal central oversight. We\nprovide a convergence proof for our method, demonstrating that it ensures\nstable and efficient model convergence. Specifically, we derive optimal\nparameters for the Markov selection model to achieve balanced and consistent\nclient participation, highlighting the benefits of AoI in enhancing convergence\nstability. Through extensive simulations, we demonstrate that our AoI-based\nmethod, particularly the optimal Markov variant, improves convergence over the\nFedAvg selection approach across both IID and non-IID data settings by $7.5\\%$\nand up to $20\\%$. Our findings underscore the effectiveness of AoI-based\nscheduling for scalable, fair, and efficient FL systems across diverse learning\nenvironments.", "published": "2025-05-08 09:55:28", "link": "http://arxiv.org/abs/2505.05099v1", "categories": ["cs.LG", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "ItDPDM: Information-Theoretic Discrete Poisson Diffusion Model", "abstract": "Existing methods for generative modeling of discrete data, such as symbolic\nmusic tokens, face two primary challenges: (1) they either embed discrete\ninputs into continuous state-spaces or (2) rely on variational losses that only\napproximate the true negative log-likelihood. Previous efforts have\nindividually targeted these limitations. While information-theoretic Gaussian\ndiffusion models alleviate the suboptimality of variational losses, they still\nperform modeling in continuous domains. In this work, we introduce the\nInformation-Theoretic Discrete Poisson Diffusion Model (ItDPDM), which\nsimultaneously addresses both limitations by directly operating in a discrete\nstate-space via a Poisson diffusion process inspired by photon arrival\nprocesses in camera sensors. We introduce a novel Poisson Reconstruction Loss\n(PRL) and derive an exact relationship between PRL and the true negative\nlog-likelihood, thereby eliminating the need for approximate evidence lower\nbounds. Experiments conducted on the Lakh MIDI symbolic music dataset and the\nCIFAR-10 image benchmark demonstrate that ItDPDM delivers significant\nimprovements, reducing test NLL by up to 80% compared to prior baselines, while\nalso achieving faster convergence.", "published": "2025-05-08 09:29:05", "link": "http://arxiv.org/abs/2505.05082v1", "categories": ["cs.LG", "cs.IT", "math.IT", "math.PR"], "primary_category": "cs.LG"}
{"title": "Statistical CSI Acquisition for Multi-frequency Massive MIMO Systems", "abstract": "Multi-frequency massive multi-input multi-output (MIMO) communication is a\npromising strategy for both 5G and future 6G systems, ensuring reliable\ntransmission while enhancing frequency resource utilization. Statistical\nchannel state information (CSI) has been widely adopted in multi-frequency\nmassive MIMO transmissions to reduce overhead and improve transmission\nperformance. In this paper, we propose efficient and accurate methods for\nobtaining statistical CSI in multi-frequency massive MIMO systems. First, we\nintroduce a multi-frequency massive MIMO channel model and analyze the mapping\nrelationship between two types of statistical CSI, namely the angular power\nspectrum (APS) and the spatial covariance matrix, along with their correlation\nacross different frequency bands. Next, we propose an autoregressive (AR)\nmethod to predict the spatial covariance matrix of any frequency band based on\nthat of another frequency band. Furthermore, we emphasize that channels across\ndifferent frequency bands share similar APS characteristics. Leveraging the\nmaximum entropy (ME) criterion, we develop a low-complexity algorithm for\nhigh-resolution APS estimation. Simulation results validate the effectiveness\nof the AR-based covariance prediction method and demonstrate the\nhigh-resolution estimation capability of the ME-based approach. Furthermore, we\ndemonstrate the effectiveness of multi-frequency cooperative transmission by\napplying the proposed methods to obtain statistical CSI from low-frequency\nbands and utilizing it for high-frequency channel transmission. This approach\nsignificantly enhances high-frequency transmission performance while\neffectively reducing system overhead.", "published": "2025-05-08 08:30:25", "link": "http://arxiv.org/abs/2505.05045v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Diffusion-enabled Secure Semantic Communication Against Eavesdropping", "abstract": "In this paper, AN is introduced into semantic communication systems for the\nfirst time to prevent semantic eavesdropping. However, the introduction of AN\nalso poses challenges for the legitimate receiver in extracting semantic\ninformation. Recently, denoising diffusion probabilistic models (DDPM) have\ndemonstrated their powerful capabilities in generating multimedia content.\nHere, the paired pluggable modules are carefully designed using DDPM.\nSpecifically, the pluggable encryption module generates AN and adds it to the\noutput of the semantic transmitter, while the pluggable decryption module\nbefore semantic receiver uses DDPM to generate the detailed semantic\ninformation by removing both AN and the channel noise. In the scenario where\nthe transmitter lacks eavesdropper's knowledge, the artificial Gaussian noise\n(AGN) is used as AN. We first model a power allocation optimization problem to\ndetermine the power of AGN, in which the objective is to minimize the weighted\nsum of data reconstruction error of legal link, the mutual information of\nillegal link, and the channel input distortion. Then, a deep reinforcement\nlearning framework using deep deterministic policy gradient is proposed to\nsolve the optimization problem. In the scenario where the transmitter is aware\nof the eavesdropper's knowledge, we propose an AN generation method based on\nadversarial residual networks (ARN). Unlike the previous scenario, the mutual\ninformation term in the objective function is replaced by the confidence of\neavesdropper correctly\n  retrieving private information. The adversarial residual network is then\ntrained to minimize the modified objective function. The simulation results\nshow that the diffusion-enabled pluggable encryption module prevents semantic\neavesdropping while the pluggable decryption module achieves the high-quality\nsemantic communication.", "published": "2025-05-08 07:47:52", "link": "http://arxiv.org/abs/2505.05018v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Dynamic Precoding for Near-Field Secure Communications: Implementation and Performance Analysis", "abstract": "The increase in antenna apertures and transmission frequencies in\nnext-generation wireless networks is catalyzing advancements in near-field\ncommunications (NFC). In this paper, we investigate secure transmission in\nnear-field multi-user multiple-input single-output (MU-MISO) scenarios.\nSpecifically, with the advent of extremely large-scale antenna arrays (ELAA)\napplied in the NFC regime, the spatial degrees of freedom in the channel matrix\nare significantly enhanced. This creates an expanded null space that can be\nexploited for designing secure communication schemes. Motivated by this\nobservation, we propose a near-field dynamic hybrid beamforming architecture\nincorporating artificial noise, which effectively disrupts eavesdroppers at any\nundesired positions, even in the absence of their channel state information\n(CSI). Furthermore, we comprehensively analyze the dynamic precoder's\nperformance in terms of the average signal-to-interference-plus-noise ratio,\nachievable rate, secrecy capacity, secrecy outage probability, and the size of\nthe secrecy zone. In contrast to far-field secure transmission techniques that\nonly enhance security in the angular dimension, the proposed algorithm exploits\nthe unique properties of spherical wave characteristics in NFC to achieve\nsecure transmission in both the angular and distance dimensions. Remarkably,\nthe proposed algorithm is applicable to arbitrary modulation types and array\nconfigurations. Numerical results demonstrate that the proposed method achieves\napproximately 20\\% higher rate capacity compared to zero-forcing and the\nweighted minimum mean squared error precoders.", "published": "2025-05-08 05:59:57", "link": "http://arxiv.org/abs/2505.04968v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Fluid Antenna-Assisted MU-MIMO Systems with Decentralized Baseband Processing", "abstract": "The fluid antenna system (FAS) has emerged as a disruptive technology,\noffering unprecedented degrees of freedom (DoF) for wireless communication\nsystems. However, optimizing fluid antenna (FA) positions entails significant\ncomputational costs, especially when the number of FAs is large. To address\nthis challenge, we introduce a decentralized baseband processing (DBP)\narchitecture to FAS, which partitions the FA array into clusters and enables\nparallel processing. Based on the DBP architecture, we formulate a weighted sum\nrate (WSR) maximization problem through joint beamforming and FA position\ndesign for FA-assisted multiuser multiple-input multiple-output (MU-MIMO)\nsystems. To solve the WSR maximization problem, we propose a novel\ndecentralized block coordinate ascent (BCA)-based algorithm that leverages\nmatrix fractional programming (FP) and majorization-minimization (MM) methods.\nThe proposed decentralized algorithm achieves low computational, communication,\nand storage costs, thus unleashing the potential of the DBP architecture.\nSimulation results show that our proposed algorithm under the DBP architecture\nreduces computational time by over 70% compared to centralized architectures\nwith negligible WSR performance loss.", "published": "2025-05-08 04:26:21", "link": "http://arxiv.org/abs/2505.04936v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Massive MIMO-OFDM Channel Acquisition with Time-Frequency Phase-Shifted Pilots", "abstract": "In this paper, we propose a channel acquisition approach with time-frequency\nphase-shifted pilots (TFPSPs) for massive multi-input multi-output orthogonal\nfrequency division multiplexing (MIMO-OFDM) systems. We first present a\ntriple-beam (TB) based channel tensor model, allowing for the representation of\nthe space-frequency-time (SFT) domain channel as the product of beam matrices\nand the TB domain channel tensor. By leveraging the specific characteristics of\nTB domain channels, we develop TFPSPs, where distinct pilot signals are\nsimultaneously transmitted in the frequency and time domains. Then, we present\nthe optimal TFPSP design and provide the corresponding pilot scheduling\nalgorithm. Further, we propose a tensor-based information geometry approach\n(IGA) to estimate the TB domain channel tensors. Leveraging the specific\nstructure of beam matrices and the properties of TFPSPs, we propose a\nlow-complexity implementation of the tensor-based IGA. We validate the\nefficiency of our proposed channel acquisition approach through extensive\nsimulations. Simulation results demonstrate the superior performance of our\napproach. The proposed approach can effectively suppress inter-UT interference\nwith low complexity and limited pilot overhead, thereby enhancing channel\nestimation performance. Particularly in scenarios with a large number of UTs,\nthe channel acquisition method outperforms existing approaches by reducing the\nnormalized mean square error (NMSE) by more than 8 dB.", "published": "2025-05-08 04:24:21", "link": "http://arxiv.org/abs/2505.04933v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Accurate and Fast Channel Estimation for Fluid Antenna Systems with Diffusion Models", "abstract": "Fluid antenna systems (FAS) offer enhanced spatial diversity for\nnext-generation wireless systems. However, acquiring accurate channel state\ninformation (CSI) remains challenging due to the large number of reconfigurable\nports and the limited availability of radio-frequency (RF) chains --\nparticularly in high-dimensional FAS scenarios. To address this challenge, we\npropose an efficient posterior sampling-based channel estimator that leverages\na diffusion model (DM) with a simplified U-Net architecture to capture the\nspatial correlation structure of two-dimensional FAS channels. The DM is\ninitially trained offline in an unsupervised way and then applied online as a\nlearned implicit prior to reconstruct CSI from partial observations via\nposterior sampling through a denoising diffusion restoration model (DDRM). To\naccelerate the online inference, we introduce a skipped sampling strategy that\nupdates only a subset of latent variables during the sampling process, thereby\nreducing the computational cost with minimal accuracy degradation. Simulation\nresults demonstrate that the proposed approach achieves significantly higher\nestimation accuracy and over 20x speedup compared to state-of-the-art\ncompressed sensing-based methods, highlighting its potential for practical\ndeployment in high-dimensional FAS.", "published": "2025-05-08 04:09:31", "link": "http://arxiv.org/abs/2505.04930v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Max-Min Secrecy Rate and Secrecy Energy Efficiency Optimization for RIS-Aided VLC Systems: RSMA Versus NOMA", "abstract": "Integrating VLC with the RIS significantly enhances physical layer security\nby enabling precise directional signal control and dynamic adaptation to the\ncommunication environment. These capabilities strengthen the confidentiality\nand security of VLC systems. This paper presents a comprehensive study on the\njoint optimization of VLC AP power allocation, RIS association, and RIS\nelements orientation angles for secure VLC systems, while considering RSMA and\npower-domain NOMA schemes. Specifically, two frameworks are proposed to\nmaximize both the minimum secrecy rate (SR) and the minimum secrecy energy\nefficiency (SEE) by jointly optimizing power allocation, RIS association, and\nRIS elements orientation angles for both power-domain NOMA and RSMA-based VLC\nsystems. The proposed frameworks consider random device orientation and\nguarantee the minimum user-rate requirement. The proposed optimization\nframeworks belong to the class of mixed integer nonlinear programming, which\nhas no known feasible solution methodology to guarantee the optimal solution.\nMoreover, the increased degree of freedom and flexibility from the joint\nconsideration of power control, RIS association and element orientation results\nin a large set of decision variables and constraints, which further complicates\nthe optimization problem. To that end, we utilize a genetic algorithm-based\nsolution method, which through its exploration and exploitation capabilities\ncan obtain a good quality solution. Additionally, comprehensive simulations\nshow that the RSMA scheme outperforms the power-domain NOMA scheme across both\nthe SR and SEE metrics over various network parameters. Furthermore, useful\ninsights on the impact of minimum user rate requirement, number of RIS\nelements, and maximum VLC AP transmit power on the minimum SR and SEE\nperformances are provided.", "published": "2025-05-08 02:01:49", "link": "http://arxiv.org/abs/2505.04893v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Facets of Disparate Impact: Evaluating Legally Consistent Bias in Machine Learning", "abstract": "Leveraging current legal standards, we define bias through the lens of\nmarginal benefits and objective testing with the novel metric \"Objective\nFairness Index\". This index combines the contextual nuances of objective\ntesting with metric stability, providing a legally consistent and reliable\nmeasure. Utilizing the Objective Fairness Index, we provide fresh insights into\nsensitive machine learning applications, such as COMPAS (recidivism\nprediction), highlighting the metric's practical and theoretical significance.\nThe Objective Fairness Index allows one to differentiate between discriminatory\ntests and systemic disparities.", "published": "2025-05-08 17:58:49", "link": "http://arxiv.org/abs/2505.05471v1", "categories": ["cs.CY", "cs.LG"], "primary_category": "cs.CY"}
{"title": "RL-DAUNCE: Reinforcement Learning-Driven Data Assimilation with Uncertainty-Aware Constrained Ensembles", "abstract": "Machine learning has become a powerful tool for enhancing data assimilation.\nWhile supervised learning remains the standard method, reinforcement learning\n(RL) offers unique advantages through its sequential decision-making framework,\nwhich naturally fits the iterative nature of data assimilation by dynamically\nbalancing model forecasts with observations. We develop RL-DAUNCE, a new\nRL-based method that enhances data assimilation with physical constraints\nthrough three key aspects. First, RL-DAUNCE inherits the computational\nefficiency of machine learning while it uniquely structures its agents to\nmirror ensemble members in conventional data assimilation methods. Second,\nRL-DAUNCE emphasizes uncertainty quantification by advancing multiple ensemble\nmembers, moving beyond simple mean-state optimization. Third, RL-DAUNCE's\nensemble-as-agents design facilitates the enforcement of physical constraints\nduring the assimilation process, which is crucial to improving the state\nestimation and subsequent forecasting. A primal-dual optimization strategy is\ndeveloped to enforce constraints, which dynamically penalizes the reward\nfunction to ensure constraint satisfaction throughout the learning process.\nAlso, state variable bounds are respected by constraining the RL action space.\nTogether, these features ensure physical consistency without sacrificing\nefficiency. RL-DAUNCE is applied to the Madden-Julian Oscillation, an\nintermittent atmospheric phenomenon characterized by strongly non-Gaussian\nfeatures and multiple physical constraints. RL-DAUNCE outperforms the standard\nensemble Kalman filter (EnKF), which fails catastrophically due to the\nviolation of physical constraints. Notably, RL-DAUNCE matches the performance\nof constrained EnKF, particularly in recovering intermittent signals, capturing\nextreme events, and quantifying uncertainties, while requiring substantially\nless computational effort.", "published": "2025-05-08 17:43:35", "link": "http://arxiv.org/abs/2505.05452v1", "categories": ["cs.LG", "math-ph", "math.MP"], "primary_category": "cs.LG"}
{"title": "Robustly optimal dynamics for active matter reservoir computing", "abstract": "We study the information processing abilities of active matter in the\nreservoir computing (RC) paradigm, using a model that is externally driven to\ninfer the future state of a chaotic signal. The simulated system closely\nfollows a previously reported model. We uncover an exceptional dynamical regime\nof agent dynamics that has been overlooked heretofore. It appears robustly\noptimal across varying physical parameters and inference tasks, thus providing\nvaluable insights into computation and inference with physical systems more\ngenerally. The ability to form effective mechanisms for information processing\nare primarily determined by the system's own intrinsic relaxation abilities.\nThese are identifiable when probing the system without a specific inference\ngoal and manifest when testing minimalistic single-particle reservoirs. The\nregime that achieves optimal computation is situated just below the critical\ndamping threshold, involving a microscopic dynamical relaxation with multiple\nstages. The optimal system is adaptable under chaotic external driving, due to\na diversity in response mechanisms that emerge like rapid alternations between\nquasi-stationary and highly nonlinear dynamical states. Both coherent and\nincoherent dynamics contribute to their operation, partly at dissimilar scales\nof space and delay time. Correlations on agent dynamics can indicate the\nbest-performing regimes and onsets of tight relationships between the\nresponding system and the fluctuating driver. As this model of computation is\ninterpretable in physical terms, it facilitates re-framing inquiries regarding\nlearning and unconventional computing with a fresh rationale for many-body\nphysics out of equilibrium.", "published": "2025-05-08 17:09:14", "link": "http://arxiv.org/abs/2505.05420v1", "categories": ["nlin.AO", "cond-mat.soft", "cs.LG", "physics.comp-ph"], "primary_category": "nlin.AO"}
{"title": "DPQ-HD: Post-Training Compression for Ultra-Low Power Hyperdimensional Computing", "abstract": "Hyperdimensional Computing (HDC) is emerging as a promising approach for edge\nAI, offering a balance between accuracy and efficiency. However, current\nHDC-based applications often rely on high-precision models and/or encoding\nmatrices to achieve competitive performance, which imposes significant\ncomputational and memory demands, especially for ultra-low power devices. While\nrecent efforts use techniques like precision reduction and pruning to increase\nthe efficiency, most require retraining to maintain performance, making them\nexpensive and impractical. To address this issue, we propose a novel Post\nTraining Compression algorithm, Decomposition-Pruning-Quantization (DPQ-HD),\nwhich aims at compressing the end-to-end HDC system, achieving near floating\npoint performance without the need of retraining. DPQ-HD reduces computational\nand memory overhead by uniquely combining the above three compression\ntechniques and efficiently adapts to hardware constraints. Additionally, we\nintroduce an energy-efficient inference approach that progressively evaluates\nsimilarity scores such as cosine similarity and performs early exit to reduce\nthe computation, accelerating prediction inference while maintaining accuracy.\nWe demonstrate that DPQ-HD achieves up to 20-100x reduction in memory for image\nand graph classification tasks with only a 1-2% drop in accuracy compared to\nuncompressed workloads. Lastly, we show that DPQ-HD outperforms the existing\npost-training compression methods and performs better or at par with\nretraining-based state-of-the-art techniques, requiring significantly less\noverall optimization time (up to 100x) and faster inference (up to 56x) on a\nmicrocontroller", "published": "2025-05-08 16:54:48", "link": "http://arxiv.org/abs/2505.05413v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Hide & Seek: Transformer Symmetries Obscure Sharpness & Riemannian Geometry Finds It", "abstract": "The concept of sharpness has been successfully applied to traditional\narchitectures like MLPs and CNNs to predict their generalization. For\ntransformers, however, recent work reported weak correlation between flatness\nand generalization. We argue that existing sharpness measures fail for\ntransformers, because they have much richer symmetries in their attention\nmechanism that induce directions in parameter space along which the network or\nits loss remain identical. We posit that sharpness must account fully for these\nsymmetries, and thus we redefine it on a quotient manifold that results from\nquotienting out the transformer symmetries, thereby removing their ambiguities.\nLeveraging tools from Riemannian geometry, we propose a fully general notion of\nsharpness, in terms of a geodesic ball on the symmetry-corrected quotient\nmanifold. In practice, we need to resort to approximating the geodesics. Doing\nso up to first order yields existing adaptive sharpness measures, and we\ndemonstrate that including higher-order terms is crucial to recover correlation\nwith generalization. We present results on diagonal networks with synthetic\ndata, and show that our geodesic sharpness reveals strong correlation for\nreal-world transformers on both text and image classification tasks.", "published": "2025-05-08 16:51:03", "link": "http://arxiv.org/abs/2505.05409v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Representing spherical tensors with scalar-based machine-learning models", "abstract": "Rotational symmetry plays a central role in physics, providing an elegant\nframework to describe how the properties of 3D objects -- from atoms to the\nmacroscopic scale -- transform under the action of rigid rotations. Equivariant\nmodels of 3D point clouds are able to approximate structure-property relations\nin a way that is fully consistent with the structure of the rotation group, by\ncombining intermediate representations that are themselves spherical tensors.\nThe symmetry constraints however make this approach computationally demanding\nand cumbersome to implement, which motivates increasingly popular unconstrained\narchitectures that learn approximate symmetries as part of the training\nprocess. In this work, we explore a third route to tackle this learning\nproblem, where equivariant functions are expressed as the product of a scalar\nfunction of the point cloud coordinates and a small basis of tensors with the\nappropriate symmetry. We also propose approximations of the general expressions\nthat, while lacking universal approximation properties, are fast, simple to\nimplement, and accurate in practical settings.", "published": "2025-05-08 16:45:28", "link": "http://arxiv.org/abs/2505.05404v1", "categories": ["physics.chem-ph", "cs.LG", "stat.ML"], "primary_category": "physics.chem-ph"}
{"title": "Denoising Diffusion Probabilistic Models for Coastal Inundation Forecasting", "abstract": "Coastal flooding poses significant risks to communities, necessitating fast\nand accurate forecasting methods to mitigate potential damage. To approach this\nproblem, we present DIFF-FLOOD, a probabilistic spatiotemporal forecasting\nmethod designed based on denoising diffusion models. DIFF-FLOOD predicts\ninundation level at a location by taking both spatial and temporal context into\naccount. It utilizes inundation levels at neighboring locations and digital\nelevation data as spatial context. Inundation history from a context time\nwindow, together with additional co-variates are used as temporal context.\nConvolutional neural networks and cross-attention mechanism are then employed\nto capture the spatiotemporal dynamics in the data. We trained and tested\nDIFF-FLOOD on coastal inundation data from the Eastern Shore of Virginia, a\nregion highly impacted by coastal flooding. Our results show that, DIFF-FLOOD\noutperforms existing forecasting methods in terms of prediction performance (6%\nto 64% improvement in terms of two performance metrics) and scalability.", "published": "2025-05-08 16:13:41", "link": "http://arxiv.org/abs/2505.05381v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "From Sleep Staging to Spindle Detection: Evaluating End-to-End Automated Sleep Analysis", "abstract": "Automation of sleep analysis, including both macrostructural (sleep stages)\nand microstructural (e.g., sleep spindles) elements, promises to enable\nlarge-scale sleep studies and to reduce variance due to inter-rater\nincongruencies. While individual steps, such as sleep staging and spindle\ndetection, have been studied separately, the feasibility of automating\nmulti-step sleep analysis remains unclear. Here, we evaluate whether a fully\nautomated analysis using state-of-the-art machine learning models for sleep\nstaging (RobustSleepNet) and subsequent spindle detection (SUMOv2) can\nreplicate findings from an expert-based study of bipolar disorder. The\nautomated analysis qualitatively reproduced key findings from the expert-based\nstudy, including significant differences in fast spindle densities between\nbipolar patients and healthy controls, accomplishing in minutes what previously\ntook months to complete manually. While the results of the automated analysis\ndiffered quantitatively from the expert-based study, possibly due to biases\nbetween expert raters or between raters and the models, the models individually\nperformed at or above inter-rater agreement for both sleep staging and spindle\ndetection. Our results demonstrate that fully automated approaches have the\npotential to facilitate large-scale sleep research. We are providing public\naccess to the tools used in our automated analysis by sharing our code and\nintroducing SomnoBot, a privacy-preserving sleep analysis platform.", "published": "2025-05-08 16:07:10", "link": "http://arxiv.org/abs/2505.05371v1", "categories": ["eess.SP", "cs.LG", "q-bio.NC"], "primary_category": "eess.SP"}
{"title": "Nearly Optimal Sample Complexity for Learning with Label Proportions", "abstract": "We investigate Learning from Label Proportions (LLP), a partial information\nsetting where examples in a training set are grouped into bags, and only\naggregate label values in each bag are available. Despite the partial\nobservability, the goal is still to achieve small regret at the level of\nindividual examples. We give results on the sample complexity of LLP under\nsquare loss, showing that our sample complexity is essentially optimal. From an\nalgorithmic viewpoint, we rely on carefully designed variants of Empirical Risk\nMinimization, and Stochastic Gradient Descent algorithms, combined with ad hoc\nvariance reduction techniques. On one hand, our theoretical results improve in\nimportant ways on the existing literature on LLP, specifically in the way the\nsample complexity depends on the bag size. On the other hand, we validate our\nalgorithmic solutions on several datasets, demonstrating improved empirical\nperformance (better accuracy for less samples) against recent baselines.", "published": "2025-05-08 15:45:23", "link": "http://arxiv.org/abs/2505.05355v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Operator-Level Quantum Acceleration of Non-Logconcave Sampling", "abstract": "Sampling from probability distributions of the form $\\sigma \\propto e^{-\\beta\nV}$, where $V$ is a continuous potential, is a fundamental task across physics,\nchemistry, biology, computer science, and statistics. However, when $V$ is\nnon-convex, the resulting distribution becomes non-logconcave, and classical\nmethods such as Langevin dynamics often exhibit poor performance. We introduce\nthe first quantum algorithm that provably accelerates a broad class of\ncontinuous-time sampling dynamics. For Langevin dynamics, our method encodes\nthe target Gibbs measure into the amplitudes of a quantum state, identified as\nthe kernel of a block matrix derived from a factorization of the Witten\nLaplacian operator. This connection enables Gibbs sampling via singular value\nthresholding and yields the first provable quantum advantage with respect to\nthe Poincar\\'e constant in the non-logconcave setting. Building on this\nframework, we further develop the first quantum algorithm that accelerates\nreplica exchange Langevin diffusion, a widely used method for sampling from\ncomplex, rugged energy landscapes.", "published": "2025-05-08 14:43:17", "link": "http://arxiv.org/abs/2505.05301v1", "categories": ["quant-ph", "cs.LG", "math.OC"], "primary_category": "quant-ph"}
{"title": "Performance Estimation in Binary Classification Using Calibrated Confidence", "abstract": "Model monitoring is a critical component of the machine learning lifecycle,\nsafeguarding against undetected drops in the model's performance after\ndeployment. Traditionally, performance monitoring has required access to ground\ntruth labels, which are not always readily available. This can result in\nunacceptable latency or render performance monitoring altogether impossible.\nRecently, methods designed to estimate the accuracy of classifier models\nwithout access to labels have shown promising results. However, there are\nvarious other metrics that might be more suitable for assessing model\nperformance in many cases. Until now, none of these important metrics has\nreceived similar interest from the scientific community. In this work, we\naddress this gap by presenting CBPE, a novel method that can estimate any\nbinary classification metric defined using the confusion matrix. In particular,\nwe choose four metrics from this large family: accuracy, precision, recall, and\nF$_1$, to demonstrate our method. CBPE treats the elements of the confusion\nmatrix as random variables and leverages calibrated confidence scores of the\nmodel to estimate their distributions. The desired metric is then also treated\nas a random variable, whose full probability distribution can be derived from\nthe estimated confusion matrix. CBPE is shown to produce estimates that come\nwith strong theoretical guarantees and valid confidence intervals.", "published": "2025-05-08 14:34:44", "link": "http://arxiv.org/abs/2505.05295v1", "categories": ["cs.LG", "I.2.6"], "primary_category": "cs.LG"}
{"title": "Morphologically Symmetric Reinforcement Learning for Ambidextrous Bimanual Manipulation", "abstract": "Humans naturally exhibit bilateral symmetry in their gross manipulation\nskills, effortlessly mirroring simple actions between left and right hands.\nBimanual robots-which also feature bilateral symmetry-should similarly exploit\nthis property to perform tasks with either hand. Unlike humans, who often favor\na dominant hand for fine dexterous skills, robots should ideally execute\nambidextrous manipulation with equal proficiency. To this end, we introduce\nSYMDEX (SYMmetric DEXterity), a reinforcement learning framework for\nambidextrous bi-manipulation that leverages the robot's inherent bilateral\nsymmetry as an inductive bias. SYMDEX decomposes complex bimanual manipulation\ntasks into per-hand subtasks and trains dedicated policies for each. By\nexploiting bilateral symmetry via equivariant neural networks, experience from\none arm is inherently leveraged by the opposite arm. We then distill the\nsubtask policies into a global ambidextrous policy that is independent of the\nhand-task assignment. We evaluate SYMDEX on six challenging simulated\nmanipulation tasks and demonstrate successful real-world deployment on two of\nthem. Our approach strongly outperforms baselines on complex task in which the\nleft and right hands perform different roles. We further demonstrate SYMDEX's\nscalability by extending it to a four-arm manipulation setup, where our\nsymmetry-aware policies enable effective multi-arm collaboration and\ncoordination. Our results highlight how structural symmetry as inductive bias\nin policy learning enhances sample efficiency, robustness, and generalization\nacross diverse dexterous manipulation tasks.", "published": "2025-05-08 14:29:00", "link": "http://arxiv.org/abs/2505.05287v1", "categories": ["cs.RO", "cs.LG"], "primary_category": "cs.RO"}
{"title": "A Two-Sample Test of Text Generation Similarity", "abstract": "The surge in digitized text data requires reliable inferential methods on\nobserved textual patterns. This article proposes a novel two-sample text test\nfor comparing similarity between two groups of documents. The hypothesis is\nwhether the probabilistic mapping generating the textual data is identical\nacross two groups of documents. The proposed test aims to assess text\nsimilarity by comparing the entropy of the documents. Entropy is estimated\nusing neural network-based language models. The test statistic is derived from\nan estimation-and-inference framework, where the entropy is first approximated\nusing an estimation set, followed by inference on the remaining data set. We\nshowed theoretically that under mild conditions, the test statistic\nasymptotically follows a normal distribution. A multiple data-splitting\nstrategy is proposed to enhance test power, which combines p-values into a\nunified decision. Various simulation studies and a real data example\ndemonstrated that the proposed two-sample text test maintains the nominal Type\none error rate while offering greater power compared to existing methods. The\nproposed method provides a novel solution to assert differences in document\nclasses, particularly in fields where large-scale textual information is\ncrucial.", "published": "2025-05-08 14:15:53", "link": "http://arxiv.org/abs/2505.05269v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "ICNN-enhanced 2SP: Leveraging input convex neural networks for solving two-stage stochastic programming", "abstract": "Two-stage stochastic programming (2SP) offers a basic framework for modelling\ndecision-making under uncertainty, yet scalability remains a challenge due to\nthe computational complexity of recourse function evaluation. Existing\nlearning-based methods like Neural Two-Stage Stochastic Programming (Neur2SP)\nemploy neural networks (NNs) as recourse function surrogates but rely on\ncomputationally intensive mixed-integer programming (MIP) formulations. We\npropose ICNN-enhanced 2SP, a method that leverages Input Convex Neural Networks\n(ICNNs) to exploit linear programming (LP) representability in convex 2SP\nproblems. By architecturally enforcing convexity and enabling exact inference\nthrough LP, our approach eliminates the need for integer variables inherent to\nthe conventional MIP-based formulation while retaining an exact embedding of\nthe ICNN surrogate within the 2SP framework. This results in a more\ncomputationally efficient alternative that maintains solution quality.\nComprehensive experiments reveal that ICNNs incur only marginally longer\ntraining times while achieving validation accuracy on par with their MIP-based\ncounterparts. Across benchmark problems, ICNN-enhanced 2SP often exhibits\nconsiderably faster solution times than the MIP-based formulations while\npreserving solution quality, with these advantages becoming significantly more\npronounced as problem scale increases. For the most challenging instances, the\nmethod achieves speedups of up to 100$\\times$ and solution quality superior to\nMIP-based formulations.", "published": "2025-05-08 14:06:38", "link": "http://arxiv.org/abs/2505.05261v1", "categories": ["math.OC", "cs.LG"], "primary_category": "math.OC"}
{"title": "Enhancing Treatment Effect Estimation via Active Learning: A Counterfactual Covering Perspective", "abstract": "Although numerous complex algorithms for treatment effect estimation have\nbeen developed in recent years, their effectiveness remains limited when\nhandling insufficiently labeled training sets due to the high cost of labeling\nthe effect after treatment, e.g., expensive tumor imaging or biopsy procedures\nneeded to evaluate treatment effects. Therefore, it becomes essential to\nactively incorporate more high-quality labeled data, all while adhering to a\nconstrained labeling budget. To enable data-efficient treatment effect\nestimation, we formalize the problem through rigorous theoretical analysis\nwithin the active learning context, where the derived key measures --\n\\textit{factual} and \\textit{counterfactual covering radius} determine the risk\nupper bound. To reduce the bound, we propose a greedy radius reduction\nalgorithm, which excels under an idealized, balanced data distribution. To\ngeneralize to more realistic data distributions, we further propose FCCM, which\ntransforms the optimization objective into the \\textit{Factual} and\n\\textit{Counterfactual Coverage Maximization} to ensure effective radius\nreduction during data acquisition. Furthermore, benchmarking FCCM against other\nbaselines demonstrates its superiority across both fully synthetic and\nsemi-synthetic datasets.", "published": "2025-05-08 13:42:00", "link": "http://arxiv.org/abs/2505.05242v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Latte: Transfering LLMs` Latent-level Knowledge for Few-shot Tabular Learning", "abstract": "Few-shot tabular learning, in which machine learning models are trained with\na limited amount of labeled data, provides a cost-effective approach to\naddressing real-world challenges. The advent of Large Language Models (LLMs)\nhas sparked interest in leveraging their pre-trained knowledge for few-shot\ntabular learning. Despite promising results, existing approaches either rely on\ntest-time knowledge extraction, which introduces undesirable latency, or\ntext-level knowledge, which leads to unreliable feature engineering. To\novercome these limitations, we propose Latte, a training-time knowledge\nextraction framework that transfers the latent prior knowledge within LLMs to\noptimize a more generalized downstream model. Latte enables general\nknowledge-guided downstream tabular learning, facilitating the weighted fusion\nof information across different feature values while reducing the risk of\noverfitting to limited labeled data. Furthermore, Latte is compatible with\nexisting unsupervised pre-training paradigms and effectively utilizes available\nunlabeled samples to overcome the performance limitations imposed by an\nextremely small labeled dataset. Extensive experiments on various few-shot\ntabular learning benchmarks demonstrate the superior performance of Latte,\nestablishing it as a state-of-the-art approach in this domain", "published": "2025-05-08 13:32:09", "link": "http://arxiv.org/abs/2505.05237v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "GFlowNets for Active Learning Based Resource Allocation in Next Generation Wireless Networks", "abstract": "In this work, we consider the radio resource allocation problem in a wireless\nsystem with various integrated functionalities, such as communication, sensing\nand computing. We design suitable resource management techniques that can\nsimultaneously cater to those heterogeneous requirements, and scale\nappropriately with the high-dimensional and discrete nature of the problem. We\npropose a novel active learning framework where resource allocation patterns\nare drawn sequentially, evaluated in the environment, and then used to\niteratively update a surrogate model of the environment. Our method leverages a\ngenerative flow network (GFlowNet) to sample favorable solutions, as such\nmodels are trained to generate compositional objects proportionally to their\ntraining reward, hence providing an appropriate coverage of its modes. As such,\nGFlowNet generates diverse and high return resource management designs that\nupdate the surrogate model and swiftly discover suitable solutions. We provide\nsimulation results showing that our method can allocate radio resources\nachieving 20% performance gains against benchmarks, while requiring less than\nhalf of the number of acquisition rounds.", "published": "2025-05-08 13:16:40", "link": "http://arxiv.org/abs/2505.05224v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Long-Term Individual Causal Effect Estimation via Identifiable Latent Representation Learning", "abstract": "Estimating long-term causal effects by combining long-term observational and\nshort-term experimental data is a crucial but challenging problem in many\nreal-world scenarios. In existing methods, several ideal assumptions, e.g.\nlatent unconfoundedness assumption or additive equi-confounding bias\nassumption, are proposed to address the latent confounder problem raised by the\nobservational data. However, in real-world applications, these assumptions are\ntypically violated which limits their practical effectiveness. In this paper,\nwe tackle the problem of estimating the long-term individual causal effects\nwithout the aforementioned assumptions. Specifically, we propose to utilize the\nnatural heterogeneity of data, such as data from multiple sources, to identify\nlatent confounders, thereby significantly avoiding reliance on idealized\nassumptions. Practically, we devise a latent representation learning-based\nestimator of long-term causal effects. Theoretically, we establish the\nidentifiability of latent confounders, with which we further achieve long-term\neffect identification. Extensive experimental studies, conducted on multiple\nsynthetic and semi-synthetic datasets, demonstrate the effectiveness of our\nproposed method.", "published": "2025-05-08 12:42:49", "link": "http://arxiv.org/abs/2505.05192v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "OpenworldAUC: Towards Unified Evaluation and Optimization for Open-world Prompt Tuning", "abstract": "Prompt tuning adapts Vision-Language Models like CLIP to open-world tasks\nwith minimal training costs. In this direction, one typical paradigm evaluates\nmodel performance separately on known classes (i.e., base domain) and unseen\nclasses (i.e., new domain). However, real-world scenarios require models to\nhandle inputs without prior domain knowledge. This practical challenge has\nspurred the development of open-world prompt tuning, which demands a unified\nevaluation of two stages: 1) detecting whether an input belongs to the base or\nnew domain (P1), and 2) classifying the sample into its correct class (P2).\nWhat's more, as domain distributions are generally unknown, a proper metric\nshould be insensitive to varying base/new sample ratios (P3). However, we find\nthat current metrics, including HM, overall accuracy, and AUROC, fail to\nsatisfy these three properties simultaneously. To bridge this gap, we propose\nOpenworldAUC, a unified metric that jointly assesses detection and\nclassification through pairwise instance comparisons. To optimize OpenworldAUC\neffectively, we introduce Gated Mixture-of-Prompts (GMoP), which employs\ndomain-specific prompts and a gating mechanism to dynamically balance detection\nand classification. Theoretical guarantees ensure generalization of GMoP under\npractical conditions. Experiments on 15 benchmarks in open-world scenarios show\nGMoP achieves SOTA performance on OpenworldAUC and other metrics. We release\nthe code at https://github.com/huacong/OpenworldAUC", "published": "2025-05-08 12:31:40", "link": "http://arxiv.org/abs/2505.05180v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Bandit Max-Min Fair Allocation", "abstract": "In this paper, we study a new decision-making problem called the bandit\nmax-min fair allocation (BMMFA) problem. The goal of this problem is to\nmaximize the minimum utility among agents with additive valuations by\nrepeatedly assigning indivisible goods to them. One key feature of this problem\nis that each agent's valuation for each item can only be observed through the\nsemi-bandit feedback, while existing work supposes that the item values are\nprovided at the beginning of each round. Another key feature is that the\nalgorithm's reward function is not additive with respect to rounds, unlike most\nbandit-setting problems.\n  Our first contribution is to propose an algorithm that has an asymptotic\nregret bound of $O(m\\sqrt{T}\\ln T/n + m\\sqrt{T \\ln(mnT)})$, where $n$ is the\nnumber of agents, $m$ is the number of items, and $T$ is the time horizon. This\nis based on a novel combination of bandit techniques and a resource allocation\nalgorithm studied in the literature on competitive analysis. Our second\ncontribution is to provide the regret lower bound of $\\Omega(m\\sqrt{T}/n)$.\nWhen $T$ is sufficiently larger than $n$, the gap between the upper and lower\nbounds is a logarithmic factor of $T$.", "published": "2025-05-08 12:09:20", "link": "http://arxiv.org/abs/2505.05169v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Local linear Fr\u00e9chet curve regression in manifolds", "abstract": "Global Fr\\'echet functional regression has been recently addressed from time\ncorrelated bivariate curve data evaluated in a manifold (see Torres et al.\n2025). For this type of curve data sets, the present paper solves the problem\nof local linear approximation of the Fr\\'echet conditional mean in an extrinsic\nand intrinsic way. The extrinsic local linear Fr\\'echet functional regression\npredictor is obtained in the time varying tangent space by projection into an\northornormal basis of the ambient Hilbert space. The conditions assumed ensure\nthe existence and uniqueness of this predictor, and its computation via\nexponential and logarithmic maps. A weighted Fr\\'echet mean approach is adopted\nin the computation of an intrinsic local linear Fr\\'echet functional regression\npredictor. The asymptotic optimality of this intrinsic local approximation is\nalso proved. The performance of the empirical version of both, extrinsic and\nintrinsic functional predictors, and of a Nadaraya-Watson type Fr\\'echet curve\npredictor is illustrated in the simulation study undertaken. The finite-sample\nsize properties are also tested in a real-data application via\ncross-validation. Specifically, functional prediction of the magnetic vector\nfield from the time-varying geocentric latitude and longitude of the satellite\nNASA's MAGSAT spacecraft is addressed.", "published": "2025-05-08 12:06:59", "link": "http://arxiv.org/abs/2505.05168v1", "categories": ["math.ST", "cs.LG", "stat.ML", "stat.TH"], "primary_category": "math.ST"}
{"title": "FedTDP: A Privacy-Preserving and Unified Framework for Trajectory Data Preparation via Federated Learning", "abstract": "Trajectory data, which capture the movement patterns of people and vehicles\nover time and space, are crucial for applications like traffic optimization and\nurban planning. However, issues such as noise and incompleteness often\ncompromise data quality, leading to inaccurate trajectory analyses and limiting\nthe potential of these applications. While Trajectory Data Preparation (TDP)\ncan enhance data quality, existing methods suffer from two key limitations: (i)\nthey do not address data privacy concerns, particularly in federated settings\nwhere trajectory data sharing is prohibited, and (ii) they typically design\ntask-specific models that lack generalizability across diverse TDP scenarios.\nTo overcome these challenges, we propose FedTDP, a privacy-preserving and\nunified framework that leverages the capabilities of Large Language Models\n(LLMs) for TDP in federated environments. Specifically, we: (i) design a\ntrajectory privacy autoencoder to secure data transmission and protect privacy,\n(ii) introduce a trajectory knowledge enhancer to improve model learning of\nTDP-related knowledge, enabling the development of TDP-oriented LLMs, and (iii)\npropose federated parallel optimization to enhance training efficiency by\nreducing data transmission and enabling parallel model training. Experiments on\n6 real datasets and 10 mainstream TDP tasks demonstrate that FedTDP\nconsistently outperforms 13 state-of-the-art baselines.", "published": "2025-05-08 11:51:23", "link": "http://arxiv.org/abs/2505.05155v1", "categories": ["cs.LG", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Overcoming Dimensional Factorization Limits in Discrete Diffusion Models through Quantum Joint Distribution Learning", "abstract": "This study explores quantum-enhanced discrete diffusion models to overcome\nclassical limitations in learning high-dimensional distributions. We rigorously\nprove that classical discrete diffusion models, which calculate per-dimension\ntransition probabilities to avoid exponential computational cost, exhibit\nworst-case linear scaling of Kullback-Leibler (KL) divergence with data\ndimension. To address this, we propose a Quantum Discrete Denoising Diffusion\nProbabilistic Model (QD3PM), which enables joint probability learning through\ndiffusion and denoising in exponentially large Hilbert spaces. By deriving\nposterior states through quantum Bayes' theorem, similar to the crucial role of\nposterior probabilities in classical diffusion models, and by learning the\njoint probability, we establish a solid theoretical foundation for\nquantum-enhanced diffusion models. For denoising, we design a quantum circuit\nusing temporal information for parameter sharing and learnable\nclassical-data-controlled rotations for encoding. Exploiting joint distribution\nlearning, our approach enables single-step sampling from pure noise,\neliminating iterative requirements of existing models. Simulations demonstrate\nthe proposed model's superior accuracy in modeling complex distributions\ncompared to factorization methods. Hence, this paper establishes a new\ntheoretical paradigm in generative models by leveraging the quantum advantage\nin joint distribution learning.", "published": "2025-05-08 11:48:21", "link": "http://arxiv.org/abs/2505.05151v1", "categories": ["quant-ph", "cs.LG"], "primary_category": "quant-ph"}
{"title": "Sparse Training from Random Initialization: Aligning Lottery Ticket Masks using Weight Symmetry", "abstract": "The Lottery Ticket Hypothesis (LTH) suggests there exists a sparse LTH mask\nand weights that achieve the same generalization performance as the dense model\nwhile using significantly fewer parameters. However, finding a LTH solution is\ncomputationally expensive, and a LTH sparsity mask does not generalize to other\nrandom weight initializations. Recent work has suggested that neural networks\ntrained from random initialization find solutions within the same basin modulo\npermutation, and proposes a method to align trained models within the same loss\nbasin. We hypothesize that misalignment of basins is the reason why LTH masks\ndo not generalize to new random initializations and propose permuting the LTH\nmask to align with the new optimization basin when performing sparse training\nfrom a different random init. We empirically show a significant increase in\ngeneralization when sparse training from random initialization with the\npermuted mask as compared to using the non-permuted LTH mask, on multiple\ndatasets (CIFAR-10, CIFAR-100 and ImageNet) and models (VGG11, ResNet20 and\nResNet50).", "published": "2025-05-08 11:27:31", "link": "http://arxiv.org/abs/2505.05143v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Taming OOD Actions for Offline Reinforcement Learning: An Advantage-Based Approach", "abstract": "Offline reinforcement learning (RL) aims to learn decision-making policies\nfrom fixed datasets without online interactions, providing a practical solution\nwhere online data collection is expensive or risky. However, offline RL often\nsuffers from distribution shift, resulting in inaccurate evaluation and\nsubstantial overestimation on out-of-distribution (OOD) actions. To address\nthis, existing approaches incorporate conservatism by indiscriminately\ndiscouraging all OOD actions, thereby hindering the agent's ability to\ngeneralize and exploit beneficial ones. In this paper, we propose\nAdvantage-based Diffusion Actor-Critic (ADAC), a novel method that\nsystematically evaluates OOD actions using the batch-optimal value function.\nBased on this evaluation, ADAC defines an advantage function to modulate the\nQ-function update, enabling more precise assessment of OOD action quality. We\ndesign a custom PointMaze environment and collect datasets to visually reveal\nthat advantage modulation can effectively identify and select superior OOD\nactions. Extensive experiments show that ADAC achieves state-of-the-art\nperformance on almost all tasks in the D4RL benchmark, with particularly clear\nmargins on the more challenging tasks.", "published": "2025-05-08 10:57:28", "link": "http://arxiv.org/abs/2505.05126v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Text2Cypher: Data Pruning using Hard Example Selection", "abstract": "Database query languages such as SQL for relational databases and Cypher for\ngraph databases have been widely adopted. Recent advancements in large language\nmodels (LLMs) enable natural language interactions with databases through\nmodels like Text2SQL and Text2Cypher. Fine-tuning these models typically\nrequires large, diverse datasets containing non-trivial examples. However, as\ndataset size increases, the cost of fine-tuning also rises. This makes smaller,\nhigh-quality datasets essential for reducing costs for the same or better\nperformance. In this paper, we propose five hard-example selection techniques\nfor pruning the Text2Cypher dataset, aiming to preserve or improve performance\nwhile reducing resource usage. Our results show that these hard-example\nselection approaches can halve training time and costs with minimal impact on\nperformance, and demonstrates that hard-example selection provides a\ncost-effective solution.", "published": "2025-05-08 10:51:13", "link": "http://arxiv.org/abs/2505.05122v1", "categories": ["cs.DB", "cs.LG"], "primary_category": "cs.DB"}
{"title": "Error Analysis of Deep PDE Solvers for Option Pricing", "abstract": "Option pricing often requires solving partial differential equations (PDEs).\nAlthough deep learning-based PDE solvers have recently emerged as quick\nsolutions to this problem, their empirical and quantitative accuracy remain not\nwell understood, hindering their real-world applicability. In this research,\nour aim is to offer actionable insights into the utility of deep PDE solvers\nfor practical option pricing implementation. Through comparative experiments in\nboth the Black--Scholes and the Heston model, we assess the empirical\nperformance of two neural network algorithms to solve PDEs: the Deep Galerkin\nMethod and the Time Deep Gradient Flow method (TDGF). We determine their\nempirical convergence rates and training time as functions of (i) the number of\nsampling stages, (ii) the number of samples, (iii) the number of layers, and\n(iv) the number of nodes per layer. For the TDGF, we also consider the order of\nthe discretization scheme and the number of time steps.", "published": "2025-05-08 10:45:59", "link": "http://arxiv.org/abs/2505.05121v1", "categories": ["q-fin.CP", "cs.LG", "q-fin.MF", "91G20, 91G60, 68T07"], "primary_category": "q-fin.CP"}
{"title": "USPR: Learning a Unified Solver for Profiled Routing", "abstract": "The Profiled Vehicle Routing Problem (PVRP) extends the classical VRP by\nincorporating vehicle-client-specific preferences and constraints, reflecting\nreal-world requirements such as zone restrictions and service-level\npreferences. While recent reinforcement learning (RL) solvers have shown\npromise, they require retraining for each new profile distribution, suffer from\npoor representation ability, and struggle to generalize to out-of-distribution\ninstances. In this paper, we address these limitations by introducing USPR\n(Unified Solver for Profiled Routing), a novel framework that natively handles\narbitrary profile types. USPR introduces three key innovations: (i) Profile\nEmbeddings (PE) to encode any combination of profile types; (ii) Multi-Head\nProfiled Attention (MHPA), an attention mechanism that models rich interactions\nbetween vehicles and clients; (iii) Profile-aware Score Reshaping (PSR), which\ndynamically adjusts decoder logits using profile scores to improve\ngeneralization. Empirical results on diverse PVRP benchmarks demonstrate that\nUSPR achieves state-of-the-art results among learning-based methods while\noffering significant gains in flexibility and computational efficiency. We make\nour source code publicly available to foster future research at\nhttps://github.com/ai4co/uspr.", "published": "2025-05-08 10:42:57", "link": "http://arxiv.org/abs/2505.05119v1", "categories": ["cs.LG", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Enhancing Text2Cypher with Schema Filtering", "abstract": "Knowledge graphs represent complex data using nodes, relationships, and\nproperties. Cypher, a powerful query language for graph databases, enables\nefficient modeling and querying. Recent advancements in large language models\nallow translation of natural language questions into Cypher queries -\nText2Cypher. A common approach is incorporating database schema into prompts.\nHowever, complex schemas can introduce noise, increase hallucinations, and\nraise computational costs. Schema filtering addresses these challenges by\nincluding only relevant schema elements, improving query generation while\nreducing token costs. This work explores various schema filtering methods for\nText2Cypher task and analyzes their impact on token length, performance, and\ncost. Results show that schema filtering effectively optimizes Text2Cypher,\nespecially for smaller models. Consistent with prior research, we find that\nlarger models benefit less from schema filtering due to their longer context\ncapabilities. However, schema filtering remains valuable for both larger and\nsmaller models in cost reduction.", "published": "2025-05-08 10:42:20", "link": "http://arxiv.org/abs/2505.05118v1", "categories": ["cs.DB", "cs.LG"], "primary_category": "cs.DB"}
{"title": "A Conjoint Graph Representation Learning Framework for Hypertension Comorbidity Risk Prediction", "abstract": "The comorbidities of hypertension impose a heavy burden on patients and\nsociety. Early identification is necessary to prompt intervention, but it\nremains a challenging task. This study aims to address this challenge by\ncombining joint graph learning with network analysis. Motivated by this\ndiscovery, we develop a Conjoint Graph Representation Learning (CGRL) framework\nthat: a) constructs two networks based on disease coding, including the patient\nnetwork and the disease difference network. Three comorbidity network features\nwere generated based on the basic difference network to capture the potential\nrelationship between comorbidities and risk diseases; b) incorporates\ncomputational structure intervention and learning feature representation, CGRL\nwas developed to predict the risks of diabetes and coronary heart disease in\npatients; and c) analysis the comorbidity patterns and exploring the pathways\nof disease progression, the pathological pathogenesis of diabetes and coronary\nheart disease may be revealed. The results show that the network features\nextracted based on the difference network are important, and the framework we\nproposed provides more accurate predictions than other strong models in terms\nof accuracy.", "published": "2025-05-08 09:47:51", "link": "http://arxiv.org/abs/2505.05094v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Learning dynamically inspired invariant subspaces for Koopman and transfer operator approximation", "abstract": "Transfer and Koopman operator methods offer a framework for representing\ncomplex, nonlinear dynamical systems via linear transformations, enabling for a\ndeeper understanding of the underlying dynamics. The spectrum of these\noperators provide important insights into system predictability and emergent\nbehaviour, although efficiently estimating them from data can be challenging.\nWe tackle this issue through the lens of general operator and representational\nlearning, in which we approximate these linear operators using efficient\nfinite-dimensional representations. Specifically, we machine-learn orthonormal,\nlocally supported basis functions that are dynamically tailored to the system.\nThis learned basis provides a particularly accurate approximation of the\noperator's action as well as a nearly invariant finite-dimensional subspace. We\nillustrate our approach with examples that showcase the retrieval of spectral\nproperties from the estimated operator, and emphasise the dynamically adaptive\nquality of the machine-learned basis.", "published": "2025-05-08 09:32:39", "link": "http://arxiv.org/abs/2505.05085v1", "categories": ["math.DS", "cs.LG", "cs.NA", "math.NA", "47A15, 37C30, 47A58, 68T07"], "primary_category": "math.DS"}
{"title": "WaterDrum: Watermarking for Data-centric Unlearning Metric", "abstract": "Large language model (LLM) unlearning is critical in real-world applications\nwhere it is necessary to efficiently remove the influence of private,\ncopyrighted, or harmful data from some users. However, existing utility-centric\nunlearning metrics (based on model utility) may fail to accurately evaluate the\nextent of unlearning in realistic settings such as when (a) the forget and\nretain set have semantically similar content, (b) retraining the model from\nscratch on the retain set is impractical, and/or (c) the model owner can\nimprove the unlearning metric without directly performing unlearning on the\nLLM. This paper presents the first data-centric unlearning metric for LLMs\ncalled WaterDrum that exploits robust text watermarking for overcoming these\nlimitations. We also introduce new benchmark datasets for LLM unlearning that\ncontain varying levels of similar data points and can be used to rigorously\nevaluate unlearning algorithms using WaterDrum. Our code is available at\nhttps://github.com/lululu008/WaterDrum and our new benchmark datasets are\nreleased at https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax.", "published": "2025-05-08 08:56:46", "link": "http://arxiv.org/abs/2505.05064v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Neural Pathways to Program Success: Hopfield Networks for PERT Analysis", "abstract": "Project and task scheduling under uncertainty remains a fundamental challenge\nin program and project management, where accurate estimation of task durations\nand dependencies is critical for delivering complex, multi project systems. The\nProgram Evaluation and Review Technique provides a probabilistic framework to\nmodel task variability and critical paths. In this paper, the author presents a\nnovel formulation of PERT scheduling as an energy minimization problem within a\nHopfield neural network architecture. By mapping task start times and\nprecedence constraints into a neural computation framework, the networks\ninherent optimization dynamics is exploited to approximate globally consistent\nschedules. The author addresses key theoretical issues related to energy\nfunction differentiability, constraint encoding, and convergence, and extends\nthe Hopfield model for structured precedence graphs. Numerical simulations on\nsynthetic project networks comprising up to 1000 tasks demonstrate the\nviability of this approach, achieving near optimal makespans with minimal\nconstraint violations. The findings suggest that neural optimization models\noffer a promising direction for scalable and adaptive project tasks scheduling\nunder uncertainty in areas such as the agentic AI workflows, microservice based\napplications that the modern AI systems are being built upon.", "published": "2025-05-08 08:34:16", "link": "http://arxiv.org/abs/2505.05047v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Dequantified Diffusion Schr\u00f6dinger Bridge for Density Ratio Estimation", "abstract": "Density ratio estimation is fundamental to tasks involving $f$-divergences,\nyet existing methods often fail under significantly different distributions or\ninadequately overlap supports, suffering from the \\textit{density-chasm} and\nthe \\textit{support-chasm} problems. Additionally, prior approaches yield\ndivergent time scores near boundaries, leading to instability. We propose\n$\\text{D}^3\\text{RE}$, a unified framework for robust and efficient density\nratio estimation. It introduces the Dequantified Diffusion-Bridge Interpolant\n(DDBI), which expands support coverage and stabilizes time scores via diffusion\nbridges and Gaussian dequantization. Building on DDBI, the Dequantified\nSchr\\\"odinger-Bridge Interpolant (DSBI) incorporates optimal transport to solve\nthe Schr\\\"odinger bridge problem, enhancing accuracy and efficiency. Our method\noffers uniform approximation and bounded time scores in theory, and outperforms\nbaselines empirically in mutual information and density estimation tasks.", "published": "2025-05-08 08:12:16", "link": "http://arxiv.org/abs/2505.05034v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Generative Models for Long Time Series: Approximately Equivariant Recurrent Network Structures for an Adjusted Training Scheme", "abstract": "We present a simple yet effective generative model for time series data based\non a Variational Autoencoder (VAE) with recurrent layers, referred to as the\nRecurrent Variational Autoencoder with Subsequent Training (RVAE-ST). Our\nmethod introduces an adapted training scheme that progressively increases the\nsequence length, addressing the challenge recurrent layers typically face when\nmodeling long sequences. By leveraging the recurrent architecture, the model\nmaintains a constant number of parameters regardless of sequence length. This\ndesign encourages approximate time-shift equivariance and enables efficient\nmodeling of long-range temporal dependencies. Rather than introducing a\nfundamentally new architecture, we show that a carefully composed combination\nof known components can match or outperform state-of-the-art generative models\non several benchmark datasets. Our model performs particularly well on time\nseries that exhibit quasi-periodic structure,while remaining competitive on\ndatasets with more irregular or partially non-stationary behavior. We evaluate\nits performance using ELBO, Fr\\'echet Distance, discriminative scores, and\nvisualizations of the learned embeddings.", "published": "2025-05-08 07:52:37", "link": "http://arxiv.org/abs/2505.05020v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "CLAM: Continuous Latent Action Models for Robot Learning from Unlabeled Demonstrations", "abstract": "Learning robot policies using imitation learning requires collecting large\namounts of costly action-labeled expert demonstrations, which fundamentally\nlimits the scale of training data. A promising approach to address this\nbottleneck is to harness the abundance of unlabeled observations-e.g., from\nvideo demonstrations-to learn latent action labels in an unsupervised way.\nHowever, we find that existing methods struggle when applied to complex robot\ntasks requiring fine-grained motions. We design continuous latent action models\n(CLAM) which incorporate two key ingredients we find necessary for learning to\nsolve complex continuous control tasks from unlabeled observation data: (a)\nusing continuous latent action labels instead of discrete representations, and\n(b) jointly training an action decoder to ensure that the latent action space\ncan be easily grounded to real actions with relatively few labeled examples.\nImportantly, the labeled examples can be collected from non-optimal play data,\nenabling CLAM to learn performant policies without access to any action-labeled\nexpert data. We demonstrate on continuous control benchmarks in DMControl\n(locomotion) and MetaWorld (manipulation), as well as on a real WidowX robot\narm that CLAM significantly outperforms prior state-of-the-art methods,\nremarkably with a 2-3x improvement in task success rate compared to the best\nbaseline. Videos and code can be found at clamrobot.github.io.", "published": "2025-05-08 07:07:58", "link": "http://arxiv.org/abs/2505.04999v1", "categories": ["cs.RO", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Boosting Statistic Learning with Synthetic Data from Pretrained Large Models", "abstract": "The rapid advancement of generative models, such as Stable Diffusion, raises\na key question: how can synthetic data from these models enhance predictive\nmodeling? While they can generate vast amounts of datasets, only a subset\nmeaningfully improves performance. We propose a novel end-to-end framework that\ngenerates and systematically filters synthetic data through domain-specific\nstatistical methods, selectively integrating high-quality samples for effective\naugmentation. Our experiments demonstrate consistent improvements in predictive\nperformance across various settings, highlighting the potential of our\nframework while underscoring the inherent limitations of generative models for\ndata augmentation. Despite the ability to produce large volumes of synthetic\ndata, the proportion that effectively improves model performance is limited.", "published": "2025-05-08 06:55:22", "link": "http://arxiv.org/abs/2505.04992v1", "categories": ["stat.ML", "cs.LG", "stat.AP"], "primary_category": "stat.ML"}
{"title": "Conformal Prediction with Cellwise Outliers: A Detect-then-Impute Approach", "abstract": "Conformal prediction is a powerful tool for constructing prediction intervals\nfor black-box models, providing a finite sample coverage guarantee for\nexchangeable data. However, this exchangeability is compromised when some\nentries of the test feature are contaminated, such as in the case of cellwise\noutliers. To address this issue, this paper introduces a novel framework called\ndetect-then-impute conformal prediction. This framework first employs an\noutlier detection procedure on the test feature and then utilizes an imputation\nmethod to fill in those cells identified as outliers. To quantify the\nuncertainty in the processed test feature, we adaptively apply the detection\nand imputation procedures to the calibration set, thereby constructing\nexchangeable features for the conformal prediction interval of the test label.\nWe develop two practical algorithms, PDI-CP and JDI-CP, and provide a\ndistribution-free coverage analysis under some commonly used detection and\nimputation procedures. Notably, JDI-CP achieves a finite sample $1-2\\alpha$\ncoverage guarantee. Numerical experiments on both synthetic and real datasets\ndemonstrate that our proposed algorithms exhibit robust coverage properties and\ncomparable efficiency to the oracle baseline.", "published": "2025-05-08 06:43:18", "link": "http://arxiv.org/abs/2505.04986v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Graph Neural Network Aided Deep Reinforcement Learning for Resource Allocation in Dynamic Terahertz UAV Networks", "abstract": "Terahertz (THz) unmanned aerial vehicle (UAV) networks with flexible\ntopologies and ultra-high data rates are expected to empower numerous\napplications in security surveillance, disaster response, and environmental\nmonitoring, among others. However, the dynamic topologies hinder the efficient\nlong-term joint power and antenna array resource allocation for THz links among\nUAVs. Furthermore, the continuous nature of power and the discrete nature of\nantennas cause this joint resource allocation problem to be a mixed-integer\nnonlinear programming (MINLP) problem with non-convexity and NP-hardness.\nInspired by recent rapid advancements in deep reinforcement learning (DRL), a\ngraph neural network (GNN) aided DRL algorithm for resource allocation in the\ndynamic THz UAV network with an emphasis on self-node features (GLOVE) is\nproposed in this paper, with the aim of resource efficiency (RE) maximization.\nWhen training the allocation policy for each UAV, GLOVE learns the relationship\nbetween this UAV and its neighboring UAVs via GNN, while also emphasizing the\nimportant self-node features of this UAV. In addition, a multi-task structure\nis leveraged by GLOVE to cooperatively train resource allocation decisions for\nthe power and sub-arrays of all UAVs. Experimental results illustrate that\nGLOVE outperforms benchmark schemes in terms of the highest RE and the lowest\nlatency. Moreover, unlike the benchmark methods with severe packet loss, GLOVE\nmaintains zero packet loss during the entire training process, demonstrating\nits better robustness under the highly dynamic THz UAV network.", "published": "2025-05-08 06:36:17", "link": "http://arxiv.org/abs/2505.04981v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Community and hyperedge inference in multiple hypergraphs", "abstract": "Hypergraphs, capable of representing high-order interactions via hyperedges,\nhave become a powerful tool for modeling real-world biological and social\nsystems. Inherent relationships within these real-world systems, such as the\nencoding relationship between genes and their protein products, drive the\nestablishment of interconnections between multiple hypergraphs. Here, we\ndemonstrate how to utilize those interconnections between multiple hypergraphs\nto synthesize integrated information from multiple higher-order systems,\nthereby enhancing understanding of underlying structures. We propose a model\nbased on the stochastic block model, which integrates information from multiple\nhypergraphs to reveal latent high-order structures. Real-world hyperedges\nexhibit preferential attachment, where certain nodes dominate hyperedge\nformation. To characterize this phenomenon, our model introduces hyperedge\ninternal degree to quantify nodes' contributions to hyperedge formation. This\nmodel is capable of mining communities, predicting missing hyperedges of\narbitrary sizes within hypergraphs, and inferring inter-hypergraph edges\nbetween hypergraphs. We apply our model to high-order datasets to evaluate its\nperformance. Experimental results demonstrate strong performance of our model\nin community detection, hyperedge prediction, and inter-hypergraph edge\nprediction tasks. Moreover, we show that our model enables analysis of multiple\nhypergraphs of different types and supports the analysis of a single hypergraph\nin the absence of inter-hypergraph edges. Our work provides a practical and\nflexible tool for analyzing multiple hypergraphs, greatly advancing the\nunderstanding of the organization in real-world high-order systems.", "published": "2025-05-08 05:52:41", "link": "http://arxiv.org/abs/2505.04967v1", "categories": ["cs.SI", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Learning Linearized Models from Nonlinear Systems under Initialization Constraints with Finite Data", "abstract": "The identification of a linear system model from data has wide applications\nin control theory. The existing work that provides finite sample guarantees for\nlinear system identification typically uses data from a single long system\ntrajectory under i.i.d. random inputs, and assumes that the underlying dynamics\nis truly linear. In contrast, we consider the problem of identifying a\nlinearized model when the true underlying dynamics is nonlinear, given that\nthere is a certain constraint on the region where one can initialize the\nexperiments. We provide a multiple trajectories-based deterministic data\nacquisition algorithm followed by a regularized least squares algorithm, and\nprovide a finite sample error bound on the learned linearized dynamics. Our\nerror bound shows that one can consistently learn the linearized dynamics, and\ndemonstrates a trade-off between the error due to nonlinearity and the error\ndue to noise. We validate our results through numerical experiments, where we\nalso show the potential insufficiency of linear system identification using a\nsingle trajectory with i.i.d. random inputs, when nonlinearity does exist.", "published": "2025-05-08 05:26:52", "link": "http://arxiv.org/abs/2505.04954v1", "categories": ["stat.ML", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "stat.ML"}
{"title": "Generalization Analysis for Contrastive Representation Learning under Non-IID Settings", "abstract": "Contrastive Representation Learning (CRL) has achieved impressive success in\nvarious domains in recent years. Nevertheless, the theoretical understanding of\nthe generalization behavior of CRL is limited. Moreover, to the best of our\nknowledge, the current literature only analyzes generalization bounds under the\nassumption that the data tuples used for contrastive learning are independently\nand identically distributed. However, in practice, we are often limited to a\nfixed pool of reusable labeled data points, making it inevitable to recycle\ndata across tuples to create sufficiently large datasets. Therefore, the\ntuple-wise independence condition imposed by previous works is invalidated. In\nthis paper, we provide a generalization analysis for the CRL framework under\nnon-$i.i.d.$ settings that adheres to practice more realistically. Drawing\ninspiration from the literature on U-statistics, we derive generalization\nbounds which indicate the required number of samples in each class scales as\nthe logarithm of the covering number of the class of learnable feature\nrepresentations associated to each class. Next, we apply our main results to\nderive excess risk bounds for common function classes such as linear maps and\nneural networks.", "published": "2025-05-08 04:26:41", "link": "http://arxiv.org/abs/2505.04937v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "VaCDA: Variational Contrastive Alignment-based Scalable Human Activity Recognition", "abstract": "Technological advancements have led to the rise of wearable devices with\nsensors that continuously monitor user activities, generating vast amounts of\nunlabeled data. This data is challenging to interpret, and manual annotation is\nlabor-intensive and error-prone. Additionally, data distribution is often\nheterogeneous due to device placement, type, and user behavior variations. As a\nresult, traditional transfer learning methods perform suboptimally, making it\ndifficult to recognize daily activities. To address these challenges, we use a\nvariational autoencoder (VAE) to learn a shared, low-dimensional latent space\nfrom available sensor data. This space generalizes data across diverse sensors,\nmitigating heterogeneity and aiding robust adaptation to the target domain. We\nintegrate contrastive learning to enhance feature representation by aligning\ninstances of the same class across domains while separating different classes.\nWe propose Variational Contrastive Domain Adaptation (VaCDA), a multi-source\ndomain adaptation framework combining VAEs and contrastive learning to improve\nfeature representation and reduce heterogeneity between source and target\ndomains. We evaluate VaCDA on multiple publicly available datasets across three\nheterogeneity scenarios: cross-person, cross-position, and cross-device. VaCDA\noutperforms the baselines in cross-position and cross-device scenarios.", "published": "2025-05-08 02:50:43", "link": "http://arxiv.org/abs/2505.04907v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "CubeDAgger: Improved Robustness of Interactive Imitation Learning without Violation of Dynamic Stability", "abstract": "Interactive imitation learning makes an agent's control policy robust by\nstepwise supervisions from an expert. The recent algorithms mostly employ\nexpert-agent switching systems to reduce the expert's burden by limitedly\nselecting the supervision timing. However, the precise selection is difficult\nand such a switching causes abrupt changes in actions, damaging the dynamic\nstability. This paper therefore proposes a novel method, so-called CubeDAgger,\nwhich improves robustness while reducing dynamic stability violations by making\nthree improvements to a baseline method, EnsembleDAgger. The first improvement\nadds a regularization to explicitly activate the threshold for deciding the\nsupervision timing. The second transforms the expert-agent switching system to\nan optimal consensus system of multiple action candidates. Third,\nautoregressive colored noise to the actions is introduced to make the\nstochastic exploration consistent over time. These improvements are verified by\nsimulations, showing that the learned policies are sufficiently robust while\nmaintaining dynamic stability during interaction.", "published": "2025-05-08 02:18:49", "link": "http://arxiv.org/abs/2505.04897v1", "categories": ["cs.RO", "cs.LG"], "primary_category": "cs.RO"}
{"title": "GCN-Based Throughput-Oriented Handover Management in Dense 5G Vehicular Networks", "abstract": "The rapid advancement of 5G has transformed vehicular networks, offering high\nbandwidth, low latency, and fast data rates essential for real-time\napplications in smart cities and vehicles. These improvements enhance traffic\nsafety and entertainment services. However, the limited coverage and frequent\nhandovers in 5G networks cause network instability, especially in high-mobility\nenvironments due to the ping-pong effect. This paper presents TH-GCN\n(Throughput-oriented Graph Convolutional Network), a novel approach for\noptimizing handover management in dense 5G networks. Using graph neural\nnetworks (GNNs), TH-GCN models vehicles and base stations as nodes in a dynamic\ngraph enriched with features such as signal quality, throughput, vehicle speed,\nand base station load. By integrating both user equipment and base station\nperspectives, this dual-centric approach enables adaptive, real-time handover\ndecisions that improve network stability. Simulation results show that TH-GCN\nreduces handovers by up to 78 percent and improves signal quality by 10\npercent, outperforming existing methods.", "published": "2025-05-08 02:03:46", "link": "http://arxiv.org/abs/2505.04894v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "FedRE: Robust and Effective Federated Learning with Privacy Preference", "abstract": "Despite Federated Learning (FL) employing gradient aggregation at the server\nfor distributed training to prevent the privacy leakage of raw data, private\ninformation can still be divulged through the analysis of uploaded gradients\nfrom clients. Substantial efforts have been made to integrate local\ndifferential privacy (LDP) into the system to achieve a strict privacy\nguarantee. However, existing methods fail to take practical issues into account\nby merely perturbing each sample with the same mechanism while each client may\nhave their own privacy preferences on privacy-sensitive information (PSI),\nwhich is not uniformly distributed across the raw data. In such a case,\nexcessive privacy protection from private-insensitive information can\nadditionally introduce unnecessary noise, which may degrade the model\nperformance. In this work, we study the PSI within data and develop FedRE, that\ncan simultaneously achieve robustness and effectiveness benefits with LDP\nprotection. More specifically, we first define PSI with regard to the privacy\npreferences of each client. Then, we optimize the LDP by allocating less\nprivacy budget to gradients with higher PSI in a layer-wise manner, thus\nproviding a stricter privacy guarantee for PSI. Furthermore, to mitigate the\nperformance degradation caused by LDP, we design a parameter aggregation\nmechanism based on the distribution of the perturbed information. We conducted\nexperiments with text tamper detection on T-SROIE and DocTamper datasets, and\nFedRE achieves competitive performance compared to state-of-the-art methods.", "published": "2025-05-08 01:50:27", "link": "http://arxiv.org/abs/2505.04889v1", "categories": ["cs.LG", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Fairness Perceptions in Regression-based Predictive Models", "abstract": "Regression-based predictive analytics used in modern kidney transplantation\nis known to inherit biases from training data. This leads to social\ndiscrimination and inefficient organ utilization, particularly in the context\nof a few social groups. Despite this concern, there is limited research on\nfairness in regression and its impact on organ utilization and placement. This\npaper introduces three novel divergence-based group fairness notions: (i)\nindependence, (ii) separation, and (iii) sufficiency to assess the fairness of\nregression-based analytics tools. In addition, fairness preferences are\ninvestigated from crowd feedback, in order to identify a socially accepted\ngroup fairness criterion for evaluating these tools. A total of 85 participants\nwere recruited from the Prolific crowdsourcing platform, and a Mixed-Logit\ndiscrete choice model was used to model fairness feedback and estimate social\nfairness preferences. The findings clearly depict a strong preference towards\nthe separation and sufficiency fairness notions, and that the predictive\nanalytics is deemed fair with respect to gender and race groups, but unfair in\nterms of age groups.", "published": "2025-05-08 01:48:27", "link": "http://arxiv.org/abs/2505.04886v1", "categories": ["cs.HC", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Physics-informed solution reconstruction in elasticity and heat transfer using the explicit constraint force method", "abstract": "One use case of ``physics-informed neural networks'' (PINNs) is solution\nreconstruction, which aims to estimate the full-field state of a physical\nsystem from sparse measurements. Parameterized governing equations of the\nsystem are used in tandem with the measurements to regularize the regression\nproblem. However, in real-world solution reconstruction problems, the\nparameterized governing equation may be inconsistent with the physical\nphenomena that give rise to the measurement data. We show that due to assuming\nconsistency between the true and parameterized physics, PINNs-based approaches\nmay fail to satisfy three basic criteria of interpretability, robustness, and\ndata consistency. As we argue, these criteria ensure that (i) the quality of\nthe reconstruction can be assessed, (ii) the reconstruction does not depend\nstrongly on the choice of physics loss, and (iii) that in certain situations,\nthe physics parameters can be uniquely recovered. In the context of elasticity\nand heat transfer, we demonstrate how standard formulations of the physics loss\nand techniques for constraining the solution to respect the measurement data\nlead to different ``constraint forces\" -- which we define as additional source\nterms arising from the constraints -- and that these constraint forces can\nsignificantly influence the reconstructed solution. To avoid the potentially\nsubstantial influence of the choice of physics loss and method of constraint\nenforcement on the reconstructed solution, we propose the ``explicit constraint\nforce method'' (ECFM) to gain control of the source term introduced by the\nconstraint. We then show that by satisfying the criteria of interpretability,\nrobustness, and data consistency, this approach leads to more predictable and\ncustomizable reconstructions from noisy measurement data, even when the\nparameterization of the missing physics is inconsistent with the measured\nsystem.", "published": "2025-05-08 01:19:11", "link": "http://arxiv.org/abs/2505.04875v1", "categories": ["cs.CE", "cs.LG"], "primary_category": "cs.CE"}
{"title": "Empowering Scientific Workflows with Federated Agents", "abstract": "Agentic systems, in which diverse agents cooperate to tackle challenging\nproblems, are exploding in popularity in the AI community. However, the agentic\nframeworks used to build these systems have not previously enabled use with\nresearch cyberinfrastructure. Here we introduce Academy, a modular and\nextensible middleware designed to deploy autonomous agents across the federated\nresearch ecosystem, including HPC systems, experimental facilities, and data\nrepositories. To meet the demands of scientific computing, Academy supports\nasynchronous execution, heterogeneous resources, high-throughput data flows,\nand dynamic resource availability. It provides abstractions for expressing\nstateful agents, managing inter-agent coordination, and integrating computation\nwith experimental control. We present microbenchmark results that demonstrate\nhigh performance and scalability in HPC environments. To demonstrate the\nbreadth of applications that can be supported by agentic workflow designs, we\nalso present case studies in materials discovery, decentralized learning, and\ninformation extraction in which agents are deployed across diverse HPC systems.", "published": "2025-05-08 17:15:39", "link": "http://arxiv.org/abs/2505.05428v1", "categories": ["cs.MA", "cs.DC"], "primary_category": "cs.MA"}
{"title": "A Multi-Agent AI Framework for Immersive Audiobook Production through Spatial Audio and Neural Narration", "abstract": "This research introduces an innovative AI-driven multi-agent framework\nspecifically designed for creating immersive audiobooks. Leveraging neural\ntext-to-speech synthesis with FastSpeech 2 and VALL-E for expressive narration\nand character-specific voices, the framework employs advanced language models\nto automatically interpret textual narratives and generate realistic spatial\naudio effects. These sound effects are dynamically synchronized with the\nstoryline through sophisticated temporal integration methods, including Dynamic\nTime Warping (DTW) and recurrent neural networks (RNNs). Diffusion-based\ngenerative models combined with higher-order ambisonics (HOA) and scattering\ndelay networks (SDN) enable highly realistic 3D soundscapes, substantially\nenhancing listener immersion and narrative realism. This technology\nsignificantly advances audiobook applications, providing richer experiences for\neducational content, storytelling platforms, and accessibility solutions for\nvisually impaired audiences. Future work will address personalization, ethical\nmanagement of synthesized voices, and integration with multi-sensory platforms.", "published": "2025-05-08 01:48:06", "link": "http://arxiv.org/abs/2505.04885v1", "categories": ["cs.SD", "cs.HC", "cs.MA", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Neural network methods for power series problems of Perron-Frobenius operators", "abstract": "Problems related to Perron-Frobenius operators (or transfer operators) have\nbeen extensively studied and applied across various fields. In this work, we\npropose neural network methods for approximating solutions to problems\ninvolving these operators. Specifically, we focus on computing the power series\nof non-expansive Perron-Frobenius operators under a given $L^p$-norm with a\nconstant damping parameter in $(0,1)$. We use PINNs and RVPINNs to approximate\nsolutions in their strong and variational forms, respectively. We provide a\npriori error estimates for quasi-minimizers of the associated loss functions.\nWe present some numerical results for 1D and 2D examples to show the\nperformance of our methods.", "published": "2025-05-08 16:46:52", "link": "http://arxiv.org/abs/2505.05407v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Finite element approximation for quantitative photoacoustic tomography in a diffusive regime", "abstract": "In this paper, we focus on the numerical analysis of quantitative\nphotoacoustic tomography. Our goal is to reconstruct the optical coefficients,\ni.e., the diffusion and absorption coefficients, using multiple internal\nobservational data. The foundation of our numerical algorithm lies in solving\nan inverse diffusivity problem and a direct problem associated with elliptic\nequations. The stability of the inverse problem depends critically on a\nnon-zero condition in the internal observations, a condition that can be met\nusing randomly chosen boundary excitation data. Utilizing these randomly\ngenerated boundary data, we implement an output least squares formulation\ncombined with finite element discretization to solve the inverse problem. In\nthis scenario, we provide a rigorous error estimate in $L^2(\\Omega)$ norm for\nthe numerical reconstruction using a weighted energy estimate, inspired by the\nanalysis of a newly proposed conditional stability result. The resulting error\nestimate serves as a valuable guide for selecting appropriate regularization\nparameters and discretization mesh sizes according to the noise levels present\nin the data. Several numerical experiments are presented to support our\ntheoretical results and illustrate the effectiveness of our numerical scheme.", "published": "2025-05-08 15:53:59", "link": "http://arxiv.org/abs/2505.05361v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Three dimensional seepage analysis using a polyhedral scaled boundary finite element method", "abstract": "This paper presents a novel polyhedral scaled boundary finite element method\n(PSBFEM) for three-dimensional seepage analysis. The proposed method combines\nthe semi-analytical capabilities of the SBFEM with the geometric flexibility of\npolyhedral and octree meshes, making it well-suited for complex seepage\nproblems. Wachspress shape functions are employed to construct shape functions\nover arbitrary polyhedral elements, enabling accurate approximation along\ncomplex boundaries. The method is implemented within the ABAQUS UEL framework\nto support steady-state, transient, and free-surface seepage simulations. A\nseries of numerical examples are provided to verify the accuracy, efficiency,\nand convergence of the proposed method, including benchmark problems and\ngeometrically complex domains. The results demonstrate that the PSBFEM achieves\nhigher accuracy and faster convergence compared to conventional FEM,\nparticularly when using hybrid octree meshes with localized refinement. This\nframework provides a robust and efficient tool for 3D seepage analysis in\ngeotechnical and hydraulic engineering applications.", "published": "2025-05-08 13:42:01", "link": "http://arxiv.org/abs/2505.05244v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Weighting operators for sparsity regularization", "abstract": "Standard regularization methods typically favor solutions which are in, or\nclose to, the orthogonal complement of the null space of the forward\noperator/matrix $\\mathsf{A}$. This particular biasedness might not be desirable\nin applications and can lead to severe challenges when $\\mathsf{A}$ is\nnon-injective.\n  We have therefore, in a series of papers, investigated how to \"remedy\" this\nfact, relative to a chosen basis and in a certain mathematical sense: Based on\na weighting procedure, it turns out that it is possible to modify both Tikhonov\nand sparsity regularization such that each member of the chosen basis can be\nalmost perfectly recovered from their image under $\\mathsf{A}$. In particular,\nwe have studied this problem for the task of using boundary data to identify\nthe source term in an elliptic PDE. However, this weighting procedure involves\n$\\mathsf{A}^\\dagger \\mathsf{A}$, where $\\mathsf{A}^\\dagger$ denotes the pseudo\ninverse of $\\mathsf{A}$, and can thus be CPU-demanding and lead to undesirable\nerror amplification.\n  We therefore, in this paper, study alternative weighting approaches and prove\nthat some of the recovery results established for the methodology involving\n$\\mathsf{A}$ hold for a broader class of weighting schemes. In fact, it turns\nout that \"any\" linear operator $\\mathsf{B}$ has an associated proper weighting\ndefined in terms of images under $\\mathsf{B}\\mathsf{A}$. We also present a\nseries of numerical experiments, employing different choices of $\\mathsf{B}$.", "published": "2025-05-08 13:29:30", "link": "http://arxiv.org/abs/2505.05234v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "On the stability and conditioning of a fictitious domain formulation for fluid-structure interaction problems", "abstract": "We consider a distributed Lagrange multiplier formulation for fluid-structure\ninteraction problems in the spirit of the fictitious domain approach. Our\nprevious studies showed that the formulation is unconditionally stable in time\nand that its mixed finite element discretization is well-posed. In this paper,\nwe analyze the behavior of the condition number with respect to mesh\nrefinement. Moreover, we observe that our formulation does not need any\nstabilization term in presence of small cut cells and conditioning is not\naffected by the interface position.", "published": "2025-05-08 13:18:41", "link": "http://arxiv.org/abs/2505.05228v1", "categories": ["math.NA", "cs.NA", "65N30, 65N12, 74F10"], "primary_category": "math.NA"}
{"title": "A Fourier-based inference method for learning interaction kernels in particle systems", "abstract": "We consider the problem of inferring the interaction kernel of stochastic\ninteracting particle systems from observations of a single particle. We adopt a\nsemi-parametric approach and represent the interaction kernel in terms of a\ngeneralized Fourier series. The basis functions in this expansion are tailored\nto the problem at hand and are chosen to be orthogonal polynomials with respect\nto the invariant measure of the mean-field dynamics. The generalized Fourier\ncoefficients are obtained as the solution of an appropriate linear system whose\ncoefficients depend on the moments of the invariant measure, and which are\napproximated from the particle trajectory that we observe. We quantify the\napproximation error in the Lebesgue space weighted by the invariant measure and\nstudy the asymptotic properties of the estimator in the joint limit as the\nobservation interval and the number of particles tend to infinity, i.e. the\njoint large time-mean field limit. We also explore the regime where an\nincreasing number of generalized Fourier coefficients is needed to represent\nthe interaction kernel. Our theoretical results are supported by extensive\nnumerical simulations.", "published": "2025-05-08 13:02:39", "link": "http://arxiv.org/abs/2505.05207v1", "categories": ["math.ST", "cs.NA", "math.NA", "stat.TH"], "primary_category": "math.ST"}
{"title": "Matrices over a Hilbert space and their low-rank approximation", "abstract": "Matrices are typically considered over fields or rings. Motivated by\napplications in parametric differential equations and data-driven modeling, we\nsuggest to study matrices with entries from a Hilbert space and present an\nelementary theory of them: from basic properties to low-rank approximation.\nSpecifically, we extend the idea of cross approximation to such matrices and\npropose an analogue of the adaptive cross approximation algorithm. Our\nnumerical experiments show that this approach can achieve quasioptimal\napproximation and be integrated with the existing computational software for\npartial differential equations.", "published": "2025-05-08 11:11:20", "link": "http://arxiv.org/abs/2505.05134v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Constraints Preserving Lax-Wendroff Flux Reconstruction for Relativistic Hydrodynamics with General Equations of State", "abstract": "In the realm of relativistic astrophysics, the ideal equation of state with a\nconstant adiabatic index provides a poor approximation due to its inconsistency\nwith relativistic kinetic theory. However, it is a common practice to use it\nfor relativistic fluid flow equations due to its simplicity. Here we develop a\nhigh-order Lax-Wendroff flux reconstruction method on Cartesian grids for\nsolving special relativistic hydrodynamics equations with several general\nequations of state available in the literature. We also study the conversion\nfrom conservative to primitive variables, which depends on the equation of\nstate in use, and provide an alternative method of conversion when the existing\napproach does not succeed. For the admissibility of the solution, we blend the\nhigh-order method with a low-order method on sub-cells and prove its physical\nadmissible property in the case of all the equations of state used here.\nLastly, we validate the scheme by several test cases having strong\ndiscontinuities, large Lorentz factor, and low density or pressure in one and\ntwo dimensions.", "published": "2025-05-08 11:00:34", "link": "http://arxiv.org/abs/2505.05128v1", "categories": ["math.NA", "cs.NA", "gr-qc", "65M60", "G.1.8"], "primary_category": "math.NA"}
{"title": "Multigrid methods for the ghost finite element approximation of elliptic problems", "abstract": "We present multigrid methods for solving elliptic partial differential\nequations on arbitrary domains using the nodal ghost finite element method, an\nunfitted boundary approach where the domain is implicitly defined by a\nlevel-set function. This method achieves second-order accuracy and offers\nsubstantial computational advantages over both direct solvers and\nfinite-difference-based multigrid methods. A key strength of the ghost finite\nelement framework is its variational formulation, which naturally enables\nconsistent transfer operators and avoids residual splitting across grid levels.\nWe provide a detailed construction of the multigrid components in both one and\ntwo spatial dimensions, including smoothers, transfer operators, and coarse\ngrid operators. The choice of the stabilization parameter plays a crucial role\nin ensuring well-posedness and optimal convergence of the multigrid method. We\nderive explicit algebraic expressions for this parameter based on the geometry\nof cut cells. In the two-dimensional setting, we further improve efficiency by\nperforming additional smoothing exclusively on cut cells, reducing\ncomputational cost without compromising convergence. Numerical results validate\nthe proposed method across a range of geometries and confirm its robustness and\nscalability.", "published": "2025-05-08 10:07:38", "link": "http://arxiv.org/abs/2505.05105v1", "categories": ["math.NA", "cs.NA", "65N55", "G.1.8"], "primary_category": "math.NA"}
{"title": "Enhanced convergence rates of Adaptive Importance Sampling with recycling schemes via quasi-Monte Carlo methods", "abstract": "This article investigates the integration of quasi-Monte Carlo (QMC) methods\nusing the Adaptive Multiple Importance Sampling (AMIS). Traditional Importance\nSampling (IS) often suffers from poor performance since it heavily relies on\nthe choice of the proposal distributions. The AMIS and the Modified version of\nAMIS (MAMIS) address this by iteratively refining proposal distributions and\nreusing all past samples through a recycling strategy. We introduce the RQMC\nmethods into the MAMIS, achieving higher convergence rates compared to the\nMonte Carlo (MC) methods. Our main contributions include a detailed convergence\nanalysis of the MAMIS estimator under randomized QMC (RQMC) sampling.\nSpecifically, we establish the $L^q$ $(q \\geq 2)$ error bound for the\nRQMC-based estimator using a smoothed projection method, which enables us to\napply the H\\\"older's inequality in the error analysis of the RQMC-based MAMIS\nestimator. As a result, we prove that the root mean square error of the\nRQMC-based MAMIS estimator converges at a rate of\n$\\mathcal{O}(\\bar{N}_T^{-1+\\epsilon})$, where $\\bar{N}_T$ is the average number\nof samples used in each step over $T$ iterations, and $\\epsilon > 0$ is\narbitrarily small. Numerical experiments validate the effectiveness of our\nmethod, including mixtures of Gaussians, a banana-shaped model, and Bayesian\nLogistic regression.", "published": "2025-05-08 08:16:58", "link": "http://arxiv.org/abs/2505.05037v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Numerical analysis for subdiffusion problem with non-positive memory", "abstract": "This work considers the subdiffusion problem with non-positive memory, which\nnot only arises from physical laws with memory, but could be transformed from\nsophisticated models such as subdiffusion or subdiffusive Fokker-Planck\nequation with variable exponent. We apply the non-uniform L1 formula and\ninterpolation quadrature to discretize the fractional derivative and the memory\nterm, respectively, and then adopt the complementary discrete convolution\nkernel approach to prove the stability and first-order temporal accuracy of the\nscheme. The main difficulty in numerical analysis lies in the non-positivity of\nthe kernel and its coupling with the complementary discrete convolution kernel\n(such that different model exponents are also coupled), and the results extend\nthose in [Chen, Thom\\'ee and Wahlbin, Math. Comp. 1992] to the subdiffusive\ncase. Numerical experiments are performed to substantiate the theoretical\nresults.", "published": "2025-05-08 03:46:20", "link": "http://arxiv.org/abs/2505.04924v1", "categories": ["math.NA", "cs.NA", "45K05, 65M12, 65M60"], "primary_category": "math.NA"}
{"title": "A mixed finite element method for a class of fourth-order stochastic evolution equations with multiplicative noise", "abstract": "We develop a fully discrete, semi-implicit mixed finite element method for\napproximating solutions to a class of fourth-order stochastic partial\ndifferential equations (SPDEs) with non-globally Lipschitz and non-monotone\nnonlinearities, perturbed by spatially smooth multiplicative Gaussian noise.\nThe proposed scheme is applicable to a range of physically relevant nonlinear\nmodels, including the stochastic Landau--Lifshitz--Baryakhtar (sLLBar)\nequation, the stochastic convective Cahn--Hilliard equation with mass source,\nand the stochastic regularised Landau--Lifshitz--Bloch (sLLB) equation, among\nothers. To overcome the difficulties posed by the interplay between the\nnonlinearities and the stochastic forcing, we adopt a\n`truncate-then-discretise' strategy: the nonlinear term is first truncated\nbefore discretising the resulting modified problem. We show that the strong\nsolution to the truncated problem converges in probability to that of the\noriginal problem. A fully discrete numerical scheme is then proposed for the\ntruncated system, and we establish both convergence in probability and strong\nconvergence (with quantitative rates) for the two fields used in the mixed\nformulation.", "published": "2025-05-08 00:31:39", "link": "http://arxiv.org/abs/2505.04866v1", "categories": ["math.NA", "cs.NA", "math.AP", "math.PR", "60H35, 65C30, 65M12, 65M60"], "primary_category": "math.NA"}
{"title": "Loss-Versus-Rebalancing under Deterministic and Generalized block-times", "abstract": "Although modern blockchains almost universally produce blocks at fixed\nintervals, existing models still lack an analytical formula for the\nloss-versus-rebalancing (LVR) incurred by Automated Market Makers (AMMs)\nliquidity providers in this setting. Leveraging tools from random walk theory,\nwe derive the following closed-form approximation for the per block per unit of\nliquidity expected LVR under constant block time:\n  \\[ \\overline{\\mathrm{ARB}}= \\frac{\\,\\sigma_b^{2}}\n{\\,2+\\sqrt{2\\pi}\\,\\gamma/(|\\zeta(1/2)|\\,\\sigma_b)\\,}+O\\!\\bigl(e^{-\\mathrm{const}\\tfrac{\\gamma}{\\sigma_b}}\\bigr)\\;\\approx\\;\n\\frac{\\sigma_b^{2}}{\\,2 + 1.7164\\,\\gamma/\\sigma_b}, \\] where $\\sigma_b$ is the\nintra-block asset volatility, $\\gamma$ the AMM spread and $\\zeta$ the Riemann\nZeta function. Our large Monte Carlo simulations show that this formula is in\nfact quasi-exact across practical parameter ranges.\n  Extending our analysis to arbitrary block-time distributions as well, we\ndemonstrate both that--under every admissible inter-block law--the probability\nthat a block carries an arbitrage trade converges to a universal limit, and\nthat only constant block spacing attains the asymptotically minimal LVR. This\nshows that constant block intervals provide the best possible protection\nagainst arbitrage for liquidity providers. \\end{abstract}", "published": "2025-05-08 10:30:24", "link": "http://arxiv.org/abs/2505.05113v1", "categories": ["q-fin.MF", "math.PR", "q-fin.PM", "q-fin.PR", "q-fin.TR"], "primary_category": "q-fin.MF"}
{"title": "Enhancing the Dynamic Range of Quantum Sensing via Quantum Circuit Learning", "abstract": "Quantum metrology is a promising application of quantum technologies,\nenabling the precise measurement of weak external fields at a local scale. In\ntypical quantum sensing protocols, a qubit interacts with an external field,\nand the amplitude of the field is estimated by analyzing the expectation value\nof a measured observable. Sensitivity can, in principle, be enhanced by\nincreasing the number of qubits within a fixed volume, thereby maintaining\nspatial resolution. However, at high qubit densities, inter-qubit interactions\ninduce complex many-body dynamics, resulting in multiple oscillations in the\nexpectation value of the observable even for small field amplitudes. This\nambiguity reduces the dynamic range of the sensing protocol. We propose a\nmethod to overcome the limitation in quantum metrology by adopting a quantum\ncircuit learning framework using a parameterized quantum circuit to approximate\na target function by optimizing the circuit parameters. In our method, after\nthe qubits interact with the external field, we apply a sequence of\nparameterized quantum gates and measure a suitable observable. By optimizing\nthe gate parameters, the expectation value is trained to exhibit a monotonic\nresponse within a target range of field amplitudes, thereby eliminating\nmultiple oscillations and enhancing the dynamic range. This method offers a\nstrategy for improving quantum sensing performance in dense qubit systems.", "published": "2025-05-08 05:40:26", "link": "http://arxiv.org/abs/2505.04958v1", "categories": ["quant-ph", "cond-mat.mes-hall", "stat.ML"], "primary_category": "quant-ph"}
{"title": "FLAM: Frame-Wise Language-Audio Modeling", "abstract": "Recent multi-modal audio-language models (ALMs) excel at text-audio retrieval\nbut struggle with frame-wise audio understanding. Prior works use\ntemporal-aware labels or unsupervised training to improve frame-wise\ncapabilities, but they still lack fine-grained labeling capability to pinpoint\nwhen an event occurs. While traditional sound event detection models can\nprecisely localize events, they are limited to pre-defined categories, making\nthem ineffective for real-world scenarios with out-of-distribution events. In\nthis work, we introduce FLAM, an open-vocabulary contrastive audio-language\nmodel capable of localizing specific sound events. FLAM employs a\nmemory-efficient and calibrated frame-wise objective with logit adjustment to\naddress spurious correlations, such as event dependencies and label imbalances\nduring training. To enable frame-wise supervision, we leverage a large-scale\ndataset with diverse audio events, LLM-generated captions and simulation.\nExperimental results and case studies demonstrate that FLAM significantly\nimproves the open-vocabulary localization capability while maintaining strong\nperformance in global retrieval and downstream tasks.", "published": "2025-05-08 15:27:43", "link": "http://arxiv.org/abs/2505.05335v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Normalize Everything: A Preconditioned Magnitude-Preserving Architecture for Diffusion-Based Speech Enhancement", "abstract": "This paper presents a new framework for diffusion-based speech enhancement.\nOur method employs a Schroedinger bridge to transform the noisy speech\ndistribution into the clean speech distribution. To stabilize and improve\ntraining, we employ time-dependent scalings of the inputs and outputs of the\nnetwork, known as preconditioning. We consider two skip connection\nconfigurations, which either include or omit the current process state in the\ndenoiser's output, enabling the network to predict either environmental noise\nor clean speech. Each approach leads to improved performance on different\nspeech enhancement metrics. To maintain stable magnitude levels and balance\nduring training, we use a magnitude-preserving network architecture that\nnormalizes all activations and network weights to unit length. Additionally, we\npropose learning the contribution of the noisy input within each network block\nfor effective input conditioning. After training, we apply a method to\napproximate different exponential moving average (EMA) profiles and investigate\ntheir effects on the speech enhancement performance. In contrast to image\ngeneration tasks, where longer EMA lengths often enhance mode coverage, we\nobserve that shorter EMA lengths consistently lead to better performance on\nstandard speech enhancement metrics. Code, audio examples, and checkpoints are\navailable online.", "published": "2025-05-08 13:10:02", "link": "http://arxiv.org/abs/2505.05216v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FlexSpeech: Towards Stable, Controllable and Expressive Text-to-Speech", "abstract": "Current speech generation research can be categorized into two primary\nclasses: non-autoregressive and autoregressive. The fundamental distinction\nbetween these approaches lies in the duration prediction strategy employed for\npredictable-length sequences. The NAR methods ensure stability in speech\ngeneration by explicitly and independently modeling the duration of each\nphonetic unit. Conversely, AR methods employ an autoregressive paradigm to\npredict the compressed speech token by implicitly modeling duration with Markov\nproperties. Although this approach improves prosody, it does not provide the\nstructural guarantees necessary for stability. To simultaneously address the\nissues of stability and naturalness in speech generation, we propose\nFlexSpeech, a stable, controllable, and expressive TTS model. The motivation\nbehind FlexSpeech is to incorporate Markov dependencies and preference\noptimization directly on the duration predictor to boost its naturalness while\nmaintaining explicit modeling of the phonetic units to ensure stability.\nSpecifically, we decompose the speech generation task into two components: an\nAR duration predictor and a NAR acoustic model. The acoustic model is trained\non a substantial amount of data to learn to render audio more stably, given\nreference audio prosody and phone durations. The duration predictor is\noptimized in a lightweight manner for different stylistic variations, thereby\nenabling rapid style transfer while maintaining a decoupled relationship with\nthe specified speaker timbre. Experimental results demonstrate that our\napproach achieves SOTA stability and naturalness in zero-shot TTS. More\nimportantly, when transferring to a specific stylistic domain, we can\naccomplish lightweight optimization of the duration module solely with about\n100 data samples, without the need to adjust the acoustic model, thereby\nenabling rapid and stable style transfer.", "published": "2025-05-08 11:55:19", "link": "http://arxiv.org/abs/2505.05159v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Regression-based Melody Estimation with Uncertainty Quantification", "abstract": "Existing machine learning models approach the task of melody estimation from\npolyphonic audio as a classification problem by discretizing the pitch values,\nwhich results in the loss of finer frequency variations present in the melody.\nTo better capture these variations, we propose to approach this task as a\nregression problem. Apart from predicting only the pitch for a particular\nregion in the audio, we also predict its uncertainty to enhance the\ntrustworthiness of the model. To perform regression-based melody estimation, we\npropose three different methods that use histogram representation to model the\npitch values. Such a representation requires the support range of the histogram\nto be continuous. The first two methods address the abrupt discontinuity\nbetween unvoiced and voiced frequency ranges by mapping them to a continuous\nrange. The third method reformulates melody estimation as a fully Bayesian\ntask, modeling voicing detection as a classification problem, and voiced pitch\nestimation as a regression problem. Additionally, we introduce a novel method\nto estimate the uncertainty from the histogram representation that correlates\nwell with the deviation of the mean of the predicted distribution from the\nground truth. Experimental results demonstrate that reformulating melody\nestimation as a regression problem significantly improves the performance over\nclassification-based approaches. Comparing the proposed methods with a\nstate-of-the-art regression model, it is observed that the Bayesian method\nperforms the best at estimating both the melody and its associated uncertainty.", "published": "2025-05-08 11:51:46", "link": "http://arxiv.org/abs/2505.05156v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Listen to Extract: Onset-Prompted Target Speaker Extraction", "abstract": "We propose $\\textit{listen to extract}$ (LExt), a highly-effective while\nextremely-simple algorithm for monaural target speaker extraction (TSE). Given\nan enrollment utterance of a target speaker, LExt aims at extracting the target\nspeaker from the speaker's mixed speech with other speakers. For each mixture,\nLExt concatenates an enrollment utterance of the target speaker to the mixture\nsignal at the waveform level, and trains deep neural networks (DNN) to extract\nthe target speech based on the concatenated mixture signal. The rationale is\nthat, this way, an artificial speech onset is created for the target speaker\nand it could prompt the DNN (a) which speaker is the target to extract; and (b)\nspectral-temporal patterns of the target speaker that could help extraction.\nThis simple approach produces strong TSE performance on multiple public TSE\ndatasets including WSJ0-2mix, WHAM! and WHAMR!.", "published": "2025-05-08 10:30:47", "link": "http://arxiv.org/abs/2505.05114v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Pairing Real-Time Piano Transcription with Symbol-level Tracking for Precise and Robust Score Following", "abstract": "Real-time music tracking systems follow a musical performance and at any time\nreport the current position in a corresponding score. Most existing methods\napproach this problem exclusively in the audio domain, typically using online\ntime warping (OLTW) techniques on incoming audio and an audio representation of\nthe score. Audio OLTW techniques have seen incremental improvements both in\nfeatures and model heuristics which reached a performance plateau in the past\nten years. We argue that converting and representing the performance in the\nsymbolic domain -- thereby transforming music tracking into a symbolic task --\ncan be a more effective approach, even when the domain transformation is\nimperfect. Our music tracking system combines two real-time components: one\nhandling audio-to-note transcription and the other a novel symbol-level tracker\nbetween transcribed input and score. We compare the performance of this mixed\naudio-symbolic approach with its equivalent audio-only counterpart, and\ndemonstrate that our method outperforms the latter in terms of both precision,\ni.e., absolute tracking error, and robustness, i.e., tracking success.", "published": "2025-05-08 09:21:23", "link": "http://arxiv.org/abs/2505.05078v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ReverbMiipher: Generative Speech Restoration meets Reverberation Characteristics Controllability", "abstract": "Reverberation encodes spatial information regarding the acoustic source\nenvironment, yet traditional Speech Restoration (SR) usually completely removes\nreverberation. We propose ReverbMiipher, an SR model extending parametric\nresynthesis framework, designed to denoise speech while preserving and enabling\ncontrol over reverberation. ReverbMiipher incorporates a dedicated\nReverbEncoder to extract a reverb feature vector from noisy input. This feature\nconditions a vocoder to reconstruct the speech signal, removing noise while\nretaining the original reverberation characteristics. A stochastic zero-vector\nreplacement strategy during training ensures the feature specifically encodes\nreverberation, disentangling it from other speech attributes. This learned\nrepresentation facilitates reverberation control via techniques such as\ninterpolation between features, replacement with features from other\nutterances, or sampling from a latent space. Objective and subjective\nevaluations confirm ReverbMiipher effectively preserves reverberation, removes\nother artifacts, and outperforms the conventional two-stage SR and convolving\nsimulated room impulse response approach. We further demonstrate its ability to\ngenerate novel reverberation effects through feature manipulation.", "published": "2025-05-08 09:17:42", "link": "http://arxiv.org/abs/2505.05077v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "How to Infer Repeat Structures in MIDI Performances", "abstract": "MIDI performances are generally expedient in performance research and music\ninformation retrieval, and even more so if they can be connected to a score.\nThis connection is usually established by means of alignment, linking either\nnotes or time points between the score and the performance. The first obstacle\nwhen trying to establish such an alignment is that a performance realizes one\n(out of many) structural versions of the score that can plausibly result from\ninstructions such as repeats, variations, and navigation markers like 'dal\nsegno/da capo al coda'. A score needs to be unfolded, that is, its repeats and\nnavigation markers need to be explicitly written out to create a single\ntimeline without jumps matching the performance, before alignment algorithms\ncan be applied. In the curation of large performance corpora this process is\ncarried out manually, as no tools are available to infer the repeat structure\nof the performance. To ease this process, we develop a method to automatically\ninfer the repeat structure of a MIDI performance, given a symbolically encoded\nscore including repeat and navigation markers. The intuition guiding our design\nis: 1) local alignment of every contiguous section of the score with a section\nof a performance containing the same material should receive high alignment\ngain, whereas local alignment with any other performance section should accrue\na low or zero gain. And 2) stitching local alignments together according to a\nvalid structural version of the score should result in an approximate full\nalignment and correspondingly high global accumulated gain if the structural\nversion corresponds to the performance, and low gain for all other, ill-fitting\nstructural versions.", "published": "2025-05-08 08:46:38", "link": "http://arxiv.org/abs/2505.05055v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On the Multiangle Discrete Fractional Fourier Transform", "abstract": "The efficiently computed multiangle centered discrete fractional Fourier\ntransform (MA-CDFRFT) [1] has proven as a useful tool for time-frequency\nanalysis; however, its scope is limited to the centered discrete fractional\nFourier transform (CDFRFT). Meanwhile, extensive research on the standard DFRFT\nhas lead to a better understanding of this transform as well as numerous\npossible choices for eigenvectors for implementation. In this letter we present\na simple adaptation of the MA-CDFRFT which allows us to efficiently compute its\nstandard counterpart, which we call the multiangle DFRFT (MA-DFRFT).\nFurthermore, we formalize the symmetries inherent to the MA-CDFRFT and MA-DFRFT\nto halve the number of FFTs needed to compute these transforms, paving the way\nfor applications in resource constrained environments.", "published": "2025-05-08 16:24:01", "link": "http://arxiv.org/abs/2505.05388v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Rodent Breathing Waveforms in ApoE Rats: Statistical and Entropic Differentiation", "abstract": "Apolipoprotein E (ApoE) gene variations are involved in lipid metabolism and\ncholesterol transport, with the ApoE4 allele being a known risk factor\nassociated with neurodegenerative conditions later in life. Emerging evidence\nsuggests these genetic variations may also influence respiratory function and\nvitality. However, the specific impact of different ApoE genotypes on breathing\npatterns remains largely unexplored. This work investigates differences in\nbreathing waveform characteristics and entropy statistics derived from\nplethysmography (PLETH) data between rat models possessing two distinct ApoE\ngenotypes (referred to herein as gene59 and gene95). Findings reveal\nsignificant distributional differences in common plethysmography metrics and\napproximate entropy between the two genotypes, observed during both active and\nresting states. Additionally, the study examines the transient impact of sighs\n(deep breaths) on these breathing metrics, demonstrating that entropy and other\nmeasures are altered in the breaths immediately following a sigh.", "published": "2025-05-08 16:20:32", "link": "http://arxiv.org/abs/2505.05387v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Chirp-Based Aliasing Analysis of Arrays in the Spherical Wavefront Regime", "abstract": "In antenna arrays, wave propagation modeling based on Euclidean principles is\ntypically represented by steering vectors or signals. This paper provides a\nnew, chirp-based, interpretation of steering vectors in the Spherical Wavefront\nRegime (SWR), establishing a relationship between the spatial spectrum of the\nreceived (resp. transmitted) signal and the geometry of the array and the\nsource (resp. target). Leveraging the well-known sampling theorem, we analyze\naliasing effects arising from spatial sampling with a finite number of antennas\nand understand how these effects degrade the Ambiguity Function (AF). Our\nframework provides geometric insight into these degradations, offering a deeper\nunderstanding of the non-space-invariant aliasing mechanisms in the SWR. The\nproposed approach is formulated for general antenna arrays and then\ninstantiated to Circular Array and to Uniform Linear Array structures operating\nin Near Field conditions.", "published": "2025-05-08 16:12:18", "link": "http://arxiv.org/abs/2505.05378v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Temperature-Resilient LC-RIS Phase-Shift Design for Multi-user Downlink Communications", "abstract": "The reflecting antenna elements in most reconfigurable intelligent surfaces\n(RISs) use semiconductor-based (e.g., positive-intrinsic-negative (PIN) diodes\nand varactors) phase shifters. Although effective, a drawback of this\ntechnology is the high power consumption and cost, which become particularly\nprohibitive in millimeter-wave (mmWave)/sub-Terahertz range. With the advances\nin Liquid Crystals (LCs) in microwave engineering, we have observed a new trend\nin using LC for realizing phase shifter networks of RISs. LC-RISs are expected\nto significantly reduce the fabrication costs and power consumption. However,\nthe nematic LC molecules are sensitive to temperature variations. Therefore,\nimplementing LC-RIS in geographical regions with varying temperatures requires\ntemperature-resilient designs. The mentioned temperature variation issue\nbecomes more significant at higher temperatures as the phase shifter range\nreduces in warmer conditions, whereas it expands in cooler ones. In this paper,\nwe study the impact of temperature on the operation of LC-RISs and develop a\ntemperature-resilient phase shift design. Specifically, we formulate a max-min\nsignal-to-interference-plus-noise ratio optimization for a multi-user downlink\nmmWave network that accounts for the impact of temperature in the LC-RIS phase\nshifts. The simulation results demonstrate a significant improvement for the\nconsidered set of parameters when using our algorithm compared to the baseline\napproach, which neglects the temperature effects.", "published": "2025-05-08 11:55:28", "link": "http://arxiv.org/abs/2505.05160v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Duplex Self-Aligning Resonant Beam Communications and Power Transfer with Coupled Spatially Distributed Laser Resonator", "abstract": "Sustainable energy supply and high-speed communications are two significant\nneeds for mobile electronic devices. This paper introduces a self-aligning\nresonant beam system for simultaneous light information and power transfer\n(SLIPT), employing a novel coupled spatially distributed resonator (CSDR). The\nsystem utilizes a resonant beam for efficient power delivery and a\nsecond-harmonic beam for concurrent data transmission, inherently minimizing\necho interference and enabling bidirectional communication. Through\ncomprehensive analyses, we investigate the CSDR's stable region, beam\nevolution, and power characteristics in relation to working distance and device\nparameters. Numerical simulations validate the CSDR-SLIPT system's feasibility\nby identifying a stable beam waist location for achieving accurate mode-match\ncoupling between two spatially distributed resonant cavities and demonstrating\nits operational range and efficient power delivery across varying distances.\nThe research reveals the system's benefits in terms of both safety and energy\ntransmission efficiency. We also demonstrate the trade-off among the\nreflectivities of the cavity mirrors in the CSDR. These findings offer valuable\ndesign insights for resonant beam systems, advancing SLIPT with significant\npotential for remote device connectivity.", "published": "2025-05-08 10:11:36", "link": "http://arxiv.org/abs/2505.05107v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Integrating Communication, Sensing, and Security: Progress and Prospects of PLS in ISAC Systems", "abstract": "The sixth generation of wireless networks defined several key performance\nindicators (KPIs) for assessing its networks, mainly in terms of reliability,\ncoverage, and sensing. In this regard, remarkable attention has been paid\nrecently to the integrated sensing and communication (ISAC) paradigm as an\nenabler for efficiently and jointly performing communication and sensing using\nthe same spectrum and hardware resources. On the other hand, ensuring\ncommunication and data security has been an imperative requirement for wireless\nnetworks throughout their evolution. The physical-layer security (PLS) concept\npaved the way to catering to the security needs in wireless networks in a\nsustainable way while guaranteeing theoretically secure transmissions,\nindependently of the computational capacity of adversaries. Therefore, it is of\nparamount importance to consider a balanced trade-off between communication\nreliability, sensing, and security in future networks, such as the 5G and\nbeyond, and the 6G. In this paper, we provide a comprehensive and system-wise\nreview of designed secure ISAC systems from a PLS point of view. In particular,\nthe impact of various physical-layer techniques, schemes, and wireless\ntechnologies to ensure the sensing-security trade-off is studied from the\nsurveyed work. Furthermore, the amalgamation of PLS and ISAC is analyzed in a\nbroader impact by considering attacks targeting data confidentiality,\ncommunication covertness, and sensing spoofing. The paper also serves as a\ntutorial by presenting several theoretical foundations on ISAC and PLS, which\nrepresent a practical guide for readers to develop novel secure ISAC network\ndesigns.", "published": "2025-05-08 09:40:14", "link": "http://arxiv.org/abs/2505.05090v1", "categories": ["cs.ET", "cs.CR", "eess.SP"], "primary_category": "cs.ET"}
{"title": "Autoregressive Stochastic Clock Jitter Compensation in Analog-to-Digital Converters", "abstract": "This paper deals with the mathematical modeling and compensation of\nstochastic discrete time clock jitter in Analog-to-Digital Converters (ADCs).\nTwo novel, computationally efficient de-jittering sample pilots-based\nalgorithms for baseband signals are proposed: one consisting in solving a\nsequence of weighted least-squares problems and another that fully leverages\nthe correlated jitter structure in a Kalman filter-type routine. Alongside, a\ncomprehensive and rigorous mathematical analysis of the linearization errors\ncommitted is presented, and the work is complemented with extensive synthetic\nsimulations and performance benchmarking with the scope of gauging and\nstress-testing the techniques in different scenarios.", "published": "2025-05-08 08:03:02", "link": "http://arxiv.org/abs/2505.05030v1", "categories": ["eess.SP", "math.OC"], "primary_category": "eess.SP"}
{"title": "Experiment Study on Reference-Path-Aided System Calibration for mmWave Bistatic ISAC Systems", "abstract": "Integrated sensing and communications (ISAC) has been regarded as a key\nenabling technology for next-generation wireless networks. Compared to\nmonostatic ISAC, bistatic ISAC can eliminate the critical challenge of\nself-interference cancellation and is well compatible with the existing network\ninfrastructures. However, the synchronization between the transmitter and the\nsensing receiver becomes a crucial problem. The extracted channel state\ninformation (CSI) for sensing under communication synchronization contains\ndifferent types of system errors, such as the sampling time offset (STO),\ncarrier frequency offset (CFO), and random phase shift, which can severely\ndegrade sensing performance or even render sensing infeasible. To address this\nproblem, a reference-path-aided system calibration scheme is designed for\nmmWave bistatic ISAC systems, where the line-of-sight (LoS) path can be\nblocked. By exploiting the delay-angle sparsity feature in mmWave ISAC systems,\nthe reference path, which can be either a LoS or a non-LoS (NLoS) path, is\nfirst identified. By leveraging the fact that all the paths suffer the same\nsystem errors, the channel parameter extracted from the reference path is\nutilized to compensate for the system errors in all other paths. A mmWave ISAC\nsystem is developed to validate our design. Experimental results demonstrate\nthat the proposed scheme can support precise estimation of Doppler shift and\ndelay, maintaining time-synchronization errors within 1 nanosecond.", "published": "2025-05-08 07:19:08", "link": "http://arxiv.org/abs/2505.05003v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Over-the-Air ODE-Inspired Neural Network for Dual Task-Oriented Semantic Communications", "abstract": "Analog machine-learning hardware platforms promise greater speed and energy\nefficiency than their digital counterparts. Specifically, over-the-air analog\ncomputation allows offloading computation to the wireless propagation through\ncarefully constructed transmitted signals. In addition, reconfigurable\nintelligent surface (RIS) is emerging as a promising solution for\nnext-generation wireless networks, offering the ability to tailor the\ncommunication environment. Leveraging the advantages of RIS, we design and\nimplement the ordinary differential equation (ODE) neural network using\nover-the-air computation (AirComp) and demonstrate its effectiveness for dual\ntasks. We engineer the ambient wireless propagation environment through\ndistributed RISs to create an architecture termed the over-the-air ordinary\ndifferential equation (Air-ODE) network. Unlike the conventional digital\nODE-inspired neural network, the Air-ODE block utilizes the physics of wave\nreflection and the reconfigurable phase shifts of RISs to implement an ODE\nblock in the analog domain, enhancing spectrum efficiency. Moreover, the\nadvantages of Air-ODE are demonstrated in a deep learning-based semantic\ncommunication (DeepSC) system by extracting effective semantic information to\nreduce the data transmission load, while achieving the dual functions of image\nreconstruction and semantic tagging simultaneously at the receiver. Simulation\nresults show that the analog Air-ODE network can achieve similar performance to\nthe digital ODE-inspired network. Specifically, for the image reconstruction\nand semantic tagging task, compared with the analog network without the Air-ODE\nblock, the Air-ODE block can achieve around 2 times gain in both reconstruction\nquality and tagging accuracy.", "published": "2025-05-08 06:02:10", "link": "http://arxiv.org/abs/2505.04970v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
