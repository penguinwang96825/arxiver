{"title": "Solving Maker-Breaker Games on 5-uniform hypergraphs is PSPACE-complete", "abstract": "Let $(X, \\mathcal{F})$ be a hypergraph. The Maker-Breaker game on $(X,\n\\mathcal{F})$ is a combinatorial game between two players, Maker and Breaker.\nBeginning with Maker, the players take turns claiming vertices from $X$ that\nhave not yet been claimed. Maker wins if she manages to claim all vertices of\nsome hyperedge $F \\in \\mathcal{F}$. Breaker wins if he claims at least one\nvertex in every hyperedge.\n  M. L. Rahman and Thomas Watson proved in 2021 that, even when only\nMaker-Breaker games on 6-uniform hypergraphs are considered, the decision\nproblem of determining which player has a winning strategy is PSPACE-complete.\nThey also showed that the problem is NL-hard when considering hypergraphs of\nrank 5.\n  In this paper, we improve the latter result by showing that deciding who wins\nMaker-Breaker games on 5-uniform hypergraphs is still a PSPACE-complete\nproblem. We achieve this by polynomial transformation from the problem of\nsolving the generalized geography game on bipartite digraphs with vertex\ndegrees 3 or less, which is known to be PSPACE-complete.", "published": "2025-02-27 16:57:05", "link": "http://arxiv.org/abs/2502.20271v1", "categories": ["cs.DM", "math.CO", "05C57", "G.2.1"], "primary_category": "cs.DM"}
{"title": "On coarse tree decompositions and coarse balanced separators", "abstract": "It is known that there is a linear dependence between the treewidth of a\ngraph and its balanced separator number: the smallest integer $k$ such that for\nevery weighing of the vertices, the graph admits a balanced separator of size\nat most $k$. We investigate whether this connection can be lifted to the\nsetting of coarse graph theory, where both the bags of the considered tree\ndecompositions and the considered separators should be coverable by a bounded\nnumber of bounded-radius balls.\n  As the first result, we prove that if an $n$-vertex graph $G$ admits balanced\nseparators coverable by $k$ balls of radius $r$, then $G$ also admits tree\ndecompositions ${\\cal T}_1$ and ${\\cal T}_2$ such that:\n  - in ${\\cal T}_1$, every bag can be covered by $O(k\\log n)$ balls of radius\n$r$; and\n  - in ${\\cal T}_2$, every bag can be covered by $O(k^2\\log k)$ balls of radius\n$r(\\log k+\\log\\log n+O(1))$.\n  As the second result, we show that if we additionally assume that $G$ has\ndoubling dimension at most $m$, then the functional equivalence between the\nexistence of small balanced separators and of tree decompositions of small\nwidth can be fully lifted to the coarse setting. Precisely, we prove that for a\npositive integer $r$ and a graph $G$ of doubling dimension at most $m$, the\nfollowing conditions are equivalent, with constants\n$k_1,k_2,k_3,k_4,\\Delta_3,\\Delta_4$ depending on each other and on $m$:\n  - $G$ admits balanced separators consisting of $k_1$ balls of radius $r$;\n  - $G$ has a tree decomposition with bags coverable by $k_2$ balls of radius\n$r$;\n  - $G$ has a tree-partition of maximum degree $\\leq \\Delta_3$ with bags\ncoverable by $k_3$ balls of radius $r$;\n  - $G$ is quasi-isometric to a graph of maximum degree $\\leq \\Delta_4$ and\ntree-partition width $\\leq k_4$.", "published": "2025-02-27 15:18:36", "link": "http://arxiv.org/abs/2502.20182v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "Computational Complexity of Covering Colored Mixed Multigraphs with Simple Degree Partitions", "abstract": "The notion of graph covers (also referred to as locally bijective\nhomomorphisms) plays an important role in topological graph theory and has\nfound its computer science applications in models of local computation. For a\nfixed target graph $H$, the {\\sc $H$-Cover} problem asks if an input graph $G$\nallows a graph covering projection onto $H$. Despite the fact that the quest\nfor characterizing the computational complexity of {\\sc $H$-Cover} had been\nstarted more than 30 years ago, only a handful of general results have been\nknown so far.\n  In this paper, we present a complete characterization of the computational\ncomplexity of covering coloured graphs for the case that every equivalence\nclass in the degree partition of the target graph has at most two vertices. We\nprove this result in a very general form. Following the lines of current\ndevelopment of topological graph theory, we study graphs in the most relaxed\nsense of the definition. In particular, we consider graphs that are mixed (they\nmay have both directed and undirected edges), may have multiple edges, loops,\nand semi-edges. We show that a strong P/NP-complete dichotomy holds true in the\nsense that for each such fixed target graph $H$, the {\\sc $H$-Cover} problem is\neither polynomial-time solvable for arbitrary inputs, or NP-complete even for\nsimple input graphs.", "published": "2025-02-27 14:46:52", "link": "http://arxiv.org/abs/2502.20151v1", "categories": ["cs.DM", "math.CO"], "primary_category": "cs.DM"}
{"title": "Aggregation of evaluations without unanimity", "abstract": "Dokow and Holzman determined which predicates over $\\{0, 1\\}$ satisfy an\nanalog of Arrow's theorem: all unanimous aggregators are dictatorial. Szegedy\nand Xu, extending earlier work of Dokow and Holzman, extended this to\npredicates over arbitrary finite alphabets.\n  Mossel extended Arrow's theorem in an orthogonal direction, determining all\naggregators without the assumption of unanimity. We bring together both threads\nof research by extending the results of Dokow-Holzman and Szegedy-Xu to the\nsetting of Mossel. As an application, we determine, for each symmetric\npredicate over $\\{0,1\\}$, all of its aggregators.", "published": "2025-02-27 13:18:27", "link": "http://arxiv.org/abs/2502.20428v2", "categories": ["math.CO", "cs.DM", "math.LO"], "primary_category": "math.CO"}
{"title": "Algebraic Machine Learning: Learning as computing an algebraic decomposition of a task", "abstract": "Statistics and Optimization are foundational to modern Machine Learning.\nHere, we propose an alternative foundation based on Abstract Algebra, with\nmathematics that facilitates the analysis of learning. In this approach, the\ngoal of the task and the data are encoded as axioms of an algebra, and a model\nis obtained where only these axioms and their logical consequences hold.\nAlthough this is not a generalizing model, we show that selecting specific\nsubsets of its breakdown into algebraic atoms obtained via subdirect\ndecomposition gives a model that generalizes. We validate this new learning\nprinciple on standard datasets such as MNIST, FashionMNIST, CIFAR-10, and\nmedical images, achieving performance comparable to optimized multilayer\nperceptrons. Beyond data-driven tasks, the new learning principle extends to\nformal problems, such as finding Hamiltonian cycles from their specifications\nand without relying on search. This algebraic foundation offers a fresh\nperspective on machine intelligence, featuring direct learning from training\ndata without the need for validation dataset, scaling through model additivity,\nand asymptotic convergence to the underlying rule in the data.", "published": "2025-02-27 10:13:42", "link": "http://arxiv.org/abs/2502.19944v1", "categories": ["cs.LG", "cs.AI", "cs.DM", "cs.SC", "math.CO", "03G10, 06A12, 06A06, 08A70, 68R01, 68T01", "G.2.3; I.1.2; I.2.6; I.2.8"], "primary_category": "cs.LG"}
{"title": "Digital Convexity and Combinatorics on Words", "abstract": "An upward (resp. downward) digitally convex word is a binary word that best\napproximates from below (resp. from above) an upward (resp. downward) convex\ncurve in the plane. We study these words from the combinatorial point of view,\nformalizing their geometric properties and highlighting connections with\nChristoffel words and finite Sturmian words. In particular, we study from the\ncombinatorial perspective the operations of inflation and deflation on\ndigitally convex words.", "published": "2025-02-27 09:54:13", "link": "http://arxiv.org/abs/2502.19926v2", "categories": ["math.CO", "cs.DM", "cs.FL", "68R15"], "primary_category": "math.CO"}
{"title": "On Piecewise Affine Reachability with Bellman Operators", "abstract": "A piecewise affine map is one of the simplest mathematical objects exhibiting\ncomplex dynamics. The reachability problem of piecewise affine maps is given as\nfollows: Given two vectors $\\mathbf{s}, \\mathbf{t} \\in \\mathbb{Q}^d$ and a\npiecewise affine map $f$, is there $n\\in \\mathbb{N}$ such that\n$f^{n}(\\mathbf{s}) = \\mathbf{t}$? Koiran, Cosnard, and Garzon show that the\nreachability problem of piecewise affine maps is undecidable even in dimension\n2.\n  Most of the recent progress has been focused on decision procedures for\none-dimensional piecewise affine maps, where the reachability problem has been\nshown to be decidable for some subclasses. However, the general undecidability\ndiscouraged research into positive results in arbitrary dimension.\n  In this work, we consider a rich subclass of piecewise affine maps defined by\nBellman operators of Markov decision processes (MDPs). We then investigate the\nrestriction of the piecewise affine reachability problem to that with Bellman\noperators and, in particular, its decidability in any dimension. As one of our\nprimary contributions, we establish the decidability of reachability for\ntwo-dimensional Bellman operators, in contrast to the negative result known for\ngeneral piecewise affine maps.", "published": "2025-02-27 09:45:06", "link": "http://arxiv.org/abs/2502.19923v1", "categories": ["cs.DM", "cs.LO", "math.DS"], "primary_category": "cs.DM"}
{"title": "Independent transversal blow-up of graphs", "abstract": "In an $r$-partite graph, an independent transversal of size $s$ (ITS)\nconsists of $s$ vertices from each part forming an independent set. Motivated\nby a question from Bollob\\'as, Erd\\H{o}s, and Szemer\\'edi (1975), Di Braccio\nand Illingworth (2024) inquired about the minimum degree needed to ensure an $n\n\\times \\cdots \\times n$ $r$-partite graph contains $K_r(s)$, a complete\n$r$-partite graph with $s$ vertices in each part. We reformulate this as\nfinding the smallest $n$ such that any $n \\times \\cdots \\times n$ $r$-partite\ngraph with maximum degree $\\Delta$ has an ITS. For any $\\varepsilon>0$, we\nprove the existence of a $\\gamma>0$ ensuring that if $G$ is a multipartite\ngraph partitioned as $(V_1, V_2, \\ldots, V_r)$, where the average degree of\neach part $V_i$ is at most $D$, the maximum degree of any vertex to any part\n$V_i$ is at most $\\gamma D$, and the size of each part $V_i$ is at least $(s +\n\\varepsilon)D$, then $G$ possesses an ITS. The constraint $(s + \\varepsilon)D$\non the part size is tight. This extends results of Loh and Sudakov (2007),\nGlock and Sudakov (2022), and Kang and Kelly (2022). We also show that any $n\n\\times \\cdots \\times n$ $r$-partite graph with minimum degree at least\n$\\left(r-1-\\frac{1}{2s^2}\\right)n$ contains $K_r(s)$ and provide a relative\nTur\\'an-type result. Additionally, this paper explores counting ITSs in\nmultipartite graphs.", "published": "2025-02-27 01:46:51", "link": "http://arxiv.org/abs/2502.19682v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "BiHRNN -- Bi-Directional Hierarchical Recurrent Neural Network for Inflation Forecasting", "abstract": "Inflation prediction guides decisions on interest rates, investments, and\nwages, playing a key role in economic stability. Yet accurate forecasting is\nchallenging due to dynamic factors and the layered structure of the Consumer\nPrice Index, which organizes goods and services into multiple categories. We\npropose the Bi-directional Hierarchical Recurrent Neural Network (BiHRNN) model\nto address these challenges by leveraging the hierarchical structure to enable\nbidirectional information flow between levels. Informative constraints on the\nRNN parameters enhance predictive accuracy at all levels without the\ninefficiencies of a unified model. We validated BiHRNN on inflation datasets\nfrom the United States, Canada, and Norway by training, tuning hyperparameters,\nand experimenting with various loss functions. Our results demonstrate that\nBiHRNN significantly outperforms traditional RNN models, with its bidirectional\narchitecture playing a pivotal role in achieving improved forecasting accuracy.", "published": "2025-02-27 16:12:03", "link": "http://arxiv.org/abs/2503.01893v1", "categories": ["cs.LG", "econ.GN", "q-fin.CP", "q-fin.EC"], "primary_category": "cs.LG"}
{"title": "Optimal risk-aware interest rates for decentralized lending protocols", "abstract": "Decentralized lending protocols within the decentralized finance ecosystem\nenable the lending and borrowing of crypto-assets without relying on\ntraditional intermediaries. Interest rates in these protocols are set\nalgorithmically and fluctuate according to the supply and demand for liquidity.\nIn this study, we propose an agent-based model tailored to a decentralized\nlending protocol and determine the optimal interest rate model. When the\nresponses of the agents are linear with respect to the interest rate, the\noptimal solution is derived from a system of Riccati-type ODEs. For nonlinear\nbehaviors, we propose a Monte-Carlo estimator, coupled with deep learning\ntechniques, to approximate the optimal solution. Finally, after calibrating the\nmodel using block-by-block data, we conduct a risk-adjusted profit and loss\nanalysis of the liquidity pool under industry-standard interest rate models and\nbenchmark them against the optimal interest rate model.", "published": "2025-02-27 08:05:44", "link": "http://arxiv.org/abs/2502.19862v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Detecting Crypto Pump-and-Dump Schemes: A Thresholding-Based Approach to Handling Market Noise", "abstract": "We propose a simple yet robust unsupervised model to detect pump-and-dump\nevents on tokens listed on the Poloniex Exchange platform. By combining\nthreshold-based criteria with exponentially weighted moving averages (EWMA) and\nvolatility measures, our approach effectively distinguishes genuine anomalies\nfrom minor trading fluctuations, even for tokens with low liquidity and\nprolonged inactivity. These characteristics present a unique challenge, as\nstandard anomaly-detection methods often over-flag negligible volume spikes.\nOur framework overcomes this issue by tailoring both price and volume\nthresholds to the specific trading patterns observed, resulting in a model that\nbalances high true-positive detection with minimal noise.", "published": "2025-02-27 09:50:14", "link": "http://arxiv.org/abs/2503.08692v1", "categories": ["q-fin.ST", "q-fin.TR"], "primary_category": "q-fin.ST"}
{"title": "Better market Maker Algorithm to Save Impermanent Loss with High Liquidity Retention", "abstract": "Decentralized exchanges (DEXs) face persistent challenges in liquidity\nretention and user engagement due to inefficiencies in conventional automated\nmarket maker (AMM) designs. This work proposes a dual-mechanism framework to\naddress these limitations: a ``Better Market Maker (BMM)'', which is a\nliquidity-optimized AMM based on a power-law invariant ($X^nY = K$, $n = 4$),\nand a dynamic rebate system (DRS) for redistributing transaction fees. The\nsegment-specific BMM reduces impermanent loss by 36\\% compared to traditional\nconstant-product ($XY = K$) models, while retaining 3.98x more liquidity during\nprice volatility. The DRS allocates fees ($\\gamma V$, $\\gamma \\in \\{0.003,\n0.005, 0.01\\}$) with a rebate ratio $\\rho \\in [0.3, 0.4]$ to incentivize trader\nparticipation and maintain continuous capital injection. Simulations under\nhigh-volatility conditions demonstrate impermanent loss reductions of 36.0\\%\nand 40\\% higher user engagement compared to static fee models. By segmenting\nmarkets into high-, mid-, and low-volatility regimes, the framework achieves\nliquidity depth comparable to centralized exchanges (CEXs) while maintaining\ndecentralized governance and retaining value within the cryptocurrency\necosystem.", "published": "2025-02-27 11:28:38", "link": "http://arxiv.org/abs/2502.20001v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "Investigating Neurons and Heads in Transformer-based LLMs for\n  Typographical Errors", "abstract": "This paper investigates how LLMs encode inputs with typos. We hypothesize\nthat specific neurons and attention heads recognize typos and fix them\ninternally using local and global contexts. We introduce a method to identify\ntypo neurons and typo heads that work actively when inputs contain typos. Our\nexperimental results suggest the following: 1) LLMs can fix typos with local\ncontexts when the typo neurons in either the early or late layers are\nactivated, even if those in the other are not. 2) Typo neurons in the middle\nlayers are responsible for the core of typo-fixing with global contexts. 3)\nTypo heads fix typos by widely considering the context not focusing on specific\ntokens. 4) Typo neurons and typo heads work not only for typo-fixing but also\nfor understanding general contexts.", "published": "2025-02-27 01:30:07", "link": "http://arxiv.org/abs/2502.19669v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GRACE: A Granular Benchmark for Evaluating Model Calibration against\n  Human Calibration", "abstract": "Language models are often miscalibrated, leading to confidently incorrect\nanswers. We introduce GRACE, a benchmark for language model calibration that\nincorporates comparison with human calibration. GRACE consists of\nquestion-answer pairs, in which each question contains a series of clues that\ngradually become easier, all leading to the same answer; models must answer\ncorrectly as early as possible as the clues are revealed. This setting permits\ngranular measurement of model calibration based on how early, accurately, and\nconfidently a model answers. After collecting these questions, we host live\nhuman vs. model competitions to gather 1,749 data points on human and model\nteams' timing, accuracy, and confidence. We propose a metric, CalScore, that\nuses GRACE to analyze model calibration errors and identify types of model\nmiscalibration that differ from human behavior. We find that although humans\nare less accurate than models, humans are generally better calibrated. Since\nstate-of-the-art models struggle on GRACE, it effectively evaluates progress on\nimproving model calibration.", "published": "2025-02-27 01:51:45", "link": "http://arxiv.org/abs/2502.19684v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Preference Learning Unlocks LLMs' Psycho-Counseling Skills", "abstract": "Applying large language models (LLMs) to assist in psycho-counseling is an\nemerging and meaningful approach, driven by the significant gap between patient\nneeds and the availability of mental health support. However, current LLMs\nstruggle to consistently provide effective responses to client speeches,\nlargely due to the lack of supervision from high-quality real psycho-counseling\ndata, whose content is typically inaccessible due to client privacy concerns.\nFurthermore, the quality of therapists' responses in available sessions can\nvary significantly based on their professional training and experience.\nAssessing the quality of therapists' responses remains an open challenge. In\nthis work, we address these challenges by first proposing a set of professional\nand comprehensive principles to evaluate therapists' responses to client\nspeeches. Using these principles, we create a preference dataset,\nPsychoCounsel-Preference, which contains 36k high-quality preference comparison\npairs. This dataset aligns with the preferences of professional\npsychotherapists, providing a robust foundation for evaluating and improving\nLLMs in psycho-counseling. Experiments on reward modeling and preference\nlearning demonstrate that PsychoCounsel-Preference is an excellent resource for\nLLMs to acquire essential skills for responding to clients in a counseling\nsession. Our best-aligned model, PsychoCounsel-Llama3-8B, achieves an\nimpressive win rate of 87% against GPT-4o. We release PsychoCounsel-Preference,\nPsychoCounsel-Llama3-8B and the reward model PsychoCounsel Llama3-8B-Reward to\nfacilitate the research of psycho-counseling with LLMs at:\nhttps://hf.co/Psychotherapy-LLM.", "published": "2025-02-27 03:50:25", "link": "http://arxiv.org/abs/2502.19731v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speculative Decoding and Beyond: An In-Depth Survey of Techniques", "abstract": "Sequential dependencies present a fundamental bottleneck in deploying\nlarge-scale autoregressive models, particularly for real-time applications.\nWhile traditional optimization approaches like pruning and quantization often\ncompromise model quality, recent advances in generation-refinement frameworks\ndemonstrate that this trade-off can be significantly mitigated.\n  This survey presents a comprehensive taxonomy of generation-refinement\nframeworks, analyzing methods across autoregressive sequence tasks. We\ncategorize methods based on their generation strategies (from simple n-gram\nprediction to sophisticated draft models) and refinement mechanisms (including\nsingle-pass verification and iterative approaches). Through systematic analysis\nof both algorithmic innovations and system-level implementations, we examine\ndeployment strategies across computing environments and explore applications\nspanning text, images, and speech generation. This systematic examination of\nboth theoretical frameworks and practical implementations provides a foundation\nfor future research in efficient autoregressive decoding.", "published": "2025-02-27 03:53:45", "link": "http://arxiv.org/abs/2502.19732v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning\n  Learning", "abstract": "Despite recent breakthroughs in reasoning-enhanced large language models\n(LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine\ntranslation (MT), where human translators naturally employ structured,\nmulti-layered reasoning chain-of-thoughts (CoTs), is yet underexplored.\nExisting methods either design a fixed CoT tailored for a specific MT sub-task\n(e.g., literature translation), or rely on synthesizing CoTs unaligned with\nhumans, limiting their adaptability to diverse translation scenarios. This\npaper introduces R1-Translator (R1-T1), a novel framework to achieve\ninference-time reasoning for general MT via reinforcement learning (RL) with\nhuman-aligned CoTs comprising six common patterns. Our approach pioneers three\ninnovations: (1) extending reasoning-based translation beyond MT sub-tasks to\nsix languages and diverse tasks (e.g., legal/medical domain adaptation, idiom\nresolution); (2) formalizing six expert-curated CoT templates that mirror\nhybrid human strategies like context-aware paraphrasing and back translation;\nand (3) enabling self-evolving CoT discovery through RL. Experimental results\nindicate a steady translation performance improvement in 11 languages and 40\ntranslation directions on Flores-101 test set, especially on the languages\nunseen from training.", "published": "2025-02-27 03:57:00", "link": "http://arxiv.org/abs/2502.19735v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XCOMPS: A Multilingual Benchmark of Conceptual Minimal Pairs", "abstract": "We introduce XCOMPS in this work, a multilingual conceptual minimal pair\ndataset covering 17 languages. Using this dataset, we evaluate LLMs'\nmultilingual conceptual understanding through metalinguistic prompting, direct\nprobability measurement, and neurolinguistic probing. By comparing base,\ninstruction-tuned, and knowledge-distilled models, we find that: 1) LLMs\nexhibit weaker conceptual understanding for low-resource languages, and\naccuracy varies across languages despite being tested on the same concept sets.\n2) LLMs excel at distinguishing concept-property pairs that are visibly\ndifferent but exhibit a marked performance drop when negative pairs share\nsubtle semantic similarities. 3) Instruction tuning improves performance in\nconcept understanding but does not enhance internal competence; knowledge\ndistillation can enhance internal competence in conceptual understanding for\nlow-resource languages with limited gains in explicit task performance. 4) More\nmorphologically complex languages yield lower concept understanding scores and\nrequire deeper layers for conceptual reasoning.", "published": "2025-02-27 04:02:13", "link": "http://arxiv.org/abs/2502.19737v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beneath the Surface: How Large Language Models Reflect Hidden Bias", "abstract": "The exceptional performance of Large Language Models (LLMs) often comes with\nthe unintended propagation of social biases embedded in their training data.\nWhile existing benchmarks evaluate overt bias through direct term associations\nbetween bias concept terms and demographic terms, LLMs have become increasingly\nadept at avoiding biased responses, creating an illusion of neutrality.\nHowever, biases persist in subtler, contextually hidden forms that traditional\nbenchmarks fail to capture. We introduce the Hidden Bias Benchmark (HBB), a\nnovel dataset designed to assess hidden bias that bias concepts are hidden\nwithin naturalistic, subtly framed contexts in real-world scenarios. We analyze\nsix state-of-the-art LLMs, revealing that while models reduce bias in response\nto overt bias, they continue to reinforce biases in nuanced settings. Data,\ncode, and results are available at\nhttps://github.com/JP-25/Hidden-Bias-Benchmark.", "published": "2025-02-27 04:25:54", "link": "http://arxiv.org/abs/2502.19749v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advancements in Natural Language Processing for Automatic Text\n  Summarization", "abstract": "The substantial growth of textual content in diverse domains and platforms\nhas led to a considerable need for Automatic Text Summarization (ATS)\ntechniques that aid in the process of text analysis. The effectiveness of text\nsummarization models has been significantly enhanced in a variety of technical\ndomains because of advancements in Natural Language Processing (NLP) and Deep\nLearning (DL). Despite this, the process of summarizing textual information\ncontinues to be significantly constrained by the intricate writing styles of a\nvariety of texts, which involve a range of technical complexities. Text\nsummarization techniques can be broadly categorized into two main types:\nabstractive summarization and extractive summarization. Extractive\nsummarization involves directly extracting sentences, phrases, or segments of\ntext from the content without making any changes. On the other hand,\nabstractive summarization is achieved by reconstructing the sentences, phrases,\nor segments from the original text using linguistic analysis. Through this\nstudy, a linguistically diverse categorizations of text summarization\napproaches have been addressed in a constructive manner. In this paper, the\nauthors explored existing hybrid techniques that have employed both extractive\nand abstractive methodologies. In addition, the pros and cons of various\napproaches discussed in the literature are also investigated. Furthermore, the\nauthors conducted a comparative analysis on different techniques and matrices\nto evaluate the generated summaries using language generation models. This\nsurvey endeavors to provide a comprehensive overview of ATS by presenting the\nprogression of language processing regarding this task through a breakdown of\ndiverse systems and architectures accompanied by technical and mathematical\nexplanations of their operations.", "published": "2025-02-27 05:17:36", "link": "http://arxiv.org/abs/2502.19773v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Team A at SemEval-2025 Task 11: Breaking Language Barriers in Emotion\n  Detection with Multilingual Models", "abstract": "This paper describes the system submitted by Team A to SemEval 2025 Task 11,\n``Bridging the Gap in Text-Based Emotion Detection.'' The task involved\nidentifying the perceived emotion of a speaker from text snippets, with each\ninstance annotated with one of six emotions: joy, sadness, fear, anger,\nsurprise, or disgust. A dataset provided by the task organizers served as the\nfoundation for training and evaluating our models. Among the various approaches\nexplored, the best performance was achieved using multilingual embeddings\ncombined with a fully connected layer. This paper details the system\narchitecture, discusses experimental results, and highlights the advantages of\nleveraging multilingual representations for robust emotion detection in text.", "published": "2025-02-27 07:59:01", "link": "http://arxiv.org/abs/2502.19856v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge", "abstract": "Knowledge editing techniques have emerged as essential tools for updating the\nfactual knowledge of large language models (LLMs) and multimodal models (LMMs),\nallowing them to correct outdated or inaccurate information without retraining\nfrom scratch. However, existing benchmarks for multimodal knowledge editing\nprimarily focus on entity-level knowledge represented as simple triplets, which\nfail to capture the complexity of real-world multimodal information. To address\nthis issue, we introduce MMKE-Bench, a comprehensive MultiModal Knowledge\nEditing Benchmark, designed to evaluate the ability of LMMs to edit diverse\nvisual knowledge in real-world scenarios. MMKE-Bench addresses these\nlimitations by incorporating three types of editing tasks: visual entity\nediting, visual semantic editing, and user-specific editing. Besides,\nMMKE-Bench uses free-form natural language to represent and edit knowledge,\noffering a more flexible and effective format. The benchmark consists of 2,940\npieces of knowledge and 8,363 images across 33 broad categories, with\nevaluation questions automatically generated and human-verified. We assess five\nstate-of-the-art knowledge editing methods on three prominent LMMs, revealing\nthat no method excels across all criteria, and that visual and user-specific\nedits are particularly challenging. MMKE-Bench sets a new standard for\nevaluating the robustness of multimodal knowledge editing techniques, driving\nprogress in this rapidly evolving field.", "published": "2025-02-27 08:21:28", "link": "http://arxiv.org/abs/2502.19870v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Picking the Cream of the Crop: Visual-Centric Data Selection with\n  Collaborative Agents", "abstract": "To improve Multimodal Large Language Models' (MLLMs) ability to process\nimages and complex instructions, researchers predominantly curate large-scale\nvisual instruction tuning datasets, which are either sourced from existing\nvision tasks or synthetically generated using LLMs and image descriptions.\nHowever, they often suffer from critical flaws, including misaligned\ninstruction-image pairs and low-quality images. Such issues hinder training\nefficiency and limit performance improvements, as models waste resources on\nnoisy or irrelevant data with minimal benefit to overall capability. To address\nthis issue, we propose a \\textbf{Vi}sual-Centric \\textbf{S}election approach\nvia \\textbf{A}gents Collaboration (ViSA), which centers on image quality\nassessment and image-instruction relevance evaluation. Specifically, our\napproach consists of 1) an image information quantification method via visual\nagents collaboration to select images with rich visual information, and 2) a\nvisual-centric instruction quality assessment method to select high-quality\ninstruction data related to high-quality images. Finally, we reorganize 80K\ninstruction data from large open-source datasets. Extensive experiments\ndemonstrate that ViSA outperforms or is comparable to current state-of-the-art\nmodels on seven benchmarks, using only 2.5\\% of the original data, highlighting\nthe efficiency of our data selection approach. Moreover, we conduct ablation\nstudies to validate the effectiveness of each component of our method. The code\nis available at https://github.com/HITsz-TMG/ViSA.", "published": "2025-02-27 09:37:30", "link": "http://arxiv.org/abs/2502.19917v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Alleviating Distribution Shift in Synthetic Data for Machine Translation\n  Quality Estimation", "abstract": "Quality Estimation (QE) models evaluate the quality of machine translations\nwithout reference translations, serving as the reward models for the\ntranslation task. Due to the data scarcity, synthetic data generation has\nemerged as a promising solution. However, synthetic QE data often suffers from\ndistribution shift, which can manifest as discrepancies between pseudo and real\ntranslations, or in pseudo labels that do not align with human preferences. To\ntackle this issue, we introduce ADSQE, a novel framework for alleviating\ndistribution shift in synthetic QE data. To reduce the difference between\npseudo and real translations, we employ the constrained beam search algorithm\nand enhance translation diversity through the use of distinct generation\nmodels. ADSQE uses references, i.e., translation supervision signals, to guide\nboth the generation and annotation processes, enhancing the quality of\nword-level labels. ADSE further identifies the shortest phrase covering\nconsecutive error tokens, mimicking human annotation behavior, to assign the\nfinal phrase-level labels. Specially, we underscore that the translation model\ncan not annotate translations of itself accurately. Extensive experiments\ndemonstrate that ADSQE outperforms SOTA baselines like COMET in both supervised\nand unsupervised settings. Further analysis offers insights into synthetic data\ngeneration that could benefit reward models for other tasks.", "published": "2025-02-27 10:11:53", "link": "http://arxiv.org/abs/2502.19941v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GeoEdit: Geometric Knowledge Editing for Large Language Models", "abstract": "Regular updates are essential for maintaining up-to-date knowledge in large\nlanguage models (LLMs). Consequently, various model editing methods have been\ndeveloped to update specific knowledge within LLMs. However, training-based\napproaches often struggle to effectively incorporate new knowledge while\npreserving unrelated general knowledge. To address this challenge, we propose a\nnovel framework called Geometric Knowledge Editing (GeoEdit). GeoEdit utilizes\nthe geometric relationships of parameter updates from fine-tuning to\ndifferentiate between neurons associated with new knowledge updates and those\nrelated to general knowledge perturbations. By employing a direction-aware\nknowledge identification method, we avoid updating neurons with directions\napproximately orthogonal to existing knowledge, thus preserving the model's\ngeneralization ability. For the remaining neurons, we integrate both old and\nnew knowledge for aligned directions and apply a \"forget-then-learn\" editing\nstrategy for opposite directions. Additionally, we introduce an\nimportance-guided task vector fusion technique that filters out redundant\ninformation and provides adaptive neuron-level weighting, further enhancing\nmodel editing performance. Extensive experiments on two publicly available\ndatasets demonstrate the superiority of GeoEdit over existing state-of-the-art\nmethods.", "published": "2025-02-27 10:27:48", "link": "http://arxiv.org/abs/2502.19953v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Lookahead Limitation: Why Multi-Operand Addition is Hard for LLMs", "abstract": "Autoregressive large language models (LLMs) exhibit impressive performance\nacross various tasks but struggle with simple arithmetic, such as addition of\ntwo or more operands. We show that this struggle arises from LLMs' use of a\nsimple one-digit lookahead heuristic, which works fairly well (but not perfect)\nfor two-operand addition but fails in multi-operand cases, where the carry-over\nlogic is more complex. Our probing experiments and digit-wise accuracy\nevaluation show that LLMs fail precisely where a one-digit lookahead is\ninsufficient to account for cascading carries. We analyze the impact of\ntokenization strategies on arithmetic performance and show that all\ninvestigated models, regardless of tokenization, are inherently limited in the\naddition of multiple operands due to their reliance on a one-digit lookahead\nheuristic. Our findings reveal fundamental limitations that prevent LLMs from\ngeneralizing to more complex numerical reasoning.", "published": "2025-02-27 11:03:27", "link": "http://arxiv.org/abs/2502.19981v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Connecting the Persian-speaking World through Transliteration", "abstract": "Despite speaking mutually intelligible varieties of the same language,\nspeakers of Tajik Persian, written in a modified Cyrillic alphabet, cannot read\nIranian and Afghan texts written in the Perso-Arabic script. As the vast\nmajority of Persian text on the Internet is written in Perso-Arabic,\nmonolingual Tajik speakers are unable to interface with the Internet in any\nmeaningful way. Due to overwhelming similarity between the formal registers of\nthese dialects and the scarcity of Tajik-Farsi parallel data, machine\ntransliteration has been proposed as more a practical and appropriate solution\nthan machine translation. This paper presents a transformer-based G2P approach\nto Tajik-Farsi transliteration, achieving chrF++ scores of 58.70 (Farsi to\nTajik) and 74.20 (Tajik to Farsi) on novel digraphic datasets, setting a\ncomparable baseline metric for future work. Our results also demonstrate the\nnon-trivial difficulty of this task in both directions. We also provide an\noverview of the differences between the two scripts and the challenges they\npresent, so as to aid future efforts in Tajik-Farsi transliteration.", "published": "2025-02-27 12:38:36", "link": "http://arxiv.org/abs/2502.20047v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LongRoPE2: Near-Lossless LLM Context Window Scaling", "abstract": "LongRoPE2 is a novel approach that extends the effective context window of\npre-trained large language models (LLMs) to the target length, while preserving\nthe performance on the original shorter context window. This is achieved by\nthree contributions: (1) a hypothesis that insufficient training in higher RoPE\ndimensions contributes to the persistent out-of-distribution (OOD) issues\nobserved in existing methods; (2) an effective RoPE rescaling algorithm that\nadopts evolutionary search guided by \"needle-driven\" perplexity to address the\ninsufficient training problem; (3) a mixed context window training approach\nthat fine-tunes model weights to adopt rescaled RoPE for long-context sequences\nwhile preserving the short-context performance with the original RoPE.\nExtensive experiments on LLaMA3-8B and Phi3-mini-3.8B across various benchmarks\nvalidate the hypothesis and demonstrate the effectiveness of LongRoPE2.\nRemarkably, LongRoPE2 extends LLaMA3-8B to achieve a 128K effective context\nlength while retaining over 98.5% of short-context performance, using only 10B\ntokens -- 80x fewer than Meta's approach, which fails to reach the target\neffective context length. Code will be available at\nhttps://github.com/microsoft/LongRoPE.", "published": "2025-02-27 13:41:07", "link": "http://arxiv.org/abs/2502.20082v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Educator Attention: How computational tools can systematically identify\n  the distribution of a key resource for students", "abstract": "Educator attention is critical for student success, yet how educators\ndistribute their attention across students remains poorly understood due to\ndata and methodological constraints. This study presents the first large-scale\ncomputational analysis of educator attention patterns, leveraging over 1\nmillion educator utterances from virtual group tutoring sessions linked to\ndetailed student demographic and academic achievement data. Using natural\nlanguage processing techniques, we systematically examine the recipient and\nnature of educator attention. Our findings reveal that educators often provide\nmore attention to lower-achieving students. However, disparities emerge across\ndemographic lines, particularly by gender. Girls tend to receive less attention\nwhen paired with boys, even when they are the lower achieving student in the\ngroup. Lower-achieving female students in mixed-gender pairs receive\nsignificantly less attention than their higher-achieving male peers, while\nlower-achieving male students receive significantly and substantially more\nattention than their higher-achieving female peers. We also find some\ndifferences by race and English learner (EL) status, with low-achieving Black\nstudents receiving additional attention only when paired with another Black\nstudent but not when paired with a non-Black peer. In contrast,\nhigher-achieving EL students receive disproportionately more attention than\ntheir lower-achieving EL peers. This work highlights how large-scale\ninteraction data and computational methods can uncover subtle but meaningful\ndisparities in teaching practices, providing empirical insights to inform more\nequitable and effective educational strategies.", "published": "2025-02-27 14:30:18", "link": "http://arxiv.org/abs/2502.20135v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChineseEcomQA: A Scalable E-commerce Concept Evaluation Benchmark for\n  Large Language Models", "abstract": "With the increasing use of Large Language Models (LLMs) in fields such as\ne-commerce, domain-specific concept evaluation benchmarks are crucial for\nassessing their domain capabilities. Existing LLMs may generate factually\nincorrect information within the complex e-commerce applications. Therefore, it\nis necessary to build an e-commerce concept benchmark. Existing benchmarks\nencounter two primary challenges: (1) handle the heterogeneous and diverse\nnature of tasks, (2) distinguish between generality and specificity within the\ne-commerce field. To address these problems, we propose \\textbf{ChineseEcomQA},\na scalable question-answering benchmark focused on fundamental e-commerce\nconcepts. ChineseEcomQA is built on three core characteristics: \\textbf{Focus\non Fundamental Concept}, \\textbf{E-commerce Generality} and \\textbf{E-commerce\nExpertise}. Fundamental concepts are designed to be applicable across a diverse\narray of e-commerce tasks, thus addressing the challenge of heterogeneity and\ndiversity. Additionally, by carefully balancing generality and specificity,\nChineseEcomQA effectively differentiates between broad e-commerce concepts,\nallowing for precise validation of domain capabilities. We achieve this through\na scalable benchmark construction process that combines LLM validation,\nRetrieval-Augmented Generation (RAG) validation, and rigorous manual\nannotation. Based on ChineseEcomQA, we conduct extensive evaluations on\nmainstream LLMs and provide some valuable insights. We hope that ChineseEcomQA\ncould guide future domain-specific evaluations, and facilitate broader LLM\nadoption in e-commerce applications.", "published": "2025-02-27 15:36:00", "link": "http://arxiv.org/abs/2502.20196v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through\n  Reflective Puzzle Solving", "abstract": "Many challenging reasoning tasks require not just rapid, intuitive responses,\nbut a more deliberate, multi-step approach. Recent progress in large language\nmodels (LLMs) highlights an important shift from the \"System 1\" way of quick\nreactions to the \"System 2\" style of reflection-and-correction problem solving.\nHowever, current benchmarks heavily rely on the final-answer accuracy, leaving\nmuch of a model's intermediate reasoning steps unexamined. This fails to assess\nthe model's ability to reflect and rectify mistakes within the reasoning\nprocess. To bridge this gap, we introduce FINEREASON, a logic-puzzle benchmark\nfor fine-grained evaluation of LLMs' reasoning capabilities. Each puzzle can be\ndecomposed into atomic steps, making it ideal for rigorous validation of\nintermediate correctness. Building on this, we introduce two tasks: state\nchecking, and state transition, for a comprehensive evaluation of how models\nassess the current situation and plan the next move. To support broader\nresearch, we also provide a puzzle training set aimed at enhancing performance\non general mathematical tasks. We show that models trained on our state\nchecking and transition data demonstrate gains in math reasoning by up to 5.1%\non GSM8K.", "published": "2025-02-27 16:23:25", "link": "http://arxiv.org/abs/2502.20238v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Retrieval to Generation: Comparing Different Approaches", "abstract": "Knowledge-intensive tasks, particularly open-domain question answering\n(ODQA), document reranking, and retrieval-augmented language modeling, require\na balance between retrieval accuracy and generative flexibility. Traditional\nretrieval models such as BM25 and Dense Passage Retrieval (DPR), efficiently\nretrieve from large corpora but often lack semantic depth. Generative models\nlike GPT-4-o provide richer contextual understanding but face challenges in\nmaintaining factual consistency. In this work, we conduct a systematic\nevaluation of retrieval-based, generation-based, and hybrid models, with a\nprimary focus on their performance in ODQA and related retrieval-augmented\ntasks. Our results show that dense retrievers, particularly DPR, achieve strong\nperformance in ODQA with a top-1 accuracy of 50.17\\% on NQ, while hybrid models\nimprove nDCG@10 scores on BEIR from 43.42 (BM25) to 52.59, demonstrating their\nstrength in document reranking. Additionally, we analyze language modeling\ntasks using WikiText-103, showing that retrieval-based approaches like BM25\nachieve lower perplexity compared to generative and hybrid methods,\nhighlighting their utility in retrieval-augmented generation. By providing\ndetailed comparisons and practical insights into the conditions where each\napproach excels, we aim to facilitate future optimizations in retrieval,\nreranking, and generative models for ODQA and related knowledge-intensive\napplications.", "published": "2025-02-27 16:29:14", "link": "http://arxiv.org/abs/2502.20245v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Natural Language Perplexity: Detecting Dead Code Poisoning in\n  Code Generation Datasets", "abstract": "The increasing adoption of large language models (LLMs) for code-related\ntasks has raised concerns about the security of their training datasets. One\ncritical threat is dead code poisoning, where syntactically valid but\nfunctionally redundant code is injected into training data to manipulate model\nbehavior. Such attacks can degrade the performance of neural code search\nsystems, leading to biased or insecure code suggestions. Existing detection\nmethods, such as token-level perplexity analysis, fail to effectively identify\ndead code due to the structural and contextual characteristics of programming\nlanguages. In this paper, we propose DePA (Dead Code Perplexity Analysis), a\nnovel line-level detection and cleansing method tailored to the structural\nproperties of code. DePA computes line-level perplexity by leveraging the\ncontextual relationships between code lines and identifies anomalous lines by\ncomparing their perplexity to the overall distribution within the file. Our\nexperiments on benchmark datasets demonstrate that DePA significantly\noutperforms existing methods, achieving 0.14-0.19 improvement in detection\nF1-score and a 44-65% increase in poisoned segment localization precision.\nFurthermore, DePA enhances detection speed by 0.62-23x, making it practical for\nlarge-scale dataset cleansing. Overall, by addressing the unique challenges of\ndead code poisoning, DePA provides a robust and efficient solution for\nsafeguarding the integrity of code generation model training datasets.", "published": "2025-02-27 16:30:00", "link": "http://arxiv.org/abs/2502.20246v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Long-Context Inference with Retrieval-Augmented Speculative Decoding", "abstract": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications.", "published": "2025-02-27 17:59:36", "link": "http://arxiv.org/abs/2502.20330v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sparse Auto-Encoder Interprets Linguistic Features in Large Language\n  Models", "abstract": "Large language models (LLMs) excel in tasks that require complex linguistic\nabilities, such as reference disambiguation and metaphor\nrecognition/generation. Although LLMs possess impressive capabilities, their\ninternal mechanisms for processing and representing linguistic knowledge remain\nlargely opaque. Previous work on linguistic mechanisms has been limited by\ncoarse granularity, insufficient causal analysis, and a narrow focus. In this\nstudy, we present a systematic and comprehensive causal investigation using\nsparse auto-encoders (SAEs). We extract a wide range of linguistic features\nfrom six dimensions: phonetics, phonology, morphology, syntax, semantics, and\npragmatics. We extract, evaluate, and intervene on these features by\nconstructing minimal contrast datasets and counterfactual sentence datasets. We\nintroduce two indices-Feature Representation Confidence (FRC) and Feature\nIntervention Confidence (FIC)-to measure the ability of linguistic features to\ncapture and control linguistic phenomena. Our results reveal inherent\nrepresentations of linguistic knowledge in LLMs and demonstrate the potential\nfor controlling model outputs. This work provides strong evidence that LLMs\npossess genuine linguistic knowledge and lays the foundation for more\ninterpretable and controllable language modeling in future research.", "published": "2025-02-27 18:16:47", "link": "http://arxiv.org/abs/2502.20344v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KEDRec-LM: A Knowledge-distilled Explainable Drug Recommendation Large\n  Language Model", "abstract": "Drug discovery is a critical task in biomedical natural language processing\n(NLP), yet explainable drug discovery remains underexplored. Meanwhile, large\nlanguage models (LLMs) have shown remarkable abilities in natural language\nunderstanding and generation. Leveraging LLMs for explainable drug discovery\nhas the potential to improve downstream tasks and real-world applications. In\nthis study, we utilize open-source drug knowledge graphs, clinical trial data,\nand PubMed publications to construct a comprehensive dataset for the\nexplainable drug discovery task, named \\textbf{expRxRec}. Furthermore, we\nintroduce \\textbf{KEDRec-LM}, an instruction-tuned LLM which distills knowledge\nfrom rich medical knowledge corpus for drug recommendation and rationale\ngeneration. To encourage further research in this area, we will publicly\nrelease\\footnote{A copy is attached with this submission} both the dataset and\nKEDRec-LM.", "published": "2025-02-27 18:22:33", "link": "http://arxiv.org/abs/2502.20350v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shades of Zero: Distinguishing Impossibility from Inconceivability", "abstract": "Some things are impossible, but some things may be even more impossible than\nimpossible. Levitating a feather using one's mind is impossible in our world,\nbut fits into our intuitive theories of possible worlds, whereas levitating a\nfeather using the number five cannot be conceived in any possible world\n(\"inconceivable\"). While prior work has examined the distinction between\nimprobable and impossible events, there has been little empirical research on\ninconceivability. Here, we investigate whether people maintain a distinction\nbetween impossibility and inconceivability, and how such distinctions might be\nmade. We find that people can readily distinguish the impossible from the\ninconceivable, using categorization studies similar to those used to\ninvestigate the differences between impossible and improbable (Experiment 1).\nHowever, this distinction is not explained by people's subjective ratings of\nevent likelihood, which are near zero and indistinguishable between impossible\nand inconceivable event descriptions (Experiment 2). Finally, we ask whether\nthe probabilities assigned to event descriptions by statistical language models\n(LMs) can be used to separate modal categories, and whether these probabilities\nalign with people's ratings (Experiment 3). We find high-level similarities\nbetween people and LMs: both distinguish among impossible and inconceivable\nevent descriptions, and LM-derived string probabilities predict people's\nratings of event likelihood across modal categories. Our findings suggest that\nfine-grained knowledge about exceedingly rare events (i.e., the impossible and\ninconceivable) may be learned via statistical learning over linguistic forms,\nyet leave open the question of whether people represent the distinction between\nimpossible and inconceivable as a difference not of degree, but of kind.", "published": "2025-02-27 19:14:39", "link": "http://arxiv.org/abs/2502.20469v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explainable AI for Clinical Outcome Prediction: A Survey of Clinician\n  Perceptions and Preferences", "abstract": "Explainable AI (XAI) techniques are necessary to help clinicians make sense\nof AI predictions and integrate predictions into their decision-making\nworkflow. In this work, we conduct a survey study to understand clinician\npreference among different XAI techniques when they are used to interpret model\npredictions over text-based EHR data. We implement four XAI techniques (LIME,\nAttention-based span highlights, exemplar patient retrieval, and free-text\nrationales generated by LLMs) on an outcome prediction model that uses ICU\nadmission notes to predict a patient's likelihood of experiencing in-hospital\nmortality. Using these XAI implementations, we design and conduct a survey\nstudy of 32 practicing clinicians, collecting their feedback and preferences on\nthe four techniques. We synthesize our findings into a set of recommendations\ndescribing when each of the XAI techniques may be more appropriate, their\npotential limitations, as well as recommendations for improvement.", "published": "2025-02-27 19:30:20", "link": "http://arxiv.org/abs/2502.20478v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Protecting multimodal large language models against misleading\n  visualizations", "abstract": "We assess the vulnerability of multimodal large language models to misleading\nvisualizations - charts that distort the underlying data using techniques such\nas truncated or inverted axes, leading readers to draw inaccurate conclusions\nthat may support misinformation or conspiracy theories. Our analysis shows that\nthese distortions severely harm multimodal large language models, reducing\ntheir question-answering accuracy to the level of the random baseline. To\nmitigate this vulnerability, we introduce six inference-time methods to improve\nperformance of MLLMs on misleading visualizations while preserving their\naccuracy on non-misleading ones. The most effective approach involves (1)\nextracting the underlying data table and (2) using a text-only large language\nmodel to answer questions based on the table. This method improves performance\non misleading visualizations by 15.4 to 19.6 percentage points.", "published": "2025-02-27 20:22:34", "link": "http://arxiv.org/abs/2502.20503v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HuAMR: A Hungarian AMR Parser and Dataset", "abstract": "We present HuAMR, the first Abstract Meaning Representation (AMR) dataset and\na suite of large language model-based AMR parsers for Hungarian, targeting the\nscarcity of semantic resources for non-English languages. To create HuAMR, we\nemployed Llama-3.1-70B to automatically generate silver-standard AMR\nannotations, which we then refined manually to ensure quality. Building on this\ndataset, we investigate how different model architectures - mT5 Large and\nLlama-3.2-1B - and fine-tuning strategies affect AMR parsing performance.\n  While incorporating silver-standard AMRs from Llama-3.1-70B into the training\ndata of smaller models does not consistently boost overall scores, our results\nshow that these techniques effectively enhance parsing accuracy on Hungarian\nnews data (the domain of HuAMR). We evaluate our parsers using Smatch scores\nand confirm the potential of HuAMR and our parsers for advancing semantic\nparsing research.", "published": "2025-02-27 21:48:11", "link": "http://arxiv.org/abs/2502.20552v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Noisy Path from Source to Citation: Measuring How Scholars Engage\n  with Past Research", "abstract": "Academic citations are widely used for evaluating research and tracing\nknowledge flows. Such uses typically rely on raw citation counts and neglect\nvariability in citation types. In particular, citations can vary in their\nfidelity as original knowledge from cited studies may be paraphrased,\nsummarized, or reinterpreted, possibly wrongly, leading to variation in how\nmuch information changes from cited to citing paper. In this study, we\nintroduce a computational pipeline to quantify citation fidelity at scale.\nUsing full texts of papers, the pipeline identifies citations in citing papers\nand the corresponding claims in cited papers, and applies supervised models to\nmeasure fidelity at the sentence level. Analyzing a large-scale\nmulti-disciplinary dataset of approximately 13 million citation sentence pairs,\nwe find that citation fidelity is higher when authors cite papers that are 1)\nmore recent and intellectually close, 2) more accessible, and 3) the first\nauthor has a lower H-index and the author team is medium-sized. Using a\nquasi-experiment, we establish the \"telephone effect\" - when citing papers have\nlow fidelity to the original claim, future papers that cite the citing paper\nand the original have lower fidelity to the original. Our work reveals\nsystematic differences in citation fidelity, underscoring the limitations of\nanalyses that rely on citation quantity alone and the potential for distortion\nof evidence.", "published": "2025-02-27 22:47:03", "link": "http://arxiv.org/abs/2502.20581v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi2: Multi-Agent Test-Time Scalable Framework for Multi-Document\n  Processing", "abstract": "Recent advances in test-time scaling have shown promising results in\nimproving Large Language Models (LLMs) performance through strategic\ncomputation allocation during inference. While this approach has demonstrated\nstrong performance improvements in logical and mathematical reasoning tasks,\nits application to natural language generation (NLG), especially summarization,\nhas yet to be explored. Multi-Document Summarization (MDS) is a challenging\ntask that focuses on extracting and synthesizing useful information from\nmultiple lengthy documents. Unlike reasoning tasks, MDS requires a more nuanced\napproach to prompt design and ensemble, as there is no \"best\" prompt to satisfy\ndiverse summarization requirements. To address this, we propose a novel\nframework that leverages inference-time scaling for this task. Precisely, we\ntake prompt ensemble approach by leveraging various prompt to first generate\ncandidate summaries and then ensemble them with an aggregator to produce a\nrefined summary. We also introduce two new evaluation metrics:\nConsistency-Aware Preference (CAP) score and LLM Atom-Content-Unit (ACU) score,\nto enhance LLM's contextual understanding while mitigating its positional bias.\nExtensive experiments demonstrate the effectiveness of our approach in\nimproving summary quality while identifying and analyzing the scaling\nboundaries in summarization tasks.", "published": "2025-02-27 23:34:47", "link": "http://arxiv.org/abs/2502.20592v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-Shot, No Problem: Descriptive Continual Relation Extraction", "abstract": "Few-shot Continual Relation Extraction is a crucial challenge for enabling AI\nsystems to identify and adapt to evolving relationships in dynamic real-world\ndomains. Traditional memory-based approaches often overfit to limited samples,\nfailing to reinforce old knowledge, with the scarcity of data in few-shot\nscenarios further exacerbating these issues by hindering effective data\naugmentation in the latent space. In this paper, we propose a novel\nretrieval-based solution, starting with a large language model to generate\ndescriptions for each relation. From these descriptions, we introduce a\nbi-encoder retrieval training paradigm to enrich both sample and class\nrepresentation learning. Leveraging these enhanced representations, we design a\nretrieval-based prediction method where each sample \"retrieves\" the best\nfitting relation via a reciprocal rank fusion score that integrates both\nrelation description vectors and class prototypes. Extensive experiments on\nmultiple datasets demonstrate that our method significantly advances the\nstate-of-the-art by maintaining robust performance across sequential tasks,\neffectively addressing catastrophic forgetting.", "published": "2025-02-27 23:44:30", "link": "http://arxiv.org/abs/2502.20596v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Taxonomy, Opportunities, and Challenges of Representation Engineering\n  for Large Language Models", "abstract": "Representation Engineering (RepE) is a novel paradigm for controlling the\nbehavior of LLMs. Unlike traditional approaches that modify inputs or fine-tune\nthe model, RepE directly manipulates the model's internal representations. As a\nresult, it may offer more effective, interpretable, data-efficient, and\nflexible control over models' behavior. We present the first comprehensive\nsurvey of RepE for LLMs, reviewing the rapidly growing literature to address\nkey questions: What RepE methods exist and how do they differ? For what\nconcepts and problems has RepE been applied? What are the strengths and\nweaknesses of RepE compared to other methods? To answer these, we propose a\nunified framework describing RepE as a pipeline comprising representation\nidentification, operationalization, and control. We posit that while RepE\nmethods offer significant potential, challenges remain, including managing\nmultiple concepts, ensuring reliability, and preserving models' performance.\nTowards improving RepE, we identify opportunities for experimental and\nmethodological improvements and construct a guide for best practices.", "published": "2025-02-27 00:40:01", "link": "http://arxiv.org/abs/2502.19649v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Unlocking Multi-Modal Potentials for Dynamic Text-Attributed Graph\n  Representation", "abstract": "Dynamic Text-Attributed Graphs (DyTAGs) are a novel graph paradigm that\ncaptures evolving temporal edges alongside rich textual attributes. A prior\napproach to representing DyTAGs leverages pre-trained language models to encode\ntext attributes and subsequently integrates them into dynamic graph models.\nHowever, it follows edge-centric modeling, as in dynamic graph learning, which\nis limited in local structures and fails to exploit the unique characteristics\nof DyTAGs, leading to suboptimal performance. We observe that DyTAGs inherently\ncomprise three distinct modalities-temporal, textual, and structural-often\nexhibiting dispersed or even orthogonal distributions, with the first two\nlargely overlooked in existing research. Building on this insight, we propose\nMoMent, a model-agnostic multi-modal framework that can seamlessly integrate\nwith dynamic graph models for structural modality learning. The core idea is to\nshift from edge-centric to node-centric modeling, fully leveraging three\nmodalities for node representation. Specifically, MoMent presents non-shared\nnode-centric encoders based on the attention mechanism to capture global\ntemporal and semantic contexts from temporal and textual modalities, together\nwith local structure learning, thus generating modality-specific tokens. To\nprevent disjoint latent space, we propose a symmetric alignment loss, an\nauxiliary objective that aligns temporal and textual tokens, ensuring global\ntemporal-semantic consistency with a theoretical guarantee. Last, we design a\nlightweight adaptor to fuse these tokens, generating comprehensive and cohesive\nnode representations. We theoretically demonstrate that MoMent enhances\ndiscriminative power over exclusive edge-centric modeling. Extensive\nexperiments across seven datasets and two downstream tasks show that MoMent\nachieves up to 33.62% improvement against the baseline using four dynamic graph\nmodels.", "published": "2025-02-27 00:49:44", "link": "http://arxiv.org/abs/2502.19651v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Med-RLVR: Emerging Medical Reasoning from a 3B base model via\n  reinforcement Learning", "abstract": "Reinforcement learning from verifiable rewards (RLVR) has recently gained\nattention for its ability to elicit self-evolved reasoning capabilitie from\nbase language models without explicit reasoning supervisions, as demonstrated\nby DeepSeek-R1. While prior work on RLVR has primarily focused on mathematical\nand coding domains, its applicability to other tasks and domains remains\nunexplored. In this work, we investigate whether medical reasoning can emerge\nfrom RLVR. We introduce Med-RLVR as an initial study of RLVR in the medical\ndomain leveraging medical multiple-choice question answering (MCQA) data as\nverifiable labels. Our results demonstrate that RLVR is not only effective for\nmath and coding but also extends successfully to medical question answering.\nNotably, Med-RLVR achieves performance comparable to traditional supervised\nfine-tuning (SFT) on in-distribution tasks while significantly improving\nout-of-distribution generalization, with an 8-point accuracy gain. Further\nanalysis of training dynamics reveals that, with no explicit reasoning\nsupervision, reasoning emerges from the 3B-parameter base model. These findings\nunderscore the potential of RLVR in domains beyond math and coding, opening new\navenues for its application in knowledge-intensive fields such as medicine.", "published": "2025-02-27 00:54:38", "link": "http://arxiv.org/abs/2502.19655v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Future Outcome Reasoning and Confidence Assessment Benchmark", "abstract": "Forecasting is an important task in many domains, such as technology and\neconomics. However existing forecasting benchmarks largely lack comprehensive\nconfidence assessment, focus on limited question types, and often consist of\nartificial questions that do not align with real-world human forecasting needs.\nTo address these gaps, we introduce FOReCAst (Future Outcome Reasoning and\nConfidence Assessment), a benchmark that evaluates models' ability to make\npredictions and their confidence in them. FOReCAst spans diverse forecasting\nscenarios involving Boolean questions, timeframe prediction, and quantity\nestimation, enabling a comprehensive evaluation of both prediction accuracy and\nconfidence calibration for real-world applications.", "published": "2025-02-27 01:36:00", "link": "http://arxiv.org/abs/2502.19676v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Sensing and Steering Stereotypes: Extracting and Applying Gender\n  Representation Vectors in LLMs", "abstract": "Large language models (LLMs) are known to perpetuate stereotypes and exhibit\nbiases. Various strategies have been proposed to mitigate potential harms that\nmay result from these biases, but most work studies biases in LLMs as a\nblack-box problem without considering how concepts are represented within the\nmodel. We adapt techniques from representation engineering to study how the\nconcept of \"gender\" is represented within LLMs. We introduce a new method that\nextracts concept representations via probability weighting without labeled data\nand efficiently selects a steering vector for measuring and manipulating the\nmodel's representation. We also present a projection-based method that enables\nprecise steering of model predictions and demonstrate its effectiveness in\nmitigating gender bias in LLMs.", "published": "2025-02-27 03:24:09", "link": "http://arxiv.org/abs/2502.19721v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Few-Shot Multilingual Open-Domain QA from 5 Examples", "abstract": "Recent approaches to multilingual open-domain question answering (MLODQA)\nhave achieved promising results given abundant language-specific training data.\nHowever, the considerable annotation cost limits the application of these\nmethods for underrepresented languages. We introduce a \\emph{few-shot learning}\napproach to synthesise large-scale multilingual data from large language models\n(LLMs). Our method begins with large-scale self-supervised pre-training using\nWikiData, followed by training on high-quality synthetic multilingual data\ngenerated by prompting LLMs with few-shot supervision. The final model,\n\\textsc{FsModQA}, significantly outperforms existing few-shot and supervised\nbaselines in MLODQA and cross-lingual and monolingual retrieval. We further\nshow our method can be extended for effective zero-shot adaptation to new\nlanguages through a \\emph{cross-lingual prompting} strategy with only\nEnglish-supervised data, making it a general and applicable solution for MLODQA\ntasks without costly large-scale annotation.", "published": "2025-02-27 03:24:57", "link": "http://arxiv.org/abs/2502.19722v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "CNsum:Automatic Summarization for Chinese News Text", "abstract": "Obtaining valuable information from massive data efficiently has become our\nresearch goal in the era of Big Data. Text summarization technology has been\ncontinuously developed to meet this demand. Recent work has also shown that\ntransformer-based pre-trained language models have achieved great success on\nvarious tasks in Natural Language Processing (NLP). Aiming at the problem of\nChinese news text summary generation and the application of Transformer\nstructure on Chinese, this paper proposes a Chinese news text summarization\nmodel (CNsum) based on Transformer structure, and tests it on Chinese datasets\nsuch as THUCNews. The results of the conducted experiments show that CNsum\nachieves better ROUGE score than the baseline models, which verifies the\noutperformance of the model.", "published": "2025-02-27 03:25:34", "link": "http://arxiv.org/abs/2502.19723v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tokens for Learning, Tokens for Unlearning: Mitigating Membership\n  Inference Attacks in Large Language Models via Dual-Purpose Training", "abstract": "Large language models (LLMs) have become the backbone of modern natural\nlanguage processing but pose privacy concerns about leaking sensitive training\ndata. Membership inference attacks (MIAs), which aim to infer whether a sample\nis included in a model's training dataset, can serve as a foundation for\nbroader privacy threats. Existing defenses designed for traditional\nclassification models do not account for the sequential nature of text data. As\na result, they either require significant computational resources or fail to\neffectively mitigate privacy risks in LLMs. In this work, we propose a\nlightweight yet effective empirical privacy defense for protecting training\ndata of language modeling by leveraging the token-specific characteristics. By\nanalyzing token dynamics during training, we propose a token selection strategy\nthat categorizes tokens into hard tokens for learning and memorized tokens for\nunlearning. Subsequently, our training-phase defense optimizes a novel\ndual-purpose token-level loss to achieve a Pareto-optimal balance between\nutility and privacy. Extensive experiments demonstrate that our approach not\nonly provides strong protection against MIAs but also improves language\nmodeling performance by around 10\\% across various LLM architectures and\ndatasets compared to the baselines.", "published": "2025-02-27 03:37:45", "link": "http://arxiv.org/abs/2502.19726v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "HaLoRA: Hardware-aware Low-Rank Adaptation for Large Language Models\n  Based on Hybrid Compute-in-Memory Architecture", "abstract": "Low-rank adaptation (LoRA) is a predominant parameter-efficient finetuning\nmethod to adapt large language models (LLMs) for downstream tasks. In this\npaper, we first propose to deploy the LoRA-finetuned LLMs on the hybrid\ncompute-in-memory (CIM) architecture (i.e., pretrained weights onto RRAM and\nLoRA onto SRAM). To address performance degradation from RRAM's inherent noise,\nwe design a novel Hardware-aware Low-rank Adaption (HaLoRA) method, aiming to\ntrain a LoRA branch that is both robust and accurate by aligning the training\nobjectives under both ideal and noisy conditions. Experiments finetuning LLaMA\n3.2 1B and 3B demonstrate HaLoRA's effectiveness across multiple reasoning\ntasks, achieving up to 22.7 improvement in average score while maintaining\nrobustness at various noise levels.", "published": "2025-02-27 04:20:47", "link": "http://arxiv.org/abs/2502.19747v2", "categories": ["cs.CL", "cs.AR"], "primary_category": "cs.CL"}
{"title": "PolyPrompt: Automating Knowledge Extraction from Multilingual Language\n  Models with Dynamic Prompt Generation", "abstract": "Large language models (LLMs) showcase increasingly impressive English\nbenchmark scores, however their performance profiles remain inconsistent across\nmultilingual settings. To address this gap, we introduce PolyPrompt, a novel,\nparameter-efficient framework for enhancing the multilingual capabilities of\nLLMs. Our method learns a set of trigger tokens for each language through a\ngradient-based search, identifying the input query's language and selecting the\ncorresponding trigger tokens which are prepended to the prompt during\ninference. We perform experiments on two ~1 billion parameter models, with\nevaluations on the global MMLU benchmark across fifteen typologically and\nresource diverse languages, demonstrating accuracy gains of 3.7%-19.9% compared\nto naive and translation-pipeline baselines.", "published": "2025-02-27 04:41:22", "link": "http://arxiv.org/abs/2502.19756v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EdiText: Controllable Coarse-to-Fine Text Editing with Diffusion\n  Language Models", "abstract": "We propose EdiText, a controllable text editing method that modify the\nreference text to desired attributes at various scales. We integrate an\nSDEdit-based editing technique that allows for broad adjustments in the degree\nof text editing. Additionally, we introduce a novel fine-level editing method\nbased on self-conditioning, which allows subtle control of reference text.\nWhile being capable of editing on its own, this fine-grained method, integrated\nwith the SDEdit approach, enables EdiText to make precise adjustments within\nthe desired range. EdiText demonstrates its controllability to robustly adjust\nreference text at broad range of levels across various tasks, including\ntoxicity control and sentiment control.", "published": "2025-02-27 05:04:33", "link": "http://arxiv.org/abs/2502.19765v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NaijaNLP: A Survey of Nigerian Low-Resource Languages", "abstract": "With over 500 languages in Nigeria, three languages -- Hausa, Yor\\`ub\\'a and\nIgbo -- spoken by over 175 million people, account for about 60% of the spoken\nlanguages. However, these languages are categorised as low-resource due to\ninsufficient resources to support tasks in computational linguistics. Several\nresearch efforts and initiatives have been presented, however, a coherent\nunderstanding of the state of Natural Language Processing (NLP) - from\ngrammatical formalisation to linguistic resources that support complex tasks\nsuch as language understanding and generation is lacking. This study presents\nthe first comprehensive review of advancements in low-resource NLP (LR-NLP)\nresearch across the three major Nigerian languages (NaijaNLP). We\nquantitatively assess the available linguistic resources and identify key\nchallenges. Although a growing body of literature addresses various NLP\ndownstream tasks in Hausa, Igbo, and Yor\\`ub\\'a, only about 25.1% of the\nreviewed studies contribute new linguistic resources. This finding highlights a\npersistent reliance on repurposing existing data rather than generating novel,\nhigh-quality resources. Additionally, language-specific challenges, such as the\naccurate representation of diacritics, remain under-explored. To advance\nNaijaNLP and LR-NLP more broadly, we emphasise the need for intensified efforts\nin resource enrichment, comprehensive annotation, and the development of open\ncollaborative initiatives.", "published": "2025-02-27 05:48:51", "link": "http://arxiv.org/abs/2502.19784v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Text classification using machine learning methods", "abstract": "In this paper we present the results of an experiment aimed to use machine\nlearning methods to obtain models that can be used for the automatic\nclassification of products. In order to apply automatic classification methods,\nwe transformed the product names from a text representation to numeric vectors,\na process called word embedding. We used several embedding methods: Count\nVectorization, TF-IDF, Word2Vec, FASTTEXT, and GloVe. Having the product names\nin a form of numeric vectors, we proceeded with a set of machine learning\nmethods for automatic classification: Logistic Regression, Multinomial Naive\nBayes, kNN, Artificial Neural Networks, Support Vector Machines, and Decision\ntrees with several variants. The results show an impressive accuracy of the\nclassification process for Support Vector Machines, Logistic Regression, and\nRandom Forests. Regarding the word embedding methods, the best results were\nobtained with the FASTTEXT technique.", "published": "2025-02-27 06:20:38", "link": "http://arxiv.org/abs/2502.19801v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Foot-In-The-Door: A Multi-turn Jailbreak for LLMs", "abstract": "Ensuring AI safety is crucial as large language models become increasingly\nintegrated into real-world applications. A key challenge is jailbreak, where\nadversarial prompts bypass built-in safeguards to elicit harmful disallowed\noutputs. Inspired by psychological foot-in-the-door principles, we introduce\nFITD,a novel multi-turn jailbreak method that leverages the phenomenon where\nminor initial commitments lower resistance to more significant or more\nunethical transgressions. Our approach progressively escalates the malicious\nintent of user queries through intermediate bridge prompts and aligns the\nmodel's response by itself to induce toxic responses. Extensive experimental\nresults on two jailbreak benchmarks demonstrate that FITD achieves an average\nattack success rate of 94% across seven widely used models, outperforming\nexisting state-of-the-art methods. Additionally, we provide an in-depth\nanalysis of LLM self-corruption, highlighting vulnerabilities in current\nalignment strategies and emphasizing the risks inherent in multi-turn\ninteractions. The code is available at\nhttps://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak.", "published": "2025-02-27 06:49:16", "link": "http://arxiv.org/abs/2502.19820v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Revisiting Self-Consistency from Dynamic Distributional Alignment\n  Perspective on Answer Aggregation", "abstract": "Self-consistency improves reasoning by aggregating diverse stochastic\nsamples, yet the dynamics behind its efficacy remain underexplored. We reframe\nself-consistency as a dynamic distributional alignment problem, revealing that\ndecoding temperature not only governs sampling randomness but also actively\nshapes the latent answer distribution. Given that high temperatures require\nprohibitively large sample sizes to stabilize, while low temperatures risk\namplifying biases, we propose a confidence-driven mechanism that dynamically\ncalibrates temperature: sharpening the sampling distribution under uncertainty\nto align with high-probability modes, and promoting exploration when confidence\nis high. Experiments on mathematical reasoning tasks show this approach\noutperforms fixed-diversity baselines under limited samples, improving both\naverage and best-case performance across varying initial temperatures without\nadditional data or modules. This establishes self-consistency as a\nsynchronization challenge between sampling dynamics and evolving answer\ndistributions.", "published": "2025-02-27 07:07:40", "link": "http://arxiv.org/abs/2502.19830v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MIND: Towards Immersive Psychological Healing with Multi-agent Inner\n  Dialogue", "abstract": "Mental health issues are worsening in today's competitive society, such as\ndepression and anxiety. Traditional healings like counseling and chatbots fail\nto engage effectively, they often provide generic responses lacking emotional\ndepth. Although large language models (LLMs) have the potential to create more\nhuman-like interactions, they still struggle to capture subtle emotions. This\nrequires LLMs to be equipped with human-like adaptability and warmth. To fill\nthis gap, we propose the MIND (Multi-agent INner Dialogue), a novel paradigm\nthat provides more immersive psychological healing environments. Considering\nthe strong generative and role-playing ability of LLM agents, we predefine an\ninteractive healing framework and assign LLM agents different roles within the\nframework to engage in interactive inner dialogues with users, thereby\nproviding an immersive healing experience. We conduct extensive human\nexperiments in various real-world healing dimensions, and find that MIND\nprovides a more user-friendly experience than traditional paradigms. This\ndemonstrates that MIND effectively leverages the significant potential of LLMs\nin psychological healing.", "published": "2025-02-27 08:04:27", "link": "http://arxiv.org/abs/2502.19860v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Order Doesn't Matter, But Reasoning Does: Training LLMs with\n  Order-Centric Augmentation", "abstract": "Logical reasoning is essential for large language models (LLMs) to ensure\naccurate and coherent inference. However, LLMs struggle with reasoning order\nvariations and fail to generalize across logically equivalent transformations.\nLLMs often rely on fixed sequential patterns rather than true logical\nunderstanding. To address this issue, we introduce an order-centric data\naugmentation framework based on commutativity in logical reasoning. We first\nrandomly shuffle independent premises to introduce condition order\naugmentation. For reasoning steps, we construct a directed acyclic graph (DAG)\nto model dependencies between steps, which allows us to identify valid\nreorderings of steps while preserving logical correctness. By leveraging\norder-centric augmentations, models can develop a more flexible and generalized\nreasoning process. Finally, we conduct extensive experiments across multiple\nlogical reasoning benchmarks, demonstrating that our method significantly\nenhances LLMs' reasoning performance and adaptability to diverse logical\nstructures. We release our codes and augmented data in\nhttps://anonymous.4open.science/r/Order-Centric-Data-Augmentation-822C/.", "published": "2025-02-27 09:25:50", "link": "http://arxiv.org/abs/2502.19907v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Collaborative Stance Detection via Small-Large Language Model\n  Consistency Verification", "abstract": "Stance detection on social media aims to identify attitudes expressed in\ntweets towards specific targets. Current studies prioritize Large Language\nModels (LLMs) over Small Language Models (SLMs) due to the overwhelming\nperformance improving provided by LLMs. However, heavily relying on LLMs for\nstance detection, regardless of the cost, is impractical for real-world social\nmedia monitoring systems that require vast data analysis. To this end, we\npropose \\textbf{\\underline{Co}}llaborative Stance Detection via Small-Large\nLanguage Model Consistency \\textbf{\\underline{Ver}}ification (\\textbf{CoVer})\nframework, which enhances LLM utilization via context-shared batch reasoning\nand logical verification between LLM and SLM. Specifically, instead of\nprocessing each text individually, CoVer processes texts batch-by-batch,\nobtaining stance predictions and corresponding explanations via LLM reasoning\nin a shared context. Then, to exclude the bias caused by context noises, CoVer\nintroduces the SLM for logical consistency verification. Finally, texts that\nrepeatedly exhibit low logical consistency are classified using\nconsistency-weighted aggregation of prior LLM stance predictions. Our\nexperiments show that CoVer outperforms state-of-the-art methods across\nmultiple benchmarks in the zero-shot setting, achieving 0.54 LLM queries per\ntweet while significantly enhancing performance. Our CoVer offers a more\npractical solution for LLM deploying for social media stance detection.", "published": "2025-02-27 10:30:50", "link": "http://arxiv.org/abs/2502.19954v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deterministic or probabilistic? The psychology of LLMs as random number\n  generators", "abstract": "Large Language Models (LLMs) have transformed text generation through\ninherently probabilistic context-aware mechanisms, mimicking human natural\nlanguage. In this paper, we systematically investigate the performance of\nvarious LLMs when generating random numbers, considering diverse configurations\nsuch as different model architectures, numerical ranges, temperature, and\nprompt languages. Our results reveal that, despite their stochastic\ntransformers-based architecture, these models often exhibit deterministic\nresponses when prompted for random numerical outputs. In particular, we find\nsignificant differences when changing the model, as well as the prompt\nlanguage, attributing this phenomenon to biases deeply embedded within the\ntraining data. Models such as DeepSeek-R1 can shed some light on the internal\nreasoning process of LLMs, despite arriving to similar results. These biases\ninduce predictable patterns that undermine genuine randomness, as LLMs are\nnothing but reproducing our own human cognitive biases.", "published": "2025-02-27 10:45:27", "link": "http://arxiv.org/abs/2502.19965v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Erasing Without Remembering: Safeguarding Knowledge Forgetting in Large\n  Language Models", "abstract": "In this paper, we explore machine unlearning from a novel dimension, by\nstudying how to safeguard model unlearning in large language models (LLMs). Our\ngoal is to prevent unlearned models from recalling any related memory of the\ntargeted knowledge.We begin by uncovering a surprisingly simple yet overlooked\nfact: existing methods typically erase only the exact expressions of the\ntargeted knowledge, leaving paraphrased or related information intact. To\nrigorously measure such oversights, we introduce UGBench, the first benchmark\ntailored for evaluating the generalisation performance across 13\nstate-of-the-art methods.UGBench reveals that unlearned models can still recall\nparaphrased answers and retain target facts in intermediate layers. To address\nthis, we propose PERMU, a perturbation-based method that significantly enhances\nthe generalisation capabilities for safeguarding LLM unlearning.Experiments\ndemonstrate that PERMU delivers up to a 50.13% improvement in unlearning while\nmaintaining a 43.53% boost in robust generalisation. Our code can be found in\nhttps://github.com/MaybeLizzy/UGBench.", "published": "2025-02-27 11:03:33", "link": "http://arxiv.org/abs/2502.19982v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Vision-Encoders (Already) Know What They See: Mitigating Object\n  Hallucination via Simple Fine-Grained CLIPScore", "abstract": "Recently, Large Vision-Language Models (LVLMs) show remarkable performance\nacross various domains. However, these models suffer from object hallucination.\nThis study revisits the previous claim that the primary cause of such\nhallucination lies in the limited representational capacity of the vision\nencoder. Our analysis reveals that the capacity of the vision encoder itself is\nalready enough for detecting object hallucination. Based on this insight, we\npropose a Fine-grained CLIPScore (F-CLIPScore), a simple yet effective\nevaluation metric that enhances object-level granularity by incorporating text\nembeddings at the noun phrase level. Evaluations on the OHD-Caps benchmark show\nthat F-CLIPScore significantly outperforms conventional CLIPScore in accuracy\nby a large margin of 39.6% without additional training. We further validate\nF-CLIPScore by showing that LVLM trained with the data filtered using\nF-CLIPScore exhibits reduced hallucination.", "published": "2025-02-27 12:20:02", "link": "http://arxiv.org/abs/2502.20034v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Polish-ASTE: Aspect-Sentiment Triplet Extraction Datasets for Polish", "abstract": "Aspect-Sentiment Triplet Extraction (ASTE) is one of the most challenging and\ncomplex tasks in sentiment analysis. It concerns the construction of triplets\nthat contain an aspect, its associated sentiment polarity, and an opinion\nphrase that serves as a rationale for the assigned polarity. Despite the\ngrowing popularity of the task and the many machine learning methods being\nproposed to address it, the number of datasets for ASTE is very limited. In\nparticular, no dataset is available for any of the Slavic languages. In this\npaper, we present two new datasets for ASTE containing customer opinions about\nhotels and purchased products expressed in Polish. We also perform experiments\nwith two ASTE techniques combined with two large language models for Polish to\ninvestigate their performance and the difficulty of the assembled datasets. The\nnew datasets are available under a permissive licence and have the same file\nformat as the English datasets, facilitating their use in future research.", "published": "2025-02-27 12:38:04", "link": "http://arxiv.org/abs/2502.20046v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Finite State Automata Inside Transformers with Chain-of-Thought: A\n  Mechanistic Study on State Tracking", "abstract": "Chain-of-Thought (CoT) significantly enhances the performance of large\nlanguage models (LLMs) across a wide range of tasks, and prior research shows\nthat CoT can theoretically increase expressiveness. However, there is limited\nmechanistic understanding of the algorithms that Transformer+CoT can learn. In\nthis work, we (1) evaluate the state tracking capabilities of Transformer+CoT\nand its variants, confirming the effectiveness of CoT. (2) Next, we identify\nthe circuit, a subset of model components, responsible for tracking the world\nstate, finding that late-layer MLP neurons play a key role. We propose two\nmetrics, compression and distinction, and show that the neuron sets for each\nstate achieve nearly 100% accuracy, providing evidence of an implicit finite\nstate automaton (FSA) embedded within the model. (3) Additionally, we explore\nthree realistic settings: skipping intermediate steps, introducing data noise,\nand testing length generalization. Our results demonstrate that Transformer+CoT\nlearns robust algorithms (FSA), highlighting its resilience in challenging\nscenarios.", "published": "2025-02-27 14:24:51", "link": "http://arxiv.org/abs/2502.20129v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Telephone Surveys Meet Conversational AI: Evaluating a LLM-Based\n  Telephone Survey System at Scale", "abstract": "Telephone surveys remain a valuable tool for gathering insights but typically\nrequire substantial resources in training and coordinating human interviewers.\nThis work presents an AI-driven telephone survey system integrating\ntext-to-speech (TTS), a large language model (LLM), and speech-to-text (STT)\nthat mimics the versatility of human-led interviews (full-duplex dialogues) at\nscale.\n  We tested the system across two populations, a pilot study in the United\nStates (n = 75) and a large-scale deployment in Peru (n = 2,739), inviting\nparticipants via web-based links and contacting them via direct phone calls.\nThe AI agent successfully administered open-ended and closed-ended questions,\nhandled basic clarifications, and dynamically navigated branching logic,\nallowing fast large-scale survey deployment without interviewer recruitment or\ntraining.\n  Our findings demonstrate that while the AI system's probing for qualitative\ndepth was more limited than human interviewers, overall data quality approached\nhuman-led standards for structured items. This study represents one of the\nfirst successful large-scale deployments of an LLM-based telephone interviewer\nin a real-world survey context. The AI-powered telephone survey system has the\npotential for expanding scalable, consistent data collecting across market\nresearch, social science, and public opinion studies, thus improving\noperational efficiency while maintaining appropriate data quality for research.", "published": "2025-02-27 14:31:42", "link": "http://arxiv.org/abs/2502.20140v2", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Similarity-Distance-Magnitude Universal Verification", "abstract": "We address the neural network robustness problem by adding Similarity (i.e.,\ncorrectly predicted depth-matches into training)-awareness and\nDistance-to-training-distribution-awareness to the existing output Magnitude\n(i.e., decision-boundary)-awareness of the softmax function. The resulting sdm\nactivation function provides strong signals of the relative epistemic\n(reducible) predictive uncertainty. We use this novel behavior to further\naddress the complementary HCI problem of mapping the output to\nhuman-interpretable summary statistics over relevant partitions of a held-out\ncalibration set. Estimates of prediction-conditional uncertainty are obtained\nvia a parsimonious learned transform over the class-conditional empirical CDFs\nof the output of a final-layer sdm activation function. For decision-making and\nas an intrinsic model check, estimates of class-conditional accuracy are\nobtained by further partitioning the high-probability regions of this\ncalibrated output into class-conditional, region-specific CDFs. The uncertainty\nestimates from sdm calibration are remarkably robust to test-time distribution\nshifts and out-of-distribution inputs; incorporate awareness of the effective\nsample size; provide estimates of uncertainty from the learning and data\nsplitting processes; and are well-suited for selective classification and\nconditional branching for additional test-time compute based on the predictive\nuncertainty, as for selective LLM generation, routing, and composition over\nmultiple models and retrieval. Finally, we construct sdm networks, LLMs with\nuncertainty-aware verification and interpretability-by-exemplar as intrinsic\nproperties. We provide open-source software implementing these results.", "published": "2025-02-27 15:05:00", "link": "http://arxiv.org/abs/2502.20167v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Representing Signs as Signs: One-Shot ISLR to Facilitate Functional Sign\n  Language Technologies", "abstract": "Isolated Sign Language Recognition (ISLR) is crucial for scalable sign\nlanguage technology, yet language-specific approaches limit current models. To\naddress this, we propose a one-shot learning approach that generalises across\nlanguages and evolving vocabularies. Our method involves pretraining a model to\nembed signs based on essential features and using a dense vector search for\nrapid, accurate recognition of unseen signs. We achieve state-of-the-art\nresults, including 50.8% one-shot MRR on a large dictionary containing 10,235\nunique signs from a different language than the training set. Our approach is\nrobust across languages and support sets, offering a scalable, adaptable\nsolution for ISLR. Co-created with the Deaf and Hard of Hearing (DHH)\ncommunity, this method aligns with real-world needs, and advances scalable sign\nlanguage recognition.", "published": "2025-02-27 15:07:51", "link": "http://arxiv.org/abs/2502.20171v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Multimodal Representation Alignment for Image Generation: Text-Image\n  Interleaved Control Is Easier Than You Think", "abstract": "The field of advanced text-to-image generation is witnessing the emergence of\nunified frameworks that integrate powerful text encoders, such as CLIP and T5,\nwith Diffusion Transformer backbones. Although there have been efforts to\ncontrol output images with additional conditions, like canny and depth map, a\ncomprehensive framework for arbitrary text-image interleaved control is still\nlacking. This gap is especially evident when attempting to merge concepts or\nvisual elements from multiple images in the generation process. To mitigate the\ngap, we conducted preliminary experiments showing that large multimodal models\n(LMMs) offer an effective shared representation space, where image and text can\nbe well-aligned to serve as a condition for external diffusion models. Based on\nthis discovery, we propose Dream Engine, an efficient and unified framework\ndesigned for arbitrary text-image interleaved control in image generation\nmodels. Building on powerful text-to-image models like SD3.5, we replace the\noriginal text-only encoders by incorporating versatile multimodal information\nencoders such as QwenVL. Our approach utilizes a two-stage training paradigm,\nconsisting of joint text-image alignment and multimodal interleaved instruction\ntuning. Our experiments demonstrate that this training method is effective,\nachieving a 0.69 overall score on the GenEval benchmark, and matching the\nperformance of state-of-the-art text-to-image models like SD3.5 and FLUX.", "published": "2025-02-27 15:08:39", "link": "http://arxiv.org/abs/2502.20172v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "An Extensive Evaluation of PDDL Capabilities in off-the-shelf LLMs", "abstract": "In recent advancements, large language models (LLMs) have exhibited\nproficiency in code generation and chain-of-thought reasoning, laying the\ngroundwork for tackling automatic formal planning tasks. This study evaluates\nthe potential of LLMs to understand and generate Planning Domain Definition\nLanguage (PDDL), an essential representation in artificial intelligence\nplanning. We conduct an extensive analysis across 20 distinct models spanning 7\nmajor LLM families, both commercial and open-source. Our comprehensive\nevaluation sheds light on the zero-shot LLM capabilities of parsing,\ngenerating, and reasoning with PDDL. Our findings indicate that while some\nmodels demonstrate notable effectiveness in handling PDDL, others pose\nlimitations in more complex scenarios requiring nuanced planning knowledge.\nThese results highlight the promise and current limitations of LLMs in formal\nplanning tasks, offering insights into their application and guiding future\nefforts in AI-driven planning paradigms.", "published": "2025-02-27 15:13:07", "link": "http://arxiv.org/abs/2502.20175v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Layer-Aware Task Arithmetic: Disentangling Task-Specific and\n  Instruction-Following Knowledge", "abstract": "Large language models (LLMs) demonstrate strong task-specific capabilities\nthrough fine-tuning, but merging multiple fine-tuned models often leads to\ndegraded performance due to overlapping instruction-following components. Task\nArithmetic (TA), which combines task vectors derived from fine-tuning, enables\nmulti-task learning and task forgetting but struggles to isolate task-specific\nknowledge from general instruction-following behavior. To address this, we\npropose Layer-Aware Task Arithmetic (LATA), a novel approach that assigns\nlayer-specific weights to task vectors based on their alignment with\ninstruction-following or task-specific components. By amplifying task-relevant\nlayers and attenuating instruction-following layers, LATA improves task\nlearning and forgetting performance while preserving overall model utility.\nExperiments on multiple benchmarks, including WikiText-2, GSM8K, and HumanEval,\ndemonstrate that LATA outperforms existing methods in both multi-task learning\nand selective task forgetting, achieving higher task accuracy and alignment\nwith minimal degradation in output quality. Our findings highlight the\nimportance of layer-wise analysis in disentangling task-specific and\ngeneral-purpose knowledge, offering a robust framework for efficient model\nmerging and editing.", "published": "2025-02-27 15:22:14", "link": "http://arxiv.org/abs/2502.20186v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Granite Embedding Models", "abstract": "We introduce the Granite Embedding models, a family of encoder-based\nembedding models designed for retrieval tasks, spanning dense-retrieval and\nsparse retrieval architectures, with both English and Multilingual\ncapabilities. This report provides the technical details of training these\nhighly effective 12 layer embedding models, along with their efficient 6 layer\ndistilled counterparts. Extensive evaluations show that the models, developed\nwith techniques like retrieval oriented pretraining, contrastive finetuning,\nknowledge distillation, and model merging significantly outperform publicly\navailable models of similar sizes on both internal IBM retrieval and search\ntasks, and have equivalent performance on widely used information retrieval\nbenchmarks, while being trained on high-quality data suitable for enterprise\nuse. We publicly release all our Granite Embedding models under the Apache 2.0\nlicense, allowing both research and commercial use at\nhttps://huggingface.co/collections/ibm-granite.", "published": "2025-02-27 15:45:16", "link": "http://arxiv.org/abs/2502.20204v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "LLM as a Broken Telephone: Iterative Generation Distorts Information", "abstract": "As large language models are increasingly responsible for online content,\nconcerns arise about the impact of repeatedly processing their own outputs.\nInspired by the \"broken telephone\" effect in chained human communication, this\nstudy investigates whether LLMs similarly distort information through iterative\ngeneration. Through translation-based experiments, we find that distortion\naccumulates over time, influenced by language choice and chain complexity.\nWhile degradation is inevitable, it can be mitigated through strategic\nprompting techniques. These findings contribute to discussions on the long-term\neffects of AI-mediated information propagation, raising important questions\nabout the reliability of LLM-generated content in iterative workflows.", "published": "2025-02-27 16:46:23", "link": "http://arxiv.org/abs/2502.20258v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Much is Enough? The Diminishing Returns of Tokenization Training\n  Data", "abstract": "Tokenization, a crucial initial step in natural language processing, is often\nassumed to benefit from larger training datasets. This paper investigates the\nimpact of tokenizer training data sizes ranging from 1GB to 900GB. Our findings\nreveal diminishing returns as the data size increases, highlighting a practical\nlimit on how much further scaling the training data can improve tokenization\nquality. We analyze this phenomenon and attribute the saturation effect to the\nconstraints imposed by the pre-tokenization stage of tokenization. These\nresults offer valuable insights for optimizing the tokenization process and\nhighlight potential avenues for future research in tokenization algorithms.", "published": "2025-02-27 17:01:23", "link": "http://arxiv.org/abs/2502.20273v1", "categories": ["cs.CL", "cs.CE"], "primary_category": "cs.CL"}
{"title": "Emergent Symbolic Mechanisms Support Abstract Reasoning in Large\n  Language Models", "abstract": "Many recent studies have found evidence for emergent reasoning capabilities\nin large language models, but debate persists concerning the robustness of\nthese capabilities, and the extent to which they depend on structured reasoning\nmechanisms. To shed light on these issues, we perform a comprehensive study of\nthe internal mechanisms that support abstract rule induction in an open-source\nlanguage model (Llama3-70B). We identify an emergent symbolic architecture that\nimplements abstract reasoning via a series of three computations. In early\nlayers, symbol abstraction heads convert input tokens to abstract variables\nbased on the relations between those tokens. In intermediate layers, symbolic\ninduction heads perform sequence induction over these abstract variables.\nFinally, in later layers, retrieval heads predict the next token by retrieving\nthe value associated with the predicted abstract variable. These results point\ntoward a resolution of the longstanding debate between symbolic and neural\nnetwork approaches, suggesting that emergent reasoning in neural networks\ndepends on the emergence of symbolic mechanisms.", "published": "2025-02-27 18:02:15", "link": "http://arxiv.org/abs/2502.20332v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Expertise Is What We Want", "abstract": "Clinical decision-making depends on expert reasoning, which is guided by\nstandardized, evidence-based guidelines. However, translating these guidelines\ninto automated clinical decision support systems risks inaccuracy and\nimportantly, loss of nuance. We share an application architecture, the Large\nLanguage Expert (LLE), that combines the flexibility and power of Large\nLanguage Models (LLMs) with the interpretability, explainability, and\nreliability of Expert Systems. LLMs help address key challenges of Expert\nSystems, such as integrating and codifying knowledge, and data normalization.\nConversely, an Expert System-like approach helps overcome challenges with LLMs,\nincluding hallucinations, atomic and inexpensive updates, and testability.\n  To highlight the power of the Large Language Expert (LLE) system, we built an\nLLE to assist with the workup of patients newly diagnosed with cancer. Timely\ninitiation of cancer treatment is critical for optimal patient outcomes.\nHowever, increasing complexity in diagnostic recommendations has made it\ndifficult for primary care physicians to ensure their patients have completed\nthe necessary workup before their first visit with an oncologist. As with many\nreal-world clinical tasks, these workups require the analysis of unstructured\nhealth records and the application of nuanced clinical decision logic. In this\nstudy, we describe the design & evaluation of an LLE system built to rapidly\nidentify and suggest the correct diagnostic workup. The system demonstrated a\nhigh degree of clinical-level accuracy (>95%) and effectively addressed gaps\nidentified in real-world data from breast and colon cancer patients at a large\nacademic center.", "published": "2025-02-27 18:05:15", "link": "http://arxiv.org/abs/2502.20335v1", "categories": ["cs.CL", "cs.AI", "I.2.7; J.3"], "primary_category": "cs.CL"}
{"title": "Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners", "abstract": "Recent advancements have demonstrated that the performance of large language\nmodels (LLMs) can be significantly enhanced by scaling computational resources\nat test time. A common strategy involves generating multiple Chain-of-Thought\n(CoT) trajectories and aggregating their outputs through various selection\nmechanisms. This raises a fundamental question: can models with lower\ncomplexity leverage their superior generation throughput to outperform\nsimilarly sized Transformers for a fixed computational budget? To address this\nquestion and overcome the lack of strong subquadratic reasoners, we distill\npure and hybrid Mamba models from pretrained Transformers. Trained on only 8\nbillion tokens, our distilled models show strong performance and scaling on\nmathematical reasoning datasets while being much faster at inference for large\nbatches and long sequences. Despite the zero-shot performance hit due to\ndistillation, both pure and hybrid Mamba models can scale their coverage and\naccuracy performance past their Transformer teacher models under fixed time\nbudgets, opening a new direction for scaling inference compute.", "published": "2025-02-27 18:08:16", "link": "http://arxiv.org/abs/2502.20339v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Responsible AI in Education: Hybrid Recommendation System for\n  K-12 Students Case Study", "abstract": "The growth of Educational Technology (EdTech) has enabled highly personalized\nlearning experiences through Artificial Intelligence (AI)-based recommendation\nsystems tailored to each student needs. However, these systems can\nunintentionally introduce biases, potentially limiting fair access to learning\nresources. This study presents a recommendation system for K-12 students,\ncombining graph-based modeling and matrix factorization to provide personalized\nsuggestions for extracurricular activities, learning resources, and\nvolunteering opportunities. To address fairness concerns, the system includes a\nframework to detect and reduce biases by analyzing feedback across protected\nstudent groups. This work highlights the need for continuous monitoring in\neducational recommendation systems to support equitable, transparent, and\neffective learning opportunities for all students.", "published": "2025-02-27 18:27:30", "link": "http://arxiv.org/abs/2502.20354v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with\n  Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix\n  Factorization", "abstract": "Agentic Generative AI, powered by Large Language Models (LLMs) with\nRetrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores\n(VSs), represents a transformative technology applicable to specialized domains\nsuch as legal systems, research, recommender systems, cybersecurity, and global\nsecurity, including proliferation research. This technology excels at inferring\nrelationships within vast unstructured or semi-structured datasets. The legal\ndomain here comprises complex data characterized by extensive, interrelated,\nand semi-structured knowledge systems with complex relations. It comprises\nconstitutions, statutes, regulations, and case law. Extracting insights and\nnavigating the intricate networks of legal documents and their relations is\ncrucial for effective legal research. Here, we introduce a generative AI system\nthat integrates RAG, VS, and KG, constructed via Non-Negative Matrix\nFactorization (NMF), to enhance legal information retrieval and AI reasoning\nand minimize hallucinations. In the legal system, these technologies empower AI\nagents to identify and analyze complex connections among cases, statutes, and\nlegal precedents, uncovering hidden relationships and predicting legal\ntrends-challenging tasks that are essential for ensuring justice and improving\noperational efficiency. Our system employs web scraping techniques to\nsystematically collect legal texts, such as statutes, constitutional\nprovisions, and case law, from publicly accessible platforms like Justia. It\nbridges the gap between traditional keyword-based searches and contextual\nunderstanding by leveraging advanced semantic representations, hierarchical\nrelationships, and latent topic discovery. This framework supports legal\ndocument clustering, summarization, and cross-referencing, for scalable,\ninterpretable, and accurate retrieval for semi-structured data while advancing\ncomputational law and AI.", "published": "2025-02-27 18:35:39", "link": "http://arxiv.org/abs/2502.20364v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security\n  Analysis", "abstract": "Recent advancements in Web AI agents have demonstrated remarkable\ncapabilities in addressing complex web navigation tasks. However, emerging\nresearch shows that these agents exhibit greater vulnerability compared to\nstandalone Large Language Models (LLMs), despite both being built upon the same\nsafety-aligned models. This discrepancy is particularly concerning given the\ngreater flexibility of Web AI Agent compared to standalone LLMs, which may\nexpose them to a wider range of adversarial user inputs. To build a scaffold\nthat addresses these concerns, this study investigates the underlying factors\nthat contribute to the increased vulnerability of Web AI agents. Notably, this\ndisparity stems from the multifaceted differences between Web AI agents and\nstandalone LLMs, as well as the complex signals - nuances that simple\nevaluation metrics, such as success rate, often fail to capture. To tackle\nthese challenges, we propose a component-level analysis and a more granular,\nsystematic evaluation framework. Through this fine-grained investigation, we\nidentify three critical factors that amplify the vulnerability of Web AI\nagents; (1) embedding user goals into the system prompt, (2) multi-step action\ngeneration, and (3) observational capabilities. Our findings highlights the\npressing need to enhance security and robustness in AI agent design and provide\nactionable insights for targeted defense strategies.", "published": "2025-02-27 18:56:26", "link": "http://arxiv.org/abs/2502.20383v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Chitranuvad: Adapting Multi-Lingual LLMs for Multimodal Translation", "abstract": "In this work, we provide the system description of our submission as part of\nthe English to Lowres Multimodal Translation Task at the Workshop on Asian\nTranslation (WAT2024). We introduce Chitranuvad, a multimodal model that\neffectively integrates Multilingual LLM and a vision module for Multimodal\nTranslation. Our method uses a ViT image encoder to extract visual\nrepresentations as visual token embeddings which are projected to the LLM space\nby an adapter layer and generates translation in an autoregressive fashion. We\nparticipated in all the three tracks (Image Captioning, Text only and\nMultimodal translation tasks) for Indic languages (ie. English translation to\nHindi, Bengali and Malyalam) and achieved SOTA results for Hindi in all of them\non the Challenge set while remaining competitive for the other languages in the\nshared task.", "published": "2025-02-27 07:14:31", "link": "http://arxiv.org/abs/2502.20420v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "SEKI: Self-Evolution and Knowledge Inspiration based Neural Architecture\n  Search via Large Language Models", "abstract": "We introduce SEKI, a novel large language model (LLM)-based neural\narchitecture search (NAS) method. Inspired by the chain-of-thought (CoT)\nparadigm in modern LLMs, SEKI operates in two key stages: self-evolution and\nknowledge distillation. In the self-evolution stage, LLMs initially lack\nsufficient reference examples, so we implement an iterative refinement\nmechanism that enhances architectures based on performance feedback. Over time,\nthis process accumulates a repository of high-performance architectures. In the\nknowledge distillation stage, LLMs analyze common patterns among these\narchitectures to generate new, optimized designs. Combining these two stages,\nSEKI greatly leverages the capacity of LLMs on NAS and without requiring any\ndomain-specific data. Experimental results show that SEKI achieves\nstate-of-the-art (SOTA) performance across various datasets and search spaces\nwhile requiring only 0.05 GPU-days, outperforming existing methods in both\nefficiency and accuracy. Furthermore, SEKI demonstrates strong generalization\ncapabilities, achieving SOTA-competitive results across multiple tasks.", "published": "2025-02-27 09:17:49", "link": "http://arxiv.org/abs/2502.20422v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TripCraft: A Benchmark for Spatio-Temporally Fine Grained Travel\n  Planning", "abstract": "Recent advancements in probing Large Language Models (LLMs) have explored\ntheir latent potential as personalized travel planning agents, yet existing\nbenchmarks remain limited in real world applicability. Existing datasets, such\nas TravelPlanner and TravelPlanner+, suffer from semi synthetic data reliance,\nspatial inconsistencies, and a lack of key travel constraints, making them\ninadequate for practical itinerary generation. To address these gaps, we\nintroduce TripCraft, a spatiotemporally coherent travel planning dataset that\nintegrates real world constraints, including public transit schedules, event\navailability, diverse attraction categories, and user personas for enhanced\npersonalization. To evaluate LLM generated plans beyond existing binary\nvalidation methods, we propose five continuous evaluation metrics, namely\nTemporal Meal Score, Temporal Attraction Score, Spatial Score, Ordering Score,\nand Persona Score which assess itinerary quality across multiple dimensions.\nOur parameter informed setting significantly enhances meal scheduling,\nimproving the Temporal Meal Score from 61% to 80% in a 7 day scenario.\nTripCraft establishes a new benchmark for LLM driven personalized travel\nplanning, offering a more realistic, constraint aware framework for itinerary\ngeneration. Dataset and Codebase will be made publicly available upon\nacceptance.", "published": "2025-02-27 20:33:28", "link": "http://arxiv.org/abs/2502.20508v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Supervised Fine-Tuning LLMs to Behave as Pedagogical Agents in\n  Programming Education", "abstract": "Large language models (LLMs) are increasingly being explored in higher\neducation, yet their effectiveness as teaching agents remains underexamined. In\nthis paper, we present the development of GuideLM, a fine-tuned LLM designed\nfor programming education. GuideLM has been integrated into the Debugging C\nCompiler (DCC), an educational C compiler that leverages LLMs to generate\npedagogically sound error explanations. Previously, DCC relied on off-the-shelf\nOpenAI models, which, while accurate, often over-assisted students by directly\nproviding solutions despite contrary prompting.\n  To address this, we employed supervised fine-tuning (SFT) on a dataset of 528\nstudent-question/teacher-answer pairs, creating two models: GuideLM and\nGuideLM-mini, fine-tuned on ChatGPT-4o and 4o-mini, respectively. We conducted\nan expert analysis of 400 responses per model, comparing their pedagogical\neffectiveness against base OpenAI models. Our evaluation, grounded in\nconstructivism and cognitive load theory, assessed factors such as conceptual\nscaffolding, clarity, and Socratic guidance.\n  Results indicate that GuideLM and GuideLM-mini improve pedagogical\nperformance, with an 8% increase in Socratic guidance and a 58% improvement in\neconomy of words compared to GPT-4o. However, this refinement comes at the cost\nof a slight reduction in general accuracy. While further work is needed, our\nfindings suggest that fine-tuning LLMs with targeted datasets is a promising\napproach for developing models better suited to educational contexts.", "published": "2025-02-27 21:23:56", "link": "http://arxiv.org/abs/2502.20527v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "HazardNet: A Small-Scale Vision Language Model for Real-Time Traffic\n  Safety Detection at Edge Devices", "abstract": "Traffic safety remains a vital concern in contemporary urban settings,\nintensified by the increase of vehicles and the complicated nature of road\nnetworks. Traditional safety-critical event detection systems predominantly\nrely on sensor-based approaches and conventional machine learning algorithms,\nnecessitating extensive data collection and complex training processes to\nadhere to traffic safety regulations. This paper introduces HazardNet, a\nsmall-scale Vision Language Model designed to enhance traffic safety by\nleveraging the reasoning capabilities of advanced language and vision models.\nWe built HazardNet by fine-tuning the pre-trained Qwen2-VL-2B model, chosen for\nits superior performance among open-source alternatives and its compact size of\ntwo billion parameters. This helps to facilitate deployment on edge devices\nwith efficient inference throughput. In addition, we present HazardQA, a novel\nVision Question Answering (VQA) dataset constructed specifically for training\nHazardNet on real-world scenarios involving safety-critical events. Our\nexperimental results show that the fine-tuned HazardNet outperformed the base\nmodel up to an 89% improvement in F1-Score and has comparable results with\nimprovement in some cases reach up to 6% when compared to larger models, such\nas GPT-4o. These advancements underscore the potential of HazardNet in\nproviding real-time, reliable traffic safety event detection, thereby\ncontributing to reduced accidents and improved traffic management in urban\nenvironments. Both HazardNet model and the HazardQA dataset are available at\nhttps://huggingface.co/Tami3/HazardNet and\nhttps://huggingface.co/datasets/Tami3/HazardQA, respectively.", "published": "2025-02-27 22:21:45", "link": "http://arxiv.org/abs/2502.20572v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Visual Reasoning at Urban Intersections: FineTuning GPT-4o for Traffic\n  Conflict Detection", "abstract": "Traffic control in unsignalized urban intersections presents significant\nchallenges due to the complexity, frequent conflicts, and blind spots. This\nstudy explores the capability of leveraging Multimodal Large Language Models\n(MLLMs), such as GPT-4o, to provide logical and visual reasoning by directly\nusing birds-eye-view videos of four-legged intersections. In this proposed\nmethod, GPT-4o acts as intelligent system to detect conflicts and provide\nexplanations and recommendations for the drivers. The fine-tuned model achieved\nan accuracy of 77.14%, while the manual evaluation of the true predicted values\nof the fine-tuned GPT-4o showed significant achievements of 89.9% accuracy for\nmodel-generated explanations and 92.3% for the recommended next actions. These\nresults highlight the feasibility of using MLLMs for real-time traffic\nmanagement using videos as inputs, offering scalable and actionable insights\ninto intersections traffic management and operation. Code used in this study is\navailable at\nhttps://github.com/sarimasri3/Traffic-Intersection-Conflict-Detection-using-images.git.", "published": "2025-02-27 22:26:29", "link": "http://arxiv.org/abs/2502.20573v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Smart Routing: Cost-Effective Multi-LLM Serving for Multi-Core AIOS", "abstract": "As large language models (LLMs) are increasingly deployed as service\nendpoints in systems, the surge in query volume creates significant scheduling\nchallenges. Existing scheduling frameworks mainly target at latency\noptimization while neglecting the capability of LLMs to serve different level\nof queries, which could lead to computational resource waste. For example,\nthose simple queries can be safely handled by small, fast and cheap LLMs, while\nthose complex and difficult queries need to be handled by large, slow, and\nexpensive LLMs. This paper addresses this challenge by proposing an efficient\ncapability-cost coordinated scheduling framework, ECCOS, for multi-LLM serving,\nwhich explicitly constrains response quality and workload to optimize LLM\ninference cost. Specifically, it introduces the two-stage scheduling by\ndesigning a multi-objective predictor and a constrained optimizer. The\npredictor estimates both model capabilities and computational costs through\ntraining-based and retrieval-based approaches, while the optimizer determines\ncost-optimal assignments under quality and workload constraints. It also\nintroduces QAServe, a dataset for sample-wise response quality and costs\ncollected by zero-shot prompting different LLMs on knowledge QA and\nmathematical reasoning. Extensive experiments demonstrate that ECCOS improves\nsuccess rates by 6.30% while reducing costs by 10.15% compared to existing\nmethods, consuming less than 0.5% of LLM response time. The code is available\nat: https://github.com/agiresearch/ECCOS, and the proposed smart routing\nmechanism has been integrated into AIOS, the AI Agent Operating System, at\nhttps://github.com/agiresearch/AIOS.", "published": "2025-02-27 22:35:31", "link": "http://arxiv.org/abs/2502.20576v4", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "MMSciBench: Benchmarking Language Models on Multimodal Scientific\n  Problems", "abstract": "Recent advances in large language models (LLMs) and vision-language models\n(LVLMs) have shown promise across many tasks, yet their scientific reasoning\ncapabilities remain untested, particularly in multimodal settings. We present\nMMSciBench, a benchmark for evaluating mathematical and physical reasoning\nthrough text-only and text-image formats, with human-annotated difficulty\nlevels, solutions with detailed explanations, and taxonomic mappings.\nEvaluation of state-of-the-art models reveals significant limitations, with\neven the best model achieving only \\textbf{63.77\\%} accuracy and particularly\nstruggling with visual reasoning tasks. Our analysis exposes critical gaps in\ncomplex reasoning and visual-textual integration, establishing MMSciBench as a\nrigorous standard for measuring progress in multimodal scientific\nunderstanding. The code for MMSciBench is open-sourced at GitHub, and the\ndataset is available at Hugging Face.", "published": "2025-02-27 15:38:43", "link": "http://arxiv.org/abs/2503.01891v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "KunlunBaize: LLM with Multi-Scale Convolution and Multi-Token Prediction\n  Under TransformerX Framework", "abstract": "Large language models have demonstrated remarkable performance across various\ntasks, yet they face challenges such as low computational efficiency, gradient\nvanishing, and difficulties in capturing complex feature interactions. To\naddress these limitations, a novel framework has been proposed. This framework\nincorporates a learnable dense residual skip connection mechanism, a\nTransformerX module a transformer based component integrating multiscale\nconvolution and adaptive activation functions and a multitoken prediction\ninteraction module. The learnable dense residual connections enhance\ninformation flow and feature capture across layers. Within the TransformerX\nmodule, large convolutional kernels aggregate semantic information from\nextensive text segments, while smaller convolutions focus on local word order\nand syntactic structures. The adaptive activation function dynamically adjusts\nits parameters based on the semantic features of the input text, improving the\nmodel's ability to handle diverse semantic expressions and complex\nrelationships. The multitoken prediction module boosts data utilization and\naccelerates inference by predicting multiple future tokens. These components\nsignificantly enhance the performance and efficiency of large language models.", "published": "2025-02-27 01:56:09", "link": "http://arxiv.org/abs/2503.04784v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mapping Trustworthiness in Large Language Models: A Bibliometric\n  Analysis Bridging Theory to Practice", "abstract": "The rapid proliferation of Large Language Models (LLMs) has raised pressing\nconcerns regarding their trustworthiness, spanning issues of reliability,\ntransparency, fairness, and ethical alignment. Despite the increasing adoption\nof LLMs across various domains, there remains a lack of consensus on how to\noperationalize trustworthiness in practice. This study bridges the gap between\ntheoretical discussions and implementation by conducting a bibliometric mapping\nanalysis of 2,006 publications from 2019 to 2025. Through co-authorship\nnetworks, keyword co-occurrence analysis, and thematic evolution tracking, we\nidentify key research trends, influential authors, and prevailing definitions\nof LLM trustworthiness. Additionally, a systematic review of 68 core papers is\nconducted to examine conceptualizations of trust and their practical\nimplications. Our findings reveal that trustworthiness in LLMs is often framed\nthrough existing organizational trust frameworks, emphasizing dimensions such\nas ability, benevolence, and integrity. However, a significant gap exists in\ntranslating these principles into concrete development strategies. To address\nthis, we propose a structured mapping of 20 trust-enhancing techniques across\nthe LLM lifecycle, including retrieval-augmented generation (RAG),\nexplainability techniques, and post-training audits. By synthesizing\nbibliometric insights with practical strategies, this study contributes towards\nfostering more transparent, accountable, and ethically aligned LLMs, ensuring\ntheir responsible deployment in real-world applications.", "published": "2025-02-27 14:02:33", "link": "http://arxiv.org/abs/2503.04785v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Analyzing the temporal dynamics of linguistic features contained in\n  misinformation", "abstract": "Consumption of misinformation can lead to negative consequences that impact\nthe individual and society. To help mitigate the influence of misinformation on\nhuman beliefs, algorithmic labels providing context about content accuracy and\nsource reliability have been developed. Since the linguistic features used by\nalgorithms to estimate information accuracy can change across time, it is\nimportant to understand their temporal dynamics. As a result, this study uses\nnatural language processing to analyze PolitiFact statements spanning between\n2010 and 2024 to quantify how the sources and linguistic features of\nmisinformation change between five-year time periods. The results show that\nstatement sentiment has decreased significantly over time, reflecting a\ngenerally more negative tone in PolitiFact statements. Moreover, statements\nassociated with misinformation realize significantly lower sentiment than\naccurate information. Additional analysis shows that recent time periods are\ndominated by sources from online social networks and other digital forums, such\nas blogs and viral images, that contain high levels of misinformation\ncontaining negative sentiment. In contrast, most statements during early time\nperiods are attributed to individual sources (i.e., politicians) that are\nrelatively balanced in accuracy ratings and contain statements with neutral or\npositive sentiment. Named-entity recognition was used to identify that\npresidential incumbents and candidates are relatively more prevalent in\nstatements containing misinformation, while US states tend to be present in\naccurate information. Finally, entity labels associated with people and\norganizations are more common in misinformation, while accurate statements are\nmore likely to contain numeric entity labels, such as percentages and dates.", "published": "2025-02-27 14:15:43", "link": "http://arxiv.org/abs/2503.04786v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "SuPreME: A Supervised Pre-training Framework for Multimodal ECG\n  Representation Learning", "abstract": "Cardiovascular diseases are a leading cause of death and disability\nworldwide. Electrocardiogram (ECG) recordings are critical for diagnosing and\nmonitoring cardiac health, but obtaining large-scale annotated ECG datasets is\nlabor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL)\nmethods mitigate this by learning features without extensive labels but fail to\ncapture fine-grained clinical semantics and require extensive task-specific\nfine-tuning. To address these challenges, we propose $\\textbf{SuPreME}$, a\n$\\textbf{Su}$pervised $\\textbf{Pre}$-training framework for\n$\\textbf{M}$ultimodal $\\textbf{E}$CG representation learning. SuPreME applies\nLarge Language Models (LLMs) to extract structured clinical entities from\nfree-text ECG reports, filter out noise and irrelevant content, enhance\nclinical representation learning, and build a high-quality, fine-grained\nlabeled dataset. By using text-based cardiac queries instead of traditional\ncategorical labels, SuPreME enables zero-shot classification of unseen diseases\nwithout additional fine-tuning. We evaluate SuPreME on six downstream datasets\ncovering 127 cardiac conditions, achieving superior zero-shot AUC performance\nover state-of-the-art eSSL and multimodal methods by over 1.96\\%. Results\ndemonstrate the effectiveness of SuPreME in leveraging structured, clinically\nrelevant knowledge for high-quality ECG representations. All code and data will\nbe released upon acceptance.", "published": "2025-02-27 01:29:51", "link": "http://arxiv.org/abs/2502.19668v1", "categories": ["eess.SP", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "eess.SP"}
{"title": "Do Retrieval-Augmented Language Models Adapt to Varying User Needs?", "abstract": "Recent advancements in Retrieval-Augmented Language Models (RALMs) have\ndemonstrated their efficacy in knowledge-intensive tasks. However, existing\nevaluation benchmarks often assume a single optimal approach to leveraging\nretrieved information, failing to account for varying user needs. This paper\nintroduces a novel evaluation framework that systematically assesses RALMs\nunder three user need cases-Context-Exclusive, Context-First, and\nMemory-First-across three distinct context settings: Context Matching,\nKnowledge Conflict, and Information Irrelevant. By varying both user\ninstructions and the nature of retrieved information, our approach captures the\ncomplexities of real-world applications where models must adapt to diverse user\nrequirements. Through extensive experiments on multiple QA datasets, including\nHotpotQA, DisentQA, and our newly constructed synthetic URAQ dataset, we find\nthat restricting memory usage improves robustness in adversarial retrieval\nconditions but decreases peak performance with ideal retrieval results and\nmodel family dominates behavioral differences. Our findings highlight the\nnecessity of user-centric evaluations in the development of retrieval-augmented\nsystems and provide insights into optimizing model performance across varied\nretrieval contexts. We will release our code and URAQ dataset upon acceptance\nof the paper.", "published": "2025-02-27 05:39:38", "link": "http://arxiv.org/abs/2502.19779v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ConvCodeWorld: Benchmarking Conversational Code Generation in\n  Reproducible Feedback Environments", "abstract": "Large language models (LLMs) have proven invaluable for code generation,\nparticularly in interactive settings. However, existing code generation\nbenchmarks fail to capture the diverse feedback encountered in multi-turn\ninteractions, limiting our ability to evaluate LLMs in these contexts. To\naddress this gap, we present a set of novel benchmarks that explicitly model\nthe quality of feedback provided to code generation LLMs. Our contributions are\nthreefold: First, we introduce CONVCODEWORLD, a novel and reproducible\nenvironment for benchmarking interactive code generation. CONVCODEWORLD\nsimulates 9 distinct interactive code generation scenarios while systematically\ncombining three types of feedback: (a) compilation feedback; (b) execution\nfeedback with varying test coverage; (c) verbal feedback generated by GPT-4o\nwith different levels of expertise. Second, we introduce CONVCODEBENCH, a fast,\nstatic version of benchmark that uses pre-generated feedback logs, eliminating\nthe need for costly dynamic verbal feedback generation while maintaining strong\nSpearman's rank correlations (0.82 to 0.99) with CONVCODEWORLD. Third,\nextensive evaluations of both closed-source and open-source LLMs including\nR1-Distill on CONVCODEWORLD reveal key insights: (a) LLM performance varies\nsignificantly based on the feedback provided; (b) Weaker LLMs, with sufficient\nfeedback, can outperform single-turn results of state-of-the-art LLMs without\nfeedback; (c) Training on a specific feedback combination can limit an LLM's\nability to utilize unseen combinations; (d) LLMs solve problems in fewer turns\n(high MRR) may not solve as many problems overall (high Recall), and vice\nversa. All implementations and benchmarks will be made publicly available at\nhttps://huggingface.co/spaces/ConvCodeWorld/ConvCodeWorld", "published": "2025-02-27 07:54:32", "link": "http://arxiv.org/abs/2502.19852v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Behind the Tip of Efficiency: Uncovering the Submerged Threats of\n  Jailbreak Attacks in Small Language Models", "abstract": "Small language models (SLMs) have become increasingly prominent in the\ndeployment on edge devices due to their high efficiency and low computational\ncost. While researchers continue to advance the capabilities of SLMs through\ninnovative training strategies and model compression techniques, the security\nrisks of SLMs have received considerably less attention compared to large\nlanguage models (LLMs).To fill this gap, we provide a comprehensive empirical\nstudy to evaluate the security performance of 13 state-of-the-art SLMs under\nvarious jailbreak attacks. Our experiments demonstrate that most SLMs are quite\nsusceptible to existing jailbreak attacks, while some of them are even\nvulnerable to direct harmful prompts.To address the safety concerns, we\nevaluate several representative defense methods and demonstrate their\neffectiveness in enhancing the security of SLMs. We further analyze the\npotential security degradation caused by different SLM techniques including\narchitecture compression, quantization, knowledge distillation, and so on. We\nexpect that our research can highlight the security challenges of SLMs and\nprovide valuable insights to future work in developing more robust and secure\nSLMs.", "published": "2025-02-27 08:44:04", "link": "http://arxiv.org/abs/2502.19883v2", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Collab-Overcooked: Benchmarking and Evaluating Large Language Models as\n  Collaborative Agents", "abstract": "Large language models (LLMs) based agent systems have made great strides in\nreal-world applications beyond traditional NLP tasks. This paper proposes a new\nLLM-powered Multi-Agent System (LLM-MAS) benchmark, Collab-Overcooked, built on\nthe popular Overcooked-AI game with more applicable and challenging tasks in\ninteractive environments. Collab-Overcooked extends existing benchmarks from\ntwo novel perspectives. First, it provides a multi-agent framework supporting\ndiverse tasks and objectives and encourages collaboration through natural\nlanguage communication. Second, it introduces a spectrum of process-oriented\nevaluation metrics to assess the fine-grained collaboration capabilities of\ndifferent LLM agents, a dimension often overlooked in prior work. We conduct\nextensive experiments over 10 popular LLMs and show that, while the LLMs\npresent a strong ability in goal interpretation, there is a significant\ndiscrepancy in active collaboration and continuous adaption that are critical\nfor efficiently fulfilling complicated tasks. Notably, we highlight the\nstrengths and weaknesses in LLM-MAS and provide insights for improving and\nevaluating LLM-MAS on a unified and open-sourced benchmark. Environments, 30\nopen-ended tasks, and an integrated evaluation package are now publicly\navailable at https://github.com/YusaeMeow/Collab-Overcooked.", "published": "2025-02-27 13:31:13", "link": "http://arxiv.org/abs/2502.20073v1", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Self-Training Elicits Concise Reasoning in Large Language Models", "abstract": "Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to\nutilize additional computation through intermediate tokens to solve complex\ntasks. However, we posit that typical reasoning traces contain many redundant\ntokens, incurring extraneous inference costs. Upon examination of the output\ndistribution of current LLMs, we find evidence on their latent ability to\nreason more concisely, relative to their default behavior. To elicit this\ncapability, we propose simple fine-tuning methods which leverage self-generated\nconcise reasoning paths obtained by best-of-N sampling and few-shot\nconditioning, in task-specific settings. Our combined method achieves a 30%\nreduction in output tokens on average, across five model families on GSM8K and\nMATH, while maintaining average accuracy. By exploiting the fundamental\nstochasticity and in-context learning capabilities of LLMs, our self-training\napproach robustly elicits concise reasoning on a wide range of models,\nincluding those with extensive post-training. Code is available at\nhttps://github.com/TergelMunkhbat/concise-reasoning", "published": "2025-02-27 14:14:50", "link": "http://arxiv.org/abs/2502.20122v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning", "abstract": "Mainstream issue-resolving frameworks predominantly rely on commercial\nmodels, leading to high costs and privacy concerns. Existing training\napproaches for issue resolving struggle with poor generalization and fail to\nfully leverage open-source development resources. We propose Subtask-oriented\nReinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue\nresolving capability of LLMs. We decomposes issue resolving into structured\nsubtasks: file localization, function localization, line localization, and code\nedit generation. SoRFT consists of two training stages: (1) rejection-sampled\nsupervised fine-tuning, Chain of Thought (CoT) data is filtered using\nground-truth before fine-tuning the LLM, and (2) rule-based reinforcement\nlearning, which leverages PPO with ground-truth based rewards. We evaluate the\nSoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving\nstate-of-the-art (SOTA) performance among open-source models (e.g., resolve\n21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental\nresults demonstrate that SoRFT significantly enhances issue-resolving\nperformance, improves model generalization, and provides a cost-efficient\nalternative to commercial models.", "published": "2025-02-27 14:19:45", "link": "http://arxiv.org/abs/2502.20127v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Re-evaluating Open-ended Evaluation of Large Language Models", "abstract": "Evaluation has traditionally focused on ranking candidates for a specific\nskill. Modern generalist models, such as Large Language Models (LLMs),\ndecidedly outpace this paradigm. Open-ended evaluation systems, where candidate\nmodels are compared on user-submitted prompts, have emerged as a popular\nsolution. Despite their many advantages, we show that the current Elo-based\nrating systems can be susceptible to and even reinforce biases in data,\nintentional or accidental, due to their sensitivity to redundancies. To address\nthis issue, we propose evaluation as a 3-player game, and introduce novel\ngame-theoretic solution concepts to ensure robustness to redundancy. We show\nthat our method leads to intuitive ratings and provide insights into the\ncompetitive landscape of LLM development.", "published": "2025-02-27 15:07:47", "link": "http://arxiv.org/abs/2502.20170v1", "categories": ["cs.GT", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.GT"}
{"title": "An exploration of features to improve the generalisability of fake news\n  detection models", "abstract": "Fake news poses global risks by influencing elections and spreading\nmisinformation, making detection critical. Existing NLP and supervised Machine\nLearning methods perform well under cross-validation but struggle to generalise\nacross datasets, even within the same domain. This issue stems from coarsely\nlabelled training data, where articles are labelled based on their publisher,\nintroducing biases that token-based models like TF-IDF and BERT are sensitive\nto. While Large Language Models (LLMs) offer promise, their application in fake\nnews detection remains limited. This study demonstrates that meaningful\nfeatures can still be extracted from coarsely labelled data to improve\nreal-world robustness. Stylistic features-lexical, syntactic, and semantic-are\nexplored due to their reduced sensitivity to dataset biases. Additionally,\nnovel social-monetisation features are introduced, capturing economic\nincentives behind fake news, such as advertisements, external links, and social\nmedia elements. The study trains on the coarsely labelled NELA 2020-21 dataset\nand evaluates using the manually labelled Facebook URLs dataset, a gold\nstandard for generalisability. Results highlight the limitations of token-based\nmodels trained on biased data and contribute to the scarce evidence on LLMs\nlike LLaMa in this field. Findings indicate that stylistic and\nsocial-monetisation features offer more generalisable predictions than\ntoken-based methods and LLMs. Statistical and permutation feature importance\nanalyses further reveal their potential to enhance performance and mitigate\ndataset biases, providing a path forward for improving fake news detection.", "published": "2025-02-27 17:26:56", "link": "http://arxiv.org/abs/2502.20299v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "M^3Builder: A Multi-Agent System for Automated Machine Learning in\n  Medical Imaging", "abstract": "Agentic AI systems have gained significant attention for their ability to\nautonomously perform complex tasks. However, their reliance on well-prepared\ntools limits their applicability in the medical domain, which requires to train\nspecialized models. In this paper, we make three contributions: (i) We present\nM3Builder, a novel multi-agent system designed to automate machine learning\n(ML) in medical imaging. At its core, M3Builder employs four specialized agents\nthat collaborate to tackle complex, multi-step medical ML workflows, from\nautomated data processing and environment configuration to self-contained auto\ndebugging and model training. These agents operate within a medical imaging ML\nworkspace, a structured environment designed to provide agents with free-text\ndescriptions of datasets, training codes, and interaction tools, enabling\nseamless communication and task execution. (ii) To evaluate progress in\nautomated medical imaging ML, we propose M3Bench, a benchmark comprising four\ngeneral tasks on 14 training datasets, across five anatomies and three imaging\nmodalities, covering both 2D and 3D data. (iii) We experiment with seven\nstate-of-the-art large language models serving as agent cores for our system,\nsuch as Claude series, GPT-4o, and DeepSeek-V3. Compared to existing ML agentic\ndesigns, M3Builder shows superior performance on completing ML tasks in medical\nimaging, achieving a 94.29% success rate using Claude-3.7-Sonnet as the agent\ncore, showing huge potential towards fully automated machine learning in\nmedical imaging.", "published": "2025-02-27 17:29:46", "link": "http://arxiv.org/abs/2502.20301v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "LangProBe: a Language Programs Benchmark", "abstract": "Composing language models (LMs) into multi-step language programs and\nautomatically optimizing their modular prompts is now a mainstream paradigm for\nbuilding AI systems, but the tradeoffs in this space have only scarcely been\nstudied before. We introduce LangProBe, the first large-scale benchmark for\nevaluating the architectures and optimization strategies for language programs,\nwith over 2000 combinations of tasks, architectures, optimizers, and choices of\nLMs. Using LangProBe, we are the first to study the impact of program\narchitectures and optimizers (and their compositions together and with\ndifferent models) on tradeoffs of quality and cost. We find that optimized\nlanguage programs offer strong cost--quality Pareto improvement over raw calls\nto models, but simultaneously demonstrate that human judgment (or empirical\ndecisions) about which compositions to pursue is still necessary for best\nperformance. We will open source the code and evaluation data for LangProBe.", "published": "2025-02-27 17:41:49", "link": "http://arxiv.org/abs/2502.20315v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bridging the Creativity Understanding Gap: Small-Scale Human Alignment\n  Enables Expert-Level Humor Ranking in LLMs", "abstract": "Large Language Models (LLMs) have shown significant limitations in\nunderstanding creative content, as demonstrated by Hessel et al. (2023)'s\ninfluential work on the New Yorker Cartoon Caption Contest (NYCCC). Their study\nexposed a substantial gap between LLMs and humans in humor comprehension,\nestablishing that understanding and evaluating creative content is key\nchallenge in AI development. We revisit this challenge by decomposing humor\nunderstanding into three components and systematically improve each: enhancing\nvisual understanding through improved annotation, utilizing LLM-generated humor\nreasoning and explanations, and implementing targeted alignment with human\npreference data. Our refined approach achieves 82.4% accuracy in caption\nranking, singificantly improving upon the previous 67% benchmark and matching\nthe performance of world-renowned human experts in this domain. Notably, while\nattempts to mimic subgroup preferences through various persona prompts showed\nminimal impact, model finetuning with crowd preferences proved remarkably\neffective. These findings reveal that LLM limitations in creative judgment can\nbe effectively addressed through focused alignment to specific subgroups and\nindividuals. Lastly, we propose the position that achieving artificial general\nintelligence necessitates systematic collection of human preference data across\ncreative domains. We advocate that just as human creativity is deeply\ninfluenced by individual and cultural preferences, training LLMs with diverse\nhuman preference data may be essential for developing true creative\nunderstanding.", "published": "2025-02-27 18:29:09", "link": "http://arxiv.org/abs/2502.20356v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation", "abstract": "High-quality benchmarks are essential for evaluating reasoning and retrieval\ncapabilities of large language models (LLMs). However, curating datasets for\nthis purpose is not a permanent solution as they are prone to data leakage and\ninflated performance results. To address these challenges, we propose\nPhantomWiki: a pipeline to generate unique, factually consistent document\ncorpora with diverse question-answer pairs. Unlike prior work, PhantomWiki is\nneither a fixed dataset, nor is it based on any existing data. Instead, a new\nPhantomWiki instance is generated on demand for each evaluation. We vary the\nquestion difficulty and corpus size to disentangle reasoning and retrieval\ncapabilities respectively, and find that PhantomWiki datasets are surprisingly\nchallenging for frontier LLMs. Thus, we contribute a scalable and data\nleakage-resistant framework for disentangled evaluation of reasoning,\nretrieval, and tool-use abilities. Our code is available at\nhttps://github.com/kilian-group/phantom-wiki.", "published": "2025-02-27 18:51:22", "link": "http://arxiv.org/abs/2502.20377v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Multi-Turn Code Generation Through Single-Step Rewards", "abstract": "We address the problem of code generation from multi-turn execution feedback.\nExisting methods either generate code without feedback or use complex,\nhierarchical reinforcement learning to optimize multi-turn rewards. We propose\na simple yet scalable approach, $\\mu$Code, that solves multi-turn code\ngeneration using only single-step rewards. Our key insight is that code\ngeneration is a one-step recoverable MDP, where the correct code can be\nrecovered from any intermediate code state in a single turn. $\\mu$Code\niteratively trains both a generator to provide code solutions conditioned on\nmulti-turn execution feedback and a verifier to score the newly generated code.\nExperimental evaluations show that our approach achieves significant\nimprovements over the state-of-the-art baselines. We provide analysis of the\ndesign choices of the reward models and policy, and show the efficacy of\n$\\mu$Code at utilizing the execution feedback. Our code is available at\nhttps://github.com/portal-cornell/muCode.", "published": "2025-02-27 18:55:05", "link": "http://arxiv.org/abs/2502.20380v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Among Them: A game-based framework for assessing persuasion capabilities\n  of LLMs", "abstract": "The proliferation of large language models (LLMs) and autonomous AI agents\nhas raised concerns about their potential for automated persuasion and social\ninfluence. While existing research has explored isolated instances of LLM-based\nmanipulation, systematic evaluations of persuasion capabilities across\ndifferent models remain limited. In this paper, we present an Among Us-inspired\ngame framework for assessing LLM deception skills in a controlled environment.\nThe proposed framework makes it possible to compare LLM models by game\nstatistics, as well as quantify in-game manipulation according to 25 persuasion\nstrategies from social psychology and rhetoric. Experiments between 8 popular\nlanguage models of different types and sizes demonstrate that all tested models\nexhibit persuasive capabilities, successfully employing 22 of the 25\nanticipated techniques. We also find that larger models do not provide any\npersuasion advantage over smaller models and that longer model outputs are\nnegatively correlated with the number of games won. Our study provides insights\ninto the deception capabilities of LLMs, as well as tools and data for\nfostering future research on the topic.", "published": "2025-02-27 12:26:21", "link": "http://arxiv.org/abs/2502.20426v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Promote, Suppress, Iterate: How Language Models Answer One-to-Many\n  Factual Queries", "abstract": "To answer one-to-many factual queries (e.g., listing cities of a country), a\nlanguage model (LM) must simultaneously recall knowledge and avoid repeating\nprevious answers. How are these two subtasks implemented and integrated\ninternally? Across multiple datasets and models, we identify a\npromote-then-suppress mechanism: the model first recalls all answers, and then\nsuppresses previously generated ones. Specifically, LMs use both the subject\nand previous answer tokens to perform knowledge recall, with attention\npropagating subject information and MLPs promoting the answers. Then, attention\nattends to and suppresses previous answer tokens, while MLPs amplify the\nsuppression signal. Our mechanism is corroborated by extensive experimental\nevidence: in addition to using early decoding and causal tracing, we analyze\nhow components use different tokens by introducing both Token Lens, which\ndecodes aggregated attention updates from specified tokens, and a knockout\nmethod that analyzes changes in MLP outputs after removing attention to\nspecified tokens. Overall, we provide new insights into how LMs' internal\ncomponents interact with different input tokens to support complex factual\nrecall. Code is available at\nhttps://github.com/Lorenayannnnn/how-lms-answer-one-to-many-factual-queries.", "published": "2025-02-27 19:23:15", "link": "http://arxiv.org/abs/2502.20475v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EgoNormia: Benchmarking Physical Social Norm Understanding", "abstract": "Human activity is moderated by norms. However, machines are often trained\nwithout explicit supervision on norm understanding and reasoning, especially\nwhen the norms are grounded in a physical and social context. To improve and\nevaluate the normative reasoning capability of vision-language models (VLMs),\nwe present EgoNormia $\\|\\epsilon\\|$, consisting of 1,853 ego-centric videos of\nhuman interactions, each of which has two related questions evaluating both the\nprediction and justification of normative actions. The normative actions\nencompass seven categories: safety, privacy, proxemics, politeness,\ncooperation, coordination/proactivity, and communication/legibility. To compile\nthis dataset at scale, we propose a novel pipeline leveraging video sampling,\nautomatic answer generation, filtering, and human validation. Our work\ndemonstrates that current state-of-the-art vision-language models lack robust\nnorm understanding, scoring a maximum of 45% on EgoNormia (versus a human bench\nof 92%). Our analysis of performance in each dimension highlights the\nsignificant risks of safety, privacy, and the lack of collaboration and\ncommunication capability when applied to real-world agents. We additionally\nshow that through a retrieval-based generation method, it is possible to use\nEgoNormia to enhance normative reasoning in VLMs.", "published": "2025-02-27 19:54:16", "link": "http://arxiv.org/abs/2502.20490v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Thousand Words or An Image: Studying the Influence of Persona Modality\n  in Multimodal LLMs", "abstract": "Large language models (LLMs) have recently demonstrated remarkable\nadvancements in embodying diverse personas, enhancing their effectiveness as\nconversational agents and virtual assistants. Consequently, LLMs have made\nsignificant strides in processing and integrating multimodal information.\nHowever, even though human personas can be expressed in both text and image,\nthe extent to which the modality of a persona impacts the embodiment by the LLM\nremains largely unexplored. In this paper, we investigate how do different\nmodalities influence the expressiveness of personas in multimodal LLMs. To this\nend, we create a novel modality-parallel dataset of 40 diverse personas varying\nin age, gender, occupation, and location. This consists of four modalities to\nequivalently represent a persona: image-only, text-only, a combination of image\nand small text, and typographical images, where text is visually stylized to\nconvey persona-related attributes. We then create a systematic evaluation\nframework with 60 questions and corresponding metrics to assess how well LLMs\nembody each persona across its attributes and scenarios. Comprehensive\nexperiments on $5$ multimodal LLMs show that personas represented by detailed\ntext show more linguistic habits, while typographical images often show more\nconsistency with the persona. Our results reveal that LLMs often overlook\npersona-specific details conveyed through images, highlighting underlying\nlimitations and paving the way for future research to bridge this gap. We\nrelease the data and code at https://github.com/claws-lab/persona-modality .", "published": "2025-02-27 20:25:00", "link": "http://arxiv.org/abs/2502.20504v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "NANOGPT: A Query-Driven Large Language Model Retrieval-Augmented\n  Generation System for Nanotechnology Research", "abstract": "This paper presents the development and application of a Large Language Model\nRetrieval-Augmented Generation (LLM-RAG) system tailored for nanotechnology\nresearch. The system leverages the capabilities of a sophisticated language\nmodel to serve as an intelligent research assistant, enhancing the efficiency\nand comprehensiveness of literature reviews in the nanotechnology domain.\nCentral to this LLM-RAG system is its advanced query backend retrieval\nmechanism, which integrates data from multiple reputable sources. The system\nretrieves relevant literature by utilizing Google Scholar's advanced search,\nand scraping open-access papers from Elsevier, Springer Nature, and ACS\nPublications. This multifaceted approach ensures a broad and diverse collection\nof up-to-date scholarly articles and papers. The proposed system demonstrates\nsignificant potential in aiding researchers by providing a streamlined,\naccurate, and exhaustive literature retrieval process, thereby accelerating\nresearch advancements in nanotechnology. The effectiveness of the LLM-RAG\nsystem is validated through rigorous testing, illustrating its capability to\nsignificantly reduce the time and effort required for comprehensive literature\nreviews, while maintaining high accuracy, query relevance and outperforming\nstandard, publicly available LLMS.", "published": "2025-02-27 21:40:22", "link": "http://arxiv.org/abs/2502.20541v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "$Q\\sharp$: Provably Optimal Distributional RL for LLM Post-Training", "abstract": "Reinforcement learning (RL) post-training is crucial for LLM alignment and\nreasoning, but existing policy-based methods, such as PPO and DPO, can fall\nshort of fixing shortcuts inherited from pre-training. In this work, we\nintroduce $Q\\sharp$, a value-based algorithm for KL-regularized RL that guides\nthe reference policy using the optimal regularized $Q$ function. We propose to\nlearn the optimal $Q$ function using distributional RL on an aggregated online\ndataset. Unlike prior value-based baselines that guide the model using\nunregularized $Q$-values, our method is theoretically principled and provably\nlearns the optimal policy for the KL-regularized RL problem. Empirically,\n$Q\\sharp$ outperforms prior baselines in math reasoning benchmarks while\nmaintaining a smaller KL divergence to the reference policy. Theoretically, we\nestablish a reduction from KL-regularized RL to no-regret online learning,\nproviding the first bounds for deterministic MDPs under only realizability.\nThanks to distributional RL, our bounds are also variance-dependent and\nconverge faster when the reference policy has small variance. In sum, our\nresults highlight $Q\\sharp$ as an effective approach for post-training LLMs,\noffering both improved performance and theoretical guarantees. The code can be\nfound at https://github.com/jinpz/q_sharp.", "published": "2025-02-27 21:43:00", "link": "http://arxiv.org/abs/2502.20548v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards Statistical Factuality Guarantee for Large Vision-Language\n  Models", "abstract": "Advancements in Large Vision-Language Models (LVLMs) have demonstrated\npromising performance in a variety of vision-language tasks involving\nimage-conditioned free-form text generation. However, growing concerns about\nhallucinations in LVLMs, where the generated text is inconsistent with the\nvisual context, are becoming a major impediment to deploying these models in\napplications that demand guaranteed reliability. In this paper, we introduce a\nframework to address this challenge, ConfLVLM, which is grounded on conformal\nprediction to achieve finite-sample distribution-free statistical guarantees on\nthe factuality of LVLM output. This framework treats an LVLM as a hypothesis\ngenerator, where each generated text detail (or claim) is considered an\nindividual hypothesis. It then applies a statistical hypothesis testing\nprocedure to verify each claim using efficient heuristic uncertainty measures\nto filter out unreliable claims before returning any responses to users. We\nconduct extensive experiments covering three representative application\ndomains, including general scene understanding, medical radiology report\ngeneration, and document understanding. Remarkably, ConfLVLM reduces the error\nrate of claims generated by LLaVa-1.5 for scene descriptions from 87.8\\% to\n10.0\\% by filtering out erroneous claims with a 95.3\\% true positive rate. Our\nresults further demonstrate that ConfLVLM is highly flexible, and can be\napplied to any black-box LVLMs paired with any uncertainty measure for any\nimage-conditioned free-form text generation task while providing a rigorous\nguarantee on controlling the risk of hallucination.", "published": "2025-02-27 22:01:22", "link": "http://arxiv.org/abs/2502.20560v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "LLMs Have Rhythm: Fingerprinting Large Language Models Using Inter-Token\n  Times and Network Traffic Analysis", "abstract": "As Large Language Models (LLMs) become increasingly integrated into many\ntechnological ecosystems across various domains and industries, identifying\nwhich model is deployed or being interacted with is critical for the security\nand trustworthiness of the systems. Current verification methods typically rely\non analyzing the generated output to determine the source model. However, these\ntechniques are susceptible to adversarial attacks, operate in a post-hoc\nmanner, and may require access to the model weights to inject a verifiable\nfingerprint. In this paper, we propose a novel passive and non-invasive\nfingerprinting technique that operates in real-time and remains effective even\nunder encrypted network traffic conditions. Our method leverages the intrinsic\nautoregressive generation nature of language models, which generate text one\ntoken at a time based on all previously generated tokens, creating a unique\ntemporal pattern like a rhythm or heartbeat that persists even when the output\nis streamed over a network. We find that measuring the Inter-Token Times\n(ITTs)-time intervals between consecutive tokens-can identify different\nlanguage models with high accuracy. We develop a Deep Learning (DL) pipeline to\ncapture these timing patterns using network traffic analysis and evaluate it on\n16 Small Language Models (SLMs) and 10 proprietary LLMs across different\ndeployment scenarios, including local host machine (GPU/CPU), Local Area\nNetwork (LAN), Remote Network, and Virtual Private Network (VPN). The\nexperimental results confirm that our proposed technique is effective and\nmaintains high accuracy even when tested in different network conditions. This\nwork opens a new avenue for model identification in real-world scenarios and\ncontributes to more secure and trustworthy language model deployment.", "published": "2025-02-27 23:22:01", "link": "http://arxiv.org/abs/2502.20589v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Societal Alignment Frameworks Can Improve LLM Alignment", "abstract": "Recent progress in large language models (LLMs) has focused on producing\nresponses that meet human expectations and align with shared values - a process\ncoined alignment. However, aligning LLMs remains challenging due to the\ninherent disconnect between the complexity of human values and the narrow\nnature of the technological approaches designed to address them. Current\nalignment methods often lead to misspecified objectives, reflecting the broader\nissue of incomplete contracts, the impracticality of specifying a contract\nbetween a model developer, and the model that accounts for every scenario in\nLLM alignment. In this paper, we argue that improving LLM alignment requires\nincorporating insights from societal alignment frameworks, including social,\neconomic, and contractual alignment, and discuss potential solutions drawn from\nthese domains. Given the role of uncertainty within societal alignment\nframeworks, we then investigate how it manifests in LLM alignment. We end our\ndiscussion by offering an alternative view on LLM alignment, framing the\nunderspecified nature of its objectives as an opportunity rather than perfect\ntheir specification. Beyond technical improvements in LLM alignment, we discuss\nthe need for participatory alignment interface designs.", "published": "2025-02-27 13:26:07", "link": "http://arxiv.org/abs/2503.00069v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "I see what you mean: Co-Speech Gestures for Reference Resolution in\n  Multimodal Dialogue", "abstract": "In face-to-face interaction, we use multiple modalities, including speech and\ngestures, to communicate information and resolve references to objects.\nHowever, how representational co-speech gestures refer to objects remains\nunderstudied from a computational perspective. In this work, we address this\ngap by introducing a multimodal reference resolution task centred on\nrepresentational gestures, while simultaneously tackling the challenge of\nlearning robust gesture embeddings. We propose a self-supervised pre-training\napproach to gesture representation learning that grounds body movements in\nspoken language. Our experiments show that the learned embeddings align with\nexpert annotations and have significant predictive power. Moreover, reference\nresolution accuracy further improves when (1) using multimodal gesture\nrepresentations, even when speech is unavailable at inference time, and (2)\nleveraging dialogue history. Overall, our findings highlight the complementary\nroles of gesture and speech in reference resolution, offering a step towards\nmore naturalistic models of human-machine interaction.", "published": "2025-02-27 17:28:12", "link": "http://arxiv.org/abs/2503.00071v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Advanced Deep Learning Techniques for Analyzing Earnings Call\n  Transcripts: Methodologies and Applications", "abstract": "This study presents a comparative analysis of deep learning methodologies\nsuch as BERT, FinBERT and ULMFiT for sentiment analysis of earnings call\ntranscripts. The objective is to investigate how Natural Language Processing\n(NLP) can be leveraged to extract sentiment from large-scale financial\ntranscripts, thereby aiding in more informed investment decisions and risk\nmanagement strategies. We examine the strengths and limitations of each model\nin the context of financial sentiment analysis, focusing on data preprocessing\nrequirements, computational efficiency, and model optimization. Through\nrigorous experimentation, we evaluate their performance using key metrics,\nincluding accuracy, precision, recall, and F1-score. Furthermore, we discuss\npotential enhancements to improve the effectiveness of these models in\nfinancial text analysis, providing insights into their applicability for\nreal-world financial decision-making.", "published": "2025-02-27 00:28:43", "link": "http://arxiv.org/abs/2503.01886v1", "categories": ["cs.CL", "cs.AI", "q-fin.RM"], "primary_category": "cs.CL"}
{"title": "FedMentalCare: Towards Privacy-Preserving Fine-Tuned LLMs to Analyze\n  Mental Health Status Using Federated Learning Framework", "abstract": "With the increasing prevalence of mental health conditions worldwide,\nAI-powered chatbots and conversational agents have emerged as accessible tools\nto support mental health. However, deploying Large Language Models (LLMs) in\nmental healthcare applications raises significant privacy concerns, especially\nregarding regulations like HIPAA and GDPR. In this work, we propose\nFedMentalCare, a privacy-preserving framework that leverages Federated Learning\n(FL) combined with Low-Rank Adaptation (LoRA) to fine-tune LLMs for mental\nhealth analysis. We investigate the performance impact of varying client data\nvolumes and model architectures (e.g., MobileBERT and MiniLM) in FL\nenvironments. Our framework demonstrates a scalable, privacy-aware approach for\ndeploying LLMs in real-world mental healthcare scenarios, addressing data\nsecurity and computational efficiency challenges.", "published": "2025-02-27 07:04:19", "link": "http://arxiv.org/abs/2503.05786v2", "categories": ["cs.CL", "cs.HC", "cs.LG", "I.2.6; I.5.1; J.3; C.2.4; D.4.6"], "primary_category": "cs.CL"}
{"title": "Does Your Voice Assistant Remember? Analyzing Conversational Context\n  Recall and Utilization in Voice Interaction Models", "abstract": "Recent advancements in multi-turn voice interaction models have improved\nuser-model communication. However, while closed-source models effectively\nretain and recall past utterances, whether open-source models share this\nability remains unexplored. To fill this gap, we systematically evaluate how\nwell open-source interaction models utilize past utterances using\nContextDialog, a benchmark we proposed for this purpose. Our findings show that\nspeech-based models have more difficulty than text-based ones, especially when\nrecalling information conveyed in speech, and even with retrieval-augmented\ngeneration, models still struggle with questions about past utterances. These\ninsights highlight key limitations in open-source models and suggest ways to\nimprove memory retention and retrieval robustness.", "published": "2025-02-27 04:53:55", "link": "http://arxiv.org/abs/2502.19759v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PrimeK-Net: Multi-scale Spectral Learning via Group Prime-Kernel\n  Convolutional Neural Networks for Single Channel Speech Enhancement", "abstract": "Single-channel speech enhancement is a challenging ill-posed problem focused\non estimating clean speech from degraded signals. Existing studies have\ndemonstrated the competitive performance of combining convolutional neural\nnetworks (CNNs) with Transformers in speech enhancement tasks. However,\nexisting frameworks have not sufficiently addressed computational efficiency\nand have overlooked the natural multi-scale distribution of the spectrum.\nAdditionally, the potential of CNNs in speech enhancement has yet to be fully\nrealized. To address these issues, this study proposes a Deep Separable Dilated\nDense Block (DSDDB) and a Group Prime Kernel Feedforward Channel Attention\n(GPFCA) module. Specifically, the DSDDB introduces higher parameter and\ncomputational efficiency to the Encoder/Decoder of existing frameworks. The\nGPFCA module replaces the position of the Conformer, extracting deep temporal\nand frequency features of the spectrum with linear complexity. The GPFCA\nleverages the proposed Group Prime Kernel Feedforward Network (GPFN) to\nintegrate multi-granularity long-range, medium-range, and short-range receptive\nfields, while utilizing the properties of prime numbers to avoid periodic\noverlap effects. Experimental results demonstrate that PrimeK-Net, proposed in\nthis study, achieves state-of-the-art (SOTA) performance on the\nVoiceBank+Demand dataset, reaching a PESQ score of 3.61 with only 1.41M\nparameters.", "published": "2025-02-27 09:25:39", "link": "http://arxiv.org/abs/2502.19906v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "UniCodec: Unified Audio Codec with Single Domain-Adaptive Codebook", "abstract": "The emergence of audio language models is empowered by neural audio codecs,\nwhich establish critical mappings between continuous waveforms and discrete\ntokens compatible with language model paradigms. The evolutionary trends from\nmulti-layer residual vector quantizer to single-layer quantizer are beneficial\nfor language-autoregressive decoding. However, the capability to handle\nmulti-domain audio signals through a single codebook remains constrained by\ninter-domain distribution discrepancies. In this work, we introduce UniCodec, a\nunified audio codec with a single codebook to support multi-domain audio data,\nincluding speech, music, and sound. To achieve this, we propose a partitioned\ndomain-adaptive codebook method and domain Mixture-of-Experts strategy to\ncapture the distinct characteristics of each audio domain. Furthermore, to\nenrich the semantic density of the codec without auxiliary modules, we propose\na self-supervised mask prediction modeling approach. Comprehensive objective\nand subjective evaluations demonstrate that UniCodec achieves excellent audio\nreconstruction performance across the three audio domains, outperforming\nexisting unified neural codecs with a single codebook, and even surpasses\nstate-of-the-art domain-specific codecs on both acoustic and semantic\nrepresentation capabilities.", "published": "2025-02-27 13:16:41", "link": "http://arxiv.org/abs/2502.20067v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DiffCSS: Diverse and Expressive Conversational Speech Synthesis with\n  Diffusion Models", "abstract": "Conversational speech synthesis (CSS) aims to synthesize both contextually\nappropriate and expressive speech, and considerable efforts have been made to\nenhance the understanding of conversational context. However, existing CSS\nsystems are limited to deterministic prediction, overlooking the diversity of\npotential responses. Moreover, they rarely employ language model (LM)-based TTS\nbackbones, limiting the naturalness and quality of synthesized speech. To\naddress these issues, in this paper, we propose DiffCSS, an innovative CSS\nframework that leverages diffusion models and an LM-based TTS backbone to\ngenerate diverse, expressive, and contextually coherent speech. A\ndiffusion-based context-aware prosody predictor is proposed to sample diverse\nprosody embeddings conditioned on multimodal conversational context. Then a\nprosody-controllable LM-based TTS backbone is developed to synthesize\nhigh-quality speech with sampled prosody embeddings. Experimental results\ndemonstrate that the synthesized speech from DiffCSS is more diverse,\ncontextually coherent, and expressive than existing CSS systems", "published": "2025-02-27 09:53:48", "link": "http://arxiv.org/abs/2502.19924v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CleanMel: Mel-Spectrogram Enhancement for Improving Both Speech Quality\n  and ASR", "abstract": "In this work, we propose CleanMel, a single-channel Mel-spectrogram denoising\nand dereverberation network for improving both speech quality and automatic\nspeech recognition (ASR) performance. The proposed network takes as input the\nnoisy and reverberant microphone recording and predicts the corresponding clean\nMel-spectrogram. The enhanced Mel-spectrogram can be either transformed to\nspeech waveform with a neural vocoder or directly used for ASR. The proposed\nnetwork is composed of interleaved cross-band and narrow-band processing in the\nMel-frequency domain, for learning the full-band spectral pattern and the\nnarrow-band properties of signals, respectively. Compared to linear-frequency\ndomain or time-domain speech enhancement, the key advantage of Mel-spectrogram\nenhancement is that Mel-frequency presents speech in a more compact way and\nthus is easier to learn, which will benefit both speech quality and ASR.\nExperimental results on four English and one Chinese datasets demonstrate a\nsignificant improvement in both speech quality and ASR performance achieved by\nthe proposed model. Code and audio examples of our model are available online\nin https://audio.westlake.edu.cn/Research/CleanMel.html.", "published": "2025-02-27 12:28:29", "link": "http://arxiv.org/abs/2502.20040v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DGFM: Full Body Dance Generation Driven by Music Foundation Models", "abstract": "In music-driven dance motion generation, most existing methods use\nhand-crafted features and neglect that music foundation models have profoundly\nimpacted cross-modal content generation. To bridge this gap, we propose a\ndiffusion-based method that generates dance movements conditioned on text and\nmusic. Our approach extracts music features by combining high-level features\nobtained by music foundation model with hand-crafted features, thereby\nenhancing the quality of generated dance sequences. This method effectively\nleverages the advantages of high-level semantic information and low-level\ntemporal details to improve the model's capability in music feature\nunderstanding. To show the merits of the proposed method, we compare it with\nfour music foundation models and two sets of hand-crafted music features. The\nresults demonstrate that our method obtains the most realistic dance sequences\nand achieves the best match with the input music.", "published": "2025-02-27 15:13:45", "link": "http://arxiv.org/abs/2502.20176v1", "categories": ["cs.SD", "cs.GR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DIN-CTS: Low-Complexity Depthwise-Inception Neural Network with\n  Contrastive Training Strategy for Deepfake Speech Detection", "abstract": "In this paper, we propose a deep neural network approach for deepfake speech\ndetection (DSD) based on a lowcomplexity Depthwise-Inception Network (DIN)\ntrained with a contrastive training strategy (CTS). In this framework, input\naudio recordings are first transformed into spectrograms using Short-Time\nFourier Transform (STFT) and Linear Filter (LF), which are then used to train\nthe DIN. Once trained, the DIN processes bonafide utterances to extract audio\nembeddings, which are used to construct a Gaussian distribution representing\ngenuine speech. Deepfake detection is then performed by computing the distance\nbetween a test utterance and this distribution to determine whether the\nutterance is fake or bonafide. To evaluate our proposed systems, we conducted\nextensive experiments on the benchmark dataset of ASVspoof 2019 LA. The\nexperimental results demonstrate the effectiveness of combining the\nDepthwise-Inception Network with the contrastive learning strategy in\ndistinguishing between fake and bonafide utterances. We achieved Equal Error\nRate (EER), Accuracy (Acc.), F1, AUC scores of 4.6%, 95.4%, 97.3%, and 98.9%\nrespectively using a single, low-complexity DIN with just 1.77 M parameters and\n985 M FLOPS on short audio segments (4 seconds). Furthermore, our proposed\nsystem outperforms the single-system submissions in the ASVspoof 2019 LA\nchallenge, showcasing its potential for real-time applications.", "published": "2025-02-27 16:09:04", "link": "http://arxiv.org/abs/2502.20225v2", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adapting Automatic Speech Recognition for Accented Air Traffic Control\n  Communications", "abstract": "Effective communication in Air Traffic Control (ATC) is critical to\nmaintaining aviation safety, yet the challenges posed by accented English\nremain largely unaddressed in Automatic Speech Recognition (ASR) systems.\nExisting models struggle with transcription accuracy for Southeast\nAsian-accented (SEA-accented) speech, particularly in noisy ATC environments.\nThis study presents the development of ASR models fine-tuned specifically for\nSoutheast Asian accents using a newly created dataset. Our research achieves\nsignificant improvements, achieving a Word Error Rate (WER) of 0.0982 or 9.82%\non SEA-accented ATC speech. Additionally, the paper highlights the importance\nof region-specific datasets and accent-focused training, offering a pathway for\ndeploying ASR systems in resource-constrained military operations. The findings\nemphasize the need for noise-robust training techniques and region-specific\ndatasets to improve transcription accuracy for non-Western accents in ATC\ncommunications.", "published": "2025-02-27 17:35:59", "link": "http://arxiv.org/abs/2502.20311v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "On Adversarial Attacks In Acoustic Drone Localization", "abstract": "Multi-rotor aerial autonomous vehicles (MAVs, more widely known as \"drones\")\nhave been generating increased interest in recent years due to their growing\napplicability in a vast and diverse range of fields (e.g., agriculture,\ncommercial delivery, search and rescue). The sensitivity of visual-based\nmethods to lighting conditions and occlusions had prompted growing study of\nnavigation reliant on other modalities, such as acoustic sensing. A major\nconcern in using drones in scale for tasks in non-controlled environments is\nthe potential threat of adversarial attacks over their navigational systems,\nexposing users to mission-critical failures, security breaches, and compromised\nsafety outcomes that can endanger operators and bystanders. While previous work\nshows impressive progress in acoustic-based drone localization, prior research\nin adversarial attacks over drone navigation only addresses visual\nsensing-based systems. In this work, we aim to compensate for this gap by\nsupplying a comprehensive analysis of the effect of PGD adversarial attacks\nover acoustic drone localization. We furthermore develop an algorithm for\nadversarial perturbation recovery, capable of markedly diminishing the affect\nof such attacks in our setting. The code for reproducing all experiments will\nbe released upon publication.", "published": "2025-02-27 17:50:17", "link": "http://arxiv.org/abs/2502.20325v1", "categories": ["cs.SD", "cs.RO", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DeePen: Penetration Testing for Audio Deepfake Detection", "abstract": "Deepfakes - manipulated or forged audio and video media - pose significant\nsecurity risks to individuals, organizations, and society at large. To address\nthese challenges, machine learning-based classifiers are commonly employed to\ndetect deepfake content. In this paper, we assess the robustness of such\nclassifiers through a systematic penetration testing methodology, which we\nintroduce as DeePen. Our approach operates without prior knowledge of or access\nto the target deepfake detection models. Instead, it leverages a set of\ncarefully selected signal processing modifications - referred to as attacks -\nto evaluate model vulnerabilities. Using DeePen, we analyze both real-world\nproduction systems and publicly available academic model checkpoints,\ndemonstrating that all tested systems exhibit weaknesses and can be reliably\ndeceived by simple manipulations such as time-stretching or echo addition.\nFurthermore, our findings reveal that while some attacks can be mitigated by\nretraining detection systems with knowledge of the specific attack, others\nremain persistently effective. We release all associated code.", "published": "2025-02-27 12:26:25", "link": "http://arxiv.org/abs/2502.20427v2", "categories": ["cs.CR", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "LiteASR: Efficient Automatic Speech Recognition with Low-Rank\n  Approximation", "abstract": "Modern automatic speech recognition (ASR) models, such as OpenAI's Whisper,\nrely on deep encoder-decoder architectures, and their encoders are a critical\nbottleneck for efficient deployment due to high computational intensity. We\nintroduce LiteASR, a low-rank compression scheme for ASR encoders that\nsignificantly reduces inference costs while maintaining transcription accuracy.\nOur approach leverages the strong low-rank properties observed in intermediate\nactivations: by applying principal component analysis (PCA) with a small\ncalibration dataset, we approximate linear transformations with a chain of\nlow-rank matrix multiplications, and further optimize self-attention to work in\nthe reduced dimension. Evaluation results show that our method can compress\nWhisper large-v3's encoder size by over 50%, matching Whisper medium's size\nwith better transcription accuracy, thereby establishing a new Pareto-optimal\nfrontier of efficiency and performance. The code of LiteASR is available at\nhttps://github.com/efeslab/LiteASR.", "published": "2025-02-27 22:52:21", "link": "http://arxiv.org/abs/2502.20583v1", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
