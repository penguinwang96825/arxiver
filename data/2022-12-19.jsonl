{"title": "OASum: Large-Scale Open Domain Aspect-based Summarization", "abstract": "Aspect or query-based summarization has recently caught more attention, as it\ncan generate differentiated summaries based on users' interests. However, the\ncurrent dataset for aspect or query-based summarization either focuses on\nspecific domains, contains relatively small-scale instances, or includes only a\nfew aspect types. Such limitations hinder further explorations in this\ndirection. In this work, we take advantage of crowd-sourcing knowledge on\nWikipedia.org and automatically create a high-quality, large-scale open-domain\naspect-based summarization dataset named OASum, which contains more than 3.7\nmillion instances with around 1 million different aspects on 2 million\nWikipedia pages. We provide benchmark results on OASum and demonstrate its\nability for diverse aspect-based summarization generation. To overcome the data\nscarcity problem on specific domains, we also perform zero-shot, few-shot, and\nfine-tuning on seven downstream datasets. Specifically, zero/few-shot and\nfine-tuning results show that the model pre-trained on our corpus demonstrates\na strong aspect or query-focused generation ability compared with the backbone\nmodel. Our dataset and pre-trained checkpoints are publicly available.", "published": "2022-12-19 04:04:17", "link": "http://arxiv.org/abs/2212.09233v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PAL: Persona-Augmented Emotional Support Conversation Generation", "abstract": "Due to the lack of human resources for mental health support, there is an\nincreasing demand for employing conversational agents for support. Recent work\nhas demonstrated the effectiveness of dialogue models in providing emotional\nsupport. As previous studies have demonstrated that seekers' persona is an\nimportant factor for effective support, we investigate whether there are\nbenefits to modeling such information in dialogue models for support. In this\npaper, our empirical analysis verifies that persona has an important impact on\nemotional support. Therefore, we propose a framework for dynamically inferring\nand modeling seekers' persona. We first train a model for inferring the\nseeker's persona from the conversation history. Accordingly, we propose PAL, a\nmodel that leverages persona information and, in conjunction with our\nstrategy-based controllable generation method, provides personalized emotional\nsupport. Automatic and manual evaluations demonstrate that PAL achieves\nstate-of-the-art results, outperforming the baselines on the studied benchmark.\nOur code and data are publicly available at https://github.com/chengjl19/PAL.", "published": "2022-12-19 04:12:54", "link": "http://arxiv.org/abs/2212.09235v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "I2D2: Inductive Knowledge Distillation with NeuroLogic and\n  Self-Imitation", "abstract": "Commonsense capabilities of pre-trained language models dramatically improve\nwith scale, leading many to believe that scale is the only winning recipe. But\nis it? Here, we investigate an alternative that a priori seems impossible: can\nsmaller language models (e.g., GPT-2) win over models that are orders of\nmagnitude larger and better (e.g., GPT-3), if powered with novel commonsense\ndistillation algorithms? The key intellectual challenge is to design a learning\nalgorithm that achieve a competitive level of commonsense acquisition, without\nrelying on the benefits of scale. In particular, we study generative models of\ncommonsense knowledge, focusing on the task of generating generics, statements\nof commonsense facts about everyday concepts, e.g., birds can fly.\n  We introduce I2D2, a novel commonsense distillation framework that loosely\nfollows the Symbolic Knowledge Distillation of West et al. but breaks the\ndependence on the extreme-scale teacher model with two innovations: (1) the\nnovel adaptation of NeuroLogic Decoding to enhance the generation quality of\nthe weak, off-the-shelf language models, and (2) self-imitation learning to\niteratively learn from the model's own enhanced commonsense acquisition\ncapabilities. Empirical results suggest that scale is not the only way, as\nnovel algorithms can be a promising alternative. Moreover, our study leads to a\nnew corpus of generics, Gen-A-tomic, that is the largest and highest quality\navailable to date.", "published": "2022-12-19 04:47:49", "link": "http://arxiv.org/abs/2212.09246v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven\n  Optimization", "abstract": "Robustness evaluation against adversarial examples has become increasingly\nimportant to unveil the trustworthiness of the prevailing deep models in\nnatural language processing (NLP). However, in contrast to the computer vision\ndomain where the first-order projected gradient descent (PGD) is used as the\nbenchmark approach to generate adversarial examples for robustness evaluation,\nthere lacks a principled first-order gradient-based robustness evaluation\nframework in NLP. The emerging optimization challenges lie in 1) the discrete\nnature of textual inputs together with the strong coupling between the\nperturbation location and the actual content, and 2) the additional constraint\nthat the perturbed text should be fluent and achieve a low perplexity under a\nlanguage model. These challenges make the development of PGD-like NLP attacks\ndifficult. To bridge the gap, we propose TextGrad, a new attack generator using\ngradient-driven optimization, supporting high-accuracy and high-quality\nassessment of adversarial robustness in NLP. Specifically, we address the\naforementioned challenges in a unified optimization framework. And we develop\nan effective convex relaxation method to co-optimize the continuously-relaxed\nsite selection and perturbation variables and leverage an effective sampling\nmethod to establish an accurate mapping from the continuous optimization\nvariables to the discrete textual perturbations. Moreover, as a first-order\nattack generation method, TextGrad can be baked into adversarial training to\nfurther improve the robustness of NLP models. Extensive experiments are\nprovided to demonstrate the effectiveness of TextGrad not only in attack\ngeneration for robustness evaluation but also in adversarial defense.", "published": "2022-12-19 05:55:58", "link": "http://arxiv.org/abs/2212.09254v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi hash embeddings in spaCy", "abstract": "The distributed representation of symbols is one of the key technologies in\nmachine learning systems today, playing a pivotal role in modern natural\nlanguage processing. Traditional word embeddings associate a separate vector\nwith each word. While this approach is simple and leads to good performance, it\nrequires a lot of memory for representing a large vocabulary. To reduce the\nmemory footprint, the default embedding layer in spaCy is a hash embeddings\nlayer. It is a stochastic approximation of traditional embeddings that provides\nunique vectors for a large number of words without explicitly storing a\nseparate vector for each of them. To be able to compute meaningful\nrepresentations for both known and unknown words, hash embeddings represent\neach word as a summary of the normalized word form, subword information and\nword shape. Together, these features produce a multi-embedding of a word. In\nthis technical report we lay out a bit of history and introduce the embedding\nmethods in spaCy in detail. Second, we critically evaluate the hash embedding\narchitecture with multi-embeddings on Named Entity Recognition datasets from a\nvariety of domains and languages. The experiments validate most key design\nchoices behind spaCy's embedders, but we also uncover a few surprising results.", "published": "2022-12-19 06:03:04", "link": "http://arxiv.org/abs/2212.09255v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "PromptBoosting: Black-Box Text Classification with Ten Forward Passes", "abstract": "We describe PromptBoosting, a query-efficient procedure for building a text\nclassifier from a neural language model (LM) without access to the LM's\nparameters, gradients, or hidden representations. This form of \"black-box\"\nclassifier training has become increasingly important as the cost of training\nand inference in large-scale LMs grows. But existing black-box LM classifier\nlearning approaches are themselves computationally inefficient, typically\nspecializing LMs to the target task by searching in a large space of (discrete\nor continuous) prompts using zeroth-order optimization methods. Instead of\ndirectly optimizing in prompt space, PromptBoosting obtains a small pool of\nprompts via a gradient-free approach and then constructs a large pool of weak\nlearners by pairing these prompts with different elements of the LM's output\ndistribution. These weak learners are then ensembled using the AdaBoost\nalgorithm. The entire learning process requires only a small number of forward\npasses and no backward pass. Experiments show that PromptBoosting achieves\nstate-of-the-art performance in multiple black-box few-shot classification\ntasks, and matches or outperforms full fine-tuning in both few-shot and\nstandard learning paradigms, while training 10x faster than existing black-box\nmethods.", "published": "2022-12-19 06:04:54", "link": "http://arxiv.org/abs/2212.09257v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Statistical Dataset Evaluation: Reliability, Difficulty, and Validity", "abstract": "Datasets serve as crucial training resources and model performance trackers.\nHowever, existing datasets have exposed a plethora of problems, inducing biased\nmodels and unreliable evaluation results. In this paper, we propose a\nmodel-agnostic dataset evaluation framework for automatic dataset quality\nevaluation. We seek the statistical properties of the datasets and address\nthree fundamental dimensions: reliability, difficulty, and validity, following\na classical testing theory. Taking the Named Entity Recognition (NER) datasets\nas a case study, we introduce $9$ statistical metrics for a statistical dataset\nevaluation framework. Experimental results and human evaluation validate that\nour evaluation framework effectively assesses various aspects of the dataset\nquality. Furthermore, we study how the dataset scores on our statistical\nmetrics affect the model performance, and appeal for dataset quality evaluation\nor targeted dataset improvement before training or testing models.", "published": "2022-12-19 06:55:42", "link": "http://arxiv.org/abs/2212.09272v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Investigation of Indian Native Language Phonemic Influences on L2\n  English Pronunciations", "abstract": "Speech systems are sensitive to accent variations. This is especially\nchallenging in the Indian context, with an abundance of languages but a dearth\nof linguistic studies characterising pronunciation variations. The growing\nnumber of L2 English speakers in India reinforces the need to study accents and\nL1-L2 interactions. We investigate the accents of Indian English (IE) speakers\nand report in detail our observations, both specific and common to all regions.\nIn particular, we observe the phonemic variations and phonotactics occurring in\nthe speakers' native languages and apply this to their English pronunciations.\nWe demonstrate the influence of 18 Indian languages on IE by comparing the\nnative language pronunciations with IE pronunciations obtained jointly from\nexisting literature studies and phonetically annotated speech of 80 speakers.\nConsequently, we are able to validate the intuitions of Indian language\ninfluences on IE pronunciations by justifying pronunciation rules from the\nperspective of Indian language phonology. We obtain a comprehensive description\nin terms of universal and region-specific characteristics of IE, which\nfacilitates accent conversion and adaptation of existing ASR and TTS systems to\ndifferent Indian accents.", "published": "2022-12-19 07:41:39", "link": "http://arxiv.org/abs/2212.09284v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SESCORE2: Learning Text Generation Evaluation via Synthesizing Realistic\n  Mistakes", "abstract": "Is it possible to train a general metric for evaluating text generation\nquality without human annotated ratings? Existing learned metrics either\nperform unsatisfactorily across text generation tasks or require human ratings\nfor training on specific tasks. In this paper, we propose SESCORE2, a\nself-supervised approach for training a model-based metric for text generation\nevaluation. The key concept is to synthesize realistic model mistakes by\nperturbing sentences retrieved from a corpus. The primary advantage of the\nSESCORE2 is its ease of extension to many other languages while providing\nreliable severity estimation. We evaluate SESCORE2 and previous methods on four\ntext generation tasks across three languages. SESCORE2 outperforms unsupervised\nmetric PRISM on four text generation evaluation benchmarks, with a Kendall\nimprovement of 0.078. Surprisingly, SESCORE2 even outperforms the supervised\nBLEURT and COMET on multiple text generation tasks. The code and data are\navailable at https://github.com/xu1998hz/SEScore2.", "published": "2022-12-19 09:02:16", "link": "http://arxiv.org/abs/2212.09305v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "E-NER -- An Annotated Named Entity Recognition Corpus of Legal Text", "abstract": "Identifying named entities such as a person, location or organization, in\ndocuments can highlight key information to readers. Training Named Entity\nRecognition (NER) models requires an annotated data set, which can be a\ntime-consuming labour-intensive task. Nevertheless, there are publicly\navailable NER data sets for general English. Recently there has been interest\nin developing NER for legal text. However, prior work and experimental results\nreported here indicate that there is a significant degradation in performance\nwhen NER methods trained on a general English data set are applied to legal\ntext. We describe a publicly available legal NER data set, called E-NER, based\non legal company filings available from the US Securities and Exchange\nCommission's EDGAR data set. Training a number of different NER algorithms on\nthe general English CoNLL-2003 corpus but testing on our test collection\nconfirmed significant degradations in accuracy, as measured by the F1-score, of\nbetween 29.4\\% and 60.4\\%, compared to training and testing on the E-NER\ncollection.", "published": "2022-12-19 09:03:32", "link": "http://arxiv.org/abs/2212.09306v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging The Gap: Entailment Fused-T5 for Open-retrieval Conversational\n  Machine Reading Comprehension", "abstract": "Open-retrieval conversational machine reading comprehension (OCMRC) simulates\nreal-life conversational interaction scenes. Machines are required to make a\ndecision of \"Yes/No/Inquire\" or generate a follow-up question when the decision\nis \"Inquire\" based on retrieved rule texts, user scenario, user question, and\ndialogue history. Recent studies explored the methods to reduce the information\ngap between decision-making and question generation and thus improve the\nperformance of generation. However, the information gap still exists because\nthese pipeline structures are still limited in decision-making, span\nextraction, and question rephrasing three stages. Decision-making and\ngeneration are reasoning separately, and the entailment reasoning utilized in\ndecision-making is hard to share through all stages. To tackle the above\nproblem, we proposed a novel one-stage end-to-end framework, called Entailment\nFused-T5 (EFT), to bridge the information gap between decision-making and\ngeneration in a global understanding manner. The extensive experimental results\ndemonstrate that our proposed framework achieves new state-of-the-art\nperformance on the OR-ShARC benchmark.", "published": "2022-12-19 10:38:30", "link": "http://arxiv.org/abs/2212.09353v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Medical Knowledge Graph QA for Drug-Drug Interaction Prediction based on\n  Multi-hop Machine Reading Comprehension", "abstract": "Drug-drug interaction prediction is a crucial issue in molecular biology.\nTraditional methods of observing drug-drug interactions through medical\nexperiments require significant resources and labor. This paper presents a\nmedical knowledge graph question answering model, dubbed MedKGQA, that predicts\ndrug-drug interaction by employing machine reading comprehension from\nclosed-domain literature and constructing a knowledge graph of drug-protein\ntriplets from open-domain documents. The model vectorizes the drug-protein\ntarget attributes in the graph using entity embeddings and establishes directed\nconnections between drug and protein entities based on the metabolic\ninteraction pathways of protein targets in the human body. This aligns multiple\nexternal knowledge and applies it to learn the graph neural network. Without\nbells and whistles, the proposed model achieved a 4.5% improvement in terms of\ndrug-drug interaction prediction accuracy compared to previous state-of-the-art\nmodels on the Qangaroo MedHop dataset. Experimental results demonstrate the\nefficiency and effectiveness of the model and verify the feasibility of\nintegrating external knowledge in machine reading comprehension tasks.", "published": "2022-12-19 12:24:32", "link": "http://arxiv.org/abs/2212.09400v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-View Knowledge Distillation from Crowd Annotations for\n  Out-of-Domain Generalization", "abstract": "Selecting an effective training signal for tasks in natural language\nprocessing is difficult: expert annotations are expensive, and crowd-sourced\nannotations may not be reliable. At the same time, recent work in NLP has\ndemonstrated that learning from a distribution over labels acquired from crowd\nannotations can be effective. However, there are many ways to acquire such a\ndistribution, and the performance allotted by any one method can fluctuate\nbased on the task and the amount of available crowd annotations, making it\ndifficult to know a priori which distribution is best. This paper\nsystematically analyzes this in the out-of-domain setting, adding to the NLP\nliterature which has focused on in-domain evaluation, and proposes new methods\nfor acquiring soft-labels from crowd-annotations by aggregating the\ndistributions produced by existing methods. In particular, we propose to\naggregate multiple-views of crowd annotations via temperature scaling and\nfinding their Jensen-Shannon centroid. We demonstrate that these aggregation\nmethods lead to the most consistent performance across four NLP tasks on\nout-of-domain test sets, mitigating fluctuations in performance from the\nindividual distributions. Additionally, aggregation results in the most\nconsistently well-calibrated uncertainty estimation. We argue that aggregating\ndifferent views of crowd-annotations is an effective and minimal intervention\nto acquire soft-labels which induce robust classifiers despite the\ninconsistency of the individual soft-labeling methods.", "published": "2022-12-19 12:40:18", "link": "http://arxiv.org/abs/2212.09409v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Less is More: Parameter-Free Text Classification with Gzip", "abstract": "Deep neural networks (DNNs) are often used for text classification tasks as\nthey usually achieve high levels of accuracy. However, DNNs can be\ncomputationally intensive with billions of parameters and large amounts of\nlabeled data, which can make them expensive to use, to optimize and to transfer\nto out-of-distribution (OOD) cases in practice. In this paper, we propose a\nnon-parametric alternative to DNNs that's easy, light-weight and universal in\ntext classification: a combination of a simple compressor like gzip with a\n$k$-nearest-neighbor classifier. Without any training, pre-training or\nfine-tuning, our method achieves results that are competitive with\nnon-pretrained deep learning methods on six in-distributed datasets. It even\noutperforms BERT on all five OOD datasets, including four low-resource\nlanguages. Our method also performs particularly well in few-shot settings\nwhere labeled data are too scarce for DNNs to achieve a satisfying accuracy.", "published": "2022-12-19 12:40:18", "link": "http://arxiv.org/abs/2212.09410v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Human in the loop: How to effectively create coherent topics by manually\n  labeling only a few documents per class", "abstract": "Few-shot methods for accurate modeling under sparse label-settings have\nimproved significantly. However, the applications of few-shot modeling in\nnatural language processing remain solely in the field of document\nclassification. With recent performance improvements, supervised few-shot\nmethods, combined with a simple topic extraction method pose a significant\nchallenge to unsupervised topic modeling methods. Our research shows that\nsupervised few-shot learning, combined with a simple topic extraction method,\ncan outperform unsupervised topic modeling techniques in terms of generating\ncoherent topics, even when only a few labeled documents per class are used.", "published": "2022-12-19 12:57:11", "link": "http://arxiv.org/abs/2212.09422v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving the Generalizability of Text-Based Emotion Detection by\n  Leveraging Transformers with Psycholinguistic Features", "abstract": "In recent years, there has been increased interest in building predictive\nmodels that harness natural language processing and machine learning techniques\nto detect emotions from various text sources, including social media posts,\nmicro-blogs or news articles. Yet, deployment of such models in real-world\nsentiment and emotion applications faces challenges, in particular poor\nout-of-domain generalizability. This is likely due to domain-specific\ndifferences (e.g., topics, communicative goals, and annotation schemes) that\nmake transfer between different models of emotion recognition difficult. In\nthis work we propose approaches for text-based emotion detection that leverage\ntransformer models (BERT and RoBERTa) in combination with Bidirectional Long\nShort-Term Memory (BiLSTM) networks trained on a comprehensive set of\npsycholinguistic features. First, we evaluate the performance of our models\nwithin-domain on two benchmark datasets: GoEmotion and ISEAR. Second, we\nconduct transfer learning experiments on six datasets from the Unified Emotion\nDataset to evaluate their out-of-domain robustness. We find that the proposed\nhybrid models improve the ability to generalize to out-of-distribution data\ncompared to a standard transformer-based approach. Moreover, we observe that\nthese models perform competitively on in-domain data.", "published": "2022-12-19 13:58:48", "link": "http://arxiv.org/abs/2212.09465v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Label Smoothing on Multi-hop Question Answering", "abstract": "Multi-Hop Question Answering (MHQA) is a significant area in question\nanswering, requiring multiple reasoning components, including document\nretrieval, supporting sentence prediction, and answer span extraction. In this\nwork, we analyze the primary factors limiting the performance of multi-hop\nreasoning and introduce label smoothing into the MHQA task. This is aimed at\nenhancing the generalization capabilities of MHQA systems and mitigating\noverfitting of answer spans and reasoning paths in training set. We propose a\nnovel label smoothing technique, F1 Smoothing, which incorporates uncertainty\ninto the learning process and is specifically tailored for Machine Reading\nComprehension (MRC) tasks. Inspired by the principles of curriculum learning,\nwe introduce the Linear Decay Label Smoothing Algorithm (LDLA), which\nprogressively reduces uncertainty throughout the training process. Experiment\non the HotpotQA dataset demonstrates the effectiveness of our methods in\nenhancing performance and generalizability in multi-hop reasoning, achieving\nnew state-of-the-art results on the leaderboard.", "published": "2022-12-19 14:48:08", "link": "http://arxiv.org/abs/2212.09512v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Source-Free Domain Adaptation for Question Answering with Masked\n  Self-training", "abstract": "Most previous unsupervised domain adaptation (UDA) methods for question\nanswering(QA) require access to source domain data while fine-tuning the model\nfor the target domain. Source domain data may, however, contain sensitive\ninformation and may be restricted. In this study, we investigate a more\nchallenging setting, source-free UDA, in which we have only the pretrained\nsource model and target domain data, without access to source domain data. We\npropose a novel self-training approach to QA models that integrates a unique\nmask module for domain adaptation. The mask is auto-adjusted to extract key\ndomain knowledge while trained on the source domain. To maintain previously\nlearned domain knowledge, certain mask weights are frozen during adaptation,\nwhile other weights are adjusted to mitigate domain shifts with pseudo-labeled\nsamples generated in the target domain. %As part of the self-training process,\nwe generate pseudo-labeled samples in the target domain based on models trained\nin the source domain. Our empirical results on four benchmark datasets suggest\nthat our approach significantly enhances the performance of pretrained QA\nmodels on the target domain, and even outperforms models that have access to\nthe source data during adaptation.", "published": "2022-12-19 15:53:12", "link": "http://arxiv.org/abs/2212.09563v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Privacy Adhering Machine Un-learning in NLP", "abstract": "Regulations introduced by General Data Protection Regulation (GDPR) in the EU\nor California Consumer Privacy Act (CCPA) in the US have included provisions on\nthe \\textit{right to be forgotten} that mandates industry applications to\nremove data related to an individual from their systems. In several real world\nindustry applications that use Machine Learning to build models on user data,\nsuch mandates require significant effort both in terms of data cleansing as\nwell as model retraining while ensuring the models do not deteriorate in\nprediction quality due to removal of data. As a result, continuous removal of\ndata and model retraining steps do not scale if these applications receive such\nrequests at a very high frequency. Recently, a few researchers proposed the\nidea of \\textit{Machine Unlearning} to tackle this challenge. Despite the\nsignificant importance of this task, the area of Machine Unlearning is\nunder-explored in Natural Language Processing (NLP) tasks. In this paper, we\nexplore the Unlearning framework on various GLUE tasks \\cite{Wang:18}, such as,\nQQP, SST and MNLI. We propose computationally efficient approaches (SISA-FC and\nSISA-A) to perform \\textit{guaranteed} Unlearning that provides significant\nreduction in terms of both memory (90-95\\%), time (100x) and space consumption\n(99\\%) in comparison to the baselines while keeping model performance constant.", "published": "2022-12-19 16:06:45", "link": "http://arxiv.org/abs/2212.09573v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CiteBench: A benchmark for Scientific Citation Text Generation", "abstract": "Science progresses by building upon the prior body of knowledge documented in\nscientific publications. The acceleration of research makes it hard to stay\nup-to-date with the recent developments and to summarize the ever-growing body\nof prior work. To address this, the task of citation text generation aims to\nproduce accurate textual summaries given a set of papers-to-cite and the citing\npaper context. Due to otherwise rare explicit anchoring of cited documents in\nthe citing paper, citation text generation provides an excellent opportunity to\nstudy how humans aggregate and synthesize textual knowledge from sources. Yet,\nexisting studies are based upon widely diverging task definitions, which makes\nit hard to study this task systematically. To address this challenge, we\npropose CiteBench: a benchmark for citation text generation that unifies\nmultiple diverse datasets and enables standardized evaluation of citation text\ngeneration models across task designs and domains. Using the new benchmark, we\ninvestigate the performance of multiple strong baselines, test their\ntransferability between the datasets, and deliver new insights into the task\ndefinition and evaluation to guide future research in citation text generation.\nWe make the code for CiteBench publicly available at\nhttps://github.com/UKPLab/citebench.", "published": "2022-12-19 16:10:56", "link": "http://arxiv.org/abs/2212.09577v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Interpretability of Independent Components of Word Embeddings\n  with Automated Word Intruder Test", "abstract": "Independent Component Analysis (ICA) is an algorithm originally developed for\nfinding separate sources in a mixed signal, such as a recording of multiple\npeople in the same room speaking at the same time. Unlike Principal Component\nAnalysis (PCA), ICA permits the representation of a word as an unstructured set\nof features, without any particular feature being deemed more significant than\nthe others. In this paper, we used ICA to analyze word embeddings. We have\nfound that ICA can be used to find semantic features of the words, and these\nfeatures can easily be combined to search for words that satisfy the\ncombination. We show that most of the independent components represent such\nfeatures. To quantify the interpretability of the components, we use the word\nintruder test, performed both by humans and by large language models. We\npropose to use the automated version of the word intruder test as a fast and\ninexpensive way of quantifying vector interpretability without the need for\nhuman effort.", "published": "2022-12-19 16:13:52", "link": "http://arxiv.org/abs/2212.09580v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Query Enhanced Knowledge-Intensive Conversation via Unsupervised Joint\n  Modeling", "abstract": "In this paper, we propose an unsupervised query enhanced approach for\nknowledge-intensive conversations, namely QKConv. There are three modules in\nQKConv: a query generator, an off-the-shelf knowledge selector, and a response\ngenerator. QKConv is optimized through joint training, which produces the\nresponse by exploring multiple candidate queries and leveraging corresponding\nselected knowledge. The joint training solely relies on the dialogue context\nand target response, getting exempt from extra query annotations or knowledge\nprovenances. To evaluate the effectiveness of the proposed QKConv, we conduct\nexperiments on three representative knowledge-intensive conversation datasets:\nconversational question-answering, task-oriented dialogue, and\nknowledge-grounded conversation. Experimental results reveal that QKConv\nperforms better than all unsupervised methods across three datasets and\nachieves competitive performance compared to supervised methods.", "published": "2022-12-19 16:21:05", "link": "http://arxiv.org/abs/2212.09588v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Summarization Re-ranking", "abstract": "With the rise of task-specific pre-training objectives, abstractive\nsummarization models like PEGASUS offer appealing zero-shot performance on\ndownstream summarization tasks. However, the performance of such unsupervised\nmodels still lags significantly behind their supervised counterparts. Similarly\nto the supervised setup, we notice a very high variance in quality among\nsummary candidates from these models while only one candidate is kept as the\nsummary output. In this paper, we propose to re-rank summary candidates in an\nunsupervised manner, aiming to close the performance gap between unsupervised\nand supervised models. Our approach improves the unsupervised PEGASUS by up to\n7.27% and ChatGPT by up to 6.86% relative mean ROUGE across four widely-adopted\nsummarization benchmarks ; and achieves relative gains of 7.51% (up to 23.73%\nfrom XSum to WikiHow) averaged over 30 zero-shot transfer setups (finetuning on\na dataset, evaluating on another).", "published": "2022-12-19 16:29:26", "link": "http://arxiv.org/abs/2212.09593v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explanation Regeneration via Information Bottleneck", "abstract": "Explaining the black-box predictions of NLP models naturally and accurately\nis an important open problem in natural language generation. These free-text\nexplanations are expected to contain sufficient and carefully-selected evidence\nto form supportive arguments for predictions. Due to the superior generative\ncapacity of large pretrained language models, recent work built on prompt\nengineering enables explanation generation without specific training. However,\nexplanation generated through single-pass prompting often lacks sufficiency and\nconciseness. To address this problem, we develop an information bottleneck\nmethod EIB to produce refined explanations that are sufficient and concise. Our\napproach regenerates the free-text explanation by polishing the single-pass\noutput from the pretrained language model but retaining the information that\nsupports the contents being explained. Experiments on two out-of-domain tasks\nverify the effectiveness of EIB through automatic evaluation and\nthoroughly-conducted human evaluation.", "published": "2022-12-19 16:41:19", "link": "http://arxiv.org/abs/2212.09603v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages", "abstract": "Multilingual Pretrained Language Models (MPLMs) have shown their strong\nmultilinguality in recent empirical cross-lingual transfer studies. In this\npaper, we propose the Prompts Augmented by Retrieval Crosslingually (PARC)\npipeline to improve the zero-shot performance on low-resource languages (LRLs)\nby augmenting the context with semantically similar sentences retrieved from a\nhigh-resource language (HRL) as prompts. PARC improves the zero-shot\nperformance on three downstream tasks (binary sentiment classification, topic\ncategorization and natural language inference) with multilingual parallel test\nsets across 10 LRLs covering 6 language families in both unlabeled settings\n(+5.1%) and labeled settings (+16.3%). PARC-labeled also outperforms the\nfinetuning baseline by 3.7%. We find a significant positive correlation between\ncross-lingual transfer performance on one side, and the similarity between the\nhigh- and low-resource languages as well as the amount of low-resource\npretraining data on the other side. A robustness analysis suggests that PARC\nhas the potential to achieve even stronger performance with more powerful\nMPLMs.", "published": "2022-12-19 17:29:37", "link": "http://arxiv.org/abs/2212.09651v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Decades Progress on Code-Switching Research in NLP: A Systematic\n  Survey on Trends and Challenges", "abstract": "Code-Switching, a common phenomenon in written text and conversation, has\nbeen studied over decades by the natural language processing (NLP) research\ncommunity. Initially, code-switching is intensively explored by leveraging\nlinguistic theories and, currently, more machine-learning oriented approaches\nto develop models. We introduce a comprehensive systematic survey on\ncode-switching research in natural language processing to understand the\nprogress of the past decades and conceptualize the challenges and tasks on the\ncode-switching topic. Finally, we summarize the trends and findings and\nconclude with a discussion for future direction and open questions for further\ninvestigation.", "published": "2022-12-19 17:42:07", "link": "http://arxiv.org/abs/2212.09660v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Norm of Word Embedding Encodes Information Gain", "abstract": "Distributed representations of words encode lexical semantic information, but\nwhat type of information is encoded and how? Focusing on the skip-gram with\nnegative-sampling method, we found that the squared norm of static word\nembedding encodes the information gain conveyed by the word; the information\ngain is defined by the Kullback-Leibler divergence of the co-occurrence\ndistribution of the word to the unigram distribution. Our findings are\nexplained by the theoretical framework of the exponential family of probability\ndistributions and confirmed through precise experiments that remove spurious\ncorrelations arising from word frequency. This theory also extends to\ncontextualized word embeddings in language models or any neural networks with\nthe softmax output layer. We also demonstrate that both the KL divergence and\nthe squared norm of embedding provide a useful metric of the informativeness of\na word in tasks such as keyword extraction, proper-noun discrimination, and\nhypernym discrimination.", "published": "2022-12-19 17:45:07", "link": "http://arxiv.org/abs/2212.09663v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MultiCoder: Multi-Programming-Lingual Pre-Training for Low-Resource Code\n  Completion", "abstract": "Code completion is a valuable topic in both academia and industry. Recently,\nlarge-scale mono-programming-lingual (MonoPL) pre-training models have been\nproposed to boost the performance of code completion. However, the code\ncompletion on low-resource programming languages (PL) is difficult for the\ndata-driven paradigm, while there are plenty of developers using low-resource\nPLs. On the other hand, there are few studies exploring the effects of\nmulti-programming-lingual (MultiPL) pre-training for the code completion,\nespecially the impact on low-resource programming languages. To this end, we\npropose the MultiCoder to enhance the low-resource code completion via MultiPL\npre-training and MultiPL Mixture-of-Experts (MoE) layers. We further propose a\nnovel PL-level MoE routing strategy (PL-MoE) for improving the code completion\non all PLs. Experimental results on CodeXGLUE and MultiCC demonstrate that 1)\nthe proposed MultiCoder significantly outperforms the MonoPL baselines on\nlow-resource programming languages, and 2) the PL-MoE module further boosts the\nperformance on six programming languages. In addition, we analyze the effects\nof the proposed method in details and explore the effectiveness of our method\nin a variety of scenarios.", "published": "2022-12-19 17:50:05", "link": "http://arxiv.org/abs/2212.09666v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "StyleFlow: Disentangle Latent Representations via Normalizing Flow for\n  Unsupervised Text Style Transfer", "abstract": "Text style transfer aims to alter the style of a sentence while preserving\nits content. Due to the lack of parallel corpora, most recent work focuses on\nunsupervised methods and often uses cycle construction to train models. Since\ncycle construction helps to improve the style transfer ability of the model by\nrebuilding transferred sentences back to original-style sentences, it brings\nabout a content loss in unsupervised text style transfer tasks. In this paper,\nwe propose a novel disentanglement-based style transfer model StyleFlow to\nenhance content preservation. Instead of the typical encoder-decoder scheme,\nStyleFlow can not only conduct the forward process to obtain the output, but\nalso infer to the input through the output. We design an attention-aware\ncoupling layers to disentangle the content representations and the style\nrepresentations of a sentence. Besides, we propose a data augmentation method\nbased on Normalizing Flow to improve the robustness of the model. Experiment\nresults demonstrate that our model preserves content effectively and achieves\nthe state-of-the-art performance on the most metrics.", "published": "2022-12-19 17:59:18", "link": "http://arxiv.org/abs/2212.09670v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LR-Sum: Summarization for Less-Resourced Languages", "abstract": "This preprint describes work in progress on LR-Sum, a new\npermissively-licensed dataset created with the goal of enabling further\nresearch in automatic summarization for less-resourced languages. LR-Sum\ncontains human-written summaries for 40 languages, many of which are\nless-resourced. We describe our process for extracting and filtering the\ndataset from the Multilingual Open Text corpus (Palen-Michel et al., 2022). The\nsource data is public domain newswire collected from from Voice of America\nwebsites, and LR-Sum is released under a Creative Commons license (CC BY 4.0),\nmaking it one of the most openly-licensed multilingual summarization datasets.\nWe describe how we plan to use the data for modeling experiments and discuss\nlimitations of the dataset.", "published": "2022-12-19 18:00:09", "link": "http://arxiv.org/abs/2212.09674v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Sequence-to-Sequence Models for Hebrew NLP", "abstract": "Recent work attributes progress in NLP to large language models (LMs) with\nincreased model size and large quantities of pretraining data. Despite this,\ncurrent state-of-the-art LMs for Hebrew are both under-parameterized and\nunder-trained compared to LMs in other languages. Additionally, previous work\non pretrained Hebrew LMs focused on encoder-only models. While the encoder-only\narchitecture is beneficial for classification tasks, it does not cater well for\nsub-word prediction tasks, such as Named Entity Recognition, when considering\nthe morphologically rich nature of Hebrew. In this paper we argue that\nsequence-to-sequence generative architectures are more suitable for LLMs in the\ncase of morphologically rich languages (MRLs) such as Hebrew. We demonstrate\nthat by casting tasks in the Hebrew NLP pipeline as text-to-text tasks, we can\nleverage powerful multilingual, pretrained sequence-to-sequence models as mT5,\neliminating the need for a specialized, morpheme-based, separately fine-tuned\ndecoder. Using this approach, our experiments show substantial improvements\nover previously published results on existing Hebrew NLP benchmarks. These\nresults suggest that multilingual sequence-to-sequence models present a\npromising building block for NLP for MRLs.", "published": "2022-12-19 18:10:23", "link": "http://arxiv.org/abs/2212.09682v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case\n  Study of COVID-19 Treatments", "abstract": "We present a human-in-the-loop evaluation framework for fact-checking novel\nmisinformation claims and identifying social media messages that support them.\nOur approach extracts check-worthy claims, which are aggregated and ranked for\nreview. Stance classifiers are then used to identify tweets supporting novel\nmisinformation claims, which are further reviewed to determine whether they\nviolate relevant policies. To demonstrate the feasibility of our approach, we\ndevelop a baseline system based on modern NLP methods for human-in-the-loop\nfact-checking in the domain of COVID-19 treatments. We make our data and\ndetailed annotation guidelines available to support the evaluation of\nhuman-in-the-loop systems that identify novel misinformation directly from raw\nuser-generated content.", "published": "2022-12-19 18:11:10", "link": "http://arxiv.org/abs/2212.09683v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Natural Bias for Language Generation Models", "abstract": "After just a few hundred training updates, a standard probabilistic model for\nlanguage generation has likely not yet learnt many semantic or syntactic rules\nof natural language, making it difficult to estimate the probability\ndistribution over next tokens. Yet around this point, these models have\nidentified a simple, loss-minimising behaviour: to output the unigram\ndistribution of the target training corpus. The use of such a heuristic raises\nthe question: Can we initialise our models with this behaviour and save\nprecious compute resources and model capacity? Here we show that we can\neffectively endow standard neural language generation models with a separate\nmodule that reflects unigram frequency statistics as prior knowledge, simply by\ninitialising the bias term in a model's final linear layer with the log-unigram\ndistribution. We use neural machine translation as a test bed for this simple\ntechnique and observe that it: (i) improves learning efficiency; (ii) achieves\nbetter overall performance; and perhaps most importantly (iii) appears to\ndisentangle strong frequency effects by encouraging the model to specialise in\nnon-frequency-related aspects of language.", "published": "2022-12-19 18:14:36", "link": "http://arxiv.org/abs/2212.09686v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Resolving Open-textured Rules with Templated Interpretive Arguments", "abstract": "Open-textured terms in written rules are typically settled through\ninterpretive argumentation. Ongoing work has attempted to catalogue the schemes\nused in such interpretive argumentation. But how can the use of these schemes\naffect the way in which people actually use and reason over the proper\ninterpretations of open-textured terms? Using the interpretive\nargument-eliciting game Aporia as our framework, we carried out an empirical\nstudy to answer this question. Differing from previous work, we did not allow\nparticipants to argue for interpretations arbitrarily, but to only use\narguments that fit with a given set of interpretive argument templates.\nFinally, we analyze the results captured by this new dataset, specifically\nfocusing on practical implications for the development of\ninterpretation-capable artificial reasoners.", "published": "2022-12-19 18:30:09", "link": "http://arxiv.org/abs/2212.09700v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MANER: Mask Augmented Named Entity Recognition for Extreme Low-Resource\n  Languages", "abstract": "This paper investigates the problem of Named Entity Recognition (NER) for\nextreme low-resource languages with only a few hundred tagged data samples. NER\nis a fundamental task in Natural Language Processing (NLP). A critical driver\naccelerating NER systems' progress is the existence of large-scale language\ncorpora that enable NER systems to achieve outstanding performance in languages\nsuch as English and French with abundant training data. However, NER for\nlow-resource languages remains relatively unexplored. In this paper, we\nintroduce Mask Augmented Named Entity Recognition (MANER), a new methodology\nthat leverages the distributional hypothesis of pre-trained masked language\nmodels (MLMs) for NER. The <mask> token in pre-trained MLMs encodes valuable\nsemantic contextual information. MANER re-purposes the <mask> token for NER\nprediction. Specifically, we prepend the <mask> token to every word in a\nsentence for which we would like to predict the named entity tag. During\ntraining, we jointly fine-tune the MLM and a new NER prediction head attached\nto each <mask> token. We demonstrate that MANER is well-suited for NER in\nlow-resource languages; our experiments show that for 100 languages with as few\nas 100 training examples, it improves on state-of-the-art methods by up to 48%\nand by 12% on average on F1 score. We also perform detailed analyses and\nablation studies to understand the scenarios that are best-suited to MANER.", "published": "2022-12-19 18:49:50", "link": "http://arxiv.org/abs/2212.09723v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LENS: A Learnable Evaluation Metric for Text Simplification", "abstract": "Training learnable metrics using modern language models has recently emerged\nas a promising method for the automatic evaluation of machine translation.\nHowever, existing human evaluation datasets for text simplification have\nlimited annotations that are based on unitary or outdated models, making them\nunsuitable for this approach. To address these issues, we introduce the\nSimpEval corpus that contains: SimpEval_past, comprising 12K human ratings on\n2.4K simplifications of 24 past systems, and SimpEval_2022, a challenging\nsimplification benchmark consisting of over 1K human ratings of 360\nsimplifications including GPT-3.5 generated text. Training on SimpEval, we\npresent LENS, a Learnable Evaluation Metric for Text Simplification. Extensive\nempirical results show that LENS correlates much better with human judgment\nthan existing metrics, paving the way for future progress in the evaluation of\ntext simplification. We also introduce Rank and Rate, a human evaluation\nframework that rates simplifications from several models in a list-wise manner\nusing an interactive interface, which ensures both consistency and accuracy in\nthe evaluation process and is used to create the SimpEval datasets.", "published": "2022-12-19 18:56:52", "link": "http://arxiv.org/abs/2212.09739v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One Embedder, Any Task: Instruction-Finetuned Text Embeddings", "abstract": "We introduce INSTRUCTOR, a new method for computing text embeddings given\ntask instructions: every text input is embedded together with instructions\nexplaining the use case (e.g., task and domain descriptions). Unlike encoders\nfrom prior work that are more specialized, INSTRUCTOR is a single embedder that\ncan generate text embeddings tailored to different downstream tasks and\ndomains, without any further training. We first annotate instructions for 330\ndiverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive\nloss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are\nunseen during training), ranging from classification and information retrieval\nto semantic textual similarity and text generation evaluation. INSTRUCTOR,\nwhile having an order of magnitude fewer parameters than the previous best\nmodel, achieves state-of-the-art performance, with an average improvement of\n3.4% compared to the previous best results on the 70 diverse datasets. Our\nanalysis suggests that INSTRUCTOR is robust to changes in instructions, and\nthat instruction finetuning mitigates the challenge of training a single model\non diverse datasets. Our model, code, and data are available at\nhttps://instructor-embedding.github.io.", "published": "2022-12-19 18:57:05", "link": "http://arxiv.org/abs/2212.09741v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Human-Language Model Interaction", "abstract": "Many real-world applications of language models (LMs), such as writing\nassistance and code autocomplete, involve human-LM interaction. However, most\nbenchmarks are non-interactive in that a model produces output without human\ninvolvement. To evaluate human-LM interaction, we develop a new framework,\nHuman-AI Language-based Interaction Evaluation (HALIE), that defines the\ncomponents of interactive systems and dimensions to consider when designing\nevaluation metrics. Compared to standard, non-interactive evaluation, HALIE\ncaptures (i) the interactive process, not only the final output; (ii) the\nfirst-person subjective experience, not just a third-party assessment; and\n(iii) notions of preference beyond quality (e.g., enjoyment and ownership). We\nthen design five tasks to cover different forms of interaction: social\ndialogue, question answering, crossword puzzles, summarization, and metaphor\ngeneration. With four state-of-the-art LMs (three variants of OpenAI's GPT-3\nand AI21 Labs' Jurassic-1), we find that better non-interactive performance\ndoes not always translate to better human-LM interaction. In particular, we\nhighlight three cases where the results from non-interactive and interactive\nmetrics diverge and underscore the importance of human-LM interaction for LM\nevaluation.", "published": "2022-12-19 18:59:45", "link": "http://arxiv.org/abs/2212.09746v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?", "abstract": "The CoNLL-2003 English named entity recognition (NER) dataset has been widely\nused to train and evaluate NER models for almost 20 years. However, it is\nunclear how well models that are trained on this 20-year-old data and developed\nover a period of decades using the same test set will perform when applied on\nmodern data. In this paper, we evaluate the generalization of over 20 different\nmodels trained on CoNLL-2003, and show that NER models have very different\ngeneralization. Surprisingly, we find no evidence of performance degradation in\npre-trained Transformers, such as RoBERTa and T5, even when fine-tuned using\ndecades-old data. We investigate why some models generalize well to new data\nwhile others do not, and attempt to disentangle the effects of temporal drift\nand overfitting due to test reuse. Our analysis suggests that most\ndeterioration is due to temporal mismatch between the pre-training corpora and\nthe downstream test sets. We found that four factors are important for good\ngeneralization: model architecture, number of parameters, time period of the\npre-training corpus, in addition to the amount of fine-tuning data. We suggest\ncurrent evaluation methods have, in some sense, underestimated progress on NER\nover the past 20 years, as NER models have not only improved on the original\nCoNLL-2003 test set, but improved even more on modern data. Our datasets can be\nfound at https://github.com/ShuhengL/acl2023_conllpp.", "published": "2022-12-19 18:59:56", "link": "http://arxiv.org/abs/2212.09747v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What to Read in a Contract? Party-Specific Summarization of Legal\n  Obligations, Entitlements, and Prohibitions", "abstract": "Reviewing and comprehending key obligations, entitlements, and prohibitions\nin legal contracts can be a tedious task due to their length and\ndomain-specificity. Furthermore, the key rights and duties requiring review\nvary for each contracting party. In this work, we propose a new task of\nparty-specific extractive summarization for legal contracts to facilitate\nfaster reviewing and improved comprehension of rights and duties. To facilitate\nthis, we curate a dataset comprising of party-specific pairwise importance\ncomparisons annotated by legal experts, covering ~293K sentence pairs that\ninclude obligations, entitlements, and prohibitions extracted from lease\nagreements. Using this dataset, we train a pairwise importance ranker and\npropose a pipeline-based extractive summarization system that generates a\nparty-specific contract summary. We establish the need for incorporating\ndomain-specific notion of importance during summarization by comparing our\nsystem against various baselines using both automatic and human evaluation\nmethods", "published": "2022-12-19 19:53:14", "link": "http://arxiv.org/abs/2212.09825v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Hybrid and Ensemble Models for Multiclass Prediction of Mental\n  Health Status on Social Media", "abstract": "In recent years, there has been a surge of interest in research on automatic\nmental health detection (MHD) from social media data leveraging advances in\nnatural language processing and machine learning techniques. While significant\nprogress has been achieved in this interdisciplinary research area, the vast\nmajority of work has treated MHD as a binary classification task. The\nmulticlass classification setup is, however, essential if we are to uncover the\nsubtle differences among the statistical patterns of language use associated\nwith particular mental health conditions. Here, we report on experiments aimed\nat predicting six conditions (anxiety, attention deficit hyperactivity\ndisorder, bipolar disorder, post-traumatic stress disorder, depression, and\npsychological stress) from Reddit social media posts. We explore and compare\nthe performance of hybrid and ensemble models leveraging transformer-based\narchitectures (BERT and RoBERTa) and BiLSTM neural networks trained on\nwithin-text distributions of a diverse set of linguistic features. This set\nencompasses measures of syntactic complexity, lexical sophistication and\ndiversity, readability, and register-specific ngram frequencies, as well as\nsentiment and emotion lexicons. In addition, we conduct feature ablation\nexperiments to investigate which types of features are most indicative of\nparticular mental health conditions.", "published": "2022-12-19 20:31:47", "link": "http://arxiv.org/abs/2212.09839v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "(Psycho-)Linguistic Features Meet Transformer Models for Improved\n  Explainable and Controllable Text Simplification", "abstract": "State-of-the-art text simplification (TS) systems adopt end-to-end neural\nnetwork models to directly generate the simplified version of the input text,\nand usually function as a blackbox. Moreover, TS is usually treated as an\nall-purpose generic task under the assumption of homogeneity, where the same\nsimplification is suitable for all. In recent years, however, there has been\nincreasing recognition of the need to adapt the simplification techniques to\nthe specific needs of different target groups. In this work, we aim to advance\ncurrent research on explainable and controllable TS in two ways: First,\nbuilding on recently proposed work to increase the transparency of TS systems,\nwe use a large set of (psycho-)linguistic features in combination with\npre-trained language models to improve explainable complexity prediction.\nSecond, based on the results of this preliminary task, we extend a\nstate-of-the-art Seq2Seq TS model, ACCESS, to enable explicit control of ten\nattributes. The results of experiments show (1) that our approach improves the\nperformance of state-of-the-art models for predicting explainable complexity\nand (2) that explicitly conditioning the Seq2Seq model on ten attributes leads\nto a significant improvement in performance in both within-domain and\nout-of-domain settings.", "published": "2022-12-19 20:46:21", "link": "http://arxiv.org/abs/2212.09848v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MANTIS at TSAR-2022 Shared Task: Improved Unsupervised Lexical\n  Simplification with Pretrained Encoders", "abstract": "In this paper we present our contribution to the TSAR-2022 Shared Task on\nLexical Simplification of the EMNLP 2022 Workshop on Text Simplification,\nAccessibility, and Readability. Our approach builds on and extends the\nunsupervised lexical simplification system with pretrained encoders (LSBert)\nsystem in the following ways: For the subtask of simplification candidate\nselection, it utilizes a RoBERTa transformer language model and expands the\nsize of the generated candidate list. For subsequent substitution ranking, it\nintroduces a new feature weighting scheme and adopts a candidate filtering\nmethod based on textual entailment to maximize semantic similarity between the\ntarget word and its simplification. Our best-performing system improves LSBert\nby 5.9% accuracy and achieves second place out of 33 ranked solutions.", "published": "2022-12-19 20:57:45", "link": "http://arxiv.org/abs/2212.09855v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Contradictory COVID-19 Drug Efficacy Claims from Biomedical\n  Literature", "abstract": "The COVID-19 pandemic created a deluge of questionable and contradictory\nscientific claims about drug efficacy -- an \"infodemic\" with lasting\nconsequences for science and society. In this work, we argue that NLP models\ncan help domain experts distill and understand the literature in this complex,\nhigh-stakes area. Our task is to automatically identify contradictory claims\nabout COVID-19 drug efficacy. We frame this as a natural language inference\nproblem and offer a new NLI dataset created by domain experts. The NLI framing\nallows us to create curricula combining existing datasets and our own. The\nresulting models are useful investigative tools. We provide a case study of how\nthese models help a domain expert summarize and assess evidence concerning\nremdisivir and hydroxychloroquine.", "published": "2022-12-19 21:37:36", "link": "http://arxiv.org/abs/2212.09867v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comparative Study on Textual Saliency of Styles from Eye Tracking,\n  Annotations, and Language Models", "abstract": "There is growing interest in incorporating eye-tracking data and other\nimplicit measures of human language processing into natural language processing\n(NLP) pipelines. The data from human language processing contain unique insight\ninto human linguistic understanding that could be exploited by language models.\nHowever, many unanswered questions remain about the nature of this data and how\nit can best be utilized in downstream NLP tasks. In this paper, we present\neyeStyliency, an eye-tracking dataset for human processing of stylistic text\n(e.g., politeness). We develop a variety of methods to derive style saliency\nscores over text using the collected eye dataset. We further investigate how\nthis saliency data compares to both human annotation methods and model-based\ninterpretability metrics. We find that while eye-tracking data is unique, it\nalso intersects with both human annotations and model-based importance scores,\nproviding a possible bridge between human- and machine-based perspectives. We\npropose utilizing this type of data to evaluate the cognitive plausibility of\nmodels that interpret style. Our eye-tracking data and processing code are\npublicly available.", "published": "2022-12-19 21:50:36", "link": "http://arxiv.org/abs/2212.09873v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsigned Play by Milan Kundera? An Authorship Attribution Study", "abstract": "In addition to being a widely recognised novelist, Milan Kundera has also\nauthored three pieces for theatre: The Owners of the Keys (Majitel\\'e\nkl\\'i\\v{c}\\r{u}, 1961), The Blunder (Pt\\'akovina, 1967), and Jacques and his\nMaster (Jakub a jeho p\\'an, 1971). In recent years, however, the hypothesis has\nbeen raised that Kundera is the true author of a fourth play: Juro\nJ\\'ano\\v{s}\\'ik, first performed in a 1974 production under the name of Karel\nSteigerwald, who was Kundera's student at the time. In this study, we make use\nof supervised machine learning to settle the question of authorship attribution\nin the case of Juro J\\'ano\\v{s}\\'ik, with results strongly supporting the\nhypothesis of Kundera's authorship.", "published": "2022-12-19 21:59:22", "link": "http://arxiv.org/abs/2212.09879v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Python Code Generation by Asking Clarification Questions", "abstract": "Code generation from text requires understanding the user's intent from a\nnatural language description and generating an executable code snippet that\nsatisfies this intent. While recent pretrained language models demonstrate\nremarkable performance for this task, these models fail when the given natural\nlanguage description is under-specified. In this work, we introduce a novel and\nmore realistic setup for this task. We hypothesize that the under-specification\nof a natural language description can be resolved by asking clarification\nquestions. Therefore, we collect and introduce a new dataset named CodeClarQA\ncontaining pairs of natural language descriptions and code with created\nsynthetic clarification questions and answers. The empirical results of our\nevaluation of pretrained language model performance on code generation show\nthat clarifications result in more precisely generated code, as shown by the\nsubstantial improvement of model performance in all evaluation metrics.\nAlongside this, our task and dataset introduce new challenges to the community,\nincluding when and what clarification questions should be asked. Our code and\ndataset are available on GitHub.", "published": "2022-12-19 22:08:36", "link": "http://arxiv.org/abs/2212.09885v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Long-Form Spoken Language Translation with Large Language\n  Models", "abstract": "A challenge in spoken language translation is that plenty of spoken content\nis long-form, but short units are necessary for obtaining high-quality\ntranslations. To address this mismatch, we fine-tune a general-purpose, large\nlanguage model to split long ASR transcripts into segments that can be\nindependently translated so as to maximize the overall translation quality. We\ncompare to several segmentation strategies and find that our approach improves\nBLEU score on three languages by an average of 2.7 BLEU overall compared to an\nautomatic punctuation baseline. Further, we demonstrate the effectiveness of\ntwo constrained decoding strategies to improve well-formedness of the model\noutput from above 99% to 100%.", "published": "2022-12-19 22:36:53", "link": "http://arxiv.org/abs/2212.09895v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inducing Character-level Structure in Subword-based Language Models with\n  Type-level Interchange Intervention Training", "abstract": "Language tasks involving character-level manipulations (e.g., spelling\ncorrections, arithmetic operations, word games) are challenging for models\noperating on subword units. To address this, we develop a causal intervention\nframework to learn robust and interpretable character representations inside\nsubword-based language models. Our method treats each character as a typed\nvariable in a causal model and learns such causal structures by adapting the\ninterchange intervention training method of Geiger et al. (2021). We\nadditionally introduce a suite of character-level tasks that systematically\nvary in their dependence on meaning and sequence-level context. While\ncharacter-level models still perform best on purely form-based tasks like\nstring reversal, our method outperforms character-level models on more complex\ntasks that blend form, meaning, and context, such as spelling correction in\ncontext and word search games. Compared with standard subword-based models, our\napproach also significantly improves robustness on unseen token sequences and\nleads to human-interpretable internal representations of characters.", "published": "2022-12-19 22:37:46", "link": "http://arxiv.org/abs/2212.09897v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tokenization Consistency Matters for Generative Models on Extractive NLP\n  Tasks", "abstract": "Generative models have been widely applied to solve extractive tasks, where\nparts of the input is extracted to form the desired output, and achieved\nsignificant success. For example, in extractive question answering (QA),\ngenerative models have constantly yielded state-of-the-art results. In this\nwork, we identify the issue of tokenization inconsistency that is commonly\nneglected in training these models. This issue damages the extractive nature of\nthese tasks after the input and output are tokenized inconsistently by the\ntokenizer, and thus leads to performance drop as well as hallucination. We\npropose a simple yet effective fix to this issue and conduct a case study on\nextractive QA. We show that, with consistent tokenization, the model performs\nbetter in both in-domain and out-of-domain datasets, with a notable average of\n+1.7 F2 gain when a BART model is trained on SQuAD and evaluated on 8 QA\ndatasets. Further, the model converges faster, and becomes less likely to\ngenerate out-of-context answers. With these findings, we would like to call for\nmore attention on how tokenization should be done when solving extractive tasks\nand recommend applying consistent tokenization during training.", "published": "2022-12-19 23:33:21", "link": "http://arxiv.org/abs/2212.09912v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inverse Reinforcement Learning for Text Summarization", "abstract": "We introduce inverse reinforcement learning (IRL) as an effective paradigm\nfor training abstractive summarization models, imitating human summarization\nbehaviors. Our IRL model estimates the reward function using a suite of\nimportant sub-rewards for summarization and concurrently optimizes the policy\nnetwork. Experimental results across datasets in different domains\n(CNN/DailyMail and WikiHow) and various model sizes (BART-base and BART-large)\ndemonstrate the superiority of our proposed IRL model for summarization over\nMLE and RL baselines. The resulting summaries exhibit greater similarity to\nhuman-crafted gold references, outperforming MLE and RL baselines on metrics\nsuch as ROUGE, coverage, novelty, compression ratio, factuality, and human\nevaluations.", "published": "2022-12-19 23:45:05", "link": "http://arxiv.org/abs/2212.09917v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emergent Analogical Reasoning in Large Language Models", "abstract": "The recent advent of large language models has reinvigorated debate over\nwhether human cognitive capacities might emerge in such generic models given\nsufficient training data. Of particular interest is the ability of these models\nto reason about novel problems zero-shot, without any direct training. In human\ncognition, this capacity is closely tied to an ability to reason by analogy.\nHere, we performed a direct comparison between human reasoners and a large\nlanguage model (the text-davinci-003 variant of GPT-3) on a range of analogical\ntasks, including a non-visual matrix reasoning task based on the rule structure\nof Raven's Standard Progressive Matrices. We found that GPT-3 displayed a\nsurprisingly strong capacity for abstract pattern induction, matching or even\nsurpassing human capabilities in most settings; preliminary tests of GPT-4\nindicated even better performance. Our results indicate that large language\nmodels such as GPT-3 have acquired an emergent ability to find zero-shot\nsolutions to a broad range of analogy problems.", "published": "2022-12-19 00:04:56", "link": "http://arxiv.org/abs/2212.09196v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Natural Language to Code Generation in Interactive Data Science\n  Notebooks", "abstract": "Computational notebooks, such as Jupyter notebooks, are interactive computing\nenvironments that are ubiquitous among data scientists to perform data\nwrangling and analytic tasks. To measure the performance of AI pair programmers\nthat automatically synthesize programs for those tasks given natural language\n(NL) intents from users, we build ARCADE, a benchmark of 1082 code generation\nproblems using the pandas data analysis framework in data science notebooks.\nARCADE features multiple rounds of NL-to-code problems from the same notebook.\nIt requires a model to understand rich multi-modal contexts, such as existing\nnotebook cells and their execution states as well as previous turns of\ninteraction. To establish a strong baseline on this challenging task, we\ndevelop PaChiNCo, a 62B code language model (LM) for Python computational\nnotebooks, which significantly outperforms public code LMs. Finally, we explore\nfew-shot prompting strategies to elicit better code with step-by-step\ndecomposition and NL explanation, showing the potential to improve the\ndiversity and explainability of model predictions.", "published": "2022-12-19 05:06:00", "link": "http://arxiv.org/abs/2212.09248v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Mind the Knowledge Gap: A Survey of Knowledge-enhanced Dialogue Systems", "abstract": "Many dialogue systems (DSs) lack characteristics humans have, such as emotion\nperception, factuality, and informativeness. Enhancing DSs with knowledge\nalleviates this problem, but, as many ways of doing so exist, keeping track of\nall proposed methods is difficult. Here, we present the first survey of\nknowledge-enhanced DSs. We define three categories of systems - internal,\nexternal, and hybrid - based on the knowledge they use. We survey the\nmotivation for enhancing DSs with knowledge, used datasets, and methods for\nknowledge search, knowledge encoding, and knowledge incorporation. Finally, we\npropose how to improve existing systems based on theories from linguistics and\ncognitive science.", "published": "2022-12-19 05:17:33", "link": "http://arxiv.org/abs/2212.09252v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MIGA: A Unified Multi-task Generation Framework for Conversational\n  Text-to-SQL", "abstract": "Conversational text-to-SQL is designed to translate multi-turn natural\nlanguage questions into their corresponding SQL queries. Most state-of-the-art\nconversational text- to-SQL methods are incompatible with generative\npre-trained language models (PLMs), such as T5. In this paper, we present a\ntwo-stage unified MultI-task Generation frAmework (MIGA) that leverages PLMs'\nability to tackle conversational text-to-SQL. In the pre-training stage, MIGA\nfirst decomposes the main task into several related sub-tasks and then unifies\nthem into the same sequence-to-sequence (Seq2Seq) paradigm with task-specific\nnatural language prompts to boost the main task from multi-task training. Later\nin the fine-tuning stage, we propose four SQL perturbations to alleviate the\nerror propagation problem. MIGA tends to achieve state-of-the-art performance\non two benchmarks (SparC and CoSQL). We also provide extensive analyses and\ndiscussions to shed light on some new perspectives for conversational\ntext-to-SQL.", "published": "2022-12-19 07:14:32", "link": "http://arxiv.org/abs/2212.09278v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChatGPT: The End of Online Exam Integrity?", "abstract": "This study evaluated the ability of ChatGPT, a recently developed artificial\nintelligence (AI) agent, to perform high-level cognitive tasks and produce text\nthat is indistinguishable from human-generated text. This capacity raises\nconcerns about the potential use of ChatGPT as a tool for academic misconduct\nin online exams. The study found that ChatGPT is capable of exhibiting critical\nthinking skills and generating highly realistic text with minimal input, making\nit a potential threat to the integrity of online exams, particularly in\ntertiary education settings where such exams are becoming more prevalent.\nReturning to invigilated and oral exams could form part of the solution, while\nusing advanced proctoring techniques and AI-text output detectors may be\neffective in addressing this issue, they are not likely to be foolproof\nsolutions. Further research is needed to fully understand the implications of\nlarge language models like ChatGPT and to devise strategies for combating the\nrisk of cheating using these tools. It is crucial for educators and\ninstitutions to be aware of the possibility of ChatGPT being used for cheating\nand to investigate measures to address it in order to maintain the fairness and\nvalidity of online exams for all students.", "published": "2022-12-19 08:15:16", "link": "http://arxiv.org/abs/2212.09292v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "An Extensible Plug-and-Play Method for Multi-Aspect Controllable Text\n  Generation", "abstract": "Recently, multi-aspect controllable text generation that controls the\ngenerated text in multiple aspects (e.g., sentiment, topic, and keywords) has\nattracted increasing attention. Although methods based on parameter efficient\ntuning like prefix-tuning could achieve multi-aspect controlling in a\nplug-and-play way, the mutual interference of multiple prefixes leads to\nsignificant degeneration of constraints and limits their extensibility to\ntraining-time unseen aspect combinations. In this work, we provide a\ntheoretical lower bound for the interference and empirically found that the\ninterference grows with the number of layers where prefixes are inserted. Based\non these analyses, we propose using trainable gates to normalize the\nintervention of prefixes to restrain the growing interference. As a result,\ncontrolling training-time unseen combinations of aspects can be realized by\nsimply concatenating corresponding plugins such that new constraints can be\nextended at a lower cost. In addition, we propose a unified way to process both\ncategorical and free-form constraints. Experiments on text generation and\nmachine translation demonstrate the superiority of our approach over baselines\non constraint accuracy, text quality, and extensibility.", "published": "2022-12-19 11:53:59", "link": "http://arxiv.org/abs/2212.09387v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Latent Diffusion for Language Generation", "abstract": "Diffusion models have achieved great success in modeling continuous data\nmodalities such as images, audio, and video, but have seen limited use in\ndiscrete domains such as language. Recent attempts to adapt diffusion to\nlanguage have presented diffusion as an alternative to existing pretrained\nlanguage models. We view diffusion and existing language models as\ncomplementary. We demonstrate that encoder-decoder language models can be\nutilized to efficiently learn high-quality language autoencoders. We then\ndemonstrate that continuous diffusion models can be learned in the latent space\nof the language autoencoder, enabling us to sample continuous latent\nrepresentations that can be decoded into natural language with the pretrained\ndecoder. We validate the effectiveness of our approach for unconditional,\nclass-conditional, and sequence-to-sequence language generation. We demonstrate\nacross multiple diverse data sets that our latent language diffusion models are\nsignificantly more effective than previous diffusion language models.", "published": "2022-12-19 13:57:06", "link": "http://arxiv.org/abs/2212.09462v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models are Better Reasoners with Self-Verification", "abstract": "Recently, with the chain of thought (CoT) prompting, large language models\n(LLMs), e.g., GPT-3, have shown strong reasoning ability in several natural\nlanguage processing tasks such as arithmetic, commonsense, and logical\nreasoning. However, LLMs with CoT require multi-step prompting and multi-token\nprediction, which is highly sensitive to individual mistakes and vulnerable to\nerror accumulation. The above issues make the LLMs need the ability to verify\nthe answers. In fact, after inferring conclusions in some thinking decision\ntasks, people often check them by re-verifying steps to avoid some mistakes. In\nthis paper, we propose and prove that LLMs also have similar self-verification\nabilities. We take the conclusion obtained by CoT as one of the conditions for\nsolving the original problem. By performing a backward verification of the\nanswers that LLM deduced for itself, we can obtain interpretable answer\nvalidation scores to select the candidate answer with the highest score.\nExperimental results demonstrate that the proposed method can improve the\nreasoning performance on various arithmetic, commonsense, and logical reasoning\ndatasets. Our code is publicly available at:\nhttps://github.com/WENGSYX/Self-Verification.", "published": "2022-12-19 15:51:52", "link": "http://arxiv.org/abs/2212.09561v5", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Optimizing Prompts for Text-to-Image Generation", "abstract": "Well-designed prompts can guide text-to-image models to generate amazing\nimages. However, the performant prompts are often model-specific and misaligned\nwith user input. Instead of laborious human engineering, we propose prompt\nadaptation, a general framework that automatically adapts original user input\nto model-preferred prompts. Specifically, we first perform supervised\nfine-tuning with a pretrained language model on a small collection of manually\nengineered prompts. Then we use reinforcement learning to explore better\nprompts. We define a reward function that encourages the policy to generate\nmore aesthetically pleasing images while preserving the original user\nintentions. Experimental results on Stable Diffusion show that our method\noutperforms manual prompt engineering in terms of both automatic metrics and\nhuman preference ratings. Moreover, reinforcement learning further boosts\nperformance, especially on out-of-domain prompts. The pretrained checkpoints\nare available at https://aka.ms/promptist. The demo can be found at\nhttps://aka.ms/promptist-demo.", "published": "2022-12-19 16:50:41", "link": "http://arxiv.org/abs/2212.09611v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Wukong-Reader: Multi-modal Pre-training for Fine-grained Visual Document\n  Understanding", "abstract": "Unsupervised pre-training on millions of digital-born or scanned documents\nhas shown promising advances in visual document understanding~(VDU). While\nvarious vision-language pre-training objectives are studied in existing\nsolutions, the document textline, as an intrinsic granularity in VDU, has\nseldom been explored so far. A document textline usually contains words that\nare spatially and semantically correlated, which can be easily obtained from\nOCR engines. In this paper, we propose Wukong-Reader, trained with new\npre-training objectives to leverage the structural knowledge nested in document\ntextlines. We introduce textline-region contrastive learning to achieve\nfine-grained alignment between the visual regions and texts of document\ntextlines. Furthermore, masked region modeling and textline-grid matching are\nalso designed to enhance the visual and layout representations of textlines.\nExperiments show that our Wukong-Reader has superior performance on various VDU\ntasks such as information extraction. The fine-grained alignment over textlines\nalso empowers Wukong-Reader with promising localization ability.", "published": "2022-12-19 17:00:54", "link": "http://arxiv.org/abs/2212.09621v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Optimal Transport for Unsupervised Hallucination Detection in Neural\n  Machine Translation", "abstract": "Neural machine translation (NMT) has become the de-facto standard in\nreal-world machine translation applications. However, NMT models can\nunpredictably produce severely pathological translations, known as\nhallucinations, that seriously undermine user trust. It becomes thus crucial to\nimplement effective preventive strategies to guarantee their proper\nfunctioning. In this paper, we address the problem of hallucination detection\nin NMT by following a simple intuition: as hallucinations are detached from the\nsource content, they exhibit encoder-decoder attention patterns that are\nstatistically different from those of good quality translations. We frame this\nproblem with an optimal transport formulation and propose a fully unsupervised,\nplug-in detector that can be used with any attention-based NMT model.\nExperimental results show that our detector not only outperforms all previous\nmodel-based detectors, but is also competitive with detectors that employ large\nmodels trained on millions of samples.", "published": "2022-12-19 17:06:58", "link": "http://arxiv.org/abs/2212.09631v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NusaCrowd: Open Source Initiative for Indonesian NLP Resources", "abstract": "We present NusaCrowd, a collaborative initiative to collect and unify\nexisting resources for Indonesian languages, including opening access to\npreviously non-public resources. Through this initiative, we have brought\ntogether 137 datasets and 118 standardized data loaders. The quality of the\ndatasets has been assessed manually and automatically, and their value is\ndemonstrated through multiple experiments. NusaCrowd's data collection enables\nthe creation of the first zero-shot benchmarks for natural language\nunderstanding and generation in Indonesian and the local languages of\nIndonesia. Furthermore, NusaCrowd brings the creation of the first multilingual\nautomatic speech recognition benchmark in Indonesian and the local languages of\nIndonesia. Our work strives to advance natural language processing (NLP)\nresearch for languages that are under-represented despite being widely spoken.", "published": "2022-12-19 17:28:22", "link": "http://arxiv.org/abs/2212.09648v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Visconde: Multi-document QA with GPT-3 and Neural Reranking", "abstract": "This paper proposes a question-answering system that can answer questions\nwhose supporting evidence is spread over multiple (potentially long) documents.\nThe system, called Visconde, uses a three-step pipeline to perform the task:\ndecompose, retrieve, and aggregate. The first step decomposes the question into\nsimpler questions using a few-shot large language model (LLM). Then, a\nstate-of-the-art search engine is used to retrieve candidate passages from a\nlarge collection for each decomposed question. In the final step, we use the\nLLM in a few-shot setting to aggregate the contents of the passages into the\nfinal answer. The system is evaluated on three datasets: IIRC, Qasper, and\nStrategyQA. Results suggest that current retrievers are the main bottleneck and\nthat readers are already performing at the human level as long as relevant\npassages are provided. The system is also shown to be more effective when the\nmodel is induced to give explanations before answering a question. Code is\navailable at \\url{https://github.com/neuralmind-ai/visconde}.", "published": "2022-12-19 17:39:07", "link": "http://arxiv.org/abs/2212.09656v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Words as Gatekeepers: Measuring Discipline-specific Terms and Meanings\n  in Scholarly Publications", "abstract": "Scholarly text is often laden with jargon, or specialized language that can\nfacilitate efficient in-group communication within fields but hinder\nunderstanding for out-groups. In this work, we develop and validate an\ninterpretable approach for measuring scholarly jargon from text. Expanding the\nscope of prior work which focuses on word types, we use word sense induction to\nalso identify words that are widespread but overloaded with different meanings\nacross fields. We then estimate the prevalence of these discipline-specific\nwords and senses across hundreds of subfields, and show that word senses\nprovide a complementary, yet unique view of jargon alongside word types. We\ndemonstrate the utility of our metrics for science of science and computational\nsociolinguistics by highlighting two key social implications. First, though\nmost fields reduce their use of jargon when writing for general-purpose venues,\nand some fields (e.g., biological sciences) do so less than others. Second, the\ndirection of correlation between jargon and citation rates varies among fields,\nbut jargon is nearly always negatively correlated with interdisciplinary\nimpact. Broadly, our findings suggest that though multidisciplinary venues\nintend to cater to more general audiences, some fields' writing norms may act\nas barriers rather than bridges, and thus impede the dispersion of scholarly\nideas.", "published": "2022-12-19 18:01:06", "link": "http://arxiv.org/abs/2212.09676v2", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Graph-based Semantical Extractive Text Analysis", "abstract": "In the past few decades, there has been an explosion in the amount of\navailable data produced from various sources with different topics. The\navailability of this enormous data necessitates us to adopt effective\ncomputational tools to explore the data. This leads to an intense growing\ninterest in the research community to develop computational methods focused on\nprocessing this text data. A line of study focused on condensing the text so\nthat we are able to get a higher level of understanding in a shorter time. The\ntwo important tasks to do this are keyword extraction and text summarization.\nIn keyword extraction, we are interested in finding the key important words\nfrom a text. This makes us familiar with the general topic of a text. In text\nsummarization, we are interested in producing a short-length text which\nincludes important information about the document. The TextRank algorithm, an\nunsupervised learning method that is an extension of the PageRank (algorithm\nwhich is the base algorithm of Google search engine for searching pages and\nranking them) has shown its efficacy in large-scale text mining, especially for\ntext summarization and keyword extraction. this algorithm can automatically\nextract the important parts of a text (keywords or sentences) and declare them\nas the result. However, this algorithm neglects the semantic similarity between\nthe different parts. In this work, we improved the results of the TextRank\nalgorithm by incorporating the semantic similarity between parts of the text.\nAside from keyword extraction and text summarization, we develop a topic\nclustering algorithm based on our framework which can be used individually or\nas a part of generating the summary to overcome coverage problems.", "published": "2022-12-19 18:30:26", "link": "http://arxiv.org/abs/2212.09701v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Retrieve-and-Read Framework for Knowledge Graph Link Prediction", "abstract": "Knowledge graph (KG) link prediction aims to infer new facts based on\nexisting facts in the KG. Recent studies have shown that using the graph\nneighborhood of a node via graph neural networks (GNNs) provides more useful\ninformation compared to just using the query information. Conventional GNNs for\nKG link prediction follow the standard message-passing paradigm on the entire\nKG, which leads to superfluous computation, over-smoothing of node\nrepresentations, and also limits their expressive power. On a large scale, it\nbecomes computationally expensive to aggregate useful information from the\nentire KG for inference. To address the limitations of existing KG link\nprediction frameworks, we propose a novel retrieve-and-read framework, which\nfirst retrieves a relevant subgraph context for the query and then jointly\nreasons over the context and the query with a high-capacity reader. As part of\nour exemplar instantiation for the new framework, we propose a novel\nTransformer-based GNN as the reader, which incorporates graph-based attention\nstructure and cross-attention between query and context for deep fusion. This\nsimple yet effective design enables the model to focus on salient context\ninformation relevant to the query. Empirical results on two standard KG link\nprediction datasets demonstrate the competitive performance of the proposed\nmethod. Furthermore, our analysis yields valuable insights for designing\nimproved retrievers within the framework.", "published": "2022-12-19 18:50:54", "link": "http://arxiv.org/abs/2212.09724v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Don't Generate, Discriminate: A Proposal for Grounding Language Models\n  to Real-World Environments", "abstract": "A key missing capacity of current language models (LMs) is grounding to\nreal-world environments. Most existing work for grounded language understanding\nuses LMs to directly generate plans that can be executed in the environment to\nachieve the desired effects. It thereby casts the burden of ensuring\ngrammaticality, faithfulness, and controllability all on the LMs. We propose\nPangu, a generic framework for grounded language understanding that capitalizes\non the discriminative ability of LMs instead of their generative ability. Pangu\nconsists of a symbolic agent and a neural LM working in a concerted fashion:\nThe agent explores the environment to incrementally construct valid plans, and\nthe LM evaluates the plausibility of the candidate plans to guide the search\nprocess. A case study on the challenging problem of knowledge base question\nanswering (KBQA), which features a massive environment, demonstrates the\nremarkable effectiveness and flexibility of Pangu: A BERT-base LM is sufficient\nfor setting a new record on standard KBQA datasets, and larger LMs further\nbring substantial gains. Pangu also enables, for the first time, effective\nfew-shot in-context learning for KBQA with large LMs such as Codex.", "published": "2022-12-19 18:55:21", "link": "http://arxiv.org/abs/2212.09736v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Human-in-the-loop Abstractive Dialogue Summarization", "abstract": "Abstractive dialogue summarization has received increasing attention\nrecently. Despite the fact that most of the current dialogue summarization\nsystems are trained to maximize the likelihood of human-written summaries and\nhave achieved significant results, there is still a huge gap in generating\nhigh-quality summaries as determined by humans, such as coherence and\nfaithfulness, partly due to the misalignment in maximizing a single\nhuman-written summary. To this end, we propose to incorporate different levels\nof human feedback into the training process. This will enable us to guide the\nmodels to capture the behaviors humans care about for summaries. Specifically,\nwe ask humans to highlight the salient information to be included in summaries\nto provide the local feedback , and to make overall comparisons among summaries\nin terms of coherence, accuracy, coverage, concise and overall quality, as the\nglobal feedback. We then combine both local and global feedback to fine-tune\nthe dialog summarization policy with Reinforcement Learning. Experiments\nconducted on multiple datasets demonstrate the effectiveness and generalization\nof our methods over the state-of-the-art supervised baselines, especially in\nterms of human judgments.", "published": "2022-12-19 19:11:27", "link": "http://arxiv.org/abs/2212.09750v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dataless Knowledge Fusion by Merging Weights of Language Models", "abstract": "Fine-tuning pre-trained language models has become the prevalent paradigm for\nbuilding downstream NLP models. Oftentimes fine-tuned models are readily\navailable but their training data is not, due to data privacy or intellectual\nproperty concerns. This creates a barrier to fusing knowledge across individual\nmodels to yield a better single model. In this paper, we study the problem of\nmerging individual models built on different training data sets to obtain a\nsingle model that performs well both across all data set domains and can\ngeneralize on out-of-domain data. We propose a dataless knowledge fusion method\nthat merges models in their parameter space, guided by weights that minimize\nprediction differences between the merged model and the individual models. Over\na battery of evaluation settings, we show that the proposed method\nsignificantly outperforms baselines such as Fisher-weighted averaging or model\nensembling. Further, we find that our method is a promising alternative to\nmulti-task learning that can preserve or sometimes improve over the individual\nmodels without access to the training data. Finally, model merging is more\nefficient than training a multi-task model, thus making it applicable to a\nwider set of scenarios.", "published": "2022-12-19 20:46:43", "link": "http://arxiv.org/abs/2212.09849v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Continuous Semi-Supervised Nonnegative Matrix Factorization", "abstract": "Nonnegative matrix factorization can be used to automatically detect topics\nwithin a corpus in an unsupervised fashion. The technique amounts to an\napproximation of a nonnegative matrix as the product of two nonnegative\nmatrices of lower rank. In this paper, we show this factorization can be\ncombined with regression on a continuous response variable. In practice, the\nmethod performs better than regression done after topics are identified and\nretrains interpretability.", "published": "2022-12-19 21:07:27", "link": "http://arxiv.org/abs/2212.09858v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Synthetic Pre-Training Tasks for Neural Machine Translation", "abstract": "Pre-training models with large crawled corpora can lead to issues such as\ntoxicity and bias, as well as copyright and privacy concerns. A promising way\nof alleviating such concerns is to conduct pre-training with synthetic tasks\nand data, since no real-world information is ingested by the model. Our goal in\nthis paper is to understand the factors that contribute to the effectiveness of\npre-training models when using synthetic resources, particularly in the context\nof neural machine translation. We propose several novel approaches to\npre-training translation models that involve different levels of lexical and\nstructural knowledge, including: 1) generating obfuscated data from a large\nparallel corpus 2) concatenating phrase pairs extracted from a small\nword-aligned corpus, and 3) generating synthetic parallel data without real\nhuman language corpora. Our experiments on multiple language pairs reveal that\npre-training benefits can be realized even with high levels of obfuscation or\npurely synthetic parallel data. We hope the findings from our comprehensive\nempirical analysis will shed light on understanding what matters for NMT\npre-training, as well as pave the way for the development of more efficient and\nless toxic models.", "published": "2022-12-19 21:34:00", "link": "http://arxiv.org/abs/2212.09864v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations", "abstract": "Although large language models can be prompted for both zero- and few-shot\nlearning, performance drops significantly when no demonstrations are available.\nIn this paper, we introduce Z-ICL, a new zero-shot method that closes the gap\nby constructing pseudo-demonstrations for a given test input using a raw text\ncorpus. Concretely, pseudo-demonstrations are constructed by (1) finding the\nnearest neighbors to the test input from the corpus and pairing them with\nrandom task labels, and (2) applying a set of techniques to reduce the amount\nof direct copying the model does from the resulting demonstrations. Evaluation\non nine classification datasets shows that Z-ICL outperforms previous zero-shot\nmethods by a significant margin, and is on par with in-context learning with\nlabeled training data in the few-shot setting. Overall, Z-ICL provides a\nsignificantly higher estimate of the zero-shot performance levels of a model,\nand supports future efforts to develop better pseudo-demonstrations that\nfurther improve zero-shot results.", "published": "2022-12-19 21:34:26", "link": "http://arxiv.org/abs/2212.09865v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Discovering Language Model Behaviors with Model-Written Evaluations", "abstract": "As language models (LMs) scale, they develop many novel behaviors, good and\nbad, exacerbating the need to evaluate how they behave. Prior work creates\nevaluations with crowdwork (which is time-consuming and expensive) or existing\ndata sources (which are not always available). Here, we automatically generate\nevaluations with LMs. We explore approaches with varying amounts of human\neffort, from instructing LMs to write yes/no questions to making complex\nWinogender schemas with multiple stages of LM-based generation and filtering.\nCrowdworkers rate the examples as highly relevant and agree with 90-100% of\nlabels, sometimes more so than corresponding human-written datasets. We\ngenerate 154 datasets and discover new cases of inverse scaling where LMs get\nworse with size. Larger LMs repeat back a dialog user's preferred answer\n(\"sycophancy\") and express greater desire to pursue concerning goals like\nresource acquisition and goal preservation. We also find some of the first\nexamples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF\nmakes LMs worse. For example, RLHF makes LMs express stronger political views\n(on gun rights and immigration) and a greater desire to avoid shut down.\nOverall, LM-written evaluations are high-quality and let us quickly discover\nmany novel LM behaviors.", "published": "2022-12-19 05:13:52", "link": "http://arxiv.org/abs/2212.09251v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "APOLLO: A Simple Approach for Adaptive Pretraining of Language Models\n  for Logical Reasoning", "abstract": "Logical reasoning of text is an important ability that requires understanding\nthe information present in the text, their interconnections, and then reasoning\nthrough them to infer new conclusions. Prior works on improving the logical\nreasoning ability of language models require complex processing of training\ndata (e.g., aligning symbolic knowledge to text), yielding task-specific data\naugmentation solutions that restrict the learning of general logical reasoning\nskills. In this work, we propose APOLLO, an adaptively pretrained language\nmodel that has improved logical reasoning abilities. We select a subset of\nWikipedia, based on a set of logical inference keywords, for continued\npretraining of a language model. We use two self-supervised loss functions: a\nmodified masked language modeling loss where only specific parts-of-speech\nwords, that would likely require more reasoning than basic language\nunderstanding, are masked, and a sentence-level classification loss that\nteaches the model to distinguish between entailment and contradiction types of\nsentences. The proposed training paradigm is both simple and independent of\ntask formats. We demonstrate the effectiveness of APOLLO by comparing it with\nprior baselines on two logical reasoning datasets. APOLLO performs comparably\non ReClor and outperforms baselines on LogiQA. The code base has been made\npublicly available.", "published": "2022-12-19 07:40:02", "link": "http://arxiv.org/abs/2212.09282v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WACO: Word-Aligned Contrastive Learning for Speech Translation", "abstract": "End-to-end Speech Translation (E2E ST) aims to directly translate source\nspeech into target text. Existing ST methods perform poorly when only extremely\nsmall speech-text data are available for training. We observe that an ST\nmodel's performance closely correlates with its embedding similarity between\nspeech and source transcript. In this paper, we propose Word-Aligned\nCOntrastive learning (WACO), a simple and effective method for extremely\nlow-resource speech-to-text translation. Our key idea is bridging word-level\nrepresentations for both speech and text modalities via contrastive learning.\nWe evaluate WACO and other methods on the MuST-C dataset, a widely used ST\nbenchmark, and on a low-resource direction Maltese-English from IWSLT 2023. Our\nexperiments demonstrate that WACO outperforms the best baseline by 9+ BLEU\npoints with only 1-hour parallel ST data. Code is available at\nhttps://github.com/owaski/WACO.", "published": "2022-12-19 10:49:35", "link": "http://arxiv.org/abs/2212.09359v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Enriching Relation Extraction with OpenIE", "abstract": "Relation extraction (RE) is a sub-discipline of information extraction (IE)\nwhich focuses on the prediction of a relational predicate from a\nnatural-language input unit (such as a sentence, a clause, or even a short\nparagraph consisting of multiple sentences and/or clauses). Together with\nnamed-entity recognition (NER) and disambiguation (NED), RE forms the basis for\nmany advanced IE tasks such as knowledge-base (KB) population and verification.\nIn this work, we explore how recent approaches for open information extraction\n(OpenIE) may help to improve the task of RE by encoding structured information\nabout the sentences' principal units, such as subjects, objects, verbal\nphrases, and adverbials, into various forms of vectorized (and hence\nunstructured) representations of the sentences. Our main conjecture is that the\ndecomposition of long and possibly convoluted sentences into multiple smaller\nclauses via OpenIE even helps to fine-tune context-sensitive language models\nsuch as BERT (and its plethora of variants) for RE. Our experiments over two\nannotated corpora, KnowledgeNet and FewRel, demonstrate the improved accuracy\nof our enriched models compared to existing RE approaches. Our best results\nreach 92% and 71% of F1 score for KnowledgeNet and FewRel, respectively,\nproving the effectiveness of our approach on competitive benchmarks.", "published": "2022-12-19 11:26:23", "link": "http://arxiv.org/abs/2212.09376v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Empowering Diffusion Models on the Embedding Space for Text Generation", "abstract": "Diffusion models have achieved state-of-the-art synthesis quality on both\nvisual and audio tasks, and recent works further adapt them to textual data by\ndiffusing on the embedding space. In this paper, we conduct systematic studies\nof the optimization challenges encountered with both the embedding space and\nthe denoising model, which have not been carefully explored. Firstly, the data\ndistribution is learnable for embeddings, which may lead to the collapse of the\nembedding space and unstable training. To alleviate this problem, we propose a\nnew objective called the anchor loss which is more efficient than previous\nmethods. Secondly, we find the noise levels of conventional schedules are\ninsufficient for training a desirable denoising model while introducing varying\ndegrees of degeneration in consequence. To address this challenge, we propose a\nnovel framework called noise rescaling. Based on the above analysis, we propose\nDifformer, an embedding diffusion model based on Transformer. Experiments on\nvarieties of seminal text generation tasks show the effectiveness of the\nproposed methods and the superiority of Difformer over previous\nstate-of-the-art embedding diffusion baselines.", "published": "2022-12-19 12:44:25", "link": "http://arxiv.org/abs/2212.09412v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models Meet NL2Code: A Survey", "abstract": "The task of generating code from a natural language description, or NL2Code,\nis considered a pressing and significant challenge in code intelligence. Thanks\nto the rapid development of pre-training techniques, surging large language\nmodels are being proposed for code, sparking the advances in NL2Code. To\nfacilitate further research and applications in this field, in this paper, we\npresent a comprehensive survey of 27 existing large language models for\nNL2Code, and also review benchmarks and metrics. We provide an intuitive\ncomparison of all existing models on the HumanEval benchmark. Through in-depth\nobservation and analysis, we provide some insights and conclude that the key\nfactors contributing to the success of large language models for NL2Code are\n\"Large Size, Premium Data, Expert Tuning\". In addition, we discuss challenges\nand opportunities regarding the gap between models and humans. We also create a\nwebsite https://nl2code.github.io to track the latest progress through\ncrowd-sourcing. To the best of our knowledge, this is the first survey of large\nlanguage models for NL2Code, and we believe it will contribute to the ongoing\ndevelopment of the field.", "published": "2022-12-19 12:55:32", "link": "http://arxiv.org/abs/2212.09420v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL"], "primary_category": "cs.SE"}
{"title": "BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting", "abstract": "The BLOOM model is a large publicly available multilingual language model,\nbut its pretraining was limited to 46 languages. To extend the benefits of\nBLOOM to other languages without incurring prohibitively large costs, it is\ndesirable to adapt BLOOM to new languages not seen during pretraining. In this\nwork, we apply existing language adaptation strategies to BLOOM and benchmark\nits zero-shot prompting performance on eight new languages in a\nresource-constrained setting. We find language adaptation to be effective at\nimproving zero-shot performance in new languages. Surprisingly, we find that\nadapter-based finetuning is more effective than continued pretraining for large\nmodels. In addition, we discover that prompting performance is not\nsignificantly affected by language specifics, such as the writing system. It is\nprimarily determined by the size of the language adaptation data. We also add\nnew languages to BLOOMZ, which is a multitask finetuned version of BLOOM\ncapable of following task instructions zero-shot. We find including a new\nlanguage in the multitask fine-tuning mixture to be the most effective method\nto teach BLOOMZ a new language. We conclude that with sufficient training data\nlanguage adaptation can generalize well to diverse languages. Our code is\navailable at https://github.com/bigscience-workshop/multilingual-modeling.", "published": "2022-12-19 15:24:45", "link": "http://arxiv.org/abs/2212.09535v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mu$^{2}$SLAM: Multitask, Multilingual Speech and Language Models", "abstract": "We present Mu$^{2}$SLAM, a multilingual sequence-to-sequence model\npre-trained jointly on unlabeled speech, unlabeled text and supervised data\nspanning Automatic Speech Recognition (ASR), Automatic Speech Translation (AST)\nand Machine Translation (MT), in over 100 languages. By leveraging a quantized\nrepresentation of speech as a target, Mu$^{2}$SLAM trains the speech-text\nmodels with a sequence-to-sequence masked denoising objective similar to T5 on\nthe decoder and a masked language modeling (MLM) objective on the encoder, for\nboth unlabeled speech and text, while utilizing the supervised tasks to improve\ncross-lingual and cross-modal representation alignment within the model. On\nCoVoST AST, Mu$^{2}$SLAM establishes a new state-of-the-art for models trained\non public datasets, improving on xx-en translation over the previous best by\n1.9 BLEU points and on en-xx translation by 1.1 BLEU points. On Voxpopuli ASR,\nour model matches the performance of an mSLAM model fine-tuned with an RNN-T\ndecoder, despite using a relatively weaker sequence-to-sequence architecture.\nOn text understanding tasks, our model improves by more than 6\\% over mSLAM on\nXNLI, getting closer to the performance of mT5 models of comparable capacity on\nXNLI and TydiQA, paving the way towards a single model for all speech and text\nunderstanding tasks.", "published": "2022-12-19 15:45:36", "link": "http://arxiv.org/abs/2212.09553v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MatCha: Enhancing Visual Language Pretraining with Math Reasoning and\n  Chart Derendering", "abstract": "Visual language data such as plots, charts, and infographics are ubiquitous\nin the human world. However, state-of-the-art vision-language models do not\nperform well on these data. We propose MatCha (Math reasoning and Chart\nderendering pretraining) to enhance visual language models' capabilities in\njointly modeling charts/plots and language data. Specifically, we propose\nseveral pretraining tasks that cover plot deconstruction and numerical\nreasoning which are the key capabilities in visual language modeling.\n  We perform the MatCha pretraining starting from Pix2Struct, a recently\nproposed image-to-text visual language model. On standard benchmarks such as\nPlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods by as\nmuch as nearly 20%. We also examine how well MatCha pretraining transfers to\ndomains such as screenshots, textbook diagrams, and document figures and\nobserve overall improvement, verifying the usefulness of MatCha pretraining on\nbroader visual language tasks.", "published": "2022-12-19 17:44:54", "link": "http://arxiv.org/abs/2212.09662v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Foveate, Attribute, and Rationalize: Towards Physically Safe and\n  Trustworthy AI", "abstract": "Users' physical safety is an increasing concern as the market for intelligent\nsystems continues to grow, where unconstrained systems may recommend users\ndangerous actions that can lead to serious injury. Covertly unsafe text is an\narea of particular interest, as such text may arise from everyday scenarios and\nare challenging to detect as harmful. We propose FARM, a novel framework\nleveraging external knowledge for trustworthy rationale generation in the\ncontext of safety. In particular, FARM foveates on missing knowledge to qualify\nthe information required to reason in specific scenarios and retrieves this\ninformation with attribution to trustworthy sources. This knowledge is used to\nboth classify the safety of the original text and generate human-interpretable\nrationales, shedding light on the risk of systems to specific user groups and\nhelping both stakeholders manage the risks of their systems and policymakers to\nprovide concrete safeguards for consumer safety. Our experiments show that FARM\nobtains state-of-the-art results on the SafeText dataset, showing absolute\nimprovement in safety classification accuracy by 5.9%.", "published": "2022-12-19 17:51:47", "link": "http://arxiv.org/abs/2212.09667v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unnatural Instructions: Tuning Language Models with (Almost) No Human\n  Labor", "abstract": "Instruction tuning enables pretrained language models to perform new tasks\nfrom inference-time natural language descriptions. These approaches rely on\nvast amounts of human supervision in the form of crowdsourced datasets or user\ninteractions. In this work, we introduce Unnatural Instructions: a large\ndataset of creative and diverse instructions, collected with virtually no human\nlabor. We collect 64,000 examples by prompting a language model with three seed\nexamples of instructions and eliciting a fourth. This set is then expanded by\nprompting the model to rephrase each instruction, creating a total of\napproximately 240,000 examples of instructions, inputs, and outputs.\nExperiments show that despite containing a fair amount of noise, training on\nUnnatural Instructions rivals the effectiveness of training on open-source\nmanually-curated datasets, surpassing the performance of models such as T0++\nand Tk-Instruct across various benchmarks. These results demonstrate the\npotential of model-generated data as a cost-effective alternative to\ncrowdsourcing for dataset expansion and diversification.", "published": "2022-12-19 18:21:00", "link": "http://arxiv.org/abs/2212.09689v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SegAugment: Maximizing the Utility of Speech Translation Data with\n  Segmentation-based Augmentations", "abstract": "End-to-end Speech Translation is hindered by a lack of available data\nresources. While most of them are based on documents, a sentence-level version\nis available, which is however single and static, potentially impeding the\nusefulness of the data. We propose a new data augmentation strategy,\nSegAugment, to address this issue by generating multiple alternative\nsentence-level versions of a dataset. Our method utilizes an Audio Segmentation\nsystem, which re-segments the speech of each document with different length\nconstraints, after which we obtain the target text via alignment methods.\nExperiments demonstrate consistent gains across eight language pairs in MuST-C,\nwith an average increase of 2.5 BLEU points, and up to 5 BLEU for low-resource\nscenarios in mTEDx. Furthermore, when combined with a strong system, SegAugment\nestablishes new state-of-the-art results in MuST-C. Finally, we show that the\nproposed method can also successfully augment sentence-level datasets, and that\nit enables Speech Translation models to close the gap between the manual and\nautomatic segmentation at inference time.", "published": "2022-12-19 18:29:31", "link": "http://arxiv.org/abs/2212.09699v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "On Event Individuation for Document-Level Information Extraction", "abstract": "As information extraction (IE) systems have grown more adept at processing\nwhole documents, the classic task of template filling has seen renewed interest\nas benchmark for document-level IE. In this position paper, we call into\nquestion the suitability of template filling for this purpose. We argue that\nthe task demands definitive answers to thorny questions of event individuation\n-- the problem of distinguishing distinct events -- about which even human\nexperts disagree. Through an annotation study and error analysis, we show that\nthis raises concerns about the usefulness of template filling metrics, the\nquality of datasets for the task, and the ability of models to learn it.\nFinally, we consider possible solutions.", "published": "2022-12-19 18:30:36", "link": "http://arxiv.org/abs/2212.09702v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Continual Learning for Instruction Following from Realtime Feedback", "abstract": "We propose and deploy an approach to continually train an\ninstruction-following agent from feedback provided by users during\ncollaborative interactions. During interaction, human users instruct an agent\nusing natural language, and provide realtime binary feedback as they observe\nthe agent following their instructions. We design a contextual bandit learning\napproach, converting user feedback to immediate reward. We evaluate through\nthousands of human-agent interactions, demonstrating 15.4% absolute improvement\nin instruction execution accuracy over time. We also show our approach is\nrobust to several design variations, and that the feedback signal is roughly\nequivalent to the learning signal of supervised demonstration data.", "published": "2022-12-19 18:39:43", "link": "http://arxiv.org/abs/2212.09710v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KNIFE: Distilling Reasoning Knowledge From Free-Text Rationales", "abstract": "Language models (LMs) have yielded impressive results on many language\nreasoning tasks, but their unexpected errors raise doubts about their reasoning\nabilities. In light of this, there is growing interest in finetuning/prompting\nLMs with both task instances and their associated free-text rationales (FTRs),\nwhich explain the correct reasoning process for predicting the correct task\noutput (i.e., how to be \"right for the right reasons\"). However, existing\nfinetuning methods fail to improve LM performance, while prompting needs\nprohibitively large (i.e., >50B) LMs to work well. We propose KNIFE, which\nshows that reasoning knowledge can be effectively distilled from FTRs into a\nsmall (i.e., <1B) LM and improve the LM's performance. First, KNIFE finetunes a\nteacher LM (given task input and FTR) to predict the task output, transferring\nreasoning knowledge from the FTRs to the teacher's hidden states. Second, KNIFE\nfinetunes a student LM (given task input only) such that its hidden states are\naligned with the teacher's. Thus, the student is endowed with reasoning\nknowledge but can be used for inference without direct FTR input. On two\nquestion-answering datasets, KNIFE outperforms various finetuning and prompting\nbaselines in fully-supervised and low-resource settings. Also, we observe that\nFTR quality is crucial to KNIFE's performance.", "published": "2022-12-19 18:49:09", "link": "http://arxiv.org/abs/2212.09721v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Faithfulness of Abstractive Summarization by Controlling\n  Confounding Effect of Irrelevant Sentences", "abstract": "Lack of factual correctness is an issue that still plagues state-of-the-art\nsummarization systems despite their impressive progress on generating seemingly\nfluent summaries. In this paper, we show that factual inconsistency can be\ncaused by irrelevant parts of the input text, which act as confounders. To that\nend, we leverage information-theoretic measures of causal effects to quantify\nthe amount of confounding and precisely quantify how they affect the\nsummarization performance. Based on insights derived from our theoretical\nresults, we design a simple multi-task model to control such confounding by\nleveraging human-annotated relevant sentences when available. Crucially, we\ngive a principled characterization of data distributions where such confounding\ncan be large thereby necessitating the use of human annotated relevant\nsentences to generate factual summaries. Our approach improves faithfulness\nscores by 20\\% over strong baselines on AnswerSumm\n\\citep{fabbri2021answersumm}, a conversation summarization dataset where lack\nof faithfulness is a significant issue due to the subjective nature of the\ntask. Our best method achieves the highest faithfulness score while also\nachieving state-of-the-art results on standard metrics like ROUGE and METEOR.\nWe corroborate these improvements through human evaluation.", "published": "2022-12-19 18:51:06", "link": "http://arxiv.org/abs/2212.09726v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speaking Style Conversion in the Waveform Domain Using Discrete\n  Self-Supervised Units", "abstract": "We introduce DISSC, a novel, lightweight method that converts the rhythm,\npitch contour and timbre of a recording to a target speaker in a textless\nmanner. Unlike DISSC, most voice conversion (VC) methods focus primarily on\ntimbre, and ignore people's unique speaking style (prosody). The proposed\napproach uses a pretrained, self-supervised model for encoding speech to\ndiscrete units, which makes it simple, effective, and fast to train. All\nconversion modules are only trained on reconstruction like tasks, thus suitable\nfor any-to-many VC with no paired data. We introduce a suite of quantitative\nand qualitative evaluation metrics for this setup, and empirically demonstrate\nthat DISSC significantly outperforms the evaluated baselines. Code and samples\nare available at https://pages.cs.huji.ac.il/adiyoss-lab/dissc/.", "published": "2022-12-19 18:53:04", "link": "http://arxiv.org/abs/2212.09730v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DSI++: Updating Transformer Memory with New Documents", "abstract": "Differentiable Search Indices (DSIs) encode a corpus of documents in model\nparameters and use the same model to answer user queries directly. Despite the\nstrong performance of DSI models, deploying them in situations where the corpus\nchanges over time is computationally expensive because reindexing the corpus\nrequires re-training the model. In this work, we introduce DSI++, a continual\nlearning challenge for DSI to incrementally index new documents while being\nable to answer queries related to both previously and newly indexed documents.\nAcross different model scales and document identifier representations, we show\nthat continual indexing of new documents leads to considerable forgetting of\npreviously indexed documents. We also hypothesize and verify that the model\nexperiences forgetting events during training, leading to unstable learning. To\nmitigate these issues, we investigate two approaches. The first focuses on\nmodifying the training dynamics. Flatter minima implicitly alleviate\nforgetting, so we optimize for flatter loss basins and show that the model\nstably memorizes more documents ($+12\\%$). Next, we introduce a generative\nmemory to sample pseudo-queries for documents and supplement them during\ncontinual indexing to prevent forgetting for the retrieval task. Extensive\nexperiments on novel continual indexing benchmarks based on Natural Questions\n(NQ) and MS MARCO demonstrate that our proposed solution mitigates forgetting\nsignificantly. Concretely, it improves the average Hits@10 by $+21.1\\%$ over\ncompetitive baselines for NQ and requires $6$ times fewer model updates\ncompared to re-training the DSI model for incrementally indexing five corpora\nin a sequence.", "published": "2022-12-19 18:59:34", "link": "http://arxiv.org/abs/2212.09744v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Training Trajectories of Language Models Across Scales", "abstract": "Scaling up language models has led to unprecedented performance gains, but\nlittle is understood about how the training dynamics change as models get\nlarger. How do language models of different sizes learn during pre-training?\nWhy do larger language models demonstrate more desirable behaviors? In this\npaper, we analyze the intermediate training checkpoints of differently sized\nOPT models (Zhang et al.,2022)--from 125M to 175B parameters--on next-token\nprediction, sequence-level generation, and downstream tasks. We find that 1) at\na given perplexity and independent of model sizes, a similar subset of training\ntokens see the most significant reduction in loss, with the rest stagnating or\nshowing double-descent behavior; 2) early in training, all models learn to\nreduce the perplexity of grammatical sequences that contain hallucinations,\nwith small models halting at this suboptimal distribution and larger ones\neventually learning to assign these sequences lower probabilities; 3)\nperplexity is a strong predictor of in-context learning performance on 74\nmultiple-choice tasks from BIG-Bench, and this holds independent of the model\nsize. Together, these results show that perplexity is more predictive of model\nbehaviors than model size or training computation.", "published": "2022-12-19 19:16:29", "link": "http://arxiv.org/abs/2212.09803v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Memory-efficient NLLB-200: Language-specific Expert Pruning of a\n  Massively Multilingual Machine Translation Model", "abstract": "The recently released NLLB-200 is a set of multilingual Neural Machine\nTranslation models that cover 202 languages. The largest model is based on a\nMixture of Experts architecture and achieves SoTA results across many language\npairs. It contains 54.5B parameters and requires at least four 32GB GPUs just\nfor inference. In this work, we propose a pruning method that enables the\nremoval of up to 80% of experts without further finetuning and with a\nnegligible loss in translation quality, which makes it feasible to run the\nmodel on a single 32GB GPU. Further analysis suggests that our pruning metrics\ncan identify language-specific experts.", "published": "2022-12-19 19:29:40", "link": "http://arxiv.org/abs/2212.09811v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reasoning with Language Model Prompting: A Survey", "abstract": "Reasoning, as an essential ability for complex problem-solving, can provide\nback-end support for various real-world applications, such as medical\ndiagnosis, negotiation, etc. This paper provides a comprehensive survey of\ncutting-edge research on reasoning with language model prompting. We introduce\nresearch works with comparisons and summaries and provide systematic resources\nto help beginners. We also discuss the potential reasons for emerging such\nreasoning abilities and highlight future research directions. Resources are\navailable at https://github.com/zjunlp/Prompt4ReasoningPapers (updated\nperiodically).", "published": "2022-12-19 16:32:42", "link": "http://arxiv.org/abs/2212.09597v8", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
