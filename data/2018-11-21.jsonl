{"title": "Contextualized Non-local Neural Networks for Sequence Learning", "abstract": "Recently, a large number of neural mechanisms and models have been proposed\nfor sequence learning, of which self-attention, as exemplified by the\nTransformer model, and graph neural networks (GNNs) have attracted much\nattention. In this paper, we propose an approach that combines and draws on the\ncomplementary strengths of these two methods. Specifically, we propose\ncontextualized non-local neural networks (CN$^{\\textbf{3}}$), which can both\ndynamically construct a task-specific structure of a sentence and leverage rich\nlocal dependencies within a particular neighborhood.\n  Experimental results on ten NLP tasks in text classification, semantic\nmatching, and sequence labeling show that our proposed model outperforms\ncompetitive baselines and discovers task-specific dependency structures, thus\nproviding better interpretability to users.", "published": "2018-11-21 05:14:37", "link": "http://arxiv.org/abs/1811.08600v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Collective Entity Linking", "abstract": "Entity Linking aims to link entity mentions in texts to knowledge bases, and\nneural models have achieved recent success in this task. However, most existing\nmethods rely on local contexts to resolve entities independently, which may\nusually fail due to the data sparsity of local information. To address this\nissue, we propose a novel neural model for collective entity linking, named as\nNCEL. NCEL applies Graph Convolutional Network to integrate both local\ncontextual features and global coherence information for entity linking. To\nimprove the computation efficiency, we approximately perform graph convolution\non a subgraph of adjacent entity mentions instead of those in the entire text.\nWe further introduce an attention scheme to improve the robustness of NCEL to\ndata noise and train the model on Wikipedia hyperlinks to avoid overfitting and\ndomain bias. In experiments, we evaluate NCEL on five publicly available\ndatasets to verify the linking performance as well as generalization ability.\nWe also conduct an extensive analysis of time complexity, the impact of key\nmodules, and qualitative results, which demonstrate the effectiveness and\nefficiency of our proposed method.", "published": "2018-11-21 06:00:23", "link": "http://arxiv.org/abs/1811.08603v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Convolutional Spatial Attention Model for Reading Comprehension with\n  Multiple-Choice Questions", "abstract": "Machine Reading Comprehension (MRC) with multiple-choice questions requires\nthe machine to read given passage and select the correct answer among several\ncandidates. In this paper, we propose a novel approach called Convolutional\nSpatial Attention (CSA) model which can better handle the MRC with\nmultiple-choice questions. The proposed model could fully extract the mutual\ninformation among the passage, question, and the candidates, to form the\nenriched representations. Furthermore, to merge various attention results, we\npropose to use convolutional operation to dynamically summarize the attention\nvalues within the different size of regions. Experimental results show that the\nproposed model could give substantial improvements over various\nstate-of-the-art systems on both RACE and SemEval-2018 Task11 datasets.", "published": "2018-11-21 06:42:47", "link": "http://arxiv.org/abs/1811.08610v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi Task Deep Morphological Analyzer: Context Aware Joint\n  Morphological Tagging and Lemma Prediction", "abstract": "The ambiguities introduced by the recombination of morphemes constructing\nseveral possible inflections for a word makes the prediction of syntactic\ntraits in Morphologically Rich Languages (MRLs) a notoriously complicated task.\nWe propose the Multi Task Deep Morphological analyzer (MT-DMA), a\ncharacter-level neural morphological analyzer based on multitask learning of\nword-level tag markers for Hindi and Urdu. MT-DMA predicts a set of six\nmorphological tags for words of Indo-Aryan languages: Parts-of-speech (POS),\nGender (G), Number (N), Person (P), Case (C), Tense-Aspect-Modality (TAM)\nmarker as well as the Lemma (L) by jointly learning all these in one trainable\nframework. We show the effectiveness of training of such deep neural networks\nby the simultaneous optimization of multiple loss functions and sharing of\ninitial parameters for context-aware morphological analysis. Exploiting\ncharacter-level features in phonological space optimized for each tag using\nmulti-objective genetic algorithm, our model establishes a new state-of-the-art\naccuracy score upon all seven of the tasks for both the languages. MT-DMA is\npublicly accessible: code, models and data are available at\nhttps://github.com/Saurav0074/morph_analyzer.", "published": "2018-11-21 07:55:16", "link": "http://arxiv.org/abs/1811.08619v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Best of Both Worlds: Lexical Resources To Improve Low-Resource\n  Part-of-Speech Tagging", "abstract": "In natural language processing, the deep learning revolution has shifted the\nfocus from conventional hand-crafted symbolic representations to dense inputs,\nwhich are adequate representations learned automatically from corpora. However,\nparticularly when working with low-resource languages, small amounts of\nsymbolic lexical resources such as user-generated lexicons are often available\neven when gold-standard corpora are not. Such additional linguistic information\nis though often neglected, and recent neural approaches to cross-lingual\ntagging typically rely only on word and subword embeddings. While these\nrepresentations are effective, our recent work has shown clear benefits of\ncombining the best of both worlds: integrating conventional lexical information\nimproves neural cross-lingual part-of-speech (PoS) tagging. However, little is\nknown on how complementary such additional information is, and to what extent\nimprovements depend on the coverage and quality of these external resources.\nThis paper seeks to fill this gap by providing the first thorough analysis on\nthe contributions of lexical resources for cross-lingual PoS tagging in neural\ntimes.", "published": "2018-11-21 14:36:30", "link": "http://arxiv.org/abs/1811.08757v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning cross-lingual phonological and orthagraphic adaptations: a case\n  study in improving neural machine translation between low-resource languages", "abstract": "Out-of-vocabulary (OOV) words can pose serious challenges for machine\ntranslation (MT) tasks, and in particular, for low-resource language (LRL)\npairs, i.e., language pairs for which few or no parallel corpora exist. Our\nwork adapts variants of seq2seq models to perform transduction of such words\nfrom Hindi to Bhojpuri (an LRL instance), learning from a set of cognate pairs\nbuilt from a bilingual dictionary of Hindi--Bhojpuri words. We demonstrate that\nour models can be effectively used for language pairs that have limited\nparallel corpora; our models work at the character level to grasp phonetic and\northographic similarities across multiple types of word adaptations, whether\nsynchronic or diachronic, loan words or cognates. We describe the training\naspects of several character level NMT systems that we adapted to this task and\ncharacterize their typical errors. Our method improves BLEU score by 6.3 on the\nHindi-to-Bhojpuri translation task. Further, we show that such transductions\ncan generalize well to other languages by applying it successfully to Hindi --\nBangla cognate pairs. Our work can be seen as an important step in the process\nof: (i) resolving the OOV words problem arising in MT tasks, (ii) creating\neffective parallel corpora for resource-constrained languages, and (iii)\nleveraging the enhanced semantic knowledge captured by word-level embeddings to\nperform character-level tasks.", "published": "2018-11-21 16:36:08", "link": "http://arxiv.org/abs/1811.08816v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation with Adequacy-Oriented Learning", "abstract": "Although Neural Machine Translation (NMT) models have advanced\nstate-of-the-art performance in machine translation, they face problems like\nthe inadequate translation. We attribute this to that the standard Maximum\nLikelihood Estimation (MLE) cannot judge the real translation quality due to\nits several limitations. In this work, we propose an adequacy-oriented learning\nmechanism for NMT by casting translation as a stochastic policy in\nReinforcement Learning (RL), where the reward is estimated by explicitly\nmeasuring translation adequacy. Benefiting from the sequence-level training of\nRL strategy and a more accurate reward designed specifically for translation,\nour model outperforms multiple strong baselines, including (1) standard and\ncoverage-augmented attention models with MLE-based training, and (2) advanced\nreinforcement and adversarial training strategies with rewards based on both\nword-level BLEU and character-level chrF3. Quantitative and qualitative\nanalyses on different language pairs and NMT architectures demonstrate the\neffectiveness and universality of the proposed approach.", "published": "2018-11-21 01:48:22", "link": "http://arxiv.org/abs/1811.08541v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Multimodal Representation Learning across Medical Images\n  and Reports", "abstract": "Joint embeddings between medical imaging modalities and associated radiology\nreports have the potential to offer significant benefits to the clinical\ncommunity, ranging from cross-domain retrieval to conditional generation of\nreports to the broader goals of multimodal representation learning. In this\nwork, we establish baseline joint embedding results measured via both local and\nglobal retrieval methods on the soon to be released MIMIC-CXR dataset\nconsisting of both chest X-ray images and the associated radiology reports. We\nexamine both supervised and unsupervised methods on this task and show that for\ndocument retrieval tasks with the learned representations, only a limited\namount of supervision is needed to yield results comparable to those of\nfully-supervised methods.", "published": "2018-11-21 07:24:31", "link": "http://arxiv.org/abs/1811.08615v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Inline Detection of Domain Generation Algorithms with Context-Sensitive\n  Word Embeddings", "abstract": "Domain generation algorithms (DGAs) are frequently employed by malware to\ngenerate domains used for connecting to command-and-control (C2) servers.\nRecent work in DGA detection leveraged deep learning architectures like\nconvolutional neural networks (CNNs) and character-level long short-term memory\nnetworks (LSTMs) to classify domains. However, these classifiers perform poorly\nwith wordlist-based DGA families, which generate domains by pseudorandomly\nconcatenating dictionary words. We propose a novel approach that combines\ncontext-sensitive word embeddings with a simple fully-connected classifier to\nperform classification of domains based on word-level information. The word\nembeddings were pre-trained on a large unrelated corpus and left frozen during\nthe training on domain data. The resulting small number of trainable parameters\nenabled extremely short training durations, while the transfer of language\nknowledge stored in the representations allowed for high-performing models with\nsmall training datasets. We show that this architecture reliably outperformed\nexisting techniques on wordlist-based DGA families with just 30 DGA training\nexamples and achieved state-of-the-art performance with around 100 DGA training\nexamples, all while requiring an order of magnitude less time to train compared\nto current techniques. Of special note is the technique's performance on the\nmatsnu DGA: the classifier attained a 89.5% detection rate with a 1:1,000 false\npositive rate (FPR) after training on only 30 examples of the DGA domains, and\na 91.2% detection rate with a 1:10,000 FPR after 90 examples. Considering that\nsome of these DGAs have wordlists of several hundred words, our results\ndemonstrate that this technique does not rely on the classifier learning the\nDGA wordlists. Instead, the classifier is able to learn the semantic signatures\nof the wordlist-based DGA families.", "published": "2018-11-21 12:14:12", "link": "http://arxiv.org/abs/1811.08705v1", "categories": ["cs.CR", "cs.CL", "cs.LG", "cs.NI", "K.6.5; C.2.0; I.2.7; I.2.6"], "primary_category": "cs.CR"}
{"title": "Towards Emotion Recognition: A Persistent Entropy Application", "abstract": "Emotion recognition and classification is a very active area of research. In\nthis paper, we present a first approach to emotion classification using\npersistent entropy and support vector machines. A topology-based model is\napplied to obtain a single real number from each raw signal. These data are\nused as input of a support vector machine to classify signals into 8 different\nemotions (calm, happy, sad, angry, fearful, disgust and surprised).", "published": "2018-11-21 19:20:43", "link": "http://arxiv.org/abs/1811.09607v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Resource Mention Extraction for MOOC Discussion Forums", "abstract": "In discussions hosted on discussion forums for MOOCs, references to online\nlearning resources are often of central importance. They contextualize the\ndiscussion, anchoring the discussion participants' presentation of the issues\nand their understanding. However they are usually mentioned in free text,\nwithout appropriate hyperlinking to their associated resource. Automated\nlearning resource mention hyperlinking and categorization will facilitate\ndiscussion and searching within MOOC forums, and also benefit the\ncontextualization of such resources across disparate views. We propose the\nnovel problem of learning resource mention identification in MOOC forums. As\nthis is a novel task with no publicly available data, we first contribute a\nlarge-scale labeled dataset, dubbed the Forum Resource Mention (FoRM) dataset,\nto facilitate our current research and future research on this task. We then\nformulate this task as a sequence tagging problem and investigate solution\narchitectures to address the problem. Importantly, we identify two major\nchallenges that hinder the application of sequence tagging models to the task:\n(1) the diversity of resource mention expression, and (2) long-range contextual\ndependencies. We address these challenges by incorporating character-level and\nthread context information into a LSTM-CRF model. First, we incorporate a\ncharacter encoder to address the out-of-vocabulary problem caused by the\ndiversity of mention expressions. Second, to address the context dependency\nchallenge, we encode thread contexts using an RNN-based context encoder, and\napply the attention mechanism to selectively leverage useful context\ninformation during sequence tagging. Experiments on FoRM show that the proposed\nmethod improves the baseline deep sequence tagging models notably,\nsignificantly bettering performance on instances that exemplify the two\nchallenges.", "published": "2018-11-21 17:59:56", "link": "http://arxiv.org/abs/1811.08853v1", "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Measuring Depression Symptom Severity from Spoken Language and 3D Facial\n  Expressions", "abstract": "With more than 300 million people depressed worldwide, depression is a global\nproblem. Due to access barriers such as social stigma, cost, and treatment\navailability, 60% of mentally-ill adults do not receive any mental health\nservices. Effective and efficient diagnosis relies on detecting clinical\nsymptoms of depression. Automatic detection of depressive symptoms would\npotentially improve diagnostic accuracy and availability, leading to faster\nintervention. In this work, we present a machine learning method for measuring\nthe severity of depressive symptoms. Our multi-modal method uses 3D facial\nexpressions and spoken language, commonly available from modern cell phones. It\ndemonstrates an average error of 3.67 points (15.3% relative) on the\nclinically-validated Patient Health Questionnaire (PHQ) scale. For detecting\nmajor depressive disorder, our model demonstrates 83.3% sensitivity and 82.6%\nspecificity. Overall, this paper shows how speech recognition, computer vision,\nand natural language processing can be combined to assist mental health\npatients and practitioners. This technology could be deployed to cell phones\nworldwide and facilitate low-cost universal access to mental health care.", "published": "2018-11-21 03:52:31", "link": "http://arxiv.org/abs/1811.08592v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Speech recognition with quaternion neural networks", "abstract": "Neural network architectures are at the core of powerful automatic speech\nrecognition systems (ASR). However, while recent researches focus on novel\nmodel architectures, the acoustic input features remain almost unchanged.\nTraditional ASR systems rely on multidimensional acoustic features such as the\nMel filter bank energies alongside with the first, and second order derivatives\nto characterize time-frames that compose the signal sequence. Considering that\nthese components describe three different views of the same element, neural\nnetworks have to learn both the internal relations that exist within these\nfeatures, and external or global dependencies that exist between the\ntime-frames. Quaternion-valued neural networks (QNN), recently received an\nimportant interest from researchers to process and learn such relations in\nmultidimensional spaces. Indeed, quaternion numbers and QNNs have shown their\nefficiency to process multidimensional inputs as entities, to encode internal\ndependencies, and to solve many tasks with up to four times less learning\nparameters than real-valued models. We propose to investigate modern\nquaternion-valued models such as convolutional and recurrent quaternion neural\nnetworks in the context of speech recognition with the TIMIT dataset. The\nexperiments show that QNNs always outperform real-valued equivalent models with\nway less free parameters, leading to a more efficient, compact, and expressive\nrepresentation of the relevant information.", "published": "2018-11-21 10:27:02", "link": "http://arxiv.org/abs/1811.09678v1", "categories": ["eess.AS", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Facilitating the Manual Annotation of Sounds When Using Large Taxonomies", "abstract": "Properly annotated multimedia content is crucial for supporting advances in\nmany Information Retrieval applications. It enables, for instance, the\ndevelopment of automatic tools for the annotation of large and diverse\nmultimedia collections. In the context of everyday sounds and online\ncollections, the content to describe is very diverse and involves many\ndifferent types of concepts, often organised in large hierarchical structures\ncalled taxonomies. This makes the task of manually annotating content arduous.\nIn this paper, we present our user-centered development of two tools for the\nmanual annotation of audio content from a wide range of types. We conducted a\npreliminary evaluation of functional prototypes involving real users. The goal\nis to evaluate them in a real context, engage in discussions with users, and\ninspire new ideas. A qualitative analysis was carried out including usability\nquestionnaires and semi-structured interviews. This revealed interesting\naspects to consider when developing tools for the manual annotation of audio\ncontent with labels drawn from large hierarchical taxonomies.", "published": "2018-11-21 16:43:11", "link": "http://arxiv.org/abs/1811.10988v1", "categories": ["cs.IR", "cs.HC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
