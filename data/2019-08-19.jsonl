{"title": "Question Answering based Clinical Text Structuring Using Pre-trained\n  Language Model", "abstract": "Clinical text structuring is a critical and fundamental task for clinical\nresearch. Traditional methods such as taskspecific end-to-end models and\npipeline models usually suffer from the lack of dataset and error propagation.\nIn this paper, we present a question answering based clinical text structuring\n(QA-CTS) task to unify different specific tasks and make dataset shareable. A\nnovel model that aims to introduce domain-specific features (e.g., clinical\nnamed entity information) into pre-trained language model is also proposed for\nQA-CTS task. Experimental results on Chinese pathology reports collected from\nRuijing Hospital demonstrate our presented QA-CTS task is very effective to\nimprove the performance on specific tasks. Our proposed model also competes\nfavorably with strong baseline models in specific tasks.", "published": "2019-08-19 06:21:29", "link": "http://arxiv.org/abs/1908.06606v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Align, Mask and Select: A Simple Method for Incorporating Commonsense\n  Knowledge into Language Representation Models", "abstract": "The state-of-the-art pre-trained language representation models, such as\nBidirectional Encoder Representations from Transformers (BERT), rarely\nincorporate commonsense knowledge or other knowledge explicitly. We propose a\npre-training approach for incorporating commonsense knowledge into language\nrepresentation models. We construct a commonsense-related multi-choice question\nanswering dataset for pre-training a neural language representation model. The\ndataset is created automatically by our proposed \"align, mask, and select\"\n(AMS) method. We also investigate different pre-training tasks. Experimental\nresults demonstrate that pre-training models using the proposed approach\nfollowed by fine-tuning achieve significant improvements over previous\nstate-of-the-art models on two commonsense-related benchmarks, including\nCommonsenseQA and Winograd Schema Challenge. We also observe that fine-tuned\nmodels after the proposed pre-training approach maintain comparable performance\non other NLP tasks, such as sentence classification and natural language\ninference tasks, compared to the original BERT models. These results verify\nthat the proposed approach, while significantly improving commonsense-related\nNLP tasks, does not degrade the general language representation capabilities.", "published": "2019-08-19 12:08:27", "link": "http://arxiv.org/abs/1908.06725v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fast End-to-End Wikification", "abstract": "Wikification of large corpora is beneficial for various NLP applications.\nExisting methods focus on quality performance rather than run-time, and are\ntherefore non-feasible for large data. Here, we introduce RedW, a run-time\noriented Wikification solution, based on Wikipedia redirects, that can Wikify\nmassive corpora with competitive performance. We further propose an efficient\nmethod for estimating RedW confidence, opening the door for applying more\ndemanding methods only on top of RedW lower-confidence results. Our\nexperimental results support the validity of the proposed approach.", "published": "2019-08-19 13:21:10", "link": "http://arxiv.org/abs/1908.06785v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-grained Sentiment Analysis with Faithful Attention", "abstract": "While the general task of textual sentiment classification has been widely\nstudied, much less research looks specifically at sentiment between a specified\nsource and target. To tackle this problem, we experimented with a\nstate-of-the-art relation extraction model. Surprisingly, we found that despite\nreasonable performance, the model's attention was often systematically\nmisaligned with the words that contribute to sentiment. Thus, we directly\ntrained the model's attention with human rationales and improved our model\nperformance by a robust 4~8 points on all tasks we defined on our data sets. We\nalso present a rigorous analysis of the model's attention, both trained and\nuntrained, using novel and intuitive metrics. Our results show that untrained\nattention does not provide faithful explanations; however, trained attention\nwith concisely annotated human rationales not only increases performance, but\nalso brings faithful explanations. Encouragingly, a small amount of annotated\nhuman rationales suffice to correct the attention in our task.", "published": "2019-08-19 15:11:27", "link": "http://arxiv.org/abs/1908.06870v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UDPipe at SIGMORPHON 2019: Contextualized Embeddings, Regularization\n  with Morphological Categories, Corpora Merging", "abstract": "We present our contribution to the SIGMORPHON 2019 Shared Task:\nCrosslinguality and Context in Morphology, Task 2: contextual morphological\nanalysis and lemmatization.\n  We submitted a modification of the UDPipe 2.0, one of best-performing systems\nof the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal\nDependencies and an overall winner of the The 2018 Shared Task on Extrinsic\nParser Evaluation.\n  As our first improvement, we use the pretrained contextualized embeddings\n(BERT) as additional inputs to the network; secondly, we use individual\nmorphological features as regularization; and finally, we merge the selected\ncorpora of the same language.\n  In the lemmatization task, our system exceeds all the submitted systems by a\nwide margin with lemmatization accuracy 95.78 (second best was 95.00, third\n94.46). In the morphological analysis, our system placed tightly second: our\nmorphological analysis accuracy was 93.19, the winning system's 93.23.", "published": "2019-08-19 17:03:03", "link": "http://arxiv.org/abs/1908.06931v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Encoder-Agnostic Adaptation for Conditional Language Generation", "abstract": "Large pretrained language models have changed the way researchers approach\ndiscriminative natural language understanding tasks, leading to the dominance\nof approaches that adapt a pretrained model for arbitrary downstream tasks.\nHowever it is an open-question how to use similar techniques for language\ngeneration. Early results in the encoder-agnostic setting have been mostly\nnegative. In this work we explore methods for adapting a pretrained language\nmodel to arbitrary conditional input. We observe that pretrained transformer\nmodels are sensitive to large parameter changes during tuning. We therefore\npropose an adaptation that directly injects arbitrary conditioning into self\nattention, an approach we call pseudo self attention. Through experiments on\nfour diverse conditional text generation tasks we show that this\nencoder-agnostic technique outperforms strong baselines, produces coherent\ngenerations, and is data efficient.", "published": "2019-08-19 17:22:58", "link": "http://arxiv.org/abs/1908.06938v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why So Down? The Role of Negative (and Positive) Pointwise Mutual\n  Information in Distributional Semantics", "abstract": "In distributional semantics, the pointwise mutual information\n($\\mathit{PMI}$) weighting of the cooccurrence matrix performs far better than\nraw counts. There is, however, an issue with unobserved pair cooccurrences as\n$\\mathit{PMI}$ goes to negative infinity. This problem is aggravated by\nunreliable statistics from finite corpora which lead to a large number of such\npairs. A common practice is to clip negative $\\mathit{PMI}$\n($\\mathit{\\texttt{-} PMI}$) at $0$, also known as Positive $\\mathit{PMI}$\n($\\mathit{PPMI}$). In this paper, we investigate alternative ways of dealing\nwith $\\mathit{\\texttt{-} PMI}$ and, more importantly, study the role that\nnegative information plays in the performance of a low-rank, weighted\nfactorization of different $\\mathit{PMI}$ matrices. Using various semantic and\nsyntactic tasks as probes into models which use either negative or positive\n$\\mathit{PMI}$ (or both), we find that most of the encoded semantics and syntax\ncome from positive $\\mathit{PMI}$, in contrast to $\\mathit{\\texttt{-} PMI}$\nwhich contributes almost exclusively syntactic information. Our findings deepen\nour understanding of distributional semantics, while also introducing novel\n$PMI$ variants and grounding the popular $PPMI$ measure.", "published": "2019-08-19 17:26:13", "link": "http://arxiv.org/abs/1908.06941v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Polly Want a Cracker: Analyzing Performance of Parroting on Paraphrase\n  Generation Datasets", "abstract": "Paraphrase generation is an interesting and challenging NLP task which has\nnumerous practical applications. In this paper, we analyze datasets commonly\nused for paraphrase generation research, and show that simply parroting input\nsentences surpasses state-of-the-art models in the literature when evaluated on\nstandard metrics. Our findings illustrate that a model could be seemingly adept\nat generating paraphrases, despite only making trivial changes to the input\nsentence or even none at all.", "published": "2019-08-19 05:40:42", "link": "http://arxiv.org/abs/1908.07831v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recurrent Graph Syntax Encoder for Neural Machine Translation", "abstract": "Syntax-incorporated machine translation models have been proven successful in\nimproving the model's reasoning and meaning preservation ability. In this\npaper, we propose a simple yet effective graph-structured encoder, the\nRecurrent Graph Syntax Encoder, dubbed \\textbf{RGSE}, which enhances the\nability to capture useful syntactic information. The RGSE is done over a\nstandard encoder (recurrent or self-attention encoder), regarding recurrent\nnetwork units as graph nodes and injects syntactic dependencies as edges, such\nthat RGSE models syntactic dependencies and sequential information\n(\\textit{i.e.}, word order) simultaneously. Our approach achieves considerable\nimprovements over several syntax-aware NMT models in English$\\Rightarrow$German\nand English$\\Rightarrow$Czech translation tasks. And RGSE-equipped big model\nobtains competitive result compared with the state-of-the-art model in WMT14\nEn-De task. Extensive analysis further verifies that RGSE could benefit long\nsentence modeling, and produces better translations.", "published": "2019-08-19 02:10:39", "link": "http://arxiv.org/abs/1908.06559v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Co-analysis Framework for Exploring Multivariate Scientific Data", "abstract": "In complex multivariate data sets, different features usually include diverse\nassociations with different variables, and different variables are associated\nwithin different regions. Therefore, exploring the associations between\nvariables and voxels locally becomes necessary to better understand the\nunderlying phenomena. In this paper, we propose a co-analysis framework based\non biclusters, which are two subsets of variables and voxels with close\nscalar-value relationships, to guide the process of visually exploring\nmultivariate data. We first automatically extract all meaningful biclusters,\neach of which only contains voxels with a similar scalar-value pattern over a\nsubset of variables. These biclusters are organized according to their variable\nsets, and biclusters in each variable set are further grouped by a similarity\nmetric to reduce redundancy and support diversity during visual exploration.\nBiclusters are visually represented in coordinated views to facilitate\ninteractive exploration of multivariate data based on the similarity between\nbiclusters and the correlation of scalar values with different variables.\nExperiments on several representative multivariate scientific data sets\ndemonstrate the effectiveness of our framework in exploring local relationships\namong variables, biclusters and scalar values in the data.", "published": "2019-08-19 03:31:48", "link": "http://arxiv.org/abs/1908.06576v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Long and Diverse Text Generation with Planning-based Hierarchical\n  Variational Model", "abstract": "Existing neural methods for data-to-text generation are still struggling to\nproduce long and diverse texts: they are insufficient to model input data\ndynamically during generation, to capture inter-sentence coherence, or to\ngenerate diversified expressions. To address these issues, we propose a\nPlanning-based Hierarchical Variational Model (PHVM). Our model first plans a\nsequence of groups (each group is a subset of input items to be covered by a\nsentence) and then realizes each sentence conditioned on the planning result\nand the previously generated context, thereby decomposing long text generation\ninto dependent sentence generation sub-tasks. To capture expression diversity,\nwe devise a hierarchical latent structure where a global planning latent\nvariable models the diversity of reasonable planning and a sequence of local\nlatent variables controls sentence realization. Experiments show that our model\noutperforms state-of-the-art baselines in long and diverse text generation.", "published": "2019-08-19 06:20:38", "link": "http://arxiv.org/abs/1908.06605v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bilingual Lexicon Induction with Semi-supervision in Non-Isometric\n  Embedding Spaces", "abstract": "Recent work on bilingual lexicon induction (BLI) has frequently depended\neither on aligned bilingual lexicons or on distribution matching, often with an\nassumption about the isometry of the two spaces. We propose a technique to\nquantitatively estimate this assumption of the isometry between two embedding\nspaces and empirically show that this assumption weakens as the languages in\nquestion become increasingly etymologically distant. We then propose Bilingual\nLexicon Induction with Semi-Supervision (BLISS) --- a semi-supervised approach\nthat relaxes the isometric assumption while leveraging both limited aligned\nbilingual lexicons and a larger set of unaligned word embeddings, as well as a\nnovel hubness filtering technique. Our proposed method obtains state of the art\nresults on 15 of 18 language pairs on the MUSE dataset, and does particularly\nwell when the embedding spaces don't appear to be isometric. In addition, we\nalso show that adding supervision stabilizes the learning procedure, and is\neffective even with minimal supervision.", "published": "2019-08-19 07:36:19", "link": "http://arxiv.org/abs/1908.06625v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Memory limitations are hidden in grammar", "abstract": "The ability to produce and understand an unlimited number of different\nsentences is a hallmark of human language. Linguists have sought to define the\nessence of this generative capacity using formal grammars that describe the\nsyntactic dependencies between constituents, independent of the computational\nlimitations of the human brain. Here, we evaluate this independence assumption\nby sampling sentences uniformly from the space of possible syntactic\nstructures. We find that the average dependency distance between syntactically\nrelated words, a proxy for memory limitations, is less than expected by chance\nin a collection of state-of-the-art classes of dependency grammars. Our\nfindings indicate that memory limitations have permeated grammatical\ndescriptions, suggesting that it may be impossible to build a parsimonious\ntheory of human linguistic productivity independent of non-linguistic cognitive\nconstraints.", "published": "2019-08-19 07:56:46", "link": "http://arxiv.org/abs/1908.06629v3", "categories": ["cs.CL", "cs.DM"], "primary_category": "cs.CL"}
{"title": "A Study of BERT for Non-Factoid Question-Answering under Passage Length\n  Constraints", "abstract": "We study the use of BERT for non-factoid question-answering, focusing on the\npassage re-ranking task under varying passage lengths. To this end, we explore\nthe fine-tuning of BERT in different learning-to-rank setups, comprising both\npoint-wise and pair-wise methods, resulting in substantial improvements over\nthe state-of-the-art. We then analyze the effectiveness of BERT for different\npassage lengths and suggest how to cope with large passages.", "published": "2019-08-19 13:14:02", "link": "http://arxiv.org/abs/1908.06780v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Style Transfer for Texts: Retrain, Report Errors, Compare with Rewrites", "abstract": "This paper shows that standard assessment methodology for style transfer has\nseveral significant problems. First, the standard metrics for style accuracy\nand semantics preservation vary significantly on different re-runs. Therefore\none has to report error margins for the obtained results. Second, starting with\ncertain values of bilingual evaluation understudy (BLEU) between input and\noutput and accuracy of the sentiment transfer the optimization of these two\nstandard metrics diverge from the intuitive goal of the style transfer task.\nFinally, due to the nature of the task itself, there is a specific dependence\nbetween these two metrics that could be easily manipulated. Under these\ncircumstances, we suggest taking BLEU between input and human-written\nreformulations into consideration for benchmarks. We also propose three new\narchitectures that outperform state of the art in terms of this metric.", "published": "2019-08-19 14:01:18", "link": "http://arxiv.org/abs/1908.06809v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Are You for Real? Detecting Identity Fraud via Dialogue Interactions", "abstract": "Identity fraud detection is of great importance in many real-world scenarios\nsuch as the financial industry. However, few studies addressed this problem\nbefore. In this paper, we focus on identity fraud detection in loan\napplications and propose to solve this problem with a novel interactive\ndialogue system which consists of two modules. One is the knowledge graph (KG)\nconstructor organizing the personal information for each loan applicant. The\nother is structured dialogue management that can dynamically generate a series\nof questions based on the personal KG to ask the applicants and determine their\nidentity states. We also present a heuristic user simulator based on problem\nanalysis to evaluate our method. Experiments have shown that the trainable\ndialogue system can effectively detect fraudsters, and achieve higher\nrecognition accuracy compared with rule-based systems. Furthermore, our learned\ndialogue strategies are interpretable and flexible, which can help promote\nreal-world applications.", "published": "2019-08-19 14:13:24", "link": "http://arxiv.org/abs/1908.06820v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automated email Generation for Targeted Attacks using Natural Language", "abstract": "With an increasing number of malicious attacks, the number of people and\norganizations falling prey to social engineering attacks is proliferating.\nDespite considerable research in mitigation systems, attackers continually\nimprove their modus operandi by using sophisticated machine learning, natural\nlanguage processing techniques with an intent to launch successful targeted\nattacks aimed at deceiving detection mechanisms as well as the victims. We\npropose a system for advanced email masquerading attacks using Natural Language\nGeneration (NLG) techniques. Using legitimate as well as an influx of varying\nmalicious content, the proposed deep learning system generates \\textit{fake}\nemails with malicious content, customized depending on the attacker's intent.\nThe system leverages Recurrent Neural Networks (RNNs) for automated text\ngeneration. We also focus on the performance of the generated emails in\ndefeating statistical detectors, and compare and analyze the emails using a\nproposed baseline.", "published": "2019-08-19 15:52:36", "link": "http://arxiv.org/abs/1908.06893v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Neural Architectures for Nested NER through Linearization", "abstract": "We propose two neural network architectures for nested named entity\nrecognition (NER), a setting in which named entities may overlap and also be\nlabeled with more than one label. We encode the nested labels using a\nlinearized scheme. In our first proposed approach, the nested labels are\nmodeled as multilabels corresponding to the Cartesian product of the nested\nlabels in a standard LSTM-CRF architecture. In the second one, the nested NER\nis viewed as a sequence-to-sequence problem, in which the input sequence\nconsists of the tokens and output sequence of the labels, using hard attention\non the word whose label is being predicted. The proposed methods outperform the\nnested NER state of the art on four corpora: ACE-2004, ACE-2005, GENIA and\nCzech CNEC. We also enrich our architectures with the recently published\ncontextual embeddings: ELMo, BERT and Flair, reaching further improvements for\nthe four nested entity corpora. In addition, we report flat NER\nstate-of-the-art results for CoNLL-2002 Dutch and Spanish and for CoNLL-2003\nEnglish.", "published": "2019-08-19 16:54:53", "link": "http://arxiv.org/abs/1908.06926v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Natural Selection of Words: Finding the Features of Fitness", "abstract": "We introduce a dataset for studying the evolution of words, constructed from\nWordNet and the Google Books Ngram Corpus. The dataset tracks the evolution of\n4,000 synonym sets (synsets), containing 9,000 English words, from 1800 AD to\n2000 AD. We present a supervised learning algorithm that is able to predict the\nfuture leader of a synset: the word in the synset that will have the highest\nfrequency. The algorithm uses features based on a word's length, the characters\nin the word, and the historical frequencies of the word. It can predict change\nof leadership (including the identity of the new leader) fifty years in the\nfuture, with an F-score considerably above random guessing. Analysis of the\nlearned models provides insight into the causes of change in the leader of a\nsynset. The algorithm confirms observations linguists have made, such as the\ntrend to replace the -ise suffix with -ize, the rivalry between the -ity and\n-ness suffixes, and the struggle between economy (shorter words are easier to\nremember and to write) and clarity (longer words are more distinctive and less\nlikely to be confused with one another). The results indicate that integration\nof the Google Books Ngram Corpus with WordNet has significant potential for\nimproving our understanding of how language evolves.", "published": "2019-08-19 18:28:59", "link": "http://arxiv.org/abs/1908.07013v1", "categories": ["cs.CL", "cs.LG", "I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "It Takes Nine to Smell a Rat: Neural Multi-Task Learning for\n  Check-Worthiness Prediction", "abstract": "We propose a multi-task deep-learning approach for estimating the\ncheck-worthiness of claims in political debates. Given a political debate, such\nas the 2016 US Presidential and Vice-Presidential ones, the task is to predict\nwhich statements in the debate should be prioritized for fact-checking. While\ndifferent fact-checking organizations would naturally make different choices\nwhen analyzing the same debate, we show that it pays to learn from multiple\nsources simultaneously (PolitiFact, FactCheck, ABC, CNN, NPR, NYT, Chicago\nTribune, The Guardian, and Washington Post) in a multi-task learning setup,\neven when a particular source is chosen as a target to imitate. Our evaluation\nshows state-of-the-art results on a standard dataset for the task of\ncheck-worthiness prediction.", "published": "2019-08-19 19:52:50", "link": "http://arxiv.org/abs/1908.07912v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Transfer in Deep Reinforcement Learning using Knowledge Graphs", "abstract": "Text adventure games, in which players must make sense of the world through\ntext descriptions and declare actions through text descriptions, provide a\nstepping stone toward grounding action in language. Prior work has demonstrated\nthat using a knowledge graph as a state representation and question-answering\nto pre-train a deep Q-network facilitates faster control policy transfer. In\nthis paper, we explore the use of knowledge graphs as a representation for\ndomain knowledge transfer for training text-adventure playing reinforcement\nlearning agents. Our methods are tested across multiple computer generated and\nhuman authored games, varying in domain and complexity, and demonstrate that\nour transfer learning methods let us learn a higher-quality control policy\nfaster.", "published": "2019-08-19 01:52:00", "link": "http://arxiv.org/abs/1908.06556v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Implicit Recursive Characteristics of STOP", "abstract": "The most important notations of Communicating Sequential Process(CSP) are the\nprocess and the prefix (event)$\\rightarrow$(process) operator. While we can\nformally apply the $\\rightarrow$ operator to define a live process's behavior,\nthe STOP process, which usually resulted from deadlock, starving or livelock,\nis lack of formal description, defined by most literatures as \"doing nothing\nbut halt\". In this paper, we argue that the STOP process should not be\nconsidered as a black box, it should follow the prefix $\\rightarrow$ schema and\nthe same inference rules so that a unified and consistent process algebra model\ncan be established. In order to achieve this goal, we introduce a special event\ncalled \"nil\" that any process can take. This nil event will do nothing\nmeaningful and leave nothing on a process's observable record. With the nil\nevent and its well-defined rules, we can successfully use the $\\rightarrow$\noperator to formally describe a process's complete behavior in its whole life\ncircle. More interestingly, we can use prefix $\\rightarrow$ and nil event to\nfully describe the STOP process's internal behavior and conclude that the\nSTOP's formal equation can be given as simple as STOP$_{\\alpha X} = \\mu$ X. nil\n$\\rightarrow$ X.", "published": "2019-08-19 05:43:13", "link": "http://arxiv.org/abs/1908.06601v2", "categories": ["cs.PL", "cs.CL", "cs.FL", "cs.LO"], "primary_category": "cs.PL"}
{"title": "Two-Staged Acoustic Modeling Adaption for Robust Speech Recognition by\n  the Example of German Oral History Interviews", "abstract": "In automatic speech recognition, often little training data is available for\nspecific challenging tasks, but training of state-of-the-art automatic speech\nrecognition systems requires large amounts of annotated speech. To address this\nissue, we propose a two-staged approach to acoustic modeling that combines\nnoise and reverberation data augmentation with transfer learning to robustly\naddress challenges such as difficult acoustic recording conditions, spontaneous\nspeech, and speech of elderly people. We evaluate our approach using the\nexample of German oral history interviews, where a relative average reduction\nof the word error rate by 19.3% is achieved.", "published": "2019-08-19 11:45:11", "link": "http://arxiv.org/abs/1908.06709v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Message Passing for Complex Question Answering over Knowledge Graphs", "abstract": "Question answering over knowledge graphs (KGQA) has evolved from simple\nsingle-fact questions to complex questions that require graph traversal and\naggregation. We propose a novel approach for complex KGQA that uses\nunsupervised message passing, which propagates confidence scores obtained by\nparsing an input question and matching terms in the knowledge graph to a set of\npossible answers. First, we identify entity, relationship, and class names\nmentioned in a natural language question, and map these to their counterparts\nin the graph. Then, the confidence scores of these mappings propagate through\nthe graph structure to locate the answer entities. Finally, these are\naggregated depending on the identified question type. This approach can be\nefficiently implemented as a series of sparse matrix multiplications mimicking\njoins over small local subgraphs. Our evaluation results show that the proposed\napproach outperforms the state-of-the-art on the LC-QuAD benchmark. Moreover,\nwe show that the performance of the approach depends only on the quality of the\nquestion interpretation results, i.e., given a correct relevance score\ndistribution, our approach always produces a correct answer ranking. Our error\nanalysis reveals correct answers missing from the benchmark dataset and\ninconsistencies in the DBpedia knowledge graph. Finally, we provide a\ncomprehensive evaluation of the proposed approach accompanied with an ablation\nstudy and an error analysis, which showcase the pitfalls for each of the\nquestion answering components in more detail.", "published": "2019-08-19 16:31:29", "link": "http://arxiv.org/abs/1908.06917v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Tale of tails using rule augmented sequence labeling for event\n  extraction", "abstract": "The problem of event extraction is a relatively difficult task for low\nresource languages due to the non-availability of sufficient annotated data.\nMoreover, the task becomes complex for tail (rarely occurring) labels wherein\nextremely less data is available. In this paper, we present a new dataset\n(InDEE-2019) in the disaster domain for multiple Indic languages, collected\nfrom news websites. Using this dataset, we evaluate several rule-based\nmechanisms to augment deep learning based models. We formulate our problem of\nevent extraction as a sequence labeling task and perform extensive experiments\nto study and understand the effectiveness of different approaches. We further\nshow that tail labels can be easily incorporated by creating new rules without\nthe requirement of large annotated data.", "published": "2019-08-19 18:43:06", "link": "http://arxiv.org/abs/1908.07018v3", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Domain-Independent turn-level Dialogue Quality Evaluation via User\n  Satisfaction Estimation", "abstract": "An automated metric to evaluate dialogue quality is vital for optimizing data\ndriven dialogue management. The common approach of relying on explicit user\nfeedback during a conversation is intrusive and sparse. Current models to\nestimate user satisfaction use limited feature sets and rely on annotation\nschemes with low inter-rater reliability, limiting generalizability to\nconversations spanning multiple domains. To address these gaps, we created a\nnew Response Quality annotation scheme, based on which we developed turn-level\nUser Satisfaction metric. We introduced five new domain-independent feature\nsets and experimented with six machine learning models to estimate the new\nsatisfaction metric.\n  Using Response Quality annotation scheme, across randomly sampled single and\nmulti-turn conversations from 26 domains, we achieved high inter-annotator\nagreement (Spearman's rho 0.94). The Response Quality labels were highly\ncorrelated (0.76) with explicit turn-level user ratings. Gradient boosting\nregression achieved best correlation of ~0.79 between predicted and annotated\nuser satisfaction labels. Multi Layer Perceptron and Gradient Boosting\nregression models generalized to an unseen domain better (linear correlation\n0.67) than other models. Finally, our ablation study verified that our novel\nfeatures significantly improved model performance.", "published": "2019-08-19 20:58:24", "link": "http://arxiv.org/abs/1908.07064v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Representing text as abstract images enables image classifiers to also\n  simultaneously classify text", "abstract": "We introduce a novel method for converting text data into abstract image\nrepresentations, which allows image-based processing techniques (e.g. image\nclassification networks) to be applied to text-based comparison problems. We\napply the technique to entity disambiguation of inventor names in US patents.\nThe method involves converting text from each pairwise comparison between two\ninventor name records into a 2D RGB (stacked) image representation. We then\ntrain an image classification neural network to discriminate between such\npairwise comparison images, and use the trained network to label each pair of\nrecords as either matched (same inventor) or non-matched (different inventors),\nobtaining highly accurate results. Our new text-to-image representation method\ncould also be used more broadly for other NLP comparison problems, such as\ndisambiguation of academic publications, or for problems that require\nsimultaneous classification of both text and image datasets.", "published": "2019-08-19 17:28:29", "link": "http://arxiv.org/abs/1908.07846v3", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Audio query-based music source separation", "abstract": "In recent years, music source separation has been one of the most intensively\nstudied research areas in music information retrieval. Improvements in deep\nlearning lead to a big progress in music source separation performance.\nHowever, most of the previous studies are restricted to separating a few\nlimited number of sources, such as vocals, drums, bass, and other. In this\nstudy, we propose a network for audio query-based music source separation that\ncan explicitly encode the source information from a query signal regardless of\nthe number and/or kind of target signals. The proposed method consists of a\nQuery-net and a Separator: given a query and a mixture, the Query-net encodes\nthe query into the latent space, and the Separator estimates masks conditioned\nby the latent vector, which is then applied to the mixture for separation. The\nSeparator can also generate masks using the latent vector from the training\nsamples, allowing separation in the absence of a query. We evaluate our method\non the MUSDB18 dataset, and experimental results show that the proposed method\ncan separate multiple sources with a single network. In addition, through\nfurther investigation of the latent space we demonstrate that our method can\ngenerate continuous outputs via latent vector interpolation.", "published": "2019-08-19 04:56:56", "link": "http://arxiv.org/abs/1908.06593v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Salient Speech Representations Based on Cloned Networks", "abstract": "We define salient features as features that are shared by signals that are\ndefined as being equivalent by a system designer. The definition allows the\ndesigner to contribute qualitative information. We aim to find salient features\nthat are useful as conditioning for generative networks. We extract salient\nfeatures by jointly training a set of clones of an encoder network. Each\nnetwork clone receives as input a different signal from a set of equivalent\nsignals. The objective function encourages the network clones to map their\ninput into a set of features that is identical across the clones. It\nadditionally encourages feature independence and, optionally, reconstruction of\na desired target signal by a decoder. As an application, we train a system that\nextracts a time-sequence of feature vectors of speech and uses it as a\nconditioning of a WaveNet generative system, facilitating both coding and\nenhancement.", "published": "2019-08-19 19:38:01", "link": "http://arxiv.org/abs/1908.07045v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fuzzy C-Means Clustering and Sonification of HRV Features", "abstract": "Linear and non-linear measures of heart rate variability (HRV) are widely\ninvestigated as non-invasive indicators of health. Stress has a profound impact\non heart rate, and different meditation techniques have been found to modulate\nheartbeat rhythm. This paper aims to explore the process of identifying\nappropriate metrices from HRV analysis for sonification. Sonification is a type\nof auditory display involving the process of mapping data to acoustic\nparameters. This work explores the use of auditory display in aiding the\nanalysis of HRV leveraged by unsupervised machine learning techniques.\nUnsupervised clustering helps select the appropriate features to improve the\nsonification interpretability. Vocal synthesis sonification techniques are\nemployed to increase comprehension and learnability of the processed data\ndisplayed through sound. These analyses are early steps in building a real-time\nsound-based biofeedback training system.", "published": "2019-08-19 23:44:01", "link": "http://arxiv.org/abs/1908.07107v2", "categories": ["cs.HC", "cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.HC"}
