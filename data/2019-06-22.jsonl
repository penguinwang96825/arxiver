{"title": "Evaluating Computational Language Models with Scaling Properties of\n  Natural Language", "abstract": "In this article, we evaluate computational models of natural language with\nrespect to the universal statistical behaviors of natural language. Statistical\nmechanical analyses have revealed that natural language text is characterized\nby scaling properties, which quantify the global structure in the vocabulary\npopulation and the long memory of a text. We study whether five scaling\nproperties (given by Zipf's law, Heaps' law, Ebeling's method, Taylor's law,\nand long-range correlation analysis) can serve for evaluation of computational\nmodels. Specifically, we test $n$-gram language models, a probabilistic\ncontext-free grammar (PCFG), language models based on Simon/Pitman-Yor\nprocesses, neural language models, and generative adversarial networks (GANs)\nfor text generation. Our analysis reveals that language models based on\nrecurrent neural networks (RNNs) with a gating mechanism (i.e., long short-term\nmemory, LSTM; a gated recurrent unit, GRU; and quasi-recurrent neural networks,\nQRNNs) are the only computational models that can reproduce the long memory\nbehavior of natural language. Furthermore, through comparison with recently\nproposed model-based evaluation methods, we find that the exponent of Taylor's\nlaw is a good indicator of model quality.", "published": "2019-06-22 03:24:32", "link": "http://arxiv.org/abs/1906.09379v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Online Topic Modeling Framework with Topics Automatically Labeled", "abstract": "In this paper, we propose a novel online topic tracking framework, named\nIEDL, for tracking the topic changes related to deep learning techniques on\nStack Exchange and automatically interpreting each identified topic. The\nproposed framework combines the prior topic distributions in a time window\nduring inferring the topics in current time slice, and introduces a new ranking\nscheme to select most representative phrases and sentences for the inferred\ntopics in each time slice. Experiments on 7,076 Stack Exchange posts show the\neffectiveness of IEDL in tracking topic changes and labeling topics.", "published": "2019-06-22 02:42:44", "link": "http://arxiv.org/abs/1907.01638v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "RLTM: An Efficient Neural IR Framework for Long Documents", "abstract": "Deep neural networks have achieved significant improvements in information\nretrieval (IR). However, most existing models are computational costly and can\nnot efficiently scale to long documents. This paper proposes a novel End-to-End\nneural ranking framework called Reinforced Long Text Matching (RLTM) which\nmatches a query with long documents efficiently and effectively. The core idea\nbehind the framework can be analogous to the human judgment process which\nfirstly locates the relevance parts quickly from the whole document and then\nmatches these parts with the query carefully to obtain the final label.\nFirstly, we select relevant sentences from the long documents by a coarse and\nefficient matching model. Secondly, we generate a relevance score by a more\nsophisticated matching model based on the sentence selected. The whole model is\ntrained jointly with reinforcement learning in a pairwise manner by maximizing\nthe expected score gaps between positive and negative examples. Experimental\nresults demonstrate that RLTM has greatly improved the efficiency and\neffectiveness of the state-of-the-art models.", "published": "2019-06-22 07:32:15", "link": "http://arxiv.org/abs/1906.09404v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Learning with fuzzy hypergraphs: a topical approach to query-oriented\n  text summarization", "abstract": "Existing graph-based methods for extractive document summarization represent\nsentences of a corpus as the nodes of a graph or a hypergraph in which edges\ndepict relationships of lexical similarity between sentences. Such approaches\nfail to capture semantic similarities between sentences when they express a\nsimilar information but have few words in common and are thus lexically\ndissimilar. To overcome this issue, we propose to extract semantic similarities\nbased on topical representations of sentences. Inspired by the Hierarchical\nDirichlet Process, we propose a probabilistic topic model in order to infer\ntopic distributions of sentences. As each topic defines a semantic connection\namong a group of sentences with a certain degree of membership for each\nsentence, we propose a fuzzy hypergraph model in which nodes are sentences and\nfuzzy hyperedges are topics. To produce an informative summary, we extract a\nset of sentences from the corpus by simultaneously maximizing their relevance\nto a user-defined query, their centrality in the fuzzy hypergraph and their\ncoverage of topics present in the corpus. We formulate a polynomial time\nalgorithm building on the theory of submodular functions to solve the\nassociated optimization problem. A thorough comparative analysis with other\ngraph-based summarization systems is included in the paper. Our obtained\nresults show the superiority of our method in terms of content coverage of the\nsummaries.", "published": "2019-06-22 13:28:32", "link": "http://arxiv.org/abs/1906.09445v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semantically Driven Auto-completion", "abstract": "The Bloomberg Terminal has been a leading source of financial data and\nanalytics for over 30 years. Through its thousands of functions, the Terminal\nallows its users to query and run analytics over a large array of data sources,\nincluding structured, semi-structured, and unstructured data; as well as plot\ncharts, set up event-driven alerts and triggers, create interactive maps,\nexchange information via instant and email-style messages, and so on. To\nimprove user experience, we have been building question answering systems that\ncan understand a wide range of natural language constructions for various\ndomains that are of fundamental interest to our users. Such natural language\ninterfaces, while exceedingly helpful to users, introduce a number of usability\nchallenges of their own. We tackle some of these challenges through\nauto-completion for query formulation. A distinguishing mark of our\nauto-complete systems is that they are based on and guided by corresponding\nsemantic parsing systems. We describe the auto-complete problem as it arises in\nthis setting, the novel algorithms that we use to solve it, and report on the\nquality of the results and the efficiency of our approach.", "published": "2019-06-22 14:02:14", "link": "http://arxiv.org/abs/1906.09450v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Bandit Approach to Posterior Dialog Orchestration Under a Budget", "abstract": "Building multi-domain AI agents is a challenging task and an open problem in\nthe area of AI. Within the domain of dialog, the ability to orchestrate\nmultiple independently trained dialog agents, or skills, to create a unified\nsystem is of particular significance. In this work, we study the task of online\nposterior dialog orchestration, where we define posterior orchestration as the\ntask of selecting a subset of skills which most appropriately answer a user\ninput using features extracted from both the user input and the individual\nskills. To account for the various costs associated with extracting skill\nfeatures, we consider online posterior orchestration under a skill execution\nbudget. We formalize this setting as Context Attentive Bandit with Observations\n(CABO), a variant of context attentive bandits, and evaluate it on simulated\nnon-conversational and proprietary conversational datasets.", "published": "2019-06-22 04:02:26", "link": "http://arxiv.org/abs/1906.09384v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "End-to-End ASR for Code-switched Hindi-English Speech", "abstract": "End-to-end (E2E) models have been explored for large speech corpora and have\nbeen found to match or outperform traditional pipeline-based systems in some\nlanguages. However, most prior work on end-to-end models use speech corpora\nexceeding hundreds or thousands of hours. In this study, we explore end-to-end\nmodels for code-switched Hindi-English language with less than 50 hours of\ndata. We utilize two specific measures to improve network performance in the\nlow-resource setting, namely multi-task learning (MTL) and balancing the corpus\nto deal with the inherent class imbalance problem i.e. the skewed frequency\ndistribution over graphemes. We compare the results of the proposed approaches\nwith traditional, cascaded ASR systems. While the lack of data adversely\naffects the performance of end-to-end models, we see promising improvements\nwith MTL and balancing the corpus.", "published": "2019-06-22 10:23:42", "link": "http://arxiv.org/abs/1906.09426v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Retrieving Sequential Information for Non-Autoregressive Neural Machine\n  Translation", "abstract": "Non-Autoregressive Transformer (NAT) aims to accelerate the Transformer model\nthrough discarding the autoregressive mechanism and generating target words\nindependently, which fails to exploit the target sequential information.\nOver-translation and under-translation errors often occur for the above reason,\nespecially in the long sentence translation scenario. In this paper, we propose\ntwo approaches to retrieve the target sequential information for NAT to enhance\nits translation ability while preserving the fast-decoding property. Firstly,\nwe propose a sequence-level training method based on a novel reinforcement\nalgorithm for NAT (Reinforce-NAT) to reduce the variance and stabilize the\ntraining procedure. Secondly, we propose an innovative Transformer decoder\nnamed FS-decoder to fuse the target sequential information into the top layer\nof the decoder. Experimental results on three translation tasks show that the\nReinforce-NAT surpasses the baseline NAT system by a significant margin on BLEU\nwithout decelerating the decoding speed and the FS-decoder achieves comparable\ntranslation performance to the autoregressive Transformer with considerable\nspeedup.", "published": "2019-06-22 13:20:57", "link": "http://arxiv.org/abs/1906.09444v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Keyword Spotting for Hearing Assistive Devices Robust to External\n  Speakers", "abstract": "Keyword spotting (KWS) is experiencing an upswing due to the pervasiveness of\nsmall electronic devices that allow interaction with them via speech. Often,\nKWS systems are speaker-independent, which means that any person --user or\nnot-- might trigger them. For applications like KWS for hearing assistive\ndevices this is unacceptable, as only the user must be allowed to handle them.\nIn this paper we propose KWS for hearing assistive devices that is robust to\nexternal speakers. A state-of-the-art deep residual network for small-footprint\nKWS is regarded as a basis to build upon. By following a multi-task learning\nscheme, this system is extended to jointly perform KWS and users'\nown-voice/external speaker detection with a negligible increase in the number\nof parameters. For experiments, we generate from the Google Speech Commands\nDataset a speech corpus emulating hearing aids as a capturing device. Our\nresults show that this multi-task deep residual network is able to achieve a\nKWS accuracy relative improvement of around 32% with respect to a system that\ndoes not deal with external speakers.", "published": "2019-06-22 09:51:14", "link": "http://arxiv.org/abs/1906.09417v2", "categories": ["cs.SD", "cs.HC", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
