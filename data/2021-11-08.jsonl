{"title": "Detecting Depression in Thai Blog Posts: a Dataset and a Baseline", "abstract": "We present the first openly available corpus for detecting depression in\nThai. Our corpus is compiled by expert verified cases of depression in several\nonline blogs. We experiment with two different LSTM based models and two\ndifferent BERT based models. We achieve a 77.53\\% accuracy with a Thai BERT\nmodel in detecting depression. This establishes a good baseline for future\nresearcher on the same corpus. Furthermore, we identify a need for Thai\nembeddings that have been trained on a more varied corpus than Wikipedia. Our\ncorpus, code and trained models have been released openly on Zenodo.", "published": "2021-11-08 15:36:21", "link": "http://arxiv.org/abs/2111.04574v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JaMIE: A Pipeline Japanese Medical Information Extraction System", "abstract": "We present an open-access natural language processing toolkit for Japanese\nmedical information extraction. We first propose a novel relation annotation\nschema for investigating the medical and temporal relations between medical\nentities in Japanese medical reports. We experiment with the practical\nannotation scenarios by separately annotating two different types of reports.\nWe design a pipeline system with three components for recognizing medical\nentities, classifying entity modalities, and extracting relations. The\nempirical results show accurate analyzing performance and suggest the\nsatisfactory annotation quality, the effective annotation strategy for\ntargeting report types, and the superiority of the latest contextual embedding\nmodels.", "published": "2021-11-08 03:54:09", "link": "http://arxiv.org/abs/2111.04261v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Debiasing Temporal Sentence Grounding in Video", "abstract": "The temporal sentence grounding in video (TSGV) task is to locate a temporal\nmoment from an untrimmed video, to match a language query, i.e., a sentence.\nWithout considering bias in moment annotations (e.g., start and end positions\nin a video), many models tend to capture statistical regularities of the moment\nannotations, and do not well learn cross-modal reasoning between video and\nlanguage query. In this paper, we propose two debiasing strategies, data\ndebiasing and model debiasing, to \"force\" a TSGV model to capture cross-modal\ninteractions. Data debiasing performs data oversampling through video\ntruncation to balance moment temporal distribution in train set. Model\ndebiasing leverages video-only and query-only models to capture the\ndistribution bias, and forces the model to learn cross-modal interactions.\nUsing VSLNet as the base model, we evaluate impact of the two strategies on two\ndatasets that contain out-of-distribution test instances. Results show that\nboth strategies are effective in improving model generalization capability.\nEquipped with both debiasing strategies, VSLNet achieves best results on both\ndatasets.", "published": "2021-11-08 08:18:25", "link": "http://arxiv.org/abs/2111.04321v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Ontology-based question answering over corporate structured data", "abstract": "Ontology-based approach to the Natural Language Understanding (NLU)\nprocessing allows to improve questions answering quality in dialogue systems.\nWe describe our NLU engine architecture and evaluate its implementation. The\nengine transforms user input into the SPARQL SELECT, ASK or INSERT query to the\nknowledge graph provided by the ontology-based data virtualization platform.\nThe transformation is based on the lexical level of the knowledge graph built\naccording to the Ontolex ontology. The described approach can be applied for\ngraph data population tasks and to the question answering systems\nimplementation, including chat bots. We describe the dialogue engine for a chat\nbot which can keep the conversation context and ask clarifying questions,\nsimulating some aspects of the human logical thinking. Our approach uses\ngraph-based algorithms to avoid gathering datasets, required in the neural\nnets-based approaches, and provide better explainability of our models. Using\nquestion answering engine in conjunction with data virtualization layer over\nthe corporate data sources allows extracting facts from the structured data to\nbe used in conversation.", "published": "2021-11-08 13:49:15", "link": "http://arxiv.org/abs/2111.04507v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Anagrammatic quotients of free groups", "abstract": "We determine the structure of the quotient of the free group on 26 generators\nby English language anagrams. This group admits a surprisingly simple\npresentation as a quotient of the free group by 301 of the possible 325\ncommutators of pairs of generators; all of the 24 missing commutators involve\nat least one of the letters j, q, x, z. We describe the algorithm which can be\nused to determine this group given any dictionary, and provide examples from\nthe SOWPODS scrabble dictionary witnessing the 301 commutators found.", "published": "2021-11-08 14:03:09", "link": "http://arxiv.org/abs/2111.04517v1", "categories": ["math.GR", "cs.CL"], "primary_category": "math.GR"}
{"title": "A Survey on Green Deep Learning", "abstract": "In recent years, larger and deeper models are springing up and continuously\npushing state-of-the-art (SOTA) results across various fields like natural\nlanguage processing (NLP) and computer vision (CV). However, despite promising\nresults, it needs to be noted that the computations required by SOTA models\nhave been increased at an exponential rate. Massive computations not only have\na surprisingly large carbon footprint but also have negative effects on\nresearch inclusiveness and deployment on real-world applications.\n  Green deep learning is an increasingly hot research field that appeals to\nresearchers to pay attention to energy usage and carbon emission during model\ntraining and inference. The target is to yield novel results with lightweight\nand efficient technologies. Many technologies can be used to achieve this goal,\nlike model compression and knowledge distillation. This paper focuses on\npresenting a systematic review of the development of Green deep learning\ntechnologies. We classify these approaches into four categories: (1) compact\nnetworks, (2) energy-efficient training strategies, (3) energy-efficient\ninference approaches, and (4) efficient data usage. For each category, we\ndiscuss the progress that has been achieved and the unresolved challenges.", "published": "2021-11-08 16:55:03", "link": "http://arxiv.org/abs/2111.05193v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Auto-Encoding Knowledge Graph for Unsupervised Medical Report Generation", "abstract": "Medical report generation, which aims to automatically generate a long and\ncoherent report of a given medical image, has been receiving growing research\ninterests. Existing approaches mainly adopt a supervised manner and heavily\nrely on coupled image-report pairs. However, in the medical domain, building a\nlarge-scale image-report paired dataset is both time-consuming and expensive.\nTo relax the dependency on paired data, we propose an unsupervised model\nKnowledge Graph Auto-Encoder (KGAE) which accepts independent sets of images\nand reports in training. KGAE consists of a pre-constructed knowledge graph, a\nknowledge-driven encoder and a knowledge-driven decoder. The knowledge graph\nworks as the shared latent space to bridge the visual and textual domains; The\nknowledge-driven encoder projects medical images and reports to the\ncorresponding coordinates in this latent space and the knowledge-driven decoder\ngenerates a medical report given a coordinate in this space. Since the\nknowledge-driven encoder and decoder can be trained with independent sets of\nimages and reports, KGAE is unsupervised. The experiments show that the\nunsupervised KGAE generates desirable medical reports without using any\nimage-report training pairs. Moreover, KGAE can also work in both\nsemi-supervised and supervised settings, and accept paired images and reports\nin training. By further fine-tuning with image-report pairs, KGAE consistently\noutperforms the current state-of-the-art models on two datasets.", "published": "2021-11-08 08:10:47", "link": "http://arxiv.org/abs/2111.04318v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "AI-UPV at IberLEF-2021 DETOXIS task: Toxicity Detection in\n  Immigration-Related Web News Comments Using Transformers and Statistical\n  Models", "abstract": "This paper describes our participation in the DEtection of TOXicity in\ncomments In Spanish (DETOXIS) shared task 2021 at the 3rd Workshop on Iberian\nLanguages Evaluation Forum. The shared task is divided into two related\nclassification tasks: (i) Task 1: toxicity detection and; (ii) Task 2: toxicity\nlevel detection. They focus on the xenophobic problem exacerbated by the spread\nof toxic comments posted in different online news articles related to\nimmigration. One of the necessary efforts towards mitigating this problem is to\ndetect toxicity in the comments. Our main objective was to implement an\naccurate model to detect xenophobia in comments about web news articles within\nthe DETOXIS shared task 2021, based on the competition's official metrics: the\nF1-score for Task 1 and the Closeness Evaluation Metric (CEM) for Task 2. To\nsolve the tasks, we worked with two types of machine learning models: (i)\nstatistical models and (ii) Deep Bidirectional Transformers for Language\nUnderstanding (BERT) models. We obtained our best results in both tasks using\nBETO, an BERT model trained on a big Spanish corpus. We obtained the 3rd place\nin Task 1 official ranking with the F1-score of 0.5996, and we achieved the 6th\nplace in Task 2 official ranking with the CEM of 0.7142. Our results suggest:\n(i) BERT models obtain better results than statistical models for toxicity\ndetection in text comments; (ii) Monolingual BERT models have an advantage over\nmultilingual BERT models in toxicity detection in text comments in their\npre-trained language.", "published": "2021-11-08 14:24:21", "link": "http://arxiv.org/abs/2111.04530v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sexism Prediction in Spanish and English Tweets Using Monolingual and\n  Multilingual BERT and Ensemble Models", "abstract": "The popularity of social media has created problems such as hate speech and\nsexism. The identification and classification of sexism in social media are\nvery relevant tasks, as they would allow building a healthier social\nenvironment. Nevertheless, these tasks are considerably challenging. This work\nproposes a system to use multilingual and monolingual BERT and data points\ntranslation and ensemble strategies for sexism identification and\nclassification in English and Spanish. It was conducted in the context of the\nsEXism Identification in Social neTworks shared 2021 (EXIST 2021) task,\nproposed by the Iberian Languages Evaluation Forum (IberLEF). The proposed\nsystem and its main components are described, and an in-depth hyperparameters\nanalysis is conducted. The main results observed were: (i) the system obtained\nbetter results than the baseline model (multilingual BERT); (ii) ensemble\nmodels obtained better results than monolingual models; and (iii) an ensemble\nmodel considering all individual models and the best standardized values\nobtained the best accuracies and F1-scores for both tasks. This work obtained\nfirst place in both tasks at EXIST, with the highest accuracies (0.780 for task\n1 and 0.658 for task 2) and F1-scores (F1-binary of 0.780 for task 1 and\nF1-macro of 0.579 for task 2).", "published": "2021-11-08 15:01:06", "link": "http://arxiv.org/abs/2111.04551v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Visual Question Answering based on Formal Logic", "abstract": "Visual question answering (VQA) has been gaining a lot of traction in the\nmachine learning community in the recent years due to the challenges posed in\nunderstanding information coming from multiple modalities (i.e., images,\nlanguage). In VQA, a series of questions are posed based on a set of images and\nthe task at hand is to arrive at the answer. To achieve this, we take a\nsymbolic reasoning based approach using the framework of formal logic. The\nimage and the questions are converted into symbolic representations on which\nexplicit reasoning is performed. We propose a formal logic framework where (i)\nimages are converted to logical background facts with the help of scene graphs,\n(ii) the questions are translated to first-order predicate logic clauses using\na transformer based deep learning model, and (iii) perform satisfiability\nchecks, by using the background knowledge and the grounding of predicate\nclauses, to obtain the answer. Our proposed method is highly interpretable and\neach step in the pipeline can be easily analyzed by a human. We validate our\napproach on the CLEVR and the GQA dataset. We achieve near perfect accuracy of\n99.6% on the CLEVR dataset comparable to the state of art models, showcasing\nthat formal logic is a viable tool to tackle visual question answering. Our\nmodel is also data efficient, achieving 99.1% accuracy on CLEVR dataset when\ntrained on just 10% of the training data.", "published": "2021-11-08 19:43:53", "link": "http://arxiv.org/abs/2111.04785v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Explaining Face Presentation Attack Detection Using Natural Language", "abstract": "A large number of deep neural network based techniques have been developed to\naddress the challenging problem of face presentation attack detection (PAD).\nWhereas such techniques' focus has been on improving PAD performance in terms\nof classification accuracy and robustness against unseen attacks and\nenvironmental conditions, there exists little attention on the explainability\nof PAD predictions. In this paper, we tackle the problem of explaining PAD\npredictions through natural language. Our approach passes feature\nrepresentations of a deep layer of the PAD model to a language model to\ngenerate text describing the reasoning behind the PAD prediction. Due to the\nlimited amount of annotated data in our study, we apply a light-weight LSTM\nnetwork as our natural language generation model. We investigate how the\nquality of the generated explanations is affected by different loss functions,\nincluding the commonly used word-wise cross entropy loss, a sentence\ndiscriminative loss, and a sentence semantic loss. We perform our experiments\nusing face images from a dataset consisting of 1,105 bona-fide and 924\npresentation attack samples. Our quantitative and qualitative results show the\neffectiveness of our model for generating proper PAD explanations through text\nas well as the power of the sentence-wise losses. To the best of our knowledge,\nthis is the first introduction of a joint biometrics-NLP task. Our dataset can\nbe obtained through our GitHub page.", "published": "2021-11-08 22:55:55", "link": "http://arxiv.org/abs/2111.04862v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.CV"}
{"title": "Cascaded Multilingual Audio-Visual Learning from Videos", "abstract": "In this paper, we explore self-supervised audio-visual models that learn from\ninstructional videos. Prior work has shown that these models can relate spoken\nwords and sounds to visual content after training on a large-scale dataset of\nvideos, but they were only trained and evaluated on videos in English. To learn\nmultilingual audio-visual representations, we propose a cascaded approach that\nleverages a model trained on English videos and applies it to audio-visual data\nin other languages, such as Japanese videos. With our cascaded approach, we\nshow an improvement in retrieval performance of nearly 10x compared to training\non the Japanese videos solely. We also apply the model trained on English\nvideos to Japanese and Hindi spoken captions of images, achieving\nstate-of-the-art performance.", "published": "2021-11-08 20:53:50", "link": "http://arxiv.org/abs/2111.04823v1", "categories": ["cs.CL", "cs.CV", "cs.MM", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CL"}
{"title": "Inter-channel Conv-TasNet for multichannel speech enhancement", "abstract": "Speech enhancement in multichannel settings has been realized by utilizing\nthe spatial information embedded in multiple microphone signals. Moreover, deep\nneural networks (DNNs) have been recently advanced in this field; however,\nstudies on the efficient multichannel network structure fully exploiting\nspatial information and inter-channel relationships is still in its early\nstages. In this study, we propose an end-to-end time-domain speech enhancement\nnetwork that can facilitate the use of inter-channel relationships at\nindividual layers of a DNN. The proposed technique is based on a fully\nconvolutional time-domain audio separation network (Conv-TasNet), originally\ndeveloped for speech separation tasks. We extend Conv-TasNet into several forms\nthat can handle multichannel input signals and learn inter-channel\nrelationships. To this end, we modify the encoder-mask-decoder structures of\nthe network to be compatible with 3-D tensors defined over spatial channels,\nfeatures, and time dimensions. In particular, we conduct extensive parameter\nanalyses on the convolution structure and propose independent assignment of the\ndepthwise and 1$\\times$1 convolution layers to the feature and spatial\ndimensions, respectively. We demonstrate that the enriched inter-channel\ninformation from the proposed network plays a significant role in suppressing\nnoisy signals impinging from various directions. The proposed inter-channel\nConv-TasNet outperforms the state-of-the-art multichannel variants of neural\nnetworks, even with one-tenth of their parameter size. The performance of the\nproposed model is evaluated using the CHiME-3 dataset, which exhibits a\nremarkable improvement in SDR, PESQ, and STOI.", "published": "2021-11-08 07:52:37", "link": "http://arxiv.org/abs/2111.04312v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A hemispheric two-channel code accounts for binaural unmasking in humans", "abstract": "Sound in noise is better detected or understood if target and masking sources\noriginate from different locations. Mammalian physiology suggests that the\nneurocomputational process that underlies this binaural unmasking is based on\ntwo hemispheric channels that encode interaural differences in their relative\nneuronal activity. Here, we introduce a mathematical formulation of the\ntwo-channel model - the complex-valued correlation coefficient. We show that\nthis formulation quantifies the amount of temporal fluctuations in interaural\ndifferences, which we suggest underlie binaural unmasking. We applied this\nmodel to an extensive library of psychoacoustic experiments, accounting for 98%\nof the variance across eight studies. Combining physiological plausibility with\nits success in explaining behavioral data, the proposed mechanism is a\nsignificant step towards a unified understanding of binaural unmasking and the\nencoding of interaural differences in general.", "published": "2021-11-08 17:05:55", "link": "http://arxiv.org/abs/2111.04637v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Characterizing the adversarial vulnerability of speech self-supervised\n  learning", "abstract": "A leaderboard named Speech processing Universal PERformance Benchmark\n(SUPERB), which aims at benchmarking the performance of a shared\nself-supervised learning (SSL) speech model across various downstream speech\ntasks with minimal modification of architectures and small amount of data, has\nfueled the research for speech representation learning. The SUPERB demonstrates\nspeech SSL upstream models improve the performance of various downstream tasks\nthrough just minimal adaptation. As the paradigm of the self-supervised\nlearning upstream model followed by downstream tasks arouses more attention in\nthe speech community, characterizing the adversarial robustness of such\nparadigm is of high priority. In this paper, we make the first attempt to\ninvestigate the adversarial vulnerability of such paradigm under the attacks\nfrom both zero-knowledge adversaries and limited-knowledge adversaries. The\nexperimental results illustrate that the paradigm proposed by SUPERB is\nseriously vulnerable to limited-knowledge adversaries, and the attacks\ngenerated by zero-knowledge adversaries are with transferability. The XAB test\nverifies the imperceptibility of crafted adversarial attacks.", "published": "2021-11-08 08:44:04", "link": "http://arxiv.org/abs/2111.04330v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RawBoost: A Raw Data Boosting and Augmentation Method applied to\n  Automatic Speaker Verification Anti-Spoofing", "abstract": "This paper introduces RawBoost, a data boosting and augmentation method for\nthe design of more reliable spoofing detection solutions which operate directly\nupon raw waveform inputs. While RawBoost requires no additional data sources,\ne.g. noise recordings or impulse responses and is data, application and model\nagnostic, it is designed for telephony scenarios. Based upon the combination of\nlinear and non-linear convolutive noise, impulsive signal-dependent additive\nnoise and stationary signal-independent additive noise, RawBoost models\nnuisance variability stemming from, e.g., encoding, transmission, microphones\nand amplifiers, and both linear and non-linear distortion. Experiments\nperformed using the ASVspoof 2021 logical access database show that RawBoost\nimproves the performance of a state-of-the-art raw end-to-end baseline system\nby 27% relative and is only outperformed by solutions that either depend on\nexternal data or that require additional intervention at the model level.", "published": "2021-11-08 12:50:51", "link": "http://arxiv.org/abs/2111.04433v2", "categories": ["eess.AS", "cs.CR", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "SEOFP-NET: Compression and Acceleration of Deep Neural Networks for\n  Speech Enhancement Using Sign-Exponent-Only Floating-Points", "abstract": "Numerous compression and acceleration strategies have achieved outstanding\nresults on classification tasks in various fields, such as computer vision and\nspeech signal processing. Nevertheless, the same strategies have yielded\nungratified performance on regression tasks because the nature between these\nand classification tasks differs. In this paper, a novel sign-exponent-only\nfloating-point network (SEOFP-NET) technique is proposed to compress the model\nsize and accelerate the inference time for speech enhancement, a regression\ntask of speech signal processing. The proposed method compressed the sizes of\ndeep neural network (DNN)-based speech enhancement models by quantizing the\nfraction bits of single-precision floating-point parameters during training.\nBefore inference implementation, all parameters in the trained SEOFP-NET model\nare slightly adjusted to accelerate the inference time by replacing the\nfloating-point multiplier with an integer-adder. For generalization, the\nSEOFP-NET technique is introduced to different speech enhancement tasks in\nspeech signal processing with different model architectures under various\ncorpora. The experimental results indicate that the size of SEOFP-NET models\ncan be significantly compressed by up to 81.249% without noticeably downgrading\ntheir speech enhancement performance, and the inference time can be accelerated\nto 1.212x compared with the baseline models. The results also verify that the\nproposed SEOFP-NET can cooperate with other efficiency strategies to achieve a\nsynergy effect for model compression. In addition, the just noticeable\ndifference (JND) was applied to the user study experiment to statistically\nanalyze the effect of speech enhancement on listening. The results indicate\nthat the listeners cannot facilely differentiate between the enhanced speech\nsignals processed by the baseline model and the proposed SEOFP-NET.", "published": "2021-11-08 12:57:41", "link": "http://arxiv.org/abs/2111.04436v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning Filterbanks for End-to-End Acoustic Beamforming", "abstract": "Recent work on monaural source separation has shown that performance can be\nincreased by using fully learned filterbanks with short windows. On the other\nhand it is widely known that, for conventional beamforming techniques,\nperformance increases with long analysis windows. This applies also to most\nhybrid neural beamforming methods which rely on a deep neural network (DNN) to\nestimate the spatial covariance matrices. In this work we try to bridge the gap\nbetween these two worlds and explore fully end-to-end hybrid neural beamforming\nin which, instead of using the Short-Time-Fourier Transform, also the analysis\nand synthesis filterbanks are learnt jointly with the DNN. In detail, we\nexplore two different types of learned filterbanks: fully learned and analytic.\nWe perform a detailed analysis using the recent Clarity Challenge data and show\nthat by using learnt filterbanks it is possible to surpass oracle-mask based\nbeamforming for short windows.", "published": "2021-11-08 16:36:34", "link": "http://arxiv.org/abs/2111.04614v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Losses, Dissonances, and Distortions", "abstract": "In this paper I present a study in using the losses and gradients obtained\nduring the training of a simple function approximator as a mechanism for\ncreating musical dissonance and visual distortion in a solo piano performance\nsetting. These dissonances and distortions become part of an artistic\nperformance not just by affecting the visualizations, but also by affecting the\nartistic musical performance. The system is designed such that the performer\ncan in turn affect the training process itself, thereby creating a closed\nfeedback loop between two processes: the training of a machine learning model\nand the performance of an improvised piano piece.", "published": "2021-11-08 15:55:02", "link": "http://arxiv.org/abs/2111.05128v1", "categories": ["cs.LG", "cs.AI", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
