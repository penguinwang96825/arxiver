{"title": "Training IBM Watson using Automatically Generated Question-Answer Pairs", "abstract": "IBM Watson is a cognitive computing system capable of question answering in\nnatural languages. It is believed that IBM Watson can understand large corpora\nand answer relevant questions more effectively than any other\nquestion-answering system currently available. To unleash the full power of\nWatson, however, we need to train its instance with a large number of\nwell-prepared question-answer pairs. Obviously, manually generating such pairs\nin a large quantity is prohibitively time consuming and significantly limits\nthe efficiency of Watson's training. Recently, a large-scale dataset of over 30\nmillion question-answer pairs was reported. Under the assumption that using\nsuch an automatically generated dataset could relieve the burden of manual\nquestion-answer generation, we tried to use this dataset to train an instance\nof Watson and checked the training efficiency and accuracy. According to our\nexperiments, using this auto-generated dataset was effective for training\nWatson, complementing manually crafted question-answer pairs. To the best of\nthe authors' knowledge, this work is the first attempt to use a large-scale\ndataset of automatically generated question-answer pairs for training IBM\nWatson. We anticipate that the insights and lessons obtained from our\nexperiments will be useful for researchers who want to expedite Watson training\nleveraged by automatically generated question-answer pairs.", "published": "2016-11-12 01:49:48", "link": "http://arxiv.org/abs/1611.03932v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistically Regularized LSTMs for Sentiment Classification", "abstract": "Sentiment understanding has been a long-term goal of AI in the past decades.\nThis paper deals with sentence-level sentiment classification. Though a variety\nof neural network models have been proposed very recently, however, previous\nmodels either depend on expensive phrase-level annotation, whose performance\ndrops substantially when trained with only sentence-level annotation; or do not\nfully employ linguistic resources (e.g., sentiment lexicons, negation words,\nintensity words), thus not being able to produce linguistically coherent\nrepresentations. In this paper, we propose simple models trained with\nsentence-level annotation, but also attempt to generating linguistically\ncoherent representations by employing regularizers that model the linguistic\nrole of sentiment lexicons, negation words, and intensity words. Results show\nthat our models are effective to capture the sentiment shifting effect of\nsentiment, negation, and intensity words, while still obtain competitive\nresults without sacrificing the models' simplicity.", "published": "2016-11-12 03:55:10", "link": "http://arxiv.org/abs/1611.03949v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Language Identification Using Convolutional Recurrent Neural\n  Network", "abstract": "Language Identification, being an important aspect of Automatic Speaker\nRecognition has had many changes and new approaches to ameliorate performance\nover the last decade. We compare the performance of using audio spectrum in the\nlog scale and using Polyphonic sound sequences from raw audio samples to train\nthe neural network and to classify speech as either English or Spanish. To\nachieve this, we use the novel approach of using a Convolutional Recurrent\nNeural Network using Long Short Term Memory (LSTM) or a Gated Recurrent Unit\n(GRU) for forward propagation of the neural network. Our hypothesis is that the\nperformance of using polyphonic sound sequence as features and both LSTM and\nGRU as the gating mechanisms for the neural network outperform the traditional\nMFCC features using a unidirectional Deep Neural Network.", "published": "2016-11-12 15:59:22", "link": "http://arxiv.org/abs/1611.04010v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-automatic Simultaneous Interpreting Quality Evaluation", "abstract": "Increasing interpreting needs a more objective and automatic measurement. We\nhold a basic idea that 'translating means translating meaning' in that we can\nassessment interpretation quality by comparing the meaning of the interpreting\noutput with the source input. That is, a translation unit of a 'chunk' named\nFrame which comes from frame semantics and its components named Frame Elements\n(FEs) which comes from Frame Net are proposed to explore their matching rate\nbetween target and source texts. A case study in this paper verifies the\nusability of semi-automatic graded semantic-scoring measurement for human\nsimultaneous interpreting and shows how to use frame and FE matches to score.\nExperiments results show that the semantic-scoring metrics have a significantly\ncorrelation coefficient with human judgment.", "published": "2016-11-12 23:24:00", "link": "http://arxiv.org/abs/1611.04052v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Knowledge Graph Embeddings for Cross-lingual Knowledge\n  Alignment", "abstract": "Many recent works have demonstrated the benefits of knowledge graph\nembeddings in completing monolingual knowledge graphs. Inasmuch as related\nknowledge bases are built in several different languages, achieving\ncross-lingual knowledge alignment will help people in constructing a coherent\nknowledge base, and assist machines in dealing with different expressions of\nentity relationships across diverse human languages. Unfortunately, achieving\nthis highly desirable crosslingual alignment by human labor is very costly and\nerrorprone. Thus, we propose MTransE, a translation-based model for\nmultilingual knowledge graph embeddings, to provide a simple and automated\nsolution. By encoding entities and relations of each language in a separated\nembedding space, MTransE provides transitions for each embedding vector to its\ncross-lingual counterparts in other spaces, while preserving the\nfunctionalities of monolingual embeddings. We deploy three different techniques\nto represent cross-lingual transitions, namely axis calibration, translation\nvectors, and linear transformations, and derive five variants for MTransE using\ndifferent loss functions. Our models can be trained on partially aligned\ngraphs, where just a small portion of triples are aligned with their\ncross-lingual counterparts. The experiments on cross-lingual entity matching\nand triple-wise alignment verification show promising results, with some\nvariants consistently outperforming others on different tasks. We also explore\nhow MTransE preserves the key properties of its monolingual counterpart TransE.", "published": "2016-11-12 04:28:04", "link": "http://arxiv.org/abs/1611.03954v3", "categories": ["cs.AI", "cs.CL", "I.2.4; I.2.6; I.2.7"], "primary_category": "cs.AI"}
{"title": "1.5 billion words Arabic Corpus", "abstract": "This study is an attempt to build a contemporary linguistic corpus for Arabic\nlanguage. The corpus produced, is a text corpus includes more than five million\nnewspaper articles. It contains over a billion and a half words in total, out\nof which, there is about three million unique words. The data were collected\nfrom newspaper articles in ten major news sources from eight Arabic countries,\nover a period of fourteen years. The corpus was encoded with two types of\nencoding, namely: UTF-8, and Windows CP-1256. Also it was marked with two\nmark-up languages, namely: SGML, and XML.", "published": "2016-11-12 18:41:58", "link": "http://arxiv.org/abs/1611.04033v1", "categories": ["cs.CL", "cs.DL", "cs.IR"], "primary_category": "cs.CL"}
