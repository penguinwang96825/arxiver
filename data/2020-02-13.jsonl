{"title": "Keyphrase Extraction with Span-based Feature Representations", "abstract": "Keyphrases are capable of providing semantic metadata characterizing\ndocuments and producing an overview of the content of a document. Since\nkeyphrase extraction is able to facilitate the management, categorization, and\nretrieval of information, it has received much attention in recent years. There\nare three approaches to address keyphrase extraction: (i) traditional two-step\nranking method, (ii) sequence labeling and (iii) generation using neural\nnetworks. Two-step ranking approach is based on feature engineering, which is\nlabor intensive and domain dependent. Sequence labeling is not able to tackle\noverlapping phrases. Generation methods (i.e., Sequence-to-sequence neural\nnetwork models) overcome those shortcomings, so they have been widely studied\nand gain state-of-the-art performance. However, generation methods can not\nutilize context information effectively. In this paper, we propose a novelty\nSpan Keyphrase Extraction model that extracts span-based feature representation\nof keyphrase directly from all the content tokens. In this way, our model\nobtains representation for each keyphrase and further learns to capture the\ninteraction between keyphrases in one document to get better ranking results.\nIn addition, with the help of tokens, our model is able to extract overlapped\nkeyphrases. Experimental results on the benchmark datasets show that our\nproposed model outperforms the existing methods by a large margin.", "published": "2020-02-13 09:48:31", "link": "http://arxiv.org/abs/2002.05407v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparison of Turkish Word Representations Trained on Different\n  Morphological Forms", "abstract": "Increased popularity of different text representations has also brought many\nimprovements in Natural Language Processing (NLP) tasks. Without need of\nsupervised data, embeddings trained on large corpora provide us meaningful\nrelations to be used on different NLP tasks. Even though training these vectors\nis relatively easy with recent methods, information gained from the data\nheavily depends on the structure of the corpus language. Since the popularly\nresearched languages have a similar morphological structure, problems occurring\nfor morphologically rich languages are mainly disregarded in studies. For\nmorphologically rich languages, context-free word vectors ignore morphological\nstructure of languages. In this study, we prepared texts in morphologically\ndifferent forms in a morphologically rich language, Turkish, and compared the\nresults on different intrinsic and extrinsic tasks. To see the effect of\nmorphological structure, we trained word2vec model on texts which lemma and\nsuffixes are treated differently. We also trained subword model fastText and\ncompared the embeddings on word analogy, text classification, sentimental\nanalysis, and language model tasks.", "published": "2020-02-13 10:09:31", "link": "http://arxiv.org/abs/2002.05417v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sparse and Structured Visual Attention", "abstract": "Visual attention mechanisms are widely used in multimodal tasks, as visual\nquestion answering (VQA). One drawback of softmax-based attention mechanisms is\nthat they assign some probability mass to all image regions, regardless of\ntheir adjacency structure and of their relevance to the text. In this paper, to\nbetter link the image structure with the text, we replace the traditional\nsoftmax attention mechanism with two alternative sparsity-promoting\ntransformations: sparsemax, which is able to select only the relevant regions\n(assigning zero weight to the rest), and a newly proposed Total-Variation\nSparse Attention (TVmax), which further encourages the joint selection of\nadjacent spatial locations. Experiments in VQA show gains in accuracy as well\nas higher similarity to human attention, which suggests better\ninterpretability.", "published": "2020-02-13 15:08:12", "link": "http://arxiv.org/abs/2002.05556v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis Using Averaged Weighted Word Vector Features", "abstract": "People use the world wide web heavily to share their experience with entities\nsuch as products, services, or travel destinations. Texts that provide online\nfeedback in the form of reviews and comments are essential to make consumer\ndecisions. These comments create a valuable source that may be used to measure\nsatisfaction related to products or services. Sentiment analysis is the task of\nidentifying opinions expressed in such text fragments. In this work, we develop\ntwo methods that combine different types of word vectors to learn and estimate\npolarity of reviews. We develop average review vectors from word vectors and\nadd weights to this review vectors using word frequencies in positive and\nnegative sensitivity-tagged reviews. We applied the methods to several datasets\nfrom different domains that are used as standard benchmarks for sentiment\nanalysis. We ensemble the techniques with each other and existing methods, and\nwe make a comparison with the approaches in the literature. The results show\nthat the performances of our approaches outperform the state-of-the-art success\nrates.", "published": "2020-02-13 16:30:34", "link": "http://arxiv.org/abs/2002.05606v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Pre-Training for Query Rewriting in A Spoken Language Understanding\n  System", "abstract": "Query rewriting (QR) is an increasingly important technique to reduce\ncustomer friction caused by errors in a spoken language understanding pipeline,\nwhere the errors originate from various sources such as speech recognition\nerrors, language understanding errors or entity resolution errors. In this\nwork, we first propose a neural-retrieval based approach for query rewriting.\nThen, inspired by the wide success of pre-trained contextual language\nembeddings, and also as a way to compensate for insufficient QR training data,\nwe propose a language-modeling (LM) based approach to pre-train query\nembeddings on historical user conversation data with a voice assistant. In\naddition, we propose to use the NLU hypotheses generated by the language\nunderstanding system to augment the pre-training. Our experiments show\npre-training provides rich prior information and help the QR task achieve\nstrong performance. We also show joint pre-training with NLU hypotheses has\nfurther benefit. Finally, after pre-training, we find a small set of rewrite\npairs is enough to fine-tune the QR model to outperform a strong baseline by\nfull training on all QR training data.", "published": "2020-02-13 16:31:50", "link": "http://arxiv.org/abs/2002.05607v1", "categories": ["cs.CL", "cs.IR", "I.2.6; I.2.7; H.3.3"], "primary_category": "cs.CL"}
{"title": "Exploiting the Matching Information in the Support Set for Few Shot\n  Event Classification", "abstract": "The existing event classification (EC) work primarily focuseson the\ntraditional supervised learning setting in which models are unableto extract\nevent mentions of new/unseen event types. Few-shot learninghas not been\ninvestigated in this area although it enables EC models toextend their\noperation to unobserved event types. To fill in this gap, inthis work, we\ninvestigate event classification under the few-shot learningsetting. We propose\na novel training method for this problem that exten-sively exploit the support\nset during the training process of a few-shotlearning model. In particular, in\naddition to matching the query exam-ple with those in the support set for\ntraining, we seek to further matchthe examples within the support set\nthemselves. This method providesmore training signals for the models and can be\napplied to every metric-learning-based few-shot learning methods. Our extensive\nexperiments ontwo benchmark EC datasets show that the proposed method can\nimprovethe best reported few-shot learning models by up to 10% on accuracyfor\nevent classification", "published": "2020-02-13 00:40:36", "link": "http://arxiv.org/abs/2002.05295v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Looking Enhances Listening: Recovering Missing Speech Using Images", "abstract": "Speech is understood better by using visual context; for this reason, there\nhave been many attempts to use images to adapt automatic speech recognition\n(ASR) systems. Current work, however, has shown that visually adapted ASR\nmodels only use images as a regularization signal, while completely ignoring\ntheir semantic content. In this paper, we present a set of experiments where we\nshow the utility of the visual modality under noisy conditions. Our results\nshow that multimodal ASR models can recover words which are masked in the input\nacoustic signal, by grounding its transcriptions using the visual\nrepresentations. We observe that integrating visual context can result in up to\n35% relative improvement in masked word recovery. These results demonstrate\nthat end-to-end multimodal ASR systems can become more robust to noise by\nleveraging the visual context.", "published": "2020-02-13 17:12:51", "link": "http://arxiv.org/abs/2002.05639v1", "categories": ["cs.CL", "cs.MM", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Computational Investigation on Denominalization", "abstract": "Language has been a dynamic system and word meanings always have been changed\nover times. Every time a novel concept or sense is introduced, we need to\nassign it a word to express it. Also, some changes have happened because the\nresult of a change can be more desirable for humans, or cognitively easier to\nbe used by humans. Finding the patterns of these changes is interesting and can\nreveal some facts about human cognitive evolution. As we have enough resources\nfor studying this problem, it is a good idea to work on the problem through\ncomputational modeling, and that can make the work easier and possible to be\nstudied on large scale. In this work, we want to study the nouns which have\nbeen used as verbs after some years of their emergence as nouns and find some\ncommonalities among these nouns. In other words, we are interested in finding\nwhat potential requirements are essential for this change.", "published": "2020-02-13 22:28:00", "link": "http://arxiv.org/abs/2003.04975v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hodge and Podge: Hybrid Supervised Sound Event Detection with Multi-Hot\n  MixMatch and Composition Consistence Training", "abstract": "In this paper, we propose a method called Hodge and Podge for sound event\ndetection. We demonstrate Hodge and Podge on the dataset of Detection and\nClassification of Acoustic Scenes and Events (DCASE) 2019 Challenge Task 4.\nThis task aims to predict the presence or absence and the onset and offset\ntimes of sound events in home environments. Sound event detection is\nchallenging due to the lack of large scale real strongly labeled data. Recently\ndeep semi-supervised learning (SSL) has proven to be effective in modeling with\nweakly labeled and unlabeled data. This work explores how to extend deep SSL to\nresult in a new, state-of-the-art sound event detection method called Hodge and\nPodge. With convolutional recurrent neural networks (CRNN) as the backbone\nnetwork, first, a multi-scale squeeze-excitation mechanism is introduced and\nadded to generate a pyramid squeeze-excitation CRNN. The pyramid\nsqueeze-excitation layer can pay attention to the issue that different sound\nevents have different durations, and to adaptively recalibrate channel-wise\nspectrogram responses. Further, in order to remedy the lack of real strongly\nlabeled data problem, we propose multi-hot MixMatch and composition consistency\ntraining with temporal-frequency augmentation. Our experiments with the public\nDCASE2019 challenge task 4 validation data resulted in an event-based F-score\nof 43.4\\%, and is about absolutely 1.6\\% better than state-of-the-art methods\nin the challenge. While the F-score of the official baseline is 25.8\\%.", "published": "2020-02-13 06:44:03", "link": "http://arxiv.org/abs/2002.06021v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Comparison of user models based on GMM-UBM and i-vectors for speech,\n  handwriting, and gait assessment of Parkinson's disease patients", "abstract": "Parkinson's disease is a neurodegenerative disorder characterized by the\npresence of different motor impairments. Information from speech, handwriting,\nand gait signals have been considered to evaluate the neurological state of the\npatients. On the other hand, user models based on Gaussian mixture models -\nuniversal background models (GMM-UBM) and i-vectors are considered the\nstate-of-the-art in biometric applications like speaker verification because\nthey are able to model specific speaker traits. This study introduces the use\nof GMM-UBM and i-vectors to evaluate the neurological state of Parkinson's\npatients using information from speech, handwriting, and gait. The results show\nthe importance of different feature sets from each type of signal in the\nassessment of the neurological state of the patients.", "published": "2020-02-13 10:01:08", "link": "http://arxiv.org/abs/2002.05412v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Efficient And Scalable Neural Residual Waveform Coding With\n  Collaborative Quantization", "abstract": "Scalability and efficiency are desired in neural speech codecs, which\nsupports a wide range of bitrates for applications on various devices. We\npropose a collaborative quantization (CQ) scheme to jointly learn the codebook\nof LPC coefficients and the corresponding residuals. CQ does not simply\nshoehorn LPC to a neural network, but bridges the computational capacity of\nadvanced neural network models and traditional, yet efficient and\ndomain-specific digital signal processing methods in an integrated manner. We\ndemonstrate that CQ achieves much higher quality than its predecessor at 9 kbps\nwith even lower model complexity. We also show that CQ can scale up to 24 kbps\nwhere it outperforms AMR-WB and Opus. As a neural waveform codec, CQ models are\nwith less than 1 million parameters, significantly less than many other\ngenerative models.", "published": "2020-02-13 16:28:09", "link": "http://arxiv.org/abs/2002.05604v1", "categories": ["eess.AS", "cs.MM", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "DNN-Based Distributed Multichannel Mask Estimation for Speech\n  Enhancement in Microphone Arrays", "abstract": "Multichannel processing is widely used for speech enhancement but several\nlimitations appear when trying to deploy these solutions to the real-world.\nDistributed sensor arrays that consider several devices with a few microphones\nis a viable alternative that allows for exploiting the multiple devices\nequipped with microphones that we are using in our everyday life. In this\ncontext, we propose to extend the distributed adaptive node-specific signal\nestimation approach to a neural networks framework. At each node, a local\nfiltering is performed to send one signal to the other nodes where a mask is\nestimated by a neural network in order to compute a global multi-channel Wiener\nfilter. In an array of two nodes, we show that this additional signal can be\nefficiently taken into account to predict the masks and leads to better speech\nenhancement performances than when the mask estimation relies only on the local\nsignals.", "published": "2020-02-13 11:08:00", "link": "http://arxiv.org/abs/2002.06016v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-supervised learning for audio-visual speaker diarization", "abstract": "Speaker diarization, which is to find the speech segments of specific\nspeakers, has been widely used in human-centered applications such as video\nconferences or human-computer interaction systems. In this paper, we propose a\nself-supervised audio-video synchronization learning method to address the\nproblem of speaker diarization without massive labeling effort. We improve the\nprevious approaches by introducing two new loss functions: the dynamic triplet\nloss and the multinomial loss. We test them on a real-world human-computer\ninteraction system and the results show our best model yields a remarkable gain\nof +8%F1-scoresas well as diarization error rate reduction. Finally, we\nintroduce a new large scale audio-video corpus designed to fill the vacancy of\naudio-video datasets in Chinese.", "published": "2020-02-13 02:36:32", "link": "http://arxiv.org/abs/2002.05314v1", "categories": ["eess.AS", "cs.LG", "cs.MM", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Identifying Audio Adversarial Examples via Anomalous Pattern Detection", "abstract": "Audio processing models based on deep neural networks are susceptible to\nadversarial attacks even when the adversarial audio waveform is 99.9% similar\nto a benign sample. Given the wide application of DNN-based audio recognition\nsystems, detecting the presence of adversarial examples is of high practical\nrelevance. By applying anomalous pattern detection techniques in the activation\nspace of these models, we show that 2 of the recent and current\nstate-of-the-art adversarial attacks on audio processing systems systematically\nlead to higher-than-expected activation at some subset of nodes and we can\ndetect these with up to an AUC of 0.98 with no degradation in performance on\nbenign samples.", "published": "2020-02-13 12:08:34", "link": "http://arxiv.org/abs/2002.05463v2", "categories": ["cs.LG", "cs.CR", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
