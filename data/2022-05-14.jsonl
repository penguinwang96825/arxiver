{"title": "Generating Literal and Implied Subquestions to Fact-check Complex Claims", "abstract": "Verifying complex political claims is a challenging task, especially when\npoliticians use various tactics to subtly misrepresent the facts. Automatic\nfact-checking systems fall short here, and their predictions like \"half-true\"\nare not very useful in isolation, since we have no idea which parts of the\nclaim are true and which are not. In this work, we focus on decomposing a\ncomplex claim into a comprehensive set of yes-no subquestions whose answers\ninfluence the veracity of the claim. We present ClaimDecomp, a dataset of\ndecompositions for over 1000 claims. Given a claim and its verification\nparagraph written by fact-checkers, our trained annotators write subquestions\ncovering both explicit propositions of the original claim and its implicit\nfacets, such as asking about additional political context that changes our view\nof the claim's veracity. We study whether state-of-the-art models can generate\nsuch subquestions, showing that these models generate reasonable questions to\nask, but predicting the comprehensive set of subquestions from the original\nclaim without evidence remains challenging. We further show that these\nsubquestions can help identify relevant evidence to fact-check the full claim\nand derive the veracity through their answers, suggesting that they can be\nuseful pieces of a fact-checking pipeline.", "published": "2022-05-14 00:40:57", "link": "http://arxiv.org/abs/2205.06938v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Neural Machine Translation of Indigenous Languages with\n  Multilingual Transfer Learning", "abstract": "Machine translation (MT) involving Indigenous languages, including those\npossibly endangered, is challenging due to lack of sufficient parallel data. We\ndescribe an approach exploiting bilingual and multilingual pretrained MT models\nin a transfer learning setting to translate from Spanish to ten South American\nIndigenous languages. Our models set new SOTA on five out of the ten language\npairs we consider, even doubling performance on one of these five pairs. Unlike\nprevious SOTA that perform data augmentation to enlarge the train sets, we\nretain the low-resource setting to test the effectiveness of our models under\nsuch a constraint. In spite of the rarity of linguistic information available\nabout the Indigenous languages, we offer a number of quantitative and\nqualitative analyses (e.g., as to morphology, tokenization, and orthography) to\ncontextualize our results.", "published": "2022-05-14 07:30:03", "link": "http://arxiv.org/abs/2205.06993v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Naturalistic Causal Probing for Morpho-Syntax", "abstract": "Probing has become a go-to methodology for interpreting and analyzing deep\nneural models in natural language processing. However, there is still a lack of\nunderstanding of the limitations and weaknesses of various types of probes. In\nthis work, we suggest a strategy for input-level intervention on naturalistic\nsentences. Using our approach, we intervene on the morpho-syntactic features of\na sentence, while keeping the rest of the sentence unchanged. Such an\nintervention allows us to causally probe pre-trained models. We apply our\nnaturalistic causal probing framework to analyze the effects of grammatical\ngender and number on contextualized representations extracted from three\npre-trained models in Spanish: the multilingual versions of BERT, RoBERTa, and\nGPT-2. Our experiments suggest that naturalistic interventions lead to stable\nestimates of the causal effects of various linguistic properties. Moreover, our\nexperiments demonstrate the importance of naturalistic causal probing when\nanalyzing pre-trained models.", "published": "2022-05-14 11:47:58", "link": "http://arxiv.org/abs/2205.07043v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What do Models Learn From Training on More Than Text? Measuring Visual\n  Commonsense Knowledge", "abstract": "There are limitations in learning language from text alone. Therefore, recent\nfocus has been on developing multimodal models. However, few benchmarks exist\nthat can measure what language models learn about language from multimodal\ntraining. We hypothesize that training on a visual modality should improve on\nthe visual commonsense knowledge in language models. Therefore, we introduce\ntwo evaluation tasks for measuring visual commonsense knowledge in language\nmodels and use them to evaluate different multimodal models and unimodal\nbaselines. Primarily, we find that the visual commonsense knowledge is not\nsignificantly different between the multimodal models and unimodal baseline\nmodels trained on visual text data.", "published": "2022-05-14 13:37:50", "link": "http://arxiv.org/abs/2205.07065v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Auto-Select Reading Passages in English Assessment Tests?", "abstract": "We show a method to auto-select reading passages in English assessment tests\nand share some key insights that can be helpful in related fields. In\nspecifics, we prove that finding a similar passage (to a passage that already\nappeared in the test) can give a suitable passage for test development. In the\nprocess, we create a simple database-tagger-filter algorithm and perform a\nhuman evaluation. However, 1. the textual features, that we analyzed, lack\ncoverage, and 2. we fail to find meaningful correlations between each feature\nand suitability score. Lastly, we describe the future developments to improve\nautomated reading passage selection.", "published": "2022-05-14 04:06:32", "link": "http://arxiv.org/abs/2205.06961v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Generating Tips from Song Reviews: A New Dataset and Framework", "abstract": "Reviews of songs play an important role in online music service platforms.\nPrior research shows that users can make quicker and more informed decisions\nwhen presented with meaningful song reviews. However, reviews of music songs\nare generally long in length and most of them are non-informative for users. It\nis difficult for users to efficiently grasp meaningful messages for making\ndecisions. To solve this problem, one practical strategy is to provide tips,\ni.e., short, concise, empathetic, and self-contained descriptions about songs.\nTips are produced from song reviews and should express non-trivial insights\nabout the songs. To the best of our knowledge, no prior studies have explored\nthe tip generation task in music domain. In this paper, we create a dataset\nnamed MTips for the task and propose a framework named GENTMS for automatically\ngenerating tips from song reviews. The dataset involves 8,003 Chinese\ntips/non-tips from 128 songs which are distributed in five different song\ngenres. Experimental results show that GENTMS achieves top-10 precision at\n85.56%, outperforming the baseline models by at least 3.34%. Besides, to\nsimulate the practical usage of our proposed framework, we also experiment with\npreviously-unseen songs, during which GENTMS also achieves the best performance\nwith top-10 precision at 78.89% on average. The results demonstrate the\neffectiveness of the proposed framework in tip generation of the music domain.", "published": "2022-05-14 06:40:49", "link": "http://arxiv.org/abs/2205.06985v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Pretraining Approaches for Spoken Language Recognition: TalTech\n  Submission to the OLR 2021 Challenge", "abstract": "This paper investigates different pretraining approaches to spoken language\nidentification. The paper is based on our submission to the Oriental Language\nRecognition 2021 Challenge. We participated in two tracks of the challenge:\nconstrained and unconstrained language recognition. For the constrained track,\nwe first trained a Conformer-based encoder-decoder model for multilingual\nautomatic speech recognition (ASR), using the provided training data that had\ntranscripts available. The shared encoder of the multilingual ASR model was\nthen finetuned for the language identification task. For the unconstrained\ntask, we relied on both externally available pretrained models as well as\nexternal data: the multilingual XLSR-53 wav2vec2.0 model was finetuned on the\nVoxLingua107 corpus for the language recognition task, and finally finetuned on\nthe provided target language training data, augmented with CommonVoice data.\nOur primary metric $C_{\\rm avg}$ values on the Test set are 0.0079 for the\nconstrained task and 0.0119 for the unconstrained task which resulted in the\nsecond place in both rankings. In post-evaluation experiments, we study the\namount of target language data needed for training an accurate backend model,\nthe importance of multilingual pretraining data, and compare different models\nas finetuning starting points.", "published": "2022-05-14 15:17:08", "link": "http://arxiv.org/abs/2205.07083v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Improved Consistency Training for Semi-Supervised Sequence-to-Sequence\n  ASR via Speech Chain Reconstruction and Self-Transcribing", "abstract": "Consistency regularization has recently been applied to semi-supervised\nsequence-to-sequence (S2S) automatic speech recognition (ASR). This principle\nencourages an ASR model to output similar predictions for the same input speech\nwith different perturbations. The existing paradigm of semi-supervised S2S ASR\nutilizes SpecAugment as data augmentation and requires a static teacher model\nto produce pseudo transcripts for untranscribed speech. However, this paradigm\nfails to take full advantage of consistency regularization. First, the masking\noperations of SpecAugment may damage the linguistic contents of the speech,\nthus influencing the quality of pseudo labels. Second, S2S ASR requires both\ninput speech and prefix tokens to make the next prediction. The static prefix\ntokens made by the offline teacher model cannot match dynamic pseudo labels\nduring consistency training. In this work, we propose an improved consistency\ntraining paradigm of semi-supervised S2S ASR. We utilize speech chain\nreconstruction as the weak augmentation to generate high-quality pseudo labels.\nMoreover, we demonstrate that dynamic pseudo transcripts produced by the\nstudent ASR model benefit the consistency training. Experiments on LJSpeech and\nLibriSpeech corpora show that compared to supervised baselines, our improved\nparadigm achieves a 12.2% CER improvement in the single-speaker setting and\n38.6% in the multi-speaker setting.", "published": "2022-05-14 04:26:13", "link": "http://arxiv.org/abs/2205.06963v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS", "68T10", "I.2.7"], "primary_category": "cs.CL"}
{"title": "ACCoRD: A Multi-Document Approach to Generating Diverse Descriptions of\n  Scientific Concepts", "abstract": "Systems that can automatically define unfamiliar terms hold the promise of\nimproving the accessibility of scientific texts, especially for readers who may\nlack prerequisite background knowledge. However, current systems assume a\nsingle \"best\" description per concept, which fails to account for the many\npotentially useful ways a concept can be described. We present ACCoRD, an\nend-to-end system tackling the novel task of generating sets of descriptions of\nscientific concepts. Our system takes advantage of the myriad ways a concept is\nmentioned across the scientific literature to produce distinct, diverse\ndescriptions of target scientific concepts in terms of different reference\nconcepts. To support research on the task, we release an expert-annotated\nresource, the ACCoRD corpus, which includes 1,275 labeled contexts and 1,787\nhand-authored concept descriptions. We conduct a user study demonstrating that\n(1) users prefer descriptions produced by our end-to-end system, and (2) users\nprefer multiple descriptions to a single \"best\" description.", "published": "2022-05-14 06:18:24", "link": "http://arxiv.org/abs/2205.06982v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model\n  for Text-to-SQL", "abstract": "Relational structures such as schema linking and schema encoding have been\nvalidated as a key component to qualitatively translating natural language into\nSQL queries. However, introducing these structural relations comes with prices:\nthey often result in a specialized model structure, which largely prohibits\nusing large pretrained models in text-to-SQL. To address this problem, we\npropose RASAT: a Transformer seq2seq architecture augmented with relation-aware\nself-attention that could leverage a variety of relational structures while\ninheriting the pretrained parameters from the T5 model effectively. Our model\ncan incorporate almost all types of existing relations in the literature, and\nin addition, we propose introducing co-reference relations for the multi-turn\nscenario. Experimental results on three widely used text-to-SQL datasets,\ncovering both single-turn and multi-turn scenarios, have shown that RASAT could\nachieve state-of-the-art results across all three benchmarks (75.5% EX on\nSpider, 52.6% IEX on SParC, and 37.4% IEX on CoSQL).", "published": "2022-05-14 06:27:40", "link": "http://arxiv.org/abs/2205.06983v2", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Integration of Text and Graph-based Features for Detecting Mental Health\n  Disorders from Voice", "abstract": "With the availability of voice-enabled devices such as smart phones, mental\nhealth disorders could be detected and treated earlier, particularly\npost-pandemic. The current methods involve extracting features directly from\naudio signals. In this paper, two methods are used to enrich voice analysis for\ndepression detection: graph transformation of voice signals, and natural\nlanguage processing of the transcript based on representational learning, fused\ntogether to produce final class labels. The results of experiments with the\nDAIC-WOZ dataset suggest that integration of text-based voice classification\nand learning from low level and graph-based voice signal features can improve\nthe detection of mental disorders like depression.", "published": "2022-05-14 08:37:19", "link": "http://arxiv.org/abs/2205.07006v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE", "I.2; J.3"], "primary_category": "cs.LG"}
{"title": "Collar-aware Training for Streaming Speaker Change Detection in\n  Broadcast Speech", "abstract": "In this paper, we present a novel training method for speaker change\ndetection models. Speaker change detection is often viewed as a binary sequence\nlabelling problem. The main challenges with this approach are the vagueness of\nannotated change points caused by the silences between speaker turns and\nimbalanced data due to the majority of frames not including a speaker change.\nConventional training methods tackle these by artificially increasing the\nproportion of positive labels in the training data. Instead, the proposed\nmethod uses an objective function which encourages the model to predict a\nsingle positive label within a specified collar. This is done by marginalizing\nover all possible subsequences that have exactly one positive label within the\ncollar. Experiments on English and Estonian datasets show large improvements\nover the conventional training method. Additionally, the model outputs have\npeaks concentrated to a single frame, removing the need for post-processing to\nfind the exact predicted change point which is particularly useful for\nstreaming applications.", "published": "2022-05-14 15:35:43", "link": "http://arxiv.org/abs/2205.07086v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The VoicePrivacy 2020 Challenge Evaluation Plan", "abstract": "The VoicePrivacy Challenge aims to promote the development of privacy\npreservation tools for speech technology by gathering a new community to define\nthe tasks of interest and the evaluation methodology, and benchmarking\nsolutions through a series of challenges. In this document, we formulate the\nvoice anonymization task selected for the VoicePrivacy 2020 Challenge and\ndescribe the datasets used for system development and evaluation. We also\npresent the attack models and the associated objective and subjective\nevaluation metrics. We introduce two anonymization baselines and report\nobjective evaluation results.", "published": "2022-05-14 20:05:51", "link": "http://arxiv.org/abs/2205.07123v1", "categories": ["cs.CL", "cs.CR", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multiformer: A Head-Configurable Transformer-Based Model for Direct\n  Speech Translation", "abstract": "Transformer-based models have been achieving state-of-the-art results in\nseveral fields of Natural Language Processing. However, its direct application\nto speech tasks is not trivial. The nature of this sequences carries problems\nsuch as long sequence lengths and redundancy between adjacent tokens.\nTherefore, we believe that regular self-attention mechanism might not be well\nsuited for it.\n  Different approaches have been proposed to overcome these problems, such as\nthe use of efficient attention mechanisms. However, the use of these methods\nusually comes with a cost, which is a performance reduction caused by\ninformation loss. In this study, we present the Multiformer, a\nTransformer-based model which allows the use of different attention mechanisms\non each head. By doing this, the model is able to bias the self-attention\ntowards the extraction of more diverse token interactions, and the information\nloss is reduced. Finally, we perform an analysis of the head contributions, and\nwe observe that those architectures where all heads relevance is uniformly\ndistributed obtain better results. Our results show that mixing attention\npatterns along the different heads and layers outperforms our baseline by up to\n0.7 BLEU.", "published": "2022-05-14 17:37:47", "link": "http://arxiv.org/abs/2205.07100v1", "categories": ["cs.CL", "cs.AI", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
