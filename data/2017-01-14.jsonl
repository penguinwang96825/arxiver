{"title": "QCRI Machine Translation Systems for IWSLT 16", "abstract": "This paper describes QCRI's machine translation systems for the IWSLT 2016\nevaluation campaign. We participated in the Arabic->English and English->Arabic\ntracks. We built both Phrase-based and Neural machine translation models, in an\neffort to probe whether the newly emerged NMT framework surpasses the\ntraditional phrase-based systems in Arabic-English language pairs. We trained a\nvery strong phrase-based system including, a big language model, the Operation\nSequence Model, Neural Network Joint Model and Class-based models along with\ndifferent domain adaptation techniques such as MML filtering, mixture modeling\nand using fine tuning over NNJM model. However, a Neural MT system, trained by\nstacking data from different genres through fine-tuning, and applying ensemble\nover 8 models, beat our very strong phrase-based system by a significant 2 BLEU\npoints margin in Arabic->English direction. We did not obtain similar gains in\nthe other direction but were still able to outperform the phrase-based system.\nWe also applied system combination on phrase-based and NMT outputs.", "published": "2017-01-14 14:18:54", "link": "http://arxiv.org/abs/1701.03924v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Balancing Novelty and Salience: Adaptive Learning to Rank Entities for\n  Timeline Summarization of High-impact Events", "abstract": "Long-running, high-impact events such as the Boston Marathon bombing often\ndevelop through many stages and involve a large number of entities in their\nunfolding. Timeline summarization of an event by key sentences eases story\ndigestion, but does not distinguish between what a user remembers and what she\nmight want to re-check. In this work, we present a novel approach for timeline\nsummarization of high-impact events, which uses entities instead of sentences\nfor summarizing the event at each individual point in time. Such entity\nsummaries can serve as both (1) important memory cues in a retrospective event\nconsideration and (2) pointers for personalized event exploration. In order to\nautomatically create such summaries, it is crucial to identify the \"right\"\nentities for inclusion. We propose to learn a ranking function for entities,\nwith a dynamically adapted trade-off between the in-document salience of\nentities and the informativeness of entities across documents, i.e., the level\nof new information associated with an entity for a time point under\nconsideration. Furthermore, for capturing collective attention for an entity we\nuse an innovative soft labeling approach based on Wikipedia. Our experiments on\na real large news datasets confirm the effectiveness of the proposed methods.", "published": "2017-01-14 16:47:51", "link": "http://arxiv.org/abs/1701.03947v1", "categories": ["cs.IR", "cs.CL", "H.3.3"], "primary_category": "cs.IR"}
