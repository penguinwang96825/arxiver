{"title": "ORB: An Open Reading Benchmark for Comprehensive Evaluation of Machine\n  Reading Comprehension", "abstract": "Reading comprehension is one of the crucial tasks for furthering research in\nnatural language understanding. A lot of diverse reading comprehension datasets\nhave recently been introduced to study various phenomena in natural language,\nranging from simple paraphrase matching and entity typing to entity tracking\nand understanding the implications of the context. Given the availability of\nmany such datasets, comprehensive and reliable evaluation is tedious and\ntime-consuming for researchers working on this problem. We present an\nevaluation server, ORB, that reports performance on seven diverse reading\ncomprehension datasets, encouraging and facilitating testing a single model's\ncapability in understanding a wide variety of reading phenomena. The evaluation\nserver places no restrictions on how models are trained, so it is a suitable\ntest bed for exploring training paradigms and representation learning for\ngeneral reading facility. As more suitable datasets are released, they will be\nadded to the evaluation server. We also collect and include synthetic\naugmentations for these datasets, testing how well models can handle\nout-of-domain questions.", "published": "2019-12-29 07:27:23", "link": "http://arxiv.org/abs/1912.12598v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\u00c6THEL: Automatically Extracted Typelogical Derivations for Dutch", "abstract": "We present {\\AE}THEL, a semantic compositionality dataset for written Dutch.\n{\\AE}THEL consists of two parts. First, it contains a lexicon of supertags for\nabout 900 000 words in context. The supertags correspond to types of the simply\ntyped linear lambda-calculus, enhanced with dependency decorations that capture\ngrammatical roles supplementary to function-argument structures. On the basis\nof these types, {\\AE}THEL further provides 72 192 validated derivations,\npresented in four formats: natural-deduction and sequent-style proofs, linear\nlogic proofnets and the associated programs (lambda terms) for meaning\ncomposition. {\\AE}THEL's types and derivations are obtained by means of an\nextraction algorithm applied to the syntactic analyses of LASSY Small, the gold\nstandard corpus of written Dutch. We discuss the extraction algorithm and show\nhow `virtual elements' in the original LASSY annotation of unbounded\ndependencies and coordination phenomena give rise to higher-order types. We\nsuggest some example usecases highlighting the benefits of a type-driven\napproach at the syntax semantics interface. The following resources are\nopen-sourced with {\\AE}THEL: the lexical mappings between words and types, a\nsubset of the dataset consisting of 7 924 semantic parses, and the Python code\nthat implements the extraction algorithm.", "published": "2019-12-29 11:31:11", "link": "http://arxiv.org/abs/1912.12635v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Deterministic plus Stochastic Model of the Residual Signal for\n  Improved Parametric Speech Synthesis", "abstract": "Speech generated by parametric synthesizers generally suffers from a typical\nbuzziness, similar to what was encountered in old LPC-like vocoders. In order\nto alleviate this problem, a more suited modeling of the excitation should be\nadopted. For this, we hereby propose an adaptation of the Deterministic plus\nStochastic Model (DSM) for the residual. In this model, the excitation is\ndivided into two distinct spectral bands delimited by the maximum voiced\nfrequency. The deterministic part concerns the low-frequency contents and\nconsists of a decomposition of pitch-synchronous residual frames on an\northonormal basis obtained by Principal Component Analysis. The stochastic\ncomponent is a high-pass filtered noise whose time structure is modulated by an\nenergy-envelope, similarly to what is done in the Harmonic plus Noise Model\n(HNM). The proposed residual model is integrated within a HMM-based speech\nsynthesizer and is compared to the traditional excitation through a subjective\ntest. Results show a significative improvement for both male and female voices.\nIn addition the proposed model requires few computational load and memory,\nwhich is essential for its integration in commercial applications.", "published": "2019-12-29 07:26:47", "link": "http://arxiv.org/abs/2001.00842v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Deterministic plus Stochastic Model of the Residual Signal and its\n  Applications", "abstract": "The modeling of speech production often relies on a source-filter approach.\nAlthough methods parameterizing the filter have nowadays reached a certain\nmaturity, there is still a lot to be gained for several speech processing\napplications in finding an appropriate excitation model. This manuscript\npresents a Deterministic plus Stochastic Model (DSM) of the residual signal.\nThe DSM consists of two contributions acting in two distinct spectral bands\ndelimited by a maximum voiced frequency. Both components are extracted from an\nanalysis performed on a speaker-dependent dataset of pitch-synchronous residual\nframes. The deterministic part models the low-frequency contents and arises\nfrom an orthonormal decomposition of these frames. As for the stochastic\ncomponent, it is a high-frequency noise modulated both in time and frequency.\nSome interesting phonetic and computational properties of the DSM are also\nhighlighted. The applicability of the DSM in two fields of speech processing is\nthen studied. First, it is shown that incorporating the DSM vocoder in\nHMM-based speech synthesis enhances the delivered quality. The proposed\napproach turns out to significantly outperform the traditional pulse excitation\nand provides a quality equivalent to STRAIGHT. In a second application, the\npotential of glottal signatures derived from the proposed DSM is investigated\nfor speaker identification purpose. Interestingly, these signatures are shown\nto lead to better recognition rates than other glottal-based methods.", "published": "2019-12-29 07:52:37", "link": "http://arxiv.org/abs/2001.01000v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Complex Cepstrum-based Decomposition of Speech for Glottal Source\n  Estimation", "abstract": "Homomorphic analysis is a well-known method for the separation of\nnon-linearly combined signals. More particularly, the use of complex cepstrum\nfor source-tract deconvolution has been discussed in various articles. However\nthere exists no study which proposes a glottal flow estimation methodology\nbased on cepstrum and reports effective results. In this paper, we show that\ncomplex cepstrum can be effectively used for glottal flow estimation by\nseparating the causal and anticausal components of a windowed speech signal as\ndone by the Zeros of the Z-Transform (ZZT) decomposition. Based on exactly the\nsame principles presented for ZZT decomposition, windowing should be applied\nsuch that the windowed speech signals exhibit mixed-phase characteristics which\nconform the speech production model that the anticausal component is mainly due\nto the glottal flow open phase. The advantage of the complex cepstrum-based\napproach compared to the ZZT decomposition is its much higher speed.", "published": "2019-12-29 07:58:18", "link": "http://arxiv.org/abs/1912.12602v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Glottal Source Processing: from Analysis to Applications", "abstract": "The great majority of current voice technology applications relies on\nacoustic features characterizing the vocal tract response, such as the widely\nused MFCC of LPC parameters. Nonetheless, the airflow passing through the vocal\nfolds, and called glottal flow, is expected to exhibit a relevant\ncomplementarity. Unfortunately, glottal analysis from speech recordings\nrequires specific and more complex processing operations, which explains why it\nhas been generally avoided. This review gives a general overview of techniques\nwhich have been designed for glottal source processing. Starting from\nfundamental analysis tools of pitch tracking, glottal closure instant\ndetection, glottal flow estimation and modelling, this paper then highlights\nhow these solutions can be properly integrated within various voice technology\napplications.", "published": "2019-12-29 08:13:58", "link": "http://arxiv.org/abs/1912.12604v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Dirichlet uncertainty wrappers for actionable algorithm accuracy\n  accountability and auditability", "abstract": "Nowadays, the use of machine learning models is becoming a utility in many\napplications. Companies deliver pre-trained models encapsulated as application\nprogramming interfaces (APIs) that developers combine with third party\ncomponents and their own models and data to create complex data products to\nsolve specific problems. The complexity of such products and the lack of\ncontrol and knowledge of the internals of each component used cause unavoidable\neffects, such as lack of transparency, difficulty in auditability, and\nemergence of potential uncontrolled risks. They are effectively black-boxes.\nAccountability of such solutions is a challenge for the auditors and the\nmachine learning community. In this work, we propose a wrapper that given a\nblack-box model enriches its output prediction with a measure of uncertainty.\nBy using this wrapper, we make the black-box auditable for the accuracy risk\n(risk derived from low quality or uncertain decisions) and at the same time we\nprovide an actionable mechanism to mitigate that risk in the form of decision\nrejection; we can choose not to issue a prediction when the risk or uncertainty\nin that decision is significant. Based on the resulting uncertainty measure, we\nadvocate for a rejection system that selects the more confident predictions,\ndiscarding those more uncertain, leading to an improvement in the trustability\nof the resulting system. We showcase the proposed technique and methodology in\na practical scenario where a simulated sentiment analysis API based on natural\nlanguage processing is applied to different domains. Results demonstrate the\neffectiveness of the uncertainty computed by the wrapper and its high\ncorrelation to bad quality predictions and misclassifications.", "published": "2019-12-29 11:05:47", "link": "http://arxiv.org/abs/1912.12628v1", "categories": ["cs.LG", "cs.CL", "stat.ML", "68T37", "I.2.7; I.2.3"], "primary_category": "cs.LG"}
{"title": "A Comparative Study of Pitch Extraction Algorithms on a Large Variety of\n  Singing Sounds", "abstract": "The problem of pitch tracking has been extensively studied in the speech\nresearch community. The goal of this paper is to investigate how these\ntechniques should be adapted to singing voice analysis, and to provide a\ncomparative evaluation of the most representative state-of-the-art approaches.\nThis study is carried out on a large database of annotated singing sounds with\naligned EGG recordings, comprising a variety of singer categories and singing\nexercises. The algorithmic performance is assessed according to the ability to\ndetect voicing boundaries and to accurately estimate pitch contour. First, we\nevaluate the usefulness of adapting existing methods to singing voice analysis.\nThen we compare the accuracy of several pitch-extraction algorithms, depending\non singer category and laryngeal mechanism. Finally, we analyze their\nrobustness to reverberation.", "published": "2019-12-29 08:45:08", "link": "http://arxiv.org/abs/1912.12609v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
