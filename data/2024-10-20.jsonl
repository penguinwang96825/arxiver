{"title": "Comparative Analysis of LSTM, GRU, and Transformer Models for Stock Price Prediction", "abstract": "In recent fast-paced financial markets, investors constantly seek ways to\ngain an edge and make informed decisions. Although achieving perfect accuracy\nin stock price predictions remains elusive, artificial intelligence (AI)\nadvancements have significantly enhanced our ability to analyze historical data\nand identify potential trends. This paper takes AI driven stock price trend\nprediction as the core research, makes a model training data set of famous\nTesla cars from 2015 to 2024, and compares LSTM, GRU, and Transformer Models.\nThe analysis is more consistent with the model of stock trend prediction, and\nthe experimental results show that the accuracy of the LSTM model is 94%. These\nmethods ultimately allow investors to make more informed decisions and gain a\nclearer insight into market behaviors.", "published": "2024-10-20 14:00:58", "link": "http://arxiv.org/abs/2411.05790v1", "categories": ["q-fin.ST", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "Back to School: Translation Using Grammar Books", "abstract": "Machine translation systems for high resource languages perform exceptionally\nwell and produce high quality translations. Unfortunately, the vast majority of\nlanguages are not considered high resource and lack the quantity of parallel\nsentences needed to train such systems. These under-represented languages are\nnot without resources, however, and bilingual dictionaries and grammar books\nare available as linguistic reference material. With current large language\nmodels (LLMs) supporting near book-length contexts, we can begin to use the\navailable material to ensure advancements are shared among all of the world's\nlanguages. In this paper, we demonstrate incorporating grammar books in the\nprompt of GPT-4 to improve machine translation and evaluate the performance on\n16 topologically diverse low-resource languages, using a combination of\nreference material to show that the machine translation performance of LLMs can\nbe improved using this method.", "published": "2024-10-20 03:28:51", "link": "http://arxiv.org/abs/2410.15263v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BRIEF: Bridging Retrieval and Inference for Multi-hop Reasoning via\n  Compression", "abstract": "Retrieval-augmented generation (RAG) can supplement large language models\n(LLMs) by integrating external knowledge. However, as the number of retrieved\ndocuments increases, the input length to LLMs grows linearly, causing a\ndramatic increase in latency and a degradation in long-context understanding.\nThis is particularly serious for multi-hop questions that require a chain of\nreasoning across documents. To accelerate inference, reduce costs, and minimize\ndistractions, this paper presents BRIEF (Bridging Retrieval and Inference\nthrough Evidence Fusion), a lightweight approach that performs query-aware\nmulti-hop reasoning by compressing retrieved documents into highly dense\ntextual summaries to integrate into in-context RAG. To enable learning\ncompression for multi-hop reasoning, we curate synthetic data by extracting\natomic propositions that encapsulate distinct factoids from the source\ndocuments to compose synthetic summaries. Based on our synthetic data built\nentirely by open-source models, BRIEF generates more concise summaries and\nenables a range of LLMs to achieve exceptional open-domain question answering\n(QA) performance. For example, on HotpotQA, BRIEF improves the compression rate\nby 2 times compared to the state-of-the-art baseline, while outperforming it by\n3.00% EM and 4.16% F1 with Flan-UL2 as the reader model. It also generates more\nconcise summaries than proprietary GPT-3.5, while demonstrating nearly\nidentical QA performance.", "published": "2024-10-20 04:24:16", "link": "http://arxiv.org/abs/2410.15277v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Language Models to Critique With Multi-agent Feedback", "abstract": "Critique ability, a meta-cognitive capability of humans, presents significant\nchallenges for LLMs to improve. Recent works primarily rely on supervised\nfine-tuning (SFT) using critiques generated by a single LLM like GPT-4.\nHowever, these model-generated critiques often exhibit flaws due to the\ninherent complexity of the critique. Consequently, fine-tuning LLMs on such\nflawed critiques typically limits the model's performance and propagates these\nflaws into the learned model. To overcome these challenges, this paper proposes\na novel data generation pipeline, named MultiCritique, that improves the\ncritique ability of LLMs by utilizing multi-agent feedback in both the SFT and\nreinforcement learning (RL) stages. First, our data generation pipeline\naggregates high-quality critiques from multiple agents instead of a single\nmodel, with crucial information as input for simplifying the critique.\nFurthermore, our pipeline improves the preference accuracy of critique quality\nthrough multi-agent feedback, facilitating the effectiveness of RL in improving\nthe critique ability of LLMs. Based on our proposed MultiCritique data\ngeneration pipeline, we construct the MultiCritiqueDataset for the SFT and RL\nfine-tuning stages. Extensive experimental results on two benchmarks\ndemonstrate: 1) the superior quality of our constructed SFT dataset compared to\nexisting critique datasets; 2) additional improvements to the critique ability\nof LLMs brought by the RL stage. Notably, our fine-tuned 7B model significantly\nsurpasses other advanced 7B-13B open-source models, approaching the performance\nof advanced 70B LLMs and GPT-4. Codes, datasets and model weights will be\npublicly available.", "published": "2024-10-20 04:57:45", "link": "http://arxiv.org/abs/2410.15287v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does ChatGPT Have a Poetic Style?", "abstract": "Generating poetry has become a popular application of LLMs, perhaps\nespecially of OpenAI's widely-used chatbot ChatGPT. What kind of poet is\nChatGPT? Does ChatGPT have its own poetic style? Can it successfully produce\npoems in different styles? To answer these questions, we prompt the GPT-3.5 and\nGPT-4 models to generate English-language poems in 24 different poetic forms\nand styles, about 40 different subjects, and in response to 3 different writing\nprompt templates. We then analyze the resulting 5.7k poems, comparing them to a\nsample of 3.7k poems from the Poetry Foundation and the Academy of American\nPoets. We find that the GPT models, especially GPT-4, can successfully produce\npoems in a range of both common and uncommon English-language forms in\nsuperficial yet noteworthy ways, such as by producing poems of appropriate\nlengths for sonnets (14 lines), villanelles (19 lines), and sestinas (39\nlines). But the GPT models also exhibit their own distinct stylistic\ntendencies, both within and outside of these specific forms. Our results show\nthat GPT poetry is much more constrained and uniform than human poetry, showing\na strong penchant for rhyme, quatrains (4-line stanzas), iambic meter,\nfirst-person plural perspectives (we, us, our), and specific vocabulary like\n\"heart,\" \"embrace,\" \"echo,\" and \"whisper.\"", "published": "2024-10-20 06:01:34", "link": "http://arxiv.org/abs/2410.15299v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KTCR: Improving Implicit Hate Detection with Knowledge Transfer driven\n  Concept Refinement", "abstract": "The constant shifts in social and political contexts, driven by emerging\nsocial movements and political events, lead to new forms of hate content and\npreviously unrecognized hate patterns that machine learning models may not have\ncaptured. Some recent literature proposes data augmentation-based techniques to\nenrich existing hate datasets by incorporating samples that reveal new implicit\nhate patterns. This approach aims to improve the model's performance on\nout-of-domain implicit hate instances. It is observed, that further addition of\nmore samples for augmentation results in the decrease of the performance of the\nmodel. In this work, we propose a Knowledge Transfer-driven Concept Refinement\nmethod that distills and refines the concepts related to implicit hate samples\nthrough novel prototype alignment and concept losses, alongside data\naugmentation based on concept activation vectors. Experiments with several\npublicly available datasets show that incorporating additional implicit samples\nreflecting new hate patterns through concept refinement enhances the model's\nperformance, surpassing baseline results while maintaining cross-dataset\ngeneralization capabilities.", "published": "2024-10-20 06:53:04", "link": "http://arxiv.org/abs/2410.15314v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Uncertainty Estimation in LLMs: Theory Meets Practice", "abstract": "As large language models (LLMs) continue to evolve, understanding and\nquantifying the uncertainty in their predictions is critical for enhancing\napplication credibility. However, the existing literature relevant to LLM\nuncertainty estimation often relies on heuristic approaches, lacking systematic\nclassification of the methods. In this survey, we clarify the definitions of\nuncertainty and confidence, highlighting their distinctions and implications\nfor model predictions. On this basis, we integrate theoretical perspectives,\nincluding Bayesian inference, information theory, and ensemble strategies, to\ncategorize various classes of uncertainty estimation methods derived from\nheuristic approaches. Additionally, we address challenges that arise when\napplying these methods to LLMs. We also explore techniques for incorporating\nuncertainty into diverse applications, including out-of-distribution detection,\ndata annotation, and question clarification. Our review provides insights into\nuncertainty estimation from both definitional and theoretical angles,\ncontributing to a comprehensive understanding of this critical aspect in LLMs.\nWe aim to inspire the development of more reliable and effective uncertainty\nestimation approaches for LLMs in real-world scenarios.", "published": "2024-10-20 07:55:44", "link": "http://arxiv.org/abs/2410.15326v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERTtime Stories: Investigating the Role of Synthetic Story Data in\n  Language Pre-training", "abstract": "We describe our contribution to the Strict and Strict-Small tracks of the 2nd\niteration of the BabyLM Challenge. The shared task is centered around efficient\npre-training given data constraints motivated by human development. In\nresponse, we study the effect of synthetic story data in language pre-training\nusing TinyStories: a recently introduced dataset of short stories. Initially,\nwe train GPT-Neo models on subsets of TinyStories, while varying the amount of\navailable data. We find that, even with access to less than 100M words, the\nmodels are able to generate high-quality, original completions to a given\nstory, and acquire substantial linguistic knowledge. To measure the effect of\nsynthetic story data, we train LTG-BERT encoder models on a combined dataset\nof: a subset of TinyStories, story completions generated by GPT-Neo, and a\nsubset of the BabyLM dataset. Our experimentation reveals that synthetic data\ncan occasionally offer modest gains, but overall have a negative influence on\nlinguistic understanding. Our work offers an initial study on synthesizing\nstory data in low resource settings and underscores their potential for\naugmentation in data-constrained language modeling. We publicly release our\nmodels and implementation on our GitHub.", "published": "2024-10-20 11:47:17", "link": "http://arxiv.org/abs/2410.15365v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CalibraEval: Calibrating Prediction Distribution to Mitigate Selection\n  Bias in LLMs-as-Judges", "abstract": "The use of large language models (LLMs) as automated evaluation tools to\nassess the quality of generated natural language, known as LLMs-as-Judges, has\ndemonstrated promising capabilities and is rapidly gaining widespread\nattention. However, when applied to pairwise comparisons of candidate\nresponses, LLM-based evaluators often exhibit selection bias. Specifically,\ntheir judgments may become inconsistent when the option positions or ID tokens\nare swapped, compromising the effectiveness and fairness of the evaluation\nresult. To address this challenge, we introduce CalibraEval, a novel label-free\nmethod for mitigating selection bias during inference. Specifically,\nCalibraEval reformulates debiasing as an optimization task aimed at adjusting\nobserved prediction distributions to align with unbiased prediction\ndistributions. To solve this optimization problem, we propose a non-parametric\norder-preserving algorithm (NOA). This algorithm leverages the partial order\nrelationships between model prediction distributions, thereby eliminating the\nneed for explicit labels and precise mathematical function modeling.Empirical\nevaluations of LLMs in multiple representative benchmarks demonstrate that\nCalibraEval effectively mitigates selection bias and improves performance\ncompared to existing debiasing methods. This work marks a step toward building\nmore robust and unbiased automated evaluation frameworks, paving the way for\nimproved reliability in AI-driven assessments", "published": "2024-10-20 13:47:39", "link": "http://arxiv.org/abs/2410.15393v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel Interpretability Metric for Explaining Bias in Language Models:\n  Applications on Multilingual Models from Southeast Asia", "abstract": "Work on bias in pretrained language models (PLMs) focuses on bias evaluation\nand mitigation and fails to tackle the question of bias attribution and\nexplainability. We propose a novel metric, the $\\textit{bias attribution\nscore}$, which draws from information theory to measure token-level\ncontributions to biased behavior in PLMs. We then demonstrate the utility of\nthis metric by applying it on multilingual PLMs, including models from\nSoutheast Asia which have not yet been thoroughly examined in bias evaluation\nliterature. Our results confirm the presence of sexist and homophobic bias in\nSoutheast Asian PLMs. Interpretability and semantic analyses also reveal that\nPLM bias is strongly induced by words relating to crime, intimate\nrelationships, and helping among other discursive categories, suggesting that\nthese are topics where PLMs strongly reproduce bias from pretraining data and\nwhere PLMs should be used with more caution.", "published": "2024-10-20 18:31:05", "link": "http://arxiv.org/abs/2410.15464v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"What is the value of {templates}?\" Rethinking Document Information\n  Extraction Datasets for LLMs", "abstract": "The rise of large language models (LLMs) for visually rich document\nunderstanding (VRDU) has kindled a need for prompt-response, document-based\ndatasets. As annotating new datasets from scratch is labor-intensive, the\nexisting literature has generated prompt-response datasets from available\nresources using simple templates. For the case of key information extraction\n(KIE), one of the most common VRDU tasks, past work has typically employed the\ntemplate \"What is the value for the {key}?\". However, given the variety of\nquestions encountered in the wild, simple and uniform templates are\ninsufficient for creating robust models in research and industrial contexts. In\nthis work, we present K2Q, a diverse collection of five datasets converted from\nKIE to a prompt-response format using a plethora of bespoke templates. The\nquestions in K2Q can span multiple entities and be extractive or boolean. We\nempirically compare the performance of seven baseline generative models on K2Q\nwith zero-shot prompting. We further compare three of these models when\ntraining on K2Q versus training on simpler templates to motivate the need of\nour work. We find that creating diverse and intricate KIE questions enhances\nthe performance and robustness of VRDU models. We hope this work encourages\nfuture studies on data quality for generative model training.", "published": "2024-10-20 19:42:30", "link": "http://arxiv.org/abs/2410.15484v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RoMemes: A multimodal meme corpus for the Romanian language", "abstract": "Memes are becoming increasingly more popular in online media, especially in\nsocial networks. They usually combine graphical representations (images,\ndrawings, animations or video) with text to convey powerful messages. In order\nto extract, process and understand the messages, AI applications need to employ\nmultimodal algorithms. In this paper, we introduce a curated dataset of real\nmemes in the Romanian language, with multiple annotation levels. Baseline\nalgorithms were employed to demonstrate the usability of the dataset. Results\nindicate that further research is needed to improve the processing capabilities\nof AI tools when faced with Internet memes.", "published": "2024-10-20 20:26:53", "link": "http://arxiv.org/abs/2410.15497v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reverse Question Answering: Can an LLM Write a Question so Hard (or Bad)\n  that it Can't Answer?", "abstract": "Question answering (QA), giving correct answers to questions, is a popular\ntask, but we test reverse question answering (RQA): for an input answer, give a\nquestion with that answer. Past work tests QA and RQA separately, but we test\nthem jointly, comparing their difficulty, aiding benchmark design, and checking\nreasoning consistency. We run 16 LLMs on QA and RQA with trivia\nquestions/answers, revealing: 1) Versus QA, LLMs are much less accurate in RQA\nfor numerical answers, but slightly more accurate in RQA for textual answers;\n2) LLMs often answer their own invalid questions from RQA accurately in QA, so\nRQA errors are not from knowledge gaps alone; 3) RQA errors correlate with\nquestion difficulty and inversely correlate with answer frequencies in the\nDolma corpus; and 4) LLMs struggle to provide valid multi-hop questions. By\nfinding question and answer types that lead to RQA errors, we suggest\nimprovements for LLM reasoning.", "published": "2024-10-20 21:17:49", "link": "http://arxiv.org/abs/2410.15512v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SceneGraMMi: Scene Graph-boosted Hybrid-fusion for Multi-Modal\n  Misinformation Veracity Prediction", "abstract": "Misinformation undermines individual knowledge and affects broader societal\nnarratives. Despite growing interest in the research community in multi-modal\nmisinformation detection, existing methods exhibit limitations in capturing\nsemantic cues, key regions, and cross-modal similarities within multi-modal\ndatasets. We propose SceneGraMMi, a Scene Graph-boosted Hybrid-fusion approach\nfor Multi-modal Misinformation veracity prediction, which integrates scene\ngraphs across different modalities to improve detection performance.\nExperimental results across four benchmark datasets show that SceneGraMMi\nconsistently outperforms state-of-the-art methods. In a comprehensive ablation\nstudy, we highlight the contribution of each component, while Shapley values\nare employed to examine the explainability of the model's decision-making\nprocess.", "published": "2024-10-20 21:55:13", "link": "http://arxiv.org/abs/2410.15517v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do RAG Systems Cover What Matters? Evaluating and Optimizing Responses\n  with Sub-Question Coverage", "abstract": "Evaluating retrieval-augmented generation (RAG) systems remains challenging,\nparticularly for open-ended questions that lack definitive answers and require\ncoverage of multiple sub-topics. In this paper, we introduce a novel evaluation\nframework based on sub-question coverage, which measures how well a RAG system\naddresses different facets of a question. We propose decomposing questions into\nsub-questions and classifying them into three types -- core, background, and\nfollow-up -- to reflect their roles and importance. Using this categorization,\nwe introduce a fine-grained evaluation protocol that provides insights into the\nretrieval and generation characteristics of RAG systems, including three\ncommercial generative answer engines: You.com, Perplexity AI, and Bing Chat.\nInterestingly, we find that while all answer engines cover core sub-questions\nmore often than background or follow-up ones, they still miss around 50% of\ncore sub-questions, revealing clear opportunities for improvement. Further,\nsub-question coverage metrics prove effective for ranking responses, achieving\n82% accuracy compared to human preference annotations. Lastly, we also\ndemonstrate that leveraging core sub-questions enhances both retrieval and\nanswer generation in a RAG system, resulting in a 74% win rate over the\nbaseline that lacks sub-questions.", "published": "2024-10-20 22:59:34", "link": "http://arxiv.org/abs/2410.15531v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lossless KV Cache Compression to 2%", "abstract": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression.", "published": "2024-10-20 02:17:35", "link": "http://arxiv.org/abs/2410.15252v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "When Machine Unlearning Meets Retrieval-Augmented Generation (RAG): Keep\n  Secret or Forget Knowledge?", "abstract": "The deployment of large language models (LLMs) like ChatGPT and Gemini has\nshown their powerful natural language generation capabilities. However, these\nmodels can inadvertently learn and retain sensitive information and harmful\ncontent during training, raising significant ethical and legal concerns. To\naddress these issues, machine unlearning has been introduced as a potential\nsolution. While existing unlearning methods take into account the specific\ncharacteristics of LLMs, they often suffer from high computational demands,\nlimited applicability, or the risk of catastrophic forgetting. To address these\nlimitations, we propose a lightweight unlearning framework based on\nRetrieval-Augmented Generation (RAG) technology. By modifying the external\nknowledge base of RAG, we simulate the effects of forgetting without directly\ninteracting with the unlearned LLM. We approach the construction of unlearned\nknowledge as a constrained optimization problem, deriving two key components\nthat underpin the effectiveness of RAG-based unlearning. This RAG-based\napproach is particularly effective for closed-source LLMs, where existing\nunlearning methods often fail. We evaluate our framework through extensive\nexperiments on both open-source and closed-source models, including ChatGPT,\nGemini, Llama-2-7b-chat-hf, and PaLM 2. The results demonstrate that our\napproach meets five key unlearning criteria: effectiveness, universality,\nharmlessness, simplicity, and robustness. Meanwhile, this approach can extend\nto multimodal large language models and LLM-based agents.", "published": "2024-10-20 03:51:01", "link": "http://arxiv.org/abs/2410.15267v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "TAGExplainer: Narrating Graph Explanations for Text-Attributed Graph\n  Learning Models", "abstract": "Representation learning of Text-Attributed Graphs (TAGs) has garnered\nsignificant attention due to its applications in various domains, including\nrecommendation systems and social networks. Despite advancements in TAG\nlearning methodologies, challenges remain in explainability due to the\nblack-box nature of existing TAG representation learning models. This paper\npresents TAGExplainer, the first method designed to generate natural language\nexplanations for TAG learning. TAGExplainer employs a generative language model\nthat maps input-output pairs to explanations reflecting the model's\ndecision-making process. To address the lack of annotated ground truth\nexplanations in real-world scenarios, we propose first generating pseudo-labels\nthat capture the model's decisions from saliency-based explanations, then the\npseudo-label generator is iteratively trained based on three training\nobjectives focusing on faithfulness and brevity via Expert Iteration, to\nimprove the quality of generated pseudo-labels. The high-quality pseudo-labels\nare finally utilized to train an end-to-end explanation generator model.\nExtensive experiments are conducted to demonstrate the effectiveness of\nTAGExplainer in producing faithful and concise natural language explanations.", "published": "2024-10-20 03:55:46", "link": "http://arxiv.org/abs/2410.15268v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Redefining Proactivity for Information Seeking Dialogue", "abstract": "Information-Seeking Dialogue (ISD) agents aim to provide accurate responses\nto user queries. While proficient in directly addressing user queries, these\nagents, as well as LLMs in general, predominantly exhibit reactive behavior,\nlacking the ability to generate proactive responses that actively engage users\nin sustained conversations. However, existing definitions of proactive dialogue\nin this context do not focus on how each response actively engages the user and\nsustains the conversation. Hence, we present a new definition of proactivity\nthat focuses on enhancing the `proactiveness' of each generated response via\nthe introduction of new information related to the initial query. To this end,\nwe construct a proactive dialogue dataset comprising 2,000 single-turn\nconversations, and introduce several automatic metrics to evaluate response\n`proactiveness' which achieved high correlation with human annotation.\nAdditionally, we introduce two innovative Chain-of-Thought (CoT) prompts, the\n3-step CoT and the 3-in-1 CoT prompts, which consistently outperform standard\nprompts by up to 90% in the zero-shot setting.", "published": "2024-10-20 05:57:10", "link": "http://arxiv.org/abs/2410.15297v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LlamaLens: Specialized Multilingual LLM for Analyzing News and Social\n  Media Content", "abstract": "Large Language Models (LLMs) have demonstrated remarkable success as\ngeneral-purpose task solvers across various fields. However, their capabilities\nremain limited when addressing domain-specific problems, particularly in\ndownstream NLP tasks. Research has shown that models fine-tuned on\ninstruction-based downstream NLP datasets outperform those that are not\nfine-tuned. While most efforts in this area have primarily focused on\nresource-rich languages like English and broad domains, little attention has\nbeen given to multilingual settings and specific domains. To address this gap,\nthis study focuses on developing a specialized LLM, LlamaLens, for analyzing\nnews and social media content in a multilingual context. To the best of our\nknowledge, this is the first attempt to tackle both domain specificity and\nmultilinguality, with a particular focus on news and social media. Our\nexperimental setup includes 18 tasks, represented by 52 datasets covering\nArabic, English, and Hindi. We demonstrate that LlamaLens outperforms the\ncurrent state-of-the-art (SOTA) on 23 testing sets, and achieves comparable\nperformance on 8 sets. We make the models and resources publicly available for\nthe research community\n(https://huggingface.co/collections/QCRI/llamalens-672f7e0604a0498c6a2f0fe9).", "published": "2024-10-20 06:37:37", "link": "http://arxiv.org/abs/2410.15308v2", "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "CompAct: Compressed Activations for Memory-Efficient LLM Training", "abstract": "We introduce CompAct, a technique that reduces peak memory utilization on GPU\nby 25-30% for pretraining and 50% for fine-tuning of LLMs. Peak device memory\nis a major limiting factor in training LLMs, with various recent works aiming\nto reduce model memory. However most works don't target the largest component\nof allocated memory during training: the model's compute graph, which is stored\nfor the backward pass. By storing low-rank, compressed activations to be used\nin the backward pass we greatly reduce the required memory, unlike previous\nmethods which only reduce optimizer overheads or the number of trained\nparameters. Our compression uses random projection matrices, thus avoiding\nadditional memory overheads. Comparisons with previous techniques for either\npretraining or fine-tuning show that CompAct substantially improves existing\ncompute-performance tradeoffs. We expect CompAct's savings to scale even higher\nfor larger models.", "published": "2024-10-20 10:24:38", "link": "http://arxiv.org/abs/2410.15352v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Comprehensive Evaluation of Cognitive Biases in LLMs", "abstract": "We present a large-scale evaluation of 30 cognitive biases in 20\nstate-of-the-art large language models (LLMs) under various decision-making\nscenarios. Our contributions include a novel general-purpose test framework for\nreliable and large-scale generation of tests for LLMs, a benchmark dataset with\n30,000 tests for detecting cognitive biases in LLMs, and a comprehensive\nassessment of the biases found in the 20 evaluated LLMs. Our work confirms and\nbroadens previous findings suggesting the presence of cognitive biases in LLMs\nby reporting evidence of all 30 tested biases in at least some of the 20 LLMs.\nWe publish our framework code to encourage future research on biases in LLMs:\nhttps://github.com/simonmalberg/cognitive-biases-in-llms", "published": "2024-10-20 15:07:51", "link": "http://arxiv.org/abs/2410.15413v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating Consistencies in LLM responses through a Semantic Clustering\n  of Question Answering", "abstract": "In the realm of Large Language Model (LLM) functionalities, providing\nreliable information is paramount, yet reports suggest that LLM outputs lack\nconsistency. This inconsistency, often at-tributed to randomness in token\nsampling, under-mines user trust as it leads to varying responses even for\nidentical queries. In this paper, we present a new approach for evaluating\nsemantic consistencies of LLM including comparison of alternative tech-niques.\nOur approach evaluates whether LLM re-sponses are semantically congruent for a\ngiven question, recognizing that as syntactically different sentences may\nconvey the same meaning. Here-tofore, To enhance LLM consistency, two main\napproaches have been explored: Leverage external knowledge as context like the\nRAG pattern or use Zero-shot-CoT to improve performance of LLM itself. We apply\nour evaluation approach to these techniques, and demonstrate to compare the\nim-pact of these methods on LLM response con-sistency across different domains\nof question an-swering tasks. Using the TruthfulQA dataset to assess LLM\nresponses, the study induces N re-sponses per question from the LLM and\nclusters semantically equivalent sentences to measure semantic consistency\nacross 37 categories. Through this, it quantitatively analyzes the\neffectiveness of the aforementioned methods in improving LLM performance before\nand after their adoption.", "published": "2024-10-20 16:21:25", "link": "http://arxiv.org/abs/2410.15440v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CROPE: Evaluating In-Context Adaptation of Vision and Language Models to\n  Culture-Specific Concepts", "abstract": "As Vision and Language models (VLMs) are reaching users across the globe,\nassessing their cultural understanding has become a critical challenge. In this\npaper, we introduce CROPE, a visual question answering benchmark designed to\nprobe the knowledge of culture-specific concepts and evaluate the capacity for\ncultural adaptation through contextual information. This allows us to\ndistinguish between parametric knowledge acquired during training and\ncontextual knowledge provided during inference via visual and textual\ndescriptions. Our evaluation of several state-of-the-art open VLMs shows large\nperformance disparities between culture-specific and common concepts in the\nparametric setting. Moreover, experiments with contextual knowledge indicate\nthat models struggle to effectively utilize multimodal information and bind\nculture-specific concepts to their depictions. Our findings reveal limitations\nin the cultural understanding and adaptability of current VLMs that need to be\naddressed toward more culturally inclusive models.", "published": "2024-10-20 17:31:19", "link": "http://arxiv.org/abs/2410.15453v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "MedLogic-AQA: Enhancing Medical Question Answering with Abstractive\n  Models Focusing on Logical Structures", "abstract": "In Medical question-answering (QA) tasks, the need for effective systems is\npivotal in delivering accurate responses to intricate medical queries. However,\nexisting approaches often struggle to grasp the intricate logical structures\nand relationships inherent in medical contexts, thus limiting their capacity to\nfurnish precise and nuanced answers. In this work, we address this gap by\nproposing a novel Abstractive QA system MedLogic-AQA that harnesses First Order\nLogic (FOL) based rules extracted from both context and questions to generate\nwell-grounded answers. Through initial experimentation, we identified six\npertinent first-order logical rules, which were then used to train a\nLogic-Understanding (LU) model capable of generating logical triples for a\ngiven context, question, and answer. These logic triples are then integrated\ninto the training of MedLogic-AQA, enabling effective and coherent reasoning\nduring answer generation. This distinctive fusion of logical reasoning with\nabstractive QA equips our system to produce answers that are logically sound,\nrelevant, and engaging. Evaluation with respect to both automated and\nhuman-based demonstrates the robustness of MedLogic-AQA against strong\nbaselines. Through empirical assessments and case studies, we validate the\nefficacy of MedLogic-AQA in elevating the quality and comprehensiveness of\nanswers in terms of reasoning as well as informativeness", "published": "2024-10-20 18:29:38", "link": "http://arxiv.org/abs/2410.15463v1", "categories": ["cs.CL", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Keep Guessing? When Considering Inference Scaling, Mind the Baselines", "abstract": "Scaling inference compute in large language models (LLMs) through repeated\nsampling consistently increases the coverage (fraction of problems solved) as\nthe number of samples increases. We conjecture that this observed improvement\nis partially due to the answer distribution of standard evaluation benchmarks,\nwhich is skewed towards a relatively small set of common answers. To test this\nconjecture, we define a baseline that enumerates answers according to their\nprevalence in the training set. Experiments spanning two domains --\nmathematical reasoning and factual knowledge -- reveal that this baseline\noutperforms repeated model sampling for some LLMs, while the coverage for\nothers is on par with that of a mixture strategy that obtains $k$ answers by\nusing only $10$ model samples and similarly guessing the remaining $k-10$\nattempts via enumeration. Our baseline enables a more accurate measurement of\nhow much repeated sampling improves coverage in such settings beyond\nprompt-agnostic guessing.", "published": "2024-10-20 18:43:05", "link": "http://arxiv.org/abs/2410.15466v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Grammatical Error Correction for Low-Resource Languages: The Case of\n  Zarma", "abstract": "Grammatical error correction (GEC) aims to improve quality and readability of\ntexts through accurate correction of linguistic mistakes. Previous work has\nfocused on high-resource languages, while low-resource languages lack robust\ntools. However, low-resource languages often face problems such as:\nnon-standard orthography, limited annotated corpora, and diverse dialects,\nwhich slows down the development of GEC tools. We present a study on GEC for\nZarma, spoken by over five million in West Africa. We compare three approaches:\nrule-based methods, machine translation (MT) models, and large language models\n(LLMs). We evaluated them using a dataset of more than 250,000 examples,\nincluding synthetic and human-annotated data. Our results showed that the\nMT-based approach using M2M100 outperforms others, with a detection rate of 95.\n82% and a suggestion accuracy of 78. 90% in automatic evaluations (AE) and an\naverage score of 3.0 out of 5.0 in manual evaluation (ME) from native speakers\nfor grammar and logical corrections. The rule-based method was effective for\nspelling errors but failed on complex context-level errors. LLMs -- MT5-small\n-- showed moderate performance. Our work supports use of MT models to enhance\nGEC in low-resource settings, and we validated these results with Bambara,\nanother West African language.", "published": "2024-10-20 23:51:36", "link": "http://arxiv.org/abs/2410.15539v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Autonomous Driving (LLM4AD): Concept,\n  Benchmark, Experiments, and Challenges", "abstract": "With the broader usage and highly successful development of Large Language\nModels (LLMs), there has been a growth of interest and demand for applying LLMs\nto autonomous driving technology. Driven by their natural language\nunderstanding and reasoning ability, LLMs have the potential to enhance various\naspects of autonomous driving systems, from perception and scene understanding\nto language interaction and decision-making. In this paper, we first introduce\nthe novel concept of designing LLMs for autonomous driving (LLM4AD). Then, we\npropose a comprehensive benchmark for evaluating the instruction-following\nabilities of LLM4AD in simulation. Furthermore, we conduct a series of\nexperiments on real-world vehicle platforms, thoroughly evaluating the\nperformance and potential of our LLM4AD systems. Finally, we envision the main\nchallenges of LLM4AD, including latency, deployment, security and privacy,\nsafety, trust and transparency, and personalization. Our research highlights\nthe significant potential of LLMs to enhance various aspects of autonomous\nvehicle technology, from perception and scene understanding to language\ninteraction and decision-making.", "published": "2024-10-20 04:36:19", "link": "http://arxiv.org/abs/2410.15281v3", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.RO"}
{"title": "Who is Undercover? Guiding LLMs to Explore Multi-Perspective Team Tactic\n  in the Game", "abstract": "Large Language Models (LLMs) are pivotal AI agents in complex tasks but still\nface challenges in open decision-making problems within complex scenarios. To\naddress this, we use the language logic game ``Who is Undercover?'' (WIU) as an\nexperimental platform to propose the Multi-Perspective Team Tactic (MPTT)\nframework. MPTT aims to cultivate LLMs' human-like language expression logic,\nmulti-dimensional thinking, and self-perception in complex scenarios. By\nalternating speaking and voting sessions, integrating techniques like\nself-perspective, identity-determination, self-reflection, self-summary and\nmulti-round find-teammates, LLM agents make rational decisions through\nstrategic concealment and communication, fostering human-like trust.\nPreliminary results show that MPTT, combined with WIU, leverages LLMs'\ncognitive capabilities to create a decision-making framework that can simulate\nreal society. This framework aids minority groups in communication and\nexpression, promoting fairness and diversity in decision-making. Additionally,\nour Human-in-the-loop experiments demonstrate that LLMs can learn and align\nwith human behaviors through interactive, indicating their potential for active\nparticipation in societal decision-making.", "published": "2024-10-20 06:41:31", "link": "http://arxiv.org/abs/2410.15311v1", "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.AI"}
{"title": "Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant", "abstract": "Large Language Models (LLMs) have revolutionized natural language processing,\nbut their application to speech-based tasks remains challenging due to the\ncomplexities of integrating audio and text modalities. This paper introduces\nIchigo, a mixed-modal model that seamlessly processes interleaved sequences of\nspeech and text. Utilizing a tokenized early-fusion approach, Ichigo quantizes\nspeech into discrete tokens and employs a uniform transformer-based\narchitecture for both speech and text modalities. This method enables joint\nreasoning and generation across modalities without the need for separate\nadapters. We present a comprehensive training methodology, including\npre-training on multilingual speech recognition datasets and fine-tuning on a\ncurated instruction dataset. Ichigo demonstrates state-of-the-art performance\non speech question-answering benchmarks, outperforming existing open-source\nspeech language models and achieving comparable results to cascaded systems.\nNotably, Ichigo exhibits a latency of just 111 ms to first token generation,\nsignificantly lower than current models. Our approach not only advances the\nfield of multimodal AI but also provides a framework for smaller research teams\nto contribute effectively to open-source speech-language models.", "published": "2024-10-20 07:03:49", "link": "http://arxiv.org/abs/2410.15316v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Causality for Large Language Models", "abstract": "Recent breakthroughs in artificial intelligence have driven a paradigm shift,\nwhere large language models (LLMs) with billions or trillions of parameters are\ntrained on vast datasets, achieving unprecedented success across a series of\nlanguage tasks. However, despite these successes, LLMs still rely on\nprobabilistic modeling, which often captures spurious correlations rooted in\nlinguistic patterns and social stereotypes, rather than the true causal\nrelationships between entities and events. This limitation renders LLMs\nvulnerable to issues such as demographic biases, social stereotypes, and LLM\nhallucinations. These challenges highlight the urgent need to integrate\ncausality into LLMs, moving beyond correlation-driven paradigms to build more\nreliable and ethically aligned AI systems.\n  While many existing surveys and studies focus on utilizing prompt engineering\nto activate LLMs for causal knowledge or developing benchmarks to assess their\ncausal reasoning abilities, most of these efforts rely on human intervention to\nactivate pre-trained models. How to embed causality into the training process\nof LLMs and build more general and intelligent models remains unexplored.\nRecent research highlights that LLMs function as causal parrots, capable of\nreciting causal knowledge without truly understanding or applying it. These\nprompt-based methods are still limited to human interventional improvements.\nThis survey aims to address this gap by exploring how causality can enhance\nLLMs at every stage of their lifecycle-from token embedding learning and\nfoundation model training to fine-tuning, alignment, inference, and\nevaluation-paving the way for more interpretable, reliable, and\ncausally-informed models. Additionally, we further outline six promising future\ndirections to advance LLM development, enhance their causal reasoning\ncapabilities, and address the current limitations these models face.", "published": "2024-10-20 07:22:23", "link": "http://arxiv.org/abs/2410.15319v1", "categories": ["cs.CL", "cs.AI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models", "abstract": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference.", "published": "2024-10-20 08:42:29", "link": "http://arxiv.org/abs/2410.15332v2", "categories": ["cs.LG", "cs.CL", "cs.DC", "cs.PF"], "primary_category": "cs.LG"}
{"title": "Faster-GCG: Efficient Discrete Optimization Jailbreak Attacks against\n  Aligned Large Language Models", "abstract": "Aligned Large Language Models (LLMs) have demonstrated remarkable performance\nacross various tasks. However, LLMs remain susceptible to jailbreak adversarial\nattacks, where adversaries manipulate prompts to elicit malicious responses\nthat aligned LLMs should have avoided. Identifying these vulnerabilities is\ncrucial for understanding the inherent weaknesses of LLMs and preventing their\npotential misuse. One pioneering work in jailbreaking is the GCG attack, a\ndiscrete token optimization algorithm that seeks to find a suffix capable of\njailbreaking aligned LLMs. Despite the success of GCG, we find it suboptimal,\nrequiring significantly large computational costs, and the achieved\njailbreaking performance is limited. In this work, we propose Faster-GCG, an\nefficient adversarial jailbreak method by delving deep into the design of GCG.\nExperiments demonstrate that Faster-GCG can surpass the original GCG with only\n1/10 of the computational cost, achieving significantly higher attack success\nrates on various open-source aligned LLMs. In addition, We demonstrate that\nFaster-GCG exhibits improved attack transferability when testing on\nclosed-sourced LLMs such as ChatGPT.", "published": "2024-10-20 11:27:41", "link": "http://arxiv.org/abs/2410.15362v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "IPO: Interpretable Prompt Optimization for Vision-Language Models", "abstract": "Pre-trained vision-language models like CLIP have remarkably adapted to\nvarious downstream tasks. Nonetheless, their performance heavily depends on the\nspecificity of the input text prompts, which requires skillful prompt template\nengineering. Instead, current approaches to prompt optimization learn the\nprompts through gradient descent, where the prompts are treated as adjustable\nparameters. However, these methods tend to lead to overfitting of the base\nclasses seen during training and produce prompts that are no longer\nunderstandable by humans. This paper introduces a simple but interpretable\nprompt optimizer (IPO), that utilizes large language models (LLMs) to generate\ntextual prompts dynamically. We introduce a Prompt Optimization Prompt that not\nonly guides LLMs in creating effective prompts but also stores past prompts\nwith their performance metrics, providing rich in-context information.\nAdditionally, we incorporate a large multimodal model (LMM) to condition on\nvisual content by generating image descriptions, which enhance the interaction\nbetween textual and visual modalities. This allows for thae creation of\ndataset-specific prompts that improve generalization performance, while\nmaintaining human comprehension. Extensive testing across 11 datasets reveals\nthat IPO not only improves the accuracy of existing gradient-descent-based\nprompt learning methods but also considerably enhances the interpretability of\nthe generated prompts. By leveraging the strengths of LLMs, our approach\nensures that the prompts remain human-understandable, thereby facilitating\nbetter transparency and oversight for vision-language models.", "published": "2024-10-20 14:10:22", "link": "http://arxiv.org/abs/2410.15397v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Hallucination Detox: Sensitivity Dropout (SenD) for Large Language Model\n  Training", "abstract": "As large language models (LLMs) are increasingly deployed across various\nindustries, concerns regarding their reliability, particularly due to\nhallucinations - outputs that are factually inaccurate or irrelevant to user\ninput - have grown. Our research investigates the relationship between the\ntraining process and the emergence of hallucinations to address a key gap in\nexisting research that focuses primarily on post hoc detection and mitigation\nstrategies. Using models from the Pythia suite (70M - 12B parameters) and\nseveral hallucination detection metrics, we analyze hallucination trends\nthroughout training and explore LLM internal dynamics. We introduce Sensitivity\nDropout (SenD), a novel training protocol designed to mitigate hallucinations\nby reducing variance during training. SenD achieves this by deterministically\ndropping embedding indices with significant variability, referred to as\nSensitive Embedding Indices. In addition, we develop an unsupervised\nhallucination detection metric, Efficient EigenScore (EES), which approximates\nthe traditional EigenScore at 2x speed. This efficient metric is integrated\ninto our protocol, allowing SenD to be both computationally scalable and\neffective at reducing hallucinations. Our empirical evaluation demonstrates\nthat our approach improves LLM reliability at test time by up to 40% compared\nto normal training while also providing an efficient method to improve factual\naccuracy when adapting LLMs to Wikipedia, Medical, and LegalBench domains.", "published": "2024-10-20 18:18:23", "link": "http://arxiv.org/abs/2410.15460v3", "categories": ["cs.AI", "cs.CL", "math.SP"], "primary_category": "cs.AI"}
{"title": "Hey GPT, Can You be More Racist? Analysis from Crowdsourced Attempts to\n  Elicit Biased Content from Generative AI", "abstract": "The widespread adoption of large language models (LLMs) and generative AI\n(GenAI) tools across diverse applications has amplified the importance of\naddressing societal biases inherent within these technologies. While the NLP\ncommunity has extensively studied LLM bias, research investigating how\nnon-expert users perceive and interact with biases from these systems remains\nlimited. As these technologies become increasingly prevalent, understanding\nthis question is crucial to inform model developers in their efforts to\nmitigate bias. To address this gap, this work presents the findings from a\nuniversity-level competition, which challenged participants to design prompts\nfor eliciting biased outputs from GenAI tools. We quantitatively and\nqualitatively analyze the competition submissions and identify a diverse set of\nbiases in GenAI and strategies employed by participants to induce bias in\nGenAI. Our finding provides unique insights into how non-expert users perceive\nand interact with biases from GenAI tools.", "published": "2024-10-20 18:44:45", "link": "http://arxiv.org/abs/2410.15467v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Exploring Curriculum Learning for Vision-Language Tasks: A Study on\n  Small-Scale Multimodal Training", "abstract": "For specialized domains, there is often not a wealth of data with which to\ntrain large machine learning models. In such limited data / compute settings,\nvarious methods exist aiming to $\\textit{do more with less}$, such as\nfinetuning from a pretrained model, modulating difficulty levels as data are\npresented to a model (curriculum learning), and considering the role of model\ntype / size. Approaches to efficient $\\textit{machine}$ learning also take\ninspiration from $\\textit{human}$ learning by considering use cases where\nmachine learning systems have access to approximately the same number of words\nexperienced by a 13 year old child (100M words). We investigate the role of 3\nprimary variables in a limited data regime as part of the multimodal track of\nthe BabyLM challenge. We contrast: (i) curriculum learning, (ii), pretraining\n(with text-only data), (iii) model type. We modulate these variables and assess\nthem on two types of tasks: (a) multimodal (text+image), and (b) unimodal\n(text-only) tasks. We find that curriculum learning benefits multimodal\nevaluations over non-curriclum learning models, particularly when combining\ntext-only pretraining. On text-only tasks, curriculum learning appears to help\nmodels with smaller trainable parameter counts. We suggest possible reasons\nbased on architectural differences and training designs as to why one might\nobserve such results.", "published": "2024-10-20 21:03:51", "link": "http://arxiv.org/abs/2410.15509v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "M-RewardBench: Evaluating Reward Models in Multilingual Settings", "abstract": "Reward models (RMs) have driven the state-of-the-art performance of LLMs\ntoday by enabling the integration of human feedback into the language modeling\nprocess. However, RMs are primarily trained and evaluated in English, and their\ncapabilities in multilingual settings remain largely understudied. In this\nwork, we conduct a systematic evaluation of several reward models in\nmultilingual settings. We first construct the first-of-its-kind multilingual RM\nevaluation benchmark, M-RewardBench, consisting of 2.87k preference instances\nfor 23 typologically diverse languages, that tests the chat, safety, reasoning,\nand translation capabilities of RMs. We then rigorously evaluate a wide range\nof reward models on M-RewardBench, offering fresh insights into their\nperformance across diverse languages. We identify a significant gap in RMs'\nperformances between English and non-English languages and show that RM\npreferences can change substantially from one language to another. We also\npresent several findings on how different multilingual aspects impact RM\nperformance. Specifically, we show that the performance of RMs is improved with\nimproved translation quality. Similarly, we demonstrate that the models exhibit\nbetter performance for high-resource languages. We release M-RewardBench\ndataset and the codebase in this study to facilitate a better understanding of\nRM evaluation in multilingual settings.", "published": "2024-10-20 22:09:44", "link": "http://arxiv.org/abs/2410.15522v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mitigating Forgetting in LLM Supervised Fine-Tuning and Preference\n  Learning", "abstract": "Post-training of pre-trained LLMs, which typically consists of the supervised\nfine-tuning (SFT) stage and the preference learning (RLHF or DPO) stage, is\ncrucial to effective and safe LLM applications. The widely adopted approach in\npost-training popular open-source LLMs is to sequentially perform SFT and\nRLHF/DPO. However, sequential training is sub-optimal in terms of SFT and\nRLHF/DPO trade-off: the LLM gradually forgets about the first stage's training\nwhen undergoing the second stage's training. We theoretically prove the\nsub-optimality of sequential post-training. Furthermore, we propose a practical\njoint post-training framework with theoretical convergence guarantees and\nempirically outperforms sequential post-training framework, while having\nsimilar computational cost. Our code is available at\nhttps://github.com/heshandevaka/XRIGHT.", "published": "2024-10-20 19:38:41", "link": "http://arxiv.org/abs/2410.15483v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.OC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Construction and Analysis of Impression Caption Dataset for\n  Environmental Sounds", "abstract": "Some datasets with the described content and order of occurrence of sounds\nhave been released for conversion between environmental sound and text.\nHowever, there are very few texts that include information on the impressions\nhumans feel, such as \"sharp\" and \"gorgeous,\" when they hear environmental\nsounds. In this study, we constructed a dataset with impression captions for\nenvironmental sounds that describe the impressions humans have when hearing\nthese sounds. We used ChatGPT to generate impression captions and selected the\nmost appropriate captions for sound by humans. Our dataset consists of 3,600\nimpression captions for environmental sounds. To evaluate the appropriateness\nof impression captions for environmental sounds, we conducted subjective and\nobjective evaluations. From our evaluation results, we indicate that\nappropriate impression captions for environmental sounds can be generated.", "published": "2024-10-20 23:01:02", "link": "http://arxiv.org/abs/2410.15532v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ConSinger: Efficient High-Fidelity Singing Voice Generation with Minimal\n  Steps", "abstract": "Singing voice synthesis (SVS) system is expected to generate high-fidelity\nsinging voice from given music scores (lyrics, duration and pitch). Recently,\ndiffusion models have performed well in this field. However, sacrificing\ninference speed to exchange with high-quality sample generation limits its\napplication scenarios. In order to obtain high quality synthetic singing voice\nmore efficiently, we propose a singing voice synthesis method based on the\nconsistency model, ConSinger, to achieve high-fidelity singing voice synthesis\nwith minimal steps. The model is trained by applying consistency constraint and\nthe generation quality is greatly improved at the expense of a small amount of\ninference speed. Our experiments show that ConSinger is highly competitive with\nthe baseline model in terms of generation speed and quality. Audio samples are\navailable at https://keylxiao.github.io/consinger.", "published": "2024-10-20 09:32:03", "link": "http://arxiv.org/abs/2410.15342v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Voice Quality in Speech Anonymization With Just\n  Perception-Informed Losses", "abstract": "The increasing use of cloud-based speech assistants has heightened the need\nfor effective speech anonymization, which aims to obscure a speaker's identity\nwhile retaining critical information for subsequent tasks. One approach to\nachieving this is through voice conversion. While existing methods often\nemphasize complex architectures and training techniques, our research\nunderscores the importance of loss functions inspired by the human auditory\nsystem. Our proposed loss functions are model-agnostic, incorporating\nhandcrafted and deep learning-based features to effectively capture quality\nrepresentations. Through objective and subjective evaluations, we demonstrate\nthat a VQVAE-based model, enhanced with our perception-driven losses, surpasses\nthe vanilla model in terms of naturalness, intelligibility, and prosody while\nmaintaining speaker anonymity. These improvements are consistently observed\nacross various datasets, languages, target speakers, and genders.", "published": "2024-10-20 20:33:44", "link": "http://arxiv.org/abs/2410.15499v1", "categories": ["cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Anonymising Elderly and Pathological Speech: Voice Conversion Using DDSP\n  and Query-by-Example", "abstract": "Speech anonymisation aims to protect speaker identity by changing personal\nidentifiers in speech while retaining linguistic content. Current methods fail\nto retain prosody and unique speech patterns found in elderly and pathological\nspeech domains, which is essential for remote health monitoring. To address\nthis gap, we propose a voice conversion-based method (DDSP-QbE) using\ndifferentiable digital signal processing and query-by-example. The proposed\nmethod, trained with novel losses, aids in disentangling linguistic, prosodic,\nand domain representations, enabling the model to adapt to uncommon speech\npatterns. Objective and subjective evaluations show that DDSP-QbE significantly\noutperforms the voice conversion state-of-the-art concerning intelligibility,\nprosody, and domain preservation across diverse datasets, pathologies, and\nspeakers while maintaining quality and speaker anonymity. Experts validate\ndomain preservation by analysing twelve clinically pertinent domain attributes.", "published": "2024-10-20 20:40:56", "link": "http://arxiv.org/abs/2410.15500v1", "categories": ["cs.AI", "cs.SD", "eess.AS", "q-bio.QM"], "primary_category": "cs.AI"}
