{"title": "Off-Policy Evaluation and Counterfactual Methods in Dynamic Auction Environments", "abstract": "Counterfactual estimators are critical for learning and refining policies\nusing logged data, a process known as Off-Policy Evaluation (OPE). OPE allows\nresearchers to assess new policies without costly experiments, speeding up the\nevaluation process. Online experimental methods, such as A/B tests, are\neffective but often slow, thus delaying the policy selection and optimization\nprocess.\n  In this work, we explore the application of OPE methods in the context of\nresource allocation in dynamic auction environments. Given the competitive\nnature of environments where rapid decision-making is crucial for gaining a\ncompetitive edge, the ability to quickly and accurately assess algorithmic\nperformance is essential. By utilizing counterfactual estimators as a\npreliminary step before conducting A/B tests, we aim to streamline the\nevaluation process, reduce the time and resources required for experimentation,\nand enhance confidence in the chosen policies. Our investigation focuses on the\nfeasibility and effectiveness of using these estimators to predict the outcomes\nof potential resource allocation strategies, evaluate their performance, and\nfacilitate more informed decision-making in policy selection. Motivated by the\noutcomes of our initial study, we envision an advanced analytics system\ndesigned to seamlessly and dynamically assess new resource allocation\nstrategies and policies.", "published": "2025-01-09 14:39:40", "link": "http://arxiv.org/abs/2501.05278v1", "categories": ["cs.AI", "cs.LG", "q-fin.CP"], "primary_category": "cs.AI"}
{"title": "Time-Varying Bidirectional Causal Relationships Between Transaction Fees and Economic Activity of Subsystems Utilizing the Ethereum Blockchain Network", "abstract": "The Ethereum blockchain network enables transaction processing and\nsmart-contract execution through levies of transaction fees, commonly known as\ngas fees. This framework mediates economic participation via a market-based\nmechanism for gas fees, permitting users to offer higher gas fees to expedite\npro-cessing. Historically, the ensuing gas fee volatility led to critical\ndisequilibria between supply and demand for block space, presenting stakeholder\nchallenges. This study examines the dynamic causal interplay between\ntransaction fees and economic subsystems leveraging the network. By utilizing\ndata related to unique active wallets and transaction volume of each subsystem\nand applying time-varying Granger causality analysis, we reveal temporal\nheterogeneity in causal relationships between economic activity and transaction\nfees across all subsystems. This includes (a) a bidirectional causal feedback\nloop between cross-blockchain bridge user activity and transaction fees, which\ndiminishes over time, potentially signaling user migration; (b) a bidirectional\nrelationship between centralized cryptocurrency exchange deposit and withdrawal\ntransaction volume and fees, indicative of increased competition for block\nspace; (c) decentralized exchange volumes causally influence fees, while fees\ncausally influence user activity, although this relationship is weakening,\npotentially due to the diminished significance of decentralized finance; (d)\nintermittent causal relationships with maximal extractable value bots; (e) fees\ncausally in-fluence non-fungible token transaction volumes; and (f) a highly\nsignificant and growing causal influence of transaction fees on stablecoin\nactivity and transaction volumes highlight its prominence.", "published": "2025-01-09 15:03:05", "link": "http://arxiv.org/abs/2501.05299v1", "categories": ["q-fin.GN", "econ.EM", "q-fin.RM", "q-fin.TR", "91G50, 91B84, 62M10, 91G70, 91B25, 91B28, 91B82, 91G15", "J.4; H.3.5; K.4.4; J.1"], "primary_category": "q-fin.GN"}
{"title": "The Intraday Bitcoin Response to Tether Minting and Burning Events: Asymmetry, Investor Sentiment, And \"Whale Alerts\" On Twitter", "abstract": "Tether Limited has the sole authority to create (mint) and destroy (burn)\nTether stablecoins (USDT). This paper investigates Bitcoin's response to USDT\nsupply change events between 2014 and 2021 and identifies an interesting\nasymmetry between Bitcoin's responses to USDT minting and burning events.\nBitcoin responds positively to USDT minting events over 5- to 30-minute event\nwindows, but this response begins declining after 60 minutes. State-dependence\nis also demonstrated, with Bitcoin prices exhibiting a greater increase when\nthe corresponding USDT minting event coincides with positive investor sentiment\nand is announced to the public by data service provider, Whale Alert, on\nTwitter.", "published": "2025-01-09 13:39:22", "link": "http://arxiv.org/abs/2501.05232v1", "categories": ["q-fin.GN", "cs.SI", "q-fin.PR", "q-fin.TR", "91G70, 91G80, 91B25, 91B28, 91B82, 91G15, 62P05", "J.4; J.1; K.4.4"], "primary_category": "q-fin.GN"}
{"title": "Investigating Numerical Translation with Large Language Models", "abstract": "The inaccurate translation of numbers can lead to significant security\nissues, ranging from financial setbacks to medical inaccuracies. While large\nlanguage models (LLMs) have made significant advancements in machine\ntranslation, their capacity for translating numbers has not been thoroughly\nexplored. This study focuses on evaluating the reliability of LLM-based machine\ntranslation systems when handling numerical data. In order to systematically\ntest the numerical translation capabilities of currently open source LLMs, we\nhave constructed a numerical translation dataset between Chinese and English\nbased on real business data, encompassing ten types of numerical translation.\nExperiments on the dataset indicate that errors in numerical translation are a\ncommon issue, with most open-source LLMs faltering when faced with our test\nscenarios. Especially when it comes to numerical types involving large units\nlike ``million\", ``billion\", and \"yi\", even the latest llama3.1 8b model can\nhave error rates as high as 20%. Finally, we introduce three potential\nstrategies to mitigate the numerical mistranslations for large units.", "published": "2025-01-09 02:32:40", "link": "http://arxiv.org/abs/2501.04927v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures", "abstract": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency.", "published": "2025-01-09 06:00:27", "link": "http://arxiv.org/abs/2501.04987v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub\n  Issue Resolution", "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na variety of complex tasks. One significant application of LLMs is in tackling\nsoftware engineering challenges, particularly in resolving real-world tasks on\nGitHub by fixing code based on the issues reported by the users. However, many\ncurrent approaches rely on proprietary LLMs, which limits reproducibility,\naccessibility, and transparency. The critical components of LLMs for addressing\nsoftware engineering issues and how their capabilities can be effectively\nenhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a\nnovel open-source framework designed to effectively and efficiently resolve\nGitHub issues. SWE-Fixer comprises two essential modules: a code file retrieval\nmodule and a code editing module. The retrieval module employs BM25 along with\na lightweight model to achieve coarse-to-fine file retrieval. Subsequently, the\ncode editing module utilizes the other model to generate patches for the\nidentified files. To mitigate the lack of publicly available datasets, we\ncompile an extensive dataset that includes 110K GitHub issues along with their\ncorresponding patches and train the two models of SWE-Fixer separately. We\nassess our approach on the SWE-Bench Lite and Verified benchmarks, achieving\nstate-of-the-art performance among open-source models with scores of 24.7% and\n32.8%, respectively. Additionally, our approach requires only two model calls\nper instance, making it significantly more efficient than existing methods.\nThese results highlight the effectiveness of SWE-Fixer in real-world\ncode-fixing scenarios. We will make our model, dataset, and code publicly\navailable at https://github.com/InternLM/SWE-Fixer.", "published": "2025-01-09 07:54:24", "link": "http://arxiv.org/abs/2501.05040v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ParaRev: Building a dataset for Scientific Paragraph Revision annotated\n  with revision instruction", "abstract": "Revision is a crucial step in scientific writing, where authors refine their\nwork to improve clarity, structure, and academic quality. Existing approaches\nto automated writing assistance often focus on sentence-level revisions, which\nfail to capture the broader context needed for effective modification. In this\npaper, we explore the impact of shifting from sentence-level to paragraph-level\nscope for the task of scientific text revision. The paragraph level definition\nof the task allows for more meaningful changes, and is guided by detailed\nrevision instructions rather than general ones. To support this task, we\nintroduce ParaRev, the first dataset of revised scientific paragraphs with an\nevaluation subset manually annotated with revision instructions. Our\nexperiments demonstrate that using detailed instructions significantly improves\nthe quality of automated revisions compared to general approaches, no matter\nthe model or the metric considered.", "published": "2025-01-09 13:19:55", "link": "http://arxiv.org/abs/2501.05222v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models for Zero-shot Lay Summarisation in\n  Biomedicine and Beyond", "abstract": "In this work, we explore the application of Large Language Models to\nzero-shot Lay Summarisation. We propose a novel two-stage framework for Lay\nSummarisation based on real-life processes, and find that summaries generated\nwith this method are increasingly preferred by human judges for larger models.\nTo help establish best practices for employing LLMs in zero-shot settings, we\nalso assess the ability of LLMs as judges, finding that they are able to\nreplicate the preferences of human judges. Finally, we take the initial steps\ntowards Lay Summarisation for Natural Language Processing (NLP) articles,\nfinding that LLMs are able to generalise to this new domain, and further\nhighlighting the greater utility of summaries generated by our proposed\napproach via an in-depth human evaluation.", "published": "2025-01-09 13:24:11", "link": "http://arxiv.org/abs/2501.05224v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LongProc: Benchmarking Long-Context Language Models on Long Procedural\n  Generation", "abstract": "Existing benchmarks for evaluating long-context language models (LCLMs)\nprimarily focus on long-context recall, requiring models to produce short\nresponses based on a few critical snippets while processing thousands of\nirrelevant tokens. We introduce LongProc (Long Procedural Generation), a new\nbenchmark that requires both the integration of highly dispersed information\nand long-form generation. LongProc consists of six diverse procedural\ngeneration tasks, such as extracting structured information from HTML pages\ninto a TSV format and executing complex search procedures to create travel\nplans. These tasks challenge LCLMs by testing their ability to follow detailed\nprocedural instructions, synthesize and reason over dispersed information, and\ngenerate structured, long-form outputs (up to 8K tokens). Furthermore, as these\ntasks adhere to deterministic procedures and yield structured outputs, they\nenable reliable rule-based evaluation. We evaluate 17 LCLMs on LongProc across\nthree difficulty levels, with maximum numbers of output tokens set at 500, 2K,\nand 8K. Notably, while all tested models claim a context window size above 32K\ntokens, open-weight models typically falter on 2K-token tasks, and\nclosed-source models like GPT-4o show significant degradation on 8K-token\ntasks. Further analysis reveals that LCLMs struggle to maintain long-range\ncoherence in long-form generations. These findings highlight critical\nlimitations in current LCLMs and suggest substantial room for improvement. Data\nand code available at: https://princeton-pli.github.io/LongProc", "published": "2025-01-09 18:16:55", "link": "http://arxiv.org/abs/2501.05414v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Large Language Models for Translating Romanian Computational\n  Problems into English", "abstract": "Recent studies have suggested that large language models (LLMs) underperform\non mathematical and computer science tasks when these problems are translated\nfrom Romanian into English, compared to their original Romanian format.\nAccurate translation is critical for applications ranging from automatic\ntranslations in programming competitions to the creation of high-quality\neducational materials, as well as minimizing errors or fraud in human\ntranslations. This study shows that robust large language models (LLMs) can\nmaintain or even enhance their performance in translating less common languages\nwhen given well-structured prompts. Our findings suggest that LLMs, with\nappropriate supervision, can be reliably used for the automatic translation of\nIOI (International Olympiad in Informatics)-style tasks. We evaluate several\ntranslation methods across multiple LLMs, including OpenRoLLM, Llama 3.1 8B,\nLlama 3.2 3B and GPT-4o, assessing their translation accuracy and performance\nstability through repeated runs. Additionally, we augment the OJI (Romanian\nCounty-Level Informatics Olympiad) Romanian dataset with accurate English\ntranslations, enhancing its utility for future LLM training and evaluation.\nThrough detailed syntactic and semantic analyses, we confirm that with human\noversight, LLMs can serve as a viable solution for multilingual\nproblem-solving. We also compare the translation quality of LLMs against human\ntranslators, as evaluated by a certified expert, underscoring the potential of\nLLMs in realworld scenarios.", "published": "2025-01-09 22:17:44", "link": "http://arxiv.org/abs/2501.05601v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AgoraSpeech: A multi-annotated comprehensive dataset of political\n  discourse through the lens of humans and AI", "abstract": "Political discourse datasets are important for gaining political insights,\nanalyzing communication strategies or social science phenomena. Although\nnumerous political discourse corpora exist, comprehensive, high-quality,\nannotated datasets are scarce. This is largely due to the substantial manual\neffort, multidisciplinarity, and expertise required for the nuanced annotation\nof rhetorical strategies and ideological contexts. In this paper, we present\nAgoraSpeech, a meticulously curated, high-quality dataset of 171 political\nspeeches from six parties during the Greek national elections in 2023. The\ndataset includes annotations (per paragraph) for six natural language\nprocessing (NLP) tasks: text classification, topic identification, sentiment\nanalysis, named entity recognition, polarization and populism detection. A\ntwo-step annotation was employed, starting with ChatGPT-generated annotations\nand followed by exhaustive human-in-the-loop validation. The dataset was\ninitially used in a case study to provide insights during the pre-election\nperiod. However, it has general applicability by serving as a rich source of\ninformation for political and social scientists, journalists, or data\nscientists, while it can be used for benchmarking and fine-tuning NLP and large\nlanguage models (LLMs).", "published": "2025-01-09 18:17:59", "link": "http://arxiv.org/abs/2501.06265v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SUGAR: Leveraging Contextual Confidence for Smarter Retrieval", "abstract": "Bearing in mind the limited parametric knowledge of Large Language Models\n(LLMs), retrieval-augmented generation (RAG) which supplies them with the\nrelevant external knowledge has served as an approach to mitigate the issue of\nhallucinations to a certain extent. However, uniformly retrieving supporting\ncontext makes response generation source-inefficient, as triggering the\nretriever is not always necessary, or even inaccurate, when a model gets\ndistracted by noisy retrieved content and produces an unhelpful answer.\nMotivated by these issues, we introduce Semantic Uncertainty Guided Adaptive\nRetrieval (SUGAR), where we leverage context-based entropy to actively decide\nwhether to retrieve and to further determine between single-step and multi-step\nretrieval. Our empirical results show that selective retrieval guided by\nsemantic uncertainty estimation improves the performance across diverse\nquestion answering tasks, as well as achieves a more efficient inference.", "published": "2025-01-09 01:24:59", "link": "http://arxiv.org/abs/2501.04899v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of\n  Large Language Models", "abstract": "It is crucial for large language models (LLMs) to follow instructions that\ninvolve multiple constraints. However, it is an unexplored area to enhance\nLLMs' ability to follow soft constraints. To bridge the gap, we initially\ndesign a pipeline to construct datasets with high-quality outputs\nautomatically. Additionally, to fully utilize the positive and negative samples\ngenerated during the data construction process, we choose Direct Preference\nOptimization (DPO) as the training method. Furthermore, taking into account the\ndifficulty of soft constraints indicated by the number of constraints, we\ndesign a curriculum learning training paradigm based on the constraint\nquantity. We experimentally evaluate the effectiveness of our methods in\nimproving LLMs' soft constraint following ability and analyze the factors\ndriving the improvements.The datasets and code are publicly available at\nhttps://github.com/Rainier-rq/FollowSoftConstraint.", "published": "2025-01-09 03:34:07", "link": "http://arxiv.org/abs/2501.04945v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SensorQA: A Question Answering Benchmark for Daily-Life Monitoring", "abstract": "With the rapid growth in sensor data, effectively interpreting and\ninterfacing with these data in a human-understandable way has become crucial.\nWhile existing research primarily focuses on learning classification models,\nfewer studies have explored how end users can actively extract useful insights\nfrom sensor data, often hindered by the lack of a proper dataset. To address\nthis gap, we introduce SensorQA, the first human-created question-answering\n(QA) dataset for long-term time-series sensor data for daily life monitoring.\nSensorQA is created by human workers and includes 5.6K diverse and practical\nqueries that reflect genuine human interests, paired with accurate answers\nderived from sensor data. We further establish benchmarks for state-of-the-art\nAI models on this dataset and evaluate their performance on typical edge\ndevices. Our results reveal a gap between current models and optimal QA\nperformance and efficiency, highlighting the need for new contributions. The\ndataset and code are available at:\nhttps://github.com/benjamin-reichman/SensorQA.", "published": "2025-01-09 05:06:44", "link": "http://arxiv.org/abs/2501.04974v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A General Retrieval-Augmented Generation Framework for Multimodal\n  Case-Based Reasoning Applications", "abstract": "Case-based reasoning (CBR) is an experience-based approach to problem\nsolving, where a repository of solved cases is adapted to solve new cases.\nRecent research shows that Large Language Models (LLMs) with\nRetrieval-Augmented Generation (RAG) can support the Retrieve and Reuse stages\nof the CBR pipeline by retrieving similar cases and using them as additional\ncontext to an LLM query. Most studies have focused on text-only applications,\nhowever, in many real-world problems the components of a case are multimodal.\nIn this paper we present MCBR-RAG, a general RAG framework for multimodal CBR\napplications. The MCBR-RAG framework converts non-text case components into\ntext-based representations, allowing it to: 1) learn application-specific\nlatent representations that can be indexed for retrieval, and 2) enrich the\nquery provided to the LLM by incorporating all case components for better\ncontext. We demonstrate MCBR-RAG's effectiveness through experiments conducted\non a simplified Math-24 application and a more complex Backgammon application.\nOur empirical results show that MCBR-RAG improves generation quality compared\nto a baseline LLM with no contextual information provided.", "published": "2025-01-09 07:41:22", "link": "http://arxiv.org/abs/2501.05030v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Enhancing Human-Like Responses in Large Language Models", "abstract": "This paper explores the advancements in making large language models (LLMs)\nmore human-like. We focus on techniques that enhance natural language\nunderstanding, conversational coherence, and emotional intelligence in AI\nsystems. The study evaluates various approaches, including fine-tuning with\ndiverse datasets, incorporating psychological principles, and designing models\nthat better mimic human reasoning patterns. Our findings demonstrate that these\nenhancements not only improve user interactions but also open new possibilities\nfor AI applications across different domains. Future work will address the\nethical implications and potential biases introduced by these human-like\nattributes.", "published": "2025-01-09 07:44:06", "link": "http://arxiv.org/abs/2501.05032v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Centurio: On Drivers of Multilingual Ability of Large Vision-Language\n  Model", "abstract": "Most Large Vision-Language Models (LVLMs) to date are trained predominantly\non English data, which makes them struggle to understand non-English input and\nfail to generate output in the desired target language. Existing efforts\nmitigate these issues by adding multilingual training data, but do so in a\nlargely ad-hoc manner, lacking insight into how different training mixes tip\nthe scale for different groups of languages. In this work, we present a\ncomprehensive investigation into the training strategies for massively\nmultilingual LVLMs. First, we conduct a series of multi-stage experiments\nspanning 13 downstream vision-language tasks and 43 languages, systematically\nexamining: (1) the number of training languages that can be included without\ndegrading English performance and (2) optimal language distributions of\npre-training as well as (3) instruction-tuning data. Further, we (4)\ninvestigate how to improve multilingual text-in-image understanding, and\nintroduce a new benchmark for the task. Surprisingly, our analysis reveals that\none can (i) include as many as 100 training languages simultaneously (ii) with\nas little as 25-50\\% of non-English data, to greatly improve multilingual\nperformance while retaining strong English performance. We further find that\n(iii) including non-English OCR data in pre-training and instruction-tuning is\nparamount for improving multilingual text-in-image understanding. Finally, we\nput all our findings together and train Centurio, a 100-language LVLM, offering\nstate-of-the-art performance in an evaluation covering 14 tasks and 56\nlanguages.", "published": "2025-01-09 10:26:14", "link": "http://arxiv.org/abs/2501.05122v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Biomedical Relation Extraction via Adaptive Document-Relation\n  Cross-Mapping and Concept Unique Identifier", "abstract": "Document-Level Biomedical Relation Extraction (Bio-RE) aims to identify\nrelations between biomedical entities within extensive texts, serving as a\ncrucial subfield of biomedical text mining. Existing Bio-RE methods struggle\nwith cross-sentence inference, which is essential for capturing relations\nspanning multiple sentences. Moreover, previous methods often overlook the\nincompleteness of documents and lack the integration of external knowledge,\nlimiting contextual richness. Besides, the scarcity of annotated data further\nhampers model training. Recent advancements in large language models (LLMs)\nhave inspired us to explore all the above issues for document-level Bio-RE.\nSpecifically, we propose a document-level Bio-RE framework via LLM Adaptive\nDocument-Relation Cross-Mapping (ADRCM) Fine-Tuning and Concept Unique\nIdentifier (CUI) Retrieval-Augmented Generation (RAG). First, we introduce the\nIteration-of-REsummary (IoRs) prompt for solving the data scarcity issue. In\nthis way, Bio-RE task-specific synthetic data can be generated by guiding\nChatGPT to focus on entity relations and iteratively refining synthetic data.\nNext, we propose ADRCM fine-tuning, a novel fine-tuning recipe that establishes\nmappings across different documents and relations, enhancing the model's\ncontextual understanding and cross-sentence inference capabilities. Finally,\nduring the inference, a biomedical-specific RAG approach, named CUI RAG, is\ndesigned to leverage CUIs as indexes for entities, narrowing the retrieval\nscope and enriching the relevant document contexts. Experiments conducted on\nthree Bio-RE datasets (GDA, CDR, and BioRED) demonstrate the state-of-the-art\nperformance of our proposed method by comparing it with other related works.", "published": "2025-01-09 11:19:40", "link": "http://arxiv.org/abs/2501.05155v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GLaM-Sign: Greek Language Multimodal Lip Reading with Integrated Sign\n  Language Accessibility", "abstract": "The Greek Language Multimodal Lip Reading with Integrated Sign Language\nAccessibility (GLaM-Sign) [1] is a groundbreaking resource in accessibility and\nmultimodal AI, designed to support Deaf and Hard-of-Hearing (DHH) individuals.\nDeveloped from the FEELIT project [2], it integrates high-resolution audio,\nvideo, textual transcriptions, and Greek Sign Language translations for\napplications like real-time sign language translation and enhanced subtitle\nsynchronization. While its primary focus is on promoting inclusivity in the\nGreek tourism sector, its adaptability extends to education, healthcare, and\npublic services. Future advancements will enhance word-level precision and\nscalability to additional languages, supported by advanced AI methodologies and\ncollaborations with diverse stakeholders. This dataset underscores the\ntransformative potential of multimodal resources in bridging communication\ngaps, fostering innovation, and setting a benchmark for ethical AI and\ninclusive technologies.", "published": "2025-01-09 13:06:47", "link": "http://arxiv.org/abs/2501.05213v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CallNavi: A Study and Challenge on Function Calling Routing and\n  Invocation in Large Language Models", "abstract": "Interacting with a software system via a chatbot can be challenging,\nespecially when the chatbot needs to generate API calls, in the right order and\nwith the right parameters, to communicate with the system. API calling in\nchatbot systems poses significant challenges, particularly in complex,\nmulti-step tasks requiring accurate API selection and execution. We contribute\nto this domain in three ways: first, by introducing a novel dataset designed to\nassess models on API function selection, parameter generation, and nested API\ncalls; second, by benchmarking state-of-the-art language models across varying\nlevels of complexity to evaluate their performance in API function generation\nand parameter accuracy; and third, by proposing an enhanced API routing method\nthat combines general-purpose large language models for API selection with\nfine-tuned models for parameter generation and some prompt engineering\napproach. These approaches lead to substantial improvements in handling complex\nAPI tasks, offering practical advancements for real-world API-driven chatbot\nsystems.", "published": "2025-01-09 14:12:43", "link": "http://arxiv.org/abs/2501.05255v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "FairCoder: Evaluating Social Bias of LLMs in Code Generation", "abstract": "Large language models (LLMs) have been widely deployed in coding tasks,\ndrawing increasing attention to the evaluation of the quality and safety of\nLLMs' outputs. However, research on bias in code generation remains limited.\nExisting studies typically identify bias by applying malicious prompts or\nreusing tasks and dataset originally designed for discriminative models. Given\nthat prior datasets are not fully optimized for code-related tasks, there is a\npressing need for benchmarks specifically designed for evaluating code models.\nIn this study, we introduce FairCoder, a novel benchmark for evaluating social\nbias in code generation. FairCoder explores the bias issue following the\npipeline in software development, from function implementation to unit test,\nwith diverse real-world scenarios. Additionally, three metrics are designed to\nassess fairness performance on this benchmark. We conduct experiments on widely\nused LLMs and provide a comprehensive analysis of the results. The findings\nreveal that all tested LLMs exhibit social bias.", "published": "2025-01-09 17:42:23", "link": "http://arxiv.org/abs/2501.05396v2", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "A survey of textual cyber abuse detection using cutting-edge language\n  models and large language models", "abstract": "The success of social media platforms has facilitated the emergence of\nvarious forms of online abuse within digital communities. This abuse manifests\nin multiple ways, including hate speech, cyberbullying, emotional abuse,\ngrooming, and sexting. In this paper, we present a comprehensive analysis of\nthe different forms of abuse prevalent in social media, with a particular focus\non how emerging technologies, such as Language Models (LMs) and Large Language\nModels (LLMs), are reshaping both the detection and generation of abusive\ncontent within these networks. We delve into the mechanisms through which\nsocial media abuse is perpetuated, exploring the psychological and social\nimpact. Additionally, we examine the dual role of advanced language\nmodels-highlighting their potential to enhance automated detection systems for\nabusive behavior while also acknowledging their capacity to generate harmful\ncontent. This paper aims to contribute to the ongoing discourse on online\nsafety and ethics, offering insights into the evolving landscape of cyberabuse\nand the technological innovations that both mitigate and exacerbate it.", "published": "2025-01-09 18:55:50", "link": "http://arxiv.org/abs/2501.05443v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ReFocus: Visual Editing as a Chain of Thought for Structured Image\n  Understanding", "abstract": "Structured image understanding, such as interpreting tables and charts,\nrequires strategically refocusing across various structures and texts within an\nimage, forming a reasoning sequence to arrive at the final answer. However,\ncurrent multimodal large language models (LLMs) lack this multihop selective\nattention capability. In this work, we introduce ReFocus, a simple yet\neffective framework that equips multimodal LLMs with the ability to generate\n\"visual thoughts\" by performing visual editing on the input image through code,\nshifting and refining their visual focuses. Specifically, ReFocus enables\nmultimodal LLMs to generate Python codes to call tools and modify the input\nimage, sequentially drawing boxes, highlighting sections, and masking out\nareas, thereby enhancing the visual reasoning process. We experiment upon a\nwide range of structured image understanding tasks involving tables and charts.\nReFocus largely improves performance on all tasks over GPT-4o without visual\nediting, yielding an average gain of 11.0% on table tasks and 6.8% on chart\ntasks. We present an in-depth analysis of the effects of different visual\nedits, and reasons why ReFocus can improve the performance without introducing\nadditional information. Further, we collect a 14k training set using ReFocus,\nand prove that such visual chain-of-thought with intermediate information\noffers a better supervision than standard VQA data, reaching a 8.0% average\ngain over the same model trained with QA pairs and 2.6% over CoT.", "published": "2025-01-09 18:59:58", "link": "http://arxiv.org/abs/2501.05452v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "The more polypersonal the better -- a short look on space geometry of\n  fine-tuned layers", "abstract": "The interpretation of deep learning models is a rapidly growing field, with\nparticular interest in language models. There are various approaches to this\ntask, including training simpler models to replicate neural network predictions\nand analyzing the latent space of the model. The latter method allows us to not\nonly identify patterns in the model's decision-making process, but also\nunderstand the features of its internal structure. In this paper, we analyze\nthe changes in the internal representation of the BERT model when it is trained\nwith additional grammatical modules and data containing new grammatical\nstructures (polypersonality). We find that adding a single grammatical layer\ncauses the model to separate the new and old grammatical systems within itself,\nimproving the overall performance on perplexity metrics.", "published": "2025-01-09 18:50:47", "link": "http://arxiv.org/abs/2501.05503v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The dynamics of meaning through time: Assessment of Large Language\n  Models", "abstract": "Understanding how large language models (LLMs) grasp the historical context\nof concepts and their semantic evolution is essential in advancing artificial\nintelligence and linguistic studies. This study aims to evaluate the\ncapabilities of various LLMs in capturing temporal dynamics of meaning,\nspecifically how they interpret terms across different time periods. We analyze\na diverse set of terms from multiple domains, using tailored prompts and\nmeasuring responses through both objective metrics (e.g., perplexity and word\ncount) and subjective human expert evaluations. Our comparative analysis\nincludes prominent models like ChatGPT, GPT-4, Claude, Bard, Gemini, and Llama.\nFindings reveal marked differences in each model's handling of historical\ncontext and semantic shifts, highlighting both strengths and limitations in\ntemporal semantic understanding. These insights offer a foundation for refining\nLLMs to better address the evolving nature of language, with implications for\nhistorical text analysis, AI design, and applications in digital humanities.", "published": "2025-01-09 19:56:44", "link": "http://arxiv.org/abs/2501.05552v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMQuoter: Enhancing RAG Capabilities Through Efficient Quote Extraction\n  From Large Contexts", "abstract": "We introduce LLMQuoter, a lightweight, distillation-based model designed to\nenhance Retrieval Augmented Generation (RAG) by extracting the most relevant\ntextual evidence for downstream reasoning tasks. Built on the LLaMA-3B\narchitecture and fine-tuned with Low-Rank Adaptation (LoRA) on a 15,000-sample\nsubset of HotpotQA, LLMQuoter adopts a \"quote-first-then-answer\" strategy,\nefficiently identifying key quotes before passing curated snippets to reasoning\nmodels. This workflow reduces cognitive overhead and outperforms full-context\napproaches like Retrieval-Augmented Fine-Tuning (RAFT), achieving over 20-point\naccuracy gains across both small and large language models. By leveraging\nknowledge distillation from a high-performing teacher model, LLMQuoter achieves\ncompetitive results in a resource-efficient fine-tuning setup. It democratizes\nadvanced RAG capabilities, delivering significant performance improvements\nwithout requiring extensive model retraining. Our results highlight the\npotential of distilled quote-based reasoning to streamline complex workflows,\noffering a scalable and practical solution for researchers and practitioners\nalike.", "published": "2025-01-09 20:01:15", "link": "http://arxiv.org/abs/2501.05554v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Harmonizing Metadata of Language Resources for Enhanced Querying and\n  Accessibility", "abstract": "This paper addresses the harmonization of metadata from diverse repositories\nof language resources (LRs). Leveraging linked data and RDF techniques, we\nintegrate data from multiple sources into a unified model based on DCAT and\nMETA-SHARE OWL ontology. Our methodology supports text-based search, faceted\nbrowsing, and advanced SPARQL queries through Linghub, a newly developed\nportal. Real user queries from the Corpora Mailing List (CML) were evaluated to\nassess Linghub capability to satisfy actual user needs. Results indicate that\nwhile some limitations persist, many user requests can be successfully\naddressed. The study highlights significant metadata issues and advocates for\nadherence to open vocabularies and standards to enhance metadata harmonization.\nThis initial research underscores the importance of API-based access to LRs,\npromoting machine usability and data subset extraction for specific purposes,\npaving the way for more efficient and standardized LR utilization.", "published": "2025-01-09 22:48:43", "link": "http://arxiv.org/abs/2501.05606v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "JELLY: Joint Emotion Recognition and Context Reasoning with LLMs for\n  Conversational Speech Synthesis", "abstract": "Recently, there has been a growing demand for conversational speech synthesis\n(CSS) that generates more natural speech by considering the conversational\ncontext. To address this, we introduce JELLY, a novel CSS framework that\nintegrates emotion recognition and context reasoning for generating appropriate\nspeech in conversation by fine-tuning a large language model (LLM) with\nmultiple partial LoRA modules. We propose an Emotion-aware Q-former encoder,\nwhich enables the LLM to perceive emotions in speech. The encoder is trained to\nalign speech emotions with text, utilizing datasets of emotional speech. The\nentire model is then fine-tuned with conversational speech data to infer\nemotional context for generating emotionally appropriate speech in\nconversation. Our experimental results demonstrate that JELLY excels in\nemotional context modeling, synthesizing speech that naturally aligns with\nconversation, while mitigating the scarcity of emotional conversational speech\ndatasets.", "published": "2025-01-09 01:32:44", "link": "http://arxiv.org/abs/2501.04904v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "FLowHigh: Towards Efficient and High-Quality Audio Super-Resolution with\n  Single-Step Flow Matching", "abstract": "Audio super-resolution is challenging owing to its ill-posed nature.\nRecently, the application of diffusion models in audio super-resolution has\nshown promising results in alleviating this challenge. However, diffusion-based\nmodels have limitations, primarily the necessity for numerous sampling steps,\nwhich causes significantly increased latency when synthesizing high-quality\naudio samples. In this paper, we propose FLowHigh, a novel approach that\nintegrates flow matching, a highly efficient generative model, into audio\nsuper-resolution. We also explore probability paths specially tailored for\naudio super-resolution, which effectively capture high-resolution audio\ndistributions, thereby enhancing reconstruction quality. The proposed method\ngenerates high-fidelity, high-resolution audio through a single-step sampling\nprocess across various input sampling rates. The experimental results on the\nVCTK benchmark dataset demonstrate that FLowHigh achieves state-of-the-art\nperformance in audio super-resolution, as evaluated by log-spectral distance\nand ViSQOL while maintaining computational efficiency with only a single-step\nsampling process.", "published": "2025-01-09 02:30:26", "link": "http://arxiv.org/abs/2501.04926v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Jailbreaking Multimodal Large Language Models via Shuffle Inconsistency", "abstract": "Multimodal Large Language Models (MLLMs) have achieved impressive performance\nand have been put into practical use in commercial applications, but they still\nhave potential safety mechanism vulnerabilities. Jailbreak attacks are red\nteaming methods that aim to bypass safety mechanisms and discover MLLMs'\npotential risks. Existing MLLMs' jailbreak methods often bypass the model's\nsafety mechanism through complex optimization methods or carefully designed\nimage and text prompts. Despite achieving some progress, they have a low attack\nsuccess rate on commercial closed-source MLLMs. Unlike previous research, we\nempirically find that there exists a Shuffle Inconsistency between MLLMs'\ncomprehension ability and safety ability for the shuffled harmful instruction.\nThat is, from the perspective of comprehension ability, MLLMs can understand\nthe shuffled harmful text-image instructions well. However, they can be easily\nbypassed by the shuffled harmful instructions from the perspective of safety\nability, leading to harmful responses. Then we innovatively propose a\ntext-image jailbreak attack named SI-Attack. Specifically, to fully utilize the\nShuffle Inconsistency and overcome the shuffle randomness, we apply a\nquery-based black-box optimization method to select the most harmful shuffled\ninputs based on the feedback of the toxic judge model. A series of experiments\nshow that SI-Attack can improve the attack's performance on three benchmarks.\nIn particular, SI-Attack can obviously improve the attack success rate for\ncommercial MLLMs such as GPT-4o or Claude-3.5-Sonnet.", "published": "2025-01-09 02:47:01", "link": "http://arxiv.org/abs/2501.04931v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Demystifying Domain-adaptive Post-training for Financial LLMs", "abstract": "Domain-adaptive post-training of large language models (LLMs) has emerged as\na promising approach for specialized domains such as medicine and finance.\nHowever, significant challenges remain in identifying optimal adaptation\ncriteria and training strategies across varying data and model configurations.\nTo address these challenges, we introduce FINDAP, a systematic and fine-grained\ninvestigation into domain adaptive post-training of LLMs for the finance\ndomain. Our approach consists of four key components: FinCap, which defines the\ncore capabilities required for the target domain; FinRec, an effective training\nrecipe that jointly optimizes continual pre-training and instruction-following,\nalong with a novel preference data distillation method leveraging process\nsignals from a generative reward model; FinTrain, a curated set of training\ndatasets supporting FinRec; and FinEval, a comprehensive evaluation suite\naligned with FinCap. The resulting model, Llama-Fin, achieves state-of-the-art\nperformance across a wide range of financial tasks. Our analysis also\nhighlights how each post-training stage contributes to distinct capabilities,\nuncovering specific challenges and effective solutions, providing valuable\ninsights for domain adaptation of LLMs.", "published": "2025-01-09 04:26:15", "link": "http://arxiv.org/abs/2501.04961v2", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VoxEval: Benchmarking the Knowledge Understanding Capabilities of\n  End-to-End Spoken Language Models", "abstract": "With the rising need for speech-based interaction models, end-to-end Spoken\nLanguage Models (SLMs) have emerged as a promising solution. While these models\nrequire comprehensive world knowledge for meaningful and reliable human\ninteractions, existing question-answering (QA) benchmarks fall short in\nevaluating SLMs' knowledge understanding due to their inability to support\nend-to-end speech evaluation and account for varied input audio conditions. To\naddress these limitations, we present VoxEval, a novel SpeechQA benchmark that\nassesses SLMs' knowledge understanding through pure speech interactions. Our\nbenchmark 1) uniquely maintains speech format for both inputs and outputs, 2)\nevaluates model robustness across diverse input audio conditions, and 3)\npioneers the assessment of complex tasks like mathematical reasoning in spoken\nformat. Systematic evaluation demonstrates that VoxEval presents significant\nchallenges to current SLMs, revealing their sensitivity to varying audio\nconditions and highlighting the need to enhance reasoning capabilities in\nfuture development. We hope this benchmark could guide the advancement of more\nsophisticated and reliable SLMs.\\footnote{VoxEval dataset is available at:\nhttps://github.com/dreamtheater123/VoxEval", "published": "2025-01-09 04:30:12", "link": "http://arxiv.org/abs/2501.04962v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Comparison of Feature Learning Methods for Metadata Extraction from PDF\n  Scholarly Documents", "abstract": "The availability of metadata for scientific documents is pivotal in\npropelling scientific knowledge forward and for adhering to the FAIR principles\n(i.e. Findability, Accessibility, Interoperability, and Reusability) of\nresearch findings. However, the lack of sufficient metadata in published\ndocuments, particularly those from smaller and mid-sized publishers, hinders\ntheir accessibility. This issue is widespread in some disciplines, such as the\nGerman Social Sciences, where publications often employ diverse templates. To\naddress this challenge, our study evaluates various feature learning and\nprediction methods, including natural language processing (NLP), computer\nvision (CV), and multimodal approaches, for extracting metadata from documents\nwith high template variance. We aim to improve the accessibility of scientific\ndocuments and facilitate their wider use. To support our comparison of these\nmethods, we provide comprehensive experimental results, analyzing their\naccuracy and efficiency in extracting metadata. Additionally, we provide\nvaluable insights into the strengths and weaknesses of various feature learning\nand prediction methods, which can guide future research in this field.", "published": "2025-01-09 09:03:43", "link": "http://arxiv.org/abs/2501.05082v1", "categories": ["cs.IR", "cs.CL", "cs.DL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "A Novel Approach to Scalable and Automatic Topic-Controlled Question\n  Generation in Education", "abstract": "The development of Automatic Question Generation (QG) models has the\npotential to significantly improve educational practices by reducing the\nteacher workload associated with creating educational content. This paper\nintroduces a novel approach to educational question generation that controls\nthe topical focus of questions. The proposed Topic-Controlled Question\nGeneration (T-CQG) method enhances the relevance and effectiveness of the\ngenerated content for educational purposes. Our approach uses fine-tuning on a\npre-trained T5-small model, employing specially created datasets tailored to\neducational needs. The research further explores the impacts of pre-training\nstrategies, quantisation, and data augmentation on the model's performance. We\nspecifically address the challenge of generating semantically aligned questions\nwith paragraph-level contexts, thereby improving the topic specificity of the\ngenerated questions. In addition, we introduce and explore novel evaluation\nmethods to assess the topical relatedness of the generated questions. Our\nresults, validated through rigorous offline and human-backed evaluations,\ndemonstrate that the proposed models effectively generate high-quality,\ntopic-focused questions. These models have the potential to reduce teacher\nworkload and support personalised tutoring systems by serving as bespoke\nquestion generators. With its relatively small number of parameters, the\nproposals not only advance the capabilities of question generation models for\nhandling specific educational topics but also offer a scalable solution that\nreduces infrastructure costs. This scalability makes them feasible for\nwidespread use in education without reliance on proprietary large language\nmodels like ChatGPT.", "published": "2025-01-09 13:13:24", "link": "http://arxiv.org/abs/2501.05220v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.IR", "H.3.3; J.1; I.2.0"], "primary_category": "cs.CY"}
{"title": "Optimizing Estonian TV Subtitles with Semi-supervised Learning and LLMs", "abstract": "This paper presents an approach for generating high-quality, same-language\nsubtitles for Estonian TV content. We fine-tune the Whisper model on\nhuman-generated Estonian subtitles and enhance it with iterative\npseudo-labeling and large language model (LLM) based post-editing. Our\nexperiments demonstrate notable subtitle quality improvement through\npseudo-labeling with an unlabeled dataset. We find that applying LLM-based\nediting at test time enhances subtitle accuracy, while its use during training\ndoes not yield further gains. This approach holds promise for creating subtitle\nquality close to human standard and could be extended to real-time\napplications.", "published": "2025-01-09 13:41:37", "link": "http://arxiv.org/abs/2501.05234v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Enhancing Plagiarism Detection in Marathi with a Weighted Ensemble of\n  TF-IDF and BERT Embeddings for Low-Resource Language Processing", "abstract": "Plagiarism involves using another person's work or concepts without proper\nattribution, presenting them as original creations. With the growing amount of\ndata communicated in regional languages such as Marathi -- one of India's\nregional languages -- it is crucial to design robust plagiarism detection\nsystems tailored for low-resource languages. Language models like Bidirectional\nEncoder Representations from Transformers (BERT) have demonstrated exceptional\ncapability in text representation and feature extraction, making them essential\ntools for semantic analysis and plagiarism detection. However, the application\nof BERT for low-resource languages remains under-explored, particularly in the\ncontext of plagiarism detection. This paper presents a method to enhance the\naccuracy of plagiarism detection for Marathi texts using BERT sentence\nembeddings in conjunction with Term Frequency-Inverse Document Frequency\n(TF-IDF) feature representation. This approach effectively captures\nstatistical, semantic, and syntactic aspects of text features through a\nweighted voting ensemble of machine learning models.", "published": "2025-01-09 14:14:18", "link": "http://arxiv.org/abs/2501.05260v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; H.3.3"], "primary_category": "cs.CL"}
{"title": "Stream Aligner: Efficient Sentence-Level Alignment via Distribution\n  Induction", "abstract": "The rapid advancement of large language models (LLMs) has led to significant\nimprovements in their capabilities, but also to increased concerns about their\nalignment with human values and intentions. Current alignment strategies,\nincluding adaptive training and inference-time methods, have demonstrated\npotential in this area. However, these approaches still struggle to balance\ndeployment complexity and capability across various tasks and difficulties. In\nthis work, we introduce the Streaming Distribution Induce Aligner (Stream\nAligner), a novel alignment paradigm that combines efficiency with enhanced\nperformance in various tasks throughout the generation process. Stream Aligner\nachieves dynamic sentence-level correction by using a small model to learn the\npreferences of the suffix sentence, iteratively correcting the suffix sentence\noutput by the upstream model, and then using the corrected sentence to replace\nthe suffix sentence in subsequent generations. Compared to Aligner, our\nexperiments demonstrate that Stream Aligner reduces reliance on the\ncapabilities of additional models, enhances the reasoning abilities of LLMs,\nand decreases latency during user interaction. Specifically, Stream Aligner-2B\nmodel has achieved an improvement of 76.1% in helpfulness, 36.0% in\nharmlessness on the tested Llama2-70B-chat model, and Stream Aligner-8B has\nachieved an improvement of 3.5% on the math ability of the tested\nLlama3-70B-Instruct model.", "published": "2025-01-09 16:02:51", "link": "http://arxiv.org/abs/2501.05336v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Search-o1: Agentic Search-Enhanced Large Reasoning Models", "abstract": "Large reasoning models (LRMs) like OpenAI-o1 have demonstrated impressive\nlong stepwise reasoning capabilities through large-scale reinforcement\nlearning. However, their extended reasoning processes often suffer from\nknowledge insufficiency, leading to frequent uncertainties and potential\nerrors. To address this limitation, we introduce \\textbf{Search-o1}, a\nframework that enhances LRMs with an agentic retrieval-augmented generation\n(RAG) mechanism and a Reason-in-Documents module for refining retrieved\ndocuments. Search-o1 integrates an agentic search workflow into the reasoning\nprocess, enabling dynamic retrieval of external knowledge when LRMs encounter\nuncertain knowledge points. Additionally, due to the verbose nature of\nretrieved documents, we design a separate Reason-in-Documents module to deeply\nanalyze the retrieved information before injecting it into the reasoning chain,\nminimizing noise and preserving coherent reasoning flow. Extensive experiments\non complex reasoning tasks in science, mathematics, and coding, as well as six\nopen-domain QA benchmarks, demonstrate the strong performance of Search-o1.\nThis approach enhances the trustworthiness and applicability of LRMs in complex\nreasoning tasks, paving the way for more reliable and versatile intelligent\nsystems. The code is available at\n\\url{https://github.com/sunnynexus/Search-o1}.", "published": "2025-01-09 16:48:17", "link": "http://arxiv.org/abs/2501.05366v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "LSEBMCL: A Latent Space Energy-Based Model for Continual Learning", "abstract": "Continual learning has become essential in many practical applications such\nas online news summaries and product classification. The primary challenge is\nknown as catastrophic forgetting, a phenomenon where a model inadvertently\ndiscards previously learned knowledge when it is trained on new tasks. Existing\nsolutions involve storing exemplars from previous classes, regularizing\nparameters during the fine-tuning process, or assigning different model\nparameters to each task. The proposed solution LSEBMCL (Latent Space\nEnergy-Based Model for Continual Learning) in this work is to use energy-based\nmodels (EBMs) to prevent catastrophic forgetting by sampling data points from\nprevious tasks when training on new ones. The EBM is a machine learning model\nthat associates an energy value with each input data point. The proposed method\nuses an EBM layer as an outer-generator in the continual learning framework for\nNLP tasks. The study demonstrates the efficacy of EBM in NLP tasks, achieving\nstate-of-the-art results in all experiments.", "published": "2025-01-09 15:47:30", "link": "http://arxiv.org/abs/2501.05495v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Spatial Information Integration in Small Language Models for Document\n  Layout Generation and Classification", "abstract": "Document layout understanding is a field of study that analyzes the spatial\narrangement of information in a document hoping to understand its structure and\nlayout. Models such as LayoutLM (and its subsequent iterations) can understand\nsemi-structured documents with SotA results; however, the lack of open\nsemi-structured data is a limitation in itself. While semi-structured data is\ncommon in everyday life (balance sheets, purchase orders, receipts), there is a\nlack of public datasets for training machine learning models for this type of\ndocument. In this investigation we propose a method to generate new, synthetic,\nlayout information that can help overcoming this data shortage. According to\nour results, the proposed method performs better than LayoutTransformer,\nanother popular layout generation method. We also show that, in some scenarios,\ntext classification can improve when supported by bounding box information.", "published": "2025-01-09 17:20:00", "link": "http://arxiv.org/abs/2501.05497v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Transformer-Squared: Self-adaptive LLMs", "abstract": "Self-adaptive large language models (LLMs) aim to solve the challenges posed\nby traditional fine-tuning methods, which are often computationally intensive\nand static in their ability to handle diverse tasks. We introduce\nTransformer-Squared, a novel self-adaptation framework that adapts LLMs for\nunseen tasks in real-time by selectively adjusting only the singular components\nof their weight matrices. During inference, Transformer-Squared employs a\ntwo-pass mechanism: first, a dispatch system identifies the task properties,\nand then task-specific 'expert' vectors, trained using reinforcement learning,\nare dynamically mixed to obtain targeted behavior for the incoming prompt. Our\nmethod consistently outperforms ubiquitous approaches such as LoRA, with fewer\nparameters and greater efficiency. Furthermore, Transformer-Squared\ndemonstrates versatility across different LLM architectures and modalities,\nincluding vision-language tasks. Transformer-Squared represents a significant\nleap forward, offering a scalable, efficient solution for enhancing the\nadaptability and task-specific performance of LLMs, paving the way for truly\ndynamic, self-organizing AI systems.", "published": "2025-01-09 01:19:21", "link": "http://arxiv.org/abs/2501.06252v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Rethinking Evaluation of Sparse Autoencoders through the Representation\n  of Polysemous Words", "abstract": "Sparse autoencoders (SAEs) have gained a lot of attention as a promising tool\nto improve the interpretability of large language models (LLMs) by mapping the\ncomplex superposition of polysemantic neurons into monosemantic features and\ncomposing a sparse dictionary of words. However, traditional performance\nmetrics like Mean Squared Error and L0 sparsity ignore the evaluation of the\nsemantic representational power of SAEs -- whether they can acquire\ninterpretable monosemantic features while preserving the semantic relationship\nof words. For instance, it is not obvious whether a learned sparse feature\ncould distinguish different meanings in one word. In this paper, we propose a\nsuite of evaluations for SAEs to analyze the quality of monosemantic features\nby focusing on polysemous words. Our findings reveal that SAEs developed to\nimprove the MSE-L0 Pareto frontier may confuse interpretability, which does not\nnecessarily enhance the extraction of monosemantic features. The analysis of\nSAEs with polysemous words can also figure out the internal mechanism of LLMs;\ndeeper layers and the Attention module contribute to distinguishing polysemy in\na word. Our semantics focused evaluation offers new insights into the polysemy\nand the existing SAE objective and contributes to the development of more\npractical SAEs.", "published": "2025-01-09 02:54:19", "link": "http://arxiv.org/abs/2501.06254v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What Matters for In-Context Learning: A Balancing Act of Look-up and\n  In-Weight Learning", "abstract": "Large Language Models (LLMs) have demonstrated impressive performance in\nvarious tasks, including In-Context Learning (ICL), where the model performs\nnew tasks by conditioning solely on the examples provided in the context,\nwithout updating the model's weights. While prior research has explored the\nroles of pretraining data and model architecture, the key mechanism behind ICL\nremains unclear. In this work, we systematically uncover properties present in\nLLMs that support the emergence of ICL. To disambiguate these factors, we\nconduct a study with a controlled dataset and data sequences using a deep\nautoregressive model. We show that conceptual repetitions in the data sequences\nare crucial for ICL, more so than previously indicated training data properties\nlike burstiness or long-tail distribution. Conceptual repetitions could refer\nto $n$-gram repetitions in textual data or exact image copies in image sequence\ndata. Such repetitions also offer other previously overlooked benefits such as\nreduced transiency in ICL performance. Furthermore, we show that the emergence\nof ICL depends on balancing the in-weight learning objective with the\nin-context solving ability during training.", "published": "2025-01-09 09:45:05", "link": "http://arxiv.org/abs/2501.06256v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large language models streamline automated systematic review: A\n  preliminary study", "abstract": "Large Language Models (LLMs) have shown promise in natural language\nprocessing tasks, with the potential to automate systematic reviews. This study\nevaluates the performance of three state-of-the-art LLMs in conducting\nsystematic review tasks. We assessed GPT-4, Claude-3, and Mistral 8x7B across\nfour systematic review tasks: study design formulation, search strategy\ndevelopment, literature screening, and data extraction. Sourced from a\npreviously published systematic review, we provided reference standard\nincluding standard PICO (Population, Intervention, Comparison, Outcome) design,\nstandard eligibility criteria, and data from 20 reference literature. Three\ninvestigators evaluated the quality of study design and eligibility criteria\nusing 5-point Liker Scale in terms of accuracy, integrity, relevance,\nconsistency and overall performance. For other tasks, the output is defined as\naccurate if it is the same as the reference standard. Search strategy\nperformance was evaluated through accuracy and retrieval efficacy. Screening\naccuracy was assessed for both abstracts screening and full texts screening.\nData extraction accuracy was evaluated across 1,120 data points comprising\n3,360 individual fields. Claude-3 demonstrated superior overall performance in\nPICO design. In search strategy formulation, GPT-4 and Claude-3 achieved\ncomparable accuracy, outperforming Mistral. For abstract screening, GPT-4\nachieved the highest accuracy, followed by Mistral and Claude-3. In data\nextraction, GPT-4 significantly outperformed other models. LLMs demonstrate\npotential for automating systematic review tasks, with GPT-4 showing superior\nperformance in search strategy formulation, literature screening and data\nextraction. These capabilities make them promising assistive tools for\nresearchers and warrant further development and validation in this field.", "published": "2025-01-09 01:59:35", "link": "http://arxiv.org/abs/2502.15702v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Bringing Order Amidst Chaos: On the Role of Artificial Intelligence in\n  Secure Software Engineering", "abstract": "Context. Developing secure and reliable software remains a key challenge in\nsoftware engineering (SE). The ever-evolving technological landscape offers\nboth opportunities and threats, creating a dynamic space where chaos and order\ncompete. Secure software engineering (SSE) must continuously address\nvulnerabilities that endanger software systems and carry broader socio-economic\nrisks, such as compromising critical national infrastructure and causing\nsignificant financial losses. Researchers and practitioners have explored\nmethodologies like Static Application Security Testing Tools (SASTTs) and\nartificial intelligence (AI) approaches, including machine learning (ML) and\nlarge language models (LLMs), to detect and mitigate these vulnerabilities.\nEach method has unique strengths and limitations.\n  Aim. This thesis seeks to bring order to the chaos in SSE by addressing\ndomain-specific differences that impact AI accuracy.\n  Methodology. The research employs a mix of empirical strategies, such as\nevaluating effort-aware metrics, analyzing SASTTs, conducting method-level\nanalysis, and leveraging evidence-based techniques like systematic dataset\nreviews. These approaches help characterize vulnerability prediction datasets.\n  Results. Key findings include limitations in static analysis tools for\nidentifying vulnerabilities, gaps in SASTT coverage of vulnerability types,\nweak relationships among vulnerability severity scores, improved defect\nprediction accuracy using just-in-time modeling, and threats posed by untouched\nmethods.\n  Conclusions. This thesis highlights the complexity of SSE and the importance\nof contextual knowledge in improving AI-driven vulnerability and defect\nprediction. The comprehensive analysis advances effective prediction models,\nbenefiting both researchers and practitioners.", "published": "2025-01-09 11:38:58", "link": "http://arxiv.org/abs/2501.05165v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CR", "cs.ET"], "primary_category": "cs.SE"}
{"title": "Vision Graph Non-Contrastive Learning for Audio Deepfake Detection with\n  Limited Labels", "abstract": "Recent advancements in audio deepfake detection have leveraged graph neural\nnetworks (GNNs) to model frequency and temporal interdependencies in audio\ndata, effectively identifying deepfake artifacts. However, the reliance of\nGNN-based methods on substantial labeled data for graph construction and robust\nperformance limits their applicability in scenarios with limited labeled data.\nAlthough vast amounts of audio data exist, the process of labeling samples as\ngenuine or fake remains labor-intensive and costly. To address this challenge,\nwe propose SIGNL (Spatio-temporal vIsion Graph Non-contrastive Learning), a\nnovel framework that maintains high GNN performance in low-label settings.\nSIGNL constructs spatio-temporal graphs by representing patches from the\naudio's visual spectrogram as nodes. These graph structures are modeled using\nvision graph convolutional (GC) encoders pre-trained through graph\nnon-contrastive learning, a label-free that maximizes the similarity between\npositive pairs. The pre-trained encoders are then fine-tuned for audio deepfake\ndetection, reducing reliance on labeled data. Experiments demonstrate that\nSIGNL outperforms state-of-the-art baselines across multiple audio deepfake\ndetection datasets, achieving the lowest Equal Error Rate (EER) with as little\nas 5% labeled data. Additionally, SIGNL exhibits strong cross-domain\ngeneralization, achieving the lowest EER in evaluations involving diverse\nattack types and languages in the In-The-Wild dataset.", "published": "2025-01-09 03:18:27", "link": "http://arxiv.org/abs/2501.04942v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Music Tagging with Classifier Group Chains", "abstract": "We propose music tagging with classifier chains that model the interplay of\nmusic tags. Most conventional methods estimate multiple tags independently by\ntreating them as multiple independent binary classification problems. This\ntreatment overlooks the conditional dependencies among music tags, leading to\nsuboptimal tagging performance. Unlike most music taggers, the proposed method\nsequentially estimates each tag based on the idea of the classifier chains.\nBeyond the naive classifier chains, the proposed method groups the multiple\ntags by category, such as genre, and performs chains by unit of groups, which\nwe call \\textit{classifier group chains}. Our method allows the modeling of the\ndependence between tag groups. We evaluate the effectiveness of the proposed\nmethod for music tagging performance through music tagging experiments using\nthe MTG-Jamendo dataset. Furthermore, we investigate the effective order of\nchains for music tagging.", "published": "2025-01-09 08:17:07", "link": "http://arxiv.org/abs/2501.05050v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DiffAttack: Diffusion-based Timbre-reserved Adversarial Attack in\n  Speaker Identification", "abstract": "Being a form of biometric identification, the security of the speaker\nidentification (SID) system is of utmost importance. To better understand the\nrobustness of SID systems, we aim to perform more realistic attacks in SID,\nwhich are challenging for both humans and machines to detect. In this study, we\npropose DiffAttack, a novel timbre-reserved adversarial attack approach that\nexploits the capability of a diffusion-based voice conversion (DiffVC) model to\ngenerate adversarial fake audio with distinct target speaker attribution. By\nintroducing adversarial constraints into the generative process of the\ndiffusion-based voice conversion model, we craft fake samples that effectively\nmislead target models while preserving speaker-wise characteristics.\nSpecifically, inspired by the use of randomly sampled Gaussian noise in\nconventional adversarial attacks and diffusion processes, we incorporate\nadversarial constraints into the reverse diffusion process. These constraints\nsubtly guide the reverse diffusion process toward aligning with the target\nspeaker distribution. Our experiments on the LibriTTS dataset indicate that\nDiffAttack significantly improves the attack success rate compared to vanilla\nDiffVC and other methods. Moreover, objective and subjective evaluations\ndemonstrate that introducing adversarial constraints does not compromise the\nspeech quality generated by the DiffVC model.", "published": "2025-01-09 10:30:58", "link": "http://arxiv.org/abs/2501.05127v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ZipEnhancer: Dual-Path Down-Up Sampling-based Zipformer for Monaural\n  Speech Enhancement", "abstract": "In contrast to other sequence tasks modeling hidden layer features with three\naxes, Dual-Path time and time-frequency domain speech enhancement models are\neffective and have low parameters but are computationally demanding due to\ntheir hidden layer features with four axes. We propose ZipEnhancer, which is\nDual-Path Down-Up Sampling-based Zipformer for Monaural Speech Enhancement,\nincorporating time and frequency domain Down-Up sampling to reduce\ncomputational costs. We introduce the ZipformerBlock as the core block and\npropose the design of the Dual-Path DownSampleStacks that symmetrically scale\ndown and scale up. Also, we introduce the ScaleAdam optimizer and Eden learning\nrate scheduler to improve the performance further. Our model achieves new\nstate-of-the-art results on the DNS 2020 Challenge and Voicebank+DEMAND\ndatasets, with a perceptual evaluation of speech quality (PESQ) of 3.69 and\n3.63, using 2.04M parameters and 62.41G FLOPS, outperforming other methods with\nsimilar complexity levels.", "published": "2025-01-09 12:09:21", "link": "http://arxiv.org/abs/2501.05183v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Probing Speaker-specific Features in Speaker Representations", "abstract": "This study explores speaker-specific features encoded in speaker embeddings\nand intermediate layers of speech self-supervised learning (SSL) models. By\nutilising a probing method, we analyse features such as pitch, tempo, and\nenergy across prominent speaker embedding models and speech SSL models,\nincluding HuBERT, WavLM, and Wav2vec 2.0. The results reveal that speaker\nembeddings like CAM++ excel in energy classification, while speech SSL models\ndemonstrate superior performance across multiple features due to their\nhierarchical feature encoding. Intermediate layers effectively capture a mix of\nacoustic and para-linguistic information, with deeper layers refining these\nrepresentations. This investigation provides insights into model design and\nhighlights the potential of these representations for downstream applications,\nsuch as speaker verification and text-to-speech synthesis, while laying the\ngroundwork for exploring additional features and advanced probing methods.", "published": "2025-01-09 15:26:33", "link": "http://arxiv.org/abs/2501.05310v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unmasking Deepfakes: Leveraging Augmentations and Features Variability\n  for Deepfake Speech Detection", "abstract": "The detection of deepfake speech has become increasingly challenging with the\nrapid evolution of deepfake technologies. In this paper, we propose a hybrid\narchitecture for deepfake speech detection, combining a self-supervised\nlearning framework for feature extraction with a classifier head to form an\nend-to-end model. Our approach incorporates both audio-level and feature-level\naugmentation techniques. Specifically, we introduce and analyze various masking\nstrategies for augmenting raw audio spectrograms and for enhancing feature\nrepresentations during training. We incorporate compression augmentations\nduring the pretraining phase of the feature extractor to address the\nlimitations of small, single-language datasets. We evaluate the model on the\nASVSpoof5 (ASVSpoof 2024) challenge, achieving state-of-the-art results in\nTrack 1 under closed conditions with an Equal Error Rate of 4.37%. By employing\ndifferent pretrained feature extractors, the model achieves an enhanced EER of\n3.39%. Our model demonstrates robust performance against unseen deepfake\nattacks and exhibits strong generalization across different codecs.", "published": "2025-01-09 19:31:10", "link": "http://arxiv.org/abs/2501.05545v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Mel-Spectrogram Inversion via Alternating Direction Method of\n  Multipliers", "abstract": "Signal reconstruction from its mel-spectrogram is known as mel-spectrogram\ninversion and has many applications, including speech and foley sound\nsynthesis. In this paper, we propose a mel-spectrogram inversion method based\non a rigorous optimization algorithm. To reconstruct a time-domain signal with\ninverse short-time Fourier transform (STFT), both full-band STFT magnitude and\nphase should be predicted from a given mel-spectrogram. Their joint estimation\nhas outperformed the cascaded full-band magnitude prediction and phase\nreconstruction by preventing error accumulation. However, the existing joint\nestimation method requires many iterations, and there remains room for\nperformance improvement. We present an alternating direction method of\nmultipliers (ADMM)-based joint estimation method motivated by its success in\nvarious nonconvex optimization problems including phase reconstruction. An\nefficient update of each variable is derived by exploiting the conditional\nindependence among the variables. Our experiments demonstrate the effectiveness\nof the proposed method on speech and foley sounds.", "published": "2025-01-09 20:05:51", "link": "http://arxiv.org/abs/2501.05557v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FreeSVC: Towards Zero-shot Multilingual Singing Voice Conversion", "abstract": "This work presents FreeSVC, a promising multilingual singing voice conversion\napproach that leverages an enhanced VITS model with Speaker-invariant\nClustering (SPIN) for better content representation and the State-of-the-Art\n(SOTA) speaker encoder ECAPA2. FreeSVC incorporates trainable language\nembeddings to handle multiple languages and employs an advanced speaker encoder\nto disentangle speaker characteristics from linguistic content. Designed for\nzero-shot learning, FreeSVC enables cross-lingual singing voice conversion\nwithout extensive language-specific training. We demonstrate that a\nmultilingual content extractor is crucial for optimal cross-language\nconversion. Our source code and models are publicly available.", "published": "2025-01-09 21:39:09", "link": "http://arxiv.org/abs/2501.05586v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "D3RM: A Discrete Denoising Diffusion Refinement Model for Piano\n  Transcription", "abstract": "Diffusion models have been widely used in the generative domain due to their\nconvincing performance in modeling complex data distributions. Moreover, they\nhave shown competitive results on discriminative tasks, such as image\nsegmentation. While diffusion models have also been explored for automatic\nmusic transcription, their performance has yet to reach a competitive level. In\nthis paper, we focus on discrete diffusion model's refinement capabilities and\npresent a novel architecture for piano transcription. Our model utilizes\nNeighborhood Attention layers as the denoising module, gradually predicting the\ntarget high-resolution piano roll, conditioned on the finetuned features of a\npretrained acoustic model. To further enhance refinement, we devise a novel\nstrategy which applies distinct transition states during training and inference\nstage of discrete diffusion models. Experiments on the MAESTRO dataset show\nthat our approach outperforms previous diffusion-based piano transcription\nmodels and the baseline model in terms of F1 score. Our code is available in\nhttps://github.com/hanshounsu/d3rm.", "published": "2025-01-09 08:44:06", "link": "http://arxiv.org/abs/2501.05068v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AnCoGen: Analysis, Control and Generation of Speech with a Masked\n  Autoencoder", "abstract": "This article introduces AnCoGen, a novel method that leverages a masked\nautoencoder to unify the analysis, control, and generation of speech signals\nwithin a single model. AnCoGen can analyze speech by estimating key attributes,\nsuch as speaker identity, pitch, content, loudness, signal-to-noise ratio, and\nclarity index. In addition, it can generate speech from these attributes and\nallow precise control of the synthesized speech by modifying them. Extensive\nexperiments demonstrated the effectiveness of AnCoGen across speech\nanalysis-resynthesis, pitch estimation, pitch modification, and speech\nenhancement.", "published": "2025-01-09 15:58:37", "link": "http://arxiv.org/abs/2501.05332v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Seeing Sound: Assembling Sounds from Visuals for Audio-to-Image\n  Generation", "abstract": "Training audio-to-image generative models requires an abundance of diverse\naudio-visual pairs that are semantically aligned. Such data is almost always\ncurated from in-the-wild videos, given the cross-modal semantic correspondence\nthat is inherent to them. In this work, we hypothesize that insisting on the\nabsolute need for ground truth audio-visual correspondence, is not only\nunnecessary, but also leads to severe restrictions in scale, quality, and\ndiversity of the data, ultimately impairing its use in the modern generative\nmodels. That is, we propose a scalable image sonification framework where\ninstances from a variety of high-quality yet disjoint uni-modal origins can be\nartificially paired through a retrieval process that is empowered by reasoning\ncapabilities of modern vision-language models. To demonstrate the efficacy of\nthis approach, we use our sonified images to train an audio-to-image generative\nmodel that performs competitively against state-of-the-art. Finally, through a\nseries of ablation studies, we exhibit several intriguing auditory capabilities\nlike semantic mixing and interpolation, loudness calibration and acoustic space\nmodeling through reverberation that our model has implicitly developed to guide\nthe image generation process.", "published": "2025-01-09 18:13:57", "link": "http://arxiv.org/abs/2501.05413v1", "categories": ["cs.SD", "cs.CV", "cs.GR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Dynamic Neural Communication and Speech Neuroprosthesis Based on\n  Viseme Decoding", "abstract": "Decoding text, speech, or images from human neural signals holds promising\npotential both as neuroprosthesis for patients and as innovative communication\ntools for general users. Although neural signals contain various information on\nspeech intentions, movements, and phonetic details, generating informative\noutputs from them remains challenging, with mostly focusing on decoding short\nintentions or producing fragmented outputs. In this study, we developed a\ndiffusion model-based framework to decode visual speech intentions from\nspeech-related non-invasive brain signals, to facilitate face-to-face neural\ncommunication. We designed an experiment to consolidate various phonemes to\ntrain visemes of each phoneme, aiming to learn the representation of\ncorresponding lip formations from neural signals. By decoding visemes from both\nisolated trials and continuous sentences, we successfully reconstructed\ncoherent lip movements, effectively bridging the gap between brain signals and\ndynamic visual interfaces. The results highlight the potential of viseme\ndecoding and talking face reconstruction from human neural signals, marking a\nsignificant step toward dynamic neural communication systems and speech\nneuroprosthesis for patients.", "published": "2025-01-09 04:47:27", "link": "http://arxiv.org/abs/2501.14790v1", "categories": ["q-bio.NC", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "q-bio.NC"}
{"title": "Generalized Linear Models with 1-Bit Measurements: Asymptotics of the\n  Maximum Likelihood Estimator", "abstract": "This work establishes regularity conditions for consistency and asymptotic\nnormality of the multiple parameter maximum likelihood estimator(MLE) from\ncensored data, where the censoring mechanism is in the form of $1$-bit\nmeasurements. The underlying distribution of the uncensored data is assumed to\nbelong to the exponential family, with natural parameters expressed as a linear\ncombination of the predictors, known as generalized linear model (GLM). As part\nof the analysis, the Fisher information matrix is also derived for both\ncensored and uncensored data, which helps to quantify the impact of censoring\nand assess the performance of the MLE. The choice of GLM allows one to consider\na variety of practical examples where 1-bit estimation is of interest. In\nparticular, it is shown how the derived results can be used to analyze two\npractically relevant scenarios: the Gaussian model with both unknown mean and\nvariance, and the Poisson model with an unknown mean.", "published": "2025-01-09 03:01:57", "link": "http://arxiv.org/abs/2501.04937v2", "categories": ["math.ST", "cs.SY", "eess.AS", "eess.SP", "eess.SY", "stat.TH"], "primary_category": "math.ST"}
