{"title": "Generating Multilingual Parallel Corpus Using Subtitles", "abstract": "Neural Machine Translation with its significant results, still has a great\nproblem: lack or absence of parallel corpus for many languages. This article\nsuggests a method for generating considerable amount of parallel corpus for any\nlanguage pairs, extracted from open source materials existing on the Internet.\nParallel corpus contents will be derived from video subtitles. It needs a set\nof video titles, with some attributes like release date, rating, duration and\netc. Process of finding and downloading subtitle pairs for desired language\npairs is automated by using a crawler. Finally sentence pairs will be extracted\nfrom synchronous dialogues in subtitles. The main problem of this method is\nunsynchronized subtitle pairs. Therefore subtitles will be verified before\ndownloading. If two subtitle were not synchronized, then another subtitle of\nthat video will be processed till it finds the matching subtitle. Using this\napproach gives ability to make context based parallel corpus through filtering\nvideos by genre. Context based corpus can be used in complex translators which\ndecode sentences by different networks after determining contents subject.\nLanguages have many differences in their formal and informal styles, including\nwords and syntax. Other advantage of this method is to make corpus of informal\nstyle of languages. Because most of movies dialogues are parts of a\nconversation. So they had informal style. This feature of generated corpus can\nbe used in real-time translators to have more accurate conversation\ntranslations.", "published": "2018-04-11 11:07:16", "link": "http://arxiv.org/abs/1804.03923v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Task Learning for Argumentation Mining in Low-Resource Settings", "abstract": "We investigate whether and where multi-task learning (MTL) can improve\nperformance on NLP problems related to argumentation mining (AM), in particular\nargument component identification. Our results show that MTL performs\nparticularly well (and better than single-task learning) when little training\ndata is available for the main task, a common scenario in AM. Our findings\nchallenge previous assumptions that conceptualizations across AM datasets are\ndivergent and that MTL is difficult for semantic or higher-level tasks.", "published": "2018-04-11 16:43:13", "link": "http://arxiv.org/abs/1804.04083v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SHAPED: Shared-Private Encoder-Decoder for Text Style Adaptation", "abstract": "Supervised training of abstractive language generation models results in\nlearning conditional probabilities over language sequences based on the\nsupervised training signal. When the training signal contains a variety of\nwriting styles, such models may end up learning an 'average' style that is\ndirectly influenced by the training data make-up and cannot be controlled by\nthe needs of an application. We describe a family of model architectures\ncapable of capturing both generic language characteristics via shared model\nparameters, as well as particular style characteristics via private model\nparameters. Such models are able to generate language according to a specific\nlearned style, while still taking advantage of their power to model generic\nlanguage phenomena. Furthermore, we describe an extension that uses a mixture\nof output distributions from all learned styles to perform on-the fly style\nadaptation based on the textual input alone. Experimentally, we find that the\nproposed models consistently outperform models that encapsulate single-style or\naverage-style language generation capabilities.", "published": "2018-04-11 16:56:04", "link": "http://arxiv.org/abs/1804.04093v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Word Embedding Hyper-Parameters for Similarity and Analogy\n  Tasks", "abstract": "The versatility of word embeddings for various applications is attracting\nresearchers from various fields. However, the impact of hyper-parameters when\ntraining embedding model is often poorly understood. How much do\nhyper-parameters such as vector dimensions and corpus size affect the quality\nof embeddings, and how do these results translate to downstream applications?\nUsing standard embedding evaluation metrics and datasets, we conduct a study to\nempirically measure the impact of these hyper-parameters.", "published": "2018-04-11 20:35:36", "link": "http://arxiv.org/abs/1804.04211v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "English Out-of-Vocabulary Lexical Evaluation Task", "abstract": "Unlike previous unknown nouns tagging task, this is the first attempt to\nfocus on out-of-vocabulary (OOV) lexical evaluation tasks that do not require\nany prior knowledge. The OOV words are words that only appear in test samples.\nThe goal of tasks is to provide solutions for OOV lexical classification and\nprediction. The tasks require annotators to conclude the attributes of the OOV\nwords based on their related contexts. Then, we utilize unsupervised word\nembedding methods such as Word2Vec and Word2GM to perform the baseline\nexperiments on the categorical classification task and OOV words attribute\nprediction tasks.", "published": "2018-04-11 22:04:07", "link": "http://arxiv.org/abs/1804.04242v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Achieving Fluency and Coherency in Task-oriented Dialog", "abstract": "We consider real world task-oriented dialog settings, where agents need to\ngenerate both fluent natural language responses and correct external actions\nlike database queries and updates. We demonstrate that, when applied to\ncustomer support chat transcripts, Sequence to Sequence (Seq2Seq) models often\ngenerate short, incoherent and ungrammatical natural language responses that\nare dominated by words that occur with high frequency in the training data.\nThese phenomena do not arise in synthetic datasets such as bAbI, where we show\nSeq2Seq models are nearly perfect. We develop techniques to learn embeddings\nthat succinctly capture relevant information from the dialog history, and\ndemonstrate that nearest neighbor based approaches in this learned neural\nembedding space generate more fluent responses. However, we see that these\nmethods are not able to accurately predict when to execute an external action.\nWe show how to combine nearest neighbor and Seq2Seq methods in a hybrid model,\nwhere nearest neighbor is used to generate fluent responses and Seq2Seq type\nmodels ensure dialog coherency and generate accurate external actions. We show\nthat this approach is well suited for customer support scenarios, where agents'\nresponses are typically script-driven, and correct external actions are\ncritically important. The hybrid model on the customer support data achieves a\n78% relative improvement in fluency scores, and a 130% improvement in accuracy\nof external calls.", "published": "2018-04-11 03:49:22", "link": "http://arxiv.org/abs/1804.03799v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reference-less Measure of Faithfulness for Grammatical Error Correction", "abstract": "We propose USim, a semantic measure for Grammatical Error Correction (GEC)\nthat measures the semantic faithfulness of the output to the source, thereby\ncomplementing existing reference-less measures (RLMs) for measuring the\noutput's grammaticality. USim operates by comparing the semantic symbolic\nstructure of the source and the correction, without relying on manually-curated\nreferences. Our experiments establish the validity of USim, by showing that (1)\nsemantic annotation can be consistently applied to ungrammatical text; (2)\nvalid corrections obtain a high USim similarity score to the source; and (3)\ninvalid corrections obtain a lower score.", "published": "2018-04-11 06:10:48", "link": "http://arxiv.org/abs/1804.03824v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generating Clues for Gender based Occupation De-biasing in Text", "abstract": "Vast availability of text data has enabled widespread training and use of AI\nsystems that not only learn and predict attributes from the text but also\ngenerate text automatically. However, these AI models also learn gender, racial\nand ethnic biases present in the training data. In this paper, we present the\nfirst system that discovers the possibility that a given text portrays a gender\nstereotype associated with an occupation. If the possibility exists, the system\noffers counter-evidences of opposite gender also being associated with the same\noccupation in the context of user-provided geography and timespan. The system\nthus enables text de-biasing by assisting a human-in-the-loop. The system can\nnot only act as a text pre-processor before training any AI model but also help\nhuman story writers write stories free of occupation-level gender bias in the\ngeographical and temporal context of their choice.", "published": "2018-04-11 07:14:09", "link": "http://arxiv.org/abs/1804.03839v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Exploiting Task-Oriented Resources to Learn Word Embeddings for Clinical\n  Abbreviation Expansion", "abstract": "In the medical domain, identifying and expanding abbreviations in clinical\ntexts is a vital task for both better human and machine understanding. It is a\nchallenging task because many abbreviations are ambiguous especially for\nintensive care medicine texts, in which phrase abbreviations are frequently\nused. Besides the fact that there is no universal dictionary of clinical\nabbreviations and no universal rules for abbreviation writing, such texts are\ndifficult to acquire, expensive to annotate and even sometimes, confusing to\ndomain experts. This paper proposes a novel and effective approach - exploiting\ntask-oriented resources to learn word embeddings for expanding abbreviations in\nclinical notes. We achieved 82.27% accuracy, close to expert human performance.", "published": "2018-04-11 21:16:39", "link": "http://arxiv.org/abs/1804.04225v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hate Lingo: A Target-based Linguistic Analysis of Hate Speech in Social\n  Media", "abstract": "While social media empowers freedom of expression and individual voices, it\nalso enables anti-social behavior, online harassment, cyberbullying, and hate\nspeech. In this paper, we deepen our understanding of online hate speech by\nfocusing on a largely neglected but crucial aspect of hate speech -- its\ntarget: either \"directed\" towards a specific person or entity, or \"generalized\"\ntowards a group of people sharing a common protected characteristic. We perform\nthe first linguistic and psycholinguistic analysis of these two forms of hate\nspeech and reveal the presence of interesting markers that distinguish these\ntypes of hate speech. Our analysis reveals that Directed hate speech, in\naddition to being more personal and directed, is more informal, angrier, and\noften explicitly attacks the target (via name calling) with fewer analytic\nwords and more words suggesting authority and influence. Generalized hate\nspeech, on the other hand, is dominated by religious hate, is characterized by\nthe use of lethal words such as murder, exterminate, and kill; and quantity\nwords such as million and many. Altogether, our work provides a data-driven\nanalysis of the nuances of online-hate speech that enables not only a deepened\nunderstanding of hate speech and its social implications but also its\ndetection.", "published": "2018-04-11 23:39:49", "link": "http://arxiv.org/abs/1804.04257v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "CoT: Cooperative Training for Generative Modeling of Discrete Data", "abstract": "In this paper, we study the generative models of sequential discrete data. To\ntackle the exposure bias problem inherent in maximum likelihood estimation\n(MLE), generative adversarial networks (GANs) are introduced to penalize the\nunrealistic generated samples. To exploit the supervision signal from the\ndiscriminator, most previous models leverage REINFORCE to address the\nnon-differentiable problem of sequential discrete data. However, because of the\nunstable property of the training signal during the dynamic process of\nadversarial training, the effectiveness of REINFORCE, in this case, is hardly\nguaranteed. To deal with such a problem, we propose a novel approach called\nCooperative Training (CoT) to improve the training of sequence generative\nmodels. CoT transforms the min-max game of GANs into a joint maximization\nframework and manages to explicitly estimate and optimize Jensen-Shannon\ndivergence. Moreover, CoT works without the necessity of pre-training via MLE,\nwhich is crucial to the success of previous methods. In the experiments,\ncompared to existing state-of-the-art methods, CoT shows superior or at least\ncompetitive performance on sample quality, diversity, as well as training\nstability.", "published": "2018-04-11 02:10:55", "link": "http://arxiv.org/abs/1804.03782v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Emergent Communication through Negotiation", "abstract": "Multi-agent reinforcement learning offers a way to study how communication\ncould emerge in communities of agents needing to solve specific problems. In\nthis paper, we study the emergence of communication in the negotiation\nenvironment, a semi-cooperative model of agent interaction. We introduce two\ncommunication protocols -- one grounded in the semantics of the game, and one\nwhich is \\textit{a priori} ungrounded and is a form of cheap talk. We show that\nself-interested agents can use the pre-grounded communication channel to\nnegotiate fairly, but are unable to effectively use the ungrounded channel.\nHowever, prosocial agents do learn to use cheap talk to find an optimal\nnegotiating strategy, suggesting that cooperation is necessary for language to\nemerge. We also study communication behaviour in a setting where one agent\ninteracts with agents in a community with different levels of prosociality and\nshow how agent identifiability can aid negotiation.", "published": "2018-04-11 13:48:08", "link": "http://arxiv.org/abs/1804.03980v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Emergence of Linguistic Communication from Referential Games with\n  Symbolic and Pixel Input", "abstract": "The ability of algorithms to evolve or learn (compositional) communication\nprotocols has traditionally been studied in the language evolution literature\nthrough the use of emergent communication tasks. Here we scale up this research\nby using contemporary deep learning methods and by training\nreinforcement-learning neural network agents on referential communication\ngames. We extend previous work, in which agents were trained in symbolic\nenvironments, by developing agents which are able to learn from raw pixel data,\na more challenging and realistic input representation. We find that the degree\nof structure found in the input data affects the nature of the emerged\nprotocols, and thereby corroborate the hypothesis that structured compositional\nlanguage is most likely to emerge when agents perceive the world as being\nstructured.", "published": "2018-04-11 13:51:19", "link": "http://arxiv.org/abs/1804.03984v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Predicting Twitter User Socioeconomic Attributes with Network and\n  Language Information", "abstract": "Inferring socioeconomic attributes of social media users such as occupation\nand income is an important problem in computational social science. Automated\ninference of such characteristics has applications in personalised recommender\nsystems, targeted computational advertising and online political campaigning.\nWhile previous work has shown that language features can reliably predict\nsocioeconomic attributes on Twitter, employing information coming from users'\nsocial networks has not yet been explored for such complex user\ncharacteristics. In this paper, we describe a method for predicting the\noccupational class and the income of Twitter users given information extracted\nfrom their extended networks by learning a low-dimensional vector\nrepresentation of users, i.e. graph embeddings. We use this representation to\ntrain predictive models for occupational class and income. Results on two\npublicly available datasets show that our method consistently outperforms the\nstate-of-the-art methods in both tasks. We also obtain further significant\nimprovements when we combine graph embeddings with textual features,\ndemonstrating that social network and language information are complementary.", "published": "2018-04-11 17:00:27", "link": "http://arxiv.org/abs/1804.04095v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Automatically Infer Human Traits and Behavior from Social Media Data", "abstract": "Given the complexity of human minds and their behavioral flexibility, it\nrequires sophisticated data analysis to sift through a large amount of human\nbehavioral evidence to model human minds and to predict human behavior. People\ncurrently spend a significant amount of time on social media such as Twitter\nand Facebook. Thus many aspects of their lives and behaviors have been\ndigitally captured and continuously archived on these platforms. This makes\nsocial media a great source of large, rich and diverse human behavioral\nevidence. In this paper, we survey the recent work on applying machine learning\nto infer human traits and behavior from social media data. We will also point\nout several future research directions.", "published": "2018-04-11 19:59:44", "link": "http://arxiv.org/abs/1804.04191v1", "categories": ["cs.SI", "cs.CL", "cs.IR"], "primary_category": "cs.SI"}
{"title": "Learning Topics using Semantic Locality", "abstract": "The topic modeling discovers the latent topic probability of the given text\ndocuments. To generate the more meaningful topic that better represents the\ngiven document, we proposed a new feature extraction technique which can be\nused in the data preprocessing stage. The method consists of three steps.\nFirst, it generates the word/word-pair from every single document. Second, it\napplies a two-way TF-IDF algorithm to word/word-pair for semantic filtering.\nThird, it uses the K-means algorithm to merge the word pairs that have the\nsimilar semantic meaning.\n  Experiments are carried out on the Open Movie Database (OMDb), Reuters\nDataset and 20NewsGroup Dataset. The mean Average Precision score is used as\nthe evaluation metric. Comparing our results with other state-of-the-art topic\nmodels, such as Latent Dirichlet allocation and traditional Restricted\nBoltzmann Machines. Our proposed data preprocessing can improve the generated\ntopic accuracy by up to 12.99\\%.", "published": "2018-04-11 20:23:23", "link": "http://arxiv.org/abs/1804.04205v1", "categories": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Word2Vec applied to Recommendation: Hyperparameters Matter", "abstract": "Skip-gram with negative sampling, a popular variant of Word2vec originally\ndesigned and tuned to create word embeddings for Natural Language Processing,\nhas been used to create item embeddings with successful applications in\nrecommendation. While these fields do not share the same type of data, neither\nevaluate on the same tasks, recommendation applications tend to use the same\nalready tuned hyperparameters values, even if optimal hyperparameters values\nare often known to be data and task dependent. We thus investigate the marginal\nimportance of each hyperparameter in a recommendation setting through large\nhyperparameter grid searches on various datasets. Results reveal that\noptimizing neglected hyperparameters, namely negative sampling distribution,\nnumber of epochs, subsampling parameter and window-size, significantly improves\nperformance on a recommendation task, and can increase it by an order of\nmagnitude. Importantly, we find that optimal hyperparameters configurations for\nNatural Language Processing tasks and Recommendation tasks are noticeably\ndifferent.", "published": "2018-04-11 20:37:35", "link": "http://arxiv.org/abs/1804.04212v3", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
