{"title": "Abstractive Opinion Tagging", "abstract": "In e-commerce, opinion tags refer to a ranked list of tags provided by the\ne-commerce platform that reflect characteristics of reviews of an item. To\nassist consumers to quickly grasp a large number of reviews about an item,\nopinion tags are increasingly being applied by e-commerce platforms. Current\nmechanisms for generating opinion tags rely on either manual labelling or\nheuristic methods, which is time-consuming and ineffective. In this paper, we\npropose the abstractive opinion tagging task, where systems have to\nautomatically generate a ranked list of opinion tags that are based on, but\nneed not occur in, a given set of user-generated reviews.\n  The abstractive opinion tagging task comes with three main challenges: (1)\nthe noisy nature of reviews; (2) the formal nature of opinion tags vs. the\ncolloquial language usage in reviews; and (3) the need to distinguish between\ndifferent items with very similar aspects. To address these challenges, we\npropose an abstractive opinion tagging framework, named AOT-Net, to generate a\nranked list of opinion tags given a large number of reviews. First, a\nsentence-level salience estimation component estimates each review's salience\nscore. Next, a review clustering and ranking component ranks reviews in two\nsteps: first, reviews are grouped into clusters and ranked by cluster size;\nthen, reviews within each cluster are ranked by their distance to the cluster\ncenter. Finally, given the ranked reviews, a rank-aware opinion tagging\ncomponent incorporates an alignment feature and alignment loss to generate a\nranked list of opinion tags. To facilitate the study of this task, we create\nand release a large-scale dataset, called eComTag, crawled from real-world\ne-commerce websites. Extensive experiments conducted on the eComTag dataset\nverify the effectiveness of the proposed AOT-Net in terms of various evaluation\nmetrics.", "published": "2021-01-18 05:08:15", "link": "http://arxiv.org/abs/2101.06880v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incremental Knowledge Based Question Answering", "abstract": "In the past years, Knowledge-Based Question Answering (KBQA), which aims to\nanswer natural language questions using facts in a knowledge base, has been\nwell developed. Existing approaches often assume a static knowledge base.\nHowever, the knowledge is evolving over time in the real world. If we directly\napply a fine-tuning strategy on an evolving knowledge base, it will suffer from\na serious catastrophic forgetting problem. In this paper, we propose a new\nincremental KBQA learning framework that can progressively expand learning\ncapacity as humans do. Specifically, it comprises a margin-distilled loss and a\ncollaborative exemplar selection method, to overcome the catastrophic\nforgetting problem by taking advantage of knowledge distillation. We reorganize\nthe SimpleQuestion dataset to evaluate the proposed incremental learning\nsolution to KBQA. The comprehensive experiments demonstrate its effectiveness\nand efficiency when working with the evolving knowledge base.", "published": "2021-01-18 09:03:38", "link": "http://arxiv.org/abs/2101.06938v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HinFlair: pre-trained contextual string embeddings for pos tagging and\n  text classification in the Hindi language", "abstract": "Recent advancements in language models based on recurrent neural networks and\ntransformers architecture have achieved state-of-the-art results on a wide\nrange of natural language processing tasks such as pos tagging, named entity\nrecognition, and text classification. However, most of these language models\nare pre-trained in high resource languages like English, German, Spanish.\nMulti-lingual language models include Indian languages like Hindi, Telugu,\nBengali in their training corpus, but they often fail to represent the\nlinguistic features of these languages as they are not the primary language of\nthe study. We introduce HinFlair, which is a language representation model\n(contextual string embeddings) pre-trained on a large monolingual Hindi corpus.\nExperiments were conducted on 6 text classification datasets and a Hindi\ndependency treebank to analyze the performance of these contextualized string\nembeddings for the Hindi language. Results show that HinFlair outperforms\nprevious state-of-the-art publicly available pre-trained embeddings for\ndownstream tasks like text classification and pos tagging. Also, HinFlair when\ncombined with FastText embeddings outperforms many transformers-based language\nmodels trained particularly for the Hindi language.", "published": "2021-01-18 09:23:35", "link": "http://arxiv.org/abs/2101.06949v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MONAH: Multi-Modal Narratives for Humans to analyze conversations", "abstract": "In conversational analyses, humans manually weave multimodal information into\nthe transcripts, which is significantly time-consuming. We introduce a system\nthat automatically expands the verbatim transcripts of video-recorded\nconversations using multimodal data streams. This system uses a set of\npreprocessing rules to weave multimodal annotations into the verbatim\ntranscripts and promote interpretability. Our feature engineering contributions\nare two-fold: firstly, we identify the range of multimodal features relevant to\ndetect rapport-building; secondly, we expand the range of multimodal\nannotations and show that the expansion leads to statistically significant\nimprovements in detecting rapport-building.", "published": "2021-01-18 21:55:58", "link": "http://arxiv.org/abs/2101.07339v2", "categories": ["cs.CL", "I.7.2"], "primary_category": "cs.CL"}
{"title": "Automatic punctuation restoration with BERT models", "abstract": "We present an approach for automatic punctuation restoration with BERT models\nfor English and Hungarian. For English, we conduct our experiments on Ted\nTalks, a commonly used benchmark for punctuation restoration, while for\nHungarian we evaluate our models on the Szeged Treebank dataset. Our best\nmodels achieve a macro-averaged $F_1$-score of 79.8 in English and 82.2 in\nHungarian. Our code is publicly available.", "published": "2021-01-18 22:13:01", "link": "http://arxiv.org/abs/2101.07343v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Energy-based Model Training for Better Calibrated Natural Language\n  Understanding Models", "abstract": "In this work, we explore joint energy-based model (EBM) training during the\nfinetuning of pretrained text encoders (e.g., Roberta) for natural language\nunderstanding (NLU) tasks. Our experiments show that EBM training can help the\nmodel reach a better calibration that is competitive to strong baselines, with\nlittle or no loss in accuracy. We discuss three variants of energy functions\n(namely scalar, hidden, and sharp-hidden) that can be defined on top of a text\nencoder, and compare them in experiments. Due to the discreteness of text data,\nwe adopt noise contrastive estimation (NCE) to train the energy-based model. To\nmake NCE training more effective, we train an auto-regressive noise model with\nthe masked language model (MLM) objective.", "published": "2021-01-18 01:41:31", "link": "http://arxiv.org/abs/2101.06829v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Red Alarm for Pre-trained Models: Universal Vulnerability to\n  Neuron-Level Backdoor Attacks", "abstract": "Pre-trained models (PTMs) have been widely used in various downstream tasks.\nThe parameters of PTMs are distributed on the Internet and may suffer backdoor\nattacks. In this work, we demonstrate the universal vulnerability of PTMs,\nwhere fine-tuned PTMs can be easily controlled by backdoor attacks in arbitrary\ndownstream tasks. Specifically, attackers can add a simple pre-training task,\nwhich restricts the output representations of trigger instances to pre-defined\nvectors, namely neuron-level backdoor attack (NeuBA). If the backdoor\nfunctionality is not eliminated during fine-tuning, the triggers can make the\nfine-tuned model predict fixed labels by pre-defined vectors. In the\nexperiments of both natural language processing (NLP) and computer vision (CV),\nwe show that NeuBA absolutely controls the predictions for trigger instances\nwithout any knowledge of downstream tasks. Finally, we apply several defense\nmethods to NeuBA and find that model pruning is a promising direction to resist\nNeuBA by excluding backdoored neurons. Our findings sound a red alarm for the\nwide use of PTMs. Our source code and models are available at\n\\url{https://github.com/thunlp/NeuBA}.", "published": "2021-01-18 10:18:42", "link": "http://arxiv.org/abs/2101.06969v5", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Mitigating the Position Bias of Transformer Models in Passage Re-Ranking", "abstract": "Supervised machine learning models and their evaluation strongly depends on\nthe quality of the underlying dataset. When we search for a relevant piece of\ninformation it may appear anywhere in a given passage. However, we observe a\nbias in the position of the correct answer in the text in two popular Question\nAnswering datasets used for passage re-ranking. The excessive favoring of\nearlier positions inside passages is an unwanted artefact. This leads to three\ncommon Transformer-based re-ranking models to ignore relevant parts in unseen\npassages. More concerningly, as the evaluation set is taken from the same\nbiased distribution, the models overfitting to that bias overestimate their\ntrue effectiveness. In this work we analyze position bias on datasets, the\ncontextualized representations, and their effect on retrieval results. We\npropose a debiasing method for retrieval datasets. Our results show that a\nmodel trained on a position-biased dataset exhibits a significant decrease in\nre-ranking effectiveness when evaluated on a debiased dataset. We demonstrate\nthat by mitigating the position bias, Transformer-based re-ranking models are\nequally effective on a biased and debiased dataset, as well as more effective\nin a transfer-learning setting between two differently biased datasets.", "published": "2021-01-18 10:38:03", "link": "http://arxiv.org/abs/2101.06980v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Model Compression for Domain Adaptation through Causal Effect Estimation", "abstract": "Recent improvements in the predictive quality of natural language processing\nsystems are often dependent on a substantial increase in the number of model\nparameters. This has led to various attempts of compressing such models, but\nexisting methods have not considered the differences in the predictive power of\nvarious model components or in the generalizability of the compressed models.\nTo understand the connection between model compression and out-of-distribution\ngeneralization, we define the task of compressing language representation\nmodels such that they perform best in a domain adaptation setting. We choose to\naddress this problem from a causal perspective, attempting to estimate the\naverage treatment effect (ATE) of a model component, such as a single layer, on\nthe model's predictions. Our proposed ATE-guided Model Compression scheme\n(AMoC), generates many model candidates, differing by the model components that\nwere removed. Then, we select the best candidate through a stepwise regression\nmodel that utilizes the ATE to predict the expected performance on the target\ndomain. AMoC outperforms strong baselines on dozens of domain pairs across\nthree text classification and sequence tagging tasks.", "published": "2021-01-18 14:18:02", "link": "http://arxiv.org/abs/2101.07086v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neural Abstractive Text Summarizer for Telugu Language", "abstract": "Abstractive Text Summarization is the process of constructing semantically\nrelevant shorter sentences which captures the essence of the overall meaning of\nthe source text. It is actually difficult and very time consuming for humans to\nsummarize manually large documents of text. Much of work in abstractive text\nsummarization is being done in English and almost no significant work has been\nreported in Telugu abstractive text summarization. So, we would like to propose\nan abstractive text summarization approach for Telugu language using Deep\nlearning. In this paper we are proposing an abstractive text summarization Deep\nlearning model for Telugu language. The proposed architecture is based on\nencoder-decoder sequential models with attention mechanism. We have applied\nthis model on manually created dataset to generate a one sentence summary of\nthe source text and have got good results measured qualitatively.", "published": "2021-01-18 15:22:50", "link": "http://arxiv.org/abs/2101.07120v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Teach me how to Label: Labeling Functions from Natural Language with\n  Text-to-text Transformers", "abstract": "Annotated data has become the most important bottleneck in training accurate\nmachine learning models, especially for areas that require domain expertise. A\nrecent approach to deal with the above issue proposes using natural language\nexplanations instead of labeling individual data points, thereby increasing\nhuman annotators' efficiency as well as decreasing costs substantially. This\npaper focuses on the task of turning these natural language descriptions into\nPython labeling functions by following a novel approach to semantic parsing\nwith pre-trained text-to-text Transformers. In a series of experiments our\napproach achieves a new state of the art on the semantic parsing benchmark\nCoNaLa, surpassing the previous best approach by 3.7 BLEU points. Furthermore,\non a manually constructed dataset of natural language descriptions-labeling\nfunctions pairs we achieve a BLEU of 0.39. Our approach can be regarded as a\nstepping stone towards models that are taught how to label in natural language,\ninstead of being provided specific labeled samples. Our code, constructed\ndataset and models are available at\nhttps://github.com/ypapanik/t5-for-code-generation.", "published": "2021-01-18 16:04:15", "link": "http://arxiv.org/abs/2101.07138v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup", "abstract": "Contrastive learning has been applied successfully to learn vector\nrepresentations of text. Previous research demonstrated that learning\nhigh-quality representations benefits from batch-wise contrastive loss with a\nlarge number of negatives. In practice, the technique of in-batch negative is\nused, where for each example in a batch, other batch examples' positives will\nbe taken as its negatives, avoiding encoding extra negatives. This, however,\nstill conditions each example's loss on all batch examples and requires fitting\nthe entire large batch into GPU memory. This paper introduces a gradient\ncaching technique that decouples backpropagation between contrastive loss and\nthe encoder, removing encoder backward pass data dependency along the batch\ndimension. As a result, gradients can be computed for one subset of the batch\nat a time, leading to almost constant memory usage.", "published": "2021-01-18 10:42:34", "link": "http://arxiv.org/abs/2101.06983v2", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Natural Language Specification of Reinforcement Learning Policies\n  through Differentiable Decision Trees", "abstract": "Human-AI policy specification is a novel procedure we define in which humans\ncan collaboratively warm-start a robot's reinforcement learning policy. This\nprocedure is comprised of two steps; (1) Policy Specification, i.e. humans\nspecifying the behavior they would like their companion robot to accomplish,\nand (2) Policy Optimization, i.e. the robot applying reinforcement learning to\nimprove the initial policy. Existing approaches to enabling collaborative\npolicy specification are often unintelligible black-box methods, and are not\ncatered towards making the autonomous system accessible to a novice end-user.\nIn this paper, we develop a novel collaborative framework to allow humans to\ninitialize and interpret an autonomous agent's behavior. Through our framework,\nwe enable humans to specify an initial behavior model via unstructured, natural\nlanguage (NL), which we convert to lexical decision trees. Next, we leverage\nthese translated specifications, to warm-start reinforcement learning and allow\nthe agent to further optimize these potentially suboptimal policies. Our\napproach warm-starts an RL agent by utilizing non-expert natural language\nspecifications without incurring the additional domain exploration costs. We\nvalidate our approach by showing that our model is able to produce >80%\ntranslation accuracy, and that policies initialized by a human can match the\nperformance of relevant RL baselines in two domains.", "published": "2021-01-18 16:07:00", "link": "http://arxiv.org/abs/2101.07140v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.LG"}
{"title": "Can a Fruit Fly Learn Word Embeddings?", "abstract": "The mushroom body of the fruit fly brain is one of the best studied systems\nin neuroscience. At its core it consists of a population of Kenyon cells, which\nreceive inputs from multiple sensory modalities. These cells are inhibited by\nthe anterior paired lateral neuron, thus creating a sparse high dimensional\nrepresentation of the inputs. In this work we study a mathematical\nformalization of this network motif and apply it to learning the correlational\nstructure between words and their context in a corpus of unstructured text, a\ncommon natural language processing (NLP) task. We show that this network can\nlearn semantic representations of words and can generate both static and\ncontext-dependent word embeddings. Unlike conventional methods (e.g., BERT,\nGloVe) that use dense representations for word embedding, our algorithm encodes\nsemantic meaning of words and their context in the form of sparse binary hash\ncodes. The quality of the learned representations is evaluated on word\nsimilarity analysis, word-sense disambiguation, and document classification. It\nis shown that not only can the fruit fly network motif achieve performance\ncomparable to existing methods in NLP, but, additionally, it uses only a\nfraction of the computational resources (shorter training time and smaller\nmemory footprint).", "published": "2021-01-18 05:41:50", "link": "http://arxiv.org/abs/2101.06887v2", "categories": ["cs.CL", "cs.LG", "cs.NE", "q-bio.NC", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Tiny Transducer: A Highly-efficient Speech Recognition Model on Edge\n  Devices", "abstract": "This paper proposes an extremely lightweight phone-based transducer model\nwith a tiny decoding graph on edge devices. First, a phone synchronous decoding\n(PSD) algorithm based on blank label skipping is first used to speed up the\ntransducer decoding process. Then, to decrease the deletion errors introduced\nby the high blank score, a blank label deweighting approach is proposed. To\nreduce parameters and computation, deep feedforward sequential memory network\n(DFSMN) layers are used in the transducer encoder, and a CNN-based stateless\npredictor is adopted. SVD technology compresses the model further. WFST-based\ndecoding graph takes the context-independent (CI) phone posteriors as input and\nallows us to flexibly bias user-specific information. Finally, with only 0.9M\nparameters after SVD, our system could give a relative 9.1% - 20.5% improvement\ncompared with a bigger conventional hybrid system on edge devices.", "published": "2021-01-18 03:07:57", "link": "http://arxiv.org/abs/2101.06856v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Hierarchical disentangled representation learning for singing voice\n  conversion", "abstract": "Conventional singing voice conversion (SVC) methods often suffer from\noperating in high-resolution audio owing to a high dimensionality of data. In\nthis paper, we propose a hierarchical representation learning that enables the\nlearning of disentangled representations with multiple resolutions\nindependently. With the learned disentangled representations, the proposed\nmethod progressively performs SVC from low to high resolutions. Experimental\nresults show that the proposed method outperforms baselines that operate with a\nsingle resolution in terms of mean opinion score (MOS), similarity score, and\npitch accuracy.", "published": "2021-01-18 02:17:24", "link": "http://arxiv.org/abs/2101.06842v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
