{"title": "A Brief Survey of Multilingual Neural Machine Translation", "abstract": "We present a survey on multilingual neural machine translation (MNMT), which\nhas gained a lot of traction in the recent years. MNMT has been useful in\nimproving translation quality as a result of knowledge transfer. MNMT is more\npromising and interesting than its statistical machine translation counterpart\nbecause end-to-end modeling and distributed representations open new avenues.\nMany approaches have been proposed in order to exploit multilingual parallel\ncorpora for improving translation quality. However, the lack of a comprehensive\nsurvey makes it difficult to determine which approaches are promising and hence\ndeserve further exploration. In this paper, we present an in-depth survey of\nexisting literature on MNMT. We categorize various approaches based on the\nresource scenarios as well as underlying modeling principles. We hope this\npaper will serve as a starting point for researchers and engineers interested\nin MNMT.", "published": "2019-05-14 05:20:41", "link": "http://arxiv.org/abs/1905.05395v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the number of k-skip-n-grams", "abstract": "The paper proves that the number of k-skip-n-grams for a corpus of size $L$\nis $$\\frac{Ln + n + k' - n^2 - nk'}{n} \\cdot \\binom{n-1+k'}{n-1}$$ where $k' =\n\\min(L - n + 1, k)$.", "published": "2019-05-14 06:18:35", "link": "http://arxiv.org/abs/1905.05407v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cognitive Graph for Multi-Hop Reading Comprehension at Scale", "abstract": "We propose a new CogQA framework for multi-hop question answering in\nweb-scale documents. Inspired by the dual process theory in cognitive science,\nthe framework gradually builds a \\textit{cognitive graph} in an iterative\nprocess by coordinating an implicit extraction module (System 1) and an\nexplicit reasoning module (System 2). While giving accurate answers, our\nframework further provides explainable reasoning paths. Specifically, our\nimplementation based on BERT and graph neural network efficiently handles\nmillions of documents for multi-hop reasoning questions in the HotpotQA\nfullwiki dataset, achieving a winning joint $F_1$ score of 34.9 on the\nleaderboard, compared to 23.6 of the best competitor.", "published": "2019-05-14 08:55:04", "link": "http://arxiv.org/abs/1905.05460v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Residual Output Layers for Neural Language Generation", "abstract": "Many tasks, including language generation, benefit from learning the\nstructure of the output space, particularly when the space of output labels is\nlarge and the data is sparse. State-of-the-art neural language models\nindirectly capture the output space structure in their classifier weights since\nthey lack parameter sharing across output labels. Learning shared output label\nmappings helps, but existing methods have limited expressivity and are prone to\noverfitting. In this paper, we investigate the usefulness of more powerful\nshared mappings for output labels, and propose a deep residual output mapping\nwith dropout between layers to better capture the structure of the output space\nand avoid overfitting. Evaluations on three language generation tasks show that\nour output label mapping can match or improve state-of-the-art recurrent and\nself-attention architectures, and suggest that the classifier does not\nnecessarily need to be high-rank to better model natural language if it is\nbetter at capturing the structure of the output space.", "published": "2019-05-14 10:51:55", "link": "http://arxiv.org/abs/1905.05513v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity-Relation Extraction as Multi-Turn Question Answering", "abstract": "In this paper, we propose a new paradigm for the task of entity-relation\nextraction. We cast the task as a multi-turn question answering problem, i.e.,\nthe extraction of entities and relations is transformed to the task of\nidentifying answer spans from the context. This multi-turn QA formalization\ncomes with several key advantages: firstly, the question query encodes\nimportant information for the entity/relation class we want to identify;\nsecondly, QA provides a natural way of jointly modeling entity and relation;\nand thirdly, it allows us to exploit the well developed machine reading\ncomprehension (MRC) models. Experiments on the ACE and the CoNLL04 corpora\ndemonstrate that the proposed paradigm significantly outperforms previous best\nmodels. We are able to obtain the state-of-the-art results on all of the ACE04,\nACE05 and CoNLL04 datasets, increasing the SOTA results on the three datasets\nto 49.4 (+1.0), 60.2 (+0.6) and 68.9 (+2.1), respectively. Additionally, we\nconstruct a newly developed dataset RESUME in Chinese, which requires\nmulti-step reasoning to construct entity dependencies, as opposed to the\nsingle-step dependency extraction in the triplet exaction in previous datasets.\nThe proposed multi-turn QA model also achieves the best performance on the\nRESUME dataset.", "published": "2019-05-14 11:44:33", "link": "http://arxiv.org/abs/1905.05529v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the Difficulty of Classifying ConceptNet Relations in a\n  Multi-Label Classification Setting", "abstract": "Commonsense knowledge relations are crucial for advanced NLU tasks. We\nexamine the learnability of such relations as represented in CONCEPTNET, taking\ninto account their specific properties, which can make relation classification\ndifficult: a given concept pair can be linked by multiple relation types, and\nrelations can have multi-word arguments of diverse semantic types. We explore a\nneural open world multi-label classification approach that focuses on the\nevaluation of classification accuracy for individual relations. Based on an\nin-depth study of the specific properties of the CONCEPTNET resource, we\ninvestigate the impact of different relation representations and model\nvariations. Our analysis reveals that the complexity of argument types and\nrelation ambiguity are the most important challenges to address. We design a\ncustomized evaluation method to address the incompleteness of the resource that\ncan be expanded in future work.", "published": "2019-05-14 12:07:13", "link": "http://arxiv.org/abs/1905.05538v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Language of Legal and Illegal Activity on the Darknet", "abstract": "The non-indexed parts of the Internet (the Darknet) have become a haven for\nboth legal and illegal anonymous activity. Given the magnitude of these\nnetworks, scalably monitoring their activity necessarily relies on automated\ntools, and notably on NLP tools. However, little is known about what\ncharacteristics texts communicated through the Darknet have, and how well\noff-the-shelf NLP tools do on this domain. This paper tackles this gap and\nperforms an in-depth investigation of the characteristics of legal and illegal\ntext in the Darknet, comparing it to a clear net website with similar content\nas a control condition. Taking drug-related websites as a test case, we find\nthat texts for selling legal and illegal drugs have several linguistic\ncharacteristics that distinguish them from one another, as well as from the\ncontrol condition, among them the distribution of POS tags, and the coverage of\ntheir named entities in Wikipedia.", "published": "2019-05-14 12:14:27", "link": "http://arxiv.org/abs/1905.05543v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Dynamic Evolutionary Framework for Timeline Generation based on\n  Distributed Representations", "abstract": "Given the collection of timestamped web documents related to the evolving\ntopic, timeline summarization (TS) highlights its most important events in the\nform of relevant summaries to represent the development of a topic over time.\nMost of the previous work focuses on fully-observable ranking models and\ndepends on hand-designed features or complex mechanisms that may not generalize\nwell. We present a novel dynamic framework for evolutionary timeline generation\nleveraging distributed representations, which dynamically finds the most likely\nsequence of evolutionary summaries in the timeline, called the Viterbi\ntimeline, and reduces the impact of events that irrelevant or repeated to the\ntopic. The assumptions of the coherence and the global view run through our\nmodel. We explore adjacent relevance to constrain timeline coherence and make\nsure the events evolve on the same topic with a global view. Experimental\nresults demonstrate that our framework is feasible to extract summaries for\ntimeline generation, outperforms various competitive baselines, and achieves\nthe state-of-the-art performance as an unsupervised approach.", "published": "2019-05-14 12:30:53", "link": "http://arxiv.org/abs/1905.05550v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Fine-Tune BERT for Text Classification?", "abstract": "Language model pre-training has proven to be useful in learning universal\nlanguage representations. As a state-of-the-art language model pre-training\nmodel, BERT (Bidirectional Encoder Representations from Transformers) has\nachieved amazing results in many language understanding tasks. In this paper,\nwe conduct exhaustive experiments to investigate different fine-tuning methods\nof BERT on text classification task and provide a general solution for BERT\nfine-tuning. Finally, the proposed solution obtains new state-of-the-art\nresults on eight widely-studied text classification datasets.", "published": "2019-05-14 13:17:26", "link": "http://arxiv.org/abs/1905.05583v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Style Transformer: Unpaired Text Style Transfer without Disentangled\n  Latent Representation", "abstract": "Disentangling the content and style in the latent space is prevalent in\nunpaired text style transfer. However, two major issues exist in most of the\ncurrent neural models. 1) It is difficult to completely strip the style\ninformation from the semantics for a sentence. 2) The recurrent neural network\n(RNN) based encoder and decoder, mediated by the latent representation, cannot\nwell deal with the issue of the long-term dependency, resulting in poor\npreservation of non-stylistic semantic content. In this paper, we propose the\nStyle Transformer, which makes no assumption about the latent representation of\nsource sentence and equips the power of attention mechanism in Transformer to\nachieve better style transfer and better content preservation.", "published": "2019-05-14 14:03:21", "link": "http://arxiv.org/abs/1905.05621v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sense Vocabulary Compression through the Semantic Knowledge of WordNet\n  for Neural Word Sense Disambiguation", "abstract": "In this article, we tackle the issue of the limited quantity of manually\nsense annotated corpora for the task of word sense disambiguation, by\nexploiting the semantic relationships between senses such as synonymy,\nhypernymy and hyponymy, in order to compress the sense vocabulary of Princeton\nWordNet, and thus reduce the number of different sense tags that must be\nobserved to disambiguate all words of the lexical database. We propose two\ndifferent methods that greatly reduces the size of neural WSD models, with the\nbenefit of improving their coverage without additional training data, and\nwithout impacting their precision. In addition to our method, we present a WSD\nsystem which relies on pre-trained BERT word vectors in order to achieve\nresults that significantly outperform the state of the art on all WSD\nevaluation tasks.", "published": "2019-05-14 15:39:58", "link": "http://arxiv.org/abs/1905.05677v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-task Learning for Multi-modal Emotion Recognition and Sentiment\n  Analysis", "abstract": "Related tasks often have inter-dependence on each other and perform better\nwhen solved in a joint framework. In this paper, we present a deep multi-task\nlearning framework that jointly performs sentiment and emotion analysis both.\nThe multi-modal inputs (i.e., text, acoustic and visual frames) of a video\nconvey diverse and distinctive information, and usually do not have equal\ncontribution in the decision making. We propose a context-level inter-modal\nattention framework for simultaneously predicting the sentiment and expressed\nemotions of an utterance. We evaluate our proposed approach on CMU-MOSEI\ndataset for multi-modal sentiment and emotion analysis. Evaluation results\nsuggest that multi-task learning framework offers improvement over the\nsingle-task framework. The proposed approach reports new state-of-the-art\nperformance for both sentiment analysis and emotion analysis.", "published": "2019-05-14 19:42:43", "link": "http://arxiv.org/abs/1905.05812v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Curriculum Learning for Domain Adaptation in Neural Machine Translation", "abstract": "We introduce a curriculum learning approach to adapt generic neural machine\ntranslation models to a specific domain. Samples are grouped by their\nsimilarities to the domain of interest and each group is fed to the training\nalgorithm with a particular schedule. This approach is simple to implement on\ntop of any neural framework or architecture, and consistently outperforms both\nunadapted and adapted baselines in experiments with two distinct domains and\ntwo language pairs.", "published": "2019-05-14 20:03:54", "link": "http://arxiv.org/abs/1905.05816v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extraction and Analysis of Clinically Important Follow-up\n  Recommendations in a Large Radiology Dataset", "abstract": "Communication of follow-up recommendations when abnormalities are identified\non imaging studies is prone to error. In this paper, we present a natural\nlanguage processing approach based on deep learning to automatically identify\nclinically important recommendations in radiology reports. Our approach first\nidentifies the recommendation sentences and then extracts reason, test, and\ntime frame of the identified recommendations. To train our extraction models,\nwe created a corpus of 567 radiology reports annotated for recommendation\ninformation. Our extraction models achieved 0.92 f-score for recommendation\nsentence, 0.65 f-score for reason, 0.73 f-score for test, and 0.84 f-score for\ntime frame. We applied the extraction models to a set of over 3.3 million\nradiology reports and analyzed the adherence of follow-up recommendations.", "published": "2019-05-14 23:11:46", "link": "http://arxiv.org/abs/1905.05877v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Neural Conversational Models with Entropy-Based Data Filtering", "abstract": "Current neural network-based conversational models lack diversity and\ngenerate boring responses to open-ended utterances. Priors such as persona,\nemotion, or topic provide additional information to dialog models to aid\nresponse generation, but annotating a dataset with priors is expensive and such\nannotations are rarely available. While previous methods for improving the\nquality of open-domain response generation focused on either the underlying\nmodel or the training objective, we present a method of filtering dialog\ndatasets by removing generic utterances from training data using a simple\nentropy-based approach that does not require human supervision. We conduct\nextensive experiments with different variations of our method, and compare\ndialog models across 17 evaluation metrics to show that training on datasets\nfiltered this way results in better conversational quality as chatbots learn to\noutput more diverse responses.", "published": "2019-05-14 09:07:30", "link": "http://arxiv.org/abs/1905.05471v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Effective Cross-lingual Transfer of Neural Machine Translation Models\n  without Shared Vocabularies", "abstract": "Transfer learning or multilingual model is essential for low-resource neural\nmachine translation (NMT), but the applicability is limited to cognate\nlanguages by sharing their vocabularies. This paper shows effective techniques\nto transfer a pre-trained NMT model to a new, unrelated language without shared\nvocabularies. We relieve the vocabulary mismatch by using cross-lingual word\nembedding, train a more language-agnostic encoder by injecting artificial\nnoises, and generate synthetic data easily from the pre-training data without\nback-translation. Our methods do not require restructuring the vocabulary or\nretraining the model. We improve plain NMT transfer by up to +5.1% BLEU in five\nlow-resource translation tasks, outperforming multilingual joint training by a\nlarge margin. We also provide extensive ablation studies on pre-trained\nembedding, synthetic data, vocabulary size, and parameter freezing for a better\nunderstanding of NMT transfer.", "published": "2019-05-14 09:13:23", "link": "http://arxiv.org/abs/1905.05475v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Is Word Segmentation Necessary for Deep Learning of Chinese\n  Representations?", "abstract": "Segmenting a chunk of text into words is usually the first step of processing\nChinese text, but its necessity has rarely been explored. In this paper, we ask\nthe fundamental question of whether Chinese word segmentation (CWS) is\nnecessary for deep learning-based Chinese Natural Language Processing. We\nbenchmark neural word-based models which rely on word segmentation against\nneural char-based models which do not involve word segmentation in four\nend-to-end NLP benchmark tasks: language modeling, machine translation,\nsentence matching/paraphrase and text classification. Through direct\ncomparisons between these two types of models, we find that char-based models\nconsistently outperform word-based models. Based on these observations, we\nconduct comprehensive experiments to study why word-based models underperform\nchar-based models in these deep learning-based NLP tasks. We show that it is\nbecause word-based models are more vulnerable to data sparsity and the presence\nof out-of-vocabulary (OOV) words, and thus more prone to overfitting. We hope\nthis paper could encourage researchers in the community to rethink the\nnecessity of word segmentation in deep learning-based Chinese Natural Language\nProcessing. \\footnote{Yuxian Meng and Xiaoya Li contributed equally to this\npaper.}", "published": "2019-05-14 11:39:43", "link": "http://arxiv.org/abs/1905.05526v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Atom Responding Machine for Dialog Generation", "abstract": "Recently, improving the relevance and diversity of dialogue system has\nattracted wide attention. For a post x, the corresponding response y is usually\ndiverse in the real-world corpus, while the conventional encoder-decoder model\ntends to output the high-frequency (safe but trivial) responses and thus is\ndifficult to handle the large number of responding styles. To address these\nissues, we propose the Atom Responding Machine (ARM), which is based on a\nproposed encoder-composer-decoder network trained by a teacher-student\nframework. To enrich the generated responses, ARM introduces a large number of\nmolecule-mechanisms as various responding styles, which are conducted by taking\ndifferent combinations from a few atom-mechanisms. In other words, even a\nlittle of atom-mechanisms can make a mickle of molecule-mechanisms. The\nexperiments demonstrate diversity and quality of the responses generated by\nARM. We also present generating process to show underlying interpretability for\nthe result.", "published": "2019-05-14 11:44:54", "link": "http://arxiv.org/abs/1905.05532v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Meta-Learning for Low-resource Natural Language Generation in\n  Task-oriented Dialogue Systems", "abstract": "Natural language generation (NLG) is an essential component of task-oriented\ndialogue systems. Despite the recent success of neural approaches for NLG, they\nare typically developed for particular domains with rich annotated training\nexamples. In this paper, we study NLG in a low-resource setting to generate\nsentences in new scenarios with handful training examples. We formulate the\nproblem from a meta-learning perspective, and propose a generalized\noptimization-based approach (Meta-NLG) based on the well-recognized\nmodel-agnostic meta-learning (MAML) algorithm. Meta-NLG defines a set of meta\ntasks, and directly incorporates the objective of adapting to new low-resource\nNLG tasks into the meta-learning optimization process. Extensive experiments\nare conducted on a large multi-domain dataset (MultiWoz) with diverse\nlinguistic variations. We show that Meta-NLG significantly outperforms other\ntraining procedures in various low-resource configurations. We analyze the\nresults, and demonstrate that Meta-NLG adapts extremely fast and well to\nlow-resource situations.", "published": "2019-05-14 14:35:06", "link": "http://arxiv.org/abs/1905.05644v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Unified Linear-Time Framework for Sentence-Level Discourse Parsing", "abstract": "We propose an efficient neural framework for sentence-level discourse\nanalysis in accordance with Rhetorical Structure Theory (RST). Our framework\ncomprises a discourse segmenter to identify the elementary discourse units\n(EDU) in a text, and a discourse parser that constructs a discourse tree in a\ntop-down fashion. Both the segmenter and the parser are based on Pointer\nNetworks and operate in linear time. Our segmenter yields an $F_1$ score of\n95.4, and our parser achieves an $F_1$ score of 81.7 on the aggregated labeled\n(relation) metric, surpassing previous approaches by a good margin and\napproaching human agreement on both tasks (98.3 and 83.0 $F_1$).", "published": "2019-05-14 15:54:57", "link": "http://arxiv.org/abs/1905.05682v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sparse Sequence-to-Sequence Models", "abstract": "Sequence-to-sequence models are a powerful workhorse of NLP. Most variants\nemploy a softmax transformation in both their attention mechanism and output\nlayer, leading to dense alignments and strictly positive output probabilities.\nThis density is wasteful, making models less interpretable and assigning\nprobability mass to many implausible outputs. In this paper, we propose sparse\nsequence-to-sequence models, rooted in a new family of $\\alpha$-entmax\ntransformations, which includes softmax and sparsemax as particular cases, and\nis sparse for any $\\alpha > 1$. We provide fast algorithms to evaluate these\ntransformations and their gradients, which scale well for large vocabulary\nsizes. Our models are able to produce sparse alignments and to assign nonzero\nprobability to a short list of plausible outputs, sometimes rendering beam\nsearch exact. Experiments on morphological inflection and machine translation\nreveal consistent gains over dense models.", "published": "2019-05-14 16:21:34", "link": "http://arxiv.org/abs/1905.05702v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-step Retriever-Reader Interaction for Scalable Open-domain\n  Question Answering", "abstract": "This paper introduces a new framework for open-domain question answering in\nwhich the retriever and the reader iteratively interact with each other. The\nframework is agnostic to the architecture of the machine reading model, only\nrequiring access to the token-level hidden representations of the reader. The\nretriever uses fast nearest neighbor search to scale to corpora containing\nmillions of paragraphs. A gated recurrent unit updates the query at each step\nconditioned on the state of the reader and the reformulated query is used to\nre-rank the paragraphs by the retriever. We conduct analysis and show that\niterative interaction helps in retrieving informative paragraphs from the\ncorpus. Finally, we show that our multi-step-reasoning framework brings\nconsistent improvement when applied to two widely used reader architectures\nDrQA and BiDAF on various large open-domain datasets --- TriviaQA-unfiltered,\nQuasarT, SearchQA, and SQuAD-Open.", "published": "2019-05-14 17:27:08", "link": "http://arxiv.org/abs/1905.05733v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Ontology-Aware Clinical Abstractive Summarization", "abstract": "Automatically generating accurate summaries from clinical reports could save\na clinician's time, improve summary coverage, and reduce errors. We propose a\nsequence-to-sequence abstractive summarization model augmented with\ndomain-specific ontological information to enhance content selection and\nsummary generation. We apply our method to a dataset of radiology reports and\nshow that it significantly outperforms the current state-of-the-art on this\ntask in terms of rouge scores. Extensive human evaluation conducted by a\nradiologist further indicates that this approach yields summaries that are less\nlikely to omit important details, without sacrificing readability or accuracy.", "published": "2019-05-14 20:09:18", "link": "http://arxiv.org/abs/1905.05818v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Correlating neural and symbolic representations of language", "abstract": "Analysis methods which enable us to better understand the representations and\nfunctioning of neural models of language are increasingly needed as deep\nlearning becomes the dominant approach in NLP. Here we present two methods\nbased on Representational Similarity Analysis (RSA) and Tree Kernels (TK) which\nallow us to directly quantify how strongly the information encoded in neural\nactivation patterns corresponds to information represented by symbolic\nstructures such as syntax trees. We first validate our methods on the case of a\nsimple synthetic language for arithmetic expressions with clearly defined\nsyntax and semantics, and show that they exhibit the expected pattern of\nresults. We then apply our methods to correlate neural representations of\nEnglish sentences with their constituency parse trees.", "published": "2019-05-14 10:09:14", "link": "http://arxiv.org/abs/1905.06401v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SP-10K: A Large-scale Evaluation Set for Selectional Preference\n  Acquisition", "abstract": "Selectional Preference (SP) is a commonly observed language phenomenon and\nproved to be useful in many natural language processing tasks. To provide a\nbetter evaluation method for SP models, we introduce SP-10K, a large-scale\nevaluation set that provides human ratings for the plausibility of 10,000 SP\npairs over five SP relations, covering 2,500 most frequent verbs, nouns, and\nadjectives in American English. Three representative SP acquisition methods\nbased on pseudo-disambiguation are evaluated with SP-10K. To demonstrate the\nimportance of our dataset, we investigate the relationship between SP-10K and\nthe commonsense knowledge in ConceptNet5 and show the potential of using SP to\nrepresent the commonsense knowledge. We also use the Winograd Schema Challenge\nto prove that the proposed new SP relations are essential for the hard pronoun\ncoreference resolution problem.", "published": "2019-05-14 08:32:39", "link": "http://arxiv.org/abs/1906.02123v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multilingual Factor Analysis", "abstract": "In this work we approach the task of learning multilingual word\nrepresentations in an offline manner by fitting a generative latent variable\nmodel to a multilingual dictionary. We model equivalent words in different\nlanguages as different views of the same word generated by a common latent\nvariable representing their latent lexical meaning. We explore the task of\nalignment by querying the fitted model for multilingual embeddings achieving\ncompetitive results across a variety of tasks. The proposed model is robust to\nnoise in the embedding space making it a suitable method for distributed\nrepresentations learned from noisy corpora.", "published": "2019-05-14 12:22:39", "link": "http://arxiv.org/abs/1905.05547v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Misleading Failures of Partial-input Baselines", "abstract": "Recent work establishes dataset difficulty and removes annotation artifacts\nvia partial-input baselines (e.g., hypothesis-only models for SNLI or\nquestion-only models for VQA). When a partial-input baseline gets high\naccuracy, a dataset is cheatable. However, the converse is not necessarily\ntrue: the failure of a partial-input baseline does not mean a dataset is free\nof artifacts. To illustrate this, we first design artificial datasets which\ncontain trivial patterns in the full input that are undetectable by any\npartial-input model. Next, we identify such artifacts in the SNLI dataset - a\nhypothesis-only model augmented with trivial patterns in the premise can solve\n15% of the examples that are previously considered \"hard\". Our work provides a\ncaveat for the use of partial-input baselines for dataset verification and\ncreation.", "published": "2019-05-14 18:01:41", "link": "http://arxiv.org/abs/1905.05778v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Generative Design in Minecraft: Chronicle Challenge", "abstract": "We introduce the Chronicle Challenge as an optional addition to the\nSettlement Generation Challenge in Minecraft. One of the foci of the overall\ncompetition is adaptive procedural content generation (PCG), an arguably\nunder-explored problem in computational creativity. In the base challenge,\nparticipants must generate new settlements that respond to and ideally interact\nwith existing content in the world, such as the landscape or climate. The goal\nis to understand the underlying creative process, and to design better PCG\nsystems. The Chronicle Challenge in particular focuses on the generation of a\nnarrative based on the history of a generated settlement, expressed in natural\nlanguage. We discuss the unique features of the Chronicle Challenge in\ncomparison to other competitions, clarify the characteristics of a chronicle\neligible for submission and describe the evaluation criteria. We furthermore\ndraw on simulation-based approaches in computational storytelling as examples\nto how this challenge could be approached.", "published": "2019-05-14 23:53:57", "link": "http://arxiv.org/abs/1905.05888v1", "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.AI"}
{"title": "Generic Encodings of Constructor Rewriting Systems", "abstract": "Rewriting is a formalism widely used in computer science and mathematical\nlogic. The classical formalism has been extended, in the context of functional\nlanguages, with an order over the rules and, in the context of rewrite based\nlanguages, with the negation over patterns. We propose in this paper a concise\nand clear algorithm computing the difference over patterns which can be used to\ndefine generic encodings of constructor term rewriting systems with negation\nand order into classical term rewriting systems. As a direct consequence,\nestablished methods used for term rewriting systems can be applied to analyze\nproperties of the extended systems. The approach can also be seen as a generic\ncompiler which targets any language providing basic pattern matching\nprimitives. The formalism provides also a new method for deciding if a set of\npatterns subsumes a given pattern and thus, for checking the presence of\nuseless patterns or the completeness of a set of patterns.", "published": "2019-05-14 14:06:09", "link": "http://arxiv.org/abs/1905.06233v2", "categories": ["cs.LO", "cs.CL", "cs.PL"], "primary_category": "cs.LO"}
{"title": "PatentBERT: Patent Classification with Fine-Tuning a pre-trained BERT\n  Model", "abstract": "In this work we focus on fine-tuning a pre-trained BERT model and applying it\nto patent classification. When applied to large datasets of over two millions\npatents, our approach outperforms the state of the art by an approach using CNN\nwith word embeddings. In addition, we focus on patent claims without other\nparts in patent documents. Our contributions include: (1) a new\nstate-of-the-art method based on pre-trained BERT model and fine-tuning for\npatent classification, (2) a large dataset USPTO-3M at the CPC subclass level\nwith SQL statements that can be used by future researchers, (3) showing that\npatent claims alone are sufficient for classification task, in contrast to\nconventional wisdom.", "published": "2019-05-14 10:33:08", "link": "http://arxiv.org/abs/1906.02124v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Strong and Simple Baselines for Multimodal Utterance Embeddings", "abstract": "Human language is a rich multimodal signal consisting of spoken words, facial\nexpressions, body gestures, and vocal intonations. Learning representations for\nthese spoken utterances is a complex research problem due to the presence of\nmultiple heterogeneous sources of information. Recent advances in multimodal\nlearning have followed the general trend of building more complex models that\nutilize various attention, memory and recurrent components. In this paper, we\npropose two simple but strong baselines to learn embeddings of multimodal\nutterances. The first baseline assumes a conditional factorization of the\nutterance into unimodal factors. Each unimodal factor is modeled using the\nsimple form of a likelihood function obtained via a linear transformation of\nthe embedding. We show that the optimal embedding can be derived in closed form\nby taking a weighted average of the unimodal features. In order to capture\nricher representations, our second baseline extends the first by factorizing\ninto unimodal, bimodal, and trimodal factors, while retaining simplicity and\nefficiency during learning and inference. From a set of experiments across two\ntasks, we show strong performance on both supervised and semi-supervised\nmultimodal prediction, as well as significant (10 times) speedups over neural\nmodels during inference. Overall, we believe that our strong baseline models\noffer new benchmarking options for future research in multimodal learning.", "published": "2019-05-14 13:44:37", "link": "http://arxiv.org/abs/1906.02125v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.CL"}
{"title": "AUTOVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss", "abstract": "Non-parallel many-to-many voice conversion, as well as zero-shot voice\nconversion, remain under-explored areas. Deep style transfer algorithms, such\nas generative adversarial networks (GAN) and conditional variational\nautoencoder (CVAE), are being applied as new solutions in this field. However,\nGAN training is sophisticated and difficult, and there is no strong evidence\nthat its generated speech is of good perceptual quality. On the other hand,\nCVAE training is simple but does not come with the distribution-matching\nproperty of a GAN. In this paper, we propose a new style transfer scheme that\ninvolves only an autoencoder with a carefully designed bottleneck. We formally\nshow that this scheme can achieve distribution-matching style transfer by\ntraining only on a self-reconstruction loss. Based on this scheme, we proposed\nAUTOVC, which achieves state-of-the-art results in many-to-many voice\nconversion with non-parallel data, and which is the first to perform zero-shot\nvoice conversion.", "published": "2019-05-14 23:19:04", "link": "http://arxiv.org/abs/1905.05879v2", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Learning to Groove with Inverse Sequence Transformations", "abstract": "We explore models for translating abstract musical ideas (scores, rhythms)\ninto expressive performances using Seq2Seq and recurrent Variational\nInformation Bottleneck (VIB) models. Though Seq2Seq models usually require\npainstakingly aligned corpora, we show that it is possible to adapt an approach\nfrom the Generative Adversarial Network (GAN) literature (e.g. Pix2Pix (Isola\net al., 2017) and Vid2Vid (Wang et al. 2018a)) to sequences, creating large\nvolumes of paired data by performing simple transformations and training\ngenerative models to plausibly invert these transformations. Music, and\ndrumming in particular, provides a strong test case for this approach because\nmany common transformations (quantization, removing voices) have clear\nsemantics, and models for learning to invert them have real-world applications.\nFocusing on the case of drum set players, we create and release a new dataset\nfor this purpose, containing over 13 hours of recordings by professional\ndrummers aligned with fine-grained timing and dynamics information. We also\nexplore some of the creative potential of these models, including demonstrating\nimprovements on state-of-the-art methods for Humanization (instantiating a\nperformance from a musical score).", "published": "2019-05-14 17:25:50", "link": "http://arxiv.org/abs/1905.06118v2", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS", "stat.ML", "J.5; I.2"], "primary_category": "cs.SD"}
