{"title": "Bridging Continuous and Discrete Spaces: Interpretable Sentence\n  Representation Learning via Compositional Operations", "abstract": "Traditional sentence embedding models encode sentences into vector\nrepresentations to capture useful properties such as the semantic similarity\nbetween sentences. However, in addition to similarity, sentence semantics can\nalso be interpreted via compositional operations such as sentence fusion or\ndifference. It is unclear whether the compositional semantics of sentences can\nbe directly reflected as compositional operations in the embedding space. To\nmore effectively bridge the continuous embedding and discrete text spaces, we\nexplore the plausibility of incorporating various compositional properties into\nthe sentence embedding space that allows us to interpret embedding\ntransformations as compositional sentence operations. We propose InterSent, an\nend-to-end framework for learning interpretable sentence embeddings that\nsupports compositional sentence operations in the embedding space. Our method\noptimizes operator networks and a bottleneck encoder-decoder model to produce\nmeaningful and interpretable sentence embeddings. Experimental results\ndemonstrate that our method significantly improves the interpretability of\nsentence embeddings on four textual generation tasks over existing approaches\nwhile maintaining strong performance on traditional semantic similarity tasks.", "published": "2023-05-24 00:44:49", "link": "http://arxiv.org/abs/2305.14599v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OpenPI2.0: An Improved Dataset for Entity Tracking in Texts", "abstract": "Much text describes a changing world (e.g., procedures, stories, newswires),\nand understanding them requires tracking how entities change. An earlier\ndataset, OpenPI, provided crowdsourced annotations of entity state changes in\ntext. However, a major limitation was that those annotations were free-form and\ndid not identify salient changes, hampering model evaluation. To overcome these\nlimitations, we present an improved dataset, OpenPI2.0, where entities and\nattributes are fully canonicalized and additional entity salience annotations\nare added. On our fairer evaluation setting, we find that current\nstate-of-the-art language models are far from competent. We also show that\nusing state changes of salient entities as a chain-of-thought prompt,\ndownstream performance is improved on tasks such as question answering and\nclassical planning, outperforming the setting involving all related entities\nindiscriminately. We offer OpenPI2.0 for the continued development of models\nthat can understand the dynamics of entities in text.", "published": "2023-05-24 00:57:35", "link": "http://arxiv.org/abs/2305.14603v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language\n  Models", "abstract": "Do the Spratly Islands belong to China, the Philippines, or Vietnam? A\npretrained large language model (LLM) may answer differently if asked in the\nlanguages of each claimant country: Chinese, Tagalog, or Vietnamese. This\ncontrasts with a multilingual human, who would likely answer consistently. In\nthis paper, we show that LLMs recall certain geographical knowledge\ninconsistently when queried in different languages -- a phenomenon we term\ngeopolitical bias. As a targeted case study, we consider territorial disputes,\nan inherently controversial and multilingual task. We introduce BorderLines, a\ndataset of territorial disputes which covers 251 territories, each associated\nwith a set of multiple-choice questions in the languages of each claimant\ncountry (49 languages in total). We also propose a suite of evaluation metrics\nto precisely quantify bias and consistency in responses across different\nlanguages. We then evaluate various multilingual LLMs on our dataset and\nmetrics to probe their internal knowledge and use the proposed metrics to\ndiscover numerous inconsistencies in how these models respond in different\nlanguages. Finally, we explore several prompt modification strategies, aiming\nto either amplify or mitigate geopolitical bias, which highlights how brittle\nLLMs are and how they tailor their responses depending on cues from the\ninteraction context. Our code and data are available at\nhttps://github.com/manestay/borderlines", "published": "2023-05-24 01:16:17", "link": "http://arxiv.org/abs/2305.14610v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Checker: Plug-and-Play Modules for Fact-Checking with Large\n  Language Models", "abstract": "Fact-checking is an essential task in NLP that is commonly utilized for\nvalidating the factual accuracy of claims. Prior work has mainly focused on\nfine-tuning pre-trained languages models on specific datasets, which can be\ncomputationally intensive and time-consuming. With the rapid development of\nlarge language models (LLMs), such as ChatGPT and GPT-3, researchers are now\nexploring their in-context learning capabilities for a wide range of tasks. In\nthis paper, we aim to assess the capacity of LLMs for fact-checking by\nintroducing Self-Checker, a framework comprising a set of plug-and-play modules\nthat facilitate fact-checking by purely prompting LLMs in an almost zero-shot\nsetting. This framework provides a fast and efficient way to construct\nfact-checking systems in low-resource environments. Empirical results\ndemonstrate the potential of Self-Checker in utilizing LLMs for fact-checking.\nHowever, there is still significant room for improvement compared to SOTA\nfine-tuned models, which suggests that LLM adoption could be a promising\napproach for future fact-checking research.", "published": "2023-05-24 01:46:07", "link": "http://arxiv.org/abs/2305.14623v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KNN-LM Does Not Improve Open-ended Text Generation", "abstract": "In this paper, we study the generation quality of interpolation-based\nretrieval-augmented language models (LMs). These methods, best exemplified by\nthe KNN-LM, interpolate the LM's predicted distribution of the next word with a\ndistribution formed from the most relevant retrievals for a given prefix. While\nthe KNN-LM and related methods yield impressive decreases in perplexity, we\ndiscover that they do not exhibit corresponding improvements in open-ended\ngeneration quality, as measured by both automatic evaluation metrics (e.g.,\nMAUVE) and human evaluations. Digging deeper, we find that interpolating with a\nretrieval distribution actually increases perplexity compared to a baseline\nTransformer LM for the majority of tokens in the WikiText-103 test set, even\nthough the overall perplexity is lower due to a smaller number of tokens for\nwhich perplexity dramatically decreases after interpolation. However, when\ndecoding a long sequence at inference time, significant improvements on this\nsmaller subset of tokens are washed out by slightly worse predictions on most\ntokens. Furthermore, we discover that the entropy of the retrieval distribution\nincreases faster than that of the base LM as the generated sequence becomes\nlonger, which indicates that retrieval is less reliable when using\nmodel-generated text as queries (i.e., is subject to exposure bias). We hope\nthat our analysis spurs future work on improved decoding algorithms and\ninterpolation strategies for retrieval-augmented language models.", "published": "2023-05-24 01:48:33", "link": "http://arxiv.org/abs/2305.14625v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Testing Causal Models of Word Meaning in GPT-3 and -4", "abstract": "Large Language Models (LLMs) have driven extraordinary improvements in NLP.\nHowever, it is unclear how such models represent lexical concepts-i.e., the\nmeanings of the words they use. This paper evaluates the lexical\nrepresentations of GPT-3 and GPT-4 through the lens of HIPE theory, a theory of\nconcept representations which focuses on representations of words describing\nartifacts (such as \"mop\", \"pencil\", and \"whistle\"). The theory posits a causal\ngraph that relates the meanings of such words to the form, use, and history of\nthe objects to which they refer. We test LLMs using the same stimuli originally\nused by Chaigneau et al. (2004) to evaluate the theory in humans, and consider\na variety of prompt designs. Our experiments concern judgements about causal\noutcomes, object function, and object naming. We find no evidence that GPT-3\nencodes the causal structure hypothesized by HIPE, but do find evidence that\nGPT-4 encodes such structure. The results contribute to a growing body of\nresearch characterizing the representational capacity of large language models.", "published": "2023-05-24 02:03:23", "link": "http://arxiv.org/abs/2305.14630v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Iteratively Improving Biomedical Entity Linking and Event Extraction via\n  Hard Expectation-Maximization", "abstract": "Biomedical entity linking and event extraction are two crucial tasks to\nsupport text understanding and retrieval in the biomedical domain. These two\ntasks intrinsically benefit each other: entity linking disambiguates the\nbiomedical concepts by referring to external knowledge bases and the domain\nknowledge further provides additional clues to understand and extract the\nbiological processes, while event extraction identifies a key trigger and\nentities involved to describe each biological process which also captures the\nstructural context to better disambiguate the biomedical entities. However,\nprevious research typically solves these two tasks separately or in a pipeline,\nleading to error propagation. What's more, it's even more challenging to solve\nthese two tasks together as there is no existing dataset that contains\nannotations for both tasks. To solve these challenges, we propose joint\nbiomedical entity linking and event extraction by regarding the event\nstructures and entity references in knowledge bases as latent variables and\nupdating the two task-specific models in a hard Expectation-Maximization (EM)\nfashion: (1) predicting the missing variables for each partially annotated\ndataset based on the current two task-specific models, and (2) updating the\nparameters of each model on the corresponding pseudo completed dataset.\nExperimental results on two benchmark datasets: Genia 2011 for event extraction\nand BC4GO for entity linking, show that our joint framework significantly\nimproves the model for each individual task and outperforms the strong\nbaselines for both tasks. We will make the code and model checkpoints publicly\navailable once the paper is accepted.", "published": "2023-05-24 02:30:31", "link": "http://arxiv.org/abs/2305.14645v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scientific Opinion Summarization: Paper Meta-review Generation Dataset,\n  Methods, and Evaluation", "abstract": "Opinions in scientific research papers can be divergent, leading to\ncontroversies among reviewers. However, most existing datasets for opinion\nsummarization are centered around product reviews and assume that the analyzed\nopinions are non-controversial, failing to account for the variability seen in\nother contexts such as academic papers, political debates, or social media\ndiscussions. To address this gap, we propose the task of scientific opinion\nsummarization, where research paper reviews are synthesized into meta-reviews.\nTo facilitate this task, we introduce the ORSUM dataset covering 15,062 paper\nmeta-reviews and 57,536 paper reviews from 47 conferences. Furthermore, we\npropose the Checklist-guided Iterative Introspection approach, which breaks\ndown scientific opinion summarization into several stages, iteratively refining\nthe summary under the guidance of questions from a checklist. Our experiments\nshow that (1) human-written summaries do not always satisfy all necessary\ncriteria such as depth of discussion, and identifying consensus and controversy\nfor the specific domain, and (2) the combination of task decomposition and\niterative self-refinement shows strong potential for enhancing the opinions and\ncan be applied to other complex text generation using black-box LLMs.", "published": "2023-05-24 02:33:35", "link": "http://arxiv.org/abs/2305.14647v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Denoising Bottleneck with Mutual Information Maximization for Video\n  Multimodal Fusion", "abstract": "Video multimodal fusion aims to integrate multimodal signals in videos, such\nas visual, audio and text, to make a complementary prediction with multiple\nmodalities contents. However, unlike other image-text multimodal tasks, video\nhas longer multimodal sequences with more redundancy and noise in both visual\nand audio modalities. Prior denoising methods like forget gate are coarse in\nthe granularity of noise filtering. They often suppress the redundant and noisy\ninformation at the risk of losing critical information. Therefore, we propose a\ndenoising bottleneck fusion (DBF) model for fine-grained video multimodal\nfusion. On the one hand, we employ a bottleneck mechanism to filter out noise\nand redundancy with a restrained receptive field. On the other hand, we use a\nmutual information maximization module to regulate the filter-out module to\npreserve key information within different modalities. Our DBF model achieves\nsignificant improvement over current state-of-the-art baselines on multiple\nbenchmarks covering multimodal sentiment analysis and multimodal summarization\ntasks. It proves that our model can effectively capture salient features from\nnoisy and redundant video, audio, and text inputs. The code for this paper is\npublicly available at https://github.com/WSXRHFG/DBF.", "published": "2023-05-24 02:39:43", "link": "http://arxiv.org/abs/2305.14652v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluate What You Can't Evaluate: Unassessable Quality for Generated\n  Response", "abstract": "LLMs (large language models) such as ChatGPT have shown remarkable language\nunderstanding and generation capabilities. Although reference-free evaluators\nbased on LLMs show better human alignment than traditional reference-based\nevaluators, there are many challenges in using reference-free evaluators based\non LLMs. Reference-free evaluators are more suitable for open-ended examples\nwith different semantics responses. But not all examples are open-ended. For\nclosed-ended examples with unique correct semantic response, reference-free\nevaluators will still consider it high quality when giving a response that is\ninconsistent with the facts and the semantic of reference. In order to\ncomprehensively evaluate the reliability of evaluators based on LLMs, we\nconstruct two adversarial meta-evaluation dialogue generation datasets\nKdConv-ADV and DSTC7-ADV based on KdConv and DSTC7-AVSD, respectively. Compared\nto previous meta-evaluation benchmarks, KdConv-ADV and DSTC7-ADV are much more\nchallenging since they requires evaluators to be able to reasonably evaluate\nclosed-ended examples with the help of external knowledge or even its own\nknowledge. Empirical results show that the ability of LLMs to identify\nunreasonable responses is insufficient. There are risks in using eference-free\nevaluators based on LLMs to evaluate the quality of dialogue responses.", "published": "2023-05-24 02:52:48", "link": "http://arxiv.org/abs/2305.14658v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InteractiveIE: Towards Assessing the Strength of Human-AI Collaboration\n  in Improving the Performance of Information Extraction", "abstract": "Learning template based information extraction from documents is a crucial\nyet difficult task. Prior template-based IE approaches assume foreknowledge of\nthe domain templates; however, real-world IE do not have pre-defined schemas\nand it is a figure-out-as you go phenomena. To quickly bootstrap templates in a\nreal-world setting, we need to induce template slots from documents with zero\nor minimal supervision. Since the purpose of question answering intersect with\nthe goal of information extraction, we use automatic question generation to\ninduce template slots from the documents and investigate how a tiny amount of a\nproxy human-supervision on-the-fly (termed as InteractiveIE) can further boost\nthe performance. Extensive experiments on biomedical and legal documents, where\nobtaining training data is expensive, reveal encouraging trends of performance\nimprovement using InteractiveIE over AI-only baseline.", "published": "2023-05-24 02:53:22", "link": "http://arxiv.org/abs/2305.14659v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Complex Mathematical Symbol Definition Structures: A Dataset and Model\n  for Coordination Resolution in Definition Extraction", "abstract": "Mathematical symbol definition extraction is important for improving\nscholarly reading interfaces and scholarly information extraction (IE).\nHowever, the task poses several challenges: math symbols are difficult to\nprocess as they are not composed of natural language morphemes; and scholarly\npapers often contain sentences that require resolving complex coordinate\nstructures. We present SymDef, an English language dataset of 5,927 sentences\nfrom full-text scientific papers where each sentence is annotated with all\nmathematical symbols linked with their corresponding definitions. This dataset\nfocuses specifically on complex coordination structures such as \"respectively\"\nconstructions, which often contain overlapping definition spans. We also\nintroduce a new definition extraction method that masks mathematical symbols,\ncreates a copy of each sentence for each symbol, specifies a target symbol, and\npredicts its corresponding definition spans using slot filling. Our experiments\nshow that our definition extraction model significantly outperforms RoBERTa and\nother strong IE baseline systems by 10.9 points with a macro F1 score of 84.82.\nWith our dataset and model, we can detect complex definitions in scholarly\ndocuments to make scientific writing more readable.", "published": "2023-05-24 02:53:48", "link": "http://arxiv.org/abs/2305.14660v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "You Are What You Annotate: Towards Better Models through Annotator\n  Representations", "abstract": "Annotator disagreement is ubiquitous in natural language processing (NLP)\ntasks. There are multiple reasons for such disagreements, including the\nsubjectivity of the task, difficult cases, unclear guidelines, and so on.\nRather than simply aggregating labels to obtain data annotations, we instead\ntry to directly model the diverse perspectives of the annotators, and\nexplicitly account for annotators' idiosyncrasies in the modeling process by\ncreating representations for each annotator (annotator embeddings) and also\ntheir annotations (annotation embeddings). In addition, we propose TID-8, The\nInherent Disagreement - 8 dataset, a benchmark that consists of eight existing\nlanguage understanding datasets that have inherent annotator disagreement. We\ntest our approach on TID-8 and show that our approach helps models learn\nsignificantly better from disagreements on six different datasets in TID-8\nwhile increasing model size by fewer than 1% parameters. By capturing the\nunique tendencies and subjectivity of individual annotators through embeddings,\nour representations prime AI models to be inclusive of diverse viewpoints.", "published": "2023-05-24 03:06:13", "link": "http://arxiv.org/abs/2305.14663v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Diffusion Models in Natural Language Processing", "abstract": "This survey paper provides a comprehensive review of the use of diffusion\nmodels in natural language processing (NLP). Diffusion models are a class of\nmathematical models that aim to capture the diffusion of information or signals\nacross a network or manifold. In NLP, diffusion models have been used in a\nvariety of applications, such as natural language generation, sentiment\nanalysis, topic modeling, and machine translation. This paper discusses the\ndifferent formulations of diffusion models used in NLP, their strengths and\nlimitations, and their applications. We also perform a thorough comparison\nbetween diffusion models and alternative generative models, specifically\nhighlighting the autoregressive (AR) models, while also examining how diverse\narchitectures incorporate the Transformer in conjunction with diffusion models.\nCompared to AR models, diffusion models have significant advantages for\nparallel generation, text interpolation, token-level controls such as syntactic\nstructures and semantic contents, and robustness. Exploring further\npermutations of integrating Transformers into diffusion models would be a\nvaluable pursuit. Also, the development of multimodal diffusion models and\nlarge-scale diffusion language models with notable capabilities for few-shot\nlearning would be important directions for the future advance of diffusion\nmodels in NLP.", "published": "2023-05-24 03:25:32", "link": "http://arxiv.org/abs/2305.14671v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GRILL: Grounded Vision-language Pre-training via Aligning Text and Image\n  Regions", "abstract": "Generalization to unseen tasks is an important ability for few-shot learners\nto achieve better zero-/few-shot performance on diverse tasks. However, such\ngeneralization to vision-language tasks including grounding and generation\ntasks has been under-explored; existing few-shot VL models struggle to handle\ntasks that involve object grounding and multiple images such as visual\ncommonsense reasoning or NLVR2. In this paper, we introduce GRILL, GRounded\nvIsion Language aLigning, a novel VL model that can be generalized to diverse\ntasks including visual question answering, captioning, and grounding tasks with\nno or very few training instances. Specifically, GRILL learns object grounding\nand localization by exploiting object-text alignments, which enables it to\ntransfer to grounding tasks in a zero-/few-shot fashion. We evaluate our model\non various zero-/few-shot VL tasks and show that it consistently surpasses the\nstate-of-the-art few-shot methods.", "published": "2023-05-24 03:33:21", "link": "http://arxiv.org/abs/2305.14676v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emergent inabilities? Inverse scaling over the course of pretraining", "abstract": "Does inverse scaling only occur as a function of model size, or can it also\noccur over the course of training? We carry out an exploratory study\ninvestigating whether the performance of language models on specific tasks can\ndecrease (while general performance remains high) during training on the\nlanguage modeling task. We find 8 tasks on which Pythia 12B (Biderman et al.,\n2023) shows decreased performance over the course of training. Five of these\ntasks (TruthfulQA-MC1, TruthfulQA-MC2, Hindsight Neglect, Memo Trap, and\nPattern Match Suppression) additionally show a consistent relationship whereby\nlarger language models show a greater decrease in performance the more they are\ntrained, despite showing standard (positive) scaling overall. This highlights\nthe importance of testing performance at all relevant benchmarks any time\nmodels are trained on additional data, even if their overall performance\nimproves", "published": "2023-05-24 03:42:43", "link": "http://arxiv.org/abs/2305.14681v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TACR: A Table-alignment-based Cell-selection and Reasoning Model for\n  Hybrid Question-Answering", "abstract": "Hybrid Question-Answering (HQA), which targets reasoning over tables and\npassages linked from table cells, has witnessed significant research in recent\nyears. A common challenge in HQA and other passage-table QA datasets is that it\nis generally unrealistic to iterate over all table rows, columns, and linked\npassages to retrieve evidence. Such a challenge made it difficult for previous\nstudies to show their reasoning ability in retrieving answers. To bridge this\ngap, we propose a novel Table-alignment-based Cell-selection and Reasoning\nmodel (TACR) for hybrid text and table QA, evaluated on the HybridQA and\nWikiTableQuestions datasets. In evidence retrieval, we design a\ntable-question-alignment enhanced cell-selection method to retrieve\nfine-grained evidence. In answer reasoning, we incorporate a QA module that\ntreats the row containing selected cells as context. Experimental results over\nthe HybridQA and WikiTableQuestions (WTQ) datasets show that TACR achieves\nstate-of-the-art results on cell selection and outperforms fine-grained\nevidence retrieval baselines on HybridQA, while achieving competitive\nperformance on WTQ. We also conducted a detailed analysis to demonstrate that\nbeing able to align questions to tables in the cell-selection stage can result\nin important gains from experiments of over 90\\% table row and column selection\naccuracy, meanwhile also improving output explainability.", "published": "2023-05-24 03:42:44", "link": "http://arxiv.org/abs/2305.14682v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SELFOOD: Self-Supervised Out-Of-Distribution Detection via Learning to\n  Rank", "abstract": "Deep neural classifiers trained with cross-entropy loss (CE loss) often\nsuffer from poor calibration, necessitating the task of out-of-distribution\n(OOD) detection. Traditional supervised OOD detection methods require expensive\nmanual annotation of in-distribution and OOD samples. To address the annotation\nbottleneck, we introduce SELFOOD, a self-supervised OOD detection method that\nrequires only in-distribution samples as supervision. We cast OOD detection as\nan inter-document intra-label (IDIL) ranking problem and train the classifier\nwith our pairwise ranking loss, referred to as IDIL loss. Specifically, given a\nset of in-distribution documents and their labels, for each label, we train the\nclassifier to rank the softmax scores of documents belonging to that label to\nbe higher than the scores of documents that belong to other labels. Unlike CE\nloss, our IDIL loss function reaches zero when the desired confidence ranking\nis achieved and gradients are backpropagated to decrease probabilities\nassociated with incorrect labels rather than continuously increasing the\nprobability of the correct label. Extensive experiments with several\nclassifiers on multiple classification datasets demonstrate the effectiveness\nof our method in both coarse- and fine-grained settings.", "published": "2023-05-24 04:01:27", "link": "http://arxiv.org/abs/2305.14696v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DecipherPref: Analyzing Influential Factors in Human Preference\n  Judgments via GPT-4", "abstract": "Human preference judgments are pivotal in guiding large language models\n(LLMs) to produce outputs that align with human values. Human evaluations are\nalso used in summarization tasks to compare outputs from various systems,\ncomplementing existing automatic metrics. Despite their significance, however,\nthere has been limited research probing these pairwise or $k$-wise comparisons.\nThe collective impact and relative importance of factors such as output length,\ninformativeness, fluency, and factual consistency are still not well\nunderstood. It is also unclear if there are other hidden factors influencing\nhuman judgments. In this paper, we conduct an in-depth examination of a\ncollection of pairwise human judgments released by OpenAI. Utilizing the\nBradley-Terry-Luce (BTL) model, we reveal the inherent preferences embedded in\nthese human judgments. We find that the most favored factors vary across tasks\nand genres, whereas the least favored factors tend to be consistent, e.g.,\noutputs are too brief, contain excessive off-focus content or hallucinated\nfacts. Our findings have implications on the construction of balanced datasets\nin human preference evaluations, which is a crucial step in shaping the\nbehaviors of future LLMs.", "published": "2023-05-24 04:13:15", "link": "http://arxiv.org/abs/2305.14702v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for\n  Large Language Models", "abstract": "Sparse Mixture-of-Experts (MoE) is a neural architecture design that can be\nutilized to add learnable parameters to Large Language Models (LLMs) without\nincreasing inference cost. Instruction tuning is a technique for training LLMs\nto follow instructions. We advocate combining these two approaches, as we find\nthat MoE models benefit more from instruction tuning than dense models. In\nparticular, we conduct empirical studies across three experimental setups: (i)\nDirect finetuning on individual downstream tasks devoid of instruction tuning;\n(ii) Instructiontuning followed by in-context few-shot or zero-shot\ngeneralization on downstream tasks; and (iii) Instruction tuning supplemented\nby further finetuning on individual downstream tasks. In the first scenario,\nMoE models overall underperform dense models of identical computational\ncapacity. This narrative, however, dramatically changes with the introduction\nof instruction tuning (second and third scenario), used independently or in\nconjunction with task-specific finetuning. Our most powerful model,\nFLAN-MOE-32B, surpasses the performance of FLAN-PALM-62B on four benchmark\ntasks, while using only a third of the FLOPs. The advancements embodied\nbyFLAN-MOE inspire a reevaluation of the design principles of large-scale,\nhigh-performance language models in the framework of task-agnostic learning.", "published": "2023-05-24 04:22:26", "link": "http://arxiv.org/abs/2305.14705v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gender Biases in Automatic Evaluation Metrics for Image Captioning", "abstract": "Model-based evaluation metrics (e.g., CLIPScore and GPTScore) have\ndemonstrated decent correlations with human judgments in various language\ngeneration tasks. However, their impact on fairness remains largely unexplored.\nIt is widely recognized that pretrained models can inadvertently encode\nsocietal biases, thus employing these models for evaluation purposes may\ninadvertently perpetuate and amplify biases. For example, an evaluation metric\nmay favor the caption \"a woman is calculating an account book\" over \"a man is\ncalculating an account book,\" even if the image only shows male accountants. In\nthis paper, we conduct a systematic study of gender biases in model-based\nautomatic evaluation metrics for image captioning tasks. We start by curating a\ndataset comprising profession, activity, and object concepts associated with\nstereotypical gender associations. Then, we demonstrate the negative\nconsequences of using these biased metrics, including the inability to\ndifferentiate between biased and unbiased generations, as well as the\npropagation of biases to generation models through reinforcement learning.\nFinally, we present a simple and effective way to mitigate the metric bias\nwithout hurting the correlations with human judgments. Our dataset and\nframework lay the foundation for understanding the potential harm of\nmodel-based evaluation metrics, and facilitate future works to develop more\ninclusive evaluation metrics.", "published": "2023-05-24 04:27:40", "link": "http://arxiv.org/abs/2305.14711v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GlobalBench: A Benchmark for Global Progress in Natural Language\n  Processing", "abstract": "Despite the major advances in NLP, significant disparities in NLP system\nperformance across languages still exist. Arguably, these are due to uneven\nresource allocation and sub-optimal incentives to work on less resourced\nlanguages. To track and further incentivize the global development of equitable\nlanguage technology, we introduce GlobalBench. Prior multilingual benchmarks\nare static and have focused on a limited number of tasks and languages. In\ncontrast, GlobalBench is an ever-expanding collection that aims to dynamically\ntrack progress on all NLP datasets in all languages. Rather than solely\nmeasuring accuracy, GlobalBench also tracks the estimated per-speaker utility\nand equity of technology across all languages, providing a multi-faceted view\nof how language technology is serving people of the world. Furthermore,\nGlobalBench is designed to identify the most under-served languages, and\nrewards research efforts directed towards those languages. At present, the most\nunder-served languages are the ones with a relatively high population, but\nnonetheless overlooked by composite multilingual benchmarks (like Punjabi,\nPortuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190\nlanguages, and has 1,128 system submissions spanning 62 languages.", "published": "2023-05-24 04:36:32", "link": "http://arxiv.org/abs/2305.14716v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leftover Lunch: Advantage-based Offline Reinforcement Learning for\n  Language Models", "abstract": "Reinforcement Learning with Human Feedback (RLHF) is the most prominent\nmethod for Language Model (LM) alignment. However, RLHF is an unstable and\ndata-hungry process that continually requires new high-quality LM-generated\ndata for finetuning. We introduce Advantage-Leftover Lunch RL (A-LoL), a new\nclass of offline policy gradient algorithms that enable RL training on any\npre-existing data. By assuming the entire LM output sequence as a single\naction, A-LoL allows incorporating sequence-level classifiers or human-designed\nscoring functions as rewards. Subsequently, by using LM's value estimate, A-LoL\nonly trains on positive advantage (leftover) data points, making it resilient\nto noise. Overall, A-LoL is an easy-to-implement, sample-efficient, and stable\nLM training recipe.\n  We demonstrate the effectiveness of A-LoL and its variants with a set of four\ndifferent language generation tasks. We compare against both online RL (PPO)\nand recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL\nbaselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant\n(HHA), LMs trained with A-LoL methods achieve the highest diversity while also\nbeing rated more safe and helpful than the baselines according to humans.\nAdditionally, in the remaining three tasks, A-LoL could optimize multiple\ndistinct reward functions even when using noisy or suboptimal training data.\n  We also release our experimental code. https://github.com/abaheti95/LoL-RL", "published": "2023-05-24 04:42:17", "link": "http://arxiv.org/abs/2305.14718v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CuRIAM: Corpus re Interpretation and Metalanguage in U.S. Supreme Court\n  Opinions", "abstract": "Most judicial decisions involve the interpretation of legal texts; as such,\njudicial opinion requires the use of language as a medium to comment on or draw\nattention to other language. Language used this way is called metalanguage. We\ndevelop an annotation schema for categorizing types of legal metalanguage and\napply our schema to a set of U.S. Supreme Court opinions, yielding a corpus\ntotaling 59k tokens. We remark on several patterns observed in the kinds of\nmetalanguage used by the justices.", "published": "2023-05-24 04:47:55", "link": "http://arxiv.org/abs/2305.14719v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes", "abstract": "We propose attribute-aware multimodal entity linking, where the input is a\nmention described with a text and image, and the goal is to predict the\ncorresponding target entity from a multimodal knowledge base (KB) where each\nentity is also described with a text description, a visual image and a set of\nattributes and values. To support this research, we construct AMELI, a\nlarge-scale dataset consisting of 18,472 reviews and 35,598 products. To\nestablish baseline performance on AMELI, we experiment with the current\nstate-of-the-art multimodal entity linking approaches and our enhanced\nattribute-aware model and demonstrate the importance of incorporating the\nattribute information into the entity linking process. To be best of our\nknowledge, we are the first to build benchmark dataset and solutions for the\nattribute-aware multimodal entity linking task. Datasets and codes will be made\npublicly available.", "published": "2023-05-24 05:01:48", "link": "http://arxiv.org/abs/2305.14725v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language\n  Representations", "abstract": "Although deep language representations have become the dominant form of\nlanguage featurization in recent years, in many settings it is important to\nunderstand a model's decision-making process. This necessitates not only an\ninterpretable model but also interpretable features. In particular, language\nmust be featurized in a way that is interpretable while still characterizing\nthe original text well. We present SenteCon, a method for introducing human\ninterpretability in deep language representations. Given a passage of text,\nSenteCon encodes the text as a layer of interpretable categories in which each\ndimension corresponds to the relevance of a specific category. Our empirical\nevaluations indicate that encoding language with SenteCon provides high-level\ninterpretability at little to no cost to predictive performance on downstream\ntasks. Moreover, we find that SenteCon outperforms existing interpretable\nlanguage representations with respect to both its downstream performance and\nits agreement with human characterizations of the text.", "published": "2023-05-24 05:06:28", "link": "http://arxiv.org/abs/2305.14728v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advancements in Arabic Grammatical Error Detection and Correction: An\n  Empirical Investigation", "abstract": "Grammatical error correction (GEC) is a well-explored problem in English with\nmany existing models and datasets. However, research on GEC in morphologically\nrich languages has been limited due to challenges such as data scarcity and\nlanguage complexity. In this paper, we present the first results on Arabic GEC\nusing two newly developed Transformer-based pretrained sequence-to-sequence\nmodels. We also define the task of multi-class Arabic grammatical error\ndetection (GED) and present the first results on multi-class Arabic GED. We\nshow that using GED information as an auxiliary input in GEC models improves\nGEC performance across three datasets spanning different genres. Moreover, we\nalso investigate the use of contextual morphological preprocessing in aiding\nGEC systems. Our models achieve SOTA results on two Arabic GEC shared task\ndatasets and establish a strong benchmark on a recently created dataset. We\nmake our code, data, and pretrained models publicly available.", "published": "2023-05-24 05:12:58", "link": "http://arxiv.org/abs/2305.14734v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Trusting Your Evidence: Hallucinate Less with Context-aware Decoding", "abstract": "Language models (LMs) often struggle to pay enough attention to the input\ncontext, and generate texts that are unfaithful or contain hallucinations. To\nmitigate this issue, we present context-aware decoding (CAD), which follows a\ncontrastive output distribution that amplifies the difference between the\noutput probabilities when a model is used with and without context. Our\nexperiments show that CAD, without additional training, significantly improves\nthe faithfulness of different LM families, including OPT, GPT, LLaMA and\nFLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality\nmetrics). Furthermore, CAD is particularly effective in overriding a model's\nprior knowledge when it contradicts the provided context, leading to\nsubstantial improvements in tasks where resolving the knowledge conflict is\nessential.", "published": "2023-05-24 05:19:15", "link": "http://arxiv.org/abs/2305.14739v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mastering the ABCDs of Complex Questions: Answer-Based Claim\n  Decomposition for Fine-grained Self-Evaluation", "abstract": "When answering complex questions, large language models (LLMs) may produce\nanswers that do not satisfy all criteria of the question. While existing\nself-evaluation techniques aim to detect if such answers are correct, these\ntechniques are unable to determine which criteria of the question are satisfied\nby the generated answers. To address this issue, we propose answer-based claim\ndecomposition (ABCD), a prompting strategy that decomposes questions into a\nseries of true/false claims that can be used to verify which criteria of the\ninput question an answer satisfies. Using the decomposed ABCD claims, we\nperform fine-grained self-evaluation. Through preliminary experiments on three\ndatasets, including a newly-collected challenge dataset ObscureQA, we find that\nGPT-3.5 has some ability to determine to what extent its answer satisfies the\ncriteria of the input question, and can give insights into the errors and\nknowledge gaps of the model.", "published": "2023-05-24 05:53:11", "link": "http://arxiv.org/abs/2305.14750v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Psychological Metrics for Dialog System Evaluation", "abstract": "We present metrics for evaluating dialog systems through a\npsychologically-grounded \"human\" lens in which conversational agents express a\ndiversity of both states (e.g., emotion) and traits (e.g., personality), just\nas people do. We present five interpretable metrics from established psychology\nthat are fundamental to human communication and relationships: emotional\nentropy, linguistic style and emotion matching, agreeableness, and empathy.\nThese metrics can be applied (1) across dialogs and (2) on turns within\ndialogs. The psychological metrics are compared against seven state-of-the-art\ntraditional metrics (e.g., BARTScore and BLEURT) on seven standard dialog\nsystem data sets. We also introduce a novel data set, the Three Bot Dialog\nEvaluation Corpus, which consists of annotated conversations from ChatGPT,\nGPT-3, and BlenderBot. We demonstrate that our proposed metrics offer novel\ninformation; they are uncorrelated with traditional metrics, can be used to\nmeaningfully compare dialog systems, and lead to increased accuracy (beyond\nexisting traditional metrics) in predicting crowd-sourced dialog judgements.\nThe interpretability and unique signal of our psychological metrics make them a\nvaluable tool for evaluating and improving dialog systems.", "published": "2023-05-24 06:02:32", "link": "http://arxiv.org/abs/2305.14757v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bi-Drop: Enhancing Fine-tuning Generalization via Synchronous sub-net\n  Estimation and Optimization", "abstract": "Pretrained language models have achieved remarkable success in natural\nlanguage understanding. However, fine-tuning pretrained models on limited\ntraining data tends to overfit and thus diminish performance. This paper\npresents Bi-Drop, a fine-tuning strategy that selectively updates model\nparameters using gradients from various sub-nets dynamically generated by\ndropout. The sub-net estimation of Bi-Drop is performed in an in-batch manner,\nso it overcomes the problem of hysteresis in sub-net updating, which is\npossessed by previous methods that perform asynchronous sub-net estimation.\nAlso, Bi-Drop needs only one mini-batch to estimate the sub-net so it achieves\nhigher utility of training data. Experiments on the GLUE benchmark demonstrate\nthat Bi-Drop consistently outperforms previous fine-tuning methods.\nFurthermore, empirical results also show that Bi-Drop exhibits excellent\ngeneralization ability and robustness for domain transfer, data imbalance, and\nlow-resource scenarios.", "published": "2023-05-24 06:09:26", "link": "http://arxiv.org/abs/2305.14760v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniChart: A Universal Vision-language Pretrained Model for Chart\n  Comprehension and Reasoning", "abstract": "Charts are very popular for analyzing data, visualizing key insights and\nanswering complex reasoning questions about data. To facilitate chart-based\ndata analysis using natural language, several downstream tasks have been\nintroduced recently such as chart question answering and chart summarization.\nHowever, most of the methods that solve these tasks use pretraining on language\nor vision-language tasks that do not attempt to explicitly model the structure\nof the charts (e.g., how data is visually encoded and how chart elements are\nrelated to each other). To address this, we first build a large corpus of\ncharts covering a wide variety of topics and visual styles. We then present\nUniChart, a pretrained model for chart comprehension and reasoning. UniChart\nencodes the relevant text, data, and visual elements of charts and then uses a\nchart-grounded text decoder to generate the expected output in natural\nlanguage. We propose several chart-specific pretraining tasks that include: (i)\nlow-level tasks to extract the visual elements (e.g., bars, lines) and data\nfrom charts, and (ii) high-level tasks to acquire chart understanding and\nreasoning skills. We find that pretraining the model on a large corpus with\nchart-specific low- and high-level tasks followed by finetuning on three\ndown-streaming tasks results in state-of-the-art performance on three\ndownstream tasks.", "published": "2023-05-24 06:11:17", "link": "http://arxiv.org/abs/2305.14761v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in\n  Large Language Models", "abstract": "The escalating debate on AI's capabilities warrants developing reliable\nmetrics to assess machine \"intelligence\". Recently, many anecdotal examples\nwere used to suggest that newer large language models (LLMs) like ChatGPT and\nGPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached\nconflicting conclusions regarding those abilities. We investigate the extent of\nLLMs' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs\nexhibit certain N-ToM abilities, this behavior is far from being robust. We\nfurther examine the factors impacting performance on N-ToM tasks and discover\nthat LLMs struggle with adversarial examples, indicating reliance on shallow\nheuristics rather than robust ToM abilities. We caution against drawing\nconclusions from anecdotal examples, limited benchmark testing, and using\nhuman-designed psychological tests to evaluate models.", "published": "2023-05-24 06:14:31", "link": "http://arxiv.org/abs/2305.14763v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Allies: Prompting Large Language Model with Beam Search", "abstract": "With the advance of large language models (LLMs), the research field of LLM\napplications becomes more and more popular and the idea of constructing\npipelines to accomplish complex tasks by stacking LLM API calls come true.\nHowever, this kind of methods face two limitations: narrow information coverage\nand low fault tolerance. In this work, we propose a novel method called ALLIES.\nGiven an input query, ALLIES leverages LLMs to iteratively generate new queries\nrelated to the original query, enabling an iterative reasoning process. By\niteratively refining and expanding the scope of the original query, ALLIES\ncaptures and utilizes hidden knowledge that may not be directly obtainable\nthrough retrieval. We take zero-shot open-domain question answering (ODQA) as\nan application scene and evaluate ALLIES on the widely-used benchmarks, such as\nNQ, WebQ and TriviaQA. The experimental results demonstrate that ALLIES\nsignificantly outperforms other zero-shot baselines, indicating its\neffectiveness in tackling those challenges. Our code is available in\nhttps://github.com/microsoft/SimXNS/tree/main/ALLIES.", "published": "2023-05-24 06:16:44", "link": "http://arxiv.org/abs/2305.14766v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Natural Language Explanations to Rescale Human Judgments", "abstract": "The rise of large language models (LLMs) has brought a critical need for\nhigh-quality human-labeled data, particularly for processes like human feedback\nand evaluation. A common practice is to label data via consensus annotation\nover human judgments. However, annotators' judgments for subjective tasks can\ndiffer in many ways: they may reflect different qualitative judgments about an\nexample, and they may be mapped to a labeling scheme in different ways. We show\nthat these nuances can be captured by natural language explanations, and\npropose a method to rescale ordinal annotations and explanations using LLMs.\nSpecifically, we feed annotators' Likert ratings and corresponding explanations\ninto an LLM and prompt it to produce a numeric score anchored in a scoring\nrubric. These scores should reflect the annotators' underlying assessments of\nthe example. The rubric can be designed or modified after annotation, and\ninclude distinctions that may not have been known when the original error\ntaxonomy was devised. We explore our technique in the context of rating system\noutputs for a document-grounded question answering task, where LLMs achieve\nnear-human performance. Our method rescales the raw judgments without impacting\nagreement and brings the scores closer to human judgments grounded in the same\nscoring rubric.", "published": "2023-05-24 06:19:14", "link": "http://arxiv.org/abs/2305.14770v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "David helps Goliath: Inference-Time Collaboration Between Small\n  Specialized and Large General Diffusion LMs", "abstract": "Diffusion-based language models are emerging as a promising alternative to\nautoregressive LMs: they approach the competence of autoregressive LMs while\noffering nuanced controllability at inference time. While autoregressive LMs\nhave benefited immensely from scaling and instruction-based learning, existing\nstudies of diffusion LMs have been conducted on a smaller scale. Starting with\na recently proposed diffusion model SSD-LM, in this work we first explore\nmethods to scale it from 0.4B to 13B parameters, proposing techniques to\nimprove its training and inference efficiency, and to finetune the model to\nfollow instructions. Armed with a more powerful, general purpose diffusion LM,\nwe introduce the primary contribution of this work -- SSD-2 -- an approach to\neasily ensemble at inference time a large general-purpose diffusion LM with\nsmaller, but specialized and contextualized diffusion LMs. We show that SSD-2\nfacilitates novel ensembles with 100x smaller models that can be customized and\ndeployed by individual users. We find that compared to autoregressive models,\nthe collaboration between diffusion LMs is more effective, leading to\nhigher-quality model responses due to their ability to dynamically incorporate\nbi-directional contexts.", "published": "2023-05-24 06:22:14", "link": "http://arxiv.org/abs/2305.14771v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Question Answering Framework for Decontextualizing User-facing\n  Snippets from Scientific Documents", "abstract": "Many real-world applications (e.g., note taking, search) require extracting a\nsentence or paragraph from a document and showing that snippet to a human\noutside of the source document. Yet, users may find snippets difficult to\nunderstand as they lack context from the original document. In this work, we\nuse language models to rewrite snippets from scientific documents to be read on\ntheir own. First, we define the requirements and challenges for this\nuser-facing decontextualization task, such as clarifying where edits occur and\nhandling references to other documents. Second, we propose a framework that\ndecomposes the task into three stages: question generation, question answering,\nand rewriting. Using this framework, we collect gold decontextualizations from\nexperienced scientific article readers. We then conduct a range of experiments\nacross state-of-the-art commercial and open-source language models to identify\nhow to best provide missing-but-relevant information to models for our task.\nFinally, we develop QaDecontext, a simple prompting strategy inspired by our\nframework that improves over end-to-end prompting. We conclude with analysis\nthat finds, while rewriting is easy, question generation and answering remain\nchallenging for today's models.", "published": "2023-05-24 06:23:02", "link": "http://arxiv.org/abs/2305.14772v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disentangled Phonetic Representation for Chinese Spelling Correction", "abstract": "Chinese Spelling Correction (CSC) aims to detect and correct erroneous\ncharacters in Chinese texts. Although efforts have been made to introduce\nphonetic information (Hanyu Pinyin) in this task, they typically merge phonetic\nrepresentations with character representations, which tends to weaken the\nrepresentation effect of normal texts. In this work, we propose to disentangle\nthe two types of features to allow for direct interaction between textual and\nphonetic information. To learn useful phonetic representations, we introduce a\npinyin-to-character objective to ask the model to predict the correct\ncharacters based solely on phonetic information, where a separation mask is\nimposed to disable attention from phonetic input to text. To avoid overfitting\nthe phonetics, we further design a self-distillation module to ensure that\nsemantic information plays a major role in the prediction. Extensive\nexperiments on three CSC benchmarks demonstrate the superiority of our method\nin using phonetic information.", "published": "2023-05-24 06:39:12", "link": "http://arxiv.org/abs/2305.14783v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting Language Models to Compress Contexts", "abstract": "Transformer-based language models (LMs) are powerful and widely-applicable\ntools, but their usefulness is constrained by a finite context window and the\nexpensive computational cost of processing long text documents. We propose to\nadapt pre-trained LMs into AutoCompressors. These language models are capable\nof compressing long contexts into compact summary vectors, which are then\naccessible to the model as soft prompts. Summary vectors are trained with an\nunsupervised objective, whereby long documents are processed in segments, and\nsummary vectors from all previous segments are used in language modeling. We\nfine-tune OPT and Llama-2 models on sequences of up to 30,720 tokens and show\nthat AutoCompressors can utilize long contexts to improve perplexity. We\nevaluate AutoCompressors on in-context learning by compressing task\ndemonstrations and find that summary vectors are good substitutes for\nplain-text demonstrations, increasing accuracy while reducing inference costs.\nFinally, we explore the benefits of pre-computing summary vectors for large\ncorpora by applying summary vectors to retrievalaugmented language modeling and\na passage re-ranking task. Overall, AutoCompressors emerge as a simple and\ninexpensive solution to extend the context window of LMs while speeding up\ninference over long contexts.", "published": "2023-05-24 06:42:44", "link": "http://arxiv.org/abs/2305.14788v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompting Large Language Models for Counterfactual Generation: An\n  Empirical Study", "abstract": "Large language models (LLMs) have made remarkable progress in a wide range of\nnatural language understanding and generation tasks. However, their ability to\ngenerate counterfactuals has not been examined systematically. To bridge this\ngap, we present a comprehensive evaluation framework on various types of NLU\ntasks, which covers all key factors in determining LLMs' capability of\ngenerating counterfactuals. Based on this framework, we 1) investigate the\nstrengths and weaknesses of LLMs as the counterfactual generator, and 2)\ndisclose the factors that affect LLMs when generating counterfactuals,\nincluding both the intrinsic properties of LLMs and prompt designing. The\nresults show that, though LLMs are promising in most cases, they face\nchallenges in complex tasks like RE since they are bounded by task-specific\nperformance, entity constraints, and inherent selection bias. We also find that\nalignment techniques, e.g., instruction-tuning and reinforcement learning from\nhuman feedback, may potentially enhance the counterfactual generation ability\nof LLMs. On the contrary, simply increasing the parameter size does not yield\nthe desired improvements. Besides, from the perspective of prompt designing,\ntask guidelines unsurprisingly play an important role. However, the\nchain-of-thought approach does not always help due to inconsistency issues.", "published": "2023-05-24 06:44:32", "link": "http://arxiv.org/abs/2305.14791v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Faithful Low-Resource Data-to-Text Generation through Cycle Training", "abstract": "Methods to generate text from structured data have advanced significantly in\nrecent years, primarily due to fine-tuning of pre-trained language models on\nlarge datasets. However, such models can fail to produce output faithful to the\ninput data, particularly on out-of-domain data. Sufficient annotated data is\noften not available for specific domains, leading us to seek an unsupervised\napproach to improve the faithfulness of output text. Since the problem is\nfundamentally one of consistency between the representations of the structured\ndata and text, we evaluate the effectiveness of cycle training in this work.\nCycle training uses two models which are inverses of each other: one that\ngenerates text from structured data, and one which generates the structured\ndata from natural language text. We show that cycle training, when initialized\nwith a small amount of supervised data (100 samples in our case), achieves\nnearly the same performance as fully supervised approaches for the data-to-text\ngeneration task on the WebNLG, E2E, WTQ, and WSQL datasets. We perform\nextensive empirical analysis with automated evaluation metrics and a newly\ndesigned human evaluation schema to reveal different cycle training strategies'\neffectiveness of reducing various types of generation errors. Our code is\npublicly available at https://github.com/Edillower/CycleNLG.", "published": "2023-05-24 06:44:42", "link": "http://arxiv.org/abs/2305.14793v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop\n  Questions", "abstract": "The information stored in large language models (LLMs) falls out of date\nquickly, and retraining from scratch is often not an option. This has recently\ngiven rise to a range of techniques for injecting new facts through updating\nmodel weights. Current evaluation paradigms are extremely limited, mainly\nvalidating the recall of edited facts, but changing one fact should cause\nrippling changes to the model's related beliefs. If we edit the UK Prime\nMinister to now be Rishi Sunak, then we should get a different answer to Who is\nmarried to the British Prime Minister? In this work, we present a benchmark,\nMQuAKE (Multi-hop Question Answering for Knowledge Editing), comprising\nmulti-hop questions that assess whether edited models correctly answer\nquestions where the answer should change as an entailed consequence of edited\nfacts. While we find that current knowledge-editing approaches can recall\nedited facts accurately, they fail catastrophically on the constructed\nmulti-hop questions. We thus propose a simple memory-based approach, MeLLo,\nwhich stores all edited facts externally while prompting the language model\niteratively to generate answers that are consistent with the edited facts.\nWhile MQuAKE remains challenging, we show that MeLLo scales well with LLMs\n(e.g., OpenAI GPT-3.5-turbo) and outperforms previous model editors by a large\nmargin.", "published": "2023-05-24 06:48:41", "link": "http://arxiv.org/abs/2305.14795v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Estimating Large Language Model Capabilities without Labeled Test Data", "abstract": "Large Language Models (LLMs) have the impressive ability to perform\nin-context learning (ICL) from only a few examples, but the success of ICL\nvaries widely from task to task. Thus, it is important to quickly determine\nwhether ICL is applicable to a new task, but directly evaluating ICL accuracy\ncan be expensive in situations where test data is expensive to annotate -- the\nexact situations where ICL is most appealing. In this paper, we propose the\ntask of ICL accuracy estimation, in which we predict the accuracy of an LLM\nwhen doing in-context learning on a new task given only unlabeled test data for\nthat task. To perform ICL accuracy estimation, we propose a method that trains\na meta-model using LLM confidence scores as features. We compare our method to\nseveral strong accuracy estimation baselines on a new benchmark that covers 4\nLLMs and 3 task collections. The meta-model improves over all baselines across\n8 out of 12 settings and achieves the same estimation performance as directly\nevaluating on 40 collected labeled test examples per task. At the same time, no\nexisting approach provides an accurate and reliable ICL accuracy estimation in\nevery setting, highlighting the need for better ways to measure the uncertainty\nof LLM predictions.", "published": "2023-05-24 06:55:09", "link": "http://arxiv.org/abs/2305.14802v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AWESOME: GPU Memory-constrained Long Document Summarization using Memory\n  Mechanism and Global Salient Content", "abstract": "Long document summarization systems are critical for domains with lengthy and\njargonladen text, yet they present significant challenges to researchers and\ndevelopers with limited computing resources. Existing solutions mainly focus on\nefficient attentions or divide-and-conquer strategies. The former reduces\ntheoretical time complexity, but is still memory-heavy. The latter methods\nsacrifice global context, leading to uninformative and incoherent summaries.\nThis work aims to leverage the memory-efficient nature of divide-and-conquer\nmethods while preserving global context. Concretely, our framework AWESOME uses\ntwo novel mechanisms: (1) External memory mechanisms track previously encoded\ndocument segments and their corresponding summaries, to enhance global document\nunderstanding and summary coherence. (2) Global salient content is further\nidentified beforehand to augment each document segment to support its\nsummarization. Extensive experiments on diverse genres of text, including\ngovernment reports, transcripts, scientific papers, and novels, show that\nAWESOME produces summaries with improved informativeness, faithfulness, and\ncoherence than competitive baselines on longer documents, while having a\nsmaller GPU memory footprint.", "published": "2023-05-24 07:00:00", "link": "http://arxiv.org/abs/2305.14806v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating Temporal Misalignment by Discarding Outdated Facts", "abstract": "While large language models are able to retain vast amounts of world\nknowledge seen during pretraining, such knowledge is prone to going out of date\nand is nontrivial to update. Furthermore, these models are often used under\ntemporal misalignment, tasked with answering questions about the present,\ndespite having only been trained on data collected in the past. To mitigate the\neffects of temporal misalignment, we propose fact duration prediction: the task\nof predicting how long a given fact will remain true. In our experiments, we\ndemonstrate that identifying which facts are prone to rapid change can help\nmodels avoid reciting outdated information and determine which predictions\nrequire seeking out up-to-date knowledge sources. We also show how modeling\nfact duration improves calibration for knowledge-intensive tasks, such as\nopen-retrieval question answering, under temporal misalignment, by discarding\nvolatile facts. Our data and code are released publicly at\nhttps://github.com/mikejqzhang/mitigating_misalignment.", "published": "2023-05-24 07:30:08", "link": "http://arxiv.org/abs/2305.14824v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-training Intent-Aware Encoders for Zero- and Few-Shot Intent\n  Classification", "abstract": "Intent classification (IC) plays an important role in task-oriented dialogue\nsystems. However, IC models often generalize poorly when training without\nsufficient annotated examples for each user intent. We propose a novel\npre-training method for text encoders that uses contrastive learning with\nintent psuedo-labels to produce embeddings that are well-suited for IC tasks,\nreducing the need for manual annotations. By applying this pre-training\nstrategy, we also introduce Pre-trained Intent-aware Encoder (PIE), which is\ndesigned to align encodings of utterances with their intent names.\nSpecifically, we first train a tagger to identify key phrases within utterances\nthat are crucial for interpreting intents. We then use these extracted phrases\nto create examples for pre-training a text encoder in a contrastive manner. As\na result, our PIE model achieves up to 5.4% and 4.0% higher accuracy than the\nprevious state-of-the-art text encoder for the N-way zero- and one-shot\nsettings on four IC datasets.", "published": "2023-05-24 07:34:32", "link": "http://arxiv.org/abs/2305.14827v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SummIt: Iterative Text Summarization via ChatGPT", "abstract": "Text summarization systems have made significant progress in recent years,\nbut typically generate summaries in one single step. However, the one-shot\nsummarization setting is sometimes inadequate, as the generated summary may\ncontain hallucinations or overlook essential details related to the reader's\ninterests. This paper addresses this limitation by proposing SummIt, an\niterative text summarization framework based on large language models like\nChatGPT. Our framework enables the model to refine the generated summary\niteratively through self-evaluation and feedback, resembling humans' iterative\nprocess when drafting and revising summaries. Furthermore, we explore the\npotential benefits of integrating knowledge and topic extractors into the\nframework to enhance summary faithfulness and controllability. We automatically\nevaluate the performance of our framework on three benchmark summarization\ndatasets. We also conduct a human evaluation to validate the effectiveness of\nthe iterative refinements and identify a potential issue of over-correction.", "published": "2023-05-24 07:40:06", "link": "http://arxiv.org/abs/2305.14835v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meta-learning For Vision-and-language Cross-lingual Transfer", "abstract": "Current pre-trained vison-language models (PVLMs) achieve excellent\nperformance on a range of multi-modal datasets. Recent work has aimed at\nbuilding multilingual models, and a range of novel multilingual multi-modal\ndatasets have been proposed. Current PVLMs typically perform poorly on these\ndatasets when used for multi-modal zero-shot or few-shot cross-lingual\ntransfer, especially for low-resource languages. To alleviate this problem, we\npropose a novel meta-learning fine-tuning framework. Our framework makes\ncurrent PVLMs rapidly adaptive to new languages in vision-language scenarios by\ndesigning MAML in a cross-lingual multi-modal manner. Experiments show that our\nmethod boosts the performance of current state-of-the-art PVLMs in both\nzero-shot and few-shot cross-lingual transfer on a range of vision-language\nunderstanding tasks and datasets (XVNLI, xGQA, MaRVL, xFlicker&Co)", "published": "2023-05-24 07:51:42", "link": "http://arxiv.org/abs/2305.14843v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Drafting Event Schemas using Language Models", "abstract": "Past work has studied event prediction and event language modeling, sometimes\nmediated through structured representations of knowledge in the form of event\nschemas. Such schemas can lead to explainable predictions and forecasting of\nunseen events given incomplete information. In this work, we look at the\nprocess of creating such schemas to describe complex events. We use large\nlanguage models (LLMs) to draft schemas directly in natural language, which can\nbe further refined by human curators as necessary. Our focus is on whether we\ncan achieve sufficient diversity and recall of key events and whether we can\nproduce the schemas in a sufficiently descriptive style. We show that large\nlanguage models are able to achieve moderate recall against schemas taken from\ntwo different datasets, with even better results when multiple prompts and\nmultiple samples are combined. Moreover, we show that textual entailment\nmethods can be used for both matching schemas to instances of events as well as\nevaluating overlap between gold and predicted schemas. Our method paves the way\nfor easier distillation of event knowledge from large language model into\nschemas.", "published": "2023-05-24 07:57:04", "link": "http://arxiv.org/abs/2305.14847v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual\n  Transfer", "abstract": "Despite remarkable advancements in few-shot generalization in natural\nlanguage processing, most models are developed and evaluated primarily in\nEnglish. To facilitate research on few-shot cross-lingual transfer, we\nintroduce a new benchmark, called BUFFET, which unifies 15 diverse tasks across\n54 languages in a sequence-to-sequence format and provides a fixed set of\nfew-shot examples and instructions. BUFFET is designed to establish a rigorous\nand equitable evaluation framework for few-shot cross-lingual transfer across a\nbroad range of tasks and languages. Using BUFFET, we perform thorough\nevaluations of state-of-the-art multilingual large language models with\ndifferent transfer methods, namely in-context learning and fine-tuning. Our\nfindings reveal significant room for improvement in few-shot in-context\ncross-lingual transfer. In particular, ChatGPT with in-context learning often\nperforms worse than much smaller mT5-base models fine-tuned on English task\ndata and few-shot in-language examples. Our analysis suggests various avenues\nfor future research in few-shot cross-lingual transfer, such as improved\npretraining, understanding, and future evaluations.", "published": "2023-05-24 08:06:33", "link": "http://arxiv.org/abs/2305.14857v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Just CHOP: Embarrassingly Simple LLM Compression", "abstract": "Large language models (LLMs) enable unparalleled few- and zero-shot reasoning\ncapabilities but at a high computational footprint. A growing assortment of\nmethods for compression promises to reduce the computational burden of LLMs in\ndeployment, but so far, only quantization approaches have been demonstrated to\nbe effective for LLM compression while maintaining zero-shot performance. A\ncritical step in the compression process, the pretrain-then-finetune paradigm,\nhas largely been overlooked when adapting existing pruning strategies to LLMs\nor proposing new ones. In this work, we show that embarrassingly simple layer\npruning coupled with an extended language model pretraining as the finetuning\nphase produces state-of-the-art results against structured and even\nsemi-structured compression of models at a 7B scale while being more inference\nefficient. We call this method LayerChop, where we deterministically remove\nlayers from a model followed by task-agnostic finetuning of the remaining\nweights by continued self-supervised pretraining. At this scale, we also show\nhow distillation, which has been super effective in task-agnostic compression\nof smaller BERT-style models, becomes inefficient against our simple pruning\ntechnique.", "published": "2023-05-24 08:18:35", "link": "http://arxiv.org/abs/2305.14864v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CAR: Conceptualization-Augmented Reasoner for Zero-Shot Commonsense\n  Question Answering", "abstract": "The task of zero-shot commonsense question answering evaluates models on\ntheir capacity to reason about general scenarios beyond those presented in\nspecific datasets. Existing approaches for tackling this task leverage external\nknowledge from CommonSense Knowledge Bases (CSKBs) by pretraining the model on\nsynthetic QA pairs constructed from CSKBs. In these approaches, negative\nexamples (distractors) are formulated by randomly sampling from CSKBs using\nfairly primitive keyword constraints. However, two bottlenecks limit these\napproaches: the inherent incompleteness of CSKBs limits the semantic coverage\nof synthetic QA pairs, and the lack of human annotations makes the sampled\nnegative examples potentially uninformative and contradictory. To tackle these\nlimitations above, we propose Conceptualization-Augmented Reasoner (CAR), a\nzero-shot commonsense question-answering framework that fully leverages the\npower of conceptualization. Specifically, CAR abstracts a commonsense knowledge\ntriple to many higher-level instances, which increases the coverage of CSKB and\nexpands the ground-truth answer space, reducing the likelihood of selecting\nfalse-negative distractors. Extensive experiments demonstrate that CAR more\nrobustly generalizes to answering questions about zero-shot commonsense\nscenarios than existing methods, including large language models, such as\nGPT3.5 and ChatGPT. Our codes, data, and model checkpoints are available at\nhttps://github.com/HKUST-KnowComp/CAR.", "published": "2023-05-24 08:21:31", "link": "http://arxiv.org/abs/2305.14869v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ClusterLLM: Large Language Models as a Guide for Text Clustering", "abstract": "We introduce ClusterLLM, a novel text clustering framework that leverages\nfeedback from an instruction-tuned large language model, such as ChatGPT.\nCompared with traditional unsupervised methods that builds upon \"small\"\nembedders, ClusterLLM exhibits two intriguing advantages: (1) it enjoys the\nemergent capability of LLM even if its embeddings are inaccessible; and (2) it\nunderstands the user's preference on clustering through textual instruction\nand/or a few annotated data. First, we prompt ChatGPT for insights on\nclustering perspective by constructing hard triplet questions <does A better\ncorrespond to B than C>, where A, B and C are similar data points that belong\nto different clusters according to small embedder. We empirically show that\nthis strategy is both effective for fine-tuning small embedder and\ncost-efficient to query ChatGPT. Second, we prompt ChatGPT for helps on\nclustering granularity by carefully designed pairwise questions <do A and B\nbelong to the same category>, and tune the granularity from cluster hierarchies\nthat is the most consistent with the ChatGPT answers. Extensive experiments on\n14 datasets show that ClusterLLM consistently improves clustering quality, at\nan average cost of ~$0.6 per dataset. The code will be available at\nhttps://github.com/zhang-yu-wei/ClusterLLM.", "published": "2023-05-24 08:24:25", "link": "http://arxiv.org/abs/2305.14871v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Probability-based Prompt Selection Through Unified Evaluation\n  and Analysis", "abstract": "Previous works in prompt engineering for large language models have\nintroduced different gradient-free probability-based prompt selection methods\nthat aim to choose the optimal prompt among the candidates for a given task but\nhave failed to provide a comprehensive and fair comparison between each other.\nIn this paper, we propose a unified framework to interpret and evaluate the\nexisting probability-based prompt selection methods by performing extensive\nexperiments on 13 common and diverse NLP tasks. We find that each of the\nexisting methods can be interpreted as some variant of the method that\nmaximizes mutual information between the input and the predicted output (MI).\nUtilizing this finding, we develop several other combinatorial variants of MI\nand increase the effectiveness of the oracle prompt selection method from\n87.79% to 94.98%, measured as the ratio of the performance of the selected\nprompt to that of the optimal oracle prompt. Furthermore, considering that all\nthe methods rely on the output probability distribution of the model that might\nbe biased, we propose a novel calibration method called Calibration by\nMarginalization (CBM) that is orthogonal to the existing methods and helps\nincrease the prompt selection effectiveness of the best method to 96.85%,\nachieving 99.44% of the oracle prompt F1 without calibration.", "published": "2023-05-24 08:29:50", "link": "http://arxiv.org/abs/2305.14877v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PIVOINE: Instruction Tuning for Open-world Information Extraction", "abstract": "We consider the problem of Open-world Information Extraction (Open-world IE),\nwhich extracts comprehensive entity profiles from unstructured texts. Different\nfrom the conventional closed-world setting of Information Extraction (IE),\nOpen-world IE considers a more general situation where entities and relations\ncould be beyond a predefined ontology. More importantly, we seek to develop a\nlarge language model (LLM) that is able to perform Open-world IE to extract\ndesirable entity profiles characterized by (possibly fine-grained) natural\nlanguage instructions. We achieve this by finetuning LLMs using instruction\ntuning. In particular, we construct INSTRUCTOPENWIKI, a substantial instruction\ntuning dataset for Open-world IE enriched with a comprehensive corpus,\nextensive annotations, and diverse instructions. We finetune the pretrained\nBLOOM models on INSTRUCTOPENWIKI and obtain PIVOINE, an LLM for Open-world IE\nwith strong instruction-following capabilities. Our experiments demonstrate\nthat PIVOINE significantly outperforms traditional closed-world methods and\nother LLM baselines, displaying impressive generalization capabilities on both\nunseen instructions and out-of-ontology cases. Consequently, PIVOINE emerges as\na promising solution to tackle the open-world challenge in IE effectively.", "published": "2023-05-24 08:52:08", "link": "http://arxiv.org/abs/2305.14898v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box\n  Machine-Generated Text Detection", "abstract": "Large language models (LLMs) have demonstrated remarkable capability to\ngenerate fluent responses to a wide variety of user queries. However, this has\nalso raised concerns about the potential misuse of such texts in journalism,\neducation, and academia. In this study, we strive to create automated systems\nthat can detect machine-generated texts and pinpoint potential misuse. We first\nintroduce a large-scale benchmark \\textbf{M4}, which is a multi-generator,\nmulti-domain, and multi-lingual corpus for machine-generated text detection.\nThrough an extensive empirical study of this dataset, we show that it is\nchallenging for detectors to generalize well on instances from unseen domains\nor LLMs. In such cases, detectors tend to misclassify machine-generated text as\nhuman-written. These results show that the problem is far from solved and that\nthere is a lot of room for improvement. We believe that our dataset will enable\nfuture research towards more robust approaches to this pressing societal\nproblem. The dataset is available at https://github.com/mbzuai-nlp/M4.", "published": "2023-05-24 08:55:11", "link": "http://arxiv.org/abs/2305.14902v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coverage-based Example Selection for In-Context Learning", "abstract": "In-context learning (ICL), the ability of large language models to perform\nnovel tasks by conditioning on a prompt with a few task examples, requires\nthese examples to be informative about the test instance. The standard approach\nof independently ranking and selecting the most similar examples selects\nredundant examples while omitting important information. In this work, we show\nthat BERTScore-Recall (BSR) selects better examples that demonstrate more of\nthe salient aspects, e.g. reasoning patterns, of the test input. We further\nextend BSR and many standard metrics to easily optimizable set-level metrics,\ngiving still better coverage of those salient aspects. On 15 datasets spanning\n6 tasks and with 7 diverse LLMs, we show that (1) BSR is the superior metric\nfor in-context example selection across the board, and (2) for compositional\ntasks, set selection using Set-BSR outperforms independent ranking by up to 17\npoints on average and, despite being training-free, surpasses methods that\nleverage task or LLM-specific training.", "published": "2023-05-24 08:58:28", "link": "http://arxiv.org/abs/2305.14907v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PURR: Efficiently Editing Language Model Hallucinations by Denoising\n  Language Model Corruptions", "abstract": "The remarkable capabilities of large language models have been accompanied by\na persistent drawback: the generation of false and unsubstantiated claims\ncommonly known as \"hallucinations\". To combat this issue, recent research has\nintroduced approaches that involve editing and attributing the outputs of\nlanguage models, particularly through prompt-based editing. However, the\ninference cost and speed of using large language models for editing currently\nbottleneck prompt-based methods. These bottlenecks motivate the training of\ncompact editors, which is challenging due to the scarcity of training data for\nthis purpose. To overcome these challenges, we exploit the power of large\nlanguage models to introduce corruptions (i.e., noise) into text and\nsubsequently fine-tune compact editors to denoise the corruptions by\nincorporating relevant evidence. Our methodology is entirely unsupervised and\nprovides us with faux hallucinations for training in any domain. Our Petite\nUnsupervised Research and Revision model, PURR, not only improves attribution\nover existing editing methods based on fine-tuning and prompting, but also\nachieves faster execution times by orders of magnitude.", "published": "2023-05-24 08:59:00", "link": "http://arxiv.org/abs/2305.14908v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoLaDa: A Collaborative Label Denoising Framework for Cross-lingual\n  Named Entity Recognition", "abstract": "Cross-lingual named entity recognition (NER) aims to train an NER system that\ngeneralizes well to a target language by leveraging labeled data in a given\nsource language. Previous work alleviates the data scarcity problem by\ntranslating source-language labeled data or performing knowledge distillation\non target-language unlabeled data. However, these methods may suffer from label\nnoise due to the automatic labeling process. In this paper, we propose CoLaDa,\na Collaborative Label Denoising Framework, to address this problem.\nSpecifically, we first explore a model-collaboration-based denoising scheme\nthat enables models trained on different data sources to collaboratively\ndenoise pseudo labels used by each other. We then present an\ninstance-collaboration-based strategy that considers the label consistency of\neach token's neighborhood in the representation space for denoising.\nExperiments on different benchmark datasets show that the proposed CoLaDa\nachieves superior results compared to previous methods, especially when\ngeneralizing to distant languages.", "published": "2023-05-24 09:03:01", "link": "http://arxiv.org/abs/2305.14913v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Frugal Prompting for Dialog Models", "abstract": "The use of large language models (LLMs) in natural language processing (NLP)\ntasks is rapidly increasing, leading to changes in how researchers approach\nproblems in the field. To fully utilize these models' abilities, a better\nunderstanding of their behavior for different input protocols is required. With\nLLMs, users can directly interact with the models through a text-based\ninterface to define and solve various tasks. Hence, understanding the\nconversational abilities of these LLMs, which may not have been specifically\ntrained for dialog modeling, is also important. This study examines different\napproaches for building dialog systems using LLMs by considering various\naspects of the prompt. As part of prompt tuning, we experiment with various\nways of providing instructions, exemplars, current query and additional\ncontext. The research also analyzes the representations of dialog history that\nhave the optimal usable-information density. Based on the findings, the paper\nsuggests more compact ways of providing dialog history information while\nensuring good performance and reducing model's inference-API costs. The\nresearch contributes to a better understanding of how LLMs can be effectively\nused for building interactive systems.", "published": "2023-05-24 09:06:49", "link": "http://arxiv.org/abs/2305.14919v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aligning Language Models to User Opinions", "abstract": "An important aspect of developing LLMs that interact with humans is to align\nmodels' behavior to their users. It is possible to prompt an LLM into behaving\nas a certain persona, especially a user group or ideological persona the model\ncaptured during its pertaining stage. But, how to best align an LLM with a\nspecific user and not a demographic or ideological group remains an open\nquestion. Mining public opinion surveys (by Pew Research), we find that the\nopinions of a user and their demographics and ideologies are not mutual\npredictors. We use this insight to align LLMs by modeling both user opinions as\nwell as user demographics and ideology, achieving up to 7 points accuracy gains\nin predicting public opinions from survey questions across a broad set of\ntopics. In addition to the typical approach of prompting LLMs with demographics\nand ideology, we discover that utilizing the most relevant past opinions from\nindividual users enables the model to predict user opinions more accurately.", "published": "2023-05-24 09:11:11", "link": "http://arxiv.org/abs/2305.14929v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Appropriate Language in Argumentation", "abstract": "Online discussion moderators must make ad-hoc decisions about whether the\ncontributions of discussion participants are appropriate or should be removed\nto maintain civility. Existing research on offensive language and the resulting\ntools cover only one aspect among many involved in such decisions. The question\nof what is considered appropriate in a controversial discussion has not yet\nbeen systematically addressed. In this paper, we operationalize appropriate\nlanguage in argumentation for the first time. In particular, we model\nappropriateness through the absence of flaws, grounded in research on argument\nquality assessment, especially in aspects from rhetoric. From these, we derive\na new taxonomy of 14 dimensions that determine inappropriate language in online\ndiscussions. Building on three argument quality corpora, we then create a\ncorpus of 2191 arguments annotated for the 14 dimensions. Empirical analyses\nsupport that the taxonomy covers the concept of appropriateness\ncomprehensively, showing several plausible correlations with argument quality\ndimensions. Moreover, results of baseline approaches to assessing\nappropriateness suggest that all dimensions can be modeled computationally on\nthe corpus.", "published": "2023-05-24 09:17:05", "link": "http://arxiv.org/abs/2305.14935v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Trade-Offs Between Fairness and Privacy in Language Modeling", "abstract": "Protecting privacy in contemporary NLP models is gaining in importance. So\ndoes the need to mitigate social biases of such models. But can we have both at\nthe same time? Existing research suggests that privacy preservation comes at\nthe price of worsening biases in classification tasks. In this paper, we\nexplore the extent to which this tradeoff really holds when we incorporate both\nprivacy preservation and de-biasing techniques into training text generation\nmodels. How does improving the model along one dimension affect the other\ndimension as well as the utility of the model? We conduct an extensive set of\nexperiments that include bias detection, privacy attacks, language modeling,\nand performance on downstream tasks.", "published": "2023-05-24 09:18:28", "link": "http://arxiv.org/abs/2305.14936v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Fair and In-Depth Evaluation of Existing End-to-End Entity Linking\n  Systems", "abstract": "Existing evaluations of entity linking systems often say little about how the\nsystem is going to perform for a particular application. There are two\nfundamental reasons for this. One is that many evaluations only use aggregate\nmeasures (like precision, recall, and F1 score), without a detailed error\nanalysis or a closer look at the results. The other is that all of the widely\nused benchmarks have strong biases and artifacts, in particular: a strong focus\non named entities, an unclear or missing specification of what else counts as\nan entity mention, poor handling of ambiguities, and an over- or\nunderrepresentation of certain kinds of entities.\n  We provide a more meaningful and fair in-depth evaluation of a variety of\nexisting end-to-end entity linkers. We characterize their strengths and\nweaknesses and also report on reproducibility aspects. The detailed results of\nour evaluation can be inspected under\nhttps://elevant.cs.uni-freiburg.de/emnlp2023 . Our evaluation is based on\nseveral widely used benchmarks, which exhibit the problems mentioned above to\nvarious degrees, as well as on two new benchmarks, which address the problems\nmentioned above. The new benchmarks can be found under\nhttps://github.com/ad-freiburg/fair-entity-linking-benchmarks .", "published": "2023-05-24 09:20:15", "link": "http://arxiv.org/abs/2305.14937v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Data Augmentation for Document-grounded Dialog Systems in\n  Low Resource Languages", "abstract": "This paper proposes a framework to address the issue of data scarcity in\nDocument-Grounded Dialogue Systems(DGDS). Our model leverages high-resource\nlanguages to enhance the capability of dialogue generation in low-resource\nlanguages. Specifically, We present a novel pipeline CLEM (Cross-Lingual\nEnhanced Model) including adversarial training retrieval (Retriever and\nRe-ranker), and Fid (fusion-in-decoder) generator. To further leverage\nhigh-resource language, we also propose an innovative architecture to conduct\nalignment across different languages with translated training. Extensive\nexperiment results demonstrate the effectiveness of our model and we achieved\n4th place in the DialDoc 2023 Competition. Therefore, CLEM can serve as a\nsolution to resource scarcity in DGDS and provide useful guidance for\nmulti-lingual alignment tasks.", "published": "2023-05-24 09:40:52", "link": "http://arxiv.org/abs/2305.14949v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Editing Common Sense in Transformers", "abstract": "Editing model parameters directly in Transformers makes updating open-source\ntransformer-based models possible without re-training (Meng et al., 2023).\nHowever, these editing methods have only been evaluated on statements about\nencyclopedic knowledge with a single correct answer. Commonsense knowledge with\nmultiple correct answers, e.g., an apple can be green or red but not\ntransparent, has not been studied but is as essential for enhancing\ntransformers' reliability and usefulness. In this paper, we investigate whether\ncommonsense judgments are causally associated with localized, editable\nparameters in Transformers, and we provide an affirmative answer. We find that\ndirectly applying the MEMIT editing algorithm results in sub-par performance\nand improve it for the commonsense domain by varying edit tokens and improving\nthe layer selection strategy, i.e., $MEMIT_{CSK}$. GPT-2 Large and XL models\nedited using $MEMIT_{CSK}$ outperform best-fine-tuned baselines by 10.97% and\n10.73% F1 scores on PEP3k and 20Q datasets. In addition, we propose a novel\nevaluation dataset, PROBE SET, that contains unaffected and affected\nneighborhoods, affected paraphrases, and affected reasoning challenges.\n$MEMIT_{CSK}$ performs well across the metrics while fine-tuning baselines show\nsignificant trade-offs between unaffected and affected metrics. These results\nsuggest a compelling future direction for incorporating feedback about common\nsense into Transformers through direct model editing.", "published": "2023-05-24 09:50:54", "link": "http://arxiv.org/abs/2305.14956v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text\n  Classification", "abstract": "We present PESCO, a novel contrastive learning framework that substantially\nimproves the performance of zero-shot text classification. We formulate text\nclassification as a neural text matching problem where each document is treated\nas a query, and the system learns the mapping from each query to the relevant\nclass labels by (1) adding prompts to enhance label matching, and (2) using\nretrieved labels to enrich the training set in a self-training loop of\ncontrastive learning. PESCO achieves state-of-the-art performance on four\nbenchmark text classification datasets. On DBpedia, we achieve 98.5\\% accuracy\nwithout any labeled data, which is close to the fully-supervised result.\nExtensive experiments and analyses show all the components of PESCO are\nnecessary for improving the performance of zero-shot text classification.", "published": "2023-05-24 09:57:06", "link": "http://arxiv.org/abs/2305.14963v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Multidimensional Political Incivility on Social Media", "abstract": "The rise of social media has been argued to intensify uncivil and hostile\nonline political discourse. Yet, to date, there is a lack of clarity on what\nincivility means in the political sphere. In this work, we utilize a\nmultidimensional perspective of political incivility, developed in the fields\nof political science and communication, that differentiates between\nimpoliteness and political intolerance. We present state-of-the-art incivility\ndetection results using a large dataset of 13K political tweets, collected and\nannotated per this distinction. Applying political incivility detection at\nlarge-scale, we observe that political incivility demonstrates a highly skewed\ndistribution over users, and examine social factors that correlate with\nincivility at subpopulation and user-level. Finally, we propose an approach for\nmodeling social context information about the tweet author alongside the tweet\ncontent, showing that this leads to improved performance on the task of\npolitical incivility detection. We believe that this latter result holds\npromise for socially-informed text processing in general.", "published": "2023-05-24 09:57:12", "link": "http://arxiv.org/abs/2305.14964v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting\n  Jailbreaks", "abstract": "Recent explorations with commercial Large Language Models (LLMs) have shown\nthat non-expert users can jailbreak LLMs by simply manipulating their prompts;\nresulting in degenerate output behavior, privacy and security breaches,\noffensive outputs, and violations of content regulator policies. Limited\nstudies have been conducted to formalize and analyze these attacks and their\nmitigations. We bridge this gap by proposing a formalism and a taxonomy of\nknown (and possible) jailbreaks. We survey existing jailbreak methods and their\neffectiveness on open-source and commercial LLMs (such as GPT-based models,\nOPT, BLOOM, and FLAN-T5-XXL). We further discuss the challenges of jailbreak\ndetection in terms of their effectiveness against known attacks. For further\nanalysis, we release a dataset of model outputs across 3700 jailbreak prompts\nover 4 tasks.", "published": "2023-05-24 09:57:37", "link": "http://arxiv.org/abs/2305.14965v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OverPrompt: Enhancing ChatGPT through Efficient In-Context Learning", "abstract": "The remarkable performance of pre-trained large language models has\nrevolutionised various natural language processing applications. Due to huge\nparametersizes and extensive running costs, companies or organisations tend to\ntransfer the models to the target task by zero-shot prompting techniques.\nHowever, the prohibitive costs of tokens and time have hindered their adoption\nin applications. We propose OverPrompt, leveraging the in-context learning\ncapability of LLMs to handle multiple task inputs, thereby reducing token and\ntime costs. This approach could potentially improve task performance during API\nqueries due to better conditional distribution mapping. Evaluated across\ndiverse classification datasets, our experiments show that OverPrompt can\nachieve cost-efficient zero-shot classification without causing significant\ndetriment to task performance, and in some cases, even improving it. An\nablation study conducted on various LLMs, along with an investigation into the\nrobustness of our prompting strategy to different input ordering, offers\nvaluable insights into the broader applicability of our method across diverse\ntasks. These findings also suggest a more seamless integration of our method\nwith LLMs through an API.", "published": "2023-05-24 10:08:04", "link": "http://arxiv.org/abs/2305.14973v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence\n  Scores from Language Models Fine-Tuned with Human Feedback", "abstract": "A trustworthy real-world prediction system should produce well-calibrated\nconfidence scores; that is, its confidence in an answer should be indicative of\nthe likelihood that the answer is correct, enabling deferral to an expert in\ncases of low-confidence predictions. Recent studies have shown that\nunsupervised pre-training produces large language models (LMs) whose\nconditional probabilities are remarkably well-calibrated. However, the most\nwidely-used LMs are fine-tuned with reinforcement learning from human feedback\n(RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional\nprobabilities that are very poorly calibrated. In light of this perceived\nweakness, we conduct a broad evaluation of methods for extracting confidence\nscores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find\nthat verbalized confidences emitted as output tokens are typically\nbetter-calibrated than the model's conditional probabilities on the TriviaQA,\nSciQ, and TruthfulQA benchmarks, often reducing the expected calibration error\nby a relative 50%.", "published": "2023-05-24 10:12:33", "link": "http://arxiv.org/abs/2305.14975v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Table-to-Text Generation Capabilities of LLMs in\n  Real-World Information Seeking Scenarios", "abstract": "Tabular data is prevalent across various industries, necessitating\nsignificant time and effort for users to understand and manipulate for their\ninformation-seeking purposes. The advancements in large language models (LLMs)\nhave shown enormous potential to improve user efficiency. However, the adoption\nof LLMs in real-world applications for table information seeking remains\nunderexplored. In this paper, we investigate the table-to-text capabilities of\ndifferent LLMs using four datasets within two real-world information seeking\nscenarios. These include the LogicNLG and our newly-constructed LoTNLG datasets\nfor data insight generation, along with the FeTaQA and our newly-constructed\nF2WTQ datasets for query-based generation. We structure our investigation\naround three research questions, evaluating the performance of LLMs in\ntable-to-text generation, automated evaluation, and feedback generation,\nrespectively. Experimental results indicate that the current high-performing\nLLM, specifically GPT-4, can effectively serve as a table-to-text generator,\nevaluator, and feedback generator, facilitating users' information seeking\npurposes in real-world scenarios. However, a significant performance gap still\nexists between other open-sourced LLMs (e.g., Tulu and LLaMA-2) and GPT-4\nmodels. Our data and code are publicly available at\nhttps://github.com/yale-nlp/LLM-T2T.", "published": "2023-05-24 10:22:30", "link": "http://arxiv.org/abs/2305.14987v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dolphin: A Challenging and Diverse Benchmark for Arabic NLG", "abstract": "We present Dolphin, a novel benchmark that addresses the need for a natural\nlanguage generation (NLG) evaluation framework dedicated to the wide collection\nof Arabic languages and varieties. The proposed benchmark encompasses a broad\nrange of 13 different NLG tasks, including dialogue generation, question\nanswering, machine translation, summarization, among others. Dolphin comprises\na substantial corpus of 40 diverse and representative public datasets across 50\ntest splits, carefully curated to reflect real-world scenarios and the\nlinguistic richness of Arabic. It sets a new standard for evaluating the\nperformance and generalization capabilities of Arabic and multilingual models,\npromising to enable researchers to push the boundaries of current\nmethodologies. We provide an extensive analysis of Dolphin, highlighting its\ndiversity and identifying gaps in current Arabic NLG research. We also offer a\npublic leaderboard that is both interactive and modular and evaluate several\nmodels on our benchmark, allowing us to set strong baselines against which\nresearchers can compare.", "published": "2023-05-24 10:24:10", "link": "http://arxiv.org/abs/2305.14989v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controlling Pre-trained Language Models for Grade-Specific Text\n  Simplification", "abstract": "Text simplification (TS) systems rewrite text to make it more readable while\npreserving its content. However, what makes a text easy to read depends on the\nintended readers. Recent work has shown that pre-trained language models can\nsimplify text using a wealth of techniques to control output simplicity,\nranging from specifying only the desired reading grade level, to directly\nspecifying low-level edit operations. Yet it remains unclear how to set these\ncontrol parameters in practice. Existing approaches set them at the corpus\nlevel, disregarding the complexity of individual inputs and considering only\none level of output complexity. In this work, we conduct an empirical study to\nunderstand how different control mechanisms impact the adequacy and simplicity\nof text simplification systems. Based on these insights, we introduce a simple\nmethod that predicts the edit operations required for simplifying a text for a\nspecific grade level on an instance-per-instance basis. This approach improves\nthe quality of the simplified outputs over corpus-level search-based\nheuristics.", "published": "2023-05-24 10:29:45", "link": "http://arxiv.org/abs/2305.14993v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RefGPT: Dialogue Generation of GPT, by GPT, and for GPT", "abstract": "Large Language Models (LLMs) have attained the impressive capability to\nresolve a wide range of NLP tasks by fine-tuning high-quality instruction data.\nHowever, collecting human-written data of high quality, especially multi-turn\ndialogues, is expensive and unattainable for most people. Though previous\nstudies have used powerful LLMs to generate the dialogues automatically, they\nall suffer from generating untruthful dialogues because of the model\nhallucination. Therefore, we propose a method called RefGPT to generate\nenormous truthful and customized dialogues without worrying about factual\nerrors caused by the model hallucination. RefGPT solves the model hallucination\nin dialogue generation by restricting the LLMs to leverage the given reference\ninstead of reciting their own knowledge to generate dialogues. Additionally,\nRefGPT adds detailed controls on every utterance to enable high customization\ncapability, which previous studies have ignored. On the basis of RefGPT, we\nalso propose two high-quality dialogue datasets generated by GPT-4, namely\nRefGPT-Fact and RefGPT-Code. RefGPT-Fact is a dataset with 100k multi-turn\ndialogues based on factual knowledge and RefGPT-Code has 76k multi-turn\ndialogues covering a wide range of coding scenarios. Our code and datasets are\nreleased in https://github.com/mutonix/RefGPT.", "published": "2023-05-24 10:30:42", "link": "http://arxiv.org/abs/2305.14994v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language\n  Models", "abstract": "Chain-of-Thought (CoT) prompting enables large language models to solve\ncomplex reasoning problems by generating intermediate steps. However, confined\nby its inherent single-pass and sequential generation process, CoT heavily\nrelies on the initial decisions, causing errors in early steps to accumulate\nand impact the final answers. In contrast, humans adopt recursive thinking when\ntackling complex reasoning problems, i.e., iteratively breaking the original\nproblem into approachable sub-problems and aggregating their answers to resolve\nthe original one. Inspired by the human cognitive process, we propose SOCRATIC\nQUESTIONING, a divide-and-conquer style algorithm that mimics the recursive\nthinking process. Specifically, SOCRATIC QUESTIONING leverages large language\nmodels to raise and answer sub-questions until collecting enough information to\ntackle the original question. Unlike CoT, SOCRATIC QUESTIONING explicitly\nnavigates the thinking space, stimulates effective recursive thinking, and is\nmore robust towards errors in the thinking process. Extensive experiments on\nseveral complex reasoning tasks, including MMLU, MATH, LogiQA, and visual\nquestion-answering demonstrate significant performance improvements over the\nstate-of-the-art prompting methods, such as CoT, and Tree-of-Thought. The\nqualitative analysis clearly shows that the intermediate reasoning steps\nelicited by SOCRATIC QUESTIONING are similar to humans' recursively thinking\nprocess of complex reasoning problems.", "published": "2023-05-24 10:36:14", "link": "http://arxiv.org/abs/2305.14999v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMDet: A Third Party Large Language Models Generated Text Detection\n  Tool", "abstract": "Generated texts from large language models (LLMs) are remarkably close to\nhigh-quality human-authored text, raising concerns about their potential misuse\nin spreading false information and academic misconduct. Consequently, there is\nan urgent need for a highly practical detection tool capable of accurately\nidentifying the source of a given text. However, existing detection tools\ntypically rely on access to LLMs and can only differentiate between\nmachine-generated and human-authored text, failing to meet the requirements of\nfine-grained tracing, intermediary judgment, and rapid detection. Therefore, we\npropose LLMDet, a model-specific, secure, efficient, and extendable detection\ntool, that can source text from specific LLMs, such as GPT-2, OPT, LLaMA, and\nothers. In LLMDet, we record the next-token probabilities of salient n-grams as\nfeatures to calculate proxy perplexity for each LLM. By jointly analyzing the\nproxy perplexities of LLMs, we can determine the source of the generated text.\nExperimental results show that LLMDet yields impressive detection performance\nwhile ensuring speed and security, achieving 98.54% precision and x5.0 faster\nfor recognizing human-authored text. Additionally, LLMDet can effortlessly\nextend its detection capabilities to a new open-source model. We will provide\nan open-source tool at https://github.com/TrustedLLM/LLMDet.", "published": "2023-05-24 10:45:16", "link": "http://arxiv.org/abs/2305.15004v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis in the Era of Large Language Models: A Reality Check", "abstract": "Sentiment analysis (SA) has been a long-standing research area in natural\nlanguage processing. It can offer rich insights into human sentiments and\nopinions and has thus seen considerable interest from both academia and\nindustry. With the advent of large language models (LLMs) such as ChatGPT,\nthere is a great potential for their employment on SA problems. However, the\nextent to which existing LLMs can be leveraged for different sentiment analysis\ntasks remains unclear. This paper aims to provide a comprehensive investigation\ninto the capabilities of LLMs in performing various sentiment analysis tasks,\nfrom conventional sentiment classification to aspect-based sentiment analysis\nand multifaceted analysis of subjective texts. We evaluate performance across\n13 tasks on 26 datasets and compare the results against small language models\n(SLMs) trained on domain-specific datasets. Our study reveals that while LLMs\ndemonstrate satisfactory performance in simpler tasks, they lag behind in more\ncomplex tasks requiring deeper understanding or structured sentiment\ninformation. However, LLMs significantly outperform SLMs in few-shot learning\nsettings, suggesting their potential when annotation resources are limited. We\nalso highlight the limitations of current evaluation practices in assessing\nLLMs' SA abilities and propose a novel benchmark, \\textsc{SentiEval}, for a\nmore comprehensive and realistic evaluation. Data and code during our\ninvestigations are available at\n\\url{https://github.com/DAMO-NLP-SG/LLM-Sentiment}.", "published": "2023-05-24 10:45:25", "link": "http://arxiv.org/abs/2305.15005v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Injecting Knowledge into Biomedical Pre-trained Models via Polymorphism\n  and Synonymous Substitution", "abstract": "Pre-trained language models (PLMs) were considered to be able to store\nrelational knowledge present in the training data. However, some relational\nknowledge seems to be discarded unsafely in PLMs due to \\textbf{report bias}:\nlow-frequency relational knowledge might be underexpressed compared to\nhigh-frequency one in PLMs. This gives us a hint that relational knowledge\nmight not be redundant to the stored knowledge of PLMs, but rather be\ncomplementary. To additionally inject relational knowledge into PLMs, we\npropose a simple-yet-effective approach to inject relational knowledge into\nPLMs, which is inspired by three observations (namely, polymorphism, synonymous\nsubstitution, and association). In particular, we switch entities in the\ntraining corpus to related entities (either hypernyms/hyponyms/synonyms, or\narbitrarily-related concepts). Experimental results show that the proposed\napproach could not only better capture relational knowledge, but also improve\nthe performance in various biomedical downstream tasks. Our model is available\nin \\url{https://github.com/StevenZHB/BioPLM_InjectingKnowledge}.", "published": "2023-05-24 10:48:53", "link": "http://arxiv.org/abs/2305.15010v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bactrian-X: Multilingual Replicable Instruction-Following Models with\n  Low-Rank Adaptation", "abstract": "Instruction tuning has shown great promise in improving the performance of\nlarge language models. However, research on multilingual instruction tuning has\nbeen limited due to the scarcity of high-quality instruction-response datasets\nacross different languages. To bridge this gap, we present Bactrian-X, a\ncomprehensive multilingual parallel dataset of 3.4 million instruction-response\npairs across 52 languages. Leveraging this dataset, we train a set of adapters\nusing low-rank adaptation (LoRA), which are lightweight components that\nseamlessly integrate with large language models. These adapters have a\nsubstantially lower parameter count than the base model, making them easily\nreplaceable and usable as plug-ins for different languages or language groups.\nExtensive experiments in various multilingual evaluation settings demonstrate\nthat models derived from LoRA-based training over Bactrian-X outperform both\nthe vanilla models and existing instruction-tuned models. The code and models\nare publicly available at https://github.com/mbzuai-nlp/bactrian-x", "published": "2023-05-24 10:50:31", "link": "http://arxiv.org/abs/2305.15011v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unlocking Temporal Question Answering for Large Language Models with\n  Tailor-Made Reasoning Logic", "abstract": "The temporal aspect is a significant dimension of our reality. We notice the\nchallenge that large language models (LLMs) face when engaging in temporal\nreasoning. Our preliminary experiments show that methods involving the\ngeneration of intermediate reasoning steps, such as chain-of-thought and\nprogram-aided language models, do not consistently boost the performance of\ncomplex temporal question-answering tasks. This limitation can be attributed to\nthe LLMs' inadequate understanding of temporal information. To address this\nproblem, we propose TempLogic, a novel framework designed specifically for\ntemporal question-answering tasks across three levels of reasoning. TempLogic\nincorporates retrieval-guided context distillation, temporal data extraction,\nand tailor-made logic reasoning. Extensive experiments and analysis demonstrate\nthe effectiveness of our framework in solving intricate time-bound reasoning\ntasks.", "published": "2023-05-24 10:57:53", "link": "http://arxiv.org/abs/2305.15014v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Efficient Multilingual Language Model Compression through Vocabulary\n  Trimming", "abstract": "Multilingual language model (LM) have become a powerful tool in NLP\nespecially for non-English languages. Nevertheless, model parameters of\nmultilingual LMs remain large due to the larger embedding matrix of the\nvocabulary covering tokens in different languages. On the contrary, monolingual\nLMs can be trained in a target language with the language-specific vocabulary\nonly, but this requires a large budget and availability of reliable corpora to\nachieve a high-quality LM from scratch. In this paper, we propose\nvocabulary-trimming (VT), a method to reduce a multilingual LM vocabulary to a\ntarget language by deleting irrelevant tokens from its vocabulary. In theory,\nVT can compress any existing multilingual LM to build monolingual LMs in any\nlanguage covered by the multilingual LM. In our experiments, we show that VT\ncan retain the original performance of the multilingual LM, while being smaller\nin size (in general around 50% of the original vocabulary size is enough) than\nthe original multilingual LM. The evaluation is performed over four NLP tasks\n(two generative and two classification tasks) among four widely used\nmultilingual LMs in seven languages. Finally, we show that this methodology can\nkeep the best of both monolingual and multilingual worlds by keeping a small\nsize as monolingual models without the need for specifically retraining them,\nand even limiting potentially harmful social biases.", "published": "2023-05-24 11:00:33", "link": "http://arxiv.org/abs/2305.15020v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dior-CVAE: Pre-trained Language Models and Diffusion Priors for\n  Variational Dialog Generation", "abstract": "Current variational dialog models have employed pre-trained language models\n(PLMs) to parameterize the likelihood and posterior distributions. However, the\nGaussian assumption made on the prior distribution is incompatible with these\ndistributions, thus restricting the diversity of generated responses. These\nmodels also suffer from posterior collapse, i.e., the decoder tends to ignore\nlatent variables and directly access information captured in the encoder\nthrough the cross-attention mechanism. In this work, we propose Dior-CVAE, a\nhierarchical conditional variational autoencoder (CVAE) with diffusion priors\nto address these challenges. We employ a diffusion model to increase the\ncomplexity of the prior distribution and its compatibility with the\ndistributions produced by a PLM. Also, we propose memory dropout to the\ncross-attention mechanism, which actively encourages the use of latent\nvariables for response generation. Overall, experiments across two commonly\nused open-domain dialog datasets show that our method can generate more diverse\nresponses without large-scale dialog pre-training. Code is available at\nhttps://github.com/UKPLab/dior-cvae.", "published": "2023-05-24 11:06:52", "link": "http://arxiv.org/abs/2305.15025v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SmartTrim: Adaptive Tokens and Attention Pruning for Efficient\n  Vision-Language Models", "abstract": "Despite achieving remarkable performance on various vision-language tasks,\nTransformer-based Vision-Language Models (VLMs) suffer from redundancy in\ninputs and parameters, significantly hampering their efficiency in real-world\napplications. Moreover, the degree of redundancy in token representations and\nmodel parameters, such as attention heads, varies significantly for different\ninputs. In light of the challenges, we propose SmartTrim, an adaptive\nacceleration framework for VLMs, which adjusts the computational overhead per\ninstance. Specifically, we integrate lightweight modules into the original\nbackbone to identify and prune redundant token representations and attention\nheads within each layer. Furthermore, we devise a self-distillation strategy to\nenhance the consistency between the predictions of the pruned model and its\nfully-capacity counterpart. Experimental results across various vision-language\ntasks consistently demonstrate that SmartTrim accelerates the original model by\n2-3 times with minimal performance degradation, highlighting the effectiveness\nand efficiency compared to previous approaches. Code will be available at\nhttps://github.com/kugwzk/SmartTrim.", "published": "2023-05-24 11:18:00", "link": "http://arxiv.org/abs/2305.15033v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-ICL: Zero-Shot In-Context Learning with Self-Generated\n  Demonstrations", "abstract": "Large language models (LLMs) have exhibited striking in-context learning\n(ICL) ability to adapt to target tasks with a few input-output demonstrations.\nFor better ICL, different methods are proposed to select representative\ndemonstrations from existing training corpora. However, such settings are not\naligned with real-world practices, as end-users usually query LMs without\naccess to demonstration pools. In this work, we introduce Self-ICL -- a simple\nframework which bootstraps LMs' intrinsic capabilities to perform zero-shot\nICL. Given a test input, Self-ICL first prompts the model to generate\npseudo-inputs. Next, the model predicts pseudo-labels for the pseudo-inputs via\nzero-shot prompting. Finally, we perform ICL for the test input with the\npseudo-input-label pairs as demonstrations. Evaluation on 23 BIG-Bench Hard\ntasks shows Self-ICL outperforms zero-shot baselines on both average accuracy\nand head-to-head comparison. Moreover, with zero-shot chain-of-thought,\nSelf-ICL achieves results comparable to using real demonstrations.\nAdditionally, we conduct a range of analyses to validate Self-ICL's\neffectiveness and provide insights for its behaviors under different settings.", "published": "2023-05-24 11:22:34", "link": "http://arxiv.org/abs/2305.15035v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is GPT-4 a Good Data Analyst?", "abstract": "As large language models (LLMs) have demonstrated their powerful capabilities\nin plenty of domains and tasks, including context understanding, code\ngeneration, language generation, data storytelling, etc., many data analysts\nmay raise concerns if their jobs will be replaced by artificial intelligence\n(AI). This controversial topic has drawn great attention in public. However, we\nare still at a stage of divergent opinions without any definitive conclusion.\nMotivated by this, we raise the research question of \"is GPT-4 a good data\nanalyst?\" in this work and aim to answer it by conducting head-to-head\ncomparative studies. In detail, we regard GPT-4 as a data analyst to perform\nend-to-end data analysis with databases from a wide range of domains. We\npropose a framework to tackle the problems by carefully designing the prompts\nfor GPT-4 to conduct experiments. We also design several task-specific\nevaluation metrics to systematically compare the performance between several\nprofessional human data analysts and GPT-4. Experimental results show that\nGPT-4 can achieve comparable performance to humans. We also provide in-depth\ndiscussions about our results to shed light on further studies before reaching\nthe conclusion that GPT-4 can replace data analysts.", "published": "2023-05-24 11:26:59", "link": "http://arxiv.org/abs/2305.15038v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Active Learning for Natural Language Generation", "abstract": "The field of Natural Language Generation (NLG) suffers from a severe shortage\nof labeled data due to the extremely expensive and time-consuming process\ninvolved in manual annotation. A natural approach for coping with this problem\nis active learning (AL), a well-known machine learning technique for improving\nannotation efficiency by selectively choosing the most informative examples to\nlabel. However, while AL has been well-researched in the context of text\nclassification, its application to NLG remains largely unexplored. In this\npaper, we present a first systematic study of active learning for NLG,\nconsidering a diverse set of tasks and multiple leading selection strategies,\nand harnessing a strong instruction-tuned model. Our results indicate that the\nperformance of existing AL strategies is inconsistent, surpassing the baseline\nof random example selection in some cases but not in others. We highlight some\nnotable differences between the classification and generation scenarios, and\nanalyze the selection behaviors of existing AL strategies. Our findings\nmotivate exploring novel approaches for applying AL to generation tasks.", "published": "2023-05-24 11:27:53", "link": "http://arxiv.org/abs/2305.15040v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Faithful Synthetic Data with Large Language Models: A Case\n  Study in Computational Social Science", "abstract": "Large Language Models (LLMs) have democratized synthetic data generation,\nwhich in turn has the potential to simplify and broaden a wide gamut of NLP\ntasks. Here, we tackle a pervasive problem in synthetic data generation: its\ngenerative distribution often differs from the distribution of real-world data\nresearchers care about (in other words, it is unfaithful). In a case study on\nsarcasm detection, we study three strategies to increase the faithfulness of\nsynthetic data: grounding, filtering, and taxonomy-based generation. We\nevaluate these strategies using the performance of classifiers trained with\ngenerated synthetic data on real-world data. While all three strategies improve\nthe performance of classifiers, we find that grounding works best for the task\nat hand. As synthetic data generation plays an ever-increasing role in NLP\nresearch, we expect this work to be a stepping stone in improving its utility.\nWe conclude this paper with some recommendations on how to generate\nhigh(er)-fidelity synthetic data for specific tasks.", "published": "2023-05-24 11:27:59", "link": "http://arxiv.org/abs/2305.15041v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is Summary Useful or Not? An Extrinsic Human Evaluation of Text\n  Summaries on Downstream Tasks", "abstract": "Research on automated text summarization relies heavily on human and\nautomatic evaluation. While recent work on human evaluation mainly adopted\nintrinsic evaluation methods, judging the generic quality of text summaries,\ne.g. informativeness and coherence, our work focuses on evaluating the\nusefulness of text summaries with extrinsic methods. We carefully design three\ndifferent downstream tasks for extrinsic human evaluation of summaries, i.e.,\nquestion answering, text classification and text similarity assessment. We\ncarry out experiments using system rankings and user behavior data to evaluate\nthe performance of different summarization models. We find summaries are\nparticularly useful in tasks that rely on an overall judgment of the text,\nwhile being less effective for question answering tasks. The results show that\nsummaries generated by fine-tuned models lead to higher consistency in\nusefulness across all three tasks, as rankings of fine-tuned summarization\nsystems are close across downstream tasks according to the proposed extrinsic\nmetrics. Summaries generated by models in the zero-shot setting, however, are\nfound to be biased towards the text classification and similarity assessment\ntasks, due to its general and less detailed summary style. We further evaluate\nthe correlation of 14 intrinsic automatic metrics with human criteria and show\nthat intrinsic automatic metrics perform well in evaluating the usefulness of\nsummaries in the question-answering task, but are less effective in the other\ntwo tasks. This highlights the limitations of relying solely on intrinsic\nautomatic metrics in evaluating the performance and usefulness of summaries.", "published": "2023-05-24 11:34:39", "link": "http://arxiv.org/abs/2305.15044v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SETI: Systematicity Evaluation of Textual Inference", "abstract": "We propose SETI (Systematicity Evaluation of Textual Inference), a novel and\ncomprehensive benchmark designed for evaluating pre-trained language models\n(PLMs) for their systematicity capabilities in the domain of textual inference.\nSpecifically, SETI offers three different NLI tasks and corresponding datasets\nto evaluate various types of systematicity in reasoning processes. In order to\nsolve these tasks, models are required to perform compositional inference based\non known primitive constituents. We conduct experiments of SETI on six widely\nused PLMs. Results show that various PLMs are able to solve unseen\ncompositional inferences when having encountered the knowledge of how to\ncombine primitives, with good performance. However, they are considerably\nlimited when this knowledge is unknown to the model (40-100% points decrease).\nFurthermore, we find that PLMs can improve drastically once exposed to crucial\ncompositional knowledge in minimalistic shots. These findings position SETI as\nthe first benchmark for measuring the future progress of PLMs in achieving\nsystematicity generalization in the textual inference.", "published": "2023-05-24 11:35:31", "link": "http://arxiv.org/abs/2305.15045v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Monte Carlo Language Model Pipeline for Zero-Shot Sociopolitical Event\n  Extraction", "abstract": "Current social science efforts automatically populate event databases of \"who\ndid what to whom?\" tuples, by applying event extraction (EE) to text such as\nnews. The event databases are used to analyze sociopolitical dynamics between\nactor pairs (dyads) in, e.g., international relations. While most EE methods\nheavily rely on rules or supervised learning, \\emph{zero-shot} event extraction\ncould potentially allow researchers to flexibly specify arbitrary event classes\nfor new research questions. Unfortunately, we find that current zero-shot EE\nmethods, as well as a naive zero-shot approach of simple generative language\nmodel (LM) prompting, perform poorly for dyadic event extraction; most suffer\nfrom word sense ambiguity, modality sensitivity, and computational\ninefficiency. We address these challenges with a new fine-grained, multi-stage\ninstruction-following generative LM pipeline, proposing a Monte Carlo approach\nto deal with, and even take advantage of, nondeterminism of generative outputs.\nOur pipeline includes explicit stages of linguistic analysis (synonym\ngeneration, contextual disambiguation, argument realization, event modality),\n\\textit{improving control and interpretability} compared to purely neural\nmethods. This method outperforms other zero-shot EE approaches, and outperforms\nnaive applications of generative LMs by at least 17 F1 percent points. The\npipeline's filtering mechanism greatly improves computational efficiency,\nallowing it to perform as few as 12% of queries that a previous zero-shot\nmethod uses. Finally, we demonstrate our pipeline's application to dyadic\ninternational relations analysis.", "published": "2023-05-24 11:41:33", "link": "http://arxiv.org/abs/2305.15051v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reasoning over Hierarchical Question Decomposition Tree for Explainable\n  Question Answering", "abstract": "Explainable question answering (XQA) aims to answer a given question and\nprovide an explanation why the answer is selected. Existing XQA methods focus\non reasoning on a single knowledge source, e.g., structured knowledge bases,\nunstructured corpora, etc. However, integrating information from heterogeneous\nknowledge sources is essential to answer complex questions. In this paper, we\npropose to leverage question decomposing for heterogeneous knowledge\nintegration, by breaking down a complex question into simpler ones, and\nselecting the appropriate knowledge source for each sub-question. To facilitate\nreasoning, we propose a novel two-stage XQA framework, Reasoning over\nHierarchical Question Decomposition Tree (RoHT). First, we build the\nHierarchical Question Decomposition Tree (HQDT) to understand the semantics of\na complex question; then, we conduct probabilistic reasoning over HQDT from\nroot to leaves recursively, to aggregate heterogeneous knowledge at different\ntree levels and search for a best solution considering the decomposing and\nanswering probabilities. The experiments on complex QA datasets KQA Pro and\nMusique show that our framework outperforms SOTA methods significantly,\ndemonstrating the effectiveness of leveraging question decomposing for\nknowledge integration and our RoHT framework.", "published": "2023-05-24 11:45:59", "link": "http://arxiv.org/abs/2305.15056v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linear-Time Modeling of Linguistic Structure: An Order-Theoretic\n  Perspective", "abstract": "Tasks that model the relation between pairs of tokens in a string are a vital\npart of understanding natural language. Such tasks, in general, require\nexhaustive pair-wise comparisons of tokens, thus having a quadratic runtime\ncomplexity in the length of the string. We show that these exhaustive\ncomparisons can be avoided, and, moreover, the complexity of such tasks can be\nreduced to linear by casting the relation between tokens as a partial order\nover the string. Our method predicts real numbers for each token in a string in\nparallel and sorts the tokens accordingly, resulting in total orders of the\ntokens in the string. Each total order implies a set of arcs oriented from\nsmaller to greater tokens, sorted by their predicted numbers. The intersection\nof total orders results in a partial order over the set of tokens in the\nstring, which is then decoded into a directed graph representing the desired\nlinguistic structure. Our experiments on dependency parsing and coreference\nresolution show that our method achieves state-of-the-art or comparable\nperformance. Moreover, the linear complexity and parallelism of our method\ndouble the speed of graph-based coreference resolution models, and bring a\n10-times speed-up over graph-based dependency parsers.", "published": "2023-05-24 11:47:35", "link": "http://arxiv.org/abs/2305.15057v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Who Wrote this Code? Watermarking for Code Generation", "abstract": "Since the remarkable generation performance of large language models raised\nethical and legal concerns, approaches to detect machine-generated text by\nembedding watermarks are being developed. However, we discover that the\nexisting works fail to function appropriately in code generation tasks due to\nthe task's nature of having low entropy. Extending a logit-modifying watermark\nmethod, we propose Selective WatErmarking via Entropy Thresholding (SWEET),\nwhich enhances detection ability and mitigates code quality degeneration by\nremoving low-entropy segments at generating and detecting watermarks. Our\nexperiments show that SWEET significantly improves code quality preservation\nwhile outperforming all baselines, including post-hoc detection methods, in\ndetecting machine-generated code text. Our code is available in\nhttps://github.com/hongcheki/sweet-watermark.", "published": "2023-05-24 11:49:52", "link": "http://arxiv.org/abs/2305.15060v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With\n  Large Language Models", "abstract": "Recent large language models (LLMs) are promising for making decisions in\ngrounded environments. However, LLMs frequently fail in complex decision-making\ntasks due to the misalignment between the pre-trained knowledge in LLMs and the\nactual rules in the environment. Existing methods require either costly\ngradient computation or lengthy in-context demonstrations. In this paper, we\npropose AutoPlan, an approach to guide LLM-based agents to accomplish\ninteractive decision-making tasks. AutoPlan augments the LLM prompt with a\ntask-solving plan and optimizes it through iterative experience collection and\nreflection. Our experiments show that AutoPlan, though using no in-context\ndemonstrations, achieves success rates on par with the baselines using\nhuman-written demonstrations on ALFWorld and even outperforms them by 8% on\nHotpotQA. The code is available at https://github.com/owaski/AutoPlan.", "published": "2023-05-24 11:52:23", "link": "http://arxiv.org/abs/2305.15064v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs\n  without Fine-tuning", "abstract": "While extreme-scale language models have demonstrated exceptional performance\non a variety of language tasks, the degree of control over these language\nmodels through pure prompting can often be limited. Directly fine-tuning such\nlanguage models can be effective for tailoring them, but it can be either\nextremely costly (e.g., GPT-3) or not even feasible for the broader community\n(e.g., GPT-4).\n  We propose Inference-time Policy Adapters (IPA), which efficiently tailors a\nlanguage model such as GPT-3 without fine-tuning it. IPA guides a large base\nmodel during decoding time through a lightweight policy adapter trained to\noptimize an arbitrary user objective with reinforcement learning.\n  On five challenging text generation tasks, such as toxicity reduction and\nlexically constrained generation, IPA consistently brings significant\nimprovements over off-the-shelf language models. It outperforms competitive\nbaseline methods, sometimes even including expensive fine-tuning. In\nparticular, tailoring GPT-2 with IPA can outperform GPT-3, while tailoring\nGPT-3 with IPA brings a major performance boost over GPT-3 (and sometimes even\nover GPT-4). Our promising results highlight the potential of IPA as a\nlightweight alternative to tailoring extreme-scale language models.", "published": "2023-05-24 11:52:55", "link": "http://arxiv.org/abs/2305.15065v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Not All Metrics Are Guilty: Improving NLG Evaluation by Diversifying\n  References", "abstract": "Most research about natural language generation (NLG) relies on evaluation\nbenchmarks with limited references for a sample, which may result in poor\ncorrelations with human judgements. The underlying reason is that one semantic\nmeaning can actually be expressed in different forms, and the evaluation with a\nsingle or few references may not accurately reflect the quality of the model's\nhypotheses. To address this issue, this paper presents a simple and effective\nmethod, named Div-Ref, to enhance existing evaluation benchmarks by enriching\nthe number of references. We leverage large language models (LLMs) to diversify\nthe expression of a single reference into multiple high-quality ones to cover\nthe semantic space of the reference sentence as much as possible. We conduct\ncomprehensive experiments to empirically demonstrate that diversifying the\nexpression of reference can significantly enhance the correlation between\nautomatic evaluation and human evaluation. This idea is compatible with recent\nLLM-based evaluation which can similarly derive advantages from incorporating\nmultiple references. We strongly encourage future generation benchmarks to\ninclude more references, even if they are generated by LLMs, which is once for\nall. We release all the code and data at https://github.com/RUCAIBox/Div-Ref to\nfacilitate research.", "published": "2023-05-24 11:53:29", "link": "http://arxiv.org/abs/2305.15067v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Annotation Imputation to Individualize Predictions: Initial Studies on\n  Distribution Dynamics and Model Predictions", "abstract": "Annotating data via crowdsourcing is time-consuming and expensive. Due to\nthese costs, dataset creators often have each annotator label only a small\nsubset of the data. This leads to sparse datasets with examples that are marked\nby few annotators. The downside of this process is that if an annotator doesn't\nget to label a particular example, their perspective on it is missed. This is\nespecially concerning for subjective NLP datasets where there is no single\ncorrect label: people may have different valid opinions. Thus, we propose using\nimputation methods to generate the opinions of all annotators for all examples,\ncreating a dataset that does not leave out any annotator's view. We then train\nand prompt models, using data from the imputed dataset, to make predictions\nabout the distribution of responses and individual annotations.\n  In our analysis of the results, we found that the choice of imputation method\nsignificantly impacts soft label changes and distribution. While the imputation\nintroduces noise in the prediction of the original dataset, it has shown\npotential in enhancing shots for prompts, particularly for low-response-rate\nannotators. We have made all of our code and data publicly available.", "published": "2023-05-24 11:54:46", "link": "http://arxiv.org/abs/2305.15070v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meta-Learning Online Adaptation of Language Models", "abstract": "Large language models encode impressively broad world knowledge in their\nparameters. However, the knowledge in static language models falls out of date,\nlimiting the model's effective \"shelf life.\" While online fine-tuning can\nreduce this degradation, we find that naively fine-tuning on a stream of\ndocuments leads to a low level of information uptake. We hypothesize that\nonline fine-tuning does not sufficiently attend to important information. That\nis, the gradient signal from important tokens representing factual information\nis drowned out by the gradient from inherently noisy tokens, suggesting that a\ndynamic, context-aware learning rate may be beneficial. We therefore propose\nlearning which tokens to upweight. We meta-train a small, autoregressive model\nto reweight the language modeling loss for each token during online\nfine-tuning, with the objective of maximizing the out-of-date base\nquestion-answering model's ability to answer questions about a document after a\nsingle weighted gradient step. We call this approach Context-aware Meta-learned\nLoss Scaling (CaMeLS). Across three different distributions of documents, our\nexperiments find that CaMeLS provides substantially improved information uptake\non streams of thousands of documents compared with standard fine-tuning and\nbaseline heuristics for reweighting token losses.", "published": "2023-05-24 11:56:20", "link": "http://arxiv.org/abs/2305.15076v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contrastive Learning of Sentence Embeddings from Scratch", "abstract": "Contrastive learning has been the dominant approach to train state-of-the-art\nsentence embeddings. Previous studies have typically learned sentence\nembeddings either through the use of human-annotated natural language inference\n(NLI) data or via large-scale unlabeled sentences in an unsupervised manner.\nHowever, even in the case of unlabeled data, their acquisition presents\nchallenges in certain domains due to various reasons. To address these issues,\nwe present SynCSE, a contrastive learning framework that trains sentence\nembeddings with synthesized data. Specifically, we explore utilizing large\nlanguage models to synthesize the required data samples for contrastive\nlearning, including (1) producing positive and negative annotations given\nunlabeled sentences (SynCSE-partial), and (2) generating sentences along with\ntheir corresponding annotations from scratch (SynCSE-scratch). Experimental\nresults on sentence similarity and reranking tasks indicate that both\nSynCSE-partial and SynCSE-scratch greatly outperform unsupervised baselines,\nand SynCSE-partial even achieves comparable performance to the supervised\nmodels in most settings.", "published": "2023-05-24 11:56:21", "link": "http://arxiv.org/abs/2305.15077v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Eliciting the Translation Ability of Large Language Models via\n  Multilingual Finetuning with Translation Instructions", "abstract": "Large-scale Pretrained Language Models (LLMs), such as ChatGPT and GPT4, have\nshown strong abilities in multilingual translations, without being explicitly\ntrained on parallel corpora. It is interesting how the LLMs obtain their\nability to carry out translation instructions for different languages. In this\npaper, we present a detailed analysis by finetuning a multilingual pretrained\nlanguage model, XGLM-7B, to perform multilingual translation following given\ninstructions. Firstly, we show that multilingual LLMs have stronger translation\nabilities than previously demonstrated. For a certain language, the performance\ndepends on its similarity to English and the amount of data used in the\npretraining phase. Secondly, we find that LLMs' ability to carry out\ntranslation instructions relies on the understanding of translation\ninstructions and the alignment among different languages. With multilingual\nfinetuning, LLMs could learn to perform the translation task well even for\nthose language pairs unseen during the instruction tuning phase.", "published": "2023-05-24 12:00:24", "link": "http://arxiv.org/abs/2305.15083v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Referral Augmentation for Zero-Shot Information Retrieval", "abstract": "We propose Referral-Augmented Retrieval (RAR), a simple technique that\nconcatenates document indices with referrals, i.e. text from other documents\nthat cite or link to the given document, to provide significant performance\ngains for zero-shot information retrieval. The key insight behind our method is\nthat referrals provide a more complete, multi-view representation of a\ndocument, much like incoming page links in algorithms like PageRank provide a\ncomprehensive idea of a webpage's importance. RAR works with both sparse and\ndense retrievers, and outperforms generative text expansion techniques such as\nDocT5Query and Query2Doc a 37% and 21% absolute improvement on ACL paper\nretrieval Recall@10 -- while also eliminating expensive model training and\ninference. We also analyze different methods for multi-referral aggregation and\nshow that RAR enables up-to-date information retrieval without re-training.", "published": "2023-05-24 12:28:35", "link": "http://arxiv.org/abs/2305.15098v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fourier Transformer: Fast Long Range Modeling by Removing Sequence\n  Redundancy with FFT Operator", "abstract": "The transformer model is known to be computationally demanding, and\nprohibitively costly for long sequences, as the self-attention module uses a\nquadratic time and space complexity with respect to sequence length. Many\nresearchers have focused on designing new forms of self-attention or\nintroducing new parameters to overcome this limitation, however a large portion\nof them prohibits the model to inherit weights from large pretrained models. In\nthis work, the transformer's inefficiency has been taken care of from another\nperspective. We propose Fourier Transformer, a simple yet effective approach by\nprogressively removing redundancies in hidden sequence using the ready-made\nFast Fourier Transform (FFT) operator to perform Discrete Cosine Transformation\n(DCT). Fourier Transformer is able to significantly reduce computational costs\nwhile retain the ability to inherit from various large pretrained models.\nExperiments show that our model achieves state-of-the-art performances among\nall transformer-based models on the long-range modeling benchmark LRA with\nsignificant improvement in both speed and space. For generative seq-to-seq\ntasks including CNN/DailyMail and ELI5, by inheriting the BART weights our\nmodel outperforms the standard BART and other efficient models. \\footnote{Our\ncode is publicly available at\n\\url{https://github.com/LUMIA-Group/FourierTransformer}}", "published": "2023-05-24 12:33:06", "link": "http://arxiv.org/abs/2305.15099v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Role of Output Vocabulary in T2T LMs for SPARQL Semantic Parsing", "abstract": "In this work, we analyse the role of output vocabulary for text-to-text (T2T)\nmodels on the task of SPARQL semantic parsing. We perform experiments within\nthe the context of knowledge graph question answering (KGQA), where the task is\nto convert questions in natural language to the SPARQL query language. We\nobserve that the query vocabulary is distinct from human vocabulary. Language\nModels (LMs) are pre-dominantly trained for human language tasks, and hence, if\nthe query vocabulary is replaced with a vocabulary more attuned to the LM\ntokenizer, the performance of models may improve. We carry out carefully\nselected vocabulary substitutions on the queries and find absolute gains in the\nrange of 17% on the GrailQA dataset.", "published": "2023-05-24 12:55:04", "link": "http://arxiv.org/abs/2305.15108v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Another Dead End for Morphological Tags? Perturbed Inputs and Parsing", "abstract": "The usefulness of part-of-speech tags for parsing has been heavily questioned\ndue to the success of word-contextualized parsers. Yet, most studies are\nlimited to coarse-grained tags and high quality written content; while we know\nlittle about their influence when it comes to models in production that face\nlexical errors. We expand these setups and design an adversarial attack to\nverify if the use of morphological information by parsers: (i) contributes to\nerror propagation or (ii) if on the other hand it can play a role to correct\nmistakes that word-only neural parsers make. The results on 14 diverse UD\ntreebanks show that under such attacks, for transition- and graph-based models\ntheir use contributes to degrade the performance even faster, while for the\n(lower-performing) sequence labeling parsers they are helpful. We also show\nthat if morphological tags were utopically robust against lexical\nperturbations, they would be able to correct parsing mistakes.", "published": "2023-05-24 13:11:04", "link": "http://arxiv.org/abs/2305.15119v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-training Multi-party Dialogue Models with Latent Discourse Inference", "abstract": "Multi-party dialogues are more difficult for models to understand than\none-to-one two-party dialogues, since they involve multiple interlocutors,\nresulting in interweaving reply-to relations and information flows. To step\nover these obstacles, an effective way is to pre-train a model that understands\nthe discourse structure of multi-party dialogues, namely, to whom each\nutterance is replying. However, due to the lack of explicitly annotated\ndiscourse labels in multi-party dialogue corpora, previous works fail to scale\nup the pre-training process by putting aside the unlabeled multi-party\nconversational data for nothing. To fully utilize the unlabeled data, we\npropose to treat the discourse structures as latent variables, then jointly\ninfer them and pre-train the discourse-aware model by unsupervised latent\nvariable inference methods. Experiments on multiple downstream tasks show that\nour pre-trained model outperforms strong baselines by large margins and\nachieves state-of-the-art (SOTA) results, justifying the effectiveness of our\nmethod. The official implementation of this paper is available at\nhttps://github.com/EricLee8/MPD_EMVI.", "published": "2023-05-24 14:06:27", "link": "http://arxiv.org/abs/2305.15175v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HiTIN: Hierarchy-aware Tree Isomorphism Network for Hierarchical Text\n  Classification", "abstract": "Hierarchical text classification (HTC) is a challenging subtask of\nmulti-label classification as the labels form a complex hierarchical structure.\nExisting dual-encoder methods in HTC achieve weak performance gains with huge\nmemory overheads and their structure encoders heavily rely on domain knowledge.\nUnder such observation, we tend to investigate the feasibility of a\nmemory-friendly model with strong generalization capability that could boost\nthe performance of HTC without prior statistics or label semantics. In this\npaper, we propose Hierarchy-aware Tree Isomorphism Network (HiTIN) to enhance\nthe text representations with only syntactic information of the label\nhierarchy. Specifically, we convert the label hierarchy into an unweighted tree\nstructure, termed coding tree, with the guidance of structural entropy. Then we\ndesign a structure encoder to incorporate hierarchy-aware information in the\ncoding tree into text representations. Besides the text encoder, HiTIN only\ncontains a few multi-layer perceptions and linear transformations, which\ngreatly saves memory. We conduct experiments on three commonly used datasets\nand the results demonstrate that HiTIN could achieve better test performance\nand less memory consumption than state-of-the-art (SOTA) methods.", "published": "2023-05-24 14:14:08", "link": "http://arxiv.org/abs/2305.15182v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Pre-trained Language Models Useful for Model Ensemble in Chinese\n  Grammatical Error Correction?", "abstract": "Model ensemble has been in widespread use for Grammatical Error Correction\n(GEC), boosting model performance. We hypothesize that model ensemble based on\nthe perplexity (PPL) computed by pre-trained language models (PLMs) should\nbenefit the GEC system. To this end, we explore several ensemble strategies\nbased on strong PLMs with four sophisticated single models. However, the\nperformance does not improve but even gets worse after the PLM-based ensemble.\nThis surprising result sets us doing a detailed analysis on the data and coming\nup with some insights on GEC. The human references of correct sentences is far\nfrom sufficient in the test data, and the gap between a correct sentence and an\nidiomatic one is worth our attention. Moreover, the PLM-based ensemble\nstrategies provide an effective way to extend and improve GEC benchmark data.\nOur source code is available at\nhttps://github.com/JamyDon/PLM-based-CGEC-Model-Ensemble.", "published": "2023-05-24 14:18:52", "link": "http://arxiv.org/abs/2305.15183v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Adaptive Prefix Tuning for Parameter-Efficient Language Model\n  Fine-tuning", "abstract": "Fine-tuning large pre-trained language models on various downstream tasks\nwith whole parameters is prohibitively expensive. Hence, Parameter-efficient\nfine-tuning has attracted attention that only optimizes a few task-specific\nparameters with the frozen pre-trained model. In this work, we focus on prefix\ntuning, which only optimizes continuous prefix vectors (i.e. pseudo tokens)\ninserted into Transformer layers. Based on the observation that the learned\nsyntax and semantics representation varies a lot at different layers, we argue\nthat the adaptive prefix will be further tailored to each layer than the fixed\none, enabling the fine-tuning more effective and efficient. Thus, we propose\nAdaptive Prefix Tuning (APT) to adjust the prefix in terms of both fine-grained\ntoken level and coarse-grained layer level with a gate mechanism. Experiments\non the SuperGLUE and NER datasets show the effectiveness of APT. In addition,\ntaking the gate as a probing, we validate the efficiency and effectiveness of\nthe variable prefix.", "published": "2023-05-24 14:51:01", "link": "http://arxiv.org/abs/2305.15212v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SAIL: Search-Augmented Instruction Learning", "abstract": "Large language models (LLMs) have been significantly improved by instruction\nfine-tuning, but still lack transparency and the ability to utilize up-to-date\nknowledge and information. In this work, we propose search-augmented\ninstruction learning (SAIL), which grounds the language generation and\ninstruction following abilities on complex search results generated by in-house\nand external search engines. With an instruction tuning corpus, we collect\nsearch results for each training case from different search APIs and domains,\nand construct a new search-grounded training set containing\n\\textit{(instruction, grounding information, response)} triplets. We then\nfine-tune the LLaMA-7B model on the constructed training set. Since the\ncollected results contain unrelated and disputing languages, the model needs to\nlearn to ground on trustworthy search results, filter out distracting passages,\nand generate the target response. The search result-denoising process entails\nexplicit trustworthy information selection and multi-hop reasoning, since the\nretrieved passages might be informative but not contain the\ninstruction-following answer. Experiments show that the fine-tuned SAIL-7B\nmodel has a strong instruction-following ability, and it performs significantly\nbetter on transparency-sensitive tasks, including open-ended question answering\nand fact checking.", "published": "2023-05-24 15:07:30", "link": "http://arxiv.org/abs/2305.15225v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Parallel Context Windows: A Frustratingly Simple Alternative\n  and Chain-of-Thought Deterioration", "abstract": "We identify two crucial limitations in the evaluation of recent\nparallel-integrated method Parallel Context Windows (PCW), which extends the\nmaximum context lengths of language models, e.g., 2048 for LLaMA, by harnessing\nwindow-wise attention and positional embedding techniques. We first show that a\nsimple yet strong baseline, weighted sum ensemble, is missing for the\nin-context few-shot classification. Moreover, on more challenging\nChain-of-Thought (CoT) reasoning (e.g., HotpotQA), PCW would present unexpected\ndeterioration regarding question miscomprehension and false inference. Based on\nour findings, we suggest that the existing PCW design may not guarantee\nsufficient improvement and practicality in handling lengthy documents in\nreal-world applications. More community efforts on enabling language models'\nlong context understanding ability should be paid.", "published": "2023-05-24 15:48:29", "link": "http://arxiv.org/abs/2305.15262v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Token Dropping Strategy in Efficient BERT Pretraining", "abstract": "Token dropping is a recently-proposed strategy to speed up the pretraining of\nmasked language models, such as BERT, by skipping the computation of a subset\nof the input tokens at several middle layers. It can effectively reduce the\ntraining time without degrading much performance on downstream tasks. However,\nwe empirically find that token dropping is prone to a semantic loss problem and\nfalls short in handling semantic-intense tasks. Motivated by this, we propose a\nsimple yet effective semantic-consistent learning method (ScTD) to improve the\ntoken dropping. ScTD aims to encourage the model to learn how to preserve the\nsemantic information in the representation space. Extensive experiments on 12\ntasks show that, with the help of our ScTD, token dropping can achieve\nconsistent and significant performance gains across all task types and model\nsizes. More encouragingly, ScTD saves up to 57% of pretraining time and brings\nup to +1.56% average improvement over the vanilla token dropping.", "published": "2023-05-24 15:59:44", "link": "http://arxiv.org/abs/2305.15273v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Evolution Learning for Discriminative Language Model Pretraining", "abstract": "Masked language modeling, widely used in discriminative language model (e.g.,\nBERT) pretraining, commonly adopts a random masking strategy. However, random\nmasking does not consider the importance of the different words in the sentence\nmeaning, where some of them are more worthy to be predicted. Therefore, various\nmasking strategies (e.g., entity-level masking) are proposed, but most of them\nrequire expensive prior knowledge and generally train from scratch without\nreusing existing model weights. In this paper, we present Self-Evolution\nlearning (SE), a simple and effective token masking and learning method to\nfully and wisely exploit the knowledge from data. SE focuses on learning the\ninformative yet under-explored tokens and adaptively regularizes the training\nby introducing a novel Token-specific Label Smoothing approach. Experiments on\n10 tasks show that our SE brings consistent and significant improvements\n(+1.43~2.12 average scores) upon different PLMs. In-depth analyses demonstrate\nthat SE improves linguistic knowledge learning and generalization.", "published": "2023-05-24 16:00:54", "link": "http://arxiv.org/abs/2305.15275v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Retrieval-Augmented Large Language Models with Iterative\n  Retrieval-Generation Synergy", "abstract": "Large language models are powerful text processors and reasoners, but are\nstill subject to limitations including outdated knowledge and hallucinations,\nwhich necessitates connecting them to the world. Retrieval-augmented large\nlanguage models have raised extensive attention for grounding model generation\non external knowledge. However, retrievers struggle to capture relevance,\nespecially for queries with complex information needs. Recent work has proposed\nto improve relevance modeling by having large language models actively involved\nin retrieval, i.e., to improve retrieval with generation. In this paper, we\nshow that strong performance can be achieved by a method we call Iter-RetGen,\nwhich synergizes retrieval and generation in an iterative manner. A model\noutput shows what might be needed to finish a task, and thus provides an\ninformative context for retrieving more relevant knowledge which in turn helps\ngenerate a better output in the next iteration. Compared with recent work which\ninterleaves retrieval with generation when producing an output, Iter-RetGen\nprocesses all retrieved knowledge as a whole and largely preserves the\nflexibility in generation without structural constraints. We evaluate\nIter-RetGen on multi-hop question answering, fact verification, and commonsense\nreasoning, and show that it can flexibly leverage parametric knowledge and\nnon-parametric knowledge, and is superior to or competitive with\nstate-of-the-art retrieval-augmented baselines while causing fewer overheads of\nretrieval and generation. We can further improve performance via\ngeneration-augmented retrieval adaptation.", "published": "2023-05-24 16:17:36", "link": "http://arxiv.org/abs/2305.15294v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncovering and Quantifying Social Biases in Code Generation", "abstract": "With the popularity of automatic code generation tools, such as Copilot, the\nstudy of the potential hazards of these tools is gaining importance. In this\nwork, we explore the social bias problem in pre-trained code generation models.\nWe propose a new paradigm to construct code prompts and successfully uncover\nsocial biases in code generation models. To quantify the severity of social\nbiases in generated code, we develop a dataset along with three metrics to\nevaluate the overall social bias and fine-grained unfairness across different\ndemographics. Experimental results on three pre-trained code generation models\n(Codex, InCoder, and CodeGen) with varying sizes, reveal severe social biases.\nMoreover, we conduct analysis to provide useful insights for further choice of\ncode generation models with low social bias. (This work contains examples that\npotentially implicate stereotypes, associations, and other harms that could be\noffensive to individuals in certain social groups.)", "published": "2023-05-24 17:37:33", "link": "http://arxiv.org/abs/2305.15377v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis Using Aligned Word Embeddings for Uralic Languages", "abstract": "In this paper, we present an approach for translating word embeddings from a\nmajority language into 4 minority languages: Erzya, Moksha, Udmurt and\nKomi-Zyrian. Furthermore, we align these word embeddings and present a novel\nneural network model that is trained on English data to conduct sentiment\nanalysis and then applied on endangered language data through the aligned word\nembeddings. To test our model, we annotated a small sentiment analysis corpus\nfor the 4 endangered languages and Finnish. Our method reached at least 56\\%\naccuracy for each endangered language. The models and the sentiment corpus will\nbe released together with this paper. Our research shows that state-of-the-art\nneural models can be used with endangered languages with the only requirement\nbeing a dictionary between the endangered language and a majority language.", "published": "2023-05-24 17:40:20", "link": "http://arxiv.org/abs/2305.15380v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparing Humans and Models on a Similar Scale: Towards Cognitive Gender\n  Bias Evaluation in Coreference Resolution", "abstract": "Spurious correlations were found to be an important factor explaining model\nperformance in various NLP tasks (e.g., gender or racial artifacts), often\nconsidered to be ''shortcuts'' to the actual task. However, humans tend to\nsimilarly make quick (and sometimes wrong) predictions based on societal and\ncognitive presuppositions. In this work we address the question: can we\nquantify the extent to which model biases reflect human behaviour? Answering\nthis question will help shed light on model performance and provide meaningful\ncomparisons against humans. We approach this question through the lens of the\ndual-process theory for human decision-making. This theory differentiates\nbetween an automatic unconscious (and sometimes biased) ''fast system'' and a\n''slow system'', which when triggered may revisit earlier automatic reactions.\nWe make several observations from two crowdsourcing experiments of gender bias\nin coreference resolution, using self-paced reading to study the ''fast''\nsystem, and question answering to study the ''slow'' system under a constrained\ntime setting. On real-world data humans make $\\sim$3\\% more gender-biased\ndecisions compared to models, while on synthetic data models are $\\sim$12\\%\nmore biased.", "published": "2023-05-24 17:51:44", "link": "http://arxiv.org/abs/2305.15389v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deriving Language Models from Masked Language Models", "abstract": "Masked language models (MLM) do not explicitly define a distribution over\nlanguage, i.e., they are not language models per se. However, recent work has\nimplicitly treated them as such for the purposes of generation and scoring.\nThis paper studies methods for deriving explicit joint distributions from MLMs,\nfocusing on distributions over two tokens, which makes it possible to calculate\nexact distributional properties. We find that an approach based on identifying\njoints whose conditionals are closest to those of the MLM works well and\noutperforms existing Markov random field-based approaches. We further find that\nthis derived model's conditionals can even occasionally outperform the original\nMLM's conditionals.", "published": "2023-05-24 18:42:45", "link": "http://arxiv.org/abs/2305.15501v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Free Lunch for Efficient Textual Commonsense Integration in Language\n  Models", "abstract": "Recent years have witnessed the emergence of textual commonsense knowledge\nbases, aimed at providing more nuanced and context-rich knowledge. The\nintegration of external commonsense into language models has been shown to be a\nkey enabler in advancing the state-of-the-art for a wide range of NLP tasks.\nHowever, incorporating textual commonsense descriptions is computationally\nexpensive, as compared to encoding conventional symbolic knowledge. In this\npaper, we propose a method to improve its efficiency without modifying the\nmodel. We group training samples with similar commonsense descriptions into a\nsingle batch, thus reusing the encoded description across multiple samples. One\nkey observation is that the upper bound of batch partitioning can be reduced to\nthe classic {\\it graph k-cut problem}. Consequently, we propose a spectral\nclustering-based algorithm to solve this problem. Extensive experiments\nillustrate that the proposed batch partitioning approach effectively reduces\nthe computational cost while preserving performance. The efficiency improvement\nis more pronounced on larger datasets and on devices with more memory capacity,\nattesting to its practical utility for large-scale applications.", "published": "2023-05-24 19:14:57", "link": "http://arxiv.org/abs/2305.15516v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Automatically Perturbed Natural Language Explanations in\n  Relation Extraction", "abstract": "Previous research has demonstrated that natural language explanations provide\nvaluable inductive biases that guide models, thereby improving the\ngeneralization ability and data efficiency. In this paper, we undertake a\nsystematic examination of the effectiveness of these explanations. Remarkably,\nwe find that corrupted explanations with diminished inductive biases can\nachieve competitive or superior performance compared to the original\nexplanations. Our findings furnish novel insights into the characteristics of\nnatural language explanations in the following ways: (1) the impact of\nexplanations varies across different training styles and datasets, with\npreviously believed improvements primarily observed in frozen language models.\n(2) While previous research has attributed the effect of explanations solely to\ntheir inductive biases, our study shows that the effect persists even when the\nexplanations are completely corrupted. We propose that the main effect is due\nto the provision of additional context space. (3) Utilizing the proposed\nautomatic perturbed context, we were able to attain comparable results to\nannotated explanations, but with a significant increase in computational\nefficiency, 20-30 times faster.", "published": "2023-05-24 19:17:13", "link": "http://arxiv.org/abs/2305.15520v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Refugee Case Analysis: An NLP Pipeline for Supporting Legal\n  Practitioners", "abstract": "In this paper, we introduce an end-to-end pipeline for retrieving,\nprocessing, and extracting targeted information from legal cases. We\ninvestigate an under-studied legal domain with a case study on refugee law in\nCanada. Searching case law for past similar cases is a key part of legal work\nfor both lawyers and judges, the potential end-users of our prototype. While\ntraditional named-entity recognition labels such as dates provide meaningful\ninformation in legal work, we propose to extend existing models and retrieve a\ntotal of 19 useful categories of items from refugee cases. After creating a\nnovel data set of cases, we perform information extraction based on\nstate-of-the-art neural named-entity recognition (NER). We test different\narchitectures including two transformer models, using contextual and\nnon-contextual embeddings, and compare general purpose versus domain-specific\npre-training. The results demonstrate that models pre-trained on legal data\nperform best despite their smaller size, suggesting that domain matching had a\nlarger effect than network architecture. We achieve a F1 score above 90% on\nfive of the targeted categories and over 80% on four further categories.", "published": "2023-05-24 19:37:23", "link": "http://arxiv.org/abs/2305.15533v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Balancing Effect of Training Dataset Distribution of Multiple Styles for\n  Multi-Style Text Transfer", "abstract": "Text style transfer is an exciting task within the field of natural language\ngeneration that is often plagued by the need for high-quality paired datasets.\nFurthermore, training a model for multi-attribute text style transfer requires\ndatasets with sufficient support across all combinations of the considered\nstylistic attributes, adding to the challenges of training a style transfer\nmodel. This paper explores the impact of training data input diversity on the\nquality of the generated text from the multi-style transfer model. We construct\na pseudo-parallel dataset by devising heuristics to adjust the style\ndistribution in the training samples. We balance our training dataset using\nmarginal and joint distributions to train our style transfer models. We observe\nthat a balanced dataset produces more effective control effects over multiple\nstyles than an imbalanced or skewed one. Through quantitative analysis, we\nexplore the impact of multiple style distributions in training data on\nstyle-transferred output. These findings will better inform the design of\nstyle-transfer datasets.", "published": "2023-05-24 21:36:15", "link": "http://arxiv.org/abs/2305.15582v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Sentence Union Generation as a Testbed for Text Consolidation", "abstract": "Tasks involving text generation based on multiple input texts, such as\nmulti-document summarization, long-form question answering and contemporary\ndialogue applications, challenge models for their ability to properly\nconsolidate partly-overlapping multi-text information. However, these tasks\nentangle the consolidation phase with the often subjective and ill-defined\ncontent selection requirement, impeding proper assessment of models'\nconsolidation capabilities. In this paper, we suggest revisiting the sentence\nunion generation task as an effective well-defined testbed for assessing text\nconsolidation capabilities, decoupling the consolidation challenge from\nsubjective content selection. To support research on this task, we present\nrefined annotation methodology and tools for crowdsourcing sentence union,\ncreate the largest union dataset to date and provide an analysis of its rich\ncoverage of various consolidation aspects. We then propose a comprehensive\nevaluation protocol for union generation, including both human and automatic\nevaluation. Finally, as baselines, we evaluate state-of-the-art language models\non the task, along with a detailed analysis of their capacity to address\nmulti-text consolidation challenges and their limitations.", "published": "2023-05-24 22:34:01", "link": "http://arxiv.org/abs/2305.15605v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RE$^2$: Region-Aware Relation Extraction from Visually Rich Documents", "abstract": "Current research in form understanding predominantly relies on large\npre-trained language models, necessitating extensive data for pre-training.\nHowever, the importance of layout structure (i.e., the spatial relationship\nbetween the entity blocks in the visually rich document) to relation extraction\nhas been overlooked. In this paper, we propose REgion-Aware Relation Extraction\n(RE$^2$) that leverages region-level spatial structure among the entity blocks\nto improve their relation prediction. We design an edge-aware graph attention\nnetwork to learn the interaction between entities while considering their\nspatial relationship defined by their region-level representations. We also\nintroduce a constraint objective to regularize the model towards consistency\nwith the inherent constraints of the relation extraction task. Extensive\nexperiments across various datasets, languages and domains demonstrate the\nsuperiority of our proposed approach.", "published": "2023-05-24 00:07:40", "link": "http://arxiv.org/abs/2305.14590v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ALGO: Synthesizing Algorithmic Programs with LLM-Generated Oracle\n  Verifiers", "abstract": "Large language models (LLMs) excel at implementing code from functionality\ndescriptions but struggle with algorithmic problems that require not only\nimplementation but also identification of the suitable algorithm. Moreover,\nLLM-generated programs lack guaranteed correctness and require human\nverification. To address these challenges, we propose ALGO, a framework that\nsynthesizes Algorithmic programs with LLM-Generated Oracles to guide the\ngeneration and verify their correctness. ALGO first generates a reference\noracle by prompting an LLM to exhaustively enumerate all the combinations of\nrelevant variables. This oracle is then utilized to guide an arbitrary search\nstrategy in exploring the algorithm space and to verify the synthesized\nalgorithms. Our study shows that the LLM-generated oracles are correct for 88%\nof the cases. With the oracles as verifiers, ALGO can be integrated with any\nexisting code generation model in a model-agnostic manner to enhance its\nperformance. Experiments show that when equipped with ALGO, we achieve an 8x\nbetter one-submission pass rate over the Codex model and a 2.6x better\none-submission pass rate over CodeT, the current state-of-the-art model on\nCodeContests. We can also get 1.3x better pass rate over the ChatGPT Code\nInterpreter on unseen problems. The problem set we used for testing, the\nprompts we used, the verifier and solution programs, and the test cases\ngenerated by ALGO are available at https://github.com/zkx06111/ALGO.", "published": "2023-05-24 00:10:15", "link": "http://arxiv.org/abs/2305.14591v3", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Meta-Tuning LLMs to Leverage Lexical Knowledge for Generalizable\n  Language Style Understanding", "abstract": "Language style is often used by writers to convey their intentions,\nidentities, and mastery of language. In this paper, we show that current large\nlanguage models struggle to capture some language styles without fine-tuning.\nTo address this challenge, we investigate whether LLMs can be meta-trained\nbased on representative lexicons to recognize new styles they have not been\nfine-tuned on. Experiments on 13 established style classification tasks, as\nwell as 63 novel tasks generated using LLMs, demonstrate that meta-training\nwith style lexicons consistently improves zero-shot transfer across styles. We\nrelease the code and data at http://github.com/octaviaguo/Style-LLM .", "published": "2023-05-24 00:17:36", "link": "http://arxiv.org/abs/2305.14592v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Increasing Probability Mass on Answer Choices Does Not Always Improve\n  Accuracy", "abstract": "When pretrained language models (LMs) are applied to discriminative tasks\nsuch as multiple-choice questions, they place probability mass on vocabulary\ntokens that aren't among the given answer choices. Spreading probability mass\nacross multiple surface forms with identical meaning (such as \"bath\" and\n\"bathtub\") is thought to cause an underestimation of a model's true\nperformance, referred to as the \"surface form competition\" (SFC) hypothesis.\nThis has motivated the introduction of various probability normalization\nmethods. However, many core questions remain unanswered. How do we measure SFC?\nAre there direct ways of reducing it, and does doing so improve task\nperformance?\n  We propose a mathematical formalism for SFC which allows us to quantify and\nbound its impact for the first time. We identify a simple method for reducing\nit -- namely, increasing probability mass on the given answer choices by a)\nincluding them in the prompt and b) using in-context learning with even just\none example. We show this method eliminates the impact of SFC in the majority\nof instances. Our experiments on three diverse datasets and six LMs reveal\nseveral additional surprising findings. For example, both normalization and\nprompting methods for reducing SFC can be ineffective or even detrimental to\ntask performance for some LMs. We conclude with practical insights for\neffectively prompting LMs for multiple-choice tasks.", "published": "2023-05-24 00:27:00", "link": "http://arxiv.org/abs/2305.14596v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Semantic Role Labeling from Compatible Label Sequences", "abstract": "Semantic role labeling (SRL) has multiple disjoint label sets, e.g., VerbNet\nand PropBank. Creating these datasets is challenging, therefore a natural\nquestion is how to use each one to help the other. Prior work has shown that\ncross-task interaction helps, but only explored multitask learning so far. A\ncommon issue with multi-task setup is that argument sequences are still\nseparately decoded, running the risk of generating structurally inconsistent\nlabel sequences (as per lexicons like Semlink). In this paper, we eliminate\nsuch issue with a framework that jointly models VerbNet and PropBank labels as\none sequence. In this setup, we show that enforcing Semlink constraints during\ndecoding constantly improves the overall F1. With special input constructions,\nour joint model infers VerbNet arguments from given PropBank arguments with\nover 99 F1. For learning, we propose a constrained marginal model that learns\nwith knowledge defined in Semlink to further benefit from the large amounts of\nPropBank-only data. On the joint benchmark based on CoNLL05, our models achieve\nstate-of-the-art F1's, outperforming the prior best in-domain model by 3.5\n(VerbNet) and 0.8 (PropBank). For out-of-domain generalization, our models\nsurpass the prior best by 3.4 (VerbNet) and 0.2 (PropBank).", "published": "2023-05-24 00:46:02", "link": "http://arxiv.org/abs/2305.14600v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Selectively Answering Ambiguous Questions", "abstract": "Trustworthy language models should abstain from answering questions when they\ndo not know the answer. However, the answer to a question can be unknown for a\nvariety of reasons. Prior research has focused on the case in which the\nquestion is clear and the answer is unambiguous but possibly unknown, but the\nanswer to a question can also be unclear due to uncertainty of the questioner's\nintent or context. We investigate question answering from this perspective,\nfocusing on answering a subset of questions with a high degree of accuracy,\nfrom a set of questions in which many are inherently ambiguous. In this\nsetting, we find that the most reliable approach to decide when to abstain\ninvolves quantifying repetition within sampled model outputs, rather than the\nmodel's likelihood or self-verification as used in prior work. We find this to\nbe the case across different types of uncertainty and model scales,and with or\nwithout instruction tuning. Our results suggest that sampling-based confidence\nscores help calibrate answers to relatively unambiguous questions, with more\ndramatic improvements on ambiguous questions.", "published": "2023-05-24 01:25:38", "link": "http://arxiv.org/abs/2305.14613v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Affordance and Situated Meaning in Image Captions: A\n  Multimodal Analysis", "abstract": "This paper explores the grounding issue regarding multimodal semantic\nrepresentation from a computational cognitive-linguistic view. We annotate\nimages from the Flickr30k dataset with five perceptual properties: Affordance,\nPerceptual Salience, Object Number, Gaze Cueing, and Ecological Niche\nAssociation (ENA), and examine their association with textual elements in the\nimage captions. Our findings reveal that images with Gibsonian affordance show\na higher frequency of captions containing 'holding-verbs' and 'container-nouns'\ncompared to images displaying telic affordance. Perceptual Salience, Object\nNumber, and ENA are also associated with the choice of linguistic expressions.\nOur study demonstrates that comprehensive understanding of objects or events\nrequires cognitive attention, semantic nuances in language, and integration\nacross multiple modalities. We highlight the vital importance of situated\nmeaning and affordance grounding in natural language understanding, with the\npotential to advance human-like interpretation in various scenarios.", "published": "2023-05-24 01:30:50", "link": "http://arxiv.org/abs/2305.14616v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "COMET-M: Reasoning about Multiple Events in Complex Sentences", "abstract": "Understanding the speaker's intended meaning often involves drawing\ncommonsense inferences to reason about what is not stated explicitly. In\nmulti-event sentences, it requires understanding the relationships between\nevents based on contextual knowledge. We propose COMET-M (Multi-Event), an\nevent-centric commonsense model capable of generating commonsense inferences\nfor a target event within a complex sentence. COMET-M builds upon COMET\n(Bosselut et al., 2019), which excels at generating event-centric inferences\nfor simple sentences, but struggles with the complexity of multi-event\nsentences prevalent in natural text. To overcome this limitation, we curate a\nmulti-event inference dataset of 35K human-written inferences. We trained\nCOMET-M on the human-written inferences and also created baselines using\nautomatically labeled examples. Experimental results demonstrate the\nsignificant performance improvement of COMET-M over COMET in generating\nmulti-event inferences. Moreover, COMET-M successfully produces distinct\ninferences for each target event, taking the complete context into\nconsideration. COMET-M holds promise for downstream tasks involving natural\ntext such as coreference resolution, dialogue, and story understanding.", "published": "2023-05-24 01:35:01", "link": "http://arxiv.org/abs/2305.14617v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Abductive Commonsense Reasoning Exploiting Mutually Exclusive\n  Explanations", "abstract": "Abductive reasoning aims to find plausible explanations for an event. This\nstyle of reasoning is critical for commonsense tasks where there are often\nmultiple plausible explanations. Existing approaches for abductive reasoning in\nnatural language processing (NLP) often rely on manually generated annotations\nfor supervision; however, such annotations can be subjective and biased.\nInstead of using direct supervision, this work proposes an approach for\nabductive commonsense reasoning that exploits the fact that only a subset of\nexplanations is correct for a given context. The method uses posterior\nregularization to enforce a mutual exclusion constraint, encouraging the model\nto learn the distinction between fluent explanations and plausible ones. We\nevaluate our approach on a diverse set of abductive reasoning datasets;\nexperimental results show that our approach outperforms or is comparable to\ndirectly applying pretrained language models in a zero-shot manner and other\nknowledge-augmented zero-shot methods.", "published": "2023-05-24 01:35:10", "link": "http://arxiv.org/abs/2305.14618v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EXnet: Efficient In-context Learning for Data-less Text classification", "abstract": "Large pre-trained language models (PLMs) have made significant progress in\nencoding world knowledge and spawned a new set of learning paradigms including\nzero-shot, few-shot, and in-context learning. Many language tasks can be\nmodeled as a set of prompts (for example, is this text about geography?) and\nlanguage models can provide binary answers, i.e., Yes or No. There is evidence\nto suggest that the next-word prediction used by many PLMs does not align well\nwith zero-shot paradigms. Therefore, PLMs are fine-tuned as a\nquestion-answering system. In-context learning extends zero-shot learning by\nincorporating prompts and examples, resulting in increased task accuracy. Our\npaper presents EXnet, a model specifically designed to perform in-context\nlearning without any limitations on the number of examples. We argue that\nin-context learning is an effective method to increase task accuracy, and\nproviding examples facilitates cross-task generalization, especially when it\ncomes to text classification tasks. With extensive experiments, we show that\neven our smallest model (15M parameters) generalizes to several unseen\nclassification tasks and domains.", "published": "2023-05-24 01:40:57", "link": "http://arxiv.org/abs/2305.14622v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Getting MoRE out of Mixture of Language Model Reasoning Experts", "abstract": "While recent large language models (LLMs) improve on various question\nanswering (QA) datasets, it remains difficult for a single model to generalize\nacross question types that require distinct reasoning abilities. We provide\nempirical evidence that state-of-the-art LLMs suffer from poor generalizability\non reasoning types beyond those seen in the prompt. To remedy this, we propose\na Mixture-of-Reasoning-Experts (MoRE) framework that ensembles diverse\nspecialized language models. We specialize the backbone language model with\nprompts optimized for different reasoning categories, including factual,\nmultihop, mathematical, and commonsense reasoning. Our key insight is to\nleverage agreement among the specialized experts to select the best answer for\neach question, or to abstain from answering. This gives MoRE higher accuracy\nthan any single specialized model on a collection of 12 QA datasets from four\nreasoning types. Beyond generalizability, the interpretable design of MoRE\nimproves selective question answering results compared to baselines without\nincorporating inter-expert agreement. This framework is also more interpretable\nand useful to human consumers of QA outputs. Our human study confirms that\npresenting expert predictions and the answer selection process helps annotators\nmore accurately calibrate when to trust the system's output. We release all\ncode and data to facilitate future work.", "published": "2023-05-24 02:00:51", "link": "http://arxiv.org/abs/2305.14628v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ExpertPrompting: Instructing Large Language Models to be Distinguished\n  Experts", "abstract": "The answering quality of an aligned large language model (LLM) can be\ndrastically improved if treated with proper crafting of prompts. In this paper,\nwe propose ExpertPrompting to elicit the potential of LLMs to answer as\ndistinguished experts. We first utilize In-Context Learning to automatically\nsynthesize detailed and customized descriptions of the expert identity for each\nspecific instruction, and then ask LLMs to provide answer conditioned on such\nagent background. Based on this augmented prompting strategy, we produce a new\nset of instruction-following data using GPT-3.5, and train a competitive\nopen-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation\nto show that 1) the expert data is of significantly higher quality than vanilla\nanswers, and 2) ExpertLLaMA outperforms existing open-source opponents and\nachieves 96\\% of the original ChatGPT's capability. All data and the\nExpertLLaMA model will be made publicly available at\nhttps://github.com/OFA-Sys/ExpertLLaMA.", "published": "2023-05-24 03:51:31", "link": "http://arxiv.org/abs/2305.14688v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Have Large Language Models Developed a Personality?: Applicability of\n  Self-Assessment Tests in Measuring Personality in LLMs", "abstract": "Have Large Language Models (LLMs) developed a personality? The short answer\nis a resounding \"We Don't Know!\". In this paper, we show that we do not yet\nhave the right tools to measure personality in language models. Personality is\nan important characteristic that influences behavior. As LLMs emulate\nhuman-like intelligence and performance in various tasks, a natural question to\nask is whether these models have developed a personality. Previous works have\nevaluated machine personality through self-assessment personality tests, which\nare a set of multiple-choice questions created to evaluate personality in\nhumans. A fundamental assumption here is that human personality tests can\naccurately measure personality in machines. In this paper, we investigate the\nemergence of personality in five LLMs of different sizes ranging from 1.5B to\n30B. We propose the Option-Order Symmetry property as a necessary condition for\nthe reliability of these self-assessment tests. Under this condition, the\nanswer to self-assessment questions is invariant to the order in which the\noptions are presented. We find that many LLMs personality test responses do not\npreserve option-order symmetry. We take a deeper look at LLMs test responses\nwhere option-order symmetry is preserved to find that in these cases, LLMs do\nnot take into account the situational statement being tested and produce the\nexact same answer irrespective of the situation being tested. We also identify\nthe existence of inherent biases in these LLMs which is the root cause of the\naforementioned phenomenon and makes self-assessment tests unreliable. These\nobservations indicate that self-assessment tests are not the correct tools to\nmeasure personality in LLMs. Through this paper, we hope to draw attention to\nthe shortcomings of current literature in measuring personality in LLMs and\ncall for developing tools for machine personality measurement.", "published": "2023-05-24 03:53:43", "link": "http://arxiv.org/abs/2305.14693v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Modeling rapid language learning by distilling Bayesian priors into\n  artificial neural networks", "abstract": "Humans can learn languages from remarkably little experience. Developing\ncomputational models that explain this ability has been a major challenge in\ncognitive science. Bayesian models that build in strong inductive biases -\nfactors that guide generalization - have been successful at explaining how\nhumans might generalize from few examples in controlled settings but are\nusually too restrictive to be tractably applied to more naturalistic data. By\ncontrast, neural networks have flexible representations that allow them to\nlearn well from naturalistic data but require many more examples than humans\nreceive. We show that learning from limited naturalistic data is possible with\nan approach that combines the strong inductive biases of a Bayesian model with\nthe flexible representations of a neural network. This approach works by\ndistilling a Bayesian model's biases into a neural network. Like a Bayesian\nmodel, the resulting system can learn formal linguistic patterns from a small\nnumber of examples. Like a neural network, it can also learn aspects of English\nsyntax from a corpus of natural language - and it outperforms a standard neural\nnetwork at acquiring the linguistic phenomena of recursion and priming.\nBridging the divide between Bayesian models and neural networks makes it\npossible to handle a broader range of learning scenarios than either approach\ncan handle on its own.", "published": "2023-05-24 04:11:59", "link": "http://arxiv.org/abs/2305.14701v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploiting Correlations Between Contexts and Definitions with Multiple\n  Definition Modeling", "abstract": "Definition modeling is an important task in advanced natural language\napplications such as understanding and conversation. Since its introduction, it\nfocus on generating one definition for a target word or phrase in a given\ncontext, which we refer to as Single Definition Modeling (SDM). However, this\napproach does not adequately model the correlations and patterns among\ndifferent contexts and definitions of words. In addition, the creation of a\ntraining dataset for SDM requires significant human expertise and effort. In\nthis paper, we carefully design a new task called Multiple Definition Modeling\n(MDM) that pool together all contexts and definition of target words. We\ndemonstrate the ease of creating a model as well as multiple training sets\nautomatically. % In the experiments, we demonstrate and analyze the benefits of\nMDM, including improving SDM's performance by using MDM as the pretraining task\nand its comparable performance in the zero-shot setting.", "published": "2023-05-24 04:38:29", "link": "http://arxiv.org/abs/2305.14717v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "In-Context Demonstration Selection with Cross Entropy Difference", "abstract": "Large language models (LLMs) can use in-context demonstrations to improve\nperformance on zero-shot tasks. However, selecting the best in-context examples\nis challenging because model performance can vary widely depending on the\nselected examples. We present a cross-entropy difference (CED) method for\nselecting in-context demonstrations. Our method is based on the observation\nthat the effectiveness of in-context demonstrations negatively correlates with\nthe perplexity of the test example by a language model that was finetuned on\nthat demonstration. We utilize parameter efficient finetuning to train small\nmodels on training data that are used for computing the cross-entropy\ndifference between a test example and every candidate in-context demonstration.\nThis metric is used to rank and select in-context demonstrations independently\nfor each test input. We evaluate our method on a mix-domain dataset that\ncombines 8 benchmarks, representing 4 text generation tasks, showing that CED\nfor in-context demonstration selection can improve performance for a variety of\nLLMs.", "published": "2023-05-24 05:04:00", "link": "http://arxiv.org/abs/2305.14726v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DialogVCS: Robust Natural Language Understanding in Dialogue System\n  Upgrade", "abstract": "In the constant updates of the product dialogue systems, we need to retrain\nthe natural language understanding (NLU) model as new data from the real users\nwould be merged into the existent data accumulated in the last updates. Within\nthe newly added data, new intents would emerge and might have semantic\nentanglement with the existing intents, e.g. new intents that are semantically\ntoo specific or generic are actually subset or superset of some existing\nintents in the semantic space, thus impairing the robustness of the NLU model.\nAs the first attempt to solve this problem, we setup a new benchmark consisting\nof 4 Dialogue Version Control dataSets (DialogVCS). We formulate the intent\ndetection with imperfect data in the system update as a multi-label\nclassification task with positive but unlabeled intents, which asks the models\nto recognize all the proper intents, including the ones with semantic\nentanglement, in the inference. We also propose comprehensive baseline models\nand conduct in-depth analyses for the benchmark, showing that the semantically\nentangled intents can be effectively recognized with an automatic workflow.", "published": "2023-05-24 05:53:38", "link": "http://arxiv.org/abs/2305.14751v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Don't Take This Out of Context! On the Need for Contextual Models and\n  Evaluations for Stylistic Rewriting", "abstract": "Most existing stylistic text rewriting methods and evaluation metrics operate\non a sentence level, but ignoring the broader context of the text can lead to\npreferring generic, ambiguous, and incoherent rewrites. In this paper, we\ninvestigate integrating the preceding textual context into both the\n$\\textit{rewriting}$ and $\\textit{evaluation}$ stages of stylistic text\nrewriting, and introduce a new composite contextual evaluation metric\n$\\texttt{CtxSimFit}$ that combines similarity to the original sentence with\ncontextual cohesiveness. We comparatively evaluate non-contextual and\ncontextual rewrites in formality, toxicity, and sentiment transfer tasks. Our\nexperiments show that humans significantly prefer contextual rewrites as more\nfitting and natural over non-contextual ones, yet existing sentence-level\nautomatic metrics (e.g., ROUGE, SBERT) correlate poorly with human preferences\n($\\rho$=0--0.3). In contrast, human preferences are much better reflected by\nboth our novel $\\texttt{CtxSimFit}$ ($\\rho$=0.7--0.9) as well as proposed\ncontext-infused versions of common metrics ($\\rho$=0.4--0.7). Overall, our\nfindings highlight the importance of integrating context into the generation\nand especially the evaluation stages of stylistic text rewriting.", "published": "2023-05-24 05:58:17", "link": "http://arxiv.org/abs/2305.14755v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Simple Linguistic Inferences of Large Language Models (LLMs): Blind\n  Spots and Blinds", "abstract": "We evaluate LLMs' language understanding capacities on simple inference tasks\nthat most humans find trivial. Specifically, we target (i)\ngrammatically-specified entailments, (ii) premises with evidential adverbs of\nuncertainty, and (iii) monotonicity entailments. We design evaluation sets for\nthese tasks and conduct experiments in both zero-shot and chain-of-thought\nsetups, and with multiple prompts and LLMs. The models exhibit moderate to low\nperformance on these evaluation sets. Subsequent experiments show that\nembedding the premise in syntactic constructions that should preserve the\nentailment relations (presupposition triggers) or change them (non-factives),\nfurther confuses the models, causing them to either under-predict or\nover-predict certain entailment labels regardless of the true relation, and\noften disregarding the nature of the embedding context. Overall these results\nsuggest that, despite LLMs' celebrated language understanding capacity, even\nthe strongest models have blindspots with respect to certain types of\nentailments, and certain information-packaging structures act as ``blinds''\novershadowing the semantics of the embedded premise.", "published": "2023-05-24 06:41:09", "link": "http://arxiv.org/abs/2305.14785v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Advancing Topic Segmentation and Outline Generation in Chinese Texts:\n  The Paragraph-level Topic Representation, Corpus, and Benchmark", "abstract": "Topic segmentation and outline generation strive to divide a document into\ncoherent topic sections and generate corresponding subheadings, unveiling the\ndiscourse topic structure of a document. Compared with sentence-level topic\nstructure, the paragraph-level topic structure can quickly grasp and understand\nthe overall context of the document from a higher level, benefitting many\ndownstream tasks such as summarization, discourse parsing, and information\nretrieval. However, the lack of large-scale, high-quality Chinese\nparagraph-level topic structure corpora restrained relative research and\napplications. To fill this gap, we build the Chinese paragraph-level topic\nrepresentation, corpus, and benchmark in this paper. Firstly, we propose a\nhierarchical paragraph-level topic structure representation with three layers\nto guide the corpus construction. Then, we employ a two-stage man-machine\ncollaborative annotation method to construct the largest Chinese\nParagraph-level Topic Structure corpus (CPTS), achieving high quality. We also\nbuild several strong baselines, including ChatGPT, to validate the\ncomputability of CPTS on two fundamental tasks (topic segmentation and outline\ngeneration) and preliminarily verified its usefulness for the downstream task\n(discourse parsing).", "published": "2023-05-24 06:43:23", "link": "http://arxiv.org/abs/2305.14790v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Machine Reading Comprehension using Case-based Reasoning", "abstract": "We present an accurate and interpretable method for answer extraction in\nmachine reading comprehension that is reminiscent of case-based reasoning (CBR)\nfrom classical AI. Our method (CBR-MRC) builds upon the hypothesis that\ncontextualized answers to similar questions share semantic similarities with\neach other. Given a test question, CBR-MRC first retrieves a set of similar\ncases from a nonparametric memory and then predicts an answer by selecting the\nspan in the test context that is most similar to the contextualized\nrepresentations of answers in the retrieved cases. The semi-parametric nature\nof our approach allows it to attribute a prediction to the specific set of\nevidence cases, making it a desirable choice for building reliable and\ndebuggable QA systems. We show that CBR-MRC provides high accuracy comparable\nwith large reader models and outperforms baselines by 11.5 and 8.4 EM on\nNaturalQuestions and NewsQA, respectively. Further, we demonstrate the ability\nof CBR-MRC in identifying not just the correct answer tokens but also the span\nwith the most relevant supporting evidence. Lastly, we observe that contexts\nfor certain question types show higher lexical diversity than others and find\nthat CBR-MRC is robust to these variations while performance using\nfully-parametric methods drops.", "published": "2023-05-24 07:09:56", "link": "http://arxiv.org/abs/2305.14815v4", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Large Language Models are In-Context Semantic Reasoners rather than\n  Symbolic Reasoners", "abstract": "The emergent few-shot reasoning capabilities of Large Language Models (LLMs)\nhave excited the natural language and machine learning community over recent\nyears. Despite of numerous successful applications, the underlying mechanism of\nsuch in-context capabilities still remains unclear. In this work, we\nhypothesize that the learned \\textit{semantics} of language tokens do the most\nheavy lifting during the reasoning process. Different from human's symbolic\nreasoning process, the semantic representations of LLMs could create strong\nconnections among tokens, thus composing a superficial logical chain. To test\nour hypothesis, we decouple semantics from the language reasoning process and\nevaluate three kinds of reasoning abilities, i.e., deduction, induction and\nabduction. Our findings reveal that semantics play a vital role in LLMs'\nin-context reasoning -- LLMs perform significantly better when semantics are\nconsistent with commonsense but struggle to solve symbolic or\ncounter-commonsense reasoning tasks by leveraging in-context new knowledge. The\nsurprising observations question whether modern LLMs have mastered the\ninductive, deductive and abductive reasoning abilities as in human\nintelligence, and motivate research on unveiling the magic existing within the\nblack-box LLMs. On the whole, our analysis provides a novel perspective on the\nrole of semantics in developing and evaluating language models' reasoning\nabilities. Code is available at {\\url{https://github.com/XiaojuanTang/ICSR}}.", "published": "2023-05-24 07:33:34", "link": "http://arxiv.org/abs/2305.14825v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Few-shot Entity Recognition in Document Images: A Graph Neural\n  Network Approach Robust to Image Manipulation", "abstract": "Recent advances of incorporating layout information, typically bounding box\ncoordinates, into pre-trained language models have achieved significant\nperformance in entity recognition from document images. Using coordinates can\neasily model the absolute position of each token, but they might be sensitive\nto manipulations in document images (e.g., shifting, rotation or scaling),\nespecially when the training data is limited in few-shot settings. In this\npaper, we propose to further introduce the topological adjacency relationship\namong the tokens, emphasizing their relative position information.\nSpecifically, we consider the tokens in the documents as nodes and formulate\nthe edges based on the topological heuristics from the k-nearest bounding\nboxes. Such adjacency graphs are invariant to affine transformations including\nshifting, rotations and scaling. We incorporate these graphs into the\npre-trained language model by adding graph neural network layers on top of the\nlanguage model embeddings, leading to a novel model LAGER. Extensive\nexperiments on two benchmark datasets show that LAGER significantly outperforms\nstrong baselines under different few-shot settings and also demonstrate better\nrobustness to manipulations.", "published": "2023-05-24 07:34:33", "link": "http://arxiv.org/abs/2305.14828v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "PaCE: Unified Multi-modal Dialogue Pre-training with Progressive and\n  Compositional Experts", "abstract": "Perceiving multi-modal information and fulfilling dialogues with humans is a\nlong-term goal of artificial intelligence. Pre-training is commonly regarded as\nan effective approach for multi-modal dialogue. However, due to the limited\navailability of multi-modal dialogue data, there is still scarce research on\nmulti-modal dialogue pre-training. Yet another intriguing challenge emerges\nfrom the encompassing nature of multi-modal dialogue, which involves various\nmodalities and tasks. Moreover, new forms of tasks may arise at unpredictable\npoints in the future. Hence, it is essential for designed multi-modal dialogue\nmodels to possess sufficient flexibility to adapt to such scenarios. This paper\nproposes \\textbf{PaCE}, a unified, structured, compositional multi-modal\ndialogue pre-training framework. It utilizes a combination of several\nfundamental experts to accommodate multiple dialogue-related tasks and can be\npre-trained using limited dialogue and extensive non-dialogue multi-modal data.\nFurthermore, we propose a progressive training method where old experts from\nthe past can assist new experts, facilitating the expansion of their\ncapabilities. Experimental results demonstrate that PaCE achieves\nstate-of-the-art results on eight multi-modal dialog benchmarks.", "published": "2023-05-24 07:43:29", "link": "http://arxiv.org/abs/2305.14839v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Exploring Sentiment Analysis Techniques in Natural Language Processing:\n  A Comprehensive Review", "abstract": "Sentiment analysis (SA) is the automated process of detecting and\nunderstanding the emotions conveyed through written text. Over the past decade,\nSA has gained significant popularity in the field of Natural Language\nProcessing (NLP). With the widespread use of social media and online platforms,\nSA has become crucial for companies to gather customer feedback and shape their\nmarketing strategies. Additionally, researchers rely on SA to analyze public\nsentiment on various topics. In this particular research study, a comprehensive\nsurvey was conducted to explore the latest trends and techniques in SA. The\nsurvey encompassed a wide range of methods, including lexicon-based,\ngraph-based, network-based, machine learning, deep learning, ensemble-based,\nrule-based, and hybrid techniques. The paper also addresses the challenges and\nopportunities in SA, such as dealing with sarcasm and irony, analyzing\nmulti-lingual data, and addressing ethical concerns. To provide a practical\ncase study, Twitter was chosen as one of the largest online social media\nplatforms. Furthermore, the researchers shed light on the diverse application\nareas of SA, including social media, healthcare, marketing, finance, and\npolitics. The paper also presents a comparative and comprehensive analysis of\nexisting trends and techniques, datasets, and evaluation metrics. The ultimate\ngoal is to offer researchers and practitioners a systematic review of SA\ntechniques, identify existing gaps, and suggest possible improvements. This\nstudy aims to enhance the efficiency and accuracy of SA processes, leading to\nsmoother and error-free outcomes.", "published": "2023-05-24 07:48:41", "link": "http://arxiv.org/abs/2305.14842v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Words to Wires: Generating Functioning Electronic Devices from\n  Natural Language Descriptions", "abstract": "In this work, we show that contemporary language models have a previously\nunknown skill -- the capacity for electronic circuit design from high-level\ntextual descriptions, akin to code generation. We introduce two benchmarks:\nPins100, assessing model knowledge of electrical components, and Micro25,\nevaluating a model's capability to design common microcontroller circuits and\ncode in the Arduino ecosystem that involve input, output, sensors, motors,\nprotocols, and logic -- with models such as GPT-4 and Claude-V1 achieving\nbetween 60% to 96% Pass@1 on generating full devices. We include six case\nstudies of using language models as a design assistant for moderately complex\ndevices, such as a radiation-powered random number generator, an emoji\nkeyboard, a visible spectrometer, and several assistive devices, while offering\na qualitative analysis performance, outlining evaluation challenges, and\nsuggesting areas of development to improve complex circuit design and practical\nutility. With this work, we aim to spur research at the juncture of natural\nlanguage processing and electronic design.", "published": "2023-05-24 08:28:59", "link": "http://arxiv.org/abs/2305.14874v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leveraging GPT-4 for Automatic Translation Post-Editing", "abstract": "While Neural Machine Translation (NMT) represents the leading approach to\nMachine Translation (MT), the outputs of NMT models still require translation\npost-editing to rectify errors and enhance quality under critical settings. In\nthis work, we formalize the task of direct translation post-editing with Large\nLanguage Models (LLMs) and explore the use of GPT-4 to automatically post-edit\nNMT outputs across several language pairs. Our results demonstrate that GPT-4\nis adept at translation post-editing, producing meaningful and trustworthy\nedits to translations that help improve its general quality as well as remove\ndifferent classes of major errors in translations. In particular, human\nevaluations on assessing edit trustworthiness show that GPT-4 exhibits a large\nimprovement over the prior state-of-the-art LLM. Notably, we improve upon\nstate-of-the-art performance on WMT-22 English-Chinese, English-German,\nChinese-English and German-English language pairs using GPT-4 based\npost-editing, as evaluated by state-of-the-art MT quality metrics. However, we\nalso show that GPT-4 could produce hallucinated edits, thereby urging caution\nin its use as an expert translation post-editor.", "published": "2023-05-24 08:30:05", "link": "http://arxiv.org/abs/2305.14878v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ByteSized32: A Corpus and Challenge Task for Generating Task-Specific\n  World Models Expressed as Text Games", "abstract": "In this work, we investigate the capacity of language models to generate\nexplicit, interpretable, and interactive world models of scientific and\ncommon-sense reasoning tasks. We operationalize this as a task of generating\ntext games, expressed as hundreds of lines of Python code. To facilitate this\ntask, we introduce ByteSized32 (Code: github.com/cognitiveailab/BYTESIZED32), a\ncorpus of 32 reasoning-focused text games totaling 20k lines of Python code. We\nempirically demonstrate that GPT-4 can use these games as templates for\nsingle-shot in-context learning, successfully producing runnable games on\nunseen topics in 28% of cases. When allowed to self-reflect on program errors,\ngame runnability substantially increases to 57%. While evaluating simulation\nfidelity is labor-intensive, we introduce a suite of automated metrics to\nassess game fidelity, technical validity, adherence to task specifications, and\nwinnability, showing a high degree of agreement with expert human ratings. We\npose this as a challenge task to spur further development at the juncture of\nworld modeling and code generation.", "published": "2023-05-24 08:31:30", "link": "http://arxiv.org/abs/2305.14879v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation\n  Metrics using Measurement Theory", "abstract": "We address a fundamental challenge in Natural Language Generation (NLG) model\nevaluation -- the design and evaluation of evaluation metrics. Recognizing the\nlimitations of existing automatic metrics and noises from how current human\nevaluation was conducted, we propose MetricEval, a framework informed by\nmeasurement theory, the foundation of educational test design, for\nconceptualizing and evaluating the reliability and validity of NLG evaluation\nmetrics. The framework formalizes the source of measurement error and offers\nstatistical tools for evaluating evaluation metrics based on empirical data.\nWith our framework, one can quantify the uncertainty of the metrics to better\ninterpret the result. To exemplify the use of our framework in practice, we\nanalyzed a set of evaluation metrics for summarization and identified issues\nrelated to conflated validity structure in human-eval and reliability in\nLLM-based metrics. Through MetricEval, we aim to promote the design,\nevaluation, and interpretation of valid and reliable metrics to advance robust\nand effective NLG models.", "published": "2023-05-24 08:38:23", "link": "http://arxiv.org/abs/2305.14889v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Extracting Psychological Indicators Using Question Answering", "abstract": "In this work, we propose a method for extracting text spans that may indicate\none of the BIG5 psychological traits using a question-answering task with\nexamples that have no answer for the asked question. We utilized the RoBERTa\nmodel fine-tuned on SQuAD 2.0 dataset. The model was further fine-tuned\nutilizing comments from Reddit. We examined the effect of the percentage of\nexamples with no answer in the training dataset on the overall performance. The\nresults obtained in this study are in line with the SQuAD 2.0 benchmark and\npresent a good baseline for further research.", "published": "2023-05-24 08:41:23", "link": "http://arxiv.org/abs/2305.14891v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Chain-of-Questions Training with Latent Answers for Robust Multistep\n  Question Answering", "abstract": "We train a language model (LM) to robustly answer multistep questions by\ngenerating and answering sub-questions. We propose Chain-of-Questions, a\nframework that trains a model to generate sub-questions and sub-answers one at\na time by leveraging human annotated question decomposition meaning\nrepresentation (QDMR). The key technical challenge is that QDMR only contains\nsub-questions but not answers to those sub-questions, so we treat sub-answers\nas latent variables and optimize them using a novel dynamic mixture of Hard-EM\nand MAPO. Chain-of-Questions greatly outperforms strong neuro-symbolic methods\nby 9.0 F1 on DROP contrast set, and outperforms GPT-3.5 by 24.3 F1 on HOTPOTQA\nadversarial set, thus demonstrating the effectiveness and robustness of our\nframework.", "published": "2023-05-24 08:55:08", "link": "http://arxiv.org/abs/2305.14901v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Structural Ambiguity and its Disambiguation in Language Model Based\n  Parsers: the Case of Dutch Clause Relativization", "abstract": "This paper addresses structural ambiguity in Dutch relative clauses. By\ninvestigating the task of disambiguation by grounding, we study how the\npresence of a prior sentence can resolve relative clause ambiguities. We apply\nthis method to two parsing architectures in an attempt to demystify the parsing\nand language model components of two present-day neural parsers. Results show\nthat a neurosymbolic parser, based on proof nets, is more open to data bias\ncorrection than an approach based on universal dependencies, although both\nsetups suffer from a comparable initial data bias.", "published": "2023-05-24 09:04:18", "link": "http://arxiv.org/abs/2305.14917v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Reliable Misinformation Mitigation: Generalization, Uncertainty,\n  and GPT-4", "abstract": "Misinformation poses a critical societal challenge, and current approaches\nhave yet to produce an effective solution. We propose focusing on\ngeneralization, uncertainty, and how to leverage recent large language models,\nin order to create more practical tools to evaluate information veracity in\ncontexts where perfect classification is impossible. We first demonstrate that\nGPT-4 can outperform prior methods in multiple settings and languages. Next, we\nexplore generalization, revealing that GPT-4 and RoBERTa-large exhibit\ndifferences in failure modes. Third, we propose techniques to handle\nuncertainty that can detect impossible examples and strongly improve outcomes.\nWe also discuss results on other language models, temperature, prompting,\nversioning, explainability, and web retrieval, each one providing practical\ninsights and directions for future research. Finally, we publish the LIAR-New\ndataset with novel paired English and French misinformation data and\nPossibility labels that indicate if there is sufficient context for veracity\nevaluation. Overall, this research lays the groundwork for future tools that\ncan drive real-world progress to combat misinformation.", "published": "2023-05-24 09:10:20", "link": "http://arxiv.org/abs/2305.14928v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GRACE: Discriminator-Guided Chain-of-Thought Reasoning", "abstract": "In the context of multi-step reasoning, e.g., with chain-of-thought, language\nmodels (LMs) can easily assign a high likelihood to incorrect steps. As a\nresult, decoding strategies that optimize for solution likelihood often yield\nincorrect solutions. To address this issue, we propose Guiding chain-of-thought\nReAsoning with a CorrectnEss Discriminator (GRACE), a stepwise decoding\napproach that steers the decoding process towards producing correct reasoning\nsteps. GRACE employs a discriminator trained with a contrastive loss over\ncorrect and incorrect steps, which is used during decoding to score next-step\ncandidates based on their correctness. Importantly, GRACE only requires\nsampling from the LM, without the need for LM training or fine-tuning. Using\nmodels from FLAN-T5 and LLaMA families, we evaluate GRACE over four math and\ntwo symbolic reasoning tasks, where it exhibits substantial performance gains\ncompared to greedy decoding, verifiers, and self-consistency in most settings.\nWhen further combined with self-consistency, GRACE outperforms all the\nbaselines by sizeable margins. Human and LLM evaluations over GSM8K show that\nGRACE not only improves the final answer accuracy but also the correctness of\nthe intermediate reasoning. Our implementation can be accessed at\n\\url{https://github.com/mukhal/grace}.", "published": "2023-05-24 09:16:51", "link": "http://arxiv.org/abs/2305.14934v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large\n  Language Models with SocKET Benchmark", "abstract": "Large language models (LLMs) have been shown to perform well at a variety of\nsyntactic, discourse, and reasoning tasks. While LLMs are increasingly deployed\nin many forms including conversational agents that interact with humans, we\nlack a grounded benchmark to measure how well LLMs understand \\textit{social}\nlanguage. Here, we introduce a new theory-driven benchmark, SocKET, that\ncontains 58 NLP tasks testing social knowledge which we group into five\ncategories: humor & sarcasm, offensiveness, sentiment & emotion, and\ntrustworthiness. In tests on the benchmark, we demonstrate that current models\nattain only moderate performance but reveal significant potential for task\ntransfer among different types and categories of tasks, which were predicted\nfrom theory. Through zero-shot evaluations, we show that pretrained models\nalready possess some innate but limited capabilities of social language\nunderstanding and training on one category of tasks can improve zero-shot\ntesting on others. Our benchmark provides a systematic way to analyze model\nperformance on an important dimension of language and points to clear room for\nimprovement to build more socially-aware LLMs. The associated resources are\nreleased at https://github.com/minjechoi/SOCKET.", "published": "2023-05-24 09:21:06", "link": "http://arxiv.org/abs/2305.14938v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Predictable Are Large Language Model Capabilities? A Case Study on\n  BIG-bench", "abstract": "We investigate the predictability of large language model (LLM) capabilities:\ngiven records of past experiments using different model families, numbers of\nparameters, tasks, and numbers of in-context examples, can we accurately\npredict LLM performance on new experiment configurations? Answering this\nquestion has practical implications for LLM users (e.g., deciding which models\nto try), developers (e.g., prioritizing evaluation on representative tasks),\nand the research community (e.g., identifying hard-to-predict capabilities that\nwarrant further investigation).\n  We study the performance prediction problem on experiment records from\nBIG-bench. On a random train-test split, an MLP-based predictor achieves an\n$R^2$ score greater than 95%, indicating the presence of learnable patterns\nwithin the experiment records. We then formulate the problem of searching for\n\"small-bench,\" an informative subset of BIG-bench tasks from which the\nperformance on the full set can be maximally recovered. We find a subset as\ninformative as BIG-bench Hard for evaluating new model families, while being\n$3\\times$ smaller. Additionally, we find competitive subsets by clustering task\nrepresentations learned by our MLP-based predictor and selecting tasks close to\ncluster centroids, highlighting the importance of task diversity in\nconstructing \"small-bench.\"", "published": "2023-05-24 09:35:34", "link": "http://arxiv.org/abs/2305.14947v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Getting Sick After Seeing a Doctor? Diagnosing and Mitigating Knowledge\n  Conflicts in Event Temporal Reasoning", "abstract": "Event temporal reasoning aims at identifying the temporal relations between\ntwo or more events from narratives. However, knowledge conflicts arise when\nthere is a mismatch between the actual temporal relations of events in the\ncontext and the prior knowledge or biases learned by the model. In this paper,\nwe propose to detect knowledge-conflict examples in event temporal reasoning\nusing bias indicators, which include event relation prior bias, tense bias,\nnarrative bias, and dependency bias. We define conflict examples as those where\nevent relations are opposite to biased or prior relations. To mitigate\nevent-related knowledge conflicts, we introduce a Counterfactual Data\nAugmentation (CDA) based method that can be applied to both Pre-trained\nLanguage Models (PLMs) and Large Language Models (LLMs) either as additional\ntraining data or demonstrations for In-Context Learning. Experiments suggest\nboth PLMs and LLMs suffer from knowledge conflicts in event temporal reasoning,\nand CDA has the potential for reducing hallucination and improving model\nperformance.", "published": "2023-05-24 10:04:06", "link": "http://arxiv.org/abs/2305.14970v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP", "abstract": "ChatGPT's emergence heralds a transformative phase in NLP, particularly\ndemonstrated through its excellent performance on many English benchmarks.\nHowever, the model's efficacy across diverse linguistic contexts remains\nlargely uncharted territory. This work aims to bridge this knowledge gap, with\na primary focus on assessing ChatGPT's capabilities on Arabic languages and\ndialectal varieties. Our comprehensive study conducts a large-scale automated\nand human evaluation of ChatGPT, encompassing 44 distinct language\nunderstanding and generation tasks on over 60 different datasets. To our\nknowledge, this marks the first extensive performance analysis of ChatGPT's\ndeployment in Arabic NLP. Our findings indicate that, despite its remarkable\nperformance in English, ChatGPT is consistently surpassed by smaller models\nthat have undergone finetuning on Arabic. We further undertake a meticulous\ncomparison of ChatGPT and GPT-4's Modern Standard Arabic (MSA) and Dialectal\nArabic (DA), unveiling the relative shortcomings of both models in handling\nArabic dialects compared to MSA. Although we further explore and confirm the\nutility of employing GPT-4 as a potential alternative for human evaluation, our\nwork adds to a growing body of research underscoring the limitations of\nChatGPT.", "published": "2023-05-24 10:12:39", "link": "http://arxiv.org/abs/2305.14976v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LAraBench: Benchmarking Arabic AI with Large Language Models", "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\ninfluenced the landscape of language and speech research. Despite this\nprogress, these models lack specific benchmarking against state-of-the-art\n(SOTA) models tailored to particular languages and tasks. LAraBench addresses\nthis gap for Arabic Natural Language Processing (NLP) and Speech Processing\ntasks, including sequence tagging and content classification across different\ndomains. We utilized models such as GPT-3.5-turbo, GPT-4, BLOOMZ,\nJais-13b-chat, Whisper, and USM, employing zero and few-shot learning\ntechniques to tackle 33 distinct tasks across 61 publicly available datasets.\nThis involved 98 experimental setups, encompassing ~296K data points, ~46 hours\nof speech, and 30 sentences for Text-to-Speech (TTS). This effort resulted in\n330+ sets of experiments. Our analysis focused on measuring the performance gap\nbetween SOTA models and LLMs. The overarching trend observed was that SOTA\nmodels generally outperformed LLMs in zero-shot learning, with a few\nexceptions. Notably, larger computational models with few-shot learning\ntechniques managed to reduce these performance gaps. Our findings provide\nvaluable insights into the applicability of LLMs for Arabic NLP and speech\nprocessing tasks.", "published": "2023-05-24 10:16:16", "link": "http://arxiv.org/abs/2305.14982v2", "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "IdealGPT: Iteratively Decomposing Vision and Language Reasoning via\n  Large Language Models", "abstract": "The field of vision-and-language (VL) understanding has made unprecedented\nprogress with end-to-end large pre-trained VL models (VLMs). However, they\nstill fall short in zero-shot reasoning tasks that require multi-step\ninferencing. To achieve this goal, previous works resort to a\ndivide-and-conquer pipeline. In this paper, we argue that previous efforts have\nseveral inherent shortcomings: 1) They rely on domain-specific sub-question\ndecomposing models. 2) They force models to predict the final answer even if\nthe sub-questions or sub-answers provide insufficient information. We address\nthese limitations via IdealGPT, a framework that iteratively decomposes VL\nreasoning using large language models (LLMs). Specifically, IdealGPT utilizes\nan LLM to generate sub-questions, a VLM to provide corresponding sub-answers,\nand another LLM to reason to achieve the final answer. These three modules\nperform the divide-and-conquer procedure iteratively until the model is\nconfident about the final answer to the main question. We evaluate IdealGPT on\nmultiple challenging VL reasoning tasks under a zero-shot setting. In\nparticular, our IdealGPT outperforms the best existing GPT-4-like models by an\nabsolute 10% on VCR and 15% on SNLI-VE. Code is available at\nhttps://github.com/Hxyou/IdealGPT", "published": "2023-05-24 10:19:57", "link": "http://arxiv.org/abs/2305.14985v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MuLER: Detailed and Scalable Reference-based Evaluation", "abstract": "We propose a novel methodology (namely, MuLER) that transforms any\nreference-based evaluation metric for text generation, such as machine\ntranslation (MT) into a fine-grained analysis tool. Given a system and a\nmetric, MuLER quantifies how much the chosen metric penalizes specific error\ntypes (e.g., errors in translating names of locations). MuLER thus enables a\ndetailed error analysis which can lead to targeted improvement efforts for\nspecific phenomena. We perform experiments in both synthetic and naturalistic\nsettings to support MuLER's validity and showcase its usability in MT\nevaluation, and other tasks, such as summarization. Analyzing all submissions\nto WMT in 2014-2020, we find consistent trends. For example, nouns and verbs\nare among the most frequent POS tags. However, they are among the hardest to\ntranslate. Performance on most POS tags improves with overall system\nperformance, but a few are not thus correlated (their identity changes from\nlanguage to language). Preliminary experiments with summarization reveal\nsimilar trends.", "published": "2023-05-24 10:26:13", "link": "http://arxiv.org/abs/2305.14991v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The ACL OCL Corpus: Advancing Open Science in Computational Linguistics", "abstract": "We present ACL OCL, a scholarly corpus derived from the ACL Anthology to\nassist Open scientific research in the Computational Linguistics domain.\nIntegrating and enhancing the previous versions of the ACL Anthology, the ACL\nOCL contributes metadata, PDF files, citation graphs and additional structured\nfull texts with sections, figures, and links to a large knowledge resource\n(Semantic Scholar). The ACL OCL spans seven decades, containing 73K papers,\nalongside 210K figures.\n  We spotlight how ACL OCL applies to observe trends in computational\nlinguistics. By detecting paper topics with a supervised neural model, we note\nthat interest in \"Syntax: Tagging, Chunking and Parsing\" is waning and \"Natural\nLanguage Generation\" is resurging. Our dataset is available from HuggingFace\n(https://huggingface.co/datasets/WINGNUS/ACL-OCL).", "published": "2023-05-24 10:35:56", "link": "http://arxiv.org/abs/2305.14996v2", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "A RelEntLess Benchmark for Modelling Graded Relations between Named\n  Entities", "abstract": "Relations such as \"is influenced by\", \"is known for\" or \"is a competitor of\"\nare inherently graded: we can rank entity pairs based on how well they satisfy\nthese relations, but it is hard to draw a line between those pairs that satisfy\nthem and those that do not. Such graded relations play a central role in many\napplications, yet they are typically not covered by existing Knowledge Graphs.\nIn this paper, we consider the possibility of using Large Language Models\n(LLMs) to fill this gap. To this end, we introduce a new benchmark, in which\nentity pairs have to be ranked according to how much they satisfy a given\ngraded relation. The task is formulated as a few-shot ranking problem, where\nmodels only have access to a description of the relation and five prototypical\ninstances. We use the proposed benchmark to evaluate state-of-the-art relation\nembedding strategies as well as several recent LLMs, covering both publicly\navailable LLMs and closed models such as GPT-4. Overall, we find a strong\ncorrelation between model size and performance, with smaller Language Models\nstruggling to outperform a naive baseline. The results of the largest Flan-T5\nand OPT models are remarkably strong, although a clear gap with human\nperformance remains.", "published": "2023-05-24 10:41:24", "link": "http://arxiv.org/abs/2305.15002v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ChatAgri: Exploring Potentials of ChatGPT on Cross-linguistic\n  Agricultural Text Classification", "abstract": "In the era of sustainable smart agriculture, a massive amount of agricultural\nnews text is being posted on the Internet, in which massive agricultural\nknowledge has been accumulated. In this context, it is urgent to explore\neffective text classification techniques for users to access the required\nagricultural knowledge with high efficiency. Mainstream deep learning\napproaches employing fine-tuning strategies on pre-trained language models\n(PLMs), have demonstrated remarkable performance gains over the past few years.\nNonetheless, these methods still face many drawbacks that are complex to solve,\nincluding: 1. Limited agricultural training data due to the expensive-cost and\nlabour-intensive annotation; 2. Poor domain transferability, especially of\ncross-linguistic ability; 3. Complex and expensive large models\ndeployment.Inspired by the extraordinary success brought by the recent ChatGPT\n(e.g. GPT-3.5, GPT-4), in this work, we systematically investigate and explore\nthe capability and utilization of ChatGPT applying to the agricultural\ninformatization field. ....(shown in article).... Code has been released on\nGithub\nhttps://github.com/albert-jin/agricultural_textual_classification_ChatGPT.", "published": "2023-05-24 11:06:23", "link": "http://arxiv.org/abs/2305.15024v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ghostbuster: Detecting Text Ghostwritten by Large Language Models", "abstract": "We introduce Ghostbuster, a state-of-the-art system for detecting\nAI-generated text. Our method works by passing documents through a series of\nweaker language models, running a structured search over possible combinations\nof their features, and then training a classifier on the selected features to\npredict whether documents are AI-generated. Crucially, Ghostbuster does not\nrequire access to token probabilities from the target model, making it useful\nfor detecting text generated by black-box models or unknown model versions. In\nconjunction with our model, we release three new datasets of human- and\nAI-generated text as detection benchmarks in the domains of student essays,\ncreative writing, and news articles. We compare Ghostbuster to a variety of\nexisting detectors, including DetectGPT and GPTZero, as well as a new RoBERTa\nbaseline. Ghostbuster achieves 99.0 F1 when evaluated across domains, which is\n5.9 F1 higher than the best preexisting model. It also outperforms all previous\napproaches in generalization across writing domains (+7.5 F1), prompting\nstrategies (+2.1 F1), and language models (+4.4 F1). We also analyze the\nrobustness of our system to a variety of perturbations and paraphrasing attacks\nand evaluate its performance on documents written by non-native English\nspeakers.", "published": "2023-05-24 11:37:10", "link": "http://arxiv.org/abs/2305.15047v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ranger: A Toolkit for Effect-Size Based Multi-Task Evaluation", "abstract": "In this paper, we introduce Ranger - a toolkit to facilitate the easy use of\neffect-size-based meta-analysis for multi-task evaluation in NLP and IR. We\nobserved that our communities often face the challenge of aggregating results\nover incomparable metrics and scenarios, which makes conclusions and take-away\nmessages less reliable. With Ranger, we aim to address this issue by providing\na task-agnostic toolkit that combines the effect of a treatment on multiple\ntasks into one statistical evaluation, allowing for comparison of metrics and\ncomputation of an overall summary effect. Our toolkit produces\npublication-ready forest plots that enable clear communication of evaluation\nresults over multiple tasks. Our goal with the ready-to-use Ranger toolkit is\nto promote robust, effect-size-based evaluation and improve evaluation\nstandards in the community. We provide two case studies for common IR and NLP\nsettings to highlight Ranger's benefits.", "published": "2023-05-24 11:38:39", "link": "http://arxiv.org/abs/2305.15048v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Decomposing Complex Queries for Tip-of-the-tongue Retrieval", "abstract": "When re-finding items, users who forget or are uncertain about identifying\ndetails often rely on creative strategies for expressing their information\nneeds -- complex queries that describe content elements (e.g., book characters\nor events), information beyond the document text (e.g., descriptions of book\ncovers), or personal context (e.g., when they read a book). This retrieval\nsetting, called tip of the tongue (TOT), is especially challenging for models\nheavily reliant on lexical and semantic overlap between query and document\ntext. In this work, we introduce a simple yet effective framework for handling\nsuch complex queries by decomposing the query into individual clues, routing\nthose as sub-queries to specialized retrievers, and ensembling the results.\nThis approach allows us to take advantage of off-the-shelf retrievers (e.g.,\nCLIP for retrieving images of book covers) or incorporate retriever-specific\nlogic (e.g., date constraints). We show that our framework incorportating query\ndecompositions into retrievers can improve gold book recall up to 7% relative\nagain for Recall@5 on a new collection of 14,441 real-world query-book pairs\nfrom an online community for resolving TOT inquiries.", "published": "2023-05-24 11:43:40", "link": "http://arxiv.org/abs/2305.15053v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models\n  using Causal Mediation Analysis", "abstract": "Mathematical reasoning in large language models (LMs) has garnered\nsignificant attention in recent work, but there is a limited understanding of\nhow these models process and store information related to arithmetic tasks\nwithin their architecture. In order to improve our understanding of this aspect\nof language models, we present a mechanistic interpretation of\nTransformer-based LMs on arithmetic questions using a causal mediation analysis\nframework. By intervening on the activations of specific model components and\nmeasuring the resulting changes in predicted probabilities, we identify the\nsubset of parameters responsible for specific predictions. This provides\ninsights into how information related to arithmetic is processed by LMs. Our\nexperimental results indicate that LMs process the input by transmitting the\ninformation relevant to the query from mid-sequence early layers to the final\ntoken using the attention mechanism. Then, this information is processed by a\nset of MLP modules, which generate result-related information that is\nincorporated into the residual stream. To assess the specificity of the\nobserved activation dynamics, we compare the effects of different model\ncomponents on arithmetic queries with other tasks, including number retrieval\nfrom prompts and factual knowledge questions.", "published": "2023-05-24 11:43:47", "link": "http://arxiv.org/abs/2305.15054v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lawyer LLaMA Technical Report", "abstract": "Large Language Models (LLMs), like LLaMA, have exhibited remarkable\nperformance across various tasks. Nevertheless, when deployed to specific\ndomains such as law or medicine, the models still confront the challenge of a\ndeficiency in domain-specific knowledge and an inadequate capability to\nleverage that knowledge to resolve domain-related problems. In this paper, we\npropose a new framework to adapt LLMs to specific domains and build Lawyer\nLLaMA, a legal domain LLM, based on this framework. Specifically, we inject\ndomain knowledge during the continual training stage and teach the model to\nlearn professional skills using properly designed supervised fine-tuning tasks.\nMoreover, to alleviate the hallucination problem during the model's generation,\nwe add a retrieval module and extract relevant legal articles before the model\nanswers any queries. When learning domain-specific skills, we find that\nexperts' experience is much more useful than experiences distilled from\nChatGPT, where hundreds of expert-written data outperform tens of thousands of\nChatGPT-generated ones. We will release our model and data.", "published": "2023-05-24 11:52:07", "link": "http://arxiv.org/abs/2305.15062v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ?\n  An Empirical Evaluation and Benchmarking", "abstract": "Large language models~(LLM) like ChatGPT have become indispensable to\nartificial general intelligence~(AGI), demonstrating excellent performance in\nvarious natural language processing tasks. In the real world, graph data is\nubiquitous and an essential part of AGI and prevails in domains like social\nnetwork analysis, bioinformatics and recommender systems. The training corpus\nof large language models often includes some algorithmic components, which\nallows them to achieve certain effects on some graph data-related problems.\nHowever, there is still little research on their performance on a broader range\nof graph-structured data. In this study, we conduct an extensive investigation\nto assess the proficiency of LLMs in comprehending graph data, employing a\ndiverse range of structural and semantic-related tasks. Our analysis\nencompasses 10 distinct tasks that evaluate the LLMs' capabilities in graph\nunderstanding. Through our study, we not only uncover the current limitations\nof language models in comprehending graph structures and performing associated\nreasoning tasks but also emphasize the necessity for further advancements and\nnovel approaches to enhance their graph processing capabilities. Our findings\ncontribute valuable insights towards bridging the gap between language models\nand graph understanding, paving the way for more effective graph mining and\nknowledge extraction.", "published": "2023-05-24 11:53:19", "link": "http://arxiv.org/abs/2305.15066v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks\n  for Exploring Theory of Mind", "abstract": "Theory of Mind (ToM), the capacity to comprehend the mental states of\ndistinct individuals, is essential for numerous practical applications. With\nthe development of large language models (LLMs), there is a heated debate about\nwhether they are able to perform ToM tasks. Previous studies have used\ndifferent tasks and prompts to test the ToM on LLMs and the results are\ninconsistent: some studies asserted these models are capable of exhibiting ToM,\nwhile others suggest the opposite. In this study, We present ToMChallenges, a\ndataset for comprehensively evaluating the Theory of Mind based on the\nSally-Anne and Smarties tests with a diverse set of tasks. In addition, we also\npropose an auto-grader to streamline the answer evaluation process. We tested\nthree models: davinci, turbo, and gpt-4. Our evaluation results and error\nanalyses show that LLMs have inconsistent behaviors across prompts and tasks.\nPerforming the ToM tasks robustly remains a challenge for the LLMs. In\naddition, our paper wants to raise awareness in evaluating the ToM in LLMs and\nwe want to invite more discussion on how to design the prompts and tasks for\nToM tasks that can better assess the LLMs' ability.", "published": "2023-05-24 11:54:07", "link": "http://arxiv.org/abs/2305.15068v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For\n  Large Language Models", "abstract": "The performance of large language models (LLMs) on existing reasoning\nbenchmarks has significantly improved over the past years. In response, we\npresent JEEBench, a considerably more challenging benchmark dataset for\nevaluating the problem solving abilities of LLMs. We curate 515 challenging\npre-engineering mathematics, physics and chemistry problems from the highly\ncompetitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep\nin-domain knowledge is essential for solving problems in this benchmark. Our\nevaluation on various open-source and proprietary models reveals that the\nhighest performance, even after using techniques like self-consistency,\nself-refinement and chain-of-thought prompting, is less than 40%. The typical\nfailure modes of GPT-4, the best model, are errors in algebraic manipulation,\ndifficulty in grounding abstract concepts into mathematical equations\naccurately and failure in retrieving relevant domain-specific concepts. We also\nobserve that by mere prompting, GPT-4 is unable to assess risk introduced by\nnegative marking for incorrect answers. For this, we develop a post-hoc\nconfidence-thresholding method over self-consistency, which enables effective\nresponse selection. We hope that our challenging benchmark will guide future\nre-search in problem-solving using LLMs.", "published": "2023-05-24 11:55:59", "link": "http://arxiv.org/abs/2305.15074v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HuatuoGPT, towards Taming Language Model to Be a Doctor", "abstract": "In this paper, we present HuatuoGPT, a large language model (LLM) for medical\nconsultation. The core recipe of HuatuoGPT is to leverage both\n\\textit{distilled data from ChatGPT} and \\textit{real-world data from doctors}\nin the supervised fine-tuned stage. The responses of ChatGPT are usually\ndetailed, well-presented and informative while it cannot perform like a doctor\nin many aspects, e.g. for integrative diagnosis. We argue that real-world data\nfrom doctors would be complementary to distilled data in the sense the former\ncould tame a distilled language model to perform like doctors. To better\nleverage the strengths of both data, we train a reward model to align the\nlanguage model with the merits that both data bring, following an RLAIF\n(reinforced learning from AI feedback) fashion. To evaluate and benchmark the\nmodels, we propose a comprehensive evaluation scheme (including automatic and\nmanual metrics). Experimental results demonstrate that HuatuoGPT achieves\nstate-of-the-art results in performing medical consultation among open-source\nLLMs in GPT-4 evaluation, human evaluation, and medical benchmark datasets. It\nis worth noting that by using additional real-world data and RLAIF, the\ndistilled language model (i.e., HuatuoGPT) outperforms its teacher model\nChatGPT in most cases. Our code, data, and models are publicly available at\n\\url{https://github.com/FreedomIntelligence/HuatuoGPT}. The online demo is\navailable at \\url{https://www.HuatuoGPT.cn/}.", "published": "2023-05-24 11:56:01", "link": "http://arxiv.org/abs/2305.15075v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Visually-Situated Natural Language Understanding with Contrastive\n  Reading Model and Frozen Large Language Models", "abstract": "Recent advances in Large Language Models (LLMs) have stimulated a surge of\nresearch aimed at extending their applications to the visual domain. While\nthese models exhibit promise in generating abstract image captions and\nfacilitating natural conversations, their performance on text-rich images still\nrequires improvement. In this paper, we introduce Contrastive Reading Model\n(Cream), a novel neural architecture designed to enhance the language-image\nunderstanding capability of LLMs by capturing intricate details that are often\noverlooked in existing methods. Cream combines vision and auxiliary encoders,\nfortified by a contrastive feature alignment technique, to achieve a more\neffective comprehension of language information in visually situated contexts\nwithin the images. Our approach bridges the gap between vision and language\nunderstanding, paving the way for the development of more sophisticated\nDocument Intelligence Assistants. Through rigorous evaluations across diverse\nvisually-situated language understanding tasks that demand reasoning\ncapabilities, we demonstrate the compelling performance of Cream, positioning\nit as a prominent model in the field of visual document understanding. We\nprovide our codebase and newly-generated datasets at\nhttps://github.com/naver-ai/cream .", "published": "2023-05-24 11:59:13", "link": "http://arxiv.org/abs/2305.15080v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pento-DIARef: A Diagnostic Dataset for Learning the Incremental\n  Algorithm for Referring Expression Generation from Examples", "abstract": "NLP tasks are typically defined extensionally through datasets containing\nexample instantiations (e.g., pairs of image i and text t), but motivated\nintensionally through capabilities invoked in verbal descriptions of the task\n(e.g., \"t is a description of i, for which the content of i needs to be\nrecognised and understood\"). We present Pento-DIARef, a diagnostic dataset in a\nvisual domain of puzzle pieces where referring expressions are generated by a\nwell-known symbolic algorithm (the \"Incremental Algorithm\"), which itself is\nmotivated by appeal to a hypothesised capability (eliminating distractors\nthrough application of Gricean maxims). Our question then is whether the\nextensional description (the dataset) is sufficient for a neural model to pick\nup the underlying regularity and exhibit this capability given the simple task\ndefinition of producing expressions from visual inputs. We find that a model\nsupported by a vision detection step and a targeted data generation scheme\nachieves an almost perfect BLEU@1 score and sentence accuracy, whereas simpler\nbaselines do not.", "published": "2023-05-24 12:05:53", "link": "http://arxiv.org/abs/2305.15087v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "STAR: Boosting Low-Resource Information Extraction by Structure-to-Text\n  Data Generation with Large Language Models", "abstract": "Information extraction tasks such as event extraction require an in-depth\nunderstanding of the output structure and sub-task dependencies. They heavily\nrely on task-specific training data in the form of (passage, target structure)\npairs to obtain reasonable performance. However, obtaining such data through\nhuman annotation is costly, leading to a pressing need for low-resource\ninformation extraction approaches that require minimal human labeling for\nreal-world applications. Fine-tuning supervised models with synthesized\ntraining data would be a generalizable method, but the existing data generation\nmethods either still rely on large-scale ground-truth data or cannot be applied\nto complicated IE tasks due to their poor performance. To address these\nchallenges, we propose STAR, a data generation method that leverages Large\nLanguage Models (LLMs) to synthesize data instances given limited seed\ndemonstrations, thereby boosting low-resource information extraction\nperformance. Our approach involves generating target structures (Y) followed by\ngenerating passages (X), all accomplished with the aid of LLMs. We design\nfine-grained step-by-step instructions to obtain the initial data instances. We\nfurther reduce errors and improve data quality through self-reflection error\nidentification and self-refinement with iterative revision. Our experiments\nshow that the data generated by STAR significantly improve the performance of\nlow-resource event extraction and relation extraction tasks, even surpassing\nthe effectiveness of human-curated data. Human assessment of the data quality\nshows STAR-generated data exhibits higher passage quality and better align with\nthe task definitions compared with the human-curated data.", "published": "2023-05-24 12:15:19", "link": "http://arxiv.org/abs/2305.15090v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dynamic Masking Rate Schedules for MLM Pretraining", "abstract": "Most works on transformers trained with the Masked Language Modeling (MLM)\nobjective use the original BERT model's fixed masking rate of 15%. We propose\nto instead dynamically schedule the masking rate throughout training. We find\nthat linearly decreasing the masking rate over the course of pretraining\nimproves average GLUE accuracy by up to 0.46% and 0.25% in BERT-base and\nBERT-large, respectively, compared to fixed rate baselines. These gains come\nfrom exposure to both high and low masking rate regimes, providing benefits\nfrom both settings. Our results demonstrate that masking rate scheduling is a\nsimple way to improve the quality of masked language models, achieving up to a\n1.89x speedup in pretraining for BERT-base as well as a Pareto improvement for\nBERT-large.", "published": "2023-05-24 12:24:12", "link": "http://arxiv.org/abs/2305.15096v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On Degrees of Freedom in Defining and Testing Natural Language\n  Understanding", "abstract": "Natural language understanding (NLU) studies often exaggerate or\nunderestimate the capabilities of systems, thereby limiting the reproducibility\nof their findings. These erroneous evaluations can be attributed to the\ndifficulty of defining and testing NLU adequately. In this position paper, we\nreconsider this challenge by identifying two types of researcher degrees of\nfreedom. We revisit Turing's original interpretation of the Turing test and\nindicate that an NLU test does not provide an operational definition; it merely\nprovides inductive evidence that the test subject understands the language\nsufficiently well to meet stakeholder objectives. In other words, stakeholders\nare free to arbitrarily define NLU through their objectives. To use the test\nresults as inductive evidence, stakeholders must carefully assess if the\ninterpretation of test scores is valid or not. However, designing and using NLU\ntests involve other degrees of freedom, such as specifying target skills and\ndefining evaluation metrics. As a result, achieving consensus among\nstakeholders becomes difficult. To resolve this issue, we propose a validity\nargument, which is a framework comprising a series of validation criteria\nacross test components. By demonstrating that current practices in NLU studies\ncan be associated with those criteria and organizing them into a comprehensive\nchecklist, we prove that the validity argument can serve as a coherent\nguideline for designing credible test sets and facilitating scientific\ncommunication.", "published": "2023-05-24 13:25:20", "link": "http://arxiv.org/abs/2305.15130v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SciReviewGen: A Large-scale Dataset for Automatic Literature Review\n  Generation", "abstract": "Automatic literature review generation is one of the most challenging tasks\nin natural language processing. Although large language models have tackled\nliterature review generation, the absence of large-scale datasets has been a\nstumbling block to the progress. We release SciReviewGen, consisting of over\n10,000 literature reviews and 690,000 papers cited in the reviews. Based on the\ndataset, we evaluate recent transformer-based summarization models on the\nliterature review generation task, including Fusion-in-Decoder extended for\nliterature review generation. Human evaluation results show that some\nmachine-generated summaries are comparable to human-written reviews, while\nrevealing the challenges of automatic literature review generation such as\nhallucinations and a lack of detailed information. Our dataset and code are\navailable at https://github.com/tetsu9923/SciReviewGen.", "published": "2023-05-24 14:26:30", "link": "http://arxiv.org/abs/2305.15186v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cross-lingual QA: A Key to Unlocking In-context Cross-lingual\n  Performance", "abstract": "Multilingual large language models (MLLMs) have demonstrated significant\ncross-lingual capabilities through in-context learning. Existing approaches\ntypically construct monolingual in-context examples, either in the source or\ntarget language. However, translating entire in-context examples into the\ntarget language might compromise contextual integrity and be costly in the case\nof long-context passages. To address this, we introduce Cross-lingual QA, a\ncross-lingual prompting method that translates only the question and answer\nparts, thus reducing translation costs. Experiments on four typologically\ndiverse multilingual benchmarks show that Cross-lingual QA prompting\neffectively stimulates models to elicit their cross-lingual knowledge,\noutperforming prior monolingual prompting approaches. Furthermore, we show that\nprompting open-source MLLMs with cross-lingual in-context examples enhances\nperformance as the model scale increases.", "published": "2023-05-24 15:14:49", "link": "http://arxiv.org/abs/2305.15233v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of\n  Language Model", "abstract": "With the rapid growth in model size, fine-tuning the large pre-trained\nlanguage model has become increasingly difficult due to its extensive memory\nusage. Previous works usually focus on reducing the number of trainable\nparameters in the network. While the model parameters do contribute to memory\nusage, the primary memory bottleneck during training arises from storing\nfeature maps, also known as activations, as they are crucial for gradient\ncalculation. Notably, neural networks are usually trained using stochastic\ngradient descent. We argue that in stochastic optimization, models can handle\nnoisy gradients as long as the gradient estimator is unbiased with reasonable\nvariance. Following this motivation, we propose a new family of unbiased\nestimators called WTA-CRS, for matrix production with reduced variance, which\nonly requires storing the sub-sampled activations for calculating the gradient.\nOur work provides both theoretical and experimental evidence that, in the\ncontext of tuning transformers, our proposed estimators exhibit lower variance\ncompared to existing ones. By replacing the linear operation with our\napproximated one in transformers, we can achieve up to 2.7$\\times$ peak memory\nreduction with almost no accuracy drop and enables up to $6.4\\times$ larger\nbatch size. Under the same hardware, WTA-CRS enables better down-streaming task\nperformance by applying larger models and/or faster training speed with larger\nbatch sizes.", "published": "2023-05-24 15:52:08", "link": "http://arxiv.org/abs/2305.15265v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "EvEval: A Comprehensive Evaluation of Event Semantics for Large Language\n  Models", "abstract": "Events serve as fundamental units of occurrence within various contexts. The\nprocessing of event semantics in textual information forms the basis of\nnumerous natural language processing (NLP) applications. Recent studies have\nbegun leveraging large language models (LLMs) to address event semantic\nprocessing. However, the extent that LLMs can effectively tackle these\nchallenges remains uncertain. Furthermore, the lack of a comprehensive\nevaluation framework for event semantic processing poses a significant\nchallenge in evaluating these capabilities. In this paper, we propose an\noverarching framework for event semantic processing, encompassing\nunderstanding, reasoning, and prediction, along with their fine-grained\naspects. To comprehensively evaluate the event semantic processing abilities of\nmodels, we introduce a novel benchmark called EVEVAL. We collect 8 datasets\nthat cover all aspects of event semantic processing. Extensive experiments are\nconducted on EVEVAL, leading to several noteworthy findings based on the\nobtained results.", "published": "2023-05-24 15:55:40", "link": "http://arxiv.org/abs/2305.15268v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Testing the General Deductive Reasoning Capacity of Large Language\n  Models Using OOD Examples", "abstract": "Given the intractably large size of the space of proofs, any model that is\ncapable of general deductive reasoning must generalize to proofs of greater\ncomplexity. Recent studies have shown that large language models (LLMs) possess\nsome abstract deductive reasoning ability given chain-of-thought prompts.\nHowever, they have primarily been tested on proofs using modus ponens or of a\nspecific size, and from the same distribution as the in-context examples. To\nmeasure the general deductive reasoning ability of LLMs, we test on a broad set\nof deduction rules and measure their ability to generalize to more complex\nproofs from simpler demonstrations from multiple angles: depth-, width-, and\ncompositional generalization. To facilitate systematic exploration, we\nconstruct a new synthetic and programmable reasoning dataset that enables\ncontrol over deduction rules and proof complexity. Our experiments on four LLMs\nof various sizes and training objectives show that they are able to generalize\nto compositional proofs. However, they have difficulty generalizing to longer\nproofs, and they require explicit demonstrations to produce hypothetical\nsubproofs, specifically in proof by cases and proof by contradiction.", "published": "2023-05-24 15:55:51", "link": "http://arxiv.org/abs/2305.15269v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Simple and Effective Framework for Strict Zero-Shot Hierarchical\n  Classification", "abstract": "In recent years, large language models (LLMs) have achieved strong\nperformance on benchmark tasks, especially in zero or few-shot settings.\nHowever, these benchmarks often do not adequately address the challenges posed\nin the real-world, such as that of hierarchical classification. In order to\naddress this challenge, we propose refactoring conventional tasks on\nhierarchical datasets into a more indicative long-tail prediction task. We\nobserve LLMs are more prone to failure in these cases. To address these\nlimitations, we propose the use of entailment-contradiction prediction in\nconjunction with LLMs, which allows for strong performance in a strict\nzero-shot setting. Importantly, our method does not require any parameter\nupdates, a resource-intensive process and achieves strong performance across\nmultiple datasets.", "published": "2023-05-24 16:04:26", "link": "http://arxiv.org/abs/2305.15282v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Foundation Models for Relational Databases [Vision Paper]", "abstract": "Tabular representation learning has recently gained a lot of attention.\nHowever, existing approaches only learn a representation from a single table,\nand thus ignore the potential to learn from the full structure of relational\ndatabases, including neighboring tables that can contain important information\nfor a contextualized representation. Moreover, current models are significantly\nlimited in scale, which prevents that they learn from large databases. In this\npaper, we thus introduce our vision of relational representation learning, that\ncan not only learn from the full relational structure, but also can scale to\nlarger database sizes that are commonly found in real-world. Moreover, we also\ndiscuss opportunities and challenges we see along the way to enable this vision\nand present initial very promising results. Overall, we argue that this\ndirection can lead to foundation models for relational databases that are today\nonly available for text and images.", "published": "2023-05-24 16:37:35", "link": "http://arxiv.org/abs/2305.15321v1", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "Gorilla: Large Language Model Connected with Massive APIs", "abstract": "Large Language Models (LLMs) have seen an impressive wave of advances\nrecently, with models now excelling in a variety of tasks, such as mathematical\nreasoning and program synthesis. However, their potential to effectively use\ntools via API calls remains unfulfilled. This is a challenging task even for\ntoday's state-of-the-art LLMs such as GPT-4, largely due to their inability to\ngenerate accurate input arguments and their tendency to hallucinate the wrong\nusage of an API call. We release Gorilla, a finetuned LLaMA-based model that\nsurpasses the performance of GPT-4 on writing API calls. When combined with a\ndocument retriever, Gorilla demonstrates a strong capability to adapt to\ntest-time document changes, enabling flexible user updates or version changes.\nIt also substantially mitigates the issue of hallucination, commonly\nencountered when prompting LLMs directly. To evaluate the model's ability, we\nintroduce APIBench, a comprehensive dataset consisting of HuggingFace,\nTorchHub, and TensorHub APIs. The successful integration of the retrieval\nsystem with Gorilla demonstrates the potential for LLMs to use tools more\naccurately, keep up with frequently updated documentation, and consequently\nincrease the reliability and applicability of their outputs. Gorilla's code,\nmodel, data, and demo are available at https://gorilla.cs.berkeley.edu", "published": "2023-05-24 16:48:11", "link": "http://arxiv.org/abs/2305.15334v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Measuring and Mitigating Constraint Violations of In-Context Learning\n  for Utterance-to-API Semantic Parsing", "abstract": "In executable task-oriented semantic parsing, the system aims to translate\nusers' utterances in natural language to machine-interpretable programs (API\ncalls) that can be executed according to pre-defined API specifications. With\nthe popularity of Large Language Models (LLMs), in-context learning offers a\nstrong baseline for such scenarios, especially in data-limited regimes.\nHowever, LLMs are known to hallucinate and therefore pose a formidable\nchallenge in constraining generated content. Thus, it remains uncertain if LLMs\ncan effectively perform task-oriented utterance-to-API generation where\nrespecting API's structural and task-specific constraints is crucial.\n  In this work, we seek to measure, analyze and mitigate such constraints\nviolations. First, we identify the categories of various constraints in\nobtaining API-semantics from task-oriented utterances, and define fine-grained\nmetrics that complement traditional ones. Second, we leverage these metrics to\nconduct a detailed error analysis of constraints violations seen in\nstate-of-the-art LLMs, which motivates us to investigate two mitigation\nstrategies: Semantic-Retrieval of Demonstrations (SRD) and API-aware\nConstrained Decoding (API-CD). Our experiments show that these strategies are\neffective at reducing constraints violations and improving the quality of the\ngenerated API calls, but require careful consideration given their\nimplementation complexity and latency.", "published": "2023-05-24 16:50:36", "link": "http://arxiv.org/abs/2305.15338v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Learning Answer Generation using Supervision from Automatic Question\n  Answering Evaluators", "abstract": "Recent studies show that sentence-level extractive QA, i.e., based on Answer\nSentence Selection (AS2), is outperformed by Generation-based QA (GenQA)\nmodels, which generate answers using the top-k answer sentences ranked by AS2\nmodels (a la retrieval-augmented generation style). In this paper, we propose a\nnovel training paradigm for GenQA using supervision from automatic QA\nevaluation models (GAVA). Specifically, we propose three strategies to transfer\nknowledge from these QA evaluation models to a GenQA model: (i) augmenting\ntraining data with answers generated by the GenQA model and labelled by GAVA\n(either statically, before training, or (ii) dynamically, at every training\nepoch); and (iii) using the GAVA score for weighting the generator loss during\nthe learning of the GenQA model. We evaluate our proposed methods on two\nacademic and one industrial dataset, obtaining a significant improvement in\nanswering accuracy over the previous state of the art.", "published": "2023-05-24 16:57:04", "link": "http://arxiv.org/abs/2305.15344v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Context-Aware Transformer Pre-Training for Answer Sentence Selection", "abstract": "Answer Sentence Selection (AS2) is a core component for building an accurate\nQuestion Answering pipeline. AS2 models rank a set of candidate sentences based\non how likely they answer a given question. The state of the art in AS2\nexploits pre-trained transformers by transferring them on large annotated\ndatasets, while using local contextual information around the candidate\nsentence. In this paper, we propose three pre-training objectives designed to\nmimic the downstream fine-tuning task of contextual AS2. This allows for\nspecializing LMs when fine-tuning for contextual AS2. Our experiments on three\npublic and two large-scale industrial datasets show that our pre-training\napproaches (applied to RoBERTa and ELECTRA) can improve baseline contextual AS2\naccuracy by up to 8% on some datasets.", "published": "2023-05-24 17:10:45", "link": "http://arxiv.org/abs/2305.15358v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Peek Across: Improving Multi-Document Modeling via Cross-Document\n  Question-Answering", "abstract": "The integration of multi-document pre-training objectives into language\nmodels has resulted in remarkable improvements in multi-document downstream\ntasks. In this work, we propose extending this idea by pre-training a generic\nmulti-document model from a novel cross-document question answering\npre-training objective. To that end, given a set (or cluster) of\ntopically-related documents, we systematically generate semantically-oriented\nquestions from a salient sentence in one document and challenge the model,\nduring pre-training, to answer these questions while \"peeking\" into other\ntopically-related documents. In a similar manner, the model is also challenged\nto recover the sentence from which the question was generated, again while\nleveraging cross-document information. This novel multi-document QA formulation\ndirects the model to better recover cross-text informational relations, and\nintroduces a natural augmentation that artificially increases the pre-training\ndata. Further, unlike prior multi-document models that focus on either\nclassification or summarization tasks, our pre-training objective formulation\nenables the model to perform tasks that involve both short text generation\n(e.g., QA) and long text generation (e.g., summarization). Following this\nscheme, we pre-train our model -- termed QAmden -- and evaluate its performance\nacross several multi-document tasks, including multi-document QA,\nsummarization, and query-focused summarization, yielding improvements of up to\n7%, and significantly outperforms zero-shot GPT-3.5 and GPT-4.", "published": "2023-05-24 17:48:40", "link": "http://arxiv.org/abs/2305.15387v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Textless Speech-to-Speech Translation With Limited Parallel Data", "abstract": "Existing speech-to-speech translation (S2ST) models fall into two camps: they\neither leverage text as an intermediate step or require hundreds of hours of\nparallel speech data. Both approaches are incompatible with textless languages\nor language pairs with limited parallel data. We present PFB, a framework for\ntraining textless S2ST models that require just dozens of hours of parallel\nspeech data. We first pretrain a model on large-scale monolingual speech data,\nfinetune it with a small amount of parallel speech data (20-60 hours), and\nlastly train with an unsupervised backtranslation objective. We train and\nevaluate our models for English-to-German, German-to-English and\nMarathi-to-English translation on three different domains (European Parliament,\nCommon Voice, and All India Radio) with single-speaker synthesized speech.\nEvaluated using the ASR-BLEU metric, our models achieve reasonable performance\non all three domains, with some being within 1-2 points of our higher-resourced\ntopline.", "published": "2023-05-24 17:59:05", "link": "http://arxiv.org/abs/2305.15405v3", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "The Larger They Are, the Harder They Fail: Language Models do not\n  Recognize Identifier Swaps in Python", "abstract": "Large Language Models (LLMs) have successfully been applied to code\ngeneration tasks, raising the question of how well these models understand\nprogramming. Typical programming languages have invariances and equivariances\nin their semantics that human programmers intuitively understand and exploit,\nsuch as the (near) invariance to the renaming of identifiers. We show that LLMs\nnot only fail to properly generate correct Python code when default function\nnames are swapped, but some of them even become more confident in their\nincorrect predictions as the model size increases, an instance of the recently\ndiscovered phenomenon of Inverse Scaling, which runs contrary to the commonly\nobserved trend of increasing prediction quality with increasing model size. Our\nfindings indicate that, despite their astonishing typical-case performance,\nLLMs still lack a deep, abstract understanding of the content they manipulate,\nmaking them unsuitable for tasks that statistically deviate from their training\ndata, and that mere scaling is not enough to achieve such capability.", "published": "2023-05-24 18:54:39", "link": "http://arxiv.org/abs/2305.15507v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models are Few-Shot Health Learners", "abstract": "Large language models (LLMs) can capture rich representations of concepts\nthat are useful for real-world tasks. However, language alone is limited. While\nexisting LLMs excel at text-based inferences, health applications require that\nmodels be grounded in numerical data (e.g., vital signs, laboratory values in\nclinical domains; steps, movement in the wellness domain) that is not easily or\nreadily expressed as text in existing training corpus. We demonstrate that with\nonly few-shot tuning, a large language model is capable of grounding various\nphysiological and behavioral time-series data and making meaningful inferences\non numerous health tasks for both clinical and wellness contexts. Using data\nfrom wearable and medical sensor recordings, we evaluate these capabilities on\nthe tasks of cardiac signal analysis, physical activity recognition, metabolic\ncalculation (e.g., calories burned), and estimation of stress reports and\nmental health screeners.", "published": "2023-05-24 19:25:16", "link": "http://arxiv.org/abs/2305.15525v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Harnessing the Power of Large Language Models for Natural Language to\n  First-Order Logic Translation", "abstract": "Translating natural language sentences to first-order logic (NL-FOL\ntranslation) is a longstanding challenge in the NLP and formal logic\nliterature. This paper introduces LogicLLaMA, a LLaMA-7B model fine-tuned for\nNL-FOL translation using LoRA on a single GPU. LogicLLaMA is capable of\ndirectly translating natural language into FOL rules, which outperforms\nGPT-3.5. LogicLLaMA is also equipped to correct FOL rules predicted by GPT-3.5,\nand can achieve similar performance as GPT-4 with a fraction of the cost. This\ncorrection ability was achieved by a novel supervised fine-tuning (SFT) +\nreinforcement learning with human feedback (RLHF) framework, which initially\ntrains on synthetically perturbed NL-FOL pairs to encourage chain-of-thought\nreasoning and then fine-tunes with RLHF on GPT-3.5 outputs using a FOL verifier\nas the reward model.\n  To train LogicLLaMA, we present MALLS (large language $\\textbf{M}$odel\ngener$\\textbf{A}$ted N$\\textbf{L}$-FO$\\textbf{L}$ pair$\\textbf{S}$), a dataset\nof 34K high-quality and diverse sentence-level NL-FOL pairs collected from\nGPT-4. The dataset was created by implementing a pipeline that prompts GPT-4\nfor pairs, and dynamically adjusts the prompts to ensure the collection of\npairs with rich and diverse contexts at different levels of complexity, and\nverifies the validity of the generated FOL rules. Codes, weights, and data are\navailable at $\\href{https://github.com/gblackout/LogicLLaMA}{{\\small\n\\text{https://github.com/gblackout/LogicLLaMA}}}$.", "published": "2023-05-24 19:59:51", "link": "http://arxiv.org/abs/2305.15541v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How do humans perceive adversarial text? A reality check on the validity\n  and naturalness of word-based adversarial attacks", "abstract": "Natural Language Processing (NLP) models based on Machine Learning (ML) are\nsusceptible to adversarial attacks -- malicious algorithms that imperceptibly\nmodify input text to force models into making incorrect predictions. However,\nevaluations of these attacks ignore the property of imperceptibility or study\nit under limited settings. This entails that adversarial perturbations would\nnot pass any human quality gate and do not represent real threats to\nhuman-checked NLP systems. To bypass this limitation and enable proper\nassessment (and later, improvement) of NLP model robustness, we have surveyed\n378 human participants about the perceptibility of text adversarial examples\nproduced by state-of-the-art methods. Our results underline that existing text\nattacks are impractical in real-world scenarios where humans are involved. This\ncontrasts with previous smaller-scale human studies, which reported overly\noptimistic conclusions regarding attack success. Through our work, we hope to\nposition human perceptibility as a first-class success criterion for text\nattacks, and provide guidance for research to build effective attack algorithms\nand, in turn, design appropriate defence mechanisms.", "published": "2023-05-24 21:52:13", "link": "http://arxiv.org/abs/2305.15587v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Don't Trust ChatGPT when Your Question is not in English: A Study of\n  Multilingual Abilities and Types of LLMs", "abstract": "Large Language Models (LLMs) have demonstrated exceptional natural language\nunderstanding abilities and have excelled in a variety of natural language\nprocessing (NLP)tasks in recent years. Despite the fact that most LLMs are\ntrained predominantly in English, multiple studies have demonstrated their\ncomparative performance in many other languages. However, fundamental questions\npersist regarding how LLMs acquire their multi-lingual abilities and how\nperformance varies across different languages. These inquiries are crucial for\nthe study of LLMs since users and researchers often come from diverse language\nbackgrounds, potentially influencing their utilization and interpretation of\nLLMs' results. In this work, we propose a systematic way of qualifying the\nperformance disparities of LLMs under multilingual settings. We investigate the\nphenomenon of across-language generalizations in LLMs, wherein insufficient\nmulti-lingual training data leads to advanced multi-lingual capabilities. To\naccomplish this, we employ a novel back-translation-based prompting method. The\nresults show that GPT exhibits highly translating-like behaviour in\nmultilingual settings.", "published": "2023-05-24 02:05:03", "link": "http://arxiv.org/abs/2305.16339v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Distributed Automatic Domain-Specific Multi-Word Term Recognition\n  Architecture using Spark Ecosystem", "abstract": "Automatic Term Recognition is used to extract domain-specific terms that\nbelong to a given domain. In order to be accurate, these corpus and\nlanguage-dependent methods require large volumes of textual data that need to\nbe processed to extract candidate terms that are afterward scored according to\na given metric. To improve text preprocessing and candidate terms extraction\nand scoring, we propose a distributed Spark-based architecture to automatically\nextract domain-specific terms. The main contributions are as follows: (1)\npropose a novel distributed automatic domain-specific multi-word term\nrecognition architecture built on top of the Spark ecosystem; (2) perform an\nin-depth analysis of our architecture in terms of accuracy and scalability; (3)\ndesign an easy-to-integrate Python implementation that enables the use of Big\nData processing in fields such as Computational Linguistics and Natural\nLanguage Processing. We prove empirically the feasibility of our architecture\nby performing experiments on two real-world datasets.", "published": "2023-05-24 10:05:59", "link": "http://arxiv.org/abs/2305.16343v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enabling and Analyzing How to Efficiently Extract Information from\n  Hybrid Long Documents with LLMs", "abstract": "Large Language Models (LLMs) demonstrate exceptional performance in textual\nunderstanding and tabular reasoning tasks. However, their ability to comprehend\nand analyze hybrid text, containing textual and tabular data, remains\nunderexplored. In this research, we specialize in harnessing the potential of\nLLMs to comprehend critical information from financial reports, which are\nhybrid long-documents. We propose an Automated Financial Information Extraction\n(AFIE) framework that enhances LLMs' ability to comprehend and extract\ninformation from financial reports. To evaluate AFIE, we develop a Financial\nReports Numerical Extraction (FINE) dataset and conduct an extensive\nexperimental analysis. Our framework is effectively validated on GPT-3.5 and\nGPT-4, yielding average accuracy increases of 53.94% and 33.77%, respectively,\ncompared to a naive method. These results suggest that the AFIE framework\noffers accuracy for automated numerical extraction from complex, hybrid\ndocuments.", "published": "2023-05-24 10:35:58", "link": "http://arxiv.org/abs/2305.16344v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Empathetic Dialogue Generation by Dynamically Infusing\n  Commonsense Knowledge", "abstract": "In empathetic conversations, individuals express their empathy towards\nothers. Previous work has mainly focused on generating empathetic responses by\nutilizing the speaker's emotion. Besides, external commonsense knowledge has\nbeen applied to enhance the system's understandings of the speaker's situation.\nHowever, given an event, commonsense knowledge base contains various relations,\npotentially leading to confusion for the dialogue system. Consequently,\ninconsistencies arise among the emotion, generated response and speaker's\ncontextual information. To this end, we propose a novel approach for empathetic\nresponse generation, which incorporates an adaptive module for commonsense\nknowledge selection to ensure consistency between the generated empathetic\nresponses and the speaker's situation. This selected knowledge is used to\nrefine the commonsense cognition and empathy expression for generated\nresponses. Experimental results show that our approach significantly\noutperforms baseline models in both automatic and human evaluations, exhibiting\nthe generation of more coherent and empathetic responses. Moreover, case\nstudies highlight the interpretability of knowledge selection in the responses\nand the effectiveness of adaptive module in our model. Code:\nhttps://github.com/Hanscal/DCKS.", "published": "2023-05-24 10:25:12", "link": "http://arxiv.org/abs/2306.04657v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Voices of Her: Analyzing Gender Differences in the AI Publication World", "abstract": "While several previous studies have analyzed gender bias in research, we are\nstill missing a comprehensive analysis of gender differences in the AI\ncommunity, covering diverse topics and different development trends. Using the\nAI Scholar dataset of 78K researchers in the field of AI, we identify several\ngender differences: (1) Although female researchers tend to have fewer overall\ncitations than males, this citation difference does not hold for all\nacademic-age groups; (2) There exist large gender homophily in co-authorship on\nAI papers; (3) Female first-authored papers show distinct linguistic styles,\nsuch as longer text, more positive emotion words, and more catchy titles than\nmale first-authored papers. Our analysis provides a window into the current\ndemographic trends in our AI community, and encourages more gender equality and\ndiversity in the future. Our code and data are at\nhttps://github.com/causalNLP/ai-scholar-gender.", "published": "2023-05-24 00:40:49", "link": "http://arxiv.org/abs/2305.14597v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enabling Large Language Models to Generate Text with Citations", "abstract": "Large language models (LLMs) have emerged as a widely-used tool for\ninformation seeking, but their generated outputs are prone to hallucination. In\nthis work, our aim is to allow LLMs to generate text with citations, improving\ntheir factual correctness and verifiability. Existing work mainly relies on\ncommercial search engines and human evaluation, making it challenging to\nreproduce and compare different modeling approaches. We propose ALCE, the first\nbenchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set\nof questions and retrieval corpora and requires building end-to-end systems to\nretrieve supporting evidence and generate answers with citations. We develop\nautomatic metrics along three dimensions -- fluency, correctness, and citation\nquality -- and demonstrate their strong correlation with human judgements. Our\nexperiments with state-of-the-art LLMs and novel prompting strategies show that\ncurrent systems have considerable room for improvement -- For example, on the\nELI5 dataset, even the best models lack complete citation support 50% of the\ntime. Our analyses further highlight promising future directions, including\ndeveloping better retrievers, advancing long-context LLMs, and improving the\nability to synthesize information from multiple sources.", "published": "2023-05-24 01:53:49", "link": "http://arxiv.org/abs/2305.14627v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation", "abstract": "End-to-end speech translation (ST) is the task of translating speech signals\nin the source language into text in the target language. As a cross-modal task,\nend-to-end ST is difficult to train with limited data. Existing methods often\ntry to transfer knowledge from machine translation (MT), but their performances\nare restricted by the modality gap between speech and text. In this paper, we\npropose Cross-modal Mixup via Optimal Transport CMOT to overcome the modality\ngap. We find the alignment between speech and text sequences via optimal\ntransport and then mix up the sequences from different modalities at a token\nlevel using the alignment. Experiments on the MuST-C ST benchmark demonstrate\nthat CMOT achieves an average BLEU of 30.0 in 8 translation directions,\noutperforming previous methods. Further analysis shows CMOT can adaptively find\nthe alignment between modalities, which helps alleviate the modality gap\nbetween speech and text. Code is publicly available at\nhttps://github.com/ictnlp/CMOT.", "published": "2023-05-24 02:13:48", "link": "http://arxiv.org/abs/2305.14635v2", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Revisit and Outstrip Entity Alignment: A Perspective of Generative\n  Models", "abstract": "Recent embedding-based methods have achieved great successes in exploiting\nentity alignment from knowledge graph (KG) embeddings of multiple modalities.\nIn this paper, we study embedding-based entity alignment (EEA) from a\nperspective of generative models. We show that EEA shares similarities with\ntypical generative models and prove the effectiveness of the recently developed\ngenerative adversarial network (GAN)-based EEA methods theoretically. We then\nreveal that their incomplete objective limits the capacity on both entity\nalignment and entity synthesis (i.e., generating new entities). We mitigate\nthis problem by introducing a generative EEA (GEEA) framework with the proposed\nmutual variational autoencoder (M-VAE) as the generative model. M-VAE enables\nentity conversion between KGs and generation of new entities from random noise\nvectors. We demonstrate the power of GEEA with theoretical analysis and\nempirical experiments on both entity alignment and entity synthesis tasks.", "published": "2023-05-24 02:39:20", "link": "http://arxiv.org/abs/2305.14651v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Quantifying Character Similarity with Vision Transformers", "abstract": "Record linkage is a bedrock of quantitative social science, as analyses often\nrequire linking data from multiple, noisy sources. Off-the-shelf string\nmatching methods are widely used, as they are straightforward and cheap to\nimplement and scale. Not all character substitutions are equally probable, and\nfor some settings there are widely used handcrafted lists denoting which string\nsubstitutions are more likely, that improve the accuracy of string matching.\nHowever, such lists do not exist for many settings, skewing research with\nlinked datasets towards a few high-resource contexts that are not\nrepresentative of the diversity of human societies. This study develops an\nextensible way to measure character substitution costs for OCR'ed documents, by\nemploying large-scale self-supervised training of vision transformers (ViT)\nwith augmented digital fonts. For each language written with the CJK script, we\ncontrastively learn a metric space where different augmentations of the same\ncharacter are represented nearby. In this space, homoglyphic characters - those\nwith similar appearance such as ``O'' and ``0'' - have similar vector\nrepresentations. Using the cosine distance between characters' representations\nas the substitution cost in an edit distance matching algorithm significantly\nimproves record linkage compared to other widely used string matching methods,\nas OCR errors tend to be homoglyphic in nature. Homoglyphs can plausibly\ncapture character visual similarity across any script, including low-resource\nsettings. We illustrate this by creating homoglyph sets for 3,000 year old\nancient Chinese characters, which are highly pictorial. Fascinatingly, a ViT is\nable to capture relationships in how different abstract concepts were\nconceptualized by ancient societies, that have been noted in the archaeological\nliterature.", "published": "2023-05-24 03:25:33", "link": "http://arxiv.org/abs/2305.14672v1", "categories": ["cs.CL", "cs.CV", "econ.GN", "q-fin.EC"], "primary_category": "cs.CL"}
{"title": "A Causal View of Entity Bias in (Large) Language Models", "abstract": "Entity bias widely affects pretrained (large) language models, causing them\nto rely on (biased) parametric knowledge to make unfaithful predictions.\nAlthough causality-inspired methods have shown great potential to mitigate\nentity bias, it is hard to precisely estimate the parameters of underlying\ncausal models in practice. The rise of black-box LLMs also makes the situation\neven worse, because of their inaccessible parameters and uncalibrated logits.\nTo address these problems, we propose a specific structured causal model (SCM)\nwhose parameters are comparatively easier to estimate. Building upon this SCM,\nwe propose causal intervention techniques to mitigate entity bias for both\nwhite-box and black-box settings. The proposed causal intervention perturbs the\noriginal entity with neighboring entities. This intervention reduces specific\nbiasing information pertaining to the original entity while still preserving\nsufficient semantic information from similar entities. Under the white-box\nsetting, our training-time intervention improves OOD performance of PLMs on\nrelation extraction (RE) and machine reading comprehension (MRC) by 5.7 points\nand by 9.1 points, respectively. Under the black-box setting, our in-context\nintervention effectively reduces the entity-based knowledge conflicts of\nGPT-3.5, achieving up to 20.5 points of improvement of exact match accuracy on\nMRC and up to 17.6 points of reduction in memorization ratio on RE. Our code is\navailable at https://github.com/luka-group/Causal-View-of-Entity-Bias.", "published": "2023-05-24 03:59:18", "link": "http://arxiv.org/abs/2305.14695v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SciFix: Outperforming GPT3 on Scientific Factual Error Correction", "abstract": "Due to the prohibitively high cost of creating error correction datasets,\nmost Factual Claim Correction methods rely on a powerful verification model to\nguide the correction process. This leads to a significant drop in performance\nin domains like scientific claims, where good verification models do not always\nexist. In this work, we introduce SciFix, a scientific claim correction system\nthat does not require a verifier but can outperform existing methods by a\nconsiderable margin -- achieving correction accuracy of 84% on the SciFact\ndataset, 77% on SciFact-Open and 72% on the CovidFact dataset, compared to next\nbest accuracies of 7%, 5%, and 15% on the same datasets respectively. Our\nmethod leverages the power of prompting with LLMs during training to create a\nrichly annotated dataset that can be used for fully supervised training and\nregularization. We additionally use a claim-aware decoding procedure to improve\nthe quality of corrected claims. Our method outperforms the very LLM that was\nused to generate the annotated dataset -- with Few-Shot Prompting on GPT3.5\nachieving 58%, 61%, and 64% on the respective datasets, a consistently lower\ncorrection accuracy, despite using nearly 800 times as many parameters as our\nmodel.", "published": "2023-05-24 04:24:16", "link": "http://arxiv.org/abs/2305.14707v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Instructions as Backdoors: Backdoor Vulnerabilities of Instruction\n  Tuning for Large Language Models", "abstract": "We investigate security concerns of the emergent instruction tuning paradigm,\nthat models are trained on crowdsourced datasets with task instructions to\nachieve superior performance. Our studies demonstrate that an attacker can\ninject backdoors by issuing very few malicious instructions (~1000 tokens) and\ncontrol model behavior through data poisoning, without even the need to modify\ndata instances or labels themselves. Through such instruction attacks, the\nattacker can achieve over 90% attack success rate across four commonly used NLP\ndatasets. As an empirical study on instruction attacks, we systematically\nevaluated unique perspectives of instruction attacks, such as poison transfer\nwhere poisoned models can transfer to 15 diverse generative datasets in a\nzero-shot manner; instruction transfer where attackers can directly apply\npoisoned instruction on many other datasets; and poison resistance to continual\nfinetuning. Lastly, we show that RLHF and clean demonstrations might mitigate\nsuch backdoors to some degree. These findings highlight the need for more\nrobust defenses against poisoning attacks in instruction-tuning models and\nunderscore the importance of ensuring data quality in instruction\ncrowdsourcing.", "published": "2023-05-24 04:27:21", "link": "http://arxiv.org/abs/2305.14710v2", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create\n  Visual Metaphors", "abstract": "Visual metaphors are powerful rhetorical devices used to persuade or\ncommunicate creative ideas through images. Similar to linguistic metaphors,\nthey convey meaning implicitly through symbolism and juxtaposition of the\nsymbols. We propose a new task of generating visual metaphors from linguistic\nmetaphors. This is a challenging task for diffusion-based text-to-image models,\nsuch as DALL$\\cdot$E 2, since it requires the ability to model implicit meaning\nand compositionality. We propose to solve the task through the collaboration\nbetween Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3\n(davinci-002) with Chain-of-Thought prompting generates text that represents a\nvisual elaboration of the linguistic metaphor containing the implicit meaning\nand relevant objects, which is then used as input to the diffusion-based\ntext-to-image models.Using a human-AI collaboration framework, where humans\ninteract both with the LLM and the top-performing diffusion model, we create a\nhigh-quality dataset containing 6,476 visual metaphors for 1,540 linguistic\nmetaphors and their associated visual elaborations. Evaluation by professional\nillustrators shows the promise of LLM-Diffusion Model collaboration for this\ntask . To evaluate the utility of our Human-AI collaboration framework and the\nquality of our dataset, we perform both an intrinsic human-based evaluation and\nan extrinsic evaluation using visual entailment as a downstream task.", "published": "2023-05-24 05:01:10", "link": "http://arxiv.org/abs/2305.14724v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Centering the Margins: Outlier-Based Identification of Harmed\n  Populations in Toxicity Detection", "abstract": "The impact of AI models on marginalized communities has traditionally been\nmeasured by identifying performance differences between specified demographic\nsubgroups. Though this approach aims to center vulnerable groups, it risks\nobscuring patterns of harm faced by intersectional subgroups or shared across\nmultiple groups. To address this, we draw on theories of marginalization from\ndisability studies and related disciplines, which state that people farther\nfrom the norm face greater adversity, to consider the \"margins\" in the domain\nof toxicity detection. We operationalize the \"margins\" of a dataset by\nemploying outlier detection to identify text about people with demographic\nattributes distant from the \"norm\". We find that model performance is\nconsistently worse for demographic outliers, with mean squared error (MSE)\nbetween outliers and non-outliers up to 70.4% worse across toxicity types. It\nis also worse for text outliers, with a MSE up to 68.4% higher for outliers\nthan non-outliers. We also find text and demographic outliers to be\nparticularly susceptible to errors in the classification of severe toxicity and\nidentity attacks. Compared to analysis of disparities using traditional\ndemographic breakdowns, we find that our outlier analysis frequently surfaces\ngreater harms faced by a larger, more intersectional group, which suggests that\noutlier analysis is particularly beneficial for identifying harms against those\ngroups.", "published": "2023-05-24 05:15:36", "link": "http://arxiv.org/abs/2305.14735v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ECHo: A Visio-Linguistic Dataset for Event Causality Inference via\n  Human-Centric Reasoning", "abstract": "We introduce ECHo (Event Causality Inference via Human-Centric Reasoning), a\ndiagnostic dataset of event causality inference grounded in visio-linguistic\nsocial scenarios. ECHo employs real-world human-centric deductive information\nbuilding on a television crime drama. ECHo requires the Theory-of-Mind (ToM)\nability to understand and reason about social interactions based on multimodal\ninformation. Using ECHo, we propose a unified Chain-of-Thought (CoT) framework\nto assess the reasoning capability of current AI systems. Our ToM-enhanced CoT\npipeline accommodates various large foundation models in both zero-shot and\nfew-shot visio-linguistic reasoning. We use this framework to scrutinize recent\nlarge foundation models such as InstructGPT and MiniGPT-4 on three diagnostic\nhuman-centric tasks. Further analysis demonstrates ECHo as a challenging\ndataset to expose imperfections and inconsistencies in reasoning. Our data and\ncode are publicly available at https://github.com/YuxiXie/ECHo.", "published": "2023-05-24 05:21:13", "link": "http://arxiv.org/abs/2305.14740v2", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Measuring the Knowledge Acquisition-Utilization Gap in Pretrained\n  Language Models", "abstract": "While pre-trained language models (PLMs) have shown evidence of acquiring\nvast amounts of knowledge, it remains unclear how much of this parametric\nknowledge is actually usable in performing downstream tasks. We propose a\nsystematic framework to measure parametric knowledge utilization in PLMs. Our\nframework first extracts knowledge from a PLM's parameters and subsequently\nconstructs a downstream task around this extracted knowledge. Performance on\nthis task thus depends exclusively on utilizing the model's possessed\nknowledge, avoiding confounding factors like insufficient signal. As an\ninstantiation, we study factual knowledge of PLMs and measure utilization\nacross 125M to 13B parameter PLMs. We observe that: (1) PLMs exhibit two gaps -\nin acquired vs. utilized knowledge, (2) they show limited robustness in\nutilizing knowledge under distribution shifts, and (3) larger models close the\nacquired knowledge gap but the utilized knowledge gap remains. Overall, our\nstudy provides insights into PLMs' capabilities beyond their acquired\nknowledge.", "published": "2023-05-24 06:26:11", "link": "http://arxiv.org/abs/2305.14775v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Alt-Text with Context: Improving Accessibility for Images on Twitter", "abstract": "In this work we present an approach for generating alternative text (or\nalt-text) descriptions for images shared on social media, specifically Twitter.\nMore than just a special case of image captioning, alt-text is both more\nliterally descriptive and context-specific. Also critically, images posted to\nTwitter are often accompanied by user-written text that despite not necessarily\ndescribing the image may provide useful context that if properly leveraged can\nbe informative. We address this task with a multimodal model that conditions on\nboth textual information from the associated social media post as well as\nvisual signal from the image, and demonstrate that the utility of these two\ninformation sources stacks. We put forward a new dataset of 371k images paired\nwith alt-text and tweets scraped from Twitter and evaluate on it across a\nvariety of automated metrics as well as human evaluation. We show that our\napproach of conditioning on both tweet text and visual information\nsignificantly outperforms prior work, by more than 2x on BLEU@4.", "published": "2023-05-24 06:35:26", "link": "http://arxiv.org/abs/2305.14779v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Anthropomorphization of AI: Opportunities and Risks", "abstract": "Anthropomorphization is the tendency to attribute human-like traits to\nnon-human entities. It is prevalent in many social contexts -- children\nanthropomorphize toys, adults do so with brands, and it is a literary device.\nIt is also a versatile tool in science, with behavioral psychology and\nevolutionary biology meticulously documenting its consequences. With widespread\nadoption of AI systems, and the push from stakeholders to make it human-like\nthrough alignment techniques, human voice, and pictorial avatars, the tendency\nfor users to anthropomorphize it increases significantly. We take a dyadic\napproach to understanding this phenomenon with large language models (LLMs) by\nstudying (1) the objective legal implications, as analyzed through the lens of\nthe recent blueprint of AI bill of rights and the (2) subtle psychological\naspects customization and anthropomorphization. We find that anthropomorphized\nLLMs customized for different user bases violate multiple provisions in the\nlegislative blueprint. In addition, we point out that anthropomorphization of\nLLMs affects the influence they can have on their users, thus having the\npotential to fundamentally change the nature of human-AI interaction, with\npotential for manipulation and negative influence. With LLMs being\nhyper-personalized for vulnerable groups like children and patients among\nothers, our work is a timely and important contribution. We propose a\nconservative strategy for the cautious use of anthropomorphization to improve\ntrustworthiness of AI systems.", "published": "2023-05-24 06:39:45", "link": "http://arxiv.org/abs/2305.14784v1", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak\n  Supervision for Text Classification", "abstract": "Recent advances in weakly supervised text classification mostly focus on\ndesigning sophisticated methods to turn high-level human heuristics into\nquality pseudo-labels. In this paper, we revisit the seed matching-based\nmethod, which is arguably the simplest way to generate pseudo-labels, and show\nthat its power was greatly underestimated. We show that the limited performance\nof seed matching is largely due to the label bias injected by the simple\nseed-match rule, which prevents the classifier from learning reliable\nconfidence for selecting high-quality pseudo-labels. Interestingly, simply\ndeleting the seed words present in the matched input texts can mitigate the\nlabel bias and help learn better confidence. Subsequently, the performance\nachieved by seed matching can be improved significantly, making it on par with\nor even better than the state-of-the-art. Furthermore, to handle the case when\nthe seed words are not made known, we propose to simply delete the word tokens\nin the input text randomly with a high deletion ratio. Remarkably, seed\nmatching equipped with this random deletion method can often achieve even\nbetter performance than that with seed deletion.", "published": "2023-05-24 06:45:33", "link": "http://arxiv.org/abs/2305.14794v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ComSL: A Composite Speech-Language Model for End-to-End Speech-to-Text\n  Translation", "abstract": "Joint speech-language training is challenging due to the large demand for\ntraining data and GPU consumption, as well as the modality gap between speech\nand language. We present ComSL, a speech-language model built atop a composite\narchitecture of public pretrained speech-only and language-only models and\noptimized data-efficiently for spoken language tasks. Particularly, we propose\nto incorporate cross-modality learning into transfer learning and conduct them\nsimultaneously for downstream tasks in a multi-task learning manner. Our\napproach has demonstrated effectiveness in end-to-end speech-to-text\ntranslation tasks, achieving a new state-of-the-art average BLEU score of 31.5\non the multilingual speech to English text translation task for 21 languages,\nas measured on the public CoVoST2 evaluation set.", "published": "2023-05-24 07:42:15", "link": "http://arxiv.org/abs/2305.14838v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Utility-Probability Duality of Neural Networks", "abstract": "It is typically understood that the training of modern neural networks is a\nprocess of fitting the probability distribution of desired output. However,\nrecent paradoxical observations in a number of language generation tasks let\none wonder if this canonical probability-based explanation can really account\nfor the empirical success of deep learning. To resolve this issue, we propose\nan alternative utility-based explanation to the standard supervised learning\nprocedure in deep learning. The basic idea is to interpret the learned neural\nnetwork not as a probability model but as an ordinal utility function that\nencodes the preference revealed in training data. In this perspective, training\nof the neural network corresponds to a utility learning process. Specifically,\nwe show that for all neural networks with softmax outputs, the SGD learning\ndynamic of maximum likelihood estimation (MLE) can be seen as an iteration\nprocess that optimizes the neural network toward an optimal utility function.\nThis utility-based interpretation can explain several otherwise-paradoxical\nobservations about the neural networks thus trained. Moreover, our\nutility-based theory also entails an equation that can transform the learned\nutility values back to a new kind of probability estimation with which\nprobability-compatible decision rules enjoy dramatic (double-digits)\nperformance improvements. These evidences collectively reveal a phenomenon of\nutility-probability duality in terms of what modern neural networks are (truly)\nmodeling: We thought they are one thing (probabilities), until the\nunexplainable showed up; changing mindset and treating them as another thing\n(utility values) largely reconcile the theory, despite remaining subtleties\nregarding its original (probabilistic) identity.", "published": "2023-05-24 08:09:07", "link": "http://arxiv.org/abs/2305.14859v2", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Dynamic Clue Bottlenecks: Towards Interpretable-by-Design Visual\n  Question Answering", "abstract": "Recent advances in multimodal large language models (LLMs) have shown extreme\neffectiveness in visual question answering (VQA). However, the design nature of\nthese end-to-end models prevents them from being interpretable to humans,\nundermining trust and applicability in critical domains. While post-hoc\nrationales offer certain insight into understanding model behavior, these\nexplanations are not guaranteed to be faithful to the model. In this paper, we\naddress these shortcomings by introducing an interpretable by design model that\nfactors model decisions into intermediate human-legible explanations, and\nallows people to easily understand why a model fails or succeeds. We propose\nthe Dynamic Clue Bottleneck Model ( (DCLUB), a method that is designed towards\nan inherently interpretable VQA system. DCLUB provides an explainable\nintermediate space before the VQA decision and is faithful from the beginning,\nwhile maintaining comparable performance to black-box systems. Given a\nquestion, DCLUB first returns a set of visual clues: natural language\nstatements of visually salient evidence from the image, and then generates the\noutput based solely on the visual clues. To supervise and evaluate the\ngeneration of VQA explanations within DCLUB, we collect a dataset of 1.7k\nreasoning-focused questions with visual clues. Evaluations show that our\ninherently interpretable system can improve 4.64% over a comparable black-box\nsystem in reasoning-focused questions while preserving 99.43% of performance on\nVQA-v2.", "published": "2023-05-24 08:33:15", "link": "http://arxiv.org/abs/2305.14882v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Privacy Implications of Retrieval-Based Language Models", "abstract": "Retrieval-based language models (LMs) have demonstrated improved\ninterpretability, factuality, and adaptability compared to their parametric\ncounterparts, by incorporating retrieved text from external datastores. While\nit is well known that parametric models are prone to leaking private data, it\nremains unclear how the addition of a retrieval datastore impacts model\nprivacy. In this work, we present the first study of privacy risks in\nretrieval-based LMs, particularly $k$NN-LMs. Our goal is to explore the optimal\ndesign and training procedure in domains where privacy is of concern, aiming to\nstrike a balance between utility and privacy. Crucially, we find that $k$NN-LMs\nare more susceptible to leaking private information from their private\ndatastore than parametric models. We further explore mitigations of privacy\nrisks. When privacy information is targeted and readily detected in the text,\nwe find that a simple sanitization step would completely eliminate the risks,\nwhile decoupling query and key encoders achieves an even better utility-privacy\ntrade-off. Otherwise, we consider strategies of mixing public and private data\nin both datastore and encoder training. While these methods offer modest\nimprovements, they leave considerable room for future work. Together, our\nfindings provide insights for practitioners to better understand and mitigate\nprivacy risks in retrieval-based LMs. Our code is available at:\nhttps://github.com/Princeton-SysML/kNNLM_privacy .", "published": "2023-05-24 08:37:27", "link": "http://arxiv.org/abs/2305.14888v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Text encoders bottleneck compositionality in contrastive vision-language\n  models", "abstract": "Performant vision-language (VL) models like CLIP represent captions using a\nsingle vector. How much information about language is lost in this bottleneck?\nWe first curate CompPrompts, a set of increasingly compositional image captions\nthat VL models should be able to capture (e.g., single object, to\nobject+property, to multiple interacting objects). Then, we train text-only\nrecovery probes that aim to reconstruct captions from single-vector text\nrepresentations produced by several VL models. This approach does not require\nimages, allowing us to test on a broader range of scenes compared to prior\nwork. We find that: 1) CLIP's text encoder falls short on more compositional\ninputs, including object relationships, attribute-object association, counting,\nand negations; 2) some text encoders work significantly better than others; and\n3) text-only recovery performance predicts multi-modal matching performance on\nControlledImCaps: a new evaluation benchmark we collect and release consisting\nof fine-grained compositional images and captions. Specifically, our results\nsuggest text-only recoverability is a necessary (but not sufficient) condition\nfor modeling compositional factors in contrastive VL models. We release our\ndatasets and code.", "published": "2023-05-24 08:48:44", "link": "http://arxiv.org/abs/2305.14897v2", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Identifying Informational Sources in News Articles", "abstract": "News articles are driven by the informational sources journalists use in\nreporting. Modeling when, how and why sources get used together in stories can\nhelp us better understand the information we consume and even help journalists\nwith the task of producing it. In this work, we take steps toward this goal by\nconstructing the largest and widest-ranging annotated dataset, to date, of\ninformational sources used in news writing. We show that our dataset can be\nused to train high-performing models for information detection and source\nattribution. We further introduce a novel task, source prediction, to study the\ncompositionality of sources in news articles. We show good performance on this\ntask, which we argue is an important proof for narrative science exploring the\ninternal structure of news articles and aiding in planning-based language\ngeneration, and an important step towards a source-recommendation system to aid\njournalists.", "published": "2023-05-24 08:56:35", "link": "http://arxiv.org/abs/2305.14904v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "From Shortcuts to Triggers: Backdoor Defense with Denoised PoE", "abstract": "Language models are often at risk of diverse backdoor attacks, especially\ndata poisoning. Thus, it is important to investigate defense solutions for\naddressing them. Existing backdoor defense methods mainly focus on backdoor\nattacks with explicit triggers, leaving a universal defense against various\nbackdoor attacks with diverse triggers largely unexplored. In this paper, we\npropose an end-to-end ensemble-based backdoor defense framework, DPoE (Denoised\nProduct-of-Experts), which is inspired by the shortcut nature of backdoor\nattacks, to defend various backdoor attacks. DPoE consists of two models: a\nshallow model that captures the backdoor shortcuts and a main model that is\nprevented from learning the backdoor shortcuts. To address the label flip\ncaused by backdoor attackers, DPoE incorporates a denoising design. Experiments\non SST-2 dataset show that DPoE significantly improves the defense performance\nagainst various types of backdoor triggers including word-level,\nsentence-level, and syntactic triggers. Furthermore, DPoE is also effective\nunder a more challenging but practical setting that mixes multiple types of\ntrigger.", "published": "2023-05-24 08:59:25", "link": "http://arxiv.org/abs/2305.14910v3", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Universal Self-Adaptive Prompting", "abstract": "A hallmark of modern large language models (LLMs) is their impressive general\nzero-shot and few-shot abilities, often elicited through in-context learning\n(ICL) via prompting. However, while highly coveted and being the most general,\nzero-shot performances in LLMs are still typically weaker due to the lack of\nguidance and the difficulty of applying existing automatic prompt design\nmethods in general tasks when ground-truth labels are unavailable. In this\nstudy, we address this by presenting Universal Self-Adaptive Prompting (USP),\nan automatic prompt design approach specifically tailored for zero-shot\nlearning (while compatible with few-shot). Requiring only a small amount of\nunlabeled data and an inference-only LLM, USP is highly versatile: to achieve\nuniversal prompting, USP categorizes a possible NLP task into one of the three\npossible task types and then uses a corresponding selector to select the most\nsuitable queries and zero-shot model-generated responses as\npseudo-demonstrations, thereby generalizing ICL to the zero-shot setup in a\nfully automated way. We evaluate USP with PaLM and PaLM 2 models and\ndemonstrate performances that are considerably stronger than standard zero-shot\nbaselines and often comparable to or even superior to few-shot baselines across\nmore than 40 natural language understanding, natural language generation, and\nreasoning tasks.", "published": "2023-05-24 09:09:48", "link": "http://arxiv.org/abs/2305.14926v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "In-Context Impersonation Reveals Large Language Models' Strengths and\n  Biases", "abstract": "In everyday conversations, humans can take on different roles and adapt their\nvocabulary to their chosen roles. We explore whether LLMs can take on, that is\nimpersonate, different roles when they generate text in-context. We ask LLMs to\nassume different personas before solving vision and language tasks. We do this\nby prefixing the prompt with a persona that is associated either with a social\nidentity or domain expertise. In a multi-armed bandit task, we find that LLMs\npretending to be children of different ages recover human-like developmental\nstages of exploration. In a language-based reasoning task, we find that LLMs\nimpersonating domain experts perform better than LLMs impersonating non-domain\nexperts. Finally, we test whether LLMs' impersonations are complementary to\nvisual information when describing different categories. We find that\nimpersonation can improve performance: an LLM prompted to be a bird expert\ndescribes birds better than one prompted to be a car expert. However,\nimpersonation can also uncover LLMs' biases: an LLM prompted to be a man\ndescribes cars better than one prompted to be a woman. These findings\ndemonstrate that LLMs are capable of taking on diverse roles and that this\nin-context impersonation can be used to uncover their hidden strengths and\nbiases.", "published": "2023-05-24 09:13:15", "link": "http://arxiv.org/abs/2305.14930v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Adversarial Demonstration Attacks on Large Language Models", "abstract": "With the emergence of more powerful large language models (LLMs), such as\nChatGPT and GPT-4, in-context learning (ICL) has gained significant prominence\nin leveraging these models for specific tasks by utilizing data-label pairs as\nprecondition prompts. While incorporating demonstrations can greatly enhance\nthe performance of LLMs across various tasks, it may introduce a new security\nconcern: attackers can manipulate only the demonstrations without changing the\ninput to perform an attack. In this paper, we investigate the security concern\nof ICL from an adversarial perspective, focusing on the impact of\ndemonstrations. We propose a novel attack method named advICL, which aims to\nmanipulate only the demonstration without changing the input to mislead the\nmodels. Our results demonstrate that as the number of demonstrations increases,\nthe robustness of in-context learning would decrease. Additionally, we also\nidentify the intrinsic property of the demonstrations is that they can be used\n(prepended) with different inputs. As a result, it introduces a more practical\nthreat model in which an attacker can attack the test input example even\nwithout knowing and manipulating it. To achieve it, we propose the transferable\nversion of advICL, named Transferable-advICL. Our experiment shows that the\nadversarial demonstration generated by Transferable-advICL can successfully\nattack the unseen test input examples. We hope that our study reveals the\ncritical security risks associated with ICL and underscores the need for\nextensive research on the robustness of ICL, particularly given its increasing\nsignificance in the advancement of LLMs.", "published": "2023-05-24 09:40:56", "link": "http://arxiv.org/abs/2305.14950v2", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Factuality of Abstractive Summarization without Sacrificing\n  Summary Quality", "abstract": "Improving factual consistency of abstractive summarization has been a widely\nstudied topic. However, most of the prior works on training factuality-aware\nmodels have ignored the negative effect it has on summary quality. We propose\nEFACTSUM (i.e., Effective Factual Summarization), a candidate summary\ngeneration and ranking technique to improve summary factuality without\nsacrificing summary quality. We show that using a contrastive learning\nframework with our refined candidate summaries leads to significant gains on\nboth factuality and similarity-based metrics. Specifically, we propose a\nranking strategy in which we effectively combine two metrics, thereby\npreventing any conflict during training. Models trained using our approach show\nup to 6 points of absolute improvement over the base model with respect to\nFactCC on XSUM and 11 points on CNN/DM, without negatively affecting either\nsimilarity-based metrics or absractiveness.", "published": "2023-05-24 10:15:17", "link": "http://arxiv.org/abs/2305.14981v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reasoning with Language Model is Planning with World Model", "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities,\nespecially when prompted to generate intermediate reasoning steps (e.g.,\nChain-of-Thought, CoT). However, LLMs can still struggle with problems that are\neasy for humans, such as generating action plans for executing tasks in a given\nenvironment, or performing complex math, logical, and commonsense reasoning.\nThe deficiency stems from the key fact that LLMs lack an internal\n$\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment\nstatus, intermediate variable values) and simulate long-term outcomes of\nactions. This prevents LLMs from performing deliberate planning akin to human\nbrains, which involves exploring alternative reasoning paths, anticipating\nfuture states and rewards, and iteratively refining existing reasoning steps.\nTo overcome the limitations, we propose a new LLM reasoning framework,\n$\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning\n$\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning\nagent, and incorporates a principled planning algorithm (based on Monto Carlo\nTree Search) for strategic exploration in the vast reasoning space. During\nreasoning, the LLM (as agent) incrementally builds a reasoning tree under the\nguidance of the LLM (as world model) and task-specific rewards, and obtains a\nhigh-reward reasoning path efficiently with a proper balance between\nexploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of\nchallenging reasoning problems including plan generation, math reasoning, and\nlogical inference. Empirical results on these tasks demonstrate the superiority\nof RAP over various strong baselines, including CoT and least-to-most prompting\nwith self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33%\nrelative improvement in a plan generation setting.", "published": "2023-05-24 10:28:28", "link": "http://arxiv.org/abs/2305.14992v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Examination of the Robustness of Reference-Free Image Captioning\n  Evaluation Metrics", "abstract": "Recently, reference-free metrics such as CLIPScore (Hessel et al., 2021),\nUMIC (Lee et al., 2021), and PAC-S (Sarto et al., 2023) have been proposed for\nautomatic reference-free evaluation of image captions. Our focus lies in\nevaluating the robustness of these metrics in scenarios that require\ndistinguishing between two captions with high lexical overlap but very\ndifferent meanings. Our findings reveal that despite their high correlation\nwith human judgments, CLIPScore, UMIC, and PAC-S struggle to identify\nfine-grained errors. While all metrics exhibit strong sensitivity to visual\ngrounding errors, their sensitivity to caption implausibility errors is\nlimited. Furthermore, we found that all metrics are sensitive to variations in\nthe size of image-relevant objects mentioned in the caption, while CLIPScore\nand PAC-S are also sensitive to the number of mentions of image-relevant\nobjects in the caption. Regarding linguistic aspects of a caption, all metrics\nshow weak comprehension of negation, and CLIPScore and PAC-S are insensitive to\nthe structure of the caption to a great extent. We hope our findings will guide\nfurther improvements in reference-free evaluation of image captioning.", "published": "2023-05-24 10:36:12", "link": "http://arxiv.org/abs/2305.14998v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Are Chatbots Ready for Privacy-Sensitive Applications? An Investigation\n  into Input Regurgitation and Prompt-Induced Sanitization", "abstract": "LLM-powered chatbots are becoming widely adopted in applications such as\nhealthcare, personal assistants, industry hiring decisions, etc. In many of\nthese cases, chatbots are fed sensitive, personal information in their prompts,\nas samples for in-context learning, retrieved records from a database, or as\npart of the conversation. The information provided in the prompt could directly\nappear in the output, which might have privacy ramifications if there is\nsensitive information there. As such, in this paper, we aim to understand the\ninput copying and regurgitation capabilities of these models during inference\nand how they can be directly instructed to limit this copying by complying with\nregulations such as HIPAA and GDPR, based on their internal knowledge of them.\nMore specifically, we find that when ChatGPT is prompted to summarize cover\nletters of a 100 candidates, it would retain personally identifiable\ninformation (PII) verbatim in 57.4% of cases, and we find this retention to be\nnon-uniform between different subgroups of people, based on attributes such as\ngender identity. We then probe ChatGPT's perception of privacy-related policies\nand privatization mechanisms by directly instructing it to provide compliant\noutputs and observe a significant omission of PII from output.", "published": "2023-05-24 10:48:05", "link": "http://arxiv.org/abs/2305.15008v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through\n  Interaction with Symbolic Systems", "abstract": "Despite outstanding performance in many tasks, language models are\nnotoriously inclined to make factual errors in tasks requiring arithmetic\ncomputation. We address this deficiency by creating Calc-X, a collection of\ndatasets that demonstrates the appropriate use of a calculator in reasoning\nchains. Calc-X is suitable for teaching language models to offload computations\nto a symbolic system. We survey and unify several existing chain-of-thought\ndatasets into a proposed format, resulting in a standard collection of over\n300,000 samples requiring arithmetic reasoning. Finally, we use the new Calc-X\ncollection to train open-source calculator-using models we call Calcformers and\nshow that these models approximately double the accuracy of generating correct\nresults compared to vanilla language model baselines. We make all Calc-X\ndatasets, source code and Calcformers models publicly available.", "published": "2023-05-24 10:58:20", "link": "http://arxiv.org/abs/2305.15017v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ImageNetVC: Zero- and Few-Shot Visual Commonsense Evaluation on 1000\n  ImageNet Categories", "abstract": "Recently, Large Language Models (LLMs) have been serving as general-purpose\ninterfaces, posing a significant demand for comprehensive visual knowledge.\nHowever, it remains unclear how well current LLMs and their visually augmented\ncounterparts (VaLMs) can master visual commonsense knowledge. To investigate\nthis, we propose ImageNetVC, a human-annotated dataset specifically designed\nfor zero- and few-shot visual commonsense evaluation across 1,000 ImageNet\ncategories. Utilizing ImageNetVC, we benchmark the fundamental visual\ncommonsense knowledge of both unimodal LLMs and VaLMs. Furthermore, we analyze\nthe factors affecting the visual commonsense knowledge of large-scale models,\nproviding insights into the development of language models enriched with visual\ncommonsense knowledge. Our code and dataset are available at\nhttps://github.com/hemingkx/ImageNetVC.", "published": "2023-05-24 11:14:31", "link": "http://arxiv.org/abs/2305.15028v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "How to Distill your BERT: An Empirical Study on the Impact of Weight\n  Initialisation and Distillation Objectives", "abstract": "Recently, various intermediate layer distillation (ILD) objectives have been\nshown to improve compression of BERT models via Knowledge Distillation (KD).\nHowever, a comprehensive evaluation of the objectives in both task-specific and\ntask-agnostic settings is lacking. To the best of our knowledge, this is the\nfirst work comprehensively evaluating distillation objectives in both settings.\nWe show that attention transfer gives the best performance overall. We also\nstudy the impact of layer choice when initializing the student from the teacher\nlayers, finding a significant impact on the performance in task-specific\ndistillation. For vanilla KD and hidden states transfer, initialisation with\nlower layers of the teacher gives a considerable improvement over higher\nlayers, especially on the task of QNLI (up to an absolute percentage change of\n17.8 in accuracy). Attention transfer behaves consistently under different\ninitialisation settings. We release our code as an efficient transformer-based\nmodel distillation framework for further studies.", "published": "2023-05-24 11:16:09", "link": "http://arxiv.org/abs/2305.15032v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "C-STS: Conditional Semantic Textual Similarity", "abstract": "Semantic textual similarity (STS), a cornerstone task in NLP, measures the\ndegree of similarity between a pair of sentences, and has broad application in\nfields such as information retrieval and natural language understanding.\nHowever, sentence similarity can be inherently ambiguous, depending on the\nspecific aspect of interest. We resolve this ambiguity by proposing a novel\ntask called Conditional STS (C-STS) which measures sentences' similarity\nconditioned on an feature described in natural language (hereon, condition). As\nan example, the similarity between the sentences \"The NBA player shoots a\nthree-pointer.\" and \"A man throws a tennis ball into the air to serve.\" is\nhigher for the condition \"The motion of the ball\" (both upward) and lower for\n\"The size of the ball\" (one large and one small). C-STS's advantages are\ntwo-fold: (1) it reduces the subjectivity and ambiguity of STS and (2) enables\nfine-grained language model evaluation through diverse natural language\nconditions. We put several state-of-the-art models to the test, and even those\nperforming well on STS (e.g. SimCSE, Flan-T5, and GPT-4) find C-STS\nchallenging; all with Spearman correlation scores below 50. To encourage a more\ncomprehensive evaluation of semantic similarity and natural language\nunderstanding, we make nearly 19K C-STS examples and code available for others\nto train and test their models.", "published": "2023-05-24 12:18:50", "link": "http://arxiv.org/abs/2305.15093v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Topic-Guided Self-Introduction Generation for Social Media Users", "abstract": "Millions of users are active on social media. To allow users to better\nshowcase themselves and network with others, we explore the auto-generation of\nsocial media self-introduction, a short sentence outlining a user's personal\ninterests. While most prior work profiles users with tags (e.g., ages), we\ninvestigate sentence-level self-introductions to provide a more natural and\nengaging way for users to know each other. Here we exploit a user's tweeting\nhistory to generate their self-introduction. The task is non-trivial because\nthe history content may be lengthy, noisy, and exhibit various personal\ninterests. To address this challenge, we propose a novel unified topic-guided\nencoder-decoder (UTGED) framework; it models latent topics to reflect salient\nuser interest, whose topic mixture then guides encoding a user's history and\ntopic words control decoding their self-introduction. For experiments, we\ncollect a large-scale Twitter dataset, and extensive results show the\nsuperiority of our UTGED to the advanced encoder-decoder models without topic\nmodeling.", "published": "2023-05-24 13:35:08", "link": "http://arxiv.org/abs/2305.15138v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Summarization of Electronic Health Records", "abstract": "Hospital discharge documentation is among the most essential, yet\ntime-consuming documents written by medical practitioners. The objective of\nthis study was to automatically generate hospital discharge summaries using\nneural network summarization models. We studied various data preparation and\nneural network training techniques that generate discharge summaries. Using\nnursing notes and discharge summaries from the MIMIC-III dataset, we studied\nthe viability of the automatic generation of various sections of a discharge\nsummary using four state-of-the-art neural network summarization models (BART,\nT5, Longformer and FLAN-T5). Our experiments indicated that training\nenvironments including nursing notes as the source, and discrete sections of\nthe discharge summary as the target output (e.g. \"History of Present Illness\")\nimprove language model efficiency and text quality. According to our findings,\nthe fine-tuned BART model improved its ROUGE F1 score by 43.6% against its\nstandard off-the-shelf version. We also found that fine-tuning the baseline\nBART model with other setups caused different degrees of improvement (up to 80%\nrelative improvement). We also observed that a fine-tuned T5 generally achieves\nhigher ROUGE F1 scores than other fine-tuned models and a fine-tuned FLAN-T5\nachieves the highest ROUGE score overall, i.e., 45.6. For majority of the\nfine-tuned language models, summarizing discharge summary report sections\nseparately outperformed the summarization the entire report quantitatively. On\nthe other hand, fine-tuning language models that were previously instruction\nfine-tuned showed better performance in summarizing entire reports. This study\nconcludes that a focused dataset designed for the automatic generation of\ndischarge summaries by a language model can produce coherent Discharge Summary\nsections.", "published": "2023-05-24 15:05:53", "link": "http://arxiv.org/abs/2305.15222v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Spoken Question Answering and Speech Continuation Using\n  Spectrogram-Powered LLM", "abstract": "We present Spectron, a novel approach to adapting pre-trained large language\nmodels (LLMs) to perform spoken question answering (QA) and speech\ncontinuation. By endowing the LLM with a pre-trained speech encoder, our model\nbecomes able to take speech inputs and generate speech outputs. The entire\nsystem is trained end-to-end and operates directly on spectrograms, simplifying\nour architecture. Key to our approach is a training objective that jointly\nsupervises speech recognition, text continuation, and speech synthesis using\nonly paired speech-text pairs, enabling a `cross-modal' chain-of-thought within\na single decoding pass. Our method surpasses existing spoken language models in\nspeaker preservation and semantic coherence. Furthermore, the proposed model\nimproves upon direct initialization in retaining the knowledge of the original\nLLM as demonstrated through spoken QA datasets. We release our audio samples\n(https://michelleramanovich.github.io/spectron/spectron) and spoken QA dataset\n(https://github.com/google-research-datasets/LLAMA1-Test-Set).", "published": "2023-05-24 15:39:43", "link": "http://arxiv.org/abs/2305.15255v4", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Science in the Era of ChatGPT, Large Language Models and Generative AI:\n  Challenges for Research Ethics and How to Respond", "abstract": "Large language models of artificial intelligence (AI), such as ChatGPT, find\nremarkable but controversial applicability in science and research. This paper\nreviews epistemological challenges, ethical and integrity risks in science\nconduct in the advent of generative AI. This is with the aim to lay new timely\nfoundations for a high-quality research ethics review. The role of AI language\nmodels as a research instrument and subject is scrutinized along with ethical\nimplications for scientists, participants and reviewers. New emerging practices\nfor research ethics review are discussed, concluding with ten recommendations\nthat shape a response for a more responsible research conduct in the era of AI.", "published": "2023-05-24 16:23:46", "link": "http://arxiv.org/abs/2305.15299v4", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Visual Programming for Text-to-Image Generation and Evaluation", "abstract": "As large language models have demonstrated impressive performance in many\ndomains, recent works have adopted language models (LMs) as controllers of\nvisual modules for vision-and-language tasks. While existing work focuses on\nequipping LMs with visual understanding, we propose two novel\ninterpretable/explainable visual programming frameworks for text-to-image (T2I)\ngeneration and evaluation. First, we introduce VPGen, an interpretable\nstep-by-step T2I generation framework that decomposes T2I generation into three\nsteps: object/count generation, layout generation, and image generation. We\nemploy an LM to handle the first two steps (object/count generation and layout\ngeneration), by finetuning it on text-layout pairs. Our step-by-step T2I\ngeneration framework provides stronger spatial control than end-to-end models,\nthe dominant approach for this task. Furthermore, we leverage the world\nknowledge of pretrained LMs, overcoming the limitation of previous\nlayout-guided T2I works that can only handle predefined object classes. We\ndemonstrate that our VPGen has improved control in counts/spatial\nrelations/scales of objects than state-of-the-art T2I generation models.\nSecond, we introduce VPEval, an interpretable and explainable evaluation\nframework for T2I generation based on visual programming. Unlike previous T2I\nevaluations with a single scoring model that is accurate in some skills but\nunreliable in others, VPEval produces evaluation programs that invoke a set of\nvisual modules that are experts in different skills, and also provides\nvisual+textual explanations of the evaluation results. Our analysis shows that\nVPEval provides a more human-correlated evaluation for skill-specific and\nopen-ended prompts than widely used single model-based evaluation. We hope that\nour work encourages future progress on interpretable/explainable generation and\nevaluation for T2I models.", "published": "2023-05-24 16:42:17", "link": "http://arxiv.org/abs/2305.15328v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "ASPER: Answer Set Programming Enhanced Neural Network Models for Joint\n  Entity-Relation Extraction", "abstract": "A plethora of approaches have been proposed for joint entity-relation (ER)\nextraction. Most of these methods largely depend on a large amount of manually\nannotated training data. However, manual data annotation is time consuming,\nlabor intensive, and error prone. Human beings learn using both data (through\ninduction) and knowledge (through deduction). Answer Set Programming (ASP) has\nbeen a widely utilized approach for knowledge representation and reasoning that\nis elaboration tolerant and adept at reasoning with incomplete information.\nThis paper proposes a new approach, ASP-enhanced Entity-Relation extraction\n(ASPER), to jointly recognize entities and relations by learning from both data\nand domain knowledge. In particular, ASPER takes advantage of the factual\nknowledge (represented as facts in ASP) and derived knowledge (represented as\nrules in ASP) in the learning process of neural network models. We have\nconducted experiments on two real datasets and compare our method with three\nbaselines. The results show that our ASPER model consistently outperforms the\nbaselines.", "published": "2023-05-24 17:32:58", "link": "http://arxiv.org/abs/2305.15374v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Vistaar: Diverse Benchmarks and Training Sets for Indian Language ASR", "abstract": "Improving ASR systems is necessary to make new LLM-based use-cases accessible\nto people across the globe. In this paper, we focus on Indian languages, and\nmake the case that diverse benchmarks are required to evaluate and improve ASR\nsystems for Indian languages. To address this, we collate Vistaar as a set of\n59 benchmarks across various language and domain combinations, on which we\nevaluate 3 publicly available ASR systems and 2 commercial systems. We also\ntrain IndicWhisper models by fine-tuning the Whisper models on publicly\navailable training datasets across 12 Indian languages totalling to 10.7K\nhours. We show that IndicWhisper significantly improves on considered ASR\nsystems on the Vistaar benchmark. Indeed, IndicWhisper has the lowest WER in 39\nout of the 59 benchmarks, with an average reduction of 4.1 WER. We open-source\nall datasets, code and models.", "published": "2023-05-24 17:46:03", "link": "http://arxiv.org/abs/2305.15386v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "AV-TranSpeech: Audio-Visual Robust Speech-to-Speech Translation", "abstract": "Direct speech-to-speech translation (S2ST) aims to convert speech from one\nlanguage into another, and has demonstrated significant progress to date.\nDespite the recent success, current S2ST models still suffer from distinct\ndegradation in noisy environments and fail to translate visual speech (i.e.,\nthe movement of lips and teeth). In this work, we present AV-TranSpeech, the\nfirst audio-visual speech-to-speech (AV-S2ST) translation model without relying\non intermediate text. AV-TranSpeech complements the audio stream with visual\ninformation to promote system robustness and opens up a host of practical\napplications: dictation or dubbing archival films. To mitigate the data\nscarcity with limited parallel AV-S2ST data, we 1) explore self-supervised\npre-training with unlabeled audio-visual data to learn contextual\nrepresentation, and 2) introduce cross-modal distillation with S2ST models\ntrained on the audio-only corpus to further reduce the requirements of visual\ndata. Experimental results on two language pairs demonstrate that AV-TranSpeech\noutperforms audio-only models under all settings regardless of the type of\nnoise. With low-resource audio-visual data (10h, 30h), cross-modal distillation\nyields an improvement of 7.6 BLEU on average compared with baselines. Audio\nsamples are available at https://AV-TranSpeech.github.io", "published": "2023-05-24 17:59:03", "link": "http://arxiv.org/abs/2305.15403v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards Revealing the Mystery behind Chain of Thought: A Theoretical\n  Perspective", "abstract": "Recent studies have discovered that Chain-of-Thought prompting (CoT) can\ndramatically improve the performance of Large Language Models (LLMs),\nparticularly when dealing with complex tasks involving mathematics or\nreasoning. Despite the enormous empirical success, the underlying mechanisms\nbehind CoT and how it unlocks the potential of LLMs remain elusive. In this\npaper, we take a first step towards theoretically answering these questions.\nSpecifically, we examine the expressivity of LLMs with CoT in solving\nfundamental mathematical and decision-making problems. By using circuit\ncomplexity theory, we first give impossibility results showing that\nbounded-depth Transformers are unable to directly produce correct answers for\nbasic arithmetic/equation tasks unless the model size grows super-polynomially\nwith respect to the input length. In contrast, we then prove by construction\nthat autoregressive Transformers of constant size suffice to solve both tasks\nby generating CoT derivations using a commonly used math language format.\nMoreover, we show LLMs with CoT can handle a general class of decision-making\nproblems known as Dynamic Programming, thus justifying its power in tackling\ncomplex real-world tasks. Finally, an extensive set of experiments show that,\nwhile Transformers always fail to directly predict the answers, they can\nconsistently learn to generate correct solutions step-by-step given sufficient\nCoT demonstrations.", "published": "2023-05-24 17:59:21", "link": "http://arxiv.org/abs/2305.15408v5", "categories": ["cs.LG", "cs.CC", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "PromptNER: Prompting For Named Entity Recognition", "abstract": "In a surprising turn, Large Language Models (LLMs) together with a growing\narsenal of prompt-based heuristics now offer powerful off-the-shelf approaches\nproviding few-shot solutions to myriad classic NLP problems. However, despite\npromising early results, these LLM-based few-shot methods remain far from the\nstate of the art in Named Entity Recognition (NER), where prevailing methods\ninclude learning representations via end-to-end structural understanding and\nfine-tuning on standard labeled corpora. In this paper, we introduce PromptNER,\na new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt to\nany new NER task PromptNER requires a set of entity definitions in addition to\nthe standard few-shot examples. Given a sentence, PromptNER prompts an LLM to\nproduce a list of potential entities along with corresponding explanations\njustifying their compatibility with the provided entity type definitions.\nRemarkably, PromptNER achieves state-of-the-art performance on few-shot NER,\nachieving a 4% (absolute) improvement in F1 score on the ConLL dataset, a 9%\n(absolute) improvement on the GENIA dataset, and a 4% (absolute) improvement on\nthe FewNERD dataset. PromptNER also moves the state of the art on Cross Domain\nNER, outperforming prior methods (including those not limited to the few-shot\nsetting), setting a new mark on 3/5 CrossNER target domains, with an average F1\ngain of 3%, despite using less than 2% of the available data.", "published": "2023-05-24 07:38:24", "link": "http://arxiv.org/abs/2305.15444v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models for User Interest Journeys", "abstract": "Large language models (LLMs) have shown impressive capabilities in natural\nlanguage understanding and generation. Their potential for deeper user\nunderstanding and improved personalized user experience on recommendation\nplatforms is, however, largely untapped. This paper aims to address this gap.\nRecommender systems today capture users' interests through encoding their\nhistorical activities on the platforms. The generated user representations are\nhard to examine or interpret. On the other hand, if we were to ask people about\ninterests they pursue in their life, they might talk about their hobbies, like\nI just started learning the ukulele, or their relaxation routines, e.g., I like\nto watch Saturday Night Live, or I want to plant a vertical garden. We argue,\nand demonstrate through extensive experiments, that LLMs as foundation models\ncan reason through user activities, and describe their interests in nuanced and\ninteresting ways, similar to how a human would.\n  We define interest journeys as the persistent and overarching user interests,\nin other words, the non-transient ones. These are the interests that we believe\nwill benefit most from the nuanced and personalized descriptions. We introduce\na framework in which we first perform personalized extraction of interest\njourneys, and then summarize the extracted journeys via LLMs, using techniques\nlike few-shot prompting, prompt-tuning and fine-tuning. Together, our results\nin prompting LLMs to name extracted user journeys in a large-scale industrial\nplatform demonstrate great potential of these models in providing deeper, more\ninterpretable, and controllable user understanding. We believe LLM powered user\nunderstanding can be a stepping stone to entirely new user experiences on\nrecommendation platforms that are journey-aware, assistive, and enabling\nfrictionless conversation down the line.", "published": "2023-05-24 18:40:43", "link": "http://arxiv.org/abs/2305.15498v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "TOAST: Transfer Learning via Attention Steering", "abstract": "Transfer learning involves adapting a pre-trained model to novel downstream\ntasks. However, we observe that current transfer learning methods often fail to\nfocus on task-relevant features. In this work, we explore refocusing model\nattention for transfer learning. We introduce Top-Down Attention Steering\n(TOAST), a novel transfer learning algorithm that keeps the pre-trained\nbackbone frozen, selects task-relevant features in the output, and feeds those\nfeatures back to the model to steer the attention to the task-specific\nfeatures. By refocusing the attention only, TOAST achieves state-of-the-art\nresults on a number of transfer learning benchmarks, while having a small\nnumber of tunable parameters. Compared to fully fine-tuning, LoRA, and prompt\ntuning, TOAST substantially improves performance across a range of fine-grained\nvisual classification datasets (e.g., 81.1% -> 86.2% on FGVC). TOAST also\noutperforms the fully fine-tuned Alpaca and Vicuna models on\ninstruction-following language generation. Code is available at\nhttps://github.com/bfshi/TOAST.", "published": "2023-05-24 20:03:04", "link": "http://arxiv.org/abs/2305.15542v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Flocks of Stochastic Parrots: Differentially Private Prompt Learning for\n  Large Language Models", "abstract": "Large language models (LLMs) are excellent in-context learners. However, the\nsensitivity of data contained in prompts raises privacy concerns. Our work\nfirst shows that these concerns are valid: we instantiate a simple but highly\neffective membership inference attack against the data used to prompt LLMs. To\naddress this vulnerability, one could forego prompting and resort to\nfine-tuning LLMs with known algorithms for private gradient descent. However,\nthis comes at the expense of the practicality and efficiency offered by\nprompting. Therefore, we propose to privately learn to prompt. We first show\nthat soft prompts can be obtained privately through gradient descent on\ndownstream data. However, this is not the case for discrete prompts. Thus, we\norchestrate a noisy vote among an ensemble of LLMs presented with different\nprompts, i.e., a flock of stochastic parrots. The vote privately transfers the\nflock's knowledge into a single public prompt. We show that LLMs prompted with\nour private algorithms closely match the non-private baselines. For example,\nusing GPT3 as the base model, we achieve a downstream accuracy of 92.7% on the\nsst2 dataset with ($\\epsilon=0.147, \\delta=10^{-6}$)-differential privacy vs.\n95.2% for the non-private baseline. Through our experiments, we also show that\nour prompt-based approach is easily deployed with existing commercial APIs.", "published": "2023-05-24 22:06:08", "link": "http://arxiv.org/abs/2305.15594v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Text-Augmented Open Knowledge Graph Completion via Pre-Trained Language\n  Models", "abstract": "The mission of open knowledge graph (KG) completion is to draw new findings\nfrom known facts. Existing works that augment KG completion require either (1)\nfactual triples to enlarge the graph reasoning space or (2) manually designed\nprompts to extract knowledge from a pre-trained language model (PLM),\nexhibiting limited performance and requiring expensive efforts from experts. To\nthis end, we propose TAGREAL that automatically generates quality query prompts\nand retrieves support information from large text corpora to probe knowledge\nfrom PLM for KG completion. The results show that TAGREAL achieves\nstate-of-the-art performance on two benchmark datasets. We find that TAGREAL\nhas superb performance even with limited training data, outperforming existing\nembedding-based, graph-based, and PLM-based methods.", "published": "2023-05-24 22:09:35", "link": "http://arxiv.org/abs/2305.15597v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Think Before You Act: Decision Transformers with Working Memory", "abstract": "Decision Transformer-based decision-making agents have shown the ability to\ngeneralize across multiple tasks. However, their performance relies on massive\ndata and computation. We argue that this inefficiency stems from the forgetting\nphenomenon, in which a model memorizes its behaviors in parameters throughout\ntraining. As a result, training on a new task may deteriorate the model's\nperformance on previous tasks. In contrast to LLMs' implicit memory mechanism,\nthe human brain utilizes distributed memory storage, which helps manage and\norganize multiple skills efficiently, mitigating the forgetting phenomenon.\nInspired by this, we propose a working memory module to store, blend, and\nretrieve information for different downstream tasks. Evaluation results show\nthat the proposed method improves training efficiency and generalization in\nAtari games and Meta-World object manipulation tasks. Moreover, we demonstrate\nthat memory fine-tuning further enhances the adaptability of the proposed\narchitecture.", "published": "2023-05-24 01:20:22", "link": "http://arxiv.org/abs/2305.16338v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model", "abstract": "Transformers have shown dominant performance across a range of domains\nincluding language and vision. However, their computational cost grows\nquadratically with the sequence length, making their usage prohibitive for\nresource-constrained applications. To counter this, our approach is to divide\nthe whole sequence into segments and apply attention to the individual\nsegments. We propose a segmented recurrent transformer (SRformer) that combines\nsegmented (local) attention with recurrent attention. The loss caused by\nreducing the attention window length is compensated by aggregating information\nacross segments with recurrent attention. SRformer leverages Recurrent\nAccumulate-and-Fire (RAF) neurons' inherent memory to update the cumulative\nproduct of keys and values. The segmented attention and lightweight RAF neurons\nensure the efficiency of the proposed transformer. Such an approach leads to\nmodels with sequential processing capability at a lower computation/memory\ncost. We apply the proposed method to T5 and BART transformers. The modified\nmodels are tested on summarization datasets including CNN-dailymail, XSUM,\nArXiv, and MediaSUM. Notably, using segmented inputs of varied sizes, the\nproposed model achieves $6-22\\%$ higher ROUGE1 scores than a segmented\ntransformer and outperforms other recurrent transformer approaches.\nFurthermore, compared to full attention, the proposed model reduces the\ncomputational complexity of cross attention by around $40\\%$.", "published": "2023-05-24 03:47:22", "link": "http://arxiv.org/abs/2305.16340v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "InterFormer: Interactive Local and Global Features Fusion for Automatic\n  Speech Recognition", "abstract": "The local and global features are both essential for automatic speech\nrecognition (ASR). Many recent methods have verified that simply combining\nlocal and global features can further promote ASR performance. However, these\nmethods pay less attention to the interaction of local and global features, and\ntheir series architectures are rigid to reflect local and global relationships.\nTo address these issues, this paper proposes InterFormer for interactive local\nand global features fusion to learn a better representation for ASR.\nSpecifically, we combine the convolution block with the transformer block in a\nparallel design. Besides, we propose a bidirectional feature interaction module\n(BFIM) and a selective fusion module (SFM) to implement the interaction and\nfusion of local and global features, respectively. Extensive experiments on\npublic ASR datasets demonstrate the effectiveness of our proposed InterFormer\nand its superior performance over the other Transformer and Conformer models.", "published": "2023-05-24 08:43:44", "link": "http://arxiv.org/abs/2305.16342v2", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Lexinvariant Language Models", "abstract": "Token embeddings, a mapping from discrete lexical symbols to continuous\nvectors, are at the heart of any language model (LM). However, lexical symbol\nmeanings can also be determined and even redefined by their structural role in\na long context. In this paper, we ask: is it possible for a language model to\nbe performant without \\emph{any} fixed token embeddings? Such a language model\nwould have to rely entirely on the co-occurence and repetition of tokens in the\ncontext rather than the \\textit{a priori} identity of any token. To answer\nthis, we study \\textit{lexinvariant}language models that are invariant to\nlexical symbols and therefore do not need fixed token embeddings in practice.\nFirst, we prove that we can construct a lexinvariant LM to converge to the true\nlanguage model at a uniform rate that is polynomial in terms of the context\nlength, with a constant factor that is sublinear in the vocabulary size.\nSecond, to build a lexinvariant LM, we simply encode tokens using random\nGaussian vectors, such that each token maps to the same representation within\neach sequence but different representations across sequences. Empirically, we\ndemonstrate that it can indeed attain perplexity comparable to that of a\nstandard language model, given a sufficiently long context. We further explore\ntwo properties of the lexinvariant language models: First, given text generated\nfrom a substitution cipher of English, it implicitly implements Bayesian\nin-context deciphering and infers the mapping to the underlying real tokens\nwith high accuracy. Second, it has on average 4X better accuracy over synthetic\nin-context reasoning tasks. Finally, we discuss regularizing standard language\nmodels towards lexinvariance and potential practical applications.", "published": "2023-05-24 19:10:46", "link": "http://arxiv.org/abs/2305.16349v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "#REVAL: a semantic evaluation framework for hashtag recommendation", "abstract": "Automatic evaluation of hashtag recommendation models is a fundamental task\nin many online social network systems. In the traditional evaluation method,\nthe recommended hashtags from an algorithm are firstly compared with the ground\ntruth hashtags for exact correspondences. The number of exact matches is then\nused to calculate the hit rate, hit ratio, precision, recall, or F1-score. This\nway of evaluating hashtag similarities is inadequate as it ignores the semantic\ncorrelation between the recommended and ground truth hashtags. To tackle this\nproblem, we propose a novel semantic evaluation framework for hashtag\nrecommendation, called #REval. This framework includes an internal module\nreferred to as BERTag, which automatically learns the hashtag embeddings. We\ninvestigate on how the #REval framework performs under different word embedding\nmethods and different numbers of synonyms and hashtags in the recommendation\nusing our proposed #REval-hit-ratio measure. Our experiments of the proposed\nframework on three large datasets show that #REval gave more meaningful hashtag\nsynonyms for hashtag recommendation evaluation. Our analysis also highlights\nthe sensitivity of the framework to the word embedding technique, with #REval\nbased on BERTag more superior over #REval based on FastText and Word2Vec.", "published": "2023-05-24 07:10:56", "link": "http://arxiv.org/abs/2305.18330v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "I.2.7"], "primary_category": "cs.IR"}
{"title": "Detecting Check-Worthy Claims in Political Debates, Speeches, and\n  Interviews Using Audio Data", "abstract": "Developing tools to automatically detect check-worthy claims in political\ndebates and speeches can greatly help moderators of debates, journalists, and\nfact-checkers. While previous work on this problem has focused exclusively on\nthe text modality, here we explore the utility of the audio modality as an\nadditional input. We create a new multimodal dataset (text and audio in\nEnglish) containing 48 hours of speech from past political debates in the USA.\nWe then experimentally demonstrate that, in the case of multiple speakers,\nadding the audio modality yields sizable improvements over using the text\nmodality alone; moreover, an audio-only model could outperform a text-only one\nfor a single speaker. With the aim to enable future research, we make all our\ndata and code publicly available at\nhttps://github.com/petar-iv/audio-checkworthiness-detection.", "published": "2023-05-24 12:09:42", "link": "http://arxiv.org/abs/2306.05535v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.SD", "eess.AS", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Generalized domain adaptation framework for parametric back-end in\n  speaker recognition", "abstract": "State-of-the-art speaker recognition systems comprise a speaker embedding\nfront-end followed by a probabilistic linear discriminant analysis (PLDA)\nback-end. The effectiveness of these components relies on the availability of a\nlarge amount of labeled training data. In practice, it is common for domains\n(e.g., language, channel, demographic) in which a system is deployed to differ\nfrom that in which a system has been trained. To close the resulting gap,\ndomain adaptation is often essential for PLDA models. Among two of its variants\nare Heavy-tailed PLDA (HT-PLDA) and Gaussian PLDA (G-PLDA). Though the former\nbetter fits real feature spaces than does the latter, its popularity has been\nseverely limited by its computational complexity and, especially, by the\ndifficulty, it presents in domain adaptation, which results from its\nnon-Gaussian property. Various domain adaptation methods have been proposed for\nG-PLDA. This paper proposes a generalized framework for domain adaptation that\ncan be applied to both of the above variants of PLDA for speaker recognition.\nIt not only includes several existing supervised and unsupervised domain\nadaptation methods but also makes possible more flexible usage of available\ndata in different domains. In particular, we introduce here two new techniques:\n(1) correlation-alignment in the model level, and (2) covariance\nregularization. To the best of our knowledge, this is the first proposed\napplication of such techniques for domain adaptation w.r.t. HT-PLDA. The\nefficacy of the proposed techniques has been experimentally validated on NIST\n2016, 2018, and 2019 Speaker Recognition Evaluation (SRE'16, SRE'18, and\nSRE'19) datasets.", "published": "2023-05-24 20:59:31", "link": "http://arxiv.org/abs/2305.15567v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Downstream Task Agnostic Speech Enhancement with Self-Supervised\n  Representation Loss", "abstract": "Self-supervised learning (SSL) is the latest breakthrough in speech\nprocessing, especially for label-scarce downstream tasks by leveraging massive\nunlabeled audio data. The noise robustness of the SSL is one of the important\nchallenges to expanding its application. We can use speech enhancement (SE) to\ntackle this issue. However, the mismatch between the SE model and SSL models\npotentially limits its effect. In this work, we propose a new SE training\ncriterion that minimizes the distance between clean and enhanced signals in the\nfeature representation of the SSL model to alleviate the mismatch. We expect\nthat the loss in the SSL domain could guide SE training to preserve or enhance\nvarious levels of characteristics of the speech signals that may be required\nfor high-level downstream tasks. Experiments show that our proposal improves\nthe performance of an SE and SSL pipeline on five downstream tasks with noisy\ninput while maintaining the SE performance.", "published": "2023-05-24 05:00:30", "link": "http://arxiv.org/abs/2305.14723v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "P-vectors: A Parallel-Coupled TDNN/Transformer Network for Speaker\n  Verification", "abstract": "Typically, the Time-Delay Neural Network (TDNN) and Transformer can serve as\na backbone for Speaker Verification (SV). Both of them have advantages and\ndisadvantages from the perspective of global and local feature modeling. How to\neffectively integrate these two style features is still an open issue. In this\npaper, we explore a Parallel-coupled TDNN/Transformer Network (p-vectors) to\nreplace the serial hybrid networks. The p-vectors allows TDNN and Transformer\nto learn the complementary information from each other through Soft Feature\nAlignment Interaction (SFAI) under the premise of preserving local and global\nfeatures. Also, p-vectors uses the Spatial Frequency-channel Attention (SFA) to\nenhance the spatial interdependence modeling for input features. Finally, the\noutputs of dual branches of p-vectors are combined by Embedding Aggregation\nLayer (EAL). Experiments show that p-vectors outperforms MACCIF-TDNN and\nMFA-Conformer with relative improvements of 11.5% and 13.9% in EER on\nVoxCeleb1-O.", "published": "2023-05-24 06:33:38", "link": "http://arxiv.org/abs/2305.14778v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Incorporating Ultrasound Tongue Images for Audio-Visual Speech\n  Enhancement through Knowledge Distillation", "abstract": "Audio-visual speech enhancement (AV-SE) aims to enhance degraded speech along\nwith extra visual information such as lip videos, and has been shown to be more\neffective than audio-only speech enhancement. This paper proposes further\nincorporating ultrasound tongue images to improve lip-based AV-SE systems'\nperformance. Knowledge distillation is employed at the training stage to\naddress the challenge of acquiring ultrasound tongue images during inference,\nenabling an audio-lip speech enhancement student model to learn from a\npre-trained audio-lip-tongue speech enhancement teacher model. Experimental\nresults demonstrate significant improvements in the quality and intelligibility\nof the speech enhanced by the proposed method compared to the traditional\naudio-lip speech enhancement baselines. Further analysis using phone error\nrates (PER) of automatic speech recognition (ASR) shows that palatal and velar\nconsonants benefit most from the introduction of ultrasound tongue images.", "published": "2023-05-24 09:15:53", "link": "http://arxiv.org/abs/2305.14933v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Music Representing Corpus Virtual: An Open Sourced Library for\n  Explorative Music Generation, Sound Design, and Instrument Creation with\n  Artificial Intelligence and Machine Learning", "abstract": "Music Representing Corpus Virtual (MRCV) is an open source software suite\ndesigned to explore the capabilities of Artificial Intelligence (AI) and\nMachine Learning (ML) in Music Generation, Sound Design, and Virtual Instrument\nCreation (MGSDIC). The software is accessible to users of varying levels of\nexperience, with an emphasis on providing an explorative approach to MGSDIC.\nThe main aim of MRCV is to facilitate creativity, allowing users to customize\ninput datasets for training the neural networks, and offering a range of\noptions for each neural network (thoroughly documented in the Github Wiki). The\nsoftware suite is designed to be accessible to musicians, audio professionals,\nsound designers, and composers, regardless of their prior experience in AI or\nML. The documentation is prepared in such a way as to abstract technical\ndetails, thereby making it easy to understand. The software is open source,\nmeaning users can contribute to its development, and the community can\ncollectively benefit from the insights and experience of other users.", "published": "2023-05-24 09:36:04", "link": "http://arxiv.org/abs/2305.14948v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "PLCMOS -- a data-driven non-intrusive metric for the evaluation of\n  packet loss concealment algorithms", "abstract": "Speech quality assessment is a problem for every researcher working on models\nthat produce or process speech. Human subjective ratings, the gold standard in\nspeech quality assessment, are expensive and time-consuming to acquire in a\nquantity that is sufficient to get reliable data, while automated objective\nmetrics show a low correlation with gold standard ratings. This paper presents\nPLCMOS, a non-intrusive data-driven tool for generating a robust, accurate\nestimate of the mean opinion score a human rater would assign an audio file\nthat has been processed by being transmitted over a degraded packet-switched\nnetwork with missing packets being healed by a packet loss concealment\nalgorithm. Our new model shows a model-wise Pearson's correlation of ~0.97 and\nrank correlation of ~0.95 with human ratings, substantially above all other\navailable intrusive and non-intrusive metrics. The model is released as an ONNX\nmodel for other researchers to use when building PLC systems.", "published": "2023-05-24 13:21:22", "link": "http://arxiv.org/abs/2305.15127v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Diffusion-Based Audio Inpainting", "abstract": "Audio inpainting aims to reconstruct missing segments in corrupted\nrecordings. Most of existing methods produce plausible reconstructions when the\ngap lengths are short, but struggle to reconstruct gaps larger than about 100\nms. This paper explores recent advancements in deep learning and, particularly,\ndiffusion models, for the task of audio inpainting. The proposed method uses an\nunconditionally trained generative model, which can be conditioned in a\nzero-shot fashion for audio inpainting, and is able to regenerate gaps of any\nsize. An improved deep neural network architecture based on the constant-Q\ntransform, which allows the model to exploit pitch-equivariant symmetries in\naudio, is also presented. The performance of the proposed algorithm is\nevaluated through objective and subjective metrics for the task of\nreconstructing short to mid-sized gaps, up to 300 ms. The results of a formal\nlistening test show that the proposed method delivers comparable performance\nagainst the compared baselines for short gaps, such as 50 ms, while retaining a\ngood audio quality and outperforming the baselines for wider gaps that are up\nto 300 ms long. The method presented in this paper can be applied to restoring\nsound recordings that suffer from severe local disturbances or dropouts, which\nmust be reconstructed.", "published": "2023-05-24 15:52:11", "link": "http://arxiv.org/abs/2305.15266v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Spoofing Attacker Also Benefits from Self-Supervised Pretrained Model", "abstract": "Large-scale pretrained models using self-supervised learning have reportedly\nimproved the performance of speech anti-spoofing. However, the attacker side\nmay also make use of such models. Also, since it is very expensive to train\nsuch models from scratch, pretrained models on the Internet are often used, but\nthe attacker and defender may possibly use the same pretrained model. This\npaper investigates whether the improvement in anti-spoofing with pretrained\nmodels holds under the condition that the models are available to attackers. As\nthe attacker, we train a model that enhances spoofed utterances so that the\nspeaker embedding extractor based on the pretrained models cannot distinguish\nbetween bona fide and spoofed utterances. Experimental results show that the\ngains the anti-spoofing models obtained by using the pretrained models almost\ndisappear if the attacker also makes use of the pretrained models.", "published": "2023-05-24 19:15:38", "link": "http://arxiv.org/abs/2305.15518v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "RAND: Robustness Aware Norm Decay For Quantized Seq2seq Models", "abstract": "With the rapid increase in the size of neural networks, model compression has\nbecome an important area of research. Quantization is an effective technique at\ndecreasing the model size, memory access, and compute load of large models.\nDespite recent advances in quantization aware training (QAT) technique, most\npapers present evaluations that are focused on computer vision tasks, which\nhave different training dynamics compared to sequence tasks. In this paper, we\nfirst benchmark the impact of popular techniques such as straight through\nestimator, pseudo-quantization noise, learnable scale parameter, clipping, etc.\non 4-bit seq2seq models across a suite of speech recognition datasets ranging\nfrom 1,000 hours to 1 million hours, as well as one machine translation dataset\nto illustrate its applicability outside of speech.\n  Through the experiments, we report that noise based QAT suffers when there is\ninsufficient regularization signal flowing back to the quantization scale. We\npropose low complexity changes to the QAT process to improve model accuracy\n(outperforming popular learnable scale and clipping methods). With the improved\naccuracy, it opens up the possibility to exploit some of the other benefits of\nnoise based QAT: 1) training a single model that performs well in mixed\nprecision mode and 2) improved generalization on long form speech recognition.", "published": "2023-05-24 19:45:56", "link": "http://arxiv.org/abs/2305.15536v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Metamathematics of Algorithmic Composition", "abstract": "This essay recounts my personal journey towards a deeper understanding of the\nmathematical foundations of algorithmic music composition. I do not spend much\ntime on specific mathematical algorithms used by composers; rather, I focus on\ngeneral issues such as fundamental limits and possibilities, by analogy with\nmetalogic, metamathematics, and computability theory. I discuss implications\nfrom these foundations for the future of algorithmic composition.", "published": "2023-05-24 22:21:13", "link": "http://arxiv.org/abs/2305.15601v1", "categories": ["cs.SD", "eess.AS", "J.5"], "primary_category": "cs.SD"}
{"title": "Interactive Neural Resonators", "abstract": "In this work, we propose a method for the controllable synthesis of real-time\ncontact sounds using neural resonators. Previous works have used physically\ninspired statistical methods and physical modelling for object materials and\nexcitation signals. Our method incorporates differentiable second-order\nresonators and estimates their coefficients using a neural network that is\nconditioned on physical parameters. This allows for interactive dynamic control\nand the generation of novel sounds in an intuitive manner. We demonstrate the\npractical implementation of our method and explore its potential creative\napplications.", "published": "2023-05-24 08:19:42", "link": "http://arxiv.org/abs/2305.14867v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Iteratively Improving Speech Recognition and Voice Conversion", "abstract": "Many existing works on voice conversion (VC) tasks use automatic speech\nrecognition (ASR) models for ensuring linguistic consistency between source and\nconverted samples. However, for the low-data resource domains, training a\nhigh-quality ASR remains to be a challenging task. In this work, we propose a\nnovel iterative way of improving both the ASR and VC models. We first train an\nASR model which is used to ensure content preservation while training a VC\nmodel. In the next iteration, the VC model is used as a data augmentation\nmethod to further fine-tune the ASR model and generalize it to diverse\nspeakers. By iteratively leveraging the improved ASR model to train VC model\nand vice-versa, we experimentally show improvement in both the models. Our\nproposed framework outperforms the ASR and one-shot VC baseline models on\nEnglish singing and Hindi speech domains in subjective and objective\nevaluations in low-data resource settings.", "published": "2023-05-24 11:45:42", "link": "http://arxiv.org/abs/2305.15055v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sound Design Strategies for Latent Audio Space Explorations Using Deep\n  Learning Architectures", "abstract": "The research in Deep Learning applications in sound and music computing have\ngathered an interest in the recent years; however, there is still a missing\nlink between these new technologies and on how they can be incorporated into\nreal-world artistic practices. In this work, we explore a well-known Deep\nLearning architecture called Variational Autoencoders (VAEs). These\narchitectures have been used in many areas for generating latent spaces where\ndata points are organized so that similar data points locate closer to each\nother. Previously, VAEs have been used for generating latent timbre spaces or\nlatent spaces of symbolic music excepts. Applying VAE to audio features of\ntimbre requires a vocoder to transform the timbre generated by the network to\nan audio signal, which is computationally expensive. In this work, we apply\nVAEs to raw audio data directly while bypassing audio feature extraction. This\napproach allows the practitioners to use any audio recording while giving\nflexibility and control over the aesthetics through dataset curation. The lower\ncomputation time in audio signal generation allows the raw audio approach to be\nincorporated into real-time applications. In this work, we propose three\nstrategies to explore latent spaces of audio and timbre for sound design\napplications. By doing so, our aim is to initiate a conversation on artistic\napproaches and strategies to utilize latent audio spaces in sound and music\npractices.", "published": "2023-05-24 21:08:42", "link": "http://arxiv.org/abs/2305.15571v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LoopBoxes -- Evaluation of a Collaborative Accessible Digital Musical\n  Instrument", "abstract": "LoopBoxes is an accessible digital musical instrument designed to create an\nintuitive access to loop based music making for children with special\neducational needs (SEN). This paper describes the evaluation of the instrument\nin the form of a pilot study during a music festival in Berlin, Germany, as\nwell as a case study with children and music teachers in a SEN school setting.\nWe created a modular system composed of three modules that afford single user\nas well as collaborative music making. The pilot study was evaluated using\ninformal observation and questionnaires (n = 39), and indicated that the\ninstrument affords music making for people with and without prior musical\nknowledge across all age groups and fosters collaborative musical processes.\nThe case study was based on observation and a qualitative interview. It\nconfirmed that the instrument meets the needs of the school settings and\nindicated how future versions could expand access to all students, especially\nthose experiencing complex disabilities. In addition, out-of-the-box\nfunctionality seems to be crucial for the long-term implementation of the\ninstrument in a school setting.", "published": "2023-05-24 08:29:14", "link": "http://arxiv.org/abs/2305.14875v1", "categories": ["cs.HC", "cs.CY", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
