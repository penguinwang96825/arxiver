{"title": "An Empirical Comparison of Simple Domain Adaptation Methods for Neural\n  Machine Translation", "abstract": "In this paper, we propose a novel domain adaptation method named \"mixed fine\ntuning\" for neural machine translation (NMT). We combine two existing\napproaches namely fine tuning and multi domain NMT. We first train an NMT model\non an out-of-domain parallel corpus, and then fine tune it on a parallel corpus\nwhich is a mix of the in-domain and out-of-domain corpora. All corpora are\naugmented with artificial tags to indicate specific domains. We empirically\ncompare our proposed method against fine tuning and multi domain methods and\ndiscuss its benefits and shortcomings.", "published": "2017-01-12 02:37:09", "link": "http://arxiv.org/abs/1701.03214v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Single-Pass, Adaptive Natural Language Filtering: Measuring Value in\n  User Generated Comments on Large-Scale, Social Media News Forums", "abstract": "There are large amounts of insight and social discovery potential in mining\ncrowd-sourced comments left on popular news forums like Reddit.com, Tumblr.com,\nFacebook.com and Hacker News. Unfortunately, due the overwhelming amount of\nparticipation with its varying quality of commentary, extracting value out of\nsuch data isn't always obvious nor timely. By designing efficient, single-pass\nand adaptive natural language filters to quickly prune spam, noise, copy-cats,\nmarketing diversions, and out-of-context posts, we can remove over a third of\nentries and return the comments with a higher probability of relatedness to the\noriginal article in question. The approach presented here uses an adaptive,\ntwo-step filtering process. It first leverages the original article posted in\nthe thread as a starting corpus to parse comments by matching intersecting\nwords and term-ratio balance per sentence then grows the corpus by adding new\nwords harvested from high-matching comments to increase filtering accuracy over\ntime.", "published": "2017-01-12 04:55:36", "link": "http://arxiv.org/abs/1701.03231v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Data-Oriented Model of Literary Language", "abstract": "We consider the task of predicting how literary a text is, with a gold\nstandard from human ratings. Aside from a standard bigram baseline, we apply\nrich syntactic tree fragments, mined from the training set, and a series of\nhand-picked features. Our model is the first to distinguish degrees of highly\nand less literary novels using a variety of lexical and syntactic features, and\nexplains 76.0 % of the variation in literary ratings.", "published": "2017-01-12 13:03:49", "link": "http://arxiv.org/abs/1701.03329v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LanideNN: Multilingual Language Identification on Character Window", "abstract": "In language identification, a common first step in natural language\nprocessing, we want to automatically determine the language of some input text.\nMonolingual language identification assumes that the given document is written\nin one language. In multilingual language identification, the document is\nusually in two or three languages and we just want their names. We aim one step\nfurther and propose a method for textual language identification where\nlanguages can change arbitrarily and the goal is to identify the spans of each\nof the languages. Our method is based on Bidirectional Recurrent Neural\nNetworks and it performs well in monolingual and multilingual language\nidentification tasks on six datasets covering 131 languages. The method keeps\nthe accuracy also for short documents and across domains, so it is ideal for\noff-the-shelf use without preparation of training data.", "published": "2017-01-12 13:41:08", "link": "http://arxiv.org/abs/1701.03338v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SMARTies: Sentiment Models for Arabic Target Entities", "abstract": "We consider entity-level sentiment analysis in Arabic, a morphologically rich\nlanguage with increasing resources. We present a system that is applied to\ncomplex posts written in response to Arabic newspaper articles. Our goal is to\nidentify important entity \"targets\" within the post along with the polarity\nexpressed about each target. We achieve significant improvements over multiple\nbaselines, demonstrating that the use of specific morphological representations\nimproves the performance of identifying both important targets and their\nsentiment, and that the use of distributional semantic clusters further boosts\nperformances for these representations, especially when richer linguistic\nresources are not available.", "published": "2017-01-12 17:51:12", "link": "http://arxiv.org/abs/1701.03434v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scalable, Trie-based Approximate Entity Extraction for Real-Time\n  Financial Transaction Screening", "abstract": "Financial institutions have to screen their transactions to ensure that they\nare not affiliated with terrorism entities. Developing appropriate solutions to\ndetect such affiliations precisely while avoiding any kind of interruption to\nlarge amount of legitimate transactions is essential. In this paper, we present\nbuilding blocks of a scalable solution that may help financial institutions to\nbuild their own software to extract terrorism entities out of both structured\nand unstructured financial messages in real time and with approximate\nsimilarity matching approach.", "published": "2017-01-12 20:14:52", "link": "http://arxiv.org/abs/1701.03492v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Prior matters: simple and general methods for evaluating and improving\n  topic quality in topic modeling", "abstract": "Latent Dirichlet Allocation (LDA) models trained without stopword removal\noften produce topics with high posterior probabilities on uninformative words,\nobscuring the underlying corpus content. Even when canonical stopwords are\nmanually removed, uninformative words common in that corpus will still dominate\nthe most probable words in a topic. In this work, we first show how the\nstandard topic quality measures of coherence and pointwise mutual information\nact counter-intuitively in the presence of common but irrelevant words, making\nit difficult to even quantitatively identify situations in which topics may be\ndominated by stopwords. We propose an additional topic quality metric that\ntargets the stopword problem, and show that it, unlike the standard measures,\ncorrectly correlates with human judgements of quality. We also propose a\nsimple-to-implement strategy for generating topics that are evaluated to be of\nmuch higher quality by both human assessment and our new metric. This approach,\na collection of informative priors easily introduced into most LDA-style\ninference methods, automatically promotes terms with domain relevance and\ndemotes domain-specific stop words. We demonstrate this approach's\neffectiveness in three very different domains: Department of Labor accident\nreports, online health forum posts, and NIPS abstracts. Overall we find that\ncurrent practices thought to solve this problem do not do so adequately, and\nthat our proposal offers a substantial improvement for those interested in\ninterpreting their topics as objects in their own right.", "published": "2017-01-12 04:26:00", "link": "http://arxiv.org/abs/1701.03227v3", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
