{"title": "DOCENT: Learning Self-Supervised Entity Representations from Large\n  Document Collections", "abstract": "This paper explores learning rich self-supervised entity representations from\nlarge amounts of the associated text. Once pre-trained, these models become\napplicable to multiple entity-centric tasks such as ranked retrieval, knowledge\nbase completion, question answering, and more. Unlike other methods that\nharvest self-supervision signals based merely on a local context within a\nsentence, we radically expand the notion of context to include any available\ntext related to an entity. This enables a new class of powerful, high-capacity\nrepresentations that can ultimately distill much of the useful information\nabout an entity from multiple text sources, without any human supervision.\n  We present several training strategies that, unlike prior approaches, learn\nto jointly predict words and entities -- strategies we compare experimentally\non downstream tasks in the TV-Movies domain, such as MovieLens tag prediction\nfrom user reviews and natural language movie search. As evidenced by results,\nour models match or outperform competitive baselines, sometimes with little or\nno fine-tuning, and can scale to very large corpora.\n  Finally, we make our datasets and pre-trained models publicly available. This\nincludes Reviews2Movielens (see https://goo.gle/research-docent ), mapping the\nup to 1B word corpus of Amazon movie reviews (He and McAuley, 2016) to\nMovieLens tags (Harper and Konstan, 2016), as well as Reddit Movie Suggestions\n(see https://urikz.github.io/docent ) with natural language queries and\ncorresponding community recommendations.", "published": "2021-02-26 01:00:12", "link": "http://arxiv.org/abs/2102.13247v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting gender and age categories in English conversations using\n  lexical, non-lexical, and turn-taking features", "abstract": "This paper examines gender and age salience and (stereo)typicality in British\nEnglish talk with the aim to predict gender and age categories based on\nlexical, phrasal and turn-taking features. We examine the SpokenBNC, a corpus\nof around 11.4 million words of British English conversations and identify\nbehavioural differences between speakers that are labelled for gender and age\ncategories. We explore differences in language use and turn-taking dynamics and\nidentify a range of characteristics that set the categories apart. We find that\nfemale speakers tend to produce more and slightly longer turns, while turns by\nmale speakers feature a higher type-token ratio and a distinct range of minimal\nparticles such as \"eh\", \"uh\" and \"em\". Across age groups, we observe, for\ninstance, that swear words and laughter characterize young speakers' talk,\nwhile old speakers tend to produce more truncated words. We then use the\nobserved characteristics to predict gender and age labels of speakers per\nconversation and per turn as a classification task, showing that non-lexical\nutterances such as minimal particles that are usually left out of dialog data\ncan contribute to setting the categories apart.", "published": "2021-02-26 08:23:08", "link": "http://arxiv.org/abs/2102.13355v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-task transfer learning for finding actionable information from\n  crisis-related messages on social media", "abstract": "The Incident streams (IS) track is a research challenge aimed at finding\nimportant information from social media during crises for emergency response\npurposes. More specifically, given a stream of crisis-related tweets, the IS\nchallenge asks a participating system to 1) classify what the types of users'\nconcerns or needs are expressed in each tweet, known as the information type\n(IT) classification task and 2) estimate how critical each tweet is with regard\nto emergency response, known as the priority level prediction task. In this\npaper, we describe our multi-task transfer learning approach for this\nchallenge. Our approach leverages state-of-the-art transformer models including\nboth encoder-based models such as BERT and a sequence-to-sequence based T5 for\njoint transfer learning on the two tasks. Based on this approach, we submitted\nseveral runs to the track. The returned evaluation results show that our runs\nsubstantially outperform other participating runs in both IT classification and\npriority level prediction.", "published": "2021-02-26 11:11:33", "link": "http://arxiv.org/abs/2102.13395v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gradient-guided Loss Masking for Neural Machine Translation", "abstract": "To mitigate the negative effect of low quality training data on the\nperformance of neural machine translation models, most existing strategies\nfocus on filtering out harmful data before training starts. In this paper, we\nexplore strategies that dynamically optimize data usage during the training\nprocess using the model's gradients on a small set of clean data. At each\ntraining step, our algorithm calculates the gradient alignment between the\ntraining data and the clean data to mask out data with negative alignment. Our\nmethod has a natural intuition: good training data should update the model\nparameters in a similar direction as the clean data. Experiments on three WMT\nlanguage pairs show that our method brings significant improvement over strong\nbaselines, and the improvements are generalizable across test data from\ndifferent domains.", "published": "2021-02-26 15:41:48", "link": "http://arxiv.org/abs/2102.13549v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluate On-the-job Learning Dialogue Systems and a Case Study for\n  Natural Language Understanding", "abstract": "On-the-job learning consists in continuously learning while being used in\nproduction, in an open environment, meaning that the system has to deal on its\nown with situations and elements never seen before. The kind of systems that\nseem to be especially adapted to on-the-job learning are dialogue systems,\nsince they can take advantage of their interactions with users to collect\nfeedback to adapt and improve their components over time. Some dialogue systems\nperforming on-the-job learning have been built and evaluated but no general\nmethodology has yet been defined. Thus in this paper, we propose a first\ngeneral methodology for evaluating on-the-job learning dialogue systems. We\nalso describe a task-oriented dialogue system which improves on-the-job its\nnatural language component through its user interactions. We finally evaluate\nour system with the described methodology.", "published": "2021-02-26 16:54:16", "link": "http://arxiv.org/abs/2102.13589v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chess as a Testbed for Language Model State Tracking", "abstract": "Transformer language models have made tremendous strides in natural language\nunderstanding tasks. However, the complexity of natural language makes it\nchallenging to ascertain how accurately these models are tracking the world\nstate underlying the text. Motivated by this issue, we consider the task of\nlanguage modeling for the game of chess. Unlike natural language, chess\nnotations describe a simple, constrained, and deterministic domain. Moreover,\nwe observe that the appropriate choice of chess notation allows for directly\nprobing the world state, without requiring any additional probing-related\nmachinery. We find that: (a) With enough training data, transformer language\nmodels can learn to track pieces and predict legal moves with high accuracy\nwhen trained solely on move sequences. (b) For small training sets providing\naccess to board state information during training can yield significant\nimprovements. (c) The success of transformer language models is dependent on\naccess to the entire game history i.e. \"full attention\". Approximating this\nfull attention results in a significant performance drop. We propose this\ntestbed as a benchmark for future work on the development and analysis of\ntransformer language models.", "published": "2021-02-26 01:16:23", "link": "http://arxiv.org/abs/2102.13249v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Methods for the Design and Evaluation of HCI+NLP Systems", "abstract": "HCI and NLP traditionally focus on different evaluation methods. While HCI\ninvolves a small number of people directly and deeply, NLP traditionally relies\non standardized benchmark evaluations that involve a larger number of people\nindirectly. We present five methodological proposals at the intersection of HCI\nand NLP and situate them in the context of ML-based NLP models. Our goal is to\nfoster interdisciplinary collaboration and progress in both fields by\nemphasizing what the fields can learn from each other.", "published": "2021-02-26 13:37:10", "link": "http://arxiv.org/abs/2102.13461v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Natural Language Video Localization: A Revisit in Span-based Question\n  Answering Framework", "abstract": "Natural Language Video Localization (NLVL) aims to locate a target moment\nfrom an untrimmed video that semantically corresponds to a text query. Existing\napproaches mainly solve the NLVL problem from the perspective of computer\nvision by formulating it as ranking, anchor, or regression tasks. These methods\nsuffer from large performance degradation when localizing on long videos. In\nthis work, we address the NLVL from a new perspective, i.e., span-based\nquestion answering (QA), by treating the input video as a text passage. We\npropose a video span localizing network (VSLNet), on top of the standard\nspan-based QA framework (named VSLBase), to address NLVL. VSLNet tackles the\ndifferences between NLVL and span-based QA through a simple yet effective\nquery-guided highlighting (QGH) strategy. QGH guides VSLNet to search for the\nmatching video span within a highlighted region. To address the performance\ndegradation on long videos, we further extend VSLNet to VSLNet-L by applying a\nmulti-scale split-and-concatenation strategy. VSLNet-L first splits the\nuntrimmed video into short clip segments; then, it predicts which clip segment\ncontains the target moment and suppresses the importance of other segments.\nFinally, the clip segments are concatenated, with different confidences, to\nlocate the target moment accurately. Extensive experiments on three benchmark\ndatasets show that the proposed VSLNet and VSLNet-L outperform the\nstate-of-the-art methods; VSLNet-L addresses the issue of performance\ndegradation on long videos. Our study suggests that the span-based QA framework\nis an effective strategy to solve the NLVL problem.", "published": "2021-02-26 15:57:59", "link": "http://arxiv.org/abs/2102.13558v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "A Meta-embedding-based Ensemble Approach for ICD Coding Prediction", "abstract": "International Classification of Diseases (ICD) are the de facto codes used\nglobally for clinical coding. These codes enable healthcare providers to claim\nreimbursement and facilitate efficient storage and retrieval of diagnostic\ninformation. The problem of automatically assigning ICD codes has been\napproached in literature as a multilabel classification, using neural models on\nunstructured data. Our proposed approach enhances the performance of neural\nmodels by effectively training word vectors using routine medical data as well\nas external knowledge from scientific articles. Furthermore, we exploit the\ngeometric properties of the two sets of word vectors and combine them into a\ncommon dimensional space, using meta-embedding techniques. We demonstrate the\nefficacy of this approach for a multimodal setting, using unstructured and\nstructured information. We empirically show that our approach improves the\ncurrent state-of-the-art deep learning architectures and benefits ensemble\nmodels.", "published": "2021-02-26 17:49:58", "link": "http://arxiv.org/abs/2102.13622v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Integration of deep learning with expectation maximization for spatial\n  cue based speech separation in reverberant conditions", "abstract": "In this paper, we formulate a blind source separation (BSS) framework, which\nallows integrating U-Net based deep learning source separation network with\nprobabilistic spatial machine learning expectation maximization (EM) algorithm\nfor separating speech in reverberant conditions. Our proposed model uses a\npre-trained deep learning convolutional neural network, U-Net, for clustering\nthe interaural level difference (ILD) cues and machine learning expectation\nmaximization (EM) algorithm for clustering the interaural phase difference\n(IPD) cues. The integrated model exploits the complementary strengths of the\ntwo approaches to BSS: the strong modeling power of supervised neural networks\nand the ease of unsupervised machine learning algorithms, whose few parameters\ncan be estimated on as little as a single segment of an audio mixture. The\nresults show an average improvement of 4.3 dB in signal to distortion ratio\n(SDR) and 4.3% in short time speech intelligibility (STOI) over the EM based\nsource separation algorithm MESSL-GS (model-based expectation-maximization\nsource separation and localization with garbage source) and 4.5 dB in SDR and\n8% in STOI over deep learning convolutional neural network (U-Net) based speech\nseparation algorithm SONET under the reverberant conditions ranging from\nanechoic to those mostly encountered in the real world.", "published": "2021-02-26 07:22:59", "link": "http://arxiv.org/abs/2102.13334v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Efficient Client Contribution Evaluation for Horizontal Federated\n  Learning", "abstract": "In federated learning (FL), fair and accurate measurement of the contribution\nof each federated participant is of great significance. The level of\ncontribution not only provides a rational metric for distributing financial\nbenefits among federated participants, but also helps to discover malicious\nparticipants that try to poison the FL framework. Previous methods for\ncontribution measurement were based on enumeration over possible combination of\nfederated participants. Their computation costs increase drastically with the\nnumber of participants or feature dimensions, making them inapplicable in\npractical situations. In this paper an efficient method is proposed to evaluate\nthe contributions of federated participants. This paper focuses on the\nhorizontal FL framework, where client servers calculate parameter gradients\nover their local data, and upload the gradients to the central server. Before\naggregating the client gradients, the central server train a data value\nestimator of the gradients using reinforcement learning techniques. As shown by\nexperimental results, the proposed method consistently outperforms the\nconventional leave-one-out method in terms of valuation authenticity as well as\ntime complexity.", "published": "2021-02-26 06:01:42", "link": "http://arxiv.org/abs/2102.13314v1", "categories": ["cs.LG", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Underwater Acoustic Communication Receiver Using Deep Belief Network", "abstract": "Underwater environments create a challenging channel for communications. In\nthis paper, we design a novel receiver system by exploring the machine learning\ntechnique--Deep Belief Network (DBN)-- to combat the signal distortion caused\nby the Doppler effect and multi-path propagation. We evaluate the performance\nof the proposed receiver system in both simulation experiments and sea trials.\nOur proposed receiver system comprises of DBN based de-noising and\nclassification of the received signal. First, the received signal is segmented\ninto frames before the each of these frames is individually pre-processed using\na novel pixelization algorithm. Then, using the DBN based de-noising algorithm,\nfeatures are extracted from these frames and used to reconstruct the received\nsignal. Finally, DBN based classification of the reconstructed signal occurs.\nOur proposed DBN based receiver system does show better performance in channels\ninfluenced by the Doppler effect and multi-path propagation with a performance\nimprovement of 13.2dB at $10^{-3}$ Bit Error Rate (BER).", "published": "2021-02-26 11:18:37", "link": "http://arxiv.org/abs/2102.13397v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Towards Explaining Expressive Qualities in Piano Recordings: Transfer of\n  Explanatory Features via Acoustic Domain Adaptation", "abstract": "Emotion and expressivity in music have been topics of considerable interest\nin the field of music information retrieval. In recent years, mid-level\nperceptual features have been suggested as means to explain computational\npredictions of musical emotion. We find that the diversity of musical styles\nand genres in the available dataset for learning these features is not\nsufficient for models to generalise well to specialised acoustic domains such\nas solo piano music. In this work, we show that by utilising unsupervised\ndomain adaptation together with receptive-field regularised deep neural\nnetworks, it is possible to significantly improve generalisation to this\ndomain. Additionally, we demonstrate that our domain-adapted models can better\npredict and explain expressive qualities in classical piano performances, as\nperceived and described by human listeners.", "published": "2021-02-26 13:49:44", "link": "http://arxiv.org/abs/2102.13479v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The NPU System for the 2020 Personalized Voice Trigger Challenge", "abstract": "This paper describes the system developed by the NPU team for the 2020\npersonalized voice trigger challenge. Our submitted system consists of two\nindependently trained subsystems: a small footprint keyword spotting (KWS)\nsystem and a speaker verification (SV) system. For the KWS system, a\nmulti-scale dilated temporal convolutional (MDTC) network is proposed to detect\nwake-up word (WuW). For SV system, Write something here. The KWS predicts\nposterior probabilities of whether an audio utterance contains WuW and\nestimates the location of WuW at the same time. When the posterior probability\nofWuW reaches a predefined threshold, the identity information of triggered\nsegment is determined by the SV system. On evaluation dataset, our submitted\nsystem obtains detection costs of 0.081and 0.091 in close talking and far-field\ntasks, respectively.", "published": "2021-02-26 15:44:38", "link": "http://arxiv.org/abs/2102.13552v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
