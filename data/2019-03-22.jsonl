{"title": "An end-to-end Neural Network Framework for Text Clustering", "abstract": "The unsupervised text clustering is one of the major tasks in natural\nlanguage processing (NLP) and remains a difficult and complex problem.\nConventional \\mbox{methods} generally treat this task using separated steps,\nincluding text representation learning and clustering the representations. As\nan improvement, neural methods have also been introduced for continuous\nrepresentation learning to address the sparsity problem. However, the\nmulti-step process still deviates from the unified optimization target.\nEspecially the second step of cluster is generally performed with conventional\nmethods such as k-Means. We propose a pure neural framework for text clustering\nin an end-to-end manner. It jointly learns the text representation and the\nclustering model. Our model works well when the context can be obtained, which\nis nearly always the case in the field of NLP. We have our method\n\\mbox{evaluated} on two widely used benchmarks: IMDB movie reviews for\nsentiment classification and $20$-Newsgroup for topic categorization. Despite\nits simplicity, experiments show the model outperforms previous clustering\nmethods by a large margin. Furthermore, the model is also verified on English\nwiki dataset as a large corpus.", "published": "2019-03-22 09:54:36", "link": "http://arxiv.org/abs/1903.09424v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LINSPECTOR: Multilingual Probing Tasks for Word Representations", "abstract": "Despite an ever growing number of word representation models introduced for a\nlarge number of languages, there is a lack of a standardized technique to\nprovide insights into what is captured by these models. Such insights would\nhelp the community to get an estimate of the downstream task performance, as\nwell as to design more informed neural architectures, while avoiding extensive\nexperimentation which requires substantial computational resources not all\nresearchers have access to. A recent development in NLP is to use simple\nclassification tasks, also called probing tasks, that test for a single\nlinguistic feature such as part-of-speech. Existing studies mostly focus on\nexploring the linguistic information encoded by the continuous representations\nof English text. However, from a typological perspective the morphologically\npoor English is rather an outlier: the information encoded by the word order\nand function words in English is often stored on a morphological level in other\nlanguages. To address this, we introduce 15 type-level probing tasks such as\ncase marking, possession, word length, morphological tag count and pseudoword\nidentification for 24 languages. We present a reusable methodology for creation\nand evaluation of such tests in a multilingual setting. We then present\nexperiments on several diverse multilingual word embedding models, in which we\nrelate the probing task performance for a diverse set of languages to a range\nof five classic NLP tasks: POS-tagging, dependency parsing, semantic role\nlabeling, named entity recognition and natural language inference. We find that\na number of probing tests have significantly high positive correlation to the\ndownstream tasks, especially for morphologically rich languages. We show that\nour tests can be used to explore word embeddings or black-box neural models for\nlinguistic cues in a multilingual setting.", "published": "2019-03-22 11:01:16", "link": "http://arxiv.org/abs/1903.09442v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Augmentation via Dependency Tree Morphing for Low-Resource\n  Languages", "abstract": "Neural NLP systems achieve high scores in the presence of sizable training\ndataset. Lack of such datasets leads to poor system performances in the case\nlow-resource languages. We present two simple text augmentation techniques\nusing dependency trees, inspired from image processing. We crop sentences by\nremoving dependency links, and we rotate sentences by moving the tree fragments\naround the root. We apply these techniques to augment the training sets of\nlow-resource languages in Universal Dependencies project. We implement a\ncharacter-level sequence tagging model and evaluate the augmented datasets on\npart-of-speech tagging task. We show that crop and rotate provides improvements\nover the models trained with non-augmented data for majority of the languages,\nespecially for languages with rich case marking systems.", "published": "2019-03-22 11:55:21", "link": "http://arxiv.org/abs/1903.09460v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing\n  Auxiliary Sentence", "abstract": "Aspect-based sentiment analysis (ABSA), which aims to identify fine-grained\nopinion polarity towards a specific aspect, is a challenging subtask of\nsentiment analysis (SA). In this paper, we construct an auxiliary sentence from\nthe aspect and convert ABSA to a sentence-pair classification task, such as\nquestion answering (QA) and natural language inference (NLI). We fine-tune the\npre-trained model from BERT and achieve new state-of-the-art results on\nSentiHood and SemEval-2014 Task 4 datasets.", "published": "2019-03-22 16:29:18", "link": "http://arxiv.org/abs/1903.09588v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-trained Language Model Representations for Language Generation", "abstract": "Pre-trained language model representations have been successful in a wide\nrange of language understanding tasks. In this paper, we examine different\nstrategies to integrate pre-trained representations into sequence to sequence\nmodels and apply it to neural machine translation and abstractive\nsummarization. We find that pre-trained representations are most effective when\nadded to the encoder network which slows inference by only 14%. Our experiments\nin machine translation show gains of up to 5.3 BLEU in a simulated\nresource-poor setup. While returns diminish with more labeled data, we still\nobserve improvements when millions of sentence-pairs are available. Finally, on\nabstractive summarization we achieve a new state of the art on the full text\nversion of CNN/DailyMail.", "published": "2019-03-22 22:14:51", "link": "http://arxiv.org/abs/1903.09722v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Type-coherent, Expressive Representation as an Initial Step to\n  Language Understanding", "abstract": "A growing interest in tasks involving language understanding by the NLP\ncommunity has led to the need for effective semantic parsing and inference.\nModern NLP systems use semantic representations that do not quite fulfill the\nnuanced needs for language understanding: adequately modeling language\nsemantics, enabling general inferences, and being accurately recoverable. This\ndocument describes underspecified logical forms (ULF) for Episodic Logic (EL),\nwhich is an initial form for a semantic representation that balances these\nneeds. ULFs fully resolve the semantic type structure while leaving issues such\nas quantifier scope, word sense, and anaphora unresolved; they provide a\nstarting point for further resolution into EL, and enable certain structural\ninferences without further resolution. This document also presents preliminary\nresults of creating a hand-annotated corpus of ULFs for the purpose of training\na precise ULF parser, showing a three-person pairwise interannotator agreement\nof 0.88 on confident annotations. We hypothesize that a divide-and-conquer\napproach to semantic parsing starting with derivation of ULFs will lead to\nsemantic analyses that do justice to subtle aspects of linguistic meaning, and\nwill enable construction of more accurate semantic parsers.", "published": "2019-03-22 03:06:36", "link": "http://arxiv.org/abs/1903.09333v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards adversarial learning of speaker-invariant representation for\n  speech emotion recognition", "abstract": "Speech emotion recognition (SER) has attracted great attention in recent\nyears due to the high demand for emotionally intelligent speech interfaces.\nDeriving speaker-invariant representations for speech emotion recognition is\ncrucial. In this paper, we propose to apply adversarial training to SER to\nlearn speaker-invariant representations. Our model consists of three parts: a\nrepresentation learning sub-network with time-delay neural network (TDNN) and\nLSTM with statistical pooling, an emotion classification network and a speaker\nclassification network. Both the emotion and speaker classification network\ntake the output of the representation learning network as input. Two training\nstrategies are employed: one based on domain adversarial training (DAT) and the\nother one based on cross-gradient training (CGT). Besides the conventional data\nset, we also evaluate our proposed models on a much larger publicly available\nemotion data set with 250 speakers. Evaluation results show that on IEMOCAP,\nDAT and CGT provides 5.6% and 7.4% improvement respectively, over a baseline\nsystem without speaker-invariant representation learning on 5-fold cross\nvalidation. On the larger emotion data set, while CGT fails to yield better\nresults than baseline, DAT can still provide 9.8% relative improvement on a\nstandalone test set.", "published": "2019-03-22 17:04:57", "link": "http://arxiv.org/abs/1903.09606v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Speech Enhancement Based on Multichannel NMF-Informed\n  Beamforming for Noise-Robust Automatic Speech Recognition", "abstract": "This paper describes multichannel speech enhancement for improving automatic\nspeech recognition (ASR) in noisy environments. Recently, the minimum variance\ndistortionless response (MVDR) beamforming has widely been used because it\nworks well if the steering vector of speech and the spatial covariance matrix\n(SCM) of noise are given. To estimating such spatial information, conventional\nstudies take a supervised approach that classifies each time-frequency (TF) bin\ninto noise or speech by training a deep neural network (DNN). The performance\nof ASR, however, is degraded in an unknown noisy environment. To solve this\nproblem, we take an unsupervised approach that decomposes each TF bin into the\nsum of speech and noise by using multichannel nonnegative matrix factorization\n(MNMF). This enables us to accurately estimate the SCMs of speech and noise not\nfrom observed noisy mixtures but from separated speech and noise components. In\nthis paper we propose online MVDR beamforming by effectively initializing and\nincrementally updating the parameters of MNMF. Another main contribution is to\ncomprehensively investigate the performances of ASR obtained by various types\nof spatial filters, i.e., time-invariant and variant versions of MVDR\nbeamformers and those of rank-1 and full-rank multichannel Wiener filters, in\ncombination with MNMF. The experimental results showed that the proposed method\noutperformed the state-of-the-art DNN-based beamforming method in unknown\nenvironments that did not match training data.", "published": "2019-03-22 03:36:43", "link": "http://arxiv.org/abs/1903.09341v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Imperceptible, Robust, and Targeted Adversarial Examples for Automatic\n  Speech Recognition", "abstract": "Adversarial examples are inputs to machine learning models designed by an\nadversary to cause an incorrect output. So far, adversarial examples have been\nstudied most extensively in the image domain. In this domain, adversarial\nexamples can be constructed by imperceptibly modifying images to cause\nmisclassification, and are practical in the physical world. In contrast,\ncurrent targeted adversarial examples applied to speech recognition systems\nhave neither of these properties: humans can easily identify the adversarial\nperturbations, and they are not effective when played over-the-air. This paper\nmakes advances on both of these fronts. First, we develop effectively\nimperceptible audio adversarial examples (verified through a human study) by\nleveraging the psychoacoustic principle of auditory masking, while retaining\n100% targeted success rate on arbitrary full-sentence targets. Next, we make\nprogress towards physical-world over-the-air audio adversarial examples by\nconstructing perturbations which remain effective even after applying realistic\nsimulated environmental distortions.", "published": "2019-03-22 17:46:35", "link": "http://arxiv.org/abs/1903.10346v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
