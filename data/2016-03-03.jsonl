{"title": "Question Answering on Freebase via Relation Extraction and Textual\n  Evidence", "abstract": "Existing knowledge-based question answering systems often rely on small\nannotated training data. While shallow methods like relation extraction are\nrobust to data scarcity, they are less expressive than the deep meaning\nrepresentation methods like semantic parsing, thereby failing at answering\nquestions involving multiple constraints. Here we alleviate this problem by\nempowering a relation extraction method with additional evidence from\nWikipedia. We first present a neural network based relation extractor to\nretrieve the candidate answers from Freebase, and then infer over Wikipedia to\nvalidate these answers. Experiments on the WebQuestions question answering\ndataset show that our method achieves an F_1 of 53.3%, a substantial\nimprovement over the state-of-the-art.", "published": "2016-03-03 03:22:01", "link": "http://arxiv.org/abs/1603.00957v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for\n  Sentence Classification", "abstract": "We introduce a novel, simple convolution neural network (CNN) architecture -\nmulti-group norm constraint CNN (MGNC-CNN) that capitalizes on multiple sets of\nword embeddings for sentence classification. MGNC-CNN extracts features from\ninput embedding sets independently and then joins these at the penultimate\nlayer in the network to form a final feature vector. We then adopt a group\nregularization strategy that differentially penalizes weights associated with\nthe subcomponents generated from the respective embedding sets. This model is\nmuch simpler than comparable alternative architectures and requires\nsubstantially less training time. Furthermore, it is flexible in that it does\nnot require input word embeddings to be of the same dimensionality. We show\nthat MGNC-CNN consistently outperforms baseline models.", "published": "2016-03-03 04:12:02", "link": "http://arxiv.org/abs/1603.00968v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Right Ideals of a Ring and Sublanguages of Science", "abstract": "Among Zellig Harris's numerous contributions to linguistics his theory of the\nsublanguages of science probably ranks among the most underrated. However, not\nonly has this theory led to some exhaustive and meaningful applications in the\nstudy of the grammar of immunology language and its changes over time, but it\nalso illustrates the nature of mathematical relations between chunks or subsets\nof a grammar and the language as a whole. This becomes most clear when dealing\nwith the connection between metalanguage and language, as well as when\nreflecting on operators.\n  This paper tries to justify the claim that the sublanguages of science stand\nin a particular algebraic relation to the rest of the language they are\nembedded in, namely, that of right ideals in a ring.", "published": "2016-03-03 09:33:09", "link": "http://arxiv.org/abs/1603.01032v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-domain Neural Network Language Generation for Spoken Dialogue\n  Systems", "abstract": "Moving from limited-domain natural language generation (NLG) to open domain\nis difficult because the number of semantic input combinations grows\nexponentially with the number of domains. Therefore, it is important to\nleverage existing resources and exploit similarities between domains to\nfacilitate domain adaptation. In this paper, we propose a procedure to train\nmulti-domain, Recurrent Neural Network-based (RNN) language generators via\nmultiple adaptation steps. In this procedure, a model is first trained on\ncounterfeited data synthesised from an out-of-domain dataset, and then fine\ntuned on a small set of in-domain utterances with a discriminative objective\nfunction. Corpus-based evaluation results show that the proposed procedure can\nachieve competitive performance in terms of BLEU score and slot error rate\nwhile significantly reducing the data needed to train generators in new, unseen\ndomains. In subjective testing, human judges confirm that the procedure greatly\nimproves generator performance when only a small amount of data is available in\nthe domain.", "published": "2016-03-03 19:49:32", "link": "http://arxiv.org/abs/1603.01232v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
