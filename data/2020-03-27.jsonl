{"title": "Information-Theoretic Probing with Minimum Description Length", "abstract": "To measure how well pretrained representations encode some linguistic\nproperty, it is common to use accuracy of a probe, i.e. a classifier trained to\npredict the property from the representations. Despite widespread adoption of\nprobes, differences in their accuracy fail to adequately reflect differences in\nrepresentations. For example, they do not substantially favour pretrained\nrepresentations over randomly initialized ones. Analogously, their accuracy can\nbe similar when probing for genuine linguistic labels and probing for random\nsynthetic tasks. To see reasonable differences in accuracy with respect to\nthese random baselines, previous work had to constrain either the amount of\nprobe training data or its model size. Instead, we propose an alternative to\nthe standard probes, information-theoretic probing with minimum description\nlength (MDL). With MDL probing, training a probe to predict labels is recast as\nteaching it to effectively transmit the data. Therefore, the measure of\ninterest changes from probe accuracy to the description length of labels given\nrepresentations. In addition to probe quality, the description length evaluates\n\"the amount of effort\" needed to achieve the quality. This amount of effort\ncharacterizes either (i) size of a probing model, or (ii) the amount of data\nneeded to achieve the high quality. We consider two methods for estimating MDL\nwhich can be easily implemented on top of the standard probing pipelines:\nvariational coding and online coding. We show that these methods agree in\nresults and are more informative and stable than the standard probes.", "published": "2020-03-27 09:35:38", "link": "http://arxiv.org/abs/2003.12298v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Enrichment of Nigerian Pidgin English for Contextual Sentiment\n  Classification", "abstract": "Nigerian English adaptation, Pidgin, has evolved over the years through\nmulti-language code switching, code mixing and linguistic adaptation. While\nPidgin preserves many of the words in the normal English language corpus, both\nin spelling and pronunciation, the fundamental meaning of these words have\nchanged significantly. For example,'ginger' is not a plant but an expression of\nmotivation and 'tank' is not a container but an expression of gratitude. The\nimplication is that the current approach of using direct English sentiment\nanalysis of social media text from Nigeria is sub-optimal, as it will not be\nable to capture the semantic variation and contextual evolution in the\ncontemporary meaning of these words. In practice, while many words in Nigerian\nPidgin adaptation are the same as the standard English, the full English\nlanguage based sentiment analysis models are not designed to capture the full\nintent of the Nigerian pidgin when used alone or code-mixed. By augmenting\nscarce human labelled code-changed text with ample synthetic code-reformatted\ntext and meaning, we achieve significant improvements in sentiment scoring. Our\nresearch explores how to understand sentiment in an intrasentential code mixing\nand switching context where there has been significant word localization.This\nwork presents a 300 VADER lexicon compatible Nigerian Pidgin sentiment tokens\nand their scores and a 14,000 gold standard Nigerian Pidgin tweets and their\nsentiments labels.", "published": "2020-03-27 14:52:55", "link": "http://arxiv.org/abs/2003.12450v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Supervised and Unsupervised Neural Machine Translation Baselines\n  for Nigerian Pidgin", "abstract": "Nigerian Pidgin is arguably the most widely spoken language in Nigeria.\nVariants of this language are also spoken across West and Central Africa,\nmaking it a very important language. This work aims to establish supervised and\nunsupervised neural machine translation (NMT) baselines between English and\nNigerian Pidgin. We implement and compare NMT models with different\ntokenization methods, creating a solid foundation for future works.", "published": "2020-03-27 22:40:01", "link": "http://arxiv.org/abs/2003.12660v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Comprehensive Named Entity Recognition on CORD-19 with Distant or Weak\n  Supervision", "abstract": "We created this CORD-NER dataset with comprehensive named entity recognition\n(NER) on the COVID-19 Open Research Dataset Challenge (CORD-19) corpus\n(2020-03-13). This CORD-NER dataset covers 75 fine-grained entity types: In\naddition to the common biomedical entity types (e.g., genes, chemicals and\ndiseases), it covers many new entity types related explicitly to the COVID-19\nstudies (e.g., coronaviruses, viral proteins, evolution, materials, substrates\nand immune responses), which may benefit research on COVID-19 related virus,\nspreading mechanisms, and potential vaccines. CORD-NER annotation is a\ncombination of four sources with different NER methods. The quality of CORD-NER\nannotation surpasses SciSpacy (over 10% higher on the F1 score based on a\nsample set of documents), a fully supervised BioNER tool. Moreover, CORD-NER\nsupports incrementally adding new documents as well as adding new entity types\nwhen needed by adding dozens of seeds as the input examples. We will constantly\nupdate CORD-NER based on the incremental updates of the CORD-19 corpus and the\nimprovement of our system.", "published": "2020-03-27 03:35:46", "link": "http://arxiv.org/abs/2003.12218v5", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Can you hear me $\\textit{now}$? Sensitive comparisons of human and\n  machine perception", "abstract": "The rise of machine-learning systems that process sensory input has brought\nwith it a rise in comparisons between human and machine perception. But such\ncomparisons face a challenge: Whereas machine perception of some stimulus can\noften be probed through direct and explicit measures, much of human perceptual\nknowledge is latent, incomplete, or unavailable for explicit report. Here, we\nexplore how this asymmetry can cause such comparisons to misestimate the\noverlap in human and machine perception. As a case study, we consider human\nperception of \\textit{adversarial speech} -- synthetic audio commands that are\nrecognized as valid messages by automated speech-recognition systems but that\nhuman listeners reportedly hear as meaningless noise. In five experiments, we\nadapt task designs from the human psychophysics literature to show that even\nwhen subjects cannot freely transcribe such speech commands (the previous\nbenchmark for human understanding), they often can demonstrate other forms of\nunderstanding, including discriminating adversarial speech from closely matched\nnon-speech (Experiments 1--2), finishing common phrases begun in adversarial\nspeech (Experiments 3--4), and solving simple math problems posed in\nadversarial speech (Experiment 5) -- even for stimuli previously described as\nunintelligible to human listeners. We recommend the adoption of such \"sensitive\ntests\" when comparing human and machine perception, and we discuss the broader\nconsequences of such approaches for assessing the overlap between systems.", "published": "2020-03-27 16:24:08", "link": "http://arxiv.org/abs/2003.12362v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Neural translation and automated recognition of ICD10 medical entities\n  from natural language", "abstract": "The recognition of medical entities from natural language is an ubiquitous\nproblem in the medical field, with applications ranging from medical act coding\nto the analysis of electronic health data for public health. It is however a\ncomplex task usually requiring human expert intervention, thus making it\nexpansive and time consuming. The recent advances in artificial intelligence,\nspecifically the raise of deep learning methods, has enabled computers to make\nefficient decisions on a number of complex problems, with the notable example\nof neural sequence models and their powerful applications in natural language\nprocessing. They however require a considerable amount of data to learn from,\nwhich is typically their main limiting factor. However, the C\\'epiDc stores an\nexhaustive database of death certificates at the French national scale,\namounting to several millions of natural language examples provided with their\nassociated human coded medical entities available to the machine learning\npractitioner. This article investigates the applications of deep neural\nsequence models to the medical entity recognition from natural language\nproblem.", "published": "2020-03-27 18:17:53", "link": "http://arxiv.org/abs/2004.13839v2", "categories": ["cs.CL", "cs.CY", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Using LSTM to Translate French to Senegalese Local Languages: Wolof as a\n  Case Study", "abstract": "In this paper, we propose a neural machine translation system for Wolof, a\nlow-resource Niger-Congo language. First we gathered a parallel corpus of 70000\naligned French-Wolof sentences. Then we developped a baseline LSTM based\nencoder-decoder architecture which was further extended to bidirectional LSTMs\nwith attention mechanisms. Our models are trained on a limited amount of\nparallel French-Wolof data of approximately 35000 parallel sentences.\nExperimental results on French-Wolof translation tasks show that our approach\nproduces promising translations in extremely low-resource conditions. The best\nmodel was able to achieve a good performance of 47% BLEU score.", "published": "2020-03-27 17:09:52", "link": "http://arxiv.org/abs/2004.13840v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Dual Attention in Time and Frequency Domain for Voice Activity Detection", "abstract": "Voice activity detection (VAD) is a challenging task in low signal-to-noise\nratio (SNR) environment, especially in non-stationary noise. To deal with this\nissue, we propose a novel attention module that can be integrated in Long\nShort-Term Memory (LSTM). Our proposed attention module refines each LSTM\nlayer's hidden states so as to make it possible to adaptively focus on both\ntime and frequency domain. Experiments are conducted on various noisy\nconditions using Aurora 4 database. Our proposed method obtains the 95.58 %\narea under the ROC curve (AUC), achieving 22.05 % relative improvement compared\nto baseline, with only 2.44 % increase in the number of parameters. Besides, we\nutilize focal loss for alleviating the performance degradation caused by\nimbalance between speech and non-speech sections in training sets. The results\nshow that the focal loss can improve the performance in various imbalance\nsituations compared to the cross entropy loss, a commonly used loss function in\nVAD.", "published": "2020-03-27 07:46:49", "link": "http://arxiv.org/abs/2003.12266v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Voice activity detection in the wild via weakly supervised sound event\n  detection", "abstract": "Traditional supervised voice activity detection (VAD) methods work well in\nclean and controlled scenarios, with performance severely degrading in\nreal-world applications. One possible bottleneck is that speech in the wild\ncontains unpredictable noise types, hence frame-level label prediction is\ndifficult, which is required for traditional supervised VAD training. In\ncontrast, we propose a general-purpose VAD (GPVAD) framework, which can be\neasily trained from noisy data in a weakly supervised fashion, requiring only\nclip-level labels. We proposed two GPVAD models, one full (GPV-F), trained on\n527 Audioset sound events, and one binary (GPV-B), only distinguishing speech\nand noise. We evaluate the two GPV models against a CRNN based standard VAD\nmodel (VAD-C) on three different evaluation protocols (clean, synthetic noise,\nreal data). Results show that our proposed GPV-F demonstrates competitive\nperformance in clean and synthetic scenarios compared to traditional VAD-C.\nFurther, in real-world evaluation, GPV-F largely outperforms VAD-C in terms of\nframe-level evaluation metrics as well as segment-level ones. With a much lower\nrequirement for frame-labeled data, the naive binary clip-level GPV-B model can\nstill achieve comparable performance to VAD-C in real-world scenarios.", "published": "2020-03-27 03:47:12", "link": "http://arxiv.org/abs/2003.12222v6", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Separating Varying Numbers of Sources with Auxiliary Autoencoding Loss", "abstract": "Many recent source separation systems are designed to separate a fixed number\nof sources out of a mixture. In the cases where the source activation patterns\nare unknown, such systems have to either adjust the number of outputs or to\nidentify invalid outputs from the valid ones. Iterative separation methods have\ngain much attention in the community as they can flexibly decide the number of\noutputs, however (1) they typically rely on long-term information to determine\nthe stopping time for the iterations, which makes them hard to operate in a\ncausal setting; (2) they lack a \"fault tolerance\" mechanism when the estimated\nnumber of sources is different from the actual number. In this paper, we\npropose a simple training method, the auxiliary autoencoding permutation\ninvariant training (A2PIT), to alleviate the two issues. A2PIT assumes a fixed\nnumber of outputs and uses auxiliary autoencoding loss to force the invalid\noutputs to be the copies of the input mixture, and detects invalid outputs in a\nfully unsupervised way during inference phase. Experiment results show that\nA2PIT is able to improve the separation performance across various numbers of\nspeakers and effectively detect the number of speakers in a mixture.", "published": "2020-03-27 10:58:09", "link": "http://arxiv.org/abs/2003.12326v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Mic2Mic: Using Cycle-Consistent Generative Adversarial Networks to\n  Overcome Microphone Variability in Speech Systems", "abstract": "Mobile and embedded devices are increasingly using microphones and\naudio-based computational models to infer user context. A major challenge in\nbuilding systems that combine audio models with commodity microphones is to\nguarantee their accuracy and robustness in the real-world. Besides many\nenvironmental dynamics, a primary factor that impacts the robustness of audio\nmodels is microphone variability. In this work, we propose Mic2Mic -- a\nmachine-learned system component -- which resides in the inference pipeline of\naudio models and at real-time reduces the variability in audio data caused by\nmicrophone-specific factors. Two key considerations for the design of Mic2Mic\nwere: a) to decouple the problem of microphone variability from the audio task,\nand b) put a minimal burden on end-users to provide training data. With these\nin mind, we apply the principles of cycle-consistent generative adversarial\nnetworks (CycleGANs) to learn Mic2Mic using unlabeled and unpaired data\ncollected from different microphones. Our experiments show that Mic2Mic can\nrecover between 66% to 89% of the accuracy lost due to microphone variability\nfor two common audio tasks.", "published": "2020-03-27 14:06:36", "link": "http://arxiv.org/abs/2003.12425v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML", "I.2.1; C.m"], "primary_category": "eess.AS"}
