{"title": "Data Augmentation for Mental Health Classification on Social Media", "abstract": "The mental disorder of online users is determined using social media posts.\nThe major challenge in this domain is to avail the ethical clearance for using\nthe user generated text on social media platforms. Academic re searchers\nidentified the problem of insufficient and unlabeled data for mental health\nclassification. To handle this issue, we have studied the effect of data\naugmentation techniques on domain specific user generated text for mental\nhealth classification. Among the existing well established data augmentation\ntechniques, we have identified Easy Data Augmentation (EDA), conditional BERT,\nand Back Translation (BT) as the potential techniques for generating additional\ntext to improve the performance of classifiers. Further, three different\nclassifiers Random Forest (RF), Support Vector Machine (SVM) and Logistic\nRegression (LR) are employed for analyzing the impact of data augmentation on\ntwo publicly available social media datasets. The experiments mental results\nshow significant improvements in classifiers performance when trained on the\naugmented data.", "published": "2021-12-19 05:09:01", "link": "http://arxiv.org/abs/2112.10064v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unified Named Entity Recognition as Word-Word Relation Classification", "abstract": "So far, named entity recognition (NER) has been involved with three major\ntypes, including flat, overlapped (aka. nested), and discontinuous NER, which\nhave mostly been studied individually. Recently, a growing interest has been\nbuilt for unified NER, tackling the above three jobs concurrently with one\nsingle model. Current best-performing methods mainly include span-based and\nsequence-to-sequence models, where unfortunately the former merely focus on\nboundary identification and the latter may suffer from exposure bias. In this\nwork, we present a novel alternative by modeling the unified NER as word-word\nrelation classification, namely W^2NER. The architecture resolves the kernel\nbottleneck of unified NER by effectively modeling the neighboring relations\nbetween entity words with Next-Neighboring-Word (NNW) and Tail-Head-Word-*\n(THW-*) relations. Based on the W^2NER scheme we develop a neural framework, in\nwhich the unified NER is modeled as a 2D grid of word pairs. We then propose\nmulti-granularity 2D convolutions for better refining the grid representations.\nFinally, a co-predictor is used to sufficiently reason the word-word relations.\nWe perform extensive experiments on 14 widely-used benchmark datasets for flat,\noverlapped, and discontinuous NER (8 English and 6 Chinese datasets), where our\nmodel beats all the current top-performing baselines, pushing the\nstate-of-the-art performances of unified NER.", "published": "2021-12-19 06:11:07", "link": "http://arxiv.org/abs/2112.10070v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Patient Readmission Risk from Medical Text via Knowledge\n  Graph Enhanced Multiview Graph Convolution", "abstract": "Unplanned intensive care unit (ICU) readmission rate is an important metric\nfor evaluating the quality of hospital care. Efficient and accurate prediction\nof ICU readmission risk can not only help prevent patients from inappropriate\ndischarge and potential dangers, but also reduce associated costs of\nhealthcare. In this paper, we propose a new method that uses medical text of\nElectronic Health Records (EHRs) for prediction, which provides an alternative\nperspective to previous studies that heavily depend on numerical and\ntime-series features of patients. More specifically, we extract discharge\nsummaries of patients from their EHRs, and represent them with multiview graphs\nenhanced by an external knowledge graph. Graph convolutional networks are then\nused for representation learning. Experimental results prove the effectiveness\nof our method, yielding state-of-the-art performance for this task.", "published": "2021-12-19 01:45:57", "link": "http://arxiv.org/abs/2201.02510v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LUC at ComMA-2021 Shared Task: Multilingual Gender Biased and Communal\n  Language Identification without using linguistic features", "abstract": "This work aims to evaluate the ability that both probabilistic and\nstate-of-the-art vector space modeling (VSM) methods provide to well known\nmachine learning algorithms to identify social network documents to be\nclassified as aggressive, gender biased or communally charged. To this end, an\nexploratory stage was performed first in order to find relevant settings to\ntest, i.e. by using training and development samples, we trained multiple\nalgorithms using multiple vector space modeling and probabilistic methods and\ndiscarded the less informative configurations. These systems were submitted to\nthe competition of the ComMA@ICON'21 Workshop on Multilingual Gender Biased and\nCommunal Language Identification.", "published": "2021-12-19 16:44:57", "link": "http://arxiv.org/abs/2112.10189v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Investigation of Densely Connected Convolutional Networks with Domain\n  Adversarial Learning for Noise Robust Speech Recognition", "abstract": "We investigate densely connected convolutional networks (DenseNets) and their\nextension with domain adversarial training for noise robust speech recognition.\nDenseNets are very deep, compact convolutional neural networks which have\ndemonstrated incredible improvements over the state-of-the-art results in\ncomputer vision. Our experimental results reveal that DenseNets are more robust\nagainst noise than other neural network based models such as deep feed forward\nneural networks and convolutional neural networks. Moreover, domain adversarial\nlearning can further improve the robustness of DenseNets against both, known\nand unknown noise conditions.", "published": "2021-12-19 10:29:17", "link": "http://arxiv.org/abs/2112.10108v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Early Detection of Security-Relevant Bug Reports using Machine Learning:\n  How Far Are We?", "abstract": "Bug reports are common artefacts in software development. They serve as the\nmain channel for users to communicate to developers information about the\nissues that they encounter when using released versions of software programs.\nIn the descriptions of issues, however, a user may, intentionally or not,\nexpose a vulnerability. In a typical maintenance scenario, such\nsecurity-relevant bug reports are prioritised by the development team when\npreparing corrective patches. Nevertheless, when security relevance is not\nimmediately expressed (e.g., via a tag) or rapidly identified by triaging\nteams, the open security-relevant bug report can become a critical leak of\nsensitive information that attackers can leverage to perform zero-day attacks.\nTo support practitioners in triaging bug reports, the research community has\nproposed a number of approaches for the detection of security-relevant bug\nreports. In recent years, approaches in this respect based on machine learning\nhave been reported with promising performance. Our work focuses on such\napproaches, and revisits their building blocks to provide a comprehensive view\non the current achievements. To that end, we built a large experimental dataset\nand performed extensive experiments with variations in feature sets and\nlearning algorithms. Eventually, our study highlights different approach\nconfigurations that yield best performing classifiers.", "published": "2021-12-19 11:30:29", "link": "http://arxiv.org/abs/2112.10123v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Multi-turn RNN-T for streaming recognition of multi-party speech", "abstract": "Automatic speech recognition (ASR) of single channel far-field recordings\nwith an unknown number of speakers is traditionally tackled by cascaded\nmodules. Recent research shows that end-to-end (E2E) multi-speaker ASR models\ncan achieve superior recognition accuracy compared to modular systems. However,\nthese models do not ensure real-time applicability due to their dependency on\nfull audio context. This work takes real-time applicability as the first\npriority in model design and addresses a few challenges in previous work on\nmulti-speaker recurrent neural network transducer (MS-RNN-T). First, we\nintroduce on-the-fly overlapping speech simulation during training, yielding\n14% relative word error rate (WER) improvement on LibriSpeechMix test set.\nSecond, we propose a novel multi-turn RNN-T (MT-RNN-T) model with an\noverlap-based target arrangement strategy that generalizes to an arbitrary\nnumber of speakers without changes in the model architecture. We investigate\nthe impact of the maximum number of speakers seen during training on MT-RNN-T\nperformance on LibriCSS test set, and report 28% relative WER improvement over\nthe two-speaker MS-RNN-T. Third, we experiment with a rich transcription\nstrategy for joint recognition and segmentation of multi-party speech. Through\nan in-depth analysis, we discuss potential pitfalls of the proposed system as\nwell as promising future research directions.", "published": "2021-12-19 17:22:58", "link": "http://arxiv.org/abs/2112.10200v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Integrating Knowledge in End-to-End Automatic Speech Recognition for\n  Mandarin-English Code-Switching", "abstract": "Code-Switching (CS) is a common linguistic phenomenon in multilingual\ncommunities that consists of switching between languages while speaking. This\npaper presents our investigations on end-to-end speech recognition for\nMandarin-English CS speech. We analyse different CS specific issues such as the\nproperties mismatches between languages in a CS language pair, the\nunpredictable nature of switching points, and the data scarcity problem. We\nexploit and improve the state-of-the-art end-to-end system by merging\nnonlinguistic symbols, by integrating language identification using\nhierarchical softmax, by modeling sub-word units, by artificially lowering the\nspeaking rate, and by augmenting data using speed perturbed technique and\nseveral monolingual datasets to improve the final performance not only on CS\nspeech but also on monolingual benchmarks in order to make the system more\napplicable on real life settings. Finally, we explore the effect of different\nlanguage model integration methods on the performance of the proposed model.\nOur experimental results reveal that all the proposed techniques improve the\nrecognition performance. The best combined system improves the baseline system\nby up to 35% relatively in terms of mixed error rate and delivers acceptable\nperformance on monolingual benchmarks.", "published": "2021-12-19 17:31:15", "link": "http://arxiv.org/abs/2112.10202v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Detect what you want: Target Sound Detection", "abstract": "Human beings can perceive a target sound type from a multi-source mixture\nsignal by the selective auditory attention, however, such functionality was\nhardly ever explored in machine hearing. This paper addresses the target sound\ndetection (TSD) task, which aims to detect the target sound signal from a\nmixture audio when a target sound's reference audio is given. We present a\nnovel target sound detection network (TSDNet) which consists of two main parts:\nA conditional network which aims at generating a sound-discriminative\nconditional embedding vector representing the target sound, and a detection\nnetwork which takes both the mixture audio and the conditional embedding vector\nas inputs and produces the detection result of the target sound. These two\nnetworks can be jointly optimized with a multi-task learning approach to\nfurther improve the performance. In addition, we study both strong-supervised\nand weakly-supervised strategies to train TSDNet and propose a data\naugmentation method by mixing two samples. To facilitate this research, we\nbuild a target sound detection dataset (\\textit{i.e.} URBAN-TSD) based on\nURBAN-SED and UrbanSound8K datasets, and experimental results indicate our\nmethod could get the segment-based F scores of 76.3$\\%$ and 56.8$\\%$ on the\nstrongly-labelled and weakly-labelled data respectively.", "published": "2021-12-19 14:12:28", "link": "http://arxiv.org/abs/2112.10153v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
