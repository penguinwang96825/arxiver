{"title": "Better Neural Machine Translation by Extracting Linguistic Information\n  from BERT", "abstract": "Adding linguistic information (syntax or semantics) to neural machine\ntranslation (NMT) has mostly focused on using point estimates from pre-trained\nmodels. Directly using the capacity of massive pre-trained contextual word\nembedding models such as BERT (Devlin et al., 2019) has been marginally useful\nin NMT because effective fine-tuning is difficult to obtain for NMT without\nmaking training brittle and unreliable. We augment NMT by extracting dense\nfine-tuned vector-based linguistic information from BERT instead of using point\nestimates. Experimental results show that our method of incorporating\nlinguistic information helps NMT to generalize better in a variety of training\ncontexts and is no more difficult to train than conventional Transformer-based\nNMT.", "published": "2021-04-07 00:03:51", "link": "http://arxiv.org/abs/2104.02831v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interpreting A Pre-trained Model Is A Key For Model Architecture\n  Optimization: A Case Study On Wav2Vec 2.0", "abstract": "A deep Transformer model with good evaluation score does not mean each\nsubnetwork (a.k.a transformer block) learns reasonable representation.\nDiagnosing abnormal representation and avoiding it can contribute to achieving\na better evaluation score. We propose an innovative perspective for analyzing\nattention patterns: summarize block-level patterns and assume abnormal patterns\ncontribute negative influence. We leverage Wav2Vec 2.0 as a research target and\nanalyze a pre-trained model's pattern. All experiments leverage\nLibrispeech-100-clean as training data. Through avoiding diagnosed abnormal\nones, our custom Wav2Vec 2.0 outperforms the original version about 4.8%\nabsolute word error rate (WER) on test-clean with viterbi decoding. Our version\nis still 0.9% better when decoding with a 4-gram language model. Moreover, we\nidentify that avoiding abnormal patterns is the main contributor for\nperformance boosting.", "published": "2021-04-07 01:41:23", "link": "http://arxiv.org/abs/2104.02851v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Write a Bias Statement: Recommendations for Submissions to the\n  Workshop on Gender Bias in NLP", "abstract": "At the Workshop on Gender Bias in NLP (GeBNLP), we'd like to encourage\nauthors to give explicit consideration to the wider aspects of bias and its\nsocial implications. For the 2020 edition of the workshop, we therefore\nrequested that all authors include an explicit bias statement in their work to\nclarify how their work relates to the social context in which NLP systems are\nused.\n  The programme committee of the workshops included a number of reviewers with\na background in the humanities and social sciences, in addition to NLP experts\ndoing the bulk of the reviewing. Each paper was assigned one of those\nreviewers, and they were asked to pay specific attention to the provided bias\nstatements in their reviews. This initiative was well received by the authors\nwho submitted papers to the workshop, several of whom said they received useful\nsuggestions and literature hints from the bias reviewers. We are therefore\nplanning to keep this feature of the review process in future editions of the\nworkshop.", "published": "2021-04-07 10:00:11", "link": "http://arxiv.org/abs/2104.03026v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Scientific Papers Summarization with Citation Graph", "abstract": "Previous work for text summarization in scientific domain mainly focused on\nthe content of the input document, but seldom considering its citation network.\nHowever, scientific papers are full of uncommon domain-specific terms, making\nit almost impossible for the model to understand its true meaning without the\nhelp of the relevant research community. In this paper, we redefine the task of\nscientific papers summarization by utilizing their citation graph and propose a\ncitation graph-based summarization model CGSum which can incorporate the\ninformation of both the source paper and its references. In addition, we\nconstruct a novel scientific papers summarization dataset Semantic Scholar\nNetwork (SSN) which contains 141K research papers in different domains and 661K\ncitation relationships. The entire dataset constitutes a large connected\ncitation graph. Extensive experiments show that our model can achieve\ncompetitive performance when compared with the pretrained models even with a\nsimple architecture. The results also indicates the citation graph is crucial\nto better understand the content of papers and generate high-quality summaries.", "published": "2021-04-07 11:13:35", "link": "http://arxiv.org/abs/2104.03057v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BreakingBERT@IITK at SemEval-2021 Task 9 : Statement Verification and\n  Evidence Finding with Tables", "abstract": "Recently, there has been an interest in factual verification and prediction\nover structured data like tables and graphs. To circumvent any false news\nincident, it is necessary to not only model and predict over structured data\nefficiently but also to explain those predictions. In this paper, as part of\nthe SemEval-2021 Task 9, we tackle the problem of fact verification and\nevidence finding over tabular data. There are two subtasks. Given a table and a\nstatement/fact, subtask A determines whether the statement is inferred from the\ntabular data, and subtask B determines which cells in the table provide\nevidence for the former subtask. We make a comparison of the baselines and\nstate-of-the-art approaches over the given SemTabFact dataset. We also propose\na novel approach CellBERT to solve evidence finding as a form of the Natural\nLanguage Inference task. We obtain a 3-way F1 score of 0.69 on subtask A and an\nF1 score of 0.65 on subtask B.", "published": "2021-04-07 11:41:07", "link": "http://arxiv.org/abs/2104.03071v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GrammarTagger: A Multilingual, Minimally-Supervised Grammar Profiler for\n  Language Education", "abstract": "We present GrammarTagger, an open-source grammar profiler which, given an\ninput text, identifies grammatical features useful for language education. The\nmodel architecture enables it to learn from a small amount of texts annotated\nwith spans and their labels, which 1) enables easier and more intuitive\nannotation, 2) supports overlapping spans, and 3) is less prone to error\npropagation, compared to complex hand-crafted rules defined on\nconstituency/dependency parses. We show that we can bootstrap a grammar\nprofiler model with $F_1 \\approx 0.6$ from only a couple hundred sentences both\nin English and Chinese, which can be further boosted via learning a\nmultilingual model. With GrammarTagger, we also build Octanove Learn, a search\nengine of language learning materials indexed by their reading difficulty and\ngrammatical features. The code and pretrained models are publicly available at\n\\url{https://github.com/octanove/grammartagger}.", "published": "2021-04-07 15:31:20", "link": "http://arxiv.org/abs/2104.03190v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Pre-trained Word Embeddings and Linguistic Features for\n  Sequential Metaphor Identification", "abstract": "We tackle the problem of identifying metaphors in text, treated as a sequence\ntagging task. The pre-trained word embeddings GloVe, ELMo and BERT have\nindividually shown good performance on sequential metaphor identification.\nThese embeddings are generated by different models, training targets and\ncorpora, thus encoding different semantic and syntactic information. We show\nthat leveraging GloVe, ELMo and feature-based BERT based on a multi-channel CNN\nand a Bidirectional LSTM model can significantly outperform any single word\nembedding method and the combination of the two embeddings. Incorporating\nlinguistic features into our model can further improve model performance,\nyielding state-of-the-art performance on three public metaphor datasets. We\nalso provide in-depth analysis on the effectiveness of leveraging multiple word\nembeddings, including analysing the spatial distribution of different embedding\nmethods for metaphors and literals, and showing how well the embeddings\ncomplement each other in different genres and parts of speech.", "published": "2021-04-07 17:43:05", "link": "http://arxiv.org/abs/2104.03285v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Spotify at TREC 2020: Genre-Aware Abstractive Podcast Summarization", "abstract": "This paper contains the description of our submissions to the summarization\ntask of the Podcast Track in TREC (the Text REtrieval Conference) 2020. The\ngoal of this challenge was to generate short, informative summaries that\ncontain the key information present in a podcast episode using automatically\ngenerated transcripts of the podcast audio. Since podcasts vary with respect to\ntheir genre, topic, and granularity of information, we propose two\nsummarization models that explicitly take genre and named entities into\nconsideration in order to generate summaries appropriate to the style of the\npodcasts. Our models are abstractive, and supervised using creator-provided\ndescriptions as ground truth summaries. The results of the submitted summaries\nshow that our best model achieves an aggregate quality score of 1.58 in\ncomparison to the creator descriptions and a baseline abstractive system which\nboth score 1.49 (an improvement of 9%) as assessed by human evaluators.", "published": "2021-04-07 18:27:28", "link": "http://arxiv.org/abs/2104.03343v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EXPATS: A Toolkit for Explainable Automated Text Scoring", "abstract": "Automated text scoring (ATS) tasks, such as automated essay scoring and\nreadability assessment, are important educational applications of natural\nlanguage processing. Due to their interpretability of models and predictions,\ntraditional machine learning (ML) algorithms based on handcrafted features are\nstill in wide use for ATS tasks. Practitioners often need to experiment with a\nvariety of models (including deep and traditional ML ones), features, and\ntraining objectives (regression and classification), although modern deep\nlearning frameworks such as PyTorch require deep ML expertise to fully utilize.\nIn this paper, we present EXPATS, an open-source framework to allow its users\nto develop and experiment with different ATS models quickly by offering\nflexible components, an easy-to-use configuration system, and the command-line\ninterface. The toolkit also provides seamless integration with the Language\nInterpretability Tool (LIT) so that one can interpret and visualize models and\ntheir predictions. We also describe two case studies where we build ATS models\nquickly with minimal engineering efforts. The toolkit is available at\n\\url{https://github.com/octanove/expats}.", "published": "2021-04-07 19:29:06", "link": "http://arxiv.org/abs/2104.03364v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interpreting Verbal Metaphors by Paraphrasing", "abstract": "Metaphorical expressions are difficult linguistic phenomena, challenging\ndiverse Natural Language Processing tasks. Previous works showed that\nparaphrasing a metaphor as its literal counterpart can help machines better\nprocess metaphors on downstream tasks. In this paper, we interpret metaphors\nwith BERT and WordNet hypernyms and synonyms in an unsupervised manner, showing\nthat our method significantly outperforms the state-of-the-art baseline. We\nalso demonstrate that our method can help a machine translation system improve\nits accuracy in translating English metaphors to 8 target languages.", "published": "2021-04-07 21:00:23", "link": "http://arxiv.org/abs/2104.03391v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynabench: Rethinking Benchmarking in NLP", "abstract": "We introduce Dynabench, an open-source platform for dynamic dataset creation\nand model benchmarking. Dynabench runs in a web browser and supports\nhuman-and-model-in-the-loop dataset creation: annotators seek to create\nexamples that a target model will misclassify, but that another person will\nnot. In this paper, we argue that Dynabench addresses a critical need in our\ncommunity: contemporary models quickly achieve outstanding performance on\nbenchmark tasks but nonetheless fail on simple challenge examples and falter in\nreal-world scenarios. With Dynabench, dataset creation, model development, and\nmodel assessment can directly inform each other, leading to more robust and\ninformative benchmarks. We report on four initial NLP tasks, illustrating these\nconcepts and highlighting the promise of the platform, and address potential\nobjections to dynamic benchmarking as a new standard for the field.", "published": "2021-04-07 17:49:17", "link": "http://arxiv.org/abs/2104.14337v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FSR: Accelerating the Inference Process of Transducer-Based Models by\n  Applying Fast-Skip Regularization", "abstract": "Transducer-based models, such as RNN-Transducer and transformer-transducer,\nhave achieved great success in speech recognition. A typical transducer model\ndecodes the output sequence conditioned on the current acoustic state and\npreviously predicted tokens step by step. Statistically, The number of blank\ntokens in the prediction results accounts for nearly 90\\% of all tokens. It\ntakes a lot of computation and time to predict the blank tokens, but only the\nnon-blank tokens will appear in the final output sequence. Therefore, we\npropose a method named fast-skip regularization, which tries to align the blank\nposition predicted by a transducer with that predicted by a CTC model. During\nthe inference, the transducer model can predict the blank tokens in advance by\na simple CTC project layer without many complicated forward calculations of the\ntransducer decoder and then skip them, which will reduce the computation and\nimprove the inference speed greatly. All experiments are conducted on a public\nChinese mandarin dataset AISHELL-1. The results show that the fast-skip\nregularization can indeed help the transducer model learn the blank position\nalignments. Besides, the inference with fast-skip can be speeded up nearly 4\ntimes with only a little performance degradation.", "published": "2021-04-07 03:15:10", "link": "http://arxiv.org/abs/2104.02882v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Question-answering Based Framework for Relation Extraction Validation", "abstract": "Relation extraction is an important task in knowledge acquisition and text\nunderstanding. Existing works mainly focus on improving relation extraction by\nextracting effective features or designing reasonable model structures.\nHowever, few works have focused on how to validate and correct the results\ngenerated by the existing relation extraction models. We argue that validation\nis an important and promising direction to further improve the performance of\nrelation extraction. In this paper, we explore the possibility of using\nquestion answering as validation. Specifically, we propose a novel\nquestion-answering based framework to validate the results from relation\nextraction models. Our proposed framework can be easily applied to existing\nrelation classifiers without any additional information. We conduct extensive\nexperiments on the popular NYT dataset to evaluate the proposed framework, and\nobserve consistent improvements over five strong baselines.", "published": "2021-04-07 06:08:36", "link": "http://arxiv.org/abs/2104.02934v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Librispeech Transducer Model with Internal Language Model Prior\n  Correction", "abstract": "We present our transducer model on Librispeech. We study variants to include\nan external language model (LM) with shallow fusion and subtract an estimated\ninternal LM. This is justified by a Bayesian interpretation where the\ntransducer model prior is given by the estimated internal LM. The subtraction\nof the internal LM gives us over 14% relative improvement over normal shallow\nfusion. Our transducer has a separate probability distribution for the\nnon-blank labels which allows for easier combination with the external LM, and\neasier estimation of the internal LM. We additionally take care of including\nthe end-of-sentence (EOS) probability of the external LM in the last blank\nprobability which further improves the performance. All our code and setups are\npublished.", "published": "2021-04-07 09:18:56", "link": "http://arxiv.org/abs/2104.03006v2", "categories": ["cs.CL", "cs.AI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Learning robust speech representation with an articulatory-regularized\n  variational autoencoder", "abstract": "It is increasingly considered that human speech perception and production\nboth rely on articulatory representations. In this paper, we investigate\nwhether this type of representation could improve the performances of a deep\ngenerative model (here a variational autoencoder) trained to encode and decode\nacoustic speech features. First we develop an articulatory model able to\nassociate articulatory parameters describing the jaw, tongue, lips and velum\nconfigurations with vocal tract shapes and spectral features. Then we\nincorporate these articulatory parameters into a variational autoencoder\napplied on spectral features by using a regularization technique that\nconstraints part of the latent space to follow articulatory trajectories. We\nshow that this articulatory constraint improves model training by decreasing\ntime to convergence and reconstruction loss at convergence, and yields better\nperformance in a speech denoising task.", "published": "2021-04-07 15:47:04", "link": "http://arxiv.org/abs/2104.03204v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multimodal Entity Linking for Tweets", "abstract": "In many information extraction applications, entity linking (EL) has emerged\nas a crucial task that allows leveraging information about named entities from\na knowledge base. In this paper, we address the task of multimodal entity\nlinking (MEL), an emerging research field in which textual and visual\ninformation is used to map an ambiguous mention to an entity in a knowledge\nbase (KB). First, we propose a method for building a fully annotated Twitter\ndataset for MEL, where entities are defined in a Twitter KB. Then, we propose a\nmodel for jointly learning a representation of both mentions and entities from\ntheir textual and visual contexts. We demonstrate the effectiveness of the\nproposed model by evaluating it on the proposed dataset and highlight the\nimportance of leveraging visual information when it is available.", "published": "2021-04-07 16:40:23", "link": "http://arxiv.org/abs/2104.03236v1", "categories": ["cs.IR", "cs.CL", "cs.MM"], "primary_category": "cs.IR"}
{"title": "Pushing the Limits of Non-Autoregressive Speech Recognition", "abstract": "We combine recent advancements in end-to-end speech recognition to\nnon-autoregressive automatic speech recognition. We push the limits of\nnon-autoregressive state-of-the-art results for multiple datasets: LibriSpeech,\nFisher+Switchboard and Wall Street Journal. Key to our recipe, we leverage CTC\non giant Conformer neural network architectures with SpecAugment and wav2vec2\npre-training. We achieve 1.8%/3.6% WER on LibriSpeech test/test-other sets,\n5.1%/9.8% WER on Switchboard, and 3.4% on the Wall Street Journal, all without\na language model.", "published": "2021-04-07 22:17:20", "link": "http://arxiv.org/abs/2104.03416v4", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speak or Chat with Me: End-to-End Spoken Language Understanding System\n  with Flexible Inputs", "abstract": "A major focus of recent research in spoken language understanding (SLU) has\nbeen on the end-to-end approach where a single model can predict intents\ndirectly from speech inputs without intermediate transcripts. However, this\napproach presents some challenges. First, since speech can be considered as\npersonally identifiable information, in some cases only automatic speech\nrecognition (ASR) transcripts are accessible. Second, intent-labeled speech\ndata is scarce. To address the first challenge, we propose a novel system that\ncan predict intents from flexible types of inputs: speech, ASR transcripts, or\nboth. We demonstrate strong performance for either modality separately, and\nwhen both speech and ASR transcripts are available, through system combination,\nwe achieve better results than using a single input modality. To address the\nsecond challenge, we leverage a semantically robust pre-trained BERT model and\nadopt a cross-modal system that co-trains text embeddings and acoustic\nembeddings in a shared latent space. We further enhance this system by\nutilizing an acoustic module pre-trained on LibriSpeech and domain-adapting the\ntext module on our target datasets. Our experiments show significant advantages\nfor these pre-training and fine-tuning strategies, resulting in a system that\nachieves competitive intent-classification performance on Snips SLU and Fluent\nSpeech Commands datasets.", "published": "2021-04-07 20:48:08", "link": "http://arxiv.org/abs/2104.05752v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "HumAID: Human-Annotated Disaster Incidents Data from Twitter with Deep\n  Learning Benchmarks", "abstract": "Social networks are widely used for information consumption and\ndissemination, especially during time-critical events such as natural\ndisasters. Despite its significantly large volume, social media content is\noften too noisy for direct use in any application. Therefore, it is important\nto filter, categorize, and concisely summarize the available content to\nfacilitate effective consumption and decision-making. To address such issues\nautomatic classification systems have been developed using supervised modeling\napproaches, thanks to the earlier efforts on creating labeled datasets.\nHowever, existing datasets are limited in different aspects (e.g., size,\ncontains duplicates) and less suitable to support more advanced and data-hungry\ndeep learning models. In this paper, we present a new large-scale dataset with\n~77K human-labeled tweets, sampled from a pool of ~24 million tweets across 19\ndisaster events that happened between 2016 and 2019. Moreover, we propose a\ndata collection and sampling pipeline, which is important for social media data\nsampling for human annotation. We report multiclass classification results\nusing classic and deep learning (fastText and transformer) based models to set\nthe ground for future studies. The dataset and associated resources are\npublicly available. https://crisisnlp.qcri.org/humaid_dataset.html", "published": "2021-04-07 12:29:36", "link": "http://arxiv.org/abs/2104.03090v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Analysis of Twitter Users' Lifestyle Choices using Joint Embedding Model", "abstract": "Multiview representation learning of data can help construct coherent and\ncontextualized users' representations on social media. This paper suggests a\njoint embedding model, incorporating users' social and textual information to\nlearn contextualized user representations used for understanding their\nlifestyle choices. We apply our model to tweets related to two lifestyle\nactivities, `Yoga' and `Keto diet' and use it to analyze users' activity type\nand motivation. We explain the data collection and annotation process in detail\nand provide an in-depth analysis of users from different classes based on their\nTwitter content. Our experiments show that our model results in performance\nimprovements in both domains.", "published": "2021-04-07 15:29:36", "link": "http://arxiv.org/abs/2104.03189v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Darts-Conformer: Towards Efficient Gradient-Based Neural Architecture\n  Search For End-to-End ASR", "abstract": "Neural architecture search (NAS) has been successfully applied to tasks like\nimage classification and language modeling for finding efficient\nhigh-performance network architectures. In ASR field especially end-to-end ASR,\nthe related research is still in its infancy. In this work, we focus on\napplying NAS on the most popular manually designed model: Conformer, and then\npropose an efficient ASR model searching method that benefits from the natural\nadvantage of differentiable architecture search (Darts) in reducing\ncomputational overheads. We fuse Darts mutator and Conformer blocks to form a\ncomplete search space, within which a modified architecture called\nDarts-Conformer cell is found automatically. The entire searching process on\nAISHELL-1 dataset costs only 0.7 GPU days. Replacing the Conformer encoder by\nstacking searched cell, we get an end-to-end ASR model (named as\nDarts-Conformner) that outperforms the Conformer baseline by 4.7\\% on the\nopen-source AISHELL-1 dataset. Besides, we verify the transferability of the\narchitecture searched on a small dataset to a larger 2k-hour dataset. To the\nbest of our knowledge, this is the first successful attempt to apply\ngradient-based architecture search in the attention-based encoder-decoder ASR\nmodel.", "published": "2021-04-07 02:37:40", "link": "http://arxiv.org/abs/2104.02868v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "S2VC: A Framework for Any-to-Any Voice Conversion with Self-Supervised\n  Pretrained Representations", "abstract": "Any-to-any voice conversion (VC) aims to convert the timbre of utterances\nfrom and to any speakers seen or unseen during training. Various any-to-any VC\napproaches have been proposed like AUTOVC, AdaINVC, and FragmentVC. AUTOVC, and\nAdaINVC utilize source and target encoders to disentangle the content and\nspeaker information of the features. FragmentVC utilizes two encoders to encode\nsource and target information and adopts cross attention to align the source\nand target features with similar phonetic content. Moreover, pre-trained\nfeatures are adopted. AUTOVC used dvector to extract speaker information, and\nself-supervised learning (SSL) features like wav2vec 2.0 is used in FragmentVC\nto extract the phonetic content information. Different from previous works, we\nproposed S2VC that utilizes Self-Supervised features as both source and target\nfeatures for VC model. Supervised phoneme posteriororgram (PPG), which is\nbelieved to be speaker-independent and widely used in VC to extract content\ninformation, is chosen as a strong baseline for SSL features. The objective\nevaluation and subjective evaluation both show models taking SSL feature CPC as\nboth source and target features outperforms that taking PPG as source feature,\nsuggesting that SSL features have great potential in improving VC.", "published": "2021-04-07 03:55:53", "link": "http://arxiv.org/abs/2104.02901v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio declipping performance enhancement via crossfading", "abstract": "Some audio declipping methods produce waveforms that do not fully respect the\nphysical process of clipping, which is why we refer to them as inconsistent.\nThis letter reports what effect on perception it has if the solution by\ninconsistent methods is forced consistent by postprocessing. We first propose a\nsimple sample replacement method, then we identify its main weaknesses and\npropose an improved variant. The experiments show that the vast majority of\ninconsistent declipping methods significantly benefit from the proposed\napproach in terms of objective perceptual metrics. In particular, we show that\nthe SS PEW method based on social sparsity combined with the proposed method\nperforms comparable to top methods from the consistent class, but at a\ncomputational cost of one order of magnitude lower.", "published": "2021-04-07 11:44:15", "link": "http://arxiv.org/abs/2104.03074v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Capturing Multi-Resolution Context by Dilated Self-Attention", "abstract": "Self-attention has become an important and widely used neural network\ncomponent that helped to establish new state-of-the-art results for various\napplications, such as machine translation and automatic speech recognition\n(ASR). However, the computational complexity of self-attention grows\nquadratically with the input sequence length. This can be particularly\nproblematic for applications such as ASR, where an input sequence generated\nfrom an utterance can be relatively long. In this work, we propose a\ncombination of restricted self-attention and a dilation mechanism, which we\nrefer to as dilated self-attention. The restricted self-attention allows\nattention to neighboring frames of the query at a high resolution, and the\ndilation mechanism summarizes distant information to allow attending to it with\na lower resolution. Different methods for summarizing distant frames are\nstudied, such as subsampling, mean-pooling, and attention-based pooling. ASR\nresults demonstrate substantial improvements compared to restricted\nself-attention alone, achieving similar results compared to full-sequence based\nself-attention with a fraction of the computational costs.", "published": "2021-04-07 02:04:18", "link": "http://arxiv.org/abs/2104.02858v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Three-class Overlapped Speech Detection using a Convolutional Recurrent\n  Neural Network", "abstract": "In this work, we propose an overlapped speech detection system trained as a\nthree-class classifier. Unlike conventional systems that perform binary\nclassification as to whether or not a frame contains overlapped speech, the\nproposed approach classifies into three classes: non-speech, single speaker\nspeech, and overlapped speech. By training a network with the more detailed\nlabel definition, the model can learn a better notion on deciding the number of\nspeakers included in a given frame. A convolutional recurrent neural network\narchitecture is explored to benefit from both convolutional layer's capability\nto model local patterns and recurrent layer's ability to model sequential\ninformation. The proposed overlapped speech detection model establishes a\nstate-of-the-art performance with a precision of 0.6648 and a recall of 0.3222\non the DIHARD II evaluation set, showing a 20% increase in recall along with\nhigher precision. In addition, we also introduce a simple approach to utilize\nthe proposed overlapped speech detection model for speaker diarization which\nranked third place in the Track 1 of the DIHARD III challenge.", "published": "2021-04-07 03:01:34", "link": "http://arxiv.org/abs/2104.02878v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Adapting Speaker Embeddings for Speaker Diarisation", "abstract": "The goal of this paper is to adapt speaker embeddings for solving the problem\nof speaker diarisation. The quality of speaker embeddings is paramount to the\nperformance of speaker diarisation systems. Despite this, prior works in the\nfield have directly used embeddings designed only to be effective on the\nspeaker verification task. In this paper, we propose three techniques that can\nbe used to better adapt the speaker embeddings for diarisation: dimensionality\nreduction, attention-based embedding aggregation, and non-speech clustering. A\nwide range of experiments is performed on various challenging datasets. The\nresults demonstrate that all three techniques contribute positively to the\nperformance of the diarisation system achieving an average relative improvement\nof 25.07% in terms of diarisation error rate over the baseline.", "published": "2021-04-07 03:04:47", "link": "http://arxiv.org/abs/2104.02879v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Siamese Neural Network with Joint Bayesian Model Structure for Speaker\n  Verification", "abstract": "Generative probability models are widely used for speaker verification (SV).\nHowever, the generative models are lack of discriminative feature selection\nability. As a hypothesis test, the SV can be regarded as a binary\nclassification task which can be designed as a Siamese neural network (SiamNN)\nwith discriminative training. However, in most of the discriminative training\nfor SiamNN, only the distribution of pair-wised sample distances is considered,\nand the additional discriminative information in joint distribution of samples\nis ignored. In this paper, we propose a novel SiamNN with consideration of the\njoint distribution of samples. The joint distribution of samples is first\nformulated based on a joint Bayesian (JB) based generative model, then a SiamNN\nis designed with dense layers to approximate the factorized affine transforms\nas used in the JB model. By initializing the SiamNN with the learned model\nparameters of the JB model, we further train the model parameters with the\npair-wised samples as a binary discrimination task for SV. We carried out SV\nexperiments on data corpus of speakers in the wild (SITW) and VoxCeleb.\nExperimental results showed that our proposed model improved the performance\nwith a large margin compared with state of the art models for SV.", "published": "2021-04-07 09:17:29", "link": "http://arxiv.org/abs/2104.03004v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The AS-NU System for the M2VoC Challenge", "abstract": "This paper describes the AS-NU systems for two tracks in MultiSpeaker\nMulti-Style Voice Cloning Challenge (M2VoC). The first track focuses on using a\nsmall number of 100 target utterances for voice cloning, while the second track\nfocuses on using only 5 target utterances for voice cloning. Due to the serious\nlack of data in the second track, we selected the speaker most similar to the\ntarget speaker from the training data of the TTS system, and used the speaker's\nutterances and the given 5 target utterances to fine-tune our model. The\nevaluation results show that our systems on the two tracks perform similarly in\nterms of quality, but there is still a clear gap between the similarity score\nof the second track and the similarity score of the first track.", "published": "2021-04-07 09:26:20", "link": "http://arxiv.org/abs/2104.03009v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Utilizing Self-supervised Representations for MOS Prediction", "abstract": "Speech quality assessment has been a critical issue in speech processing for\ndecades. Existing automatic evaluations usually require clean references or\nparallel ground truth data, which is infeasible when the amount of data soars.\nSubjective tests, on the other hand, do not need any additional clean or\nparallel data and correlates better to human perception. However, such a test\nis expensive and time-consuming because crowd work is necessary. It thus\nbecomes highly desired to develop an automatic evaluation approach that\ncorrelates well with human perception while not requiring ground truth data. In\nthis paper, we use self-supervised pre-trained models for MOS prediction. We\nshow their representations can distinguish between clean and noisy audios.\nThen, we fine-tune these pre-trained models followed by simple linear layers in\nan end-to-end manner. The experiment results showed that our framework\noutperforms the two previous state-of-the-art models by a significant\nimprovement on Voice Conversion Challenge 2018 and achieves comparable or\nsuperior performance on Voice Conversion Challenge 2016. We also conducted an\nablation study to further investigate how each module benefits the task. The\nexperiment results are implemented and reproducible with publicly available\ntoolkits.", "published": "2021-04-07 09:44:36", "link": "http://arxiv.org/abs/2104.03017v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Partially-Connected Differentiable Architecture Search for Deepfake and\n  Spoofing Detection", "abstract": "This paper reports the first successful application of a differentiable\narchitecture search (DARTS) approach to the deepfake and spoofing detection\nproblems. An example of neural architecture search, DARTS operates upon a\ncontinuous, differentiable search space which enables both the architecture and\nparameters to be optimised via gradient descent. Solutions based on\npartially-connected DARTS use random channel masking in the search space to\nreduce GPU time and automatically learn and optimise complex neural\narchitectures composed of convolutional operations and residual blocks. Despite\nbeing learned quickly with little human effort, the resulting networks are\ncompetitive with the best performing systems reported in the literature. Some\nare also far less complex, containing 85% fewer parameters than a Res2Net\ncompetitor.", "published": "2021-04-07 13:53:20", "link": "http://arxiv.org/abs/2104.03123v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Contrastive Learning of Global-Local Video Representations", "abstract": "Contrastive learning has delivered impressive results for various tasks in\nthe self-supervised regime. However, existing approaches optimize for learning\nrepresentations specific to downstream scenarios, i.e., \\textit{global}\nrepresentations suitable for tasks such as classification or \\textit{local}\nrepresentations for tasks such as detection and localization. While they\nproduce satisfactory results in the intended downstream scenarios, they often\nfail to generalize to tasks that they were not originally designed for. In this\nwork, we propose to learn video representations that generalize to both the\ntasks which require global semantic information (e.g., classification) and the\ntasks that require local fine-grained spatio-temporal information (e.g.,\nlocalization). We achieve this by optimizing two contrastive objectives that\ntogether encourage our model to learn global-local visual information given\naudio signals. We show that the two objectives mutually improve the\ngeneralizability of the learned global-local representations, significantly\noutperforming their disjointly learned counterparts. We demonstrate our\napproach on various tasks including action/sound classification, lip reading,\ndeepfake detection, event and sound localization\n(https://github.com/yunyikristy/global\\_local).", "published": "2021-04-07 07:35:08", "link": "http://arxiv.org/abs/2104.05418v2", "categories": ["cs.LG", "cs.CV", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.LG"}
