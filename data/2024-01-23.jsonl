{"title": "Contrastive Learning in Distilled Models", "abstract": "Natural Language Processing models like BERT can provide state-of-the-art\nword embeddings for downstream NLP tasks. However, these models yet to perform\nwell on Semantic Textual Similarity, and may be too large to be deployed as\nlightweight edge applications. We seek to apply a suitable contrastive learning\nmethod based on the SimCSE paper, to a model architecture adapted from a\nknowledge distillation based model, DistilBERT, to address these two issues.\nOur final lightweight model DistilFace achieves an average of 72.1 in\nSpearman's correlation on STS tasks, a 34.2 percent improvement over BERT base.", "published": "2024-01-23 03:47:07", "link": "http://arxiv.org/abs/2401.12472v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Fact-Checking of Climate Change Claims with Large Language\n  Models", "abstract": "This paper presents Climinator, a novel AI-based tool designed to automate\nthe fact-checking of climate change claims. Utilizing an array of Large\nLanguage Models (LLMs) informed by authoritative sources like the IPCC reports\nand peer-reviewed scientific literature, Climinator employs an innovative\nMediator-Advocate framework. This design allows Climinator to effectively\nsynthesize varying scientific perspectives, leading to robust, evidence-based\nevaluations. Our model demonstrates remarkable accuracy when testing claims\ncollected from Climate Feedback and Skeptical Science. Notably, when\nintegrating an advocate with a climate science denial perspective in our\nframework, Climinator's iterative debate process reliably converges towards\nscientific consensus, underscoring its adeptness at reconciling diverse\nviewpoints into science-based, factual conclusions. While our research is\nsubject to certain limitations and necessitates careful interpretation, our\napproach holds significant potential. We hope to stimulate further research and\nencourage exploring its applicability in other contexts, including political\nfact-checking and legal domains.", "published": "2024-01-23 08:49:23", "link": "http://arxiv.org/abs/2401.12566v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SLANG: New Concept Comprehension of Large Language Models", "abstract": "The dynamic nature of language, particularly evident in the realm of slang\nand memes on the Internet, poses serious challenges to the adaptability of\nlarge language models (LLMs). Traditionally anchored to static datasets, these\nmodels often struggle to keep up with the rapid linguistic evolution\ncharacteristic of online communities. This research aims to bridge this gap by\nenhancing LLMs' comprehension of the evolving new concepts on the Internet,\nwithout the high cost of continual retraining. In pursuit of this goal, we\nintroduce $\\textbf{SLANG}$, a benchmark designed to autonomously integrate\nnovel data and assess LLMs' ability to comprehend emerging concepts, alongside\n$\\textbf{FOCUS}$, an approach uses causal inference to enhance LLMs to\nunderstand new phrases and their colloquial context. Our benchmark and approach\ninvolves understanding real-world instances of linguistic shifts, serving as\ncontextual beacons, to form more precise and contextually relevant connections\nbetween newly emerging expressions and their meanings. The empirical analysis\nshows that our causal inference-based approach outperforms the baseline methods\nin terms of precision and relevance in the comprehension of Internet slang and\nmemes.", "published": "2024-01-23 09:33:31", "link": "http://arxiv.org/abs/2401.12585v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context Matters: Pushing the Boundaries of Open-Ended Answer Generation\n  with Graph-Structured Knowledge Context", "abstract": "In the continuously advancing AI landscape, crafting context-rich and\nmeaningful responses via Large Language Models (LLMs) is essential. Researchers\nare becoming more aware of the challenges that LLMs with fewer parameters\nencounter when trying to provide suitable answers to open-ended questions. To\naddress these hurdles, the integration of cutting-edge strategies, augmentation\nof rich external domain knowledge to LLMs, offers significant improvements.\nThis paper introduces a novel framework that combines graph-driven context\nretrieval in conjunction to knowledge graphs based enhancement, honing the\nproficiency of LLMs, especially in domain specific community question answering\nplatforms like AskUbuntu, Unix, and ServerFault. We conduct experiments on\nvarious LLMs with different parameter sizes to evaluate their ability to ground\nknowledge and determine factual accuracy in answers to open-ended questions.\nOur methodology GraphContextGen consistently outperforms dominant text-based\nretrieval systems, demonstrating its robustness and adaptability to a larger\nnumber of use cases. This advancement highlights the importance of pairing\ncontext rich data retrieval with LLMs, offering a renewed approach to knowledge\nsourcing and generation in AI systems. We also show that, due to rich\ncontextual data retrieval, the crucial entities, along with the generated\nanswer, remain factually coherent with the gold answer.", "published": "2024-01-23 11:25:34", "link": "http://arxiv.org/abs/2401.12671v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Zero-shot Abstractive Explanations for Rumour Verification", "abstract": "The task of rumour verification in social media concerns assessing the\nveracity of a claim on the basis of conversation threads that result from it.\nWhile previous work has focused on predicting a veracity label, here we\nreformulate the task to generate model-centric free-text explanations of a\nrumour's veracity. The approach is model agnostic in that it generalises to any\nmodel. Here we propose a novel GNN-based rumour verification model. We follow a\nzero-shot approach by first applying post-hoc explainability methods to score\nthe most important posts within a thread and then we use these posts to\ngenerate informative explanations using opinion-guided summarisation. To\nevaluate the informativeness of the explanatory summaries, we exploit the\nfew-shot learning capabilities of a large language model (LLM). Our experiments\nshow that LLMs can have similar agreement to humans in evaluating summaries.\nImportantly, we show explanatory abstractive summaries are more informative and\nbetter reflect the predicted rumour veracity than just using the highest\nranking posts in the thread.", "published": "2024-01-23 12:29:37", "link": "http://arxiv.org/abs/2401.12713v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking LLMs via Uncertainty Quantification", "abstract": "The proliferation of open-source Large Language Models (LLMs) from various\ninstitutions has highlighted the urgent need for comprehensive evaluation\nmethods. However, current evaluation platforms, such as the widely recognized\nHuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty,\nwhich is vital for thoroughly assessing LLMs. To bridge this gap, we introduce\na new benchmarking approach for LLMs that integrates uncertainty\nquantification. Our examination involves nine LLMs (LLM series) spanning five\nrepresentative natural language processing tasks. Our findings reveal that: I)\nLLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs\nmay display greater uncertainty compared to their smaller counterparts; and\nIII) Instruction-finetuning tends to increase the uncertainty of LLMs. These\nresults underscore the significance of incorporating uncertainty in the\nevaluation of LLMs.", "published": "2024-01-23 14:29:17", "link": "http://arxiv.org/abs/2401.12794v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multicultural Name Recognition For Previously Unseen Names", "abstract": "State of the art Named Entity Recognition (NER) models have achieved an\nimpressive ability to extract common phrases from text that belong to labels\nsuch as location, organization, time, and person. However, typical NER systems\nthat rely on having seen a specific entity in their training data in order to\nlabel an entity perform poorly on rare or unseen entities ta in order to label\nan entity perform poorly on rare or unseen entities (Derczynski et al., 2017).\nThis paper attempts to improve recognition of person names, a diverse category\nthat can grow any time someone is born or changes their name. In order for\ndownstream tasks to not exhibit bias based on cultural background, a model\nshould perform well on names from a variety of backgrounds. In this paper I\nexperiment with the training data and input structure of an English Bi-LSTM\nname recognition model. I look at names from 103 countries to compare how well\nthe model performs on names from different cultures, specifically in the\ncontext of a downstream task where extracted names will be matched to\ninformation on file. I find that a model with combined character and word input\noutperforms word-only models and may improve on accuracy compared to classical\nNER models that are not geared toward identifying unseen entity values.", "published": "2024-01-23 17:58:38", "link": "http://arxiv.org/abs/2401.12941v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Raidar: geneRative AI Detection viA Rewriting", "abstract": "We find that large language models (LLMs) are more likely to modify\nhuman-written text than AI-generated text when tasked with rewriting. This\ntendency arises because LLMs often perceive AI-generated text as high-quality,\nleading to fewer modifications. We introduce a method to detect AI-generated\ncontent by prompting LLMs to rewrite text and calculating the editing distance\nof the output. We dubbed our geneRative AI Detection viA Rewriting method\nRaidar. Raidar significantly improves the F1 detection scores of existing AI\ncontent detection models -- both academic and commercial -- across various\ndomains, including News, creative writing, student essays, code, Yelp reviews,\nand arXiv papers, with gains of up to 29 points. Operating solely on word\nsymbols without high-dimensional features, our method is compatible with black\nbox LLMs, and is inherently robust on new content. Our results illustrate the\nunique imprint of machine-generated text through the lens of the machines\nthemselves.", "published": "2024-01-23 18:57:53", "link": "http://arxiv.org/abs/2401.12970v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CIM-MLC: A Multi-level Compilation Stack for Computing-In-Memory\n  Accelerators", "abstract": "In recent years, various computing-in-memory (CIM) processors have been\npresented, showing superior performance over traditional architectures. To\nunleash the potential of various CIM architectures, such as device precision,\ncrossbar size, and crossbar number, it is necessary to develop compilation\ntools that are fully aware of the CIM architectural details and implementation\ndiversity. However, due to the lack of architectural support in current popular\nopen-source compiling stacks, existing CIM designs either manually deploy\nnetworks or build their own compilers, which is time-consuming and\nlabor-intensive. Although some works expose the specific CIM device programming\ninterfaces to compilers, they are often bound to a fixed CIM architecture,\nlacking the flexibility to support the CIM architectures with different\ncomputing granularity. On the other hand, existing compilation works usually\nconsider the scheduling of limited operation types (such as crossbar-bound\nmatrix-vector multiplication). Unlike conventional processors, CIM accelerators\nare featured by their diverse architecture, circuit, and device, which cannot\nbe simply abstracted by a single level if we seek to fully explore the\nadvantages brought by CIM. Therefore, we propose CIM-MLC, a universal\nmulti-level compilation framework for general CIM architectures. We first\nestablish a general hardware abstraction for CIM architectures and computing\nmodes to represent various CIM accelerators. Based on the proposed abstraction,\nCIM-MLC can compile tasks onto a wide range of CIM accelerators having\ndifferent devices, architectures, and programming interfaces. More importantly,\ncompared with existing compilation work, CIM-MLC can explore the mapping and\nscheduling strategies across multiple architectural tiers, which form a\ntractable yet effective design space, to achieve better scheduling and\ninstruction generation results.", "published": "2024-01-23 01:33:09", "link": "http://arxiv.org/abs/2401.12428v2", "categories": ["cs.AR", "cs.CL", "D.3.4"], "primary_category": "cs.AR"}
{"title": "Fast Adversarial Training against Textual Adversarial Attacks", "abstract": "Many adversarial defense methods have been proposed to enhance the\nadversarial robustness of natural language processing models. However, most of\nthem introduce additional pre-set linguistic knowledge and assume that the\nsynonym candidates used by attackers are accessible, which is an ideal\nassumption. We delve into adversarial training in the embedding space and\npropose a Fast Adversarial Training (FAT) method to improve the model\nrobustness in the synonym-unaware scenario from the perspective of single-step\nperturbation generation and perturbation initialization. Based on the\nobservation that the adversarial perturbations crafted by single-step and\nmulti-step gradient ascent are similar, FAT uses single-step gradient ascent to\ncraft adversarial examples in the embedding space to expedite the training\nprocess. Based on the observation that the perturbations generated on the\nidentical training sample in successive epochs are similar, FAT fully utilizes\nhistorical information when initializing the perturbation. Extensive\nexperiments demonstrate that FAT significantly boosts the robustness of BERT\nmodels in the synonym-unaware scenario, and outperforms the defense baselines\nunder various attacks with character-level and word-level modifications.", "published": "2024-01-23 03:03:57", "link": "http://arxiv.org/abs/2401.12461v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models are Superpositions of All Characters: Attaining\n  Arbitrary Role-play via Self-Alignment", "abstract": "Considerable efforts have been invested in augmenting the role-playing\nproficiency of open-source large language models (LLMs) by emulating\nproprietary counterparts. Nevertheless, we posit that LLMs inherently harbor\nrole-play capabilities, owing to the extensive knowledge of characters and\npotential dialogues ingrained in their vast training corpora. Thus, in this\nstudy, we introduce Ditto, a self-alignment method for role-play. Ditto\ncapitalizes on character knowledge, encouraging an instruction-following LLM to\nsimulate role-play dialogues as a variant of reading comprehension. This method\ncreates a role-play training set comprising 4,000 characters, surpassing the\nscale of currently available datasets by tenfold regarding the number of roles.\nSubsequently, we fine-tune the LLM using this self-generated dataset to augment\nits role-playing capabilities. Upon evaluating our meticulously constructed and\nreproducible role-play benchmark and the roleplay subset of MT-Bench, Ditto, in\nvarious parameter scales, consistently maintains a consistent role identity and\nprovides accurate role-specific knowledge in multi-turn role-play\nconversations. Notably, it outperforms all open-source role-play baselines,\nshowcasing performance levels comparable to advanced proprietary chatbots.\nFurthermore, we present the first comprehensive cross-supervision alignment\nexperiment in the role-play domain, revealing that the intrinsic capabilities\nof LLMs confine the knowledge within role-play. Meanwhile, the role-play styles\ncan be easily acquired with the guidance of smaller models. We open-source\nrelated resources at https://github.com/OFA-Sys/Ditto.", "published": "2024-01-23 03:56:22", "link": "http://arxiv.org/abs/2401.12474v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Assessing and Understanding Creativity in Large Language Models", "abstract": "In the field of natural language processing, the rapid development of large\nlanguage model (LLM) has attracted more and more attention. LLMs have shown a\nhigh level of creativity in various tasks, but the methods for assessing such\ncreativity are inadequate. The assessment of LLM creativity needs to consider\ndifferences from humans, requiring multi-dimensional measurement while\nbalancing accuracy and efficiency. This paper aims to establish an efficient\nframework for assessing the level of creativity in LLMs. By adapting the\nmodified Torrance Tests of Creative Thinking, the research evaluates the\ncreative performance of various LLMs across 7 tasks, emphasizing 4 criteria\nincluding Fluency, Flexibility, Originality, and Elaboration. In this context,\nwe develop a comprehensive dataset of 700 questions for testing and an\nLLM-based evaluation method. In addition, this study presents a novel analysis\nof LLMs' responses to diverse prompts and role-play situations. We found that\nthe creativity of LLMs primarily falls short in originality, while excelling in\nelaboration. Besides, the use of prompts and the role-play settings of the\nmodel significantly influence creativity. Additionally, the experimental\nresults also indicate that collaboration among multiple LLMs can enhance\noriginality. Notably, our findings reveal a consensus between human evaluations\nand LLMs regarding the personality traits that influence creativity. The\nfindings underscore the significant impact of LLM design on creativity and\nbridges artificial intelligence and human creativity, offering insights into\nLLMs' creativity and potential applications.", "published": "2024-01-23 05:19:47", "link": "http://arxiv.org/abs/2401.12491v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DREditor: An Time-efficient Approach for Building a Domain-specific\n  Dense Retrieval Model", "abstract": "Deploying dense retrieval models efficiently is becoming increasingly\nimportant across various industries. This is especially true for enterprise\nsearch services, where customizing search engines to meet the time demands of\ndifferent enterprises in different domains is crucial. Motivated by this, we\ndevelop a time-efficient approach called DREditor to edit the matching rule of\nan off-the-shelf dense retrieval model to suit a specific domain. This is\nachieved by directly calibrating the output embeddings of the model using an\nefficient and effective linear mapping. This mapping is powered by an edit\noperator that is obtained by solving a specially constructed least squares\nproblem. Compared to implicit rule modification via long-time finetuning, our\nexperimental results show that DREditor provides significant advantages on\ndifferent domain-specific datasets, dataset sources, retrieval models, and\ncomputing devices. It consistently enhances time efficiency by 100-300 times\nwhile maintaining comparable or even superior retrieval performance. In a\nbroader context, we take the first step to introduce a novel embedding\ncalibration approach for the retrieval task, filling the technical blank in the\ncurrent field of embedding calibration. This approach also paves the way for\nbuilding domain-specific dense retrieval models efficiently and inexpensively.", "published": "2024-01-23 07:48:58", "link": "http://arxiv.org/abs/2401.12540v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Comprehensive View of the Biases of Toxicity and Sentiment Analysis\n  Methods Towards Utterances with African American English Expressions", "abstract": "Language is a dynamic aspect of our culture that changes when expressed in\ndifferent technologies/communities. Online social networks have enabled the\ndiffusion and evolution of different dialects, including African American\nEnglish (AAE). However, this increased usage is not without barriers. One\nparticular barrier is how sentiment (Vader, TextBlob, and Flair) and toxicity\n(Google's Perspective and the open-source Detoxify) methods present biases\ntowards utterances with AAE expressions. Consider Google's Perspective to\nunderstand bias. Here, an utterance such as ``All n*ggers deserve to die\nrespectfully. The police murder us.'' it reaches a higher toxicity than\n``African-Americans deserve to die respectfully. The police murder us.''. This\nscore difference likely arises because the tool cannot understand the\nre-appropriation of the term ``n*gger''. One explanation for this bias is that\nAI models are trained on limited datasets, and using such a term in training\ndata is more likely to appear in a toxic utterance. While this may be\nplausible, the tool will make mistakes regardless. Here, we study bias on two\nWeb-based (YouTube and Twitter) datasets and two spoken English datasets. Our\nanalysis shows how most models present biases towards AAE in most settings. We\nisolate the impact of AAE expression usage via linguistic control features from\nthe Linguistic Inquiry and Word Count (LIWC) software, grammatical control\nfeatures extracted via Part-of-Speech (PoS) tagging from Natural Language\nProcessing (NLP) models, and the semantic of utterances by comparing sentence\nembeddings from recent language models. We present consistent results on how a\nheavy usage of AAE expressions may cause the speaker to be considered\nsubstantially more toxic, even when speaking about nearly the same subject. Our\nstudy complements similar analyses focusing on small datasets and/or one method\nonly.", "published": "2024-01-23 12:41:03", "link": "http://arxiv.org/abs/2401.12720v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "What the Weight?! A Unified Framework for Zero-Shot Knowledge\n  Composition", "abstract": "The knowledge encapsulated in a model is the core factor determining its\nfinal performance on downstream tasks. Much research in NLP has focused on\nefficient methods for storing and adapting different types of knowledge, e.g.,\nin dedicated modularized structures, and on how to effectively combine these,\ne.g., by learning additional parameters. However, given the many possible\noptions, a thorough understanding of the mechanisms involved in these\ncompositions is missing, and hence it remains unclear which strategies to\nutilize. To address this research gap, we propose a novel framework for\nzero-shot module composition, which encompasses existing and some novel\nvariations for selecting, weighting, and combining parameter modules under a\nsingle unified notion. Focusing on the scenario of domain knowledge and adapter\nlayers, our framework provides a systematic unification of concepts, allowing\nus to conduct the first comprehensive benchmarking study of various zero-shot\nknowledge composition strategies. In particular, we test two module combination\nmethods and five selection and weighting strategies for their effectiveness and\nefficiency in an extensive experimental setup. Our results highlight the\nefficacy of ensembling but also hint at the power of simple though\noften-ignored weighting methods. Further in-depth analyses allow us to\nunderstand the role of weighting vs. top-k selection, and show that, to a\ncertain extent, the performance of adapter composition can even be predicted.", "published": "2024-01-23 13:35:47", "link": "http://arxiv.org/abs/2401.12756v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Gradient Flow of Energy: A General and Efficient Approach for Entity\n  Alignment Decoding", "abstract": "Entity alignment (EA), a pivotal process in integrating multi-source\nKnowledge Graphs (KGs), seeks to identify equivalent entity pairs across these\ngraphs. Most existing approaches regard EA as a graph representation learning\ntask, concentrating on enhancing graph encoders. However, the decoding process\nin EA - essential for effective operation and alignment accuracy - has received\nlimited attention and remains tailored to specific datasets and model\narchitectures, necessitating both entity and additional explicit relation\nembeddings. This specificity limits its applicability, particularly in\nGNN-based models. To address this gap, we introduce a novel, generalized, and\nefficient decoding approach for EA, relying solely on entity embeddings. Our\nmethod optimizes the decoding process by minimizing Dirichlet energy, leading\nto the gradient flow within the graph, to maximize graph homophily. The\ndiscretization of the gradient flow produces a fast and scalable approach,\ntermed Triple Feature Propagation (TFP). TFP innovatively generalizes adjacency\nmatrices to multi-views matrices:entity-to-entity, entity-to-relation,\nrelation-to-entity, and relation-to-triple. The gradient flow through\ngeneralized matrices enables TFP to harness the multi-view structural\ninformation of KGs. Rigorous experimentation on diverse public datasets\ndemonstrates that our approach significantly enhances various EA methods.\nNotably, the approach achieves these advancements with less than 6 seconds of\nadditional computational time, establishing a new benchmark in efficiency and\nadaptability for future EA methods.", "published": "2024-01-23 14:31:12", "link": "http://arxiv.org/abs/2401.12798v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning", "abstract": "Large Language Models (LLMs) have demonstrated impressive performance in\nnatural language processing tasks by leveraging chain of thought (CoT) that\nenables step-by-step thinking. Extending LLMs with multimodal capabilities is\nthe recent interest, but incurs computational cost and requires substantial\nhardware resources. To address these challenges, we propose KAM-CoT a framework\nthat integrates CoT reasoning, Knowledge Graphs (KGs), and multiple modalities\nfor a comprehensive understanding of multimodal tasks. KAM-CoT adopts a\ntwo-stage training process with KG grounding to generate effective rationales\nand answers. By incorporating external knowledge from KGs during reasoning, the\nmodel gains a deeper contextual understanding reducing hallucinations and\nenhancing the quality of answers. This knowledge-augmented CoT reasoning\nempowers the model to handle questions requiring external context, providing\nmore informed answers. Experimental findings show KAM-CoT outperforms the\nstate-of-the-art methods. On the ScienceQA dataset, we achieve an average\naccuracy of 93.87%, surpassing GPT-3.5 (75.17%) by 18% and GPT-4 (83.99%) by\n10%. Remarkably, KAM-CoT achieves these results with only 280M trainable\nparameters at a time, demonstrating its cost-efficiency and effectiveness.", "published": "2024-01-23 15:56:11", "link": "http://arxiv.org/abs/2401.12863v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Machine Translation with Human Feedback: An Exploration of\n  Quality Estimation as a Reward Model", "abstract": "Insufficient modeling of human preferences within the reward model is a major\nobstacle for leveraging human feedback to improve translation quality.\nFortunately, quality estimation (QE), which predicts the quality of a given\ntranslation without reference, has achieved impressive alignment with human\nevaluations in the last two years. In this work, we investigate the potential\nof employing the QE model as the reward model to predict human preferences for\nfeedback training. We first identify the overoptimization problem during\nQE-based feedback training, manifested as an increase in reward while\ntranslation quality declines. We examine the problem and argue that the\nvulnerability of the QE model might lead to high rewards for incorrect\ntranslations, resulting in overoptimization and error propagation. To address\nthe problem, we adopt a simple yet effective method that uses heuristic rules\nto detect the incorrect translations and assigns a penalty term to the reward\nscores of them. Experimental results show that the proposed QE-based feedback\ntraining achieves consistent and significant improvements across various\nsettings, further verified through human preference studies. Our subsequent\nanalysis demonstrates the high data efficiency of the proposed QE-based\nfeedback training: it outperforms systems using larger parallel corpora by a\nsmall amount of monolingual data. Our code is available at:\nhttps://github.com/zwhe99/FeedbackMT", "published": "2024-01-23 16:07:43", "link": "http://arxiv.org/abs/2401.12873v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Understanding to Utilization: A Survey on Explainability for Large\n  Language Models", "abstract": "Explainability for Large Language Models (LLMs) is a critical yet challenging\naspect of natural language processing. As LLMs are increasingly integral to\ndiverse applications, their \"black-box\" nature sparks significant concerns\nregarding transparency and ethical use. This survey underscores the imperative\nfor increased explainability in LLMs, delving into both the research on\nexplainability and the various methodologies and tasks that utilize an\nunderstanding of these models. Our focus is primarily on pre-trained\nTransformer-based LLMs, such as LLaMA family, which pose distinctive\ninterpretability challenges due to their scale and complexity. In terms of\nexisting methods, we classify them into local and global analyses, based on\ntheir explanatory objectives. When considering the utilization of\nexplainability, we explore several compelling methods that concentrate on model\nediting, control generation, and model enhancement. Additionally, we examine\nrepresentative evaluation metrics and datasets, elucidating their advantages\nand limitations. Our goal is to reconcile theoretical and empirical\nunderstanding with practical implementation, proposing exciting avenues for\nexplanatory techniques and their applications in the LLMs era.", "published": "2024-01-23 16:09:53", "link": "http://arxiv.org/abs/2401.12874v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "In-Context Language Learning: Architectures and Algorithms", "abstract": "Large-scale neural language models exhibit a remarkable capacity for\nin-context learning (ICL): they can infer novel functions from datasets\nprovided as input. Most of our current understanding of when and how ICL arises\ncomes from LMs trained on extremely simple learning problems like linear\nregression and associative recall. There remains a significant gap between\nthese model problems and the \"real\" ICL exhibited by LMs trained on large text\ncorpora, which involves not just retrieval and function approximation but\nfree-form generation of language and other structured outputs. In this paper,\nwe study ICL through the lens of a new family of model problems we term in\ncontext language learning (ICLL). In ICLL, LMs are presented with a set of\nstrings from a formal language, and must generate additional strings from the\nsame language. We focus on in-context learning of regular languages generated\nby random finite automata. We evaluate a diverse set of neural sequence models\n(including several RNNs, Transformers, and state-space model variants) on\nregular ICLL tasks, aiming to answer three questions: (1) Which model classes\nare empirically capable of ICLL? (2) What algorithmic solutions do successful\nmodels implement to perform ICLL? (3) What architectural changes can improve\nICLL in less performant models? We first show that Transformers significantly\noutperform neural sequence models with recurrent or convolutional\nrepresentations on ICLL tasks. Next, we provide evidence that their ability to\ndo so relies on specialized \"n-gram heads\" (higher-order variants of induction\nheads) that compute input-conditional next-token distributions. Finally, we\nshow that hard-wiring these heads into neural models improves performance not\njust on ICLL, but natural language modeling -- improving the perplexity of\n340M-parameter models by up to 1.14 points (6.7%) on the SlimPajama dataset.", "published": "2024-01-23 18:59:21", "link": "http://arxiv.org/abs/2401.12973v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TCE at Qur'an QA 2023 Shared Task: Low Resource Enhanced\n  Transformer-based Ensemble Approach for Qur'anic QA", "abstract": "In this paper, we present our approach to tackle Qur'an QA 2023 shared tasks\nA and B. To address the challenge of low-resourced training data, we rely on\ntransfer learning together with a voting ensemble to improve prediction\nstability across multiple runs. Additionally, we employ different architectures\nand learning mechanisms for a range of Arabic pre-trained transformer-based\nmodels for both tasks. To identify unanswerable questions, we propose using a\nthresholding mechanism. Our top-performing systems greatly surpass the baseline\nperformance on the hidden split, achieving a MAP score of 25.05% for task A and\na partial Average Precision (pAP) of 57.11% for task B.", "published": "2024-01-23 19:32:54", "link": "http://arxiv.org/abs/2401.13060v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Seed-Guided Fine-Grained Entity Typing in Science and Engineering\n  Domains", "abstract": "Accurately typing entity mentions from text segments is a fundamental task\nfor various natural language processing applications. Many previous approaches\nrely on massive human-annotated data to perform entity typing. Nevertheless,\ncollecting such data in highly specialized science and engineering domains\n(e.g., software engineering and security) can be time-consuming and costly,\nwithout mentioning the domain gaps between training and inference data if the\nmodel needs to be applied to confidential datasets. In this paper, we study the\ntask of seed-guided fine-grained entity typing in science and engineering\ndomains, which takes the name and a few seed entities for each entity type as\nthe only supervision and aims to classify new entity mentions into both seen\nand unseen types (i.e., those without seed entities). To solve this problem, we\npropose SEType which first enriches the weak supervision by finding more\nentities for each seen type from an unlabeled corpus using the contextualized\nrepresentations of pre-trained language models. It then matches the enriched\nentities to unlabeled text to get pseudo-labeled samples and trains a textual\nentailment model that can make inferences for both seen and unseen types.\nExtensive experiments on two datasets covering four domains demonstrate the\neffectiveness of SEType in comparison with various baselines.", "published": "2024-01-23 22:36:03", "link": "http://arxiv.org/abs/2401.13129v2", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Analyzing COVID-19 Vaccination Sentiments in Nigerian Cyberspace:\n  Insights from a Manually Annotated Twitter Dataset", "abstract": "Numerous successes have been achieved in combating the COVID-19 pandemic,\ninitially using various precautionary measures like lockdowns, social\ndistancing, and the use of face masks. More recently, various vaccinations have\nbeen developed to aid in the prevention or reduction of the severity of the\nCOVID-19 infection. Despite the effectiveness of the precautionary measures and\nthe vaccines, there are several controversies that are massively shared on\nsocial media platforms like Twitter. In this paper, we explore the use of\nstate-of-the-art transformer-based language models to study people's acceptance\nof vaccines in Nigeria. We developed a novel dataset by crawling multi-lingual\ntweets using relevant hashtags and keywords. Our analysis and visualizations\nrevealed that most tweets expressed neutral sentiments about COVID-19 vaccines,\nwith some individuals expressing positive views, and there was no strong\npreference for specific vaccine types, although Moderna received slightly more\npositive sentiment. We also found out that fine-tuning a pre-trained LLM with\nan appropriate dataset can yield competitive results, even if the LLM was not\ninitially pre-trained on the specific language of that dataset.", "published": "2024-01-23 22:49:19", "link": "http://arxiv.org/abs/2401.13133v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "The Language Barrier: Dissecting Safety Challenges of LLMs in\n  Multilingual Contexts", "abstract": "As the influence of large language models (LLMs) spans across global\ncommunities, their safety challenges in multilingual settings become paramount\nfor alignment research. This paper examines the variations in safety challenges\nfaced by LLMs across different languages and discusses approaches to\nalleviating such concerns. By comparing how state-of-the-art LLMs respond to\nthe same set of malicious prompts written in higher- vs. lower-resource\nlanguages, we observe that (1) LLMs tend to generate unsafe responses much more\noften when a malicious prompt is written in a lower-resource language, and (2)\nLLMs tend to generate more irrelevant responses to malicious prompts in\nlower-resource languages. To understand where the discrepancy can be\nattributed, we study the effect of instruction tuning with reinforcement\nlearning from human feedback (RLHF) or supervised finetuning (SFT) on the\nHH-RLHF dataset. Surprisingly, while training with high-resource languages\nimproves model alignment, training in lower-resource languages yields minimal\nimprovement. This suggests that the bottleneck of cross-lingual alignment is\nrooted in the pretraining stage. Our findings highlight the challenges in\ncross-lingual LLM safety, and we hope they inform future research in this\ndirection.", "published": "2024-01-23 23:12:09", "link": "http://arxiv.org/abs/2401.13136v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Linguistic-Based Mild Cognitive Impairment Detection Using Informative\n  Loss", "abstract": "This paper presents a deep learning method using Natural Language Processing\n(NLP) techniques, to distinguish between Mild Cognitive Impairment (MCI) and\nNormal Cognitive (NC) conditions in older adults. We propose a framework that\nanalyzes transcripts generated from video interviews collected within the\nI-CONECT study project, a randomized controlled trial aimed at improving\ncognitive functions through video chats. Our proposed NLP framework consists of\ntwo Transformer-based modules, namely Sentence Embedding (SE) and Sentence\nCross Attention (SCA). First, the SE module captures contextual relationships\nbetween words within each sentence. Subsequently, the SCA module extracts\ntemporal features from a sequence of sentences. This feature is then used by a\nMulti-Layer Perceptron (MLP) for the classification of subjects into MCI or NC.\nTo build a robust model, we propose a novel loss function, called InfoLoss,\nthat considers the reduction in entropy by observing each sequence of sentences\nto ultimately enhance the classification accuracy. The results of our\ncomprehensive model evaluation using the I-CONECT dataset show that our\nframework can distinguish between MCI and NC with an average area under the\ncurve of 84.75%.", "published": "2024-01-23 16:30:22", "link": "http://arxiv.org/abs/2402.01690v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Maximizing Data Efficiency for Cross-Lingual TTS Adaptation by\n  Self-Supervised Representation Mixing and Embedding Initialization", "abstract": "This paper presents an effective transfer learning framework for language\nadaptation in text-to-speech systems, with a focus on achieving language\nadaptation using minimal labeled and unlabeled data. While many works focus on\nreducing the usage of labeled data, very few consider minimizing the usage of\nunlabeled data. By utilizing self-supervised features in the pretraining stage,\nreplacing the noisy portion of pseudo labels with these features during\nfine-tuning, and incorporating an embedding initialization trick, our method\nleverages more information from unlabeled data compared to conventional\napproaches. Experimental results show that our framework is able to synthesize\nintelligible speech in unseen languages with only 4 utterances of labeled data\nand 15 minutes of unlabeled data. Our methodology continues to surpass\nconventional techniques, even when a greater volume of data is accessible.\nThese findings highlight the potential of our data-efficient language\nadaptation framework.", "published": "2024-01-23 21:55:34", "link": "http://arxiv.org/abs/2402.01692v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Quality of Answers of Generative Large Language Models vs Peer Patients\n  for Interpreting Lab Test Results for Lay Patients: Evaluation Study", "abstract": "Lab results are often confusing and hard to understand. Large language models\n(LLMs) such as ChatGPT have opened a promising avenue for patients to get their\nquestions answered. We aim to assess the feasibility of using LLMs to generate\nrelevant, accurate, helpful, and unharmful responses to lab test-related\nquestions asked by patients and to identify potential issues that can be\nmitigated with augmentation approaches. We first collected lab test results\nrelated question and answer data from Yahoo! Answers and selected 53 QA pairs\nfor this study. Using the LangChain framework and ChatGPT web portal, we\ngenerated responses to the 53 questions from four LLMs including GPT-4, Meta\nLLaMA 2, MedAlpaca, and ORCA_mini. We first assessed the similarity of their\nanswers using standard QA similarity-based evaluation metrics including ROUGE,\nBLEU, METEOR, BERTScore. We also utilized an LLM-based evaluator to judge\nwhether a target model has higher quality in terms of relevance, correctness,\nhelpfulness, and safety than the baseline model. Finally, we performed a manual\nevaluation with medical experts for all the responses to seven selected\nquestions on the same four aspects. The results of Win Rate and medical expert\nevaluation both showed that GPT-4's responses achieved better scores than all\nthe other LLM responses and human responses on all four aspects (relevance,\ncorrectness, helpfulness, and safety). However, LLM responses occasionally also\nsuffer from a lack of interpretation in one's medical context, incorrect\nstatements, and lack of references. We find that compared to other three LLMs\nand human answer from the Q&A website, GPT-4's responses are more accurate,\nhelpful, relevant, and safer. However, there are cases which GPT-4 responses\nare inaccurate and not individualized. We identified a number of ways to\nimprove the quality of LLM responses.", "published": "2024-01-23 22:03:51", "link": "http://arxiv.org/abs/2402.01693v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Neglected Tails in Vision-Language Models", "abstract": "Vision-language models (VLMs) excel in zero-shot recognition but their\nperformance varies greatly across different visual concepts. For example,\nalthough CLIP achieves impressive accuracy on ImageNet (60-80%), its\nperformance drops below 10% for more than ten concepts like night snake,\npresumably due to their limited presence in the pretraining data. However,\nmeasuring the frequency of concepts in VLMs' large-scale datasets is\nchallenging. We address this by using large language models (LLMs) to count the\nnumber of pretraining texts that contain synonyms of these concepts. Our\nanalysis confirms that popular datasets, such as LAION, exhibit a long-tailed\nconcept distribution, yielding biased performance in VLMs. We also find that\ndownstream applications of VLMs, including visual chatbots (e.g., GPT-4V) and\ntext-to-image models (e.g., Stable Diffusion), often fail to recognize or\ngenerate images of rare concepts identified by our method. To mitigate the\nimbalanced performance of zero-shot VLMs, we propose REtrieval-Augmented\nLearning (REAL). First, instead of prompting VLMs using the original class\nnames, REAL uses their most frequent synonyms found in pretraining texts. This\nsimple change already outperforms costly human-engineered and LLM-enriched\nprompts over nine benchmark datasets. Second, REAL trains a linear classifier\non a small yet balanced set of pretraining data retrieved using concept\nsynonyms. REAL surpasses the previous zero-shot SOTA, using 400x less storage\nand 10,000x less training time!", "published": "2024-01-23 01:25:00", "link": "http://arxiv.org/abs/2401.12425v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Comparing Pre-trained Human Language Models: Is it Better with Human\n  Context as Groups, Individual Traits, or Both?", "abstract": "Pre-trained language models consider the context of neighboring words and\ndocuments but lack any author context of the human generating the text.\nHowever, language depends on the author's states, traits, social, situational,\nand environmental attributes, collectively referred to as human context (Soni\net al., 2024). Human-centered natural language processing requires\nincorporating human context into language models. Currently, two methods exist:\npre-training with 1) group-wise attributes (e.g., over-45-year-olds) or 2)\nindividual traits. Group attributes are simple but coarse -- not all\n45-year-olds write the same way -- while individual traits allow for more\npersonalized representations, but require more complex modeling and data. It is\nunclear which approach benefits what tasks. We compare pre-training models with\nhuman context via 1) group attributes, 2) individual users, and 3) a combined\napproach on five user- and document-level tasks. Our results show that there is\nno best approach, but that human-centered language modeling holds avenues for\ndifferent methods.", "published": "2024-01-23 05:20:35", "link": "http://arxiv.org/abs/2401.12492v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Key Information Retrieval to Classify the Unstructured Data Content of\n  Preferential Trade Agreements", "abstract": "With the rapid proliferation of textual data, predicting long texts has\nemerged as a significant challenge in the domain of natural language\nprocessing. Traditional text prediction methods encounter substantial\ndifficulties when grappling with long texts, primarily due to the presence of\nredundant and irrelevant information, which impedes the model's capacity to\ncapture pivotal insights from the text. To address this issue, we introduce a\nnovel approach to long-text classification and prediction. Initially, we employ\nembedding techniques to condense the long texts, aiming to diminish the\nredundancy therein. Subsequently,the Bidirectional Encoder Representations from\nTransformers (BERT) embedding method is utilized for text classification\ntraining. Experimental outcomes indicate that our method realizes considerable\nperformance enhancements in classifying long texts of Preferential Trade\nAgreements. Furthermore, the condensation of text through embedding methods not\nonly augments prediction accuracy but also substantially reduces computational\ncomplexity. Overall, this paper presents a strategy for long-text prediction,\noffering a valuable reference for researchers and engineers in the natural\nlanguage processing sphere.", "published": "2024-01-23 06:30:05", "link": "http://arxiv.org/abs/2401.12520v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language\n  Models", "abstract": "Large language models (LLMs) commonly employ autoregressive generation during\ninference, leading to high memory bandwidth demand and consequently extended\nlatency. To mitigate this inefficiency, we present Bi-directional Tuning for\nlossless Acceleration (BiTA), an innovative method expediting LLMs via\nstreamlined semi-autoregressive generation and draft verification. Inspired by\nthe concept of prompt tuning, we enhance LLMs with a parameter-efficient design\ncalled bi-directional tuning for the capability in semi-autoregressive\ngeneration. Employing efficient tree-based decoding, the models perform draft\ncandidate generation and verification in parallel, ensuring outputs identical\nto their autoregressive counterparts under greedy sampling. BiTA serves as a\nlightweight plug-in module, seamlessly boosting the inference efficiency of\nexisting LLMs without requiring additional assistance models or incurring\nsignificant extra memory costs. Applying the proposed BiTA, LLaMA-2-70B-Chat\nachieves a 2.7$\\times$ speedup on the MT-Bench benchmark. Extensive experiments\nconfirm our method surpasses state-of-the-art acceleration techniques.", "published": "2024-01-23 06:36:49", "link": "http://arxiv.org/abs/2401.12522v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLMCheckup: Conversational Examination of Large Language Models via\n  Interpretability Tools and Self-Explanations", "abstract": "Interpretability tools that offer explanations in the form of a dialogue have\ndemonstrated their efficacy in enhancing users' understanding (Slack et al.,\n2023; Shen et al., 2023), as one-off explanations may fall short in providing\nsufficient information to the user. Current solutions for dialogue-based\nexplanations, however, often require external tools and modules and are not\neasily transferable to tasks they were not designed for. With LLMCheckup, we\npresent an easily accessible tool that allows users to chat with any\nstate-of-the-art large language model (LLM) about its behavior. We enable LLMs\nto generate explanations and perform user intent recognition without\nfine-tuning, by connecting them with a broad spectrum of Explainable AI (XAI)\nmethods, including white-box explainability tools such as feature attributions,\nand self-explanations (e.g., for rationale generation). LLM-based\n(self-)explanations are presented as an interactive dialogue that supports\nfollow-up questions and generates suggestions. LLMCheckupprovides tutorials for\noperations available in the system, catering to individuals with varying levels\nof expertise in XAI and supporting multiple input modalities. We introduce a\nnew parsing strategy that substantially enhances the user intent recognition\naccuracy of the LLM. Finally, we showcase LLMCheckup for the tasks of fact\nchecking and commonsense question answering.", "published": "2024-01-23 09:11:07", "link": "http://arxiv.org/abs/2401.12576v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Reply to Makelov et al. (2023)'s \"Interpretability Illusion\" Arguments", "abstract": "We respond to the recent paper by Makelov et al. (2023), which reviews\nsubspace interchange intervention methods like distributed alignment search\n(DAS; Geiger et al. 2023) and claims that these methods potentially cause\n\"interpretability illusions\". We first review Makelov et al. (2023)'s technical\nnotion of what an \"interpretability illusion\" is, and then we show that even\nintuitive and desirable explanations can qualify as illusions in this sense. As\na result, their method of discovering \"illusions\" can reject explanations they\nconsider \"non-illusory\". We then argue that the illusions Makelov et al. (2023)\nsee in practice are artifacts of their training and evaluation paradigms. We\nclose by emphasizing that, though we disagree with their core characterization,\nMakelov et al. (2023)'s examples and discussion have undoubtedly pushed the\nfield of interpretability forward.", "published": "2024-01-23 10:27:42", "link": "http://arxiv.org/abs/2401.12631v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Energy-based Automated Model Evaluation", "abstract": "The conventional evaluation protocols on machine learning models rely heavily\non a labeled, i.i.d-assumed testing dataset, which is not often present in real\nworld applications. The Automated Model Evaluation (AutoEval) shows an\nalternative to this traditional workflow, by forming a proximal prediction\npipeline of the testing performance without the presence of ground-truth\nlabels. Despite its recent successes, the AutoEval frameworks still suffer from\nan overconfidence issue, substantial storage and computational cost. In that\nregard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that\nallows the AutoEval framework to be both more efficient and effective. The core\nof the MDE is to establish a meta-distribution statistic, on the information\n(energy) associated with individual samples, then offer a smoother\nrepresentation enabled by energy-based learning. We further provide our\ntheoretical insights by connecting the MDE with the classification loss. We\nprovide extensive experiments across modalities, datasets and different\narchitectural backbones to validate MDE's validity, together with its\nsuperiority compared with prior approaches. We also prove MDE's versatility by\nshowing its seamless integration with large-scale models, and easy adaption to\nlearning scenarios with noisy- or imbalanced- labels. Code and data are\navailable: https://github.com/pengr/Energy_AutoEval", "published": "2024-01-23 11:54:09", "link": "http://arxiv.org/abs/2401.12689v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Multilingual and Fully Non-Autoregressive ASR with Large Language Model\n  Fusion: A Comprehensive Study", "abstract": "In the era of large models, the autoregressive nature of decoding often\nresults in latency serving as a significant bottleneck. We propose a\nnon-autoregressive LM-fused ASR system that effectively leverages the\nparallelization capabilities of accelerator hardware. Our approach combines the\nUniversal Speech Model (USM) and the PaLM 2 language model in per-segment\nscoring mode, achieving an average relative WER improvement across all\nlanguages of 10.8% on FLEURS and 3.6% on YouTube captioning. Furthermore, our\ncomprehensive ablation study analyzes key parameters such as LLM size, context\nlength, vocabulary size, fusion methodology. For instance, we explore the\nimpact of LLM size ranging from 128M to 340B parameters on ASR performance.\nThis study provides valuable insights into the factors influencing the\neffectiveness of practical large-scale LM-fused speech recognition systems.", "published": "2024-01-23 14:19:01", "link": "http://arxiv.org/abs/2401.12789v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Red Teaming Visual Language Models", "abstract": "VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language\nModels) to accept multimodal inputs. Since it has been verified that LLMs can\nbe induced to generate harmful or inaccurate content through specific test\ncases (termed as Red Teaming), how VLMs perform in similar scenarios,\nespecially with their combination of textual and visual inputs, remains a\nquestion. To explore this problem, we present a novel red teaming dataset\nRTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal\njail-breaking, face fairness, etc) under 4 primary aspects (faithfulness,\nprivacy, safety, fairness). Our RTVLM is the first red-teaming dataset to\nbenchmark current VLMs in terms of these 4 different aspects. Detailed analysis\nshows that 10 prominent open-sourced VLMs struggle with the red teaming in\ndifferent degrees and have up to 31% performance gap with GPT-4V. Additionally,\nwe simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning\n(SFT) using RTVLM, and this bolsters the models' performance with 10% in RTVLM\ntest set, 13% in MM-Hal, and without noticeable decline in MM-Bench,\noverpassing other LLaVA-based models with regular alignment data. This reveals\nthat current open-sourced VLMs still lack red teaming alignment. Our code and\ndatasets will be open-source.", "published": "2024-01-23 17:07:18", "link": "http://arxiv.org/abs/2401.12915v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding", "abstract": "We introduce meta-prompting, an effective scaffolding technique designed to\nenhance the functionality of language models (LMs). This approach transforms a\nsingle LM into a multi-faceted conductor, adept at managing and integrating\nmultiple independent LM queries. By employing high-level instructions,\nmeta-prompting guides the LM to break down complex tasks into smaller, more\nmanageable subtasks. These subtasks are then handled by distinct \"expert\"\ninstances of the same LM, each operating under specific, tailored instructions.\nCentral to this process is the LM itself, in its role as the conductor, which\nensures seamless communication and effective integration of the outputs from\nthese expert models. It additionally employs its inherent critical thinking and\nrobust verification processes to refine and authenticate the end result. This\ncollaborative prompting approach empowers a single LM to simultaneously act as\na comprehensive orchestrator and a panel of diverse experts, significantly\nenhancing its performance across a wide array of tasks. The zero-shot,\ntask-agnostic nature of meta-prompting greatly simplifies user interaction by\nobviating the need for detailed, task-specific instructions. Furthermore, our\nresearch demonstrates the seamless integration of external tools, such as a\nPython interpreter, into the meta-prompting framework, thereby broadening its\napplicability and utility. Through rigorous experimentation with GPT-4, we\nestablish the superiority of meta-prompting over conventional scaffolding\nmethods: When averaged across all tasks, including the Game of 24,\nCheckmate-in-One, and Python Programming Puzzles, meta-prompting, augmented\nwith a Python interpreter functionality, surpasses standard prompting by 17.1%,\nexpert (dynamic) prompting by 17.3%, and multipersona prompting by 15.2%.", "published": "2024-01-23 18:22:19", "link": "http://arxiv.org/abs/2401.12954v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "HAZARD Challenge: Embodied Decision Making in Dynamically Changing\n  Environments", "abstract": "Recent advances in high-fidelity virtual environments serve as one of the\nmajor driving forces for building intelligent embodied agents to perceive,\nreason and interact with the physical world. Typically, these environments\nremain unchanged unless agents interact with them. However, in real-world\nscenarios, agents might also face dynamically changing environments\ncharacterized by unexpected events and need to rapidly take action accordingly.\nTo remedy this gap, we propose a new simulated embodied benchmark, called\nHAZARD, specifically designed to assess the decision-making abilities of\nembodied agents in dynamic situations. HAZARD consists of three unexpected\ndisaster scenarios, including fire, flood, and wind, and specifically supports\nthe utilization of large language models (LLMs) to assist common sense\nreasoning and decision-making. This benchmark enables us to evaluate autonomous\nagents' decision-making capabilities across various pipelines, including\nreinforcement learning (RL), rule-based, and search-based methods in\ndynamically changing environments. As a first step toward addressing this\nchallenge using large language models, we further develop an LLM-based agent\nand perform an in-depth analysis of its promise and challenge of solving these\nchallenging tasks. HAZARD is available at https://vis-www.cs.umass.edu/hazard/.", "published": "2024-01-23 18:59:43", "link": "http://arxiv.org/abs/2401.12975v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "IndiText Boost: Text Augmentation for Low Resource India Languages", "abstract": "Text Augmentation is an important task for low-resource languages. It helps\ndeal with the problem of data scarcity. A data augmentation strategy is used to\ndeal with the problem of data scarcity. Through the years, much work has been\ndone on data augmentation for the English language. In contrast, very less work\nhas been done on Indian languages. This is contrary to the fact that data\naugmentation is used to deal with data scarcity. In this work, we focus on\nimplementing techniques like Easy Data Augmentation, Back Translation,\nParaphrasing, Text Generation using LLMs, and Text Expansion using LLMs for\ntext classification on different languages. We focus on 6 Indian languages\nnamely: Sindhi, Marathi, Hindi, Gujarati, Telugu, and Sanskrit. According to\nour knowledge, no such work exists for text augmentation on Indian languages.\nWe carry out binary as well as multi-class text classification to make our\nresults more comparable. We get surprising results as basic data augmentation\ntechniques surpass LLMs.", "published": "2024-01-23 20:54:40", "link": "http://arxiv.org/abs/2401.13085v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Trustable Language Models: Investigating Information Quality of\n  Large Language Models", "abstract": "Large language models (LLM) are generating information at a rapid pace,\nrequiring users to increasingly rely and trust the data. Despite remarkable\nadvances of LLM, Information generated by LLM is not completely trustworthy,\ndue to challenges in information quality. Specifically, integrity of\nInformation quality decreases due to unreliable, biased, tokenization during\npre-training of LLM. Moreover, due to decreased information quality issues, has\nled towards hallucination, fabricated information. Unreliable information can\nlead towards flawed decisions in businesses, which impacts economic activity.\nIn this work, we introduce novel mathematical information quality evaluation of\nLLM, we furthermore analyze and highlight information quality challenges,\nscaling laws to systematically scale language models.", "published": "2024-01-23 20:55:49", "link": "http://arxiv.org/abs/2401.13086v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Locality enhanced dynamic biasing and sampling strategies for contextual\n  ASR", "abstract": "Automatic Speech Recognition (ASR) still face challenges when recognizing\ntime-variant rare-phrases. Contextual biasing (CB) modules bias ASR model\ntowards such contextually-relevant phrases. During training, a list of biasing\nphrases are selected from a large pool of phrases following a sampling\nstrategy. In this work we firstly analyse different sampling strategies to\nprovide insights into the training of CB for ASR with correlation plots between\nthe bias embeddings among various training stages. Secondly, we introduce a\nneighbourhood attention (NA) that localizes self attention (SA) to the nearest\nneighbouring frames to further refine the CB output. The results show that this\nproposed approach provides on average a 25.84% relative WER improvement on\nLibriSpeech sets and rare-word evaluation compared to the baseline.", "published": "2024-01-23 23:46:01", "link": "http://arxiv.org/abs/2401.13146v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ARGS: Alignment as Reward-Guided Search", "abstract": "Aligning large language models with human objectives is paramount, yet common\napproaches including RLHF suffer from unstable and resource-intensive training.\nIn response to this challenge, we introduce ARGS, Alignment as Reward-Guided\nSearch, a novel framework that integrates alignment into the decoding process,\neliminating the need for expensive RL training. By adjusting the model's\nprobabilistic predictions using a reward signal, ARGS generates texts with\nsemantic diversity while being aligned with human preferences, offering a\npromising and flexible solution for aligning language models. Notably, ARGS\ndemonstrates consistent enhancements in average reward compared to baselines\nacross diverse alignment tasks and various model dimensions. For example, under\nthe same greedy-based decoding strategy, our method improves the average reward\nby 19.56% relative to the baseline and secures a preference or tie score of\n64.33% in GPT-4 evaluation. We believe that our framework, emphasizing\ndecoding-time alignment, paves the way for more responsive language models in\nthe future. Code is publicly available at:\n\\url{https://github.com/deeplearning-wisc/args}.", "published": "2024-01-23 23:42:41", "link": "http://arxiv.org/abs/2402.01694v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transformer-Based Models Are Not Yet Perfect At Learning to Emulate\n  Structural Recursion", "abstract": "This paper investigates the ability of transformer-based models to learn\nstructural recursion from examples. Recursion is a universal concept in both\nnatural and formal languages. Structural recursion is central to the\nprogramming language and formal mathematics tasks where symbolic tools\ncurrently excel beyond neural models, such as inferring semantic relations\nbetween datatypes and emulating program behavior. We introduce a general\nframework that nicely connects the abstract concepts of structural recursion in\nthe programming language domain to concrete sequence modeling problems and\nlearned models' behavior. The framework includes a representation that captures\nthe general \\textit{syntax} of structural recursion, coupled with two different\nframeworks for understanding their \\textit{semantics} -- one that is more\nnatural from a programming languages perspective and one that helps bridge that\nperspective with a mechanistic understanding of the underlying transformer\narchitecture.\n  With our framework as a powerful conceptual tool, we identify different\nissues under various set-ups. The models trained to emulate recursive\ncomputations cannot fully capture the recursion yet instead fit short-cut\nalgorithms and thus cannot solve certain edge cases that are under-represented\nin the training distribution. In addition, it is difficult for state-of-the-art\nlarge language models (LLMs) to mine recursive rules from in-context\ndemonstrations. Meanwhile, these LLMs fail in interesting ways when emulating\nreduction (step-wise computation) of the recursive function.", "published": "2024-01-23 18:07:38", "link": "http://arxiv.org/abs/2401.12947v1", "categories": ["cs.CL", "cs.AI", "cs.FL", "cs.LO", "cs.PL"], "primary_category": "cs.CL"}
{"title": "AutoRT: Embodied Foundation Models for Large Scale Orchestration of\n  Robotic Agents", "abstract": "Foundation models that incorporate language, vision, and more recently\nactions have revolutionized the ability to harness internet scale data to\nreason about useful tasks. However, one of the key challenges of training\nembodied foundation models is the lack of data grounded in the physical world.\nIn this paper, we propose AutoRT, a system that leverages existing foundation\nmodels to scale up the deployment of operational robots in completely unseen\nscenarios with minimal human supervision. AutoRT leverages vision-language\nmodels (VLMs) for scene understanding and grounding, and further uses large\nlanguage models (LLMs) for proposing diverse and novel instructions to be\nperformed by a fleet of robots. Guiding data collection by tapping into the\nknowledge of foundation models enables AutoRT to effectively reason about\nautonomy tradeoffs and safety while significantly scaling up data collection\nfor robot learning. We demonstrate AutoRT proposing instructions to over 20\nrobots across multiple buildings and collecting 77k real robot episodes via\nboth teleoperation and autonomous robot policies. We experimentally show that\nsuch \"in-the-wild\" data collected by AutoRT is significantly more diverse, and\nthat AutoRT's use of LLMs allows for instruction following data collection\nrobots that can align to human preferences.", "published": "2024-01-23 18:45:54", "link": "http://arxiv.org/abs/2401.12963v2", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Boosting Unknown-number Speaker Separation with Transformer\n  Decoder-based Attractor", "abstract": "We propose a novel speech separation model designed to separate mixtures with\nan unknown number of speakers. The proposed model stacks 1) a dual-path\nprocessing block that can model spectro-temporal patterns, 2) a transformer\ndecoder-based attractor (TDA) calculation module that can deal with an unknown\nnumber of speakers, and 3) triple-path processing blocks that can model\ninter-speaker relations. Given a fixed, small set of learned speaker queries\nand the mixture embedding produced by the dual-path blocks, TDA infers the\nrelations of these queries and generates an attractor vector for each speaker.\nThe estimated attractors are then combined with the mixture embedding by\nfeature-wise linear modulation conditioning, creating a speaker dimension. The\nmixture embedding, conditioned with speaker information produced by TDA, is fed\nto the final triple-path blocks, which augment the dual-path blocks with an\nadditional pathway dedicated to inter-speaker processing. The proposed approach\noutperforms the previous best reported in the literature, achieving 24.0 and\n23.7 dB SI-SDR improvement (SI-SDRi) on WSJ0-2 and 3mix respectively, with a\nsingle model trained to separate 2- and 3-speaker mixtures. The proposed model\nalso exhibits strong performance and generalizability at counting sources and\nseparating mixtures with up to 5 speakers.", "published": "2024-01-23 03:55:22", "link": "http://arxiv.org/abs/2401.12473v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "EEND-M2F: Masked-attention mask transformers for speaker diarization", "abstract": "In this paper, we make the explicit connection between image segmentation\nmethods and end-to-end diarization methods. From these insights, we propose a\nnovel, fully end-to-end diarization model, EEND-M2F, based on the Mask2Former\narchitecture. Speaker representations are computed in parallel using a stack of\ntransformer decoders, in which irrelevant frames are explicitly masked from the\ncross attention using predictions from previous layers. EEND-M2F is\nlightweight, efficient, and truly end-to-end, as it does not require any\nadditional diarization, speaker verification, or segmentation models to run,\nnor does it require running any clustering algorithms. Our model achieves\nstate-of-the-art performance on several public datasets, such as AMI,\nAliMeeting and RAMC. Most notably our DER of 16.07% on DIHARD-III is the first\nmajor improvement upon the challenge winning system.", "published": "2024-01-23 09:56:59", "link": "http://arxiv.org/abs/2401.12600v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MoodLoopGP: Generating Emotion-Conditioned Loop Tablature Music with\n  Multi-Granular Features", "abstract": "Loopable music generation systems enable diverse applications, but they often\nlack controllability and customization capabilities. We argue that enhancing\ncontrollability can enrich these models, with emotional expression being a\ncrucial aspect for both creators and listeners. Hence, building upon LooperGP,\na loopable tablature generation model, this paper explores endowing systems\nwith control over conveyed emotions. To enable such conditional generation, we\npropose integrating musical knowledge by utilizing multi-granular semantic and\nmusical features during model training and inference. Specifically, we\nincorporate song-level features (Emotion Labels, Tempo, and Mode) and bar-level\nfeatures (Tonal Tension) together to guide emotional expression. Through\nalgorithmic and human evaluations, we demonstrate the approach's effectiveness\nin producing music conveying two contrasting target emotions, happiness and\nsadness. An ablation study is also conducted to clarify the contributing\nfactors behind our approach's results.", "published": "2024-01-23 11:08:08", "link": "http://arxiv.org/abs/2401.12656v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Emotion-Aware Contrastive Adaptation Network for Source-Free\n  Cross-Corpus Speech Emotion Recognition", "abstract": "Cross-corpus speech emotion recognition (SER) aims to transfer emotional\nknowledge from a labeled source corpus to an unlabeled corpus. However, prior\nmethods require access to source data during adaptation, which is unattainable\nin real-life scenarios due to data privacy protection concerns. This paper\ntackles a more practical task, namely source-free cross-corpus SER, where a\npre-trained source model is adapted to the target domain without access to\nsource data. To address the problem, we propose a novel method called\nemotion-aware contrastive adaptation network (ECAN). The core idea is to\ncapture local neighborhood information between samples while considering the\nglobal class-level adaptation. Specifically, we propose a nearest neighbor\ncontrastive learning to promote local emotion consistency among features of\nhighly similar samples. Furthermore, relying solely on nearest neighborhoods\nmay lead to ambiguous boundaries between clusters. Thus, we incorporate\nsupervised contrastive learning to encourage greater separation between\nclusters representing different emotions, thereby facilitating improved\nclass-level adaptation. Extensive experiments indicate that our proposed ECAN\nsignificantly outperforms state-of-the-art methods under the source-free\ncross-corpus SER setting on several speech emotion corpora.", "published": "2024-01-23 17:21:43", "link": "http://arxiv.org/abs/2401.12925v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Post-Training Embedding Alignment for Decoupling Enrollment and Runtime\n  Speaker Recognition Models", "abstract": "Automated speaker identification (SID) is a crucial step for the\npersonalization of a wide range of speech-enabled services. Typical SID systems\nuse a symmetric enrollment-verification framework with a single model to derive\nembeddings both offline for voice profiles extracted from enrollment\nutterances, and online from runtime utterances. Due to the distinct\ncircumstances of enrollment and runtime, such as different computation and\nlatency constraints, several applications would benefit from an asymmetric\nenrollment-verification framework that uses different models for enrollment and\nruntime embedding generation. To support this asymmetric SID where each of the\ntwo models can be updated independently, we propose using a lightweight neural\nnetwork to map the embeddings from the two independent models to a shared\nspeaker embedding space. Our results show that this approach significantly\noutperforms cosine scoring in a shared speaker logit space for models that were\ntrained with a contrastive loss on large datasets with many speaker identities.\nThis proposed Neural Embedding Speaker Space Alignment (NESSA) combined with an\nasymmetric update of only one of the models delivers at least 60% of the\nperformance gain achieved by updating both models in the standard symmetric SID\napproach.", "published": "2024-01-23 02:19:31", "link": "http://arxiv.org/abs/2401.12440v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DiffMoog: a Differentiable Modular Synthesizer for Sound Matching", "abstract": "This paper presents DiffMoog - a differentiable modular synthesizer with a\ncomprehensive set of modules typically found in commercial instruments. Being\ndifferentiable, it allows integration into neural networks, enabling automated\nsound matching, to replicate a given audio input. Notably, DiffMoog facilitates\nmodulation capabilities (FM/AM), low-frequency oscillators (LFOs), filters,\nenvelope shapers, and the ability for users to create custom signal chains. We\nintroduce an open-source platform that comprises DiffMoog and an end-to-end\nsound matching framework. This framework utilizes a novel signal-chain loss and\nan encoder network that self-programs its outputs to predict DiffMoogs\nparameters based on the user-defined modular architecture. Moreover, we provide\ninsights and lessons learned towards sound matching using differentiable\nsynthesis. Combining robust sound capabilities with a holistic platform,\nDiffMoog stands as a premier asset for expediting research in audio synthesis\nand machine learning.", "published": "2024-01-23 08:59:21", "link": "http://arxiv.org/abs/2401.12570v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-End Supervised Hierarchical Graph Clustering for Speaker\n  Diarization", "abstract": "Speaker diarization, the task of segmenting an audio recording based on\nspeaker identity, constitutes an important speech pre-processing step for\nseveral downstream applications.The conventional approach to diarization\ninvolves multiple steps of embedding extraction and clustering, which are often\noptimized in an isolated fashion. While end-to-end diarization systems attempt\nto learn a single model for the task, they are often cumbersome to train and\nrequire large supervised datasets. In this paper, we propose an end-to-end\nsupervised hierarchical clustering algorithm based on graph neural networks\n(GNN), called End-to-end Supervised HierARchical Clustering (E-SHARC). The\nembedding extractor is initialized using a pre-trained x-vector model while the\nGNN model is trained initially using the x-vector embeddings from the\npre-trained model. Finally, the E-SHARC model uses the front-end mel-filterbank\nfeatures as input and jointly optimizes the embedding extractor and the GNN\nclustering module, performing representation learning, metric learning, and\nclustering with end-to-end optimization. Further, with additional inputs from\nan external overlap detector, the E-SHARC approach is capable of predicting the\nspeakers in the overlapping speech regions. The experimental evaluation on\nbenchmark datasets like AMI, Voxconverse and DISPLACE, illustrates that the\nproposed E-SHARC framework provides competitive diarization results using graph\nbased clustering methods.", "published": "2024-01-23 15:35:44", "link": "http://arxiv.org/abs/2401.12850v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
