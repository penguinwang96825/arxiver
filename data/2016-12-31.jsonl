{"title": "A POS Tagger for Code Mixed Indian Social Media Text - ICON-2016 NLP\n  Tools Contest Entry from Surukam", "abstract": "Building Part-of-Speech (POS) taggers for code-mixed Indian languages is a\nparticularly challenging problem in computational linguistics due to a dearth\nof accurately annotated training corpora. ICON, as part of its NLP tools\ncontest has organized this challenge as a shared task for the second\nconsecutive year to improve the state-of-the-art. This paper describes the POS\ntagger built at Surukam to predict the coarse-grained and fine-grained POS tags\nfor three language pairs - Bengali-English, Telugu-English and Hindi-English,\nwith the text spanning three popular social media platforms - Facebook,\nWhatsApp and Twitter. We employed Conditional Random Fields as the sequence\ntagging algorithm and used a library called sklearn-crfsuite - a thin wrapper\naround CRFsuite for training our model. Among the features we used include -\ncharacter n-grams, language information and patterns for emoji, number,\npunctuation and web-address. Our submissions in the constrained\nenvironment,i.e., without making any use of monolingual POS taggers or the\nlike, obtained an overall average F1-score of 76.45%, which is comparable to\nthe 2015 winning score of 76.79%.", "published": "2016-12-31 07:09:52", "link": "http://arxiv.org/abs/1701.00066v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Expanding Subjective Lexicons for Social Media Mining with Embedding\n  Subspaces", "abstract": "Recent approaches for sentiment lexicon induction have capitalized on\npre-trained word embeddings that capture latent semantic properties. However,\nembeddings obtained by optimizing performance of a given task (e.g. predicting\ncontextual words) are sub-optimal for other applications. In this paper, we\naddress this problem by exploiting task-specific representations, induced via\nembedding sub-space projection. This allows us to expand lexicons describing\nmultiple semantic properties. For each property, our model jointly learns\nsuitable representations and the concomitant predictor. Experiments conducted\nover multiple subjective lexicons, show that our model outperforms previous\nwork and other baselines; even in low training data regimes. Furthermore,\nlexicon-based sentiment classifiers built on top of our lexicons outperform\nsimilar resources and yield performances comparable to those of supervised\nmodels.", "published": "2016-12-31 17:00:08", "link": "http://arxiv.org/abs/1701.00145v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Social Media Argumentation Mining: The Quest for Deliberateness in\n  Raucousness", "abstract": "Argumentation mining from social media content has attracted increasing\nattention. The task is both challenging and rewarding. The informal nature of\nuser-generated content makes the task dauntingly difficult. On the other hand,\nthe insights that could be gained by a large-scale analysis of social media\nargumentation make it a very worthwhile task. In this position paper I discuss\nthe motivation for social media argumentation mining, as well as the tasks and\nchallenges involved.", "published": "2016-12-31 21:37:10", "link": "http://arxiv.org/abs/1701.00168v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cutting-off Redundant Repeating Generations for Neural Abstractive\n  Summarization", "abstract": "This paper tackles the reduction of redundant repeating generation that is\noften observed in RNN-based encoder-decoder models. Our basic idea is to\njointly estimate the upper-bound frequency of each target vocabulary in the\nencoder and control the output words based on the estimation in the decoder.\nOur method shows significant improvement over a strong RNN-based\nencoder-decoder baseline and achieved its best results on an abstractive\nsummarization benchmark.", "published": "2016-12-31 16:41:43", "link": "http://arxiv.org/abs/1701.00138v2", "categories": ["cs.CL", "cs.AI", "stat.ML"], "primary_category": "cs.CL"}
