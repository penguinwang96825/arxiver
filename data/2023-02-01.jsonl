{"title": "Detecting Lexical Borrowings from Dominant Languages in Multilingual\n  Wordlists", "abstract": "Language contact is a pervasive phenomenon reflected in the borrowing of\nwords from donor to recipient languages. Most computational approaches to\nborrowing detection treat all languages under study as equally important, even\nthough dominant languages have a stronger impact on heritage languages than\nvice versa. We test new methods for lexical borrowing detection in contact\nsituations where dominant languages play an important role, applying two\nclassical sequence comparison methods and one machine learning method to a\nsample of seven Latin American languages which have all borrowed extensively\nfrom Spanish. All methods perform well, with the supervised machine learning\nsystem outperforming the classical systems. A review of detection errors shows\nthat borrowing detection could be substantially improved by taking into account\ndonor words with divergent meanings from recipient words.", "published": "2023-02-01 02:44:28", "link": "http://arxiv.org/abs/2302.00189v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study on the Transferability of Transformer Modules in\n  Parameter-Efficient Fine-Tuning", "abstract": "Parameter-efficient fine-tuning approaches have recently garnered a lot of\nattention. Having considerably lower number of trainable weights, these methods\ncan bring about scalability and computational effectiveness. In this paper, we\nlook for optimal sub-networks and investigate the capability of different\ntransformer modules in transferring knowledge from a pre-trained model to a\ndownstream task. Our empirical results suggest that every transformer module in\nBERT can act as a winning ticket: fine-tuning each specific module while\nkeeping the rest of the network frozen can lead to comparable performance to\nthe full fine-tuning. Among different modules, LayerNorms exhibit the best\ncapacity for knowledge transfer with limited trainable weights, to the extent\nthat, with only 0.003% of all parameters in the layer-wise analysis, they show\nacceptable performance on various target tasks. On the reasons behind their\neffectiveness, we argue that their notable performance could be attributed to\ntheir high-magnitude weights compared to that of the other modules in the\npre-trained BERT.", "published": "2023-02-01 11:20:18", "link": "http://arxiv.org/abs/2302.00378v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KNNs of Semantic Encodings for Rating Prediction", "abstract": "This paper explores a novel application of textual semantic similarity to\nuser-preference representation for rating prediction. The approach represents a\nuser's preferences as a graph of textual snippets from review text, where the\nedges are defined by semantic similarity. This textual, memory-based approach\nto rating prediction enables review-based explanations for recommendations. The\nmethod is evaluated quantitatively, highlighting that leveraging text in this\nway outperforms both strong memory-based and model-based collaborative\nfiltering baselines.", "published": "2023-02-01 12:53:31", "link": "http://arxiv.org/abs/2302.00412v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Knowledge Distillation for Pre-trained Language Models via\n  Knowledge Selection", "abstract": "Knowledge distillation addresses the problem of transferring knowledge from a\nteacher model to a student model. In this process, we typically have multiple\ntypes of knowledge extracted from the teacher model. The problem is to make\nfull use of them to train the student model. Our preliminary study shows that:\n(1) not all of the knowledge is necessary for learning a good student model,\nand (2) knowledge distillation can benefit from certain knowledge at different\ntraining steps. In response to these, we propose an actor-critic approach to\nselecting appropriate knowledge to transfer during the process of knowledge\ndistillation. In addition, we offer a refinement of the training algorithm to\nease the computational burden. Experimental results on the GLUE datasets show\nthat our method outperforms several strong knowledge distillation baselines\nsignificantly.", "published": "2023-02-01 13:40:19", "link": "http://arxiv.org/abs/2302.00444v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HunSum-1: an Abstractive Summarization Dataset for Hungarian", "abstract": "We introduce HunSum-1: a dataset for Hungarian abstractive summarization,\nconsisting of 1.14M news articles. The dataset is built by collecting, cleaning\nand deduplicating data from 9 major Hungarian news sites through CommonCrawl.\nUsing this dataset, we build abstractive summarizer models based on huBERT and\nmT5. We demonstrate the value of the created dataset by performing a\nquantitative and qualitative analysis on the models' results. The HunSum-1\ndataset, all models used in our experiments and our code are available open\nsource.", "published": "2023-02-01 13:59:45", "link": "http://arxiv.org/abs/2302.00455v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Feed-Forward Blocks in Transformers through the Lens of\n  Attention Maps", "abstract": "Transformers are ubiquitous in wide tasks. Interpreting their internals is a\npivotal goal. Nevertheless, their particular components, feed-forward (FF)\nblocks, have typically been less analyzed despite their substantial parameter\namounts. We analyze the input contextualization effects of FF blocks by\nrendering them in the attention maps as a human-friendly visualization scheme.\nOur experiments with both masked- and causal-language models reveal that FF\nnetworks modify the input contextualization to emphasize specific types of\nlinguistic compositions. In addition, FF and its surrounding components tend to\ncancel out each other's effects, suggesting potential redundancy in the\nprocessing of the Transformer layer.", "published": "2023-02-01 13:59:47", "link": "http://arxiv.org/abs/2302.00456v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "You Are What You Talk About: Inducing Evaluative Topics for Personality\n  Analysis", "abstract": "Expressing attitude or stance toward entities and concepts is an integral\npart of human behavior and personality. Recently, evaluative language data has\nbecome more accessible with social media's rapid growth, enabling large-scale\nopinion analysis. However, surprisingly little research examines the\nrelationship between personality and evaluative language. To bridge this gap,\nwe introduce the notion of evaluative topics, obtained by applying topic models\nto pre-filtered evaluative text from social media. We then link evaluative\ntopics to individual text authors to build their evaluative profiles. We apply\nevaluative profiling to Reddit comments labeled with personality scores and\nconduct an exploratory study on the relationship between evaluative topics and\nBig Five personality facets, aiming for a more interpretable, facet-level\nanalysis. Finally, we validate our approach by observing correlations\nconsistent with prior research in personality psychology.", "published": "2023-02-01 15:04:04", "link": "http://arxiv.org/abs/2302.00493v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The RW3D: A multi-modal panel dataset to understand the psychological\n  impact of the pandemic", "abstract": "Besides far-reaching public health consequences, the COVID-19 pandemic had a\nsignificant psychological impact on people around the world. To gain further\ninsight into this matter, we introduce the Real World Worry Waves Dataset\n(RW3D). The dataset combines rich open-ended free-text responses with survey\ndata on emotions, significant life events, and psychological stressors in a\nrepeated-measures design in the UK over three years (2020: n=2441, 2021: n=1716\nand 2022: n=1152). This paper provides background information on the data\ncollection procedure, the recorded variables, participants' demographics, and\nhigher-order psychological and text-based derived variables that emerged from\nthe data. The RW3D is a unique primary data resource that could inspire new\nresearch questions on the psychological impact of the pandemic, especially\nthose that connect modalities (here: text data, psychological survey variables\nand demographics) over time.", "published": "2023-02-01 17:13:06", "link": "http://arxiv.org/abs/2302.00606v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-shot Transfer of Article-aware Legal Outcome Classification for\n  European Court of Human Rights Cases", "abstract": "In this paper, we cast Legal Judgment Prediction on European Court of Human\nRights cases into an article-aware classification task, where the case outcome\nis classified from a combined input of case facts and convention articles. This\nconfiguration facilitates the model learning some legal reasoning ability in\nmapping article text to specific case fact text. It also provides an\nopportunity to evaluate the model's ability to generalize to zero-shot settings\nwhen asked to classify the case outcome with respect to articles not seen\nduring training. We devise zero-shot experiments and apply domain adaptation\nmethods based on domain discrimination and Wasserstein distance. Our results\ndemonstrate that the article-aware architecture outperforms straightforward\nfact classification. We also find that domain adaptation methods improve\nzero-shot transfer performance, with article relatedness and encoder\npre-training influencing the effect.", "published": "2023-02-01 17:20:52", "link": "http://arxiv.org/abs/2302.00609v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Synthetic Prompting: Generating Chain-of-Thought Demonstrations for\n  Large Language Models", "abstract": "Large language models can perform various reasoning tasks by using\nchain-of-thought prompting, which guides them to find answers through\nstep-by-step demonstrations. However, the quality of the prompts depends on the\ndemonstrations given to the models, and creating many of them by hand is\ncostly. We introduce Synthetic prompting, a method that leverages a few\nhandcrafted examples to prompt the model to generate more examples by itself,\nand selects effective demonstrations to elicit better reasoning. Our method\nalternates between a backward and forward process to generate new examples. The\nbackward process generates a question that match a sampled reasoning chain, so\nthat the question is solvable and clear. The forward process produces a more\ndetailed reasoning chain for the question, improving the quality of the\nexample. We evaluate our method on numerical, symbolic, and algorithmic\nreasoning tasks, and show that it outperforms existing prompting techniques.", "published": "2023-02-01 17:33:12", "link": "http://arxiv.org/abs/2302.00618v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are UD Treebanks Getting More Consistent? A Report Card for English UD", "abstract": "Recent efforts to consolidate guidelines and treebanks in the Universal\nDependencies project raise the expectation that joint training and dataset\ncomparison is increasingly possible for high-resource languages such as\nEnglish, which have multiple corpora. Focusing on the two largest UD English\ntreebanks, we examine progress in data consolidation and answer several\nquestions: Are UD English treebanks becoming more internally consistent? Are\nthey becoming more like each other and to what extent? Is joint training a good\nidea, and if so, since which UD version? Our results indicate that while\nconsolidation has made progress, joint models may still suffer from\ninconsistencies, which hamper their ability to leverage a larger pool of\ntraining data.", "published": "2023-02-01 17:58:28", "link": "http://arxiv.org/abs/2302.00636v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does Vision Accelerate Hierarchical Generalization in Neural Language\n  Learners?", "abstract": "Neural language models (LMs) are arguably less data-efficient than humans\nfrom a language acquisition perspective. One fundamental question is why this\nhuman-LM gap arises. This study explores the advantage of grounded language\nacquisition, specifically the impact of visual information -- which humans can\nusually rely on but LMs largely do not have access to during language\nacquisition -- on syntactic generalization in LMs. Our experiments, following\nthe poverty of stimulus paradigm under two scenarios (using artificial vs.\nnaturalistic images), demonstrate that if the alignments between the linguistic\nand visual components are clear in the input, access to vision data does help\nwith the syntactic generalization of LMs, but if not, visual input does not\nhelp. This highlights the need for additional biases or signals, such as mutual\ngaze, to enhance cross-modal alignment and enable efficient syntactic\ngeneralization in multimodal LMs.", "published": "2023-02-01 18:53:42", "link": "http://arxiv.org/abs/2302.00667v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inference of Partial Colexifications from Multilingual Wordlists", "abstract": "The past years have seen a drastic rise in studies devoted to the\ninvestigation of colexification patterns in individual languages families in\nparticular and the languages of the world in specific. Specifically\ncomputational studies have profited from the fact that colexification as a\nscientific construct is easy to operationalize, enabling scholars to infer\ncolexification patterns for large collections of cross-linguistic data. Studies\ndevoted to partial colexifications -- colexification patterns that do not\ninvolve entire words, but rather various parts of words--, however, have been\nrarely conducted so far. This is not surprising, since partial colexifications\nare less easy to deal with in computational approaches and may easily suffer\nfrom all kinds of noise resulting from false positive matches. In order to\naddress this problem, this study proposes new approaches to the handling of\npartial colexifications by (1) proposing new models with which partial\ncolexification patterns can be represented, (2) developing new efficient\nmethods and workflows which help to infer various types of partial\ncolexification patterns from multilingual wordlists, and (3) illustrating how\ninferred patterns of partial colexifications can be computationally analyzed\nand interactively visualized.", "published": "2023-02-01 20:22:20", "link": "http://arxiv.org/abs/2302.00739v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AmbiCoref: Evaluating Human and Model Sensitivity to Ambiguous\n  Coreference", "abstract": "Given a sentence \"Abby told Brittney that she upset Courtney\", one would\nstruggle to understand who \"she\" refers to, and ask for clarification. However,\nif the word \"upset\" were replaced with \"hugged\", \"she\" unambiguously refers to\nAbby. We study if modern coreference resolution models are sensitive to such\npronominal ambiguity. To this end, we construct AmbiCoref, a diagnostic corpus\nof minimal sentence pairs with ambiguous and unambiguous referents. Our\nexamples generalize psycholinguistic studies of human perception of ambiguity\naround particular arrangements of verbs and their arguments. Analysis shows\nthat (1) humans are less sure of referents in ambiguous AmbiCoref examples than\nunambiguous ones, and (2) most coreference models show little difference in\noutput between ambiguous and unambiguous pairs. We release AmbiCoref as a\ndiagnostic corpus for testing whether models treat ambiguity similarly to\nhumans.", "published": "2023-02-01 21:25:34", "link": "http://arxiv.org/abs/2302.00762v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Task Dependency and Contrastive Learning for Case Outcome\n  Classification on European Court of Human Rights Cases", "abstract": "We report on an experiment in case outcome classification on European Court\nof Human Rights cases where our model first learns to identify the convention\narticles allegedly violated by the state from case facts descriptions, and\nsubsequently uses that information to classify whether the court finds a\nviolation of those articles. We assess the dependency between these two tasks\nat the feature and outcome level. Furthermore, we leverage a hierarchical\ncontrastive loss to pull together article-specific representations of cases at\nthe higher level, leading to distinctive article clusters. The cases in each\narticle cluster are further pulled closer based on their outcome, leading to\nsub-clusters of cases with similar outcomes. Our experiment results demonstrate\nthat, given a static pre-trained encoder, our models produce a small but\nconsistent improvement in classification performance over single-task and joint\nmodels without contrastive loss.", "published": "2023-02-01 21:38:47", "link": "http://arxiv.org/abs/2302.00768v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Evaluation of Persian-English Machine Translation Datasets with\n  Transformers", "abstract": "Nowadays, many researchers are focusing their attention on the subject of\nmachine translation (MT). However, Persian machine translation has remained\nunexplored despite a vast amount of research being conducted in languages with\nhigh resources, such as English. Moreover, while a substantial amount of\nresearch has been undertaken in statistical machine translation for some\ndatasets in Persian, there is currently no standard baseline for\ntransformer-based text2text models on each corpus. This study collected and\nanalysed the most popular and valuable parallel corpora, which were used for\nPersian-English translation. Furthermore, we fine-tuned and evaluated two\nstate-of-the-art attention-based seq2seq models on each dataset separately (48\nresults). We hope this paper will assist researchers in comparing their Persian\nto English and vice versa machine translation results to a standard baseline.", "published": "2023-02-01 08:55:08", "link": "http://arxiv.org/abs/2302.00321v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating TCFD Reporting: A New Application of Zero-Shot Analysis to\n  Climate-Related Financial Disclosures", "abstract": "We examine climate-related disclosures in a large sample of reports published\nby banks that officially endorsed the recommendations of the Task Force for\nClimate-related Financial Disclosures (TCFD). In doing so, we introduce a new\napplication of the zero-shot text classification. By developing a set of\nfine-grained TCFD labels, we show that zero-shot analysis is a useful tool for\nclassifying climate-related disclosures without further model training.\nOverall, our findings indicate that corporate climate-related disclosures grew\ndynamically after the launch of the TCFD recommendations. However, there are\nmarked differences in the extent of reporting by recommended disclosure topic,\nsuggesting that some recommendations have not yet been fully met. Our findings\nyield important conclusions for the design of climate-related disclosure\nframeworks.", "published": "2023-02-01 09:11:05", "link": "http://arxiv.org/abs/2302.00326v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Attention Link: An Efficient Attention-Based Low Resource Machine\n  Translation Architecture", "abstract": "Transformers have achieved great success in machine translation, but\ntransformer-based NMT models often require millions of bilingual parallel\ncorpus for training. In this paper, we propose a novel architecture named as\nattention link (AL) to help improve transformer models' performance, especially\nin low training resources. We theoretically demonstrate the superiority of our\nattention link architecture in low training resources. Besides, we have done a\nlarge number of experiments, including en-de, de-en, en-fr, en-it, it-en, en-ro\ntranslation tasks on the IWSLT14 dataset as well as real low resources scene on\nbn-gu and gu-ta translation tasks on the CVIT PIB dataset. All the experiment\nresults show our attention link is powerful and can lead to a significant\nimprovement. In addition, we achieve a 37.9 BLEU score, a new sota, on the\nIWSLT14 de-en task by combining our attention link and other advanced methods.", "published": "2023-02-01 09:52:36", "link": "http://arxiv.org/abs/2302.00340v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Role of Morphological Information for Contextual Lemmatization", "abstract": "Lemmatization is a natural language processing (NLP) task which consists of\nproducing, from a given inflected word, its canonical form or lemma.\nLemmatization is one of the basic tasks that facilitate downstream NLP\napplications, and is of particular importance for high-inflected languages.\nGiven that the process to obtain a lemma from an inflected word can be\nexplained by looking at its morphosyntactic category, including fine-grained\nmorphosyntactic information to train contextual lemmatizers has become common\npractice, without considering whether that is the optimum in terms of\ndownstream performance. In order to address this issue, in this paper we\nempirically investigate the role of morphological information to develop\ncontextual lemmatizers in six languages within a varied spectrum of\nmorphological complexity: Basque, Turkish, Russian, Czech, Spanish and English.\nFurthermore, and unlike the vast majority of previous work, we also evaluate\nlemmatizers in out-of-domain settings, which constitutes, after all, their most\ncommon application use. The results of our study are rather surprising. It\nturns out that providing lemmatizers with fine-grained morphological features\nduring training is not that beneficial, not even for agglutinative languages.\nIn fact, modern contextual word representations seem to implicitly encode\nenough morphological information to obtain competitive contextual lemmatizers\nwithout seeing any explicit morphological signal. Moreover, our experiments\nsuggest that the best lemmatizers out-of-domain are those using simple UPOS\ntags or those trained without morphology and, finally, that current evaluation\npractices for lemmatization are not adequate to clearly discriminate between\nmodels.", "published": "2023-02-01 12:47:09", "link": "http://arxiv.org/abs/2302.00407v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary\n  Data", "abstract": "Few-shot learning is valuable in many real-world applications, but learning a\ngeneralizable model without overfitting to the few labeled datapoints is\nchallenging. In this work, we focus on Few-shot Learning with Auxiliary Data\n(FLAD), a training paradigm that assumes access to auxiliary data during\nfew-shot learning in hopes of improving generalization. Previous works have\nproposed automated methods for mixing auxiliary and target data, but these\nmethods typically scale linearly (or worse) with the number of auxiliary\ndatasets, limiting their practicality. In this work we relate FLAD to the\nexplore-exploit dilemma that is central to the multi-armed bandit setting and\nderive algorithms whose computational complexity is independent of the number\nof auxiliary datasets, allowing us to scale to 100x more auxiliary datasets\nthan prior methods. We propose two algorithms -- EXP3-FLAD and UCB1-FLAD -- and\ncompare them with prior FLAD methods that either explore or exploit, finding\nthat the combination of exploration and exploitation is crucial. Through\nextensive experimentation we find that our methods outperform all pre-existing\nFLAD methods by 4% and lead to the first 3 billion parameter language models\nthat outperform the 175 billion parameter GPT-3. Overall, our work suggests\nthat the discovery of better, more efficient mixing strategies for FLAD may\nprovide a viable path towards substantially improving generalization in\nfew-shot learning.", "published": "2023-02-01 18:59:36", "link": "http://arxiv.org/abs/2302.00674v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "User Study for Improving Tools for Bible Translation", "abstract": "Technology has increasingly become an integral part of the Bible translation\nprocess. Over time, both the translation process and relevant technology have\nevolved greatly. More recently, the field of Natural Language Processing (NLP)\nhas made great progress in solving some problems previously thought\nimpenetrable. Through this study we endeavor to better understand and\ncommunicate about a segment of the current landscape of the Bible translation\nprocess as it relates to technology and identify pertinent issues. We conduct\nseveral interviews with individuals working in different levels of the Bible\ntranslation process from multiple organizations to identify gaps and\nbottlenecks where technology (including recent advances in AI) could\npotentially play a pivotal role in reducing translation time and improving\noverall quality.", "published": "2023-02-01 22:22:03", "link": "http://arxiv.org/abs/2302.00778v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Grading Conversational Responses Of Chatbots", "abstract": "Chatbots have long been capable of answering basic questions and even\nresponding to obscure prompts, but recently their improvements have been far\nmore significant. Modern chatbots like Open AIs ChatGPT3 not only have the\nability to answer basic questions but can write code and movie scripts and\nimitate well-known people. In this paper, we analyze ChatGPTs' responses to\nvarious questions from a dataset of queries from the popular Quora forum. We\nsubmitted sixty questions to ChatGPT and scored the answers based on three\nindustry-standard metrics for grading machine translation: BLEU, METEOR, and\nROUGE. These metrics allow us to compare the machine responses with the most\nupvoted human answer to the same question to assess ChatGPT's ability to submit\na humanistic reply. The results showed that while the responses and translation\nabilities of ChatGPT are remarkable, they still fall short of what a typical\nhuman reaction would be.", "published": "2023-02-01 02:54:43", "link": "http://arxiv.org/abs/2303.12038v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Program Generation from Diverse Video Demonstrations", "abstract": "The ability to use inductive reasoning to extract general rules from multiple\nobservations is a vital indicator of intelligence. As humans, we use this\nability to not only interpret the world around us, but also to predict the\noutcomes of the various interactions we experience. Generalising over multiple\nobservations is a task that has historically presented difficulties for\nmachines to grasp, especially when requiring computer vision. In this paper, we\npropose a model that can extract general rules from video demonstrations by\nsimultaneously performing summarisation and translation. Our approach differs\nfrom prior works by framing the problem as a multi-sequence-to-sequence task,\nwherein summarisation is learnt by the model. This allows our model to utilise\nedge cases that would otherwise be suppressed or discarded by traditional\nsummarisation techniques. Additionally, we show that our approach can handle\nnoisy specifications without the need for additional filtering methods. We\nevaluate our model by synthesising programs from video demonstrations in the\nVizdoom environment achieving state-of-the-art results with a relative increase\nof 11.75% program accuracy on prior works", "published": "2023-02-01 01:51:45", "link": "http://arxiv.org/abs/2302.00178v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Filtering Context Mitigates Scarcity and Selection Bias in Political\n  Ideology Prediction", "abstract": "We propose a novel supervised learning approach for political ideology\nprediction (PIP) that is capable of predicting out-of-distribution inputs. This\nproblem is motivated by the fact that manual data-labeling is expensive, while\nself-reported labels are often scarce and exhibit significant selection bias.\nWe propose a novel statistical model that decomposes the document embeddings\ninto a linear superposition of two vectors; a latent neutral \\emph{context}\nvector independent of ideology, and a latent \\emph{position} vector aligned\nwith ideology. We train an end-to-end model that has intermediate contextual\nand positional vectors as outputs. At deployment time, our model predicts\nlabels for input documents by exclusively leveraging the predicted positional\nvectors. On two benchmark datasets we show that our model is capable of\noutputting predictions even when trained with as little as 5\\% biased data, and\nis significantly more accurate than the state-of-the-art. Through\ncrowd-sourcing we validate the neutrality of contextual vectors, and show that\ncontext filtering results in ideological concentration, allowing for prediction\non out-of-distribution examples.", "published": "2023-02-01 04:34:48", "link": "http://arxiv.org/abs/2302.00239v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image\n  and Video", "abstract": "Recent years have witnessed a big convergence of language, vision, and\nmulti-modal pretraining. In this work, we present mPLUG-2, a new unified\nparadigm with modularized design for multi-modal pretraining, which can benefit\nfrom modality collaboration while addressing the problem of modality\nentanglement. In contrast to predominant paradigms of solely relying on\nsequence-to-sequence generation or encoder-based instance discrimination,\nmPLUG-2 introduces a multi-module composition network by sharing common\nuniversal modules for modality collaboration and disentangling different\nmodality modules to deal with modality entanglement. It is flexible to select\ndifferent modules for different understanding and generation tasks across all\nmodalities including text, image, and video. Empirical study shows that mPLUG-2\nachieves state-of-the-art or competitive results on a broad range of over 30\ndownstream tasks, spanning multi-modal tasks of image-text and video-text\nunderstanding and generation, and uni-modal tasks of text-only, image-only, and\nvideo-only understanding. Notably, mPLUG-2 shows new state-of-the-art results\nof 48.0 top-1 accuracy and 80.3 CIDEr on the challenging MSRVTT video QA and\nvideo caption tasks with a far smaller model size and data scale. It also\ndemonstrates strong zero-shot transferability on vision-language and\nvideo-language tasks. Code and models will be released in\nhttps://github.com/alibaba/AliceMind.", "published": "2023-02-01 12:40:03", "link": "http://arxiv.org/abs/2302.00402v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Exploring Semantic Perturbations on Grover", "abstract": "With news and information being as easy to access as they currently are, it\nis more important than ever to ensure that people are not mislead by what they\nread. Recently, the rise of neural fake news (AI-generated fake news) and its\ndemonstrated effectiveness at fooling humans has prompted the development of\nmodels to detect it. One such model is the Grover model, which can both detect\nneural fake news to prevent it, and generate it to demonstrate how a model\ncould be misused to fool human readers. In this work we explore the Grover\nmodel's fake news detection capabilities by performing targeted attacks through\nperturbations on input news articles. Through this we test Grover's resilience\nto these adversarial attacks and expose some potential vulnerabilities which\nshould be addressed in further iterations to ensure it can detect all types of\nfake news accurately.", "published": "2023-02-01 15:28:55", "link": "http://arxiv.org/abs/2302.00509v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Co-Writing with Opinionated Language Models Affects Users' Views", "abstract": "If large language models like GPT-3 preferably produce a particular point of\nview, they may influence people's opinions on an unknown scale. This study\ninvestigates whether a language-model-powered writing assistant that generates\nsome opinions more often than others impacts what users write - and what they\nthink. In an online experiment, we asked participants (N=1,506) to write a post\ndiscussing whether social media is good for society. Treatment group\nparticipants used a language-model-powered writing assistant configured to\nargue that social media is good or bad for society. Participants then completed\na social media attitude survey, and independent judges (N=500) evaluated the\nopinions expressed in their writing. Using the opinionated language model\naffected the opinions expressed in participants' writing and shifted their\nopinions in the subsequent attitude survey. We discuss the wider implications\nof our results and argue that the opinions built into AI language technologies\nneed to be monitored and engineered more carefully.", "published": "2023-02-01 16:26:32", "link": "http://arxiv.org/abs/2302.00560v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "'Generative CI' through Collective Response Systems", "abstract": "How can many people (who may disagree) come together to answer a question or\nmake a decision? \"Collective response systems\" are a type of generative\ncollective intelligence (CI) facilitation process meant to address this\nchallenge. They enable a form of \"generative voting\", where both the votes, and\nthe choices of what to vote on, are provided by the group. Such systems\novercome the traditional limitations of polling, town halls, standard voting,\nreferendums, etc. The generative CI outputs of collective response systems can\nalso be chained together into iterative \"collective dialogues\", analogously to\nsome kinds of generative AI.\n  Technical advances across domains including recommender systems, language\nmodels, and human-computer interaction have led to the development of\ninnovative and scalable collective response systems. For example, Polis has\nbeen used around the world to support policy-making at different levels of\ngovernment, and Remesh has been used by the UN to understand the challenges and\nneeds of ordinary people across war-torn countries. This paper aims to develop\na shared language by defining the structure, processes, properties, and\nprinciples of such systems.\n  Collective response systems allow non-confrontational exploration of divisive\nissues, help identify common ground, and elicit insights from those closest to\nthe issues. As a result, they can help overcome gridlock around conflict and\ngovernance challenges, increase trust, and develop mandates. Continued progress\ntoward their development and adoption could help revitalize democracies,\nreimagine corporate governance, transform conflict, and govern powerful AI\nsystems -- both as a complement to deeper deliberative democratic processes and\nas an option where deeper processes are not applicable or possible.", "published": "2023-02-01 18:59:02", "link": "http://arxiv.org/abs/2302.00672v1", "categories": ["cs.HC", "cs.CL", "cs.IR"], "primary_category": "cs.HC"}
{"title": "A Formal Algebraic Framework for DSL Composition", "abstract": "We discuss a formal framework for using algebraic structures to model a\nmeta-language that can write, compose, and provide interoperability between\nabstractions of DSLs. The purpose of this formal framework is to provide a\nverification of compositional properties of the meta-language. Throughout our\npaper we discuss the construction of this formal framework, as well its\nrelation to our team's work on the DARPA V-SPELLS program via the pipeline we\nhave developed for completing our verification tasking on V-SPELLS. We aim to\ngive a broad overview of this verification pipeline in our paper. The pipeline\ncan be split into four main components: the first is providing a formal model\nof the meta-language in Coq; the second is to give a specification in Coq of\nour chosen algebraic structures; third, we need to implement specific instances\nof our algebraic structures in Coq, as well as give a proof in Coq that this\nimplementation is an algebraic structure according to our specification in the\nsecond step; and lastly, we need to give a proof in Coq that the formal model\nfor the meta-language in the first step is an instance of the implementation in\nthe third step.", "published": "2023-02-01 20:39:37", "link": "http://arxiv.org/abs/2302.00744v1", "categories": ["math.CT", "cs.CL", "cs.LO", "cs.PL"], "primary_category": "math.CT"}
{"title": "Collaborating with language models for embodied reasoning", "abstract": "Reasoning in a complex and ambiguous environment is a key goal for\nReinforcement Learning (RL) agents. While some sophisticated RL agents can\nsuccessfully solve difficult tasks, they require a large amount of training\ndata and often struggle to generalize to new unseen environments and new tasks.\nOn the other hand, Large Scale Language Models (LSLMs) have exhibited strong\nreasoning ability and the ability to to adapt to new tasks through in-context\nlearning. However, LSLMs do not inherently have the ability to interrogate or\nintervene on the environment. In this work, we investigate how to combine these\ncomplementary abilities in a single system consisting of three parts: a\nPlanner, an Actor, and a Reporter. The Planner is a pre-trained language model\nthat can issue commands to a simple embodied agent (the Actor), while the\nReporter communicates with the Planner to inform its next command. We present a\nset of tasks that require reasoning, test this system's ability to generalize\nzero-shot and investigate failure cases, and demonstrate how components of this\nsystem can be trained with reinforcement-learning to improve performance.", "published": "2023-02-01 21:26:32", "link": "http://arxiv.org/abs/2302.00763v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Visually Grounded Keyword Detection and Localisation for Low-Resource\n  Languages", "abstract": "This study investigates the use of Visually Grounded Speech (VGS) models for\nkeyword localisation in speech. The study focusses on two main research\nquestions: (1) Is keyword localisation possible with VGS models and (2) Can\nkeyword localisation be done cross-lingually in a real low-resource setting?\nFour methods for localisation are proposed and evaluated on an English dataset,\nwith the best-performing method achieving an accuracy of 57%. A new dataset\ncontaining spoken captions in Yoruba language is also collected and released\nfor cross-lingual keyword localisation. The cross-lingual model obtains a\nprecision of 16% in actual keyword localisation and this performance can be\nimproved by initialising from a model pretrained on English data. The study\npresents a detailed analysis of the model's success and failure modes and\nhighlights the challenges of using VGS models for keyword localisation in\nlow-resource settings.", "published": "2023-02-01 21:32:15", "link": "http://arxiv.org/abs/2302.00765v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Unsupervised Entity Alignment for Temporal Knowledge Graphs", "abstract": "Entity alignment (EA) is a fundamental data integration task that identifies\nequivalent entities between different knowledge graphs (KGs). Temporal\nKnowledge graphs (TKGs) extend traditional knowledge graphs by introducing\ntimestamps, which have received increasing attention. State-of-the-art\ntime-aware EA studies have suggested that the temporal information of TKGs\nfacilitates the performance of EA. However, existing studies have not\nthoroughly exploited the advantages of temporal information in TKGs. Also, they\nperform EA by pre-aligning entity pairs, which can be labor-intensive and thus\ninefficient.\n  In this paper, we present DualMatch which effectively fuses the relational\nand temporal information for EA. DualMatch transfers EA on TKGs into a weighted\ngraph matching problem. More specifically, DualMatch is equipped with an\nunsupervised method, which achieves EA without necessitating seed alignment.\nDualMatch has two steps: (i) encoding temporal and relational information into\nembeddings separately using a novel label-free encoder, Dual-Encoder; and (ii)\nfusing both information and transforming it into alignment using a novel\ngraph-matching-based decoder, GM-Decoder. DualMatch is able to perform EA on\nTKGs with or without supervision, due to its capability of effectively\ncapturing temporal information. Extensive experiments on three real-world TKG\ndatasets offer the insight that DualMatch outperforms the state-of-the-art\nmethods in terms of H@1 by 2.4% - 10.7% and MRR by 1.7% - 7.6%, respectively.", "published": "2023-02-01 23:03:22", "link": "http://arxiv.org/abs/2302.00796v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "DANES: Deep Neural Network Ensemble Architecture for Social and Textual\n  Context-aware Fake News Detection", "abstract": "The growing popularity of social media platforms has simplified the creation\nand distribution of news articles but also creates a conduit for spreading fake\nnews. In consequence, the need arises for effective context-aware fake news\ndetection mechanisms, where the contextual information can be built either from\nthe textual content of posts or from available social data (e.g., information\nabout the users, reactions to posts, or the social network). In this paper, we\npropose DANES, a Deep Neural Network Ensemble Architecture for Social and\nTextual Context-aware Fake News Detection. DANES comprises a Text Branch for a\ntextual content-based context and a Social Branch for the social context. These\ntwo branches are used to create a novel Network Embedding. Preliminary ablation\nresults on 3 real-world datasets, i.e., BuzzFace, Twitter15, and Twitter16, are\npromising, with an accuracy that outperforms state-of-the-art solutions when\nemploying both social and textual content features.", "published": "2023-02-01 20:05:53", "link": "http://arxiv.org/abs/2302.01756v1", "categories": ["cs.AI", "cs.CL", "cs.NE", "cs.SI", "68T50"], "primary_category": "cs.AI"}
{"title": "A Transaction Represented with Weighted Finite-State Transducers", "abstract": "Not all contracts are good, but all good contracts can be expressed as a\nfinite-state transition system (\"State-Transition Contracts\"). Contracts that\ncan be represented as State-Transition Contracts discretize fat-tailed risk to\nforeseeable, managed risk, define the boundary of relevant events governed by\nthe relationship, and eliminate the potential of inconsistent contractual\nprovisions. Additionally, State-Transition Contracts reap the substantial\nbenefit of being able to be analyzed under the rules governing the science of\nthe theory of computation. Simple State-Transition Contracts can be represented\nas discrete finite automata; more complicated State-Transition Contracts, such\nas those that have downstream effects on other agreements or complicated\npathways of performance, benefit from representation as weighted finite-state\ntransducers, with weights assigned as costs, penalties, or probabilities of\ntransitions. This research paper (the \"Research\" or \"Paper\") presents a complex\nlegal transaction represented as weighted finite-state transducers.\nFurthermore, we show that the mathematics/algorithms permitted by the algebraic\nstructure of weighted finite-state transducers provides actionable, legal\ninsight into the transaction.", "published": "2023-02-01 03:05:14", "link": "http://arxiv.org/abs/2302.00200v1", "categories": ["cs.FL", "cs.AI", "cs.CL", "cs.LG", "math.RA"], "primary_category": "cs.FL"}
{"title": "Jointist: Simultaneous Improvement of Multi-instrument Transcription and\n  Music Source Separation via Joint Training", "abstract": "In this paper, we introduce Jointist, an instrument-aware multi-instrument\nframework that is capable of transcribing, recognizing, and separating multiple\nmusical instruments from an audio clip. Jointist consists of an instrument\nrecognition module that conditions the other two modules: a transcription\nmodule that outputs instrument-specific piano rolls, and a source separation\nmodule that utilizes instrument information and transcription results. The\njoint training of the transcription and source separation modules serves to\nimprove the performance of both tasks. The instrument module is optional and\ncan be directly controlled by human users. This makes Jointist a flexible\nuser-controllable framework. Our challenging problem formulation makes the\nmodel highly useful in the real world given that modern popular music typically\nconsists of multiple instruments. Its novelty, however, necessitates a new\nperspective on how to evaluate such a model. In our experiments, we assess the\nproposed model from various aspects, providing a new evaluation perspective for\nmulti-instrument transcription. Our subjective listening study shows that\nJointist achieves state-of-the-art performance on popular music, outperforming\nexisting multi-instrument transcription models such as MT3. We conducted\nexperiments on several downstream tasks and found that the proposed method\nimproved transcription by more than 1 percentage points (ppt.), source\nseparation by 5 SDR, downbeat detection by 1.8 ppt., chord recognition by 1.4\nppt., and key estimation by 1.4 ppt., when utilizing transcription results\nobtained from Jointist.\n  Demo available at \\url{https://jointist.github.io/Demo}.", "published": "2023-02-01 07:35:02", "link": "http://arxiv.org/abs/2302.00286v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Epic-Sounds: A Large-scale Dataset of Actions That Sound", "abstract": "We introduce Epic-Sounds, a large-scale dataset of audio annotations\ncapturing temporal extents and class labels within the audio stream of the\negocentric videos. We propose an annotation pipeline where annotators\ntemporally label distinguishable audio segments and describe the action that\ncould have caused this sound. We identify actions that can be discriminated\npurely from audio, through grouping these free-form descriptions of audio into\nclasses. For actions that involve objects colliding, we collect human\nannotations of the materials of these objects (e.g. a glass object being placed\non a wooden surface), which we verify from video, discarding ambiguities.\nOverall, Epic-Sounds includes 78.4k categorised segments of audible events and\nactions, distributed across 44 classes as well as 39.2k non-categorised\nsegments. We train and evaluate state-of-the-art audio recognition and\ndetection models on our dataset, for both audio-only and audio-visual methods.\nWe also conduct analysis on: the temporal overlap between audio events, the\ntemporal and label correlations between audio and visual modalities, the\nambiguities in annotating materials from audio-only input, the importance of\naudio-only labels and the limitations of current models to understand actions\nthat sound. Project page : https://epic-kitchens.github.io/epic-sounds/", "published": "2023-02-01 18:19:37", "link": "http://arxiv.org/abs/2302.00646v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
