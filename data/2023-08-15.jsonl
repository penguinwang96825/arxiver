{"title": "VBD-MT Chinese-Vietnamese Translation Systems for VLSP 2022", "abstract": "We present our systems participated in the VLSP 2022 machine translation\nshared task. In the shared task this year, we participated in both translation\ntasks, i.e., Chinese-Vietnamese and Vietnamese-Chinese translations. We build\nour systems based on the neural-based Transformer model with the powerful\nmultilingual denoising pre-trained model mBART. The systems are enhanced by a\nsampling method for backtranslation, which leverage large scale available\nmonolingual data. Additionally, several other methods are applied to improve\nthe translation quality including ensembling and postprocessing. We achieve\n38.9 BLEU on ChineseVietnamese and 38.0 BLEU on VietnameseChinese on the public\ntest sets, which outperform several strong baselines.", "published": "2023-08-15 07:10:41", "link": "http://arxiv.org/abs/2308.07601v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Steering Language Generation: Harnessing Contrastive Expert Guidance and\n  Negative Prompting for Coherent and Diverse Synthetic Data Generation", "abstract": "Large Language Models (LLMs) hold immense potential to generate synthetic\ndata of high quality and utility, which has numerous applications from\ndownstream model training to practical data utilisation. However, contemporary\nmodels, despite their impressive capacities, consistently struggle to produce\nboth coherent and diverse data. To address the coherency issue, we introduce\ncontrastive expert guidance, where the difference between the logit\ndistributions of fine-tuned and base language models is emphasised to ensure\ndomain adherence. In order to ensure diversity, we utilise existing real and\nsynthetic examples as negative prompts to the model. We deem this dual-pronged\napproach to logit reshaping as STEER: Semantic Text Enhancement via Embedding\nRepositioning. STEER operates at inference-time and systematically guides the\nLLMs to strike a balance between adherence to the data distribution (ensuring\nsemantic fidelity) and deviation from prior synthetic examples or existing real\ndatasets (ensuring diversity and authenticity). This delicate balancing act is\nachieved by dynamically moving towards or away from chosen representations in\nthe latent space. STEER demonstrates improved performance over previous\nsynthetic data generation techniques, exhibiting better balance between data\ndiversity and coherency across three distinct tasks: hypothesis generation,\ntoxic and non-toxic comment generation, and commonsense reasoning task\ngeneration. We demonstrate how STEER allows for fine-tuned control over the\ndiversity-coherency trade-off via its hyperparameters, highlighting its\nversatility.", "published": "2023-08-15 08:49:14", "link": "http://arxiv.org/abs/2308.07645v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Better Zero-Shot Reasoning with Role-Play Prompting", "abstract": "Modern large language models (LLMs) exhibit a remarkable capacity for\nrole-playing, enabling them to embody not only human characters but also\nnon-human entities. This versatility allows them to simulate complex human-like\ninteractions and behaviors within various contexts, as well as to emulate\nspecific objects or systems. While these capabilities have enhanced user\nengagement and introduced novel modes of interaction, the influence of\nrole-playing on LLMs' reasoning abilities remains underexplored. In this study,\nwe introduce a strategically designed role-play prompting methodology and\nassess its performance under the zero-shot setting across twelve diverse\nreasoning benchmarks. Our empirical results illustrate that role-play prompting\nconsistently surpasses the standard zero-shot approach across most datasets.\nNotably, in experiments conducted using ChatGPT, accuracy on AQuA rises from\n53.5% to 63.8%, and on Last Letter from 23.8% to 84.2%.Upon further comparison\nwith the Zero-Shot-CoT technique, which prompts the model to \"think step by\nstep\", our study demonstrates that role-play prompting acts as a more effective\ntrigger for the CoT process. This highlights its potential to augment the\nreasoning capabilities of LLMs. We release our code at\nhttps://github.com/NKU-HLT/Role-Play-Prompting.", "published": "2023-08-15 11:08:30", "link": "http://arxiv.org/abs/2308.07702v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Visually-Rich Document Understanding via Layout Structure\n  Modeling", "abstract": "In recent years, the use of multi-modal pre-trained Transformers has led to\nsignificant advancements in visually-rich document understanding. However,\nexisting models have mainly focused on features such as text and vision while\nneglecting the importance of layout relationship between text nodes. In this\npaper, we propose GraphLayoutLM, a novel document understanding model that\nleverages the modeling of layout structure graph to inject document layout\nknowledge into the model. GraphLayoutLM utilizes a graph reordering algorithm\nto adjust the text sequence based on the graph structure. Additionally, our\nmodel uses a layout-aware multi-head self-attention layer to learn document\nlayout knowledge. The proposed model enables the understanding of the spatial\narrangement of text elements, improving document comprehension. We evaluate our\nmodel on various benchmarks, including FUNSD, XFUND and CORD, and achieve\nstate-of-the-art results among these datasets. Our experimental results\ndemonstrate that our proposed method provides a significant improvement over\nexisting approaches and showcases the importance of incorporating layout\ninformation into document understanding models. We also conduct an ablation\nstudy to investigate the contribution of each component of our model. The\nresults show that both the graph reordering algorithm and the layout-aware\nmulti-head self-attention layer play a crucial role in achieving the best\nperformance.", "published": "2023-08-15 13:53:52", "link": "http://arxiv.org/abs/2308.07777v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Teach LLMs to Personalize -- An Approach inspired by Writing Education", "abstract": "Personalized text generation is an emerging research area that has attracted\nmuch attention in recent years. Most studies in this direction focus on a\nparticular domain by designing bespoke features or models. In this work, we\npropose a general approach for personalized text generation using large\nlanguage models (LLMs). Inspired by the practice of writing education, we\ndevelop a multistage and multitask framework to teach LLMs for personalized\ngeneration. In writing instruction, the task of writing from sources is often\ndecomposed into multiple steps that involve finding, evaluating, summarizing,\nsynthesizing, and integrating information. Analogously, our approach to\npersonalized text generation consists of multiple stages: retrieval, ranking,\nsummarization, synthesis, and generation. In addition, we introduce a multitask\nsetting that helps the model improve its generation ability further, which is\ninspired by the observation in education that a student's reading proficiency\nand writing ability are often correlated. We evaluate our approach on three\npublic datasets, each of which covers a different and representative domain.\nOur results show significant improvements over a variety of baselines.", "published": "2023-08-15 18:06:23", "link": "http://arxiv.org/abs/2308.07968v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Beware of deception\": Detecting Half-Truth and Debunking it through\n  Controlled Claim Editing", "abstract": "The prevalence of half-truths, which are statements containing some truth but\nthat are ultimately deceptive, has risen with the increasing use of the\ninternet. To help combat this problem, we have created a comprehensive pipeline\nconsisting of a half-truth detection model and a claim editing model. Our\napproach utilizes the T5 model for controlled claim editing; \"controlled\" here\nmeans precise adjustments to select parts of a claim. Our methodology achieves\nan average BLEU score of 0.88 (on a scale of 0-1) and a disinfo-debunk score of\n85% on edited claims. Significantly, our T5-based approach outperforms other\nLanguage Models such as GPT2, RoBERTa, PEGASUS, and Tailor, with average\nimprovements of 82%, 57%, 42%, and 23% in disinfo-debunk scores, respectively.\nBy extending the LIAR PLUS dataset, we achieve an F1 score of 82% for the\nhalf-truth detection model, setting a new benchmark in the field. While\nprevious attempts have been made at half-truth detection, our approach is, to\nthe best of our knowledge, the first to attempt to debunk half-truths.", "published": "2023-08-15 18:21:26", "link": "http://arxiv.org/abs/2308.07973v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Anaphoric Structure Emerges Between Neural Networks", "abstract": "Pragmatics is core to natural language, enabling speakers to communicate\nefficiently with structures like ellipsis and anaphora that can shorten\nutterances without loss of meaning. These structures require a listener to\ninterpret an ambiguous form - like a pronoun - and infer the speaker's intended\nmeaning - who that pronoun refers to. Despite potential to introduce ambiguity,\nanaphora is ubiquitous across human language. In an effort to better understand\nthe origins of anaphoric structure in natural language, we look to see if\nanalogous structures can emerge between artificial neural networks trained to\nsolve a communicative task. We show that: first, despite the potential for\nincreased ambiguity, languages with anaphoric structures are learnable by\nneural models. Second, anaphoric structures emerge between models 'naturally'\nwithout need for additional constraints. Finally, introducing an explicit\nefficiency pressure on the speaker increases the prevalence of these\nstructures. We conclude that certain pragmatic structures straightforwardly\nemerge between neural networks, without explicit efficiency pressures, but that\nthe competing needs of speakers and listeners conditions the degree and nature\nof their emergence.", "published": "2023-08-15 18:34:26", "link": "http://arxiv.org/abs/2308.07984v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Artificial Populations to Study Psychological Phenomena in Neural\n  Models", "abstract": "The recent proliferation of research into transformer based natural language\nprocessing has led to a number of studies which attempt to detect the presence\nof human-like cognitive behavior in the models. We contend that, as is true of\nhuman psychology, the investigation of cognitive behavior in language models\nmust be conducted in an appropriate population of an appropriate size for the\nresults to be meaningful. We leverage work in uncertainty estimation in a novel\napproach to efficiently construct experimental populations. The resultant tool,\nPopulationLM, has been made open source. We provide theoretical grounding in\nthe uncertainty estimation literature and motivation from current cognitive\nwork regarding language models. We discuss the methodological lessons from\nother scientific communities and attempt to demonstrate their application to\ntwo artificial population studies. Through population based experimentation we\nfind that language models exhibit behavior consistent with typicality effects\namong categories highly represented in training. However, we find that language\nmodels don't tend to exhibit structural priming effects. Generally, our results\nshow that single models tend to over estimate the presence of cognitive\nbehaviors in neural models.", "published": "2023-08-15 20:47:51", "link": "http://arxiv.org/abs/2308.08032v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DS4DH at #SMM4H 2023: Zero-Shot Adverse Drug Events Normalization using\n  Sentence Transformers and Reciprocal-Rank Fusion", "abstract": "This paper outlines the performance evaluation of a system for adverse drug\nevent normalization, developed by the Data Science for Digital Health (DS4DH)\ngroup for the Social Media Mining for Health Applications (SMM4H) 2023 shared\ntask 5. Shared task 5 targeted the normalization of adverse drug event mentions\nin Twitter to standard concepts of the Medical Dictionary for Regulatory\nActivities terminology. Our system hinges on a two-stage approach: BERT\nfine-tuning for entity recognition, followed by zero-shot normalization using\nsentence transformers and reciprocal-rank fusion. The approach yielded a\nprecision of 44.9%, recall of 40.5%, and an F1-score of 42.6%. It outperformed\nthe median performance in shared task 5 by 10% and demonstrated the highest\nperformance among all participants. These results substantiate the\neffectiveness of our approach and its potential application for adverse drug\nevent normalization in the realm of social media text mining.", "published": "2023-08-15 14:07:59", "link": "http://arxiv.org/abs/2308.12877v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Race Detection Using Large Language Models", "abstract": "Large language models (LLMs) are demonstrating significant promise as an\nalternate strategy to facilitate analyses and optimizations of high-performance\ncomputing programs, circumventing the need for resource-intensive manual tool\ncreation. In this paper, we explore a novel LLM-based data race detection\napproach combining prompting engineering and fine-tuning techniques. We create\na dedicated dataset named DRB-ML, which is derived from DataRaceBench, with\nfine-grain labels showing the presence of data race pairs and their associated\nvariables, line numbers, and read/write information. DRB-ML is then used to\nevaluate representative LLMs and fine-tune open-source ones. Our experiment\nshows that LLMs can be a viable approach to data race detection. However, they\nstill cannot compete with traditional data race detection tools when we need\ndetailed information about variable pairs causing data races.", "published": "2023-08-15 00:08:43", "link": "http://arxiv.org/abs/2308.07505v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Finding Stakeholder-Material Information from 10-K Reports using\n  Fine-Tuned BERT and LSTM Models", "abstract": "All public companies are required by federal securities law to disclose their\nbusiness and financial activities in their annual 10-K reports. Each report\ntypically spans hundreds of pages, making it difficult for human readers to\nidentify and extract the material information efficiently. To solve the\nproblem, I have fine-tuned BERT models and RNN models with LSTM layers to\nidentify stakeholder-material information, defined as statements that carry\ninformation about a company's influence on its stakeholders, including\ncustomers, employees, investors, and the community and natural environment. The\nexisting practice uses keyword search to identify such information, which is my\nbaseline model. Using business expert-labeled training data of nearly 6,000\nsentences from 62 10-K reports published in 2022, the best model has achieved\nan accuracy of 0.904 and an F1 score of 0.899 in test data, significantly above\nthe baseline model's 0.781 and 0.749 respectively. Furthermore, the same work\nwas replicated on more granular taxonomies, based on which four distinct groups\nof stakeholders (i.e., customers, investors, employees, and the community and\nnatural environment) are tested separately. Similarly, fined-tuned BERT models\noutperformed LSTM and the baseline. The implications for industry application\nand ideas for future extensions are discussed.", "published": "2023-08-15 01:25:34", "link": "http://arxiv.org/abs/2308.07522v1", "categories": ["cs.CL", "cs.CE"], "primary_category": "cs.CL"}
{"title": "CALYPSO: LLMs as Dungeon Masters' Assistants", "abstract": "The role of a Dungeon Master, or DM, in the game Dungeons & Dragons is to\nperform multiple tasks simultaneously. The DM must digest information about the\ngame setting and monsters, synthesize scenes to present to other players, and\nrespond to the players' interactions with the scene. Doing all of these tasks\nwhile maintaining consistency within the narrative and story world is no small\nfeat of human cognition, making the task tiring and unapproachable to new\nplayers. Large language models (LLMs) like GPT-3 and ChatGPT have shown\nremarkable abilities to generate coherent natural language text. In this paper,\nwe conduct a formative evaluation with DMs to establish the use cases of LLMs\nin D&D and tabletop gaming generally. We introduce CALYPSO, a system of\nLLM-powered interfaces that support DMs with information and inspiration\nspecific to their own scenario. CALYPSO distills game context into bite-sized\nprose and helps brainstorm ideas without distracting the DM from the game. When\ngiven access to CALYPSO, DMs reported that it generated high-fidelity text\nsuitable for direct presentation to players, and low-fidelity ideas that the DM\ncould develop further while maintaining their creative agency. We see CALYPSO\nas exemplifying a paradigm of AI-augmented tools that provide synchronous\ncreative assistance within established game worlds, and tabletop gaming more\nbroadly.", "published": "2023-08-15 02:57:00", "link": "http://arxiv.org/abs/2308.07540v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "A User-Centered Evaluation of Spanish Text Simplification", "abstract": "We present an evaluation of text simplification (TS) in Spanish for a\nproduction system, by means of two corpora focused in both complex-sentence and\ncomplex-word identification. We compare the most prevalent Spanish-specific\nreadability scores with neural networks, and show that the latter are\nconsistently better at predicting user preferences regarding TS. As part of our\nanalysis, we find that multilingual models underperform against equivalent\nSpanish-only models on the same task, yet all models focus too often on\nspurious statistical features, such as sentence length. We release the corpora\nin our evaluation to the broader community with the hopes of pushing forward\nthe state-of-the-art in Spanish natural language processing.", "published": "2023-08-15 03:49:59", "link": "http://arxiv.org/abs/2308.07556v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interpretable Online Log Analysis Using Large Language Models with\n  Prompt Strategies", "abstract": "Automated log analysis is crucial in modern software-intensive systems for\nfacilitating program comprehension throughout software maintenance and\nengineering life cycles. Existing methods perform tasks such as log parsing and\nlog anomaly detection by providing a single prediction value without\ninterpretation. However, given the increasing volume of system events, the\nlimited interpretability of analysis results hinders analysts' comprehension of\nprogram status and their ability to take appropriate actions. Moreover, these\nmethods require substantial in-domain training data, and their performance\ndeclines sharply (by up to 62.5%) in online scenarios involving unseen logs\nfrom new domains, a common occurrence due to rapid software updates. In this\npaper, we propose LogPrompt, a novel interpretable log analysis approach for\nonline scenarios. LogPrompt employs large language models (LLMs) to perform\nonline log analysis tasks via a suite of advanced prompt strategies tailored\nfor log tasks, which enhances LLMs' performance by up to 380.7% compared with\nsimple prompts. Experiments on nine publicly available evaluation datasets\nacross two tasks demonstrate that LogPrompt, despite requiring no in-domain\ntraining, outperforms existing approaches trained on thousands of logs by up to\n55.9%. We also conduct a human evaluation of LogPrompt's interpretability, with\nsix practitioners possessing over 10 years of experience, who highly rated the\ngenerated content in terms of usefulness and readability (averagely 4.42/5).\nLogPrompt also exhibits remarkable compatibility with open-source and\nsmaller-scale LLMs, making it flexible for practical deployment. Code of\nLogPrompt is available at https://github.com/lunyiliu/LogPrompt.", "published": "2023-08-15 07:40:21", "link": "http://arxiv.org/abs/2308.07610v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "A Survey on Model Compression for Large Language Models", "abstract": "Large Language Models (LLMs) have transformed natural language processing\ntasks successfully. Yet, their large size and high computational needs pose\nchallenges for practical use, especially in resource-limited settings. Model\ncompression has emerged as a key research area to address these challenges.\nThis paper presents a survey of model compression techniques for LLMs. We cover\nmethods like quantization, pruning, and knowledge distillation, highlighting\nrecent advancements. We also discuss benchmarking strategies and evaluation\nmetrics crucial for assessing compressed LLMs. This survey offers valuable\ninsights for researchers and practitioners, aiming to enhance efficiency and\nreal-world applicability of LLMs while laying a foundation for future\nadvancements.", "published": "2023-08-15 08:31:05", "link": "http://arxiv.org/abs/2308.07633v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM-Mini-CEX: Automatic Evaluation of Large Language Model for\n  Diagnostic Conversation", "abstract": "There is an increasing interest in developing LLMs for medical diagnosis to\nimprove diagnosis efficiency. Despite their alluring technological potential,\nthere is no unified and comprehensive evaluation criterion, leading to the\ninability to evaluate the quality and potential risks of medical LLMs, further\nhindering the application of LLMs in medical treatment scenarios. Besides,\ncurrent evaluations heavily rely on labor-intensive interactions with LLMs to\nobtain diagnostic dialogues and human evaluation on the quality of diagnosis\ndialogue. To tackle the lack of unified and comprehensive evaluation criterion,\nwe first initially establish an evaluation criterion, termed LLM-specific\nMini-CEX to assess the diagnostic capabilities of LLMs effectively, based on\noriginal Mini-CEX. To address the labor-intensive interaction problem, we\ndevelop a patient simulator to engage in automatic conversations with LLMs, and\nutilize ChatGPT for evaluating diagnosis dialogues automatically. Experimental\nresults show that the LLM-specific Mini-CEX is adequate and necessary to\nevaluate medical diagnosis dialogue. Besides, ChatGPT can replace manual\nevaluation on the metrics of humanistic qualities and provides reproducible and\nautomated comparisons between different LLMs.", "published": "2023-08-15 08:32:20", "link": "http://arxiv.org/abs/2308.07635v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SPM: Structured Pretraining and Matching Architectures for Relevance\n  Modeling in Meituan Search", "abstract": "In e-commerce search, relevance between query and documents is an essential\nrequirement for satisfying user experience. Different from traditional\ne-commerce platforms that offer products, users search on life service\nplatforms such as Meituan mainly for product providers, which usually have\nabundant structured information, e.g. name, address, category, thousands of\nproducts. Modeling search relevance with these rich structured contents is\nchallenging due to the following issues: (1) there is language distribution\ndiscrepancy among different fields of structured document, making it difficult\nto directly adopt off-the-shelf pretrained language model based methods like\nBERT. (2) different fields usually have different importance and their length\nvary greatly, making it difficult to extract document information helpful for\nrelevance matching.\n  To tackle these issues, in this paper we propose a novel two-stage\npretraining and matching architecture for relevance matching with rich\nstructured documents. At pretraining stage, we propose an effective pretraining\nmethod that employs both query and multiple fields of document as inputs,\nincluding an effective information compression method for lengthy fields. At\nrelevance matching stage, a novel matching method is proposed by leveraging\ndomain knowledge in search query to generate more effective document\nrepresentations for relevance scoring. Extensive offline experiments and online\nA/B tests on millions of users verify that the proposed architectures\neffectively improve the performance of relevance modeling. The model has\nalready been deployed online, serving the search traffic of Meituan for over a\nyear.", "published": "2023-08-15 11:45:34", "link": "http://arxiv.org/abs/2308.07711v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Comprehensive Study on Knowledge Graph Embedding over Relational\n  Patterns Based on Rule Learning", "abstract": "Knowledge Graph Embedding (KGE) has proven to be an effective approach to\nsolving the Knowledge Graph Completion (KGC) task. Relational patterns which\nrefer to relations with specific semantics exhibiting graph patterns are an\nimportant factor in the performance of KGE models. Though KGE models'\ncapabilities are analyzed over different relational patterns in theory and a\nrough connection between better relational patterns modeling and better\nperformance of KGC has been built, a comprehensive quantitative analysis on KGE\nmodels over relational patterns remains absent so it is uncertain how the\ntheoretical support of KGE to a relational pattern contributes to the\nperformance of triples associated to such a relational pattern. To address this\nchallenge, we evaluate the performance of 7 KGE models over 4 common relational\npatterns on 2 benchmarks, then conduct an analysis in theory, entity frequency,\nand part-to-whole three aspects and get some counterintuitive conclusions.\nFinally, we introduce a training-free method Score-based Patterns Adaptation\n(SPA) to enhance KGE models' performance over various relational patterns. This\napproach is simple yet effective and can be applied to KGE models without\nadditional training. Our experimental results demonstrate that our method\ngenerally enhances performance over specific relational patterns. Our source\ncode is available from GitHub at\nhttps://github.com/zjukg/Comprehensive-Study-over-Relational-Patterns.", "published": "2023-08-15 17:30:57", "link": "http://arxiv.org/abs/2308.07889v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Link-Context Learning for Multimodal LLMs", "abstract": "The ability to learn from context with novel concepts, and deliver\nappropriate responses are essential in human conversations. Despite current\nMultimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being\ntrained on mega-scale datasets, recognizing unseen images or understanding\nnovel concepts in a training-free manner remains a challenge. In-Context\nLearning (ICL) explores training-free few-shot learning, where models are\nencouraged to ``learn to learn\" from limited tasks and generalize to unseen\ntasks. In this work, we propose link-context learning (LCL), which emphasizes\n\"reasoning from cause and effect\" to augment the learning capabilities of\nMLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal\nrelationship between the support set and the query set. By providing\ndemonstrations with causal links, LCL guides the model to discern not only the\nanalogy but also the underlying causal associations between data points, which\nempowers MLLMs to recognize unseen images and understand novel concepts more\neffectively. To facilitate the evaluation of this novel approach, we introduce\nthe ISEKAI dataset, comprising exclusively of unseen generated image-label\npairs designed for link-context learning. Extensive experiments show that our\nLCL-MLLM exhibits strong link-context learning capabilities to novel concepts\nover vanilla MLLMs. Code and data will be released at\nhttps://github.com/isekai-portal/Link-Context-Learning.", "published": "2023-08-15 17:33:24", "link": "http://arxiv.org/abs/2308.07891v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Through the Lens of Core Competency: Survey on Evaluation of Large\n  Language Models", "abstract": "From pre-trained language model (PLM) to large language model (LLM), the\nfield of natural language processing (NLP) has witnessed steep performance\ngains and wide practical uses. The evaluation of a research field guides its\ndirection of improvement. However, LLMs are extremely hard to thoroughly\nevaluate for two reasons. First of all, traditional NLP tasks become inadequate\ndue to the excellent performance of LLM. Secondly, existing evaluation tasks\nare difficult to keep up with the wide range of applications in real-world\nscenarios. To tackle these problems, existing works proposed various benchmarks\nto better evaluate LLMs. To clarify the numerous evaluation tasks in both\nacademia and industry, we investigate multiple papers concerning LLM\nevaluations. We summarize 4 core competencies of LLM, including reasoning,\nknowledge, reliability, and safety. For every competency, we introduce its\ndefinition, corresponding benchmarks, and metrics. Under this competency\narchitecture, similar tasks are combined to reflect corresponding ability,\nwhile new tasks can also be easily added into the system. Finally, we give our\nsuggestions on the future direction of LLM's evaluation.", "published": "2023-08-15 17:40:34", "link": "http://arxiv.org/abs/2308.07902v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MultiSChuBERT: Effective Multimodal Fusion for Scholarly Document\n  Quality Prediction", "abstract": "Automatic assessment of the quality of scholarly documents is a difficult\ntask with high potential impact. Multimodality, in particular the addition of\nvisual information next to text, has been shown to improve the performance on\nscholarly document quality prediction (SDQP) tasks. We propose the multimodal\npredictive model MultiSChuBERT. It combines a textual model based on chunking\nfull paper text and aggregating computed BERT chunk-encodings (SChuBERT), with\na visual model based on Inception V3.Our work contributes to the current\nstate-of-the-art in SDQP in three ways. First, we show that the method of\ncombining visual and textual embeddings can substantially influence the\nresults. Second, we demonstrate that gradual-unfreezing of the weights of the\nvisual sub-model, reduces its tendency to ovefit the data, improving results.\nThird, we show the retained benefit of multimodality when replacing standard\nBERT$_{\\textrm{BASE}}$ embeddings with more recent state-of-the-art text\nembedding models.\n  Using BERT$_{\\textrm{BASE}}$ embeddings, on the (log) number of citations\nprediction task with the ACL-BiblioMetry dataset, our MultiSChuBERT\n(text+visual) model obtains an $R^{2}$ score of 0.454 compared to 0.432 for the\nSChuBERT (text only) model. Similar improvements are obtained on the PeerRead\naccept/reject prediction task. In our experiments using SciBERT, scincl,\nSPECTER and SPECTER2.0 embeddings, we show that each of these tailored\nembeddings adds further improvements over the standard BERT$_{\\textrm{BASE}}$\nembeddings, with the SPECTER2.0 embeddings performing best.", "published": "2023-08-15 18:18:34", "link": "http://arxiv.org/abs/2308.07971v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "DiagGPT: An LLM-based and Multi-agent Dialogue System with Automatic\n  Topic Management for Flexible Task-Oriented Dialogue", "abstract": "A significant application of Large Language Models (LLMs), like ChatGPT, is\ntheir deployment as chat agents, which respond to human inquiries across a\nvariety of domains. While current LLMs proficiently answer general questions,\nthey often fall short in complex diagnostic scenarios such as legal, medical,\nor other specialized consultations. These scenarios typically require\nTask-Oriented Dialogue (TOD), where an AI chat agent must proactively pose\nquestions and guide users toward specific goals or task completion. Previous\nfine-tuning models have underperformed in TOD and the full potential of\nconversational capability in current LLMs has not yet been fully explored. In\nthis paper, we introduce DiagGPT (Dialogue in Diagnosis GPT), an innovative\napproach that extends LLMs to more TOD scenarios. In addition to guiding users\nto complete tasks, DiagGPT can effectively manage the status of all topics\nthroughout the dialogue development. This feature enhances user experience and\noffers a more flexible interaction in TOD. Our experiments demonstrate that\nDiagGPT exhibits outstanding performance in conducting TOD with users, showing\nits potential for practical applications in various fields.", "published": "2023-08-15 21:14:09", "link": "http://arxiv.org/abs/2308.08043v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detection of ChatGPT Fake Science with the xFakeSci Learning Algorithm", "abstract": "Generative AI tools exemplified by ChatGPT are becoming a new reality. This\nstudy is motivated by the premise that ``AI generated content may exhibit a\ndistinctive behavior that can be separated from scientific articles''. In this\nstudy, we show how articles can be generated using means of prompt engineering\nfor various diseases and conditions. We then show how we tested this premise in\ntwo phases and prove its validity. Subsequently, we introduce xFakeSci, a novel\nlearning algorithm, that is capable of distinguishing ChatGPT-generated\narticles from publications produced by scientists. The algorithm is trained\nusing network models driven from both sources. As for the classification step,\nit was performed using 300 articles per condition. The actual label steps took\nplace against an equal mix of 50 generated articles and 50 authentic PubMed\nabstracts. The testing also spanned publication periods from 2010 to 2024 and\nencompassed research on three distinct diseases: cancer, depression, and\nAlzheimer's. Further, we evaluated the accuracy of the xFakeSci algorithm\nagainst some of the classical data mining algorithms (e.g., Support Vector\nMachines, Regression, and Naive Bayes). The xFakeSci algorithm achieved F1\nscores ranging from 80% to 94%, outperforming common data mining algorithms,\nwhich scored F1 values between 38% and 52%. We attribute the noticeable\ndifference to the introduction of calibration and a proximity distance\nheuristic, which underscores this promising performance. Indeed, the prediction\nof fake science generated by ChatGPT presents a considerable challenge.\nNonetheless, the introduction of the xFakeSci algorithm is a significant step\non the way to combating fake science.", "published": "2023-08-15 23:22:37", "link": "http://arxiv.org/abs/2308.11767v4", "categories": ["cs.CL", "cs.IR", "I.2.7; F.2; I.7"], "primary_category": "cs.CL"}
{"title": "SEER: Super-Optimization Explorer for HLS using E-graph Rewriting with\n  MLIR", "abstract": "High-level synthesis (HLS) is a process that automatically translates a\nsoftware program in a high-level language into a low-level hardware\ndescription. However, the hardware designs produced by HLS tools still suffer\nfrom a significant performance gap compared to manual implementations. This is\nbecause the input HLS programs must still be written using hardware design\nprinciples.\n  Existing techniques either leave the program source unchanged or perform a\nfixed sequence of source transformation passes, potentially missing\nopportunities to find the optimal design. We propose a super-optimization\napproach for HLS that automatically rewrites an arbitrary software program into\nefficient HLS code that can be used to generate an optimized hardware design.\nWe developed a toolflow named SEER, based on the e-graph data structure, to\nefficiently explore equivalent implementations of a program at scale. SEER\nprovides an extensible framework, orchestrating existing software compiler\npasses and hardware synthesis optimizers.\n  Our work is the first attempt to exploit e-graph rewriting for large software\ncompiler frameworks, such as MLIR. Across a set of open-source benchmarks, we\nshow that SEER achieves up to 38x the performance within 1.4x the area of the\noriginal program. Via an Intel-provided case study, SEER demonstrates the\npotential to outperform manually optimized designs produced by hardware\nexperts.", "published": "2023-08-15 09:05:27", "link": "http://arxiv.org/abs/2308.07654v1", "categories": ["cs.PL", "cs.AR", "cs.CL"], "primary_category": "cs.PL"}
{"title": "Attention Is Not All You Need Anymore", "abstract": "In recent years, the popular Transformer architecture has achieved great\nsuccess in many application areas, including natural language processing and\ncomputer vision. Many existing works aim to reduce the computational and memory\ncomplexity of the self-attention mechanism in the Transformer by trading off\nperformance. However, performance is key for the continuing success of the\nTransformer. In this paper, a family of drop-in replacements for the\nself-attention mechanism in the Transformer, called the Extractors, is\nproposed. Four types of the Extractors, namely the super high-performance\nExtractor (SHE), the higher-performance Extractor (HE), the worthwhile\nExtractor (WE), and the minimalist Extractor (ME), are proposed as examples.\nExperimental results show that replacing the self-attention mechanism with the\nSHE evidently improves the performance of the Transformer, whereas the\nsimplified versions of the SHE, i.e., the HE, the WE, and the ME, perform close\nto or better than the self-attention mechanism with less computational and\nmemory complexity. Furthermore, the proposed Extractors have the potential or\nare able to run faster than the self-attention mechanism since their critical\npaths of computation are much shorter. Additionally, the sequence prediction\nproblem in the context of text generation is formulated using variable-length\ndiscrete-time Markov chains, and the Transformer is reviewed based on our\nunderstanding.", "published": "2023-08-15 09:24:38", "link": "http://arxiv.org/abs/2308.07661v2", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Exploring Transfer Learning in Medical Image Segmentation using\n  Vision-Language Models", "abstract": "Medical image segmentation allows quantifying target structure size and\nshape, aiding in disease diagnosis, prognosis, surgery planning, and\ncomprehension.Building upon recent advancements in foundation Vision-Language\nModels (VLMs) from natural image-text pairs, several studies have proposed\nadapting them to Vision-Language Segmentation Models (VLSMs) that allow using\nlanguage text as an additional input to segmentation models. Introducing\nauxiliary information via text with human-in-the-loop prompting during\ninference opens up unique opportunities, such as open vocabulary segmentation\nand potentially more robust segmentation models against out-of-distribution\ndata. Although transfer learning from natural to medical images has been\nexplored for image-only segmentation models, the joint representation of\nvision-language in segmentation problems remains underexplored. This study\nintroduces the first systematic study on transferring VLSMs to 2D medical\nimages, using carefully curated $11$ datasets encompassing diverse modalities\nand insightful language prompts and experiments. Our findings demonstrate that\nalthough VLSMs show competitive performance compared to image-only models for\nsegmentation after finetuning in limited medical image datasets, not all VLSMs\nutilize the additional information from language prompts, with image features\nplaying a dominant role. While VLSMs exhibit enhanced performance in handling\npooled datasets with diverse modalities and show potential robustness to domain\nshifts compared to conventional segmentation models, our results suggest that\nnovel approaches are required to enable VLSMs to leverage the various auxiliary\ninformation available through language prompts. The code and datasets are\navailable at https://github.com/naamiinepal/medvlsm.", "published": "2023-08-15 11:28:21", "link": "http://arxiv.org/abs/2308.07706v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Forward-Backward Reasoning in Large Language Models for Mathematical\n  Verification", "abstract": "Self-Consistency samples diverse reasoning chains with answers and chooses\nthe final answer by majority voting. It is based on forward reasoning and\ncannot further improve performance by sampling more reasoning chains when\nsaturated. To further boost performance, we introduce backward reasoning to\nverify candidate answers. Specifically, for mathematical tasks, we mask a\nnumber in the question and ask the LLM to answer a backward question created by\na simple template, i.e., to predict the masked number when a candidate answer\nis provided. Instead of using forward or backward reasoning alone, we propose\nFOBAR to combine FOrward and BAckward Reasoning for verification. Extensive\nexperiments on six standard mathematical data sets and three LLMs show that\nFOBAR achieves state-of-the-art performance. In particular, FOBAR outperforms\nSelf-Consistency, which uses forward reasoning alone, demonstrating that\ncombining forward and forward reasoning is better. In addition, FOBAR performs\nbetter than existing verification methods, showing the effectiveness of the\nsimple template used in backward reasoning and the proposed combination.\nExtensions to non-mathematical problems are also discussed and validated\nempirically.", "published": "2023-08-15 13:19:59", "link": "http://arxiv.org/abs/2308.07758v6", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Informed Named Entity Recognition Decoding for Generative Language\n  Models", "abstract": "Ever-larger language models with ever-increasing capabilities are by now\nwell-established text processing tools. Alas, information extraction tasks such\nas named entity recognition are still largely unaffected by this progress as\nthey are primarily based on the previous generation of encoder-only transformer\nmodels. Here, we propose a simple yet effective approach, Informed Named Entity\nRecognition Decoding (iNERD), which treats named entity recognition as a\ngenerative process. It leverages the language understanding capabilities of\nrecent generative models in a future-proof manner and employs an informed\ndecoding scheme incorporating the restricted nature of information extraction\ninto open-ended text generation, improving performance and eliminating any risk\nof hallucinations. We coarse-tune our model on a merged named entity corpus to\nstrengthen its performance, evaluate five generative language models on eight\nnamed entity recognition datasets, and achieve remarkable results, especially\nin an environment with an unknown entity class set, demonstrating the\nadaptability of the approach.", "published": "2023-08-15 14:16:29", "link": "http://arxiv.org/abs/2308.07791v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Emotion Embeddings $\\unicode{x2014}$ Learning Stable and Homogeneous\n  Abstractions from Heterogeneous Affective Datasets", "abstract": "Human emotion is expressed in many communication modalities and media formats\nand so their computational study is equally diversified into natural language\nprocessing, audio signal analysis, computer vision, etc. Similarly, the large\nvariety of representation formats used in previous research to describe\nemotions (polarity scales, basic emotion categories, dimensional approaches,\nappraisal theory, etc.) have led to an ever proliferating diversity of\ndatasets, predictive models, and software tools for emotion analysis. Because\nof these two distinct types of heterogeneity, at the expressional and\nrepresentational level, there is a dire need to unify previous work on\nincreasingly diverging data and label types. This article presents such a\nunifying computational model. We propose a training procedure that learns a\nshared latent representation for emotions, so-called emotion embeddings,\nindependent of different natural languages, communication modalities, media or\nrepresentation label formats, and even disparate model architectures.\nExperiments on a wide range of heterogeneous affective datasets indicate that\nthis approach yields the desired interoperability for the sake of reusability,\ninterpretability and flexibility, without penalizing prediction quality. Code\nand data are archived under https://doi.org/10.5281/zenodo.7405327 .", "published": "2023-08-15 16:39:10", "link": "http://arxiv.org/abs/2308.07871v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Leveraging Codebook Knowledge with NLI and ChatGPT for Zero-Shot\n  Political Relation Classification", "abstract": "Is it possible accurately classify political relations within evolving event\nontologies without extensive annotations? This study investigates zero-shot\nlearning methods that use expert knowledge from existing annotation codebook,\nand evaluates the performance of advanced ChatGPT (GPT-3.5/4) and a natural\nlanguage inference (NLI)-based model called ZSP. ChatGPT uses codebook's\nlabeled summaries as prompts, whereas ZSP breaks down the classification task\ninto context, event mode, and class disambiguation to refine task-specific\nhypotheses. This decomposition enhances interpretability, efficiency, and\nadaptability to schema changes. The experiments reveal ChatGPT's strengths and\nlimitations, and crucially show ZSP's outperformance of dictionary-based\nmethods and its competitive edge over some supervised models. These findings\naffirm the value of ZSP for validating event records and advancing ontology\ndevelopment. Our study underscores the efficacy of leveraging transfer learning\nand existing domain expertise to enhance research efficiency and scalability.", "published": "2023-08-15 16:41:53", "link": "http://arxiv.org/abs/2308.07876v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Correct and Optimal: the Regular Expression Inference Challenge", "abstract": "We propose regular expression inference (REI) as a challenge for\ncode/language modelling, and the wider machine learning community. REI is a\nsupervised machine learning (ML) and program optimisation task, and poses the\nproblem of finding minimal regular expressions from examples: Given two finite\nsets of strings $P$ and $N$ and a cost function $cost(\\cdot)$, the task is to\ngenerate an expression $r$ that accepts all strings in $P$ and rejects all\nstrings in $N$, while no other such expression $r'$ exists with\n$cost(r')<cost(r)$. REI has advantages as a challenge problem: (i) regular\nexpressions are well-known, widely used, and a natural idealisation of code;\n(ii) REI's asymptotic worst-case complexity is well understood; (iii) REI has a\nsmall number of easy to understand parameters (e.g. $P$ or $N$ cardinality,\nstring lengths of examples, or the cost function); this lets us easily finetune\nREI-hardness; (iv) REI, with its emphasis on optimisation, is an unsolved\nproblem for deep learning based ML. Recently, an REI solver was implemented on\nGPUs, using program synthesis techniques. This enabled, for the first time,\nfast generation of minimal regular expressions for complex REI instances.\nBuilding on this advance, we generate and publish the first large-scale\ndatasets for REI, and devise and evaluate several initial heuristic and machine\nlearning baselines. We invite the community to participate and explore ML\nmethods that learn to solve REI problems. We believe that progress in REI\ndirectly translates to progress in code/language modelling.", "published": "2023-08-15 17:40:10", "link": "http://arxiv.org/abs/2308.07899v2", "categories": ["cs.LG", "cs.CL", "cs.FL"], "primary_category": "cs.LG"}
{"title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with\n  Code-based Self-Verification", "abstract": "Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 has\nbrought significant advancements in addressing math reasoning problems. In\nparticular, OpenAI's latest version of GPT-4, known as GPT-4 Code Interpreter,\nshows remarkable performance on challenging math datasets. In this paper, we\nexplore the effect of code on enhancing LLMs' reasoning capability by\nintroducing different constraints on the \\textit{Code Usage Frequency} of GPT-4\nCode Interpreter. We found that its success can be largely attributed to its\npowerful skills in generating and executing code, evaluating the output of code\nexecution, and rectifying its solution when receiving unreasonable outputs.\nBased on this insight, we propose a novel and effective prompting method,\nexplicit \\uline{c}ode-based \\uline{s}elf-\\uline{v}erification~(CSV), to further\nboost the mathematical reasoning potential of GPT-4 Code Interpreter. This\nmethod employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to\nuse code to self-verify its answers. In instances where the verification state\nregisters as ``False'', the model shall automatically amend its solution,\nanalogous to our approach of rectifying errors during a mathematics\nexamination. Furthermore, we recognize that the states of the verification\nresult indicate the confidence of a solution, which can improve the\neffectiveness of majority voting. With GPT-4 Code Interpreter and CSV, we\nachieve an impressive zero-shot accuracy on MATH dataset \\textbf{(53.9\\% $\\to$\n84.3\\%)}.", "published": "2023-08-15 17:58:45", "link": "http://arxiv.org/abs/2308.07921v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder\n  Language Models", "abstract": "In this paper, we investigate the in-context learning ability of\nretrieval-augmented encoder-decoder language models. We first conduct a\ncomprehensive analysis of existing models and identify their limitations in\nin-context learning, primarily due to a mismatch between pretraining and\ninference, as well as a restricted context length. To address these issues, we\npropose RAVEN, a model that combines retrieval-augmented masked language\nmodeling and prefix language modeling. We further introduce Fusion-in-Context\nLearning to enhance the few-shot performance by enabling the model to leverage\nmore in-context examples without requiring additional training. Through\nextensive experiments, we demonstrate that our simple yet effective design\nsignificantly improves performance, achieving results comparable to the most\nadvanced language models in certain scenarios, despite having substantially\nfewer parameters. Our work underscores the potential of retrieval-augmented\nencoder-decoder language models for in-context learning and encourages further\nresearch in this direction.", "published": "2023-08-15 17:59:18", "link": "http://arxiv.org/abs/2308.07922v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "End-to-End Open Vocabulary Keyword Search With Multilingual Neural\n  Representations", "abstract": "Conventional keyword search systems operate on automatic speech recognition\n(ASR) outputs, which causes them to have a complex indexing and search\npipeline. This has led to interest in ASR-free approaches to simplify the\nsearch procedure. We recently proposed a neural ASR-free keyword search model\nwhich achieves competitive performance while maintaining an efficient and\nsimplified pipeline, where queries and documents are encoded with a pair of\nrecurrent neural network encoders and the encodings are combined with a\ndot-product. In this article, we extend this work with multilingual pretraining\nand detailed analysis of the model. Our experiments show that the proposed\nmultilingual training significantly improves the model performance and that\ndespite not matching a strong ASR-based conventional keyword search system for\nshort queries and queries comprising in-vocabulary words, the proposed model\noutperforms the ASR-based system for long queries and queries that do not\nappear in the training data.", "published": "2023-08-15 20:33:25", "link": "http://arxiv.org/abs/2308.08027v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Costly Dilemma: Generalization, Evaluation and Cost-Optimal\n  Deployment of Large Language Models", "abstract": "When deploying machine learning models in production for any\nproduct/application, there are three properties that are commonly desired.\nFirst, the models should be generalizable, in that we can extend it to further\nuse cases as our knowledge of the domain area develops. Second they should be\nevaluable, so that there are clear metrics for performance and the calculation\nof those metrics in production settings are feasible. Finally, the deployment\nshould be cost-optimal as far as possible. In this paper we propose that these\nthree objectives (i.e. generalization, evaluation and cost-optimality) can\noften be relatively orthogonal and that for large language models, despite\ntheir performance over conventional NLP models, enterprises need to carefully\nassess all the three factors before making substantial investments in this\ntechnology. We propose a framework for generalization, evaluation and\ncost-modeling specifically tailored to large language models, offering insights\ninto the intricacies of development, deployment and management for these large\nlanguage models.", "published": "2023-08-15 22:26:58", "link": "http://arxiv.org/abs/2308.08061v1", "categories": ["cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Improving CTC-AED model with integrated-CTC and auxiliary loss\n  regularization", "abstract": "Connectionist temporal classification (CTC) and attention-based encoder\ndecoder (AED) joint training has been widely applied in automatic speech\nrecognition (ASR). Unlike most hybrid models that separately calculate the CTC\nand AED losses, our proposed integrated-CTC utilizes the attention mechanism of\nAED to guide the output of CTC. In this paper, we employ two fusion methods,\nnamely direct addition of logits (DAL) and preserving the maximum probability\n(PMP). We achieve dimensional consistency by adaptively affine transforming the\nattention results to match the dimensions of CTC. To accelerate model\nconvergence and improve accuracy, we introduce auxiliary loss regularization\nfor accelerated convergence. Experimental results demonstrate that the DAL\nmethod performs better in attention rescoring, while the PMP method excels in\nCTC prefix beam search and greedy search.", "published": "2023-08-15 03:31:47", "link": "http://arxiv.org/abs/2308.08449v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Trustable LSTM-Autoencoder Network for Cyberbullying Detection on\n  Social Media Using Synthetic Data", "abstract": "Social media cyberbullying has a detrimental effect on human life. As online\nsocial networking grows daily, the amount of hate speech also increases. Such\nterrible content can cause depression and actions related to suicide. This\npaper proposes a trustable LSTM-Autoencoder Network for cyberbullying detection\non social media using synthetic data. We have demonstrated a cutting-edge\nmethod to address data availability difficulties by producing\nmachine-translated data. However, several languages such as Hindi and Bangla\nstill lack adequate investigations due to a lack of datasets. We carried out\nexperimental identification of aggressive comments on Hindi, Bangla, and\nEnglish datasets using the proposed model and traditional models, including\nLong Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM),\nLSTM-Autoencoder, Word2vec, Bidirectional Encoder Representations from\nTransformers (BERT), and Generative Pre-trained Transformer 2 (GPT-2) models.\nWe employed evaluation metrics such as f1-score, accuracy, precision, and\nrecall to assess the models performance. Our proposed model outperformed all\nthe models on all datasets, achieving the highest accuracy of 95%. Our model\nachieves state-of-the-art results among all the previous works on the dataset\nwe used in this paper.", "published": "2023-08-15 17:20:05", "link": "http://arxiv.org/abs/2308.09722v1", "categories": ["cs.LG", "cs.CL", "cs.SI"], "primary_category": "cs.LG"}
{"title": "MVMR: A New Framework for Evaluating Faithfulness of Video Moment\n  Retrieval against Multiple Distractors", "abstract": "With the explosion of multimedia content, video moment retrieval (VMR), which\naims to detect a video moment that matches a given text query from a video, has\nbeen studied intensively as a critical problem. However, the existing VMR\nframework evaluates video moment retrieval performance, assuming that a video\nis given, which may not reveal whether the models exhibit overconfidence in the\nfalsely given video. In this paper, we propose the MVMR (Massive Videos Moment\nRetrieval for Faithfulness Evaluation) task that aims to retrieve video moments\nwithin a massive video set, including multiple distractors, to evaluate the\nfaithfulness of VMR models. For this task, we suggest an automated massive\nvideo pool construction framework to categorize negative (distractors) and\npositive (false-negative) video sets using textual and visual semantic distance\nverification methods. We extend existing VMR datasets using these methods and\nnewly construct three practical MVMR datasets. To solve the task, we further\npropose a strong informative sample-weighted learning method, CroCs, which\nemploys two contrastive learning mechanisms: (1) weakly-supervised potential\nnegative learning and (2) cross-directional hard-negative learning.\nExperimental results on the MVMR datasets reveal that existing VMR models are\neasily distracted by the misinformation (distractors), whereas our model shows\nsignificantly robust performance, demonstrating that CroCs is essential to\ndistinguishing positive moments against distractors. Our code and datasets are\npublicly available: https://github.com/yny0506/Massive-Videos-Moment-Retrieval.", "published": "2023-08-15 17:38:55", "link": "http://arxiv.org/abs/2309.16701v4", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "The DKU-MSXF Diarization System for the VoxCeleb Speaker Recognition\n  Challenge 2023", "abstract": "This paper describes the DKU-MSXF submission to track 4 of the VoxCeleb\nSpeaker Recognition Challenge 2023 (VoxSRC-23). Our system pipeline contains\nvoice activity detection, clustering-based diarization, overlapped speech\ndetection, and target-speaker voice activity detection, where each procedure\nhas a fused output from 3 sub-models. Finally, we fuse different\nclustering-based and TSVAD-based diarization systems using DOVER-Lap and\nachieve the 4.30% diarization error rate (DER), which ranks first place on\ntrack 4 of the challenge leaderboard.", "published": "2023-08-15 06:50:07", "link": "http://arxiv.org/abs/2308.07595v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "GIST-AiTeR Speaker Diarization System for VoxCeleb Speaker Recognition\n  Challenge (VoxSRC) 2023", "abstract": "This report describes the submission system by the GIST-AiTeR team for the\nVoxCeleb Speaker Recognition Challenge 2023 (VoxSRC-23) Track 4. Our submission\nsystem focuses on implementing diverse speaker diarization (SD) techniques,\nincluding ResNet293 and MFA-Conformer with different combinations of segment\nand hop length. Then, those models are combined into an ensemble model. The\nResNet293 and MFA-Conformer models exhibited the diarization error rates (DERs)\nof 3.65% and 3.83% on VAL46, respectively. The submitted ensemble model\nprovided a DER of 3.50% on VAL46, and consequently, it achieved a DER of 4.88%\non the VoxSRC-23 test set.", "published": "2023-08-15 14:08:26", "link": "http://arxiv.org/abs/2308.07788v4", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Preliminary investigation of the short-term in situ performance of an\n  automatic masker selection system", "abstract": "Soundscape augmentation or \"masking\" introduces wanted sounds into the\nacoustic environment to improve acoustic comfort. Usually, the masker selection\nand playback strategies are either arbitrary or based on simple rules (e.g. -3\ndBA), which may lead to sub-optimal increment or even reduction in acoustic\ncomfort for dynamic acoustic environments. To reduce ambiguity in the selection\nof maskers, an automatic masker selection system (AMSS) was recently developed.\nThe AMSS uses a deep-learning model trained on a large-scale dataset of\nsubjective responses to maximize the derived ISO pleasantness (ISO 12913-2).\nHence, this study investigates the short-term in situ performance of the AMSS\nimplemented in a gazebo in an urban park. Firstly, the predicted ISO\npleasantness from the AMSS is evaluated in comparison to the in situ subjective\nevaluation scores. Secondly, the effect of various masker selection schemes on\nthe perceived affective quality and appropriateness would be evaluated. In\ntotal, each participant evaluated 6 conditions: (1) ambient environment with no\nmaskers; (2) AMSS; (3) bird and (4) water masker from prior art; (5) random\nselection from same pool of maskers used to train the AMSS; and (6) selection\nof best-performing maskers based on the analysis of the dataset used to train\nthe AMSS.", "published": "2023-08-15 13:37:03", "link": "http://arxiv.org/abs/2308.07767v1", "categories": ["eess.AS", "cs.SD", "J.2; J.4"], "primary_category": "eess.AS"}
{"title": "AKVSR: Audio Knowledge Empowered Visual Speech Recognition by\n  Compressing Audio Knowledge of a Pretrained Model", "abstract": "Visual Speech Recognition (VSR) is the task of predicting spoken words from\nsilent lip movements. VSR is regarded as a challenging task because of the\ninsufficient information on lip movements. In this paper, we propose an Audio\nKnowledge empowered Visual Speech Recognition framework (AKVSR) to complement\nthe insufficient speech information of visual modality by using audio modality.\nDifferent from the previous methods, the proposed AKVSR 1) utilizes rich audio\nknowledge encoded by a large-scale pretrained audio model, 2) saves the\nlinguistic information of audio knowledge in compact audio memory by discarding\nthe non-linguistic information from the audio through quantization, and 3)\nincludes Audio Bridging Module which can find the best-matched audio features\nfrom the compact audio memory, which makes our training possible without audio\ninputs, once after the compact audio memory is composed. We validate the\neffectiveness of the proposed method through extensive experiments, and achieve\nnew state-of-the-art performances on the widely-used LRS3 dataset.", "published": "2023-08-15 06:38:38", "link": "http://arxiv.org/abs/2308.07593v2", "categories": ["cs.CV", "cs.MM", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "DiffV2S: Diffusion-based Video-to-Speech Synthesis with Vision-guided\n  Speaker Embedding", "abstract": "Recent research has demonstrated impressive results in video-to-speech\nsynthesis which involves reconstructing speech solely from visual input.\nHowever, previous works have struggled to accurately synthesize speech due to a\nlack of sufficient guidance for the model to infer the correct content with the\nappropriate sound. To resolve the issue, they have adopted an extra speaker\nembedding as a speaking style guidance from a reference auditory information.\nNevertheless, it is not always possible to obtain the audio information from\nthe corresponding video input, especially during the inference time. In this\npaper, we present a novel vision-guided speaker embedding extractor using a\nself-supervised pre-trained model and prompt tuning technique. In doing so, the\nrich speaker embedding information can be produced solely from input visual\ninformation, and the extra audio information is not necessary during the\ninference time. Using the extracted vision-guided speaker embedding\nrepresentations, we further develop a diffusion-based video-to-speech synthesis\nmodel, so called DiffV2S, conditioned on those speaker embeddings and the\nvisual representation extracted from the input video. The proposed DiffV2S not\nonly maintains phoneme details contained in the input video frames, but also\ncreates a highly intelligible mel-spectrogram in which the speaker identities\nof the multiple speakers are all preserved. Our experimental results show that\nDiffV2S achieves the state-of-the-art performance compared to the previous\nvideo-to-speech synthesis technique.", "published": "2023-08-15 14:07:41", "link": "http://arxiv.org/abs/2308.07787v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
