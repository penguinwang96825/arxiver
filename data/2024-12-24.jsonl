{"title": "Generalized Mean Absolute Directional Loss as a Solution to Overfitting and High Transaction Costs in Machine Learning Models Used in High-Frequency Algorithmic Investment Strategies", "abstract": "Regardless of the selected asset class and the level of model complexity\n(Transformer versus LSTM versus Perceptron/RNN), the GMADL loss function\nproduces superior results than standard MSE-type loss functions and has better\nnumerical properties in the context of optimization than MADL. Better results\nmean the possibility of achieving a higher risk-weighted return based on buy\nand sell signals built on forecasts generated by a given theoretical model\nestimated using the GMADL versus MSE or MADL function. In practice, GMADL\nsolves the problem of selecting the most preferable feature in both\nclassification and regression problems, improving the performance of each\nestimation. What is important is that, through additional parameterization,\nGMADL also solves the problem of optimizing investment systems on\nhigh-frequency data in such a way that they focus on strategy variants that\ncontain fewer transactions so that transaction costs do not reduce the\neffectiveness of a given strategy to zero. Moreover, the implementation\nleverages state-of-the-art machine learning tools, including frameworks for\nhyperparameter tuning, architecture testing, and walk-forward optimization,\nensuring robust and scalable solutions for real-world algorithmic trading.", "published": "2024-12-24 12:51:40", "link": "http://arxiv.org/abs/2412.18405v1", "categories": ["q-fin.CP", "q-fin.TR"], "primary_category": "q-fin.CP"}
{"title": "INVESTORBENCH: A Benchmark for Financial Decision-Making Tasks with LLM-based Agent", "abstract": "Recent advancements have underscored the potential of large language model\n(LLM)-based agents in financial decision-making. Despite this progress, the\nfield currently encounters two main challenges: (1) the lack of a comprehensive\nLLM agent framework adaptable to a variety of financial tasks, and (2) the\nabsence of standardized benchmarks and consistent datasets for assessing agent\nperformance. To tackle these issues, we introduce \\textsc{InvestorBench}, the\nfirst benchmark specifically designed for evaluating LLM-based agents in\ndiverse financial decision-making contexts. InvestorBench enhances the\nversatility of LLM-enabled agents by providing a comprehensive suite of tasks\napplicable to different financial products, including single equities like\nstocks, cryptocurrencies and exchange-traded funds (ETFs). Additionally, we\nassess the reasoning and decision-making capabilities of our agent framework\nusing thirteen different LLMs as backbone models, across various market\nenvironments and tasks. Furthermore, we have curated a diverse collection of\nopen-source, multi-modal datasets and developed a comprehensive suite of\nenvironments for financial decision-making. This establishes a highly\naccessible platform for evaluating financial agents' performance across various\nscenarios.", "published": "2024-12-24 05:22:33", "link": "http://arxiv.org/abs/2412.18174v1", "categories": ["cs.CE", "cs.AI", "q-fin.CP"], "primary_category": "cs.CE"}
{"title": "A mathematical framework for modelling CLMM dynamics in continuous time", "abstract": "This paper develops a rigorous mathematical framework for analyzing\nConcentrated Liquidity Market Makers (CLMMs) in Decentralized Finance (DeFi)\nwithin a continuous-time setting. We model the evolution of liquidity profiles\nas measure-valued processes and characterize their dynamics under continuous\ntrading. Our analysis encompasses two critical aspects of CLMMs: the mechanics\nof concentrated liquidity provision and the strategic behavior of arbitrageurs.\nWe examine three distinct arbitrage models -- myopic, finite-horizon, and\ninfinite-horizon with discounted and ergodic controls -- and derive closed-form\nsolutions for optimal arbitrage strategies under each scenario. Importantly, we\ndemonstrate that the presence of trading fees fundamentally constrains the\nadmissible price processes, as the inclusion of fees precludes the existence of\ndiffusion terms in the price process to avoid infinite fee generation. This\nfinding has significant implications for CLMM design and market efficiency.", "published": "2024-12-24 18:15:51", "link": "http://arxiv.org/abs/2412.18580v1", "categories": ["q-fin.MF", "q-fin.TR", "62P05, 93E20"], "primary_category": "q-fin.MF"}
{"title": "Dynamic Mean-Variance Asset Allocation in General Incomplete Markets A Nonlocal BSDE-based Feedback Control Approach", "abstract": "This paper studies dynamic mean-variance (MV) asset allocation problems in\ngeneral incomplete markets. Besides of the conventional MV objective on\nportfolio's terminal wealth, our framework can accommodate running MV\nobjectives with general (non-exponential) discounting factors while in general,\nany time-dependent preferences. We attempt the problem with a game-theoretic\nframework while decompose the equilibrium control policies into two parts: the\nfirst part is a myopic strategy characterized by a linear Volterra integral\nequation of the second kind and the second part reveals the hedging demand\ngoverned by a system of nonlocal backward stochastic differential equations. We\nmanage to establish the well-posedness of the solutions to the two\naforementioned equations in tailored Bananch spaces by the fixed-point theorem.\nIt allows us to devise a numerical scheme for solving for the equilibrium\ncontrol policy with guarantee and to conclude that the dynamic (equilibrium)\nmean-variance policy in general settings is well-defined. Our probabilistic\napproach allows us to consider a board range of stochastic factor models, such\nas the Chan--Karolyi--Longstaff--Sanders (CKLS) model. For which, we verify all\ntechnical assumptions and provide a sound numerical scheme. Numerical examples\nare provided to illustrate our framework.", "published": "2024-12-24 15:30:18", "link": "http://arxiv.org/abs/2412.18498v1", "categories": ["q-fin.MF", "math.OC", "math.PR", "49L20, 60H10, 91G10, 45D05"], "primary_category": "q-fin.MF"}
{"title": "Developing Cryptocurrency Trading Strategy Based on Autoencoder-CNN-GANs Algorithms", "abstract": "This paper leverages machine learning algorithms to forecast and analyze\nfinancial time series. The process begins with a denoising autoencoder to\nfilter out random noise fluctuations from the main contract price data. Then,\none-dimensional convolution reduces the dimensionality of the filtered data and\nextracts key information. The filtered and dimensionality-reduced price data is\nfed into a GANs network, and its output serve as input of a fully connected\nnetwork. Through cross-validation, a model is trained to capture features that\nprecede large price fluctuations. The model predicts the likelihood and\ndirection of significant price changes in real-time price sequences, placing\ntrades at moments of high prediction accuracy. Empirical results demonstrate\nthat using autoencoders and convolution to filter and denoise financial data,\ncombined with GANs, achieves a certain level of predictive performance,\nvalidating the capabilities of machine learning algorithms to discover\nunderlying patterns in financial sequences. Keywords - CNN;GANs;\nCryptocurrency; Prediction.", "published": "2024-12-24 06:14:34", "link": "http://arxiv.org/abs/2412.18202v4", "categories": ["cs.LG", "q-fin.ST"], "primary_category": "cs.LG"}
{"title": "Improving Factuality with Explicit Working Memory", "abstract": "Large language models can generate factually inaccurate content, a problem\nknown as hallucination. Recent works have built upon retrieved-augmented\ngeneration to improve factuality through iterative prompting but these methods\nare limited by the traditional RAG design. To address these challenges, we\nintroduce EWE (Explicit Working Memory), a novel approach that enhances\nfactuality in long-form text generation by integrating a working memory that\nreceives real-time feedback from external resources. The memory is refreshed\nbased on online fact-checking and retrieval feedback, allowing EWE to rectify\nfalse claims during the generation process and ensure more accurate and\nreliable outputs. Our experiments demonstrate that Ewe outperforms strong\nbaselines on four fact-seeking long-form generation datasets, increasing the\nfactuality metric, VeriScore, by 2 to 6 points absolute without sacrificing the\nhelpfulness of the responses. Further analysis reveals that the design of rules\nfor memory updates, configurations of memory units, and the quality of the\nretrieval datastore are crucial factors for influencing model performance.", "published": "2024-12-24 00:55:59", "link": "http://arxiv.org/abs/2412.18069v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Molly: Making Large Language Model Agents Solve Python Problem More\n  Logically", "abstract": "Applying large language models (LLMs) as teaching assists has attracted much\nattention as an integral part of intelligent education, particularly in\ncomputing courses. To reduce the gap between the LLMs and the computer\nprogramming education expert, fine-tuning and retrieval augmented generation\n(RAG) are the two mainstream methods in existing researches. However,\nfine-tuning for specific tasks is resource-intensive and may diminish the\nmodel`s generalization capabilities. RAG can perform well on reducing the\nillusion of LLMs, but the generation of irrelevant factual content during\nreasoning can cause significant confusion for learners. To address these\nproblems, we introduce the Molly agent, focusing on solving the proposed\nproblem encountered by learners when learning Python programming language. Our\nagent automatically parse the learners' questioning intent through a\nscenario-based interaction, enabling precise retrieval of relevant documents\nfrom the constructed knowledge base. At generation stage, the agent reflect on\nthe generated responses to ensure that they not only align with factual content\nbut also effectively answer the user's queries. Extensive experimentation on a\nconstructed Chinese Python QA dataset shows the effectiveness of the Molly\nagent, indicating an enhancement in its performance for providing useful\nresponses to Python questions.", "published": "2024-12-24 02:08:38", "link": "http://arxiv.org/abs/2412.18093v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LSAQ: Layer-Specific Adaptive Quantization for Large Language Model\n  Deployment", "abstract": "As large language models (LLMs) demonstrate exceptional performance across\nvarious domains, the deployment of these models on edge devices has emerged as\na new trend. Quantization techniques, which reduce the size and memory\nfootprint of LLMs, are effective for enabling deployment on\nresource-constrained edge devices. However, existing one-size-fits-all\nquantization methods often fail to dynamically adjust the memory consumption of\nLLMs based on specific hardware characteristics and usage scenarios. To address\nthis limitation, we propose LSAQ (Layer-Specific Adaptive Quantization), a\nsystem for adaptive quantization and dynamic deployment of LLMs based on layer\nimportance. LSAQ evaluates layer importance by constructing top-k token sets\nfrom the inputs and outputs of each layer and calculating their Jaccard\ncoefficient. Using this evaluation, the system adaptively adjusts quantization\nstrategies in real time according to the resource availability of edge devices,\nassigning different precision levels to layers of varying importance. This\napproach significantly reduces the storage requirements of LLMs while\nmaintaining model performance, enabling efficient deployment across diverse\nhardware platforms and usage scenarios.", "published": "2024-12-24 03:43:15", "link": "http://arxiv.org/abs/2412.18135v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ensuring Consistency for In-Image Translation", "abstract": "The in-image machine translation task involves translating text embedded\nwithin images, with the translated results presented in image format. While\nthis task has numerous applications in various scenarios such as film poster\ntranslation and everyday scene image translation, existing methods frequently\nneglect the aspect of consistency throughout this process. We propose the need\nto uphold two types of consistency in this task: translation consistency and\nimage generation consistency. The former entails incorporating image\ninformation during translation, while the latter involves maintaining\nconsistency between the style of the text-image and the original image,\nensuring background integrity. To address these consistency requirements, we\nintroduce a novel two-stage framework named HCIIT (High-Consistency In-Image\nTranslation) which involves text-image translation using a multimodal\nmultilingual large language model in the first stage and image backfilling with\na diffusion model in the second stage. Chain of thought learning is utilized in\nthe first stage to enhance the model's ability to leverage image information\nduring translation. Subsequently, a diffusion model trained for\nstyle-consistent text-image generation ensures uniformity in text style within\nimages and preserves background details. A dataset comprising 400,000\nstyle-consistent pseudo text-image pairs is curated for model training. Results\nobtained on both curated test sets and authentic image test sets validate the\neffectiveness of our framework in ensuring consistency and producing\nhigh-quality translated images.", "published": "2024-12-24 03:50:03", "link": "http://arxiv.org/abs/2412.18139v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoAM: Corpus of All-Type Multiword Expressions", "abstract": "Multiword expressions (MWEs) refer to idiomatic sequences of multiple words.\nMWE identification, i.e., detecting MWEs in text, can play a key role in\ndownstream tasks such as machine translation. Existing datasets for MWE\nidentification are inconsistently annotated, limited to a single type of MWE,\nor limited in size. To enable reliable and comprehensive evaluation, we created\nCoAM: Corpus of All-Type Multiword Expressions, a dataset of 1.3K sentences\nconstructed through a multi-step process to enhance data quality consisting of\nhuman annotation, human review, and automated consistency checking. MWEs in\nCoAM are tagged with MWE types, such as Noun and Verb, to enable fine-grained\nerror analysis. Annotations for CoAM were collected using a new interface\ncreated with our interface generator, which allows easy and flexible annotation\nof MWEs in any form, including discontinuous ones. Through experiments using\nCoAM, we find that a fine-tuned large language model outperforms the current\nstate-of-the-art approach for MWE identification. Furthermore, analysis using\nour MWE type tagged data reveals that Verb MWEs are easier than Noun MWEs to\nidentify across approaches.", "published": "2024-12-24 04:09:33", "link": "http://arxiv.org/abs/2412.18151v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Large Language Models for Code Vulnerability Detection: An\n  Experimental Study", "abstract": "Code vulnerability detection (CVD) is essential for addressing and preventing\nsystem security issues, playing a crucial role in ensuring software security.\nPrevious learning-based vulnerability detection methods rely on either\nfine-tuning medium-size sequence models or training smaller neural networks\nfrom scratch. Recent advancements in large pre-trained language models (LLMs)\nhave showcased remarkable capabilities in various code intelligence tasks\nincluding code understanding and generation. However, the effectiveness of LLMs\nin detecting code vulnerabilities is largely under-explored. This work aims to\ninvestigate the gap by fine-tuning LLMs for the CVD task, involving four\nwidely-used open-source LLMs. We also implement other five previous graph-based\nor medium-size sequence models for comparison. Experiments are conducted on\nfive commonly-used CVD datasets, including both the part of short samples and\nlong samples. In addition, we conduct quantitative experiments to investigate\nthe class imbalance issue and the model's performance on samples of different\nlengths, which are rarely studied in previous works. To better facilitate\ncommunities, we open-source all codes and resources of this study in\nhttps://github.com/SakiRinn/LLM4CVD and\nhttps://huggingface.co/datasets/xuefen/VulResource.", "published": "2024-12-24 08:20:29", "link": "http://arxiv.org/abs/2412.18260v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting triples from dialogues for conversational social agents", "abstract": "Obtaining an explicit understanding of communication within a Hybrid\nIntelligence collaboration is essential to create controllable and transparent\nagents. In this paper, we describe a number of Natural Language Understanding\nmodels that extract explicit symbolic triples from social conversation. Triple\nextraction has mostly been developed and tested for Knowledge Base Completion\nusing Wikipedia text and data for training and testing. However, social\nconversation is very different as a genre in which interlocutors exchange\ninformation in sequences of utterances that involve statements, questions, and\nanswers. Phenomena such as co-reference, ellipsis, coordination, and implicit\nand explicit negation or confirmation are more prominent in conversation than\nin Wikipedia text. We therefore describe an attempt to fill this gap by\nreleasing data sets for training and testing triple extraction from social\nconversation. We also created five triple extraction models and tested them in\nour evaluation data. The highest precision is 51.14 for complete triples and\n69.32 for triple elements when tested on single utterances. However, scores for\nconversational triples that span multiple turns are much lower, showing that\nextracting knowledge from true conversational data is much more challenging.", "published": "2024-12-24 11:48:16", "link": "http://arxiv.org/abs/2412.18364v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Global AI Inclusivity: A Large-Scale Multilingual Terminology\n  Dataset (GIST)", "abstract": "The field of machine translation has achieved significant advancements, yet\ndomain-specific terminology translation, particularly in AI, remains\nchallenging. We introduce GIST, a large-scale multilingual AI terminology\ndataset containing 5K terms extracted from top AI conference papers spanning\n2000 to 2023. The terms are translated into Arabic, Chinese, French, Japanese,\nand Russian using a hybrid framework that combines LLMs for extraction with\nhuman expertise for translation. The dataset's quality is benchmarked against\nexisting resources, demonstrating superior translation accuracy through\ncrowdsourced evaluation. GIST is integrated into translation workflows using\npost-translation refinement methods that require no retraining, where LLM\nprompting consistently improves BLEU and COMET scores. A web demonstration on\nthe ACL Anthology platform highlights its practical application, showcasing\nimproved accessibility for non-English speakers. This work aims to address\ncritical gaps in AI terminology resources and fosters global inclusivity and\ncollaboration in AI research.", "published": "2024-12-24 11:50:18", "link": "http://arxiv.org/abs/2412.18367v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unlocking the Potential of Multiple BERT Models for Bangla Question\n  Answering in NCTB Textbooks", "abstract": "Evaluating text comprehension in educational settings is critical for\nunderstanding student performance and improving curricular effectiveness. This\nstudy investigates the capability of state-of-the-art language models-RoBERTa\nBase, Bangla-BERT, and BERT Base-in automatically assessing Bangla\npassage-based question-answering from the National Curriculum and Textbook\nBoard (NCTB) textbooks for classes 6-10. A dataset of approximately 3,000\nBangla passage-based question-answering instances was compiled, and the models\nwere evaluated using F1 Score and Exact Match (EM) metrics across various\nhyperparameter configurations. Our findings revealed that Bangla-BERT\nconsistently outperformed the other models, achieving the highest F1 (0.75) and\nEM (0.53) scores, particularly with smaller batch sizes, the inclusion of stop\nwords, and a moderate learning rate. In contrast, RoBERTa Base demonstrated the\nweakest performance, with the lowest F1 (0.19) and EM (0.27) scores under\ncertain configurations. The results underscore the importance of fine-tuning\nhyperparameters for optimizing model performance and highlight the potential of\nmachine learning models in evaluating text comprehension in educational\ncontexts. However, limitations such as dataset size, spelling inconsistencies,\nand computational constraints emphasize the need for further research to\nenhance the robustness and applicability of these models. This study lays the\ngroundwork for the future development of automated evaluation systems in\neducational institutions, providing critical insights into model performance in\nthe context of Bangla text comprehension.", "published": "2024-12-24 13:59:23", "link": "http://arxiv.org/abs/2412.18440v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is Large Language Model Good at Triple Set Prediction? An Empirical\n  Study", "abstract": "The core of the Knowledge Graph Completion (KGC) task is to predict and\ncomplete the missing relations or nodes in a KG. Common KGC tasks are mostly\nabout inferring unknown elements with one or two elements being known in a\ntriple. In comparison, the Triple Set Prediction (TSP) task is a more realistic\nknowledge graph completion task. It aims to predict all elements of unknown\ntriples based on the information from known triples. In recent years, large\nlanguage models (LLMs) have exhibited significant advancements in language\ncomprehension, demonstrating considerable potential for KGC tasks. However, the\npotential of LLM on the TSP task has not yet to be investigated. Thus in this\npaper we proposed a new framework to explore the strengths and limitations of\nLLM in the TSP task. Specifically, the framework consists of LLM-based rule\nmining and LLM-based triple set prediction. The relation list of KG embedded\nwithin rich semantic information is first leveraged to prompt LLM in the\ngeneration of rules. This process is both efficient and independent of\nstatistical information, making it easier to mine effective and realistic\nrules. For each subgraph, the specified rule is applied in conjunction with the\nrelevant triples within that subgraph to guide the LLM in predicting the\nmissing triples. Subsequently, the predictions from all subgraphs are\nconsolidated to derive the complete set of predicted triples on KG. Finally,\nthe method is evaluated on the relatively complete CFamily dataset. The\nexperimental results indicate that when LLMs are required to adhere to a large\namount of factual knowledge to predict missing triples, significant\nhallucinations occurs, leading to a noticeable decline in performance. To\nfurther explore the causes of this phenomenon, this paper presents a\ncomprehensive analysis supported by a detailed case study.", "published": "2024-12-24 14:03:07", "link": "http://arxiv.org/abs/2412.18443v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Segment-Based Attention Masking for GPTs", "abstract": "Modern Language Models (LMs) owe much of their success to masked causal\nattention, the backbone of Generative Pre-Trained Transformer (GPT) models.\nAlthough GPTs can process the entire user prompt at once, the causal masking is\napplied to all input tokens step-by-step, mimicking the generation process.\nThis imposes an unnecessary constraint during the initial \"prefill\" phase when\nthe model processes the input prompt and generates the internal representations\nbefore producing any output tokens. In this work, attention is masked based on\nthe known block structure at the prefill phase, followed by the conventional\ntoken-by-token autoregressive process after that. For example, in a typical\nchat prompt, the system prompt is treated as one block, and the user prompt as\nthe next one. Each of these is treated as a unit for the purpose of masking,\nsuch that the first tokens in each block can access the subsequent tokens in a\nnon-causal manner. Then, the model answer is generated in the conventional\ncausal manner. This Segment-by-Segment scheme entails no additional\ncomputational overhead. When integrating it into models such as Llama and Qwen,\nstate-of-the-art performance is consistently achieved.", "published": "2024-12-24 15:18:52", "link": "http://arxiv.org/abs/2412.18487v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating event descriptions under syntactic and semantic constraints", "abstract": "With the goal of supporting scalable lexical semantic annotation, analysis,\nand theorizing, we conduct a comprehensive evaluation of different methods for\ngenerating event descriptions under both syntactic constraints -- e.g. desired\nclause structure -- and semantic constraints -- e.g. desired verb sense. We\ncompare three different methods -- (i) manual generation by experts; (ii)\nsampling from a corpus annotated for syntactic and semantic information; and\n(iii) sampling from a language model (LM) conditioned on syntactic and semantic\ninformation -- along three dimensions of the generated event descriptions: (a)\nnaturalness, (b) typicality, and (c) distinctiveness. We find that all methods\nreliably produce natural, typical, and distinctive event descriptions, but that\nmanual generation continues to produce event descriptions that are more\nnatural, typical, and distinctive than the automated generation methods. We\nconclude that the automated methods we consider produce event descriptions of\nsufficient quality for use in downstream annotation and analysis insofar as the\nmethods used for this annotation and analysis are robust to a small amount of\ndegradation in the resulting event descriptions.", "published": "2024-12-24 15:28:41", "link": "http://arxiv.org/abs/2412.18496v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Think or Remember? Detecting and Directing LLMs Towards Memorization or\n  Generalization", "abstract": "In this paper, we explore the foundational mechanisms of memorization and\ngeneralization in Large Language Models (LLMs), inspired by the functional\nspecialization observed in the human brain. Our investigation serves as a case\nstudy leveraging specially designed datasets and experimental-scale LLMs to lay\nthe groundwork for understanding these behaviors. Specifically, we aim to first\nenable LLMs to exhibit both memorization and generalization by training with\nthe designed dataset, then (a) examine whether LLMs exhibit neuron-level\nspatial differentiation for memorization and generalization, (b) predict these\nbehaviors using model internal representations, and (c) steer the behaviors\nthrough inference-time interventions. Our findings reveal that neuron-wise\ndifferentiation of memorization and generalization is observable in LLMs, and\ntargeted interventions can successfully direct their behavior.", "published": "2024-12-24 15:28:56", "link": "http://arxiv.org/abs/2412.18497v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Harnessing Large Language Models for Knowledge Graph Question Answering\n  via Adaptive Multi-Aspect Retrieval-Augmentation", "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities, yet\nstruggle with hallucination and outdated knowledge when tasked with complex\nknowledge reasoning, resulting in factually incorrect outputs. Previous studies\nhave attempted to mitigate it by retrieving factual knowledge from large-scale\nknowledge graphs (KGs) to assist LLMs in logical reasoning and prediction of\nanswers. However, this kind of approach often introduces noise and irrelevant\ndata, especially in situations with extensive context from multiple knowledge\naspects. In this way, LLM attention can be potentially mislead from question\nand relevant information. In our study, we introduce an Adaptive Multi-Aspect\nRetrieval-augmented over KGs (Amar) framework. This method retrieves knowledge\nincluding entities, relations, and subgraphs, and converts each piece of\nretrieved text into prompt embeddings. The Amar framework comprises two key\nsub-components: 1) a self-alignment module that aligns commonalities among\nentities, relations, and subgraphs to enhance retrieved text, thereby reducing\nnoise interference; 2) a relevance gating module that employs a soft gate to\nlearn the relevance score between question and multi-aspect retrieved data, to\ndetermine which information should be used to enhance LLMs' output, or even\nfiltered altogether. Our method has achieved state-of-the-art performance on\ntwo common datasets, WebQSP and CWQ, showing a 1.9\\% improvement in accuracy\nover its best competitor and a 6.6\\% improvement in logical form generation\nover a method that directly uses retrieved text as context prompts. These\nresults demonstrate the effectiveness of Amar in improving the reasoning of\nLLMs.", "published": "2024-12-24 16:38:04", "link": "http://arxiv.org/abs/2412.18537v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Libra-Leaderboard: Towards Responsible AI through a Balanced Leaderboard\n  of Safety and Capability", "abstract": "To address this gap, we introduce Libra-Leaderboard, a comprehensive\nframework designed to rank LLMs through a balanced evaluation of performance\nand safety. Combining a dynamic leaderboard with an interactive LLM arena,\nLibra-Leaderboard encourages the joint optimization of capability and safety.\nUnlike traditional approaches that average performance and safety metrics,\nLibra-Leaderboard uses a distance-to-optimal-score method to calculate the\noverall rankings. This approach incentivizes models to achieve a balance rather\nthan excelling in one dimension at the expense of some other ones. In the first\nrelease, Libra-Leaderboard evaluates 26 mainstream LLMs from 14 leading\norganizations, identifying critical safety challenges even in state-of-the-art\nmodels.", "published": "2024-12-24 17:03:44", "link": "http://arxiv.org/abs/2412.18551v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distilling Fine-grained Sentiment Understanding from Large Language\n  Models", "abstract": "Fine-grained sentiment analysis (FSA) aims to extract and summarize user\nopinions from vast opinionated text. Recent studies demonstrate that large\nlanguage models (LLMs) possess exceptional sentiment understanding\ncapabilities. However, directly deploying LLMs for FSA applications incurs high\ninference costs. Therefore, this paper investigates the distillation of\nfine-grained sentiment understanding from LLMs into small language models\n(SLMs). We prompt LLMs to examine and interpret the sentiments of given reviews\nand then utilize the generated content to pretrain SLMs. Additionally, we\ndevelop a comprehensive FSA benchmark to evaluate both SLMs and LLMs. Extensive\nexperiments on this benchmark reveal that: (1) distillation significantly\nenhances the performance of SLMs in FSA tasks, achieving a 6.00\\% improvement\nin $F_1$-score, and the distilled model can outperform Llama-2-7b with only\n220M parameters; (2) distillation equips SLMs with excellent zero-shot\nsentiment classification capabilities, enabling them to match or even exceed\ntheir teacher models. These results suggest that distillation from LLMs is a\nhighly promising direction for FSA. We will release our code, data, and\npretrained model weights at https://github.com/HITSZ-HLT/FSA-Distillation.", "published": "2024-12-24 17:05:26", "link": "http://arxiv.org/abs/2412.18552v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simple is not Enough: Document-level Text Simplification using\n  Readability and Coherence", "abstract": "In this paper, we present the SimDoc system, a simplification model\nconsidering simplicity, readability, and discourse aspects, such as coherence.\nIn the past decade, the progress of the Text Simplification (TS) field has been\nmostly shown at a sentence level, rather than considering paragraphs or\ndocuments, a setting from which most TS audiences would benefit. We propose a\nsimplification system that is initially fine-tuned with professionally created\ncorpora. Further, we include multiple objectives during training, considering\nsimplicity, readability, and coherence altogether. Our contributions include\nthe extension of professionally annotated simplification corpora by the\nassociation of existing annotations into (complex text, simple text,\nreadability label) triples to benefit from readability during training. Also,\nwe present a comparative analysis in which we evaluate our proposed models in a\nzero-shot, few-shot, and fine-tuning setting using document-level TS corpora,\ndemonstrating novel methods for simplification. Finally, we show a detailed\nanalysis of outputs, highlighting the difficulties of simplification at a\ndocument level.", "published": "2024-12-24 19:05:21", "link": "http://arxiv.org/abs/2412.18655v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multiple References with Meaningful Variations Improve Literary Machine\n  Translation", "abstract": "While a source sentence can be translated in many ways, most machine\ntranslation (MT) models are trained with only a single reference. Previous work\nhas shown that using synthetic paraphrases can improve MT. This paper\ninvestigates best practices for employing multiple references by analyzing the\nsemantic similarity among different English translations of world literature in\nthe Par3 dataset. We classify the semantic similarity between paraphrases into\nthree levels: low, medium, and high, and fine-tune three different models\n(mT5-large, LLaMA-2-7B, and Opus-MT) for literary MT tasks. Across different\nmodels, holding the total training instances constant, single-reference but\nmore source texts only marginally outperforms multiple-reference with half of\nthe source texts. Moreover, when fine-tuning an LLM, using paraphrases with\nmedium and high semantic similarity outperforms an unfiltered dataset, with\nimprovements in BLEU (0.3-0.5), COMET (0.1-0.9), and chrF++ (0.17-0.32). Our\ncode is publicly available on GitHub.", "published": "2024-12-24 23:49:12", "link": "http://arxiv.org/abs/2412.18707v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neuron Empirical Gradient: Discovering and Quantifying Neurons Global\n  Linear Controllability", "abstract": "Although feed-forward neurons in pre-trained language models (PLMs) can store\nknowledge and their importance in influencing model outputs has been studied,\nexisting work focuses on finding a limited set of neurons and analyzing their\nrelative importance. However, the global quantitative role of activation values\nin shaping outputs remains unclear, hindering further advancements in\napplications like knowledge editing. Our study first investigates the numerical\nrelationship between neuron activations and model output and discovers the\nglobal linear relationship between them through neuron interventions on a\nknowledge probing dataset. We refer to the gradient of this linear relationship\nas neuron empirical gradient (NEG), and introduce NeurGrad, an accurate and\nefficient method for computing NEG. NeurGrad enables quantitative analysis of\nall neurons in PLMs, advancing our understanding of neurons' controllability.\nFurthermore, we explore NEG's ability to represent language skills across\ndiverse prompts via skill neuron probing. Experiments on MCEval8k, a\nmulti-choice knowledge benchmark spanning various genres, validate NEG's\nrepresentational ability. The data and code are released.", "published": "2024-12-24 00:01:24", "link": "http://arxiv.org/abs/2412.18053v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do Language Models Understand the Cognitive Tasks Given to Them?\n  Investigations with the N-Back Paradigm", "abstract": "Cognitive tasks originally developed for humans are now increasingly used to\nstudy language models. While applying these tasks is often straightforward,\ninterpreting their results can be challenging. In particular, when a model\nunderperforms, it is often unclear whether this results from a limitation in\nthe cognitive ability being tested or a failure to understand the task itself.\nA recent study argues that GPT 3.5's declining performance on 2-back and 3-back\ntasks reflects a working memory capacity limit similar to humans (Gong et al.,\n2024). By analyzing a range of open-source language models of varying\nperformance levels on these tasks, we show that the poor performance instead\nreflects a limitation in task comprehension and task set maintenance. In\naddition, we challenge the best-performing model with progressively harder\nversions of the task (up to 10-back) and experiment with alternative prompting\nstrategies, before analyzing model attentions. Our larger aim is to contribute\nto the ongoing conversation around refining methodologies for the cognitive\nevaluation of language models.", "published": "2024-12-24 03:06:52", "link": "http://arxiv.org/abs/2412.18120v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AEIOU: A Unified Defense Framework against NSFW Prompts in Text-to-Image\n  Models", "abstract": "As text-to-image (T2I) models continue to advance and gain widespread\nadoption, their associated safety issues are becoming increasingly prominent.\nMalicious users often exploit these models to generate Not-Safe-for-Work (NSFW)\nimages using harmful or adversarial prompts, highlighting the critical need for\nrobust safeguards to ensure the integrity and compliance of model outputs.\nCurrent internal safeguards frequently degrade image quality, while external\ndetection methods often suffer from low accuracy and inefficiency.\n  In this paper, we introduce AEIOU, a defense framework that is Adaptable,\nEfficient, Interpretable, Optimizable, and Unified against NSFW prompts in T2I\nmodels. AEIOU extracts NSFW features from the hidden states of the model's text\nencoder, utilizing the separable nature of these features to detect NSFW\nprompts. The detection process is efficient, requiring minimal inference time.\nAEIOU also offers real-time interpretation of results and supports optimization\nthrough data augmentation techniques. The framework is versatile, accommodating\nvarious T2I architectures. Our extensive experiments show that AEIOU\nsignificantly outperforms both commercial and open-source moderation tools,\nachieving over 95% accuracy across all datasets and improving efficiency by at\nleast tenfold. It effectively counters adaptive attacks and excels in few-shot\nand multi-label scenarios.", "published": "2024-12-24 03:17:45", "link": "http://arxiv.org/abs/2412.18123v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Survey of Pseudonymization, Abstractive Summarization & Spell Checker\n  for Hindi and Marathi", "abstract": "India's vast linguistic diversity presents unique challenges and\nopportunities for technological advancement, especially in the realm of Natural\nLanguage Processing (NLP). While there has been significant progress in NLP\napplications for widely spoken languages, the regional languages of India, such\nas Marathi and Hindi, remain underserved. Research in the field of NLP for\nIndian regional languages is at a formative stage and holds immense\nsignificance. The paper aims to build a platform which enables the user to use\nvarious features like text anonymization, abstractive text summarization and\nspell checking in English, Hindi and Marathi language. The aim of these tools\nis to serve enterprise and consumer clients who predominantly use Indian\nRegional Languages.", "published": "2024-12-24 04:51:32", "link": "http://arxiv.org/abs/2412.18163v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Applicability of Zero-Shot Cross-Lingual Transfer Learning for\n  Sentiment Classification in Distant Language Pairs", "abstract": "This research explores the applicability of cross-lingual transfer learning\nfrom English to Japanese and Indonesian using the XLM-R pre-trained model. The\nresults are compared with several previous works, either by models using a\nsimilar zero-shot approach or a fully-supervised approach, to provide an\noverview of the zero-shot transfer learning approach's capability using XLM-R\nin comparison with existing models. Our models achieve the best result in one\nJapanese dataset and comparable results in other datasets in Japanese and\nIndonesian languages without being trained using the target language.\nFurthermore, the results suggest that it is possible to train a multi-lingual\nmodel, instead of one model for each language, and achieve promising results.", "published": "2024-12-24 05:50:18", "link": "http://arxiv.org/abs/2412.18188v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Analysis on Automated Metrics for Evaluating Japanese-English Chat\n  Translation", "abstract": "This paper analyses how traditional baseline metrics, such as BLEU and TER,\nand neural-based methods, such as BERTScore and COMET, score several NMT models\nperformance on chat translation and how these metrics perform when compared to\nhuman-annotated scores. The results show that for ranking NMT models in chat\ntranslations, all metrics seem consistent in deciding which model outperforms\nthe others. This implies that traditional baseline metrics, which are faster\nand simpler to use, can still be helpful. On the other hand, when it comes to\nbetter correlation with human judgment, neural-based metrics outperform\ntraditional metrics, with COMET achieving the highest correlation with the\nhuman-annotated score on a chat translation. However, we show that even the\nbest metric struggles when scoring English translations from sentences with\nanaphoric zero-pronoun in Japanese.", "published": "2024-12-24 05:54:40", "link": "http://arxiv.org/abs/2412.18190v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Robustness-aware Automatic Prompt Optimization", "abstract": "The performance of Large Language Models (LLMs) depends on the quality of\nprompts and the semantic and structural integrity of the input data. However,\nexisting prompt generation methods primarily focus on well-structured input\ndata, often neglecting the impact of perturbed inputs on prompt effectiveness.\nTo address this limitation, we propose BATprompt (By Adversarial Training\nprompt), a novel method for prompt generation designed to withstand input\nperturbations (such as typos in the input). Inspired by adversarial training\ntechniques, BATprompt demonstrates strong performance on a variety of perturbed\ntasks through a two-step process: adversarial perturbation and iterative\noptimization on unperturbed input via LLM. Unlike conventional adversarial\nattack methods, BATprompt does not need access to model parameters and\ngradients. Instead, BATprompt leverages the advanced reasoning, language\nunderstanding and self reflection capabilities of LLMs to simulate gradients,\nguiding the generation of adversarial perturbations and optimizing prompt\nperformance. We evaluate BATprompt on multiple datasets across both language\nunderstanding and generation tasks. The results indicate that BATprompt\noutperforms existing prompt generation methods, delivering superior robustness\nand performance under diverse perturbation scenarios.", "published": "2024-12-24 06:05:08", "link": "http://arxiv.org/abs/2412.18196v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ICM-Assistant: Instruction-tuning Multimodal Large Language Models for\n  Rule-based Explainable Image Content Moderation", "abstract": "Controversial contents largely inundate the Internet, infringing various\ncultural norms and child protection standards. Traditional Image Content\nModeration (ICM) models fall short in producing precise moderation decisions\nfor diverse standards, while recent multimodal large language models (MLLMs),\nwhen adopted to general rule-based ICM, often produce classification and\nexplanation results that are inconsistent with human moderators. Aiming at\nflexible, explainable, and accurate ICM, we design a novel rule-based dataset\ngeneration pipeline, decomposing concise human-defined rules and leveraging\nwell-designed multi-stage prompts to enrich short explicit image annotations.\nOur ICM-Instruct dataset includes detailed moderation explanation and\nmoderation Q-A pairs. Built upon it, we create our ICM-Assistant model in the\nframework of rule-based ICM, making it readily applicable in real practice. Our\nICM-Assistant model demonstrates exceptional performance and flexibility.\nSpecifically, it significantly outperforms existing approaches on various\nsources, improving both the moderation classification (36.8% on average) and\nmoderation explanation quality (26.6% on average) consistently over existing\nMLLMs. Code/Data is available at https://github.com/zhaoyuzhi/ICM-Assistant.", "published": "2024-12-24 06:45:36", "link": "http://arxiv.org/abs/2412.18216v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "GenAI Content Detection Task 2: AI vs. Human -- Academic Essay\n  Authenticity Challenge", "abstract": "This paper presents a comprehensive overview of the first edition of the\nAcademic Essay Authenticity Challenge, organized as part of the GenAI Content\nDetection shared tasks collocated with COLING 2025. This challenge focuses on\ndetecting machine-generated vs. human-authored essays for academic purposes.\nThe task is defined as follows: \"Given an essay, identify whether it is\ngenerated by a machine or authored by a human.'' The challenge involves two\nlanguages: English and Arabic. During the evaluation phase, 25 teams submitted\nsystems for English and 21 teams for Arabic, reflecting substantial interest in\nthe task. Finally, seven teams submitted system description papers. The\nmajority of submissions utilized fine-tuned transformer-based models, with one\nteam employing Large Language Models (LLMs) such as Llama 2 and Llama 3. This\npaper outlines the task formulation, details the dataset construction process,\nand explains the evaluation framework. Additionally, we present a summary of\nthe approaches adopted by participating teams. Nearly all submitted systems\noutperformed the n-gram-based baseline, with the top-performing systems\nachieving F1 scores exceeding 0.98 for both languages, indicating significant\nprogress in the detection of machine-generated text.", "published": "2024-12-24 08:33:44", "link": "http://arxiv.org/abs/2412.18274v1", "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "M-Ped: Multi-Prompt Ensemble Decoding for Large Language Models", "abstract": "With the widespread application of Large Language Models (LLMs) in the field\nof Natural Language Processing (NLP), enhancing their performance has become a\nresearch hotspot. This paper presents a novel multi-prompt ensemble decoding\napproach designed to bolster the generation quality of LLMs by leveraging the\naggregation of outcomes from multiple prompts. Given a unique input $X$, we\nsubmit $n$ variations of prompts with $X$ to LLMs in batch mode to decode and\nderive probability distributions. For each token prediction, we calculate the\nensemble probability by averaging the $n$ probability distributions within the\nbatch, utilizing this aggregated probability to generate the token. This\ntechnique is dubbed Inner-Batch Ensemble. To facilitate efficient batch\ninference, we implement a Left-Padding strategy to maintain uniform input\nlengths across the n prompts. Through extensive experimentation on diverse NLP\ntasks, including machine translation, code generation, and text simplification,\nwe demonstrate the efficacy of our method in enhancing LLM performance. The\nresults show substantial improvements in BLEU scores, pass@$k$ rates, and LENS\nmetrics over conventional methods.", "published": "2024-12-24 09:06:58", "link": "http://arxiv.org/abs/2412.18299v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Agents Based on Large Language Models for Knowledge-based Visual\n  Question Answering", "abstract": "Large Language Models (LLMs) have achieved impressive results in\nknowledge-based Visual Question Answering (VQA). However existing methods still\nhave challenges: the inability to use external tools autonomously, and the\ninability to work in teams. Humans tend to know whether they need to use\nexternal tools when they encounter a new question, e.g., they tend to be able\nto give a direct answer to a familiar question, whereas they tend to use tools\nsuch as search engines when they encounter an unfamiliar question. In addition,\nhumans also tend to collaborate and discuss with others to get better answers.\nInspired by this, we propose the multi-agent voting framework. We design three\nLLM-based agents that simulate different levels of staff in a team, and assign\nthe available tools according to the levels. Each agent provides the\ncorresponding answer, and finally all the answers provided by the agents are\nvoted to get the final answer. Experiments on OK-VQA and A-OKVQA show that our\napproach outperforms other baselines by 2.2 and 1.0, respectively.", "published": "2024-12-24 11:24:56", "link": "http://arxiv.org/abs/2412.18351v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bidirectional Topic Matching: Quantifying Thematic Overlap Between\n  Corpora Through Topic Modelling", "abstract": "This study introduces Bidirectional Topic Matching (BTM), a novel method for\ncross-corpus topic modeling that quantifies thematic overlap and divergence\nbetween corpora. BTM is a flexible framework that can incorporate various topic\nmodeling approaches, including BERTopic, Top2Vec, and Latent Dirichlet\nAllocation (LDA). BTM employs a dual-model approach, training separate topic\nmodels for each corpus and applying them reciprocally to enable comprehensive\ncross-corpus comparisons. This methodology facilitates the identification of\nshared themes and unique topics, providing nuanced insights into thematic\nrelationships. Validation against cosine similarity-based methods demonstrates\nthe robustness of BTM, with strong agreement metrics and distinct advantages in\nhandling outlier topics. A case study on climate news articles showcases BTM's\nutility, revealing significant thematic overlaps and distinctions between\ncorpora focused on climate change and climate action. BTM's flexibility and\nprecision make it a valuable tool for diverse applications, from political\ndiscourse analysis to interdisciplinary studies. By integrating shared and\nunique topic analyses, BTM offers a comprehensive framework for exploring\nthematic relationships, with potential extensions to multilingual and dynamic\ndatasets. This work highlights BTM's methodological contributions and its\ncapacity to advance discourse analysis across various domains.", "published": "2024-12-24 12:02:43", "link": "http://arxiv.org/abs/2412.18376v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi\n  and English", "abstract": "Large Language Models (LLMs) excel in linguistic tasks but struggle with\nmathematical reasoning, particularly in non English languages like Hindi. This\nresearch aims to enhance the mathematical reasoning skills of smaller, resource\nefficient open-source LLMs in both Hindi and English. We evaluate models like\nOpenHathi 7B, LLaMA-2 7B, WizardMath 7B, Mistral 7B, LLeMMa 7B, MAmmoTH 7B,\nGemini Pro, and GPT-4 using zero-shot, few-shot chain-of-thought (CoT) methods,\nand supervised fine-tuning. Our approach incorporates curriculum learning,\nprogressively training models on increasingly difficult problems, a novel\nDecomposition Strategy to simplify complex arithmetic operations, and a\nStructured Solution Design that divides solutions into phases. Our experiments\nresult in notable performance enhancements. WizardMath 7B exceeds Gemini's\naccuracy on English datasets by +6% and matches Gemini's performance on Hindi\ndatasets. Adopting a bilingual approach that combines English and Hindi samples\nachieves results comparable to individual language models, demonstrating the\ncapability to learn mathematical reasoning in both languages. This research\nhighlights the potential for improving mathematical reasoning in open-source\nLLMs.", "published": "2024-12-24 13:07:29", "link": "http://arxiv.org/abs/2412.18415v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LongDocURL: a Comprehensive Multimodal Long Document Benchmark\n  Integrating Understanding, Reasoning, and Locating", "abstract": "Large vision language models (LVLMs) have improved the document understanding\ncapabilities remarkably, enabling the handling of complex document elements,\nlonger contexts, and a wider range of tasks. However, existing document\nunderstanding benchmarks have been limited to handling only a small number of\npages and fail to provide a comprehensive analysis of layout elements locating.\nIn this paper, we first define three primary task categories: Long Document\nUnderstanding, numerical Reasoning, and cross-element Locating, and then\npropose a comprehensive benchmark, LongDocURL, integrating above three primary\ntasks and comprising 20 sub-tasks categorized based on different primary tasks\nand answer evidences. Furthermore, we develop a semi-automated construction\npipeline and collect 2,325 high-quality question-answering pairs, covering more\nthan 33,000 pages of documents, significantly outperforming existing\nbenchmarks. Subsequently, we conduct comprehensive evaluation experiments on\nboth open-source and closed-source models across 26 different configurations,\nrevealing critical performance gaps in this field.", "published": "2024-12-24 13:39:32", "link": "http://arxiv.org/abs/2412.18424v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Explainable Multi-Modal Data Exploration in Natural Language via LLM\n  Agent", "abstract": "International enterprises, organizations, or hospitals collect large amounts\nof multi-modal data stored in databases, text documents, images, and videos.\nWhile there has been recent progress in the separate fields of multi-modal data\nexploration as well as in database systems that automatically translate natural\nlanguage questions to database query languages, the research challenge of\nquerying database systems combined with other unstructured modalities such as\nimages in natural language is widely unexplored.\n  In this paper, we propose XMODE - a system that enables explainable,\nmulti-modal data exploration in natural language. Our approach is based on the\nfollowing research contributions: (1) Our system is inspired by a real-world\nuse case that enables users to explore multi-modal information systems. (2)\nXMODE leverages a LLM-based agentic AI framework to decompose a natural\nlanguage question into subtasks such as text-to-SQL generation and image\nanalysis. (3) Experimental results on multi-modal datasets over relational data\nand images demonstrate that our system outperforms state-of-the-art multi-modal\nexploration systems, excelling not only in accuracy but also in various\nperformance metrics such as query latency, API costs, planning efficiency, and\nexplanation quality, thanks to the more effective utilization of the reasoning\ncapabilities of LLMs.", "published": "2024-12-24 13:42:44", "link": "http://arxiv.org/abs/2412.18428v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Zero-resource Speech Translation and Recognition with LLMs", "abstract": "Despite recent advancements in speech processing, zero-resource speech\ntranslation (ST) and automatic speech recognition (ASR) remain challenging\nproblems. In this work, we propose to leverage a multilingual Large Language\nModel (LLM) to perform ST and ASR in languages for which the model has never\nseen paired audio-text data. We achieve this by using a pre-trained\nmultilingual speech encoder, a multilingual LLM, and a lightweight adaptation\nmodule that maps the audio representations to the token embedding space of the\nLLM. We perform several experiments both in ST and ASR to understand how to\nbest train the model and what data has the most impact on performance in\npreviously unseen languages. In ST, our best model is capable to achieve BLEU\nscores over 23 in CoVoST2 for two previously unseen languages, while in ASR, we\nachieve WERs of up to 28.2\\%. We finally show that the performance of our\nsystem is bounded by the ability of the LLM to output text in the desired\nlanguage.", "published": "2024-12-24 17:37:11", "link": "http://arxiv.org/abs/2412.18566v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Exploring Embedding Priors in Prompt-Tuning for Improved\n  Interpretability and Control", "abstract": "Prompt-Tuning is an efficient method for adapting pre-trained language models\nto new tasks with minimal computational overhead by modifying prompt\nembeddings. In this work, we investigate how crucial the phenomenon of\nembedding collapse, frequently observed in Prompt-Tuning, is for the final\nperformance of the model. To address this question, we designed embedding\npriors and compared them with posteriors of the converged Soft and Deep\nPrompt-Tuning methods. Our findings suggest that priors strongly affect the\nposition of the tuned embeddings, and models can effectively work with\nembeddings from different parts of activation spaces, including completely new\nregions. As the final Prompt-Tuning capabilities are limited, we hypothesize\nthat controllable Prompt-Tuning posteriors may serve as a good starting point\nfor tasks such as chain-of-thought (COT) distillation. Our experiments also\nshow that generated trajectories are not localized in the activation space of\nthe models. However, there are distinct clusters of activations for distant\ntasks (e.g., NLP and arithmetic), while activations between NLP tasks (e.g.,\nQuestion-Answering and MLM) lie in the same cluster. These observations raise\nquestions about the importance of a single activation cluster for the\ngeneralization abilities of large language models.", "published": "2024-12-24 18:18:52", "link": "http://arxiv.org/abs/2412.18582v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Advancing Explainability in Neural Machine Translation: Analytical\n  Metrics for Attention and Alignment Consistency", "abstract": "Neural Machine Translation (NMT) models have shown remarkable performance but\nremain largely opaque in their decision making processes. The interpretability\nof these models, especially their internal attention mechanisms, is critical\nfor building trust and verifying that these systems behave as intended. In this\nwork, we introduce a systematic framework to quantitatively evaluate the\nexplainability of an NMT model attention patterns by comparing them against\nstatistical alignments and correlating them with standard machine translation\nquality metrics. We present a set of metrics attention entropy and alignment\nagreement and validate them on an English-German test subset from WMT14 using a\npre trained mT5 model. Our results indicate that sharper attention\ndistributions correlate with improved interpretability but do not always\nguarantee better translation quality. These findings advance our understanding\nof NMT explainability and guide future efforts toward building more transparent\nand reliable machine translation systems.", "published": "2024-12-24 20:08:33", "link": "http://arxiv.org/abs/2412.18669v1", "categories": ["cs.AI", "cs.CL", "68T50", "I.2.7; I.2.3"], "primary_category": "cs.AI"}
{"title": "From Hallucinations to Facts: Enhancing Language Models with Curated\n  Knowledge Graphs", "abstract": "Hallucination, a persistent challenge plaguing language models, undermines\ntheir efficacy and trustworthiness in various natural language processing\nendeavors by generating responses that deviate from factual accuracy or\ncoherence. This paper addresses language model hallucination by integrating\ncurated knowledge graph (KG) triples to anchor responses in empirical data. We\nmeticulously select and integrate relevant KG triples tailored to specific\ncontexts, enhancing factual grounding and alignment with input. Our\ncontribution involves constructing a comprehensive KG repository from Wikipedia\nand refining data to spotlight essential information for model training. By\nimbuing language models with access to this curated knowledge, we aim to\ngenerate both linguistically fluent responses and deeply rooted in factual\naccuracy and context relevance. This integration mitigates hallucinations by\nproviding a robust foundation of information, enabling models to draw upon a\nrich reservoir of factual data during response generation. Experimental\nevaluations demonstrate the effectiveness of multiple approaches in reducing\nhallucinatory responses, underscoring the role of curated knowledge graphs in\nimproving the reliability and trustworthiness of language model outputs.", "published": "2024-12-24 20:16:10", "link": "http://arxiv.org/abs/2412.18672v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AgreeMate: Teaching LLMs to Haggle", "abstract": "We introduce AgreeMate, a framework for training Large Language Models (LLMs)\nto perform strategic price negotiations through natural language. We apply\nrecent advances to a negotiation setting where two agents (i.e. buyer or\nseller) use natural language to bargain on goods using coarse actions.\nSpecifically, we present the performance of Large Language Models when used as\nagents within a decoupled (modular) bargaining architecture. We demonstrate\nthat using prompt engineering, fine-tuning, and chain-of-thought prompting\nenhances model performance, as defined by novel metrics. We use attention\nprobing to show model attention to semantic relationships between tokens during\nnegotiations.", "published": "2024-12-24 21:57:17", "link": "http://arxiv.org/abs/2412.18690v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Novel Task-Driven Method with Evolvable Interactive Agents Using Event\n  Trees for Enhanced Emergency Decision Support", "abstract": "As climate change and other global challenges increase the likelihood of\nunforeseen emergencies, the limitations of human-driven strategies in critical\nsituations become more pronounced. Inadequate pre-established emergency plans\ncan lead operators to become overwhelmed during complex systems malfunctions.\nThis study addresses the urgent need for agile decision-making in response to\nvarious unforeseen incidents through a novel approach, EvoTaskTree (a\ntask-driven method with evolvable interactive agents using event trees for\nemergency decision support). This advanced approach integrates two types of\nagents powered by large language models (LLMs): task executors, responsible for\nexecuting critical procedures, and task validators, ensuring the efficacy of\nthose actions. By leveraging insights from event tree analysis, our framework\nencompasses three crucial tasks: initiating event subevent analysis, event tree\nheader event analysis, and decision recommendations. The agents learn from both\nsuccessful and unsuccessful responses from these tasks. Finally, we use nuclear\npower plants as a demonstration of a safety-critical system. Our findings\nindicate that the designed agents are not only effective but also outperform\nexisting approaches, achieving an impressive accuracy rate of up to 100 % in\nprocessing previously unencoun32 tered incident scenarios. This paper\ndemonstrates that EvoTaskTree significantly enhances the rapid formulation of\nemergency decision-making.", "published": "2024-12-24 04:53:46", "link": "http://arxiv.org/abs/2501.06193v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Lla-VAP: LSTM Ensemble of Llama and VAP for Turn-Taking Prediction", "abstract": "Turn-taking prediction is the task of anticipating when the speaker in a\nconversation will yield their turn to another speaker to begin speaking. This\nproject expands on existing strategies for turn-taking prediction by employing\na multi-modal ensemble approach that integrates large language models (LLMs)\nand voice activity projection (VAP) models. By combining the linguistic\ncapabilities of LLMs with the temporal precision of VAP models, we aim to\nimprove the accuracy and efficiency of identifying TRPs in both scripted and\nunscripted conversational scenarios. Our methods are evaluated on the\nIn-Conversation Corpus (ICC) and Coached Conversational Preference Elicitation\n(CCPE) datasets, highlighting the strengths and limitations of current models\nwhile proposing a potentially more robust framework for enhanced prediction.", "published": "2024-12-24 00:20:38", "link": "http://arxiv.org/abs/2412.18061v1", "categories": ["cs.SD", "cs.CL", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MMFactory: A Universal Solution Search Engine for Vision-Language Tasks", "abstract": "With advances in foundational and vision-language models, and effective\nfine-tuning techniques, a large number of both general and special-purpose\nmodels have been developed for a variety of visual tasks. Despite the\nflexibility and accessibility of these models, no single model is able to\nhandle all tasks and/or applications that may be envisioned by potential users.\nRecent approaches, such as visual programming and multimodal LLMs with\nintegrated tools aim to tackle complex visual tasks, by way of program\nsynthesis. However, such approaches overlook user constraints (e.g.,\nperformance / computational needs), produce test-time sample-specific solutions\nthat are difficult to deploy, and, sometimes, require low-level instructions\nthat maybe beyond the abilities of a naive user. To address these limitations,\nwe introduce MMFactory, a universal framework that includes model and metrics\nrouting components, acting like a solution search engine across various\navailable models. Based on a task description and few sample input-output pairs\nand (optionally) resource and/or performance constraints, MMFactory can suggest\na diverse pool of programmatic solutions by instantiating and combining\nvisio-lingual tools from its model repository. In addition to synthesizing\nthese solutions, MMFactory also proposes metrics and benchmarks performance /\nresource characteristics, allowing users to pick a solution that meets their\nunique design constraints. From the technical perspective, we also introduced a\ncommittee-based solution proposer that leverages multi-agent LLM conversation\nto generate executable, diverse, universal, and robust solutions for the user.\nExperimental results show that MMFactory outperforms existing methods by\ndelivering state-of-the-art solutions tailored to user problem specifications.\nProject page is available at https://davidhalladay.github.io/mmfactory_demo.", "published": "2024-12-24 00:59:16", "link": "http://arxiv.org/abs/2412.18072v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Are We in the AI-Generated Text World Already? Quantifying and\n  Monitoring AIGT on Social Media", "abstract": "Social media platforms are experiencing a growing presence of AI-Generated\nTexts (AIGTs). However, the misuse of AIGTs could have profound implications\nfor public opinion, such as spreading misinformation and manipulating\nnarratives. Despite its importance, it remains unclear how prevalent AIGTs are\non social media. To address this gap, this paper aims to quantify and monitor\nthe AIGTs on online social media platforms. We first collect a dataset (SM-D)\nwith around 2.4M posts from 3 major social media platforms: Medium, Quora, and\nReddit. Then, we construct a diverse dataset (AIGTBench) to train and evaluate\nAIGT detectors. AIGTBench combines popular open-source datasets and our AIGT\ndatasets generated from social media texts by 12 LLMs, serving as a benchmark\nfor evaluating mainstream detectors. With this setup, we identify the\nbest-performing detector (OSM-Det). We then apply OSM-Det to SM-D to track\nAIGTs across social media platforms from January 2022 to October 2024, using\nthe AI Attribution Rate (AAR) as the metric. Specifically, Medium and Quora\nexhibit marked increases in AAR, rising from 1.77% to 37.03% and 2.06% to\n38.95%, respectively. In contrast, Reddit shows slower growth, with AAR\nincreasing from 1.31% to 2.45% over the same period. Our further analysis\nindicates that AIGTs on social media differ from human-written texts across\nseveral dimensions, including linguistic patterns, topic distributions,\nengagement levels, and the follower distribution of authors. We envision our\nanalysis and findings on AIGTs in social media can shed light on future\nresearch in this domain.", "published": "2024-12-24 04:04:54", "link": "http://arxiv.org/abs/2412.18148v2", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.SI"], "primary_category": "cs.AI"}
{"title": "GeneSUM: Large Language Model-based Gene Summary Extraction", "abstract": "Emerging topics in biomedical research are continuously expanding, providing\na wealth of information about genes and their function. This rapid\nproliferation of knowledge presents unprecedented opportunities for scientific\ndiscovery and formidable challenges for researchers striving to keep abreast of\nthe latest advancements. One significant challenge is navigating the vast\ncorpus of literature to extract vital gene-related information, a\ntime-consuming and cumbersome task. To enhance the efficiency of this process,\nit is crucial to address several key challenges: (1) the overwhelming volume of\nliterature, (2) the complexity of gene functions, and (3) the automated\nintegration and generation. In response, we propose GeneSUM, a two-stage\nautomated gene summary extractor utilizing a large language model (LLM). Our\napproach retrieves and eliminates redundancy of target gene literature and then\nfine-tunes the LLM to refine and streamline the summarization process. We\nconducted extensive experiments to validate the efficacy of our proposed\nframework. The results demonstrate that LLM significantly enhances the\nintegration of gene-specific information, allowing more efficient\ndecision-making in ongoing research.", "published": "2024-12-24 04:20:43", "link": "http://arxiv.org/abs/2412.18154v1", "categories": ["q-bio.GN", "cs.AI", "cs.CL"], "primary_category": "q-bio.GN"}
{"title": "scReader: Prompting Large Language Models to Interpret scRNA-seq Data", "abstract": "Large language models (LLMs) have demonstrated remarkable advancements,\nprimarily due to their capabilities in modeling the hidden relationships within\ntext sequences. This innovation presents a unique opportunity in the field of\nlife sciences, where vast collections of single-cell omics data from multiple\nspecies provide a foundation for training foundational models. However, the\nchallenge lies in the disparity of data scales across different species,\nhindering the development of a comprehensive model for interpreting genetic\ndata across diverse organisms. In this study, we propose an innovative hybrid\napproach that integrates the general knowledge capabilities of LLMs with\ndomain-specific representation models for single-cell omics data\ninterpretation. We begin by focusing on genes as the fundamental unit of\nrepresentation. Gene representations are initialized using functional\ndescriptions, leveraging the strengths of mature language models such as\nLLaMA-2. By inputting single-cell gene-level expression data with prompts, we\neffectively model cellular representations based on the differential expression\nlevels of genes across various species and cell types. In the experiments, we\nconstructed developmental cells from humans and mice, specifically targeting\ncells that are challenging to annotate. We evaluated our methodology through\nbasic tasks such as cell annotation and visualization analysis. The results\ndemonstrate the efficacy of our approach compared to other methods using LLMs,\nhighlighting significant improvements in accuracy and interoperability. Our\nhybrid approach enhances the representation of single-cell data and offers a\nrobust framework for future research in cross-species genetic analysis.", "published": "2024-12-24 04:28:42", "link": "http://arxiv.org/abs/2412.18156v1", "categories": ["q-bio.GN", "cs.AI", "cs.CL"], "primary_category": "q-bio.GN"}
{"title": "VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics\n  Manipulation with Long-Horizon Reasoning Tasks", "abstract": "General-purposed embodied agents are designed to understand the users'\nnatural instructions or intentions and act precisely to complete universal\ntasks. Recently, methods based on foundation models especially\nVision-Language-Action models (VLAs) have shown a substantial potential to\nsolve language-conditioned manipulation (LCM) tasks well. However, existing\nbenchmarks do not adequately meet the needs of VLAs and relative algorithms. To\nbetter define such general-purpose tasks in the context of LLMs and advance the\nresearch in VLAs, we present VLABench, an open-source benchmark for evaluating\nuniversal LCM task learning. VLABench provides 100 carefully designed\ncategories of tasks, with strong randomization in each category of task and a\ntotal of 2000+ objects. VLABench stands out from previous benchmarks in four\nkey aspects: 1) tasks requiring world knowledge and common sense transfer, 2)\nnatural language instructions with implicit human intentions rather than\ntemplates, 3) long-horizon tasks demanding multi-step reasoning, and 4)\nevaluation of both action policies and language model capabilities. The\nbenchmark assesses multiple competencies including understanding of\nmesh\\&texture, spatial relationship, semantic instruction, physical laws,\nknowledge transfer and reasoning, etc. To support the downstream finetuning, we\nprovide high-quality training data collected via an automated framework\nincorporating heuristic skills and prior information. The experimental results\nindicate that both the current state-of-the-art pretrained VLAs and the\nworkflow based on VLMs face challenges in our tasks.", "published": "2024-12-24 06:03:42", "link": "http://arxiv.org/abs/2412.18194v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "DeepCRCEval: Revisiting the Evaluation of Code Review Comment Generation", "abstract": "Code review is a vital but demanding aspect of software development,\ngenerating significant interest in automating review comments. Traditional\nevaluation methods for these comments, primarily based on text similarity, face\ntwo major challenges: inconsistent reliability of human-authored comments in\nopen-source projects and the weak correlation of text similarity with\nobjectives like enhancing code quality and detecting defects.\n  This study empirically analyzes benchmark comments using a novel set of\ncriteria informed by prior research and developer interviews. We then similarly\nrevisit the evaluation of existing methodologies. Our evaluation framework,\nDeepCRCEval, integrates human evaluators and Large Language Models (LLMs) for a\ncomprehensive reassessment of current techniques based on the criteria set.\nBesides, we also introduce an innovative and efficient baseline, LLM-Reviewer,\nleveraging the few-shot learning capabilities of LLMs for a target-oriented\ncomparison.\n  Our research highlights the limitations of text similarity metrics, finding\nthat less than 10% of benchmark comments are high quality for automation. In\ncontrast, DeepCRCEval effectively distinguishes between high and low-quality\ncomments, proving to be a more reliable evaluation mechanism. Incorporating LLM\nevaluators into DeepCRCEval significantly boosts efficiency, reducing time and\ncost by 88.78% and 90.32%, respectively. Furthermore, LLM-Reviewer demonstrates\nsignificant potential of focusing task real targets in comment generation.", "published": "2024-12-24 08:53:54", "link": "http://arxiv.org/abs/2412.18291v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "ChaI-TeA: A Benchmark for Evaluating Autocompletion of Interactions with\n  LLM-based Chatbots", "abstract": "The rise of LLMs has deflected a growing portion of human-computer\ninteractions towards LLM-based chatbots. The remarkable abilities of these\nmodels allow users to interact using long, diverse natural language text\ncovering a wide range of topics and styles. Phrasing these messages is a time\nand effort consuming task, calling for an autocomplete solution to assist\nusers. We introduce the task of chatbot interaction autocomplete. We present\nChaI-TeA: CHat InTEraction Autocomplete; An autcomplete evaluation framework\nfor LLM-based chatbot interactions. The framework includes a formal definition\nof the task, coupled with suitable datasets and metrics. We use the framework\nto evaluate After formally defining the task along with suitable datasets and\nmetrics, we test 9 models on the defined auto completion task, finding that\nwhile current off-the-shelf models perform fairly, there is still much room for\nimprovement, mainly in ranking of the generated suggestions. We provide\ninsights for practitioners working on this task and open new research\ndirections for researchers in the field. We release our framework to serve as a\nfoundation for future research.", "published": "2024-12-24 12:03:36", "link": "http://arxiv.org/abs/2412.18377v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GeAR: Graph-enhanced Agent for Retrieval-augmented Generation", "abstract": "Retrieval-augmented generation systems rely on effective document retrieval\ncapabilities. By design, conventional sparse or dense retrievers face\nchallenges in multi-hop retrieval scenarios. In this paper, we present GeAR,\nwhich advances RAG performance through two key innovations: (i) graph\nexpansion, which enhances any conventional base retriever, such as BM25, and\n(ii) an agent framework that incorporates graph expansion. Our evaluation\ndemonstrates GeAR's superior retrieval performance on three multi-hop question\nanswering datasets. Additionally, our system achieves state-of-the-art results\nwith improvements exceeding 10% on the challenging MuSiQue dataset, while\nrequiring fewer tokens and iterations compared to other multi-step retrieval\nsystems.", "published": "2024-12-24 13:45:22", "link": "http://arxiv.org/abs/2412.18431v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "How \"Real\" is Your Real-Time Simultaneous Speech-to-Text Translation\n  System?", "abstract": "Simultaneous speech-to-text translation (SimulST) translates source-language\nspeech into target-language text concurrently with the speaker's speech,\nensuring low latency for better user comprehension. Despite its intended\napplication to unbounded speech, most research has focused on human\npre-segmented speech, simplifying the task and overlooking significant\nchallenges. This narrow focus, coupled with widespread terminological\ninconsistencies, is limiting the applicability of research outcomes to\nreal-world applications, ultimately hindering progress in the field. Our\nextensive literature review of 110 papers not only reveals these critical\nissues in current research but also serves as the foundation for our key\ncontributions. We 1) define the steps and core components of a SimulST system,\nproposing a standardized terminology and taxonomy; 2) conduct a thorough\nanalysis of community trends, and 3) offer concrete recommendations and future\ndirections to bridge the gaps in existing literature, from evaluation\nframeworks to system architectures, for advancing the field towards more\nrealistic and effective SimulST solutions.", "published": "2024-12-24 15:26:31", "link": "http://arxiv.org/abs/2412.18495v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Consistency Checks for Language Model Forecasters", "abstract": "Forecasting is a task that is difficult to evaluate: the ground truth can\nonly be known in the future. Recent work showing LLM forecasters rapidly\napproaching human-level performance begs the question: how can we benchmark and\nevaluate these forecasters instantaneously? Following the consistency check\nframework, we measure the performance of forecasters in terms of the\nconsistency of their predictions on different logically-related questions. We\npropose a new, general consistency metric based on arbitrage: for example, if a\nforecasting AI illogically predicts that both the Democratic and Republican\nparties have 60% probability of winning the 2024 US presidential election, an\narbitrageur can trade against the forecaster's predictions and make a profit.\nWe build an automated evaluation system that generates a set of base questions,\ninstantiates consistency checks from these questions, elicits the predictions\nof the forecaster, and measures the consistency of the predictions. We then\nbuild a standard, proper-scoring-rule forecasting benchmark, and show that our\n(instantaneous) consistency metrics correlate with LLM forecasters' ground\ntruth Brier scores (which are only known in the future). We also release a\nconsistency benchmark that resolves in 2028, providing a long-term evaluation\ntool for forecasting.", "published": "2024-12-24 16:51:35", "link": "http://arxiv.org/abs/2412.18544v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Token-Budget-Aware LLM Reasoning", "abstract": "Reasoning is critical for large language models (LLMs) to excel in a wide\nrange of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM\nperformance by decomposing problems into intermediate steps, they also incur\nsignificant overhead in token usage, leading to increased costs. We find that\nthe reasoning process of current LLMs is unnecessarily lengthy and it can be\ncompressed by including a reasonable token budget in the prompt, but the choice\nof token budget plays a crucial role in the actual compression effectiveness.\nWe then propose a token-budget-aware LLM reasoning framework, which dynamically\nestimates token budgets for different problems based on reasoning complexity\nand uses the estimated token budgets to guide the reasoning process.\nExperiments show that our method effectively reduces token costs in CoT\nreasoning with only a slight performance reduction, offering a practical\nsolution to balance efficiency and accuracy in LLM reasoning. Code:\nhttps://github.com/GeniusHTX/TALE.", "published": "2024-12-24 16:55:45", "link": "http://arxiv.org/abs/2412.18547v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Top General Performance = Top Domain Performance? DomainCodeBench: A\n  Multi-domain Code Generation Benchmark", "abstract": "With the rapid advancement of large language models (LLMs), extensive\nresearch has been conducted to investigate the code generation capabilities of\nLLMs. However, existing efforts primarily focus on general-domain tasks,\nleaving LLMs' code generation performance in real-world application domains\nunderexplored. This raises a critical question: can a model's general-domain\ncoding ability reliably represent its ability in specialized domains? In this\npaper, we introduce DomainCodeBench, a multi-domain code generation benchmark\ndesigned to systematically evaluate LLMs across 12 software application domains\nand 15 programming languages. DomainCodeBench contains 2,400 manually verified\ntasks with ground truth, human-annotated docstrings, and fine-grained\ndependency information to ensure more coverage of domain-specific challenges.\nSpecifically, we first identify the most popular application domains by topic\nmining. Then, we curate coding tasks based on commonly used frameworks and\nplatforms in each domain. We obtain several findings through extensive\nexperiments on DomainCodeBench with ten mainstream LLMs. (1) Performance\ndecoupling: experiments reveal that top general-domain models do not\nconsistently excel in specific application domains; (2) Domain-specific\nweaknesses: LLMs often fail due to domain knowledge gaps and third-party\nlibrary misusage; (3) Contextual enhancement: we show that augmenting prompts\nwith domain-specific knowledge improves performance by around 38.17%, providing\nactionable insights for performance optimization. Our replication package,\nincluding the benchmark, source code, and experimental results, is available at\nhttps://github.com/DeepSoftwareAnalytics/DomainCodeBench.", "published": "2024-12-24 17:56:08", "link": "http://arxiv.org/abs/2412.18573v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Long-Form Speech Generation with Spoken Language Models", "abstract": "We consider the generative modeling of speech over multiple minutes, a\nrequirement for long-form multimedia generation and audio-native voice\nassistants. However, current spoken language models struggle to generate\nplausible speech past tens of seconds, from high temporal resolution of speech\ntokens causing loss of coherence, to architectural issues with long-sequence\ntraining or extrapolation, to memory costs at inference time. With these\nconsiderations we propose SpeechSSM, the first speech language model to learn\nfrom and sample long-form spoken audio (e.g., 16 minutes of read or\nextemporaneous speech) in a single decoding session without text intermediates,\nbased on recent advances in linear-time sequence modeling. Furthermore, to\naddress growing challenges in spoken language evaluation, especially in this\nnew long-form setting, we propose: new embedding-based and LLM-judged metrics;\nquality measurements over length and time; and a new benchmark for long-form\nspeech processing and generation, LibriSpeech-Long. Speech samples and the\ndataset are released at\nhttps://google.github.io/tacotron/publications/speechssm/", "published": "2024-12-24 18:56:46", "link": "http://arxiv.org/abs/2412.18603v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "DynaGRAG | Exploring the Topology of Information for Advancing Language\n  Understanding and Generation in Graph Retrieval-Augmented Generation", "abstract": "Graph Retrieval-Augmented Generation (GRAG or Graph RAG) architectures aim to\nenhance language understanding and generation by leveraging external knowledge.\nHowever, effectively capturing and integrating the rich semantic information\npresent in textual and structured data remains a challenge. To address this, a\nnovel GRAG framework, Dynamic Graph Retrieval-Agumented Generation (DynaGRAG),\nis proposed to focus on enhancing subgraph representation and diversity within\nthe knowledge graph. By improving graph density, capturing entity and relation\ninformation more effectively, and dynamically prioritizing relevant and diverse\nsubgraphs and information within them, the proposed approach enables a more\ncomprehensive understanding of the underlying semantic structure. This is\nachieved through a combination of de-duplication processes, two-step mean\npooling of embeddings, query-aware retrieval considering unique nodes, and a\nDynamic Similarity-Aware BFS (DSA-BFS) traversal algorithm. Integrating Graph\nConvolutional Networks (GCNs) and Large Language Models (LLMs) through hard\nprompting further enhances the learning of rich node and edge representations\nwhile preserving the hierarchical subgraph structure. Experimental results\ndemonstrate the effectiveness of DynaGRAG, showcasing the significance of\nenhanced subgraph representation and diversity for improved language\nunderstanding and generation.", "published": "2024-12-24 16:06:53", "link": "http://arxiv.org/abs/2412.18644v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Diverse and Effective Red Teaming with Auto-generated Rewards and\n  Multi-step Reinforcement Learning", "abstract": "Automated red teaming can discover rare model failures and generate\nchallenging examples that can be used for training or evaluation. However, a\ncore challenge in automated red teaming is ensuring that the attacks are both\ndiverse and effective. Prior methods typically succeed in optimizing either for\ndiversity or for effectiveness, but rarely both. In this paper, we provide\nmethods that enable automated red teaming to generate a large number of diverse\nand successful attacks.\n  Our approach decomposes the task into two steps: (1) automated methods for\ngenerating diverse attack goals and (2) generating effective attacks for those\ngoals. While we provide multiple straightforward methods for generating diverse\ngoals, our key contributions are to train an RL attacker that both follows\nthose goals and generates diverse attacks for those goals. First, we\ndemonstrate that it is easy to use a large language model (LLM) to generate\ndiverse attacker goals with per-goal prompts and rewards, including rule-based\nrewards (RBRs) to grade whether the attacks are successful for the particular\ngoal. Second, we demonstrate how training the attacker model with multi-step\nRL, where the model is rewarded for generating attacks that are different from\npast attempts further increases diversity while remaining effective. We use our\napproach to generate both prompt injection attacks and prompts that elicit\nunsafe responses. In both cases, we find that our approach is able to generate\nhighly-effective and considerably more diverse attacks than past general\nred-teaming approaches.", "published": "2024-12-24 22:38:46", "link": "http://arxiv.org/abs/2412.18693v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CypherBench: Towards Precise Retrieval over Full-scale Modern Knowledge\n  Graphs in the LLM Era", "abstract": "Retrieval from graph data is crucial for augmenting large language models\n(LLM) with both open-domain knowledge and private enterprise data, and it is\nalso a key component in the recent GraphRAG system (edge et al., 2024). Despite\ndecades of research on knowledge graphs and knowledge base question answering,\nleading LLM frameworks (e.g. Langchain and LlamaIndex) have only minimal\nsupport for retrieval from modern encyclopedic knowledge graphs like Wikidata.\nIn this paper, we analyze the root cause and suggest that modern RDF knowledge\ngraphs (e.g. Wikidata, Freebase) are less efficient for LLMs due to overly\nlarge schemas that far exceed the typical LLM context window, use of resource\nidentifiers, overlapping relation types and lack of normalization. As a\nsolution, we propose property graph views on top of the underlying RDF graph\nthat can be efficiently queried by LLMs using Cypher. We instantiated this idea\non Wikidata and introduced CypherBench, the first benchmark with 11\nlarge-scale, multi-domain property graphs with 7.8 million entities and over\n10,000 questions. To achieve this, we tackled several key challenges, including\ndeveloping an RDF-to-property graph conversion engine, creating a systematic\npipeline for text-to-Cypher task generation, and designing new evaluation\nmetrics.", "published": "2024-12-24 23:22:04", "link": "http://arxiv.org/abs/2412.18702v2", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Generating Traffic Scenarios via In-Context Learning to Learn Better\n  Motion Planner", "abstract": "Motion planning is a crucial component in autonomous driving.\nState-of-the-art motion planners are trained on meticulously curated datasets,\nwhich are not only expensive to annotate but also insufficient in capturing\nrarely seen critical scenarios. Failing to account for such scenarios poses a\nsignificant risk to motion planners and may lead to incidents during testing.\nAn intuitive solution is to manually compose such scenarios by programming and\nexecuting a simulator (e.g., CARLA). However, this approach incurs substantial\nhuman costs. Motivated by this, we propose an inexpensive method for generating\ndiverse critical traffic scenarios to train more robust motion planners. First,\nwe represent traffic scenarios as scripts, which are then used by the simulator\nto generate traffic scenarios. Next, we develop a method that accepts\nuser-specified text descriptions, which a Large Language Model (LLM) translates\ninto scripts using in-context learning. The output scripts are sent to the\nsimulator that produces the corresponding traffic scenarios. As our method can\ngenerate abundant safety-critical traffic scenarios, we use them as synthetic\ntraining data for motion planners. To demonstrate the value of generated\nscenarios, we train existing motion planners on our synthetic data, real-world\ndatasets, and a combination of both. Our experiments show that motion planners\ntrained with our data significantly outperform those trained solely on\nreal-world data, showing the usefulness of our synthetic data and the\neffectiveness of our data generation method. Our source code is available at\nhttps://ezharjan.github.io/AutoSceneGen.", "published": "2024-12-24 01:52:19", "link": "http://arxiv.org/abs/2412.18086v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.GR", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Characterizations of Language Generation With Breadth", "abstract": "We study language generation in the limit, introduced by Kleinberg and\nMullainathan [KM24], building on classical works of Gold [Gol67] and Angluin\n[Ang79]. [KM24] proposed an algorithm that generates strings from any countable\nlanguage collection in the limit. While their algorithm eventually outputs\nstrings from the target language $K$, it sacrifices breadth, i.e., the ability\nto generate all strings in $K$. A key open question in [KM24] is whether this\ntrade-off between consistency and breadth is inherrent.\n  Recent works proposed different notions of consistent generation with\nbreadth. Kalavasis, Mehrotra, and Velegkas [KVM24] introduced three\ndefinitions: generation with exact breadth, approximate breadth, and\nunambiguous generation. Concurrently and independently, Charikar and Pabbaraju\n[CP24a] proposed exhaustive generation. Both works examined when generation\nwith these notions of breadth is possible.\n  Building on [CP24a, KVM24], we fully characterize language generation for\nthese notions and their natural combinations. For exact breadth, we provide an\nunconditional lower bound, removing a technical condition from [KVM24] and\nextending the result of [CP24a] that holds for specific collections of\nlanguages. We show that generation with exact breadth is characterized by\nAngluin's condition for identification. We further introduce a weaker version\nof Angluin's condition that tightly characterizes both approximate breadth and\nexhaustive generation, proving their equivalence. Additionally, we show that\nunambiguous generation is also characterized by Angluin's condition as a\nspecial case of a broader result. Finally, we strengthen [KVM24] by giving\nunconditional lower bounds for stable generators, showing that Angluin's\ncondition characterizes the previous breadth notions for stable generators.\nThis shows a separation between stable and unstable generation with approximate\nbreadth.", "published": "2024-12-24 16:24:43", "link": "http://arxiv.org/abs/2412.18530v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Neural Directed Speech Enhancement with Dual Microphone Array in High\n  Noise Scenario", "abstract": "In multi-speaker scenarios, leveraging spatial features is essential for\nenhancing target speech. While with limited microphone arrays, developing a\ncompact multi-channel speech enhancement system remains challenging, especially\nin extremely low signal-to-noise ratio (SNR) conditions. To tackle this issue,\nwe propose a triple-steering spatial selection method, a flexible framework\nthat uses three steering vectors to guide enhancement and determine the\nenhancement range. Specifically, we introduce a causal-directed U-Net (CDUNet)\nmodel, which takes raw multi-channel speech and the desired enhancement width\nas inputs. This enables dynamic adjustment of steering vectors based on the\ntarget direction and fine-tuning of the enhancement region according to the\nangular separation between the target and interference signals. Our model with\nonly a dual microphone array, excels in both speech quality and downstream task\nperformance. It operates in real-time with minimal parameters, making it ideal\nfor low-latency, on-device streaming applications.", "published": "2024-12-24 03:54:17", "link": "http://arxiv.org/abs/2412.18141v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Explaining Speaker and Spoof Embeddings via Probing", "abstract": "This study investigates the explainability of embedding representations,\nspecifically those used in modern audio spoofing detection systems based on\ndeep neural networks, known as spoof embeddings. Building on established work\nin speaker embedding explainability, we examine how well these spoof embeddings\ncapture speaker-related information. We train simple neural classifiers using\neither speaker or spoof embeddings as input, with speaker-related attributes as\ntarget labels. These attributes are categorized into two groups: metadata-based\ntraits (e.g., gender, age) and acoustic traits (e.g., fundamental frequency,\nspeaking rate). Our experiments on the ASVspoof 2019 LA evaluation set\ndemonstrate that spoof embeddings preserve several key traits, including\ngender, speaking rate, F0, and duration. Further analysis of gender and\nspeaking rate indicates that the spoofing detector partially preserves these\ntraits, potentially to ensure the decision process remains robust against them.", "published": "2024-12-24 05:56:49", "link": "http://arxiv.org/abs/2412.18191v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Zero-Shot Physics-Informed Dictionary Learning Approach for Sound\n  Field Reconstruction", "abstract": "Sound field reconstruction aims to estimate pressure fields in areas lacking\ndirect measurements. Existing techniques often rely on strong assumptions or\nface challenges related to data availability or the explicit modeling of\nphysical properties. To bridge these gaps, this study introduces a zero-shot,\nphysics-informed dictionary learning approach to perform sound field\nreconstruction. Our method relies only on a few sparse measurements to learn a\ndictionary, without the need for additional training data. Moreover, by\nenforcing the Helmholtz equation during the optimization process, the proposed\napproach ensures that the reconstructed sound field is represented as a linear\ncombination of a few physically meaningful atoms. Evaluations on real-world\ndata show that our approach achieves comparable performance to state-of-the-art\ndictionary learning techniques, with the advantage of requiring only a few\nobservations of the sound field and no training on a dataset.", "published": "2024-12-24 11:16:17", "link": "http://arxiv.org/abs/2412.18348v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "SongGLM: Lyric-to-Melody Generation with 2D Alignment Encoding and\n  Multi-Task Pre-Training", "abstract": "Lyric-to-melody generation aims to automatically create melodies based on\ngiven lyrics, requiring the capture of complex and subtle correlations between\nthem. However, previous works usually suffer from two main challenges: 1)\nlyric-melody alignment modeling, which is often simplified to\none-syllable/word-to-one-note alignment, while others have the problem of low\nalignment accuracy; 2) lyric-melody harmony modeling, which usually relies\nheavily on intermediates or strict rules, limiting model's capabilities and\ngenerative diversity. In this paper, we propose SongGLM, a lyric-to-melody\ngeneration system that leverages 2D alignment encoding and multi-task\npre-training based on the General Language Model (GLM) to guarantee the\nalignment and harmony between lyrics and melodies. Specifically, 1) we\nintroduce a unified symbolic song representation for lyrics and melodies with\nword-level and phrase-level (2D) alignment encoding to capture the lyric-melody\nalignment; 2) we design a multi-task pre-training framework with hierarchical\nblank infilling objectives (n-gram, phrase, and long span), and incorporate\nlyric-melody relationships into the extraction of harmonized n-grams to ensure\nthe lyric-melody harmony. We also construct a large-scale lyric-melody paired\ndataset comprising over 200,000 English song pieces for pre-training and\nfine-tuning. The objective and subjective results indicate that SongGLM can\ngenerate melodies from lyrics with significant improvements in both alignment\nand harmony, outperforming all the previous baseline methods.", "published": "2024-12-24 02:30:07", "link": "http://arxiv.org/abs/2412.18107v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Text-Aware Adapter for Few-Shot Keyword Spotting", "abstract": "Recent advances in flexible keyword spotting (KWS) with text enrollment allow\nusers to personalize keywords without uttering them during enrollment. However,\nthere is still room for improvement in target keyword performance. In this\nwork, we propose a novel few-shot transfer learning method, called text-aware\nadapter (TA-adapter), designed to enhance a pre-trained flexible KWS model for\nspecific keywords with limited speech samples. To adapt the acoustic encoder,\nwe leverage a jointly pre-trained text encoder to generate a text embedding\nthat acts as a representative vector for the keyword. By fine-tuning only a\nsmall portion of the network while keeping the core components' weights intact,\nthe TA-adapter proves highly efficient for few-shot KWS, enabling a seamless\nreturn to the original pre-trained model. In our experiments, the TA-adapter\ndemonstrated significant performance improvements across 35 distinct keywords\nfrom the Google Speech Commands V2 dataset, with only a 0.14% increase in the\ntotal number of parameters.", "published": "2024-12-24 03:54:40", "link": "http://arxiv.org/abs/2412.18142v1", "categories": ["eess.AS", "cs.AI", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Smooth-Foley: Creating Continuous Sound for Video-to-Audio Generation\n  Under Semantic Guidance", "abstract": "The video-to-audio (V2A) generation task has drawn attention in the field of\nmultimedia due to the practicality in producing Foley sound. Semantic and\ntemporal conditions are fed to the generation model to indicate sound events\nand temporal occurrence. Recent studies on synthesizing immersive and\nsynchronized audio are faced with challenges on videos with moving visual\npresence. The temporal condition is not accurate enough while low-resolution\nsemantic condition exacerbates the problem. To tackle these challenges, we\npropose Smooth-Foley, a V2A generative model taking semantic guidance from the\ntextual label across the generation to enhance both semantic and temporal\nalignment in audio. Two adapters are trained to leverage pre-trained\ntext-to-audio generation models. A frame adapter integrates high-resolution\nframe-wise video features while a temporal adapter integrates temporal\nconditions obtained from similarities of visual frames and textual labels. The\nincorporation of semantic guidance from textual labels achieves precise\naudio-video alignment. We conduct extensive quantitative and qualitative\nexperiments. Results show that Smooth-Foley performs better than existing\nmodels on both continuous sound scenarios and general scenarios. With semantic\nguidance, the audio generated by Smooth-Foley exhibits higher quality and\nbetter adherence to physical laws.", "published": "2024-12-24 04:29:46", "link": "http://arxiv.org/abs/2412.18157v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "U-Mamba-Net: A highly efficient Mamba-based U-net style network for\n  noisy and reverberant speech separation", "abstract": "The topic of speech separation involves separating mixed speech with multiple\noverlapping speakers into several streams, with each stream containing speech\nfrom only one speaker. Many highly effective models have emerged and\nproliferated rapidly over time. However, the size and computational load of\nthese models have also increased accordingly. This is a disaster for the\ncommunity, as researchers need more time and computational resources to\nreproduce and compare existing models. In this paper, we propose U-mamba-net: a\nlightweight Mamba-based U-style model for speech separation in complex\nenvironments. Mamba is a state space sequence model that incorporates feature\nselection capabilities. U-style network is a fully convolutional neural network\nwhose symmetric contracting and expansive paths are able to learn\nmulti-resolution features. In our work, Mamba serves as a feature filter,\nalternating with U-Net. We test the proposed model on Libri2mix. The results\nshow that U-Mamba-Net achieves improved performance with quite low\ncomputational cost.", "published": "2024-12-24 06:51:21", "link": "http://arxiv.org/abs/2412.18217v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Detection and Forecasting of Parkinson Disease Progression from Speech\n  Signal Features Using MultiLayer Perceptron and LSTM", "abstract": "Accurate diagnosis of Parkinson disease, especially in its early stages, can\nbe a challenging task. The application of machine learning techniques helps\nimprove the diagnostic accuracy of Parkinson disease detection but only few\nstudies have presented work towards the prediction of disease progression. In\nthis research work, Long Short Term Memory LSTM was trained using the\ndiagnostic features on Parkinson patients speech signals, to predict the\ndisease progression while a Multilayer Perceptron MLP was trained on the same\ndiagnostic features to detect the disease. Diagnostic features selected using\ntwo well-known feature selection methods named Relief-F and Sequential Forward\nSelection and applied on LSTM and MLP have shown to accurately predict the\ndisease progression as stage 2 and 3 and its existence respectively.", "published": "2024-12-24 08:02:43", "link": "http://arxiv.org/abs/2412.18248v1", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Sound-Based Recognition of Touch Gestures and Emotions for Enhanced\n  Human-Robot Interaction", "abstract": "Emotion recognition and touch gesture decoding are crucial for advancing\nhuman-robot interaction (HRI), especially in social environments where\nemotional cues and tactile perception play important roles. However, many\nhumanoid robots, such as Pepper, Nao, and Furhat, lack full-body tactile skin,\nlimiting their ability to engage in touch-based emotional and gesture\ninteractions. In addition, vision-based emotion recognition methods usually\nface strict GDPR compliance challenges due to the need to collect personal\nfacial data. To address these limitations and avoid privacy issues, this paper\nstudies the potential of using the sounds produced by touching during HRI to\nrecognise tactile gestures and classify emotions along the arousal and valence\ndimensions. Using a dataset of tactile gestures and emotional interactions from\n28 participants with the humanoid robot Pepper, we design an audio-only\nlightweight touch gesture and emotion recognition model with only 0.24M\nparameters, 0.94MB model size, and 0.7G FLOPs. Experimental results show that\nthe proposed sound-based touch gesture and emotion recognition model\neffectively recognises the arousal and valence states of different emotions, as\nwell as various tactile gestures, when the input audio length varies. The\nproposed model is low-latency and achieves similar results as well-known\npretrained audio neural networks (PANNs), but with much smaller FLOPs,\nparameters, and model size.", "published": "2024-12-24 09:51:00", "link": "http://arxiv.org/abs/2501.00038v1", "categories": ["cs.HC", "cs.RO", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
