{"title": "Pretrained Models for Multilingual Federated Learning", "abstract": "Since the advent of Federated Learning (FL), research has applied these\nmethods to natural language processing (NLP) tasks. Despite a plethora of\npapers in FL for NLP, no previous works have studied how multilingual text\nimpacts FL algorithms. Furthermore, multilingual text provides an interesting\navenue to examine the impact of non-IID text (e.g. different languages) on FL\nin naturally occurring data. We explore three multilingual language tasks,\nlanguage modeling, machine translation, and text classification using differing\nfederated and non-federated learning algorithms. Our results show that using\npretrained models reduces the negative effects of FL, helping them to perform\nnear or better than centralized (no privacy) learning, even when using non-IID\npartitioning.", "published": "2022-06-06 00:20:30", "link": "http://arxiv.org/abs/2206.02291v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Break the Loop: Analyzing and Mitigating Repetitions for\n  Neural Text Generation", "abstract": "While large-scale neural language models, such as GPT2 and BART, have\nachieved impressive results on various text generation tasks, they tend to get\nstuck in undesirable sentence-level loops with maximization-based decoding\nalgorithms (\\textit{e.g.}, greedy search). This phenomenon is counter-intuitive\nsince there are few consecutive sentence-level repetitions in human corpora\n(e.g., 0.02\\% in Wikitext-103). To investigate the underlying reasons for\ngenerating consecutive sentence-level repetitions, we study the relationship\nbetween the probabilities of the repetitive tokens and their previous\nrepetitions in the context. Through our quantitative experiments, we find that\n1) Language models have a preference to repeat the previous sentence; 2) The\nsentence-level repetitions have a \\textit{self-reinforcement effect}: the more\ntimes a sentence is repeated in the context, the higher the probability of\ncontinuing to generate that sentence; 3) The sentences with higher initial\nprobabilities usually have a stronger self-reinforcement effect. Motivated by\nour findings, we propose a simple and effective training method \\textbf{DITTO}\n(Pseu\\underline{D}o-Repet\\underline{IT}ion\nPenaliza\\underline{T}i\\underline{O}n), where the model learns to penalize\nprobabilities of sentence-level repetitions from pseudo repetitive data.\nAlthough our method is motivated by mitigating repetitions, experiments show\nthat DITTO not only mitigates the repetition issue without sacrificing\nperplexity, but also achieves better generation quality. Extensive experiments\non open-ended text generation (Wikitext-103) and text summarization\n(CNN/DailyMail) demonstrate the generality and effectiveness of our method.", "published": "2022-06-06 05:51:12", "link": "http://arxiv.org/abs/2206.02369v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MorisienMT: A Dataset for Mauritian Creole Machine Translation", "abstract": "In this paper, we describe MorisienMT, a dataset for benchmarking machine\ntranslation quality of Mauritian Creole. Mauritian Creole (Morisien) is the\nlingua franca of the Republic of Mauritius and is a French-based creole\nlanguage. MorisienMT consists of a parallel corpus between English and\nMorisien, French and Morisien and a monolingual corpus for Morisien. We first\ngive an overview of Morisien and then describe the steps taken to create the\ncorpora and, from it, the training and evaluation splits. Thereafter, we\nestablish a variety of baseline models using the created parallel corpora as\nwell as large French--English corpora for transfer learning. We release our\ndatasets publicly for research purposes and hope that this spurs research for\nMorisien machine translation.", "published": "2022-06-06 08:30:03", "link": "http://arxiv.org/abs/2206.02421v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain-specific Language Pre-training for Dialogue Comprehension on\n  Clinical Inquiry-Answering Conversations", "abstract": "There is growing interest in the automated extraction of relevant information\nfrom clinical dialogues. However, it is difficult to collect and construct\nlarge annotated resources for clinical dialogue tasks. Recent developments in\nnatural language processing suggest that large-scale pre-trained language\nbackbones could be leveraged for such machine comprehension and information\nextraction tasks. Yet, due to the gap between pre-training and downstream\nclinical domains, it remains challenging to exploit the generic backbones for\ndomain-specific applications. Therefore, in this work, we propose a\ndomain-specific language pre-training, to improve performance on downstream\ntasks like dialogue comprehension. Aside from the common token-level masking\npre-training method, according to the nature of human conversations and\ninteractive flow of multi-topic inquiry-answering dialogues, we further propose\nsample generation strategies with speaker and utterance manipulation. The\nconversational pre-training guides the language backbone to reconstruct the\nutterances coherently based on the remaining context, thus bridging the gap\nbetween general and specific domains. Experiments are conducted on a clinical\nconversation dataset for symptom checking, where nurses inquire and discuss\nsymptom information with patients. We empirically show that the neural model\nwith our proposed approach brings improvement in the dialogue comprehension\ntask, and can achieve favorable results in the low resource training scenario.", "published": "2022-06-06 08:45:03", "link": "http://arxiv.org/abs/2206.02428v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A computational psycholinguistic evaluation of the syntactic abilities\n  of Galician BERT models at the interface of dependency resolution and\n  training time", "abstract": "This paper explores the ability of Transformer models to capture subject-verb\nand noun-adjective agreement dependencies in Galician. We conduct a series of\nword prediction experiments in which we manipulate dependency length together\nwith the presence of an attractor noun that acts as a lure. First, we evaluate\nthe overall performance of the existing monolingual and multilingual models for\nGalician. Secondly, to observe the effects of the training process, we compare\nthe different degrees of achievement of two monolingual BERT models at\ndifferent training points. We also release their checkpoints and propose an\nalternative evaluation metric. Our results confirm previous findings by similar\nworks that use the agreement prediction task and provide interesting insights\ninto the number of training steps required by a Transformer model to solve\nlong-distance dependencies.", "published": "2022-06-06 09:03:11", "link": "http://arxiv.org/abs/2206.02440v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What do tokens know about their characters and how do they know it?", "abstract": "Pre-trained language models (PLMs) that use subword tokenization schemes can\nsucceed at a variety of language tasks that require character-level\ninformation, despite lacking explicit access to the character composition of\ntokens. Here, studying a range of models (e.g., GPT- J, BERT, RoBERTa, GloVe),\nwe probe what word pieces encode about character-level information by training\nclassifiers to predict the presence or absence of a particular alphabetical\ncharacter in a token, based on its embedding (e.g., probing whether the model\nembedding for \"cat\" encodes that it contains the character \"a\"). We find that\nthese models robustly encode character-level information and, in general,\nlarger models perform better at the task. We show that these results generalize\nto characters from non-Latin alphabets (Arabic, Devanagari, and Cyrillic).\nThen, through a series of experiments and analyses, we investigate the\nmechanisms through which PLMs acquire English-language character information\nduring training and argue that this knowledge is acquired through multiple\nphenomena, including a systematic relationship between particular characters\nand particular parts of speech, as well as natural variability in the\ntokenization of related strings.", "published": "2022-06-06 13:27:26", "link": "http://arxiv.org/abs/2206.02608v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Ask Like a Physician", "abstract": "Existing question answering (QA) datasets derived from electronic health\nrecords (EHR) are artificially generated and consequently fail to capture\nrealistic physician information needs. We present Discharge Summary Clinical\nQuestions (DiSCQ), a newly curated question dataset composed of 2,000+\nquestions paired with the snippets of text (triggers) that prompted each\nquestion. The questions are generated by medical experts from 100+ MIMIC-III\ndischarge summaries. We analyze this dataset to characterize the types of\ninformation sought by medical experts. We also train baseline models for\ntrigger detection and question generation (QG), paired with unsupervised answer\nretrieval over EHRs. Our baseline model is able to generate high quality\nquestions in over 62% of cases when prompted with human selected triggers. We\nrelease this dataset (and all code to reproduce baseline model results) to\nfacilitate further research into realistic clinical QA and QG:\nhttps://github.com/elehman16/discq.", "published": "2022-06-06 15:50:54", "link": "http://arxiv.org/abs/2206.02696v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Curriculum-Based Self-Training Makes Better Few-Shot Learners for\n  Data-to-Text Generation", "abstract": "Despite the success of text-to-text pre-trained models in various natural\nlanguage generation (NLG) tasks, the generation performance is largely\nrestricted by the number of labeled data in downstream tasks, particularly in\ndata-to-text generation tasks. Existing works mostly utilize abundant unlabeled\nstructured data to conduct unsupervised pre-training for task adaption, which\nfail to model the complex relationship between source structured data and\ntarget texts. Thus, we introduce self-training as a better few-shot learner\nthan task-adaptive pre-training, which explicitly captures this relationship\nvia pseudo-labeled data generated by the pre-trained model. To alleviate the\nside-effect of low-quality pseudo-labeled data during self-training, we propose\na novel method called Curriculum-Based Self-Training (CBST) to effectively\nleverage unlabeled data in a rearranged order determined by the difficulty of\ntext generation. Experimental results show that our method can outperform\nfine-tuning and task-adaptive pre-training methods, and achieve\nstate-of-the-art performance in the few-shot setting of data-to-text\ngeneration.", "published": "2022-06-06 16:11:58", "link": "http://arxiv.org/abs/2206.02712v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating the use of Paraphrase Generation for Question\n  Reformulation in the FRANK QA system", "abstract": "We present a study into the ability of paraphrase generation methods to\nincrease the variety of natural language questions that the FRANK Question\nAnswering system can answer. We first evaluate paraphrase generation methods on\nthe LC-QuAD 2.0 dataset using both automatic metrics and human judgement, and\ndiscuss their correlation. Error analysis on the dataset is also performed\nusing both automatic and manual approaches, and we discuss how paraphrase\ngeneration and evaluation is affected by data points which contain error. We\nthen simulate an implementation of the best performing paraphrase generation\nmethod (an English-French backtranslation) into FRANK in order to test our\noriginal hypothesis, using a small challenge dataset. Our two main conclusions\nare that cleaning of LC-QuAD 2.0 is required as the errors present can affect\nevaluation; and that, due to limitations of FRANK's parser, paraphrase\ngeneration is not a method which we can rely on to improve the variety of\nnatural language questions that FRANK can answer.", "published": "2022-06-06 16:46:36", "link": "http://arxiv.org/abs/2206.02737v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discriminative Models Can Still Outperform Generative Models in Aspect\n  Based Sentiment Analysis", "abstract": "Aspect-based Sentiment Analysis (ABSA) helps to explain customers' opinions\ntowards products and services. In the past, ABSA models were discriminative,\nbut more recently generative models have been used to generate aspects and\npolarities directly from text. In contrast, discriminative models commonly\nfirst select aspects from the text, and then classify the aspect's polarity.\nPrevious results showed that generative models outperform discriminative models\non several English ABSA datasets. Here, we evaluate and contrast two\nstate-of-the-art discriminative and generative models in several settings:\ncross-lingual, cross-domain, and cross-lingual and domain, to understand\ngeneralizability in settings other than English mono-lingual in-domain. Our\nmore thorough evaluation shows that, contrary to previous studies,\ndiscriminative models can still outperform generative models in almost all\nsettings.", "published": "2022-06-06 20:32:58", "link": "http://arxiv.org/abs/2206.02892v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CHEF: A Pilot Chinese Dataset for Evidence-Based Fact-Checking", "abstract": "The explosion of misinformation spreading in the media ecosystem urges for\nautomated fact-checking. While misinformation spans both geographic and\nlinguistic boundaries, most work in the field has focused on English. Datasets\nand tools available in other languages, such as Chinese, are limited. In order\nto bridge this gap, we construct CHEF, the first CHinese Evidence-based\nFact-checking dataset of 10K real-world claims. The dataset covers multiple\ndomains, ranging from politics to public health, and provides annotated\nevidence retrieved from the Internet. Further, we develop established baselines\nand a novel approach that is able to model the evidence retrieval as a latent\nvariable, allowing jointly training with the veracity prediction model in an\nend-to-end fashion. Extensive experiments show that CHEF will provide a\nchallenging testbed for the development of fact-checking systems designed to\nretrieve and reason over non-English claims.", "published": "2022-06-06 09:11:03", "link": "http://arxiv.org/abs/2206.11863v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Making Large Language Models Better Reasoners with Step-Aware Verifier", "abstract": "Few-shot learning is a challenging task that requires language models to\ngeneralize from limited examples. Large language models like GPT-3 and PaLM\nhave made impressive progress in this area, but they still face difficulties in\nreasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve\ntheir reasoning skills, previous work has proposed to guide the language model\nwith prompts that elicit a series of reasoning steps before giving the final\nanswer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in\nproblem-solving rate. In this paper, we present DIVERSE (Diverse Verifier on\nReasoning Step), a novel approach that further enhances the reasoning\ncapability of language models. DIVERSE has three main components: first, it\ngenerates diverse prompts to explore different reasoning paths for the same\nquestion; second, it uses a verifier to filter out incorrect answers based on a\nweighted voting scheme; and third, it verifies each reasoning step individually\ninstead of the whole chain. We evaluate DIVERSE on the latest language model\ncode-davinci-002 and show that it achieves new state-of-the-art results on six\nof eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%).", "published": "2022-06-06 03:38:36", "link": "http://arxiv.org/abs/2206.02336v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge-based Document Classification with Shannon Entropy", "abstract": "Document classification is the detection specific content of interest in text\ndocuments. In contrast to the data-driven machine learning classifiers,\nknowledge-based classifiers can be constructed based on domain specific\nknowledge, which usually takes the form of a collection of subject related\nkeywords. While typical knowledge-based classifiers compute a prediction score\nbased on the keyword abundance, it generally suffers from noisy detections due\nto the lack of guiding principle in gauging the keyword matches. In this paper,\nwe propose a novel knowledge-based model equipped with Shannon Entropy, which\nmeasures the richness of information and favors uniform and diverse keyword\nmatches. Without invoking any positive sample, such method provides a simple\nand explainable solution for document classification. We show that the Shannon\nEntropy significantly improves the recall at fixed level of false positive\nrate. Also, we show that the model is more robust against change of data\ndistribution at inference while compared with traditional machine learning,\nparticularly when the positive training samples are very limited.", "published": "2022-06-06 05:39:10", "link": "http://arxiv.org/abs/2206.02363v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Bi-SimCut: A Simple Strategy for Boosting Neural Machine Translation", "abstract": "We introduce Bi-SimCut: a simple but effective training strategy to boost\nneural machine translation (NMT) performance. It consists of two procedures:\nbidirectional pretraining and unidirectional finetuning. Both procedures\nutilize SimCut, a simple regularization method that forces the consistency\nbetween the output distributions of the original and the cutoff sentence pairs.\nWithout leveraging extra dataset via back-translation or integrating\nlarge-scale pretrained model, Bi-SimCut achieves strong translation performance\nacross five translation benchmarks (data sizes range from 160K to 20.2M): BLEU\nscores of 31.16 for en -> de and 38.37 for de -> en on the IWSLT14 dataset,\n30.78 for en -> de and 35.15 for de -> en on the WMT14 dataset, and 27.17 for\nzh -> en on the WMT17 dataset. SimCut is not a new method, but a version of\nCutoff (Shen et al., 2020) simplified and adapted for NMT, and it could be\nconsidered as a perturbation-based method. Given the universality and\nsimplicity of SimCut and Bi-SimCut, we believe they can serve as strong\nbaselines for future NMT research.", "published": "2022-06-06 05:49:00", "link": "http://arxiv.org/abs/2206.02368v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A sentiment analysis model for car review texts based on adversarial\n  training and whole word mask BERT", "abstract": "In the field of car evaluation, more and more netizens choose to express\ntheir opinions on the Internet platform, and these comments will affect the\ndecision-making of buyers and the trend of car word-of-mouth. As an important\nbranch of natural language processing (NLP), sentiment analysis provides an\neffective research method for analyzing the sentiment types of massive car\nreview texts. However, due to the lexical professionalism and large text noise\nof review texts in the automotive field, when a general sentiment analysis\nmodel is applied to car reviews, the accuracy of the model will be poor. To\novercome these above challenges, we aim at the sentiment analysis task of car\nreview texts. From the perspective of word vectors, pre-training is carried out\nby means of whole word mask of proprietary vocabulary in the automotive field,\nand then training data is carried out through the strategy of an adversarial\ntraining set. Based on this, we propose a car review text sentiment analysis\nmodel based on adversarial training and whole word mask BERT(ATWWM-BERT).", "published": "2022-06-06 06:45:43", "link": "http://arxiv.org/abs/2206.02389v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Improving Contrastive Learning of Sentence Embeddings with\n  Case-Augmented Positives and Retrieved Negatives", "abstract": "Following SimCSE, contrastive learning based methods have achieved the\nstate-of-the-art (SOTA) performance in learning sentence embeddings. However,\nthe unsupervised contrastive learning methods still lag far behind the\nsupervised counterparts. We attribute this to the quality of positive and\nnegative samples, and aim to improve both. Specifically, for positive samples,\nwe propose switch-case augmentation to flip the case of the first letter of\nrandomly selected words in a sentence. This is to counteract the intrinsic bias\nof pre-trained token embeddings to frequency, word cases and subwords. For\nnegative samples, we sample hard negatives from the whole dataset based on a\npre-trained language model. Combining the above two methods with SimCSE, our\nproposed Contrastive learning with Augmented and Retrieved Data for Sentence\nembedding (CARDS) method significantly surpasses the current SOTA on STS\nbenchmarks in the unsupervised setting.", "published": "2022-06-06 09:46:12", "link": "http://arxiv.org/abs/2206.02457v1", "categories": ["cs.CL", "cs.IR", "H.3.3"], "primary_category": "cs.CL"}
{"title": "Norm Participation Grounds Language", "abstract": "The striking recent advances in eliciting seemingly meaningful language\nbehaviour from language-only machine learning models have only made more\napparent, through the surfacing of clear limitations, the need to go beyond the\nlanguage-only mode and to ground these models \"in the world\". Proposals for\ndoing so vary in the details, but what unites them is that the solution is\nsought in the addition of non-linguistic data types such as images or video\nstreams, while largely keeping the mode of learning constant. I propose a\ndifferent, and more wide-ranging conception of how grounding should be\nunderstood: What grounds language is its normative nature. There are standards\nfor doing things right, these standards are public and authoritative, while at\nthe same time acceptance of authority can and must be disputed and negotiated,\nin interactions in which only bearers of normative status can rightfully\nparticipate. What grounds language, then, is the determined use that language\nusers make of it, and what it is grounded in is the community of language\nusers. I sketch this idea, and draw some conclusions for work on computational\nmodelling of meaningful language use.", "published": "2022-06-06 20:21:59", "link": "http://arxiv.org/abs/2206.02885v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Online Neural Diarization of Unlimited Numbers of Speakers Using Global\n  and Local Attractors", "abstract": "A method to perform offline and online speaker diarization for an unlimited\nnumber of speakers is described in this paper. End-to-end neural diarization\n(EEND) has achieved overlap-aware speaker diarization by formulating it as a\nmulti-label classification problem. It has also been extended for a flexible\nnumber of speakers by introducing speaker-wise attractors. However, the output\nnumber of speakers of attractor-based EEND is empirically capped; it cannot\ndeal with cases where the number of speakers appearing during inference is\nhigher than that during training because its speaker counting is trained in a\nfully supervised manner. Our method, EEND-GLA, solves this problem by\nintroducing unsupervised clustering into attractor-based EEND. In the method,\nthe input audio is first divided into short blocks, then attractor-based\ndiarization is performed for each block, and finally, the results of each block\nare clustered on the basis of the similarity between locally-calculated\nattractors. While the number of output speakers is limited within each block,\nthe total number of speakers estimated for the entire input can be higher than\nthe limitation. To use EEND-GLA in an online manner, our method also extends\nthe speaker-tracing buffer, which was originally proposed to enable online\ninference of conventional EEND. We introduce a block-wise buffer update to make\nthe speaker-tracing buffer compatible with EEND-GLA. Finally, to improve online\ndiarization, our method improves the buffer update method and revisits the\nvariable chunk-size training of EEND. The experimental results demonstrate that\nEEND-GLA can perform speaker diarization of an unseen number of speakers in\nboth offline and online inferences.", "published": "2022-06-06 08:48:26", "link": "http://arxiv.org/abs/2206.02432v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Automatically Drafting Ontologies from Competency Questions with FrODO", "abstract": "We present the Frame-based ontology Design Outlet (FrODO), a novel method and\ntool for drafting ontologies from competency questions automatically.\nCompetency questions are expressed as natural language and are a common\nsolution for representing requirements in a number of agile ontology\nengineering methodologies, such as the eXtreme Design (XD) or SAMOD. FrODO\nbuilds on top of FRED. In fact, it leverages the frame semantics for drawing\ndomain-relevant boundaries around the RDF produced by FRED from a competency\nquestion, thus drafting domain ontologies. We carried out a user-based study\nfor assessing FrODO in supporting engineers for ontology design tasks. The\nstudy shows that FrODO is effective in this and the resulting ontology drafts\nare qualitative.", "published": "2022-06-06 10:39:16", "link": "http://arxiv.org/abs/2206.02485v2", "categories": ["cs.AI", "cs.CL", "cs.DB"], "primary_category": "cs.AI"}
{"title": "A Bird's-Eye Tutorial of Graph Attention Architectures", "abstract": "Graph Neural Networks (GNNs) have shown tremendous strides in performance for\ngraph-structured problems especially in the domains of natural language\nprocessing, computer vision and recommender systems. Inspired by the success of\nthe transformer architecture, there has been an ever-growing body of work on\nattention variants of GNNs attempting to advance the state of the art in many\nof these problems. Incorporating \"attention\" into graph mining has been viewed\nas a way to overcome the noisiness, heterogenity and complexity associated with\ngraph-structured data as well as to encode soft-inductive bias. It is hence\ncrucial and advantageous to study these variants from a bird's-eye view to\nassess their strengths and weaknesses. We provide a systematic and focused\ntutorial centered around attention based GNNs in a hope to benefit researchers\ndealing with graph-structured problems. Our tutorial looks at GNN variants from\nthe point of view of the attention function and iteratively builds the reader's\nunderstanding of different graph attention variants.", "published": "2022-06-06 18:47:48", "link": "http://arxiv.org/abs/2206.02849v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "No Parameter Left Behind: How Distillation and Model Size Affect\n  Zero-Shot Retrieval", "abstract": "Recent work has shown that small distilled language models are strong\ncompetitors to models that are orders of magnitude larger and slower in a wide\nrange of information retrieval tasks. This has made distilled and dense models,\ndue to latency constraints, the go-to choice for deployment in real-world\nretrieval applications. In this work, we question this practice by showing that\nthe number of parameters and early query-document interaction play a\nsignificant role in the generalization ability of retrieval models. Our\nexperiments show that increasing model size results in marginal gains on\nin-domain test sets, but much larger gains in new domains never seen during\nfine-tuning. Furthermore, we show that rerankers largely outperform dense ones\nof similar size in several tasks. Our largest reranker reaches the state of the\nart in 12 of the 18 datasets of the Benchmark-IR (BEIR) and surpasses the\nprevious state of the art by 3 average points. Finally, we confirm that\nin-domain effectiveness is not a good indicator of zero-shot effectiveness.\nCode is available at\nhttps://github.com/guilhermemr04/scaling-zero-shot-retrieval.git", "published": "2022-06-06 19:56:14", "link": "http://arxiv.org/abs/2206.02873v5", "categories": ["cs.IR", "cs.CL", "cs.PF"], "primary_category": "cs.IR"}
{"title": "Schema-Guided Event Graph Completion", "abstract": "We tackle a new task, event graph completion, which aims to predict missing\nevent nodes for event graphs. Existing link prediction or graph completion\nmethods have difficulty dealing with event graphs because they are usually\ndesigned for a single large graph such as a social network or a knowledge\ngraph, rather than multiple small dynamic event graphs. Moreover, they can only\npredict missing edges rather than missing nodes. In this work, we propose to\nutilize event schema, a template that describes the stereotypical structure of\nevent graphs, to address the above issues. Our schema-guided event graph\ncompletion approach first maps an instance event graph to a subgraph of the\nschema graph by a heuristic subgraph matching algorithm. Then it predicts\nwhether a candidate event node in the schema graph should be added to the\ninstantiated schema subgraph by characterizing two types of local topology of\nthe schema graph: neighbors of the candidate node and the subgraph, and paths\nthat connect the candidate node and the subgraph. These two modules are later\ncombined together for the final prediction. We also propose a self-supervised\nstrategy to construct training samples, as well as an inference algorithm that\nis specifically designed to complete event graphs. Extensive experimental\nresults on four datasets demonstrate that our proposed method achieves\nstate-of-the-art performance, with 4.3% to 19.4% absolute F1 gains over the\nbest baseline method on the four datasets.", "published": "2022-06-06 21:51:10", "link": "http://arxiv.org/abs/2206.02921v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Neuro-Symbolic Procedural Planning with Commonsense Prompting", "abstract": "Procedural planning aims to implement complex high-level goals by\ndecomposition into sequential simpler low-level steps. Although procedural\nplanning is a basic skill set for humans in daily life, it remains a challenge\nfor large language models (LLMs) that lack a deep understanding of the\ncause-effect relations in procedures. Previous methods require manual exemplars\nto acquire procedural planning knowledge from LLMs in the zero-shot setting.\nHowever, such elicited pre-trained knowledge in LLMs induces spurious\ncorrelations between goals and steps, which impair the model generalization to\nunseen tasks. In contrast, this paper proposes a neuro-symbolic procedural\nPLANner (PLAN) that elicits procedural planning knowledge from the LLMs with\ncommonsense-infused prompting. To mitigate spurious goal-step correlations, we\nuse symbolic program executors on the latent procedural representations to\nformalize prompts from commonsense knowledge bases as a causal intervention\ntoward the Structural Causal Model. Both automatic and human evaluations on\nWikiHow and RobotHow show the superiority of PLAN on procedural planning\nwithout further training or manual exemplars.", "published": "2022-06-06 22:09:52", "link": "http://arxiv.org/abs/2206.02928v6", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised TTS Acoustic Modeling for TTS with Conditional Disentangled\n  Sequential VAE", "abstract": "In this paper, we propose a novel unsupervised text-to-speech acoustic model\ntraining scheme, named UTTS, which does not require text-audio pairs. UTTS is a\nmulti-speaker speech synthesizer that supports zero-shot voice cloning, it is\ndeveloped from a perspective of disentangled speech representation learning.\nThe framework offers a flexible choice of a speaker's duration model, timbre\nfeature (identity) and content for TTS inference. We leverage recent\nadvancements in self-supervised speech representation learning as well as\nspeech synthesis front-end techniques for system development. Specifically, we\nemploy our recently formulated Conditional Disentangled Sequential Variational\nAuto-encoder (C-DSVAE) as the backbone UTTS AM, which offers well-structured\ncontent representations given unsupervised alignment (UA) as condition during\ntraining. For UTTS inference, we utilize a lexicon to map input text to the\nphoneme sequence, which is expanded to the frame-level forced alignment (FA)\nwith a speaker-dependent duration model. Then, we develop an alignment mapping\nmodule that converts FA to UA. Finally, the C-DSVAE, serving as the\nself-supervised TTS AM, takes the predicted UA and a target speaker embedding\nto generate the mel spectrogram, which is ultimately converted to waveform with\na neural vocoder. We show how our method enables speech synthesis without using\na paired TTS corpus in AM development stage. Experiments demonstrate that UTTS\ncan synthesize speech of high naturalness and intelligibility measured by human\nand objective evaluations. Audio samples are available at our demo page\nhttps://neurtts.github.io/utts\\_demo/.", "published": "2022-06-06 11:51:22", "link": "http://arxiv.org/abs/2206.02512v4", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FedNST: Federated Noisy Student Training for Automatic Speech\n  Recognition", "abstract": "Federated Learning (FL) enables training state-of-the-art Automatic Speech\nRecognition (ASR) models on user devices (clients) in distributed systems,\nhence preventing transmission of raw user data to a central server. A key\nchallenge facing practical adoption of FL for ASR is obtaining ground-truth\nlabels on the clients. Existing approaches rely on clients to manually\ntranscribe their speech, which is impractical for obtaining large training\ncorpora. A promising alternative is using semi-/self-supervised learning\napproaches to leverage unlabelled user data. To this end, we propose FedNST, a\nnovel method for training distributed ASR models using private and unlabelled\nuser data. We explore various facets of FedNST, such as training models with\ndifferent proportions of labelled and unlabelled data, and evaluate the\nproposed approach on 1173 simulated clients. Evaluating FedNST on LibriSpeech,\nwhere 960 hours of speech data is split equally into server (labelled) and\nclient (unlabelled) data, showed a 22.5% relative word error rate reduction}\n(WERR) over a supervised baseline trained only on server data.", "published": "2022-06-06 16:18:45", "link": "http://arxiv.org/abs/2206.02797v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.CV", "cs.DC", "cs.LG", "I.2.11"], "primary_category": "eess.AS"}
{"title": "Continuous-Time Analog Filters for Audio Edge Intelligence: Review on\n  Circuit Designs", "abstract": "Edge audio devices can reduce data bandwidth requirements by pre-processing\ninput speech on the device before transmission to the cloud. As edge devices\nare required to ensure always-on operation, their stringent power constraints\npose several design challenges and force IC designers to look for solutions\nthat use low standby power. One promising bio-inspired approach is to combine\nthe continuous-time analog filter channels with a small memory footprint deep\nneural network that is trained on edge tasks such as keyword spotting, thereby\nallowing all blocks to be embedded in an IC. This paper reviews the historical\nbackground of the continuous-time analog filter circuits that have been used as\nfeature extractors for current edge audio devices. Starting from the\ninterpretation of a basic biquad filter as a two-integrator-loop topology, we\nintroduce the progression in the design of second-order low-pass and band-pass\nfilters ranging from OTA-based to source-follower-based architectures. We also\nderive and analyze the small-signal transfer function and discuss their usage\nin edge audio applications.", "published": "2022-06-06 14:22:58", "link": "http://arxiv.org/abs/2206.02639v2", "categories": ["eess.AS", "cs.AR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Canonical Cortical Graph Neural Networks and its Application for Speech\n  Enhancement in Audio-Visual Hearing Aids", "abstract": "Despite the recent success of machine learning algorithms, most models face\ndrawbacks when considering more complex tasks requiring interaction between\ndifferent sources, such as multimodal input data and logical time sequences. On\nthe other hand, the biological brain is highly sharpened in this sense,\nempowered to automatically manage and integrate such streams of information. In\nthis context, this work draws inspiration from recent discoveries in brain\ncortical circuits to propose a more biologically plausible self-supervised\nmachine learning approach. This combines multimodal information using\nintra-layer modulations together with Canonical Correlation Analysis, and a\nmemory mechanism to keep track of temporal data, the overall approach termed\nCanonical Cortical Graph Neural networks. This is shown to outperform recent\nstate-of-the-art models in terms of clean audio reconstruction and energy\nefficiency for a benchmark audio-visual speech dataset. The enhanced\nperformance is demonstrated through a reduced and smother neuron firing rate\ndistribution. suggesting that the proposed model is amenable for speech\nenhancement in future audio-visual hearing aid devices.", "published": "2022-06-06 15:20:07", "link": "http://arxiv.org/abs/2206.02671v3", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
