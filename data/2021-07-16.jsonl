{"title": "Are Multilingual Models the Best Choice for Moderately Under-resourced\n  Languages? A Comprehensive Assessment for Catalan", "abstract": "Multilingual language models have been a crucial breakthrough as they\nconsiderably reduce the need of data for under-resourced languages.\nNevertheless, the superiority of language-specific models has already been\nproven for languages having access to large amounts of data. In this work, we\nfocus on Catalan with the aim to explore to what extent a medium-sized\nmonolingual language model is competitive with state-of-the-art large\nmultilingual models. For this, we: (1) build a clean, high-quality textual\nCatalan corpus (CaText), the largest to date (but only a fraction of the usual\nsize of the previous work in monolingual language models), (2) train a\nTransformer-based language model for Catalan (BERTa), and (3) devise a thorough\nevaluation in a diversity of settings, comprising a complete array of\ndownstream tasks, namely, Part of Speech Tagging, Named Entity Recognition and\nClassification, Text Classification, Question Answering, and Semantic Textual\nSimilarity, with most of the corresponding datasets being created ex novo. The\nresult is a new benchmark, the Catalan Language Understanding Benchmark (CLUB),\nwhich we publish as an open resource, together with the clean textual corpus,\nthe language model, and the cleaning pipeline. Using state-of-the-art\nmultilingual models and a monolingual model trained only on Wikipedia as\nbaselines, we consistently observe the superiority of our model across tasks\nand settings.", "published": "2021-07-16 13:52:01", "link": "http://arxiv.org/abs/2107.07903v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Rich Syntax for Better Knowledge Base Question Answering", "abstract": "Recent studies on Knowledge Base Question Answering (KBQA) have shown great\nprogress on this task via better question understanding. Previous works for\nencoding questions mainly focus on the word sequences, but seldom consider the\ninformation from syntactic trees.In this paper, we propose an approach to learn\nsyntax-based representations for KBQA. First, we encode path-based syntax by\nconsidering the shortest dependency paths between keywords. Then, we propose\ntwo encoding strategies to mode the information of whole syntactic trees to\nobtain tree-based syntax. Finally, we combine both path-based and tree-based\nsyntax representations for KBQA. We conduct extensive experiments on a widely\nused benchmark dataset and the experimental results show that our syntax-aware\nsystems can make full use of syntax information in different settings and\nachieve state-of-the-art performance of KBQA.", "published": "2021-07-16 14:59:05", "link": "http://arxiv.org/abs/2107.07940v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Vulnerable Are Automatic Fake News Detection Methods to Adversarial\n  Attacks?", "abstract": "As the spread of false information on the internet has increased dramatically\nin recent years, more and more attention is being paid to automated fake news\ndetection. Some fake news detection methods are already quite successful.\nNevertheless, there are still many vulnerabilities in the detection algorithms.\nThe reason for this is that fake news publishers can structure and formulate\ntheir texts in such a way that a detection algorithm does not expose this text\nas fake news. This paper shows that it is possible to automatically attack\nstate-of-the-art models that have been trained to detect Fake News, making\nthese vulnerable. For this purpose, corresponding models were first trained\nbased on a dataset. Then, using Text-Attack, an attempt was made to manipulate\nthe trained models in such a way that previously correctly identified fake news\nwas classified as true news. The results show that it is possible to\nautomatically bypass Fake News detection mechanisms, leading to implications\nconcerning existing policy initiatives.", "published": "2021-07-16 15:36:03", "link": "http://arxiv.org/abs/2107.07970v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Law of Large Documents: Understanding the Structure of Legal\n  Contracts Using Visual Cues", "abstract": "Large, pre-trained transformer models like BERT have achieved\nstate-of-the-art results on document understanding tasks, but most\nimplementations can only consider 512 tokens at a time. For many real-world\napplications, documents can be much longer, and the segmentation strategies\ntypically used on longer documents miss out on document structure and\ncontextual information, hurting their results on downstream tasks. In our work\non legal agreements, we find that visual cues such as layout, style, and\nplacement of text in a document are strong features that are crucial to\nachieving an acceptable level of accuracy on long documents. We measure the\nimpact of incorporating such visual cues, obtained via computer vision methods,\non the accuracy of document understanding tasks including document\nsegmentation, entity extraction, and attribute classification. Our method of\nsegmenting documents based on structural metadata out-performs existing methods\non four long-document understanding tasks as measured on the Contract\nUnderstanding Atticus Dataset.", "published": "2021-07-16 21:21:50", "link": "http://arxiv.org/abs/2107.08128v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Picard understanding Darmok: A Dataset and Model for Metaphor-Rich\n  Translation in a Constructed Language", "abstract": "Tamarian, a fictional language introduced in the Star Trek episode Darmok,\ncommunicates meaning through utterances of metaphorical references, such as\n\"Darmok and Jalad at Tanagra\" instead of \"We should work together.\" This work\nassembles a Tamarian-English dictionary of utterances from the original episode\nand several follow-on novels, and uses this to construct a parallel corpus of\n456 English-Tamarian utterances. A machine translation system based on a large\nlanguage model (T5) is trained using this parallel corpus, and is shown to\nproduce an accuracy of 76% when translating from English to Tamarian on known\nutterances.", "published": "2021-07-16 23:35:45", "link": "http://arxiv.org/abs/2107.08146v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TAPEX: Table Pre-training via Learning a Neural SQL Executor", "abstract": "Recent progress in language model pre-training has achieved a great success\nvia leveraging large-scale unstructured textual data. However, it is still a\nchallenge to apply pre-training on structured tabular data due to the absence\nof large-scale high-quality tabular data. In this paper, we propose TAPEX to\nshow that table pre-training can be achieved by learning a neural SQL executor\nover a synthetic corpus, which is obtained by automatically synthesizing\nexecutable SQL queries and their execution outputs. TAPEX addresses the data\nscarcity challenge via guiding the language model to mimic a SQL executor on\nthe diverse, large-scale and high-quality synthetic corpus. We evaluate TAPEX\non four benchmark datasets. Experimental results demonstrate that TAPEX\noutperforms previous table pre-training approaches by a large margin and\nachieves new state-of-the-art results on all of them. This includes the\nimprovements on the weakly-supervised WikiSQL denotation accuracy to 89.5%\n(+2.3%), the WikiTableQuestions denotation accuracy to 57.5% (+4.8%), the SQA\ndenotation accuracy to 74.5% (+3.5%), and the TabFact accuracy to 84.2%\n(+3.2%). To our knowledge, this is the first work to exploit table pre-training\nvia synthetic executable programs and to achieve new state-of-the-art results\non various downstream tasks. Our code can be found at\nhttps://github.com/microsoft/Table-Pretraining.", "published": "2021-07-16 00:40:11", "link": "http://arxiv.org/abs/2107.07653v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Intersectional Bias in Causal Language Models", "abstract": "To examine whether intersectional bias can be observed in language\ngeneration, we examine \\emph{GPT-2} and \\emph{GPT-NEO} models, ranging in size\nfrom 124 million to ~2.7 billion parameters. We conduct an experiment combining\nup to three social categories - gender, religion and disability - into\nunconditional or zero-shot prompts used to generate sentences that are then\nanalysed for sentiment. Our results confirm earlier tests conducted with\nauto-regressive causal models, including the \\emph{GPT} family of models. We\nalso illustrate why bias may be resistant to techniques that target single\ncategories (e.g. gender, religion and race), as it can also manifest, in often\nsubtle ways, in texts prompted by concatenated social categories. To address\nthese difficulties, we suggest technical and community-based approaches need to\ncombine to acknowledge and address complex and intersectional language model\nbias.", "published": "2021-07-16 03:46:08", "link": "http://arxiv.org/abs/2107.07691v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Know Deeper: Knowledge-Conversation Cyclic Utilization Mechanism for\n  Open-domain Dialogue Generation", "abstract": "End-to-End intelligent neural dialogue systems suffer from the problems of\ngenerating inconsistent and repetitive responses. Existing dialogue models pay\nattention to unilaterally incorporating personal knowledge into the dialog\nwhile ignoring the fact that incorporating the personality-related conversation\ninformation into personal knowledge taken as the bilateral information flow\nboosts the quality of the subsequent conversation. Besides, it is indispensable\nto control personal knowledge utilization over the conversation level. In this\npaper, we propose a conversation-adaption multi-view persona aware response\ngeneration model that aims at enhancing conversation consistency and\nalleviating the repetition from two folds. First, we consider conversation\nconsistency from multiple views. From the view of the persona profile, we\ndesign a novel interaction module that not only iteratively incorporates\npersonalized knowledge into each turn conversation but also captures the\npersonality-related information from conversation to enhance personalized\nknowledge semantic representation. From the view of speaking style, we\nintroduce the speaking style vector and feed it into the decoder to keep the\nspeaking style consistency. To avoid conversation repetition, we devise a\ncoverage mechanism to keep track of the activation of personal knowledge\nutilization. Experiments on both automatic and human evaluation verify the\nsuperiority of our model over previous models.", "published": "2021-07-16 08:59:06", "link": "http://arxiv.org/abs/2107.07771v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "POS tagging, lemmatization and dependency parsing of West Frisian", "abstract": "We present a lemmatizer/POS-tagger/dependency parser for West Frisian using a\ncorpus of 44,714 words in 3,126 sentences that were annotated according to the\nguidelines of Universal Dependency version 2. POS tags were assigned to words\nby using a Dutch POS tagger that was applied to a literal word-by-word\ntranslation, or to sentences of a Dutch parallel text. Best results were\nobtained when using literal translations that were created by using the Frisian\ntranslation program Oersetter. Morphologic and syntactic annotations were\ngenerated on the basis of a literal Dutch translation as well. The performance\nof the lemmatizer/tagger/annotator when it was trained using default parameters\nwas compared to the performance that was obtained when using the parameter\nvalues that were used for training the LassySmall UD 2.5 corpus. A significant\nimprovement was found for `lemma'. The Frisian lemmatizer/PoS tagger/dependency\nparser is released as a web app and as a web service.", "published": "2021-07-16 15:41:37", "link": "http://arxiv.org/abs/2107.07974v2", "categories": ["cs.CL", "stat.ML", "68U15", "J.5"], "primary_category": "cs.CL"}
{"title": "Architectures of Meaning, A Systematic Corpus Analysis of NLP Systems", "abstract": "This paper proposes a novel statistical corpus analysis framework targeted\ntowards the interpretation of Natural Language Processing (NLP) architectural\npatterns at scale. The proposed approach combines saturation-based lexicon\nconstruction, statistical corpus analysis methods and graph collocations to\ninduce a synthesis representation of NLP architectural patterns from corpora.\nThe framework is validated in the full corpus of Semeval tasks and demonstrated\ncoherent architectural patterns which can be used to answer architectural\nquestions on a data-driven fashion, providing a systematic mechanism to\ninterpret a largely dynamic and exponentially growing field.", "published": "2021-07-16 21:10:43", "link": "http://arxiv.org/abs/2107.08124v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Urdu & Hindi Poetry Generation using Neural Networks", "abstract": "One of the major problems writers and poets face is the writer's block. It is\na condition in which an author loses the ability to produce new work or\nexperiences a creative slowdown. The problem is more difficult in the context\nof poetry than prose, as in the latter case authors need not be very concise\nwhile expressing their ideas, also the various aspects such as rhyme, poetic\nmeters are not relevant for prose. One of the most effective ways to overcome\nthis writing block for poets can be, to have a prompt system, which would help\ntheir imagination and open their minds for new ideas. A prompt system can\npossibly generate one liner, two liner or full ghazals. The purpose of this\nwork is to give an ode to the Urdu, Hindi poets, and helping them start their\nnext line of poetry, a couplet or a complete ghazal considering various factors\nlike rhymes, refrain, and meters. The result will help aspiring poets to get\nnew ideas and help them overcome writer's block by auto-generating pieces of\npoetry using Deep Learning techniques. A concern with creative works like this,\nespecially in the literary context, is to ensure that the output is not\nplagiarized. This work also addresses the concern and makes sure that the\nresulting odes are not exact match with input data using parameters like\ntemperature and manual plagiarism check against input corpus. To the best of\nour knowledge, although the automatic text generation problem has been studied\nquite extensively in the literature, the specific problem of Urdu, Hindi poetry\ngeneration has not been explored much. Apart from developing system to\nauto-generate Urdu, Hindi poetry, another key contribution of our work is to\ncreate a cleaned and preprocessed corpus of Urdu, Hindi poetry (derived from\nauthentic resources) and making it freely available for researchers in the\narea.", "published": "2021-07-16 16:12:51", "link": "http://arxiv.org/abs/2107.14587v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Application of Active Query K-Means in Text Classification", "abstract": "Active learning is a state-of-art machine learning approach to deal with an\nabundance of unlabeled data. In the field of Natural Language Processing,\ntypically it is costly and time-consuming to have all the data annotated. This\ninefficiency inspires out our application of active learning in text\nclassification. Traditional unsupervised k-means clustering is first modified\ninto a semi-supervised version in this research. Then, a novel attempt is\napplied to further extend the algorithm into active learning scenario with\nPenalized Min-Max-selection, so as to make limited queries that yield more\nstable initial centroids. This method utilizes both the interactive query\nresults from users and the underlying distance representation. After tested on\na Chinese news dataset, it shows a consistent increase in accuracy while\nlowering the cost in training.", "published": "2021-07-16 03:06:35", "link": "http://arxiv.org/abs/2107.07682v1", "categories": ["cs.CL", "cs.LG", "math.ST", "stat.TH"], "primary_category": "cs.CL"}
{"title": "Pseudo-labelling Enhanced Media Bias Detection", "abstract": "Leveraging unlabelled data through weak or distant supervision is a\ncompelling approach to developing more effective text classification models.\nThis paper proposes a simple but effective data augmentation method, which\nleverages the idea of pseudo-labelling to select samples from noisy distant\nsupervision annotation datasets. The result shows that the proposed method\nimproves the accuracy of biased news detection models.", "published": "2021-07-16 04:47:50", "link": "http://arxiv.org/abs/2107.07705v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Comparison of Methods for OOV-word Recognition on a New Public Dataset", "abstract": "A common problem for automatic speech recognition systems is how to recognize\nwords that they did not see during training. Currently there is no established\nmethod of evaluating different techniques for tackling this problem. We propose\nusing the CommonVoice dataset to create test sets for multiple languages which\nhave a high out-of-vocabulary (OOV) ratio relative to a training set and\nrelease a new tool for calculating relevant performance metrics. We then\nevaluate, within the context of a hybrid ASR system, how much better subword\nmodels are at recognizing OOVs, and how much benefit one can get from\nincorporating OOV-word information into an existing system by modifying WFSTs.\nAdditionally, we propose a new method for modifying a subword-based language\nmodel so as to better recognize OOV-words. We showcase very large improvements\nin OOV-word recognition and make both the data and code available.", "published": "2021-07-16 19:39:30", "link": "http://arxiv.org/abs/2107.08091v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Recognizing bird species in diverse soundscapes under weak supervision", "abstract": "We present a robust classification approach for avian vocalization in complex\nand diverse soundscapes, achieving second place in the BirdCLEF2021 challenge.\nWe illustrate how to make full use of pre-trained convolutional neural\nnetworks, by using an efficient modeling and training routine supplemented by\nnovel augmentation methods. Thereby, we improve the generalization of weakly\nlabeled crowd-sourced data to productive data collected by autonomous recording\nunits. As such, we illustrate how to progress towards an accurate automated\nassessment of avian population which would enable global biodiversity\nmonitoring at scale, impossible by manual annotation.", "published": "2021-07-16 06:54:38", "link": "http://arxiv.org/abs/2107.07728v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Continual Learning for Automated Audio Captioning Using The Learning\n  Without Forgetting Approach", "abstract": "Automated audio captioning (AAC) is the task of automatically creating\ntextual descriptions (i.e. captions) for the contents of a general audio\nsignal. Most AAC methods are using existing datasets to optimize and/or\nevaluate upon. Given the limited information held by the AAC datasets, it is\nvery likely that AAC methods learn only the information contained in the\nutilized datasets. In this paper we present a first approach for continuously\nadapting an AAC method to new information, using a continual learning method.\nIn our scenario, a pre-optimized AAC method is used for some unseen general\naudio signals and can update its parameters in order to adapt to the new\ninformation, given a new reference caption. We evaluate our method using a\nfreely available, pre-optimized AAC method and two freely available AAC\ndatasets. We compare our proposed method with three scenarios, two of training\non one of the datasets and evaluating on the other and a third of training on\none dataset and fine-tuning on the other. Obtained results show that our method\nachieves a good balance between distilling new knowledge and not forgetting the\nprevious one.", "published": "2021-07-16 17:44:26", "link": "http://arxiv.org/abs/2107.08028v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Controlled AutoEncoders to Generate Faces from Voices", "abstract": "Multiple studies in the past have shown that there is a strong correlation\nbetween human vocal characteristics and facial features. However, existing\napproaches generate faces simply from voice, without exploring the set of\nfeatures that contribute to these observed correlations. A computational\nmethodology to explore this can be devised by rephrasing the question to: \"how\nmuch would a target face have to change in order to be perceived as the\noriginator of a source voice?\" With this in perspective, we propose a framework\nto morph a target face in response to a given voice in a way that facial\nfeatures are implicitly guided by learned voice-face correlation in this paper.\nOur framework includes a guided autoencoder that converts one face to another,\ncontrolled by a unique model-conditioning component called a gating controller\nwhich modifies the reconstructed face based on input voice recordings. We\nevaluate the framework on VoxCelab and VGGFace datasets through human subjects\nand face retrieval. Various experiments demonstrate the effectiveness of our\nproposed model.", "published": "2021-07-16 16:04:29", "link": "http://arxiv.org/abs/2107.07988v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
