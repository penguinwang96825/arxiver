{"title": "Bilevel Scheduled Sampling for Dialogue Generation", "abstract": "Exposure bias poses a common challenge in numerous natural language\nprocessing tasks, particularly in the dialog generation. In response to this\nissue, researchers have devised various techniques, among which scheduled\nsampling has proven to be an effective method for mitigating exposure bias.\nHowever, the existing state-of-the-art scheduled sampling methods solely\nconsider the current sampling words' quality for threshold truncation sampling,\nwhich overlooks the importance of sentence-level information and the method of\nthreshold truncation warrants further discussion. In this paper, we propose a\nbilevel scheduled sampling model that takes the sentence-level information into\naccount and incorporates it with word-level quality. To enhance sampling\ndiversity and improve the model's adaptability, we propose a smooth function\nthat maps the combined result of sentence-level and word-level information to\nan appropriate range, and employ probabilistic sampling based on the mapped\nvalues instead of threshold truncation. Experiments conducted on the\nDailyDialog and PersonaChat datasets demonstrate the effectiveness of our\nproposed methods, which significantly alleviate the exposure bias problem and\noutperform state-of-the-art scheduled sampling methods.", "published": "2023-09-05 05:05:06", "link": "http://arxiv.org/abs/2309.01953v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Automatic Evaluation Framework for Multi-turn Medical Consultations\n  Capabilities of Large Language Models", "abstract": "Large language models (LLMs) have achieved significant success in interacting\nwith human. However, recent studies have revealed that these models often\nsuffer from hallucinations, leading to overly confident but incorrect\njudgments. This limits their application in the medical domain, where tasks\nrequire the utmost accuracy. This paper introduces an automated evaluation\nframework that assesses the practical capabilities of LLMs as virtual doctors\nduring multi-turn consultations. Consultation tasks are designed to require\nLLMs to be aware of what they do not know, to inquire about missing medical\ninformation from patients, and to ultimately make diagnoses. To evaluate the\nperformance of LLMs for these tasks, a benchmark is proposed by reformulating\nmedical multiple-choice questions from the United States Medical Licensing\nExaminations (USMLE), and comprehensive evaluation metrics are developed and\nevaluated on three constructed test sets. A medical consultation training set\nis further constructed to improve the consultation ability of LLMs. The results\nof the experiments show that fine-tuning with the training set can alleviate\nhallucinations and improve LLMs' performance on the proposed benchmark.\nExtensive experiments and ablation studies are conducted to validate the\neffectiveness and robustness of the proposed framework.", "published": "2023-09-05 09:24:48", "link": "http://arxiv.org/abs/2309.02077v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Where are We in Event-centric Emotion Analysis? Bridging Emotion Role\n  Labeling and Appraisal-based Approaches", "abstract": "The term emotion analysis in text subsumes various natural language\nprocessing tasks which have in common the goal to enable computers to\nunderstand emotions. Most popular is emotion classification in which one or\nmultiple emotions are assigned to a predefined textual unit. While such setting\nis appropriate for identifying the reader's or author's emotion, emotion role\nlabeling adds the perspective of mentioned entities and extracts text spans\nthat correspond to the emotion cause. The underlying emotion theories agree on\none important point; that an emotion is caused by some internal or external\nevent and comprises several subcomponents, including the subjective feeling and\na cognitive evaluation. We therefore argue that emotions and events are related\nin two ways. (1) Emotions are events; and this perspective is the fundament in\nnatural language processing for emotion role labeling. (2) Emotions are caused\nby events; a perspective that is made explicit with research how to incorporate\npsychological appraisal theories in NLP models to interpret events. These two\nresearch directions, role labeling and (event-focused) emotion classification,\nhave by and large been tackled separately. In this paper, we contextualize both\nperspectives and discuss open research questions.", "published": "2023-09-05 09:56:29", "link": "http://arxiv.org/abs/2309.02092v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advancing Text-to-GLOSS Neural Translation Using a Novel Hyper-parameter\n  Optimization Technique", "abstract": "In this paper, we investigate the use of transformers for Neural Machine\nTranslation of text-to-GLOSS for Deaf and Hard-of-Hearing communication. Due to\nthe scarcity of available data and limited resources for text-to-GLOSS\ntranslation, we treat the problem as a low-resource language task. We use our\nnovel hyper-parameter exploration technique to explore a variety of\narchitectural parameters and build an optimal transformer-based architecture\nspecifically tailored for text-to-GLOSS translation. The study aims to improve\nthe accuracy and fluency of Neural Machine Translation generated GLOSS. This is\nachieved by examining various architectural parameters including layer count,\nattention heads, embedding dimension, dropout, and label smoothing to identify\nthe optimal architecture for improving text-to-GLOSS translation performance.\nThe experiments conducted on the PHOENIX14T dataset reveal that the optimal\ntransformer architecture outperforms previous work on the same dataset. The\nbest model reaches a ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\nscore of 55.18% and a BLEU-1 (BiLingual Evaluation Understudy 1) score of\n63.6%, outperforming state-of-the-art results on the BLEU1 and ROUGE score by\n8.42 and 0.63 respectively.", "published": "2023-09-05 11:59:31", "link": "http://arxiv.org/abs/2309.02162v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Dialog Action-Aware Transformer for Dialog Policy Learning", "abstract": "Recent works usually address Dialog policy learning DPL by training a\nreinforcement learning (RL) agent to determine the best dialog action. However,\nexisting works on deep RL require a large volume of agent-user interactions to\nachieve acceptable performance. In this paper, we propose to make full use of\nthe plain text knowledge from the pre-trained language model to accelerate the\nRL agent's learning speed. Specifically, we design a dialog action-aware\ntransformer encoder (DaTrans), which integrates a new fine-tuning procedure\nnamed masked last action task to encourage DaTrans to be dialog-aware and\ndistils action-specific features. Then, DaTrans is further optimized in an RL\nsetting with ongoing interactions and evolves through exploration in the dialog\naction space toward maximizing long-term accumulated rewards. The effectiveness\nand efficiency of the proposed model are demonstrated with both simulator\nevaluation and human evaluation.", "published": "2023-09-05 13:47:25", "link": "http://arxiv.org/abs/2309.02240v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Weigh Your Own Words: Improving Hate Speech Counter Narrative Generation\n  via Attention Regularization", "abstract": "Recent computational approaches for combating online hate speech involve the\nautomatic generation of counter narratives by adapting Pretrained\nTransformer-based Language Models (PLMs) with human-curated data. This process,\nhowever, can produce in-domain overfitting, resulting in models generating\nacceptable narratives only for hatred similar to training data, with little\nportability to other targets or to real-world toxic language. This paper\nintroduces novel attention regularization methodologies to improve the\ngeneralization capabilities of PLMs for counter narratives generation.\nOverfitting to training-specific terms is then discouraged, resulting in more\ndiverse and richer narratives. We experiment with two attention-based\nregularization techniques on a benchmark English dataset. Regularized models\nproduce better counter narratives than state-of-the-art approaches in most\ncases, both in terms of automatic metrics and human evaluation, especially when\nhateful targets are not present in the training data. This work paves the way\nfor better and more flexible counter-speech generation models, a task for which\ndatasets are highly challenging to produce.", "published": "2023-09-05 15:27:22", "link": "http://arxiv.org/abs/2309.02311v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "nanoT5: A PyTorch Framework for Pre-training and Fine-tuning T5-style\n  Models with Limited Resources", "abstract": "State-of-the-art language models like T5 have revolutionized the NLP\nlandscape, but their computational demands hinder a large portion of the\nresearch community. To address this challenge, we present nanoT5, a\nspecially-optimized PyTorch framework for efficient pre-training and\nfine-tuning of T5 models. Drawing on insights from optimizer differences and\nprioritizing efficiency, nanoT5 allows a T5-Base model to be pre-trained on a\nsingle GPU in just 16 hours, without any loss in performance. With the\nintroduction of this open-source framework, we hope to widen the accessibility\nto language modelling research and cater to the community's demand for more\nuser-friendly T5 (Encoder-Decoder) implementations. We make our contributions,\nincluding configurations, codebase, pre-training insights, and pre-trained\nmodels, available to the public.", "published": "2023-09-05 16:35:41", "link": "http://arxiv.org/abs/2309.02373v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Substitution-based Semantic Change Detection using Contextual Embeddings", "abstract": "Measuring semantic change has thus far remained a task where methods using\ncontextual embeddings have struggled to improve upon simpler techniques relying\nonly on static word vectors. Moreover, many of the previously proposed\napproaches suffer from downsides related to scalability and ease of\ninterpretation. We present a simplified approach to measuring semantic change\nusing contextual embeddings, relying only on the most probable substitutes for\nmasked terms. Not only is this approach directly interpretable, it is also far\nmore efficient in terms of storage, achieves superior average performance\nacross the most frequently cited datasets for this task, and allows for more\nnuanced investigation of change than is possible with static word vectors.", "published": "2023-09-05 17:33:59", "link": "http://arxiv.org/abs/2309.02403v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Planning, Search, and Memorization Capabilities of Large Language\n  Models", "abstract": "The rapid advancement of large language models, such as the Generative\nPre-trained Transformer (GPT) series, has had significant implications across\nvarious disciplines. In this study, we investigate the potential of the\nstate-of-the-art large language model (GPT-4) for planning tasks. We explore\nits effectiveness in multiple planning subfields, highlighting both its\nstrengths and limitations. Through a comprehensive examination, we identify\nareas where large language models excel in solving planning problems and reveal\nthe constraints that limit their applicability. Our empirical analysis focuses\non GPT-4's performance in planning domain extraction, graph search path\nplanning, and adversarial planning. We then propose a way of fine-tuning a\ndomain-specific large language model to improve its Chain of Thought (CoT)\ncapabilities for the above-mentioned tasks. The results provide valuable\ninsights into the potential applications of large language models in the\nplanning domain and pave the way for future research to overcome their\nlimitations and expand their capabilities.", "published": "2023-09-05 00:19:31", "link": "http://arxiv.org/abs/2309.01868v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CodeApex: A Bilingual Programming Evaluation Benchmark for Large\n  Language Models", "abstract": "With the emergence of Large Language Models (LLMs), there has been a\nsignificant improvement in the programming capabilities of models, attracting\ngrowing attention from researchers. Evaluating the programming capabilities of\nLLMs is crucial as it reflects the multifaceted abilities of LLMs, and it has\nnumerous downstream applications. In this paper, we propose CodeApex, a\nbilingual benchmark dataset focusing on the programming comprehension, code\ngeneration, and code correction abilities of LLMs. Programming comprehension\ntask tests LLMs on multiple-choice exam questions covering conceptual\nunderstanding, commonsense reasoning, and multi-hop reasoning. The code\ngeneration task evaluates LLMs through completing C++ functions based on\nprovided descriptions and prototypes. The code correction task asks LLMs to fix\nreal-world erroneous code segments with different error messages. We evaluate\n12 widely used LLMs, including both general-purpose and specialized models.\nGPT-4 exhibits the best programming capabilities, achieving approximate\naccuracy of 69%, 54%, and 66% on the three tasks, respectively. Compared to\nhuman performance, there is still significant room for improvement in LLM\nprogramming. We hope that CodeApex can serve as a reference for evaluating the\ncoding capabilities of LLMs, further promoting their development and growth.", "published": "2023-09-05 04:12:01", "link": "http://arxiv.org/abs/2309.01940v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Query-Focused Meeting Summarization with Query-Relevant\n  Knowledge", "abstract": "Query-Focused Meeting Summarization (QFMS) aims to generate a summary of a\ngiven meeting transcript conditioned upon a query. The main challenges for QFMS\nare the long input text length and sparse query-relevant information in the\nmeeting transcript. In this paper, we propose a knowledge-enhanced two-stage\nframework called Knowledge-Aware Summarizer (KAS) to tackle the challenges. In\nthe first stage, we introduce knowledge-aware scores to improve the\nquery-relevant segment extraction. In the second stage, we incorporate\nquery-relevant knowledge in the summary generation. Experimental results on the\nQMSum dataset show that our approach achieves state-of-the-art performance.\nFurther analysis proves the competency of our methods in generating relevant\nand faithful summaries.", "published": "2023-09-05 10:26:02", "link": "http://arxiv.org/abs/2309.02105v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Wordle: A Microcosm of Life. Luck, Skill, Cheating, Loyalty, and\n  Influence!", "abstract": "Wordle is a popular, online word game offered by the New York Times\n(nytimes.com). Currently there are some 2 million players of the English\nversion worldwide. Players have 6 attempts to guess the daily word (target\nword) and after each attempt, the player receives color-coded information about\nthe correctness and position of each letter in the guess. After either a\nsuccessful completion of the puzzle or the final unsuccessful attempt, software\ncan assess the player's luck and skill using Information Theory and can display\ndata for the first, second, ..., sixth guesses of a random sample of all\nplayers. Recently, I discovered that the latter data is presented in a format\nthat can easily be copied and pasted into a spreadsheet. I compiled data on\nWordle players' first guesses from May 2023 - August 2023 and inferred some\ninteresting information about Wordle players. A) Every day, about 0.2-0.5% of\nplayers solve the puzzle in one attempt. Because the odds of guessing the one\nof 2,315 possible target words at random is 0.043%, this implies that 4,000 -\n10,000 players cheat by obtaining the target word outside of playing the game!\nB) At least 1/3 of the players have a favorite starting word, or cycle through\nseveral. And even though players should be aware that target words are never\nrepeated, most players appear to remain loyal to their starting word even after\nits appearance as a target word. C) On August 15, 2023, about 30,000 players\nabruptly changed their starting word, presumably based on a crossword puzzle\nclue! Wordle players can be influenced! This study goes beyond social media\npostings, surveys, and Google Trends to provide solid, quantitative evidence\nabout cheating in Wordle.", "published": "2023-09-05 10:38:53", "link": "http://arxiv.org/abs/2309.02110v3", "categories": ["math.HO", "cs.CL"], "primary_category": "math.HO"}
{"title": "Incorporating Dictionaries into a Neural Network Architecture to Extract\n  COVID-19 Medical Concepts From Social Media", "abstract": "We investigate the potential benefit of incorporating dictionary information\ninto a neural network architecture for natural language processing. In\nparticular, we make use of this architecture to extract several concepts\nrelated to COVID-19 from an on-line medical forum. We use a sample from the\nforum to manually curate one dictionary for each concept. In addition, we use\nMetaMap, which is a tool for extracting biomedical concepts, to identify a\nsmall number of semantic concepts. For a supervised concept extraction task on\nthe forum data, our best model achieved a macro $F_1$ score of 90\\%. A major\ndifficulty in medical concept extraction is obtaining labelled data from which\nto build supervised models. We investigate the utility of our models to\ntransfer to data derived from a different source in two ways. First for\nproducing labels via weak learning and second to perform concept extraction.\nThe dataset we use in this case comprises COVID-19 related tweets and we\nachieve an $F_1$ score 81\\% for symptom concept extraction trained on weakly\nlabelled data. The utility of our dictionaries is compared with a COVID-19\nsymptom dictionary that was constructed directly from Twitter. Further\nexperiments that incorporate BERT and a COVID-19 version of BERTweet\ndemonstrate that the dictionaries provide a commensurate result. Our results\nshow that incorporating small domain dictionaries to deep learning models can\nimprove concept extraction tasks. Moreover, models built using dictionaries\ngeneralize well and are transferable to different datasets on a similar task.", "published": "2023-09-05 12:47:44", "link": "http://arxiv.org/abs/2309.02188v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question\n  Answering", "abstract": "Large-scale language models (LLMs) like ChatGPT have demonstrated impressive\nabilities in generating responses based on human instructions. However, their\nuse in the medical field can be challenging due to their lack of specific,\nin-depth knowledge. In this study, we present a system called LLMs Augmented\nwith Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs in\nspecialized domains. LLM-AMT integrates authoritative medical textbooks into\nthe LLMs' framework using plug-and-play modules. These modules include a Query\nAugmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together,\nthey incorporate authoritative medical knowledge. Additionally, an LLM Reader\naids in contextual understanding. Our experimental results on three medical QA\ntasks demonstrate that LLMAMT significantly improves response quality, with\naccuracy gains ranging from 11.6% to 16.6%. Notably, with GPT-4-Turbo as the\nbase model, LLM-AMT outperforms the specialized Med-PaLM 2 model pre-trained on\na massive amount of medical corpus by 2-3%. We found that despite being 100x\nsmaller in size, medical textbooks as a retrieval corpus is proven to be a more\neffective knowledge database than Wikipedia in the medical domain, boosting\nperformance by 7.8%-13.7%.", "published": "2023-09-05 13:39:38", "link": "http://arxiv.org/abs/2309.02233v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automating Behavioral Testing in Machine Translation", "abstract": "Behavioral testing in NLP allows fine-grained evaluation of systems by\nexamining their linguistic capabilities through the analysis of input-output\nbehavior. Unfortunately, existing work on behavioral testing in Machine\nTranslation (MT) is currently restricted to largely handcrafted tests covering\na limited range of capabilities and languages. To address this limitation, we\npropose to use Large Language Models (LLMs) to generate a diverse set of source\nsentences tailored to test the behavior of MT models in a range of situations.\nWe can then verify whether the MT model exhibits the expected behavior through\nmatching candidate sets that are also generated using LLMs. Our approach aims\nto make behavioral testing of MT systems practical while requiring only minimal\nhuman effort. In our experiments, we apply our proposed evaluation framework to\nassess multiple available MT systems, revealing that while in general\npass-rates follow the trends observable from traditional accuracy-based\nmetrics, our method was able to uncover several important differences and\npotential bugs that go unnoticed when relying only on accuracy.", "published": "2023-09-05 19:40:45", "link": "http://arxiv.org/abs/2309.02553v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploiting Language Models as a Source of Knowledge for Cognitive Agents", "abstract": "Large language models (LLMs) provide capabilities far beyond sentence\ncompletion, including question answering, summarization, and natural-language\ninference. While many of these capabilities have potential application to\ncognitive systems, our research is exploiting language models as a source of\ntask knowledge for cognitive agents, that is, agents realized via a cognitive\narchitecture. We identify challenges and opportunities for using language\nmodels as an external knowledge source for cognitive systems and possible ways\nto improve the effectiveness of knowledge extraction by integrating extraction\nwith cognitive architecture capabilities, highlighting with examples from our\nrecent work in this area.", "published": "2023-09-05 15:18:04", "link": "http://arxiv.org/abs/2310.06846v1", "categories": ["cs.AI", "cs.CL", "I.2.7; I.2.11"], "primary_category": "cs.AI"}
{"title": "QuantEase: Optimization-based Quantization for Language Models", "abstract": "With the rising popularity of Large Language Models (LLMs), there has been an\nincreasing interest in compression techniques that enable their efficient\ndeployment. This study focuses on the Post-Training Quantization (PTQ) of LLMs.\nDrawing from recent advances, our work introduces QuantEase, a layer-wise\nquantization framework where individual layers undergo separate quantization.\nThe problem is framed as a discrete-structured non-convex optimization,\nprompting the development of algorithms rooted in Coordinate Descent (CD)\ntechniques. These CD-based methods provide high-quality solutions to the\ncomplex non-convex layer-wise quantization problems. Notably, our CD-based\napproach features straightforward updates, relying solely on matrix and vector\noperations, circumventing the need for matrix inversion or decomposition. We\nalso explore an outlier-aware variant of our approach, allowing for retaining\nsignificant weights (outliers) with complete precision. Our proposal attains\nstate-of-the-art performance in terms of perplexity and zero-shot accuracy in\nempirical evaluations across various LLMs and datasets, with relative\nimprovements up to 15% over methods such as GPTQ. Leveraging careful linear\nalgebra optimizations, QuantEase can quantize models like Falcon-180B on a\nsingle NVIDIA A100 GPU in $\\sim$3 hours. Particularly noteworthy is our\noutlier-aware algorithm's capability to achieve near or sub-3-bit quantization\nof LLMs with an acceptable drop in accuracy, obviating the need for non-uniform\nquantization or grouping techniques, improving upon methods such as SpQR by up\nto two times in terms of perplexity.", "published": "2023-09-05 01:39:09", "link": "http://arxiv.org/abs/2309.01885v2", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "TODM: Train Once Deploy Many Efficient Supernet-Based RNN-T Compression\n  For On-device ASR Models", "abstract": "Automatic Speech Recognition (ASR) models need to be optimized for specific\nhardware before they can be deployed on devices. This can be done by tuning the\nmodel's hyperparameters or exploring variations in its architecture.\nRe-training and re-validating models after making these changes can be a\nresource-intensive task. This paper presents TODM (Train Once Deploy Many), a\nnew approach to efficiently train many sizes of hardware-friendly on-device ASR\nmodels with comparable GPU-hours to that of a single training job. TODM\nleverages insights from prior work on Supernet, where Recurrent Neural Network\nTransducer (RNN-T) models share weights within a Supernet. It reduces layer\nsizes and widths of the Supernet to obtain subnetworks, making them smaller\nmodels suitable for all hardware types. We introduce a novel combination of\nthree techniques to improve the outcomes of the TODM Supernet: adaptive\ndropouts, an in-place Alpha-divergence knowledge distillation, and the use of\nScaledAdam optimizer. We validate our approach by comparing Supernet-trained\nversus individually tuned Multi-Head State Space Model (MH-SSM) RNN-T using\nLibriSpeech. Results demonstrate that our TODM Supernet either matches or\nsurpasses the performance of manually tuned models by up to a relative of 3%\nbetter in word error rate (WER), while efficiently keeping the cost of training\nmany models at a small constant.", "published": "2023-09-05 04:47:55", "link": "http://arxiv.org/abs/2309.01947v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Enhance Multi-domain Sentiment Analysis of Review Texts through\n  Prompting Strategies", "abstract": "Large Language Models (LLMs) have made significant strides in both scientific\nresearch and practical applications. Existing studies have demonstrated the\nstate-of-the-art (SOTA) performance of LLMs in various natural language\nprocessing tasks. However, the question of how to further enhance LLMs'\nperformance in specific task using prompting strategies remains a pivotal\nconcern. This paper explores the enhancement of LLMs' performance in sentiment\nanalysis through the application of prompting strategies. We formulate the\nprocess of prompting for sentiment analysis tasks and introduce two novel\nstrategies tailored for sentiment analysis: RolePlaying (RP) prompting and\nChain-of-thought (CoT) prompting. Specifically, we also propose the RP-CoT\nprompting strategy which is a combination of RP prompting and CoT prompting. We\nconduct comparative experiments on three distinct domain datasets to evaluate\nthe effectiveness of the proposed sentiment analysis strategies. The results\ndemonstrate that the adoption of the proposed prompting strategies leads to a\nincreasing enhancement in sentiment analysis accuracy. Further, the CoT\nprompting strategy exhibits a notable impact on implicit sentiment analysis,\nwith the RP-CoT prompting strategy delivering the most superior performance\namong all strategies.", "published": "2023-09-05 08:44:23", "link": "http://arxiv.org/abs/2309.02045v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Label Information for Multimodal Emotion Recognition", "abstract": "Multimodal emotion recognition (MER) aims to detect the emotional status of a\ngiven expression by combining the speech and text information. Intuitively,\nlabel information should be capable of helping the model locate the salient\ntokens/frames relevant to the specific emotion, which finally facilitates the\nMER task. Inspired by this, we propose a novel approach for MER by leveraging\nlabel information. Specifically, we first obtain the representative label\nembeddings for both text and speech modalities, then learn the label-enhanced\ntext/speech representations for each utterance via label-token and label-frame\ninteractions. Finally, we devise a novel label-guided attentive fusion module\nto fuse the label-aware text and speech representations for emotion\nclassification. Extensive experiments were conducted on the public IEMOCAP\ndataset, and experimental results demonstrate that our proposed approach\noutperforms existing baselines and achieves new state-of-the-art performance.", "published": "2023-09-05 10:26:32", "link": "http://arxiv.org/abs/2309.02106v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Evaluating Methods for Ground-Truth-Free Foreign Accent Conversion", "abstract": "Foreign accent conversion (FAC) is a special application of voice conversion\n(VC) which aims to convert the accented speech of a non-native speaker to a\nnative-sounding speech with the same speaker identity. FAC is difficult since\nthe native speech from the desired non-native speaker to be used as the\ntraining target is impossible to collect. In this work, we evaluate three\nrecently proposed methods for ground-truth-free FAC, where all of them aim to\nharness the power of sequence-to-sequence (seq2seq) and non-parallel VC models\nto properly convert the accent and control the speaker identity. Our\nexperimental evaluation results show that no single method was significantly\nbetter than the others in all evaluation axes, which is in contrast to\nconclusions drawn in previous studies. We also explain the effectiveness of\nthese methods with the training input and output of the seq2seq model and\nexamine the design choice of the non-parallel VC model, and show that\nintelligibility measures such as word error rates do not correlate well with\nsubjective accentedness. Finally, our implementation is open-sourced to promote\nreproducible research and help future researchers improve upon the compared\nsystems.", "published": "2023-09-05 11:22:08", "link": "http://arxiv.org/abs/2309.02133v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Making Large Language Models Better Reasoners with Alignment", "abstract": "Reasoning is a cognitive process of using evidence to reach a sound\nconclusion. The reasoning capability is essential for large language models\n(LLMs) to serve as the brain of the artificial general intelligence agent.\nRecent studies reveal that fine-tuning LLMs on data with the chain of thought\n(COT) reasoning process can significantly enhance their reasoning capabilities.\nHowever, we find that the fine-tuned LLMs suffer from an \\textit{Assessment\nMisalignment} problem, i.e., they frequently assign higher scores to subpar\nCOTs, leading to potential limitations in their reasoning abilities. To address\nthis problem, we introduce an \\textit{Alignment Fine-Tuning (AFT)} paradigm,\nwhich involves three steps: 1) fine-tuning LLMs with COT training data; 2)\ngenerating multiple COT responses for each question, and categorizing them into\npositive and negative ones based on whether they achieve the correct answer; 3)\ncalibrating the scores of positive and negative responses given by LLMs with a\nnovel constraint alignment loss. Specifically, the constraint alignment loss\nhas two objectives: a) Alignment, which guarantees that positive scores surpass\nnegative scores to encourage answers with high-quality COTs; b) Constraint,\nwhich keeps the negative scores confined to a reasonable range to prevent the\nmodel degradation. Beyond just the binary positive and negative feedback, the\nconstraint alignment loss can be seamlessly adapted to the ranking situations\nwhen ranking feedback is accessible. Furthermore, we also delve deeply into\nrecent ranking-based alignment methods, such as DPO, RRHF, and PRO, and\ndiscover that the constraint, which has been overlooked by these approaches, is\nalso crucial for their performance. Extensive experiments on four reasoning\nbenchmarks with both binary and ranking feedback demonstrate the effectiveness\nof AFT.", "published": "2023-09-05 11:32:48", "link": "http://arxiv.org/abs/2309.02144v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bring the Noise: Introducing Noise Robustness to Pretrained Automatic\n  Speech Recognition", "abstract": "In recent research, in the domain of speech processing, large End-to-End\n(E2E) systems for Automatic Speech Recognition (ASR) have reported\nstate-of-the-art performance on various benchmarks. These systems intrinsically\nlearn how to handle and remove noise conditions from speech. Previous research\nhas shown, that it is possible to extract the denoising capabilities of these\nmodels into a preprocessor network, which can be used as a frontend for\ndownstream ASR models. However, the proposed methods were limited to specific\nfully convolutional architectures. In this work, we propose a novel method to\nextract the denoising capabilities, that can be applied to any encoder-decoder\narchitecture. We propose the Cleancoder preprocessor architecture that extracts\nhidden activations from the Conformer ASR model and feeds them to a decoder to\npredict denoised spectrograms. We train our pre-processor on the Noisy Speech\nDatabase (NSD) to reconstruct denoised spectrograms from noisy inputs. Then, we\nevaluate our model as a frontend to a pretrained Conformer ASR model as well as\na frontend to train smaller Conformer ASR models from scratch. We show that the\nCleancoder is able to filter noise from speech and that it improves the total\nWord Error Rate (WER) of the downstream model in noisy conditions for both\napplications.", "published": "2023-09-05 11:34:21", "link": "http://arxiv.org/abs/2309.02145v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Leveraging BERT Language Models for Multi-Lingual ESG Issue\n  Identification", "abstract": "Environmental, Social, and Governance (ESG) has been used as a metric to\nmeasure the negative impacts and enhance positive outcomes of companies in\nareas such as the environment, society, and governance. Recently, investors\nhave increasingly recognized the significance of ESG criteria in their\ninvestment choices, leading businesses to integrate ESG principles into their\noperations and strategies. The Multi-Lingual ESG Issue Identification (ML-ESG)\nshared task encompasses the classification of news documents into 35 distinct\nESG issue labels. In this study, we explored multiple strategies harnessing\nBERT language models to achieve accurate classification of news documents\nacross these labels. Our analysis revealed that the RoBERTa classifier emerged\nas one of the most successful approaches, securing the second-place position\nfor the English test dataset, and sharing the fifth-place position for the\nFrench test dataset. Furthermore, our SVM-based binary model tailored for the\nChinese language exhibited exceptional performance, earning the second-place\nrank on the test dataset.", "published": "2023-09-05 12:48:21", "link": "http://arxiv.org/abs/2309.02189v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PromptTTS 2: Describing and Generating Voices with Text Prompt", "abstract": "Speech conveys more information than text, as the same word can be uttered in\nvarious voices to convey diverse information. Compared to traditional\ntext-to-speech (TTS) methods relying on speech prompts (reference speech) for\nvoice variability, using text prompts (descriptions) is more user-friendly\nsince speech prompts can be hard to find or may not exist at all. TTS\napproaches based on the text prompt face two main challenges: 1) the\none-to-many problem, where not all details about voice variability can be\ndescribed in the text prompt, and 2) the limited availability of text prompt\ndatasets, where vendors and large cost of data labeling are required to write\ntext prompts for speech. In this work, we introduce PromptTTS 2 to address\nthese challenges with a variation network to provide variability information of\nvoice not captured by text prompts, and a prompt generation pipeline to utilize\nthe large language models (LLM) to compose high quality text prompts.\nSpecifically, the variation network predicts the representation extracted from\nthe reference speech (which contains full information about voice variability)\nbased on the text prompt representation. For the prompt generation pipeline, it\ngenerates text prompts for speech with a speech language understanding model to\nrecognize voice attributes (e.g., gender, speed) from speech and a large\nlanguage model to formulate text prompts based on the recognition results.\nExperiments on a large-scale (44K hours) speech dataset demonstrate that\ncompared to the previous works, PromptTTS 2 generates voices more consistent\nwith text prompts and supports the sampling of diverse voice variability,\nthereby offering users more choices on voice generation. Additionally, the\nprompt generation pipeline produces high-quality text prompts, eliminating the\nlarge labeling cost. The demo page of PromptTTS 2 is available online.", "published": "2023-09-05 14:45:27", "link": "http://arxiv.org/abs/2309.02285v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cognitive Architectures for Language Agents", "abstract": "Recent efforts have augmented large language models (LLMs) with external\nresources (e.g., the Internet) or internal control flows (e.g., prompt\nchaining) for tasks requiring grounding or reasoning, leading to a new class of\nlanguage agents. While these agents have achieved substantial empirical\nsuccess, we lack a systematic framework to organize existing agents and plan\nfuture developments. In this paper, we draw on the rich history of cognitive\nscience and symbolic artificial intelligence to propose Cognitive Architectures\nfor Language Agents (CoALA). CoALA describes a language agent with modular\nmemory components, a structured action space to interact with internal memory\nand external environments, and a generalized decision-making process to choose\nactions. We use CoALA to retrospectively survey and organize a large body of\nrecent work, and prospectively identify actionable directions towards more\ncapable agents. Taken together, CoALA contextualizes today's language agents\nwithin the broader history of AI and outlines a path towards language-based\ngeneral intelligence.", "published": "2023-09-05 17:56:20", "link": "http://arxiv.org/abs/2309.02427v3", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SC"], "primary_category": "cs.AI"}
{"title": "Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction\n  Tuning", "abstract": "We present CM3Leon (pronounced \"Chameleon\"), a retrieval-augmented,\ntoken-based, decoder-only multi-modal language model capable of generating and\ninfilling both text and images. CM3Leon uses the CM3 multi-modal architecture\nbut additionally shows the extreme benefits of scaling up and tuning on more\ndiverse instruction-style data. It is the first multi-modal model trained with\na recipe adapted from text-only language models, including a large-scale\nretrieval-augmented pre-training stage and a second multi-task supervised\nfine-tuning (SFT) stage. It is also a general-purpose model that can do both\ntext-to-image and image-to-text generation, allowing us to introduce\nself-contained contrastive decoding methods that produce high-quality outputs.\nExtensive experiments demonstrate that this recipe is highly effective for\nmulti-modal models. CM3Leon achieves state-of-the-art performance in\ntext-to-image generation with 5x less training compute than comparable methods\n(zero-shot MS-COCO FID of 4.88). After SFT, CM3Leon can also demonstrate\nunprecedented levels of controllability in tasks ranging from language-guided\nimage editing to image-controlled generation and segmentation.", "published": "2023-09-05 21:27:27", "link": "http://arxiv.org/abs/2309.02591v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "AGIBench: A Multi-granularity, Multimodal, Human-referenced,\n  Auto-scoring Benchmark for Large Language Models", "abstract": "Large language models (LLMs) like ChatGPT have revealed amazing intelligence.\nHow to evaluate the question-solving abilities of LLMs and their degrees of\nintelligence is a hot-spot but challenging issue. First, the question-solving\nabilities are interlaced with different ability branches like understanding and\nmassive knowledge categories like mathematics. Second, the inputs of questions\nare multimodal that may involve text and images. Third, the response format of\nLLMs is diverse and thus poses great challenges for result extraction and\nevaluation. In this paper, we propose AGIBench -- a multi-granularity,\nmultimodal, human-referenced, and auto-scoring benchmarking methodology for\nLLMs. Instead of a collection of blended questions, AGIBench focuses on three\ntypical ability branches and adopts a four-tuple <ability branch, knowledge,\ndifficulty, modal> to label the attributes of each question. First, it supports\nmulti-granularity benchmarking, e.g., per-question, per-ability branch,\nper-knowledge, per-modal, per-dataset, and per-difficulty level granularities.\nSecond, it contains multimodal input, including text and images. Third, it\nclassifies all the questions into five degrees of difficulty according to the\naverage accuracy rate of abundant educated humans (human-referenced). Fourth,\nit adopts zero-shot learning to avoid introducing additional unpredictability\nand provides an auto-scoring method to extract and judge the result. Finally,\nit defines multi-dimensional metrics, including accuracy under the average,\nworst, best, and majority voting cases, and repeatability. AGIBench is\npublically available from \\url{https://www.benchcouncil.org/agibench}.", "published": "2023-09-05 13:43:37", "link": "http://arxiv.org/abs/2309.06495v1", "categories": ["cs.CL", "cs.AI", "cs.PF"], "primary_category": "cs.CL"}
{"title": "ChatGPT Assisting Diagnosis of Neuro-ophthalmology Diseases Based on\n  Case Reports", "abstract": "Objective: To evaluate the efficiency of large language models (LLMs) such as\nChatGPT to assist in diagnosing neuro-ophthalmic diseases based on detailed\ncase descriptions. Methods: We selected 22 different case reports of\nneuro-ophthalmic diseases from a publicly available online database. These\ncases included a wide range of chronic and acute diseases that are commonly\nseen by neuro-ophthalmic sub-specialists. We inserted the text from each case\nas a new prompt into both ChatGPT v3.5 and ChatGPT Plus v4.0 and asked for the\nmost probable diagnosis. We then presented the exact information to two\nneuro-ophthalmologists and recorded their diagnoses followed by comparison to\nresponses from both versions of ChatGPT. Results: ChatGPT v3.5, ChatGPT Plus\nv4.0, and the two neuro-ophthalmologists were correct in 13 (59%), 18 (82%), 19\n(86%), and 19 (86%) out of 22 cases, respectively. The agreement between the\nvarious diagnostic sources were as follows: ChatGPT v3.5 and ChatGPT Plus v4.0,\n13 (59%); ChatGPT v3.5 and the first neuro-ophthalmologist, 12 (55%); ChatGPT\nv3.5 and the second neuro-ophthalmologist, 12 (55%); ChatGPT Plus v4.0 and the\nfirst neuro-ophthalmologist, 17 (77%); ChatGPT Plus v4.0 and the second\nneuro-ophthalmologist, 16 (73%); and first and second neuro-ophthalmologists 17\n(17%). Conclusions: The accuracy of ChatGPT v3.5 and ChatGPT Plus v4.0 in\ndiagnosing patients with neuro-ophthalmic diseases was 59% and 82%,\nrespectively. With further development, ChatGPT Plus v4.0 may have potential to\nbe used in clinical care settings to assist clinicians in providing quick,\naccurate diagnoses of patients in neuro-ophthalmology. The applicability of\nusing LLMs like ChatGPT in clinical settings that lack access to subspeciality\ntrained neuro-ophthalmologists deserves further research.", "published": "2023-09-05 00:44:23", "link": "http://arxiv.org/abs/2309.12361v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "PESTO: Pitch Estimation with Self-supervised Transposition-equivariant\n  Objective", "abstract": "In this paper, we address the problem of pitch estimation using Self\nSupervised Learning (SSL). The SSL paradigm we use is equivariance to pitch\ntransposition, which enables our model to accurately perform pitch estimation\non monophonic audio after being trained only on a small unlabeled dataset. We\nuse a lightweight ($<$ 30k parameters) Siamese neural network that takes as\ninputs two different pitch-shifted versions of the same audio represented by\nits Constant-Q Transform. To prevent the model from collapsing in an\nencoder-only setting, we propose a novel class-based transposition-equivariant\nobjective which captures pitch information. Furthermore, we design the\narchitecture of our network to be transposition-preserving by introducing\nlearnable Toeplitz matrices.\n  We evaluate our model for the two tasks of singing voice and musical\ninstrument pitch estimation and show that our model is able to generalize\nacross tasks and datasets while being lightweight, hence remaining compatible\nwith low-resource devices and suitable for real-time applications. In\nparticular, our results surpass self-supervised baselines and narrow the\nperformance gap between self-supervised and supervised methods for pitch\nestimation.", "published": "2023-09-05 14:20:08", "link": "http://arxiv.org/abs/2309.02265v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "In-Ear-Voice: Towards Milli-Watt Audio Enhancement With Bone-Conduction\n  Microphones for In-Ear Sensing Platforms", "abstract": "The recent ubiquitous adoption of remote conferencing has been accompanied by\nomnipresent frustration with distorted or otherwise unclear voice\ncommunication. Audio enhancement can compensate for low-quality input signals\nfrom, for example, small true wireless earbuds, by applying noise suppression\ntechniques. Such processing relies on voice activity detection (VAD) with low\nlatency and the added capability of discriminating the wearer's voice from\nothers - a task of significant computational complexity. The tight energy\nbudget of devices as small as modern earphones, however, requires any system\nattempting to tackle this problem to do so with minimal power and processing\noverhead, while not relying on speaker-specific voice samples and training due\nto usability concerns.\n  This paper presents the design and implementation of a custom research\nplatform for low-power wireless earbuds based on novel, commercial, MEMS\nbone-conduction microphones. Such microphones can record the wearer's speech\nwith much greater isolation, enabling personalized voice activity detection and\nfurther audio enhancement applications. Furthermore, the paper accurately\nevaluates a proposed low-power personalized speech detection algorithm based on\nbone conduction data and a recurrent neural network running on the implemented\nresearch platform. This algorithm is compared to an approach based on\ntraditional microphone input. The performance of the bone conduction system,\nachieving detection of speech within 12.8ms at an accuracy of 95\\% is\nevaluated. Different SoC choices are contrasted, with the final implementation\nbased on the cutting-edge Ambiq Apollo 4 Blue SoC achieving 2.64mW average\npower consumption at 14uJ per inference, reaching 43h of battery life on a\nminiature 32mAh li-ion cell and without duty cycling.", "published": "2023-09-05 17:04:09", "link": "http://arxiv.org/abs/2309.02393v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Employing Real Training Data for Deep Noise Suppression", "abstract": "Most deep noise suppression (DNS) models are trained with reference-based\nlosses requiring access to clean speech. However, sometimes an additive\nmicrophone model is insufficient for real-world applications. Accordingly, ways\nto use real training data in supervised learning for DNS models promise to\nreduce a potential training/inference mismatch. Employing real data for DNS\ntraining requires either generative approaches or a reference-free loss without\naccess to the corresponding clean speech. In this work, we propose to employ an\nend-to-end non-intrusive deep neural network (DNN), named PESQ-DNN, to estimate\nperceptual evaluation of speech quality (PESQ) scores of enhanced real data. It\nprovides a reference-free perceptual loss for employing real data during DNS\ntraining, maximizing the PESQ scores. Furthermore, we use an epoch-wise\nalternating training protocol, updating the DNS model on real data, followed by\nPESQ-DNN updating on synthetic data. The DNS model trained with the PESQ-DNN\nemploying real data outperforms all reference methods employing only synthetic\ntraining data. On synthetic test data, our proposed method excels the\nInterspeech 2021 DNS Challenge baseline by a significant 0.32 PESQ points. Both\non synthetic and real test data, the proposed method beats the baseline by 0.05\nDNSMOS points - although PESQ-DNN optimizes for a different perceptual metric.", "published": "2023-09-05 17:58:58", "link": "http://arxiv.org/abs/2309.02432v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "BWSNet: Automatic Perceptual Assessment of Audio Signals", "abstract": "This paper introduces BWSNet, a model that can be trained from raw human\njudgements obtained through a Best-Worst scaling (BWS) experiment. It maps\nsound samples into an embedded space that represents the perception of a\nstudied attribute. To this end, we propose a set of cost functions and\nconstraints, interpreting trial-wise ordinal relations as distance comparisons\nin a metric learning task. We tested our proposal on data from two BWS studies\ninvestigating the perception of speech social attitudes and timbral qualities.\nFor both datasets, our results show that the structure of the latent space is\nfaithful to human judgements.", "published": "2023-09-05 21:27:32", "link": "http://arxiv.org/abs/2309.02592v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Music Source Separation with Band-Split RoPE Transformer", "abstract": "Music source separation (MSS) aims to separate a music recording into\nmultiple musically distinct stems, such as vocals, bass, drums, and more.\nRecently, deep learning approaches such as convolutional neural networks (CNNs)\nand recurrent neural networks (RNNs) have been used, but the improvement is\nstill limited. In this paper, we propose a novel frequency-domain approach\nbased on a Band-Split RoPE Transformer (called BS-RoFormer). BS-RoFormer relies\non a band-split module to project the input complex spectrogram into\nsubband-level representations, and then arranges a stack of hierarchical\nTransformers to model the inner-band as well as inter-band sequences for\nmulti-band mask estimation. To facilitate training the model for MSS, we\npropose to use the Rotary Position Embedding (RoPE). The BS-RoFormer system\ntrained on MUSDB18HQ and 500 extra songs ranked the first place in the MSS\ntrack of Sound Demixing Challenge (SDX23). Benchmarking a smaller version of\nBS-RoFormer on MUSDB18HQ, we achieve state-of-the-art result without extra\ntraining data, with 9.80 dB of average SDR.", "published": "2023-09-05 23:11:50", "link": "http://arxiv.org/abs/2309.02612v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FSD: An Initial Chinese Dataset for Fake Song Detection", "abstract": "Singing voice synthesis and singing voice conversion have significantly\nadvanced, revolutionizing musical experiences. However, the rise of \"Deepfake\nSongs\" generated by these technologies raises concerns about authenticity.\nUnlike Audio DeepFake Detection (ADD), the field of song deepfake detection\nlacks specialized datasets or methods for song authenticity verification. In\nthis paper, we initially construct a Chinese Fake Song Detection (FSD) dataset\nto investigate the field of song deepfake detection. The fake songs in the FSD\ndataset are generated by five state-of-the-art singing voice synthesis and\nsinging voice conversion methods. Our initial experiments on FSD revealed the\nineffectiveness of existing speech-trained ADD models for the task of song\ndeepFake detection. Thus, we employ the FSD dataset for the training of ADD\nmodels. We subsequently evaluate these models under two scenarios: one with the\noriginal songs and another with separated vocal tracks. Experiment results show\nthat song-trained ADD models exhibit a 38.58% reduction in average equal error\nrate compared to speech-trained ADD models on the FSD test set.", "published": "2023-09-05 13:37:30", "link": "http://arxiv.org/abs/2309.02232v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-Similarity-Based and Novelty-based loss for music structure\n  analysis", "abstract": "Music Structure Analysis (MSA) is the task aiming at identifying musical\nsegments that compose a music track and possibly label them based on their\nsimilarity. In this paper we propose a supervised approach for the task of\nmusic boundary detection. In our approach we simultaneously learn features and\nconvolution kernels. For this we jointly optimize -- a loss based on the\nSelf-Similarity-Matrix (SSM) obtained with the learned features, denoted by\nSSM-loss, and -- a loss based on the novelty score obtained applying the\nlearned kernels to the estimated SSM, denoted by novelty-loss. We also\ndemonstrate that relative feature learning, through self-attention, is\nbeneficial for the task of MSA. Finally, we compare the performances of our\napproach to previously proposed approaches on the standard RWC-Pop, and various\nsubsets of SALAMI.", "published": "2023-09-05 13:49:29", "link": "http://arxiv.org/abs/2309.02243v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Batik-plays-Mozart Corpus: Linking Performance to Score to\n  Musicological Annotations", "abstract": "We present the Batik-plays-Mozart Corpus, a piano performance dataset\ncombining professional Mozart piano sonata performances with expert-labelled\nscores at a note-precise level. The performances originate from a recording by\nViennese pianist Roland Batik on a computer-monitored B\\\"osendorfer grand\npiano, and are available both as MIDI files and audio recordings. They have\nbeen precisely aligned, note by note, with a current standard edition of the\ncorresponding scores (the New Mozart Edition) in such a way that they can\nfurther be connected to the musicological annotations (harmony, cadences,\nphrases) on these scores that were recently published by Hentschel et al.\n(2021).\n  The result is a high-quality, high-precision corpus mapping scores and\nmusical structure annotations to precise note-level professional performance\ninformation. As the first of its kind, it can serve as a valuable resource for\nstudying various facets of expressive performance and their relationship with\nstructural aspects. In the paper, we outline the curation process of the\nalignment and conduct two exploratory experiments to demonstrate its usefulness\nin analyzing expressive performance.", "published": "2023-09-05 17:13:47", "link": "http://arxiv.org/abs/2309.02399v2", "categories": ["cs.SD", "cs.DL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Voice Morphing: Two Identities in One Voice", "abstract": "In a biometric system, each biometric sample or template is typically\nassociated with a single identity. However, recent research has demonstrated\nthe possibility of generating \"morph\" biometric samples that can successfully\nmatch more than a single identity. Morph attacks are now recognized as a\npotential security threat to biometric systems. However, most morph attacks\nhave been studied on biometric modalities operating in the image domain, such\nas face, fingerprint, and iris. In this preliminary work, we introduce Voice\nIdentity Morphing (VIM) - a voice-based morph attack that can synthesize speech\nsamples that impersonate the voice characteristics of a pair of individuals.\nOur experiments evaluate the vulnerabilities of two popular speaker recognition\nsystems, ECAPA-TDNN and x-vector, to VIM, with a success rate (MMPMR) of over\n80% at a false match rate of 1% on the Librispeech dataset.", "published": "2023-09-05 17:36:34", "link": "http://arxiv.org/abs/2309.02404v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generating Realistic Images from In-the-wild Sounds", "abstract": "Representing wild sounds as images is an important but challenging task due\nto the lack of paired datasets between sound and images and the significant\ndifferences in the characteristics of these two modalities. Previous studies\nhave focused on generating images from sound in limited categories or music. In\nthis paper, we propose a novel approach to generate images from in-the-wild\nsounds. First, we convert sound into text using audio captioning. Second, we\npropose audio attention and sentence attention to represent the rich\ncharacteristics of sound and visualize the sound. Lastly, we propose a direct\nsound optimization with CLIPscore and AudioCLIP and generate images with a\ndiffusion-based model. In experiments, it shows that our model is able to\ngenerate high quality images from wild sounds and outperforms baselines in both\nquantitative and qualitative evaluations on wild audio datasets.", "published": "2023-09-05 17:36:40", "link": "http://arxiv.org/abs/2309.02405v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Personalized Adaptation with Pre-trained Speech Encoders for Continuous\n  Emotion Recognition", "abstract": "There are individual differences in expressive behaviors driven by cultural\nnorms and personality. This between-person variation can result in reduced\nemotion recognition performance. Therefore, personalization is an important\nstep in improving the generalization and robustness of speech emotion\nrecognition. In this paper, to achieve unsupervised personalized emotion\nrecognition, we first pre-train an encoder with learnable speaker embeddings in\na self-supervised manner to learn robust speech representations conditioned on\nspeakers. Second, we propose an unsupervised method to compensate for the label\ndistribution shifts by finding similar speakers and leveraging their label\ndistributions from the training set. Extensive experimental results on the\nMSP-Podcast corpus indicate that our method consistently outperforms strong\npersonalization baselines and achieves state-of-the-art performance for valence\nestimation.", "published": "2023-09-05 17:50:12", "link": "http://arxiv.org/abs/2309.02418v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "A Generalized Bandsplit Neural Network for Cinematic Audio Source\n  Separation", "abstract": "Cinematic audio source separation is a relatively new subtask of audio source\nseparation, with the aim of extracting the dialogue, music, and effects stems\nfrom their mixture. In this work, we developed a model generalizing the\nBandsplit RNN for any complete or overcomplete partitions of the frequency\naxis. Psychoacoustically motivated frequency scales were used to inform the\nband definitions which are now defined with redundancy for more reliable\nfeature extraction. A loss function motivated by the signal-to-noise ratio and\nthe sparsity-promoting property of the 1-norm was proposed. We additionally\nexploit the information-sharing property of a common-encoder setup to reduce\ncomputational complexity during both training and inference, improve separation\nperformance for hard-to-generalize classes of sounds, and allow flexibility\nduring inference time with detachable decoders. Our best model sets the state\nof the art on the Divide and Remaster dataset with performance above the ideal\nratio mask for the dialogue stem.", "published": "2023-09-05 19:19:22", "link": "http://arxiv.org/abs/2309.02539v3", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Symbolic Music Representations for Classification Tasks: A Systematic\n  Evaluation", "abstract": "Music Information Retrieval (MIR) has seen a recent surge in deep\nlearning-based approaches, which often involve encoding symbolic music (i.e.,\nmusic represented in terms of discrete note events) in an image-like or\nlanguage like fashion. However, symbolic music is neither an image nor a\nsentence, and research in the symbolic domain lacks a comprehensive overview of\nthe different available representations. In this paper, we investigate matrix\n(piano roll), sequence, and graph representations and their corresponding\nneural architectures, in combination with symbolic scores and performances on\nthree piece-level classification tasks. We also introduce a novel graph\nrepresentation for symbolic performances and explore the capability of graph\nrepresentations in global classification tasks. Our systematic evaluation shows\nadvantages and limitations of each input representation. Our results suggest\nthat the graph representation, as the newest and least explored among the three\napproaches, exhibits promising performance, while being more light-weight in\ntraining.", "published": "2023-09-05 20:27:31", "link": "http://arxiv.org/abs/2309.02567v2", "categories": ["eess.AS", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "RADIO: Reference-Agnostic Dubbing Video Synthesis", "abstract": "One of the most challenging problems in audio-driven talking head generation\nis achieving high-fidelity detail while ensuring precise synchronization. Given\nonly a single reference image, extracting meaningful identity attributes\nbecomes even more challenging, often causing the network to mirror the facial\nand lip structures too closely. To address these issues, we introduce RADIO, a\nframework engineered to yield high-quality dubbed videos regardless of the pose\nor expression in reference images. The key is to modulate the decoder layers\nusing latent space composed of audio and reference features. Additionally, we\nincorporate ViT blocks into the decoder to emphasize high-fidelity details,\nespecially in the lip region. Our experimental results demonstrate that RADIO\ndisplays high synchronization without the loss of fidelity. Especially in harsh\nscenarios where the reference frame deviates significantly from the ground\ntruth, our method outperforms state-of-the-art methods, highlighting its\nrobustness.", "published": "2023-09-05 04:56:18", "link": "http://arxiv.org/abs/2309.01950v2", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
