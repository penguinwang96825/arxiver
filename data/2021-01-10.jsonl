{"title": "Adaptive Prototypical Networks with Label Words and Joint Representation\n  Learning for Few-Shot Relation Classification", "abstract": "Relation classification (RC) task is one of fundamental tasks of information\nextraction, aiming to detect the relation information between entity pairs in\nunstructured natural language text and generate structured data in the form of\nentity-relation triple. Although distant supervision methods can effectively\nalleviate the problem of lack of training data in supervised learning, they\nalso introduce noise into the data, and still cannot fundamentally solve the\nlong-tail distribution problem of the training instances. In order to enable\nthe neural network to learn new knowledge through few instances like humans,\nthis work focuses on few-shot relation classification (FSRC), where a\nclassifier should generalize to new classes that have not been seen in the\ntraining set, given only a number of samples for each class. To make full use\nof the existing information and get a better feature representation for each\ninstance, we propose to encode each class prototype in an adaptive way from two\naspects. First, based on the prototypical networks, we propose an adaptive\nmixture mechanism to add label words to the representation of the class\nprototype, which, to the best of our knowledge, is the first attempt to\nintegrate the label information into features of the support samples of each\nclass so as to get more interactive class prototypes. Second, to more\nreasonably measure the distances between samples of each category, we introduce\na loss function for joint representation learning to encode each support\ninstance in an adaptive manner. Extensive experiments have been conducted on\nFewRel under different few-shot (FS) settings, and the results show that the\nproposed adaptive prototypical networks with label words and joint\nrepresentation learning has not only achieved significant improvements in\naccuracy, but also increased the generalization ability of few-shot RC models.", "published": "2021-01-10 11:25:42", "link": "http://arxiv.org/abs/2101.03526v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Logic for a Mildly Context-Sensitive Fragment of the Lambek-Grishin\n  Calculus", "abstract": "While context-free grammars are characterized by a simple proof-theoretic\ngrammatical formalism namely categorial grammar and its logic the Lambek\ncalculus, no such characterizations were known for tree-adjoining grammars, and\neven for any mildly context-sensitive languages classes in the last forty years\ndespite some efforts. We settle this problem in this paper. On the basis of the\nexisting fragment of the Lambek-Grishin calculus which captures tree-adjoining\nlanguages, we present a logic called HLG: a proof-theoretic characterization of\ntree-adjoining languages based on the Lambek-Grishin calculus restricted to\nHyperedge-replacement grammar with rank two studied by Moot. HLG is defined in\ndisplay calculus with cut-admissibility. Several new techniques are introduced\nfor the proofs, such as purely structural connectives, usefulness, and a\ngraph-theoretic argument on proof nets for HLG.", "published": "2021-01-10 22:28:05", "link": "http://arxiv.org/abs/2101.03634v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT & Family Eat Word Salad: Experiments with Text Understanding", "abstract": "In this paper, we study the response of large models from the BERT family to\nincoherent inputs that should confuse any model that claims to understand\nnatural language. We define simple heuristics to construct such examples. Our\nexperiments show that state-of-the-art models consistently fail to recognize\nthem as ill-formed, and instead produce high confidence predictions on them. As\na consequence of this phenomenon, models trained on sentences with randomly\npermuted word order perform close to state-of-the-art models. To alleviate\nthese issues, we show that if models are explicitly trained to recognize\ninvalid inputs, they can be robust to such attacks without a drop in\nperformance.", "published": "2021-01-10 01:32:57", "link": "http://arxiv.org/abs/2101.03453v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detecting Hostile Posts using Relational Graph Convolutional Network", "abstract": "This work is based on the submission to the competition Hindi Constraint\nconducted by AAAI@2021 for detection of hostile posts in Hindi on social media\nplatforms. Here, a model is presented for detection and classification of\nhostile posts and further classify into fake, offensive, hate and defamation\nusing Relational Graph Convolutional Networks. Unlike other existing work, our\napproach is focused on using semantic meaning along with contextutal\ninformation for better classification. The results from AAAI@2021 indicates\nthat the proposed model is performing at par with Google's XLM-RoBERTa on the\ngiven dataset. Our best submission with RGCN achieves an F1 score of 0.97 (7th\nRank) on coarse-grained evaluation and achieved best performance on identifying\nfake posts. Among all submissions to the challenge, our classification system\nwith XLM-Roberta secured 2nd rank on fine-grained classification.", "published": "2021-01-10 06:50:22", "link": "http://arxiv.org/abs/2101.03485v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TIB's Visual Analytics Group at MediaEval '20: Detecting Fake News on\n  Corona Virus and 5G Conspiracy", "abstract": "Fake news on social media has become a hot topic of research as it negatively\nimpacts the discourse of real news in the public. Specifically, the ongoing\nCOVID-19 pandemic has seen a rise of inaccurate and misleading information due\nto the surrounding controversies and unknown details at the beginning of the\npandemic. The FakeNews task at MediaEval 2020 tackles this problem by creating\na challenge to automatically detect tweets containing misinformation based on\ntext and structure from Twitter follower network. In this paper, we present a\nsimple approach that uses BERT embeddings and a shallow neural network for\nclassifying tweets using only text, and discuss our findings and limitations of\nthe approach in text-based misinformation detection.", "published": "2021-01-10 11:52:17", "link": "http://arxiv.org/abs/2101.03529v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "A Heuristic-driven Ensemble Framework for COVID-19 Fake News Detection", "abstract": "The significance of social media has increased manifold in the past few\ndecades as it helps people from even the most remote corners of the world stay\nconnected. With the COVID-19 pandemic raging, social media has become more\nrelevant and widely used than ever before, and along with this, there has been\na resurgence in the circulation of fake news and tweets that demand immediate\nattention. In this paper, we describe our Fake News Detection system that\nautomatically identifies whether a tweet related to COVID-19 is \"real\" or\n\"fake\", as a part of CONSTRAINT COVID19 Fake News Detection in English\nchallenge. We have used an ensemble model consisting of pre-trained models that\nhas helped us achieve a joint 8th position on the leader board. We have\nachieved an F1-score of 0.9831 against a top score of 0.9869. Post completion\nof the competition, we have been able to drastically improve our system by\nincorporating a novel heuristic algorithm based on username handles and link\ndomains in tweets fetching an F1-score of 0.9883 and achieving state-of-the art\nresults on the given dataset.", "published": "2021-01-10 13:21:08", "link": "http://arxiv.org/abs/2101.03545v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cisco at AAAI-CAD21 shared task: Predicting Emphasis in Presentation\n  Slides using Contextualized Embeddings", "abstract": "This paper describes our proposed system for the AAAI-CAD21 shared task:\nPredicting Emphasis in Presentation Slides. In this specific task, given the\ncontents of a slide we are asked to predict the degree of emphasis to be laid\non each word in the slide. We propose 2 approaches to this problem including a\nBiLSTM-ELMo approach and a transformers based approach based on RoBERTa and\nXLNet architectures. We achieve a score of 0.518 on the evaluation leaderboard\nwhich ranks us 3rd and 0.543 on the post-evaluation leaderboard which ranks us\n1st at the time of writing the paper.", "published": "2021-01-10 10:43:12", "link": "http://arxiv.org/abs/2101.11422v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Summaformers @ LaySumm 20, LongSumm 20", "abstract": "Automatic text summarization has been widely studied as an important task in\nnatural language processing. Traditionally, various feature engineering and\nmachine learning based systems have been proposed for extractive as well as\nabstractive text summarization. Recently, deep learning based, specifically\nTransformer-based systems have been immensely popular. Summarization is a\ncognitively challenging task - extracting summary worthy sentences is\nlaborious, and expressing semantics in brief when doing abstractive\nsummarization is complicated. In this paper, we specifically look at the\nproblem of summarizing scientific research papers from multiple domains. We\ndifferentiate between two types of summaries, namely, (a) LaySumm: A very short\nsummary that captures the essence of the research paper in layman terms\nrestricting overtly specific technical jargon and (b) LongSumm: A much longer\ndetailed summary aimed at providing specific insights into various ideas\ntouched upon in the paper. While leveraging latest Transformer-based models,\nour systems are simple, intuitive and based on how specific paper sections\ncontribute to human summaries of the two types described above. Evaluations\nagainst gold standard summaries using ROUGE metrics prove the effectiveness of\nour approach. On blind test corpora, our system ranks first and third for the\nLongSumm and LaySumm tasks respectively.", "published": "2021-01-10 13:48:12", "link": "http://arxiv.org/abs/2101.03553v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
