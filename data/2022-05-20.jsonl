{"title": "Descartes: Generating Short Descriptions of Wikipedia Articles", "abstract": "Wikipedia is one of the richest knowledge sources on the Web today. In order\nto facilitate navigating, searching, and maintaining its content, Wikipedia's\nguidelines state that all articles should be annotated with a so-called short\ndescription indicating the article's topic (e.g., the short description of beer\nis \"Alcoholic drink made from fermented cereal grains\"). Nonetheless, a large\nfraction of articles (ranging from 10.2% in Dutch to 99.7% in Kazakh) have no\nshort description yet, with detrimental effects for millions of Wikipedia\nusers. Motivated by this problem, we introduce the novel task of automatically\ngenerating short descriptions for Wikipedia articles and propose Descartes, a\nmultilingual model for tackling it. Descartes integrates three sources of\ninformation to generate an article description in a target language: the text\nof the article in all its language versions, the already-existing descriptions\n(if any) of the article in other languages, and semantic type information\nobtained from a knowledge graph. We evaluate a Descartes model trained for\nhandling 25 languages simultaneously, showing that it beats baselines\n(including a strong translation-based baseline) and performs on par with\nmonolingual models tailored for specific languages. A human evaluation on three\nlanguages further shows that the quality of Descartes's descriptions is largely\nindistinguishable from that of human-written descriptions; e.g., 91.3% of our\nEnglish descriptions (vs. 92.1% of human-written descriptions) pass the bar for\ninclusion in Wikipedia, suggesting that Descartes is ready for production, with\nthe potential to support human editors in filling a major gap in today's\nWikipedia across languages.", "published": "2022-05-20 08:03:07", "link": "http://arxiv.org/abs/2205.10012v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transition-based Semantic Role Labeling with Pointer Networks", "abstract": "Semantic role labeling (SRL) focuses on recognizing the predicate-argument\nstructure of a sentence and plays a critical role in many natural language\nprocessing tasks such as machine translation and question answering.\nPractically all available methods do not perform full SRL, since they rely on\npre-identified predicates, and most of them follow a pipeline strategy, using\nspecific models for undertaking one or several SRL subtasks. In addition,\nprevious approaches have a strong dependence on syntactic information to\nachieve state-of-the-art performance, despite being syntactic trees equally\nhard to produce. These simplifications and requirements make the majority of\nSRL systems impractical for real-world applications. In this article, we\npropose the first transition-based SRL approach that is capable of completely\nprocessing an input sentence in a single left-to-right pass, with neither\nleveraging syntactic information nor resorting to additional modules. Thanks to\nour implementation based on Pointer Networks, full SRL can be accurately and\nefficiently done in $O(n^2)$, achieving the best performance to date on the\nmajority of languages from the CoNLL-2009 shared task.", "published": "2022-05-20 08:38:44", "link": "http://arxiv.org/abs/2205.10023v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Beyond the Granularity: Multi-Perspective Dialogue Collaborative\n  Selection for Dialogue State Tracking", "abstract": "In dialogue state tracking, dialogue history is a crucial material, and its\nutilization varies between different models. However, no matter how the\ndialogue history is used, each existing model uses its own consistent dialogue\nhistory during the entire state tracking process, regardless of which slot is\nupdated. Apparently, it requires different dialogue history to update different\nslots in different turns. Therefore, using consistent dialogue contents may\nlead to insufficient or redundant information for different slots, which\naffects the overall performance. To address this problem, we devise DiCoS-DST\nto dynamically select the relevant dialogue contents corresponding to each slot\nfor state updating. Specifically, it first retrieves turn-level utterances of\ndialogue history and evaluates their relevance to the slot from a combination\nof three perspectives: (1) its explicit connection to the slot name; (2) its\nrelevance to the current turn dialogue; (3) Implicit Mention Oriented\nReasoning. Then these perspectives are combined to yield a decision, and only\nthe selected dialogue contents are fed into State Generator, which explicitly\nminimizes the distracting information passed to the downstream state\nprediction. Experimental results show that our approach achieves new\nstate-of-the-art performance on MultiWOZ 2.1 and MultiWOZ 2.2, and achieves\nsuperior performance on multiple mainstream benchmark datasets (including\nSim-M, Sim-R, and DSTC2).", "published": "2022-05-20 10:08:45", "link": "http://arxiv.org/abs/2205.10059v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uzbek affix finite state machine for stemming", "abstract": "This work presents a morphological analyzer for the Uzbek language using a\nfinite state machine. The proposed methodology is a morphologic analysis of\nUzbek words by using an affix striping to find a root and without including any\nlexicon. This method helps to perform morphological analysis of words from a\nlarge amount of text at high speed as well as it is not required using of\nmemory for keeping vocabulary. According to Uzbek, an agglutinative language\ncan be designed with finite state machines (FSMs). In contrast to the previous\nworks, this study modeled the completed FSMs for all word classes by using the\nUzbek language's morphotactic rules in right to left order. This paper shows\nthe stages of this methodology including the classification of the affixes, the\ngeneration of the FSMs for each affix class, and the combination into a head\nmachine to make analysis a word.", "published": "2022-05-20 10:46:53", "link": "http://arxiv.org/abs/2205.10078v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to keep text private? A systematic review of deep learning methods\n  for privacy-preserving natural language processing", "abstract": "Deep learning (DL) models for natural language processing (NLP) tasks often\nhandle private data, demanding protection against breaches and disclosures.\nData protection laws, such as the European Union's General Data Protection\nRegulation (GDPR), thereby enforce the need for privacy. Although many\nprivacy-preserving NLP methods have been proposed in recent years, no\ncategories to organize them have been introduced yet, making it hard to follow\nthe progress of the literature. To close this gap, this article systematically\nreviews over sixty DL methods for privacy-preserving NLP published between 2016\nand 2020, covering theoretical foundations, privacy-enhancing technologies, and\nanalysis of their suitability for real-world scenarios. First, we introduce a\nnovel taxonomy for classifying the existing methods into three categories: data\nsafeguarding methods, trusted methods, and verification methods. Second, we\npresent an extensive summary of privacy threats, datasets for applications, and\nmetrics for privacy evaluation. Third, throughout the review, we describe\nprivacy issues in the NLP pipeline in a holistic view. Further, we discuss open\nchallenges in privacy-preserving NLP regarding data traceability, computation\noverhead, dataset size, the prevalence of human biases in embeddings, and the\nprivacy-utility tradeoff. Finally, this review presents future research\ndirections to guide successive research and development of privacy-preserving\nNLP models.", "published": "2022-05-20 11:29:44", "link": "http://arxiv.org/abs/2205.10095v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Visually-Augmented Language Modeling", "abstract": "Human language is grounded on multimodal knowledge including visual knowledge\nlike colors, sizes, and shapes. However, current large-scale pre-trained\nlanguage models rely on text-only self-supervised training with massive text\ndata, which precludes them from utilizing relevant visual information when\nnecessary. To address this, we propose a novel pre-training framework, named\nVaLM, to Visually-augment text tokens with retrieved relevant images for\nLanguage Modeling. Specifically, VaLM builds on a novel latent text-image\nalignment method via an image retrieval module to fetch corresponding images\ngiven a textual context. With the visually-augmented context, VaLM uses a\nvisual knowledge fusion layer to enable multimodal grounded language modeling\nby attending to both text context and visual knowledge in images. We evaluate\nVaLM on various visual knowledge-intensive commonsense reasoning tasks, which\nrequire visual information to excel. The experimental results illustrate that\nVaLM outperforms all strong language-only and vision-language baselines with\nsubstantial gains in reasoning object commonsense including color, size, and\nshape. Our code is available at https://github.com/Victorwz/VaLM.", "published": "2022-05-20 13:41:12", "link": "http://arxiv.org/abs/2205.10178v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prototypical Calibration for Few-shot Learning of Language Models", "abstract": "In-context learning of GPT-like models has been recognized as fragile across\ndifferent hand-crafted templates, and demonstration permutations. In this work,\nwe propose prototypical calibration to adaptively learn a more robust decision\nboundary for zero- and few-shot classification, instead of greedy decoding.\nConcretely, our method first adopts Gaussian mixture distribution to estimate\nthe prototypical clusters for all categories. Then we assign each cluster to\nthe corresponding label by solving a weighted bipartite matching problem. Given\nan example, its prediction is calibrated by the likelihood of prototypical\nclusters. Experimental results show that prototypical calibration yields a\nsubstantial improvement on a diverse set of tasks. Extensive analysis across\ndifferent scales also indicates that our method calibrates the decision\nboundary as expected, greatly improving the robustness of GPT to templates,\npermutations, and class imbalance.", "published": "2022-05-20 13:50:07", "link": "http://arxiv.org/abs/2205.10183v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Trade-off between Redundancy and Local Coherence in Summarization", "abstract": "Extractive summaries are usually presented as lists of sentences with no\nexpected cohesion between them and with plenty of redundant information if not\naccounted for. In this paper, we investigate the trade-offs incurred when\naiming to control for inter-sentential cohesion and redundancy in extracted\nsummaries, and their impact on their informativeness. As case study, we focus\non the summarization of long, highly redundant documents and consider two\noptimization scenarios, reward-guided and with no supervision. In the\nreward-guided scenario, we compare systems that control for redundancy and\ncohesion during sentence scoring. In the unsupervised scenario, we introduce\ntwo systems that aim to control all three properties -- informativeness,\nredundancy, and cohesion -- in a principled way. Both systems implement a\npsycholinguistic theory that simulates how humans keep track of relevant\ncontent units and how cohesion and non-redundancy constraints are applied in\nshort-term memory during reading. Extensive automatic and human evaluations\nreveal that systems optimizing for -- among other properties -- cohesion are\ncapable of better organizing content in summaries compared to systems that\noptimize only for redundancy, while maintaining comparable informativeness. We\nfind that the proposed unsupervised systems manage to extract highly cohesive\nsummaries across varying levels of document redundancy, although sacrificing\ninformativeness in the process. Finally, we lay evidence as to how simulated\ncognitive processes impact the trade-off between the analyzed summary\nproperties.", "published": "2022-05-20 14:10:28", "link": "http://arxiv.org/abs/2205.10192v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Task-Oriented Dialogue Generation with Contrastive Pre-training\n  and Adversarial Filtering", "abstract": "Data artifacts incentivize machine learning models to learn non-transferable\ngeneralizations by taking advantage of shortcuts in the data, and there is\ngrowing evidence that data artifacts play a role for the strong results that\ndeep learning models achieve in recent natural language processing benchmarks.\nIn this paper, we focus on task-oriented dialogue and investigate whether\npopular datasets such as MultiWOZ contain such data artifacts. We found that by\nonly keeping frequent phrases in the training examples, state-of-the-art models\nperform similarly compared to the variant trained with full data, suggesting\nthey exploit these spurious correlations to solve the task. Motivated by this,\nwe propose a contrastive learning based framework to encourage the model to\nignore these cues and focus on learning generalisable patterns. We also\nexperiment with adversarial filtering to remove \"easy\" training instances so\nthat the model would focus on learning from the \"harder\" instances. We conduct\na number of generalization experiments -- e.g., cross-domain/dataset and\nadversarial tests -- to assess the robustness of our approach and found that it\nworks exceptionally well.", "published": "2022-05-20 03:13:02", "link": "http://arxiv.org/abs/2205.10363v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi2WOZ: A Robust Multilingual Dataset and Conversational Pretraining\n  for Task-Oriented Dialog", "abstract": "Research on (multi-domain) task-oriented dialog (TOD) has predominantly\nfocused on the English language, primarily due to the shortage of robust TOD\ndatasets in other languages, preventing the systematic investigation of\ncross-lingual transfer for this crucial NLP application area. In this work, we\nintroduce Multi2WOZ, a new multilingual multi-domain TOD dataset, derived from\nthe well-established English dataset MultiWOZ, that spans four typologically\ndiverse languages: Chinese, German, Arabic, and Russian. In contrast to\nconcurrent efforts, Multi2WOZ contains gold-standard dialogs in target\nlanguages that are directly comparable with development and test portions of\nthe English dataset, enabling reliable and comparative estimates of\ncross-lingual transfer performance for TOD. We then introduce a new framework\nfor multilingual conversational specialization of pretrained language models\n(PrLMs) that aims to facilitate cross-lingual transfer for arbitrary downstream\nTOD tasks. Using such conversational PrLMs specialized for concrete target\nlanguages, we systematically benchmark a number of zero-shot and few-shot\ncross-lingual transfer approaches on two standard TOD tasks: Dialog State\nTracking and Response Retrieval. Our experiments show that, in most setups, the\nbest performance entails the combination of (I) conversational specialization\nin the target language and (ii) few-shot transfer for the concrete TOD task.\nMost importantly, we show that our conversational specialization in the target\nlanguage allows for an exceptionally sample-efficient few-shot transfer for\ndownstream TOD tasks.", "published": "2022-05-20 18:35:38", "link": "http://arxiv.org/abs/2205.10400v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Searching for PETs: Using Distributional and Sentiment-Based Methods to\n  Find Potentially Euphemistic Terms", "abstract": "This paper presents a linguistically driven proof of concept for finding\npotentially euphemistic terms, or PETs. Acknowledging that PETs tend to be\ncommonly used expressions for a certain range of sensitive topics, we make use\nof distributional similarities to select and filter phrase candidates from a\nsentence and rank them using a set of simple sentiment-based metrics. We\npresent the results of our approach tested on a corpus of sentences containing\neuphemisms, demonstrating its efficacy for detecting single and multi-word PETs\nfrom a broad range of topics. We also discuss future potential for\nsentiment-based methods on this task.", "published": "2022-05-20 22:21:21", "link": "http://arxiv.org/abs/2205.10451v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-training Transformer Models with Sentence-Level Objectives for\n  Answer Sentence Selection", "abstract": "An important task for designing QA systems is answer sentence selection\n(AS2): selecting the sentence containing (or constituting) the answer to a\nquestion from a set of retrieved relevant documents. In this paper, we propose\nthree novel sentence-level transformer pre-training objectives that incorporate\nparagraph-level semantics within and across documents, to improve the\nperformance of transformers for AS2, and mitigate the requirement of large\nlabeled datasets. Specifically, the model is tasked to predict whether: (i) two\nsentences are extracted from the same paragraph, (ii) a given sentence is\nextracted from a given paragraph, and (iii) two paragraphs are extracted from\nthe same document. Our experiments on three public and one industrial AS2\ndatasets demonstrate the empirical superiority of our pre-trained transformers\nover baseline models such as RoBERTa and ELECTRA for AS2.", "published": "2022-05-20 22:39:00", "link": "http://arxiv.org/abs/2205.10455v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KERPLE: Kernelized Relative Positional Embedding for Length\n  Extrapolation", "abstract": "Relative positional embeddings (RPE) have received considerable attention\nsince RPEs effectively model the relative distance among tokens and enable\nlength extrapolation. We propose KERPLE, a framework that generalizes relative\nposition embedding for extrapolation by kernelizing positional differences. We\nachieve this goal using conditionally positive definite (CPD) kernels, a class\nof functions known for generalizing distance metrics. To maintain the inner\nproduct interpretation of self-attention, we show that a CPD kernel can be\ntransformed into a PD kernel by adding a constant offset. This offset is\nimplicitly absorbed in the Softmax normalization during self-attention. The\ndiversity of CPD kernels allows us to derive various RPEs that enable length\nextrapolation in a principled way. Experiments demonstrate that the logarithmic\nvariant achieves excellent extrapolation performance on three large language\nmodeling datasets. Our implementation and pretrained checkpoints are released\nat https://github.com/chijames/KERPLE.git.", "published": "2022-05-20 01:25:57", "link": "http://arxiv.org/abs/2205.09921v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SALTED: A Framework for SAlient Long-Tail Translation Error Detection", "abstract": "Traditional machine translation (MT) metrics provide an average measure of\ntranslation quality that is insensitive to the long tail of behavioral problems\nin MT. Examples include translation of numbers, physical units, dropped content\nand hallucinations. These errors, which occur rarely and unpredictably in\nNeural Machine Translation (NMT), greatly undermine the reliability of\nstate-of-the-art MT systems. Consequently, it is important to have visibility\ninto these problems during model development. Towards this direction, we\nintroduce SALTED, a specifications-based framework for behavioral testing of MT\nmodels that provides fine-grained views of salient long-tail errors, permitting\ntrustworthy visibility into previously invisible problems. At the core of our\napproach is the development of high-precision detectors that flag errors (or\nalternatively, verify output correctness) between a source sentence and a\nsystem output. We demonstrate that such detectors could be used not just to\nidentify salient long-tail errors in MT systems, but also for higher-recall\nfiltering of the training data, fixing targeted errors with model fine-tuning\nin NMT and generating novel data for metamorphic testing to elicit further bugs\nin models.", "published": "2022-05-20 06:45:07", "link": "http://arxiv.org/abs/2205.09988v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Extreme Parameter Compression for Pre-trained Language Models", "abstract": "Recent work explored the potential of large-scale Transformer-based\npre-trained models, especially Pre-trained Language Models (PLMs) in natural\nlanguage processing. This raises many concerns from various perspectives, e.g.,\nfinancial costs and carbon emissions. Compressing PLMs like BERT with\nnegligible performance loss for faster inference and cheaper deployment has\nattracted much attention. In this work, we aim to explore larger compression\nratios for PLMs, among which tensor decomposition is a potential but\nunder-investigated one. Two decomposition and reconstruction protocols are\nfurther proposed to improve the effectiveness and efficiency during\ncompression. Our compressed BERT with ${1}/{7}$ parameters in Transformer\nlayers performs on-par with, sometimes slightly better than the original BERT\nin GLUE benchmark. A tiny version achieves $96.7\\%$ performance of BERT-base\nwith $ {1}/{48} $ encoder parameters (i.e., less than 2M parameters excluding\nthe embedding layer) and $2.7 \\times$ faster on inference. To show that the\nproposed method is orthogonal to existing compression methods like knowledge\ndistillation, we also explore the benefit of the proposed method on a distilled\nBERT.", "published": "2022-05-20 09:16:55", "link": "http://arxiv.org/abs/2205.10036v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Progressive Class Semantic Matching for Semi-supervised Text\n  Classification", "abstract": "Semi-supervised learning is a promising way to reduce the annotation cost for\ntext-classification. Combining with pre-trained language models (PLMs), e.g.,\nBERT, recent semi-supervised learning methods achieved impressive performance.\nIn this work, we further investigate the marriage between semi-supervised\nlearning and a pre-trained language model. Unlike existing approaches that\nutilize PLMs only for model parameter initialization, we explore the inherent\ntopic matching capability inside PLMs for building a more powerful\nsemi-supervised learning approach. Specifically, we propose a joint\nsemi-supervised learning process that can progressively build a standard\n$K$-way classifier and a matching network for the input text and the Class\nSemantic Representation (CSR). The CSR will be initialized from the given\nlabeled sentences and progressively updated through the training process. By\nmeans of extensive experiments, we show that our method can not only bring\nremarkable improvement to baselines, but also overall be more stable, and\nachieves state-of-the-art performance in semi-supervised text classification.", "published": "2022-05-20 13:59:03", "link": "http://arxiv.org/abs/2205.10189v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Heterformer: Transformer-based Deep Node Representation Learning on\n  Heterogeneous Text-Rich Networks", "abstract": "Representation learning on networks aims to derive a meaningful vector\nrepresentation for each node, thereby facilitating downstream tasks such as\nlink prediction, node classification, and node clustering. In heterogeneous\ntext-rich networks, this task is more challenging due to (1) presence or\nabsence of text: Some nodes are associated with rich textual information, while\nothers are not; (2) diversity of types: Nodes and edges of multiple types form\na heterogeneous network structure. As pretrained language models (PLMs) have\ndemonstrated their effectiveness in obtaining widely generalizable text\nrepresentations, a substantial amount of effort has been made to incorporate\nPLMs into representation learning on text-rich networks. However, few of them\ncan jointly consider heterogeneous structure (network) information as well as\nrich textual semantic information of each node effectively. In this paper, we\npropose Heterformer, a Heterogeneous Network-Empowered Transformer that\nperforms contextualized text encoding and heterogeneous structure encoding in a\nunified model. Specifically, we inject heterogeneous structure information into\neach Transformer layer when encoding node texts. Meanwhile, Heterformer is\ncapable of characterizing node/edge type heterogeneity and encoding nodes with\nor without texts. We conduct comprehensive experiments on three tasks (i.e.,\nlink prediction, node classification, and node clustering) on three large-scale\ndatasets from different domains, where Heterformer outperforms competitive\nbaselines significantly and consistently.", "published": "2022-05-20 16:26:39", "link": "http://arxiv.org/abs/2205.10282v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lossless Acceleration for Seq2seq Generation with Aggressive Decoding", "abstract": "We study lossless acceleration for seq2seq generation with a novel decoding\nalgorithm -- Aggressive Decoding. Unlike the previous efforts (e.g.,\nnon-autoregressive decoding) speeding up seq2seq generation at the cost of\nquality loss, our approach aims to yield the identical (or better) generation\ncompared with autoregressive decoding but in a significant speedup, achieved by\ninnovative cooperation of aggressive decoding and verification that are both\nefficient due to parallel computing.\n  We propose two Aggressive Decoding paradigms for 2 kinds of seq2seq tasks: 1)\nFor the seq2seq tasks whose inputs and outputs are highly similar (e.g.,\nGrammatical Error Correction), we propose Input-guided Aggressive Decoding\n(IAD) that aggressively copies from the input sentence as drafted decoded\ntokens to verify in parallel; 2) For other general seq2seq tasks (e.g., Machine\nTranslation), we propose Generalized Aggressive Decoding (GAD) that first\nemploys an additional non-autoregressive decoding model for aggressive decoding\nand then verifies in parallel in the autoregressive manner.\n  We test Aggressive Decoding on the most popular 6-layer Transformer model on\nGPU in multiple seq2seq tasks: 1) For IAD, we show that it can introduce a\n7x-9x speedup for the Transformer in Grammatical Error Correction and Text\nSimplification tasks with the identical results as greedy decoding; 2) For GAD,\nwe observe a 3x-5x speedup with the identical or even better quality in two\nimportant seq2seq tasks: Machine Translation and Abstractive Summarization.\nMoreover, Aggressive Decoding can benefit even more from stronger computing\ndevices that are better at parallel computing. Given the lossless quality as\nwell as significant and promising speedup, we believe Aggressive Decoding may\npotentially evolve into a de facto standard for efficient and lossless seq2seq\ngeneration in the near future.", "published": "2022-05-20 17:59:00", "link": "http://arxiv.org/abs/2205.10350v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multilingual Normalization of Temporal Expressions with Masked Language\n  Models", "abstract": "The detection and normalization of temporal expressions is an important task\nand preprocessing step for many applications. However, prior work on\nnormalization is rule-based, which severely limits the applicability in\nreal-world multilingual settings, due to the costly creation of new rules. We\npropose a novel neural method for normalizing temporal expressions based on\nmasked language modeling. Our multilingual method outperforms prior rule-based\nsystems in many languages, and in particular, for low-resource languages with\nperformance improvements of up to 33 F1 on average compared to the state of the\nart.", "published": "2022-05-20 18:34:23", "link": "http://arxiv.org/abs/2205.10399v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Forecasting COVID-19 Caseloads Using Unsupervised Embedding Clusters of\n  Social Media Posts", "abstract": "We present a novel approach incorporating transformer-based language models\ninto infectious disease modelling. Text-derived features are quantified by\ntracking high-density clusters of sentence-level representations of Reddit\nposts within specific US states' COVID-19 subreddits. We benchmark these\nclustered embedding features against features extracted from other high-quality\ndatasets. In a threshold-classification task, we show that they outperform all\nother feature types at predicting upward trend signals, a significant result\nfor infectious disease modelling in areas where epidemiological data is\nunreliable. Subsequently, in a time-series forecasting task we fully utilise\nthe predictive power of the caseload and compare the relative strengths of\nusing different supplementary datasets as covariate feature sets in a\ntransformer-based time-series model.", "published": "2022-05-20 18:59:04", "link": "http://arxiv.org/abs/2205.10408v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Down and Across: Introducing Crossword-Solving as a New NLP Benchmark", "abstract": "Solving crossword puzzles requires diverse reasoning capabilities, access to\na vast amount of knowledge about language and the world, and the ability to\nsatisfy the constraints imposed by the structure of the puzzle. In this work,\nwe introduce solving crossword puzzles as a new natural language understanding\ntask. We release the specification of a corpus of crossword puzzles collected\nfrom the New York Times daily crossword spanning 25 years and comprised of a\ntotal of around nine thousand puzzles. These puzzles include a diverse set of\nclues: historic, factual, word meaning, synonyms/antonyms, fill-in-the-blank,\nabbreviations, prefixes/suffixes, wordplay, and cross-lingual, as well as clues\nthat depend on the answers to other clues. We separately release the\nclue-answer pairs from these puzzles as an open-domain question answering\ndataset containing over half a million unique clue-answer pairs. For the\nquestion answering task, our baselines include several sequence-to-sequence and\nretrieval-based generative models. We also introduce a non-parametric\nconstraint satisfaction baseline for solving the entire crossword puzzle.\nFinally, we propose an evaluation framework which consists of several\ncomplementary performance metrics.", "published": "2022-05-20 21:16:44", "link": "http://arxiv.org/abs/2205.10442v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Am I No Good? Towards Detecting Perceived Burdensomeness and Thwarted\n  Belongingness from Suicide Notes", "abstract": "The World Health Organization (WHO) has emphasized the importance of\nsignificantly accelerating suicide prevention efforts to fulfill the United\nNations' Sustainable Development Goal (SDG) objective of 2030. In this paper,\nwe present an end-to-end multitask system to address a novel task of detection\nof two interpersonal risk factors of suicide, Perceived Burdensomeness (PB) and\nThwarted Belongingness (TB) from suicide notes. We also introduce a manually\ntranslated code-mixed suicide notes corpus, CoMCEASE-v2.0, based on the\nbenchmark CEASE-v2.0 dataset, annotated with temporal orientation, PB and TB\nlabels. We exploit the temporal orientation and emotion information in the\nsuicide notes to boost overall performance. For comprehensive evaluation of our\nproposed method, we compare it to several state-of-the-art approaches on the\nexisting CEASE-v2.0 dataset and the newly announced CoMCEASE-v2.0 dataset.\nEmpirical evaluation suggests that temporal and emotional information can\nsubstantially improve the detection of PB and TB.", "published": "2022-05-20 06:31:08", "link": "http://arxiv.org/abs/2206.06141v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Translating Hanja Historical Documents to Contemporary Korean and\n  English", "abstract": "The Annals of Joseon Dynasty (AJD) contain the daily records of the Kings of\nJoseon, the 500-year kingdom preceding the modern nation of Korea. The Annals\nwere originally written in an archaic Korean writing system, `Hanja', and were\ntranslated into Korean from 1968 to 1993. The resulting translation was however\ntoo literal and contained many archaic Korean words; thus, a new expert\ntranslation effort began in 2012. Since then, the records of only one king have\nbeen completed in a decade. In parallel, expert translators are working on\nEnglish translation, also at a slow pace and produced only one king's records\nin English so far. Thus, we propose H2KE, a neural machine translation model,\nthat translates historical documents in Hanja to more easily understandable\nKorean and to English. Built on top of multilingual neural machine translation,\nH2KE learns to translate a historical document written in Hanja, from both a\nfull dataset of outdated Korean translation and a small dataset of more\nrecently translated contemporary Korean and English. We compare our method\nagainst two baselines: a recent model that simultaneously learns to restore and\ntranslate Hanja historical document and a Transformer based model trained only\non newly translated corpora. The experiments reveal that our method\nsignificantly outperforms the baselines in terms of BLEU scores for both\ncontemporary Korean and English translations. We further conduct extensive\nhuman evaluation which shows that our translation is preferred over the\noriginal expert translations by both experts and non-expert Korean speakers.", "published": "2022-05-20 08:25:11", "link": "http://arxiv.org/abs/2205.10019v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Understanding and Mitigating the Uncertainty in Zero-Shot Translation", "abstract": "Zero-shot translation is a promising direction for building a comprehensive\nmultilingual neural machine translation~(MNMT) system. However, its quality is\nstill not satisfactory due to off-target issues. In this paper, we aim to\nunderstand and alleviate the off-target issues from the perspective of\nuncertainty in zero-shot translation. By carefully examining the translation\noutput and model confidence, we identify two uncertainties that are responsible\nfor the off-target issues, namely, extrinsic data uncertainty and intrinsic\nmodel uncertainty. Based on the observations, we propose two lightweight and\ncomplementary approaches to denoise the training data for model training and\nexplicitly penalize the off-target translations by unlikelihood training during\nmodel training. Extensive experiments on both balanced and imbalanced datasets\nshow that our approaches significantly improve the performance of zero-shot\ntranslation over strong MNMT baselines.", "published": "2022-05-20 10:29:46", "link": "http://arxiv.org/abs/2205.10068v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semi-self-supervised Automated ICD Coding", "abstract": "Clinical Text Notes (CTNs) contain physicians' reasoning process, written in\nan unstructured free text format, as they examine and interview patients. In\nrecent years, several studies have been published that provide evidence for the\nutility of machine learning for predicting doctors' diagnoses from CTNs, a task\nknown as ICD coding. Data annotation is time consuming, particularly when a\ndegree of specialization is needed, as is the case for medical data. This paper\npresents a method of augmenting a sparsely annotated dataset of Icelandic CTNs\nwith a machine-learned imputation in a semi-self-supervised manner. We train a\nneural network on a small set of annotated CTNs and use it to extract clinical\nfeatures from a set of un-annotated CTNs. These clinical features consist of\nanswers to about a thousand potential questions that a physician might find the\nanswers to during a consultation of a patient. The features are then used to\ntrain a classifier for the diagnosis of certain types of diseases. We report\nthe results of an evaluation of this data augmentation method over three tiers\nof data availability to the physician. Our data augmentation method shows a\nsignificant positive effect which is diminished when clinical features from the\nexamination of the patient and diagnostics are made available. We recommend our\nmethod for augmenting scarce datasets for systems that take decisions based on\nclinical features that do not include examinations or tests.", "published": "2022-05-20 11:12:54", "link": "http://arxiv.org/abs/2205.10088v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "ClusterEA: Scalable Entity Alignment with Stochastic Training and\n  Normalized Mini-batch Similarities", "abstract": "Entity alignment (EA) aims at finding equivalent entities in different\nknowledge graphs (KGs). Embedding-based approaches have dominated the EA task\nin recent years. Those methods face problems that come from the geometric\nproperties of embedding vectors, including hubness and isolation. To solve\nthese geometric problems, many normalization approaches have been adopted for\nEA. However, the increasing scale of KGs renders it hard for EA models to adopt\nthe normalization processes, thus limiting their usage in real-world\napplications. To tackle this challenge, we present ClusterEA, a general\nframework that is capable of scaling up EA models and enhancing their results\nby leveraging normalization methods on mini-batches with a high entity\nequivalent rate. ClusterEA contains three components to align entities between\nlarge-scale KGs, including stochastic training, ClusterSampler, and\nSparseFusion. It first trains a large-scale Siamese GNN for EA in a stochastic\nfashion to produce entity embeddings. Based on the embeddings, a novel\nClusterSampler strategy is proposed for sampling highly overlapped\nmini-batches. Finally, ClusterEA incorporates SparseFusion, which normalizes\nlocal and global similarity and then fuses all similarity matrices to obtain\nthe final similarity matrix. Extensive experiments with real-life datasets on\nEA benchmarks offer insight into the proposed framework, and suggest that it is\ncapable of outperforming the state-of-the-art scalable EA framework by up to 8\ntimes in terms of Hits@1.", "published": "2022-05-20 17:29:50", "link": "http://arxiv.org/abs/2205.10312v2", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.DB"}
{"title": "Modernizing Open-Set Speech Language Identification", "abstract": "While most modern speech Language Identification methods are closed-set, we\nwant to see if they can be modified and adapted for the open-set problem. When\nswitching to the open-set problem, the solution gains the ability to reject an\naudio input when it fails to match any of our known language options. We tackle\nthe open-set task by adapting two modern-day state-of-the-art approaches to\nclosed-set language identification: the first using a CRNN with attention and\nthe second using a TDNN. In addition to enhancing our input feature embeddings\nusing MFCCs, log spectral features, and pitch, we will be attempting two\napproaches to out-of-set language detection: one using thresholds, and the\nother essentially performing a verification task. We will compare both the\nperformance of the TDNN and the CRNN, as well as our detection approaches.", "published": "2022-05-20 18:28:16", "link": "http://arxiv.org/abs/2205.10397v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Current Trends and Approaches in Synonyms Extraction: Potential\n  Adaptation to Arabic", "abstract": "Extracting synonyms from dictionaries or corpora is gaining special attention\nas synonyms play an important role in improving NLP application performance.\nThis paper presents a survey of the different approaches and trends used in\nautomatically extracting the synonyms. These approaches can be divided into\nfour main categories. The first approach is to find the Synonyms using a\ntranslation graph. The second approach is to discover new transition pairs such\nas (Arabic-English) (English-France) then (Arabic-France). The third approach\nis to construct new WordNets by exploring synonymy graphs, and the fourth\napproach is to find similar words from corpora using Deep Learning methods,\nsuch as word embeddings and recently BERT models. The paper also presents a\ncomparative analysis between these approaches and highlights potential\nadaptation to generate synonyms automatically in the Arabic language as future\nwork.", "published": "2022-05-20 19:05:10", "link": "http://arxiv.org/abs/2205.10412v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Evaluating and Inducing Personality in Pre-trained Language Models", "abstract": "Standardized and quantified evaluation of machine behaviors is a crux of\nunderstanding LLMs. In this study, we draw inspiration from psychometric\nstudies by leveraging human personality theory as a tool for studying machine\nbehaviors. Originating as a philosophical quest for human behaviors, the study\nof personality delves into how individuals differ in thinking, feeling, and\nbehaving. Toward building and understanding human-like social machines, we are\nmotivated to ask: Can we assess machine behaviors by leveraging human\npsychometric tests in a principled and quantitative manner? If so, can we\ninduce a specific personality in LLMs? To answer these questions, we introduce\nthe Machine Personality Inventory (MPI) tool for studying machine behaviors;\nMPI follows standardized personality tests, built upon the Big Five Personality\nFactors (Big Five) theory and personality assessment inventories. By\nsystematically evaluating LLMs with MPI, we provide the first piece of evidence\ndemonstrating the efficacy of MPI in studying LLMs behaviors. We further devise\na Personality Prompting (P^2) method to induce LLMs with specific personalities\nin a controllable way, capable of producing diverse and verifiable behaviors.\nWe hope this work sheds light on future studies by adopting personality as the\nessential indicator for various downstream tasks, and could further motivate\nresearch into equally intriguing human-like machine behaviors.", "published": "2022-05-20 07:32:57", "link": "http://arxiv.org/abs/2206.07550v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Audio Declipping with (Weighted) Analysis Social Sparsity", "abstract": "We develop the analysis (cosparse) variant of the popular audio declipping\nalgorithm of Siedenburg et al. (2014). Furthermore, we extend both the old and\nthe new variants by the possibility of weighting the time-frequency\ncoefficients. We examine the audio reconstruction performance of several\ncombinations of weights and shrinkage operators. The weights are shown to\nimprove the reconstruction quality in some cases; however, the best scores\nachieved by the non-weighted methods are not surpassed with the help of\nweights. Yet, the analysis Empirical Wiener (EW) shrinkage was able to reach\nthe quality of a computationally more expensive competitor, the Persistent\nEmpirical Wiener (PEW). Moreover, the proposed analysis variant incorporating\nPEW slightly outperforms the synthesis counterpart in terms of an auditorily\nmotivated metric.", "published": "2022-05-20 14:41:15", "link": "http://arxiv.org/abs/2205.10215v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "NeuralEcho: A Self-Attentive Recurrent Neural Network For Unified\n  Acoustic Echo Suppression And Speech Enhancement", "abstract": "Acoustic echo cancellation (AEC) plays an important role in the full-duplex\nspeech communication as well as the front-end speech enhancement for\nrecognition in the conditions when the loudspeaker plays back. In this paper,\nwe present an all-deep-learning framework that implicitly estimates the second\norder statistics of echo/noise and target speech, and jointly solves echo and\nnoise suppression through an attention based recurrent neural network. The\nproposed model outperforms the state-of-the-art joint echo cancellation and\nspeech enhancement method F-T-LSTM in terms of objective speech quality\nmetrics, speech recognition accuracy and model complexity. We show that this\nmodel can work with speaker embedding for better target speech enhancement and\nfurthermore develop a branch for automatic gain control (AGC) task to form an\nall-in-one front-end speech enhancement system.", "published": "2022-05-20 18:36:45", "link": "http://arxiv.org/abs/2205.10401v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "PaddleSpeech: An Easy-to-Use All-in-One Speech Toolkit", "abstract": "PaddleSpeech is an open-source all-in-one speech toolkit. It aims at\nfacilitating the development and research of speech processing technologies by\nproviding an easy-to-use command-line interface and a simple code structure.\nThis paper describes the design philosophy and core architecture of\nPaddleSpeech to support several essential speech-to-text and text-to-speech\ntasks. PaddleSpeech achieves competitive or state-of-the-art performance on\nvarious speech datasets and implements the most popular methods. It also\nprovides recipes and pretrained models to quickly reproduce the experimental\nresults in this paper. PaddleSpeech is publicly avaiable at\nhttps://github.com/PaddlePaddle/PaddleSpeech.", "published": "2022-05-20 10:14:53", "link": "http://arxiv.org/abs/2205.12007v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Estimation of binary time-frequency masks from ambient noise", "abstract": "We investigate the retrieval of a binary time-frequency mask from a few\nobservations of filtered white ambient noise. Confirming household wisdom in\nacoustic modeling, we show that this is possible by inspecting the average\nspectrogram of ambient noise. Specifically, we show that the lower quantile of\nthe average of $\\mathcal{O}(\\log(|\\Omega|/\\varepsilon))$ masked spectrograms is\nenough to identify a rather general mask $\\Omega$ with confidence at least\n$\\varepsilon$, up to shape details concentrated near the boundary of $\\Omega$.\nAs an application, the expected measure of the estimation error is dominated by\nthe perimeter of the time-frequency mask. The estimator requires no knowledge\nof the noise variance, and only a very qualitative profile of the filtering\nwindow, but no exact knowledge of it.", "published": "2022-05-20 14:28:35", "link": "http://arxiv.org/abs/2205.10205v3", "categories": ["cs.SD", "eess.AS", "math.FA", "math.ST", "stat.TH", "42C40, 46E10, 60H40, 47B35, 46E22"], "primary_category": "cs.SD"}
