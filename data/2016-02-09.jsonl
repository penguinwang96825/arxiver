{"title": "Toward Optimal Feature Selection in Naive Bayes for Text Categorization", "abstract": "Automated feature selection is important for text categorization to reduce\nthe feature size and to speed up the learning process of classifiers. In this\npaper, we present a novel and efficient feature selection framework based on\nthe Information Theory, which aims to rank the features with their\ndiscriminative capacity for classification. We first revisit two information\nmeasures: Kullback-Leibler divergence and Jeffreys divergence for binary\nhypothesis testing, and analyze their asymptotic properties relating to type I\nand type II errors of a Bayesian classifier. We then introduce a new divergence\nmeasure, called Jeffreys-Multi-Hypothesis (JMH) divergence, to measure\nmulti-distribution divergence for multi-class classification. Based on the\nJMH-divergence, we develop two efficient feature selection methods, termed\nmaximum discrimination ($MD$) and $MD-\\chi^2$ methods, for text categorization.\nThe promising results of extensive experiments demonstrate the effectiveness of\nthe proposed approaches.", "published": "2016-02-09 03:43:21", "link": "http://arxiv.org/abs/1602.02850v1", "categories": ["stat.ML", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "stat.ML"}
{"title": "A Convolutional Attention Network for Extreme Summarization of Source\n  Code", "abstract": "Attention mechanisms in neural networks have proved useful for problems in\nwhich the input and output do not have fixed dimension. Often there exist\nfeatures that are locally translation invariant and would be valuable for\ndirecting the model's attention, but previous attentional architectures are not\nconstructed to learn such features specifically. We introduce an attentional\nneural network that employs convolution on the input tokens to detect local\ntime-invariant and long-range topical attention features in a context-dependent\nway. We apply this architecture to the problem of extreme summarization of\nsource code snippets into short, descriptive function name-like summaries.\nUsing those features, the model sequentially generates a summary by\nmarginalizing over two attention mechanisms: one that predicts the next summary\ntoken based on the attention weights of the input tokens and another that is\nable to copy a code token as-is directly into the summary. We demonstrate our\nconvolutional attention neural network's performance on 10 popular Java\nprojects showing that it achieves better performance compared to previous\nattentional mechanisms.", "published": "2016-02-09 14:36:49", "link": "http://arxiv.org/abs/1602.03001v2", "categories": ["cs.LG", "cs.CL", "cs.SE"], "primary_category": "cs.LG"}
