{"title": "Revisiting Generative Commonsense Reasoning: A Pre-Ordering Approach", "abstract": "Pre-trained models (PTMs) have lead to great improvements in natural language\ngeneration (NLG). However, it is still unclear how much commonsense knowledge\nthey possess. With the goal of evaluating commonsense knowledge of NLG models,\nrecent work has proposed the problem of generative commonsense reasoning, e.g.,\nto compose a logical sentence given a set of unordered concepts. Existing\napproaches to this problem hypothesize that PTMs lack sufficient parametric\nknowledge for this task, which can be overcome by introducing external\nknowledge or task-specific pre-training objectives. Different from this trend,\nwe argue that PTM's inherent ability for generative commonsense reasoning is\nunderestimated due to the order-agnostic property of its input. In particular,\nwe hypothesize that the order of the input concepts can affect the PTM's\nability to utilize its commonsense knowledge. To this end, we propose a\npre-ordering approach to elaborately manipulate the order of the given concepts\nbefore generation. Experiments show that our approach can outperform the more\nsophisticated models that have access to a lot of external data and resources.", "published": "2022-05-26 06:36:53", "link": "http://arxiv.org/abs/2205.13183v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Other Roles Matter! Enhancing Role-Oriented Dialogue Summarization via\n  Role Interactions", "abstract": "Role-oriented dialogue summarization is to generate summaries for different\nroles in the dialogue, e.g., merchants and consumers. Existing methods handle\nthis task by summarizing each role's content separately and thus are prone to\nignore the information from other roles. However, we believe that other roles'\ncontent could benefit the quality of summaries, such as the omitted information\nmentioned by other roles. Therefore, we propose a novel role interaction\nenhanced method for role-oriented dialogue summarization. It adopts cross\nattention and decoder self-attention interactions to interactively acquire\nother roles' critical information. The cross attention interaction aims to\nselect other roles' critical dialogue utterances, while the decoder\nself-attention interaction aims to obtain key information from other roles'\nsummaries. Experimental results have shown that our proposed method\nsignificantly outperforms strong baselines on two public role-oriented dialogue\nsummarization datasets. Extensive analyses have demonstrated that other roles'\ncontent could help generate summaries with more complete semantics and correct\ntopic structures.", "published": "2022-05-26 06:58:02", "link": "http://arxiv.org/abs/2205.13190v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Target-aware Abstractive Related Work Generation with Contrastive\n  Learning", "abstract": "The related work section is an important component of a scientific paper,\nwhich highlights the contribution of the target paper in the context of the\nreference papers. Authors can save their time and effort by using the\nautomatically generated related work section as a draft to complete the final\nrelated work. Most of the existing related work section generation methods rely\non extracting off-the-shelf sentences to make a comparative discussion about\nthe target work and the reference papers. However, such sentences need to be\nwritten in advance and are hard to obtain in practice. Hence, in this paper, we\npropose an abstractive target-aware related work generator (TAG), which can\ngenerate related work sections consisting of new sentences. Concretely, we\nfirst propose a target-aware graph encoder, which models the relationships\nbetween reference papers and the target paper with target-centered attention\nmechanisms. In the decoding process, we propose a hierarchical decoder that\nattends to the nodes of different levels in the graph with keyphrases as\nsemantic indicators. Finally, to generate a more informative related work, we\npropose multi-level contrastive optimization objectives, which aim to maximize\nthe mutual information between the generated related work with the references\nand minimize that with non-references. Extensive experiments on two public\nscholar datasets show that the proposed model brings substantial improvements\nover several strong baselines in terms of automatic and tailored human\nevaluations.", "published": "2022-05-26 13:20:51", "link": "http://arxiv.org/abs/2205.13339v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Keywords and Instances: A Hierarchical Contrastive Learning Framework\n  Unifying Hybrid Granularities for Text Generation", "abstract": "Contrastive learning has achieved impressive success in generation tasks to\nmilitate the \"exposure bias\" problem and discriminatively exploit the different\nquality of references. Existing works mostly focus on contrastive learning on\nthe instance-level without discriminating the contribution of each word, while\nkeywords are the gist of the text and dominant the constrained mapping\nrelationships. Hence, in this work, we propose a hierarchical contrastive\nlearning mechanism, which can unify hybrid granularities semantic meaning in\nthe input text. Concretely, we first propose a keyword graph via contrastive\ncorrelations of positive-negative pairs to iteratively polish the keyword\nrepresentations. Then, we construct intra-contrasts within instance-level and\nkeyword-level, where we assume words are sampled nodes from a sentence\ndistribution. Finally, to bridge the gap between independent contrast levels\nand tackle the common contrast vanishing problem, we propose an inter-contrast\nmechanism that measures the discrepancy between contrastive keyword nodes\nrespectively to the instance distribution. Experiments demonstrate that our\nmodel outperforms competitive baselines on paraphrasing, dialogue generation,\nand storytelling tasks.", "published": "2022-05-26 13:26:03", "link": "http://arxiv.org/abs/2205.13346v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Abstractive Dialogue Summarization with Word Graphs and POV\n  Conversion", "abstract": "We advance the state-of-the-art in unsupervised abstractive dialogue\nsummarization by utilizing multi-sentence compression graphs. Starting from\nwell-founded assumptions about word graphs, we present simple but reliable\npath-reranking and topic segmentation schemes. Robustness of our method is\ndemonstrated on datasets across multiple domains, including meetings,\ninterviews, movie scripts, and day-to-day conversations. We also identify\npossible avenues to augment our heuristic-based system with deep learning. We\nopen-source our code, to provide a strong, reproducible baseline for future\nresearch into unsupervised dialogue summarization.", "published": "2022-05-26 02:18:12", "link": "http://arxiv.org/abs/2205.13108v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Understanding Metrics for Paraphrasing", "abstract": "Paraphrase generation is a difficult problem. This is not only because of the\nlimitations in text generation capabilities but also due that to the lack of a\nproper definition of what qualifies as a paraphrase and corresponding metrics\nto measure how good it is. Metrics for evaluation of paraphrasing quality is an\non going research problem. Most of the existing metrics in use having been\nborrowed from other tasks do not capture the complete essence of a good\nparaphrase, and often fail at borderline-cases. In this work, we propose a\nnovel metric $ROUGE_P$ to measure the quality of paraphrases along the\ndimensions of adequacy, novelty and fluency. We also provide empirical evidence\nto show that the current natural language generation metrics are insufficient\nto measure these desired properties of a good paraphrase. We look at paraphrase\nmodel fine-tuning and generation from the lens of metrics to gain a deeper\nunderstanding of what it takes to generate and evaluate a good paraphrase.", "published": "2022-05-26 03:03:16", "link": "http://arxiv.org/abs/2205.13119v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Reinforcement Adaptation for Class-Imbalanced Text\n  Classification", "abstract": "Class imbalance naturally exists when train and test models in different\ndomains. Unsupervised domain adaptation (UDA) augments model performance with\nonly accessible annotations from the source domain and unlabeled data from the\ntarget domain. However, existing state-of-the-art UDA models learn\ndomain-invariant representations and evaluate primarily on class-balanced data\nacross domains. In this work, we propose an unsupervised domain adaptation\napproach via reinforcement learning that jointly leverages feature variants and\nimbalanced labels across domains. We experiment with the text classification\ntask for its easily accessible datasets and compare the proposed method with\nfive baselines. Experiments on three datasets prove that our proposed method\ncan effectively learn robust domain-invariant representations and successfully\nadapt text classifiers on imbalanced classes over domains. The code is\navailable at https://github.com/woqingdoua/ImbalanceClass.", "published": "2022-05-26 04:05:51", "link": "http://arxiv.org/abs/2205.13139v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Grammar Detection for Sentiment Analysis through Improved Viterbi\n  Algorithm", "abstract": "Grammar Detection, also referred to as Parts of Speech Tagging of raw text,\nis considered an underlying building block of the various Natural Language\nProcessing pipelines like named entity recognition, question answering, and\nsentiment analysis. In short, forgiven a sentence, Parts of Speech tagging is\nthe task of specifying and tagging each word of a sentence with nouns, verbs,\nadjectives, adverbs, and more. Sentiment Analysis may well be a procedure\naccustomed to determining if a given sentence's emotional tone is neutral,\npositive or negative. To assign polarity scores to the thesis or entities\nwithin phrase, in-text analysis and analytics, machine learning and natural\nlanguage processing, approaches are incorporated. This Sentiment Analysis using\nPOS tagger helps us urge a summary of the broader public over a specific topic.\nFor this, we are using the Viterbi algorithm, Hidden Markov Model, Constraint\nbased Viterbi algorithm for POS tagging. By comparing the accuracies, we select\nthe foremost accurate result of the model for Sentiment Analysis for\ndetermining the character of the sentence.", "published": "2022-05-26 04:40:31", "link": "http://arxiv.org/abs/2205.13148v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Dependency Grammar for Fine-Grained Offensive Language\n  Detection using Graph Convolutional Networks", "abstract": "The last few years have witnessed an exponential rise in the propagation of\noffensive text on social media. Identification of this text with high precision\nis crucial for the well-being of society. Most of the existing approaches tend\nto give high toxicity scores to innocuous statements (e.g., \"I am a gay man\").\nThese false positives result from over-generalization on the training data\nwhere specific terms in the statement may have been used in a pejorative sense\n(e.g., \"gay\"). Emphasis on such words alone can lead to discrimination against\nthe classes these systems are designed to protect. In this paper, we address\nthe problem of offensive language detection on Twitter, while also detecting\nthe type and the target of the offence. We propose a novel approach called\nSyLSTM, which integrates syntactic features in the form of the dependency parse\ntree of a sentence and semantic features in the form of word embeddings into a\ndeep learning architecture using a Graph Convolutional Network. Results show\nthat the proposed approach significantly outperforms the state-of-the-art BERT\nmodel with orders of magnitude fewer number of parameters.", "published": "2022-05-26 05:27:50", "link": "http://arxiv.org/abs/2205.13164v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Document Vectors Using Cosine Similarity Revisited", "abstract": "The current state-of-the-art test accuracy (97.42\\%) on the IMDB movie\nreviews dataset was reported by \\citet{thongtan-phienthrakul-2019-sentiment}\nand achieved by the logistic regression classifier trained on the Document\nVectors using Cosine Similarity (DV-ngrams-cosine) proposed in their paper and\nthe Bag-of-N-grams (BON) vectors scaled by Naive Bayesian weights. While large\npre-trained Transformer-based models have shown SOTA results across many\ndatasets and tasks, the aforementioned model has not been surpassed by them,\ndespite being much simpler and pre-trained on the IMDB dataset only.\n  In this paper, we describe an error in the evaluation procedure of this\nmodel, which was found when we were trying to analyze its excellent performance\non the IMDB dataset. We further show that the previously reported test accuracy\nof 97.42\\% is invalid and should be corrected to 93.68\\%. We also analyze the\nmodel performance with different amounts of training data (subsets of the IMDB\ndataset) and compare it to the Transformer-based RoBERTa model. The results\nshow that while RoBERTa has a clear advantage for larger training sets, the\nDV-ngrams-cosine performs better than RoBERTa when the labelled training set is\nvery small (10 or 20 documents). Finally, we introduce a sub-sampling scheme\nbased on Naive Bayesian weights for the training process of the\nDV-ngrams-cosine, which leads to faster training and better quality.", "published": "2022-05-26 13:36:54", "link": "http://arxiv.org/abs/2205.13357v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Jointly Learning Span Extraction and Sequence Labeling for Information\n  Extraction from Business Documents", "abstract": "This paper introduces a new information extraction model for business\ndocuments. Different from prior studies which only base on span extraction or\nsequence labeling, the model takes into account advantage of both span\nextraction and sequence labeling. The combination allows the model to deal with\nlong documents with sparse information (the small amount of extracted\ninformation). The model is trained end-to-end to jointly optimize the two tasks\nin a unified manner. Experimental results on four business datasets in English\nand Japanese show that the model achieves promising results and is\nsignificantly faster than the normal span-based extraction method. The code is\nalso available.", "published": "2022-05-26 15:37:24", "link": "http://arxiv.org/abs/2205.13434v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semantic Parsing of Interpage Relations", "abstract": "Page-level analysis of documents has been a topic of interest in digitization\nefforts, and multimodal approaches have been applied to both classification and\npage stream segmentation. In this work, we focus on capturing finer semantic\nrelations between pages of a multi-page document. To this end, we formalize the\ntask as semantic parsing of interpage relations and we propose an end-to-end\napproach for interpage dependency extraction, inspired by the dependency\nparsing literature. We further design a multi-task training approach to jointly\noptimize for page embeddings to be used in segmentation, classification, and\nparsing of the page dependencies using textual and visual features extracted\nfrom the pages. Moreover, we also combine the features from two modalities to\nobtain multimodal page embeddings. To the best of our knowledge, this is the\nfirst study to extract rich semantic interpage relations from multi-page\ndocuments. Our experimental results show that the proposed method increased LAS\nby 41 percentage points for semantic parsing, increased accuracy by 33\npercentage points for page stream segmentation, and 45 percentage points for\npage classification over a naive baseline.", "published": "2022-05-26 17:50:43", "link": "http://arxiv.org/abs/2205.13530v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Learning Dialogue Representations from Consecutive Utterances", "abstract": "Learning high-quality dialogue representations is essential for solving a\nvariety of dialogue-oriented tasks, especially considering that dialogue\nsystems often suffer from data scarcity. In this paper, we introduce Dialogue\nSentence Embedding (DSE), a self-supervised contrastive learning method that\nlearns effective dialogue representations suitable for a wide range of dialogue\ntasks. DSE learns from dialogues by taking consecutive utterances of the same\ndialogue as positive pairs for contrastive learning. Despite its simplicity,\nDSE achieves significantly better representation capability than other dialogue\nrepresentation and universal sentence representation models. We evaluate DSE on\nfive downstream dialogue tasks that examine dialogue representation at\ndifferent semantic granularities. Experiments in few-shot and zero-shot\nsettings show that DSE outperforms baselines by a large margin. For example, it\nachieves 13% average performance improvement over the strongest unsupervised\nbaseline in 1-shot intent classification on 6 datasets. We also provide\nanalyses on the benefits and limitations of our model.", "published": "2022-05-26 18:15:13", "link": "http://arxiv.org/abs/2205.13568v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Clinical Dialogue Transcription Error Correction using Seq2Seq Models", "abstract": "Good communication is critical to good healthcare. Clinical dialogue is a\nconversation between health practitioners and their patients, with the explicit\ngoal of obtaining and sharing medical information. This information contributes\nto medical decision-making regarding the patient and plays a crucial role in\ntheir healthcare journey. The reliance on note taking and manual scribing\nprocesses are extremely inefficient and leads to manual transcription errors\nwhen digitizing notes. Automatic Speech Recognition (ASR) plays a significant\nrole in speech-to-text applications, and can be directly used as a text\ngenerator in conversational applications. However, recording clinical dialogue\npresents a number of general and domain-specific challenges. In this paper, we\npresent a seq2seq learning approach for ASR transcription error correction of\nclinical dialogues. We introduce a new Gastrointestinal Clinical Dialogue (GCD)\nDataset which was gathered by healthcare professionals from a NHS Inflammatory\nBowel Disease clinic and use this in a comparative study with four commercial\nASR systems. Using self-supervision strategies, we fine-tune a seq2seq model on\na mask-filling task using a domain-specific PubMed dataset which we have shared\npublicly for future research. The BART model fine-tuned for mask-filling was\nable to correct transcription errors and achieve lower word error rates for\nthree out of four commercial ASR outputs.", "published": "2022-05-26 18:27:17", "link": "http://arxiv.org/abs/2205.13572v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Differentially Private Decoding in Large Language Models", "abstract": "Recent large-scale natural language processing (NLP) systems use a\npre-trained Large Language Model (LLM) on massive and diverse corpora as a\nheadstart. In practice, the pre-trained model is adapted to a wide array of\ntasks via fine-tuning on task-specific datasets. LLMs, while effective, have\nbeen shown to memorize instances of training data thereby potentially revealing\nprivate information processed during pre-training. The potential leakage might\nfurther propagate to the downstream tasks for which LLMs are fine-tuned. On the\nother hand, privacy-preserving algorithms usually involve retraining from\nscratch, which is prohibitively expensive for LLMs. In this work, we propose a\nsimple, easy to interpret, and computationally lightweight perturbation\nmechanism to be applied to an already trained model at the decoding stage. Our\nperturbation mechanism is model-agnostic and can be used in conjunction with\nany LLM. We provide theoretical analysis showing that the proposed mechanism is\ndifferentially private, and experimental results showing a privacy-utility\ntrade-off.", "published": "2022-05-26 20:50:58", "link": "http://arxiv.org/abs/2205.13621v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Quark: Controllable Text Generation with Reinforced Unlearning", "abstract": "Large-scale language models often learn behaviors that are misaligned with\nuser expectations. Generated text may contain offensive or toxic language,\ncontain significant repetition, or be of a different sentiment than desired by\nthe user. We consider the task of unlearning these misalignments by fine-tuning\nthe language model on signals of what not to do. We introduce Quantized Reward\nKonditioning (Quark), an algorithm for optimizing a reward function that\nquantifies an (un)wanted property, while not straying too far from the original\nmodel. Quark alternates between (i) collecting samples with the current\nlanguage model, (ii) sorting them into quantiles based on reward, with each\nquantile identified by a reward token prepended to the language model's input,\nand (iii) using a standard language modeling loss on samples from each quantile\nconditioned on its reward token, while remaining nearby the original language\nmodel via a KL-divergence penalty. By conditioning on a high-reward token at\ngeneration time, the model generates text that exhibits less of the unwanted\nproperty. For unlearning toxicity, negative sentiment, and repetition, our\nexperiments show that Quark outperforms both strong baselines and\nstate-of-the-art reinforcement learning methods like PPO (Schulman et al.\n2017), while relying only on standard language modeling primitives.", "published": "2022-05-26 21:11:51", "link": "http://arxiv.org/abs/2205.13636v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Contextual Adapters for Personalized Speech Recognition in Neural\n  Transducers", "abstract": "Personal rare word recognition in end-to-end Automatic Speech Recognition\n(E2E ASR) models is a challenge due to the lack of training data. A standard\nway to address this issue is with shallow fusion methods at inference time.\nHowever, due to their dependence on external language models and the\ndeterministic approach to weight boosting, their performance is limited. In\nthis paper, we propose training neural contextual adapters for personalization\nin neural transducer based ASR models. Our approach can not only bias towards\nuser-defined words, but also has the flexibility to work with pretrained ASR\nmodels. Using an in-house dataset, we demonstrate that contextual adapters can\nbe applied to any general purpose pretrained ASR model to improve\npersonalization. Our method outperforms shallow fusion, while retaining\nfunctionality of the pretrained models by not altering any of the model\nweights. We further show that the adapter style training is superior to\nfull-fine-tuning of the ASR models on datasets with user-defined content.", "published": "2022-05-26 22:46:28", "link": "http://arxiv.org/abs/2205.13660v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fine-grained Image Captioning with CLIP Reward", "abstract": "Modern image captioning models are usually trained with text similarity\nobjectives. However, since reference captions in public datasets often describe\nthe most salient common objects, models trained with text similarity objectives\ntend to ignore specific and detailed aspects of an image that distinguish it\nfrom others. Toward more descriptive and distinctive caption generation, we\npropose using CLIP, a multimodal encoder trained on huge image-text pairs from\nweb, to calculate multimodal similarity and use it as a reward function. We\nalso propose a simple finetuning strategy of the CLIP text encoder to improve\ngrammar that does not require extra text annotation. This completely eliminates\nthe need for reference captions during the reward computation. To\ncomprehensively evaluate descriptive captions, we introduce FineCapEval, a new\ndataset for caption evaluation with fine-grained criteria: overall, background,\nobject, relations. In our experiments on text-to-image retrieval and\nFineCapEval, the proposed CLIP-guided model generates more distinctive captions\nthan the CIDEr-optimized model. We also show that our unsupervised grammar\nfinetuning of the CLIP text encoder alleviates the degeneration problem of the\nnaive CLIP reward. Lastly, we show human analysis where the annotators strongly\nprefer the CLIP reward to the CIDEr and MLE objectives according to various\ncriteria. Code and Data: https://github.com/j-min/CLIP-Caption-Reward", "published": "2022-05-26 02:46:09", "link": "http://arxiv.org/abs/2205.13115v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Federated Split BERT for Heterogeneous Text Classification", "abstract": "Pre-trained BERT models have achieved impressive performance in many natural\nlanguage processing (NLP) tasks. However, in many real-world situations,\ntextual data are usually decentralized over many clients and unable to be\nuploaded to a central server due to privacy protection and regulations.\nFederated learning (FL) enables multiple clients collaboratively to train a\nglobal model while keeping the local data privacy. A few researches have\ninvestigated BERT in federated learning setting, but the problem of performance\nloss caused by heterogeneous (e.g., non-IID) data over clients remain\nunder-explored. To address this issue, we propose a framework, FedSplitBERT,\nwhich handles heterogeneous data and decreases the communication cost by\nsplitting the BERT encoder layers into local part and global part. The local\npart parameters are trained by the local client only while the global part\nparameters are trained by aggregating gradients of multiple clients. Due to the\nsheer size of BERT, we explore a quantization method to further reduce the\ncommunication cost with minimal performance loss. Our framework is ready-to-use\nand compatible to many existing federated learning algorithms, including\nFedAvg, FedProx and FedAdam. Our experiments verify the effectiveness of the\nproposed framework, which outperforms baseline methods by a significant margin,\nwhile FedSplitBERT with quantization can reduce the communication cost by\n$11.9\\times$.", "published": "2022-05-26 12:21:57", "link": "http://arxiv.org/abs/2205.13299v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Federated Non-negative Matrix Factorization for Short Texts Topic\n  Modeling with Mutual Information", "abstract": "Non-negative matrix factorization (NMF) based topic modeling is widely used\nin natural language processing (NLP) to uncover hidden topics of short text\ndocuments. Usually, training a high-quality topic model requires large amount\nof textual data. In many real-world scenarios, customer textual data should be\nprivate and sensitive, precluding uploading to data centers. This paper\nproposes a Federated NMF (FedNMF) framework, which allows multiple clients to\ncollaboratively train a high-quality NMF based topic model with locally stored\ndata. However, standard federated learning will significantly undermine the\nperformance of topic models in downstream tasks (e.g., text classification)\nwhen the data distribution over clients is heterogeneous. To alleviate this\nissue, we further propose FedNMF+MI, which simultaneously maximizes the mutual\ninformation (MI) between the count features of local texts and their topic\nweight vectors to mitigate the performance degradation. Experimental results\nshow that our FedNMF+MI methods outperform Federated Latent Dirichlet\nAllocation (FedLDA) and the FedNMF without MI methods for short texts by a\nsignificant margin on both coherence score and classification F1 score.", "published": "2022-05-26 12:22:34", "link": "http://arxiv.org/abs/2205.13300v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Causal Inference for Explainable Automatic Program Repair", "abstract": "Deep learning models have made significant progress in automatic program\nrepair. However, the black-box nature of these methods has restricted their\npractical applications. To address this challenge, this paper presents an\ninterpretable approach for program repair based on sequence-to-sequence models\nwith causal inference and our method is called CPR, short for causal program\nrepair. Our CPR can generate explanations in the process of decision making,\nwhich consists of groups of causally related input-output tokens. Firstly, our\nmethod infers these relations by querying the model with inputs disturbed by\ndata augmentation. Secondly, it generates a graph over tokens from the\nresponses and solves a partitioning problem to select the most relevant\ncomponents. The experiments on four programming languages (Java, C, Python, and\nJavaScript) show that CPR can generate causal graphs for reasonable\ninterpretations and boost the performance of bug fixing in automatic program\nrepair.", "published": "2022-05-26 13:25:33", "link": "http://arxiv.org/abs/2205.13342v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Your Transformer May Not be as Powerful as You Expect", "abstract": "Relative Positional Encoding (RPE), which encodes the relative distance\nbetween any pair of tokens, is one of the most successful modifications to the\noriginal Transformer. As far as we know, theoretical understanding of the\nRPE-based Transformers is largely unexplored. In this work, we mathematically\nanalyze the power of RPE-based Transformers regarding whether the model is\ncapable of approximating any continuous sequence-to-sequence functions. One may\nnaturally assume the answer is in the affirmative -- RPE-based Transformers are\nuniversal function approximators. However, we present a negative result by\nshowing there exist continuous sequence-to-sequence functions that RPE-based\nTransformers cannot approximate no matter how deep and wide the neural network\nis. One key reason lies in that most RPEs are placed in the softmax attention\nthat always generates a right stochastic matrix. This restricts the network\nfrom capturing positional information in the RPEs and limits its capacity. To\novercome the problem and make the model more powerful, we first present\nsufficient conditions for RPE-based Transformers to achieve universal function\napproximation. With the theoretical guidance, we develop a novel attention\nmodule, called Universal RPE-based (URPE) Attention, which satisfies the\nconditions. Therefore, the corresponding URPE-based Transformers become\nuniversal function approximators. Extensive experiments covering typical\narchitectures and tasks demonstrate that our model is parameter-efficient and\ncan achieve superior performance to strong baselines in a wide range of\napplications. The code will be made publicly available at\nhttps://github.com/lsj2408/URPE.", "published": "2022-05-26 14:51:30", "link": "http://arxiv.org/abs/2205.13401v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Global Normalization for Streaming Speech Recognition in a Modular\n  Framework", "abstract": "We introduce the Globally Normalized Autoregressive Transducer (GNAT) for\naddressing the label bias problem in streaming speech recognition. Our solution\nadmits a tractable exact computation of the denominator for the sequence-level\nnormalization. Through theoretical and empirical results, we demonstrate that\nby switching to a globally normalized model, the word error rate gap between\nstreaming and non-streaming speech-recognition models can be greatly reduced\n(by more than 50\\% on the Librispeech dataset). This model is developed in a\nmodular framework which encompasses all the common neural speech recognition\nmodels. The modularity of this framework enables controlled comparison of\nmodelling choices and creation of new models.", "published": "2022-05-26 23:34:21", "link": "http://arxiv.org/abs/2205.13674v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Symbiotic Child Emotional Support with Social Robots and Temporal\n  Knowledge Graphs", "abstract": "In current youth-care programs, children with needs (mental health, family\nissues, learning disabilities, and autism) receive support from youth and\nfamily experts as one-to-one assistance at schools or hospitals. Occasionally,\nsocial robots have featured in such settings as support roles in a one-to-one\ninteraction with the child. In this paper, we suggest the development of a\nsymbiotic framework for real-time Emotional Support (ES) with social robots\nKnowledge Graphs (KG). By augmenting a domain-specific corpus from the\nliterature on ES for children (between the age of 8 and 12) and providing\nscenario-driven context including the history of events, we suggest developing\nan experimental knowledge-aware ES framework. The framework both guides the\nsocial robot in providing ES statements to the child and assists the expert in\ntracking and interpreting the child's emotional state and related events over\ntime.", "published": "2022-05-26 08:44:31", "link": "http://arxiv.org/abs/2205.13229v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.RO"}
{"title": "An enhanced Conv-TasNet model for speech separation using a speaker\n  distance-based loss function", "abstract": "This work addresses the problem of speech separation in the Spanish Language\nusing pre-trained deep learning models. As with many speech processing tasks,\nlarge databases in other languages different from English are scarce. Therefore\nthis work explores different training strategies using the Conv-TasNet model as\na benchmark. A scale-invariant signal distortion ratio (SI-SDR) metric value of\n9.9 dB was achieved for the best training strategy. Then, experimentally, we\nidentified an inverse relationship between the speakers' similarity and the\nmodel's performance, so an improved ConvTasNet architecture was proposed. The\nenhanced Conv-TasNet model uses pre-trained speech embeddings to add a\nbetween-speakers cosine similarity term in the cost function, yielding an\nSI-SDR of 10.6 dB. Lastly, final experiments regarding real-time deployment\nshow some drawbacks in the speakers' channel synchronization due to the need to\nprocess small speech segments where only one of the speakers appears.", "published": "2022-05-26 22:39:22", "link": "http://arxiv.org/abs/2205.13657v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Joint Training of Speech Enhancement and Self-supervised Model for\n  Noise-robust ASR", "abstract": "Speech enhancement (SE) is usually required as a front end to improve the\nspeech quality in noisy environments, while the enhanced speech might not be\noptimal for automatic speech recognition (ASR) systems due to speech\ndistortion. On the other hand, it was shown that self-supervised pre-training\nenables the utilization of a large amount of unlabeled noisy data, which is\nrather beneficial for the noise robustness of ASR. However, the potential of\nthe (optimal) integration of SE and self-supervised pre-training still remains\nunclear. In order to find an appropriate combination and reduce the impact of\nspeech distortion caused by SE, in this paper we therefore propose a joint\npre-training approach for the SE module and the self-supervised model. First,\nin the pre-training phase the original noisy waveform or the waveform obtained\nby SE is fed into the self-supervised model to learn the contextual\nrepresentation, where the quantified clean speech acts as the target. Second,\nwe propose a dual-attention fusion method to fuse the features of noisy and\nenhanced speeches, which can compensate the information loss caused by\nseparately using individual modules. Due to the flexible exploitation of\nclean/noisy/enhanced branches, the proposed method turns out to be a\ngeneralization of some existing noise-robust ASR models, e.g., enhanced\nwav2vec2.0. Finally, experimental results on both synthetic and real noisy\ndatasets show that the proposed joint training approach can improve the ASR\nperformance under various noisy settings, leading to a stronger noise\nrobustness.", "published": "2022-05-26 12:13:15", "link": "http://arxiv.org/abs/2205.13293v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DT-SV: A Transformer-based Time-domain Approach for Speaker Verification", "abstract": "Speaker verification (SV) aims to determine whether the speaker's identity of\na test utterance is the same as the reference speech. In the past few years,\nextracting speaker embeddings using deep neural networks for SV systems has\ngone mainstream. Recently, different attention mechanisms and Transformer\nnetworks have been explored widely in SV fields. However, utilizing the\noriginal Transformer in SV directly may have frame-level information waste on\noutput features, which could lead to restrictions on capacity and\ndiscrimination of speaker embeddings. Therefore, we propose an approach to\nderive utterance-level speaker embeddings via a Transformer architecture that\nuses a novel loss function named diffluence loss to integrate the feature\ninformation of different Transformer layers. Therein, the diffluence loss aims\nto aggregate frame-level features into an utterance-level representation, and\nit could be integrated into the Transformer expediently. Besides, we also\nintroduce a learnable mel-fbank energy feature extractor named time-domain\nfeature extractor that computes the mel-fbank features more precisely and\nefficiently than the standard mel-fbank extractor. Combining Diffluence loss\nand Time-domain feature extractor, we propose a novel Transformer-based\ntime-domain SV model (DT-SV) with faster training speed and higher accuracy.\nExperiments indicate that our proposed model can achieve better performance in\ncomparison with other models.", "published": "2022-05-26 09:36:26", "link": "http://arxiv.org/abs/2205.13249v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Let the paintings play", "abstract": "In this paper, we introduce a mathematical method to extract similarities\nbetween paintings and musical tracks. Our approach is based on the\ndigitalization of both paintings and musical tracks by means of finite\nexpansions in terms of orthogonal basis functions (with both Fourier and\nwavelet bases). The best fit between a specific painting and a sample of\nmusical tracks from a given composer is achieved via an $L^2$ projection upon a\nfinite-dimensional subspace. Several examples are provided for the analysis of\na collection of works of art by the Italian artist Marcello Morandini. Finally,\nwe have developed an original applet that implements the process above and\nwhich can be freely downloaded from the site\nhttps://github.com/pgerva/playing-paintings.git", "published": "2022-05-26 14:03:42", "link": "http://arxiv.org/abs/2206.14142v3", "categories": ["cs.MM", "cs.SD", "eess.AS", "42C40, 42B05, 41-04, 65T50, 65T60"], "primary_category": "cs.MM"}
