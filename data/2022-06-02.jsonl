{"title": "TSTR: Too Short to Represent, Summarize with Details! Intro-Guided\n  Extended Summary Generation", "abstract": "Many scientific papers such as those in arXiv and PubMed data collections\nhave abstracts with varying lengths of 50-1000 words and average length of\napproximately 200 words, where longer abstracts typically convey more\ninformation about the source paper. Up to recently, scientific summarization\nresearch has typically focused on generating short, abstract-like summaries\nfollowing the existing datasets used for scientific summarization. In domains\nwhere the source text is relatively long-form, such as in scientific documents,\nsuch summary is not able to go beyond the general and coarse overview and\nprovide salient information from the source document. The recent interest to\ntackle this problem motivated curation of scientific datasets, arXiv-Long and\nPubMed-Long, containing human-written summaries of 400-600 words, hence,\nproviding a venue for research in generating long/extended summaries. Extended\nsummaries facilitate a faster read while providing details beyond coarse\ninformation. In this paper, we propose TSTR, an extractive summarizer that\nutilizes the introductory information of documents as pointers to their salient\ninformation. The evaluations on two existing large-scale extended summarization\ndatasets indicate statistically significant improvement in terms of Rouge and\naverage Rouge (F1) scores (except in one case) as compared to strong baselines\nand state-of-the-art. Comprehensive human evaluations favor our generated\nextended summaries in terms of cohesion and completeness.", "published": "2022-06-02 02:45:31", "link": "http://arxiv.org/abs/2206.00847v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MentSum: A Resource for Exploring Summarization of Mental Health Online\n  Posts", "abstract": "Mental health remains a significant challenge of public health worldwide.\nWith increasing popularity of online platforms, many use the platforms to share\ntheir mental health conditions, express their feelings, and seek help from the\ncommunity and counselors. Some of these platforms, such as Reachout, are\ndedicated forums where the users register to seek help. Others such as Reddit\nprovide subreddits where the users publicly but anonymously post their mental\nhealth distress. Although posts are of varying length, it is beneficial to\nprovide a short, but informative summary for fast processing by the counselors.\nTo facilitate research in summarization of mental health online posts, we\nintroduce Mental Health Summarization dataset, MentSum, containing over 24k\ncarefully selected user posts from Reddit, along with their short user-written\nsummary (called TLDR) in English from 43 mental health subreddits. This\ndomain-specific dataset could be of interest not only for generating short\nsummaries on Reddit, but also for generating summaries of posts on the\ndedicated mental health forums such as Reachout. We further evaluate both\nextractive and abstractive state-of-the-art summarization baselines in terms of\nRouge scores, and finally conduct an in-depth human evaluation study of both\nuser-written and system-generated summaries, highlighting challenges in this\nresearch.", "published": "2022-06-02 03:08:34", "link": "http://arxiv.org/abs/2206.00856v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The ParlaSent-BCS dataset of sentiment-annotated parliamentary debates\n  from Bosnia-Herzegovina, Croatia, and Serbia", "abstract": "Expression of sentiment in parliamentary debates is deemed to be\nsignificantly different from that on social media or in product reviews. This\npaper adds to an emerging body of research on parliamentary debates with a\ndataset of sentences annotated for detection sentiment polarity in political\ndiscourse. We sample the sentences for annotation from the proceedings of three\nSoutheast European parliaments: Croatia, Bosnia-Herzegovina, and Serbia. A\nsix-level schema is applied to the data with the aim of training a\nclassification model for the detection of sentiment in parliamentary\nproceedings. Krippendorff's alpha measuring the inter-annotator agreement\nranges from 0.6 for the six-level annotation schema to 0.75 for the three-level\nschema and 0.83 for the two-level schema. Our initial experiments on the\ndataset show that transformer models perform significantly better than those\nusing a simpler architecture. Furthermore, regardless of the similarity of the\nthree languages, we observe differences in performance across different\nlanguages. Performing parliament-specific training and evaluation shows that\nthe main reason for the differing performance between parliaments seems to be\nthe different complexity of the automatic classification task, which is not\nobservable in annotator performance. Language distance does not seem to play\nany role neither in annotator nor in automatic classification performance. We\nrelease the dataset and the best-performing model under permissive licences.", "published": "2022-06-02 08:45:14", "link": "http://arxiv.org/abs/2206.00929v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MMTM: Multi-Tasking Multi-Decoder Transformer for Math Word Problems", "abstract": "Recently, quite a few novel neural architectures were derived to solve math\nword problems by predicting expression trees. These architectures varied from\nseq2seq models, including encoders leveraging graph relationships combined with\ntree decoders. These models achieve good performance on various MWPs datasets\nbut perform poorly when applied to an adversarial challenge dataset, SVAMP. We\npresent a novel model MMTM that leverages multi-tasking and multi-decoder\nduring pre-training. It creates variant tasks by deriving labels using\npre-order, in-order and post-order traversal of expression trees, and uses\ntask-specific decoders in a multi-tasking framework. We leverage transformer\narchitectures with lower dimensionality and initialize weights from RoBERTa\nmodel. MMTM model achieves better mathematical reasoning ability and\ngeneralisability, which we demonstrate by outperforming the best state of the\nart baseline models from Seq2Seq, GTS, and Graph2Tree with a relative\nimprovement of 19.4% on an adversarial challenge dataset SVAMP.", "published": "2022-06-02 19:48:36", "link": "http://arxiv.org/abs/2206.01268v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VL-BEiT: Generative Vision-Language Pretraining", "abstract": "We introduce a vision-language foundation model called VL-BEiT, which is a\nbidirectional multimodal Transformer learned by generative pretraining. Our\nminimalist solution conducts masked prediction on both monomodal and multimodal\ndata with a shared Transformer. Specifically, we perform masked vision-language\nmodeling on image-text pairs, masked language modeling on texts, and masked\nimage modeling on images. VL-BEiT is learned from scratch with one unified\npretraining task, one shared backbone, and one-stage training. Our method is\nconceptually simple and empirically effective. Experimental results show that\nVL-BEiT obtains strong results on various vision-language benchmarks, such as\nvisual question answering, visual reasoning, and image-text retrieval.\nMoreover, our method learns transferable visual features, achieving competitive\nperformance on image classification, and semantic segmentation.", "published": "2022-06-02 16:14:19", "link": "http://arxiv.org/abs/2206.01127v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Finding the Right Recipe for Low Resource Domain Adaptation in Neural\n  Machine Translation", "abstract": "General translation models often still struggle to generate accurate\ntranslations in specialized domains. To guide machine translation practitioners\nand characterize the effectiveness of domain adaptation methods under different\ndata availability scenarios, we conduct an in-depth empirical exploration of\nmonolingual and parallel data approaches to domain adaptation of pre-trained,\nthird-party, NMT models in settings where architecture change is impractical.\nWe compare data centric adaptation methods in isolation and combination. We\nstudy method effectiveness in very low resource (8k parallel examples) and\nmoderately low resource (46k parallel examples) conditions and propose an\nensemble approach to alleviate reductions in original domain translation\nquality. Our work includes three domains: consumer electronic, clinical, and\nbiomedical and spans four language pairs - Zh-En, Ja-En, Es-En, and Ru-En. We\nalso make concrete recommendations for achieving high in-domain performance and\nrelease our consumer electronic and medical domain datasets for all languages\nand make our code publicly available.", "published": "2022-06-02 16:38:33", "link": "http://arxiv.org/abs/2206.01137v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual\n  Question Answering", "abstract": "This paper revisits visual representation in knowledge-based visual question\nanswering (VQA) and demonstrates that using regional information in a better\nway can significantly improve the performance. While visual representation is\nextensively studied in traditional VQA, it is under-explored in knowledge-based\nVQA even though these two tasks share the common spirit, i.e., rely on visual\ninput to answer the question. Specifically, we observe that in most\nstate-of-the-art knowledge-based VQA methods: 1) visual features are extracted\neither from the whole image or in a sliding window manner for retrieving\nknowledge, and the important relationship within/among object regions is\nneglected; 2) visual features are not well utilized in the final answering\nmodel, which is counter-intuitive to some extent. Based on these observations,\nwe propose a new knowledge-based VQA method REVIVE, which tries to utilize the\nexplicit information of object regions not only in the knowledge retrieval\nstage but also in the answering model. The key motivation is that object\nregions and inherent relationship are important for knowledge-based VQA. We\nperform extensive experiments on the standard OK-VQA dataset and achieve new\nstate-of-the-art performance, i.e., 58.0% accuracy, surpassing previous\nstate-of-the-art method by a large margin (+3.6%). We also conduct detailed\nanalysis and show the necessity of regional information in different framework\ncomponents for knowledge-based VQA. Code is publicly available at\nhttps://github.com/yzleroy/REVIVE.", "published": "2022-06-02 17:59:56", "link": "http://arxiv.org/abs/2206.01201v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Multiset Version of Even-Odd Permutations Identity", "abstract": "In this paper, we give a new bijective proof of a multiset analogue of\neven-odd permutations identity. This multiset version is equivalent to the\noriginal coin arrangements lemma which is a key combinatorial lemma in the\nSherman's Proof of a conjecture of Feynman about an identity on paths in planar\ngraphs related to combinatorial solution of two dimensional Ising model in\nstatistical physics.", "published": "2022-06-02 20:23:44", "link": "http://arxiv.org/abs/2206.01291v1", "categories": ["math.CO", "cs.CL"], "primary_category": "math.CO"}
{"title": "Augmenting Scientific Creativity with Retrieval across Knowledge Domains", "abstract": "Exposure to ideas in domains outside a scientist's own may benefit her in\nreformulating existing research problems in novel ways and discovering new\napplication domains for existing solution ideas. While improved performance in\nscholarly search engines can help scientists efficiently identify relevant\nadvances in domains they may already be familiar with, it may fall short of\nhelping them explore diverse ideas \\textit{outside} such domains. In this paper\nwe explore the design of systems aimed at augmenting the end-user ability in\ncross-domain exploration with flexible query specification. To this end, we\ndevelop an exploratory search system in which end-users can select a portion of\ntext core to their interest from a paper abstract and retrieve papers that have\na high similarity to the user-selected core aspect but differ in terms of\ndomains. Furthermore, end-users can `zoom in' to specific domain clusters to\nretrieve more papers from them and understand nuanced differences within the\nclusters. Our case studies with scientists uncover opportunities and design\nimplications for systems aimed at facilitating cross-domain exploration and\ninspiration.", "published": "2022-06-02 22:55:51", "link": "http://arxiv.org/abs/2206.01328v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "BayesFormer: Transformer with Uncertainty Estimation", "abstract": "Transformer has become ubiquitous due to its dominant performance in various\nNLP and image processing tasks. However, it lacks understanding of how to\ngenerate mathematically grounded uncertainty estimates for transformer\narchitectures. Models equipped with such uncertainty estimates can typically\nimprove predictive performance, make networks robust, avoid over-fitting and\nused as acquisition function in active learning. In this paper, we introduce\nBayesFormer, a Transformer model with dropouts designed by Bayesian theory. We\nproposed a new theoretical framework to extend the approximate variational\ninference-based dropout to Transformer-based architectures. Through extensive\nexperiments, we validate the proposed architecture in four paradigms and show\nimprovements across the board: language modeling and classification,\nlong-sequence understanding, machine translation and acquisition function for\nactive learning.", "published": "2022-06-02 01:54:58", "link": "http://arxiv.org/abs/2206.00826v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Squeezeformer: An Efficient Transformer for Automatic Speech Recognition", "abstract": "The recently proposed Conformer model has become the de facto backbone model\nfor various downstream speech tasks based on its hybrid attention-convolution\narchitecture that captures both local and global features. However, through a\nseries of systematic studies, we find that the Conformer architecture's design\nchoices are not optimal. After re-examining the design choices for both the\nmacro and micro-architecture of Conformer, we propose Squeezeformer which\nconsistently outperforms the state-of-the-art ASR models under the same\ntraining schemes. In particular, for the macro-architecture, Squeezeformer\nincorporates (i) the Temporal U-Net structure which reduces the cost of the\nmulti-head attention modules on long sequences, and (ii) a simpler block\nstructure of multi-head attention or convolution modules followed up by\nfeed-forward module instead of the Macaron structure proposed in Conformer.\nFurthermore, for the micro-architecture, Squeezeformer (i) simplifies the\nactivations in the convolutional block, (ii) removes redundant Layer\nNormalization operations, and (iii) incorporates an efficient depthwise\ndown-sampling layer to efficiently sub-sample the input signal. Squeezeformer\nachieves state-of-the-art results of 7.5%, 6.5%, and 6.0% word-error-rate (WER)\non LibriSpeech test-other without external language models, which are 3.1%,\n1.4%, and 0.6% better than Conformer-CTC with the same number of FLOPs. Our\ncode is open-sourced and available online.", "published": "2022-06-02 06:06:29", "link": "http://arxiv.org/abs/2206.00888v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "NeuralSympCheck: A Symptom Checking and Disease Diagnostic Neural Model\n  with Logic Regularization", "abstract": "The symptom checking systems inquire users for their symptoms and perform a\nrapid and affordable medical assessment of their condition. The basic symptom\nchecking systems based on Bayesian methods, decision trees, or information gain\nmethods are easy to train and do not require significant computational\nresources. However, their drawbacks are low relevance of proposed symptoms and\ninsufficient quality of diagnostics. The best results on these tasks are\nachieved by reinforcement learning models. Their weaknesses are the difficulty\nof developing and training such systems and limited applicability to cases with\nlarge and sparse decision spaces. We propose a new approach based on the\nsupervised learning of neural models with logic regularization that combines\nthe advantages of the different methods. Our experiments on real and synthetic\ndata show that the proposed approach outperforms the best existing methods in\nthe accuracy of diagnosis when the number of diagnoses and symptoms is large.", "published": "2022-06-02 07:57:17", "link": "http://arxiv.org/abs/2206.00906v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Transfer Language Selection for Zero-Shot Cross-Lingual Abusive Language\n  Detection", "abstract": "We study the selection of transfer languages for automatic abusive language\ndetection. Instead of preparing a dataset for every language, we demonstrate\nthe effectiveness of cross-lingual transfer learning for zero-shot abusive\nlanguage detection. This way we can use existing data from higher-resource\nlanguages to build better detection systems for low-resource languages. Our\ndatasets are from seven different languages from three language families. We\nmeasure the distance between the languages using several language similarity\nmeasures, especially by quantifying the World Atlas of Language Structures. We\nshow that there is a correlation between linguistic similarity and classifier\nperformance. This discovery allows us to choose an optimal transfer language\nfor zero shot abusive language detection.", "published": "2022-06-02 09:53:15", "link": "http://arxiv.org/abs/2206.00962v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language and Culture Internalisation for Human-Like Autotelic AI", "abstract": "Building autonomous agents able to grow open-ended repertoires of skills\nacross their lives is a fundamental goal of artificial intelligence (AI). A\npromising developmental approach recommends the design of intrinsically\nmotivated agents that learn new skills by generating and pursuing their own\ngoals - autotelic agents. But despite recent progress, existing algorithms\nstill show serious limitations in terms of goal diversity, exploration,\ngeneralisation or skill composition. This perspective calls for the immersion\nof autotelic agents into rich socio-cultural worlds, an immensely important\nattribute of our environment that shapes human cognition but is mostly omitted\nin modern AI. Inspired by the seminal work of Vygotsky, we propose Vygotskian\nautotelic agents - agents able to internalise their interactions with others\nand turn them into cognitive tools. We focus on language and show how its\nstructure and informational content may support the development of new\ncognitive functions in artificial agents as it does in humans. We justify the\napproach by uncovering several examples of new artificial cognitive functions\nemerging from interactions between language and embodiment in recent works at\nthe intersection of deep reinforcement learning and natural language\nprocessing. Looking forward, we highlight future opportunities and challenges\nfor Vygotskian Autotelic AI research, including the use of language models as\ncultural models supporting artificial cognitive development.", "published": "2022-06-02 16:35:41", "link": "http://arxiv.org/abs/2206.01134v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Musical Instrument Recognition by XGBoost Combining Feature Fusion", "abstract": "Musical instrument classification is one of the focuses of Music Information\nRetrieval (MIR). In order to solve the problem of poor performance of current\nmusical instrument classification models, we propose a musical instrument\nclassification algorithm based on multi-channel feature fusion and XGBoost.\nBased on audio feature extraction and fusion of the dataset, the features are\ninput into the XGBoost model for training; secondly, we verified the superior\nperformance of the algorithm in the musical instrument classification task by\ncom-paring different feature combinations and several classical machine\nlearning models such as Naive Bayes. The algorithm achieves an accuracy of\n97.65% on the Medley-solos-DB dataset, outperforming existing models. The\nexperiments provide a reference for feature selection in feature engineering\nfor musical instrument classification.", "published": "2022-06-02 07:32:33", "link": "http://arxiv.org/abs/2206.00901v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Pronunciation Dictionary-Free Multilingual Speech Synthesis by Combining\n  Unsupervised and Supervised Phonetic Representations", "abstract": "This paper proposes a multilingual speech synthesis method which combines\nunsupervised phonetic representations (UPR) and supervised phonetic\nrepresentations (SPR) to avoid reliance on the pronunciation dictionaries of\ntarget languages. In this method, a pretrained wav2vec 2.0 model is adopted to\nextract UPRs and a language-independent automatic speech recognition (LI-ASR)\nmodel is built with a connectionist temporal classification (CTC) loss to\nextract segment-level SPRs from the audio data of target languages. Then, an\nacoustic model is designed, which first predicts UPRs and SPRs from texts\nseparately and then combines the predicted UPRs and SPRs to generate\nmel-spectrograms. The results of our experiments on six languages show that the\nproposed method outperformed the methods that directly predicted\nmel-spectrograms from character or phoneme sequences and the ablated models\nthat utilized only UPRs or SPRs.", "published": "2022-06-02 09:28:04", "link": "http://arxiv.org/abs/2206.00951v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-supervised Learning of Audio Representations from Audio-Visual Data\n  using Spatial Alignment", "abstract": "Learning from audio-visual data offers many possibilities to express\ncorrespondence between the audio and visual content, similar to the human\nperception that relates aural and visual information. In this work, we present\na method for self-supervised representation learning based on audio-visual\nspatial alignment (AVSA), a more sophisticated alignment task than the\naudio-visual correspondence (AVC). In addition to the correspondence, AVSA also\nlearns from the spatial location of acoustic and visual content. Based on\n360$^\\text{o}$ video and Ambisonics audio, we propose selection of visual\nobjects using object detection, and beamforming of the audio signal towards the\ndetected objects, attempting to learn the spatial alignment between objects and\nthe sound they produce. We investigate the use of spatial audio features to\nrepresent the audio input, and different audio formats: Ambisonics, mono, and\nstereo. Experimental results show a 10 $\\%$ improvement on AVSA for the first\norder ambisonics intensity vector (FOA-IV) in comparison with log-mel\nspectrogram features; the addition of object-oriented crops also brings\nsignificant performance increases for the human action recognition downstream\ntask. A number of audio-only downstream tasks are devised for testing the\neffectiveness of the learnt audio feature representation, obtaining performance\ncomparable to state-of-the-art methods on acoustic scene classification from\nambisonic and binaural audio.", "published": "2022-06-02 10:14:33", "link": "http://arxiv.org/abs/2206.00970v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Musical Arrow of Time -- The Role of Temporal Asymmetry in Music and\n  Its Organicist Implications", "abstract": "Adopting a performer-centric perspective, we frequently encounter two\nstatements: \"music flows\", and \"music is life-like\". This dissertation builds\non top of the two statements above, resulting in an exploration of the role of\ntemporal asymmetry in music (generalizing \"music flows\") and its relation to\nthe idea of organicism (generalizing \"music is life-like\"). We focus on two\naspects of temporal asymmetry. The first aspect concerns the vastly different\nepistemic mechanisms with which we obtain knowledge of the past and the future.\nA particular musical consequence follows: recurrence. The epistemic difference\nbetween the past and the future shapes our experience and interpretation of\nrecurring events in music. The second aspect concerns the arrow of time: the\nunambiguous ordering imposed on temporal events gives rise to the a priori\npointedness of time, rendering time asymmetrical and irreversible. A discussion\non thermodynamics informs us musically: the arrow of time effectuates itself in\nmusical forms by delaying the placement of the climax.\n  Organicism serves as a mediating topic, engaging with the concept of life as\nin organisms. On the one hand, organicism is related to temporal asymmetry in\nscience via a thermodynamical interpretation of life as entropy-reducing\nentities. On the other hand, organicism is a topic native to music via the\nuniversally acknowledged artistic idea that music should be interpreted as a\nvital force possessing volitional power. With organicism as a mediator, we\nbetter understand the role of temporal asymmetry in music. In particular, we\nview musical form as a process of expansion and elaboration analogous to\norganic growth. Finally, we present an organicist interpretation of delaying\nthe climax: viewing musical form as the result of organic growth, the arrow of\ntime translates to a preference for prepending structure over appending\nstructure.", "published": "2022-06-02 21:19:11", "link": "http://arxiv.org/abs/2206.01305v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Partitura: A Python Package for Symbolic Music Processing", "abstract": "Partitura is a lightweight Python package for handling symbolic musical\ninformation. It provides easy access to features commonly used in music\ninformation retrieval tasks, like note arrays (lists of timed pitched events)\nand 2D piano roll matrices, as well as other score elements such as time and\nkey signatures, performance directives, and repeat structures. Partitura can\nload musical scores (in MEI, MusicXML, Kern, and MIDI formats), MIDI\nperformances, and score-to-performance alignments. The package includes some\ntools for music analysis, such as automatic pitch spelling, key signature\nidentification, and voice separation. Partitura is an open-source project and\nis available at https://github.com/CPJKU/partitura/.", "published": "2022-06-02 14:39:32", "link": "http://arxiv.org/abs/2206.01071v1", "categories": ["cs.SD", "cs.DL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The match file format: Encoding Alignments between Scores and\n  Performances", "abstract": "This paper presents the specifications of match: a file format that extends a\nMIDI human performance with note-, beat-, and downbeat-level alignments to a\ncorresponding musical score. This enables advanced analyses of the performance\nthat are relevant for various tasks, such as expressive performance modeling,\nscore following, music transcription, and performer classification. The match\nfile includes a set of score-related descriptors that makes it usable also as a\nbare-bones score representation. For applications that require the use of\nstructural score elements (e.g., voices, parts, beams, slurs), the match file\ncan be easily combined with the symbolic score. To support the practical\napplication of our work, we release a corrected and upgraded version of the\nVienna4x22 dataset of scores and performances aligned with match files.", "published": "2022-06-02 15:33:25", "link": "http://arxiv.org/abs/2206.01104v1", "categories": ["cs.SD", "cs.DL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Detecting the Severity of Major Depressive Disorder from Speech: A Novel\n  HARD-Training Methodology", "abstract": "Major Depressive Disorder (MDD) is a common worldwide mental health issue\nwith high associated socioeconomic costs. The prediction and automatic\ndetection of MDD can, therefore, make a huge impact on society. Speech, as a\nnon-invasive, easy to collect signal, is a promising marker to aid the\ndiagnosis and assessment of MDD. In this regard, speech samples were collected\nas part of the Remote Assessment of Disease and Relapse in Major Depressive\nDisorder (RADAR-MDD) research programme. RADAR-MDD was an observational cohort\nstudy in which speech and other digital biomarkers were collected from a cohort\nof individuals with a history of MDD in Spain, United Kingdom and the\nNetherlands. In this paper, the RADAR-MDD speech corpus was taken as an\nexperimental framework to test the efficacy of a Sequence-to-Sequence model\nwith a local attention mechanism in a two-class depression severity\nclassification paradigm. Additionally, a novel training method, HARD-Training,\nis proposed. It is a methodology based on the selection of more ambiguous\nsamples for the model training, and inspired by the curriculum learning\nparadigm. HARD-Training was found to consistently improve - with an average\nincrement of 8.6% - the performance of our classifiers for both of two speech\nelicitation tasks used and each collection site of the RADAR-MDD speech corpus.\nWith this novel methodology, our Sequence-to-Sequence model was able to\neffectively detect MDD severity regardless of language. Finally, recognising\nthe need for greater awareness of potential algorithmic bias, we conduct an\nadditional analysis of our results separately for each gender.", "published": "2022-06-02 13:26:03", "link": "http://arxiv.org/abs/2206.01542v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "q-bio.QM"], "primary_category": "cs.SD"}
