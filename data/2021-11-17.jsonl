{"title": "Towards Interpretable and Reliable Reading Comprehension: A Pipeline\n  Model with Unanswerability Prediction", "abstract": "Multi-hop QA with annotated supporting facts, which is the task of reading\ncomprehension (RC) considering the interpretability of the answer, has been\nextensively studied. In this study, we define an interpretable reading\ncomprehension (IRC) model as a pipeline model with the capability of predicting\nunanswerable queries. The IRC model justifies the answer prediction by\nestablishing consistency between the predicted supporting facts and the actual\nrationale for interpretability. The IRC model detects unanswerable questions,\ninstead of outputting the answer forcibly based on the insufficient\ninformation, to ensure the reliability of the answer. We also propose an\nend-to-end training method for the pipeline RC model. To evaluate the\ninterpretability and the reliability, we conducted the experiments considering\nunanswerability in a multi-hop question for a given passage. We show that our\nend-to-end trainable pipeline model outperformed a non-interpretable model on\nour modified HotpotQA dataset. Experimental results also show that the IRC\nmodel achieves comparable results to the previous non-interpretable models in\nspite of the trade-off between prediction performance and interpretability.", "published": "2021-11-17 10:47:47", "link": "http://arxiv.org/abs/2111.09029v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Guiding Generative Language Models for Data Augmentation in Few-Shot\n  Text Classification", "abstract": "Data augmentation techniques are widely used for enhancing the performance of\nmachine learning models by tackling class imbalance issues and data sparsity.\nState-of-the-art generative language models have been shown to provide\nsignificant gains across different NLP tasks. However, their applicability to\ndata augmentation for text classification tasks in few-shot settings have not\nbeen fully explored, especially for specialised domains. In this paper, we\nleverage GPT-2 (Radford A et al, 2019) for generating artificial training\ninstances in order to improve classification performance. Our aim is to analyse\nthe impact the selection process of seed training examples have over the\nquality of GPT-generated samples and consequently the classifier performance.\nWe perform experiments with several seed selection strategies that, among\nothers, exploit class hierarchical structures and domain expert selection. Our\nresults show that fine-tuning GPT-2 in a handful of label instances leads to\nconsistent classification improvements and outperform competitive baselines.\nFinally, we show that guiding this process through domain expert selection can\nlead to further improvements, which opens up interesting research avenues for\ncombining generative models and active learning.", "published": "2021-11-17 12:10:03", "link": "http://arxiv.org/abs/2111.09064v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Character Transformations for Non-Autoregressive GEC Tagging", "abstract": "We propose a character-based nonautoregressive GEC approach, with\nautomatically generated character transformations. Recently, per-word\nclassification of correction edits has proven an efficient, parallelizable\nalternative to current encoder-decoder GEC systems. We show that word\nreplacement edits may be suboptimal and lead to explosion of rules for\nspelling, diacritization and errors in morphologically rich languages, and\npropose a method for generating character transformations from GEC corpus.\nFinally, we train character transformation models for Czech, German and\nRussian, reaching solid results and dramatic speedup compared to autoregressive\nsystems. The source code is released at\nhttps://github.com/ufal/wnut2021_character_transformations_gec.", "published": "2021-11-17 18:30:34", "link": "http://arxiv.org/abs/2111.09280v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Achieving Human Parity on Visual Question Answering", "abstract": "The Visual Question Answering (VQA) task utilizes both visual image and\nlanguage analysis to answer a textual question with respect to an image. It has\nbeen a popular research topic with an increasing number of real-world\napplications in the last decade. This paper describes our recent research of\nAliceMind-MMU (ALIbaba's Collection of Encoder-decoders from Machine\nIntelligeNce lab of Damo academy - MultiMedia Understanding) that obtains\nsimilar or even slightly better results than human being does on VQA. This is\nachieved by systematically improving the VQA pipeline including: (1)\npre-training with comprehensive visual and textual feature representation; (2)\neffective cross-modal interaction with learning to attend; and (3) A novel\nknowledge mining framework with specialized expert modules for the complex VQA\ntask. Treating different types of visual questions with corresponding expertise\nneeded plays an important role in boosting the performance of our VQA\narchitecture up to the human level. An extensive set of experiments and\nanalysis are conducted to demonstrate the effectiveness of the new research\nwork.", "published": "2021-11-17 04:25:11", "link": "http://arxiv.org/abs/2111.08896v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Using Sampling to Estimate and Improve Performance of Automated Scoring\n  Systems with Guarantees", "abstract": "Automated Scoring (AS), the natural language processing task of scoring\nessays and speeches in an educational testing setting, is growing in popularity\nand being deployed across contexts from government examinations to companies\nproviding language proficiency services. However, existing systems either forgo\nhuman raters entirely, thus harming the reliability of the test, or score every\nresponse by both human and machine thereby increasing costs. We target the\nspectrum of possible solutions in between, making use of both humans and\nmachines to provide a higher quality test while keeping costs reasonable to\ndemocratize access to AS. In this work, we propose a combination of the\nexisting paradigms, sampling responses to be scored by humans intelligently. We\npropose reward sampling and observe significant gains in accuracy (19.80%\nincrease on average) and quadratic weighted kappa (QWK) (25.60% on average)\nwith a relatively small human budget (30% samples) using our proposed sampling.\nThe accuracy increase observed using standard random and importance sampling\nbaselines are 8.6% and 12.2% respectively. Furthermore, we demonstrate the\nsystem's model agnostic nature by measuring its performance on a variety of\nmodels currently deployed in an AS setting as well as pseudo models. Finally,\nwe propose an algorithm to estimate the accuracy/QWK with statistical\nguarantees (Our code is available at https://git.io/J1IOy).", "published": "2021-11-17 05:00:51", "link": "http://arxiv.org/abs/2111.08906v1", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Transparent Human Evaluation for Image Captioning", "abstract": "We establish THumB, a rubric-based human evaluation protocol for image\ncaptioning models. Our scoring rubrics and their definitions are carefully\ndeveloped based on machine- and human-generated captions on the MSCOCO dataset.\nEach caption is evaluated along two main dimensions in a tradeoff (precision\nand recall) as well as other aspects that measure the text quality (fluency,\nconciseness, and inclusive language). Our evaluations demonstrate several\ncritical problems of the current evaluation practice. Human-generated captions\nshow substantially higher quality than machine-generated ones, especially in\ncoverage of salient information (i.e., recall), while most automatic metrics\nsay the opposite. Our rubric-based results reveal that CLIPScore, a recent\nmetric that uses image features, better correlates with human judgments than\nconventional text-only metrics because it is more sensitive to recall. We hope\nthat this work will promote a more transparent evaluation protocol for image\ncaptioning and its automatic metrics.", "published": "2021-11-17 07:09:59", "link": "http://arxiv.org/abs/2111.08940v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Induce, Edit, Retrieve: Language Grounded Multimodal Schema for\n  Instructional Video Retrieval", "abstract": "Schemata are structured representations of complex tasks that can aid\nartificial intelligence by allowing models to break down complex tasks into\nintermediate steps. We propose a novel system that induces schemata from web\nvideos and generalizes them to capture unseen tasks with the goal of improving\nvideo retrieval performance. Our system proceeds in three major phases: (1)\nGiven a task with related videos, we construct an initial schema for a task\nusing a joint video-text model to match video segments with text representing\nsteps from wikiHow; (2) We generalize schemata to unseen tasks by leveraging\nlanguage models to edit the text within existing schemata. Through\ngeneralization, we can allow our schemata to cover a more extensive range of\ntasks with a small amount of learning data; (3) We conduct zero-shot\ninstructional video retrieval with the unseen task names as the queries. Our\nschema-guided approach outperforms existing methods for video retrieval, and we\ndemonstrate that the schemata induced by our system are better than those\ngenerated by other models.", "published": "2021-11-17 18:20:04", "link": "http://arxiv.org/abs/2111.09276v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Fine-grained prediction of food insecurity using news streams", "abstract": "Anticipating the outbreak of a food crisis is crucial to efficiently allocate\nemergency relief and reduce human suffering. However, existing food insecurity\nearly warning systems rely on risk measures that are often delayed, outdated,\nor incomplete. Here, we leverage recent advances in deep learning to extract\nhigh-frequency precursors to food crises from the text of a large corpus of\nnews articles about fragile states published between 1980 and 2020. Our text\nfeatures are causally grounded, interpretable, validated by existing data, and\nallow us to predict 32% more food crises than existing models up to three\nmonths ahead of time at the district level across 15 fragile states. These\nresults could have profound implications on how humanitarian aid gets allocated\nand open new avenues for machine learning to improve decision making in\ndata-scarce environments.", "published": "2021-11-17 17:35:00", "link": "http://arxiv.org/abs/2111.15602v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-Attribute Relation Extraction (MARE) -- Simplifying the\n  Application of Relation Extraction", "abstract": "Natural language understanding's relation extraction makes innovative and\nencouraging novel business concepts possible and facilitates new digitilized\ndecision-making processes. Current approaches allow the extraction of relations\nwith a fixed number of entities as attributes. Extracting relations with an\narbitrary amount of attributes requires complex systems and costly\nrelation-trigger annotations to assist these systems. We introduce\nmulti-attribute relation extraction (MARE) as an assumption-less problem\nformulation with two approaches, facilitating an explicit mapping from business\nuse cases to the data annotations. Avoiding elaborated annotation constraints\nsimplifies the application of relation extraction approaches. The evaluation\ncompares our models to current state-of-the-art event extraction and binary\nrelation extraction methods. Our approaches show improvement compared to these\non the extraction of general multi-attribute relations.", "published": "2021-11-17 11:06:39", "link": "http://arxiv.org/abs/2111.09035v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "High Quality Streaming Speech Synthesis with Low,\n  Sentence-Length-Independent Latency", "abstract": "This paper presents an end-to-end text-to-speech system with low latency on a\nCPU, suitable for real-time applications. The system is composed of an\nautoregressive attention-based sequence-to-sequence acoustic model and the\nLPCNet vocoder for waveform generation. An acoustic model architecture that\nadopts modules from both the Tacotron 1 and 2 models is proposed, while\nstability is ensured by using a recently proposed purely location-based\nattention mechanism, suitable for arbitrary sentence length generation. During\ninference, the decoder is unrolled and acoustic feature generation is performed\nin a streaming manner, allowing for a nearly constant latency which is\nindependent from the sentence length. Experimental results show that the\nacoustic model can produce feature sequences with minimal latency about 31\ntimes faster than real-time on a computer CPU and 6.5 times on a mobile CPU,\nenabling it to meet the conditions required for real-time applications on both\ndevices. The full end-to-end system can generate almost natural quality speech,\nwhich is verified by listening tests.", "published": "2021-11-17 11:46:43", "link": "http://arxiv.org/abs/2111.09052v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cross-lingual Low Resource Speaker Adaptation Using Phonological\n  Features", "abstract": "The idea of using phonological features instead of phonemes as input to\nsequence-to-sequence TTS has been recently proposed for zero-shot multilingual\nspeech synthesis. This approach is useful for code-switching, as it facilitates\nthe seamless uttering of foreign text embedded in a stream of native text. In\nour work, we train a language-agnostic multispeaker model conditioned on a set\nof phonologically derived features common across different languages, with the\ngoal of achieving cross-lingual speaker adaptation. We first experiment with\nthe effect of language phonological similarity on cross-lingual TTS of several\nsource-target language combinations. Subsequently, we fine-tune the model with\nvery limited data of a new speaker's voice in either a seen or an unseen\nlanguage, and achieve synthetic speech of equal quality, while preserving the\ntarget speaker's identity. With as few as 32 and 8 utterances of target speaker\ndata, we obtain high speaker similarity scores and naturalness comparable to\nthe corresponding literature. In the extreme case of only 2 available\nadaptation utterances, we find that our model behaves as a few-shot learner, as\nthe performance is similar in both the seen and unseen adaptation language\nscenarios.", "published": "2021-11-17 12:33:42", "link": "http://arxiv.org/abs/2111.09075v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Rapping-Singing Voice Synthesis based on Phoneme-level Prosody Control", "abstract": "In this paper, a text-to-rapping/singing system is introduced, which can be\nadapted to any speaker's voice. It utilizes a Tacotron-based multispeaker\nacoustic model trained on read-only speech data and which provides prosody\ncontrol at the phoneme level. Dataset augmentation and additional prosody\nmanipulation based on traditional DSP algorithms are also investigated. The\nneural TTS model is fine-tuned to an unseen speaker's limited recordings,\nallowing rapping/singing synthesis with the target's speaker voice. The\ndetailed pipeline of the system is described, which includes the extraction of\nthe target pitch and duration values from an a capella song and their\nconversion into target speaker's valid range of notes before synthesis. An\nadditional stage of prosodic manipulation of the output via WSOLA is also\ninvestigated for better matching the target duration values. The synthesized\nutterances can be mixed with an instrumental accompaniment track to produce a\ncomplete song. The proposed system is evaluated via subjective listening tests\nas well as in comparison to an available alternate system which also aims to\nproduce synthetic singing voice from read-only training data. Results show that\nthe proposed approach can produce high quality rapping/singing voice with\nincreased naturalness.", "published": "2021-11-17 14:31:55", "link": "http://arxiv.org/abs/2111.09146v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at\n  Scale", "abstract": "This paper presents XLS-R, a large-scale model for cross-lingual speech\nrepresentation learning based on wav2vec 2.0. We train models with up to 2B\nparameters on nearly half a million hours of publicly available speech audio in\n128 languages, an order of magnitude more public data than the largest known\nprior work. Our evaluation covers a wide range of tasks, domains, data regimes\nand languages, both high and low-resource. On the CoVoST-2 speech translation\nbenchmark, we improve the previous state of the art by an average of 7.4 BLEU\nover 21 translation directions into English. For speech recognition, XLS-R\nimproves over the best known prior work on BABEL, MLS, CommonVoice as well as\nVoxPopuli, lowering error rates by 14-34% relative on average. XLS-R also sets\na new state of the art on VoxLingua107 language identification. Moreover, we\nshow that with sufficient model size, cross-lingual pretraining can outperform\nEnglish-only pretraining when translating English speech into other languages,\na setting which favors monolingual pretraining. We hope XLS-R can help to\nimprove speech processing tasks for many more languages of the world.", "published": "2021-11-17 18:49:42", "link": "http://arxiv.org/abs/2111.09296v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MEDCOD: A Medically-Accurate, Emotive, Diverse, and Controllable Dialog\n  System", "abstract": "We present MEDCOD, a Medically-Accurate, Emotive, Diverse, and Controllable\nDialog system with a unique approach to the natural language generator module.\nMEDCOD has been developed and evaluated specifically for the history taking\ntask. It integrates the advantage of a traditional modular approach to\nincorporate (medical) domain knowledge with modern deep learning techniques to\ngenerate flexible, human-like natural language expressions. Two key aspects of\nMEDCOD's natural language output are described in detail. First, the generated\nsentences are emotive and empathetic, similar to how a doctor would communicate\nto the patient. Second, the generated sentence structures and phrasings are\nvaried and diverse while maintaining medical consistency with the desired\nmedical concept (provided by the dialogue manager module of MEDCOD).\nExperimental results demonstrate the effectiveness of our approach in creating\na human-like medical dialogue system. Relevant code is available at\nhttps://github.com/curai/curai-research/tree/main/MEDCOD", "published": "2021-11-17 20:31:16", "link": "http://arxiv.org/abs/2111.09381v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "High Quality Rather than High Model Probability: Minimum Bayes Risk\n  Decoding with Neural Metrics", "abstract": "In Neural Machine Translation, it is typically assumed that the sentence with\nthe highest estimated probability should also be the translation with the\nhighest quality as measured by humans. In this work, we question this\nassumption and show that model estimates and translation quality only vaguely\ncorrelate. We apply Minimum Bayes Risk (MBR) decoding on unbiased samples to\noptimize diverse automated metrics of translation quality as an alternative\ninference strategy to beam search. Instead of targeting the hypotheses with the\nhighest model probability, MBR decoding extracts the hypotheses with the\nhighest estimated quality. Our experiments show that the combination of a\nneural translation model with a neural reference-based metric, BLEURT, results\nin significant improvement in human evaluations. This improvement is obtained\nwith translations different from classical beam-search output: these\ntranslations have much lower model likelihood and are less favored by surface\nmetrics like BLEU.", "published": "2021-11-17 20:48:02", "link": "http://arxiv.org/abs/2111.09388v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Information Fusion in Attention Networks Using Adaptive and Multi-level\n  Factorized Bilinear Pooling for Audio-visual Emotion Recognition", "abstract": "Multimodal emotion recognition is a challenging task in emotion computing as\nit is quite difficult to extract discriminative features to identify the subtle\ndifferences in human emotions with abstract concept and multiple expressions.\nMoreover, how to fully utilize both audio and visual information is still an\nopen problem. In this paper, we propose a novel multimodal fusion attention\nnetwork for audio-visual emotion recognition based on adaptive and multi-level\nfactorized bilinear pooling (FBP). First, for the audio stream, a fully\nconvolutional network (FCN) equipped with 1-D attention mechanism and local\nresponse normalization is designed for speech emotion recognition. Next, a\nglobal FBP (G-FBP) approach is presented to perform audio-visual information\nfusion by integrating selfattention based video stream with the proposed audio\nstream. To improve G-FBP, an adaptive strategy (AG-FBP) to dynamically\ncalculate the fusion weight of two modalities is devised based on the\nemotion-related representation vectors from the attention mechanism of\nrespective modalities. Finally, to fully utilize the local emotion information,\nadaptive and multi-level FBP (AMFBP) is introduced by combining both\nglobal-trunk and intratrunk data in one recording on top of AG-FBP. Tested on\nthe IEMOCAP corpus for speech emotion recognition with only audio stream, the\nnew FCN method outperforms the state-ofthe-art results with an accuracy of\n71.40%. Moreover, validated on the AFEW database of EmotiW2019 sub-challenge\nand the IEMOCAP corpus for audio-visual emotion recognition, the proposed\nAM-FBP approach achieves the best accuracy of 63.09% and 75.49% respectively on\nthe test set.", "published": "2021-11-17 05:22:22", "link": "http://arxiv.org/abs/2111.08910v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Subject Enveloped Deep Sample Fuzzy Ensemble Learning Algorithm of\n  Parkinson's Speech Data", "abstract": "Parkinson disease (PD)'s speech recognition is an effective way for its\ndiagnosis, which has become a hot and difficult research area in recent years.\nAs we know, there are large corpuses (segments) within one subject. However,\ntoo large segments will increase the complexity of the classification model.\nBesides, the clinicians interested in finding diagnostic speech markers that\nreflect the pathology of the whole subject. Since the optimal relevant features\nof each speech sample segment are different, it is difficult to find the\nuniform diagnostic speech markers. Therefore, it is necessary to reconstruct\nthe existing large segments within one subject into few segments even one\nsegment within one subject, which can facilitate the extraction of relevant\nspeech features to characterize diagnostic markers for the whole subject. To\naddress this problem, an enveloped deep speech sample learning algorithm for\nParkinson's subjects based on multilayer fuzzy c-mean (MlFCM) clustering and\ninterlayer consistency preservation is proposed in this paper. The algorithm\ncan be used to achieve intra-subject sample reconstruction for Parkinson's\ndisease (PD) to obtain a small number of high-quality prototype sample\nsegments. At the end of the paper, several representative PD speech datasets\nare selected and compared with the state-of-the-art related methods,\nrespectively. The experimental results show that the proposed algorithm is\neffective signifcantly.", "published": "2021-11-17 10:12:20", "link": "http://arxiv.org/abs/2111.09014v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "BLOOM-Net: Blockwise Optimization for Masking Networks Toward Scalable\n  and Efficient Speech Enhancement", "abstract": "In this paper, we present a blockwise optimization method for masking-based\nnetworks (BLOOM-Net) for training scalable speech enhancement networks. Here,\nwe design our network with a residual learning scheme and train the internal\nseparator blocks sequentially to obtain a scalable masking-based deep neural\nnetwork for speech enhancement. Its scalability lets it dynamically adjust the\nrun-time complexity depending on the test time environment. To this end, we\nmodularize our models in that they can flexibly accommodate varying needs for\nenhancement performance and constraints on the resources, incurring minimal\nmemory or training overhead due to the added scalability. Our experiments on\nspeech enhancement demonstrate that the proposed blockwise optimization method\nachieves the desired scalability with only a slight performance degradation\ncompared to corresponding models trained end-to-end.", "published": "2021-11-17 20:11:07", "link": "http://arxiv.org/abs/2111.09372v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
