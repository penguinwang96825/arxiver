{"title": "Spelling Correction as a Foreign Language", "abstract": "In this paper, we reformulated the spell correction problem as a machine\ntranslation task under the encoder-decoder framework. This reformulation\nenabled us to use a single model for solving the problem that is traditionally\nformulated as learning a language model and an error model. This model employs\nmulti-layer recurrent neural networks as an encoder and a decoder. We\ndemonstrate the effectiveness of this model using an internal dataset, where\nthe training data is automatically obtained from user logs. The model offers\ncompetitive performance as compared to the state of the art methods but does\nnot require any feature engineering nor hand tuning between models.", "published": "2017-05-21 00:14:07", "link": "http://arxiv.org/abs/1705.07371v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recurrent Additive Networks", "abstract": "We introduce recurrent additive networks (RANs), a new gated RNN which is\ndistinguished by the use of purely additive latent state updates. At every time\nstep, the new state is computed as a gated component-wise sum of the input and\nthe previous state, without any of the non-linearities commonly used in RNN\ntransition dynamics. We formally show that RAN states are weighted sums of the\ninput vectors, and that the gates only contribute to computing the weights of\nthese sums. Despite this relatively simple functional form, experiments\ndemonstrate that RANs perform on par with LSTMs on benchmark language modeling\nproblems. This result shows that many of the non-linear computations in LSTMs\nand related networks are not essential, at least for the problems we consider,\nand suggests that the gates are doing more of the computational work than\npreviously understood.", "published": "2017-05-21 04:42:11", "link": "http://arxiv.org/abs/1705.07393v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Semantic Relatedness From Human Feedback Using Metric Learning", "abstract": "Assessing the degree of semantic relatedness between words is an important\ntask with a variety of semantic applications, such as ontology learning for the\nSemantic Web, semantic search or query expansion. To accomplish this in an\nautomated fashion, many relatedness measures have been proposed. However, most\nof these metrics only encode information contained in the underlying corpus and\nthus do not directly model human intuition. To solve this, we propose to\nutilize a metric learning approach to improve existing semantic relatedness\nmeasures by learning from additional information, such as explicit human\nfeedback. For this, we argue to use word embeddings instead of traditional\nhigh-dimensional vector representations in order to leverage their semantic\ndensity and to reduce computational cost. We rigorously test our approach on\nseveral domains including tagging data as well as publicly available embeddings\nbased on Wikipedia texts and navigation. Human feedback about semantic\nrelatedness for learning and evaluation is extracted from publicly available\ndatasets such as MEN or WS-353. We find that our method can significantly\nimprove semantic relatedness measures by learning from additional information,\nsuch as explicit human feedback. For tagging data, we are the first to generate\nand study embeddings. Our results are of special interest for ontology and\nrecommendation engineers, but also for any other researchers and practitioners\nof Semantic Web techniques.", "published": "2017-05-21 10:16:49", "link": "http://arxiv.org/abs/1705.07425v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
