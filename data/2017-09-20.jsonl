{"title": "De-identification of medical records using conditional random fields and\n  long short-term memory networks", "abstract": "The CEGS N-GRID 2016 Shared Task 1 in Clinical Natural Language Processing\nfocuses on the de-identification of psychiatric evaluation records. This paper\ndescribes two participating systems of our team, based on conditional random\nfields (CRFs) and long short-term memory networks (LSTMs). A pre-processing\nmodule was introduced for sentence detection and tokenization before\nde-identification. For CRFs, manually extracted rich features were utilized to\ntrain the model. For LSTMs, a character-level bi-directional LSTM network was\napplied to represent tokens and classify tags for each token, following which a\ndecoding layer was stacked to decode the most probable protected health\ninformation (PHI) terms. The LSTM-based system attained an i2b2 strict\nmicro-F_1 measure of 89.86%, which was higher than that of the CRF-based\nsystem.", "published": "2017-09-20 14:30:17", "link": "http://arxiv.org/abs/1709.06901v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Use of Machine Translation-Based Approaches for Vietnamese\n  Diacritic Restoration", "abstract": "This paper presents an empirical study of two machine translation-based\napproaches for Vietnamese diacritic restoration problem, including phrase-based\nand neural-based machine translation models. This is the first work that\napplies neural-based machine translation method to this problem and gives a\nthorough comparison to the phrase-based machine translation method which is the\ncurrent state-of-the-art method for this problem. On a large dataset, the\nphrase-based approach has an accuracy of 97.32% while that of the neural-based\napproach is 96.15%. While the neural-based method has a slightly lower\naccuracy, it is about twice faster than the phrase-based method in terms of\ninference speed. Moreover, neural-based machine translation method has much\nroom for future improvement such as incorporating pre-trained word embeddings\nand collecting more training data.", "published": "2017-09-20 23:56:24", "link": "http://arxiv.org/abs/1709.07104v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Restaurant Features via Sentiment Analysis on Yelp Reviews", "abstract": "Many people use Yelp to find a good restaurant. Nonetheless, with only an\noverall rating for each restaurant, Yelp offers not enough information for\nindependently judging its various aspects such as environment, service or\nflavor. In this paper, we introduced a machine learning based method to\ncharacterize such aspects for particular types of restaurants. The main\napproach used in this paper is to use a support vector machine (SVM) model to\ndecipher the sentiment tendency of each review from word frequency. Word scores\ngenerated from the SVM models are further processed into a polarity index\nindicating the significance of each word for special types of restaurant.\nCustomers overall tend to express more sentiment regarding service. As for the\ndistinction between different cuisines, results that match the common sense are\nobtained: Japanese cuisines are usually fresh, some French cuisines are\noverpriced while Italian Restaurants are often famous for their pizzas.", "published": "2017-09-20 03:51:19", "link": "http://arxiv.org/abs/1709.08698v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constructing a Hierarchical User Interest Structure based on User\n  Profiles", "abstract": "The interests of individual internet users fall into a hierarchical structure\nwhich is useful in regards to building personalized searches and\nrecommendations. Most studies on this subject construct the interest hierarchy\nof a single person from the document perspective. In this study, we constructed\nthe user interest hierarchy via user profiles. We organized 433,397 user\ninterests, referred to here as \"attentions\", into a user attention network\n(UAN) from 200 million user profiles; we then applied the Louvain algorithm to\ndetect hierarchical clusters in these attentions. Finally, a 26-level hierarchy\nwith 34,676 clusters was obtained. We found that these attention clusters were\naggregated according to certain topics as opposed to the hyponymy-relation\nbased conceptual ontologies. The topics can be entities or concepts, and the\nrelations were not restrained by hyponymy. The concept relativity encapsulated\nin the user's interest can be captured by labeling the attention clusters with\ncorresponding concepts.", "published": "2017-09-20 15:03:51", "link": "http://arxiv.org/abs/1709.06918v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Updating the silent speech challenge benchmark with deep learning", "abstract": "The 2010 Silent Speech Challenge benchmark is updated with new results\nobtained in a Deep Learning strategy, using the same input features and\ndecoding strategy as in the original article. A Word Error Rate of 6.4% is\nobtained, compared to the published value of 17.4%. Additional results\ncomparing new auto-encoder-based features with the original features at reduced\ndimensionality, as well as decoding scenarios on two different language models,\nare also presented. The Silent Speech Challenge archive has been updated to\ncontain both the original and the new auto-encoder features, in addition to the\noriginal raw data.", "published": "2017-09-20 11:28:40", "link": "http://arxiv.org/abs/1709.06818v1", "categories": ["cs.CL", "cs.CV", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Doctoral Advisor or Medical Condition: Towards Entity-specific Rankings\n  of Knowledge Base Properties [Extended Version]", "abstract": "In knowledge bases such as Wikidata, it is possible to assert a large set of\nproperties for entities, ranging from generic ones such as name and place of\nbirth to highly profession-specific or background-specific ones such as\ndoctoral advisor or medical condition. Determining a preference or ranking in\nthis large set is a challenge in tasks such as prioritisation of edits or\nnatural-language generation. Most previous approaches to ranking knowledge base\nproperties are purely data-driven, that is, as we show, mistake frequency for\ninterestingness.\n  In this work, we have developed a human-annotated dataset of 350 preference\njudgments among pairs of knowledge base properties for fixed entities. From\nthis set, we isolate a subset of pairs for which humans show a high level of\nagreement (87.5% on average). We show, however, that baseline and\nstate-of-the-art techniques achieve only 61.3% precision in predicting human\npreferences for this subset.\n  We then analyze what contributes to one property being rated as more\nimportant than another one, and identify that at least three factors play a\nrole, namely (i) general frequency, (ii) applicability to similar entities and\n(iii) semantic similarity between property and entity. We experimentally\nanalyze the contribution of each factor and show that a combination of\ntechniques addressing all the three factors achieves 74% precision on the task.\n  The dataset is available at\nwww.kaggle.com/srazniewski/wikidatapropertyranking.", "published": "2017-09-20 14:43:08", "link": "http://arxiv.org/abs/1709.06907v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DB"], "primary_category": "cs.IR"}
{"title": "Text Compression for Sentiment Analysis via Evolutionary Algorithms", "abstract": "Can textual data be compressed intelligently without losing accuracy in\nevaluating sentiment? In this study, we propose a novel evolutionary\ncompression algorithm, PARSEC (PARts-of-Speech for sEntiment Compression),\nwhich makes use of Parts-of-Speech tags to compress text in a way that\nsacrifices minimal classification accuracy when used in conjunction with\nsentiment analysis algorithms. An analysis of PARSEC with eight commercial and\nnon-commercial sentiment analysis algorithms on twelve English sentiment data\nsets reveals that accurate compression is possible with (0%, 1.3%, 3.3%) loss\nin sentiment classification accuracy for (20%, 50%, 75%) data compression with\nPARSEC using LingPipe, the most accurate of the sentiment algorithms. Other\nsentiment analysis algorithms are more severely affected by compression. We\nconclude that significant compression of text data is possible for sentiment\nanalysis depending on the accuracy demands of the specific application and the\nspecific sentiment analysis algorithm used.", "published": "2017-09-20 17:57:16", "link": "http://arxiv.org/abs/1709.06990v1", "categories": ["cs.NE", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.NE"}
{"title": "Neural Network Alternatives to Convolutive Audio Models for Source\n  Separation", "abstract": "Convolutive Non-Negative Matrix Factorization model factorizes a given audio\nspectrogram using frequency templates with a temporal dimension. In this paper,\nwe present a convolutional auto-encoder model that acts as a neural network\nalternative to convolutive NMF. Using the modeling flexibility granted by\nneural networks, we also explore the idea of using a Recurrent Neural Network\nin the encoder. Experimental results on speech mixtures from TIMIT dataset\nindicate that the convolutive architecture provides a significant improvement\nin separation performance in terms of BSSeval metrics.", "published": "2017-09-20 20:45:53", "link": "http://arxiv.org/abs/1709.07908v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
