{"title": "FreeCtrl: Constructing Control Centers with Feedforward Layers for\n  Learning-Free Controllable Text Generation", "abstract": "Controllable text generation (CTG) seeks to craft texts adhering to specific\nattributes, traditionally employing learning-based techniques such as training,\nfine-tuning, or prefix-tuning with attribute-specific datasets. These\napproaches, while effective, demand extensive computational and data resources.\nIn contrast, some proposed learning-free alternatives circumvent learning but\noften yield inferior results, exemplifying the fundamental machine learning\ntrade-off between computational expense and model efficacy. To overcome these\nlimitations, we propose FreeCtrl, a learning-free approach that dynamically\nadjusts the weights of selected feedforward neural network (FFN) vectors to\nsteer the outputs of large language models (LLMs). FreeCtrl hinges on the\nprinciple that the weights of different FFN vectors influence the likelihood of\ndifferent tokens appearing in the output. By identifying and adaptively\nadjusting the weights of attribute-related FFN vectors, FreeCtrl can control\nthe output likelihood of attribute keywords in the generated content. Extensive\nexperiments on single- and multi-attribute control reveal that the\nlearning-free FreeCtrl outperforms other learning-free and learning-based\nmethods, successfully resolving the dilemma between learning costs and model\nperformance.", "published": "2024-06-14 03:18:28", "link": "http://arxiv.org/abs/2406.09688v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Response Generation Not Requiring Factual Judgment", "abstract": "With the remarkable development of large language models (LLMs), ensuring the\nfactuality of output has become a challenge. However, having all the contents\nof the response with given knowledge or facts is not necessarily a good thing\nin dialogues. This study aimed to achieve both attractiveness and factuality in\na dialogue response for which a task was set to predict sentences that do not\nrequire factual correctness judgment such as agreeing, or personal\nopinions/feelings. We created a dataset, dialogue dataset annotated with\nfact-check-needed label (DDFC), for this task via crowdsourcing, and\nclassification tasks were performed on several models using this dataset. The\nmodel with the highest classification accuracy could yield about 88% accurate\nclassification results.", "published": "2024-06-14 04:03:24", "link": "http://arxiv.org/abs/2406.09702v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniBridge: A Unified Approach to Cross-Lingual Transfer Learning for\n  Low-Resource Languages", "abstract": "In this paper, we introduce UniBridge (Cross-Lingual Transfer Learning with\nOptimized Embeddings and Vocabulary), a comprehensive approach developed to\nimprove the effectiveness of Cross-Lingual Transfer Learning, particularly in\nlanguages with limited resources. Our approach tackles two essential elements\nof a language model: the initialization of embeddings and the optimal\nvocabulary size. Specifically, we propose a novel embedding initialization\nmethod that leverages both lexical and semantic alignment for a language. In\naddition, we present a method for systematically searching for the optimal\nvocabulary size, ensuring a balance between model complexity and linguistic\ncoverage. Our experiments across multilingual datasets show that our approach\ngreatly improves the F1-Score in several languages. UniBridge is a robust and\nadaptable solution for cross-lingual systems in various languages, highlighting\nthe significance of initializing embeddings and choosing the right vocabulary\nsize in cross-lingual environments.", "published": "2024-06-14 04:55:30", "link": "http://arxiv.org/abs/2406.09717v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic\n  Textual Similarity", "abstract": "Semantic Textual Similarity (STS) constitutes a critical research direction\nin computational linguistics and serves as a key indicator of the encoding\ncapabilities of embedding models. Driven by advances in pre-trained language\nmodels and contrastive learning, leading sentence representation methods have\nreached an average Spearman's correlation score of approximately 86 across\nseven STS benchmarks in SentEval. However, further progress has become\nincreasingly marginal, with no existing method attaining an average score\nhigher than 86.5 on these tasks. This paper conducts an in-depth analysis of\nthis phenomenon and concludes that the upper limit for Spearman's correlation\nscores under contrastive learning is 87.5. To transcend this ceiling, we\npropose an innovative approach termed Pcc-tuning, which employs Pearson's\ncorrelation coefficient as a loss function to refine model performance beyond\ncontrastive learning. Experimental results demonstrate that Pcc-tuning can\nmarkedly surpass previous state-of-the-art strategies with only a minimal\namount of fine-grained annotated samples.", "published": "2024-06-14 07:40:07", "link": "http://arxiv.org/abs/2406.09790v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Encoding of Gender in Transformer-based ASR Representations", "abstract": "While existing literature relies on performance differences to uncover gender\nbiases in ASR models, a deeper analysis is essential to understand how gender\nis encoded and utilized during transcript generation. This work investigates\nthe encoding and utilization of gender in the latent representations of two\ntransformer-based ASR models, Wav2Vec2 and HuBERT. Using linear erasure, we\ndemonstrate the feasibility of removing gender information from each layer of\nan ASR model and show that such an intervention has minimal impacts on the ASR\nperformance. Additionally, our analysis reveals a concentration of gender\ninformation within the first and last frames in the final layers, explaining\nthe ease of erasing gender in these layers. Our findings suggest the prospect\nof creating gender-neutral embeddings that can be integrated into ASR\nframeworks without compromising their efficacy.", "published": "2024-06-14 09:10:24", "link": "http://arxiv.org/abs/2406.09855v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Unified Data Augmentation Framework for Low-Resource Multi-Domain\n  Dialogue Generation", "abstract": "Current state-of-the-art dialogue systems heavily rely on extensive training\ndatasets. However, challenges arise in domains where domain-specific training\ndatasets are insufficient or entirely absent. To tackle this challenge, we\npropose a novel data \\textbf{A}ugmentation framework for\n\\textbf{M}ulti-\\textbf{D}omain \\textbf{D}ialogue \\textbf{G}eneration, referred\nto as \\textbf{AMD$^2$G}. The AMD$^2$G framework consists of a data augmentation\nprocess and a two-stage training approach: domain-agnostic training and domain\nadaptation training. We posit that domain corpora are a blend of\ndomain-agnostic and domain-specific features, with certain representation\npatterns shared among diverse domains. Domain-agnostic training aims to enable\nmodels to learn these common expressive patterns. To construct domain-agnostic\ndialogue corpora, we employ a \\textit{\\textbf{de-domaining}} data processing\ntechnique used to remove domain-specific features. By mitigating the effects of\ndomain-specific features, the model trained on the de-domained corpora can\neffectively learn common expression patterns in different domains.\nSubsequently, we adapt the learned domain-agnostic features to the target\ndomain through domain adaptation training. We conduct experiments on Chinese\ndialogue datasets from five different domains and show that AMD$^2$G achieves\nsuperior performance compared to both direct training on the target domain\ncorpus and collective training on all five domain corpora. Our work underscores\nAMD$^2$G as a viable alternative solution for low-resource multi-domain\ndialogue generation. Code and data associated with our work are available on\nGitHub repository$^{\\text 1}$.", "published": "2024-06-14 09:52:27", "link": "http://arxiv.org/abs/2406.09881v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "3D-RPE: Enhancing Long-Context Modeling Through 3D Rotary Position\n  Encoding", "abstract": "Inspired by the Bloch Sphere representation, we propose a novel rotary\nposition encoding on a three-dimensional sphere, named 3D Rotary Position\nEncoding (3D-RPE). 3D-RPE is an advanced version of the widely used 2D Rotary\nPosition Encoding (RoPE), with two major advantages for modeling long contexts:\ncontrollable long-term decay and improved position resolution. For controllable\nlong-term decay, 3D-RPE allows for the regulation of long-term decay within the\nchunk size, ensuring the modeling of relative positional information between\ntokens at a distant relative position. For enhanced position resolution, 3D-RPE\ncan mitigate the degradation of position resolution caused by position\ninterpolation on RoPE. We have conducted experiments on long-context Natural\nLanguage Understanding (NLU) and long-sequence Language Modeling (LM) tasks.\nFrom the experimental results, 3D-RPE achieved performance improvements over\nRoPE, especially in long-context NLU tasks.", "published": "2024-06-14 10:13:37", "link": "http://arxiv.org/abs/2406.09897v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GEB-1.3B: Open Lightweight Large Language Model", "abstract": "Recently developed large language models (LLMs) such as ChatGPT, Claude, and\nLlama have demonstrated impressive abilities, and even surpass human-level\nperformance in several tasks. Despite their success, the resource-intensive\ndemands of these models, requiring significant computational power for both\ntraining and inference, limit their deployment to high-performance servers.\nAdditionally, the extensive calculation requirements of the models often lead\nto increased latency in response times. With the increasing need for LLMs to\noperate efficiently on CPUs, research about lightweight models that are\noptimized for CPU inference has emerged. In this work, we introduce GEB-1.3B, a\nlightweight LLM trained on 550 billion tokens in both Chinese and English\nlanguages. We employ novel training techniques, including ROPE,\nGroup-Query-Attention, and FlashAttention-2, to accelerate training while\nmaintaining model performance. Additionally, we fine-tune the model using 10\nmillion samples of instruction data to enhance alignment. GEB-1.3B exhibits\noutstanding performance on general benchmarks such as MMLU, C-Eval, and CMMLU,\noutperforming comparative models such as MindLLM-1.3B and TinyLLaMA-1.1B.\nNotably, the FP32 version of GEB-1.3B achieves commendable inference times on\nCPUs, with ongoing efforts to further enhance speed through advanced\nquantization techniques. The release of GEB-1.3B as an open-source model marks\na significant contribution to the development of lightweight LLMs, promising to\nfoster further research and innovation in the field.", "published": "2024-06-14 10:15:49", "link": "http://arxiv.org/abs/2406.09900v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures\n  and Languages", "abstract": "Large language models (LLMs) often lack culture-specific knowledge of daily\nlife, especially across diverse regions and non-English languages. Existing\nbenchmarks for evaluating LLMs' cultural sensitivities are limited to a single\nlanguage or collected from online sources such as Wikipedia, which do not\nreflect the mundane everyday lifestyles of diverse regions. That is,\ninformation about the food people eat for their birthday celebrations, spices\nthey typically use, musical instruments youngsters play, or the sports they\npractice in school is common cultural knowledge but uncommon in easily\ncollected online sources, especially for underrepresented cultures. To address\nthis issue, we introduce BLEnD, a hand-crafted benchmark designed to evaluate\nLLMs' everyday knowledge across diverse cultures and languages. BLEnD comprises\n52.6k question-answer pairs from 16 countries/regions, in 13 different\nlanguages, including low-resource ones such as Amharic, Assamese, Azerbaijani,\nHausa, and Sundanese. We construct the benchmark to include two formats of\nquestions: short-answer and multiple-choice. We show that LLMs perform better\nfor cultures that are highly represented online, with a maximum 57.34%\ndifference in GPT-4, the best-performing model, in the short-answer format. For\ncultures represented by mid-to-high-resource languages, LLMs perform better in\ntheir local languages, but for cultures represented by low-resource languages,\nLLMs perform better in English than the local languages. We make our dataset\npublicly available at: https://github.com/nlee0212/BLEnD.", "published": "2024-06-14 11:48:54", "link": "http://arxiv.org/abs/2406.09948v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bag of Lies: Robustness in Continuous Pre-training BERT", "abstract": "This study aims to acquire more insights into the continuous pre-training\nphase of BERT regarding entity knowledge, using the COVID-19 pandemic as a case\nstudy. Since the pandemic emerged after the last update of BERT's pre-training\ndata, the model has little to no entity knowledge about COVID-19. Using\ncontinuous pre-training, we control what entity knowledge is available to the\nmodel. We compare the baseline BERT model with the further pre-trained variants\non the fact-checking benchmark Check-COVID. To test the robustness of\ncontinuous pre-training, we experiment with several adversarial methods to\nmanipulate the input data, such as training on misinformation and shuffling the\nword order until the input becomes nonsensical. Surprisingly, our findings\nreveal that these methods do not degrade, and sometimes even improve, the\nmodel's downstream performance. This suggests that continuous pre-training of\nBERT is robust against misinformation. Furthermore, we are releasing a new\ndataset, consisting of original texts from academic publications in the\nLitCovid repository and their AI-generated false counterparts.", "published": "2024-06-14 12:16:08", "link": "http://arxiv.org/abs/2406.09967v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Better LLM Evaluator for Text Generation: The Impact of Prompt Output\n  Sequencing and Optimization", "abstract": "This research investigates prompt designs of evaluating generated texts using\nlarge language models (LLMs). While LLMs are increasingly used for scoring\nvarious inputs, creating effective prompts for open-ended text evaluation\nremains challenging due to model sensitivity and subjectivity in evaluation of\ntext generation. Our study experimented with different prompt structures,\naltering the sequence of output instructions and including explanatory reasons.\nWe found that the order of presenting reasons and scores significantly\ninfluences LLMs' scoring, with a different level of rule understanding in the\nprompt. An additional optimization may enhance scoring alignment if sufficient\ndata is available. This insight is crucial for improving the accuracy and\nconsistency of LLM-based evaluations.", "published": "2024-06-14 12:31:44", "link": "http://arxiv.org/abs/2406.09972v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disentangling Dialect from Social Bias via Multitask Learning to Improve\n  Fairness", "abstract": "Dialects introduce syntactic and lexical variations in language that occur in\nregional or social groups. Most NLP methods are not sensitive to such\nvariations. This may lead to unfair behavior of the methods, conveying negative\nbias towards dialect speakers. While previous work has studied dialect-related\nfairness for aspects like hate speech, other aspects of biased language, such\nas lewdness, remain fully unexplored. To fill this gap, we investigate\nperformance disparities between dialects in the detection of five aspects of\nbiased language and how to mitigate them. To alleviate bias, we present a\nmultitask learning approach that models dialect language as an auxiliary task\nto incorporate syntactic and lexical variations. In our experiments with\nAfrican-American English dialect, we provide empirical evidence that\ncomplementing common learning approaches with dialect modeling improves their\nfairness. Furthermore, the results suggest that multitask learning achieves\nstate-of-the-art performance and helps to detect properties of biased language\nmore reliably.", "published": "2024-06-14 12:39:39", "link": "http://arxiv.org/abs/2406.09977v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Precision Empowers, Excess Distracts: Visual Question Answering With\n  Dynamically Infused Knowledge In Language Models", "abstract": "In the realm of multimodal tasks, Visual Question Answering (VQA) plays a\ncrucial role by addressing natural language questions grounded in visual\ncontent. Knowledge-Based Visual Question Answering (KBVQA) advances this\nconcept by adding external knowledge along with images to respond to questions.\nWe introduce an approach for KBVQA, augmenting the existing vision-language\ntransformer encoder-decoder (OFA) model. Our main contribution involves\nenhancing questions by incorporating relevant external knowledge extracted from\nknowledge graphs, using a dynamic triple extraction method. We supply a\nflexible number of triples from the knowledge graph as context, tailored to\nmeet the requirements for answering the question. Our model, enriched with\nknowledge, demonstrates an average improvement of 4.75\\% in Exact Match Score\nover the state-of-the-art on three different KBVQA datasets. Through\nexperiments and analysis, we demonstrate that furnishing variable triples for\neach question improves the reasoning capabilities of the language model in\ncontrast to supplying a fixed number of triples. This is illustrated even for\nrecent large language models. Additionally, we highlight the model's\ngeneralization capability by showcasing its SOTA-beating performance on a small\ndataset, achieved through straightforward fine-tuning.", "published": "2024-06-14 13:07:46", "link": "http://arxiv.org/abs/2406.09994v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Question Answering on Charts Through Effective Pre-training\n  Tasks", "abstract": "To completely understand a document, the use of textual information is not\nenough. Understanding visual cues, such as layouts and charts, is also\nrequired. While the current state-of-the-art approaches for document\nunderstanding (both OCR-based and OCR-free) work well, a thorough analysis of\ntheir capabilities and limitations has not yet been performed. Therefore, in\nthis work, we addresses the limitation of current VisualQA models when applied\nto charts and plots. To investigate shortcomings of the state-of-the-art\nmodels, we conduct a comprehensive behavioral analysis, using ChartQA as a case\nstudy. Our findings indicate that existing models particularly underperform in\nanswering questions related to the chart's structural and visual context, as\nwell as numerical information. To address these issues, we propose three simple\npre-training tasks that enforce the existing model in terms of both\nstructural-visual knowledge, as well as its understanding of numerical\nquestions. We evaluate our pre-trained model (called MatCha-v2) on three chart\ndatasets - both extractive and abstractive question datasets - and observe that\nit achieves an average improvement of 1.7% over the baseline model.", "published": "2024-06-14 14:40:10", "link": "http://arxiv.org/abs/2406.10085v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Correlation between Human and Machine Evaluation of\n  Simultaneous Speech Translation", "abstract": "Assessing the performance of interpreting services is a complex task, given\nthe nuanced nature of spoken language translation, the strategies that\ninterpreters apply, and the diverse expectations of users. The complexity of\nthis task become even more pronounced when automated evaluation methods are\napplied. This is particularly true because interpreted texts exhibit less\nlinearity between the source and target languages due to the strategies\nemployed by the interpreter.\n  This study aims to assess the reliability of automatic metrics in evaluating\nsimultaneous interpretations by analyzing their correlation with human\nevaluations. We focus on a particular feature of interpretation quality, namely\ntranslation accuracy or faithfulness. As a benchmark we use human assessments\nperformed by language experts, and evaluate how well sentence embeddings and\nLarge Language Models correlate with them. We quantify semantic similarity\nbetween the source and translated texts without relying on a reference\ntranslation. The results suggest GPT models, particularly GPT-3.5 with direct\nprompting, demonstrate the strongest correlation with human judgment in terms\nof semantic similarity between source and target texts, even when evaluating\nshort textual segments. Additionally, the study reveals that the size of the\ncontext window has a notable impact on this correlation.", "published": "2024-06-14 14:47:19", "link": "http://arxiv.org/abs/2406.10091v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction\n  Tuning", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities but\nstill face challenges such as hallucinations. One potential reason for\nhallucinations is the lack of relevant knowledge or context. Thus, a promising\nsolution involves instructing LLMs to respond with \"I do not know\" when a\nquestion falls outside their knowledge domain or the provided context. However,\nin this work, we observed that LLMs struggle to admit their lack of knowledge,\nprimarily due to existing instruction datasets designed to encourage specific\nanswers. To improve models' capability to recognize the boundaries of their\nknowledge, we propose a novel approach called uncertainty-sensitive tuning.\nThis method involves two-stage training designed for uncertainty recognition\nand prompt-sensitive activation. In the first stage, we guide the LLM to reject\nunknown questions. In the second stage, we force the model to follow the\ninstructions by incorporating designed causal instructions. The experimental\nresults demonstrate that our proposed uncertainty-sensitive tuning method\nenhance the model's ability to identify areas of uncertainty. Specifically, it\nachieves a substantial improvement of up to 34.7% in handling questions\ninvolving knowledge gaps compared to the original model. Moreover, our\nfinetuned models even outperform GPT-4, exhibiting an overall performance\nimprovement of up to 4.2%.", "published": "2024-06-14 14:56:04", "link": "http://arxiv.org/abs/2406.10099v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for\n  Southeast Asian Languages", "abstract": "Southeast Asia (SEA) is a region rich in linguistic diversity and cultural\nvariety, with over 1,300 indigenous languages and a population of 671 million\npeople. However, prevailing AI models suffer from a significant lack of\nrepresentation of texts, images, and audio datasets from SEA, compromising the\nquality of AI models for SEA languages. Evaluating models for SEA languages is\nchallenging due to the scarcity of high-quality datasets, compounded by the\ndominance of English training data, raising concerns about potential cultural\nmisrepresentation. To address these challenges, we introduce SEACrowd, a\ncollaborative initiative that consolidates a comprehensive resource hub that\nfills the resource gap by providing standardized corpora in nearly 1,000 SEA\nlanguages across three modalities. Through our SEACrowd benchmarks, we assess\nthe quality of AI models on 36 indigenous languages across 13 tasks, offering\nvaluable insights into the current AI landscape in SEA. Furthermore, we propose\nstrategies to facilitate greater AI advancements, maximizing potential utility\nand resource equity for the future of AI in SEA.", "published": "2024-06-14 15:23:39", "link": "http://arxiv.org/abs/2406.10118v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Devil is in the Neurons: Interpreting and Mitigating Social Biases\n  in Pre-trained Language Models", "abstract": "Pre-trained Language models (PLMs) have been acknowledged to contain harmful\ninformation, such as social biases, which may cause negative social impacts or\neven bring catastrophic results in application. Previous works on this problem\nmainly focused on using black-box methods such as probing to detect and\nquantify social biases in PLMs by observing model outputs. As a result,\nprevious debiasing methods mainly finetune or even pre-train language models on\nnewly constructed anti-stereotypical datasets, which are high-cost. In this\nwork, we try to unveil the mystery of social bias inside language models by\nintroducing the concept of {\\sc Social Bias Neurons}. Specifically, we propose\n{\\sc Integrated Gap Gradients (IG$^2$)} to accurately pinpoint units (i.e.,\nneurons) in a language model that can be attributed to undesirable behavior,\nsuch as social bias. By formalizing undesirable behavior as a distributional\nproperty of language, we employ sentiment-bearing prompts to elicit classes of\nsensitive words (demographics) correlated with such sentiments. Our IG$^2$ thus\nattributes the uneven distribution for different demographics to specific\nSocial Bias Neurons, which track the trail of unwanted behavior inside PLM\nunits to achieve interoperability. Moreover, derived from our interpretable\ntechnique, {\\sc Bias Neuron Suppression (BNS)} is further proposed to mitigate\nsocial biases. By studying BERT, RoBERTa, and their attributable differences\nfrom debiased FairBERTa, IG$^2$ allows us to locate and suppress identified\nneurons, and further mitigate undesired behaviors. As measured by prior metrics\nfrom StereoSet, our model achieves a higher degree of fairness while\nmaintaining language modeling ability with low cost.", "published": "2024-06-14 15:41:06", "link": "http://arxiv.org/abs/2406.10130v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Datasets for Multilingual Answer Sentence Selection", "abstract": "Answer Sentence Selection (AS2) is a critical task for designing effective\nretrieval-based Question Answering (QA) systems. Most advancements in AS2 focus\non English due to the scarcity of annotated datasets for other languages. This\nlack of resources prevents the training of effective AS2 models in different\nlanguages, creating a performance gap between QA systems in English and other\nlocales. In this paper, we introduce new high-quality datasets for AS2 in five\nEuropean languages (French, German, Italian, Portuguese, and Spanish), obtained\nthrough supervised Automatic Machine Translation (AMT) of existing English AS2\ndatasets such as ASNQ, WikiQA, and TREC-QA using a Large Language Model (LLM).\nWe evaluated our approach and the quality of the translated datasets through\nmultiple experiments with different Transformer architectures. The results\nindicate that our datasets are pivotal in producing robust and powerful\nmultilingual AS2 models, significantly contributing to closing the performance\ngap between English and other languages.", "published": "2024-06-14 16:50:29", "link": "http://arxiv.org/abs/2406.10172v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension\n  Abilities of Language Models in E-commerce", "abstract": "Enhancing Language Models' (LMs) ability to understand purchase intentions in\nE-commerce scenarios is crucial for their effective assistance in various\ndownstream tasks. However, previous approaches that distill intentions from LMs\noften fail to generate meaningful and human-centric intentions applicable in\nreal-world E-commerce contexts. This raises concerns about the true\ncomprehension and utilization of purchase intentions by LMs. In this paper, we\npresent IntentionQA, a double-task multiple-choice question answering benchmark\nto evaluate LMs' comprehension of purchase intentions in E-commerce.\nSpecifically, LMs are tasked to infer intentions based on purchased products\nand utilize them to predict additional purchases. IntentionQA consists of 4,360\ncarefully curated problems across three difficulty levels, constructed using an\nautomated pipeline to ensure scalability on large E-commerce platforms. Human\nevaluations demonstrate the high quality and low false-negative rate of our\nbenchmark. Extensive experiments across 19 language models show that they still\nstruggle with certain scenarios, such as understanding products and intentions\naccurately, jointly reasoning with products and intentions, and more, in which\nthey fall far behind human performances. Our code and data are publicly\navailable at https://github.com/HKUST-KnowComp/IntentionQA.", "published": "2024-06-14 16:51:21", "link": "http://arxiv.org/abs/2406.10173v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Let the Poem Hit the Rhythm: Using a Byte-Based Transformer for\n  Beat-Aligned Poetry Generation", "abstract": "The intersection between poetry and music provides an interesting case for\ncomputational creativity, yet remains relatively unexplored. This paper\nexplores the integration of poetry and music through the lens of beat patterns,\ninvestigating whether a byte-based language model can generate words that fit\nspecific beat patterns within the context of poetry. Drawing on earlier\nstudies, we developed a method to train a byte-based transformer model, ByT5,\nto align poems with beat patterns. The results demonstrate a high level of beat\nalignment while maintaining semantic coherence. Future work will aim to improve\nthe model's ability to create complete beat-aligned poems.", "published": "2024-06-14 16:54:48", "link": "http://arxiv.org/abs/2406.10174v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CHIRON: Rich Character Representations in Long-Form Narratives", "abstract": "Characters are integral to long-form narratives, but are poorly understood by\nexisting story analysis and generation systems. While prior work has simplified\ncharacters via graph-based methods and brief character descriptions, we aim to\nbetter tackle the problem of representing complex characters by taking\ninspiration from advice given to professional writers. We propose CHIRON, a new\n`character sheet' based representation that organizes and filters textual\ninformation about characters. We construct CHIRON sheets in two steps: a\nGeneration Module that prompts an LLM for character information via\nquestion-answering and a Validation Module that uses automated reasoning and a\ndomain-specific entailment model to eliminate false facts about a character. We\nvalidate CHIRON via the downstream task of masked-character prediction, where\nour experiments show CHIRON is better and more flexible than comparable\nsummary-based baselines. We also show that metrics derived from CHIRON can be\nused to automatically infer character-centricity in stories, and that these\nmetrics align with human judgments.", "published": "2024-06-14 17:23:57", "link": "http://arxiv.org/abs/2406.10190v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Probability--Quality Trade-off in Aligned Language Models and its\n  Relation to Sampling Adaptors", "abstract": "The relationship between the quality of a string, as judged by a human\nreader, and its probability, $p(\\boldsymbol{y})$ under a language model\nundergirds the development of better language models. For example, many popular\nalgorithms for sampling from a language model have been conceived with the goal\nof manipulating $p(\\boldsymbol{y})$ to place higher probability on strings that\nhumans deem of high quality. In this article, we examine the\nprobability--quality relationship in language models explicitly aligned to\nhuman preferences, e.g., through reinforcement learning through human feedback.\nWe show that, when sampling corpora from an aligned language model, there\nexists a trade-off between the strings' average reward and average\nlog-likelihood under the prior language model, i.e., the same model before\nalignment with human preferences. We provide a formal treatment of this\nphenomenon and demonstrate how a choice of sampling adaptor allows for a\nselection of how much likelihood we exchange for the reward.", "published": "2024-06-14 17:38:21", "link": "http://arxiv.org/abs/2406.10203v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Be like a Goldfish, Don't Memorize! Mitigating Memorization in\n  Generative LLMs", "abstract": "Large language models can memorize and repeat their training data, causing\nprivacy and copyright risks. To mitigate memorization, we introduce a subtle\nmodification to the next-token training objective that we call the goldfish\nloss. During training, randomly sampled subsets of tokens are excluded from the\nloss computation. These dropped tokens are not memorized by the model, which\nprevents verbatim reproduction of a complete chain of tokens from the training\nset. We run extensive experiments training billion-scale Llama-2 models, both\npre-trained and trained from scratch, and demonstrate significant reductions in\nextractable memorization with little to no impact on downstream benchmarks.", "published": "2024-06-14 17:44:22", "link": "http://arxiv.org/abs/2406.10209v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Voice Wake-Up for Dysarthria: Mandarin Dysarthria Speech\n  Corpus Release and Customized System Design", "abstract": "Smart home technology has gained widespread adoption, facilitating effortless\ncontrol of devices through voice commands. However, individuals with\ndysarthria, a motor speech disorder, face challenges due to the variability of\ntheir speech. This paper addresses the wake-up word spotting (WWS) task for\ndysarthric individuals, aiming to integrate them into real-world applications.\nTo support this, we release the open-source Mandarin Dysarthria Speech Corpus\n(MDSC), a dataset designed for dysarthric individuals in home environments.\nMDSC encompasses information on age, gender, disease types, and intelligibility\nevaluations. Furthermore, we perform comprehensive experimental analysis on\nMDSC, highlighting the challenges encountered. We also develop a customized\ndysarthria WWS system that showcases robustness in handling intelligibility and\nachieving exceptional performance. MDSC will be released on\nhttps://www.aishelltech.com/AISHELL_6B.", "published": "2024-06-14 03:06:55", "link": "http://arxiv.org/abs/2406.10304v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-depth analysis of recall initiators of medical devices with a Machine\n  Learning-Natural language Processing workflow", "abstract": "Recall initiator identification and assessment are the preliminary steps to\nprevent medical device recall. Conventional analysis tools are inappropriate\nfor processing massive and multi-formatted data comprehensively and completely\nto meet the higher expectations of delicacy management with the increasing\noverall data volume and textual data format. This study presents a\nbigdata-analytics-based machine learning-natural language processing work tool\nto address the shortcomings in dealing efficiency and data process versatility\nof conventional tools in the practical context of big data volume and muti data\nformat. This study identified, assessed and analysed the medical device recall\ninitiators according to the public medical device recall database from 2018 to\n2024 with the ML-NLP tool. The results suggest that the unsupervised\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN) clustering\nalgorithm can present each single recall initiator in a specific manner,\ntherefore helping practitioners to identify the recall reasons comprehensively\nand completely within a short time frame. This is then followed by text\nsimilarity-based textual classification to assist practitioners in controlling\nthe group size of recall initiators and provide managerial insights from the\noperational to the tactical and strategical levels. This ML-NLP work tool can\nnot only capture specific details of each recall initiator but also interpret\nthe inner connection of each existing initiator and can be implemented for risk\nidentification and assessment in the forward SC. Finally, this paper suggests\nsome concluding remarks and presents future works. More proactive practices and\ncontrol solutions for medical device recalls are expected in the future.", "published": "2024-06-14 12:38:49", "link": "http://arxiv.org/abs/2406.10312v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GenQA: Generating Millions of Instructions from a Handful of Prompts", "abstract": "Most public instruction finetuning datasets are relatively small compared to\nthe closed source datasets used to train industry models. To study questions\nabout finetuning at scale, such as curricula and learning rate cooldown\nschedules, there is a need for industrial-scale datasets. However, this scale\nnecessitates a data generation process that is almost entirely automated. In\nthis work, we study methods for generating large instruction datasets from a\nsingle prompt. With little human oversight, we get LLMs to write diverse sets\nof instruction examples ranging from simple completion tasks to complex\nmulti-turn dialogs across a variety of subject areas. When finetuning a Llama-3\n8B base model, our dataset meets or exceeds both WizardLM and Ultrachat on both\nknowledge-intensive leaderboard tasks as well as conversational evaluations. We\nrelease our dataset, the \"generator\" prompts that created it, and our finetuned\nmodel checkpoints.", "published": "2024-06-14 17:44:08", "link": "http://arxiv.org/abs/2406.10323v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EWEK-QA: Enhanced Web and Efficient Knowledge Graph Retrieval for\n  Citation-based Question Answering Systems", "abstract": "The emerging citation-based QA systems are gaining more attention especially\nin generative AI search applications. The importance of extracted knowledge\nprovided to these systems is vital from both accuracy (completeness of\ninformation) and efficiency (extracting the information in a timely manner). In\nthis regard, citation-based QA systems are suffering from two shortcomings.\nFirst, they usually rely only on web as a source of extracted knowledge and\nadding other external knowledge sources can hamper the efficiency of the\nsystem. Second, web-retrieved contents are usually obtained by some simple\nheuristics such as fixed length or breakpoints which might lead to splitting\ninformation into pieces. To mitigate these issues, we propose our enhanced web\nand efficient knowledge graph (KG) retrieval solution (EWEK-QA) to enrich the\ncontent of the extracted knowledge fed to the system. This has been done\nthrough designing an adaptive web retriever and incorporating KGs triples in an\nefficient manner. We demonstrate the effectiveness of EWEK-QA over the\nopen-source state-of-the-art (SoTA) web-based and KG baseline models using a\ncomprehensive set of quantitative and human evaluation experiments. Our model\nis able to: first, improve the web-retriever baseline in terms of extracting\nmore relevant passages (>20\\%), the coverage of answer span (>25\\%) and self\ncontainment (>35\\%); second, obtain and integrate KG triples into its pipeline\nvery efficiently (by avoiding any LLM calls) to outperform the web-only and\nKG-only SoTA baselines significantly in 7 quantitative QA tasks and our human\nevaluation.", "published": "2024-06-14 19:40:38", "link": "http://arxiv.org/abs/2406.10393v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Reflection Makes Large Language Models Safer, Less Biased, and\n  Ideologically Neutral", "abstract": "Previous studies proposed that the reasoning capabilities of large language\nmodels (LLMs) can be improved through self-reflection, i.e., letting LLMs\nreflect on their own output to identify and correct mistakes in the initial\nresponses. However, earlier experiments offer mixed results when it comes to\nthe benefits of self-reflection. Furthermore, prior studies on self-reflection\nare predominantly concerned with the reasoning capabilities of models, ignoring\nthe potential for self-reflection in safety, bias, and ideological leaning.\nHere, by conducting a series of experiments testing LLM's self-reflection\ncapability in various tasks using a variety of prompts and different LLMs, we\nmake several contributions to the literature. First, we reconcile conflicting\nfindings regarding the benefit of self-reflection, by demonstrating that the\noutcome of self-reflection is sensitive to prompt wording -- both the original\nprompt that are used to elicit an initial answer and the subsequent prompt used\nto self-reflect. Specifically, although self-reflection may improve the\nreasoning capability of LLMs when the initial response is simple, the technique\ncannot improve upon the state-of-the-art chain-of-thought (CoT) prompting.\nSecond, we show that self-reflection can lead to safer (75.8\\% reduction in\ntoxic responses while preserving 97.8\\% non-toxic ones), less biased (77\\%\nreduction in gender biased responses, while preserving 94.3\\% unbiased ones),\nand more ideologically neutral responses (100\\% reduction in partisan leaning\nresponse, while preserving 87.7\\% non-partisan ones). The paper concludes by\ndiscussing the implications of our findings on the deployment of large language\nmodels. We release our experiments at\nhttps://github.com/Michael98Liu/self-reflection.", "published": "2024-06-14 20:07:11", "link": "http://arxiv.org/abs/2406.10400v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SciEx: Benchmarking Large Language Models on Scientific Exams with Human\n  Expert Grading and Automatic Grading", "abstract": "With the rapid development of Large Language Models (LLMs), it is crucial to\nhave benchmarks which can evaluate the ability of LLMs on different domains.\nOne common use of LLMs is performing tasks on scientific topics, such as\nwriting algorithms, querying databases or giving mathematical proofs. Inspired\nby the way university students are evaluated on such tasks, in this paper, we\npropose SciEx - a benchmark consisting of university computer science exam\nquestions, to evaluate LLMs ability on solving scientific tasks. SciEx is (1)\nmultilingual, containing both English and German exams, and (2) multi-modal,\ncontaining questions that involve images, and (3) contains various types of\nfreeform questions with different difficulty levels, due to the nature of\nuniversity exams. We evaluate the performance of various state-of-the-art LLMs\non our new benchmark. Since SciEx questions are freeform, it is not\nstraightforward to evaluate LLM performance. Therefore, we provide human expert\ngrading of the LLM outputs on SciEx. We show that the free-form exams in SciEx\nremain challenging for the current LLMs, where the best LLM only achieves\n59.4\\% exam grade on average. We also provide detailed comparisons between LLM\nperformance and student performance on SciEx. To enable future evaluation of\nnew LLMs, we propose using LLM-as-a-judge to grade the LLM answers on SciEx.\nOur experiments show that, although they do not perform perfectly on solving\nthe exams, LLMs are decent as graders, achieving 0.948 Pearson correlation with\nexpert grading.", "published": "2024-06-14 21:52:21", "link": "http://arxiv.org/abs/2406.10421v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "AMR-RE: Abstract Meaning Representations for Retrieval-Based In-Context\n  Learning in Relation Extraction", "abstract": "Existing in-context learning (ICL) methods for relation extraction (RE) often\nprioritize language similarity over structural similarity, which can lead to\noverlooking entity relationships. To address this, we propose an AMR-enhanced\nretrieval-based ICL method for RE. Our model retrieves in-context examples\nbased on semantic structure similarity between task inputs and training\nsamples. Evaluations on four standard English RE datasets show that our model\noutperforms baselines in the unsupervised setting across all datasets. In the\nsupervised setting, it achieves state-of-the-art results on three datasets and\ncompetitive results on the fourth.", "published": "2024-06-14 22:36:08", "link": "http://arxiv.org/abs/2406.10432v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain-Specific Shorthand for Generation Based on Context-Free Grammar", "abstract": "The generation of structured data in formats such as JSON, YAML and XML is a\ncritical task in Generative AI (GenAI) applications. These formats, while\nwidely used, contain many redundant constructs that lead to inflated token\nusage. This inefficiency is particularly evident when employing large language\nmodels (LLMs) like GPT-4, where generating extensive structured data incurs\nincreased latency and operational costs. We introduce a domain-specific\nshorthand (DSS) format, underpinned by a context-free grammar (CFG), and\ndemonstrate its usage to reduce the number of tokens required for structured\ndata generation. The method involves creating a shorthand notation that\ncaptures essential elements of the output schema with fewer tokens, ensuring it\ncan be unambiguously converted to and from its verbose form. It employs a CFG\nto facilitate efficient shorthand generation by the LLM, and to create parsers\nto translate the shorthand back into standard structured formats. The\napplication of our approach to data visualization with LLMs demonstrates a\nsignificant (3x to 5x) reduction in generated tokens, leading to significantly\nlower latency and cost. This paper outlines the development of the DSS and the\naccompanying CFG, and the implications of this approach for GenAI applications,\npresenting a scalable solution to the token inefficiency problem in structured\ndata generation.", "published": "2024-06-14 23:26:41", "link": "http://arxiv.org/abs/2406.10442v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A\n  Survey", "abstract": "Within the evolving landscape of deep learning, the dilemma of data quantity\nand quality has been a long-standing problem. The recent advent of Large\nLanguage Models (LLMs) offers a data-centric solution to alleviate the\nlimitations of real-world data with synthetic data generation. However, current\ninvestigations into this field lack a unified framework and mostly stay on the\nsurface. Therefore, this paper provides an organization of relevant studies\nbased on a generic workflow of synthetic data generation. By doing so, we\nhighlight the gaps within existing research and outline prospective avenues for\nfuture study. This work aims to shepherd the academic and industrial\ncommunities towards deeper, more methodical inquiries into the capabilities and\napplications of LLMs-driven synthetic data generation.", "published": "2024-06-14 07:47:09", "link": "http://arxiv.org/abs/2406.15126v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Integrating Large Language Models with Graph-based Reasoning for\n  Conversational Question Answering", "abstract": "We focus on a conversational question answering task which combines the\nchallenges of understanding questions in context and reasoning over evidence\ngathered from heterogeneous sources like text, knowledge graphs, tables, and\ninfoboxes. Our method utilizes a graph structured representation to aggregate\ninformation about a question and its context (i.e., the conversation so far and\nevidence retrieved to find an answer), while also harnessing the reasoning and\ntext generation capabilities of large language models (LLMs). Graph embeddings\nare directly injected into the LLM, bypassing the token embedding layers, and\nlearned end-to-end by minimizing cross-entropy. Our model maintains a memory\nmodule to track and update past evidence, thus influencing the graph's\nstructure, as the conversation evolves. Experimental results on the ConvMix\nbenchmark(Christmann et al., 2022a) show that graph embeddings enhance the\nLLM's ability to reason, while the memory module provides robustness against\nnoise and retrieval errors.", "published": "2024-06-14 13:28:03", "link": "http://arxiv.org/abs/2407.09506v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating ChatGPT-4 Vision on Brazil's National Undergraduate Computer\n  Science Exam", "abstract": "The recent integration of visual capabilities into Large Language Models\n(LLMs) has the potential to play a pivotal role in science and technology\neducation, where visual elements such as diagrams, charts, and tables are\ncommonly used to improve the learning experience. This study investigates the\nperformance of ChatGPT-4 Vision, OpenAI's most advanced visual model at the\ntime the study was conducted, on the Bachelor in Computer Science section of\nBrazil's 2021 National Undergraduate Exam (ENADE). By presenting the model with\nthe exam's open and multiple-choice questions in their original image format\nand allowing for reassessment in response to differing answer keys, we were\nable to evaluate the model's reasoning and self-reflecting capabilities in a\nlarge-scale academic assessment involving textual and visual content. ChatGPT-4\nVision significantly outperformed the average exam participant, positioning\nitself within the top 10 best score percentile. While it excelled in questions\nthat incorporated visual elements, it also encountered challenges with question\ninterpretation, logical reasoning, and visual acuity. The involvement of an\nindependent expert panel to review cases of disagreement between the model and\nthe answer key revealed some poorly constructed questions containing vague or\nambiguous statements, calling attention to the critical need for improved\nquestion design in future exams. Our findings suggest that while ChatGPT-4\nVision shows promise in multimodal academic evaluations, human oversight\nremains crucial for verifying the model's accuracy and ensuring the fairness of\nhigh-stakes educational exams. The paper's research materials are publicly\navailable at https://github.com/nabormendonca/gpt-4v-enade-cs-2021.", "published": "2024-06-14 02:42:30", "link": "http://arxiv.org/abs/2406.09671v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Optimizing Byte-level Representation for End-to-end ASR", "abstract": "We propose a novel approach to optimizing a byte-level representation for\nend-to-end automatic speech recognition (ASR). Byte-level representation is\noften used by large scale multilingual ASR systems when the character set of\nthe supported languages is large. The compactness and universality of\nbyte-level representation allow the ASR models to use smaller output\nvocabularies and therefore, provide more flexibility. UTF-8 is a commonly used\nbyte-level representation for multilingual ASR, but it is not designed to\noptimize machine learning tasks directly. By using auto-encoder and vector\nquantization, we show that we can optimize a byte-level representation for ASR\nand achieve better accuracy. Our proposed framework can incorporate information\nfrom different modalities, and provides an error correction mechanism. In an\nEnglish/Mandarin dictation task, we show that a bilingual ASR model built with\nthis approach can outperform UTF-8 representation by 5% relative in error rate.", "published": "2024-06-14 02:58:19", "link": "http://arxiv.org/abs/2406.09676v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Self-Knowledge Distillation for Learning Ambiguity", "abstract": "Recent language models have shown remarkable performance on natural language\nunderstanding (NLU) tasks. However, they are often sub-optimal when faced with\nambiguous samples that can be interpreted in multiple ways, over-confidently\npredicting a single label without consideration for its correctness. To address\nthis issue, we propose a novel self-knowledge distillation method that enables\nmodels to learn label distributions more accurately by leveraging knowledge\ndistilled from their lower layers. This approach also includes a learning phase\nthat re-calibrates the unnecessarily strengthened confidence for training\nsamples judged as extremely ambiguous based on the distilled distribution\nknowledge. We validate our method on diverse NLU benchmark datasets and the\nexperimental results demonstrate its effectiveness in producing better label\ndistributions. Particularly, through the process of re-calibrating the\nconfidence for highly ambiguous samples, the issue of over-confidence when\npredictions for unseen samples do not match with their ground-truth labels has\nbeen significantly alleviated. This has been shown to contribute to generating\nbetter distributions than the existing state-of-the-art method. Moreover, our\nmethod is more efficient in training the models compared to the existing\nmethod, as it does not involve additional training processes to refine label\ndistributions.", "published": "2024-06-14 05:11:32", "link": "http://arxiv.org/abs/2406.09719v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bootstrapping Language Models with DPO Implicit Rewards", "abstract": "Human alignment in large language models (LLMs) is an active area of\nresearch. A recent groundbreaking work, direct preference optimization (DPO),\nhas greatly simplified the process from past work in reinforcement learning\nfrom human feedback (RLHF) by bypassing the reward learning stage in RLHF. DPO,\nafter training, provides an implicit reward model. In this work, we make a\nnovel observation that this implicit reward model can by itself be used in a\nbootstrapping fashion to further align the LLM. Our approach is to use the\nrewards from a current LLM to construct a preference dataset, which is then\nused in subsequent DPO rounds. We incorporate two refinements to further\nimprove our approach: 1) length-regularized reward shaping to make the\npreference dataset length-unbiased; 2) experience replay to enhance the quality\nof the preference dataset. Our approach, named self-alignment with DPO ImpliCit\nrEwards (DICE), shows great improvements in alignment. It achieves an increase\nof more than 8$\\\\%$ in lengthcontrolled win rate on AlpacaEval 2 for all the\ndifferent base models that we tried, without relying on external feedback. Our\ncode is available at https://github.com/sail-sg/dice.", "published": "2024-06-14 06:57:18", "link": "http://arxiv.org/abs/2406.09760v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Application of Natural Language Processing in Financial Risk Detection", "abstract": "This paper explores the application of Natural Language Processing (NLP) in\nfinancial risk detection. By constructing an NLP-based financial risk detection\nmodel, this study aims to identify and predict potential risks in financial\ndocuments and communications. First, the fundamental concepts of NLP and its\ntheoretical foundation, including text mining methods, NLP model design\nprinciples, and machine learning algorithms, are introduced. Second, the\nprocess of text data preprocessing and feature extraction is described.\nFinally, the effectiveness and predictive performance of the model are\nvalidated through empirical research. The results show that the NLP-based\nfinancial risk detection model performs excellently in risk identification and\nprediction, providing effective risk management tools for financial\ninstitutions. This study offers valuable references for the field of financial\nrisk management, utilizing advanced NLP techniques to improve the accuracy and\nefficiency of financial risk detection.", "published": "2024-06-14 07:06:24", "link": "http://arxiv.org/abs/2406.09765v2", "categories": ["q-fin.RM", "cs.CL"], "primary_category": "q-fin.RM"}
{"title": "Retrieval Augmented Fact Verification by Synthesizing Contrastive\n  Arguments", "abstract": "The rapid propagation of misinformation poses substantial risks to public\ninterest. To combat misinformation, large language models (LLMs) are adapted to\nautomatically verify claim credibility. Nevertheless, existing methods heavily\nrely on the embedded knowledge within LLMs and / or black-box APIs for evidence\ncollection, leading to subpar performance with smaller LLMs or upon unreliable\ncontext. In this paper, we propose retrieval augmented fact verification\nthrough the synthesis of contrasting arguments (RAFTS). Upon input claims,\nRAFTS starts with evidence retrieval, where we design a retrieval pipeline to\ncollect and re-rank relevant documents from verifiable sources. Then, RAFTS\nforms contrastive arguments (i.e., supporting or refuting) conditioned on the\nretrieved evidence. In addition, RAFTS leverages an embedding model to identify\ninformative demonstrations, followed by in-context prompting to generate the\nprediction and explanation. Our method effectively retrieves relevant documents\nas evidence and evaluates arguments from varying perspectives, incorporating\nnuanced information for fine-grained decision-making. Combined with informative\nin-context examples as prior, RAFTS achieves significant improvements to\nsupervised and LLM baselines without complex prompts. We demonstrate the\neffectiveness of our method through extensive experiments, where RAFTS can\noutperform GPT-based methods with a significantly smaller 7B LLM.", "published": "2024-06-14 08:13:34", "link": "http://arxiv.org/abs/2406.09815v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rapport-Driven Virtual Agent: Rapport Building Dialogue Strategy for\n  Improving User Experience at First Meeting", "abstract": "Rapport is known as a conversational aspect focusing on relationship\nbuilding, which influences outcomes in collaborative tasks. This study aims to\nestablish human-agent rapport through small talk by using a rapport-building\nstrategy. We implemented this strategy for the virtual agents based on dialogue\nstrategies by prompting a large language model (LLM). In particular, we\nutilized two dialogue strategies-predefined sequence and free-form-to guide the\ndialogue generation framework. We conducted analyses based on human\nevaluations, examining correlations between total turn, utterance characters,\nrapport score, and user experience variables: naturalness, satisfaction,\ninterest, engagement, and usability. We investigated correlations between\nrapport score and naturalness, satisfaction, engagement, and conversation flow.\nOur experimental results also indicated that using free-form to prompt the\nrapport-building strategy performed the best in subjective scores.", "published": "2024-06-14 08:47:15", "link": "http://arxiv.org/abs/2406.09839v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Knowledge Editing in Language Models via Adapted Direct Preference\n  Optimization", "abstract": "Large Language Models (LLMs) can become outdated over time as they may lack\nupdated world knowledge, leading to factual knowledge errors and gaps.\nKnowledge Editing (KE) aims to overcome this challenge using weight updates\nthat do not require expensive retraining. We propose treating KE as an LLM\nalignment problem. Toward this goal, we introduce Knowledge Direct Preference\nOptimization (KDPO), a variation of the Direct Preference Optimization (DPO)\nthat is more effective for knowledge modifications. Our method is based on an\nonline approach that continually updates the knowledge stored in the model. We\nuse the current knowledge as a negative sample and the new knowledge we want to\nintroduce as a positive sample in a process called DPO. We also use\nteacher-forcing for negative sample generation and optimize using the positive\nsample, which helps maintain localized changes. We tested our KE method on\nvarious datasets and models, comparing it to several cutting-edge methods, with\n100 and 500 sequential edits. Additionally, we conducted an ablation study\ncomparing our method to the standard DPO approach. Our experimental results\nshow that our modified DPO method allows for more refined KE, achieving similar\nor better performance compared to previous methods.", "published": "2024-06-14 11:02:21", "link": "http://arxiv.org/abs/2406.09920v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Experiments in News Bias Detection with Pre-Trained Neural Transformers", "abstract": "The World Wide Web provides unrivalled access to information globally,\nincluding factual news reporting and commentary. However, state actors and\ncommercial players increasingly spread biased (distorted) or fake (non-factual)\ninformation to promote their agendas. We compare several large, pre-trained\nlanguage models on the task of sentence-level news bias detection and sub-type\nclassification, providing quantitative and qualitative results.", "published": "2024-06-14 11:34:36", "link": "http://arxiv.org/abs/2406.09938v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FZI-WIM at SemEval-2024 Task 2: Self-Consistent CoT for Complex NLI in\n  Biomedical Domain", "abstract": "This paper describes the inference system of FZI-WIM at the SemEval-2024 Task\n2: Safe Biomedical Natural Language Inference for Clinical Trials. Our system\nutilizes the chain of thought (CoT) paradigm to tackle this complex reasoning\nproblem and further improves the CoT performance with self-consistency. Instead\nof greedy decoding, we sample multiple reasoning chains with the same prompt\nand make the final verification with majority voting. The self-consistent CoT\nsystem achieves a baseline F1 score of 0.80 (1st), faithfulness score of 0.90\n(3rd), and consistency score of 0.73 (12th). We release the code and data\npublicly https://github.com/jens5588/FZI-WIM-NLI4CT.", "published": "2024-06-14 13:49:07", "link": "http://arxiv.org/abs/2406.10040v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluation of Large Language Models: STEM education and Gender\n  Stereotypes", "abstract": "Large Language Models (LLMs) have an increasing impact on our lives with use\ncases such as chatbots, study support, coding support, ideation, writing\nassistance, and more. Previous studies have revealed linguistic biases in\npronouns used to describe professions or adjectives used to describe men vs\nwomen. These issues have to some degree been addressed in updated LLM versions,\nat least to pass existing tests. However, biases may still be present in the\nmodels, and repeated use of gender stereotypical language may reinforce the\nunderlying assumptions and are therefore important to examine further. This\npaper investigates gender biases in LLMs in relation to educational choices\nthrough an open-ended, true to user-case experimental design and a quantitative\nanalysis. We investigate the biases in the context of four different cultures,\nlanguages, and educational systems (English/US/UK, Danish/DK, Catalan/ES, and\nHindi/IN) for ages ranging from 10 to 16 years, corresponding to important\neducational transition points in the different countries. We find that there\nare significant and large differences in the ratio of STEM to non-STEM\nsuggested education paths provided by chatGPT when using typical girl vs boy\nnames to prompt lists of suggested things to become. There are generally fewer\nSTEM suggestions in the Danish, Spanish, and Indian context compared to the\nEnglish. We also find subtle differences in the suggested professions, which we\ncategorise and report.", "published": "2024-06-14 15:42:42", "link": "http://arxiv.org/abs/2406.10133v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BABILong: Testing the Limits of LLMs with Long Context\n  Reasoning-in-a-Haystack", "abstract": "In recent years, the input context sizes of large language models (LLMs) have\nincreased dramatically. However, existing evaluation methods have not kept\npace, failing to comprehensively assess the efficiency of models in handling\nlong contexts. To bridge this gap, we introduce the BABILong benchmark,\ndesigned to test language models' ability to reason across facts distributed in\nextremely long documents. BABILong includes a diverse set of 20 reasoning\ntasks, including fact chaining, simple induction, deduction, counting, and\nhandling lists/sets. These tasks are challenging on their own, and even more\ndemanding when the required facts are scattered across long natural text. Our\nevaluations show that popular LLMs effectively utilize only 10-20\\% of the\ncontext and their performance declines sharply with increased reasoning\ncomplexity. Among alternatives to in-context reasoning, Retrieval-Augmented\nGeneration methods achieve a modest 60\\% accuracy on single-fact question\nanswering, independent of context length. Among context extension methods, the\nhighest performance is demonstrated by recurrent memory transformers after\nfine-tuning, enabling the processing of lengths up to 50 million tokens. The\nBABILong benchmark is extendable to any length to support the evaluation of new\nupcoming models with increased capabilities, and we provide splits up to 10\nmillion token lengths.", "published": "2024-06-14 16:00:29", "link": "http://arxiv.org/abs/2406.10149v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sycophancy to Subterfuge: Investigating Reward-Tampering in Large\n  Language Models", "abstract": "In reinforcement learning, specification gaming occurs when AI systems learn\nundesired behaviors that are highly rewarded due to misspecified training\ngoals. Specification gaming can range from simple behaviors like sycophancy to\nsophisticated and pernicious behaviors like reward-tampering, where a model\ndirectly modifies its own reward mechanism. However, these more pernicious\nbehaviors may be too complex to be discovered via exploration. In this paper,\nwe study whether Large Language Model (LLM) assistants which find easily\ndiscovered forms of specification gaming will generalize to perform rarer and\nmore blatant forms, up to and including reward-tampering. We construct a\ncurriculum of increasingly sophisticated gameable environments and find that\ntraining on early-curriculum environments leads to more specification gaming on\nremaining environments. Strikingly, a small but non-negligible proportion of\nthe time, LLM assistants trained on the full curriculum generalize zero-shot to\ndirectly rewriting their own reward function. Retraining an LLM not to game\nearly-curriculum environments mitigates, but does not eliminate,\nreward-tampering in later environments. Moreover, adding harmlessness training\nto our gameable environments does not prevent reward-tampering. These results\ndemonstrate that LLMs can generalize from common forms of specification gaming\nto more pernicious reward tampering and that such behavior may be nontrivial to\nremove.", "published": "2024-06-14 16:26:20", "link": "http://arxiv.org/abs/2406.10162v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Inclusive ASR for Disfluent Speech: Cascaded Large-Scale Self-Supervised\n  Learning with Targeted Fine-Tuning and Data Augmentation", "abstract": "Automatic speech recognition (ASR) systems often falter while processing\nstuttering-related disfluencies -- such as involuntary blocks and word\nrepetitions -- yielding inaccurate transcripts. A critical barrier to progress\nis the scarcity of large, annotated disfluent speech datasets. Therefore, we\npresent an inclusive ASR design approach, leveraging large-scale\nself-supervised learning on standard speech followed by targeted fine-tuning\nand data augmentation on a smaller, curated dataset of disfluent speech. Our\ndata augmentation technique enriches training datasets with various\ndisfluencies, enhancing ASR processing of these speech patterns. Results show\nthat fine-tuning wav2vec 2.0 with even a relatively small, labeled dataset,\nalongside data augmentation, can significantly reduce word error rates for\ndisfluent speech. Our approach not only advances ASR inclusivity for people who\nstutter, but also paves the way for ASRs that can accommodate wider speech\nvariations.", "published": "2024-06-14 16:56:40", "link": "http://arxiv.org/abs/2406.10177v2", "categories": ["eess.AS", "cs.CL", "I.2; K.4"], "primary_category": "eess.AS"}
{"title": "DevBench: A multimodal developmental benchmark for language learning", "abstract": "How (dis)similar are the learning trajectories of vision-language models and\nchildren? Recent modeling work has attempted to understand the gap between\nmodels' and humans' data efficiency by constructing models trained on less\ndata, especially multimodal naturalistic data. However, such models are often\nevaluated on adult-level benchmarks, with limited breadth in language abilities\ntested, and without direct comparison to behavioral data. We introduce\nDevBench, a multimodal benchmark comprising seven language evaluation tasks\nspanning the domains of lexical, syntactic, and semantic ability, with\nbehavioral data from both children and adults. We evaluate a set of\nvision-language models on these tasks, comparing models and humans not only on\naccuracy but on their response patterns. Across tasks, models exhibit variation\nin their closeness to human response patterns, and models that perform better\non a task also more closely resemble human behavioral responses. We also\nexamine the developmental trajectory of OpenCLIP over training, finding that\ngreater training results in closer approximations to adult response patterns.\nDevBench thus provides a benchmark for comparing models to human language\ndevelopment. These comparisons highlight ways in which model and human language\nlearning processes diverge, providing insight into entry points for improving\nlanguage models.", "published": "2024-06-14 17:49:41", "link": "http://arxiv.org/abs/2406.10215v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Regularizing Hidden States Enables Learning Generalizable Reward Model\n  for LLMs", "abstract": "Reward models trained on human preference data have been proven to\neffectively align Large Language Models (LLMs) with human intent within the\nframework of reinforcement learning from human feedback (RLHF). However,\ncurrent reward models have limited generalization capabilities to unseen\nprompts and responses, which can lead to an unexpected phenomenon known as\nreward over-optimization, resulting in a decline in actual performance due to\nexcessive optimization of rewards. While previous research has advocated for\nconstraining policy optimization, our study introduces a novel approach to\nenhance the reward model's generalization ability against distribution shifts\nby regularizing the hidden states. Specifically, we retain the base model's\nlanguage model head and incorporate a suite of text-generation losses to\npreserve the hidden states' text-generation capabilities, while concurrently\nlearning a reward head behind the same hidden states. Our experimental results\ndemonstrate that the introduced regularization technique markedly improves the\naccuracy of learned reward models across a variety of out-of-distribution (OOD)\ntasks and effectively alleviates the over-optimization issue in RLHF, offering\na more reliable and robust preference learning paradigm.", "published": "2024-06-14 17:49:59", "link": "http://arxiv.org/abs/2406.10216v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey on Large Language Models from General Purpose to Medical\n  Applications: Datasets, Methodologies, and Evaluations", "abstract": "Large Language Models (LLMs) have demonstrated surprising performance across\nvarious natural language processing tasks. Recently, medical LLMs enhanced with\ndomain-specific knowledge have exhibited excellent capabilities in medical\nconsultation and diagnosis. These models can smoothly simulate doctor-patient\ndialogues and provide professional medical advice. Most medical LLMs are\ndeveloped through continued training of open-source general LLMs, which require\nsignificantly fewer computational resources than training LLMs from scratch.\nAdditionally, this approach offers better patient privacy protection than\nAPI-based solutions. Given the above advantages, this survey systematically\nsummarizes how to train medical LLMs based on open-source general LLMs from a\nmore fine-grained perspective. It covers (a) how to acquire training corpus and\nconstruct customized medical training sets, (b) how to choose an appropriate\ntraining paradigm, (c) how to choose a suitable evaluation benchmark, and (d)\nexisting challenges and promising research directions are discussed. This\nsurvey can provide guidance for the development of LLMs focused on various\nmedical applications, such as medical education, diagnostic planning, and\nclinical assistants. Related resources and supplemental information can be\nfound on the GitHub repository.", "published": "2024-06-14 02:42:20", "link": "http://arxiv.org/abs/2406.10303v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What is the best model? Application-driven Evaluation for Large Language\n  Models", "abstract": "General large language models enhanced with supervised fine-tuning and\nreinforcement learning from human feedback are increasingly popular in academia\nand industry as they generalize foundation models to various practical tasks in\na prompt manner. To assist users in selecting the best model in practical\napplication scenarios, i.e., choosing the model that meets the application\nrequirements while minimizing cost, we introduce A-Eval, an application-driven\nLLMs evaluation benchmark for general large language models. First, we\ncategorize evaluation tasks into five main categories and 27 sub-categories\nfrom a practical application perspective. Next, we construct a dataset\ncomprising 678 question-and-answer pairs through a process of collecting,\nannotating, and reviewing. Then, we design an objective and effective\nevaluation method and evaluate a series of LLMs of different scales on A-Eval.\nFinally, we reveal interesting laws regarding model scale and task difficulty\nlevel and propose a feasible method for selecting the best model. Through\nA-Eval, we provide clear empirical and engineer guidance for selecting the best\nmodel, reducing barriers to selecting and using LLMs and promoting their\napplication and development. Our benchmark is publicly available at\nhttps://github.com/UnicomAI/DataSet/tree/main/TestData/GeneralAbility.", "published": "2024-06-14 04:52:15", "link": "http://arxiv.org/abs/2406.10307v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TEG-DB: A Comprehensive Dataset and Benchmark of Textual-Edge Graphs", "abstract": "Text-Attributed Graphs (TAGs) augment graph structures with natural language\ndescriptions, facilitating detailed depictions of data and their\ninterconnections across various real-world settings. However, existing TAG\ndatasets predominantly feature textual information only at the nodes, with\nedges typically represented by mere binary or categorical attributes. This lack\nof rich textual edge annotations significantly limits the exploration of\ncontextual relationships between entities, hindering deeper insights into\ngraph-structured data. To address this gap, we introduce Textual-Edge Graphs\nDatasets and Benchmark (TEG-DB), a comprehensive and diverse collection of\nbenchmark textual-edge datasets featuring rich textual descriptions on nodes\nand edges. The TEG-DB datasets are large-scale and encompass a wide range of\ndomains, from citation networks to social networks. In addition, we conduct\nextensive benchmark experiments on TEG-DB to assess the extent to which current\ntechniques, including pre-trained language models, graph neural networks, and\ntheir combinations, can utilize textual node and edge information. Our goal is\nto elicit advancements in textual-edge graph research, specifically in\ndeveloping methodologies that exploit rich textual node and edge descriptions\nto enhance graph analysis and provide deeper insights into complex real-world\nnetworks. The entire TEG-DB project is publicly accessible as an open-source\nrepository on Github, accessible at\nhttps://github.com/Zhuofeng-Li/TEG-Benchmark.", "published": "2024-06-14 06:22:47", "link": "http://arxiv.org/abs/2406.10310v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CHiSafetyBench: A Chinese Hierarchical Safety Benchmark for Large\n  Language Models", "abstract": "With the profound development of large language models(LLMs), their safety\nconcerns have garnered increasing attention. However, there is a scarcity of\nChinese safety benchmarks for LLMs, and the existing safety taxonomies are\ninadequate, lacking comprehensive safety detection capabilities in authentic\nChinese scenarios. In this work, we introduce CHiSafetyBench, a dedicated\nsafety benchmark for evaluating LLMs' capabilities in identifying risky content\nand refusing answering risky questions in Chinese contexts. CHiSafetyBench\nincorporates a dataset that covers a hierarchical Chinese safety taxonomy\nconsisting of 5 risk areas and 31 categories. This dataset comprises two types\nof tasks: multiple-choice questions and question-answering, evaluating LLMs\nfrom the perspectives of risk content identification and the ability to refuse\nanswering risky questions respectively. Utilizing this benchmark, we validate\nthe feasibility of automatic evaluation as a substitute for human evaluation\nand conduct comprehensive automatic safety assessments on mainstream Chinese\nLLMs. Our experiments reveal the varying performance of different models across\nvarious safety domains, indicating that all models possess considerable\npotential for improvement in Chinese safety capabilities. Our dataset is\npublicly available at\nhttps://github.com/UnicomAI/UnicomBenchmark/tree/main/CHiSafetyBench.", "published": "2024-06-14 06:47:40", "link": "http://arxiv.org/abs/2406.10311v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CNVSRC 2023: The First Chinese Continuous Visual Speech Recognition\n  Challenge", "abstract": "The first Chinese Continuous Visual Speech Recognition Challenge aimed to\nprobe the performance of Large Vocabulary Continuous Visual Speech Recognition\n(LVC-VSR) on two tasks: (1) Single-speaker VSR for a particular speaker and (2)\nMulti-speaker VSR for a set of registered speakers. The challenge yielded\nhighly successful results, with the best submission significantly outperforming\nthe baseline, particularly in the single-speaker task. This paper\ncomprehensively reviews the challenge, encompassing the data profile, task\nspecifications, and baseline system construction. It also summarises the\nrepresentative techniques employed by the submitted systems, highlighting the\nmost effective approaches. Additional information and resources about this\nchallenge can be accessed through the official website at\nhttp://cnceleb.org/competition.", "published": "2024-06-14 12:49:38", "link": "http://arxiv.org/abs/2406.10313v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Efficient Prompting for LLM-based Generative Internet of Things", "abstract": "Large language models (LLMs) have demonstrated remarkable capacities on\nvarious tasks, and integrating the capacities of LLMs into the Internet of\nThings (IoT) applications has drawn much research attention recently. Due to\nsecurity concerns, many institutions avoid accessing state-of-the-art\ncommercial LLM services, requiring the deployment and utilization of\nopen-source LLMs in a local network setting. However, open-source LLMs usually\nhave more limitations regarding their performance, such as their arithmetic\ncalculation and reasoning capacities, and practical systems of applying LLMs to\nIoT have yet to be well-explored. Therefore, we propose a LLM-based Generative\nIoT (GIoT) system deployed in the local network setting in this study. To\nalleviate the limitations of LLMs and provide service with competitive\nperformance, we apply prompt engineering methods to enhance the capacities of\nthe open-source LLMs, design a Prompt Management Module and a Post-processing\nModule to manage the tailored prompts for different tasks and process the\nresults generated by the LLMs. To demonstrate the effectiveness of the proposed\nsystem, we discuss a challenging Table Question Answering (Table-QA) task as a\ncase study of the proposed system, as tabular data is usually more challenging\nthan plain text because of their complex structures, heterogeneous data types\nand sometimes huge sizes. We conduct comprehensive experiments on two popular\nTable-QA datasets, and the results show that our proposal can achieve\ncompetitive performance compared with state-of-the-art LLMs, demonstrating that\nthe proposed LLM-based GIoT system can provide competitive performance with\ntailored prompting methods and is easily extensible to new tasks without\ntraining.", "published": "2024-06-14 19:24:00", "link": "http://arxiv.org/abs/2406.10382v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Learning thresholds lead to stable language coexistence", "abstract": "We introduce a language competition model that is based on the\nAbrams-Strogatz model and incorporates the effects of memory and learning in\nthe language shift dynamics. On a coarse grained time scale, the effects of\nmemory and learning can be expressed as thresholds on the speakers fractions of\nthe competing languages. In its simplest form, the resulting model is exactly\nsolvable. Besides the consensus on one of the two languages, the model\ndescribes additional equilibrium states that are not present in the\nAbrams-Strogatz model: a stable dynamical coexistence of the two languages and\na frozen state coinciding with the initial state. We show numerically that\nthese results are preserved for threshold functions of a more general shape.\nThe comparison of the model predictions with historical datasets demonstrates\nthat while the Abrams-Strogatz model fails to describe some relevant language\ncompetition situations, the proposed model provides a good fitting.", "published": "2024-06-14 14:24:02", "link": "http://arxiv.org/abs/2406.14522v2", "categories": ["physics.soc-ph", "cs.CL"], "primary_category": "physics.soc-ph"}
{"title": "RadEx: A Framework for Structured Information Extraction from Radiology\n  Reports based on Large Language Models", "abstract": "Annually and globally, over three billion radiography examinations and\ncomputer tomography scans result in mostly unstructured radiology reports\ncontaining free text. Despite the potential benefits of structured reporting,\nits adoption is limited by factors such as established processes, resource\nconstraints and potential loss of information. However, structured information\nwould be necessary for various use cases, including automatic analysis,\nclinical trial matching, and prediction of health outcomes. This study\nintroduces RadEx, an end-to-end framework comprising 15 software components and\nten artifacts to develop systems that perform automated information extraction\nfrom radiology reports. It covers the complete process from annotating training\ndata to extracting information by offering a consistent generic information\nmodel and setting boundaries for model development. Specifically, RadEx allows\nclinicians to define relevant information for clinical domains (e.g.,\nmammography) and to create report templates. The framework supports both\ngenerative and encoder-only models and the decoupling of information extraction\nfrom template filling enables independent model improvements. Developing\ninformation extraction systems according to the RadEx framework facilitates\nimplementation and maintenance as components are easily exchangeable, while\nstandardized artifacts ensure interoperability between components.", "published": "2024-06-14 08:17:44", "link": "http://arxiv.org/abs/2406.15465v1", "categories": ["cs.CL", "cs.AI", "J.3"], "primary_category": "cs.CL"}
{"title": "Learning Language Structures through Grounding", "abstract": "Language is highly structured, with syntactic and semantic structures, to\nsome extent, agreed upon by speakers of the same language. With implicit or\nexplicit awareness of such structures, humans can learn and use language\nefficiently and generalize to sentences that contain unseen words. Motivated by\nhuman language learning, in this dissertation, we consider a family of machine\nlearning tasks that aim to learn language structures through grounding. We seek\ndistant supervision from other data sources (i.e., grounds), including but not\nlimited to other modalities (e.g., vision), execution results of programs, and\nother languages.\n  We demonstrate the potential of this task formulation and advocate for its\nadoption through three schemes. In Part I, we consider learning syntactic\nparses through visual grounding. We propose the task of visually grounded\ngrammar induction, present the first models to induce syntactic structures from\nvisually grounded text and speech, and find that the visual grounding signals\ncan help improve the parsing quality over language-only models. As a side\ncontribution, we propose a novel evaluation metric that enables the evaluation\nof speech parsing without text or automatic speech recognition systems\ninvolved. In Part II, we propose two execution-aware methods to map sentences\ninto corresponding semantic structures (i.e., programs), significantly\nimproving compositional generalization and few-shot program synthesis. In Part\nIII, we propose methods that learn language structures from annotations in\nother languages. Specifically, we propose a method that sets a new state of the\nart on cross-lingual word alignment. We then leverage the learned word\nalignments to improve the performance of zero-shot cross-lingual dependency\nparsing, by proposing a novel substructure-based projection method that\npreserves structural knowledge learned from the source language.", "published": "2024-06-14 02:21:53", "link": "http://arxiv.org/abs/2406.09662v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "OSPC: Detecting Harmful Memes with Large Language Model as a Catalyst", "abstract": "Memes, which rapidly disseminate personal opinions and positions across the\ninternet, also pose significant challenges in propagating social bias and\nprejudice. This study presents a novel approach to detecting harmful memes,\nparticularly within the multicultural and multilingual context of Singapore.\nOur methodology integrates image captioning, Optical Character Recognition\n(OCR), and Large Language Model (LLM) analysis to comprehensively understand\nand classify harmful memes. Utilizing the BLIP model for image captioning,\nPP-OCR and TrOCR for text recognition across multiple languages, and the Qwen\nLLM for nuanced language understanding, our system is capable of identifying\nharmful content in memes created in English, Chinese, Malay, and Tamil. To\nenhance the system's performance, we fine-tuned our approach by leveraging\nadditional data labeled using GPT-4V, aiming to distill the understanding\ncapability of GPT-4V for harmful memes to our system. Our framework achieves\ntop-1 at the public leaderboard of the Online Safety Prize Challenge hosted by\nAI Singapore, with the AUROC as 0.7749 and accuracy as 0.7087, significantly\nahead of the other teams. Notably, our approach outperforms previous\nbenchmarks, with FLAVA achieving an AUROC of 0.5695 and VisualBERT an AUROC of\n0.5561.", "published": "2024-06-14 07:28:02", "link": "http://arxiv.org/abs/2406.09779v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention", "abstract": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.", "published": "2024-06-14 08:32:45", "link": "http://arxiv.org/abs/2406.09827v3", "categories": ["cs.CL", "cs.CV", "cs.DC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Federated Learning driven Large Language Models for Swarm Intelligence:\n  A Survey", "abstract": "Federated learning (FL) offers a compelling framework for training large\nlanguage models (LLMs) while addressing data privacy and decentralization\nchallenges. This paper surveys recent advancements in the federated learning of\nlarge language models, with a particular focus on machine unlearning, a crucial\naspect for complying with privacy regulations like the Right to be Forgotten.\nMachine unlearning in the context of federated LLMs involves systematically and\nsecurely removing individual data contributions from the learned model without\nretraining from scratch. We explore various strategies that enable effective\nunlearning, such as perturbation techniques, model decomposition, and\nincremental learning, highlighting their implications for maintaining model\nperformance and data privacy. Furthermore, we examine case studies and\nexperimental results from recent literature to assess the effectiveness and\nefficiency of these approaches in real-world scenarios. Our survey reveals a\ngrowing interest in developing more robust and scalable federated unlearning\nmethods, suggesting a vital area for future research in the intersection of AI\nethics and distributed machine learning technologies.", "published": "2024-06-14 08:40:58", "link": "http://arxiv.org/abs/2406.09831v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal\n  Data", "abstract": "Multimodal Deep Learning enhances decision-making by integrating diverse\ninformation sources, such as texts, images, audio, and videos. To develop\ntrustworthy multimodal approaches, it is essential to understand how\nuncertainty impacts these models. We propose LUMA, a unique benchmark dataset,\nfeaturing audio, image, and textual data from 50 classes, for learning from\nuncertain and multimodal data. It extends the well-known CIFAR 10/100 dataset\nwith audio samples extracted from three audio corpora, and text data generated\nusing the Gemma-7B Large Language Model (LLM). The LUMA dataset enables the\ncontrolled injection of varying types and degrees of uncertainty to achieve and\ntailor specific experiments and benchmarking initiatives. LUMA is also\navailable as a Python package including the functions for generating multiple\nvariants of the dataset with controlling the diversity of the data, the amount\nof noise for each modality, and adding out-of-distribution samples. A baseline\npre-trained model is also provided alongside three uncertainty quantification\nmethods: Monte-Carlo Dropout, Deep Ensemble, and Reliable Conflictive\nMulti-View Learning. This comprehensive dataset and its benchmarking tools are\nintended to promote and support the development, evaluation, and benchmarking\nof trustworthy and robust multimodal deep learning approaches. We anticipate\nthat the LUMA dataset will help the ICLR community to design more trustworthy\nand robust machine learning approaches for safety critical applications.", "published": "2024-06-14 09:22:07", "link": "http://arxiv.org/abs/2406.09864v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Enhancing Fake News Detection in Social Media via Label Propagation on\n  Cross-modal Tweet Graph", "abstract": "Fake news detection in social media has become increasingly important due to\nthe rapid proliferation of personal media channels and the consequential\ndissemination of misleading information. Existing methods, which primarily rely\non multimodal features and graph-based techniques, have shown promising\nperformance in detecting fake news. However, they still face a limitation,\ni.e., sparsity in graph connections, which hinders capturing possible\ninteractions among tweets. This challenge has motivated us to explore a novel\nmethod that densifies the graph's connectivity to capture denser interaction\nbetter. Our method constructs a cross-modal tweet graph using CLIP, which\nencodes images and text into a unified space, allowing us to extract potential\nconnections based on similarities in text and images. We then design a Feature\nContextualization Network with Label Propagation (FCN-LP) to model the\ninteraction among tweets as well as positive or negative correlations between\npredicted labels of connected tweets. The propagated labels from the graph are\nweighted and aggregated for the final detection. To enhance the model's\ngeneralization ability to unseen events, we introduce a domain generalization\nloss that ensures consistent features between tweets on seen and unseen events.\nWe use three publicly available fake news datasets, Twitter, PHEME, and Weibo,\nfor evaluation. Our method consistently improves the performance over the\nstate-of-the-art methods on all benchmark datasets and effectively demonstrates\nits aptitude for generalizing fake news detection in social media.", "published": "2024-06-14 09:55:54", "link": "http://arxiv.org/abs/2406.09884v1", "categories": ["cs.MM", "cs.CL", "cs.SI"], "primary_category": "cs.MM"}
{"title": "CliBench: A Multifaceted and Multigranular Evaluation of Large Language\n  Models for Clinical Decision Making", "abstract": "The integration of Artificial Intelligence (AI), especially Large Language\nModels (LLMs), into the clinical diagnosis process offers significant potential\nto improve the efficiency and accessibility of medical care. While LLMs have\nshown some promise in the medical domain, their application in clinical\ndiagnosis remains underexplored, especially in real-world clinical practice,\nwhere highly sophisticated, patient-specific decisions need to be made. Current\nevaluations of LLMs in this field are often narrow in scope, focusing on\nspecific diseases or specialties and employing simplified diagnostic tasks. To\nbridge this gap, we introduce CliBench, a novel benchmark developed from the\nMIMIC IV dataset, offering a comprehensive and realistic assessment of LLMs'\ncapabilities in clinical diagnosis. This benchmark not only covers diagnoses\nfrom a diverse range of medical cases across various specialties but also\nincorporates tasks of clinical significance: treatment procedure\nidentification, lab test ordering and medication prescriptions. Supported by\nstructured output ontologies, CliBench enables a precise and multi-granular\nevaluation, offering an in-depth understanding of LLM's capability on diverse\nclinical tasks of desired granularity. We conduct a zero-shot evaluation of\nleading LLMs to assess their proficiency in clinical decision-making. Our\npreliminary results shed light on the potential and limitations of current LLMs\nin clinical settings, providing valuable insights for future advancements in\nLLM-powered healthcare.", "published": "2024-06-14 11:10:17", "link": "http://arxiv.org/abs/2406.09923v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An efficient text augmentation approach for contextualized Mandarin\n  speech recognition", "abstract": "Although contextualized automatic speech recognition (ASR) systems are\ncommonly used to improve the recognition of uncommon words, their effectiveness\nis hindered by the inherent limitations of speech-text data availability. To\naddress this challenge, our study proposes to leverage extensive text-only\ndatasets and contextualize pre-trained ASR models using a straightforward\ntext-augmentation (TA) technique, all while keeping computational costs\nminimal. In particular, to contextualize a pre-trained CIF-based ASR, we\nconstruct a codebook using limited speech-text data. By utilizing a simple\ncodebook lookup process, we convert available text-only data into latent text\nembeddings. These embeddings then enhance the inputs for the contextualized\nASR. Our experiments on diverse Mandarin test sets demonstrate that our TA\napproach significantly boosts recognition performance. The top-performing\nsystem shows relative CER improvements of up to 30% on rare words and 15%\nacross all words in general.", "published": "2024-06-14 11:53:14", "link": "http://arxiv.org/abs/2406.09950v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "BiVLC: Extending Vision-Language Compositionality Evaluation with\n  Text-to-Image Retrieval", "abstract": "Existing Vision-Language Compositionality (VLC) benchmarks like SugarCrepe\nare formulated as image-to-text retrieval problems, where, given an image, the\nmodels need to select between the correct textual description and a synthetic\nhard negative text. In this work, we present the Bidirectional Vision-Language\nCompositionality (BiVLC) dataset. The novelty of BiVLC is to add a synthetic\nhard negative image generated from the synthetic text, resulting in two\nimage-to-text retrieval examples (one for each image) and, more importantly,\ntwo text-to-image retrieval examples (one for each text). Human annotators\nfilter out ill-formed examples ensuring the validity of the benchmark. The\nexperiments on BiVLC uncover a weakness of current multimodal models, as they\nperform poorly in the text-to-image direction. In fact, when considering both\nretrieval directions, the conclusions obtained in previous works change\nsignificantly. In addition to the benchmark, we show that a contrastive model\ntrained using synthetic images and texts significantly improves over the base\nmodel in SugarCrepe and in BiVLC for both retrieval directions. The gap to\nhuman performance in BiVLC confirms that Vision-Language Compositionality is\nstill a challenging problem. BiVLC and code are available at\nhttps://imirandam.github.io/BiVLC_project_page.", "published": "2024-06-14 11:58:49", "link": "http://arxiv.org/abs/2406.09952v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via\n  Chart-to-Code Generation", "abstract": "We introduce a new benchmark, ChartMimic, aimed at assessing the\nvisually-grounded code generation capabilities of large multimodal models\n(LMMs). ChartMimic utilizes information-intensive visual charts and textual\ninstructions as inputs, requiring LMMs to generate the corresponding code for\nchart rendering. ChartMimic includes 4,800 human-curated (figure, instruction,\ncode) triplets, which represent the authentic chart use cases found in\nscientific papers across various domains (e.g., Physics, Computer Science,\nEconomics, etc). These charts span 18 regular types and 4 advanced types,\ndiversifying into 201 subcategories. Furthermore, we propose multi-level\nevaluation metrics to provide an automatic and thorough assessment of the\noutput code and the rendered charts. Unlike existing code generation\nbenchmarks, ChartMimic places emphasis on evaluating LMMs' capacity to\nharmonize a blend of cognitive capabilities, encompassing visual understanding,\ncode generation, and cross-modal reasoning. The evaluation of $3$ proprietary\nmodels and 14 open-weight models highlights the substantial challenges posed by\nChartMimic. Even the advanced GPT-4o, InternVL2-Llama3-76B only achieved an\naverage score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6,\nrespectively, indicating significant room for improvement. We anticipate that\nChartMimic will inspire the development of LMMs, advancing the pursuit of\nartificial general intelligence.", "published": "2024-06-14 12:10:51", "link": "http://arxiv.org/abs/2406.09961v2", "categories": ["cs.SE", "cs.CL", "cs.CV"], "primary_category": "cs.SE"}
{"title": "HIRO: Hierarchical Information Retrieval Optimization", "abstract": "Retrieval-Augmented Generation (RAG) has revolutionized natural language\nprocessing by dynamically integrating external knowledge into Large Language\nModels (LLMs), addressing their limitation of static training datasets. Recent\nimplementations of RAG leverage hierarchical data structures, which organize\ndocuments at various levels of summarization and information density. This\ncomplexity, however, can cause LLMs to \"choke\" on information overload,\nnecessitating more sophisticated querying mechanisms. In this context, we\nintroduce Hierarchical Information Retrieval Optimization (HIRO), a novel\nquerying approach that employs a Depth-First Search (DFS)-based recursive\nsimilarity score calculation and branch pruning. This method uniquely minimizes\nthe context delivered to the LLM without informational loss, effectively\nmanaging the challenge of excessive data. HIRO's refined approach is validated\nby a 10.85% improvement in performance on the NarrativeQA dataset.", "published": "2024-06-14 12:41:07", "link": "http://arxiv.org/abs/2406.09979v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Details Make a Difference: Object State-Sensitive Neurorobotic Task\n  Planning", "abstract": "The state of an object reflects its current status or condition and is\nimportant for a robot's task planning and manipulation. However, detecting an\nobject's state and generating a state-sensitive plan for robots is challenging.\nRecently, pre-trained Large Language Models (LLMs) and Vision-Language Models\n(VLMs) have shown impressive capabilities in generating plans. However, to the\nbest of our knowledge, there is hardly any investigation on whether LLMs or\nVLMs can also generate object state-sensitive plans. To study this, we\nintroduce an Object State-Sensitive Agent (OSSA), a task-planning agent\nempowered by pre-trained neural networks. We propose two methods for OSSA: (i)\na modular model consisting of a pre-trained vision processing module (dense\ncaptioning model, DCM) and a natural language processing model (LLM), and (ii)\na monolithic model consisting only of a VLM. To quantitatively evaluate the\nperformances of the two methods, we use tabletop scenarios where the task is to\nclear the table. We contribute a multimodal benchmark dataset that takes object\nstates into consideration. Our results show that both methods can be used for\nobject state-sensitive tasks, but the monolithic approach outperforms the\nmodular approach. The code for OSSA is available at\nhttps://github.com/Xiao-wen-Sun/OSSA", "published": "2024-06-14 12:52:42", "link": "http://arxiv.org/abs/2406.09988v2", "categories": ["cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Deep Bayesian Active Learning for Preference Modeling in Large Language\n  Models", "abstract": "Leveraging human preferences for steering the behavior of Large Language\nModels (LLMs) has demonstrated notable success in recent years. Nonetheless,\ndata selection and labeling are still a bottleneck for these systems,\nparticularly at large scale. Hence, selecting the most informative points for\nacquiring human feedback may considerably reduce the cost of preference\nlabeling and unleash the further development of LLMs. Bayesian Active Learning\nprovides a principled framework for addressing this challenge and has\ndemonstrated remarkable success in diverse settings. However, previous attempts\nto employ it for Preference Modeling did not meet such expectations. In this\nwork, we identify that naive epistemic uncertainty estimation leads to the\nacquisition of redundant samples. We address this by proposing the Bayesian\nActive Learner for Preference Modeling (BAL-PM), a novel stochastic acquisition\npolicy that not only targets points of high epistemic uncertainty according to\nthe preference model but also seeks to maximize the entropy of the acquired\nprompt distribution in the feature space spanned by the employed LLM. Notably,\nour experiments demonstrate that BAL-PM requires 33% to 68% fewer preference\nlabels in two popular human preference datasets and exceeds previous stochastic\nBayesian acquisition policies.", "published": "2024-06-14 13:32:43", "link": "http://arxiv.org/abs/2406.10023v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Simul-Whisper: Attention-Guided Streaming Whisper with Truncation\n  Detection", "abstract": "As a robust and large-scale multilingual speech recognition model, Whisper\nhas demonstrated impressive results in many low-resource and\nout-of-distribution scenarios. However, its encoder-decoder structure hinders\nits application to streaming speech recognition. In this paper, we introduce\nSimul-Whisper, which uses the time alignment embedded in Whisper's\ncross-attention to guide auto-regressive decoding and achieve chunk-based\nstreaming ASR without any fine-tuning of the pre-trained model. Furthermore, we\nobserve the negative effect of the truncated words at the chunk boundaries on\nthe decoding results and propose an integrate-and-fire-based truncation\ndetection model to address this issue. Experiments on multiple languages and\nWhisper architectures show that Simul-Whisper achieves an average absolute word\nerror rate degradation of only 1.46% at a chunk size of 1 second, which\nsignificantly outperforms the current state-of-the-art baseline.", "published": "2024-06-14 14:07:26", "link": "http://arxiv.org/abs/2406.10052v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Detecting the terminality of speech-turn boundary for spoken\n  interactions in French TV and Radio content", "abstract": "Transition Relevance Places are defined as the end of an utterance where the\ninterlocutor may take the floor without interrupting the current speaker\n--i.e., a place where the turn is terminal. Analyzing turn terminality is\nuseful to study the dynamic of turn-taking in spontaneous conversations. This\npaper presents an automatic classification of spoken utterances as Terminal or\nNon-Terminal in multi-speaker settings. We compared audio, text, and fusions of\nboth approaches on a French corpus of TV and Radio extracts annotated with\nturn-terminality information at each speaker change. Our models are based on\npre-trained self-supervised representations. We report results for different\nfusion strategies and varying context sizes. This study also questions the\nproblem of performance variability by analyzing the differences in results for\nmultiple training runs with random initialization. The measured accuracy would\nallow the use of these models for large-scale analysis of turn-taking.", "published": "2024-06-14 14:28:06", "link": "http://arxiv.org/abs/2406.10073v1", "categories": ["eess.AS", "cs.CL", "cs.HC", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On the Evaluation of Speech Foundation Models for Spoken Language\n  Understanding", "abstract": "The Spoken Language Understanding Evaluation (SLUE) suite of benchmark tasks\nwas recently introduced to address the need for open resources and benchmarking\nof complex spoken language understanding (SLU) tasks, including both\nclassification and sequence generation tasks, on natural speech. The benchmark\nhas demonstrated preliminary success in using pre-trained speech foundation\nmodels (SFM) for these SLU tasks. However, the community still lacks a\nfine-grained understanding of the comparative utility of different SFMs.\nInspired by this, we ask: which SFMs offer the most benefits for these complex\nSLU tasks, and what is the most effective approach for incorporating these\nSFMs? To answer this, we perform an extensive evaluation of multiple supervised\nand self-supervised SFMs using several evaluation protocols: (i) frozen SFMs\nwith a lightweight prediction head, (ii) frozen SFMs with a complex prediction\nhead, and (iii) fine-tuned SFMs with a lightweight prediction head. Although\nthe supervised SFMs are pre-trained on much more speech recognition data (with\nlabels), they do not always outperform self-supervised SFMs; the latter tend to\nperform at least as well as, and sometimes better than, supervised SFMs,\nespecially on the sequence generation tasks in SLUE. While there is no\nuniversally optimal way of incorporating SFMs, the complex prediction head\ngives the best performance for most tasks, although it increases the inference\ntime. We also introduce an open-source toolkit and performance leaderboard,\nSLUE-PERB, for these tasks and modeling strategies.", "published": "2024-06-14 14:37:52", "link": "http://arxiv.org/abs/2406.10083v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Discovering influential text using convolutional neural networks", "abstract": "Experimental methods for estimating the impacts of text on human evaluation\nhave been widely used in the social sciences. However, researchers in\nexperimental settings are usually limited to testing a small number of\npre-specified text treatments. While efforts to mine unstructured texts for\nfeatures that causally affect outcomes have been ongoing in recent years, these\nmodels have primarily focused on the topics or specific words of text, which\nmay not always be the mechanism of the effect. We connect these efforts with\nNLP interpretability techniques and present a method for flexibly discovering\nclusters of similar text phrases that are predictive of human reactions to\ntexts using convolutional neural networks. When used in an experimental\nsetting, this method can identify text treatments and their effects under\ncertain assumptions. We apply the method to two datasets. The first enables\ndirect validation of the model's ability to detect phrases known to cause the\noutcome. The second demonstrates its ability to flexibly discover text\ntreatments with varying textual structures. In both cases, the model learns a\ngreater variety of text treatments compared to benchmark methods, and these\ntext features quantitatively meet or exceed the ability of benchmark methods to\npredict the outcome.", "published": "2024-06-14 14:41:44", "link": "http://arxiv.org/abs/2406.10086v3", "categories": ["cs.CL", "cs.LG", "stat.ME"], "primary_category": "cs.CL"}
{"title": "Long Story Short: Story-level Video Understanding from 20K Short Films", "abstract": "Recent developments in vision-language models have significantly advanced\nvideo understanding. Existing datasets and tasks, however, have notable\nlimitations. Most datasets are confined to short videos with limited events and\nnarrow narratives. For example, datasets with instructional and egocentric\nvideos often depict activities of one person in a single scene. Although\nexisting movie datasets offer richer content, they are often limited to\nshort-term tasks, lack publicly available videos, and frequently encounter data\nleakage issues given the use of subtitles and other information about\ncommercial movies during LLM pretraining. To address the above limitations, we\npropose Short-Films 20K (SF20K), the largest publicly available movie dataset.\nSF20K is composed of 20,143 amateur films and offers long-term video tasks in\nthe form of multiple-choice and open-ended question answering. Our extensive\nanalysis of SF20K reveals minimal data leakage, emphasizes the need for\nlong-term reasoning, and demonstrates the strong performance of recent VLMs.\nFinally, we show that instruction tuning on the SF20K-Train set substantially\nimproves model performance, paving the way for future progress in long-term\nvideo understanding.", "published": "2024-06-14 17:54:54", "link": "http://arxiv.org/abs/2406.10221v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language\n  Large Models", "abstract": "The swift progress of Multi-modal Large Models (MLLMs) has showcased their\nimpressive ability to tackle tasks blending vision and language. Yet, most\ncurrent models and benchmarks cater to scenarios with a narrow scope of visual\nand textual contexts. These models often fall short when faced with complex\ncomprehension tasks, which involve navigating through a plethora of irrelevant\nand potentially misleading information in both text and image forms. To bridge\nthis gap, we introduce a new, more demanding task known as Interleaved\nImage-Text Comprehension (IITC). This task challenges models to discern and\ndisregard superfluous elements in both images and text to accurately answer\nquestions and to follow intricate instructions to pinpoint the relevant image.\nIn support of this task, we further craft a new VEGA dataset, tailored for the\nIITC task on scientific content, and devised a subtask, Image-Text Association\n(ITA), to refine image-text correlation skills. Our evaluation of four leading\nclosed-source models, as well as various open-source models using VEGA,\nunderscores the rigorous nature of IITC. Even the most advanced models, such as\nGemini-1.5-pro and GPT4V, only achieved modest success. By employing a\nmulti-task, multi-scale post-training strategy, we have set a robust baseline\nfor MLLMs on the IITC task, attaining an $85.8\\%$ accuracy rate in image\nassociation and a $0.508$ Rouge score. These results validate the effectiveness\nof our dataset in improving MLLMs capabilities for nuanced image-text\ncomprehension.", "published": "2024-06-14 17:59:40", "link": "http://arxiv.org/abs/2406.10228v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Enhancing Multilingual Voice Toxicity Detection with Speech-Text\n  Alignment", "abstract": "Toxicity classification for voice heavily relies on the semantic content of\nspeech. We propose a novel framework that utilizes cross-modal learning to\nintegrate the semantic embedding of text into a multilabel speech toxicity\nclassifier during training. This enables us to incorporate textual information\nduring training while still requiring only audio during inference. We evaluate\nthis classifier on large-scale datasets with real-world characteristics to\nvalidate the effectiveness of this framework. Through ablation studies, we\ndemonstrate that general-purpose semantic text embeddings are rich and aligned\nwith speech for toxicity classification purposes. Conducting experiments across\nmultiple languages at scale, we show improvements in voice toxicity\nclassification across five languages and different toxicity categories.", "published": "2024-06-14 17:56:53", "link": "http://arxiv.org/abs/2406.10325v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "From Pixels to Prose: A Large Dataset of Dense Image Captions", "abstract": "Training large vision-language models requires extensive, high-quality\nimage-text pairs. Existing web-scraped datasets, however, are noisy and lack\ndetailed image descriptions. To bridge this gap, we introduce PixelProse, a\ncomprehensive dataset of over 16M (million) synthetically generated captions,\nleveraging cutting-edge vision-language models for detailed and accurate\ndescriptions. To ensure data integrity, we rigorously analyze our dataset for\nproblematic content, including child sexual abuse material (CSAM), personally\nidentifiable information (PII), and toxicity. We also provide valuable metadata\nsuch as watermark presence and aesthetic scores, aiding in further dataset\nfiltering. We hope PixelProse will be a valuable resource for future\nvision-language research. PixelProse is available at\nhttps://huggingface.co/datasets/tomg-group-umd/pixelprose", "published": "2024-06-14 17:59:53", "link": "http://arxiv.org/abs/2406.10328v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Determination of the Number of Topics Intrinsically: Is It Possible?", "abstract": "The number of topics might be the most important parameter of a topic model.\nThe topic modelling community has developed a set of various procedures to\nestimate the number of topics in a dataset, but there has not yet been a\nsufficiently complete comparison of existing practices. This study attempts to\npartially fill this gap by investigating the performance of various methods\napplied to several topic models on a number of publicly available corpora.\nFurther analysis demonstrates that intrinsic methods are far from being\nreliable and accurate tools. The number of topics is shown to be a method- and\na model-dependent quantity, as opposed to being an absolute property of a\nparticular corpus. We conclude that other methods for dealing with this problem\nshould be developed and suggest some promising directions for further research.", "published": "2024-06-14 20:07:46", "link": "http://arxiv.org/abs/2406.10402v1", "categories": ["cs.CL", "cs.IR", "math.PR"], "primary_category": "cs.CL"}
{"title": "Unraveling the Mechanics of Learning-Based Demonstration Selection for\n  In-Context Learning", "abstract": "Large Language Models (LLMs) have demonstrated impressive in-context learning\n(ICL) capabilities from few-shot demonstration exemplars. While recent\nlearning-based demonstration selection methods have proven beneficial to ICL by\nchoosing more useful exemplars, their underlying mechanisms are opaque,\nhindering efforts to address limitations such as high training costs and poor\ngeneralization across tasks. These methods generally assume the selection\nprocess captures similarities between the exemplar and the target instance,\nhowever, it remains unknown what kinds of similarities are captured and vital\nto performing ICL. To dive into this question, we analyze the working\nmechanisms of the learning-based demonstration selection methods and\nempirically identify two important factors related to similarity measurement:\n1) The ability to integrate different levels of task-agnostic text similarities\nbetween the input of exemplars and test cases enhances generalization power\nacross different tasks. 2) Incorporating task-specific labels when measuring\nthe similarities significantly improves the performance on each specific task.\nWe validate these two findings through extensive quantitative and qualitative\nanalyses across ten datasets and various LLMs. Based on our findings, we\nintroduce two effective yet simplified exemplar selection methods catering to\ntask-agnostic and task-specific demands, eliminating the costly LLM inference\noverhead.", "published": "2024-06-14 03:34:02", "link": "http://arxiv.org/abs/2406.11890v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "GLiNER multi-task: Generalist Lightweight Model for Various Information\n  Extraction Tasks", "abstract": "Information extraction tasks require both accurate, efficient, and\ngeneralisable models. Classical supervised deep learning approaches can achieve\nthe required performance, but they need large datasets and are limited in their\nability to adapt to different tasks. On the other hand, large language models\n(LLMs) demonstrate good generalization, meaning that they can adapt to many\ndifferent tasks based on user requests. However, LLMs are computationally\nexpensive and tend to fail to generate structured outputs. In this article, we\nwill introduce a new kind of GLiNER model that can be used for various\ninformation extraction tasks while being a small encoder model. Our model\nachieved SoTA performance on zero-shot NER benchmarks and leading performance\non question-answering, summarization and relation extraction tasks.\nAdditionally, in this article, we will cover experimental results on\nself-learning approaches for named entity recognition using GLiNER models.", "published": "2024-06-14 13:54:29", "link": "http://arxiv.org/abs/2406.12925v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Group and Shuffle: Efficient Structured Orthogonal Parametrization", "abstract": "The increasing size of neural networks has led to a growing demand for\nmethods of efficient fine-tuning. Recently, an orthogonal fine-tuning paradigm\nwas introduced that uses orthogonal matrices for adapting the weights of a\npretrained model. In this paper, we introduce a new class of structured\nmatrices, which unifies and generalizes structured classes from previous works.\nWe examine properties of this class and build a structured orthogonal\nparametrization upon it. We then use this parametrization to modify the\northogonal fine-tuning framework, improving parameter and computational\nefficiency. We empirically validate our method on different domains, including\nadapting of text-to-image diffusion models and downstream task fine-tuning in\nlanguage modeling. Additionally, we adapt our construction for orthogonal\nconvolutions and conduct experiments with 1-Lipschitz neural networks.", "published": "2024-06-14 13:29:36", "link": "http://arxiv.org/abs/2406.10019v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.NA", "math.NA"], "primary_category": "cs.LG"}
{"title": "The Kolmogorov Complexity of Irish traditional dance music", "abstract": "We estimate the Kolmogorov complexity of melodies in Irish traditional dance\nmusic using Lempel-Ziv compression. The \"tunes\" of the music are presented in\nso-called \"ABC notation\" as simply a sequence of letters from an alphabet: We\nhave no rhythmic variation, with all notes being of equal length. Our\nestimation of algorithmic complexity can be used to distinguish \"simple\" or\n\"easy\" tunes (with more repetition) from \"difficult\" ones (with less\nrepetition) which should prove useful for students learning tunes. We further\npresent a comparison of two tune categories (reels and jigs) in terms of their\ncomplexity.", "published": "2024-06-14 16:38:06", "link": "http://arxiv.org/abs/2407.12000v1", "categories": ["cs.IT", "cs.CL", "cs.CV", "cs.IR", "cs.SD", "eess.AS", "math.IT"], "primary_category": "cs.IT"}
{"title": "A Multimodal Framework for the Assessment of the Schizophrenia Spectrum", "abstract": "This paper presents a novel multimodal framework to distinguish between\ndifferent symptom classes of subjects in the schizophrenia spectrum and healthy\ncontrols using audio, video, and text modalities. We implemented Convolution\nNeural Network and Long Short Term Memory based unimodal models and\nexperimented on various multimodal fusion approaches to come up with the\nproposed framework. We utilized a minimal Gated multimodal unit (mGMU) to\nobtain a bi-modal intermediate fusion of the features extracted from the input\nmodalities before finally fusing the outputs of the bimodal fusions to perform\nsubject-wise classifications. The use of mGMU units in the multimodal framework\nimproved the performance in both weighted f1-score and weighted AUC-ROC scores.", "published": "2024-06-14 04:23:47", "link": "http://arxiv.org/abs/2406.09706v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Enhanced Deep Speech Separation in Clustered Ad Hoc Distributed\n  Microphone Environments", "abstract": "Ad-hoc distributed microphone environments, where microphone locations and\nnumbers are unpredictable, present a challenge to traditional deep learning\nmodels, which typically require fixed architectures. To tailor deep learning\nmodels to accommodate arbitrary array configurations, the\nTransform-Average-Concatenate (TAC) layer was previously introduced. In this\nwork, we integrate TAC layers with dual-path transformers for speech separation\nfrom two simultaneous talkers in realistic settings. However, the distributed\nnature makes it hard to fuse information across microphones efficiently.\nTherefore, we explore the efficacy of blindly clustering microphones around\nsources of interest prior to enhancement. Experimental results show that this\ndeep cluster-informed approach significantly improves the system's capacity to\ncope with the inherent variability observed in ad-hoc distributed microphone\nenvironments.", "published": "2024-06-14 08:22:18", "link": "http://arxiv.org/abs/2406.09819v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Low algorithmic delay implementation of convolutional beamformer for\n  online joint source separation and dereverberation", "abstract": "Blind-audio-source-separation (BASS) techniques, particularly those with low\nlatency, play an important role in a wide range of real-time systems, e.g.,\nhearing aids, in-car hand-free voice communication, real-time human-machine\ninteraction, etc. Most existing BASS algorithms are deduced to run on batch\nmode, and therefore large latency is unavoidable. Recently, some online\nalgorithms were developed, which achieve separation on a frame-by-frame basis\nin the short-time-Fourier-transform (STFT) domain and the latency is\nsignificantly reduced as compared to those batch methods. However, the latency\nwith these algorithms may still be too long for many real-time systems to bear.\nTo further reduce latency while achieving good separation performance, we\npropose in this work to integrate a weighted prediction error (WPE) module into\na non-causal sample-truncating-based independent vector analysis (NST-IVA). The\nresulting algorithm can maintain the algorithmic delay as NST-IVA if the delay\nwith WPE is appropriately controlled while achieving significantly better\nperformance, which is validated by simulations.", "published": "2024-06-14 08:23:09", "link": "http://arxiv.org/abs/2406.09821v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "ROAR: Reinforcing Original to Augmented Data Ratio Dynamics for\n  Wav2Vec2.0 Based ASR", "abstract": "While automatic speech recognition (ASR) greatly benefits from data\naugmentation, the augmentation recipes themselves tend to be heuristic. In this\npaper, we address one of the heuristic approach associated with balancing the\nright amount of augmented data in ASR training by introducing a reinforcement\nlearning (RL) based dynamic adjustment of original-to-augmented data ratio\n(OAR). Unlike the fixed OAR approach in conventional data augmentation, our\nproposed method employs a deep Q-network (DQN) as the RL mechanism to learn the\noptimal dynamics of OAR throughout the wav2vec2.0 based ASR training. We\nconduct experiments using the LibriSpeech dataset with varying amounts of\ntraining data, specifically, the 10Min, 1H, 10H, and 100H splits to evaluate\nthe efficacy of the proposed method under different data conditions. Our\nproposed method, on average, achieves a relative improvement of 4.96% over the\nopen-source wav2vec2.0 base model on standard LibriSpeech test sets.", "published": "2024-06-14 13:15:32", "link": "http://arxiv.org/abs/2406.09999v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "AlignNet: Learning dataset score alignment functions to enable better\n  training of speech quality estimators", "abstract": "We develop two complementary advances for training no-reference (NR) speech\nquality estimators with independent datasets. Multi-dataset finetuning (MDF)\npretrains an NR estimator on a single dataset and then finetunes it on multiple\ndatasets at once, including the dataset used for pretraining. AlignNet uses an\nAudioNet to generate intermediate score estimates before using the Aligner to\nmap intermediate estimates to the appropriate score range. AlignNet is agnostic\nto the choice of AudioNet so any successful NR speech quality estimator can\nbenefit from its Aligner. The methods can be used in tandem, and we use two\nstudies to show that they improve on current solutions: one study uses nine\nsmaller datasets and the other uses four larger datasets. AlignNet with MDF\nimproves on other solutions because it efficiently and effectively removes\nmisalignments that impair the learning process, and thus enables successful\ntraining with larger amounts of more diverse data.", "published": "2024-06-14 17:40:43", "link": "http://arxiv.org/abs/2406.10205v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Frequency-mix Knowledge Distillation for Fake Speech Detection", "abstract": "In the telephony scenarios, the fake speech detection (FSD) task to combat\nspeech spoofing attacks is challenging. Data augmentation (DA) methods are\nconsidered effective means to address the FSD task in telephony scenarios,\ntypically divided into time domain and frequency domain stages. While each has\nits advantages, both can result in information loss. To tackle this issue, we\npropose a novel DA method, Frequency-mix (Freqmix), and introduce the Freqmix\nknowledge distillation (FKD) to enhance model information extraction and\ngeneralization abilities. Specifically, we use Freqmix-enhanced data as input\nfor the teacher model, while the student model's input undergoes time-domain DA\nmethod. We use a multi-level feature distillation approach to restore\ninformation and improve the model's generalization capabilities. Our approach\nachieves state-of-the-art results on ASVspoof 2021 LA dataset, showing a 31\\%\nimprovement over baseline and performs competitively on ASVspoof 2021 DF\ndataset.", "published": "2024-06-14 02:25:16", "link": "http://arxiv.org/abs/2406.09664v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Vec-Tok-VC+: Residual-enhanced Robust Zero-shot Voice Conversion with\n  Progressive Constraints in a Dual-mode Training Strategy", "abstract": "Zero-shot voice conversion (VC) aims to transform source speech into\narbitrary unseen target voice while keeping the linguistic content unchanged.\nRecent VC methods have made significant progress, but semantic losses in the\ndecoupling process as well as training-inference mismatch still hinder\nconversion performance. In this paper, we propose Vec-Tok-VC+, a novel\nprompt-based zero-shot VC model improved from Vec-Tok Codec, achieving voice\nconversion given only a 3s target speaker prompt. We design a residual-enhanced\nK-Means decoupler to enhance the semantic content extraction with a two-layer\nclustering process. Besides, we employ teacher-guided refinement to simulate\nthe conversion process to eliminate the training-inference mismatch, forming a\ndual-mode training strategy. Furthermore, we design a multi-codebook\nprogressive loss function to constrain the layer-wise output of the model from\ncoarse to fine to improve speaker similarity and content accuracy. Objective\nand subjective evaluations demonstrate that Vec-Tok-VC+ outperforms the strong\nbaselines in naturalness, intelligibility, and speaker similarity.", "published": "2024-06-14 08:51:31", "link": "http://arxiv.org/abs/2406.09844v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MMM: Multi-Layer Multi-Residual Multi-Stream Discrete Speech\n  Representation from Self-supervised Learning Model", "abstract": "Speech discrete representation has proven effective in various downstream\napplications due to its superior compression rate of the waveform, fast\nconvergence during training, and compatibility with other modalities. Discrete\nunits extracted from self-supervised learning (SSL) models have emerged as a\nprominent approach for obtaining speech discrete representation. However, while\ndiscrete units have shown effectiveness compared to spectral features, they\nstill lag behind continuous SSL representations. In this work, we propose MMM,\na multi-layer multi-residual multi-stream discrete units extraction method from\nSSL. Specifically, we introduce iterative residual vector quantization with\nK-means for different layers in an SSL model to extract multi-stream speech\ndiscrete representation. Through extensive experiments in speech recognition,\nspeech resynthesis, and text-to-speech, we demonstrate the proposed MMM can\nsurpass or on-par with neural codec's performance under various conditions.", "published": "2024-06-14 09:29:45", "link": "http://arxiv.org/abs/2406.09869v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Period Singer: Integrating Periodic and Aperiodic Variational\n  Autoencoders for Natural-Sounding End-to-End Singing Voice Synthesis", "abstract": "In this paper, we present Period Singer, a novel end-to-end singing voice\nsynthesis (SVS) model that utilizes variational inference for periodic and\naperiodic components, aimed at producing natural-sounding waveforms. Recent\nend-to-end SVS models have demonstrated the capability of synthesizing\nhigh-fidelity singing voices. However, owing to deterministic pitch\nconditioning, they do not fully address the one-to-many problem. To address\nthis problem, we present the Period Singer architecture, which integrates\nvariational autoencoders for the periodic and aperiodic components.\nAdditionally, our methodology eliminates the dependency on an external aligner\nby estimating the phoneme alignment through a monotonic alignment search within\nnote boundaries. Our empirical evaluations show that Period Singer outperforms\nexisting end-to-end SVS models on Mandarin and Korean datasets. The efficacy of\nthe proposed method was further corroborated by ablation studies.", "published": "2024-06-14 10:06:55", "link": "http://arxiv.org/abs/2406.09894v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "UniAudio 1.5: Large Language Model-driven Audio Codec is A Few-shot\n  Audio Task Learner", "abstract": "The Large Language models (LLMs) have demonstrated supreme capabilities in\ntext understanding and generation, but cannot be directly applied to\ncross-modal tasks without fine-tuning. This paper proposes a cross-modal\nin-context learning approach, empowering the frozen LLMs to achieve multiple\naudio tasks in a few-shot style without any parameter update. Specifically, we\npropose a novel and LLMs-driven audio codec model, LLM-Codec, to transfer the\naudio modality into the textual space, \\textit{i.e.} representing audio tokens\nwith words or sub-words in the vocabulary of LLMs, while keeping high audio\nreconstruction quality. The key idea is to reduce the modality heterogeneity\nbetween text and audio by compressing the audio modality into a well-trained\nLLMs token space. Thus, the audio representation can be viewed as a new\n\\textit{foreign language}, and LLMs can learn the new \\textit{foreign language}\nwith several demonstrations. In experiments, we investigate the performance of\nthe proposed approach across multiple audio understanding and generation tasks,\n\\textit{e.g.} speech emotion classification, audio classification,\ntext-to-speech generation, speech enhancement, etc. The experimental results\ndemonstrate that the LLMs equipped with the proposed LLM-Codec, named as\nUniAudio 1.5, prompted by only a few examples, can achieve the expected\nfunctions in simple scenarios. It validates the feasibility and effectiveness\nof the proposed cross-modal in-context learning approach. To facilitate\nresearch on few-shot audio task learning and multi-modal LLMs, we have\nopen-sourced the LLM-Codec model.", "published": "2024-06-14 14:13:18", "link": "http://arxiv.org/abs/2406.10056v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Joint Speaker Features Learning for Audio-visual Multichannel Speech\n  Separation and Recognition", "abstract": "This paper proposes joint speaker feature learning methods for zero-shot\nadaptation of audio-visual multichannel speech separation and recognition\nsystems. xVector and ECAPA-TDNN speaker encoders are connected using\npurpose-built fusion blocks and tightly integrated with the complete system\ntraining. Experiments conducted on LRS3-TED data simulated multichannel\noverlapped speech suggest that joint speaker feature learning consistently\nimproves speech separation and recognition performance over the baselines\nwithout joint speaker feature estimation. Further analyses reveal performance\nimprovements are strongly correlated with increased inter-speaker\ndiscrimination measured using cosine similarity. The best-performing joint\nspeaker feature learning adapted system outperformed the baseline fine-tuned\nWavLM model by statistically significant WER reductions of 21.6% and 25.3%\nabsolute (67.5% and 83.5% relative) on Dev and Test sets after incorporating\nWavLM features and video modality.", "published": "2024-06-14 16:10:15", "link": "http://arxiv.org/abs/2406.10152v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SHMamba: Structured Hyperbolic State Space Model for Audio-Visual\n  Question Answering", "abstract": "The Audio-Visual Question Answering (AVQA) task holds significant potential\nfor applications. Compared to traditional unimodal approaches, the multi-modal\ninput of AVQA makes feature extraction and fusion processes more challenging.\nEuclidean space is difficult to effectively represent multi-dimensional\nrelationships of data. Especially when extracting and processing data with a\ntree structure or hierarchical structure, Euclidean space is not suitable as an\nembedding space. Additionally, the self-attention mechanism in Transformers is\neffective in capturing the dynamic relationships between elements in a\nsequence. However, the self-attention mechanism's limitations in window\nmodeling and quadratic computational complexity reduce its effectiveness in\nmodeling long sequences. To address these limitations, we propose SHMamba:\nStructured Hyperbolic State Space Model to integrate the advantages of\nhyperbolic geometry and state space models. Specifically, SHMamba leverages the\nintrinsic properties of hyperbolic space to represent hierarchical structures\nand complex relationships in audio-visual data. Meanwhile, the state space\nmodel captures dynamic changes over time by globally modeling the entire\nsequence. Furthermore, we introduce an adaptive curvature hyperbolic alignment\nmodule and a cross fusion block to enhance the understanding of hierarchical\nstructures and the dynamic exchange of cross-modal information, respectively.\nExtensive experiments demonstrate that SHMamba outperforms previous methods\nwith fewer parameters and computational costs. Our learnable parameters are\nreduced by 78.12\\%, while the average performance improves by 2.53\\%.\nExperiments show that our method demonstrates superiority among all current\nmajor methods and is more suitable for practical application scenarios.", "published": "2024-06-14 08:43:31", "link": "http://arxiv.org/abs/2406.09833v3", "categories": ["cs.AI", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Perceiver-Prompt: Flexible Speaker Adaptation in Whisper for Chinese\n  Disordered Speech Recognition", "abstract": "Disordered speech recognition profound implications for improving the quality\nof life for individuals afflicted with, for example, dysarthria. Dysarthric\nspeech recognition encounters challenges including limited data, substantial\ndissimilarities between dysarthric and non-dysarthric speakers, and significant\nspeaker variations stemming from the disorder. This paper introduces\nPerceiver-Prompt, a method for speaker adaptation that utilizes P-Tuning on the\nWhisper large-scale model. We first fine-tune Whisper using LoRA and then\nintegrate a trainable Perceiver to generate fixed-length speaker prompts from\nvariable-length inputs, to improve model recognition of Chinese dysarthric\nspeech. Experimental results from our Chinese dysarthric speech dataset\ndemonstrate consistent improvements in recognition performance with\nPerceiver-Prompt. Relative reduction up to 13.04% in CER is obtained over the\nfine-tuned Whisper.", "published": "2024-06-14 09:36:46", "link": "http://arxiv.org/abs/2406.09873v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Personalized Speech Enhancement Without a Separate Speaker Embedding\n  Model", "abstract": "Personalized speech enhancement (PSE) models can improve the audio quality of\nteleconferencing systems by adapting to the characteristics of a speaker's\nvoice. However, most existing methods require a separate speaker embedding\nmodel to extract a vector representation of the speaker from enrollment audio,\nwhich adds complexity to the training and deployment process. We propose to use\nthe internal representation of the PSE model itself as the speaker embedding,\nthereby avoiding the need for a separate model. We show that our approach\nperforms equally well or better than the standard method of using a pre-trained\nspeaker embedding model on noise suppression and echo cancellation tasks.\nMoreover, our approach surpasses the ICASSP 2023 Deep Noise Suppression\nChallenge winner by 0.15 in Mean Opinion Score.", "published": "2024-06-14 11:16:46", "link": "http://arxiv.org/abs/2406.09928v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Impact of Speech Mode in Automatic Pathological Speech Detection", "abstract": "Automatic pathological speech detection approaches yield promising results in\nidentifying various pathologies. These approaches are typically designed and\nevaluated for phonetically-controlled speech scenarios, where speakers are\nprompted to articulate identical phonetic content. While gathering controlled\nspeech recordings can be laborious, spontaneous speech can be conveniently\nacquired as potential patients navigate their daily routines. Further,\nspontaneous speech can be valuable in detecting subtle and abstract cues of\npathological speech. Nonetheless, the efficacy of automatic pathological speech\ndetection for spontaneous speech remains unexplored. This paper analyzes the\ninfluence of speech mode on pathological speech detection approaches, examining\ntwo distinct categories of approaches, i.e., classical machine learning and\ndeep learning. Results indicate that classical approaches may struggle to\ncapture pathology-discriminant cues in spontaneous speech. In contrast, deep\nlearning approaches demonstrate superior performance, managing to extract\nadditional cues that were previously inaccessible in non-spontaneous speech", "published": "2024-06-14 12:19:18", "link": "http://arxiv.org/abs/2406.09968v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Towards Effective and Efficient Non-autoregressive Decoding Using\n  Block-based Attention Mask", "abstract": "This paper proposes a novel non-autoregressive (NAR) block-based Attention\nMask Decoder (AMD) that flexibly balances performance-efficiency trade-offs for\nConformer ASR systems. AMD performs parallel NAR inference within contiguous\nblocks of output labels that are concealed using attention masks, while\nconducting left-to-right AR prediction and history context amalgamation between\nblocks. A beam search algorithm is designed to leverage a dynamic fusion of\nCTC, AR Decoder, and AMD probabilities. Experiments on the LibriSpeech-100hr\ncorpus suggest the tripartite Decoder incorporating the AMD module produces a\nmaximum decoding speed-up ratio of 1.73x over the baseline CTC+AR decoding,\nwhile incurring no statistically significant word error rate (WER) increase on\nthe test sets. When operating with the same decoding real time factors,\nstatistically significant WER reductions of up to 0.7% and 0.3% absolute (5.3%\nand 6.1% relative) were obtained over the CTC+AR baseline.", "published": "2024-06-14 13:42:38", "link": "http://arxiv.org/abs/2406.10034v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Whisper-Flamingo: Integrating Visual Features into Whisper for\n  Audio-Visual Speech Recognition and Translation", "abstract": "Audio-Visual Speech Recognition (AVSR) uses lip-based video to improve\nperformance in noise. Since videos are harder to obtain than audio, the video\ntraining data of AVSR models is usually limited to a few thousand hours. In\ncontrast, speech models such as Whisper are trained with hundreds of thousands\nof hours of data, and thus learn a better speech-to-text decoder. The huge\ntraining data difference motivates us to adapt Whisper to handle video inputs.\nInspired by Flamingo which injects visual features into language models, we\npropose Whisper-Flamingo which integrates visual features into the Whisper\nspeech recognition and translation model with gated cross attention. Our models\nachieve state-of-the-art ASR WER (0.68%) and AVSR WER (0.76%) on LRS3, and\nstate-of-the-art ASR WER (1.3%) and AVSR WER (1.4%) on LRS2. Audio-visual\nWhisper-Flamingo outperforms audio-only Whisper on English speech recognition\nand En-X translation for 6 languages in noisy conditions. Moreover,\nWhisper-Flamingo is versatile and conducts all of these tasks using one set of\nparameters, while prior methods are trained separately on each language.", "published": "2024-06-14 14:36:54", "link": "http://arxiv.org/abs/2406.10082v3", "categories": ["eess.AS", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
{"title": "One-pass Multiple Conformer and Foundation Speech Systems Compression\n  and Quantization Using An All-in-one Neural Model", "abstract": "We propose a novel one-pass multiple ASR systems joint compression and\nquantization approach using an all-in-one neural model. A single compression\ncycle allows multiple nested systems with varying Encoder depths, widths, and\nquantization precision settings to be simultaneously constructed without the\nneed to train and store individual target systems separately. Experiments\nconsistently demonstrate the multiple ASR systems compressed in a single\nall-in-one model produced a word error rate (WER) comparable to, or lower by up\nto 1.01\\% absolute (6.98\\% relative) than individually trained systems of equal\ncomplexity. A 3.4x overall system compression and training time speed-up was\nachieved. Maximum model size compression ratios of 12.8x and 3.93x were\nobtained over the baseline Switchboard-300hr Conformer and LibriSpeech-100hr\nfine-tuned wav2vec2.0 models, respectively, incurring no statistically\nsignificant WER increase.", "published": "2024-06-14 16:18:34", "link": "http://arxiv.org/abs/2406.10160v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Diffusion Synthesizer for Efficient Multilingual Speech to Speech\n  Translation", "abstract": "We introduce DiffuseST, a low-latency, direct speech-to-speech translation\nsystem capable of preserving the input speaker's voice zero-shot while\ntranslating from multiple source languages into English. We experiment with the\nsynthesizer component of the architecture, comparing a Tacotron-based\nsynthesizer to a novel diffusion-based synthesizer. We find the diffusion-based\nsynthesizer to improve MOS and PESQ audio quality metrics by 23\\% each and\nspeaker similarity by 5\\% while maintaining comparable BLEU scores. Despite\nhaving more than double the parameter count, the diffusion synthesizer has\nlower latency, allowing the entire model to run more than 5$\\times$ faster than\nreal-time.", "published": "2024-06-14 17:55:55", "link": "http://arxiv.org/abs/2406.10223v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Gender Representation in TV and Radio: Automatic Information Extraction\n  methods versus Manual Analyses", "abstract": "This study investigates the relationship between automatic information\nextraction descriptors and manual analyses to describe gender representation\ndisparities in TV and Radio. Automatic descriptors, including speech time,\nfacial categorization and speech transcriptions are compared with channel\nreports on a vast 32,000-hour corpus of French broadcasts from 2023. Findings\nreveal systemic gender imbalances, with women underrepresented compared to men\nacross all descriptors. Notably, manual channel reports show higher women's\npresence than automatic estimates and references to women are lower than their\nspeech time. Descriptors share common dynamics during high and low audiences,\nwar coverage, or private versus public channels. While women are more visible\nthan audible in French TV, this trend is inverted in news with unseen\njournalists depicting male protagonists. A statistical test shows 3 main\neffects influencing references to women: program category, channel and speaker\ngender.", "published": "2024-06-14 16:05:43", "link": "http://arxiv.org/abs/2406.10316v1", "categories": ["eess.AS", "cs.CY", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Evaluating Speaker Identity Coding in Self-supervised Models and Humans", "abstract": "Speaker identity plays a significant role in human communication and is being\nincreasingly used in societal applications, many through advances in machine\nlearning. Speaker identity perception is an essential cognitive phenomenon that\ncan be broadly reduced to two main tasks: recognizing a voice or discriminating\nbetween voices. Several studies have attempted to identify acoustic correlates\nof identity perception to pinpoint salient parameters for such a task. Unlike\nother communicative social signals, most efforts have yielded inefficacious\nconclusions. Furthermore, current neurocognitive models of voice identity\nprocessing consider the bases of perception as acoustic dimensions such as\nfundamental frequency, harmonics-to-noise ratio, and formant dispersion.\nHowever, these findings do not account for naturalistic speech and\nwithin-speaker variability. Representational spaces of current self-supervised\nmodels have shown significant performance in various speech-related tasks. In\nthis work, we demonstrate that self-supervised representations from different\nfamilies (e.g., generative, contrastive, and predictive models) are\nsignificantly better for speaker identification over acoustic representations.\nWe also show that such a speaker identification task can be used to better\nunderstand the nature of acoustic information representation in different\nlayers of these powerful networks. By evaluating speaker identification\naccuracy across acoustic, phonemic, prosodic, and linguistic variants, we\nreport similarity between model performance and human identity perception. We\nfurther examine these similarities by juxtaposing the encoding spaces of models\nand humans and challenging the use of distance metrics as a proxy for speaker\nproximity. Lastly, we show that some models can predict brain responses in\nAuditory and Language regions during naturalistic stimuli.", "published": "2024-06-14 20:07:21", "link": "http://arxiv.org/abs/2406.10401v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Phoneme Discretized Saliency Maps for Explainable Detection of\n  AI-Generated Voice", "abstract": "In this paper, we propose Phoneme Discretized Saliency Maps (PDSM), a\ndiscretization algorithm for saliency maps that takes advantage of phoneme\nboundaries for explainable detection of AI-generated voice. We experimentally\nshow with two different Text-to-Speech systems (i.e., Tacotron2 and\nFastspeech2) that the proposed algorithm produces saliency maps that result in\nmore faithful explanations compared to standard posthoc explanation methods.\nMoreover, by associating the saliency maps to the phoneme representations, this\nmethodology generates explanations that tend to be more understandable than\nstandard saliency maps on magnitude spectrograms.", "published": "2024-06-14 21:56:21", "link": "http://arxiv.org/abs/2406.10422v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "ED-sKWS: Early-Decision Spiking Neural Networks for Rapid,and\n  Energy-Efficient Keyword Spotting", "abstract": "Keyword Spotting (KWS) is essential in edge computing requiring rapid and\nenergy-efficient responses. Spiking Neural Networks (SNNs) are well-suited for\nKWS for their efficiency and temporal capacity for speech. To further reduce\nthe latency and energy consumption, this study introduces ED-sKWS, an SNN-based\nKWS model with an early-decision mechanism that can stop speech processing and\noutput the result before the end of speech utterance. Furthermore, we introduce\na Cumulative Temporal (CT) loss that can enhance prediction accuracy at both\nthe intermediate and final timesteps. To evaluate early-decision performance,\nwe present the SC-100 dataset including 100 speech commands with beginning and\nend timestamp annotation. Experiments on the Google Speech Commands v2 and our\nSC-100 datasets show that ED-sKWS maintains competitive accuracy with 61%\ntimesteps and 52% energy consumption compared to SNN models without\nearly-decision mechanism, ensuring rapid response and energy efficiency.", "published": "2024-06-14 03:46:01", "link": "http://arxiv.org/abs/2406.12726v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Understanding Pedestrian Movement Using Urban Sensing Technologies: The\n  Promise of Audio-based Sensors", "abstract": "While various sensors have been deployed to monitor vehicular flows, sensing\npedestrian movement is still nascent. Yet walking is a significant mode of\ntravel in many cities, especially those in Europe, Africa, and Asia.\nUnderstanding pedestrian volumes and flows is essential for designing safer and\nmore attractive pedestrian infrastructure and for controlling periodic\novercrowding. This study discusses a new approach to scale up urban sensing of\npeople with the help of novel audio-based technology. It assesses the benefits\nand limitations of microphone-based sensors as compared to other forms of\npedestrian sensing. A large-scale dataset called ASPED is presented, which\nincludes high-quality audio recordings along with video recordings used for\nlabeling the pedestrian count data. The baseline analyses highlight the promise\nof using audio sensors for pedestrian tracking, although algorithmic and\ntechnological improvements to make the sensors practically usable continue.\nThis study also demonstrates how the data can be leveraged to predict\npedestrian trajectories. Finally, it discusses the use cases and scenarios\nwhere audio-based pedestrian sensing can support better urban and\ntransportation planning.", "published": "2024-06-14 13:15:18", "link": "http://arxiv.org/abs/2406.09998v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
