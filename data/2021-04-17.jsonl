{"title": "Identifying the Limits of Cross-Domain Knowledge Transfer for Pretrained\n  Models", "abstract": "There is growing evidence that pretrained language models improve\ntask-specific fine-tuning not just for the languages seen in pretraining, but\nalso for new languages and even non-linguistic data. What is the nature of this\nsurprising cross-domain transfer? We offer a partial answer via a systematic\nexploration of how much transfer occurs when models are denied any information\nabout word identity via random scrambling. In four classification tasks and two\nsequence labeling tasks, we evaluate baseline models, LSTMs using GloVe\nembeddings, and BERT. We find that only BERT shows high rates of transfer into\nour scrambled domains, and for classification but not sequence labeling tasks.\nOur analyses seek to explain why transfer succeeds for some tasks but not\nothers, to isolate the separate contributions of pretraining versus\nfine-tuning, and to quantify the role of word frequency. These findings help\nexplain where and why cross-domain transfer occurs, which can guide future\nstudies and practical fine-tuning efforts.", "published": "2021-04-17 00:14:39", "link": "http://arxiv.org/abs/2104.08410v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequential Cross-Document Coreference Resolution", "abstract": "Relating entities and events in text is a key component of natural language\nunderstanding. Cross-document coreference resolution, in particular, is\nimportant for the growing interest in multi-document analysis tasks. In this\nwork we propose a new model that extends the efficient sequential prediction\nparadigm for coreference resolution to cross-document settings and achieves\ncompetitive results for both entity and event coreference while provides strong\nevidence of the efficacy of both sequential models and higher-order inference\nin cross-document settings. Our model incrementally composes mentions into\ncluster representations and predicts links between a mention and the already\nconstructed clusters, approximating a higher-order model. In addition, we\nconduct extensive ablation studies that provide new insights into the\nimportance of various inputs and representation types in coreference.", "published": "2021-04-17 00:46:57", "link": "http://arxiv.org/abs/2104.08413v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Full Text-Dependent End to End Mispronunciation Detection and\n  Diagnosis with Easy Data Augmentation Techniques", "abstract": "Recently, end-to-end mispronunciation detection and diagnosis (MD&D) systems\nhas become a popular alternative to greatly simplify the model-building process\nof conventional hybrid DNN-HMM systems by representing complicated modules with\na single deep network architecture. In this paper, in order to utilize the\nprior text in the end-to-end structure, we present a novel text-dependent model\nwhich is difference with sed-mdd, the model achieves a fully end-to-end system\nby aligning the audio with the phoneme sequences of the prior text inside the\nmodel through the attention mechanism. Moreover, the prior text as input will\nbe a problem of imbalance between positive and negative samples in the phoneme\nsequence. To alleviate this problem, we propose three simple data augmentation\nmethods, which effectively improve the ability of model to capture\nmispronounced phonemes. We conduct experiments on L2-ARCTIC, and our best\nperformance improved from 49.29% to 56.08% in F-measure metric compared to the\nCNN-RNN-CTC model.", "published": "2021-04-17 03:11:41", "link": "http://arxiv.org/abs/2104.08428v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Three-level Hierarchical Transformer Networks for Long-sequence and\n  Multiple Clinical Documents Classification", "abstract": "We present a Three-level Hierarchical Transformer Network (3-level-HTN) for\nmodeling long-term dependencies across clinical notes for the purpose of\npatient-level prediction. The network is equipped with three levels of\nTransformer-based encoders to learn progressively from words to sentences,\nsentences to notes, and finally notes to patients. The first level from word to\nsentence directly applies a pre-trained BERT model as a fully trainable\ncomponent. While the second and third levels both implement a stack of\ntransformer-based encoders, before the final patient representation is fed into\na classification layer for clinical predictions. Compared to conventional BERT\nmodels, our model increases the maximum input length from 512 tokens to much\nlonger sequences that are appropriate for modeling large numbers of clinical\nnotes. We empirically examine different hyper-parameters to identify an optimal\ntrade-off given computational resource limits. Our experiment results on the\nMIMIC-III dataset for different prediction tasks demonstrate that the proposed\nHierarchical Transformer Network outperforms previous state-of-the-art models,\nincluding but not limited to BigBird.", "published": "2021-04-17 04:45:52", "link": "http://arxiv.org/abs/2104.08444v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Distillation for Text Classification", "abstract": "Deep learning techniques have achieved great success in many fields, while at\nthe same time deep learning models are getting more complex and expensive to\ncompute. It severely hinders the wide applications of these models. In order to\nalleviate this problem, model distillation emerges as an effective means to\ncompress a large model into a smaller one without a significant drop in\naccuracy. In this paper, we study a related but orthogonal issue, data\ndistillation, which aims to distill the knowledge from a large training dataset\ndown to a smaller and synthetic one. It has the potential to address the large\nand growing neural network training problem based on the small dataset. We\ndevelop a novel data distillation method for text classification. We evaluate\nour method on eight benchmark datasets. The results that the distilled data\nwith the size of 0.1% of the original text data achieves approximately 90%\nperformance of the original is rather impressive.", "published": "2021-04-17 04:54:54", "link": "http://arxiv.org/abs/2104.08448v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "R&R: Metric-guided Adversarial Sentence Generation", "abstract": "Adversarial examples are helpful for analyzing and improving the robustness\nof text classifiers. Generating high-quality adversarial examples is a\nchallenging task as it requires generating fluent adversarial sentences that\nare semantically similar to the original sentences and preserve the original\nlabels, while causing the classifier to misclassify them. Existing methods\nprioritize misclassification by maximizing each perturbation's effectiveness at\nmisleading a text classifier; thus, the generated adversarial examples fall\nshort in terms of fluency and similarity. In this paper, we propose a rewrite\nand rollback (R&R) framework for adversarial attack. It improves the quality of\nadversarial examples by optimizing a critique score which combines the fluency,\nsimilarity, and misclassification metrics. R&R generates high-quality\nadversarial examples by allowing exploration of perturbations that do not have\nimmediate impact on the misclassification metric but can improve fluency and\nsimilarity metrics. We evaluate our method on 5 representative datasets and 3\nclassifier architectures. Our method outperforms current state-of-the-art in\nattack success rate by +16.2%, +12.8%, and +14.0% on the classifiers\nrespectively. Code is available at https://github.com/DAI-Lab/fibber", "published": "2021-04-17 05:21:35", "link": "http://arxiv.org/abs/2104.08453v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path\n  Grounding", "abstract": "Dialogue systems powered by large pre-trained language models (LM) exhibit an\ninnate ability to deliver fluent and natural-looking responses. Despite their\nimpressive generation performance, these models can often generate factually\nincorrect statements impeding their widespread adoption. In this paper, we\nfocus on the task of improving the faithfulness -- and thus reduce\nhallucination -- of Neural Dialogue Systems to known facts supplied by a\nKnowledge Graph (KG). We propose Neural Path Hunter which follows a\ngenerate-then-refine strategy whereby a generated response is amended using the\nk-hop subgraph of a KG. Neural Path Hunter leverages a separate token-level\nfact critic to identify plausible sources of hallucination followed by a\nrefinement stage consisting of a chain of two neural LM's that retrieves\ncorrect entities by crafting a query signal that is propagated over the k-hop\nsubgraph. Our proposed model can easily be applied to any dialogue generated\nresponses without retraining the model. We empirically validate our proposed\napproach on the OpenDialKG dataset against a suite of metrics and report a\nrelative improvement of faithfulness over dialogue responses by 20.35% based on\nFeQA (Durmus et al., 2020).", "published": "2021-04-17 05:23:44", "link": "http://arxiv.org/abs/2104.08455v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Moving on from OntoNotes: Coreference Resolution Model Transfer", "abstract": "Academic neural models for coreference resolution (coref) are typically\ntrained on a single dataset, OntoNotes, and model improvements are benchmarked\non that same dataset. However, real-world applications of coref depend on the\nannotation guidelines and the domain of the target dataset, which often differ\nfrom those of OntoNotes. We aim to quantify transferability of coref models\nbased on the number of annotated documents available in the target dataset. We\nexamine eleven target datasets and find that continued training is consistently\neffective and especially beneficial when there are few target documents. We\nestablish new benchmarks across several datasets, including state-of-the-art\nresults on PreCo.", "published": "2021-04-17 05:35:07", "link": "http://arxiv.org/abs/2104.08457v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntactic structures and the general Markov models", "abstract": "We study phylogenetic signal present in syntactic information by considering\nthe syntactic structures data from Longobardi (2017b), Collins (2010), Ceolin\net al. (2020) and Koopman (2011). Focusing first on the general Markov models,\nwe explore how well the the syntactic structures data conform to the hypothesis\nrequired by these models. We do this by comparing derived phylogenetic trees\nagainst trees agreed on by the linguistics community. We then interpret the\nmethods of Ceolin et al. (2020) as an infinite sites evolutionary model and\ncompare the consistency of the data with this alternative. The ideas and\nmethods discussed in the present paper are more generally applicable than to\nthe specific setting of syntactic structures, and can be used in other\ncontexts, when analyzing consistency of data with against hypothesized\nevolutionary models.", "published": "2021-04-17 05:58:16", "link": "http://arxiv.org/abs/2104.08462v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A multilabel approach to morphosyntactic probing", "abstract": "We introduce a multilabel probing task to assess the morphosyntactic\nrepresentations of word embeddings from multilingual language models. We\ndemonstrate this task with multilingual BERT (Devlin et al., 2018), training\nprobes for seven typologically diverse languages of varying morphological\ncomplexity: Afrikaans, Croatian, Finnish, Hebrew, Korean, Spanish, and Turkish.\nThrough this simple but robust paradigm, we show that multilingual BERT renders\nmany morphosyntactic features easily and simultaneously extractable (e.g.,\ngender, grammatical case, pronominal type). We further evaluate the probes on\nsix \"held-out\" languages in a zero-shot transfer setting: Arabic, Chinese,\nMarathi, Slovenian, Tagalog, and Yoruba. This style of probing has the added\nbenefit of revealing the linguistic properties that language models recognize\nas being shared across languages. For instance, the probes performed well on\nrecognizing nouns in the held-out languages, suggesting that multilingual BERT\nhas a conception of noun-hood that transcends individual languages; yet, the\nsame was not true of adjectives.", "published": "2021-04-17 06:24:04", "link": "http://arxiv.org/abs/2104.08464v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Frequency-based Distortions in Contextualized Word Embeddings", "abstract": "How does word frequency in pre-training data affect the behavior of\nsimilarity metrics in contextualized BERT embeddings? Are there systematic ways\nin which some word relationships are exaggerated or understated? In this work,\nwe explore the geometric characteristics of contextualized word embeddings with\ntwo novel tools: (1) an identity probe that predicts the identity of a word\nusing its embedding; (2) the minimal bounding sphere for a word's\ncontextualized representations. Our results reveal that words of high and low\nfrequency differ significantly with respect to their representational geometry.\nSuch differences introduce distortions: when compared to human judgments, point\nestimates of embedding similarity (e.g., cosine similarity) can over- or\nunder-estimate the semantic similarity of two words, depending on the frequency\nof those words in the training data. This has downstream societal implications:\nBERT-Base has more trouble differentiating between South American and African\ncountries than North American and European ones. We find that these distortions\npersist when using BERT-Multilingual, suggesting that they cannot be easily\nfixed with additional data, which in turn introduces new distortions.", "published": "2021-04-17 06:35:48", "link": "http://arxiv.org/abs/2104.08465v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence Concatenation Approach to Data Augmentation for Neural Machine\n  Translation", "abstract": "Neural machine translation (NMT) has recently gained widespread attention\nbecause of its high translation accuracy. However, it shows poor performance in\nthe translation of long sentences, which is a major issue in low-resource\nlanguages. It is assumed that this issue is caused by insufficient number of\nlong sentences in the training data. Therefore, this study proposes a simple\ndata augmentation method to handle long sentences. In this method, we use only\nthe given parallel corpora as the training data and generate long sentences by\nconcatenating two sentences. Based on the experimental results, we confirm\nimprovements in long sentence translation by the proposed data augmentation\nmethod, despite its simplicity. Moreover, the translation quality is further\nimproved by the proposed method, when combined with back-translation.", "published": "2021-04-17 08:04:42", "link": "http://arxiv.org/abs/2104.08478v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Share by Masking the Non-shared for Multi-domain Sentiment\n  Classification", "abstract": "Multi-domain sentiment classification deals with the scenario where labeled\ndata exists for multiple domains but insufficient for training effective\nsentiment classifiers that work across domains. Thus, fully exploiting\nsentiment knowledge shared across domains is crucial for real world\napplications. While many existing works try to extract domain-invariant\nfeatures in high-dimensional space, such models fail to explicitly distinguish\nbetween shared and private features at text-level, which to some extent lacks\ninterpretablity. Based on the assumption that removing domain-related tokens\nfrom texts would help improve their domain-invariance, we instead first\ntransform original sentences to be domain-agnostic. To this end, we propose the\nBertMasker network which explicitly masks domain-related words from texts,\nlearns domain-invariant sentiment features from these domain-agnostic texts,\nand uses those masked words to form domain-aware sentence representations.\nEmpirical experiments on a well-adopted multiple domain sentiment\nclassification dataset demonstrate the effectiveness of our proposed model on\nboth multi-domain sentiment classification and cross-domain settings, by\nincreasing the accuracy by 0.94% and 1.8% respectively. Further analysis on\nmasking proves that removing those domain-related and sentiment irrelevant\ntokens decreases texts' domain distinction, resulting in the performance\ndegradation of a BERT-based domain classifier by over 12%.", "published": "2021-04-17 08:15:29", "link": "http://arxiv.org/abs/2104.08480v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Few-shot Relation Classification: Evaluation Data and\n  Classification Schemes", "abstract": "We explore Few-Shot Learning (FSL) for Relation Classification (RC). Focusing\non the realistic scenario of FSL, in which a test instance might not belong to\nany of the target categories (none-of-the-above, aka NOTA), we first revisit\nthe recent popular dataset structure for FSL, pointing out its unrealistic data\ndistribution. To remedy this, we propose a novel methodology for deriving more\nrealistic few-shot test data from available datasets for supervised RC, and\napply it to the TACRED dataset. This yields a new challenging benchmark for FSL\nRC, on which state of the art models show poor performance. Next, we analyze\nclassification schemes within the popular embedding-based nearest-neighbor\napproach for FSL, with respect to constraints they impose on the embedding\nspace. Triggered by this analysis we propose a novel classification scheme, in\nwhich the NOTA category is represented as learned vectors, shown empirically to\nbe an appealing option for FSL.", "published": "2021-04-17 08:16:49", "link": "http://arxiv.org/abs/2104.08481v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Minimal Supervision for Morphological Inflection", "abstract": "Neural models for the various flavours of morphological inflection tasks have\nproven to be extremely accurate given ample labeled data -- data that may be\nslow and costly to obtain. In this work we aim to overcome this annotation\nbottleneck by bootstrapping labeled data from a seed as little as {\\em five}\nlabeled paradigms, accompanied by a large bulk of unlabeled text. Our approach\nexploits different kinds of regularities in morphological systems in a\ntwo-phased setup, where word tagging based on {\\em analogies} is followed by\nword pairing based on {\\em distances}. We experiment with the Paradigm Cell\nFilling Problem over eight typologically different languages, and find that, in\nlanguages with relatively simple morphology, orthographic regularities on their\nown allow inflection models to achieve respectable accuracy. Combined\northographic and semantic regularities alleviate difficulties with particularly\ncomplex morpho-phonological systems. Our results suggest that hand-crafting\nmany tagged examples might be an unnecessary effort. However, more work is\nneeded in order to address rarely used forms.", "published": "2021-04-17 11:07:36", "link": "http://arxiv.org/abs/2104.08512v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual and Cross-Lingual Intent Detection from Spoken Data", "abstract": "We present a systematic study on multilingual and cross-lingual intent\ndetection from spoken data. The study leverages a new resource put forth in\nthis work, termed MInDS-14, a first training and evaluation resource for the\nintent detection task with spoken data. It covers 14 intents extracted from a\ncommercial system in the e-banking domain, associated with spoken examples in\n14 diverse language varieties. Our key results indicate that combining machine\ntranslation models with state-of-the-art multilingual sentence encoders (e.g.,\nLaBSE) can yield strong intent detectors in the majority of target languages\ncovered in MInDS-14, and offer comparative analyses across different axes:\ne.g., zero-shot versus few-shot learning, translation direction, and impact of\nspeech recognition. We see this work as an important step towards more\ninclusive development and evaluation of multilingual intent detectors from\nspoken data, in a much wider spectrum of languages compared to prior work.", "published": "2021-04-17 12:17:28", "link": "http://arxiv.org/abs/2104.08524v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Impact of ASR on the Automatic Analysis of Linguistic Complexity and\n  Sophistication in Spontaneous L2 Speech", "abstract": "In recent years, automated approaches to assessing linguistic complexity in\nsecond language (L2) writing have made significant progress in gauging learner\nperformance, predicting human ratings of the quality of learner productions,\nand benchmarking L2 development. In contrast, there is comparatively little\nwork in the area of speaking, particularly with respect to fully automated\napproaches to assessing L2 spontaneous speech. While the importance of a\nwell-performing ASR system is widely recognized, little research has been\nconducted to investigate the impact of its performance on subsequent automatic\ntext analysis. In this paper, we focus on this issue and examine the impact of\nusing a state-of-the-art ASR system for subsequent automatic analysis of\nlinguistic complexity in spontaneously produced L2 speech. A set of 30 selected\nmeasures were considered, falling into four categories: syntactic, lexical,\nn-gram frequency, and information-theoretic measures. The agreement between the\nscores for these measures obtained on the basis of ASR-generated vs. manual\ntranscriptions was determined through correlation analysis. A more differential\neffect of ASR performance on specific types of complexity measures when\ncontrolling for task type effects is also presented.", "published": "2021-04-17 12:45:49", "link": "http://arxiv.org/abs/2104.08529v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Topic Confusion Task: A Novel Scenario for Authorship Attribution", "abstract": "Authorship attribution is the problem of identifying the most plausible\nauthor of an anonymous text from a set of candidate authors. Researchers have\ninvestigated same-topic and cross-topic scenarios of authorship attribution,\nwhich differ according to whether new, unseen topics are used in the testing\nphase. However, neither scenario allows us to explain whether errors are caused\nby a failure to capture authorship writing style or by a topic shift. Motivated\nby this, we propose the \\emph{topic confusion} task where we switch the\nauthor-topic configuration between the training and testing sets. This setup\nallows us to distinguish two types of errors: those caused by the topic shift\nand those caused by the features' inability to capture the writing styles. We\nshow that stylometric features with part-of-speech tags are the least\nsusceptible to topic variations. We further show that combining them with other\nfeatures leads to significantly lower topic confusion and higher attribution\naccuracy. Finally, we show that pretrained language models such as BERT and\nRoBERTa perform poorly on this task and are surpassed by simple features such\nas word-level $n$-grams.", "published": "2021-04-17 12:50:58", "link": "http://arxiv.org/abs/2104.08530v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The challenges of temporal alignment on Twitter during crises", "abstract": "Language use changes over time, and this impacts the effectiveness of NLP\nsystems. This phenomenon is even more prevalent in social media data during\ncrisis events where meaning and frequency of word usage may change over the\ncourse of days. Contextual language models fail to adapt temporally,\nemphasizing the need for temporal adaptation in models which need to be\ndeployed over an extended period of time. While existing approaches consider\ndata spanning large periods of time (from years to decades), shorter time spans\nare critical for crisis data. We quantify temporal degradation for this\nscenario and propose methods to cope with performance loss by leveraging\ntechniques from domain adaptation. To the best of our knowledge, this is the\nfirst effort to explore effects of rapid language change driven by adversarial\nadaptations, particularly during natural and human-induced disasters. Through\nextensive experimentation on diverse crisis datasets, we analyze under what\nconditions our approaches outperform strong baselines while highlighting the\ncurrent limitations of temporal adaptation methods in scenarios where access to\nunlabeled data is scarce.", "published": "2021-04-17 13:11:41", "link": "http://arxiv.org/abs/2104.08535v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Multi-Perspective Abstractive Answer Summarization", "abstract": "Community Question Answering (CQA) forums such as Stack Overflow and Yahoo!\nAnswers contain a rich resource of answers to a wide range of questions. Each\nquestion thread can receive a large number of answers with different\nperspectives. The goal of multi-perspective answer summarization is to produce\na summary that includes all perspectives of the answer. A major obstacle for\nmulti-perspective, abstractive answer summarization is the absence of a dataset\nto provide supervision for producing such summaries. This work introduces a\nnovel dataset creation method to automatically create multi-perspective,\nbullet-point abstractive summaries from an existing CQA forum. Supervision\nprovided by this dataset trains models to inherently produce multi-perspective\nsummaries. Additionally, to train models to output more diverse, faithful\nanswer summaries while retaining multiple perspectives, we propose a\nmulti-reward optimization technique coupled with a sentence-relevance\nprediction multi-task loss. Our methods demonstrate improved coverage of\nperspectives and faithfulness as measured by automatic and human evaluations\ncompared to a strong baseline.", "published": "2021-04-17 13:15:29", "link": "http://arxiv.org/abs/2104.08536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DWUG: A large Resource of Diachronic Word Usage Graphs in Four Languages", "abstract": "Word meaning is notoriously difficult to capture, both synchronically and\ndiachronically. In this paper, we describe the creation of the largest resource\nof graded contextualized, diachronic word meaning annotation in four different\nlanguages, based on 100,000 human semantic proximity judgments. We thoroughly\ndescribe the multi-round incremental annotation process, the choice for a\nclustering algorithm to group usages into senses, and possible - diachronic and\nsynchronic - uses for this dataset.", "published": "2021-04-17 13:34:45", "link": "http://arxiv.org/abs/2104.08540v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Crossing the Conversational Chasm: A Primer on Natural Language\n  Processing for Multilingual Task-Oriented Dialogue Systems", "abstract": "In task-oriented dialogue (ToD), a user holds a conversation with an\nartificial agent to complete a concrete task. Although this technology\nrepresents one of the central objectives of AI and has been the focus of ever\nmore intense research and development efforts, it is currently limited to a few\nnarrow domains (e.g., food ordering, ticket booking) and a handful of languages\n(e.g., English, Chinese). This work provides an extensive overview of existing\nmethods and resources in multilingual ToD as an entry point to this exciting\nand emerging field. We find that the most critical factor preventing the\ncreation of truly multilingual ToD systems is the lack of datasets in most\nlanguages for both training and evaluation. In fact, acquiring annotations or\nhuman feedback for each component of modular systems or for data-hungry\nend-to-end systems is expensive and tedious. Hence, state-of-the-art approaches\nto multilingual ToD mostly rely on (zero- or few-shot) cross-lingual transfer\nfrom resource-rich languages (almost exclusively English), either by means of\nmachine translation or multilingual representations. These approaches are\ncurrently viable only for typologically similar languages and languages with\nparallel / monolingual corpora available. On the other hand, their\neffectiveness beyond these boundaries is doubtful or hard to assess due to the\nlack of linguistically diverse benchmarks (especially for natural language\ngeneration and end-to-end evaluation). To overcome this limitation, we draw\nparallels between components of the ToD pipeline and other NLP tasks, which can\ninspire solutions for learning in low-resource scenarios. Finally, we list\nadditional challenges that multilinguality poses for related areas (such as\nspeech and human-centred evaluation), and indicate future directions that hold\npromise to further expand language coverage and dialogue capabilities of\ncurrent ToD systems.", "published": "2021-04-17 15:19:56", "link": "http://arxiv.org/abs/2104.08570v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence Alignment with Parallel Documents Facilitates Biomedical\n  Machine Translation", "abstract": "Objective: Today's neural machine translation (NMT) can achieve near\nhuman-level translation quality and greatly facilitates international\ncommunications, but the lack of parallel corpora poses a key problem to the\ndevelopment of translation systems for highly specialized domains, such as\nbiomedicine. This work presents an unsupervised algorithm for deriving parallel\ncorpora from document-level translations by using sentence alignment and\nexplores how training materials affect the performance of biomedical NMT\nsystems. Materials and Methods: Document-level translations are mixed to train\nbilingual word embeddings (BWEs) for the evaluation of cross-lingual word\nsimilarity, and sentence distance is defined by combining semantic and\npositional similarities of the sentences. The alignment of sentences is\nformulated as an extended earth mover's distance problem. A Chinese-English\nbiomedical parallel corpus is derived with the proposed algorithm using\nbilingual articles from UpToDate and translations of PubMed abstracts, which is\nthen used for the training and evaluation of NMT. Results: On two manually\naligned translation datasets, the proposed algorithm achieved accurate sentence\nalignment in the 1-to-1 cases and outperformed competing algorithms in the\nmany-to-many cases. The NMT model fine-tuned on biomedical data significantly\nimproved the in-domain translation quality (zh-en: +17.72 BLEU; en-zh: +17.02\nBLEU). Both the size of the training data and the combination of different\ncorpora can significantly affect the model's performance. Conclusion: The\nproposed algorithm relaxes the assumption for sentence alignment and\neffectively generates accurate translation pairs that facilitate training high\nquality biomedical NMT models.", "published": "2021-04-17 16:09:30", "link": "http://arxiv.org/abs/2104.08588v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XLEnt: Mining a Large Cross-lingual Entity Dataset with\n  Lexical-Semantic-Phonetic Word Alignment", "abstract": "Cross-lingual named-entity lexica are an important resource to multilingual\nNLP tasks such as machine translation and cross-lingual wikification. While\nknowledge bases contain a large number of entities in high-resource languages\nsuch as English and French, corresponding entities for lower-resource languages\nare often missing. To address this, we propose Lexical-Semantic-Phonetic Align\n(LSP-Align), a technique to automatically mine cross-lingual entity lexica from\nmined web data. We demonstrate LSP-Align outperforms baselines at extracting\ncross-lingual entity pairs and mine 164 million entity pairs from 120 different\nlanguages aligned with English. We release these cross-lingual entity pairs\nalong with the massively multilingual tagged named entity corpus as a resource\nto the NLP community.", "published": "2021-04-17 16:58:05", "link": "http://arxiv.org/abs/2104.08597v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Stylistic Analysis of Honest Deception: The Case of Seinfeld TV Series\n  Sitcom", "abstract": "Language is a powerful tool if used in the correct manner. It is the major\nmode of communication, and using the correct choice of words and styles can\nserve to have a long-lasting impact. Stylistics is the study of the use of\nvarious language styles in communication to pass a message with a bigger impact\nor to communicate indirectly. Stylistic analysis, therefore, is the study of\nthe use of linguistic styles in texts to determine how a style has been used,\nwhat is communicated and how it is communicated. Honest deception is the use of\na choice of words to imply something different from the literal meaning. A\nperson listening or reading a text where honest deception has been used and\nwith a literal understanding may completely miss out on the point. This is\nbecause the issue of honesty and falsehood arises. However, it would be better\nto understand that honest deception is used with the intention of having a\nlasting impact rather than to deceive the readers, viewers or listeners. The\nmajor styles used in honest deception are hyperboles, litotes, irony and\nsarcasm. The Seinfeld Sitcom TV series was a situational TV comedy show aired\nfrom 1990 to 1998. the show attempts to bring to the understanding the daily\nlife of a comedian and how comedian views life experiences and convert them\ninto hilarious jokes. It also shows Jerry's struggle with getting the right\npartner from the many women who come into his life. Reflecting on honest\ndeception in the Seinfeld sitcom TV series, this paper is going to investigate\nhow honest deception has been used in the series, why it has been used and what\nis being communicated. The study is going to use a recapitulative form to give\na better analysis and grouping of the different styles used in honest deception\nthroughout the series.", "published": "2021-04-17 17:17:03", "link": "http://arxiv.org/abs/2104.08599v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Who Responded to Whom: The Joint Effects of Latent Topics and Discourse\n  in Conversation Structure", "abstract": "Numerous online conversations are produced on a daily basis, resulting in a\npressing need to conversation understanding. As a basis to structure a\ndiscussion, we identify the responding relations in the conversation discourse,\nwhich link response utterances to their initiations. To figure out who\nresponded to whom, here we explore how the consistency of topic contents and\ndependency of discourse roles indicate such interactions, whereas most prior\nwork ignore the effects of latent factors underlying word occurrences. We\npropose a model to learn latent topics and discourse in word distributions, and\npredict pairwise initiation-response links via exploiting topic consistency and\ndiscourse dependency. Experimental results on both English and Chinese\nconversations show that our model significantly outperforms the previous state\nof the arts, such as 79 vs. 73 MRR on Chinese customer service dialogues. We\nfurther probe into our outputs and shed light on how topics and discourse\nindicate conversational user interactions.", "published": "2021-04-17 17:46:00", "link": "http://arxiv.org/abs/2104.08601v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotion Classification in a Resource Constrained Language Using\n  Transformer-based Approach", "abstract": "Although research on emotion classification has significantly progressed in\nhigh-resource languages, it is still infancy for resource-constrained languages\nlike Bengali. However, unavailability of necessary language processing tools\nand deficiency of benchmark corpora makes the emotion classification task in\nBengali more challenging and complicated. This work proposes a\ntransformer-based technique to classify the Bengali text into one of the six\nbasic emotions: anger, fear, disgust, sadness, joy, and surprise. A Bengali\nemotion corpus consists of 6243 texts is developed for the classification task.\nExperimentation carried out using various machine learning (LR, RF, MNB, SVM),\ndeep neural networks (CNN, BiLSTM, CNN+BiLSTM) and transformer (Bangla-BERT,\nm-BERT, XLM-R) based approaches. Experimental outcomes indicate that XLM-R\noutdoes all other techniques by achieving the highest weighted $f_1$-score of\n$69.73\\%$ on the test data. The dataset is publicly available at\nhttps://github.com/omar-sharif03/NAACL-SRW-2021.", "published": "2021-04-17 18:28:39", "link": "http://arxiv.org/abs/2104.08613v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UPB at SemEval-2021 Task 5: Virtual Adversarial Training for Toxic Spans\n  Detection", "abstract": "The real-world impact of polarization and toxicity in the online sphere\nmarked the end of 2020 and the beginning of this year in a negative way.\nSemeval-2021, Task 5 - Toxic Spans Detection is based on a novel annotation of\na subset of the Jigsaw Unintended Bias dataset and is the first language\ntoxicity detection task dedicated to identifying the toxicity-level spans. For\nthis task, participants had to automatically detect character spans in short\ncomments that render the message as toxic. Our model considers applying Virtual\nAdversarial Training in a semi-supervised setting during the fine-tuning\nprocess of several Transformer-based models (i.e., BERT and RoBERTa), in\ncombination with Conditional Random Fields. Our approach leads to performance\nimprovements and more robust models, enabling us to achieve an F1-score of\n65.73% in the official submission and an F1-score of 66.13% after further\ntuning during post-evaluation.", "published": "2021-04-17 19:42:12", "link": "http://arxiv.org/abs/2104.08635v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AM2iCo: Evaluating Word Meaning in Context across Low-Resource Languages\n  with Adversarial Examples", "abstract": "Capturing word meaning in context and distinguishing between correspondences\nand variations across languages is key to building successful multilingual and\ncross-lingual text representation models. However, existing multilingual\nevaluation datasets that evaluate lexical semantics \"in-context\" have various\nlimitations. In particular, 1) their language coverage is restricted to\nhigh-resource languages and skewed in favor of only a few language families and\nareas, 2) a design that makes the task solvable via superficial cues, which\nresults in artificially inflated (and sometimes super-human) performances of\npretrained encoders, on many target languages, which limits their usefulness\nfor model probing and diagnostics, and 3) little support for cross-lingual\nevaluation. In order to address these gaps, we present AM2iCo (Adversarial and\nMultilingual Meaning in Context), a wide-coverage cross-lingual and\nmultilingual evaluation set; it aims to faithfully assess the ability of\nstate-of-the-art (SotA) representation models to understand the identity of\nword meaning in cross-lingual contexts for 14 language pairs. We conduct a\nseries of experiments in a wide range of setups and demonstrate the challenging\nnature of AM2iCo. The results reveal that current SotA pretrained encoders\nsubstantially lag behind human performance, and the largest gaps are observed\nfor low-resource languages and languages dissimilar to English.", "published": "2021-04-17 20:23:45", "link": "http://arxiv.org/abs/2104.08639v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Customized determination of stop words using Random Matrix Theory\n  approach", "abstract": "The distances between words calculated in word units are studied and compared\nwith the distributions of the Random Matrix Theory (RMT). It is found that the\ndistribution of distance between the same words can be well described by the\nsingle-parameter Brody distribution. Using the Brody distribution fit, we found\nthat the distance between given words in a set of texts can show mixed\ndynamics, coexisting regular and chaotic regimes. It is found that\ndistributions correctly fitted by the Brody distribution with a certain\ngoodness of the fit threshold can be identifid as stop words, usually\nconsidered as the uninformative part of the text. By applying various threshold\nvalues for the goodness of fit, we can extract uninformative words from the\ntexts under analysis to the desired extent. On this basis we formulate a fully\nagnostic recipe that can be used in the creation of a customized set of stop\nwords for texts in any language based on words.", "published": "2021-04-17 20:42:28", "link": "http://arxiv.org/abs/2104.08642v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Zero-Shot Cross-Lingual Transfer Learning via Robust Training", "abstract": "Pre-trained multilingual language encoders, such as multilingual BERT and\nXLM-R, show great potential for zero-shot cross-lingual transfer. However,\nthese multilingual encoders do not precisely align words and phrases across\nlanguages. Especially, learning alignments in the multilingual embedding space\nusually requires sentence-level or word-level parallel corpora, which are\nexpensive to be obtained for low-resource languages. An alternative is to make\nthe multilingual encoders more robust; when fine-tuning the encoder using\ndownstream task, we train the encoder to tolerate noise in the contextual\nembedding spaces such that even if the representations of different languages\nare not aligned well, the model can still achieve good performance on zero-shot\ncross-lingual transfer. In this work, we propose a learning strategy for\ntraining robust models by drawing connections between adversarial examples and\nthe failure cases of zero-shot cross-lingual transfer. We adopt two widely used\nrobust training methods, adversarial training and randomized smoothing, to\ntrain the desired robust model. The experimental results demonstrate that\nrobust training improves zero-shot cross-lingual transfer on text\nclassification tasks. The improvement is more significant in the generalized\ncross-lingual transfer setting, where the pair of input sentences belong to two\ndifferent languages.", "published": "2021-04-17 21:21:53", "link": "http://arxiv.org/abs/2104.08645v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Competency Problems: On Finding and Removing Artifacts in Language Data", "abstract": "Much recent work in NLP has documented dataset artifacts, bias, and spurious\ncorrelations between input features and output labels. However, how to tell\nwhich features have \"spurious\" instead of legitimate correlations is typically\nleft unspecified. In this work we argue that for complex language understanding\ntasks, all simple feature correlations are spurious, and we formalize this\nnotion into a class of problems which we call competency problems. For example,\nthe word \"amazing\" on its own should not give information about a sentiment\nlabel independent of the context in which it appears, which could include\nnegation, metaphor, sarcasm, etc. We theoretically analyze the difficulty of\ncreating data for competency problems when human bias is taken into account,\nshowing that realistic datasets will increasingly deviate from competency\nproblems as dataset size increases. This analysis gives us a simple statistical\ntest for dataset artifacts, which we use to show more subtle biases than were\ndescribed in prior work, including demonstrating that models are\ninappropriately affected by these less extreme biases. Our theoretical\ntreatment of this problem also allows us to analyze proposed solutions, such as\nmaking local edits to dataset instances, and to give recommendations for future\ndata collection and model design efforts that target competency problems.", "published": "2021-04-17 21:34:10", "link": "http://arxiv.org/abs/2104.08646v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning from Noisy Labels for Entity-Centric Information Extraction", "abstract": "Recent information extraction approaches have relied on training deep neural\nmodels. However, such models can easily overfit noisy labels and suffer from\nperformance degradation. While it is very costly to filter noisy labels in\nlarge learning resources, recent studies show that such labels take more\ntraining steps to be memorized and are more frequently forgotten than clean\nlabels, therefore are identifiable in training. Motivated by such properties,\nwe propose a simple co-regularization framework for entity-centric information\nextraction, which consists of several neural models with identical structures\nbut different parameter initialization. These models are jointly optimized with\nthe task-specific losses and are regularized to generate similar predictions\nbased on an agreement loss, which prevents overfitting on noisy labels.\nExtensive experiments on two widely used but noisy benchmarks for information\nextraction, TACRED and CoNLL03, demonstrate the effectiveness of our framework.\nWe release our code to the community for future research.", "published": "2021-04-17 22:49:12", "link": "http://arxiv.org/abs/2104.08656v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Monotonicity Marking from Universal Dependency Trees", "abstract": "Dependency parsing is a tool widely used in the field of Natural language\nprocessing and computational linguistics. However, there is hardly any work\nthat connects dependency parsing to monotonicity, which is an essential part of\nlogic and linguistic semantics. In this paper, we present a system that\nautomatically annotates monotonicity information based on Universal Dependency\nparse trees. Our system utilizes surface-level monotonicity facts about\nquantifiers, lexical items, and token-level polarity information. We compared\nour system's performance with existing systems in the literature, including\nNatLog and ccg2mono, on a small evaluation dataset. Results show that our\nsystem outperforms NatLog and ccg2mono.", "published": "2021-04-17 23:01:10", "link": "http://arxiv.org/abs/2104.08659v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Characterizing Idioms: Conventionality and Contingency", "abstract": "Idioms are unlike most phrases in two important ways. First, the words in an\nidiom have non-canonical meanings. Second, the non-canonical meanings of words\nin an idiom are contingent on the presence of other words in the idiom.\nLinguistic theories differ on whether these properties depend on one another,\nas well as whether special theoretical machinery is needed to accommodate\nidioms. We define two measures that correspond to the properties above, and we\nimplement them using BERT (Devlin et al., 2019) and XLNet(Yang et al., 2019).\nWe show that idioms fall at the expected intersection of the two dimensions,\nbut that the dimensions themselves are not correlated. Our results suggest that\nspecial machinery to handle idioms may not be warranted.", "published": "2021-04-17 23:46:57", "link": "http://arxiv.org/abs/2104.08664v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Embeddings Via Distributions", "abstract": "Despite recent monumental advances in the field, many Natural Language\nProcessing (NLP) models still struggle to perform adequately on noisy domains.\nWe propose a novel probabilistic embedding-level method to improve the\nrobustness of NLP models. Our method, Robust Embeddings via Distributions\n(RED), incorporates information from both noisy tokens and surrounding context\nto obtain distributions over embedding vectors that can express uncertainty in\nsemantic space more fully than any deterministic method. We evaluate our method\non a number of downstream tasks using existing state-of-the-art models in the\npresence of both natural and synthetic noise, and demonstrate a clear\nimprovement over other embedding approaches to robustness from the literature.", "published": "2021-04-17 02:02:36", "link": "http://arxiv.org/abs/2104.08420v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Are Word Embedding Methods Stable and Should We Care About It?", "abstract": "A representation learning method is considered stable if it consistently\ngenerates similar representation of the given data across multiple runs. Word\nEmbedding Methods (WEMs) are a class of representation learning methods that\ngenerate dense vector representation for each word in the given text data. The\ncentral idea of this paper is to explore the stability measurement of WEMs\nusing intrinsic evaluation based on word similarity. We experiment with three\npopular WEMs: Word2Vec, GloVe, and fastText. For stability measurement, we\ninvestigate the effect of five parameters involved in training these models. We\nperform experiments using four real-world datasets from different domains:\nWikipedia, News, Song lyrics, and European parliament proceedings. We also\nobserve the effect of WEM stability on three downstream tasks: Clustering, POS\ntagging, and Fairness evaluation. Our experiments indicate that amongst the\nthree WEMs, fastText is the most stable, followed by GloVe and Word2Vec.", "published": "2021-04-17 03:29:22", "link": "http://arxiv.org/abs/2104.08433v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Graph-guided Multi-round Retrieval Method for Conversational\n  Open-domain Question Answering", "abstract": "In recent years, conversational agents have provided a natural and convenient\naccess to useful information in people's daily life, along with a broad and new\nresearch topic, conversational question answering (QA). Among the popular\nconversational QA tasks, conversational open-domain QA, which requires to\nretrieve relevant passages from the Web to extract exact answers, is more\npractical but less studied. The main challenge is how to well capture and fully\nexplore the historical context in conversation to facilitate effective\nlarge-scale retrieval. The current work mainly utilizes history questions to\nrefine the current question or to enhance its representation, yet the relations\nbetween history answers and the current answer in a conversation, which is also\ncritical to the task, are totally neglected. To address this problem, we\npropose a novel graph-guided retrieval method to model the relations among\nanswers across conversation turns. In particular, it utilizes a passage graph\nderived from the hyperlink-connected passages that contains history answers and\npotential current answers, to retrieve more relevant passages for subsequent\nanswer extraction. Moreover, in order to collect more complementary information\nin the historical context, we also propose to incorporate the multi-round\nrelevance feedback technique to explore the impact of the retrieval context on\ncurrent question understanding. Experimental results on the public dataset\nverify the effectiveness of our proposed method. Notably, the F1 score is\nimproved by 5% and 11% with predicted history answers and true history answers,\nrespectively.", "published": "2021-04-17 04:39:41", "link": "http://arxiv.org/abs/2104.08443v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Joint Passage Ranking for Diverse Multi-Answer Retrieval", "abstract": "We study multi-answer retrieval, an under-explored problem that requires\nretrieving passages to cover multiple distinct answers for a given question.\nThis task requires joint modeling of retrieved passages, as models should not\nrepeatedly retrieve passages containing the same answer at the cost of missing\na different valid answer. In this paper, we introduce JPR, the first joint\npassage retrieval model for multi-answer retrieval. JPR makes use of an\nautoregressive reranker that selects a sequence of passages, each conditioned\non previously selected passages. JPR is trained to select passages that cover\nnew answers at each timestep and uses a tree-decoding algorithm to enable\nflexibility in the degree of diversity. Compared to prior approaches, JPR\nachieves significantly better answer coverage on three multi-answer datasets.\nWhen combined with downstream question answering, the improved retrieval\nenables larger answer generation models since they need to consider fewer\npassages, establishing a new state-of-the-art.", "published": "2021-04-17 04:48:36", "link": "http://arxiv.org/abs/2104.08445v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Context-Aware Interaction Network for Question Matching", "abstract": "Impressive milestones have been achieved in text matching by adopting a\ncross-attention mechanism to capture pertinent semantic connections between two\nsentence representations. However, regular cross-attention focuses on\nword-level links between the two input sequences, neglecting the importance of\ncontextual information. We propose a context-aware interaction network (COIN)\nto properly align two sequences and infer their semantic relationship.\nSpecifically, each interaction block includes (1) a context-aware\ncross-attention mechanism to effectively integrate contextual information when\naligning two sequences, and (2) a gate fusion layer to flexibly interpolate\naligned representations. We apply multiple stacked interaction blocks to\nproduce alignments at different levels and gradually refine the attention\nresults. Experiments on two question matching datasets and detailed analyses\ndemonstrate the effectiveness of our model.", "published": "2021-04-17 05:03:56", "link": "http://arxiv.org/abs/2104.08451v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mobile App Tasks with Iterative Feedback (MoTIF): Addressing Task\n  Feasibility in Interactive Visual Environments", "abstract": "In recent years, vision-language research has shifted to study tasks which\nrequire more complex reasoning, such as interactive question answering, visual\ncommon sense reasoning, and question-answer plausibility prediction. However,\nthe datasets used for these problems fail to capture the complexity of real\ninputs and multimodal environments, such as ambiguous natural language requests\nand diverse digital domains. We introduce Mobile app Tasks with Iterative\nFeedback (MoTIF), a dataset with natural language commands for the greatest\nnumber of interactive environments to date. MoTIF is the first to contain\nnatural language requests for interactive environments that are not\nsatisfiable, and we obtain follow-up questions on this subset to enable\nresearch on task uncertainty resolution. We perform initial feasibility\nclassification experiments and only reach an F1 score of 37.3, verifying the\nneed for richer vision-language representations and improved architectures to\nreason about task feasibility.", "published": "2021-04-17 14:48:02", "link": "http://arxiv.org/abs/2104.08560v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Zero-shot Slot Filling with DPR and RAG", "abstract": "The ability to automatically extract Knowledge Graphs (KG) from a given\ncollection of documents is a long-standing problem in Artificial Intelligence.\nOne way to assess this capability is through the task of slot filling. Given an\nentity query in form of [Entity, Slot, ?], a system is asked to `fill' the slot\nby generating or extracting the missing value from a relevant passage or\npassages. This capability is crucial to create systems for automatic knowledge\nbase population, which is becoming in ever-increasing demand, especially in\nenterprise applications. Recently, there has been a promising direction in\nevaluating language models in the same way we would evaluate knowledge bases,\nand the task of slot filling is the most suitable to this intent. The recent\nadvancements in the field try to solve this task in an end-to-end fashion using\nretrieval-based language models. Models like Retrieval Augmented Generation\n(RAG) show surprisingly good performance without involving complex information\nextraction pipelines. However, the results achieved by these models on the two\nslot filling tasks in the KILT benchmark are still not at the level required by\nreal-world information extraction systems. In this paper, we describe several\nstrategies we adopted to improve the retriever and the generator of RAG in\norder to make it a better slot filler. Our KGI0 system (available at\nhttps://github.com/IBM/retrieve-write-slot-filling) reached the top-1 position\non the KILT leaderboard on both T-REx and zsRE dataset with a large margin.", "published": "2021-04-17 18:24:51", "link": "http://arxiv.org/abs/2104.08610v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as\n  a Target for NLP", "abstract": "Cryptic crosswords, the dominant crossword variety in the UK, are a promising\ntarget for advancing NLP systems that seek to process semantically complex,\nhighly compositional language. Cryptic clues read like fluent natural language\nbut are adversarially composed of two parts: a definition and a wordplay cipher\nrequiring character-level manipulations. Expert humans use creative\nintelligence to solve cryptics, flexibly combining linguistic, world, and\ndomain knowledge. In this paper, we make two main contributions. First, we\npresent a dataset of cryptic clues as a challenging new benchmark for NLP\nsystems that seek to process compositional language in more creative,\nhuman-like ways. After showing that three non-neural approaches and T5, a\nstate-of-the-art neural language model, do not achieve good performance, we\nmake our second main contribution: a novel curriculum approach, in which the\nmodel is first fine-tuned on related tasks such as unscrambling words.We also\nintroduce a challenging data split, examine the meta-linguistic capabilities of\nsubword-tokenized models, and investigate model systematicity by perturbing the\nwordplay part of clues, showing that T5 exhibits behavior partially consistent\nwith human solving strategies. Although our curricular approach considerably\nimproves on the T5 baseline, our best-performing model still fails to\ngeneralize to the extent that humans can. Thus, cryptic crosswords remain an\nunsolved challenge for NLP systems and a potential source of future innovation.", "published": "2021-04-17 18:54:00", "link": "http://arxiv.org/abs/2104.08620v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Question Decomposition with Dependency Graphs", "abstract": "QDMR is a meaning representation for complex questions, which decomposes\nquestions into a sequence of atomic steps. While state-of-the-art QDMR parsers\nuse the common sequence-to-sequence (seq2seq) approach, a QDMR structure\nfundamentally describes labeled relations between spans in the input question,\nand thus dependency-based approaches seem appropriate for this task. In this\nwork, we present a QDMR parser that is based on dependency graphs (DGs), where\nnodes in the graph are words and edges describe logical relations that\ncorrespond to the different computation steps. We propose (a) a\nnon-autoregressive graph parser, where all graph edges are computed\nsimultaneously, and (b) a seq2seq parser that uses gold graph as auxiliary\nsupervision. We find that a graph parser leads to a moderate reduction in\nperformance (0.47 to 0.44), but to a 16x speed-up in inference time due to the\nnon-autoregressive nature of the parser, and to improved sample complexity\ncompared to a seq2seq model. Second, a seq2seq model trained with auxiliary\ngraph supervision has better generalization to new domains compared to a\nseq2seq model, and also performs better on questions with long sequences of\ncomputation steps.", "published": "2021-04-17 21:35:31", "link": "http://arxiv.org/abs/2104.08647v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DiS-ReX: A Multilingual Dataset for Distantly Supervised Relation\n  Extraction", "abstract": "Distant supervision (DS) is a well established technique for creating\nlarge-scale datasets for relation extraction (RE) without using human\nannotations. However, research in DS-RE has been mostly limited to the English\nlanguage. Constraining RE to a single language inhibits utilization of large\namounts of data in other languages which could allow extraction of more diverse\nfacts. Very recently, a dataset for multilingual DS-RE has been released.\nHowever, our analysis reveals that the proposed dataset exhibits unrealistic\ncharacteristics such as 1) lack of sentences that do not express any relation,\nand 2) all sentences for a given entity pair expressing exactly one relation.\nWe show that these characteristics lead to a gross overestimation of the model\nperformance. In response, we propose a new dataset, DiS-ReX, which alleviates\nthese issues. Our dataset has more than 1.5 million sentences, spanning across\n4 languages with 36 relation classes + 1 no relation (NA) class. We also modify\nthe widely used bag attention models by encoding sentences using mBERT and\nprovide the first benchmark results on multilingual DS-RE. Unlike the competing\ndataset, we show that our dataset is challenging and leaves enough room for\nfuture research to take place in this field.", "published": "2021-04-17 22:44:38", "link": "http://arxiv.org/abs/2104.08655v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Explaining Answers with Entailment Trees", "abstract": "Our goal, in the context of open-domain textual question-answering (QA), is\nto explain answers by showing the line of reasoning from what is known to the\nanswer, rather than simply showing a fragment of textual evidence (a\n\"rationale'\"). If this could be done, new opportunities for understanding and\ndebugging the system's reasoning become possible. Our approach is to generate\nexplanations in the form of entailment trees, namely a tree of multipremise\nentailment steps from facts that are known, through intermediate conclusions,\nto the hypothesis of interest (namely the question + answer). To train a model\nwith this skill, we created ENTAILMENTBANK, the first dataset to contain\nmultistep entailment trees. Given a hypothesis (question + answer), we define\nthree increasingly difficult explanation tasks: generate a valid entailment\ntree given (a) all relevant sentences (b) all relevant and some irrelevant\nsentences, or (c) a corpus. We show that a strong language model can partially\nsolve these tasks, in particular when the relevant sentences are included in\nthe input (e.g., 35% of trees for (a) are perfect), and with indications of\ngeneralization to other domains. This work is significant as it provides a new\ntype of dataset (multistep entailments) and baselines, offering a new avenue\nfor the community to generate richer, more systematic explanations.", "published": "2021-04-17 23:13:56", "link": "http://arxiv.org/abs/2104.08661v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KazakhTTS: An Open-Source Kazakh Text-to-Speech Synthesis Dataset", "abstract": "This paper introduces a high-quality open-source speech synthesis dataset for\nKazakh, a low-resource language spoken by over 13 million people worldwide. The\ndataset consists of about 93 hours of transcribed audio recordings spoken by\ntwo professional speakers (female and male). It is the first publicly available\nlarge-scale dataset developed to promote Kazakh text-to-speech (TTS)\napplications in both academia and industry. In this paper, we share our\nexperience by describing the dataset development procedures and faced\nchallenges, and discuss important future directions. To demonstrate the\nreliability of our dataset, we built baseline end-to-end TTS models and\nevaluated them using the subjective mean opinion score (MOS) measure.\nEvaluation results show that the best TTS models trained on our dataset achieve\nMOS above 4 for both speakers, which makes them applicable for practical use.\nThe dataset, training recipe, and pretrained TTS models are freely available.", "published": "2021-04-17 05:49:57", "link": "http://arxiv.org/abs/2104.08459v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Embodying Pre-Trained Word Embeddings Through Robot Actions", "abstract": "We propose a promising neural network model with which to acquire a grounded\nrepresentation of robot actions and the linguistic descriptions thereof.\nProperly responding to various linguistic expressions, including polysemous\nwords, is an important ability for robots that interact with people via\nlinguistic dialogue. Previous studies have shown that robots can use words that\nare not included in the action-description paired datasets by using pre-trained\nword embeddings. However, the word embeddings trained under the distributional\nhypothesis are not grounded, as they are derived purely from a text corpus. In\nthis letter, we transform the pre-trained word embeddings to embodied ones by\nusing the robot's sensory-motor experiences. We extend a bidirectional\ntranslation model for actions and descriptions by incorporating non-linear\nlayers that retrofit the word embeddings. By training the retrofit layer and\nthe bidirectional translation model alternately, our proposed model is able to\ntransform the pre-trained word embeddings to adapt to a paired\naction-description dataset. Our results demonstrate that the embeddings of\nsynonyms form a semantic cluster by reflecting the experiences (actions and\nenvironments) of a robot. These embeddings allow the robot to properly generate\nactions from unseen words that are not paired with actions in a dataset.", "published": "2021-04-17 12:04:49", "link": "http://arxiv.org/abs/2104.08521v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Multi-source Neural Topic Modeling in Multi-view Embedding Spaces", "abstract": "Though word embeddings and topics are complementary representations, several\npast works have only used pretrained word embeddings in (neural) topic modeling\nto address data sparsity in short-text or small collection of documents. This\nwork presents a novel neural topic modeling framework using multi-view\nembedding spaces: (1) pretrained topic-embeddings, and (2) pretrained\nword-embeddings (context insensitive from Glove and context-sensitive from BERT\nmodels) jointly from one or many sources to improve topic quality and better\ndeal with polysemy. In doing so, we first build respective pools of pretrained\ntopic (i.e., TopicPool) and word embeddings (i.e., WordPool). We then identify\none or more relevant source domain(s) and transfer knowledge to guide\nmeaningful learning in the sparse target domain. Within neural topic modeling,\nwe quantify the quality of topics and document representations via\ngeneralization (perplexity), interpretability (topic coherence) and information\nretrieval (IR) using short-text, long-text, small and large document\ncollections from news and medical domains. Introducing the multi-source\nmulti-view embedding spaces, we have shown state-of-the-art neural topic\nmodeling using 6 source (high-resource) and 5 target (low-resource) corpora.", "published": "2021-04-17 14:08:00", "link": "http://arxiv.org/abs/2104.08551v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GupShup: An Annotated Corpus for Abstractive Summarization of\n  Open-Domain Code-Switched Conversations", "abstract": "Code-switching is the communication phenomenon where speakers switch between\ndifferent languages during a conversation. With the widespread adoption of\nconversational agents and chat platforms, code-switching has become an integral\npart of written conversations in many multi-lingual communities worldwide. This\nmakes it essential to develop techniques for summarizing and understanding\nthese conversations. Towards this objective, we introduce abstractive\nsummarization of Hindi-English code-switched conversations and develop the\nfirst code-switched conversation summarization dataset - GupShup, which\ncontains over 6,831 conversations in Hindi-English and their corresponding\nhuman-annotated summaries in English and Hindi-English. We present a detailed\naccount of the entire data collection and annotation processes. We analyze the\ndataset using various code-switching statistics. We train state-of-the-art\nabstractive summarization models and report their performances using both\nautomated metrics and human evaluation. Our results show that multi-lingual\nmBART and multi-view seq2seq models obtain the best performances on the new\ndataset", "published": "2021-04-17 15:42:01", "link": "http://arxiv.org/abs/2104.08578v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "IITP@COLIEE 2019: Legal Information Retrieval using BM25 and BERT", "abstract": "Natural Language Processing (NLP) and Information Retrieval (IR) in the\njudicial domain is an essential task. With the advent of availability\ndomain-specific data in electronic form and aid of different Artificial\nintelligence (AI) technologies, automated language processing becomes more\ncomfortable, and hence it becomes feasible for researchers and developers to\nprovide various automated tools to the legal community to reduce human burden.\nThe Competition on Legal Information Extraction/Entailment (COLIEE-2019) run in\nassociation with the International Conference on Artificial Intelligence and\nLaw (ICAIL)-2019 has come up with few challenging tasks. The shared defined\nfour sub-tasks (i.e. Task1, Task2, Task3 and Task4), which will be able to\nprovide few automated systems to the judicial system. The paper presents our\nworking note on the experiments carried out as a part of our participation in\nall the sub-tasks defined in this shared task. We make use of different\nInformation Retrieval(IR) and deep learning based approaches to tackle these\nproblems. We obtain encouraging results in all these four sub-tasks.", "published": "2021-04-17 22:28:15", "link": "http://arxiv.org/abs/2104.08653v4", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information\n  Retrieval Models", "abstract": "Existing neural information retrieval (IR) models have often been studied in\nhomogeneous and narrow settings, which has considerably limited insights into\ntheir out-of-distribution (OOD) generalization capabilities. To address this,\nand to facilitate researchers to broadly evaluate the effectiveness of their\nmodels, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous\nevaluation benchmark for information retrieval. We leverage a careful selection\nof 18 publicly available datasets from diverse text retrieval tasks and domains\nand evaluate 10 state-of-the-art retrieval systems including lexical, sparse,\ndense, late-interaction and re-ranking architectures on the BEIR benchmark. Our\nresults show BM25 is a robust baseline and re-ranking and\nlate-interaction-based models on average achieve the best zero-shot\nperformances, however, at high computational costs. In contrast, dense and\nsparse-retrieval models are computationally more efficient but often\nunderperform other approaches, highlighting the considerable room for\nimprovement in their generalization capabilities. We hope this framework allows\nus to better evaluate and understand existing retrieval systems, and\ncontributes to accelerating progress towards better robust and generalizable\nsystems in the future. BEIR is publicly available at\nhttps://github.com/UKPLab/beir.", "published": "2021-04-17 23:29:55", "link": "http://arxiv.org/abs/2104.08663v4", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Transductive Learning for Abstractive News Summarization", "abstract": "Pre-trained and fine-tuned news summarizers are expected to generalize to\nnews articles unseen in the fine-tuning (training) phase. However, these\narticles often contain specifics, such as new events and people, a summarizer\ncould not learn about in training. This applies to scenarios such as a news\npublisher training a summarizer on dated news and summarizing incoming recent\nnews. In this work, we explore the first application of transductive learning\nto summarization where we further fine-tune models on test set inputs.\nSpecifically, we construct pseudo summaries from salient article sentences and\ninput randomly masked articles. Moreover, this approach is also beneficial in\nthe fine-tuning phase, where we jointly predict extractive pseudo references\nand abstractive gold summaries in the training set. We show that our approach\nyields state-of-the-art results on CNN/DM and NYT datasets, improving ROUGE-L\nby 1.05 and 0.74, respectively. Importantly, our approach does not require any\nchanges of the original architecture. Moreover, we show the benefits of\ntransduction from dated to more recent CNN news. Finally, through human and\nautomatic evaluation, we demonstrate improvements in summary abstractiveness\nand coherence.", "published": "2021-04-17 17:33:12", "link": "http://arxiv.org/abs/2104.09500v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cetacean Translation Initiative: a roadmap to deciphering the\n  communication of sperm whales", "abstract": "The past decade has witnessed a groundbreaking rise of machine learning for\nhuman language analysis, with current methods capable of automatically\naccurately recovering various aspects of syntax and semantics - including\nsentence structure and grounded word meaning - from large data collections.\nRecent research showed the promise of such tools for analyzing acoustic\ncommunication in nonhuman species. We posit that machine learning will be the\ncornerstone of future collection, processing, and analysis of multimodal\nstreams of data in animal communication studies, including bioacoustic,\nbehavioral, biological, and environmental data. Cetaceans are unique non-human\nmodel species as they possess sophisticated acoustic communications, but\nutilize a very different encoding system that evolved in an aquatic rather than\nterrestrial medium. Sperm whales, in particular, with their highly-developed\nneuroanatomical features, cognitive abilities, social structures, and discrete\nclick-based encoding make for an excellent starting point for advanced machine\nlearning tools that can be applied to other animals in the future. This paper\ndetails a roadmap toward this goal based on currently existing technology and\nmultidisciplinary scientific community effort. We outline the key elements\nrequired for the collection and processing of massive bioacoustic data of sperm\nwhales, detecting their basic communication units and language-like\nhigher-level structures, and validating these models through interactive\nplayback experiments. The technological capabilities developed by such an\nundertaking are likely to yield cross-applications and advancements in broader\ncommunities investigating non-human communication and animal behavioral\nresearch.", "published": "2021-04-17 18:39:22", "link": "http://arxiv.org/abs/2104.08614v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "cs.RO", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Metric Optimization using Generative Adversarial Networks for\n  Near-End Speech Intelligibility Enhancement", "abstract": "The intelligibility of speech severely degrades in the presence of\nenvironmental noise and reverberation. In this paper, we propose a novel deep\nlearning based system for modifying the speech signal to increase its\nintelligibility under the equal-power constraint, i.e., signal power before and\nafter modification must be the same. To achieve this, we use generative\nadversarial networks (GANs) to obtain time-frequency dependent amplification\nfactors, which are then applied to the input raw speech to reallocate the\nspeech energy. Instead of optimizing only a single, simple metric, we train a\ndeep neural network (DNN) model to simultaneously optimize multiple advanced\nspeech metrics, including both intelligibility- and quality-related ones, which\nresults in notable improvements in performance and robustness. Our system can\nnot only work in non-realtime mode for offline audio playback but also support\npractical real-time speech applications. Experimental results using both\nobjective measurements and subjective listening tests indicate that the\nproposed system significantly outperforms state-ofthe-art baseline systems\nunder various noisy and reverberant listening conditions.", "published": "2021-04-17 09:48:27", "link": "http://arxiv.org/abs/2104.08499v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Comparison of remote experiments using crowdsourcing and laboratory\n  experiments on speech intelligibility", "abstract": "Many subjective experiments have been performed to develop objective speech\nintelligibility measures, but the novel coronavirus outbreak has made it very\ndifficult to conduct experiments in a laboratory. One solution is to perform\nremote testing using crowdsourcing; however, because we cannot control the\nlistening conditions, it is unclear whether the results are entirely reliable.\nIn this study, we compared speech intelligibility scores obtained in remote and\nlaboratory experiments. The results showed that the mean and standard deviation\n(SD) of the remote experiments' speech reception threshold (SRT) were higher\nthan those of the laboratory experiments. However, the variance in the SRTs\nacross the speech-enhancement conditions revealed similarities, implying that\nremote testing results may be as useful as laboratory experiments to develop an\nobjective measure. We also show that the practice session scores correlate with\nthe SRT values. This is a priori information before performing the main tests\nand would be useful for data screening to reduce the variability of the SRT\ndistribution.", "published": "2021-04-17 02:00:15", "link": "http://arxiv.org/abs/2104.10001v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MIMO Self-attentive RNN Beamformer for Multi-speaker Speech Separation", "abstract": "Recently, our proposed recurrent neural network (RNN) based all deep learning\nminimum variance distortionless response (ADL-MVDR) beamformer method yielded\nsuperior performance over the conventional MVDR by replacing the matrix\ninversion and eigenvalue decomposition with two recurrent neural networks. In\nthis work, we present a self-attentive RNN beamformer to further improve our\nprevious RNN-based beamformer by leveraging on the powerful modeling capability\nof self-attention. Temporal-spatial self-attention module is proposed to better\nlearn the beamforming weights from the speech and noise spatial covariance\nmatrices. The temporal self-attention module could help RNN to learn global\nstatistics of covariance matrices. The spatial self-attention module is\ndesigned to attend on the cross-channel correlation in the covariance matrices.\nFurthermore, a multi-channel input with multi-speaker directional features and\nmulti-speaker speech separation outputs (MIMO) model is developed to improve\nthe inference efficiency. The evaluations demonstrate that our proposed MIMO\nself-attentive RNN beamformer improves both the automatic speech recognition\n(ASR) accuracy and the perceptual estimation of speech quality (PESQ) against\nprior arts.", "published": "2021-04-17 05:02:04", "link": "http://arxiv.org/abs/2104.08450v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring Deep Learning for Joint Audio-Visual Lip Biometrics", "abstract": "Audio-visual (AV) lip biometrics is a promising authentication technique that\nleverages the benefits of both the audio and visual modalities in speech\ncommunication. Previous works have demonstrated the usefulness of AV lip\nbiometrics. However, the lack of a sizeable AV database hinders the exploration\nof deep-learning-based audio-visual lip biometrics. To address this problem, we\ncompile a moderate-size database using existing public databases. Meanwhile, we\nestablish the DeepLip AV lip biometrics system realized with a convolutional\nneural network (CNN) based video module, a time-delay neural network (TDNN)\nbased audio module, and a multimodal fusion module. Our experiments show that\nDeepLip outperforms traditional speaker recognition models in context modeling\nand achieves over 50% relative improvements compared with our best single\nmodality baseline, with an equal error rate of 0.75% and 1.11% on the test\ndatasets, respectively.", "published": "2021-04-17 10:51:55", "link": "http://arxiv.org/abs/2104.08510v1", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Uncovering audio patterns in music with Nonnegative Tucker Decomposition\n  for structural segmentation", "abstract": "Recent work has proposed the use of tensor decomposition to model repetitions\nand to separate tracks in loop-based electronic music. The present work\ninvestigates further on the ability of Nonnegative Tucker Decompositon (NTD) to\nuncover musical patterns and structure in pop songs in their audio form.\nExploiting the fact that NTD tends to express the content of bars as linear\ncombinations of a few patterns, we illustrate the ability of the decomposition\nto capture and single out repeated motifs in the corresponding compressed\nspace, which can be interpreted from a musical viewpoint. The resulting\nfeatures also turn out to be efficient for structural segmentation, leading to\nexperimental results on the RWC Pop data set which are potentially challenging\nstate-of-the-art approaches that rely on extensive example-based learning\nschemes.", "published": "2021-04-17 15:48:24", "link": "http://arxiv.org/abs/2104.08580v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "H.5.5"], "primary_category": "cs.SD"}
