{"title": "Deep Learning Embeddings for Discontinuous Linguistic Units", "abstract": "Deep learning embeddings have been successfully used for many natural\nlanguage processing problems. Embeddings are mostly computed for word forms\nalthough a number of recent papers have extended this to other linguistic units\nlike morphemes and phrases. In this paper, we argue that learning embeddings\nfor discontinuous linguistic units should also be considered. In an\nexperimental evaluation on coreference resolution, we show that such embeddings\nperform better than word form embeddings.", "published": "2013-12-18 13:34:16", "link": "http://arxiv.org/abs/1312.5129v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Semantic Script Knowledge with Event Embeddings", "abstract": "Induction of common sense knowledge about prototypical sequences of events\nhas recently received much attention. Instead of inducing this knowledge in the\nform of graphs, as in much of the previous work, in our method, distributed\nrepresentations of event realizations are computed based on distributed\nrepresentations of predicates and their arguments, and then these\nrepresentations are used to predict prototypical event orderings. The\nparameters of the compositional process for computing the event representations\nand the ranking component of the model are jointly estimated from texts. We\nshow that this approach results in a substantial boost in ordering performance\nwith respect to previous methods.", "published": "2013-12-18 16:13:08", "link": "http://arxiv.org/abs/1312.5198v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML", "I.2.6; I.2.7"], "primary_category": "cs.LG"}
