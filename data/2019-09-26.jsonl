{"title": "SIM: A Slot-Independent Neural Model for Dialogue State Tracking", "abstract": "Dialogue state tracking is an important component in task-oriented dialogue\nsystems to identify users' goals and requests as a dialogue proceeds. However,\nas most previous models are dependent on dialogue slots, the model complexity\nsoars when the number of slots increases. In this paper, we put forward a\nslot-independent neural model (SIM) to track dialogue states while keeping the\nmodel complexity invariant to the number of dialogue slots. The model utilizes\nattention mechanisms between user utterance and system actions. SIM achieves\nstate-of-the-art results on WoZ and DSTC2 tasks, with only 20% of the model\nsize of previous models.", "published": "2019-09-26 00:51:58", "link": "http://arxiv.org/abs/1909.11833v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large-scale Pretraining for Neural Machine Translation with Tens of\n  Billions of Sentence Pairs", "abstract": "In this paper, we investigate the problem of training neural machine\ntranslation (NMT) systems with a dataset of more than 40 billion bilingual\nsentence pairs, which is larger than the largest dataset to date by orders of\nmagnitude. Unprecedented challenges emerge in this situation compared to\nprevious NMT work, including severe noise in the data and prohibitively long\ntraining time. We propose practical solutions to handle these issues and\ndemonstrate that large-scale pretraining significantly improves NMT\nperformance. We are able to push the BLEU score of WMT17 Chinese-English\ndataset to 32.3, with a significant performance boost of +3.2 over existing\nstate-of-the-art results.", "published": "2019-09-26 03:06:47", "link": "http://arxiv.org/abs/1909.11861v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aspect and Opinion Term Extraction for Hotel Reviews using Transfer\n  Learning and Auxiliary Labels", "abstract": "Aspect and opinion term extraction is a critical step in Aspect-Based\nSentiment Analysis (ABSA). Our study focuses on evaluating transfer learning\nusing pre-trained BERT (Devlin et al., 2018) to classify tokens from hotel\nreviews in bahasa Indonesia. The primary challenge is the language informality\nof the review texts. By utilizing transfer learning from a multilingual model,\nwe achieved up to 2% difference on token level F1-score compared to the\nstate-of-the-art Bi-LSTM model with fewer training epochs (3 vs. 200 epochs).\nThe fine-tuned model clearly outperforms the Bi-LSTM model on the entity level.\nFurthermore, we propose a method to include CRF with auxiliary labels as an\noutput layer for the BERT-based models. The CRF addition further improves the\nF1-score for both token and entity level.", "published": "2019-09-26 04:17:48", "link": "http://arxiv.org/abs/1909.11879v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-tune Bert for DocRED with Two-step Process", "abstract": "Modelling relations between multiple entities has attracted increasing\nattention recently, and a new dataset called DocRED has been collected in order\nto accelerate the research on the document-level relation extraction. Current\nbaselines for this task uses BiLSTM to encode the whole document and are\ntrained from scratch. We argue that such simple baselines are not strong enough\nto model to complex interaction between entities. In this paper, we further\napply a pre-trained language model (BERT) to provide a stronger baseline for\nthis task. We also find that solving this task in phases can further improve\nthe performance. The first step is to predict whether or not two entities have\na relation, the second step is to predict the specific relation.", "published": "2019-09-26 05:25:10", "link": "http://arxiv.org/abs/1909.11898v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Selecting Artificially-Generated Sentences for Fine-Tuning Neural\n  Machine Translation", "abstract": "Neural Machine Translation (NMT) models tend to achieve best performance when\nlarger sets of parallel sentences are provided for training. For this reason,\naugmenting the training set with artificially-generated sentence pairs can\nboost performance.\n  Nonetheless, the performance can also be improved with a small number of\nsentences if they are in the same domain as the test set. Accordingly, we want\nto explore the use of artificially-generated sentences along with\ndata-selection algorithms to improve German-to-English NMT models trained\nsolely with authentic data.\n  In this work, we show how artificially-generated sentences can be more\nbeneficial than authentic pairs, and demonstrate their advantages when used in\ncombination with data-selection algorithms.", "published": "2019-09-26 10:37:39", "link": "http://arxiv.org/abs/1909.12016v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Fine-grained Entity Typing with Entity Linking", "abstract": "Fine-grained entity typing is a challenging problem since it usually involves\na relatively large tag set and may require to understand the context of the\nentity mention. In this paper, we use entity linking to help with the\nfine-grained entity type classification process. We propose a deep neural model\nthat makes predictions based on both the context and the information obtained\nfrom entity linking results. Experimental results on two commonly used datasets\ndemonstrates the effectiveness of our approach. On both datasets, it achieves\nmore than 5\\% absolute strict accuracy improvement over the state of the art.", "published": "2019-09-26 13:20:10", "link": "http://arxiv.org/abs/1909.12079v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MinWikiSplit: A Sentence Splitting Corpus with Minimal Propositions", "abstract": "We compiled a new sentence splitting corpus that is composed of 203K pairs of\naligned complex source and simplified target sentences. Contrary to previously\nproposed text simplification corpora, which contain only a small number of\nsplit examples, we present a dataset where each input sentence is broken down\ninto a set of minimal propositions, i.e. a sequence of sound, self-contained\nutterances with each of them presenting a minimal semantic unit that cannot be\nfurther decomposed into meaningful propositions. This corpus is useful for\ndeveloping sentence splitting approaches that learn how to transform sentences\nwith a complex linguistic structure into a fine-grained representation of short\nsentences that present a simple and more regular structure which is easier to\nprocess for downstream applications and thus facilitates and improves their\nperformance.", "published": "2019-09-26 14:13:39", "link": "http://arxiv.org/abs/1909.12131v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Change and Emerging Tropes In a Large Corpus of New High German\n  Poetry", "abstract": "Due to its semantic succinctness and novelty of expression, poetry is a great\ntest bed for semantic change analysis. However, so far there is a scarcity of\nlarge diachronic corpora. Here, we provide a large corpus of German poetry\nwhich consists of about 75k poems with more than 11 million tokens, with poems\nranging from the 16th to early 20th century. We then track semantic change in\nthis corpus by investigating the rise of tropes (`love is magic') over time and\ndetecting change points of meaning, which we find to occur particularly within\nthe German Romantic period. Additionally, through self-similarity, we\nreconstruct literary periods and find evidence that the law of linear semantic\nchange also applies to poetry.", "published": "2019-09-26 14:18:09", "link": "http://arxiv.org/abs/1909.12136v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DisSim: A Discourse-Aware Syntactic Text Simplification Frameworkfor\n  English and German", "abstract": "We introduce DisSim, a discourse-aware sentence splitting framework for\nEnglish and German whose goal is to transform syntactically complex sentences\ninto an intermediate representation that presents a simple and more regular\nstructure which is easier to process for downstream semantic applications. For\nthis purpose, we turn input sentences into a two-layered semantic hierarchy in\nthe form of core facts and accompanying contexts, while identifying the\nrhetorical relations that hold between them. In that way, we preserve the\ncoherence structure of the input and, hence, its interpretability for\ndownstream tasks.", "published": "2019-09-26 14:21:33", "link": "http://arxiv.org/abs/1909.12140v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DARTS: Dialectal Arabic Transcription System", "abstract": "We present the speech to text transcription system, called DARTS, for low\nresource Egyptian Arabic dialect. We analyze the following; transfer learning\nfrom high resource broadcast domain to low-resource dialectal domain and\nsemi-supervised learning where we use in-domain unlabeled audio data collected\nfrom YouTube. Key features of our system are: A deep neural network acoustic\nmodel that consists of a front end Convolutional Neural Network (CNN) followed\nby several layers of Time Delayed Neural Network (TDNN) and Long-Short Term\nMemory Recurrent Neural Network (LSTM); sequence discriminative training of the\nacoustic model; n-gram and recurrent neural network language model for decoding\nand N-best list rescoring. We show that a simple transfer learning method can\nachieve good results. The results are further improved by using unlabeled data\nfrom YouTube in a semi-supervised setup. Various systems are combined to give\nthe final system that achieves the lowest word error on on the community\nstandard Egyptian-Arabic speech dataset (MGB-3).", "published": "2019-09-26 14:46:58", "link": "http://arxiv.org/abs/1909.12163v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Importance of Subword Information for Morphological Tasks in\n  Truly Low-Resource Languages", "abstract": "Recent work has validated the importance of subword information for word\nrepresentation learning. Since subwords increase parameter sharing ability in\nneural models, their value should be even more pronounced in low-data regimes.\nIn this work, we therefore provide a comprehensive analysis focused on the\nusefulness of subwords for word representation learning in truly low-resource\nscenarios and for three representative morphological tasks: fine-grained entity\ntyping, morphological tagging, and named entity recognition. We conduct a\nsystematic study that spans several dimensions of comparison: 1) type of data\nscarcity which can stem from the lack of task-specific training data, or even\nfrom the lack of unannotated data required to train word embeddings, or both;\n2) language type by working with a sample of 16 typologically diverse languages\nincluding some truly low-resource ones (e.g. Rusyn, Buryat, and Zulu); 3) the\nchoice of the subword-informed word representation method. Our main results\nshow that subword-informed models are universally useful across all language\ntypes, with large gains over subword-agnostic embeddings. They also suggest\nthat the effective use of subwords largely depends on the language (type) and\nthe task at hand, as well as on the amount of available data for training the\nembeddings and task-based models, where having sufficient in-task data is a\nmore critical requirement.", "published": "2019-09-26 20:26:51", "link": "http://arxiv.org/abs/1909.12375v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Monotonic Multihead Attention", "abstract": "Simultaneous machine translation models start generating a target sequence\nbefore they have encoded or read the source sequence. Recent approaches for\nthis task either apply a fixed policy on a state-of-the art Transformer model,\nor a learnable monotonic attention on a weaker recurrent neural network-based\nstructure. In this paper, we propose a new attention mechanism, Monotonic\nMultihead Attention (MMA), which extends the monotonic attention mechanism to\nmultihead attention. We also introduce two novel and interpretable approaches\nfor latency control that are specifically designed for multiple attentions\nheads. We apply MMA to the simultaneous machine translation task and\ndemonstrate better latency-quality tradeoffs compared to MILk, the previous\nstate-of-the-art approach. We also analyze how the latency controls affect the\nattention span and we motivate the introduction of our model by analyzing the\neffect of the number of decoder layers and heads on quality and latency.", "published": "2019-09-26 21:32:50", "link": "http://arxiv.org/abs/1909.12406v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Biomedical relation extraction with pre-trained language representations\n  and minimal task-specific architecture", "abstract": "This paper presents our participation in the AGAC Track from the 2019 BioNLP\nOpen Shared Tasks. We provide a solution for Task 3, which aims to extract\n\"gene - function change - disease\" triples, where \"gene\" and \"disease\" are\nmentions of particular genes and diseases respectively and \"function change\" is\none of four pre-defined relationship types. Our system extends BERT (Devlin et\nal., 2018), a state-of-the-art language model, which learns contextual language\nrepresentations from a large unlabelled corpus and whose parameters can be\nfine-tuned to solve specific tasks with minimal additional architecture. We\nencode the pair of mentions and their textual context as two consecutive\nsequences in BERT, separated by a special symbol. We then use a single linear\nlayer to classify their relationship into five classes (four pre-defined, as\nwell as 'no relation'). Despite considerable class imbalance, our system\nsignificantly outperforms a random baseline while relying on an extremely\nsimple setup with no specially engineered features.", "published": "2019-09-26 21:58:26", "link": "http://arxiv.org/abs/1909.12411v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Pre-Trained Multilingual Models with Vocabulary Expansion", "abstract": "Recently, pre-trained language models have achieved remarkable success in a\nbroad range of natural language processing tasks. However, in multilingual\nsetting, it is extremely resource-consuming to pre-train a deep language model\nover large-scale corpora for each language. Instead of exhaustively\npre-training monolingual language models independently, an alternative solution\nis to pre-train a powerful multilingual deep language model over large-scale\ncorpora in hundreds of languages. However, the vocabulary size for each\nlanguage in such a model is relatively small, especially for low-resource\nlanguages. This limitation inevitably hinders the performance of these\nmultilingual models on tasks such as sequence labeling, wherein in-depth\ntoken-level or sentence-level understanding is essential.\n  In this paper, inspired by previous methods designed for monolingual\nsettings, we investigate two approaches (i.e., joint mapping and mixture\nmapping) based on a pre-trained multilingual model BERT for addressing the\nout-of-vocabulary (OOV) problem on a variety of tasks, including part-of-speech\ntagging, named entity recognition, machine translation quality estimation, and\nmachine reading comprehension. Experimental results show that using mixture\nmapping is more promising. To the best of our knowledge, this is the first work\nthat attempts to address and discuss the OOV issue in multilingual settings.", "published": "2019-09-26 23:56:07", "link": "http://arxiv.org/abs/1909.12440v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decomposing Textual Information For Style Transfer", "abstract": "This paper focuses on latent representations that could effectively decompose\ndifferent aspects of textual information. Using a framework of style transfer\nfor texts, we propose several empirical methods to assess information\ndecomposition quality. We validate these methods with several state-of-the-art\ntextual style transfer methods. Higher quality of information decomposition\ncorresponds to higher performance in terms of bilingual evaluation understudy\n(BLEU) between output and human-written reformulations.", "published": "2019-09-26 11:34:04", "link": "http://arxiv.org/abs/1909.12928v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-train, Interact, Fine-tune: A Novel Interaction Representation for\n  Text Classification", "abstract": "Text representation can aid machines in understanding text. Previous work on\ntext representation often focuses on the so-called forward implication, i.e.,\npreceding words are taken as the context of later words for creating\nrepresentations, thus ignoring the fact that the semantics of a text segment is\na product of the mutual implication of words in the text: later words\ncontribute to the meaning of preceding words. We introduce the concept of\ninteraction and propose a two-perspective interaction representation, that\nencapsulates a local and a global interaction representation. Here, a local\ninteraction representation is one that interacts among words with\nparent-children relationships on the syntactic trees and a global interaction\ninterpretation is one that interacts among all the words in a sentence. We\ncombine the two interaction representations to develop a Hybrid Interaction\nRepresentation (HIR).\n  Inspired by existing feature-based and fine-tuning-based pretrain-finetuning\napproaches to language models, we integrate the advantages of feature-based and\nfine-tuning-based methods to propose the Pre-train, Interact, Fine-tune (PIF)\narchitecture.\n  We evaluate our proposed models on five widely-used datasets for text\nclassification tasks. Our ensemble method, outperforms state-of-the-art\nbaselines with improvements ranging from 2.03% to 3.15% in terms of error rate.\nIn addition, we find that, the improvements of PIF against most\nstate-of-the-art methods is not affected by increasing of the length of the\ntext.", "published": "2019-09-26 00:14:22", "link": "http://arxiv.org/abs/1909.11824v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language\n  Representations", "abstract": "Increasing model size when pretraining natural language representations often\nresults in improved performance on downstream tasks. However, at some point\nfurther model increases become harder due to GPU/TPU memory limitations and\nlonger training times. To address these problems, we present two\nparameter-reduction techniques to lower memory consumption and increase the\ntraining speed of BERT. Comprehensive empirical evidence shows that our\nproposed methods lead to models that scale much better compared to the original\nBERT. We also use a self-supervised loss that focuses on modeling\ninter-sentence coherence, and show it consistently helps downstream tasks with\nmulti-sentence inputs. As a result, our best model establishes new\nstate-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having\nfewer parameters compared to BERT-large. The code and the pretrained models are\navailable at https://github.com/google-research/ALBERT.", "published": "2019-09-26 07:06:13", "link": "http://arxiv.org/abs/1909.11942v6", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Low-Resource Response Generation with Template Prior", "abstract": "We study open domain response generation with limited message-response pairs.\nThe problem exists in real-world applications but is less explored by the\nexisting work. Since the paired data now is no longer enough to train a neural\ngeneration model, we consider leveraging the large scale of unpaired data that\nare much easier to obtain, and propose response generation with both paired and\nunpaired data. The generation model is defined by an encoder-decoder\narchitecture with templates as prior, where the templates are estimated from\nthe unpaired data as a neural hidden semi-markov model. By this means, response\ngeneration learned from the small paired data can be aided by the semantic and\nsyntactic knowledge in the large unpaired data. To balance the effect of the\nprior and the input message to response generation, we propose learning the\nwhole generation model with an adversarial approach. Empirical studies on\nquestion response generation and sentiment response generation indicate that\nwhen only a few pairs are available, our model can significantly outperform\nseveral state-of-the-art response generation models in terms of both automatic\nand human evaluation.", "published": "2019-09-26 08:19:34", "link": "http://arxiv.org/abs/1909.11968v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Investigation into the Effectiveness of Enhancement in ASR Training\n  and Test for CHiME-5 Dinner Party Transcription", "abstract": "Despite the strong modeling power of neural network acoustic models, speech\nenhancement has been shown to deliver additional word error rate improvements\nif multi-channel data is available. However, there has been a longstanding\ndebate whether enhancement should also be carried out on the ASR training data.\nIn an extensive experimental evaluation on the acoustically very challenging\nCHiME-5 dinner party data we show that: (i) cleaning up the training data can\nlead to substantial error rate reductions, and (ii) enhancement in training is\nadvisable as long as enhancement in test is at least as strong as in training.\nThis approach stands in contrast and delivers larger gains than the common\nstrategy reported in the literature to augment the training database with\nadditional artificially degraded speech. Together with an acoustic model\ntopology consisting of initial CNN layers followed by factorized TDNN layers we\nachieve with 41.6% and 43.2% WER on the DEV and EVAL test sets, respectively, a\nnew single-system state-of-the-art result on the CHiME-5 data. This is a 8%\nrelative improvement compared to the best word error rate published so far for\na speech recognizer without system combination.", "published": "2019-09-26 15:59:37", "link": "http://arxiv.org/abs/1909.12208v1", "categories": ["cs.CL", "eess.AS", "68T10"], "primary_category": "cs.CL"}
{"title": "Rethinking Text Attribute Transfer: A Lexical Analysis", "abstract": "Text attribute transfer is modifying certain linguistic attributes (e.g.\nsentiment, style, authorship, etc.) of a sentence and transforming them from\none type to another. In this paper, we aim to analyze and interpret what is\nchanged during the transfer process. We start from the observation that in many\nexisting models and datasets, certain words within a sentence play important\nroles in determining the sentence attribute class. These words are referred to\nas \\textit{the Pivot Words}. Based on these pivot words, we propose a lexical\nanalysis framework, \\textit{the Pivot Analysis}, to quantitatively analyze the\neffects of these words in text attribute classification and transfer. We apply\nthis framework to existing datasets and models and show that: (1) the pivot\nwords are strong features for the classification of sentence attributes; (2) to\nchange the attribute of a sentence, many datasets only requires to change\ncertain pivot words; (3) consequently, many transfer models only perform the\nlexical-level modification, while leaving higher-level sentence structures\nunchanged. Our work provides an in-depth understanding of linguistic attribute\ntransfer and further identifies the future requirements and challenges of this\ntask\\footnote{Our code can be found at\nhttps://github.com/FranxYao/pivot_analysis}.", "published": "2019-09-26 18:59:53", "link": "http://arxiv.org/abs/1909.12335v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Coin_flipper at eHealth-KD Challenge 2019: Voting LSTMs for Key Phrases\n  and Semantic Relation Identification Applied to Spanish eHealth Texts", "abstract": "This paper describes our approach presented for the eHealth-KD 2019\nchallenge. Our participation was aimed at testing how far we could go using\ngeneric tools for Text-Processing but, at the same time, using common\noptimization techniques in the field of Data Mining. The architecture proposed\nfor both tasks of the challenge is a standard stacked 2-layer bi-LSTM. The main\nparticularities of our approach are: (a) The use of a surrogate function of F1\nas loss function to close the gap between the minimization function and the\nevaluation metric, and (b) The generation of an ensemble of models for\ngenerating predictions by majority vote. Our system ranked second with an F1\nscore of 62.18% in the main task by a narrow margin with the winner that scored\n63.94%.", "published": "2019-09-26 19:03:54", "link": "http://arxiv.org/abs/1909.12339v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving RNN Transducer Modeling for End-to-End Speech Recognition", "abstract": "In the last few years, an emerging trend in automatic speech recognition\nresearch is the study of end-to-end (E2E) systems. Connectionist Temporal\nClassification (CTC), Attention Encoder-Decoder (AED), and RNN Transducer\n(RNN-T) are the most popular three methods. Among these three methods, RNN-T\nhas the advantages to do online streaming which is challenging to AED and it\ndoesn't have CTC's frame-independence assumption. In this paper, we improve the\nRNN-T training in two aspects. First, we optimize the training algorithm of\nRNN-T to reduce the memory consumption so that we can have larger training\nminibatch for faster training speed. Second, we propose better model structures\nso that we obtain RNN-T models with the very good accuracy but small footprint.\nTrained with 30 thousand hours anonymized and transcribed Microsoft production\ndata, the best RNN-T model with even smaller model size (216 Megabytes)\nachieves up-to 11.8% relative word error rate (WER) reduction from the baseline\nRNN-T model. This best RNN-T model is significantly better than the device\nhybrid model with similar size by achieving up-to 15.0% relative WER reduction,\nand obtains similar WERs as the server hybrid model of 5120 Megabytes in size.", "published": "2019-09-26 22:09:09", "link": "http://arxiv.org/abs/1909.12415v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Read, Attend and Comment: A Deep Architecture for Automatic News Comment\n  Generation", "abstract": "Automatic news comment generation is a new testbed for techniques of natural\nlanguage generation. In this paper, we propose a \"read-attend-comment\"\nprocedure for news comment generation and formalize the procedure with a\nreading network and a generation network. The reading network comprehends a\nnews article and distills some important points from it, then the generation\nnetwork creates a comment by attending to the extracted discrete points and the\nnews title. We optimize the model in an end-to-end manner by maximizing a\nvariational lower bound of the true objective using the back-propagation\nalgorithm. Experimental results on two datasets indicate that our model can\nsignificantly outperform existing methods in terms of both automatic evaluation\nand human judgment.", "published": "2019-09-26 08:34:05", "link": "http://arxiv.org/abs/1909.11974v3", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Spoken Conversational Search for General Knowledge", "abstract": "We present a spoken conversational question answering proof of concept that\nis able to answer questions about general knowledge from Wikidata. The dialogue\ncomponent does not only orchestrate various components but also solve\ncoreferences and ellipsis.", "published": "2019-09-26 08:46:02", "link": "http://arxiv.org/abs/1909.11980v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Towards a Metric for Automated Conversational Dialogue System Evaluation\n  and Improvement", "abstract": "We present \"AutoJudge\", an automated evaluation method for conversational\ndialogue systems. The method works by first generating dialogues based on\nself-talk, i.e. dialogue systems talking to itself. Then, it uses human ratings\non these dialogues to train an automated judgement model. Our experiments show\nthat AutoJudge correlates well with the human ratings and can be used to\nautomatically evaluate dialogue systems, even in deployed systems. In a second\npart, we attempt to apply AutoJudge to improve existing systems. This works\nwell for re-ranking a set of candidate utterances. However, our experiments\nshow that AutoJudge cannot be applied as reward for reinforcement learning,\nalthough the metric can distinguish good from bad dialogues. We discuss\npotential reasons, but state here already that this is still an open question\nfor further research.", "published": "2019-09-26 12:55:14", "link": "http://arxiv.org/abs/1909.12066v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "GECOR: An End-to-End Generative Ellipsis and Co-reference Resolution\n  Model for Task-Oriented Dialogue", "abstract": "Ellipsis and co-reference are common and ubiquitous especially in multi-turn\ndialogues. In this paper, we treat the resolution of ellipsis and co-reference\nin dialogue as a problem of generating omitted or referred expressions from the\ndialogue context. We therefore propose a unified end-to-end Generative Ellipsis\nand CO-reference Resolution model (GECOR) in the context of dialogue. The model\ncan generate a new pragmatically complete user utterance by alternating the\ngeneration and copy mode for each user utterance. A multi-task learning\nframework is further proposed to integrate the GECOR into an end-to-end\ntask-oriented dialogue. In order to train both the GECOR and the multi-task\nlearning framework, we manually construct a new dataset on the basis of the\npublic dataset CamRest676 with both ellipsis and co-reference annotation. On\nthis dataset, intrinsic evaluations on the resolution of ellipsis and\nco-reference show that the GECOR model significantly outperforms the\nsequence-to-sequence (seq2seq) baseline model in terms of EM, BLEU and F1 while\nextrinsic evaluations on the downstream dialogue task demonstrate that our\nmulti-task learning framework with GECOR achieves a higher success rate of task\ncompletion than TSCP, a state-of-the-art end-to-end task-oriented dialogue\nmodel.", "published": "2019-09-26 13:34:26", "link": "http://arxiv.org/abs/1909.12086v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Attention Forcing for Sequence-to-sequence Model Training", "abstract": "Auto-regressive sequence-to-sequence models with attention mechanism have\nachieved state-of-the-art performance in many tasks such as machine translation\nand speech synthesis. These models can be difficult to train. The standard\napproach, teacher forcing, guides a model with reference output history during\ntraining. The problem is that the model is unlikely to recover from its\nmistakes during inference, where the reference output is replaced by generated\noutput. Several approaches deal with this problem, largely by guiding the model\nwith generated output history. To make training stable, these approaches often\nrequire a heuristic schedule or an auxiliary classifier. This paper introduces\nattention forcing, which guides the model with generated output history and\nreference attention. This approach can train the model to recover from its\nmistakes, in a stable fashion, without the need for a schedule or a classifier.\nIn addition, it allows the model to generate output sequences aligned with the\nreferences, which can be important for cascaded systems like many speech\nsynthesis systems. Experiments on speech synthesis show that attention forcing\nyields significant performance gain. Experiments on machine translation show\nthat for tasks where various re-orderings of the output are valid, guiding the\nmodel with generated output history is challenging, while guiding the model\nwith reference attention is beneficial.", "published": "2019-09-26 17:52:15", "link": "http://arxiv.org/abs/1909.12289v2", "categories": ["cs.LG", "cs.CL", "eess.AS", "stat.ML", "I.2"], "primary_category": "cs.LG"}
{"title": "Expert2Coder: Capturing Divergent Brain Regions Using Mixture of\n  Regression Experts", "abstract": "fMRI semantic category understanding using linguistic encoding models\nattempts to learn a forward mapping that relates stimuli to the corresponding\nbrain activation. State-of-the-art encoding models use a single global model\n(linear or non-linear) to predict brain activation given the stimulus. However,\nthe critical assumption in these methods is that a priori different brain\nregions respond the same way to all the stimuli, that is, there is no\nmodularity or specialization assumed for any region. This goes against the\nmodularity theory, supported by many cognitive neuroscience investigations\nsuggesting that there are functionally specialized regions in the brain. In\nthis paper, we achieve this by clustering similar regions together and for\nevery cluster we learn a different linear regression model using a mixture of\nlinear experts model. The key idea here is that each linear expert captures the\nbehaviour of similar brain regions. Given a new stimulus, the utility of the\nproposed model is twofold (i) predicts the brain activation as a weighted\nlinear combination of the activations of multiple linear experts and (ii) to\nlearn multiple experts corresponding to different brain regions. We argue that\neach expert captures activity patterns related to a particular region of\ninterest (ROI) in the human brain. This study helps in understanding the brain\nregions that are activated together given different kinds of stimuli.\nImportantly, we suggest that the mixture of regression experts (MoRE) framework\nsuccessfully combines the two principles of organization of function in the\nbrain, namely that of specialization and integration. Experiments on fMRI data\nfrom paradigm 1 [1]where participants view linguistic stimuli show that the\nproposed MoRE model has better prediction accuracy compared to that of\nconventional models.", "published": "2019-09-26 17:59:33", "link": "http://arxiv.org/abs/1909.12299v2", "categories": ["cs.LG", "cs.CL", "q-bio.NC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Hierarchical Approach for Visual Storytelling Using Image Description", "abstract": "One of the primary challenges of visual storytelling is developing techniques\nthat can maintain the context of the story over long event sequences to\ngenerate human-like stories. In this paper, we propose a hierarchical deep\nlearning architecture based on encoder-decoder networks to address this\nproblem. To better help our network maintain this context while also generating\nlong and diverse sentences, we incorporate natural language image descriptions\nalong with the images themselves to generate each story sentence. We evaluate\nour system on the Visual Storytelling (VIST) dataset and show that our method\noutperforms state-of-the-art techniques on a suite of different automatic\nevaluation metrics. The empirical results from this evaluation demonstrate the\nnecessities of different components of our proposed architecture and shows the\neffectiveness of the architecture for visual storytelling.", "published": "2019-09-26 21:25:41", "link": "http://arxiv.org/abs/1909.12401v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CV"}
{"title": "Optimizing Speech Recognition For The Edge", "abstract": "While most deployed speech recognition systems today still run on servers, we\nare in the midst of a transition towards deployments on edge devices. This leap\nto the edge is powered by the progression from traditional speech recognition\npipelines to end-to-end (E2E) neural architectures, and the parallel\ndevelopment of more efficient neural network topologies and optimization\ntechniques. Thus, we are now able to create highly accurate speech recognizers\nthat are both small and fast enough to execute on typical mobile devices. In\nthis paper, we begin with a baseline RNN-Transducer architecture comprised of\nLong Short-Term Memory (LSTM) layers. We then experiment with a variety of more\ncomputationally efficient layer types, as well as apply optimization techniques\nlike neural connection pruning and parameter quantization to construct a small,\nhigh quality, on-device speech recognizer that is an order of magnitude smaller\nthan the baseline system without any optimizations.", "published": "2019-09-26 21:43:53", "link": "http://arxiv.org/abs/1909.12408v3", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Learning the Difference that Makes a Difference with\n  Counterfactually-Augmented Data", "abstract": "Despite alarm over the reliance of machine learning systems on so-called\nspurious patterns, the term lacks coherent meaning in standard statistical\nframeworks. However, the language of causality offers clarity: spurious\nassociations are due to confounding (e.g., a common cause), but not direct or\nindirect causal effects. In this paper, we focus on natural language\nprocessing, introducing methods and resources for training models less\nsensitive to spurious patterns. Given documents and their initial labels, we\ntask humans with revising each document so that it (i) accords with a\ncounterfactual target label; (ii) retains internal coherence; and (iii) avoids\nunnecessary changes. Interestingly, on sentiment analysis and natural language\ninference tasks, classifiers trained on original data fail on their\ncounterfactually-revised counterparts and vice versa. Classifiers trained on\ncombined datasets perform remarkably well, just shy of those specialized to\neither domain. While classifiers trained on either original or manipulated data\nalone are sensitive to spurious features (e.g., mentions of genre), models\ntrained on the combined data are less sensitive to this signal. Both datasets\nare publicly available.", "published": "2019-09-26 23:25:25", "link": "http://arxiv.org/abs/1909.12434v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Self-Adaptive Soft Voice Activity Detection using Deep Neural Networks\n  for Robust Speaker Verification", "abstract": "Voice activity detection (VAD), which classifies frames as speech or\nnon-speech, is an important module in many speech applications including\nspeaker verification. In this paper, we propose a novel method, called\nself-adaptive soft VAD, to incorporate a deep neural network (DNN)-based VAD\ninto a deep speaker embedding system. The proposed method is a combination of\nthe following two approaches. The first approach is soft VAD, which performs a\nsoft selection of frame-level features extracted from a speaker feature\nextractor. The frame-level features are weighted by their corresponding speech\nposteriors estimated from the DNN-based VAD, and then aggregated to generate a\nspeaker embedding. The second approach is self-adaptive VAD, which fine-tunes\nthe pre-trained VAD on the speaker verification data to reduce the domain\nmismatch. Here, we introduce two unsupervised domain adaptation (DA) schemes,\nnamely speech posterior-based DA (SP-DA) and joint learning-based DA (JL-DA).\nExperiments on a Korean speech database demonstrate that the verification\nperformance is improved significantly in real-world environments by using\nself-adaptive soft VAD.", "published": "2019-09-26 04:38:01", "link": "http://arxiv.org/abs/1909.11886v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Improving the Intelligibility of Electric and Acoustic Stimulation\n  Speech Using Fully Convolutional Networks Based Speech Enhancement", "abstract": "The combined electric and acoustic stimulation (EAS) has demonstrated better\nspeech recognition than conventional cochlear implant (CI) and yielded\nsatisfactory performance under quiet conditions. However, when noise signals\nare involved, both the electric signal and the acoustic signal may be\ndistorted, thereby resulting in poor recognition performance. To suppress noise\neffects, speech enhancement (SE) is a necessary unit in EAS devices. Recently,\na time-domain speech enhancement algorithm based on the fully convolutional\nneural networks (FCN) with a short-time objective intelligibility (STOI)-based\nobjective function (termed FCN(S) in short) has received increasing attention\ndue to its simple structure and effectiveness of restoring clean speech signals\nfrom noisy counterparts. With evidence showing the benefits of FCN(S) for\nnormal speech, this study sets out to assess its ability to improve the\nintelligibility of EAS simulated speech. Objective evaluations and listening\ntests were conducted to examine the performance of FCN(S) in improving the\nspeech intelligibility of normal and vocoded speech in noisy environments. The\nexperimental results show that, compared with the traditional minimum-mean\nsquare-error SE method and the deep denoising autoencoder SE method, FCN(S) can\nobtain better gain in the speech intelligibility for normal as well as vocoded\nspeech. This study, being the first to evaluate deep learning SE approaches for\nEAS, confirms that FCN(S) is an effective SE approach that may potentially be\nintegrated into an EAS processor to benefit users in noisy environments.", "published": "2019-09-26 05:55:38", "link": "http://arxiv.org/abs/1909.11912v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Study of Joint Effect on Denoising Techniques and Visual Cues to\n  Improve Speech Intelligibility in Cochlear Implant Simulation", "abstract": "Speech perception is key to verbal communication. For people with hearing\nloss, the capability to recognize speech is restricted, particularly in a noisy\nenvironment or the situations without visual cues, such as lip-reading\nunavailable via phone call. This study aimed to understand the improvement of\nvocoded speech intelligibility in cochlear implant (CI) simulation through two\npotential methods: Speech Enhancement (SE) and Audiovisual Integration. A fully\nconvolutional neural network (FCN) using an intelligibility-oriented objective\nfunction was recently proposed and proven to effectively facilitate the speech\nintelligibility as an advanced denoising SE approach. Furthermore, audiovisual\nintegration is reported to supply better speech comprehension compared to\naudio-only information. An experiment was designed to test speech\nintelligibility using tone-vocoded speech in CI simulation with a group of\nnormal-hearing listeners. Experimental results confirmed the effectiveness of\nthe FCN-based denoising SE and audiovisual integration on vocoded speech. Also,\nit positively recommended that these two methods could become a blended feature\nin a CI processor to improve the speech intelligibility for CI users under\nnoisy conditions.", "published": "2019-09-26 06:10:46", "link": "http://arxiv.org/abs/1909.11919v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multichannel Speech Enhancement by Raw Waveform-mapping using Fully\n  Convolutional Networks", "abstract": "In recent years, waveform-mapping-based speech enhancement (SE) methods have\ngarnered significant attention. These methods generally use a deep learning\nmodel to directly process and reconstruct speech waveforms. Because both the\ninput and output are in waveform format, the waveform-mapping-based SE methods\ncan overcome the distortion caused by imperfect phase estimation, which may be\nencountered in spectral-mapping-based SE systems. So far, most\nwaveform-mapping-based SE methods have focused on single-channel tasks. In this\npaper, we propose a novel fully convolutional network (FCN) with Sinc and\ndilated convolutional layers (termed SDFCN) for multichannel SE that operates\nin the time domain. We also propose an extended version of SDFCN, called the\nresidual SDFCN (termed rSDFCN). The proposed methods are evaluated on two\nmultichannel SE tasks, namely the dual-channel inner-ear microphones SE task\nand the distributed microphones SE task. The experimental results confirm the\noutstanding denoising capability of the proposed SE systems on both tasks and\nthe benefits of using the residual architecture on the overall SE performance.", "published": "2019-09-26 05:51:05", "link": "http://arxiv.org/abs/1909.11909v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Ephemeral instruments", "abstract": "This article questions the notion of ephemerality of digital musical\ninstruments (DMI). Longevity is generally regarded as a valuable quality that\ngood design criteria should help to achieve. However, the nature of the tools,\nof the performance conditions and of the music itself may lead to think of\nephemerality as an intrinsic modality of the existence of DMIs. In particular,\nthe conditions of contemporary musical production suggest that contextual\nadaptations of instrumental devices beyond the monolithic unity of classical\ninstruments should be considered. The first two parts of this article analyse\nvarious reasons to reassess the issue of longevity and ephemerality. The last\ntwo sections attempt to propose an articulation of these two aspects to inform\nboth the design of the DMI and their learning.", "published": "2019-09-26 09:02:34", "link": "http://arxiv.org/abs/1909.13775v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
