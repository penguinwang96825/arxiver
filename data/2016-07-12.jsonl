{"title": "Open-Vocabulary Semantic Parsing with both Distributional Statistics and\n  Formal Knowledge", "abstract": "Traditional semantic parsers map language onto compositional, executable\nqueries in a fixed schema. This mapping allows them to effectively leverage the\ninformation contained in large, formal knowledge bases (KBs, e.g., Freebase) to\nanswer questions, but it is also fundamentally limiting---these semantic\nparsers can only assign meaning to language that falls within the KB's\nmanually-produced schema. Recently proposed methods for open vocabulary\nsemantic parsing overcome this limitation by learning execution models for\narbitrary language, essentially using a text corpus as a kind of knowledge\nbase. However, all prior approaches to open vocabulary semantic parsing replace\na formal KB with textual information, making no use of the KB in their models.\nWe show how to combine the disparate representations used by these two\napproaches, presenting for the first time a semantic parser that (1) produces\ncompositional, executable representations of language, (2) can successfully\nleverage the information contained in both a formal KB and a large corpus, and\n(3) is not limited to the schema of the underlying KB. We demonstrate\nsignificantly improved performance over state-of-the-art baselines on an\nopen-domain natural language question answering task.", "published": "2016-07-12 23:13:26", "link": "http://arxiv.org/abs/1607.03542v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Separating Answers from Queries for Neural Reading Comprehension", "abstract": "We present a novel neural architecture for answering queries, designed to\noptimally leverage explicit support in the form of query-answer memories. Our\nmodel is able to refine and update a given query while separately accumulating\nevidence for predicting the answer. Its architecture reflects this separation\nwith dedicated embedding matrices and loosely connected information pathways\n(modules) for updating the query and accumulating evidence. This separation of\nresponsibilities effectively decouples the search for query related support and\nthe prediction of the answer. On recent benchmark datasets for reading\ncomprehension, our model achieves state-of-the-art results. A qualitative\nanalysis reveals that the model effectively accumulates weighted evidence from\nthe query and over multiple support retrieval cycles which results in a robust\nanswer prediction.", "published": "2016-07-12 11:43:15", "link": "http://arxiv.org/abs/1607.03316v3", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Recurrent Highway Networks", "abstract": "Many sequential processing tasks require complex nonlinear transition\nfunctions from one step to the next. However, recurrent neural networks with\n'deep' transition functions remain difficult to train, even when using Long\nShort-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of\nrecurrent networks based on Gersgorin's circle theorem that illuminates several\nmodeling and optimization issues and improves our understanding of the LSTM\ncell. Based on this analysis we propose Recurrent Highway Networks, which\nextend the LSTM architecture to allow step-to-step transition depths larger\nthan one. Several language modeling experiments demonstrate that the proposed\narchitecture results in powerful and efficient models. On the Penn Treebank\ncorpus, solely increasing the transition depth from 1 to 10 improves word-level\nperplexity from 90.6 to 65.4 using the same number of parameters. On the larger\nWikipedia datasets for character prediction (text8 and enwik8), RHNs outperform\nall previous results and achieve an entropy of 1.27 bits per character.", "published": "2016-07-12 19:36:50", "link": "http://arxiv.org/abs/1607.03474v5", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
