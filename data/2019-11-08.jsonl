{"title": "Why Do Masked Neural Language Models Still Need Common Sense Knowledge?", "abstract": "Currently, contextualized word representations are learned by intricate\nneural network models, such as masked neural language models (MNLMs). The new\nrepresentations significantly enhanced the performance in automated question\nanswering by reading paragraphs. However, identifying the detailed knowledge\ntrained in the MNLMs is difficult owing to numerous and intermingled\nparameters. This paper provides empirical but insightful analyses on the\npretrained MNLMs with respect to common sense knowledge. First, we propose a\ntest that measures what types of common sense knowledge do pretrained MNLMs\nunderstand. From the test, we observed that MNLMs partially understand various\ntypes of common sense knowledge but do not accurately understand the semantic\nmeaning of relations. In addition, based on the difficulty of the\nquestion-answering task problems, we observed that pretrained MLM-based models\nare still vulnerable to problems that require common sense knowledge. We also\nexperimentally demonstrated that we can elevate existing MNLM-based models by\ncombining knowledge from an external common sense repository.", "published": "2019-11-08 03:45:55", "link": "http://arxiv.org/abs/1911.03024v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Graph Embedding Methods for Natural Language Processing", "abstract": "Knowledge graphs are structured representations of facts in a graph, where\nnodes represent entities and edges represent relationships between them. Recent\nresearch has resulted in the development of several large KGs. However, all of\nthem tend to be sparse with very few facts per entity. In the first part of the\nthesis, we propose two solutions to alleviate this problem: (1) KG\nCanonicalization, i.e., identifying and merging duplicate entities in a KG, (2)\nRelation Extraction which involves automating the process of extracting\nsemantic relationships between entities from unstructured text. Traditional\nNeural Networks like CNNs and RNNs are constrained to handle Euclidean data.\nHowever, graphs in Natural Language Processing (NLP) are prominent. Recently,\nGraph Convolutional Networks (GCNs) have been proposed to address this\nshortcoming and have been successfully applied for several problems. In the\nsecond part of the thesis, we utilize GCNs for Document Timestamping problem\nand for learning word embeddings using dependency context of a word instead of\nsequential context. In this third part of the thesis, we address two\nlimitations of existing GCN models, i.e., (1) The standard neighborhood\naggregation scheme puts no constraints on the number of nodes that can\ninfluence the representation of a target node. This leads to a noisy\nrepresentation of hub-nodes which coves almost the entire graph in a few hops.\n(2) Most of the existing GCN models are limited to handle undirected graphs.\nHowever, a more general and pervasive class of graphs are relational graphs\nwhere each edge has a label and direction associated with it. Existing\napproaches to handle such graphs suffer from over-parameterization and are\nrestricted to learning representation of nodes only.", "published": "2019-11-08 04:31:59", "link": "http://arxiv.org/abs/1911.03042v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contrastive Multi-document Question Generation", "abstract": "Multi-document question generation focuses on generating a question that\ncovers the common aspect of multiple documents. Such a model is useful in\ngenerating clarifying options. However, a naive model trained only using the\ntargeted (\"positive\") document set may generate too generic questions that\ncover a larger scope than delineated by the document set. To address this\nchallenge, we introduce the contrastive learning strategy where given\n\"positive\" and \"negative\" sets of documents, we generate a question that is\nclosely related to the \"positive\" set but is far away from the \"negative\" set.\nThis setting allows generated questions to be more specific and related to the\ntarget document set. To generate such specific questions, we propose\nMulti-Source Coordinated Question Generator (MSCQG), a novel framework that\nincludes a supervised learning (SL) stage and a reinforcement learning (RL)\nstage. In the SL stage, a single-document question generator is trained. In the\nRL stage, a coordinator model is trained to find optimal attention weights to\nalign multiple single-document generators, by optimizing a reward designed to\npromote specificity of generated questions. We also develop an effective\nauxiliary objective, named Set-induced Contrastive Regularization (SCR) that\nimproves the coordinator's contrastive learning during the RL stage. We show\nthat our model significantly outperforms several strong baselines, as measured\nby automatic metrics and human evaluation. The source repository is publicly\navailable at \\url{www.github.com/woonsangcho/contrast_qgen}.", "published": "2019-11-08 04:45:55", "link": "http://arxiv.org/abs/1911.03047v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Should All Cross-Lingual Embeddings Speak English?", "abstract": "Most of recent work in cross-lingual word embeddings is severely\nAnglocentric. The vast majority of lexicon induction evaluation dictionaries\nare between English and another language, and the English embedding space is\nselected by default as the hub when learning in a multilingual setting. With\nthis work, however, we challenge these practices. First, we show that the\nchoice of hub language can significantly impact downstream lexicon induction\nperformance. Second, we both expand the current evaluation dictionary\ncollection to include all language pairs using triangulation, and also create\nnew dictionaries for under-represented languages. Evaluating established\nmethods over all these language pairs sheds light into their suitability and\npresents new challenges for the field. Finally, in our analysis we identify\ngeneral guidelines for strong cross-lingual embeddings baselines, based on more\nthan just Anglocentric experiments.", "published": "2019-11-08 05:29:57", "link": "http://arxiv.org/abs/1911.03058v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Comparison of Machine Learning Based Methods Used in\n  Bengali Question Classification", "abstract": "QA classification system maps questions asked by humans to an appropriate\nanswer category. A sound question classification (QC) system model is the\npre-requisite of a sound QA system. This work demonstrates phases of assembling\na QA type classification model. We present a comprehensive comparison\n(performance and computational complexity) among some machine learning based\napproaches used in QC for Bengali language.", "published": "2019-11-08 05:30:33", "link": "http://arxiv.org/abs/1911.03059v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Would Elsa Do? Freezing Layers During Transformer Fine-Tuning", "abstract": "Pretrained transformer-based language models have achieved state of the art\nacross countless tasks in natural language processing. These models are highly\nexpressive, comprising at least a hundred million parameters and a dozen\nlayers. Recent evidence suggests that only a few of the final layers need to be\nfine-tuned for high quality on downstream tasks. Naturally, a subsequent\nresearch question is, \"how many of the last layers do we need to fine-tune?\" In\nthis paper, we precisely answer this question. We examine two recent pretrained\nlanguage models, BERT and RoBERTa, across standard tasks in textual entailment,\nsemantic similarity, sentiment analysis, and linguistic acceptability. We vary\nthe number of final layers that are fine-tuned, then study the resulting change\nin task-specific effectiveness. We show that only a fourth of the final layers\nneed to be fine-tuned to achieve 90% of the original quality. Surprisingly, we\nalso find that fine-tuning all layers does not always help.", "published": "2019-11-08 07:05:20", "link": "http://arxiv.org/abs/1911.03090v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain Robustness in Neural Machine Translation", "abstract": "Translating text that diverges from the training domain is a key challenge\nfor machine translation. Domain robustness---the generalization of models to\nunseen test domains---is low for both statistical (SMT) and neural machine\ntranslation (NMT). In this paper, we study the performance of SMT and NMT\nmodels on out-of-domain test sets. We find that in unknown domains, SMT and NMT\nsuffer from very different problems: SMT systems are mostly adequate but not\nfluent, while NMT systems are mostly fluent, but not adequate. For NMT, we\nidentify such hallucinations (translations that are fluent but unrelated to the\nsource) as a key reason for low domain robustness. To mitigate this problem, we\nempirically compare methods that are reported to improve adequacy or in-domain\nrobustness in terms of their effectiveness at improving domain robustness. In\nexperiments on German to English OPUS data, and German to Romansh (a\nlow-resource setting) we find that several methods improve domain robustness.\nWhile those methods do lead to higher BLEU scores overall, they only slightly\nincrease the adequacy of translations compared to SMT.", "published": "2019-11-08 07:57:46", "link": "http://arxiv.org/abs/1911.03109v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pretrained Language Models for Document-Level Neural Machine Translation", "abstract": "Previous work on document-level NMT usually focuses on limited contexts\nbecause of degraded performance on larger contexts. In this paper, we\ninvestigate on using large contexts with three main contributions: (1)\nDifferent from previous work which pertrained models on large-scale\nsentence-level parallel corpora, we use pretrained language models,\nspecifically BERT, which are trained on monolingual documents; (2) We propose\ncontext manipulation methods to control the influence of large contexts, which\nlead to comparable results on systems using small and large contexts; (3) We\nintroduce a multi-task training for regularization to avoid models overfitting\nour training corpora, which further improves our systems together with a deeper\nencoder. Experiments are conducted on the widely used IWSLT data sets with\nthree language pairs, i.e., Chinese--English, French--English and\nSpanish--English. Results show that our systems are significantly better than\nthree previously reported document-level systems.", "published": "2019-11-08 08:00:45", "link": "http://arxiv.org/abs/1911.03110v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "iSarcasm: A Dataset of Intended Sarcasm", "abstract": "We consider the distinction between intended and perceived sarcasm in the\ncontext of textual sarcasm detection. The former occurs when an utterance is\nsarcastic from the perspective of its author, while the latter occurs when the\nutterance is interpreted as sarcastic by the audience. We show the limitations\nof previous labelling methods in capturing intended sarcasm and introduce the\niSarcasm dataset of tweets labeled for sarcasm directly by their authors.\nExamining the state-of-the-art sarcasm detection models on our dataset showed\nlow performance compared to previously studied datasets, which indicates that\nthese datasets might be biased or obvious and sarcasm could be a phenomenon\nunder-studied computationally thus far. By providing the iSarcasm dataset, we\naim to encourage future NLP research to develop methods for detecting sarcasm\nin text as intended by the authors of the text, not as labeled under\nassumptions that we demonstrate to be sub-optimal.", "published": "2019-11-08 08:40:22", "link": "http://arxiv.org/abs/1911.03123v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A General Framework for Adaptation of Neural Machine Translation to\n  Simultaneous Translation", "abstract": "Despite the success of neural machine translation (NMT), simultaneous neural\nmachine translation (SNMT), the task of translating in real time before a full\nsentence has been observed, remains challenging due to the syntactic structure\ndifference and simultaneity requirements. In this paper, we propose a general\nframework for adapting neural machine translation to translate simultaneously.\nOur framework contains two parts: prefix translation that utilizes a\nconsecutive NMT model to translate source prefixes and a stopping criterion\nthat determines when to stop the prefix translation. Experiments on three\ntranslation corpora and two language pairs show the efficacy of the proposed\nframework on balancing the quality and latency in adapting NMT to perform\nsimultaneous translation.", "published": "2019-11-08 09:55:37", "link": "http://arxiv.org/abs/1911.03154v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controlled Crowdsourcing for High-Quality QA-SRL Annotation", "abstract": "Question-answer driven Semantic Role Labeling (QA-SRL) was proposed as an\nattractive open and natural flavour of SRL, potentially attainable from laymen.\nRecently, a large-scale crowdsourced QA-SRL corpus and a trained parser were\nreleased. Trying to replicate the QA-SRL annotation for new texts, we found\nthat the resulting annotations were lacking in quality, particularly in\ncoverage, making them insufficient for further research and evaluation. In this\npaper, we present an improved crowdsourcing protocol for complex semantic\nannotation, involving worker selection and training, and a data consolidation\nphase. Applying this protocol to QA-SRL yielded high-quality annotation with\ndrastically higher coverage, producing a new gold evaluation dataset. We\nbelieve that our annotation protocol and gold standard will facilitate future\nreplicable research of natural semantic annotations.", "published": "2019-11-08 13:22:12", "link": "http://arxiv.org/abs/1911.03243v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Composing and Embedding the Words-as-Classifiers Model of Grounded\n  Semantics", "abstract": "The words-as-classifiers model of grounded lexical semantics learns a\nsemantic fitness score between physical entities and the words that are used to\ndenote those entities. In this paper, we explore how such a model can\nincrementally perform composition and how the model can be unified with a\ndistributional representation. For the latter, we leverage the classifier\ncoefficients as an embedding. For composition, we leverage the underlying\nmechanics of three different classifier types (i.e., logistic regression,\ndecision trees, and multi-layer perceptrons) to arrive at a several systematic\napproaches to composition unique to each classifier including both denotational\nand connotational methods of composition. We compare these approaches to each\nother and to prior work in a visual reference resolution task using the refCOCO\ndataset. Our results demonstrate the need to expand upon existing composition\nstrategies and bring together grounded and distributional representations.", "published": "2019-11-08 14:33:06", "link": "http://arxiv.org/abs/1911.03283v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Language-Neutral is Multilingual BERT?", "abstract": "Multilingual BERT (mBERT) provides sentence representations for 104\nlanguages, which are useful for many multi-lingual tasks. Previous work probed\nthe cross-linguality of mBERT using zero-shot transfer learning on\nmorphological and syntactic tasks. We instead focus on the semantic properties\nof mBERT. We show that mBERT representations can be split into a\nlanguage-specific component and a language-neutral component, and that the\nlanguage-neutral component is sufficiently general in terms of modeling\nsemantics to allow high-accuracy word-alignment and sentence retrieval but is\nnot yet good enough for the more difficult task of MT quality estimation. Our\nwork presents interesting challenges which must be solved to build better\nlanguage-neutral representations, particularly for tasks requiring linguistic\ntransfer of semantics.", "published": "2019-11-08 15:12:36", "link": "http://arxiv.org/abs/1911.03310v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transforming Wikipedia into Augmented Data for Query-Focused\n  Summarization", "abstract": "The limited size of existing query-focused summarization datasets renders\ntraining data-driven summarization models challenging. Meanwhile, the manual\nconstruction of a query-focused summarization corpus is costly and\ntime-consuming. In this paper, we use Wikipedia to automatically collect a\nlarge query-focused summarization dataset (named WIKIREF) of more than 280, 000\nexamples, which can serve as a means of data augmentation. We also develop a\nBERT-based query-focused summarization model (Q-BERT) to extract sentences from\nthe documents as summaries. To better adapt a huge model containing millions of\nparameters to tiny benchmarks, we identify and fine-tune only a sparse\nsubnetwork, which corresponds to a small fraction of the whole model\nparameters. Experimental results on three DUC benchmarks show that the model\npre-trained on WIKIREF has already achieved reasonable performance. After\nfine-tuning on the specific benchmark datasets, the model with data\naugmentation outperforms strong comparison systems. Moreover, both our proposed\nQ-BERT model and subnetwork fine-tuning further improve the model performance.\nThe dataset is publicly available at https://aka.ms/wikiref.", "published": "2019-11-08 15:28:21", "link": "http://arxiv.org/abs/1911.03324v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Negated and Misprimed Probes for Pretrained Language Models: Birds Can\n  Talk, But Cannot Fly", "abstract": "Building on Petroni et al. (2019), we propose two new probing tasks analyzing\nfactual knowledge stored in Pretrained Language Models (PLMs). (1) Negation. We\nfind that PLMs do not distinguish between negated (\"Birds cannot [MASK]\") and\nnon-negated (\"Birds can [MASK]\") cloze questions. (2) Mispriming. Inspired by\npriming methods in human psychology, we add \"misprimes\" to cloze questions\n(\"Talk? Birds can [MASK]\"). We find that PLMs are easily distracted by\nmisprimes. These results suggest that PLMs still have a long way to go to\nadequately learn human-like factual knowledge.", "published": "2019-11-08 16:08:48", "link": "http://arxiv.org/abs/1911.03343v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Good Sample is Hard to Find: Noise Injection Sampling and\n  Self-Training for Neural Language Generation Models", "abstract": "Deep neural networks (DNN) are quickly becoming the de facto standard\nmodeling method for many natural language generation (NLG) tasks. In order for\nsuch models to truly be useful, they must be capable of correctly generating\nutterances for novel meaning representations (MRs) at test time. In practice,\neven sophisticated DNNs with various forms of semantic control frequently fail\nto generate utterances faithful to the input MR. In this paper, we propose an\narchitecture agnostic self-training method to sample novel MR/text utterance\npairs to augment the original training data. Remarkably, after training on the\naugmented data, even simple encoder-decoder models with greedy decoding are\ncapable of generating semantically correct utterances that are as good as\nstate-of-the-art outputs in both automatic and human evaluations of quality.", "published": "2019-11-08 16:54:58", "link": "http://arxiv.org/abs/1911.03373v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low-Level Linguistic Controls for Style Transfer and Content\n  Preservation", "abstract": "Despite the success of style transfer in image processing, it has seen\nlimited progress in natural language generation. Part of the problem is that\ncontent is not as easily decoupled from style in the text domain. Curiously, in\nthe field of stylometry, content does not figure prominently in practical\nmethods of discriminating stylistic elements, such as authorship and genre.\nRather, syntax and function words are the most salient features. Drawing on\nthis work, we model style as a suite of low-level linguistic controls, such as\nfrequency of pronouns, prepositions, and subordinate clause constructions. We\ntrain a neural encoder-decoder model to reconstruct reference sentences given\nonly content words and the setting of the controls. We perform style transfer\nby keeping the content words fixed while adjusting the controls to be\nindicative of another style. In experiments, we show that the model reliably\nresponds to the linguistic controls and perform both automatic and manual\nevaluations on style transfer. We find we can fool a style classifier 84% of\nthe time, and that our model produces highly diverse and stylistically\ndistinctive outputs. This work introduces a formal, extendable model of style\nthat can add control to any neural text generation system.", "published": "2019-11-08 17:10:49", "link": "http://arxiv.org/abs/1911.03385v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question Generation from Paragraphs: A Tale of Two Hierarchical Models", "abstract": "Automatic question generation from paragraphs is an important and challenging\nproblem, particularly due to the long context from paragraphs. In this paper,\nwe propose and study two hierarchical models for the task of question\ngeneration from paragraphs. Specifically, we propose (a) a novel hierarchical\nBiLSTM model with selective attention and (b) a novel hierarchical Transformer\narchitecture, both of which learn hierarchical representations of paragraphs.\nWe model a paragraph in terms of its constituent sentences, and a sentence in\nterms of its constituent words. While the introduction of the attention\nmechanism benefits the hierarchical BiLSTM model, the hierarchical Transformer,\nwith its inherent attention and positional encoding mechanisms also performs\nbetter than flat transformer model. We conducted empirical evaluation on the\nwidely used SQuAD and MS MARCO datasets using standard metrics. The results\ndemonstrate the overall effectiveness of the hierarchical models over their\nflat counterparts. Qualitatively, our hierarchical models are able to generate\nfluent and relevant questions", "published": "2019-11-08 17:49:08", "link": "http://arxiv.org/abs/1911.03407v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Annotation Scheme of A Large-scale Multi-party Dialogues Dataset for\n  Discourse Parsing and Machine Comprehension", "abstract": "In this paper, we propose the scheme for annotating large-scale multi-party\nchat dialogues for discourse parsing and machine comprehension. The main goal\nof this project is to help understand multi-party dialogues. Our dataset is\nbased on the Ubuntu Chat Corpus. For each multi-party dialogue, we annotate the\ndiscourse structure and question-answer pairs for dialogues. As we know, this\nis the first large scale corpus for multi-party dialogues discourse parsing,\nand we firstly propose the task for multi-party dialogues machine reading\ncomprehension.", "published": "2019-11-08 19:33:06", "link": "http://arxiv.org/abs/1911.03514v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The TechQA Dataset", "abstract": "We introduce TechQA, a domain-adaptation question answering dataset for the\ntechnical support domain. The TechQA corpus highlights two real-world issues\nfrom the automated customer support domain. First, it contains actual questions\nposed by users on a technical forum, rather than questions generated\nspecifically for a competition or a task. Second, it has a real-world size --\n600 training, 310 dev, and 490 evaluation question/answer pairs -- thus\nreflecting the cost of creating large labeled datasets with actual data.\nConsequently, TechQA is meant to stimulate research in domain adaptation rather\nthan being a resource to build QA systems from scratch. The dataset was\nobtained by crawling the IBM Developer and IBM DeveloperWorks forums for\nquestions with accepted answers that appear in a published IBM Technote---a\ntechnical document that addresses a specific technical issue. We also release a\ncollection of the 801,998 publicly available Technotes as of April 4, 2019 as a\ncompanion resource that might be used for pretraining, to learn representations\nof the IT domain language.", "published": "2019-11-08 02:04:39", "link": "http://arxiv.org/abs/1911.02984v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Relevance Transfer for Document Retrieval", "abstract": "Recent work has shown the surprising ability of multi-lingual BERT to serve\nas a zero-shot cross-lingual transfer model for a number of language processing\ntasks. We combine this finding with a similarly-recently proposal on\nsentence-level relevance modeling for document retrieval to demonstrate the\nability of multi-lingual BERT to transfer models of relevance across languages.\nExperiments on test collections in five different languages from diverse\nlanguage families (Chinese, Arabic, French, Hindi, and Bengali) show that\nmodels trained with English data improve ranking quality, without any special\nprocessing, both for (non-English) mono-lingual retrieval as well as\ncross-lingual retrieval.", "published": "2019-11-08 02:19:52", "link": "http://arxiv.org/abs/1911.02989v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Resurrecting Submodularity for Neural Text Generation", "abstract": "Submodularity is desirable for a variety of objectives in content selection\nwhere the current neural encoder-decoder framework is inadequate. However, it\nhas so far not been explored in the neural encoder-decoder system for text\ngeneration. In this work, we define diminishing attentions with submodular\nfunctions and in turn, prove the submodularity of the effective neural\ncoverage. The greedy algorithm approximating the solution to the submodular\nmaximization problem is not suited to attention score optimization in\nauto-regressive generation. Therefore instead of following how submodular\nfunction has been widely used, we propose a simplified yet principled solution.\nThe resulting attention module offers an architecturally simple and empirically\neffective method to improve the coverage of neural text generation. We run\nexperiments on three directed text generation tasks with different levels of\nrecovering rate, across two modalities, three different neural model\narchitectures and two training strategy variations. The results and analyses\ndemonstrate that our method generalizes well across these settings, produces\ntexts of good quality and outperforms state-of-the-art baselines.", "published": "2019-11-08 03:17:54", "link": "http://arxiv.org/abs/1911.03014v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interactive Refinement of Cross-Lingual Word Embeddings", "abstract": "Cross-lingual word embeddings transfer knowledge between languages: models\ntrained on high-resource languages can predict in low-resource languages. We\nintroduce CLIME, an interactive system to quickly refine cross-lingual word\nembeddings for a given classification problem. First, CLIME ranks words by\ntheir salience to the downstream task. Then, users mark similarity between\nkeywords and their nearest neighbors in the embedding space. Finally, CLIME\nupdates the embeddings using the annotations. We evaluate CLIME on identifying\nhealth-related text in four low-resource languages: Ilocano, Sinhalese,\nTigrinya, and Uyghur. Embeddings refined by CLIME capture more nuanced word\nsemantics and have higher test accuracy than the original embeddings. CLIME\noften improves accuracy faster than an active learning baseline and can be\neasily combined with active learning to improve results.", "published": "2019-11-08 06:07:25", "link": "http://arxiv.org/abs/1911.03070v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Are we asking the right questions in MovieQA?", "abstract": "Joint vision and language tasks like visual question answering are\nfascinating because they explore high-level understanding, but at the same\ntime, can be more prone to language biases. In this paper, we explore the\nbiases in the MovieQA dataset and propose a strikingly simple model which can\nexploit them. We find that using the right word embedding is of utmost\nimportance. By using an appropriately trained word embedding, about half the\nQuestion-Answers (QAs) can be answered by looking at the questions and answers\nalone, completely ignoring narrative context from video clips, subtitles, and\nmovie scripts. Compared to the best published papers on the leaderboard, our\nsimple question + answer only model improves accuracy by 5% for video +\nsubtitle category, 5% for subtitle, 15% for DVS and 6% higher for scripts.", "published": "2019-11-08 06:49:45", "link": "http://arxiv.org/abs/1911.03083v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Not Enough Data? Deep Learning to the Rescue!", "abstract": "Based on recent advances in natural language modeling and those in text\ngeneration capabilities, we propose a novel data augmentation method for text\nclassification tasks. We use a powerful pre-trained neural network model to\nartificially synthesize new labeled data for supervised learning. We mainly\nfocus on cases with scarce labeled data. Our method, referred to as\nlanguage-model-based data augmentation (LAMBADA), involves fine-tuning a\nstate-of-the-art language generator to a specific task through an initial\ntraining phase on the existing (usually small) labeled data. Using the\nfine-tuned model and given a class label, new sentences for the class are\ngenerated. Our process then filters these new sentences by using a classifier\ntrained on the original data. In a series of experiments, we show that LAMBADA\nimproves classifiers' performance on a variety of datasets. Moreover, LAMBADA\nsignificantly improves upon the state-of-the-art techniques for data\naugmentation, specifically those applicable to text classification tasks with\nlittle data.", "published": "2019-11-08 08:30:22", "link": "http://arxiv.org/abs/1911.03118v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lipschitz Constrained Parameter Initialization for Deep Transformers", "abstract": "The Transformer translation model employs residual connection and layer\nnormalization to ease the optimization difficulties caused by its multi-layer\nencoder/decoder structure. Previous research shows that even with residual\nconnection and layer normalization, deep Transformers still have difficulty in\ntraining, and particularly Transformer models with more than 12 encoder/decoder\nlayers fail to converge. In this paper, we first empirically demonstrate that a\nsimple modification made in the official implementation, which changes the\ncomputation order of residual connection and layer normalization, can\nsignificantly ease the optimization of deep Transformers. We then compare the\nsubtle differences in computation order in considerable detail, and present a\nparameter initialization method that leverages the Lipschitz constraint on the\ninitialization of Transformer parameters that effectively ensures training\nconvergence. In contrast to findings in previous research we further\ndemonstrate that with Lipschitz parameter initialization, deep Transformers\nwith the original computation order can converge, and obtain significant BLEU\nimprovements with up to 24 layers. In contrast to previous research which\nfocuses on deep encoders, our approach additionally enables Transformers to\nalso benefit from deep decoders.", "published": "2019-11-08 10:52:43", "link": "http://arxiv.org/abs/1911.03179v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Char-RNN and Active Learning for Hashtag Segmentation", "abstract": "We explore the abilities of character recurrent neural network (char-RNN) for\nhashtag segmentation. Our approach to the task is the following: we generate\nsynthetic training dataset according to frequent n-grams that satisfy\npredefined morpho-syntactic patterns to avoid any manual annotation. The active\nlearning strategy limits the training dataset and selects informative training\nsubset. The approach does not require any language-specific settings and is\ncompared for two languages, which differ in inflection degree.", "published": "2019-11-08 14:03:55", "link": "http://arxiv.org/abs/1911.03270v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Ask to Learn: A Study on Curiosity-driven Question Generation", "abstract": "We propose a novel text generation task, namely Curiosity-driven Question\nGeneration. We start from the observation that the Question Generation task has\ntraditionally been considered as the dual problem of Question Answering, hence\ntackling the problem of generating a question given the text that contains its\nanswer. Such questions can be used to evaluate machine reading comprehension.\nHowever, in real life, and especially in conversational settings, humans tend\nto ask questions with the goal of enriching their knowledge and/or clarifying\naspects of previously gathered information. We refer to these inquisitive\nquestions as Curiosity-driven: these questions are generated with the goal of\nobtaining new information (the answer) which is not present in the input text.\nIn this work, we experiment on this new task using a conversational Question\nAnswering (QA) dataset; further, since the majority of QA dataset are not built\nin a conversational manner, we describe a methodology to derive data for this\nnovel task from non-conversational QA data. We investigate several automated\nmetrics to measure the different properties of Curious Questions, and\nexperiment different approaches on the Curiosity-driven Question Generation\ntask, including model pre-training and reinforcement learning. Finally, we\nreport a qualitative evaluation of the generated outputs.", "published": "2019-11-08 16:17:40", "link": "http://arxiv.org/abs/1911.03350v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SEPT: Improving Scientific Named Entity Recognition with Span\n  Representation", "abstract": "We introduce a new scientific named entity recognizer called SEPT, which\nstands for Span Extractor with Pre-trained Transformers. In recent papers, span\nextractors have been demonstrated to be a powerful model compared with sequence\nlabeling models. However, we discover that with the development of pre-trained\nlanguage models, the performance of span extractors appears to become similar\nto sequence labeling models. To keep the advantages of span representation, we\nmodified the model by under-sampling to balance the positive and negative\nsamples and reduce the search space. Furthermore, we simplify the origin\nnetwork architecture to combine the span extractor with BERT. Experiments\ndemonstrate that even simplified architecture achieves the same performance and\nSEPT achieves a new state of the art result in scientific named entity\nrecognition even without relation information involved.", "published": "2019-11-08 16:19:26", "link": "http://arxiv.org/abs/1911.03353v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Neural Arabic Text Diacritization: State of the Art Results and a Novel\n  Approach for Machine Translation", "abstract": "In this work, we present several deep learning models for the automatic\ndiacritization of Arabic text. Our models are built using two main approaches,\nviz. Feed-Forward Neural Network (FFNN) and Recurrent Neural Network (RNN),\nwith several enhancements such as 100-hot encoding, embeddings, Conditional\nRandom Field (CRF) and Block-Normalized Gradient (BNG). The models are tested\non the only freely available benchmark dataset and the results show that our\nmodels are either better or on par with other models, which require\nlanguage-dependent post-processing steps, unlike ours. Moreover, we show that\ndiacritics in Arabic can be used to enhance the models of NLP tasks such as\nMachine Translation (MT) by proposing the Translation over Diacritization (ToD)\napproach.", "published": "2019-11-08 20:52:12", "link": "http://arxiv.org/abs/1911.03531v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Graph-to-Graph Transformer for Transition-based Dependency Parsing", "abstract": "We propose the Graph2Graph Transformer architecture for conditioning on and\npredicting arbitrary graphs, and apply it to the challenging task of\ntransition-based dependency parsing. After proposing two novel Transformer\nmodels of transition-based dependency parsing as strong baselines, we show that\nadding the proposed mechanisms for conditioning on and predicting graphs of\nGraph2Graph Transformer results in significant improvements, both with and\nwithout BERT pre-training. The novel baselines and their integration with\nGraph2Graph Transformer significantly outperform the state-of-the-art in\ntraditional transition-based dependency parsing on both English Penn Treebank,\nand 13 languages of Universal Dependencies Treebanks. Graph2Graph Transformer\ncan be integrated with many previous structured prediction methods, making it\neasy to apply to a wide range of NLP tasks.", "published": "2019-11-08 22:14:35", "link": "http://arxiv.org/abs/1911.03561v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The State of NLP Literature: A Diachronic Analysis of the ACL Anthology", "abstract": "The ACL Anthology (AA) is a digital repository of tens of thousands of\narticles on Natural Language Processing (NLP). This paper examines the\nliterature as a whole to identify broad trends in productivity, focus, and\nimpact. It presents the analyses in a sequence of questions and answers. The\ngoal is to record the state of the AA literature: who and how many of us are\npublishing? what are we publishing on? where and in what form are we\npublishing? and what is the impact of our publications? The answers are usually\nin the form of numbers, graphs, and inter-connected visualizations. Special\nemphasis is laid on the demographics and inclusiveness of NLP publishing.\nNotably, we find that only about 30% of first authors are female, and that this\npercentage has not improved since the year 2000. We also show that, on average,\nfemale first authors are cited less than male first authors, even when\ncontrolling for experience. We hope that recording citation and participation\ngaps across demographic groups will encourage more inclusiveness and fairness\nin research.", "published": "2019-11-08 22:15:32", "link": "http://arxiv.org/abs/1911.03562v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Reducing Sentiment Bias in Language Models via Counterfactual Evaluation", "abstract": "Advances in language modeling architectures and the availability of large\ntext corpora have driven progress in automatic text generation. While this\nresults in models capable of generating coherent texts, it also prompts models\nto internalize social biases present in the training corpus. This paper aims to\nquantify and reduce a particular type of bias exhibited by language models:\nbias in the sentiment of generated text. Given a conditioning context (e.g., a\nwriting prompt) and a language model, we analyze if (and how) the sentiment of\nthe generated text is affected by changes in values of sensitive attributes\n(e.g., country names, occupations, genders) in the conditioning context using a\nform of counterfactual evaluation. We quantify sentiment bias by adopting\nindividual and group fairness metrics from the fair machine learning\nliterature, and demonstrate that large-scale models trained on two different\ncorpora (news articles, and Wikipedia) exhibit considerable levels of bias. We\nthen propose embedding and sentiment prediction-derived regularization on the\nlanguage model's latent representations. The regularizations improve fairness\nmetrics while retaining comparable levels of perplexity and semantic\nsimilarity.", "published": "2019-11-08 05:56:01", "link": "http://arxiv.org/abs/1911.03064v3", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Europarl-ST: A Multilingual Corpus For Speech Translation Of\n  Parliamentary Debates", "abstract": "Current research into spoken language translation (SLT),or speech-to-text\ntranslation, is often hampered by the lack of specific data resources for this\ntask, as currently available SLT datasets are restricted to a limited set of\nlanguage pairs. In this paper we present Europarl-ST, a novel multilingual SLT\ncorpus containing paired audio-text samples for SLT from and into 6 European\nlanguages, for a total of 30 different translation directions. This corpus has\nbeen compiled using the debates held in the European Parliament in the period\nbetween 2008 and 2012. This paper describes the corpus creation process and\npresents a series of automatic speech recognition, machine translation and\nspoken language translation experiments that highlight the potential of this\nnew resource. The corpus is released under a Creative Commons license and is\nfreely accessible and downloadable.", "published": "2019-11-08 10:22:10", "link": "http://arxiv.org/abs/1911.03167v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Language Grounding through Social Interactions and Curiosity-Driven\n  Multi-Goal Learning", "abstract": "Autonomous reinforcement learning agents, like children, do not have access\nto predefined goals and reward functions. They must discover potential goals,\nlearn their own reward functions and engage in their own learning trajectory.\nChildren, however, benefit from exposure to language, helping to organize and\nmediate their thought. We propose LE2 (Language Enhanced Exploration), a\nlearning algorithm leveraging intrinsic motivations and natural language (NL)\ninteractions with a descriptive social partner (SP). Using NL descriptions from\nthe SP, it can learn an NL-conditioned reward function to formulate goals for\nintrinsically motivated goal exploration and learn a goal-conditioned policy.\nBy exploring, collecting descriptions from the SP and jointly learning the\nreward function and the policy, the agent grounds NL descriptions into real\nbehavioral goals. From simple goals discovered early to more complex goals\ndiscovered by experimenting on simpler ones, our agent autonomously builds its\nown behavioral repertoire. This naturally occurring curriculum is supplemented\nby an active learning curriculum resulting from the agent's intrinsic\nmotivations. Experiments are presented with a simulated robotic arm that\ninteracts with several objects including tools.", "published": "2019-11-08 12:42:22", "link": "http://arxiv.org/abs/1911.03219v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Advances in Machine Learning for the Behavioral Sciences", "abstract": "The areas of machine learning and knowledge discovery in databases have\nconsiderably matured in recent years. In this article, we briefly review recent\ndevelopments as well as classical algorithms that stood the test of time. Our\ngoal is to provide a general introduction into different tasks such as learning\nfrom tabular data, behavioral data, or textual data, with a particular focus on\nactual and potential applications in behavioral sciences. The supplemental\nappendix to the article also provides practical guidance for using the methods\nby pointing the reader to proven software implementations. The focus is on R,\nbut we also cover some libraries in other programming languages as well as\nsystems with easy-to-use graphical interfaces.", "published": "2019-11-08 13:30:21", "link": "http://arxiv.org/abs/1911.03249v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Memory-Augmented Recurrent Neural Networks Can Learn Generalized Dyck\n  Languages", "abstract": "We introduce three memory-augmented Recurrent Neural Networks (MARNNs) and\nexplore their capabilities on a series of simple language modeling tasks whose\nsolutions require stack-based mechanisms. We provide the first demonstration of\nneural networks recognizing the generalized Dyck languages, which express the\ncore of what it means to be a language with hierarchical structure. Our\nmemory-augmented architectures are easy to train in an end-to-end fashion and\ncan learn the Dyck languages over as many as six parenthesis-pairs, in addition\nto two deterministic palindrome languages and the string-reversal transduction\ntask, by emulating pushdown automata. Our experiments highlight the increased\nmodeling capacity of memory-augmented models over simple RNNs, while inflecting\nour understanding of the limitations of these models.", "published": "2019-11-08 15:33:51", "link": "http://arxiv.org/abs/1911.03329v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Investigation of Error Simulation Techniques for Learning Dialog\n  Policies for Conversational Error Recovery", "abstract": "Training dialog policies for speech-based virtual assistants requires a\nplethora of conversational data. The data collection phase is often expensive\nand time consuming due to human involvement. To address this issue, a common\nsolution is to build user simulators for data generation. For the successful\ndeployment of the trained policies into real world domains, it is vital that\nthe user simulator mimics realistic conditions. In particular, speech-based\nassistants are heavily affected by automatic speech recognition and language\nunderstanding errors, hence the user simulator should be able to simulate\nsimilar errors. In this paper, we review the existing error simulation methods\nthat induce errors at audio, phoneme, text, or semantic level; and conduct\ndetailed comparisons between the audio-level and text-level methods. In the\nprocess, we improve the existing text-level method by introducing confidence\nscore prediction and out-of-vocabulary word mapping. We also explore the impact\nof audio-level and text-level methods on learning a simple clarification dialog\npolicy to recover from errors to provide insight on future improvement for both\napproaches.", "published": "2019-11-08 16:59:17", "link": "http://arxiv.org/abs/1911.03378v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ERASER: A Benchmark to Evaluate Rationalized NLP Models", "abstract": "State-of-the-art models in NLP are now predominantly based on deep neural\nnetworks that are opaque in terms of how they come to make predictions. This\nlimitation has increased interest in designing more interpretable deep models\nfor NLP that reveal the `reasoning' behind model outputs. But work in this\ndirection has been conducted on different datasets and tasks with\ncorrespondingly unique aims and metrics; this makes it difficult to track\nprogress. We propose the Evaluating Rationales And Simple English Reasoning\n(ERASER) benchmark to advance research on interpretable models in NLP. This\nbenchmark comprises multiple datasets and tasks for which human annotations of\n\"rationales\" (supporting evidence) have been collected. We propose several\nmetrics that aim to capture how well the rationales provided by models align\nwith human rationales, and also how faithful these rationales are (i.e., the\ndegree to which provided rationales influenced the corresponding predictions).\nOur hope is that releasing this benchmark facilitates progress on designing\nmore interpretable NLP systems. The benchmark, code, and documentation are\navailable at https://www.eraserbenchmark.com/", "published": "2019-11-08 18:29:03", "link": "http://arxiv.org/abs/1911.03429v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language\n  Models through Principled Regularized Optimization", "abstract": "Transfer learning has fundamentally changed the landscape of natural language\nprocessing (NLP) research. Many existing state-of-the-art models are first\npre-trained on a large text corpus and then fine-tuned on downstream tasks.\nHowever, due to limited data resources from downstream tasks and the extremely\nlarge capacity of pre-trained models, aggressive fine-tuning often causes the\nadapted model to overfit the data of downstream tasks and forget the knowledge\nof the pre-trained model. To address the above issue in a more principled\nmanner, we propose a new computational framework for robust and efficient\nfine-tuning for pre-trained language models. Specifically, our proposed\nframework contains two important ingredients: 1. Smoothness-inducing\nregularization, which effectively manages the capacity of the model; 2. Bregman\nproximal point optimization, which is a class of trust-region methods and can\nprevent knowledge forgetting. Our experiments demonstrate that our proposed\nmethod achieves the state-of-the-art performance on multiple NLP benchmarks.", "published": "2019-11-08 18:41:31", "link": "http://arxiv.org/abs/1911.03437v5", "categories": ["cs.CL", "cs.LG", "math.OC"], "primary_category": "cs.CL"}
{"title": "On the Relationship between Self-Attention and Convolutional Layers", "abstract": "Recent trends of incorporating attention mechanisms in vision have led\nresearchers to reconsider the supremacy of convolutional layers as a primary\nbuilding block. Beyond helping CNNs to handle long-range dependencies,\nRamachandran et al. (2019) showed that attention can completely replace\nconvolution and achieve state-of-the-art performance on vision tasks. This\nraises the question: do learned attention layers operate similarly to\nconvolutional layers? This work provides evidence that attention layers can\nperform convolution and, indeed, they often learn to do so in practice.\nSpecifically, we prove that a multi-head self-attention layer with sufficient\nnumber of heads is at least as expressive as any convolutional layer. Our\nnumerical experiments then show that self-attention layers attend to pixel-grid\npatterns similarly to CNN layers, corroborating our analysis. Our code is\npublicly available.", "published": "2019-11-08 23:48:38", "link": "http://arxiv.org/abs/1911.03584v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Instance-based Transfer Learning for Multilingual Deep Retrieval", "abstract": "We focus on the problem of search in the multilingual setting. Examining the\nproblems of next-sentence prediction and inverse cloze, we show that at large\nscale, instance-based transfer learning is surprisingly effective in the\nmultilingual setting, leading to positive transfer on all of the 35 target\nlanguages and two tasks tested. We analyze this improvement and argue that the\nmost natural explanation, namely direct vocabulary overlap between languages,\nonly partially explains the performance gains: in fact, we demonstrate\ntarget-language improvement can occur after adding data from an auxiliary\nlanguage even with no vocabulary in common with the target. This surprising\nresult is due to the effect of transitive vocabulary overlaps between pairs of\nauxiliary and target languages.", "published": "2019-11-08 22:23:30", "link": "http://arxiv.org/abs/1911.06111v3", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Towards Hierarchical Importance Attribution: Explaining Compositional\n  Semantics for Neural Sequence Models", "abstract": "The impressive performance of neural networks on natural language processing\ntasks attributes to their ability to model complicated word and phrase\ncompositions. To explain how the model handles semantic compositions, we study\nhierarchical explanation of neural network predictions. We identify\nnon-additivity and context independent importance attributions within\nhierarchies as two desirable properties for highlighting word and phrase\ncompositions. We show some prior efforts on hierarchical explanations, e.g.\ncontextual decomposition, do not satisfy the desired properties mathematically,\nleading to inconsistent explanation quality in different models. In this paper,\nwe start by proposing a formal and general way to quantify the importance of\neach word and phrase. Following the formulation, we propose Sampling and\nContextual Decomposition (SCD) algorithm and Sampling and Occlusion (SOC)\nalgorithm. Human and metrics evaluation on both LSTM models and BERT\nTransformer models on multiple datasets show that our algorithms outperform\nprior hierarchical explanation algorithms. Our algorithms help to visualize\nsemantic composition captured by models, extract classification rules and\nimprove human trust of models. Project page: https://inklab.usc.edu/hiexpl/", "published": "2019-11-08 03:25:04", "link": "http://arxiv.org/abs/1911.06194v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Towards automatic extractive text summarization of A-133 Single Audit\n  reports with machine learning", "abstract": "The rapid growth of text data has motivated the development of\nmachine-learning based automatic text summarization strategies that concisely\ncapture the essential ideas in a larger text. This study aimed to devise an\nextractive summarization method for A-133 Single Audits, which assess if\nrecipients of federal grants are compliant with program requirements for use of\nfederal funding. Currently, these voluminous audits must be manually analyzed\nby officials for oversight, risk management, and prioritization purposes.\nAutomated summarization has the potential to streamline these processes.\nAnalysis focused on the \"Findings\" section of ~20,000 Single Audits spanning\n2016-2018. Following text preprocessing and GloVe embedding, sentence-level\nk-means clustering was performed to partition sentences by topic and to\nestablish the importance of each sentence. For each audit, key summary\nsentences were extracted by proximity to cluster centroids. Summaries were\njudged by non-expert human evaluation and compared to human-generated summaries\nusing the ROUGE metric. Though the goal was to fully automate summarization of\nA-133 audits, human input was required at various stages due to large\nvariability in audit writing style, content, and context. Examples of human\ninputs include the number of clusters, the choice to keep or discard certain\nclusters based on their content relevance, and the definition of a top\nsentence. Overall, this approach made progress towards automated extractive\nsummaries of A-133 audits, with future work to focus on full automation and\nimproving summary consistency. This work highlights the inherent difficulty and\nsubjective nature of automated summarization in a real-world application.", "published": "2019-11-08 14:49:25", "link": "http://arxiv.org/abs/1911.06197v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Adversarial Attacks on GMM i-vector based Speaker Verification Systems", "abstract": "This work investigates the vulnerability of Gaussian Mixture Model (GMM)\ni-vector based speaker verification systems to adversarial attacks, and the\ntransferability of adversarial samples crafted from GMM i-vector based systems\nto x-vector based systems. In detail, we formulate the GMM i-vector system as a\nscoring function of enrollment and testing utterance pairs. Then we leverage\nthe fast gradient sign method (FGSM) to optimize testing utterances for\nadversarial samples generation. These adversarial samples are used to attack\nboth GMM i-vector and x-vector systems. We measure the system vulnerability by\nthe degradation of equal error rate and false acceptance rate. Experiment\nresults show that GMM i-vector systems are seriously vulnerable to adversarial\nattacks, and the crafted adversarial samples prove to be transferable and pose\nthreats to neuralnetwork speaker embedding based systems (e.g. x-vector\nsystems).", "published": "2019-11-08 06:38:14", "link": "http://arxiv.org/abs/1911.03078v2", "categories": ["eess.AS", "cs.CL", "cs.CR", "cs.LG", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Relation Adversarial Network for Low Resource Knowledge Graph Completion", "abstract": "Knowledge Graph Completion (KGC) has been proposed to improve Knowledge\nGraphs by filling in missing connections via link prediction or relation\nextraction. One of the main difficulties for KGC is a low resource problem.\nPrevious approaches assume sufficient training triples to learn versatile\nvectors for entities and relations, or a satisfactory number of labeled\nsentences to train a competent relation extraction model. However, low resource\nrelations are very common in KGs, and those newly added relations often do not\nhave many known samples for training. In this work, we aim at predicting new\nfacts under a challenging setting where only limited training instances are\navailable. We propose a general framework called Weighted Relation Adversarial\nNetwork, which utilizes an adversarial procedure to help adapt\nknowledge/features learned from high resource relations to different but\nrelated low resource relations. Specifically, the framework takes advantage of\na relation discriminator to distinguish between samples from different\nrelations, and help learn relation-invariant features more transferable from\nsource relations to target relations. Experimental results show that the\nproposed approach outperforms previous methods regarding low resource settings\nfor both link prediction and relation extraction.", "published": "2019-11-08 07:05:25", "link": "http://arxiv.org/abs/1911.03091v6", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Recurrent Neural Network Transducer for Audio-Visual Speech Recognition", "abstract": "This work presents a large-scale audio-visual speech recognition system based\non a recurrent neural network transducer (RNN-T) architecture. To support the\ndevelopment of such a system, we built a large audio-visual (A/V) dataset of\nsegmented utterances extracted from YouTube public videos, leading to 31k hours\nof audio-visual training content. The performance of an audio-only,\nvisual-only, and audio-visual system are compared on two large-vocabulary test\nsets: a set of utterance segments from public YouTube videos called YTDEV18 and\nthe publicly available LRS3-TED set. To highlight the contribution of the\nvisual modality, we also evaluated the performance of our system on the YTDEV18\nset artificially corrupted with background noise and overlapping speech. To the\nbest of our knowledge, our system significantly improves the state-of-the-art\non the LRS3-TED set.", "published": "2019-11-08 22:01:42", "link": "http://arxiv.org/abs/1911.04890v1", "categories": ["eess.AS", "cs.CL", "cs.CV", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Online Spectrogram Inversion for Low-Latency Audio Source Separation", "abstract": "Audio source separation is usually achieved by estimating the short-time\nFourier transform (STFT) magnitude of each source, and then applying a\nspectrogram inversion algorithm to retrieve time-domain signals. In particular,\nthe multiple input spectrogram inversion (MISI) algorithm has been exploited\nsuccessfully in several recent works. However, this algorithm suffers from two\ndrawbacks, which we address in this paper. First, it has originally been\nintroduced in a heuristic fashion: we propose here a rigorous optimization\nframework in which MISI is derived, thus proving the convergence of this\nalgorithm. Besides, while MISI operates offline, we propose here an online\nversion of MISI called oMISI, which is suitable for low-latency source\nseparation, an important requirement for e.g., hearing aids applications. oMISI\nalso allows one to use alternative phase initialization schemes exploiting the\ntemporal structure of audio signals. Experiments conducted on a speech\nseparation task show that oMISI performs as well as its offline counterpart,\nthus demonstrating its potential for real-time source separation.", "published": "2019-11-08 08:45:58", "link": "http://arxiv.org/abs/1911.03128v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Voice Activity Detection in presence of background noise using EEG", "abstract": "In this paper we demonstrate that performance of voice activity detection\n(VAD) system operating in presence of background noise can be improved by\nconcatenating acoustic input features with electroencephalography (EEG)\nfeatures. We also demonstrate that VAD using only EEG features shows better\nperformance than VAD using only acoustic features in presence of background\nnoise. We implemented a recurrent neural network (RNN) based VAD system and we\ndemonstrate our results for two different data sets recorded in presence of\ndifferent noise conditions in this paper. We finally demonstrate the ability to\npredict whether a person wish to continue speaking a sentence or not from EEG\nfeatures.", "published": "2019-11-08 07:16:27", "link": "http://arxiv.org/abs/1911.04261v5", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
