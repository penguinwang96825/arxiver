{"title": "Learning Phonotactics from Linguistic Informants", "abstract": "We propose an interactive approach to language learning that utilizes\nlinguistic acceptability judgments from an informant (a competent language\nuser) to learn a grammar. Given a grammar formalism and a framework for\nsynthesizing data, our model iteratively selects or synthesizes a data-point\naccording to one of a range of information-theoretic policies, asks the\ninformant for a binary judgment, and updates its own parameters in preparation\nfor the next query. We demonstrate the effectiveness of our model in the domain\nof phonotactics, the rules governing what kinds of sound-sequences are\nacceptable in a language, and carry out two experiments, one with\ntypologically-natural linguistic data and another with a range of\nprocedurally-generated languages. We find that the information-theoretic\npolicies that our model uses to select items to query the informant achieve\nsample efficiency comparable to, and sometimes greater than, fully supervised\napproaches.", "published": "2024-05-08 00:18:56", "link": "http://arxiv.org/abs/2405.04726v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empathy Through Multimodality in Conversational Interfaces", "abstract": "Agents represent one of the most emerging applications of Large Language\nModels (LLMs) and Generative AI, with their effectiveness hinging on multimodal\ncapabilities to navigate complex user environments. Conversational Health\nAgents (CHAs), a prime example of this, are redefining healthcare by offering\nnuanced support that transcends textual analysis to incorporate emotional\nintelligence. This paper introduces an LLM-based CHA engineered for rich,\nmultimodal dialogue-especially in the realm of mental health support. It\nadeptly interprets and responds to users' emotional states by analyzing\nmultimodal cues, thus delivering contextually aware and empathetically resonant\nverbal responses. Our implementation leverages the versatile openCHA framework,\nand our comprehensive evaluation involves neutral prompts expressed in diverse\nemotional tones: sadness, anger, and joy. We evaluate the consistency and\nrepeatability of the planning capability of the proposed CHA. Furthermore,\nhuman evaluators critique the CHA's empathic delivery, with findings revealing\na striking concordance between the CHA's outputs and evaluators' assessments.\nThese results affirm the indispensable role of vocal (soon multimodal) emotion\nrecognition in strengthening the empathetic connection built by CHAs, cementing\ntheir place at the forefront of interactive, compassionate digital health\nsolutions.", "published": "2024-05-08 02:48:29", "link": "http://arxiv.org/abs/2405.04777v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CourseGPT-zh: an Educational Large Language Model Based on Knowledge\n  Distillation Incorporating Prompt Optimization", "abstract": "Large language models (LLMs) have demonstrated astonishing capabilities in\nnatural language processing (NLP) tasks, sparking interest in their application\nto professional domains with higher specialized requirements. However,\nrestricted access to closed-source LLMs via APIs and the difficulty in\ncollecting massive high-quality datasets pose obstacles to the development of\nlarge language models in education fields of various courses. Given these\nchallenges, we propose CourseGPT-zh, a course-oriented education LLM that\nsupports customization and low-cost deployment. To address the\ncomprehensiveness and diversity requirements of course-specific corpora, we\ndesign a high-quality question-answering corpus distillation framework\nincorporating prompt optimization, which effectively mines textbook knowledge\nand enhances its diversity. Moreover, considering the alignment of LLM\nresponses with user needs, a novel method for discrete prompt optimization\nbased on LLM-as-Judge is introduced. During optimization, this framework\nleverages the LLM's ability to reflect on and exploit error feedback and\npatterns, allowing for prompts that meet user needs and preferences while\nsaving response length. Lastly, we obtain CourseGPT-zh based on the open-source\nLLM using parameter-efficient fine-tuning. Experimental results show that our\ndiscrete prompt optimization framework effectively improves the response\nquality of ChatGPT, and CourseGPT-zh exhibits strong professional capabilities\nin specialized knowledge question-answering, significantly outperforming\ncomparable open-source models.", "published": "2024-05-08 03:11:12", "link": "http://arxiv.org/abs/2405.04781v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ACORN: Aspect-wise Commonsense Reasoning Explanation Evaluation", "abstract": "Evaluating the quality of free-text explanations is a multifaceted,\nsubjective, and labor-intensive task. Large language models (LLMs) present an\nappealing alternative due to their potential for consistency, scalability, and\ncost-efficiency. In this work, we present ACORN, a new dataset of 3,500\nfree-text explanations and aspect-wise quality ratings, and use it to evaluate\nhow LLMs rate explanations. We observed that larger models outputted labels\nthat maintained or increased the inter-annotator agreement, suggesting that\nthey are within the expected variance between human raters. However, their\ncorrelation with majority-voted human ratings varied across different quality\naspects, indicating that they are not a complete replacement. In turn, using\nLLMs as a supplement to a smaller group of human raters in some cases improved\nthe correlation with the original majority labels. However, the effect was\nlimited to cases where human raters were scarce, and an additional human rater\nhad a more pronounced effect in all cases. Overall, we recommend against using\nLLMs as a complete replacement for human raters but encourage using them in\nconfigurations that end with targeted human involvement. Data available here:\nhttps://github.com/a-brassard/ACORN", "published": "2024-05-08 05:36:52", "link": "http://arxiv.org/abs/2405.04818v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChuXin: 1.6B Technical Report", "abstract": "In this report, we present ChuXin, an entirely open-source language model\nwith a size of 1.6 billion parameters. Unlike the majority of works that only\nopen-sourced the model weights and architecture, we have made everything needed\nto train a model available, including the training data, the training process,\nand the evaluation code. Our goal is to empower and strengthen the open\nresearch community, fostering transparency and enabling a new wave of\ninnovation in the field of language modeling. Furthermore, we extend the\ncontext length to 1M tokens through lightweight continual pretraining and\ndemonstrate strong needle-in-a-haystack retrieval performance. The weights for\nboth models are available at Hugging Face to download and use.", "published": "2024-05-08 05:54:44", "link": "http://arxiv.org/abs/2405.04828v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-tuning Pre-trained Named Entity Recognition Models For Indian\n  Languages", "abstract": "Named Entity Recognition (NER) is a useful component in Natural Language\nProcessing (NLP) applications. It is used in various tasks such as Machine\nTranslation, Summarization, Information Retrieval, and Question-Answering\nsystems. The research on NER is centered around English and some other major\nlanguages, whereas limited attention has been given to Indian languages. We\nanalyze the challenges and propose techniques that can be tailored for\nMultilingual Named Entity Recognition for Indian Languages. We present a human\nannotated named entity corpora of 40K sentences for 4 Indian languages from two\nof the major Indian language families. Additionally,we present a multilingual\nmodel fine-tuned on our dataset, which achieves an F1 score of 0.80 on our\ndataset on average. We achieve comparable performance on completely unseen\nbenchmark datasets for Indian languages which affirms the usability of our\nmodel.", "published": "2024-05-08 05:54:54", "link": "http://arxiv.org/abs/2405.04829v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "P-ICL: Point In-Context Learning for Named Entity Recognition with Large\n  Language Models", "abstract": "In recent years, the rise of large language models (LLMs) has made it\npossible to directly achieve named entity recognition (NER) without any\ndemonstration samples or only using a few samples through in-context learning\n(ICL). However, standard ICL only helps LLMs understand task instructions,\nformat and input-label mapping, but neglects the particularity of the NER task\nitself. In this paper, we propose a new prompting framework P-ICL to better\nachieve NER with LLMs, in which some point entities are leveraged as the\nauxiliary information to recognize each entity type. With such significant\ninformation, the LLM can achieve entity classification more precisely. To\nobtain optimal point entities for prompting LLMs, we also proposed a point\nentity selection method based on K-Means clustering. Our extensive experiments\non some representative NER benchmarks verify the effectiveness of our proposed\nstrategies in P-ICL and point entity selection.", "published": "2024-05-08 11:01:21", "link": "http://arxiv.org/abs/2405.04960v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ADELIE: Aligning Large Language Models on Information Extraction", "abstract": "Large language models (LLMs) usually fall short on information extraction\n(IE) tasks and struggle to follow the complex instructions of IE tasks. This\nprimarily arises from LLMs not being aligned with humans, as mainstream\nalignment datasets typically do not include IE data. In this paper, we\nintroduce ADELIE (Aligning large language moDELs on Information Extraction), an\naligned LLM that effectively solves various IE tasks, including closed IE, open\nIE, and on-demand IE. We first collect and construct a high-quality alignment\ncorpus IEInstruct for IE. Then we train ADELIE_SFT using instruction tuning on\nIEInstruct. We further train ADELIE_SFT with direct preference optimization\n(DPO) objective, resulting in ADELIE_DPO. Extensive experiments on various\nheld-out IE datasets demonstrate that our models (ADELIE_SFT and ADELIE_DPO)\nachieve state-of-the-art (SoTA) performance among open-source models. We\nfurther explore the general capabilities of ADELIE, and experimental results\nreveal that their general capabilities do not exhibit a noticeable decline. We\nwill release the code, data, and models to facilitate further research.", "published": "2024-05-08 12:24:52", "link": "http://arxiv.org/abs/2405.05008v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Seeds of Stereotypes: A Large-Scale Textual Analysis of Race and Gender\n  Associations with Diseases in Online Sources", "abstract": "Background Advancements in Large Language Models (LLMs) hold transformative\npotential in healthcare, however, recent work has raised concern about the\ntendency of these models to produce outputs that display racial or gender\nbiases. Although training data is a likely source of such biases, exploration\nof disease and demographic associations in text data at scale has been limited.\n  Methods We conducted a large-scale textual analysis using a dataset\ncomprising diverse web sources, including Arxiv, Wikipedia, and Common Crawl.\nThe study analyzed the context in which various diseases are discussed\nalongside markers of race and gender. Given that LLMs are pre-trained on\nsimilar datasets, this approach allowed us to examine the potential biases that\nLLMs may learn and internalize. We compared these findings with actual\ndemographic disease prevalence as well as GPT-4 outputs in order to evaluate\nthe extent of bias representation.\n  Results Our findings indicate that demographic terms are disproportionately\nassociated with specific disease concepts in online texts. gender terms are\nprominently associated with disease concepts, while racial terms are much less\nfrequently associated. We find widespread disparities in the associations of\nspecific racial and gender terms with the 18 diseases analyzed. Most\nprominently, we see an overall significant overrepresentation of Black race\nmentions in comparison to population proportions.\n  Conclusions Our results highlight the need for critical examination and\ntransparent reporting of biases in LLM pretraining datasets. Our study suggests\nthe need to develop mitigation strategies to counteract the influence of biased\ntraining data in LLMs, particularly in sensitive domains such as healthcare.", "published": "2024-05-08 13:38:56", "link": "http://arxiv.org/abs/2405.05049v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conversational Topic Recommendation in Counseling and Psychotherapy with\n  Decision Transformer and Large Language Models", "abstract": "Given the increasing demand for mental health assistance, artificial\nintelligence (AI), particularly large language models (LLMs), may be valuable\nfor integration into automated clinical support systems. In this work, we\nleverage a decision transformer architecture for topic recommendation in\ncounseling conversations between patients and mental health professionals. The\narchitecture is utilized for offline reinforcement learning, and we extract\nstates (dialogue turn embeddings), actions (conversation topics), and rewards\n(scores measuring the alignment between patient and therapist) from previous\nturns within a conversation to train a decision transformer model. We\ndemonstrate an improvement over baseline reinforcement learning methods, and\npropose a novel system of utilizing our model's output as synthetic labels for\nfine-tuning a large language model for the same task. Although our\nimplementation based on LLaMA-2 7B has mixed results, future work can\nundoubtedly build on the design.", "published": "2024-05-08 13:55:25", "link": "http://arxiv.org/abs/2405.05060v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XAMPLER: Learning to Retrieve Cross-Lingual In-Context Examples", "abstract": "Recent studies indicate that leveraging off-the-shelf or fine-tuned\nretrievers, capable of retrieving relevant in-context examples tailored to the\ninput query, enhances few-shot in-context learning of English. However,\nadapting these methods to other languages, especially low-resource ones, poses\nchallenges due to the scarcity of cross-lingual retrievers and annotated data.\nThus, we introduce XAMPLER: Cross-Lingual Example Retrieval, a method tailored\nto tackle the challenge of cross-lingual in-context learning using only\nannotated English data. XAMPLER first trains a retriever based on Glot500, a\nmultilingual small language model, using positive and negative English examples\nconstructed from the predictions of a multilingual large language model, i.e.,\nMaLA500. Leveraging the cross-lingual capacity of the retriever, it can\ndirectly retrieve English examples as few-shot examples for in-context learning\nof target languages. Experiments on two multilingual text classification\nbenchmarks, namely SIB200 with 176 languages and MasakhaNEWS with 16 languages,\ndemonstrate that XAMPLER substantially improves the in-context learning\nperformance across languages. Our code is available at\nhttps://github.com/cisnlp/XAMPLER.", "published": "2024-05-08 15:13:33", "link": "http://arxiv.org/abs/2405.05116v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Encoder-Decoder Framework for Interactive Free Verses with Generation\n  with Controllable High-Quality Rhyming", "abstract": "Composing poetry or lyrics involves several creative factors, but a\nchallenging aspect of generation is the adherence to a more or less strict\nmetric and rhyming pattern. To address this challenge specifically, previous\nwork on the task has mainly focused on reverse language modeling, which brings\nthe critical selection of each rhyming word to the forefront of each verse. On\nthe other hand, reversing the word order requires that models be trained from\nscratch with this task-specific goal and cannot take advantage of transfer\nlearning from a Pretrained Language Model (PLM). We propose a novel fine-tuning\napproach that prepends the rhyming word at the start of each lyric, which\nallows the critical rhyming decision to be made before the model commits to the\ncontent of the lyric (as during reverse language modeling), but maintains\ncompatibility with the word order of regular PLMs as the lyric itself is still\ngenerated in left-to-right order. We conducted extensive experiments to compare\nthis fine-tuning against the current state-of-the-art strategies for rhyming,\nfinding that our approach generates more readable text and better rhyming\ncapabilities. Furthermore, we furnish a high-quality dataset in English and 12\nother languages, analyse the approach's feasibility in a multilingual context,\nprovide extensive experimental results shedding light on good and bad practices\nfor lyrics generation, and propose metrics to compare methods in the future.", "published": "2024-05-08 16:13:40", "link": "http://arxiv.org/abs/2405.05176v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "CARE-SD: Classifier-based analysis for recognizing and eliminating\n  stigmatizing and doubt marker labels in electronic health records: model\n  development and validation", "abstract": "Objective: To detect and classify features of stigmatizing and biased\nlanguage in intensive care electronic health records (EHRs) using natural\nlanguage processing techniques. Materials and Methods: We first created a\nlexicon and regular expression lists from literature-driven stem words for\nlinguistic features of stigmatizing patient labels, doubt markers, and scare\nquotes within EHRs. The lexicon was further extended using Word2Vec and GPT\n3.5, and refined through human evaluation. These lexicons were used to search\nfor matches across 18 million sentences from the de-identified Medical\nInformation Mart for Intensive Care-III (MIMIC-III) dataset. For each\nlinguistic bias feature, 1000 sentence matches were sampled, labeled by expert\nclinical and public health annotators, and used to supervised learning\nclassifiers. Results: Lexicon development from expanded literature stem-word\nlists resulted in a doubt marker lexicon containing 58 expressions, and a\nstigmatizing labels lexicon containing 127 expressions. Classifiers for doubt\nmarkers and stigmatizing labels had the highest performance, with macro\nF1-scores of .84 and .79, positive-label recall and precision values ranging\nfrom .71 to .86, and accuracies aligning closely with human annotator agreement\n(.87). Discussion: This study demonstrated the feasibility of supervised\nclassifiers in automatically identifying stigmatizing labels and doubt markers\nin medical text, and identified trends in stigmatizing language use in an EHR\nsetting. Additional labeled data may help improve lower scare quote model\nperformance. Conclusions: Classifiers developed in this study showed high model\nperformance and can be applied to identify patterns and target interventions to\nreduce stigmatizing labels and doubt markers in healthcare systems.", "published": "2024-05-08 16:40:18", "link": "http://arxiv.org/abs/2405.05204v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "You Only Cache Once: Decoder-Decoder Architectures for Language Models", "abstract": "We introduce a decoder-decoder architecture, YOCO, for large language models,\nwhich only caches key-value pairs once. It consists of two components, i.e., a\ncross-decoder stacked upon a self-decoder. The self-decoder efficiently encodes\nglobal key-value (KV) caches that are reused by the cross-decoder via\ncross-attention. The overall model behaves like a decoder-only Transformer,\nalthough YOCO only caches once. The design substantially reduces GPU memory\ndemands, yet retains global attention capability. Additionally, the computation\nflow enables prefilling to early exit without changing the final output,\nthereby significantly speeding up the prefill stage. Experimental results\ndemonstrate that YOCO achieves favorable performance compared to Transformer in\nvarious settings of scaling up model size and number of training tokens. We\nalso extend YOCO to 1M context length with near-perfect needle retrieval\naccuracy. The profiling results show that YOCO improves inference memory,\nprefill latency, and throughput by orders of magnitude across context lengths\nand model sizes. Code is available at https://aka.ms/YOCO.", "published": "2024-05-08 17:57:39", "link": "http://arxiv.org/abs/2405.05254v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Krey\u00f2l-MT: Building MT for Latin American, Caribbean and Colonial\n  African Creole Languages", "abstract": "A majority of language technologies are tailored for a small number of\nhigh-resource languages, while relatively many low-resource languages are\nneglected. One such group, Creole languages, have long been marginalized in\nacademic study, though their speakers could benefit from machine translation\n(MT). These languages are predominantly used in much of Latin America, Africa\nand the Caribbean. We present the largest cumulative dataset to date for Creole\nlanguage MT, including 14.5M unique Creole sentences with parallel translations\n-- 11.6M of which we release publicly, and the largest bitexts gathered to date\nfor 41 languages -- the first ever for 21. In addition, we provide MT models\nsupporting all 41 Creole languages in 172 translation directions. Given our\ndiverse dataset, we produce a model for Creole language MT exposed to more\ngenre diversity than ever before, which outperforms a genre-specific Creole MT\nmodel on its own benchmark for 26 of 34 translation directions.", "published": "2024-05-08 19:06:19", "link": "http://arxiv.org/abs/2405.05376v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fishing for Magikarp: Automatically Detecting Under-trained Tokens in\n  Large Language Models", "abstract": "The disconnect between tokenizer creation and model training in language\nmodels allows for specific inputs, such as the infamous SolidGoldMagikarp\ntoken, to induce unwanted model behaviour. Although such `glitch tokens',\ntokens present in the tokenizer vocabulary but that are nearly or entirely\nabsent during model training, have been observed across various models, a\nreliable method to identify and address them has been missing. We present a\ncomprehensive analysis of Large Language Model tokenizers, specifically\ntargeting this issue of detecting under-trained tokens. Through a combination\nof tokenizer analysis, model weight-based indicators, and prompting techniques,\nwe develop novel and effective methods for automatically detecting these\nproblematic tokens. Our findings demonstrate the prevalence of such tokens\nacross a diverse set of models and provide insights into improving the\nefficiency and safety of language models.", "published": "2024-05-08 20:37:56", "link": "http://arxiv.org/abs/2405.05417v2", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Mitigating Exaggerated Safety in Large Language Models", "abstract": "As the popularity of Large Language Models (LLMs) grow, combining model\nsafety with utility becomes increasingly important. The challenge is making\nsure that LLMs can recognize and decline dangerous prompts without sacrificing\ntheir ability to be helpful. The problem of \"exaggerated safety\" demonstrates\nhow difficult this can be. To reduce excessive safety behaviours -- which was\ndiscovered to be 26.1% of safe prompts being misclassified as dangerous and\nrefused -- we use a combination of XSTest dataset prompts as well as\ninteractive, contextual, and few-shot prompting to examine the decision bounds\nof LLMs such as Llama2, Gemma Command R+, and Phi-3. We find that few-shot\nprompting works best for Llama2, interactive prompting works best Gemma, and\ncontextual prompting works best for Command R+ and Phi-3. Using a combination\nof these prompting strategies, we are able to mitigate exaggerated safety\nbehaviors by an overall 92.9% across all LLMs. Our work presents a multiple\nprompting strategies to jailbreak LLMs' decision-making processes, allowing\nthem to navigate the tight line between refusing unsafe prompts and remaining\nhelpful.", "published": "2024-05-08 20:39:54", "link": "http://arxiv.org/abs/2405.05418v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BiasKG: Adversarial Knowledge Graphs to Induce Bias in Large Language\n  Models", "abstract": "Modern large language models (LLMs) have a significant amount of world\nknowledge, which enables strong performance in commonsense reasoning and\nknowledge-intensive tasks when harnessed properly. The language model can also\nlearn social biases, which has a significant potential for societal harm. There\nhave been many mitigation strategies proposed for LLM safety, but it is unclear\nhow effective they are for eliminating social biases. In this work, we propose\na new methodology for attacking language models with knowledge graph augmented\ngeneration. We refactor natural language stereotypes into a knowledge graph,\nand use adversarial attacking strategies to induce biased responses from\nseveral open- and closed-source language models. We find our method increases\nbias in all models, even those trained with safety guardrails. This\ndemonstrates the need for further research in AI safety, and further work in\nthis new adversarial space.", "published": "2024-05-08 01:51:29", "link": "http://arxiv.org/abs/2405.04756v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's\n  Disease Questions with Scientific Literature", "abstract": "Recent advancements in large language models (LLMs) have achieved promising\nperformances across various applications. Nonetheless, the ongoing challenge of\nintegrating long-tail knowledge continues to impede the seamless adoption of\nLLMs in specialized domains. In this work, we introduce DALK, a.k.a. Dynamic\nCo-Augmentation of LLMs and KG, to address this limitation and demonstrate its\nability on studying Alzheimer's Disease (AD), a specialized sub-field in\nbiomedicine and a global health priority. With a synergized framework of LLM\nand KG mutually enhancing each other, we first leverage LLM to construct an\nevolving AD-specific knowledge graph (KG) sourced from AD-related scientific\nliterature, and then we utilize a coarse-to-fine sampling method with a novel\nself-aware knowledge retrieval approach to select appropriate knowledge from\nthe KG to augment LLM inference capabilities. The experimental results,\nconducted on our constructed AD question answering (ADQA) benchmark, underscore\nthe efficacy of DALK. Additionally, we perform a series of detailed analyses\nthat can offer valuable insights and guidelines for the emerging topic of\nmutually enhancing KG and LLM. We will release the code and data at\nhttps://github.com/David-Li0406/DALK.", "published": "2024-05-08 05:38:20", "link": "http://arxiv.org/abs/2405.04819v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "APrompt4EM: Augmented Prompt Tuning for Generalized Entity Matching", "abstract": "Generalized Entity Matching (GEM), which aims at judging whether two records\nrepresented in different formats refer to the same real-world entity, is an\nessential task in data management. The prompt tuning paradigm for pre-trained\nlanguage models (PLMs), including the recent PromptEM model, effectively\naddresses the challenges of low-resource GEM in practical applications,\noffering a robust solution when labeled data is scarce. However, existing\nprompt tuning models for GEM face the challenges of prompt design and\ninformation gap. This paper introduces an augmented prompt tuning framework for\nthe challenges, which consists of two main improvements. The first is an\naugmented contextualized soft token-based prompt tuning method that extracts a\nguiding soft token benefit for the PLMs' prompt tuning, and the second is a\ncost-effective information augmentation strategy leveraging large language\nmodels (LLMs). Our approach performs well on the low-resource GEM challenges.\nExtensive experiments show promising advancements of our basic model without\ninformation augmentation over existing methods based on moderate-size PLMs\n(average 5.24%+), and our model with information augmentation achieves\ncomparable performance compared with fine-tuned LLMs, using less than 14% of\nthe API fee.", "published": "2024-05-08 05:38:56", "link": "http://arxiv.org/abs/2405.04820v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Machine Learning-based NLP for Emotion Classification on a Cholera X\n  Dataset", "abstract": "Recent social media posts on the cholera outbreak in Hammanskraal have\nhighlighted the diverse range of emotions people experienced in response to\nsuch an event. The extent of people's opinions varies greatly depending on\ntheir level of knowledge and information about the disease. The documented\nre-search about Cholera lacks investigations into the classification of\nemotions. This study aims to examine the emotions expressed in social media\nposts about Chol-era. A dataset of 23,000 posts was extracted and\npre-processed. The Python Nat-ural Language Toolkit (NLTK) sentiment analyzer\nlibrary was applied to deter-mine the emotional significance of each text.\nAdditionally, Machine Learning (ML) models were applied for emotion\nclassification, including Long short-term memory (LSTM), Logistic regression,\nDecision trees, and the Bidirectional En-coder Representations from\nTransformers (BERT) model. The results of this study demonstrated that LSTM\nachieved the highest accuracy of 75%. Emotion classification presents a\npromising tool for gaining a deeper understanding of the impact of Cholera on\nsociety. The findings of this study might contribute to the development of\neffective interventions in public health strategies.", "published": "2024-05-08 09:05:02", "link": "http://arxiv.org/abs/2405.04897v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Long Text Understanding with Knowledge Distilled from\n  Summarization Model", "abstract": "Long text understanding is important yet challenging for natural language\nprocessing. A long article or document usually contains many redundant words\nthat are not pertinent to its gist and sometimes can be regarded as noise. With\nrecent advances of abstractive summarization, we propose our \\emph{Gist\nDetector} to leverage the gist detection ability of a summarization model and\nintegrate the extracted gist into downstream models to enhance their long text\nunderstanding ability. Specifically, Gist Detector first learns the gist\ndetection knowledge distilled from a summarization model, and then produces\ngist-aware representations to augment downstream models. We evaluate our method\non three different tasks: long document classification, distantly supervised\nopen-domain question answering, and non-parallel text style transfer. The\nexperimental results show that our method can significantly improve the\nperformance of baseline models on all tasks.", "published": "2024-05-08 10:49:39", "link": "http://arxiv.org/abs/2405.04955v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "QFMTS: Generating Query-Focused Summaries over Multi-Table Inputs", "abstract": "Table summarization is a crucial task aimed at condensing information from\ntabular data into concise and comprehensible textual summaries. However,\nexisting approaches often fall short of adequately meeting users' information\nand quality requirements and tend to overlook the complexities of real-world\nqueries. In this paper, we propose a novel method to address these limitations\nby introducing query-focused multi-table summarization. Our approach, which\ncomprises a table serialization module, a summarization controller, and a large\nlanguage model (LLM), utilizes textual queries and multiple tables to generate\nquery-dependent table summaries tailored to users' information needs. To\nfacilitate research in this area, we present a comprehensive dataset\nspecifically tailored for this task, consisting of 4909 query-summary pairs,\neach associated with multiple tables. Through extensive experiments using our\ncurated dataset, we demonstrate the effectiveness of our proposed method\ncompared to baseline approaches. Our findings offer insights into the\nchallenges of complex table reasoning for precise summarization, contributing\nto the advancement of research in query-focused multi-table summarization.", "published": "2024-05-08 15:05:55", "link": "http://arxiv.org/abs/2405.05109v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Motion Capture Analysis of Verb and Adjective Types in Austrian Sign\n  Language", "abstract": "Across a number of sign languages, temporal and spatial characteristics of\ndominant hand articulation are used to express semantic and grammatical\nfeatures. In this study of Austrian Sign Language (\\\"Osterreichische\nGeb\\\"ardensprache, or \\\"OGS), motion capture data of four Deaf signers is used\nto quantitatively characterize the kinematic parameters of sign production in\nverbs and adjectives. We investigate (1) the difference in production between\nverbs involving a natural endpoint (telic verbs; e.g. arrive) and verbs lacking\nan endpoint (atelic verbs; e.g. analyze), and (2) adjective signs in\nintensified vs. non-intensified (plain) forms. Motion capture data analysis\nusing linear-mixed effects models (LME) indicates that both the endpoint\nmarking in verbs, as well as marking of intensification in adjectives, are\nexpressed by movement modulation in \\\"OGS. While the semantic distinction\nbetween verb types (telic/atelic) is marked by higher peak velocity and shorter\nduration for telic signs compared to atelic ones, the grammatical distinction\n(intensification) in adjectives is expressed by longer duration for intensified\ncompared to non-intensified adjectives. The observed individual differences of\nsigners might be interpreted as personal signing style.", "published": "2024-05-08 15:54:12", "link": "http://arxiv.org/abs/2405.05161v2", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "MIDGARD: Self-Consistency Using Minimum Description Length for\n  Structured Commonsense Reasoning", "abstract": "We study the task of conducting structured reasoning as generating a\nreasoning graph from natural language input using large language models (LLMs).\nPrevious approaches have explored various prompting schemes, yet they suffer\nfrom error propagation due to the autoregressive nature and single-pass-based\ndecoding, which lack error correction capability. Additionally, relying solely\non a single sample may result in the omission of true nodes and edges. To\ncounter this, we draw inspiration from self-consistency (SC), which involves\nsampling a diverse set of reasoning chains and taking the majority vote as the\nfinal answer. To tackle the substantial challenge of applying SC on generated\ngraphs, we propose MIDGARD (MInimum Description length Guided Aggregation of\nReasoning in Directed acyclic graph) that leverages Minimum Description Length\n(MDL)-based formulation to identify consistent properties among the different\ngraph samples generated by an LLM. This formulation helps reject properties\nthat appear in only a few samples, which are likely to be erroneous, while\nenabling the inclusion of missing elements without compromising precision. Our\nmethod demonstrates superior performance than comparisons across various\nstructured reasoning tasks, including argument structure extraction,\nexplanation graph generation, inferring dependency relations among actions for\neveryday tasks, and semantic graph generation from natural texts.", "published": "2024-05-08 16:25:42", "link": "http://arxiv.org/abs/2405.05189v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "QuaLLM: An LLM-based Framework to Extract Quantitative Insights from\n  Online Forums", "abstract": "Online discussion forums provide crucial data to understand the concerns of a\nwide range of real-world communities. However, the typical qualitative and\nquantitative methodologies used to analyze those data, such as thematic\nanalysis and topic modeling, are infeasible to scale or require significant\nhuman effort to translate outputs to human readable forms. This study\nintroduces QuaLLM, a novel LLM-based framework to analyze and extract\nquantitative insights from text data on online forums. The framework consists\nof a novel prompting and human evaluation methodology. We applied this\nframework to analyze over one million comments from two of Reddit's rideshare\nworker communities, marking the largest study of its type. We uncover\nsignificant worker concerns regarding AI and algorithmic platform decisions,\nresponding to regulatory calls about worker insights. In short, our work sets a\nnew precedent for AI-assisted quantitative data analysis to surface concerns\nfrom online forums.", "published": "2024-05-08 18:20:03", "link": "http://arxiv.org/abs/2405.05345v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Evaluating Students' Open-ended Written Responses with LLMs: Using the\n  RAG Framework for GPT-3.5, GPT-4, Claude-3, and Mistral-Large", "abstract": "Evaluating open-ended written examination responses from students is an\nessential yet time-intensive task for educators, requiring a high degree of\neffort, consistency, and precision. Recent developments in Large Language\nModels (LLMs) present a promising opportunity to balance the need for thorough\nevaluation with efficient use of educators' time. In our study, we explore the\neffectiveness of LLMs ChatGPT-3.5, ChatGPT-4, Claude-3, and Mistral-Large in\nassessing university students' open-ended answers to questions made about\nreference material they have studied. Each model was instructed to evaluate 54\nanswers repeatedly under two conditions: 10 times (10-shot) with a temperature\nsetting of 0.0 and 10 times with a temperature of 0.5, expecting a total of\n1,080 evaluations per model and 4,320 evaluations across all models. The RAG\n(Retrieval Augmented Generation) framework was used as the framework to make\nthe LLMs to process the evaluation of the answers. As of spring 2024, our\nanalysis revealed notable variations in consistency and the grading outcomes\nprovided by studied LLMs. There is a need to comprehend strengths and\nweaknesses of LLMs in educational settings for evaluating open-ended written\nresponses. Further comparative research is essential to determine the accuracy\nand cost-effectiveness of using LLMs for educational assessments.", "published": "2024-05-08 22:23:58", "link": "http://arxiv.org/abs/2405.05444v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Poser: Unmasking Alignment Faking LLMs by Manipulating Their Internals", "abstract": "Like a criminal under investigation, Large Language Models (LLMs) might\npretend to be aligned while evaluated and misbehave when they have a good\nopportunity. Can current interpretability methods catch these 'alignment\nfakers?' To answer this question, we introduce a benchmark that consists of 324\npairs of LLMs fine-tuned to select actions in role-play scenarios. One model in\neach pair is consistently benign (aligned). The other model misbehaves in\nscenarios where it is unlikely to be caught (alignment faking). The task is to\nidentify the alignment faking model using only inputs where the two models\nbehave identically. We test five detection strategies, one of which identifies\n98% of alignment-fakers.", "published": "2024-05-08 23:44:08", "link": "http://arxiv.org/abs/2405.05466v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Utilizing Large Language Models to Generate Synthetic Data to Increase\n  the Performance of BERT-Based Neural Networks", "abstract": "An important issue impacting healthcare is a lack of available experts.\nMachine learning (ML) models could resolve this by aiding in diagnosing\npatients. However, creating datasets large enough to train these models is\nexpensive. We evaluated large language models (LLMs) for data creation. Using\nAutism Spectrum Disorders (ASD), we prompted ChatGPT and GPT-Premium to\ngenerate 4,200 synthetic observations to augment existing medical data. Our\ngoal is to label behaviors corresponding to autism criteria and improve model\naccuracy with synthetic training data. We used a BERT classifier pre-trained on\nbiomedical literature to assess differences in performance between models. A\nrandom sample (N=140) from the LLM-generated data was evaluated by a clinician\nand found to contain 83% correct example-label pairs. Augmenting data increased\nrecall by 13% but decreased precision by 16%, correlating with higher quality\nand lower accuracy across pairs. Future work will analyze how different\nsynthetic data traits affect ML outcomes.", "published": "2024-05-08 03:18:12", "link": "http://arxiv.org/abs/2405.06695v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-level Shared Knowledge Guided Learning for Knowledge Graph\n  Completion", "abstract": "In the task of Knowledge Graph Completion (KGC), the existing datasets and\ntheir inherent subtasks carry a wealth of shared knowledge that can be utilized\nto enhance the representation of knowledge triplets and overall performance.\nHowever, no current studies specifically address the shared knowledge within\nKGC. To bridge this gap, we introduce a multi-level Shared Knowledge Guided\nlearning method (SKG) that operates at both the dataset and task levels. On the\ndataset level, SKG-KGC broadens the original dataset by identifying shared\nfeatures within entity sets via text summarization. On the task level, for the\nthree typical KGC subtasks - head entity prediction, relation prediction, and\ntail entity prediction - we present an innovative multi-task learning\narchitecture with dynamically adjusted loss weights. This approach allows the\nmodel to focus on more challenging and underperforming tasks, effectively\nmitigating the imbalance of knowledge sharing among subtasks. Experimental\nresults demonstrate that SKG-KGC outperforms existing text-based methods\nsignificantly on three well-known datasets, with the most notable improvement\non WN18RR.", "published": "2024-05-08 03:27:46", "link": "http://arxiv.org/abs/2405.06696v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automated Conversion of Static to Dynamic Scheduler via Natural Language", "abstract": "In this paper, we explore the potential application of Large Language Models\n(LLMs) that will automatically model constraints and generate code for dynamic\nscheduling problems given an existing static model. Static scheduling problems\nare modelled and coded by optimization experts. These models may be easily\nobsoleted as the underlying constraints may need to be fine-tuned in order to\nreflect changes in the scheduling rules. Furthermore, it may be necessary to\nturn a static model into a dynamic one in order to cope with disturbances in\nthe environment. In this paper, we propose a Retrieval-Augmented Generation\n(RAG) based LLM model to automate the process of implementing constraints for\nDynamic Scheduling (RAGDyS), without seeking help from an optimization modeling\nexpert. Our framework aims to minimize technical complexities related to\nmathematical modelling and computational workload for end-users, thereby\nallowing end-users to quickly obtain a new schedule close to the original\nschedule with changes reflected by natural language constraint descriptions.", "published": "2024-05-08 04:07:38", "link": "http://arxiv.org/abs/2405.06697v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChatSOS: Vector Database Augmented Generative Question Answering\n  Assistant in Safety Engineering", "abstract": "With the rapid advancement of natural language processing technologies,\ngenerative artificial intelligence techniques, represented by large language\nmodels (LLMs), are gaining increasing prominence and demonstrating significant\npotential for applications in safety engineering. However, fundamental LLMs\nface constraints such as limited training data coverage and unreliable\nresponses. This study develops a vector database from 117 explosion accident\nreports in China spanning 2013 to 2023, employing techniques such as corpus\nsegmenting and vector embedding. By utilizing the vector database, which\noutperforms the relational database in information retrieval quality, we\nprovide LLMs with richer, more relevant knowledge. Comparative analysis of LLMs\ndemonstrates that ChatSOS significantly enhances reliability, accuracy, and\ncomprehensiveness, improves adaptability and clarification of responses. These\nresults illustrate the effectiveness of supplementing LLMs with an external\ndatabase, highlighting their potential to handle professional queries in safety\nengineering and laying a foundation for broader applications.", "published": "2024-05-08 07:21:26", "link": "http://arxiv.org/abs/2405.06699v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Lightweight Spatial Modeling for Combinatorial Information Extraction\n  From Documents", "abstract": "Documents that consist of diverse templates and exhibit complex spatial\nstructures pose a challenge for document entity classification. We propose\nKNN-former, which incorporates a new kind of spatial bias in attention\ncalculation based on the K-nearest-neighbor (KNN) graph of document entities.\nWe limit entities' attention only to their local radius defined by the KNN\ngraph. We also use combinatorial matching to address the one-to-one mapping\nproperty that exists in many documents, where one field has only one\ncorresponding entity. Moreover, our method is highly parameter-efficient\ncompared to existing approaches in terms of the number of trainable parameters.\nDespite this, experiments across various datasets show our method outperforms\nbaselines in most entity types. Many real-world documents exhibit combinatorial\nproperties which can be leveraged as inductive biases to improve extraction\naccuracy, but existing datasets do not cover these documents. To facilitate\nfuture research into these types of documents, we release a new ID document\ndataset that covers diverse templates and languages. We also release enhanced\nannotations for an existing dataset.", "published": "2024-05-08 10:10:38", "link": "http://arxiv.org/abs/2405.06701v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Malayalam Sign Language Identification using Finetuned YOLOv8 and\n  Computer Vision Techniques", "abstract": "Technological advancements and innovations are advancing our daily life in\nall the ways possible but there is a larger section of society who are deprived\nof accessing the benefits due to their physical inabilities. To reap the real\nbenefits and make it accessible to society, these talented and gifted people\nshould also use such innovations without any hurdles. Many applications\ndeveloped these days address these challenges, but localized communities and\nother constrained linguistic groups may find it difficult to use them.\nMalayalam, a Dravidian language spoken in the Indian state of Kerala is one of\nthe twenty-two scheduled languages in India. Recent years have witnessed a\nsurge in the development of systems and tools in Malayalam, addressing the\nneeds of Kerala, but many of them are not empathetically designed to cater to\nthe needs of hearing-impaired people. One of the major challenges is the\nlimited or no availability of sign language data for the Malayalam language and\nsufficient efforts are not made in this direction. In this connection, this\npaper proposes an approach for sign language identification for the Malayalam\nlanguage using advanced deep learning and computer vision techniques. We start\nby developing a labeled dataset for Malayalam letters and for the\nidentification we use advanced deep learning techniques such as YOLOv8 and\ncomputer vision. Experimental results show that the identification accuracy is\ncomparable to other sign language identification systems and other researchers\nin sign language identification can use the model as a baseline to develop\nadvanced models.", "published": "2024-05-08 11:37:54", "link": "http://arxiv.org/abs/2405.06702v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Honeyfile Camouflage: Hiding Fake Files in Plain Sight", "abstract": "Honeyfiles are a particularly useful type of honeypot: fake files deployed to\ndetect and infer information from malicious behaviour. This paper considers the\nchallenge of naming honeyfiles so they are camouflaged when placed amongst real\nfiles in a file system. Based on cosine distances in semantic vector spaces, we\ndevelop two metrics for filename camouflage: one based on simple averaging and\none on clustering with mixture fitting. We evaluate and compare the metrics,\nshowing that both perform well on a publicly available GitHub software\nrepository dataset.", "published": "2024-05-08 02:01:17", "link": "http://arxiv.org/abs/2405.04758v2", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Zero-shot LLM-guided Counterfactual Generation: A Case Study on NLP\n  Model Evaluation", "abstract": "With the development and proliferation of large, complex, black-box models\nfor solving many natural language processing (NLP) tasks, there is also an\nincreasing necessity of methods to stress-test these models and provide some\ndegree of interpretability or explainability. While counterfactual examples are\nuseful in this regard, automated generation of counterfactuals is a data and\nresource intensive process. such methods depend on models such as pre-trained\nlanguage models that are then fine-tuned on auxiliary, often task-specific\ndatasets, that may be infeasible to build in practice, especially for new tasks\nand data domains. Therefore, in this work we explore the possibility of\nleveraging large language models (LLMs) for zero-shot counterfactual generation\nin order to stress-test NLP models. We propose a structured pipeline to\nfacilitate this generation, and we hypothesize that the instruction-following\nand textual understanding capabilities of recent LLMs can be effectively\nleveraged for generating high quality counterfactuals in a zero-shot manner,\nwithout requiring any training or fine-tuning. Through comprehensive\nexperiments on a variety of propreitary and open-source LLMs, along with\nvarious downstream tasks in NLP, we explore the efficacy of LLMs as zero-shot\ncounterfactual generators in evaluating and explaining black-box NLP models.", "published": "2024-05-08 03:57:45", "link": "http://arxiv.org/abs/2405.04793v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Logical Negation Augmenting and Debiasing for Prompt-based Methods", "abstract": "Prompt-based methods have gained increasing attention on NLP and shown\nvalidity on many downstream tasks. Many works have focused on mining these\nmethods' potential for knowledge extraction, but few explore their ability to\nmake logical reasoning. In this work, we focus on the effectiveness of the\nprompt-based methods on first-order logical reasoning and find that the\nbottleneck lies in logical negation. Based on our analysis, logical negation\ntends to result in spurious correlations to negative answers, while\npropositions without logical negation correlate to positive answers. To solve\nthe problem, we propose a simple but effective method, Negation Augmenting and\nNegation Debiasing (NAND), which introduces negative propositions to\nprompt-based methods without updating parameters. Specifically, these negative\npropositions can counteract spurious correlations by providing \"not\" for all\ninstances so that models cannot make decisions only by whether expressions\ncontain a logical negation. Experiments on three datasets show that NAND not\nonly solves the problem of calibrating logical negation but also significantly\nenhances prompt-based methods of logical reasoning without model retraining.", "published": "2024-05-08 08:05:47", "link": "http://arxiv.org/abs/2405.04872v1", "categories": ["cs.CL", "cs.AI", "cs.LO"], "primary_category": "cs.CL"}
{"title": "VisionGraph: Leveraging Large Multimodal Models for Graph Theory\n  Problems in Visual Context", "abstract": "Large Multimodal Models (LMMs) have achieved impressive success in visual\nunderstanding and reasoning, remarkably improving the performance of\nmathematical reasoning in a visual context. Yet, a challenging type of visual\nmath lies in the multimodal graph theory problem, which demands that LMMs\nunderstand the graphical structures accurately and perform multi-step reasoning\non the visual graph. Additionally, exploring multimodal graph theory problems\nwill lead to more effective strategies in fields like biology, transportation,\nand robotics planning. To step forward in this direction, we are the first to\ndesign a benchmark named VisionGraph, used to explore the capabilities of\nadvanced LMMs in solving multimodal graph theory problems. It encompasses eight\ncomplex graph problem tasks, from connectivity to shortest path problems.\nSubsequently, we present a Description-Program-Reasoning (DPR) chain to enhance\nthe logical accuracy of reasoning processes through graphical structure\ndescription generation and algorithm-aware multi-step reasoning. Our extensive\nstudy shows that 1) GPT-4V outperforms Gemini Pro in multi-step graph\nreasoning; 2) All LMMs exhibit inferior perception accuracy for graphical\nstructures, whether in zero/few-shot settings or with supervised fine-tuning\n(SFT), which further affects problem-solving performance; 3) DPR significantly\nimproves the multi-step graph reasoning capabilities of LMMs and the GPT-4V\n(DPR) agent achieves SOTA performance.", "published": "2024-05-08 10:42:48", "link": "http://arxiv.org/abs/2405.04950v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "AirGapAgent: Protecting Privacy-Conscious Conversational Agents", "abstract": "The growing use of large language model (LLM)-based conversational agents to\nmanage sensitive user data raises significant privacy concerns. While these\nagents excel at understanding and acting on context, this capability can be\nexploited by malicious actors. We introduce a novel threat model where\nadversarial third-party apps manipulate the context of interaction to trick\nLLM-based agents into revealing private information not relevant to the task at\nhand.\n  Grounded in the framework of contextual integrity, we introduce AirGapAgent,\na privacy-conscious agent designed to prevent unintended data leakage by\nrestricting the agent's access to only the data necessary for a specific task.\nExtensive experiments using Gemini, GPT, and Mistral models as agents validate\nour approach's effectiveness in mitigating this form of context hijacking while\nmaintaining core agent functionality. For example, we show that a single-query\ncontext hijacking attack on a Gemini Ultra agent reduces its ability to protect\nuser data from 94% to 45%, while an AirGapAgent achieves 97% protection,\nrendering the same attack ineffective.", "published": "2024-05-08 16:12:45", "link": "http://arxiv.org/abs/2405.05175v2", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "LLMs with Personalities in Multi-issue Negotiation Games", "abstract": "Powered by large language models (LLMs), AI agents have become capable of\nmany human tasks. Using the most canonical definitions of the Big Five\npersonality, we measure the ability of LLMs to negotiate within a\ngame-theoretical framework, as well as methodological challenges to measuring\nnotions of fairness and risk. Simulations (n=1,500) for both single-issue and\nmulti-issue negotiation reveal increase in domain complexity with asymmetric\nissue valuations improve agreement rates but decrease surplus from aggressive\nnegotiation. Through gradient-boosted regression and Shapley explainers, we\nfind high openness, conscientiousness, and neuroticism are associated with fair\ntendencies; low agreeableness and low openness are associated with rational\ntendencies. Low conscientiousness is associated with high toxicity. These\nresults indicate that LLMs may have built-in guardrails that default to fair\nbehavior, but can be \"jail broken\" to exploit agreeable opponents. We also\noffer pragmatic insight in how negotiation bots can be designed, and a\nframework of assessing negotiation behavior based on game theory and\ncomputational social science.", "published": "2024-05-08 17:51:53", "link": "http://arxiv.org/abs/2405.05248v2", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Open Source Language Models Can Provide Feedback: Evaluating LLMs'\n  Ability to Help Students Using GPT-4-As-A-Judge", "abstract": "Large language models (LLMs) have shown great potential for the automatic\ngeneration of feedback in a wide range of computing contexts. However, concerns\nhave been voiced around the privacy and ethical implications of sending student\nwork to proprietary models. This has sparked considerable interest in the use\nof open source LLMs in education, but the quality of the feedback that such\nopen models can produce remains understudied. This is a concern as providing\nflawed or misleading generated feedback could be detrimental to student\nlearning. Inspired by recent work that has utilised very powerful LLMs, such as\nGPT-4, to evaluate the outputs produced by less powerful models, we conduct an\nautomated analysis of the quality of the feedback produced by several open\nsource models using a dataset from an introductory programming course. First,\nwe investigate the viability of employing GPT-4 as an automated evaluator by\ncomparing its evaluations with those of a human expert. We observe that GPT-4\ndemonstrates a bias toward positively rating feedback while exhibiting moderate\nagreement with human raters, showcasing its potential as a feedback evaluator.\nSecond, we explore the quality of feedback generated by several leading\nopen-source LLMs by using GPT-4 to evaluate the feedback. We find that some\nmodels offer competitive performance with popular proprietary LLMs, such as\nChatGPT, indicating opportunities for their responsible use in educational\nsettings.", "published": "2024-05-08 17:57:39", "link": "http://arxiv.org/abs/2405.05253v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "KV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache\n  Generation", "abstract": "Large Language Model or LLM inference has two phases, the prompt (or prefill)\nphase to output the first token and the extension (or decoding) phase to the\ngenerate subsequent tokens. In this work, we propose an efficient\nparallelization scheme, KV-Runahead to accelerate the prompt phase. The key\nobservation is that the extension phase generates tokens faster than the prompt\nphase because of key-value cache (KV-cache). Hence, KV-Runahead parallelizes\nthe prompt phase by orchestrating multiple processes to populate the KV-cache\nand minimizes the time-to-first-token (TTFT). Dual-purposing the KV-cache\nscheme has two main benefits. First, since KV-cache is designed to leverage the\ncausal attention map, we minimize computation and computation automatically.\nSecond, since it already exists for the extension phase, KV-Runahead is easy to\nimplement. We further propose context-level load-balancing to handle uneven\nKV-cache generation (due to the causal attention) and to optimize TTFT.\nCompared with an existing parallelization scheme such as tensor or sequential\nparallelization where keys and values are locally generated and exchanged via\nall-gather collectives, our experimental results demonstrate that KV-Runahead\ncan offer over 1.4x and 1.6x speedups for Llama 7B and Falcon 7B respectively.", "published": "2024-05-08 18:03:22", "link": "http://arxiv.org/abs/2405.05329v2", "categories": ["cs.DC", "cs.AI", "cs.CL"], "primary_category": "cs.DC"}
{"title": "Benchmarking Educational Program Repair", "abstract": "The emergence of large language models (LLMs) has sparked enormous interest\ndue to their potential application across a range of educational tasks. For\nexample, recent work in programming education has used LLMs to generate\nlearning resources, improve error messages, and provide feedback on code.\nHowever, one factor that limits progress within the field is that much of the\nresearch uses bespoke datasets and different evaluation metrics, making direct\ncomparisons between results unreliable. Thus, there is a pressing need for\nstandardization and benchmarks that facilitate the equitable comparison of\ncompeting approaches. One task where LLMs show great promise is program repair,\nwhich can be used to provide debugging support and next-step hints to students.\nIn this article, we propose a novel educational program repair benchmark. We\ncurate two high-quality publicly available programming datasets, present a\nunified evaluation procedure introducing a novel evaluation metric rouge@k for\napproximating the quality of repairs, and evaluate a set of five recent models\nto establish baseline performance.", "published": "2024-05-08 18:23:59", "link": "http://arxiv.org/abs/2405.05347v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.SE"}
{"title": "The Effect of Model Size on LLM Post-hoc Explainability via LIME", "abstract": "Large language models (LLMs) are becoming bigger to boost performance.\nHowever, little is known about how explainability is affected by this trend.\nThis work explores LIME explanations for DeBERTaV3 models of four different\nsizes on natural language inference (NLI) and zero-shot classification (ZSC)\ntasks. We evaluate the explanations based on their faithfulness to the models'\ninternal decision processes and their plausibility, i.e. their agreement with\nhuman explanations. The key finding is that increased model size does not\ncorrelate with plausibility despite improved model performance, suggesting a\nmisalignment between the LIME explanations and the models' internal processes\nas model size increases. Our results further suggest limitations regarding\nfaithfulness metrics in NLI contexts.", "published": "2024-05-08 18:27:20", "link": "http://arxiv.org/abs/2405.05348v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Arctic-Embed: Scalable, Efficient, and Accurate Text Embedding Models", "abstract": "This report describes the training dataset creation and recipe behind the\nfamily of \\texttt{arctic-embed} text embedding models (a set of five models\nranging from 22 to 334 million parameters with weights open-sourced under an\nApache-2 license). At the time of their release, each model achieved\nstate-of-the-art retrieval accuracy for models of their size on the MTEB\nRetrieval leaderboard, with the largest model, arctic-embed-l outperforming\nclosed source embedding models such as Cohere's embed-v3 and Open AI's\ntext-embed-3-large. In addition to the details of our training recipe, we have\nprovided several informative ablation studies, which we believe are the cause\nof our model performance.", "published": "2024-05-08 19:05:18", "link": "http://arxiv.org/abs/2405.05374v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Interpretability Needs a New Paradigm", "abstract": "Interpretability is the study of explaining models in understandable terms to\nhumans. At present, interpretability is divided into two paradigms: the\nintrinsic paradigm, which believes that only models designed to be explained\ncan be explained, and the post-hoc paradigm, which believes that black-box\nmodels can be explained. At the core of this debate is how each paradigm\nensures its explanations are faithful, i.e., true to the model's behavior. This\nis important, as false but convincing explanations lead to unsupported\nconfidence in artificial intelligence (AI), which can be dangerous. This\npaper's position is that we should think about new paradigms while staying\nvigilant regarding faithfulness. First, by examining the history of paradigms\nin science, we see that paradigms are constantly evolving. Then, by examining\nthe current paradigms, we can understand their underlying beliefs, the value\nthey bring, and their limitations. Finally, this paper presents 3 emerging\nparadigms for interpretability. The first paradigm designs models such that\nfaithfulness can be easily measured. Another optimizes models such that\nexplanations become faithful. The last paradigm proposes to develop models that\nproduce both a prediction and an explanation.", "published": "2024-05-08 19:31:06", "link": "http://arxiv.org/abs/2405.05386v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Analysis and prevention of AI-based phishing email attacks", "abstract": "Phishing email attacks are among the most common and most harmful\ncybersecurity attacks. With the emergence of generative AI, phishing attacks\ncan be based on emails generated automatically, making it more difficult to\ndetect them. That is, instead of a single email format sent to a large number\nof recipients, generative AI can be used to send each potential victim a\ndifferent email, making it more difficult for cybersecurity systems to identify\nthe scam email before it reaches the recipient. Here we describe a corpus of\nAI-generated phishing emails. We also use different machine learning tools to\ntest the ability of automatic text analysis to identify AI-generated phishing\nemails. The results are encouraging, and show that machine learning tools can\nidentify an AI-generated phishing email with high accuracy compared to regular\nemails or human-generated scam email. By applying descriptive analytic, the\nspecific differences between AI-generated emails and manually crafted scam\nemails are profiled, and show that AI-generated emails are different in their\nstyle from human-generated phishing email scams. Therefore, automatic\nidentification tools can be used as a warning for the user. The paper also\ndescribes the corpus of AI-generated phishing emails that is made open to the\npublic, and can be used for consequent studies. While the ability of machine\nlearning to detect AI-generated phishing email is encouraging, AI-generated\nphishing emails are different from regular phishing emails, and therefore it is\nimportant to train machine learning systems also with AI-generated emails in\norder to repel future phishing attacks that are powered by generative AI.", "published": "2024-05-08 21:40:49", "link": "http://arxiv.org/abs/2405.05435v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Vidur: A Large-Scale Simulation Framework For LLM Inference", "abstract": "Optimizing the deployment of Large language models (LLMs) is expensive today\nsince it requires experimentally running an application workload against an LLM\nimplementation while exploring large configuration space formed by system knobs\nsuch as parallelization strategies, batching techniques, and scheduling\npolicies. To address this challenge, we present Vidur - a large-scale,\nhigh-fidelity, easily-extensible simulation framework for LLM inference\nperformance. Vidur models the performance of LLM operators using a combination\nof experimental profiling and predictive modeling, and evaluates the end-to-end\ninference performance for different workloads by estimating several metrics of\ninterest such as latency and throughput. We validate the fidelity of Vidur on\nseveral LLMs and show that it estimates inference latency with less than 9%\nerror across the range. Further, we present Vidur-Search, a configuration\nsearch tool that helps optimize LLM deployment. Vidur-Search uses Vidur to\nautomatically identify the most cost-effective deployment configuration that\nmeets application performance constraints. For example, Vidur-Search finds the\nbest deployment configuration for LLaMA2-70B in one hour on a CPU machine, in\ncontrast to a deployment-based exploration which would require 42K GPU hours -\ncosting ~218K dollars. Source code for Vidur is available at\nhttps://github.com/microsoft/vidur.", "published": "2024-05-08 23:42:13", "link": "http://arxiv.org/abs/2405.05465v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Interpretable Cross-Examination Technique (ICE-T): Using highly\n  informative features to boost LLM performance", "abstract": "In this paper, we introduce the Interpretable Cross-Examination Technique\n(ICE-T), a novel approach that leverages structured multi-prompt techniques\nwith Large Language Models (LLMs) to improve classification performance over\nzero-shot and few-shot methods. In domains where interpretability is crucial,\nsuch as medicine and law, standard models often fall short due to their\n\"black-box\" nature. ICE-T addresses these limitations by using a series of\ngenerated prompts that allow an LLM to approach the problem from multiple\ndirections. The responses from the LLM are then converted into numerical\nfeature vectors and processed by a traditional classifier. This method not only\nmaintains high interpretability but also allows for smaller, less capable\nmodels to achieve or exceed the performance of larger, more advanced models\nunder zero-shot conditions. We demonstrate the effectiveness of ICE-T across a\ndiverse set of data sources, including medical records and legal documents,\nconsistently surpassing the zero-shot baseline in terms of classification\nmetrics such as F1 scores. Our results indicate that ICE-T can be used for\nimproving both the performance and transparency of AI applications in complex\ndecision-making environments.", "published": "2024-05-08 19:20:34", "link": "http://arxiv.org/abs/2405.06703v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "\"They are uncultured\": Unveiling Covert Harms and Social Threats in LLM\n  Generated Conversations", "abstract": "Large language models (LLMs) have emerged as an integral part of modern\nsocieties, powering user-facing applications such as personal assistants and\nenterprise applications like recruitment tools. Despite their utility, research\nindicates that LLMs perpetuate systemic biases. Yet, prior works on LLM harms\npredominantly focus on Western concepts like race and gender, often overlooking\ncultural concepts from other parts of the world. Additionally, these studies\ntypically investigate \"harm\" as a singular dimension, ignoring the various and\nsubtle forms in which harms manifest. To address this gap, we introduce the\nCovert Harms and Social Threats (CHAST), a set of seven metrics grounded in\nsocial science literature. We utilize evaluation models aligned with human\nassessments to examine the presence of covert harms in LLM-generated\nconversations, particularly in the context of recruitment. Our experiments\nreveal that seven out of the eight LLMs included in this study generated\nconversations riddled with CHAST, characterized by malign views expressed in\nseemingly neutral language unlikely to be detected by existing methods.\nNotably, these LLMs manifested more extreme views and opinions when dealing\nwith non-Western concepts like caste, compared to Western ones such as race.", "published": "2024-05-08 19:08:45", "link": "http://arxiv.org/abs/2405.05378v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Harmonizing Program Induction with Rate-Distortion Theory", "abstract": "Many aspects of human learning have been proposed as a process of\nconstructing mental programs: from acquiring symbolic number representations to\nintuitive theories about the world. In parallel, there is a long-tradition of\nusing information processing to model human cognition through Rate Distortion\nTheory (RDT). Yet, it is still poorly understood how to apply RDT when mental\nrepresentations take the form of programs. In this work, we adapt RDT by\nproposing a three way trade-off among rate (description length), distortion\n(error), and computational costs (search budget). We use simulations on a\nmelody task to study the implications of this trade-off, and show that\nconstructing a shared program library across tasks provides global benefits.\nHowever, this comes at the cost of sensitivity to curricula, which is also\ncharacteristic of human learners. Finally, we use methods from partial\ninformation decomposition to generate training curricula that induce more\neffective libraries and better generalization.", "published": "2024-05-08 10:03:50", "link": "http://arxiv.org/abs/2405.05294v1", "categories": ["cs.HC", "cs.CL", "cs.IT", "cs.LG", "cs.SC", "math.IT", "stat.ML"], "primary_category": "cs.HC"}
{"title": "HILCodec: High-Fidelity and Lightweight Neural Audio Codec", "abstract": "The recent advancement of end-to-end neural audio codecs enables compressing\naudio at very low bitrates while reconstructing the output audio with high\nfidelity. Nonetheless, such improvements often come at the cost of increased\nmodel complexity. In this paper, we identify and address the problems of\nexisting neural audio codecs. We show that the performance of the SEANet-based\ncodec does not increase consistently as the network depth increases. We analyze\nthe root cause of such a phenomenon and suggest a variance-constrained design.\nAlso, we reveal various distortions in previous waveform domain discriminators\nand propose a novel distortion-free discriminator. The resulting model,\nHILCodec, is a real-time streaming audio codec that demonstrates\nstate-of-the-art quality across various bitrates and audio types.", "published": "2024-05-08 01:40:13", "link": "http://arxiv.org/abs/2405.04752v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Codecfake Dataset and Countermeasures for the Universally Detection\n  of Deepfake Audio", "abstract": "With the proliferation of Audio Language Model (ALM) based deepfake audio,\nthere is an urgent need for generalized detection methods. ALM-based deepfake\naudio currently exhibits widespread, high deception, and type versatility,\nposing a significant challenge to current audio deepfake detection (ADD) models\ntrained solely on vocoded data. To effectively detect ALM-based deepfake audio,\nwe focus on the mechanism of the ALM-based audio generation method, the\nconversion from neural codec to waveform. We initially constructed the\nCodecfake dataset, an open-source, large-scale collection comprising over 1\nmillion audio samples in both English and Chinese, focus on ALM-based audio\ndetection. As countermeasure, to achieve universal detection of deepfake audio\nand tackle domain ascent bias issue of original sharpness aware minimization\n(SAM), we propose the CSAM strategy to learn a domain balanced and generalized\nminima. In our experiments, we first demonstrate that ADD model training with\nthe Codecfake dataset can effectively detects ALM-based audio. Furthermore, our\nproposed generalization countermeasure yields the lowest average equal error\nrate (EER) of 0.616% across all test conditions compared to baseline models.\nThe dataset and associated code are available online.", "published": "2024-05-08 08:28:40", "link": "http://arxiv.org/abs/2405.04880v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An LSTM-Based Chord Generation System Using Chroma Histogram\n  Representations", "abstract": "This paper proposes a system for chord generation to monophonic symbolic\nmelodies using an LSTM-based model trained on chroma histogram representations\nof chords. Chroma representations promise more harmonically rich generation\nthan chord label-based approaches, whilst maintaining a small number of\ndimensions in the dataset. This system is shown to be suitable for limited\nreal-time use. While it does not meet the state-of-the-art for coherent\nlong-term generation, it does show diatonic generation with cadential chord\nrelationships. The need for further study into chroma histograms as an\nextracted feature in chord generation tasks is highlighted.", "published": "2024-05-08 17:36:29", "link": "http://arxiv.org/abs/2405.05240v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SVDD Challenge 2024: A Singing Voice Deepfake Detection Challenge\n  Evaluation Plan", "abstract": "The rapid advancement of AI-generated singing voices, which now closely mimic\nnatural human singing and align seamlessly with musical scores, has led to\nheightened concerns for artists and the music industry. Unlike spoken voice,\nsinging voice presents unique challenges due to its musical nature and the\npresence of strong background music, making singing voice deepfake detection\n(SVDD) a specialized field requiring focused attention. To promote SVDD\nresearch, we recently proposed the \"SVDD Challenge,\" the very first research\nchallenge focusing on SVDD for lab-controlled and in-the-wild bonafide and\ndeepfake singing voice recordings. The challenge will be held in conjunction\nwith the 2024 IEEE Spoken Language Technology Workshop (SLT 2024).", "published": "2024-05-08 17:40:12", "link": "http://arxiv.org/abs/2405.05244v1", "categories": ["eess.AS", "cs.AI", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AFEN: Respiratory Disease Classification using Ensemble Learning", "abstract": "We present AFEN (Audio Feature Ensemble Learning), a model that leverages\nConvolutional Neural Networks (CNN) and XGBoost in an ensemble learning fashion\nto perform state-of-the-art audio classification for a range of respiratory\ndiseases. We use a meticulously selected mix of audio features which provide\nthe salient attributes of the data and allow for accurate classification. The\nextracted features are then used as an input to two separate model classifiers\n1) a multi-feature CNN classifier and 2) an XGBoost Classifier. The outputs of\nthe two models are then fused with the use of soft voting. Thus, by exploiting\nensemble learning, we achieve increased robustness and accuracy. We evaluate\nthe performance of the model on a database of 920 respiratory sounds, which\nundergoes data augmentation techniques to increase the diversity of the data\nand generalizability of the model. We empirically verify that AFEN sets a new\nstate-of-the-art using Precision and Recall as metrics, while decreasing\ntraining time by 60%.", "published": "2024-05-08 23:50:54", "link": "http://arxiv.org/abs/2405.05467v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
