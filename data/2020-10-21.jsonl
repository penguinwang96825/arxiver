{"title": "An Investigation of the Relation Between Grapheme Embeddings and\n  Pronunciation for Tacotron-based Systems", "abstract": "End-to-end models, particularly Tacotron-based ones, are currently a popular\nsolution for text-to-speech synthesis. They allow the production of\nhigh-quality synthesized speech with little to no text preprocessing. Indeed,\nthey can be trained using either graphemes or phonemes as input directly.\nHowever, in the case of grapheme inputs, little is known concerning the\nrelation between the underlying representations learned by the model and word\npronunciations. This work investigates this relation in the case of a Tacotron\nmodel trained on French graphemes. Our analysis shows that grapheme embeddings\nare related to phoneme information despite no such information being present\nduring training. Thanks to this property, we show that grapheme embeddings\nlearned by Tacotron models can be useful for tasks such as grapheme-to-phoneme\nconversion and control of the pronunciation in synthetic speech.", "published": "2020-10-21 00:58:29", "link": "http://arxiv.org/abs/2010.10694v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Unit Transformers for Neural Machine Translation", "abstract": "Transformer models achieve remarkable success in Neural Machine Translation.\nMany efforts have been devoted to deepening the Transformer by stacking several\nunits (i.e., a combination of Multihead Attentions and FFN) in a cascade, while\nthe investigation over multiple parallel units draws little attention. In this\npaper, we propose the Multi-Unit Transformers (MUTE), which aim to promote the\nexpressiveness of the Transformer by introducing diverse and complementary\nunits. Specifically, we use several parallel units and show that modeling with\nmultiple units improves model performance and introduces diversity. Further, to\nbetter leverage the advantage of the multi-unit setting, we design biased\nmodule and sequential dependency that guide and encourage complementariness\namong different units. Experimental results on three machine translation tasks,\nthe NIST Chinese-to-English, WMT'14 English-to-German and WMT'18\nChinese-to-English, show that the MUTE models significantly outperform the\nTransformer-Base, by up to +1.52, +1.90 and +1.10 BLEU points, with only a mild\ndrop in inference speed (about 3.1%). In addition, our methods also surpass the\nTransformer-Big model, with only 54\\% of its parameters. These results\ndemonstrate the effectiveness of the MUTE, as well as its efficiency in both\nthe inference process and parameter usage.", "published": "2020-10-21 03:41:49", "link": "http://arxiv.org/abs/2010.10743v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RECONSIDER: Re-Ranking using Span-Focused Cross-Attention for Open\n  Domain Question Answering", "abstract": "State-of-the-art Machine Reading Comprehension (MRC) models for Open-domain\nQuestion Answering (QA) are typically trained for span selection using\ndistantly supervised positive examples and heuristically retrieved negative\nexamples. This training scheme possibly explains empirical observations that\nthese models achieve a high recall amongst their top few predictions, but a low\noverall accuracy, motivating the need for answer re-ranking. We develop a\nsimple and effective re-ranking approach (RECONSIDER) for span-extraction\ntasks, that improves upon the performance of large pre-trained MRC models.\nRECONSIDER is trained on positive and negative examples extracted from high\nconfidence predictions of MRC models, and uses in-passage span annotations to\nperform span-focused re-ranking over a smaller candidate set. As a result,\nRECONSIDER learns to eliminate close false positive passages, and achieves a\nnew state of the art on four QA tasks, including 45.5% Exact Match accuracy on\nNatural Questions with real user questions, and 61.7% on TriviaQA.", "published": "2020-10-21 04:28:42", "link": "http://arxiv.org/abs/2010.10757v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quasi Error-free Text Classification and Authorship Recognition in a\n  large Corpus of English Literature based on a Novel Feature Set", "abstract": "The Gutenberg Literary English Corpus (GLEC) provides a rich source of\ntextual data for research in digital humanities, computational linguistics or\nneurocognitive poetics. However, so far only a small subcorpus, the Gutenberg\nEnglish Poetry Corpus, has been submitted to quantitative text analyses\nproviding predictions for scientific studies of literature. Here we show that\nin the entire GLEC quasi error-free text classification and authorship\nrecognition is possible with a method using the same set of five style and five\ncontent features, computed via style and sentiment analysis, in both tasks. Our\nresults identify two standard and two novel features (i.e., type-token ratio,\nfrequency, sonority score, surprise) as most diagnostic in these tasks. By\nproviding a simple tool applicable to both short poems and long novels\ngenerating quantitative predictions about features that co-determe the\ncognitive and affective processing of specific text categories or authors, our\ndata pave the way for many future computational and empirical studies of\nliterature or experiments in reading psychology.", "published": "2020-10-21 07:39:55", "link": "http://arxiv.org/abs/2010.10801v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "STN4DST: A Scalable Dialogue State Tracking based on Slot Tagging\n  Navigation", "abstract": "Scalability for handling unknown slot values is a important problem in\ndialogue state tracking (DST). As far as we know, previous scalable DST\napproaches generally rely on either the candidate generation from slot tagging\noutput or the span extraction in dialogue context. However, the candidate\ngeneration based DST often suffers from error propagation due to its pipelined\ntwo-stage process; meanwhile span extraction based DST has the risk of\ngenerating invalid spans in the lack of semantic constraints between start and\nend position pointers. To tackle the above drawbacks, in this paper, we propose\na novel scalable dialogue state tracking method based on slot tagging\nnavigation, which implements an end-to-end single-step pointer to locate and\nextract slot value quickly and accurately by the joint learning of slot tagging\nand slot value position prediction in the dialogue context, especially for\nunknown slot values. Extensive experiments over several benchmark datasets show\nthat the proposed model performs better than state-of-the-art baselines\ngreatly.", "published": "2020-10-21 08:09:20", "link": "http://arxiv.org/abs/2010.10811v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Contextual Affective Analysis of LGBT People Portrayals in\n  Wikipedia", "abstract": "Specific lexical choices in narrative text reflect both the writer's\nattitudes towards people in the narrative and influence the audience's\nreactions. Prior work has examined descriptions of people in English using\ncontextual affective analysis, a natural language processing (NLP) technique\nthat seeks to analyze how people are portrayed along dimensions of power,\nagency, and sentiment. Our work presents an extension of this methodology to\nmultilingual settings, which is enabled by a new corpus that we collect and a\nnew multilingual model. We additionally show how word connotations differ\nacross languages and cultures, highlighting the difficulty of generalizing\nexisting English datasets and methods. We then demonstrate the usefulness of\nour method by analyzing Wikipedia biography pages of members of the LGBT\ncommunity across three languages: English, Russian, and Spanish. Our results\nshow systematic differences in how the LGBT community is portrayed across\nlanguages, surfacing cultural differences in narratives and signs of social\nbiases. Practically, this model can be used to identify Wikipedia articles for\nfurther manual analysis -- articles that might contain content gaps or an\nimbalanced representation of particular social groups.", "published": "2020-10-21 08:27:36", "link": "http://arxiv.org/abs/2010.10820v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KnowDis: Knowledge Enhanced Data Augmentation for Event Causality\n  Detection via Distant Supervision", "abstract": "Modern models of event causality detection (ECD) are mainly based on\nsupervised learning from small hand-labeled corpora. However, hand-labeled\ntraining data is expensive to produce, low coverage of causal expressions and\nlimited in size, which makes supervised methods hard to detect causal relations\nbetween events. To solve this data lacking problem, we investigate a data\naugmentation framework for ECD, dubbed as Knowledge Enhanced Distant Data\nAugmentation (KnowDis). Experimental results on two benchmark datasets\nEventStoryLine corpus and Causal-TimeBank show that 1) KnowDis can augment\navailable training data assisted with the lexical and causal commonsense\nknowledge for ECD via distant supervision, and 2) our method outperforms\nprevious methods by a large margin assisted with automatically labeled training\ndata.", "published": "2020-10-21 08:44:54", "link": "http://arxiv.org/abs/2010.10833v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TMT: A Transformer-based Modal Translator for Improving Multimodal\n  Sequence Representations in Audio Visual Scene-aware Dialog", "abstract": "Audio Visual Scene-aware Dialog (AVSD) is a task to generate responses when\ndiscussing about a given video. The previous state-of-the-art model shows\nsuperior performance for this task using Transformer-based architecture.\nHowever, there remain some limitations in learning better representation of\nmodalities. Inspired by Neural Machine Translation (NMT), we propose the\nTransformer-based Modal Translator (TMT) to learn the representations of the\nsource modal sequence by translating the source modal sequence to the related\ntarget modal sequence in a supervised manner. Based on Multimodal Transformer\nNetworks (MTN), we apply TMT to video and dialog, proposing MTN-TMT for the\nvideo-grounded dialog system. On the AVSD track of the Dialog System Technology\nChallenge 7, MTN-TMT outperforms the MTN and other submission models in both\nVideo and Text task and Text Only task. Compared with MTN, MTN-TMT improves all\nmetrics, especially, achieving relative improvement up to 14.1% on CIDEr. Index\nTerms: multimodal learning, audio-visual scene-aware dialog, neural machine\ntranslation, multi-task learning", "published": "2020-10-21 09:02:30", "link": "http://arxiv.org/abs/2010.10839v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gender Prediction Based on Vietnamese Names with Machine Learning\n  Techniques", "abstract": "As biological gender is one of the aspects of presenting individual human,\nmuch work has been done on gender classification based on people names. The\nproposals for English and Chinese languages are tremendous; still, there have\nbeen few works done for Vietnamese so far. We propose a new dataset for gender\nprediction based on Vietnamese names. This dataset comprises over 26,000 full\nnames annotated with genders. This dataset is available on our website for\nresearch purposes. In addition, this paper describes six machine learning\nalgorithms (Support Vector Machine, Multinomial Naive Bayes, Bernoulli Naive\nBayes, Decision Tree, Random Forrest and Logistic Regression) and a deep\nlearning model (LSTM) with fastText word embedding for gender prediction on\nVietnamese names. We create a dataset and investigate the impact of each name\ncomponent on detecting gender. As a result, the best F1-score that we have\nachieved is up to 96% on LSTM model and we generate a web API based on our\ntrained model.", "published": "2020-10-21 09:25:48", "link": "http://arxiv.org/abs/2010.10852v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PARENTing via Model-Agnostic Reinforcement Learning to Correct\n  Pathological Behaviors in Data-to-Text Generation", "abstract": "In language generation models conditioned by structured data, the classical\ntraining via maximum likelihood almost always leads models to pick up on\ndataset divergence (i.e., hallucinations or omissions), and to incorporate them\nerroneously in their own generations at inference. In this work, we build ontop\nof previous Reinforcement Learning based approaches and show that a\nmodel-agnostic framework relying on the recently introduced PARENT metric is\nefficient at reducing both hallucinations and omissions. Evaluations on the\nwidely used WikiBIO and WebNLG benchmarks demonstrate the effectiveness of this\nframework compared to state-of-the-art models.", "published": "2020-10-21 09:49:47", "link": "http://arxiv.org/abs/2010.10866v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Decouple Relations: Few-Shot Relation Classification with\n  Entity-Guided Attention and Confusion-Aware Training", "abstract": "This paper aims to enhance the few-shot relation classification especially\nfor sentences that jointly describe multiple relations. Due to the fact that\nsome relations usually keep high co-occurrence in the same context, previous\nfew-shot relation classifiers struggle to distinguish them with few annotated\ninstances. To alleviate the above relation confusion problem, we propose CTEG,\na model equipped with two mechanisms to learn to decouple these easily-confused\nrelations. On the one hand, an Entity-Guided Attention (EGA) mechanism, which\nleverages the syntactic relations and relative positions between each word and\nthe specified entity pair, is introduced to guide the attention to filter out\ninformation causing confusion. On the other hand, a Confusion-Aware Training\n(CAT) method is proposed to explicitly learn to distinguish relations by\nplaying a pushing-away game between classifying a sentence into a true relation\nand its confusing relation. Extensive experiments are conducted on the FewRel\ndataset, and the results show that our proposed model achieves comparable and\neven much better results to strong baselines in terms of accuracy. Furthermore,\nthe ablation test and case study verify the effectiveness of our proposed EGA\nand CAT, especially in addressing the relation confusion problem.", "published": "2020-10-21 11:07:53", "link": "http://arxiv.org/abs/2010.10894v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing the Source and Target Contributions to Predictions in Neural\n  Machine Translation", "abstract": "In Neural Machine Translation (and, more generally, conditional language\nmodeling), the generation of a target token is influenced by two types of\ncontext: the source and the prefix of the target sequence. While many attempts\nto understand the internal workings of NMT models have been made, none of them\nexplicitly evaluates relative source and target contributions to a generation\ndecision. We argue that this relative contribution can be evaluated by adopting\na variant of Layerwise Relevance Propagation (LRP). Its underlying\n'conservation principle' makes relevance propagation unique: differently from\nother methods, it evaluates not an abstract quantity reflecting token\nimportance, but the proportion of each token's influence. We extend LRP to the\nTransformer and conduct an analysis of NMT models which explicitly evaluates\nthe source and target relative contributions to the generation process. We\nanalyze changes in these contributions when conditioning on different types of\nprefixes, when varying the training objective or the amount of training data,\nand during the training process. We find that models trained with more data\ntend to rely on source information more and to have more sharp token\ncontributions; the training process is non-monotonic with several stages of\ndifferent nature.", "published": "2020-10-21 11:37:27", "link": "http://arxiv.org/abs/2010.10907v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Complaint Identification in Social Media with Transformer Networks", "abstract": "Complaining is a speech act extensively used by humans to communicate a\nnegative inconsistency between reality and expectations. Previous work on\nautomatically identifying complaints in social media has focused on using\nfeature-based and task-specific neural network models. Adapting\nstate-of-the-art pre-trained neural language models and their combinations with\nother linguistic information from topics or sentiment for complaint prediction\nhas yet to be explored. In this paper, we evaluate a battery of neural models\nunderpinned by transformer networks which we subsequently combine with\nlinguistic information. Experiments on a publicly available data set of\ncomplaints demonstrate that our models outperform previous state-of-the-art\nmethods by a large margin achieving a macro F1 up to 87.", "published": "2020-10-21 11:44:04", "link": "http://arxiv.org/abs/2010.10910v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LemMED: Fast and Effective Neural Morphological Analysis with Short\n  Context Windows", "abstract": "We present LemMED, a character-level encoder-decoder for contextual\nmorphological analysis (combined lemmatization and tagging). LemMED extends and\nis named after two other attention-based models, namely Lematus, a contextual\nlemmatizer, and MED, a morphological (re)inflection model. Our approach does\nnot require training separate lemmatization and tagging models, nor does it\nneed additional resources and tools, such as morphological dictionaries or\ntransducers. Moreover, LemMED relies solely on character-level representations\nand on local context. Although the model can, in principle, account for global\ncontext on sentence level, our experiments show that using just a single word\nof context around each target word is not only more computationally feasible,\nbut yields better results as well. We evaluate LemMED in the framework of the\nSIMGMORPHON-2019 shared task on combined lemmatization and tagging. In terms of\naverage performance LemMED ranks 5th among 13 systems and is bested only by the\nsubmissions that use contextualized embeddings.", "published": "2020-10-21 12:08:02", "link": "http://arxiv.org/abs/2010.10921v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is Retriever Merely an Approximator of Reader?", "abstract": "The state of the art in open-domain question answering (QA) relies on an\nefficient retriever that drastically reduces the search space for the expensive\nreader. A rather overlooked question in the community is the relationship\nbetween the retriever and the reader, and in particular, if the whole purpose\nof the retriever is just a fast approximation for the reader. Our empirical\nevidence indicates that the answer is no, and that the reader and the retriever\nare complementary to each other even in terms of accuracy only. We make a\ncareful conjecture that the architectural constraint of the retriever, which\nhas been originally intended for enabling approximate search, seems to also\nmake the model more robust in large-scale search. We then propose to distill\nthe reader into the retriever so that the retriever absorbs the strength of the\nreader while keeping its own benefit. Experimental results show that our method\ncan enhance the document recall rate as well as the end-to-end QA accuracy of\noff-the-shelf retrievers in open-domain QA tasks.", "published": "2020-10-21 13:40:15", "link": "http://arxiv.org/abs/2010.10999v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controllable Text Simplification with Explicit Paraphrasing", "abstract": "Text Simplification improves the readability of sentences through several\nrewriting transformations, such as lexical paraphrasing, deletion, and\nsplitting. Current simplification systems are predominantly\nsequence-to-sequence models that are trained end-to-end to perform all these\noperations simultaneously. However, such systems limit themselves to mostly\ndeleting words and cannot easily adapt to the requirements of different target\naudiences. In this paper, we propose a novel hybrid approach that leverages\nlinguistically-motivated rules for splitting and deletion, and couples them\nwith a neural paraphrasing model to produce varied rewriting styles. We\nintroduce a new data augmentation method to improve the paraphrasing capability\nof our model. Through automatic and manual evaluations, we show that our\nproposed model establishes a new state-of-the-art for the task, paraphrasing\nmore often than the existing systems, and can control the degree of each\nsimplification operation applied to the input texts.", "published": "2020-10-21 13:44:40", "link": "http://arxiv.org/abs/2010.11004v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Token Drop mechanism for Neural Machine Translation", "abstract": "Neural machine translation with millions of parameters is vulnerable to\nunfamiliar inputs. We propose Token Drop to improve generalization and avoid\noverfitting for the NMT model. Similar to word dropout, whereas we replace\ndropped token with a special token instead of setting zero to words. We further\nintroduce two self-supervised objectives: Replaced Token Detection and Dropped\nToken Prediction. Our method aims to force model generating target translation\nwith less information, in this way the model can learn textual representation\nbetter. Experiments on Chinese-English and English-Romanian benchmark\ndemonstrate the effectiveness of our approach and our model achieves\nsignificant improvements over a strong Transformer baseline.", "published": "2020-10-21 14:02:27", "link": "http://arxiv.org/abs/2010.11018v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LT3 at SemEval-2020 Task 9: Cross-lingual Embeddings for Sentiment\n  Analysis of Hinglish Social Media Text", "abstract": "This paper describes our contribution to the SemEval-2020 Task 9 on Sentiment\nAnalysis for Code-mixed Social Media Text. We investigated two approaches to\nsolve the task of Hinglish sentiment analysis. The first approach uses\ncross-lingual embeddings resulting from projecting Hinglish and pre-trained\nEnglish FastText word embeddings in the same space. The second approach\nincorporates pre-trained English embeddings that are incrementally retrained\nwith a set of Hinglish tweets. The results show that the second approach\nperforms best, with an F1-score of 70.52% on the held-out test data.", "published": "2020-10-21 14:03:16", "link": "http://arxiv.org/abs/2010.11019v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Classifying Syntactic Errors in Learner Language", "abstract": "We present a method for classifying syntactic errors in learner language,\nnamely errors whose correction alters the morphosyntactic structure of a\nsentence.\n  The methodology builds on the established Universal Dependencies syntactic\nrepresentation scheme, and provides complementary information to other\nerror-classification systems.\n  Unlike existing error classification methods, our method is applicable across\nlanguages, which we showcase by producing a detailed picture of syntactic\nerrors in learner English and learner Russian. We further demonstrate the\nutility of the methodology for analyzing the outputs of leading Grammatical\nError Correction (GEC) systems.", "published": "2020-10-21 14:28:22", "link": "http://arxiv.org/abs/2010.11032v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deciphering Undersegmented Ancient Scripts Using Phonetic Prior", "abstract": "Most undeciphered lost languages exhibit two characteristics that pose\nsignificant decipherment challenges: (1) the scripts are not fully segmented\ninto words; (2) the closest known language is not determined. We propose a\ndecipherment model that handles both of these challenges by building on rich\nlinguistic constraints reflecting consistent patterns in historical sound\nchange. We capture the natural phonological geometry by learning character\nembeddings based on the International Phonetic Alphabet (IPA). The resulting\ngenerative framework jointly models word segmentation and cognate alignment,\ninformed by phonological constraints. We evaluate the model on both deciphered\nlanguages (Gothic, Ugaritic) and an undeciphered one (Iberian). The experiments\nshow that incorporating phonetic geometry leads to clear and consistent gains.\nAdditionally, we propose a measure for language closeness which correctly\nidentifies related languages for Gothic and Ugaritic. For Iberian, the method\ndoes not show strong evidence supporting Basque as a related language,\nconcurring with the favored position by the current scholarship.", "published": "2020-10-21 15:03:52", "link": "http://arxiv.org/abs/2010.11054v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Online Conversation Disentanglement with Pointer Networks", "abstract": "Huge amounts of textual conversations occur online every day, where multiple\nconversations take place concurrently. Interleaved conversations lead to\ndifficulties in not only following the ongoing discussions but also extracting\nrelevant information from simultaneous messages. Conversation disentanglement\naims to separate intermingled messages into detached conversations. However,\nexisting disentanglement methods rely mostly on handcrafted features that are\ndataset specific, which hinders generalization and adaptability. In this work,\nwe propose an end-to-end online framework for conversation disentanglement that\navoids time-consuming domain-specific feature engineering. We design a novel\nway to embed the whole utterance that comprises timestamp, speaker, and message\ntext, and proposes a custom attention mechanism that models disentanglement as\na pointing problem while effectively capturing inter-utterance interactions in\nan end-to-end fashion. We also introduce a joint-learning objective to better\ncapture contextual information. Our experiments on the Ubuntu IRC dataset show\nthat our method achieves state-of-the-art performance in both link and\nconversation prediction tasks.", "published": "2020-10-21 15:43:07", "link": "http://arxiv.org/abs/2010.11080v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NeuSpell: A Neural Spelling Correction Toolkit", "abstract": "We introduce NeuSpell, an open-source toolkit for spelling correction in\nEnglish. Our toolkit comprises ten different models, and benchmarks them on\nnaturally occurring misspellings from multiple sources. We find that many\nsystems do not adequately leverage the context around the misspelt token. To\nremedy this, (i) we train neural models using spelling errors in context,\nsynthetically constructed by reverse engineering isolated misspellings; and\n(ii) use contextual representations. By training on our synthetic examples,\ncorrection rates improve by 9% (absolute) compared to the case when models are\ntrained on randomly sampled character perturbations. Using richer contextual\nrepresentations boosts the correction rate by another 3%. Our toolkit enables\npractitioners to use our proposed and existing spelling correction systems,\nboth via a unified command line, as well as a web interface. Among many\npotential applications, we demonstrate the utility of our spell-checkers in\ncombating adversarial misspellings. The toolkit can be accessed at\nneuspell.github.io. Code and pretrained models are available at\nhttp://github.com/neuspell/neuspell.", "published": "2020-10-21 15:53:29", "link": "http://arxiv.org/abs/2010.11085v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DuoRAT: Towards Simpler Text-to-SQL Models", "abstract": "Recent neural text-to-SQL models can effectively translate natural language\nquestions to corresponding SQL queries on unseen databases. Working mostly on\nthe Spider dataset, researchers have proposed increasingly sophisticated\nsolutions to the problem. Contrary to this trend, in this paper we focus on\nsimplifications. We begin by building DuoRAT, a re-implementation of the\nstate-of-the-art RAT-SQL model that unlike RAT-SQL is using only relation-aware\nor vanilla transformers as the building blocks. We perform several ablation\nexperiments using DuoRAT as the baseline model. Our experiments confirm the\nusefulness of some techniques and point out the redundancy of others, including\nstructural SQL features and features that link the question with the schema.", "published": "2020-10-21 16:27:49", "link": "http://arxiv.org/abs/2010.11119v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cascaded Models With Cyclic Feedback For Direct Speech Translation", "abstract": "Direct speech translation describes a scenario where only speech inputs and\ncorresponding translations are available. Such data are notoriously limited. We\npresent a technique that allows cascades of automatic speech recognition (ASR)\nand machine translation (MT) to exploit in-domain direct speech translation\ndata in addition to out-of-domain MT and ASR data. After pre-training MT and\nASR, we use a feedback cycle where the downstream performance of the MT system\nis used as a signal to improve the ASR system by self-training, and the MT\ncomponent is fine-tuned on multiple ASR outputs, making it more tolerant\ntowards spelling variations. A comparison to end-to-end speech translation\nusing components of identical architecture and the same data shows gains of up\nto 3.8 BLEU points on LibriVoxDeEn and up to 5.1 BLEU points on CoVoST for\nGerman-to-English speech translation.", "published": "2020-10-21 17:18:51", "link": "http://arxiv.org/abs/2010.11153v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Simultaneous Translation by Incorporating Pseudo-References\n  with Fewer Reorderings", "abstract": "Simultaneous translation is vastly different from full-sentence translation,\nin the sense that it starts translation before the source sentence ends, with\nonly a few words delay. However, due to the lack of large-scale, high-quality\nsimultaneous translation datasets, most such systems are still trained on\nconventional full-sentence bitexts. This is far from ideal for the simultaneous\nscenario due to the abundance of unnecessary long-distance reorderings in those\nbitexts. We propose a novel method that rewrites the target side of existing\nfull-sentence corpora into simultaneous-style translation. Experiments on\nZh->En and Ja->En simultaneous translation show substantial improvements (up to\n+2.7 BLEU) with the addition of these generated pseudo-references.", "published": "2020-10-21 19:03:06", "link": "http://arxiv.org/abs/2010.11247v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clustering-based Inference for Biomedical Entity Linking", "abstract": "Due to large number of entities in biomedical knowledge bases, only a small\nfraction of entities have corresponding labelled training data. This\nnecessitates entity linking models which are able to link mentions of unseen\nentities using learned representations of entities. Previous approaches link\neach mention independently, ignoring the relationships within and across\ndocuments between the entity mentions. These relations can be very useful for\nlinking mentions in biomedical text where linking decisions are often difficult\ndue mentions having a generic or a highly specialized form. In this paper, we\nintroduce a model in which linking decisions can be made not merely by linking\nto a knowledge base entity but also by grouping multiple mentions together via\nclustering and jointly making linking predictions. In experiments on the\nlargest publicly available biomedical dataset, we improve the best independent\nprediction for entity linking by 3.0 points of accuracy, and our\nclustering-based inference model further improves entity linking by 2.3 points.", "published": "2020-10-21 19:16:27", "link": "http://arxiv.org/abs/2010.11253v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Document-Level Relation Extraction with Adaptive Thresholding and\n  Localized Context Pooling", "abstract": "Document-level relation extraction (RE) poses new challenges compared to its\nsentence-level counterpart. One document commonly contains multiple entity\npairs, and one entity pair occurs multiple times in the document associated\nwith multiple possible relations. In this paper, we propose two novel\ntechniques, adaptive thresholding and localized context pooling, to solve the\nmulti-label and multi-entity problems. The adaptive thresholding replaces the\nglobal threshold for multi-label classification in the prior work with a\nlearnable entities-dependent threshold. The localized context pooling directly\ntransfers attention from pre-trained language models to locate relevant context\nthat is useful to decide the relation. We experiment on three document-level RE\nbenchmark datasets: DocRED, a recently released large-scale RE dataset, and two\ndatasets CDRand GDA in the biomedical domain. Our ATLOP (Adaptive Thresholding\nand Localized cOntext Pooling) model achieves an F1 score of 63.4, and also\nsignificantly outperforms existing models on both CDR and GDA.", "published": "2020-10-21 20:41:23", "link": "http://arxiv.org/abs/2010.11304v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing and Fine-tuning Reading Comprehension Models for Few-shot Event\n  Extraction", "abstract": "We study the problem of event extraction from text data, which requires both\ndetecting target event types and their arguments. Typically, both the event\ndetection and argument detection subtasks are formulated as supervised sequence\nlabeling problems. We argue that the event extraction models so trained are\ninherently label-hungry, and can generalize poorly across domains and text\ngenres.We propose a reading comprehension framework for event\nextraction.Specifically, we formulate event detection as a textual entailment\nprediction problem, and argument detection as a question answer-ing problem. By\nconstructing proper query templates, our approach can effectively distill rich\nknowledge about tasks and label semantics from pretrained reading comprehension\nmodels. Moreover, our model can be fine-tuned with a small amount of data to\nboost its performance. Our experiment results show that our method performs\nstrongly for zero-shot and few-shot event extraction, and it achieves\nstate-of-the-art performance on the ACE 2005 benchmark when trained with full\nsupervision.", "published": "2020-10-21 21:48:39", "link": "http://arxiv.org/abs/2010.11325v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linking Entities to Unseen Knowledge Bases with Arbitrary Schemas", "abstract": "In entity linking, mentions of named entities in raw text are disambiguated\nagainst a knowledge base (KB). This work focuses on linking to unseen KBs that\ndo not have training data and whose schema is unknown during training. Our\napproach relies on methods to flexibly convert entities from arbitrary KBs with\nseveral attribute-value pairs into flat strings, which we use in conjunction\nwith state-of-the-art models for zero-shot linking. To improve the\ngeneralization of our model, we use two regularization schemes based on\nshuffling of entity attributes and handling of unseen attributes. Experiments\non English datasets where models are trained on the CoNLL dataset, and tested\non the TAC-KBP 2010 dataset show that our models outperform baseline models by\nover 12 points of accuracy. Unlike prior work, our approach also allows for\nseamlessly combining multiple training datasets. We test this ability by adding\nboth a completely different dataset (Wikia), as well as increasing amount of\ntraining data from the TAC-KBP 2010 training set. Our models perform favorably\nacross the board.", "published": "2020-10-21 22:07:31", "link": "http://arxiv.org/abs/2010.11333v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A General Multi-Task Learning Framework to Leverage Text Data for Speech\n  to Text Tasks", "abstract": "Attention-based sequence-to-sequence modeling provides a powerful and elegant\nsolution for applications that need to map one sequence to a different\nsequence. Its success heavily relies on the availability of large amounts of\ntraining data. This presents a challenge for speech applications where labelled\nspeech data is very expensive to obtain, such as automatic speech recognition\n(ASR) and speech translation (ST). In this study, we propose a general\nmulti-task learning framework to leverage text data for ASR and ST tasks. Two\nauxiliary tasks, a denoising autoencoder task and machine translation task, are\nproposed to be co-trained with ASR and ST tasks respectively. We demonstrate\nthat representing text input as phoneme sequences can reduce the difference\nbetween speech and text inputs, and enhance the knowledge transfer from text\ncorpora to the speech to text tasks. Our experiments show that the proposed\nmethod achieves a relative 10~15% word error rate reduction on the English\nLibrispeech task compared with our baseline, and improves the speech\ntranslation quality on the MuST-C tasks by 3.6~9.2 BLEU.", "published": "2020-10-21 22:40:43", "link": "http://arxiv.org/abs/2010.11338v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LSTM-LM with Long-Term History for First-Pass Decoding in Conversational\n  Speech Recognition", "abstract": "LSTM language models (LSTM-LMs) have been proven to be powerful and yielded\nsignificant performance improvements over count based n-gram LMs in modern\nspeech recognition systems. Due to its infinite history states and\ncomputational load, most previous studies focus on applying LSTM-LMs in the\nsecond-pass for rescoring purpose. Recent work shows that it is feasible and\ncomputationally affordable to adopt the LSTM-LMs in the first-pass decoding\nwithin a dynamic (or tree based) decoder framework. In this work, the LSTM-LM\nis composed with a WFST decoder on-the-fly for the first-pass decoding.\nFurthermore, motivated by the long-term history nature of LSTM-LMs, the use of\ncontext beyond the current utterance is explored for the first-pass decoding in\nconversational speech recognition. The context information is captured by the\nhidden states of LSTM-LMs across utterance and can be used to guide the\nfirst-pass search effectively. The experimental results in our internal meeting\ntranscription system show that significant performance improvements can be\nobtained by incorporating the contextual information with LSTM-LMs in the\nfirst-pass decoding, compared to applying the contextual information in the\nsecond-pass rescoring.", "published": "2020-10-21 23:40:26", "link": "http://arxiv.org/abs/2010.11349v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Latte-Mix: Measuring Sentence Semantic Similarity with Latent\n  Categorical Mixtures", "abstract": "Measuring sentence semantic similarity using pre-trained language models such\nas BERT generally yields unsatisfactory zero-shot performance, and one main\nreason is ineffective token aggregation methods such as mean pooling. In this\npaper, we demonstrate under a Bayesian framework that distance between\nprimitive statistics such as the mean of word embeddings are fundamentally\nflawed for capturing sentence-level semantic similarity. To remedy this issue,\nwe propose to learn a categorical variational autoencoder (VAE) based on\noff-the-shelf pre-trained language models. We theoretically prove that\nmeasuring the distance between the latent categorical mixtures, namely\nLatte-Mix, can better reflect the true sentence semantic similarity. In\naddition, our Bayesian framework provides explanations for why models finetuned\non labelled sentence pairs have better zero-shot performance. We also\nempirically demonstrate that these finetuned models could be further improved\nby Latte-Mix. Our method not only yields the state-of-the-art zero-shot\nperformance on semantic similarity datasets such as STS, but also enjoy the\nbenefits of fast training and having small memory footprints.", "published": "2020-10-21 23:45:18", "link": "http://arxiv.org/abs/2010.11351v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explicitly Modeling Syntax in Language Models with Incremental Parsing\n  and a Dynamic Oracle", "abstract": "Syntax is fundamental to our thinking about language. Failing to capture the\nstructure of input language could lead to generalization problems and\nover-parametrization. In the present work, we propose a new syntax-aware\nlanguage model: Syntactic Ordered Memory (SOM). The model explicitly models the\nstructure with an incremental parser and maintains the conditional probability\nsetting of a standard language model (left-to-right). To train the incremental\nparser and avoid exposure bias, we also propose a novel dynamic oracle, so that\nSOM is more robust to wrong parsing decisions. Experiments show that SOM can\nachieve strong results in language modeling, incremental parsing and syntactic\ngeneralization tests, while using fewer parameters than other models.", "published": "2020-10-21 17:39:15", "link": "http://arxiv.org/abs/2011.07960v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Weighted Heterogeneous Graph Based Dialogue System", "abstract": "Knowledge based dialogue systems have attracted increasing research interest\nin diverse applications. However, for disease diagnosis, the widely used\nknowledge graph is hard to represent the symptom-symptom relations and\nsymptom-disease relations since the edges of traditional knowledge graph are\nunweighted. Most research on disease diagnosis dialogue systems highly rely on\ndata-driven methods and statistical features, lacking profound comprehension of\nsymptom-disease relations and symptom-symptom relations. To tackle this issue,\nthis work presents a weighted heterogeneous graph based dialogue system for\ndisease diagnosis. Specifically, we build a weighted heterogeneous graph based\non symptom co-occurrence and a proposed symptom frequency-inverse disease\nfrequency. Then this work proposes a graph based deep Q-network (Graph-DQN) for\ndialogue management. By combining Graph Convolutional Network (GCN) with DQN to\nlearn the embeddings of diseases and symptoms from both the structural and\nattribute information in the weighted heterogeneous graph, Graph-DQN could\ncapture the symptom-disease relations and symptom-symptom relations better.\nExperimental results show that the proposed dialogue system rivals the\nstate-of-the-art models. More importantly, the proposed dialogue system can\ncomplete the task with less dialogue turns and possess a better distinguishing\ncapability on diseases with similar symptoms.", "published": "2020-10-21 01:22:37", "link": "http://arxiv.org/abs/2010.10699v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FreeDOM: A Transferable Neural Architecture for Structured Information\n  Extraction on Web Documents", "abstract": "Extracting structured data from HTML documents is a long-studied problem with\na broad range of applications like augmenting knowledge bases, supporting\nfaceted search, and providing domain-specific experiences for key verticals\nlike shopping and movies. Previous approaches have either required a small\nnumber of examples for each target site or relied on carefully handcrafted\nheuristics built over visual renderings of websites. In this paper, we present\na novel two-stage neural approach, named FreeDOM, which overcomes both these\nlimitations. The first stage learns a representation for each DOM node in the\npage by combining both the text and markup information. The second stage\ncaptures longer range distance and semantic relatedness using a relational\nneural network. By combining these stages, FreeDOM is able to generalize to\nunseen sites after training on a small number of seed sites from that vertical\nwithout requiring expensive hand-crafted features over visual renderings of the\npage. Through experiments on a public dataset with 8 different verticals, we\nshow that FreeDOM beats the previous state of the art by nearly 3.7 F1 points\non average without requiring features over rendered pages or expensive\nhand-crafted features.", "published": "2020-10-21 04:20:13", "link": "http://arxiv.org/abs/2010.10755v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "ProphetNet-Ads: A Looking Ahead Strategy for Generative Retrieval Models\n  in Sponsored Search Engine", "abstract": "In a sponsored search engine, generative retrieval models are recently\nproposed to mine relevant advertisement keywords for users' input queries.\nGenerative retrieval models generate outputs token by token on a path of the\ntarget library prefix tree (Trie), which guarantees all of the generated\noutputs are legal and covered by the target library. In actual use, we found\nseveral typical problems caused by Trie-constrained searching length. In this\npaper, we analyze these problems and propose a looking ahead strategy for\ngenerative retrieval models named ProphetNet-Ads. ProphetNet-Ads improves the\nretrieval ability by directly optimizing the Trie-constrained searching space.\nWe build a dataset from a real-word sponsored search engine and carry out\nexperiments to analyze different generative retrieval models. Compared with\nTrie-based LSTM generative retrieval model proposed recently, our single model\nresult and integrated result improve the recall by 15.58\\% and 18.8\\%\nrespectively with beam size 5. Case studies further demonstrate how these\nproblems are alleviated by ProphetNet-Ads clearly.", "published": "2020-10-21 07:03:20", "link": "http://arxiv.org/abs/2010.10789v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "PBoS: Probabilistic Bag-of-Subwords for Generalizing Word Embedding", "abstract": "We look into the task of \\emph{generalizing} word embeddings: given a set of\npre-trained word vectors over a finite vocabulary, the goal is to predict\nembedding vectors for out-of-vocabulary words, \\emph{without} extra contextual\ninformation. We rely solely on the spellings of words and propose a model,\nalong with an efficient algorithm, that simultaneously models subword\nsegmentation and computes subword-based compositional word embedding. We call\nthe model probabilistic bag-of-subwords (PBoS), as it applies bag-of-subwords\nfor all possible segmentations based on their likelihood. Inspections and affix\nprediction experiment show that PBoS is able to produce meaningful subword\nsegmentations and subword rankings without any source of explicit morphological\nknowledge. Word similarity and POS tagging experiments show clear advantages of\nPBoS over previous subword-level models in the quality of generated word\nembeddings across languages.", "published": "2020-10-21 08:11:08", "link": "http://arxiv.org/abs/2010.10813v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Explaining black-box text classifiers for disease-treatment information\n  extraction", "abstract": "Deep neural networks and other intricate Artificial Intelligence (AI) models\nhave reached high levels of accuracy on many biomedical natural language\nprocessing tasks. However, their applicability in real-world use cases may be\nlimited due to their vague inner working and decision logic. A post-hoc\nexplanation method can approximate the behavior of a black-box AI model by\nextracting relationships between feature values and outcomes. In this paper, we\nintroduce a post-hoc explanation method that utilizes confident itemsets to\napproximate the behavior of black-box classifiers for medical information\nextraction. Incorporating medical concepts and semantics into the explanation\nprocess, our explanator finds semantic relations between inputs and outputs in\ndifferent parts of the decision space of a black-box classifier. The\nexperimental results show that our explanation method can outperform\nperturbation and decision set based explanators in terms of fidelity and\ninterpretability of explanations produced for predictions on a\ndisease-treatment information extraction task.", "published": "2020-10-21 09:58:00", "link": "http://arxiv.org/abs/2010.10873v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "TurnGPT: a Transformer-based Language Model for Predicting Turn-taking\n  in Spoken Dialog", "abstract": "Syntactic and pragmatic completeness is known to be important for turn-taking\nprediction, but so far machine learning models of turn-taking have used such\nlinguistic information in a limited way. In this paper, we introduce TurnGPT, a\ntransformer-based language model for predicting turn-shifts in spoken dialog.\nThe model has been trained and evaluated on a variety of written and spoken\ndialog datasets. We show that the model outperforms two baselines used in prior\nwork. We also report on an ablation study, as well as attention and gradient\nanalyses, which show that the model is able to utilize the dialog context and\npragmatic completeness for turn-taking prediction. Finally, we explore the\nmodel's potential in not only detecting, but also projecting, turn-completions.", "published": "2020-10-21 09:58:39", "link": "http://arxiv.org/abs/2010.10874v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "German's Next Language Model", "abstract": "In this work we present the experiments which lead to the creation of our\nBERT and ELECTRA based German language models, GBERT and GELECTRA. By varying\nthe input training data, model size, and the presence of Whole Word Masking\n(WWM) we were able to attain SoTA performance across a set of document\nclassification and named entity recognition (NER) tasks for both models of base\nand large size. We adopt an evaluation driven approach in training these models\nand our results indicate that both adding more data and utilizing WWM improve\nmodel performance. By benchmarking against existing German models, we show that\nthese models are the best German models to date. Our trained models will be\nmade publicly available to the research community.", "published": "2020-10-21 11:28:23", "link": "http://arxiv.org/abs/2010.10906v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Open-Domain Frame Semantic Parsing Using Transformers", "abstract": "Frame semantic parsing is a complex problem which includes multiple\nunderlying subtasks. Recent approaches have employed joint learning of subtasks\n(such as predicate and argument detection), and multi-task learning of related\ntasks (such as syntactic and semantic parsing). In this paper, we explore\nmulti-task learning of all subtasks with transformer-based models. We show that\na purely generative encoder-decoder architecture handily beats the previous\nstate of the art in FrameNet 1.7 parsing, and that a mixed decoding multi-task\napproach achieves even better performance. Finally, we show that the multi-task\nmodel also outperforms recent state of the art systems for PropBank SRL parsing\non the CoNLL 2012 benchmark.", "published": "2020-10-21 13:38:04", "link": "http://arxiv.org/abs/2010.10998v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Multiple Choices Question Answering: Start Learning from\n  Basic Knowledge", "abstract": "In this paper, we study the possibility of almost unsupervised Multiple\nChoices Question Answering (MCQA). Starting from very basic knowledge, MCQA\nmodel knows that some choices have higher probabilities of being correct than\nthe others. The information, though very noisy, guides the training of an MCQA\nmodel. The proposed method is shown to outperform the baseline approaches on\nRACE and even comparable with some supervised learning approaches on MC500.", "published": "2020-10-21 13:44:35", "link": "http://arxiv.org/abs/2010.11003v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Stacking Neural Network Models for Automatic Short Answer Scoring", "abstract": "Automatic short answer scoring is one of the text classification problems to\nassess students' answers during exams automatically. Several challenges can\narise in making an automatic short answer scoring system, one of which is the\nquantity and quality of the data. The data labeling process is not easy because\nit requires a human annotator who is an expert in their field. Further, the\ndata imbalance process is also a challenge because the number of labels for\ncorrect answers is always much less than the wrong answers. In this paper, we\npropose the use of a stacking model based on neural network and XGBoost for\nclassification process with sentence embedding feature. We also propose to use\ndata upsampling method to handle imbalance classes and hyperparameters\noptimization algorithm to find a robust model automatically. We use Ukara 1.0\nChallenge dataset and our best model obtained an F1-score of 0.821 exceeding\nthe previous work at the same dataset.", "published": "2020-10-21 16:00:09", "link": "http://arxiv.org/abs/2010.11092v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond English-Centric Multilingual Machine Translation", "abstract": "Existing work in translation demonstrated the potential of massively\nmultilingual machine translation by training a single model able to translate\nbetween any pair of languages. However, much of this work is English-Centric by\ntraining only on data which was translated from or to English. While this is\nsupported by large sources of training data, it does not reflect translation\nneeds worldwide. In this work, we create a true Many-to-Many multilingual\ntranslation model that can translate directly between any pair of 100\nlanguages. We build and open source a training dataset that covers thousands of\nlanguage directions with supervised data, created through large-scale mining.\nThen, we explore how to effectively increase model capacity through a\ncombination of dense scaling and language-specific sparse parameters to create\nhigh quality models. Our focus on non-English-Centric models brings gains of\nmore than 10 BLEU when directly translating between non-English directions\nwhile performing competitively to the best single systems of WMT. We\nopen-source our scripts so that others may reproduce the data, evaluation, and\nfinal M2M-100 model.", "published": "2020-10-21 17:01:23", "link": "http://arxiv.org/abs/2010.11125v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Simple and Efficient Multi-Task Learning Approach for Conditioned\n  Dialogue Generation", "abstract": "Conditioned dialogue generation suffers from the scarcity of labeled\nresponses. In this work, we exploit labeled non-dialogue text data related to\nthe condition, which are much easier to collect. We propose a multi-task\nlearning approach to leverage both labeled dialogue and text data. The 3 tasks\njointly optimize the same pre-trained Transformer -- conditioned dialogue\ngeneration task on the labeled dialogue data, conditioned language encoding\ntask and conditioned language generation task on the labeled text data.\nExperimental results show that our approach outperforms the state-of-the-art\nmodels by leveraging the labeled texts, and it also obtains larger improvement\nin performance comparing to the previous methods to leverage text data.", "published": "2020-10-21 16:56:49", "link": "http://arxiv.org/abs/2010.11140v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semantic Role Labeling as Syntactic Dependency Parsing", "abstract": "We reduce the task of (span-based) PropBank-style semantic role labeling\n(SRL) to syntactic dependency parsing. Our approach is motivated by our\nempirical analysis that shows three common syntactic patterns account for over\n98% of the SRL annotations for both English and Chinese data. Based on this\nobservation, we present a conversion scheme that packs SRL annotations into\ndependency tree representations through joint labels that permit highly\naccurate recovery back to the original format. This representation allows us to\ntrain statistical dependency parsers to tackle SRL and achieve competitive\nperformance with the current state of the art. Our findings show the promise of\nsyntactic dependency trees in encoding semantic role relations within their\nsyntactic domain of locality, and point to potential further integration of\nsyntactic methods into semantic role labeling in the future.", "published": "2020-10-21 17:46:11", "link": "http://arxiv.org/abs/2010.11170v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Contrastive Learning for Efficient User Satisfaction\n  Prediction in Conversational Agents", "abstract": "Turn-level user satisfaction is one of the most important performance metrics\nfor conversational agents. It can be used to monitor the agent's performance\nand provide insights about defective user experiences. Moreover, a powerful\nsatisfaction model can be used as an objective function that a conversational\nagent continuously optimizes for. While end-to-end deep learning has shown\npromising results, having access to a large number of reliable annotated\nsamples required by these methods remains challenging. In a large-scale\nconversational system, there is a growing number of newly developed skills,\nmaking the traditional data collection, annotation, and modeling process\nimpractical due to the required annotation costs as well as the turnaround\ntimes. In this paper, we suggest a self-supervised contrastive learning\napproach that leverages the pool of unlabeled data to learn user-agent\ninteractions. We show that the pre-trained models using the self-supervised\nobjective are transferable to the user satisfaction prediction. In addition, we\npropose a novel few-shot transfer learning approach that ensures better\ntransferability for very small sample sizes. The suggested few-shot method does\nnot require any inner loop optimization process and is scalable to very large\ndatasets and complex models. Based on our experiments using real-world data\nfrom a large-scale commercial system, the suggested approach is able to\nsignificantly reduce the required number of annotations, while improving the\ngeneralization on unseen out-of-domain skills.", "published": "2020-10-21 18:10:58", "link": "http://arxiv.org/abs/2010.11230v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Detection of COVID-19 informative tweets using RoBERTa", "abstract": "Social media such as Twitter is a hotspot of user-generated information. In\nthis ongoing Covid-19 pandemic, there has been an abundance of data on social\nmedia which can be classified as informative and uninformative content. In this\npaper, we present our work to detect informative Covid-19 English tweets using\nRoBERTa model as a part of the W-NUT workshop 2020. We show the efficacy of our\nmodel on a public dataset with an F1-score of 0.89 on the validation dataset\nand 0.87 on the leaderboard.", "published": "2020-10-21 18:43:13", "link": "http://arxiv.org/abs/2010.11238v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "On the Potential of Lexico-logical Alignments for Semantic Parsing to\n  SQL Queries", "abstract": "Large-scale semantic parsing datasets annotated with logical forms have\nenabled major advances in supervised approaches. But can richer supervision\nhelp even more? To explore the utility of fine-grained, lexical-level\nsupervision, we introduce Squall, a dataset that enriches 11,276\nWikiTableQuestions English-language questions with manually created SQL\nequivalents plus alignments between SQL and question fragments. Our annotation\nenables new training possibilities for encoder-decoder models, including\napproaches from machine translation previously precluded by the absence of\nalignments. We propose and test two methods: (1) supervised attention; (2)\nadopting an auxiliary objective of disambiguating references in the input\nqueries to table columns. In 5-fold cross validation, these strategies improve\nover strong baselines by 4.4% execution accuracy. Oracle experiments suggest\nthat annotated alignments can support further accuracy gains of up to 23.9%.", "published": "2020-10-21 19:01:00", "link": "http://arxiv.org/abs/2010.11246v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Learning to Summarize Long Texts with Memory Compression and Transfer", "abstract": "We introduce Mem2Mem, a memory-to-memory mechanism for hierarchical recurrent\nneural network based encoder decoder architectures and we explore its use for\nabstractive document summarization. Mem2Mem transfers \"memories\" via\nreadable/writable external memory modules that augment both the encoder and\ndecoder. Our memory regularization compresses an encoded input article into a\nmore compact set of sentence representations. Most importantly, the memory\ncompression step performs implicit extraction without labels, sidestepping\nissues with suboptimal ground-truth data and exposure bias of hybrid\nextractive-abstractive summarization techniques. By allowing the decoder to\nread/write over the encoded input memory, the model learns to read salient\ninformation about the input article while keeping track of what has been\ngenerated. Our Mem2Mem approach yields results that are competitive with state\nof the art transformer based summarization methods, but with 16 times fewer\nparameters", "published": "2020-10-21 21:45:44", "link": "http://arxiv.org/abs/2010.11322v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NADI 2020: The First Nuanced Arabic Dialect Identification Shared Task", "abstract": "We present the results and findings of the First Nuanced Arabic Dialect\nIdentification Shared Task (NADI). This Shared Task includes two subtasks:\ncountry-level dialect identification (Subtask 1) and province-level sub-dialect\nidentification (Subtask 2). The data for the shared task covers a total of 100\nprovinces from 21 Arab countries and are collected from the Twitter domain. As\nsuch, NADI is the first shared task to target naturally-occurring fine-grained\ndialectal text at the sub-country level. A total of 61 teams from 25 countries\nregistered to participate in the tasks, thus reflecting the interest of the\ncommunity in this area. We received 47 submissions for Subtask 1 from 18 teams\nand 9 submissions for Subtask 2 from 9 teams.", "published": "2020-10-21 22:14:28", "link": "http://arxiv.org/abs/2010.11334v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Emformer: Efficient Memory Transformer Based Acoustic Model For Low\n  Latency Streaming Speech Recognition", "abstract": "This paper proposes an efficient memory transformer Emformer for low latency\nstreaming speech recognition. In Emformer, the long-range history context is\ndistilled into an augmented memory bank to reduce self-attention's computation\ncomplexity. A cache mechanism saves the computation for the key and value in\nself-attention for the left context. Emformer applies a parallelized block\nprocessing in training to support low latency models. We carry out experiments\non benchmark LibriSpeech data. Under average latency of 960 ms, Emformer gets\nWER $2.50\\%$ on test-clean and $5.62\\%$ on test-other. Comparing with a strong\nbaseline augmented memory transformer (AM-TRF), Emformer gets $4.6$ folds\ntraining speedup and $18\\%$ relative real-time factor (RTF) reduction in\ndecoding with relative WER reduction $17\\%$ on test-clean and $9\\%$ on\ntest-other. For a low latency scenario with an average latency of 80 ms,\nEmformer achieves WER $3.01\\%$ on test-clean and $7.09\\%$ on test-other.\nComparing with the LSTM baseline with the same latency and model size, Emformer\ngets relative WER reduction $9\\%$ and $16\\%$ on test-clean and test-other,\nrespectively.", "published": "2020-10-21 04:38:09", "link": "http://arxiv.org/abs/2010.10759v4", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Using the Full-text Content of Academic Articles to Identify and\n  Evaluate Algorithm Entities in the Domain of Natural Language Processing", "abstract": "In the era of big data, the advancement, improvement, and application of\nalgorithms in academic research have played an important role in promoting the\ndevelopment of different disciplines. Academic papers in various disciplines,\nespecially computer science, contain a large number of algorithms. Identifying\nthe algorithms from the full-text content of papers can determine popular or\nclassical algorithms in a specific field and help scholars gain a comprehensive\nunderstanding of the algorithms and even the field. To this end, this article\ntakes the field of natural language processing (NLP) as an example and\nidentifies algorithms from academic papers in the field. A dictionary of\nalgorithms is constructed by manually annotating the contents of papers, and\nsentences containing algorithms in the dictionary are extracted through\ndictionary-based matching. The number of articles mentioning an algorithm is\nused as an indicator to analyze the influence of that algorithm. Our results\nreveal the algorithm with the highest influence in NLP papers and show that\nclassification algorithms represent the largest proportion among the\nhigh-impact algorithms. In addition, the evolution of the influence of\nalgorithms reflects the changes in research tasks and topics in the field, and\nthe changes in the influence of different algorithms show different trends. As\na preliminary exploration, this paper conducts an analysis of the impact of\nalgorithms mentioned in the academic text, and the results can be used as\ntraining data for the automatic extraction of large-scale algorithms in the\nfuture. The methodology in this paper is domain-independent and can be applied\nto other domains.", "published": "2020-10-21 08:24:18", "link": "http://arxiv.org/abs/2010.10817v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ReSCo-CC: Unsupervised Identification of Key Disinformation Sentences", "abstract": "Disinformation is often presented in long textual articles, especially when\nit relates to domains such as health, often seen in relation to COVID-19. These\narticles are typically observed to have a number of trustworthy sentences among\nwhich core disinformation sentences are scattered. In this paper, we propose a\nnovel unsupervised task of identifying sentences containing key disinformation\nwithin a document that is known to be untrustworthy. We design a three-phase\nstatistical NLP solution for the task which starts with embedding sentences\nwithin a bespoke feature space designed for the task. Sentences represented\nusing those features are then clustered, following which the key sentences are\nidentified through proximity scoring. We also curate a new dataset with\nsentence level disinformation scorings to aid evaluation for this task; the\ndataset is being made publicly available to facilitate further research. Based\non a comprehensive empirical evaluation against techniques from related tasks\nsuch as claim detection and summarization, as well as against simplified\nvariants of our proposed approach, we illustrate that our method is able to\nidentify core disinformation effectively.", "published": "2020-10-21 08:53:36", "link": "http://arxiv.org/abs/2010.10836v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BERT for Joint Multichannel Speech Dereverberation with Spatial-aware\n  Tasks", "abstract": "We propose a method for joint multichannel speech dereverberation with two\nspatial-aware tasks: direction-of-arrival (DOA) estimation and speech\nseparation. The proposed method addresses involved tasks as a sequence to\nsequence mapping problem, which is general enough for a variety of front-end\nspeech enhancement tasks. The proposed method is inspired by the excellent\nsequence modeling capability of bidirectional encoder representation from\ntransformers (BERT). Instead of utilizing explicit representations from\npretraining in a self-supervised manner, we utilizes transformer encoded hidden\nrepresentations in a supervised manner. Both multichannel spectral magnitude\nand spectral phase information of varying length utterances are encoded.\nExperimental result demonstrates the effectiveness of the proposed method.", "published": "2020-10-21 11:05:17", "link": "http://arxiv.org/abs/2010.10892v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring Sequence-to-Sequence Models for SPARQL Pattern Composition", "abstract": "A booming amount of information is continuously added to the Internet as\nstructured and unstructured data, feeding knowledge bases such as DBpedia and\nWikidata with billions of statements describing millions of entities. The aim\nof Question Answering systems is to allow lay users to access such data using\nnatural language without needing to write formal queries. However, users often\nsubmit questions that are complex and require a certain level of abstraction\nand reasoning to decompose them into basic graph patterns. In this short paper,\nwe explore the use of architectures based on Neural Machine Translation called\nNeural SPARQL Machines to learn pattern compositions. We show that\nsequence-to-sequence models are a viable and promising option to transform long\nutterances into complex SPARQL queries.", "published": "2020-10-21 11:12:01", "link": "http://arxiv.org/abs/2010.10900v1", "categories": ["cs.CL", "cs.AI", "cs.DB", "68T99", "I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Deep learning-based citation recommendation system for patents", "abstract": "In this study, we address the challenges in developing a deep learning-based\nautomatic patent citation recommendation system. Although deep learning-based\nrecommendation systems have exhibited outstanding performance in various\ndomains (such as movies, products, and paper citations), their validity in\npatent citations has not been investigated, owing to the lack of a freely\navailable high-quality dataset and relevant benchmark model. To solve these\nproblems, we present a novel dataset called PatentNet that includes textual\ninformation and metadata for approximately 110,000 patents from the Google Big\nQuery service. Further, we propose strong benchmark models considering the\nsimilarity of textual information and metadata (such as cooperative patent\nclassification code). Compared with existing recommendation methods, the\nproposed benchmark method achieved a mean reciprocal rank of 0.2377 on the test\nset, whereas the existing state-of-the-art recommendation method achieved\n0.2073.", "published": "2020-10-21 12:18:21", "link": "http://arxiv.org/abs/2010.10932v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Neural Networks for Entity Matching: A Survey", "abstract": "Entity matching is the problem of identifying which records refer to the same\nreal-world entity. It has been actively researched for decades, and a variety\nof different approaches have been developed. Even today, it remains a\nchallenging problem, and there is still generous room for improvement. In\nrecent years we have seen new methods based upon deep learning techniques for\nnatural language processing emerge.\n  In this survey, we present how neural networks have been used for entity\nmatching. Specifically, we identify which steps of the entity matching process\nexisting work have targeted using neural networks, and provide an overview of\nthe different techniques used at each step. We also discuss contributions from\ndeep learning in entity matching compared to traditional methods, and propose a\ntaxonomy of deep neural networks for entity matching.", "published": "2020-10-21 15:36:03", "link": "http://arxiv.org/abs/2010.11075v2", "categories": ["cs.DB", "cs.CL", "cs.LG"], "primary_category": "cs.DB"}
{"title": "Sentence Boundary Augmentation For Neural Machine Translation Robustness", "abstract": "Neural Machine Translation (NMT) models have demonstrated strong state of the\nart performance on translation tasks where well-formed training and evaluation\ndata are provided, but they remain sensitive to inputs that include errors of\nvarious types. Specifically, in the context of long-form speech translation\nsystems, where the input transcripts come from Automatic Speech Recognition\n(ASR), the NMT models have to handle errors including phoneme substitutions,\ngrammatical structure, and sentence boundaries, all of which pose challenges to\nNMT robustness. Through in-depth error analysis, we show that sentence boundary\nsegmentation has the largest impact on quality, and we develop a simple data\naugmentation strategy to improve segmentation robustness.", "published": "2020-10-21 16:44:48", "link": "http://arxiv.org/abs/2010.11132v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multi-Domain Dialogue State Tracking based on State Graph", "abstract": "We investigate the problem of multi-domain Dialogue State Tracking (DST) with\nopen vocabulary, which aims to extract the state from the dialogue. Existing\napproaches usually concatenate previous dialogue state with dialogue history as\nthe input to a bi-directional Transformer encoder. They rely on the\nself-attention mechanism of Transformer to connect tokens in them. However,\nattention may be paid to spurious connections, leading to wrong inference. In\nthis paper, we propose to construct a dialogue state graph in which domains,\nslots and values from the previous dialogue state are connected properly.\nThrough training, the graph node and edge embeddings can encode co-occurrence\nrelations between domain-domain, slot-slot and domain-slot, reflecting the\nstrong transition paths in general dialogue. The state graph, encoded with\nrelational-GCN, is fused into the Transformer encoder. Experimental results\nshow that our approach achieves a new state of the art on the task while\nremaining efficient. It outperforms existing open-vocabulary DST approaches.", "published": "2020-10-21 16:55:18", "link": "http://arxiv.org/abs/2010.11137v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Towards End-to-End Training of Automatic Speech Recognition for Nigerian\n  Pidgin", "abstract": "Nigerian Pidgin remains one of the most popular languages in West Africa.\nWith at least 75 million speakers along the West African coast, the language\nhas spread to diasporic communities through Nigerian immigrants in England,\nCanada, and America, amongst others. In contrast, the language remains an\nunder-resourced one in the field of natural language processing, particularly\non speech recognition and translation tasks. In this work, we present the first\nparallel (speech-to-text) data on Nigerian pidgin. We also trained the first\nend-to-end speech recognition system (QuartzNet and Jasper model) on this\nlanguage which were both optimized using Connectionist Temporal Classification\n(CTC) loss. With baseline results, we were able to achieve a low word error\nrate (WER) of 0.77% using a greedy decoder on our dataset. Finally, we\nopen-source the data and code along with this publication in order to encourage\nfuture research in this direction.", "published": "2020-10-21 16:32:58", "link": "http://arxiv.org/abs/2010.11123v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FastEmit: Low-latency Streaming ASR with Sequence-level Emission\n  Regularization", "abstract": "Streaming automatic speech recognition (ASR) aims to emit each hypothesized\nword as quickly and accurately as possible. However, emitting fast without\ndegrading quality, as measured by word error rate (WER), is highly challenging.\nExisting approaches including Early and Late Penalties and Constrained\nAlignments penalize emission delay by manipulating per-token or per-frame\nprobability prediction in sequence transducer models. While being successful in\nreducing delay, these approaches suffer from significant accuracy regression\nand also require additional word alignment information from an existing model.\nIn this work, we propose a sequence-level emission regularization method, named\nFastEmit, that applies latency regularization directly on per-sequence\nprobability in training transducer models, and does not require any alignment.\nWe demonstrate that FastEmit is more suitable to the sequence-level\noptimization of transducer models for streaming ASR by applying it on various\nend-to-end streaming ASR networks including RNN-Transducer,\nTransformer-Transducer, ConvNet-Transducer and Conformer-Transducer. We achieve\n150-300 ms latency reduction with significantly better accuracy over previous\ntechniques on a Voice Search test set. FastEmit also improves streaming ASR\naccuracy from 4.4%/8.9% to 3.1%/7.5% WER, meanwhile reduces 90th percentile\nlatency from 210 ms to only 30 ms on LibriSpeech.", "published": "2020-10-21 17:05:01", "link": "http://arxiv.org/abs/2010.11148v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Contextualized Attention-based Knowledge Transfer for Spoken\n  Conversational Question Answering", "abstract": "Spoken conversational question answering (SCQA) requires machines to model\ncomplex dialogue flow given the speech utterances and text corpora. Different\nfrom traditional text question answering (QA) tasks, SCQA involves audio signal\nprocessing, passage comprehension, and contextual understanding. However, ASR\nsystems introduce unexpected noisy signals to the transcriptions, which result\nin performance degradation on SCQA. To overcome the problem, we propose CADNet,\na novel contextualized attention-based distillation approach, which applies\nboth cross-attention and self-attention to obtain ASR-robust contextualized\nembedding representations of the passage and dialogue history for performance\nimprovements. We also introduce the spoken conventional knowledge distillation\nframework to distill the ASR-robust knowledge from the estimated probabilities\nof the teacher model to the student. We conduct extensive experiments on the\nSpoken-CoQA dataset and demonstrate that our approach achieves remarkable\nperformance in this task.", "published": "2020-10-21 15:17:18", "link": "http://arxiv.org/abs/2010.11066v4", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Knowledge Distillation for Improved Accuracy in Spoken Question\n  Answering", "abstract": "Spoken question answering (SQA) is a challenging task that requires the\nmachine to fully understand the complex spoken documents. Automatic speech\nrecognition (ASR) plays a significant role in the development of QA systems.\nHowever, the recent work shows that ASR systems generate highly noisy\ntranscripts, which critically limit the capability of machine comprehension on\nthe SQA task. To address the issue, we present a novel distillation framework.\nSpecifically, we devise a training strategy to perform knowledge distillation\n(KD) from spoken documents and written counterparts. Our work makes a step\ntowards distilling knowledge from the language model as a supervision signal to\nlead to better student accuracy by reducing the misalignment between automatic\nand manual transcriptions. Experiments demonstrate that our approach\noutperforms several state-of-the-art language models on the Spoken-SQuAD\ndataset.", "published": "2020-10-21 15:18:01", "link": "http://arxiv.org/abs/2010.11067v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Real-time Speech Frequency Bandwidth Extension", "abstract": "In this paper we propose a lightweight model for frequency bandwidth\nextension of speech signals, increasing the sampling frequency from 8kHz to\n16kHz while restoring the high frequency content to a level almost\nindistinguishable from the 16kHz ground truth. The model architecture is based\non SEANet (Sound EnhAncement Network), a wave-to-wave fully convolutional\nmodel, which uses a combination of feature losses and adversarial losses to\nreconstruct an enhanced version of the input speech. In addition, we propose a\nvariant of SEANet that can be deployed on-device in streaming mode, achieving\nan architectural latency of 16ms. When profiled on a single core of a mobile\nCPU, processing one 16ms frame takes only 1.5ms. The low latency makes it\nviable for bi-directional voice communication systems.", "published": "2020-10-21 00:01:19", "link": "http://arxiv.org/abs/2010.10677v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-task Metric Learning for Text-independent Speaker Verification", "abstract": "In this work, we introduce metric learning (ML) to enhance the deep embedding\nlearning for text-independent speaker verification (SV). Specifically, the deep\nspeaker embedding network is trained with conventional cross entropy loss and\nauxiliary pair-based ML loss function. For the auxiliary ML task, training\nsamples of a mini-batch are first arranged into pairs, then positive and\nnegative pairs are selected and weighted through their own and relative\nsimilarities, and finally the auxiliary ML loss is calculated by the similarity\nof the selected pairs. To evaluate the proposed method, we conduct experiments\non the Speaker in the Wild (SITW) dataset. The results demonstrate the\neffectiveness of the proposed method.", "published": "2020-10-21 12:03:36", "link": "http://arxiv.org/abs/2010.10919v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The UPC Speaker Verification System Submitted to VoxCeleb Speaker\n  Recognition Challenge 2020 (VoxSRC-20)", "abstract": "This report describes the submission from Technical University of Catalonia\n(UPC) to the VoxCeleb Speaker Recognition Challenge (VoxSRC-20) at Interspeech\n2020. The final submission is a combination of three systems. System-1 is an\nautoencoder based approach which tries to reconstruct similar i-vectors,\nwhereas System-2 and -3 are Convolutional Neural Network (CNN) based siamese\narchitectures. The siamese networks have two and three branches, respectively,\nwhere each branch is a CNN encoder. The double-branch siamese performs binary\nclassification using cross entropy loss during training. Whereas, our\ntriple-branch siamese is trained to learn speaker embeddings using triplet\nloss. We provide results of our systems on VoxCeleb-1 test, VoxSRC-20\nvalidation and test sets.", "published": "2020-10-21 12:21:17", "link": "http://arxiv.org/abs/2010.10937v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Addressing the Recitative Problem in Real-time Opera Tracking", "abstract": "Robust real-time opera tracking (score following) would be extremely useful\nfor many processes surrounding live opera staging and streaming, including\nautomatic lyrics displays, camera control, or live video cutting. Recent work\nhas shown that, with some appropriate measures to account for common problems\nsuch as breaks and interruptions, spontaneous applause, various noises and\ninterludes, current audio-to-audio alignment algorithms can be made to follow\nan entire opera from beginning to end, in a relatively robust way. However,\nthey remain inaccurate when the textual content becomes prominent against the\nmelody or music -- notably, during recitativo passages. In this paper, we\naddress this specific problem by proposing to use two specialized trackers in\nparallel, one focusing on music-, the other on speech-sensitive features. We\nfirst carry out a systematic study on speech-related features, targeting the\nprecise alignment of corresponding recitatives from different performances of\nthe same opera. Then we propose different solutions, based on pre-trained music\nand speech classifiers, to combine the two trackers in order to improve the\nglobal accuracy over the course of the entire opera.", "published": "2020-10-21 13:52:47", "link": "http://arxiv.org/abs/2010.11013v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The IDLAB VoxSRC-20 Submission: Large Margin Fine-Tuning and\n  Quality-Aware Score Calibration in DNN Based Speaker Verification", "abstract": "In this paper we propose and analyse a large margin fine-tuning strategy and\na quality-aware score calibration in text-independent speaker verification.\nLarge margin fine-tuning is a secondary training stage for DNN based speaker\nverification systems trained with margin-based loss functions. It enables the\nnetwork to create more robust speaker embeddings by enabling the use of longer\ntraining utterances in combination with a more aggressive margin penalty. Score\ncalibration is a common practice in speaker verification systems to map output\nscores to well-calibrated log-likelihood-ratios, which can be converted to\ninterpretable probabilities. By including quality features in the calibration\nsystem, the decision thresholds of the evaluation metrics become\nquality-dependent and more consistent across varying trial conditions. Applying\nboth enhancements on the ECAPA-TDNN architecture leads to state-of-the-art\nresults on all publicly available VoxCeleb1 test sets and contributed to our\nwinning submissions in the supervised verification tracks of the VoxCeleb\nSpeaker Recognition Challenge 2020.", "published": "2020-10-21 19:20:28", "link": "http://arxiv.org/abs/2010.11255v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Audio Anomalies Recognition Using Temporal Convolutional\n  Attention Network", "abstract": "Anomalous audio in speech recordings is often caused by speaker voice\ndistortion, external noise, or even electric interferences. These obstacles\nhave become a serious problem in some fields, such as high-quality music mixing\nand speech processing. In this paper, a novel approach using a temporal\nconvolutional attention network (TCAN) is proposed to tackle this problem. The\nuse of temporal conventional network (TCN) can capture long range patterns\nusing a hierarchy of temporal convolutional filters. To enhance the ability to\ntackle audio anomalies in different acoustic conditions, an attention mechanism\nis used in TCN, where a self-attention block is added after each temporal\nconvolutional layer. This aims to highlight the target related features and\nmitigate the interferences from irrelevant information. To evaluate the\nperformance of the proposed model, audio recordings are collected from the\nTIMIT dataset, and are then changed by adding five different types of audio\ndistortions: gaussian noise, magnitude drift, random dropout, reduction of\ntemporal resolution, and time warping. Distortions are mixed at different\nsignal-to-noise ratios (SNRs) (5dB, 10dB, 15dB, 20dB, 25dB, 30dB). The\nexperimental results show that the use of proposed model can yield better\nclassification performances than some strong baseline methods, such as the LSTM\nand TCN based models, by approximate 3$\\sim$ 10\\% relative improvements.", "published": "2020-10-21 19:53:24", "link": "http://arxiv.org/abs/2010.11286v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "VenoMave: Targeted Poisoning Against Speech Recognition", "abstract": "Despite remarkable improvements, automatic speech recognition is susceptible\nto adversarial perturbations. Compared to standard machine learning\narchitectures, these attacks are significantly more challenging, especially\nsince the inputs to a speech recognition system are time series that contain\nboth acoustic and linguistic properties of speech. Extracting all\nrecognition-relevant information requires more complex pipelines and an\nensemble of specialized components. Consequently, an attacker needs to consider\nthe entire pipeline. In this paper, we present VENOMAVE, the first\ntraining-time poisoning attack against speech recognition. Similar to the\npredominantly studied evasion attacks, we pursue the same goal: leading the\nsystem to an incorrect and attacker-chosen transcription of a target audio\nwaveform. In contrast to evasion attacks, however, we assume that the attacker\ncan only manipulate a small part of the training data without altering the\ntarget audio waveform at runtime. We evaluate our attack on two datasets:\nTIDIGITS and Speech Commands. When poisoning less than 0.17% of the dataset,\nVENOMAVE achieves attack success rates of more than 80.0%, without access to\nthe victim's network architecture or hyperparameters. In a more realistic\nscenario, when the target audio waveform is played over the air in different\nrooms, VENOMAVE maintains a success rate of up to 73.3%. Finally, VENOMAVE\nachieves an attack transferability rate of 36.4% between two different model\narchitectures.", "published": "2020-10-21 00:30:08", "link": "http://arxiv.org/abs/2010.10682v3", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Prediction of Object Geometry from Acoustic Scattering Using\n  Convolutional Neural Networks", "abstract": "Acoustic scattering is strongly influenced by boundary geometry of objects\nover which sound scatters. The present work proposes a method to infer object\ngeometry from scattering features by training convolutional neural networks.\nThe training data is generated from a fast numerical solver developed on CUDA.\nThe complete set of simulations is sampled to generate multiple datasets\ncontaining different amounts of channels and diverse image resolutions. The\nrobustness of our approach in response to data degradation is evaluated by\ncomparing the performance of networks trained using the datasets with varying\nlevels of data degradation. The present work has found that the predictions\nmade from our models match ground truth with high accuracy. In addition,\naccuracy does not degrade when fewer data channels or lower resolutions are\nused.", "published": "2020-10-21 00:51:14", "link": "http://arxiv.org/abs/2010.10691v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Detection of COVID-19 through the analysis of vocal fold oscillations", "abstract": "Phonation, or the vibration of the vocal folds, is the primary source of\nvocalization in the production of voiced sounds by humans. It is a complex\nbio-mechanical process that is highly sensitive to changes in the speaker's\nrespiratory parameters. Since most symptomatic cases of COVID-19 present with\nmoderate to severe impairment of respiratory functions, we hypothesize that\nsignatures of COVID-19 may be observable by examining the vibrations of the\nvocal folds. Our goal is to validate this hypothesis, and to quantitatively\ncharacterize the changes observed to enable the detection of COVID-19 from\nvoice. For this, we use a dynamical system model for the oscillation of the\nvocal folds, and solve it using our recently developed ADLES algorithm to yield\nvocal fold oscillation patterns directly from recorded speech. Experimental\nresults on a clinically curated dataset of COVID-19 positive and negative\nsubjects reveal characteristic patterns of vocal fold oscillations that are\ncorrelated with COVID-19. We show that these are prominent and discriminative\nenough that even simple classifiers such as logistic regression yields high\ndetection accuracies using just the recordings of isolated extended vowels.", "published": "2020-10-21 01:44:42", "link": "http://arxiv.org/abs/2010.10707v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning Disentangled Phone and Speaker Representations in a\n  Semi-Supervised VQ-VAE Paradigm", "abstract": "We present a new approach to disentangle speaker voice and phone content by\nintroducing new components to the VQ-VAE architecture for speech synthesis. The\noriginal VQ-VAE does not generalize well to unseen speakers or content. To\nalleviate this problem, we have incorporated a speaker encoder and speaker VQ\ncodebook that learns global speaker characteristics entirely separate from the\nexisting sub-phone codebooks. We also compare two training methods:\nself-supervised with global conditions and semi-supervised with speaker labels.\nAdding a speaker VQ component improves objective measures of speech synthesis\nquality (estimated MOS, speaker similarity, ASR-based intelligibility) and\nprovides learned representations that are meaningful. Our speaker VQ codebook\nindices can be used in a simple speaker diarization task and perform slightly\nbetter than an x-vector baseline. Additionally, phones can be recognized from\nsub-phone VQ codebook indices in our semi-supervised VQ-VAE better than\nself-supervised with global conditions.", "published": "2020-10-21 02:41:11", "link": "http://arxiv.org/abs/2010.10727v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Trends at NIME -- Reflections on Editing \"A NIME Reader\"", "abstract": "This paper provides an overview of the process of editing the forthcoming\nanthology \"A NIME Reader - Fifteen Years of New Interfaces for Musical\nExpression.\" The selection process is presented, and we reflect on some of the\ntrends we have observed in re-discovering the collection of more than 1200 NIME\npapers published throughout the 15-year long history of the conference. An\nanthology is necessarily selective, and ours is no exception. As we present in\nthis paper, the aim has been to represent the wide range of artistic,\nscientific, and technological approaches that characterize the NIME conference.\nThe anthology also includes critical discourse, and through acknowledgment of\nthe strengths and weaknesses of the NIME community, we propose activities that\ncould further diversify and strengthen the field.", "published": "2020-10-21 07:43:16", "link": "http://arxiv.org/abs/2010.10803v1", "categories": ["cs.HC", "cs.SD", "eess.AS", "H.5.5"], "primary_category": "cs.HC"}
{"title": "Contrastive Learning of General-Purpose Audio Representations", "abstract": "We introduce COLA, a self-supervised pre-training approach for learning a\ngeneral-purpose representation of audio. Our approach is based on contrastive\nlearning: it learns a representation which assigns high similarity to audio\nsegments extracted from the same recording while assigning lower similarity to\nsegments from different recordings. We build on top of recent advances in\ncontrastive learning for computer vision and reinforcement learning to design a\nlightweight, easy-to-implement self-supervised model of audio. We pre-train\nembeddings on the large-scale Audioset database and transfer these\nrepresentations to 9 diverse classification tasks, including speech, music,\nanimal sounds, and acoustic scenes. We show that despite its simplicity, our\nmethod significantly outperforms previous self-supervised systems. We\nfurthermore conduct ablation studies to identify key design choices and release\na library to pre-train and fine-tune COLA models.", "published": "2020-10-21 11:56:22", "link": "http://arxiv.org/abs/2010.10915v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Complex data labeling with deep learning methods: Lessons from fisheries\n  acoustics", "abstract": "Quantitative and qualitative analysis of acoustic backscattered signals from\nthe seabed bottom to the sea surface is used worldwide for fish stocks\nassessment and marine ecosystem monitoring. Huge amounts of raw data are\ncollected yet require tedious expert labeling. This paper focuses on a case\nstudy where the ground truth labels are non-obvious: echograms labeling, which\nis time-consuming and critical for the quality of fisheries and ecological\nanalysis. We investigate how these tasks can benefit from supervised learning\nalgorithms and demonstrate that convolutional neural networks trained with\nnon-stationary datasets can be used to stress parts of a new dataset needing\nhuman expert correction. Further development of this approach paves the way\ntoward a standardization of the labeling process in fisheries acoustics and is\na good case study for non-obvious data labeling processes.", "published": "2020-10-21 13:49:34", "link": "http://arxiv.org/abs/2010.11010v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "WaveTransformer: A Novel Architecture for Audio Captioning Based on\n  Learning Temporal and Time-Frequency Information", "abstract": "Automated audio captioning (AAC) is a novel task, where a method takes as an\ninput an audio sample and outputs a textual description (i.e. a caption) of its\ncontents. Most AAC methods are adapted from from image captioning of machine\ntranslation fields. In this work we present a novel AAC novel method,\nexplicitly focused on the exploitation of the temporal and time-frequency\npatterns in audio. We employ three learnable processes for audio encoding, two\nfor extracting the local and temporal information, and one to merge the output\nof the previous two processes. To generate the caption, we employ the widely\nused Transformer decoder. We assess our method utilizing the freely available\nsplits of Clotho dataset. Our results increase previously reported highest\nSPIDEr to 17.3, from 16.2.", "published": "2020-10-21 16:02:25", "link": "http://arxiv.org/abs/2010.11098v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Joint Blind Room Acoustic Characterization From Speech And Music Signals\n  Using Convolutional Recurrent Neural Networks", "abstract": "Acoustic environment characterization opens doors for sound reproduction\ninnovations, smart EQing, speech enhancement, hearing aids, and forensics.\nReverberation time, clarity, and direct-to-reverberant ratio are acoustic\nparameters that have been defined to describe reverberant environments. They\nare closely related to speech intelligibility and sound quality. As explained\nin the ISO3382 standard, they can be derived from a room measurement called the\nRoom Impulse Response (RIR). However, measuring RIRs requires specific\nequipment and intrusive sound to be played. The recent audio combined with\nmachine learning suggests that one could estimate those parameters blindly\nusing speech or music signals. We follow these advances and propose a robust\nend-to-end method to achieve blind joint acoustic parameter estimation using\nspeech and/or music signals. Our results indicate that convolutional recurrent\nneural networks perform best for this task, and including music in training\nalso helps improve inference from speech.", "published": "2020-10-21 17:41:21", "link": "http://arxiv.org/abs/2010.11167v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AttendAffectNet: Self-Attention based Networks for Predicting Affective\n  Responses from Movies", "abstract": "In this work, we propose different variants of the self-attention based\nnetwork for emotion prediction from movies, which we call AttendAffectNet. We\ntake both audio and video into account and incorporate the relation among\nmultiple modalities by applying self-attention mechanism in a novel manner into\nthe extracted features for emotion prediction. We compare it to the typically\ntemporal integration of the self-attention based model, which in our case,\nallows to capture the relation of temporal representations of the movie while\nconsidering the sequential dependencies of emotion responses. We demonstrate\nthe effectiveness of our proposed architectures on the extended COGNIMUSE\ndataset [1], [2] and the MediaEval 2016 Emotional Impact of Movies Task [3],\nwhich consist of movies with emotion annotations. Our results show that\napplying the self-attention mechanism on the different audio-visual features,\nrather than in the time domain, is more effective for emotion prediction. Our\napproach is also proven to outperform many state-ofthe-art models for emotion\nprediction. The code to reproduce our results with the models' implementation\nis available at: https://github.com/ivyha010/AttendAffectNet.", "published": "2020-10-21 05:13:24", "link": "http://arxiv.org/abs/2010.11188v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning Speaker Embedding from Text-to-Speech", "abstract": "Zero-shot multi-speaker Text-to-Speech (TTS) generates target speaker voices\ngiven an input text and the corresponding speaker embedding. In this work, we\ninvestigate the effectiveness of the TTS reconstruction objective to improve\nrepresentation learning for speaker verification. We jointly trained end-to-end\nTacotron 2 TTS and speaker embedding networks in a self-supervised fashion. We\nhypothesize that the embeddings will contain minimal phonetic information since\nthe TTS decoder will obtain that information from the textual input. TTS\nreconstruction can also be combined with speaker classification to enhance\nthese embeddings further. Once trained, the speaker encoder computes\nrepresentations for the speaker verification task, while the rest of the TTS\nblocks are discarded. We investigated training TTS from either manual or\nASR-generated transcripts. The latter allows us to train embeddings on datasets\nwithout manual transcripts. We compared ASR transcripts and Kaldi phone\nalignments as TTS inputs, showing that the latter performed better due to their\nfiner resolution. Unsupervised TTS embeddings improved EER by 2.06\\% absolute\nwith regard to i-vectors for the LibriTTS dataset. TTS with speaker\nclassification loss improved EER by 0.28\\% and 0.73\\% absolutely from a model\nusing only speaker classification loss in LibriTTS and Voxceleb1 respectively.", "published": "2020-10-21 18:03:16", "link": "http://arxiv.org/abs/2010.11221v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dynamic Layer Customization for Noise Robust Speech Emotion Recognition\n  in Heterogeneous Condition Training", "abstract": "Robustness to environmental noise is important to creating automatic speech\nemotion recognition systems that are deployable in the real world. Prior work\non noise robustness has assumed that systems would not make use of\nsample-by-sample training noise conditions, or that they would have access to\nunlabelled testing data to generalize across noise conditions. We avoid these\nassumptions and introduce the resulting task as heterogeneous condition\ntraining. We show that with full knowledge of the test noise conditions, we can\nimprove performance by dynamically routing samples to specialized feature\nencoders for each noise condition, and with partial knowledge, we can use known\nnoise conditions and domain adaptation algorithms to train systems that\ngeneralize well to unseen noise conditions. We then extend these improvements\nto the multimodal setting by dynamically routing samples to maintain temporal\nordering, resulting in significant improvements over approaches that do not\nspecialize or generalize based on noise type.", "published": "2020-10-21 18:07:32", "link": "http://arxiv.org/abs/2010.11226v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Ultra-low power on-chip learning of speech commands with phase-change\n  memories", "abstract": "Embedding artificial intelligence at the edge (edge-AI) is an elegant\nsolution to tackle the power and latency issues in the rapidly expanding\nInternet of Things. As edge devices typically spend most of their time in sleep\nmode and only wake-up infrequently to collect and process sensor data,\nnon-volatile in-memory computing (NVIMC) is a promising approach to design the\nnext generation of edge-AI devices. Recently, we proposed an NVIMC-based\nneuromorphic accelerator using the phase change memories (PCMs), which we call\nas Raven. In this work, we demonstrate the ultra-low-power on-chip training and\ninference of speech commands using Raven. We showed that Raven can be trained\non-chip with power consumption as low as 30~uW, which is suitable for edge\napplications. Furthermore, we showed that at iso-accuracies, Raven needs 70.36x\nand 269.23x less number of computations to be performed than a deep neural\nnetwork (DNN) during inference and training, respectively. Owing to such low\npower and computational requirements, Raven provides a promising pathway\ntowards ultra-low-power training and inference at the edge.", "published": "2020-10-21 04:08:46", "link": "http://arxiv.org/abs/2010.11741v1", "categories": ["eess.AS", "cond-mat.dis-nn", "cs.AR", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
