{"title": "Multi-granularity hierarchical attention fusion networks for reading\n  comprehension and question answering", "abstract": "This paper describes a novel hierarchical attention network for reading\ncomprehension style question answering, which aims to answer questions for a\ngiven narrative paragraph. In the proposed method, attention and fusion are\nconducted horizontally and vertically across layers at different levels of\ngranularity between question and paragraph. Specifically, it first encode the\nquestion and paragraph with fine-grained language embeddings, to better capture\nthe respective representations at semantic level. Then it proposes a\nmulti-granularity fusion approach to fully fuse information from both global\nand attended representations. Finally, it introduces a hierarchical attention\nnetwork to focuses on the answer span progressively with multi-level\nsoftalignment. Extensive experiments on the large-scale SQuAD and TriviaQA\ndatasets validate the effectiveness of the proposed method. At the time of\nwriting the paper (Jan. 12th 2018), our model achieves the first position on\nthe SQuAD leaderboard for both single and ensemble models. We also achieves\nstate-of-the-art results on TriviaQA, AddSent and AddOne-Sent datasets.", "published": "2018-11-29 02:44:07", "link": "http://arxiv.org/abs/1811.11934v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large-scale Generative Modeling to Improve Automated Veterinary Disease\n  Coding", "abstract": "Supervised learning is limited both by the quantity and quality of the\nlabeled data. In the field of medical record tagging, writing styles between\nhospitals vary drastically. The knowledge learned from one hospital might not\ntransfer well to another. This problem is amplified in veterinary medicine\ndomain because veterinary clinics rarely apply medical codes to their records.\nWe proposed and trained the first large-scale generative modeling algorithm in\nautomated disease coding. We demonstrate that generative modeling can learn\ndiscriminative features when additionally trained with supervised fine-tuning.\nWe systematically ablate and evaluate the effect of generative modeling on the\nfinal system's performance. We compare the performance of our model with\nseveral baselines in a challenging cross-hospital setting with substantial\ndomain shift. We outperform competitive baselines by a large margin. In\naddition, we provide interpretation for what is learned by our model.", "published": "2018-11-29 04:26:07", "link": "http://arxiv.org/abs/1811.11958v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Non-entailed subsequences as a challenge for natural language inference", "abstract": "Neural network models have shown great success at natural language inference\n(NLI), the task of determining whether a premise entails a hypothesis. However,\nrecent studies suggest that these models may rely on fallible heuristics rather\nthan deep language understanding. We introduce a challenge set to test whether\nNLI systems adopt one such heuristic: assuming that a sentence entails all of\nits subsequences, such as assuming that \"Alice believes Mary is lying\" entails\n\"Alice believes Mary.\" We evaluate several competitive NLI models on this\nchallenge set and find strong evidence that they do rely on the subsequence\nheuristic.", "published": "2018-11-29 13:05:18", "link": "http://arxiv.org/abs/1811.12112v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Robustness of Neural Dialog Systems in a Data-Efficient Way\n  with Turn Dropout", "abstract": "Neural network-based dialog models often lack robustness to anomalous,\nout-of-domain (OOD) user input which leads to unexpected dialog behavior and\nthus considerably limits such models' usage in mission-critical production\nenvironments. The problem is especially relevant in the setting of dialog\nsystem bootstrapping with limited training data and no access to OOD examples.\nIn this paper, we explore the problem of robustness of such systems to\nanomalous input and the associated to it trade-off in accuracies on seen and\nunseen data. We present a new dataset for studying the robustness of dialog\nsystems to OOD input, which is bAbI Dialog Task 6 augmented with OOD content in\na controlled way. We then present turn dropout, a simple yet efficient negative\nsampling-based technique for improving robustness of neural dialog models. We\ndemonstrate its effectiveness applied to Hybrid Code Network-family models\n(HCNs) which reach state-of-the-art results on our OOD-augmented dataset as\nwell as the original one. Specifically, an HCN trained with turn dropout\nachieves state-of-the-art performance of more than 75% per-utterance accuracy\non the augmented dataset's OOD turns and 74% F1-score as an OOD detector.\nFurthermore, we introduce a Variational HCN enhanced with turn dropout which\nachieves more than 56.5% accuracy on the original bAbI Task 6 dataset, thus\noutperforming the initially reported HCN's result.", "published": "2018-11-29 14:03:00", "link": "http://arxiv.org/abs/1811.12148v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "HYPE: A High Performing NLP System for Automatically Detecting\n  Hypoglycemia Events from Electronic Health Record Notes", "abstract": "Hypoglycemia is common and potentially dangerous among those treated for\ndiabetes. Electronic health records (EHRs) are important resources for\nhypoglycemia surveillance. In this study, we report the development and\nevaluation of deep learning-based natural language processing systems to\nautomatically detect hypoglycemia events from the EHR narratives. Experts in\nPublic Health annotated 500 EHR notes from patients with diabetes. We used this\nannotated dataset to train and evaluate HYPE, supervised NLP systems for\nhypoglycemia detection. In our experiment, the convolutional neural network\nmodel yielded promising performance $Precision=0.96 \\pm 0.03, Recall=0.86 \\pm\n0.03, F1=0.91 \\pm 0.03$ in a 10-fold cross-validation setting. Despite the\nannotated data is highly imbalanced, our CNN-based HYPE system still achieved a\nhigh performance for hypoglycemia detection. HYPE could be used for EHR-based\nhypoglycemia surveillance and to facilitate clinicians for timely treatment of\nhigh-risk patients.", "published": "2018-11-29 03:40:55", "link": "http://arxiv.org/abs/1811.11945v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EvoMSA: A Multilingual Evolutionary Approach for Sentiment Analysis", "abstract": "Sentiment analysis (SA) is a task related to understanding people's feelings\nin written text; the starting point would be to identify the polarity level\n(positive, neutral or negative) of a given text, moving on to identify emotions\nor whether a text is humorous or not. This task has been the subject of several\nresearch competitions in a number of languages, e.g., English, Spanish, and\nArabic, among others. In this contribution, we propose an SA system, namely\nEvoMSA, that unifies our participating systems in various SA competitions,\nmaking it domain independent and multilingual by processing text using only\nlanguage-independent techniques. EvoMSA is a classifier, based on Genetic\nProgramming, that works by combining the output of different text classifiers\nand text models to produce the final prediction. We analyze EvoMSA on different\nSA competitions to provide a global overview of its performance, and as the\nresults show, EvoMSA is competitive obtaining top rankings in several SA\ncompetitions. Furthermore, we performed an analysis of EvoMSA's components to\nmeasure their contribution to the performance; the idea is to facilitate a\npractitioner or newcomer to implement a competitive SA classifier. Finally, it\nis worth to mention that EvoMSA is available as open-source software.", "published": "2018-11-29 23:33:59", "link": "http://arxiv.org/abs/1812.02307v4", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Counterfactual Learning from Human Proofreading Feedback for Semantic\n  Parsing", "abstract": "In semantic parsing for question-answering, it is often too expensive to\ncollect gold parses or even gold answers as supervision signals. We propose to\nconvert model outputs into a set of human-understandable statements which allow\nnon-expert users to act as proofreaders, providing error markings as learning\nsignals to the parser. Because model outputs were suggested by a historic\nsystem, we operate in a counterfactual, or off-policy, learning setup. We\nintroduce new estimators which can effectively leverage the given feedback and\nwhich avoid known degeneracies in counterfactual learning, while still being\napplicable to stochastic gradient optimization for neural semantic parsing.\nFurthermore, we discuss how our feedback collection method can be seamlessly\nintegrated into deployed virtual personal assistants that embed a semantic\nparser. Our work is the first to show that semantic parsers can be improved\nsignificantly by counterfactual learning from logged human feedback data.", "published": "2018-11-29 15:20:30", "link": "http://arxiv.org/abs/1811.12239v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Improving Hospital Mortality Prediction with Medical Named Entities and\n  Multimodal Learning", "abstract": "Clinical text provides essential information to estimate the acuity of a\npatient during hospital stays in addition to structured clinical data. In this\nstudy, we explore how clinical text can complement a clinical predictive\nlearning task. We leverage an internal medical natural language processing\nservice to perform named entity extraction and negation detection on clinical\nnotes and compose selected entities into a new text corpus to train document\nrepresentations. We then propose a multimodal neural network to jointly train\ntime series signals and unstructured clinical text representations to predict\nthe in-hospital mortality risk for ICU patients. Our model outperforms the\nbenchmark by 2% AUC.", "published": "2018-11-29 16:10:41", "link": "http://arxiv.org/abs/1811.12276v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Touchdown: Natural Language Navigation and Spatial Reasoning in Visual\n  Street Environments", "abstract": "We study the problem of jointly reasoning about language and vision through a\nnavigation and spatial reasoning task. We introduce the Touchdown task and\ndataset, where an agent must first follow navigation instructions in a\nreal-life visual urban environment, and then identify a location described in\nnatural language to find a hidden object at the goal position. The data\ncontains 9,326 examples of English instructions and spatial descriptions paired\nwith demonstrations. Empirical analysis shows the data presents an open\nchallenge to existing methods, and qualitative linguistic analysis shows that\nthe data displays richer use of spatial reasoning compared to related\nresources.", "published": "2018-11-29 18:06:22", "link": "http://arxiv.org/abs/1811.12354v7", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "The Effect of Heterogeneous Data for Alzheimer's Disease Detection from\n  Speech", "abstract": "Speech datasets for identifying Alzheimer's disease (AD) are generally\nrestricted to participants performing a single task, e.g. describing an image\nshown to them. As a result, models trained on linguistic features derived from\nsuch datasets may not be generalizable across tasks. Building on prior work\ndemonstrating that same-task data of healthy participants helps improve AD\ndetection on a single-task dataset of pathological speech, we augment an\nAD-specific dataset consisting of subjects describing a picture with multi-task\nhealthy data. We demonstrate that normative data from multiple speech-based\ntasks helps improve AD detection by up to 9%. Visualization of decision\nboundaries reveals that models trained on a combination of structured picture\ndescriptions and unstructured conversational speech have the least out-of-task\nerror and show the most potential to generalize to multiple tasks. We analyze\nthe impact of age of the added samples and if they affect fairness in\nclassification. We also provide explanations for a possible inductive bias\neffect across tasks using model-agnostic feature anchors. This work highlights\nthe need for heterogeneous datasets for encoding changes in multiple facets of\ncognition and for developing a task-independent AD detection model.", "published": "2018-11-29 15:37:45", "link": "http://arxiv.org/abs/1811.12254v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "LP-WaveNet: Linear Prediction-based WaveNet Speech Synthesis", "abstract": "We propose a linear prediction (LP)-based waveform generation method via\nWaveNet vocoding framework. A WaveNet-based neural vocoder has significantly\nimproved the quality of parametric text-to-speech (TTS) systems. However, it is\nchallenging to effectively train the neural vocoder when the target database\ncontains massive amount of acoustical information such as prosody, style or\nexpressiveness. As a solution, the approaches that only generate the vocal\nsource component by a neural vocoder have been proposed. However, they tend to\ngenerate synthetic noise because the vocal source component is independently\nhandled without considering the entire speech production process; where it is\ninevitable to come up with a mismatch between vocal source and vocal tract\nfilter. To address this problem, we propose an LP-WaveNet vocoder, where the\ncomplicated interactions between vocal source and vocal tract components are\njointly trained within a mixture density network-based WaveNet model. The\nexperimental results verify that the proposed system outperforms the\nconventional WaveNet vocoders both objectively and subjectively. In particular,\nthe proposed method achieves 4.47 MOS within the TTS framework.", "published": "2018-11-29 01:48:39", "link": "http://arxiv.org/abs/1811.11913v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Tuplemax Loss for Language Identification", "abstract": "In many scenarios of a language identification task, the user will specify a\nsmall set of languages which he/she can speak instead of a large set of all\npossible languages. We want to model such prior knowledge into the way we train\nour neural networks, by replacing the commonly used softmax loss function with\na novel loss function named tuplemax loss. As a matter of fact, a typical\nlanguage identification system launched in North America has about 95% users\nwho could speak no more than two languages. Using the tuplemax loss, our system\nachieved a 2.33% error rate, which is a relative 39.4% improvement over the\n3.85% error rate of standard softmax loss method.", "published": "2018-11-29 16:28:49", "link": "http://arxiv.org/abs/1811.12290v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "From Context to Concept: Exploring Semantic Relationships in Music with\n  Word2Vec", "abstract": "We explore the potential of a popular distributional semantics vector space\nmodel, word2vec, for capturing meaningful relationships in ecological (complex\npolyphonic) music. More precisely, the skip-gram version of word2vec is used to\nmodel slices of music from a large corpus spanning eight musical genres. In\nthis newly learned vector space, a metric based on cosine distance is able to\ndistinguish between functional chord relationships, as well as harmonic\nassociations in the music. Evidence, based on cosine distance between\nchord-pair vectors, suggests that an implicit circle-of-fifths exists in the\nvector space. In addition, a comparison between pieces in different keys\nreveals that key relationships are represented in word2vec space. These results\nsuggest that the newly learned embedded vector representation does in fact\ncapture tonal and harmonic characteristics of music, without receiving explicit\ninformation about the musical content of the constituent slices. In order to\ninvestigate whether proximity in the discovered space of embeddings is\nindicative of `semantically-related' slices, we explore a music generation\ntask, by automatically replacing existing slices from a given piece of music\nwith new slices. We propose an algorithm to find substitute slices based on\nspatial proximity and the pitch class distribution inferred in the chosen\nsubspace. The results indicate that the size of the subspace used has a\nsignificant effect on whether slices belonging to the same key are selected. In\nsum, the proposed word2vec model is able to learn music-vector embeddings that\ncapture meaningful tonal and harmonic relationships in music, thereby providing\na useful tool for exploring musical properties and comparisons across pieces,\nas a potential input representation for deep learning models, and as a music\ngeneration device.", "published": "2018-11-29 13:52:13", "link": "http://arxiv.org/abs/1811.12408v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS", "stat.ML", "68Txx, 68Wxx"], "primary_category": "cs.SD"}
{"title": "Naive Dictionary On Musical Corpora: From Knowledge Representation To\n  Pattern Recognition", "abstract": "In this paper, we propose and develop the novel idea of treating musical\nsheets as literary documents in the traditional text analytics parlance, to\nfully benefit from the vast amount of research already existing in statistical\ntext mining and topic modelling. We specifically introduce the idea of\nrepresenting any given piece of music as a collection of \"musical words\" that\nwe codenamed \"muselets\", which are essentially musical words of various\nlengths. Given the novelty and therefore the extremely difficulty of properly\nforming a complete version of a dictionary of muselets, the present paper\nfocuses on a simpler albeit naive version of the ultimate dictionary, which we\nrefer to as a Naive Dictionary because of the fact that all the words are of\nthe same length. We specifically herein construct a naive dictionary featuring\na corpus made up of African American, Chinese, Japanese and Arabic music, on\nwhich we perform both topic modelling and pattern recognition. Although some of\nthe results based on the Naive Dictionary are reasonably good, we anticipate\nphenomenal predictive performances once we get around to actually building a\nfull scale complete version of our intended dictionary of muselets.", "published": "2018-11-29 02:10:57", "link": "http://arxiv.org/abs/1811.12802v1", "categories": ["cs.IR", "cs.LG", "cs.SD", "eess.AS", "stat.ML", "62P15, 62P25, 62P99, 68W40, 68W01, 91E10, 91E45, 82-08, 62-07", "E.2; F.1.1; F.2.0; I.1.3; I.1.4; I.2.4; I.2.1; I.2.6; I.5.5; I.7.0"], "primary_category": "cs.IR"}
