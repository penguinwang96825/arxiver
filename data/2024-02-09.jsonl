{"title": "Exploring Group and Symmetry Principles in Large Language Models", "abstract": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of applications; however, assessing their reasoning capabilities\nremains a significant challenge. In this paper, we introduce a framework\ngrounded in group and symmetry principles, which have played a crucial role in\nfields such as physics and mathematics, and offer another way to evaluate their\ncapabilities. While the proposed framework is general, to showcase the benefits\nof employing these properties, we focus on arithmetic reasoning and investigate\nthe performance of these models on four group properties: closure, identity,\ninverse, and associativity. Our findings reveal that LLMs studied in this work\nstruggle to preserve group properties across different test regimes. In the\nclosure test, we observe biases towards specific outputs and an abrupt\ndegradation in their performance from 100% to 0% after a specific sequence\nlength. They also perform poorly in the identity test, which represents adding\nirrelevant information in the context, and show sensitivity when subjected to\ninverse test, which examines the robustness of the model with respect to\nnegation. In addition, we demonstrate that breaking down problems into smaller\nsteps helps LLMs in the associativity test that we have conducted. To support\nthese tests we have developed a synthetic dataset which will be released.", "published": "2024-02-09 01:10:25", "link": "http://arxiv.org/abs/2402.06120v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Model Sentence Completion with a Parser-Driven Rhetorical\n  Control Method", "abstract": "Controlled text generation (CTG) seeks to guide large language model (LLM)\noutput to produce text that conforms to desired criteria. The current study\npresents a novel CTG algorithm that enforces adherence toward specific\nrhetorical relations in an LLM sentence-completion context by a parser-driven\ndecoding scheme that requires no model fine-tuning. The method is validated\nboth with automatic and human evaluation. The code is accessible on GitHub.", "published": "2024-02-09 01:15:42", "link": "http://arxiv.org/abs/2402.06125v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Model Editing with Canonical Examples", "abstract": "We introduce model editing with canonical examples, a setting in which (1) a\nsingle learning example is provided per desired behavior, (2) evaluation is\nperformed exclusively out-of-distribution, and (3) deviation from an initial\nmodel is strictly limited. A canonical example is a simple instance of good\nbehavior, e.g., The capital of Mauritius is Port Louis) or bad behavior, e.g.,\nAn aspect of researchers is coldhearted). The evaluation set contains more\ncomplex examples of each behavior (like a paragraph in which the capital of\nMauritius is called for.) We create three datasets and modify three more for\nmodel editing with canonical examples, covering knowledge-intensive\nimprovements, social bias mitigation, and syntactic edge cases. In our\nexperiments on Pythia language models, we find that LoRA outperforms full\nfinetuning and MEMIT. We then turn to the Backpack language model architecture\nbecause it is intended to enable targeted improvement. The Backpack defines a\nlarge bank of sense vectors--a decomposition of the different uses of each\nword--which are weighted and summed to form the output logits of the model. We\npropose sense finetuning, which selects and finetunes a few ($\\approx$ 10)\nsense vectors for each canonical example, and find that it outperforms other\nfinetuning methods, e.g., 4.8% improvement vs 0.3%. Finally, we improve\nGPT-J-6B by an inference-time ensemble with just the changes from sense\nfinetuning of a 35x smaller Backpack, in one setting outperforming editing\nGPT-J itself (4.1% vs 1.0%).", "published": "2024-02-09 03:08:12", "link": "http://arxiv.org/abs/2402.06155v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Unified Causal View of Instruction Tuning", "abstract": "Instruction tuning on a mixture of tasks has improved zero-shot capabilities\nin natural language processing (NLP). Nevertheless, existing methods often\nlearn features that exhibit correlations between instruction-formatted samples\nand target labels, rather than causal relationships. Termed as ``spurious\ncorrelation'' in statistics, such a correlation may change drastically in a new\ntask, making the effect from the learned features to be misleading. To this\nend, we develop a meta Structural Causal Model (meta-SCM) to integrate\ndifferent NLP tasks under a single causal structure of the data. Specifically,\nthe meta-SCM introduces multiple latent factors that represent properties of\nsource context, only some of which causally influence the target labels for a\nspecific task. The key idea is to learn task-required causal factors and only\nuse those to make predictions for a given task. Theoretically, we prove the\ncausal factor can be identified without mixing information from others. Guided\nby the identifiability, we propose a Structural Instruction Tuning (SIT) method\nto learn the task-required causal representations that can mimic the causal\nfactors for each task. The utility of our approach is verified by improvements\nof zero-shot ability on a range of unseen datasets and tasks.", "published": "2024-02-09 07:12:56", "link": "http://arxiv.org/abs/2402.06220v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InternLM-Math: Open Math Large Language Models Toward Verifiable\n  Reasoning", "abstract": "The math abilities of large language models can represent their abstract\nreasoning ability. In this paper, we introduce and open-source our math\nreasoning LLMs InternLM-Math which is continue pre-trained from InternLM2. We\nunify chain-of-thought reasoning, reward modeling, formal reasoning, data\naugmentation, and code interpreter in a unified seq2seq format and supervise\nour model to be a versatile math reasoner, verifier, prover, and augmenter.\nThese abilities can be used to develop the next math LLMs or self-iteration.\nInternLM-Math obtains open-sourced state-of-the-art performance under the\nsetting of in-context learning, supervised fine-tuning, and code-assisted\nreasoning in various informal and formal benchmarks including GSM8K, MATH,\nHungary math exam, MathBench-ZH, and MiniF2F. Our pre-trained model achieves\n30.3 on the MiniF2F test set without fine-tuning. We further explore how to use\nLEAN to solve math problems and study its performance under the setting of\nmulti-task learning which shows the possibility of using LEAN as a unified\nplatform for solving and proving in math. Our models, codes, and data are\nreleased at \\url{https://github.com/InternLM/InternLM-Math}.", "published": "2024-02-09 11:22:08", "link": "http://arxiv.org/abs/2402.06332v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RareBench: Can LLMs Serve as Rare Diseases Specialists?", "abstract": "Generalist Large Language Models (LLMs), such as GPT-4, have shown\nconsiderable promise in various domains, including medical diagnosis. Rare\ndiseases, affecting approximately 300 million people worldwide, often have\nunsatisfactory clinical diagnosis rates primarily due to a lack of experienced\nphysicians and the complexity of differentiating among many rare diseases. In\nthis context, recent news such as \"ChatGPT correctly diagnosed a 4-year-old's\nrare disease after 17 doctors failed\" underscore LLMs' potential, yet\nunderexplored, role in clinically diagnosing rare diseases. To bridge this\nresearch gap, we introduce RareBench, a pioneering benchmark designed to\nsystematically evaluate the capabilities of LLMs on 4 critical dimensions\nwithin the realm of rare diseases. Meanwhile, we have compiled the largest\nopen-source dataset on rare disease patients, establishing a benchmark for\nfuture studies in this domain. To facilitate differential diagnosis of rare\ndiseases, we develop a dynamic few-shot prompt methodology, leveraging a\ncomprehensive rare disease knowledge graph synthesized from multiple knowledge\nbases, significantly enhancing LLMs' diagnostic performance. Moreover, we\npresent an exhaustive comparative study of GPT-4's diagnostic capabilities\nagainst those of specialist physicians. Our experimental findings underscore\nthe promising potential of integrating LLMs into the clinical diagnostic\nprocess for rare diseases. This paves the way for exciting possibilities in\nfuture advancements in this field.", "published": "2024-02-09 11:34:16", "link": "http://arxiv.org/abs/2402.06341v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Promoting Target Data in Context-aware Neural Machine Translation", "abstract": "Standard context-aware neural machine translation (NMT) typically relies on\nparallel document-level data, exploiting both source and target contexts.\nConcatenation-based approaches in particular, still a strong baseline for\ndocument-level NMT, prepend source and/or target context sentences to the\nsentences to be translated, with model variants that exploit equal amounts of\nsource and target data on each side achieving state-of-the-art results. In this\nwork, we investigate whether target data should be further promoted within\nstandard concatenation-based approaches, as most document-level phenomena rely\non information that is present on the target language side. We evaluate novel\nconcatenation-based variants where the target context is prepended to the\nsource language, either in isolation or in combination with the source context.\nExperimental results in English-Russian and Basque-Spanish show that including\ntarget context in the source leads to large improvements on target language\nphenomena. On source-dependent phenomena, using only target language context in\nthe source achieves parity with state-of-the-art concatenation approaches, or\nslightly underperforms, whereas combining source and target context on the\nsource side leads to significant gains across the board.", "published": "2024-02-09 11:34:39", "link": "http://arxiv.org/abs/2402.06342v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Findings of the First Workshop on Simulating Conversational Intelligence\n  in Chat", "abstract": "The aim of the workshop was to bring together experts working on open-domain\ndialogue research. In this speedily advancing research area many challenges\nstill exist, such as learning information from conversations, and engaging in a\nrealistic and convincing simulation of human intelligence and reasoning.\nSCI-CHAT follows previous workshops on open domain dialogue but in contrast the\nfocus of the shared task is simulation of intelligent conversation as judged in\na live human evaluation. Models aim to include the ability to follow a\nchallenging topic over a multi-turn conversation, while positing, refuting and\nreasoning over arguments. The workshop included both a research track and\nshared task. The main goal of this paper is to provide an overview of the\nshared task, and an in depth analysis of the shared task results following\npresentation at the workshop. The current paper is an extension of that made\navailable prior to presentation of results at the workshop at EACL Malta\n(Graham et al., 2024). The data collected in the evaluation was made publicly\navailable to aide future research. The code was also made available for the\nsame purpose.", "published": "2024-02-09 14:08:23", "link": "http://arxiv.org/abs/2402.06420v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explaining Veracity Predictions with Evidence Summarization: A\n  Multi-Task Model Approach", "abstract": "The rapid dissemination of misinformation through social media increased the\nimportance of automated fact-checking. Furthermore, studies on what deep neural\nmodels pay attention to when making predictions have increased in recent years.\nWhile significant progress has been made in this field, it has not yet reached\na level of reasoning comparable to human reasoning. To address these gaps, we\npropose a multi-task explainable neural model for misinformation detection.\nSpecifically, this work formulates an explanation generation process of the\nmodel's veracity prediction as a text summarization problem. Additionally, the\nperformance of the proposed model is discussed on publicly available datasets\nand the findings are evaluated with related studies.", "published": "2024-02-09 14:39:20", "link": "http://arxiv.org/abs/2402.06443v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FaBERT: Pre-training BERT on Persian Blogs", "abstract": "We introduce FaBERT, a Persian BERT-base model pre-trained on the HmBlogs\ncorpus, encompassing both informal and formal Persian texts. FaBERT is designed\nto excel in traditional Natural Language Understanding (NLU) tasks, addressing\nthe intricacies of diverse sentence structures and linguistic styles prevalent\nin the Persian language. In our comprehensive evaluation of FaBERT on 12\ndatasets in various downstream tasks, encompassing Sentiment Analysis (SA),\nNamed Entity Recognition (NER), Natural Language Inference (NLI), Question\nAnswering (QA), and Question Paraphrasing (QP), it consistently demonstrated\nimproved performance, all achieved within a compact model size. The findings\nhighlight the importance of utilizing diverse and cleaned corpora, such as\nHmBlogs, to enhance the performance of language models like BERT in Persian\nNatural Language Processing (NLP) applications. FaBERT is openly accessible at\nhttps://huggingface.co/sbunlp/fabert", "published": "2024-02-09 18:50:51", "link": "http://arxiv.org/abs/2402.06617v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding the Effects of Iterative Prompting on Truthfulness", "abstract": "The development of Large Language Models (LLMs) has notably transformed\nnumerous sectors, offering impressive text generation capabilities. Yet, the\nreliability and truthfulness of these models remain pressing concerns. To this\nend, we investigate iterative prompting, a strategy hypothesized to refine LLM\nresponses, assessing its impact on LLM truthfulness, an area which has not been\nthoroughly explored. Our extensive experiments delve into the intricacies of\niterative prompting variants, examining their influence on the accuracy and\ncalibration of model responses. Our findings reveal that naive prompting\nmethods significantly undermine truthfulness, leading to exacerbated\ncalibration errors. In response to these challenges, we introduce several\nprompting variants designed to address the identified issues. These variants\ndemonstrate marked improvements over existing baselines, signaling a promising\ndirection for future research. Our work provides a nuanced understanding of\niterative prompting and introduces novel approaches to enhance the truthfulness\nof LLMs, thereby contributing to the development of more accurate and\ntrustworthy AI systems.", "published": "2024-02-09 18:57:08", "link": "http://arxiv.org/abs/2402.06625v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EntGPT: Linking Generative Large Language Models with Knowledge Bases", "abstract": "Entity Linking in natural language processing seeks to match text entities to\ntheir corresponding entries in a dictionary or knowledge base. Traditional\napproaches rely on contextual models, which can be complex, hard to train, and\nhave limited transferability across different domains. Generative large\nlanguage models like GPT offer a promising alternative but often underperform\nwith naive prompts. In this study, we introduce EntGPT, employing advanced\nprompt engineering to enhance EL tasks. Our three-step hard-prompting method\n(EntGPT-P) significantly boosts the micro-F_1 score by up to 36% over vanilla\nprompts, achieving competitive performance across 10 datasets without\nsupervised fine-tuning. Additionally, our instruction tuning method (EntGPT-I)\nimproves micro-F_1 scores by 2.1% on average in supervised EL tasks and\noutperforms several baseline models in six Question Answering tasks. Our\nmethods are compatible with both open-source and proprietary LLMs. All data and\ncode are available on GitHub at https://github.com/yifding/In_Context_EL.", "published": "2024-02-09 19:16:27", "link": "http://arxiv.org/abs/2402.06738v2", "categories": ["cs.CL", "H.3.3"], "primary_category": "cs.CL"}
{"title": "Large Language Models: A Survey", "abstract": "Large Language Models (LLMs) have drawn a lot of attention due to their\nstrong performance on a wide range of natural language tasks, since the release\nof ChatGPT in November 2022. LLMs' ability of general-purpose language\nunderstanding and generation is acquired by training billions of model's\nparameters on massive amounts of text data, as predicted by scaling laws\n\\cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while\nvery recent, is evolving rapidly in many different ways. In this paper, we\nreview some of the most prominent LLMs, including three popular LLM families\n(GPT, LLaMA, PaLM), and discuss their characteristics, contributions and\nlimitations. We also give an overview of techniques developed to build, and\naugment LLMs. We then survey popular datasets prepared for LLM training,\nfine-tuning, and evaluation, review widely used LLM evaluation metrics, and\ncompare the performance of several popular LLMs on a set of representative\nbenchmarks. Finally, we conclude the paper by discussing open challenges and\nfuture research directions.", "published": "2024-02-09 05:37:09", "link": "http://arxiv.org/abs/2402.06196v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Generative AI Paradox on Evaluation: What It Can Solve, It May Not\n  Evaluate", "abstract": "This paper explores the assumption that Large Language Models (LLMs) skilled\nin generation tasks are equally adept as evaluators. We assess the performance\nof three LLMs and one open-source LM in Question-Answering (QA) and evaluation\ntasks using the TriviaQA (Joshi et al., 2017) dataset. Results indicate a\nsignificant disparity, with LLMs exhibiting lower performance in evaluation\ntasks compared to generation tasks. Intriguingly, we discover instances of\nunfaithful evaluation where models accurately evaluate answers in areas where\nthey lack competence, underscoring the need to examine the faithfulness and\ntrustworthiness of LLMs as evaluators. This study contributes to the\nunderstanding of \"the Generative AI Paradox\" (West et al., 2023), highlighting\na need to explore the correlation between generative excellence and evaluation\nproficiency, and the necessity to scrutinize the faithfulness aspect in model\nevaluations.", "published": "2024-02-09 06:16:08", "link": "http://arxiv.org/abs/2402.06204v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ResumeFlow: An LLM-facilitated Pipeline for Personalized Resume\n  Generation and Refinement", "abstract": "Crafting the ideal, job-specific resume is a challenging task for many job\napplicants, especially for early-career applicants. While it is highly\nrecommended that applicants tailor their resume to the specific role they are\napplying for, manually tailoring resumes to job descriptions and role-specific\nrequirements is often (1) extremely time-consuming, and (2) prone to human\nerrors. Furthermore, performing such a tailoring step at scale while applying\nto several roles may result in a lack of quality of the edited resumes. To\ntackle this problem, in this demo paper, we propose ResumeFlow: a Large\nLanguage Model (LLM) aided tool that enables an end user to simply provide\ntheir detailed resume and the desired job posting, and obtain a personalized\nresume specifically tailored to that specific job posting in the matter of a\nfew seconds. Our proposed pipeline leverages the language understanding and\ninformation extraction capabilities of state-of-the-art LLMs such as OpenAI's\nGPT-4 and Google's Gemini, in order to (1) extract details from a job\ndescription, (2) extract role-specific details from the user-provided resume,\nand then (3) use these to refine and generate a role-specific resume for the\nuser. Our easy-to-use tool leverages the user-chosen LLM in a completely\noff-the-shelf manner, thus requiring no fine-tuning. We demonstrate the\neffectiveness of our tool via a video demo and propose novel task-specific\nevaluation metrics to control for alignment and hallucination. Our tool is\navailable at https://job-aligned-resume.streamlit.app.", "published": "2024-02-09 07:13:44", "link": "http://arxiv.org/abs/2402.06221v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "On the Efficacy of Eviction Policy for Key-Value Constrained Generative\n  Language Model Inference", "abstract": "Despite the recent success associated with Large Language Models (LLMs), they\nare notably cost-prohibitive to deploy in resource-constrained environments due\nto their excessive memory and computational demands. In addition to model\nparameters, the key-value cache is also stored in GPU memory, growing linearly\nwith batch size and sequence length. As a remedy, recent works have proposed\nvarious eviction policies for maintaining the overhead of key-value cache under\na given budget. This paper embarks on the efficacy of existing eviction\npolicies in terms of importance score calculation and eviction scope\nconstruction. We identify the deficiency of prior policies in these two aspects\nand introduce RoCo, a robust cache omission policy based on temporal attention\nscores and robustness measures. Extensive experimentation spanning prefilling\nand auto-regressive decoding stages validates the superiority of RoCo. Finally,\nwe release EasyKV, a versatile software package dedicated to user-friendly\nkey-value constrained generative inference. Code available at\nhttps://github.com/DRSY/EasyKV.", "published": "2024-02-09 09:20:59", "link": "http://arxiv.org/abs/2402.06262v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Asking the Right Question at the Right Time: Human and Model Uncertainty\n  Guidance to Ask Clarification Questions", "abstract": "Clarification questions are an essential dialogue tool to signal\nmisunderstanding, ambiguities, and under-specification in language use. While\nhumans are able to resolve uncertainty by asking questions since childhood,\nmodern dialogue systems struggle to generate effective questions. To make\nprogress in this direction, in this work we take a collaborative dialogue task\nas a testbed and study how model uncertainty relates to human uncertainty -- an\nas yet under-explored problem. We show that model uncertainty does not mirror\nhuman clarification-seeking behavior, which suggests that using human\nclarification questions as supervision for deciding when to ask may not be the\nmost effective way to resolve model uncertainty. To address this issue, we\npropose an approach to generating clarification questions based on model\nuncertainty estimation, compare it to several alternatives, and show that it\nleads to significant improvements in terms of task success. Our findings\nhighlight the importance of equipping dialogue systems with the ability to\nassess their own uncertainty and exploit in interaction.", "published": "2024-02-09 16:15:30", "link": "http://arxiv.org/abs/2402.06509v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multimodal Clinical Trial Outcome Prediction with Large Language Models", "abstract": "The clinical trial is a pivotal and costly process, often spanning multiple\nyears and requiring substantial financial resources. Therefore, the development\nof clinical trial outcome prediction models aims to exclude drugs likely to\nfail and holds the potential for significant cost savings. Recent data-driven\nattempts leverage deep learning methods to integrate multimodal data for\npredicting clinical trial outcomes. However, these approaches rely on manually\ndesigned modal-specific encoders, which limits both the extensibility to adapt\nnew modalities and the ability to discern similar information patterns across\ndifferent modalities. To address these issues, we propose a multimodal\nmixture-of-experts (LIFTED) approach for clinical trial outcome prediction.\nSpecifically, LIFTED unifies different modality data by transforming them into\nnatural language descriptions. Then, LIFTED constructs unified noise-resilient\nencoders to extract information from modal-specific language descriptions.\nSubsequently, a sparse Mixture-of-Experts framework is employed to further\nrefine the representations, enabling LIFTED to identify similar information\npatterns across different modalities and extract more consistent\nrepresentations from those patterns using the same expert model. Finally, a\nmixture-of-experts module is further employed to dynamically integrate\ndifferent modality representations for prediction, which gives LIFTED the\nability to automatically weigh different modalities and pay more attention to\ncritical information. The experiments demonstrate that LIFTED significantly\nenhances performance in predicting clinical trial outcomes across all three\nphases compared to the best baseline, showcasing the effectiveness of our\nproposed key components.", "published": "2024-02-09 16:18:38", "link": "http://arxiv.org/abs/2402.06512v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Bryndza at ClimateActivism 2024: Stance, Target and Hate Event Detection\n  via Retrieval-Augmented GPT-4 and LLaMA", "abstract": "This study details our approach for the CASE 2024 Shared Task on Climate\nActivism Stance and Hate Event Detection, focusing on Hate Speech Detection,\nHate Speech Target Identification, and Stance Detection as classification\nchallenges. We explored the capability of Large Language Models (LLMs),\nparticularly GPT-4, in zero- or few-shot settings enhanced by retrieval\naugmentation and re-ranking for Tweet classification. Our goal was to determine\nif LLMs could match or surpass traditional methods in this context.\n  We conducted an ablation study with LLaMA for comparison, and our results\nindicate that our models significantly outperformed the baselines, securing\nsecond place in the Target Detection task. The code for our submission is\navailable at https://github.com/NaiveNeuron/bryndza-case-2024", "published": "2024-02-09 17:02:41", "link": "http://arxiv.org/abs/2402.06549v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "G-SciEdBERT: A Contextualized LLM for Science Assessment Tasks in German", "abstract": "The advancement of natural language processing has paved the way for\nautomated scoring systems in various languages, such as German (e.g., German\nBERT [G-BERT]). Automatically scoring written responses to science questions in\nGerman is a complex task and challenging for standard G-BERT as they lack\ncontextual knowledge in the science domain and may be unaligned with student\nwriting styles. This paper presents a contextualized German Science Education\nBERT (G-SciEdBERT), an innovative large language model tailored for scoring\nGerman-written responses to science tasks and beyond. Using G-BERT, we\npre-trained G-SciEdBERT on a corpus of 30K German written science responses\nwith 3M tokens on the Programme for International Student Assessment (PISA)\n2018. We fine-tuned G-SciEdBERT on an additional 20K student-written responses\nwith 2M tokens and examined the scoring accuracy. We then compared its scoring\nperformance with G-BERT. Our findings revealed a substantial improvement in\nscoring accuracy with G-SciEdBERT, demonstrating a 10.2% increase of quadratic\nweighted Kappa compared to G-BERT (mean difference = 0.1026, SD = 0.069). These\ninsights underline the significance of specialized language models like\nG-SciEdBERT, which is trained to enhance the accuracy of contextualized\nautomated scoring, offering a substantial contribution to the field of AI in\neducation.", "published": "2024-02-09 18:05:03", "link": "http://arxiv.org/abs/2402.06584v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TIC: Translate-Infer-Compile for accurate \"text to plan\" using LLMs and\n  Logical Representations", "abstract": "We study the problem of generating plans for given natural language planning\ntask requests. On one hand, LLMs excel at natural language processing but do\nnot perform well on planning. On the other hand, classical planning tools excel\nat planning tasks but require input in a structured language such as the\nPlanning Domain Definition Language (PDDL). We leverage the strengths of both\nthe techniques by using an LLM for generating the PDDL representation (task\nPDDL) of planning task requests followed by using a classical planner for\ncomputing a plan. Unlike previous approaches that use LLMs for generating task\nPDDLs directly, our approach comprises of (a) translate: using an LLM only for\ngenerating a logically interpretable intermediate representation of natural\nlanguage task description, (b) infer: deriving additional logically dependent\ninformation from the intermediate representation using a logic reasoner\n(currently, Answer Set Programming solver), and (c) compile: generating the\ntarget task PDDL from the base and inferred information. We observe that using\nan LLM to only output the intermediate representation significantly reduces LLM\nerrors. Consequently, TIC approach achieves, for at least one LLM, high\naccuracy on task PDDL generation for all seven domains of our evaluation\ndataset.", "published": "2024-02-09 18:39:13", "link": "http://arxiv.org/abs/2402.06608v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Aya Dataset: An Open-Access Collection for Multilingual Instruction\n  Tuning", "abstract": "Datasets are foundational to many breakthroughs in modern artificial\nintelligence. Many recent achievements in the space of natural language\nprocessing (NLP) can be attributed to the finetuning of pre-trained models on a\ndiverse set of tasks that enables a large language model (LLM) to respond to\ninstructions. Instruction fine-tuning (IFT) requires specifically constructed\nand annotated datasets. However, existing datasets are almost all in the\nEnglish language. In this work, our primary goal is to bridge the language gap\nby building a human-curated instruction-following dataset spanning 65\nlanguages. We worked with fluent speakers of languages from around the world to\ncollect natural instances of instructions and completions. Furthermore, we\ncreate the most extensive multilingual collection to date, comprising 513\nmillion instances through templating and translating existing datasets across\n114 languages. In total, we contribute four key resources: we develop and\nopen-source the Aya Annotation Platform, the Aya Dataset, the Aya Collection,\nand the Aya Evaluation Suite. The Aya initiative also serves as a valuable case\nstudy in participatory research, involving collaborators from 119 countries. We\nsee this as a valuable framework for future research collaborations that aim to\nbridge gaps in resources.", "published": "2024-02-09 18:51:49", "link": "http://arxiv.org/abs/2402.06619v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluation Metrics for Text Data Augmentation in NLP", "abstract": "Recent surveys on data augmentation for natural language processing have\nreported different techniques and advancements in the field. Several\nframeworks, tools, and repositories promote the implementation of text data\naugmentation pipelines. However, a lack of evaluation criteria and standards\nfor method comparison due to different tasks, metrics, datasets, architectures,\nand experimental settings makes comparisons meaningless. Also, a lack of\nmethods unification exists and text data augmentation research would benefit\nfrom unified metrics to compare different augmentation methods. Thus, academics\nand the industry endeavor relevant evaluation metrics for text data\naugmentation techniques. The contribution of this work is to provide a taxonomy\nof evaluation metrics for text augmentation methods and serve as a direction\nfor a unified benchmark. The proposed taxonomy organizes categories that\ninclude tools for implementation and metrics calculation. Finally, with this\nstudy, we intend to present opportunities to explore the unification and\nstandardization of text data augmentation metrics.", "published": "2024-02-09 19:59:34", "link": "http://arxiv.org/abs/2402.06766v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Debating with More Persuasive LLMs Leads to More Truthful Answers", "abstract": "Common methods for aligning large language models (LLMs) with desired\nbehaviour heavily rely on human-labelled data. However, as models grow\nincreasingly sophisticated, they will surpass human expertise, and the role of\nhuman evaluation will evolve into non-experts overseeing experts. In\nanticipation of this, we ask: can weaker models assess the correctness of\nstronger models? We investigate this question in an analogous setting, where\nstronger models (experts) possess the necessary information to answer questions\nand weaker models (non-experts) lack this information. The method we evaluate\nis debate, where two LLM experts each argue for a different answer, and a\nnon-expert selects the answer. We find that debate consistently helps both\nnon-expert models and humans answer questions, achieving 76% and 88% accuracy\nrespectively (naive baselines obtain 48% and 60%). Furthermore, optimising\nexpert debaters for persuasiveness in an unsupervised manner improves\nnon-expert ability to identify the truth in debates. Our results provide\nencouraging empirical evidence for the viability of aligning models with debate\nin the absence of ground truth.", "published": "2024-02-09 21:05:01", "link": "http://arxiv.org/abs/2402.06782v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Forecasting Events in Soccer Matches Through Language", "abstract": "This paper introduces an approach to predicting the next event in a soccer\nmatch, a challenge bearing remarkable similarities to the problem faced by\nLarge Language Models (LLMs). Unlike other methods that severely limit event\ndynamics in soccer, often abstracting from many variables or relying on a mix\nof sequential models, our research proposes a novel technique inspired by the\nmethodologies used in LLMs. These models predict a complete chain of variables\nthat compose an event, significantly simplifying the construction of Large\nEvent Models (LEMs) for soccer. Utilizing deep learning on the publicly\navailable WyScout dataset, the proposed approach notably surpasses the\nperformance of previous LEM proposals in critical areas, such as the prediction\naccuracy of the next event type. This paper highlights the utility of LEMs in\nvarious applications, including match prediction and analytics. Moreover, we\nshow that LEMs provide a simulation backbone for users to build many analytics\npipelines, an approach opposite to the current specialized single-purpose\nmodels. LEMs represent a pivotal advancement in soccer analytics, establishing\na foundational framework for multifaceted analytics pipelines through a\nsingular machine-learning model.", "published": "2024-02-09 23:02:57", "link": "http://arxiv.org/abs/2402.06820v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Zero-shot Explainable Mental Health Analysis on Social Media by\n  Incorporating Mental Scales", "abstract": "Traditional discriminative approaches in mental health analysis are known for\ntheir strong capacity but lack interpretability and demand large-scale\nannotated data. The generative approaches, such as those based on large\nlanguage models (LLMs), have the potential to get rid of heavy annotations and\nprovide explanations but their capabilities still fall short compared to\ndiscriminative approaches, and their explanations may be unreliable due to the\nfact that the generation of explanation is a black-box process. Inspired by the\npsychological assessment practice of using scales to evaluate mental states,\nour method which is called Mental Analysis by Incorporating Mental Scales\n(MAIMS), incorporates two procedures via LLMs. First, the patient completes\nmental scales, and second, the psychologist interprets the collected\ninformation from the mental scales and makes informed decisions. Experimental\nresults show that MAIMS outperforms other zero-shot methods. MAIMS can generate\nmore rigorous explanation based on the outputs of mental scales", "published": "2024-02-09 09:44:06", "link": "http://arxiv.org/abs/2402.10948v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learn To be Efficient: Build Structured Sparsity in Large Language\n  Models", "abstract": "Large Language Models (LLMs) have achieved remarkable success with their\nbillion-level parameters, yet they incur high inference overheads. The\nemergence of activation sparsity in LLMs provides a natural approach to reduce\nthis cost by involving only parts of the parameters for inference. However,\nexisting methods only focus on utilizing this naturally formed activation\nsparsity in a post-training setting, overlooking the potential for further\namplifying this inherent sparsity. In this paper, we hypothesize that LLMs can\nlearn to be efficient by achieving more structured activation sparsity. To\nachieve this, we introduce a novel training algorithm, Learn-To-be-Efficient\n(LTE), designed to train efficiency-aware LLMs to learn to activate fewer\nneurons and achieve a better trade-off between sparsity and performance.\nFurthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based\nmodels, LTE can also be applied to LLMs like LLaMA using non-ReLU activations.\nExtensive evaluation on language understanding, language generation, and\ninstruction tuning tasks show that LTE consistently outperforms SOTA baselines.\nAlong with our hardware-aware custom kernel implementation, LTE reduces\nLLaMA2-7B inference latency by 25% at 50% sparsity.", "published": "2024-02-09 01:18:16", "link": "http://arxiv.org/abs/2402.06126v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fight Back Against Jailbreaking via Prompt Adversarial Tuning", "abstract": "While Large Language Models (LLMs) have achieved tremendous success in\nvarious applications, they are also susceptible to jailbreaking attacks.\nSeveral primary defense strategies have been proposed to protect LLMs from\nproducing harmful information, mostly focusing on model fine-tuning or\nheuristical defense designs. However, how to achieve intrinsic robustness\nthrough prompt optimization remains an open problem. In this paper, motivated\nby adversarial training paradigms for achieving reliable robustness, we propose\nan approach named Prompt Adversarial Tuning (PAT) that trains a prompt control\nattached to the user prompt as a guard prefix. To achieve our defense goal\nwhilst maintaining natural performance, we optimize the control prompt with\nboth adversarial and benign prompts. Comprehensive experiments show that our\nmethod is effective against both grey-box and black-box attacks, reducing the\nsuccess rate of advanced attacks to nearly 0%, while maintaining the model's\nutility on the benign task and incurring only negligible computational\noverhead, charting a new perspective for future explorations in LLM security.\nOur code is available at https://github.com/PKU-ML/PAT.", "published": "2024-02-09 09:09:39", "link": "http://arxiv.org/abs/2402.06255v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "LLaVA-Docent: Instruction Tuning with Multimodal Large Language Model to\n  Support Art Appreciation Education", "abstract": "Despite the development of various AI systems to support learning in various\ndomains, AI assistance for art appreciation education has not been extensively\nexplored. Art appreciation, often perceived as an unfamiliar and challenging\nendeavor for most students, can be more accessible with a generative AI enabled\nconversation partner that provides tailored questions and encourages the\naudience to deeply appreciate artwork. This study explores the application of\nmultimodal large language models (MLLMs) in art appreciation education, with a\nfocus on developing LLaVA-Docent, a model designed to serve as a personal tutor\nfor art appreciation. Our approach involved design and development research,\nfocusing on iterative enhancement to design and develop the application to\nproduce a functional MLLM-enabled chatbot along with a data design framework\nfor art appreciation education. To that end, we established a virtual dialogue\ndataset that was generated by GPT-4, which was instrumental in training our\nMLLM, LLaVA-Docent. The performance of LLaVA-Docent was evaluated by\nbenchmarking it against alternative settings and revealed its distinct\nstrengths and weaknesses. Our findings highlight the efficacy of the MMLM-based\npersonalized art appreciation chatbot and demonstrate its applicability for a\nnovel approach in which art appreciation is taught and experienced.", "published": "2024-02-09 09:25:18", "link": "http://arxiv.org/abs/2402.06264v3", "categories": ["cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.AI"}
{"title": "ExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs", "abstract": "ExaRanker recently introduced an approach to training information retrieval\n(IR) models, incorporating natural language explanations as additional labels.\nThe method addresses the challenge of limited labeled examples, leading to\nimprovements in the effectiveness of IR models. However, the initial results\nwere based on proprietary language models such as GPT-3.5, which posed\nconstraints on dataset size due to its cost and data privacy. In this paper, we\nintroduce ExaRanker-Open, where we adapt and explore the use of open-source\nlanguage models to generate explanations. The method has been tested using\ndifferent LLMs and datasets sizes to better comprehend the effective\ncontribution of data augmentation. Our findings reveal that incorporating\nexplanations consistently enhances neural rankers, with benefits escalating as\nthe LLM size increases. Notably, the data augmentation method proves\nadvantageous even with large datasets, as evidenced by ExaRanker surpassing the\ntarget baseline by 0.6 nDCG@10 points in our study. To encourage further\nadvancements by the research community, we have open-sourced both the code and\ndatasets at https://github.com/unicamp-dl/ExaRanker.", "published": "2024-02-09 11:23:14", "link": "http://arxiv.org/abs/2402.06334v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "CoSearchAgent: A Lightweight Collaborative Search Agent with Large\n  Language Models", "abstract": "Collaborative search supports multiple users working together to accomplish a\nspecific search task. Research has found that designing lightweight\ncollaborative search plugins within instant messaging platforms aligns better\nwith users' collaborative habits. However, due to the complexity of multi-user\ninteraction scenarios, it is challenging to implement a fully functioning\nlightweight collaborative search system. Therefore, previous studies on\nlightweight collaborative search had to rely on the Wizard of Oz paradigm. In\nrecent years, large language models (LLMs) have been demonstrated to interact\nnaturally with users and achieve complex information-seeking tasks through\nLLM-based agents. Hence, to better support the research in collaborative\nsearch, in this demo, we propose CoSearchAgent, a lightweight collaborative\nsearch agent powered by LLMs. CoSearchAgent is designed as a Slack plugin that\ncan support collaborative search during multi-party conversations on this\nplatform. Equipped with the capacity to understand the queries and context in\nmulti-user conversations and the ability to search the Web for relevant\ninformation via APIs, CoSearchAgent can respond to user queries with answers\ngrounded on the relevant search results. It can also ask clarifying questions\nwhen the information needs are unclear. The proposed CoSearchAgent is highly\nflexible and would be useful for supporting further research on collaborative\nsearch. The code and demo video are accessible.", "published": "2024-02-09 12:10:00", "link": "http://arxiv.org/abs/2402.06360v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "V-STaR: Training Verifiers for Self-Taught Reasoners", "abstract": "Common self-improvement approaches for large language models (LLMs), such as\nSTaR, iteratively fine-tune LLMs on self-generated solutions to improve their\nproblem-solving ability. However, these approaches discard the large amounts of\nincorrect solutions generated during this process, potentially neglecting\nvaluable information in such solutions. To address this shortcoming, we propose\nV-STaR that utilizes both the correct and incorrect solutions generated during\nthe self-improvement process to train a verifier using DPO that judges\ncorrectness of model-generated solutions. This verifier is used at inference\ntime to select one solution among many candidate solutions. Running V-STaR for\nmultiple iterations results in progressively better reasoners and verifiers,\ndelivering a 4% to 17% test accuracy improvement over existing self-improvement\nand verification approaches on common code generation and math reasoning\nbenchmarks with LLaMA2 models.", "published": "2024-02-09 15:02:56", "link": "http://arxiv.org/abs/2402.06457v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Inducing Systematicity in Transformers by Attending to Structurally\n  Quantized Embeddings", "abstract": "Transformers generalize to novel compositions of structures and entities\nafter being trained on a complex dataset, but easily overfit on datasets of\ninsufficient complexity. We observe that when the training set is sufficiently\ncomplex, the model encodes sentences that have a common syntactic structure\nusing a systematic attention pattern. Inspired by this observation, we propose\nSQ-Transformer (Structurally Quantized) that explicitly encourages\nsystematicity in the embeddings and attention layers, even with a training set\nof low complexity. At the embedding level, we introduce Structure-oriented\nVector Quantization (SoVQ) to cluster word embeddings into several classes of\nstructurally equivalent entities. At the attention level, we devise the\nSystematic Attention Layer (SAL) and an alternative, Systematically Regularized\nLayer (SRL) that operate on the quantized word embeddings so that sentences of\nthe same structure are encoded with invariant or similar attention patterns.\nEmpirically, we show that SQ-Transformer achieves stronger compositional\ngeneralization than the vanilla Transformer on multiple low-complexity semantic\nparsing and machine translation datasets. In our analysis, we show that SoVQ\nindeed learns a syntactically clustered embedding space and SAL/SRL induces\ngeneralizable attention patterns, which lead to improved systematicity.", "published": "2024-02-09 15:53:15", "link": "http://arxiv.org/abs/2402.06492v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scalable Interactive Machine Learning for Future Command and Control", "abstract": "Future warfare will require Command and Control (C2) personnel to make\ndecisions at shrinking timescales in complex and potentially ill-defined\nsituations. Given the need for robust decision-making processes and\ndecision-support tools, integration of artificial and human intelligence holds\nthe potential to revolutionize the C2 operations process to ensure adaptability\nand efficiency in rapidly changing operational environments. We propose to\nleverage recent promising breakthroughs in interactive machine learning, in\nwhich humans can cooperate with machine learning algorithms to guide machine\nlearning algorithm behavior. This paper identifies several gaps in\nstate-of-the-art science and technology that future work should address to\nextend these approaches to function in complex C2 contexts. In particular, we\ndescribe three research focus areas that together, aim to enable scalable\ninteractive machine learning (SIML): 1) developing human-AI interaction\nalgorithms to enable planning in complex, dynamic situations; 2) fostering\nresilient human-AI teams through optimizing roles, configurations, and trust;\nand 3) scaling algorithms and human-AI teams for flexibility across a range of\npotential contexts and situations.", "published": "2024-02-09 16:11:04", "link": "http://arxiv.org/abs/2402.06501v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC", "I.2.6; I.2.7; J.7"], "primary_category": "cs.LG"}
{"title": "Introspective Planning: Aligning Robots' Uncertainty with Inherent Task\n  Ambiguity", "abstract": "Large language models (LLMs) exhibit advanced reasoning skills, enabling\nrobots to comprehend natural language instructions and strategically plan\nhigh-level actions through proper grounding. However, LLM hallucination may\nresult in robots confidently executing plans that are misaligned with user\ngoals or even unsafe in critical scenarios. Additionally, inherent ambiguity in\nnatural language instructions can introduce uncertainty into the LLM's\nreasoning and planning processes.We propose introspective planning, a\nsystematic approach that align LLM's uncertainty with the inherent ambiguity of\nthe task. Our approach constructs a knowledge base containing introspective\nreasoning examples as post-hoc rationalizations of human-selected safe and\ncompliant plans, which are retrieved during deployment. Evaluations on three\ntasks, including a newly introduced safe mobile manipulation benchmark,\ndemonstrate that introspection substantially improves both compliance and\nsafety over state-of-the-art LLM-based planning methods. Furthermore, we\nempirically show that introspective planning, in combination with conformal\nprediction, achieves tighter confidence bounds, maintaining statistical success\nguarantees while minimizing unnecessary user clarification requests. The\nwebpage and code are accessible at https://introplan.github.io.", "published": "2024-02-09 16:40:59", "link": "http://arxiv.org/abs/2402.06529v4", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Calibrating Long-form Generations from Large Language Models", "abstract": "To enhance Large Language Models' (LLMs) reliability, calibration is\nessential -- the model's assessed confidence scores should align with the\nactual likelihood of its responses being correct. However, current confidence\nelicitation methods and calibration metrics typically rely on a binary\ntrue/false assessment of response correctness. This approach does not apply to\nlong-form generation, where an answer can be partially correct. Addressing this\ngap, we introduce a unified calibration framework, in which both the\ncorrectness of the LLMs' responses and their associated confidence levels are\ntreated as distributions across a range of scores. Within this framework, we\ndevelop three metrics to precisely evaluate LLM calibration and further propose\ntwo confidence elicitation methods based on self-consistency and\nself-evaluation. Our experiments, which include long-form QA and summarization\ntasks, demonstrate that larger models don't necessarily guarantee better\ncalibration, that calibration performance is found to be metric-dependent, and\nthat self-consistency methods excel in factoid datasets. We also find that\ncalibration can be enhanced through techniques such as fine-tuning, integrating\nrelevant source documents, scaling the temperature, and combining\nself-consistency with self-evaluation. Lastly, we showcase a practical\napplication of our system: selecting and cascading open-source models and\nChatGPT to optimize correctness given a limited API budget. This research not\nonly challenges existing notions of LLM calibration but also offers practical\nmethodologies for improving trustworthiness in long-form generation.", "published": "2024-02-09 17:00:32", "link": "http://arxiv.org/abs/2402.06544v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous\n  Driving and Zero-Shot Instruction Following", "abstract": "Diffusion models excel at modeling complex and multimodal trajectory\ndistributions for decision-making and control. Reward-gradient guided denoising\nhas been recently proposed to generate trajectories that maximize both a\ndifferentiable reward function and the likelihood under the data distribution\ncaptured by a diffusion model. Reward-gradient guided denoising requires a\ndifferentiable reward function fitted to both clean and noised samples,\nlimiting its applicability as a general trajectory optimizer. In this paper, we\npropose DiffusionES, a method that combines gradient-free optimization with\ntrajectory denoising to optimize black-box non-differentiable objectives while\nstaying in the data manifold. Diffusion-ES samples trajectories during\nevolutionary search from a diffusion model and scores them using a black-box\nreward function. It mutates high-scoring trajectories using a truncated\ndiffusion process that applies a small number of noising and denoising steps,\nallowing for much more efficient exploration of the solution space. We show\nthat DiffusionES achieves state-of-the-art performance on nuPlan, an\nestablished closed-loop planning benchmark for autonomous driving. Diffusion-ES\noutperforms existing sampling-based planners, reactive deterministic or\ndiffusion-based policies, and reward-gradient guidance. Additionally, we show\nthat unlike prior guidance methods, our method can optimize non-differentiable\nlanguage-shaped reward functions generated by few-shot LLM prompting. When\nguided by a human teacher that issues instructions to follow, our method can\ngenerate novel, highly complex behaviors, such as aggressive lane weaving,\nwhich are not present in the training data. This allows us to solve the hardest\nnuPlan scenarios which are beyond the capabilities of existing trajectory\noptimization methods and driving policies.", "published": "2024-02-09 17:18:33", "link": "http://arxiv.org/abs/2402.06559v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.LG"}
{"title": "Self-consistent context aware conformer transducer for speech\n  recognition", "abstract": "We introduce a novel neural network module that adeptly handles recursive\ndata flow in neural network architectures. At its core, this module employs a\nself-consistent approach where a set of recursive equations is solved\niteratively, halting when the difference between two consecutive iterations\nfalls below a defined threshold. Leveraging this mechanism, we construct a new\nneural network architecture, an extension of the conformer transducer, which\nenriches automatic speech recognition systems with a stream of contextual\ninformation. Our method notably improves the accuracy of recognizing rare words\nwithout adversely affecting the word error rate for common vocabulary. We\ninvestigate the improvement in accuracy for these uncommon words using our\nnovel model, both independently and in conjunction with shallow fusion with a\ncontext language model. Our findings reveal that the combination of both\napproaches can improve the accuracy of detecting rare words by as much as 4.5\ntimes. Our proposed self-consistent recursive methodology is versatile and\nadaptable, compatible with many recently developed encoders, and has the\npotential to drive model improvements in speech recognition and beyond.", "published": "2024-02-09 18:12:11", "link": "http://arxiv.org/abs/2402.06592v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Feedback Loops With Language Models Drive In-Context Reward Hacking", "abstract": "Language models influence the external world: they query APIs that read and\nwrite to web pages, generate content that shapes human behavior, and run system\ncommands as autonomous agents. These interactions form feedback loops: LLM\noutputs affect the world, which in turn affect subsequent LLM outputs. In this\nwork, we show that feedback loops can cause in-context reward hacking (ICRH),\nwhere the LLM at test-time optimizes a (potentially implicit) objective but\ncreates negative side effects in the process. For example, consider an LLM\nagent deployed to increase Twitter engagement; the LLM may retrieve its\nprevious tweets into the context window and make them more controversial,\nincreasing engagement but also toxicity. We identify and study two processes\nthat lead to ICRH: output-refinement and policy-refinement. For these\nprocesses, evaluations on static datasets are insufficient -- they miss the\nfeedback effects and thus cannot capture the most harmful behavior. In\nresponse, we provide three recommendations for evaluation to capture more\ninstances of ICRH. As AI development accelerates, the effects of feedback loops\nwill proliferate, increasing the need to understand their role in shaping LLM\nbehavior.", "published": "2024-02-09 18:59:29", "link": "http://arxiv.org/abs/2402.06627v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "NICE: To Optimize In-Context Examples or Not?", "abstract": "Recent work shows that in-context learning and optimization of in-context\nexamples (ICE) can significantly improve the accuracy of large language models\n(LLMs) on a wide range of tasks, leading to an apparent consensus that ICE\noptimization is crucial for better performance. However, most of these studies\nassume a fixed or no instruction provided in the prompt. We challenge this\nconsensus by investigating the necessity of optimizing ICE when task-specific\ninstructions are provided and find that there are many tasks for which it\nyields diminishing returns. In particular, using a diverse set of tasks and a\nsystematically created instruction set with gradually added details, we find\nthat as the prompt instruction becomes more detailed, the returns on ICE\noptimization diminish. To characterize this behavior, we introduce a\ntask-specific metric called Normalized Invariability to Choice of Examples\n(NICE) that quantifies the learnability of tasks from a given instruction, and\nprovides a heuristic to help decide whether to optimize instructions or ICE for\na new task. Given a task, the proposed metric can reliably predict the utility\nof optimizing ICE compared to using random ICE. Our code is available at\nhttps://github.com/microsoft/nice-icl.", "published": "2024-02-09 19:09:19", "link": "http://arxiv.org/abs/2402.06733v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Re-Envisioning Command and Control", "abstract": "Future warfare will require Command and Control (C2) decision-making to occur\nin more complex, fast-paced, ill-structured, and demanding conditions. C2 will\nbe further complicated by operational challenges such as Denied, Degraded,\nIntermittent, and Limited (DDIL) communications and the need to account for\nmany data streams, potentially across multiple domains of operation. Yet,\ncurrent C2 practices -- which stem from the industrial era rather than the\nemerging intelligence era -- are linear and time-consuming. Critically, these\napproaches may fail to maintain overmatch against adversaries on the future\nbattlefield. To address these challenges, we propose a vision for future C2\nbased on robust partnerships between humans and artificial intelligence (AI)\nsystems. This future vision is encapsulated in three operational impacts:\nstreamlining the C2 operations process, maintaining unity of effort, and\ndeveloping adaptive collective knowledge systems. This paper illustrates the\nenvisaged future C2 capabilities, discusses the assumptions that shaped them,\nand describes how the proposed developments could transform C2 in future\nwarfare.", "published": "2024-02-09 16:10:29", "link": "http://arxiv.org/abs/2402.07946v2", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG", "I.2.6; I.2.7; J.7"], "primary_category": "cs.HC"}
{"title": "A Multi-faceted Semi-Synthetic Dataset for Automated Cyberbullying\n  Detection", "abstract": "In recent years, the rising use of social media has propelled automated\ncyberbullying detection into a prominent research domain. However, challenges\npersist due to the absence of a standardized definition and universally\naccepted datasets. Many researchers now view cyberbullying as a facet of\ncyberaggression, encompassing factors like repetition, peer relationships, and\nharmful intent in addition to online aggression. Acquiring comprehensive data\nreflective of all cyberbullying components from social media networks proves to\nbe a complex task. This paper provides a description of an extensive\nsemi-synthetic cyberbullying dataset that incorporates all of the essential\naspects of cyberbullying, including aggression, repetition, peer relationships,\nand intent to harm. The method of creating the dataset is succinctly outlined,\nand a detailed overview of the publicly accessible dataset is additionally\npresented. This accompanying data article provides an in-depth look at the\ndataset, increasing transparency and enabling replication. It also aids in a\ndeeper understanding of the data, supporting broader research use.", "published": "2024-02-09 16:53:19", "link": "http://arxiv.org/abs/2402.10231v1", "categories": ["cs.SI", "cs.CL", "cs.LG", "I.2"], "primary_category": "cs.SI"}
{"title": "CultureLLM: Incorporating Cultural Differences into Large Language\n  Models", "abstract": "Large language models (LLMs) are reported to be partial to certain cultures\nowing to the training data dominance from the English corpora. Since\nmultilingual cultural data are often expensive to collect, existing efforts\nhandle this by prompt engineering or culture-specific pre-training. However,\nthey might overlook the knowledge deficiency of low-resource culture and\nrequire extensive computing resources. In this paper, we propose CultureLLM, a\ncost-effective solution to incorporate cultural differences into LLMs.\nCultureLLM adopts World Value Survey (WVS) as seed data and generates\nsemantically equivalent training data via the proposed semantic data\naugmentation. Using only 50 seed samples from WVS with augmented data, we\nfine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9\ncultures covering rich and low-resource languages. Extensive experiments on 60\nculture-related datasets demonstrate that CultureLLM significantly outperforms\nvarious counterparts such as GPT-3.5 (by 8.1%) and Gemini Pro (by 9.5%) with\ncomparable performance to GPT-4 or even better. Our human study shows that the\ngenerated samples are semantically equivalent to the original samples,\nproviding an effective solution for LLMs augmentation. Code is released at\nhttps://github.com/Scarelette/CultureLLM.", "published": "2024-02-09 04:02:43", "link": "http://arxiv.org/abs/2402.10946v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Unreasonable Effectiveness of Eccentric Automatic Prompts", "abstract": "Large Language Models (LLMs) have demonstrated remarkable problem-solving and\nbasic mathematics abilities. However, their efficacy is highly contingent on\nthe formulation of the prompt. This study endeavors to quantify the influence\nof incorporating \"positive thinking\" into the system message of the prompt,\nthen compare that to systematic prompt optimization. We assess the performance\nof 60 combinations of system message snippets, tested with and without Chain of\nThought prompting, across three models with parameters ranging from 7 to 70\nbillion on the GSM8K dataset. Our findings reveal that results do not\nuniversally generalize across models. In most instances, the inclusion of\n\"positive thinking\" prompts positively affected model performance. Notably,\nhowever, Llama2-70B exhibited an exception when not utilizing Chain of Thought,\nas the optimal system message was found to be none at all. Given the\ncombinatorial complexity, and thus computation time, of experimenting with\nhand-tuning prompts for large black-box models, we then compared the\nperformance of the best \"positive thinking\" prompt against the output of\nsystematic prompt optimization. We show that employing an automated prompt\noptimizer emerges as the most effective method for enhancing performance, even\nwhen working with smaller open-source models. Additionally, our findings reveal\nthat the highest-scoring, automatically-optimized prompt exhibits a degree of\npeculiarity far beyond expectations.", "published": "2024-02-09 22:48:45", "link": "http://arxiv.org/abs/2402.10949v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Verif.ai: Towards an Open-Source Scientific Generative\n  Question-Answering System with Referenced and Verifiable Answers", "abstract": "In this paper, we present the current progress of the project Verif.ai, an\nopen-source scientific generative question-answering system with referenced and\nverified answers. The components of the system are (1) an information retrieval\nsystem combining semantic and lexical search techniques over scientific papers\n(PubMed), (2) a fine-tuned generative model (Mistral 7B) taking top answers and\ngenerating answers with references to the papers from which the claim was\nderived, and (3) a verification engine that cross-checks the generated claim\nand the abstract or paper from which the claim was derived, verifying whether\nthere may have been any hallucinations in generating the claim. We are\nreinforcing the generative model by providing the abstract in context, but in\naddition, an independent set of methods and models are verifying the answer and\nchecking for hallucinations. Therefore, we believe that by using our method, we\ncan make scientists more productive, while building trust in the use of\ngenerative language models in scientific environments, where hallucinations and\nmisinformation cannot be tolerated.", "published": "2024-02-09 10:25:01", "link": "http://arxiv.org/abs/2402.18589v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Detection of Opioid Users from Reddit Posts via an Attention-based\n  Bidirectional Recurrent Neural Network", "abstract": "The opioid epidemic, referring to the growing hospitalizations and deaths\nbecause of overdose of opioid usage and addiction, has become a severe health\nproblem in the United States. Many strategies have been developed by the\nfederal and local governments and health communities to combat this crisis.\nAmong them, improving our understanding of the epidemic through better health\nsurveillance is one of the top priorities. In addition to direct testing,\nmachine learning approaches may also allow us to detect opioid users by\nanalyzing data from social media because many opioid users may choose not to do\nthe tests but may share their experiences on social media anonymously. In this\npaper, we take advantage of recent advances in machine learning, collect and\nanalyze user posts from a popular social network Reddit with the goal to\nidentify opioid users. Posts from more than 1,000 users who have posted on\nthree sub-reddits over a period of one month have been collected. In addition\nto the ones that contain keywords such as opioid, opiate, or heroin, we have\nalso collected posts that contain slang words of opioid such as black or\nchocolate. We apply an attention-based bidirectional long short memory model to\nidentify opioid users. Experimental results show that the approaches\nsignificantly outperform competitive algorithms in terms of F1-score.\nFurthermore, the model allows us to extract most informative words, such as\nopiate, opioid, and black, from posts via the attention layer, which provides\nmore insights on how the machine learning algorithm works in distinguishing\ndrug users from non-drug users.", "published": "2024-02-09 22:12:20", "link": "http://arxiv.org/abs/2403.15393v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "What is Hiding in Medicine's Dark Matter? Learning with Missing Data in\n  Medical Practices", "abstract": "Electronic patient records (EPRs) produce a wealth of data but contain\nsignificant missing information. Understanding and handling this missing data\nis an important part of clinical data analysis and if left unaddressed could\nresult in bias in analysis and distortion in critical conclusions. Missing data\nmay be linked to health care professional practice patterns and imputation of\nmissing data can increase the validity of clinical decisions. This study\nfocuses on statistical approaches for understanding and interpreting the\nmissing data and machine learning based clinical data imputation using a single\ncentre's paediatric emergency data and the data from UK's largest clinical\naudit for traumatic injury database (TARN). In the study of 56,961 data points\nrelated to initial vital signs and observations taken on children presenting to\nan Emergency Department, we have shown that missing data are likely to be\nnon-random and how these are linked to health care professional practice\npatterns. We have then examined 79 TARN fields with missing values for 5,791\ntrauma cases. Singular Value Decomposition (SVD) and k-Nearest Neighbour (kNN)\nbased missing data imputation methods are used and imputation results against\nthe original dataset are compared and statistically tested. We have concluded\nthat the 1NN imputer is the best imputation which indicates a usual pattern of\nclinical decision making: find the most similar patients and take their\nattributes as imputation.", "published": "2024-02-09 17:27:35", "link": "http://arxiv.org/abs/2402.06563v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "Quantitative Analysis of AI-Generated Texts in Academic Research: A\n  Study of AI Presence in Arxiv Submissions using AI Detection Tool", "abstract": "Many people are interested in ChatGPT since it has become a prominent AIGC\nmodel that provides high-quality responses in various contexts, such as\nsoftware development and maintenance. Misuse of ChatGPT might cause significant\nissues, particularly in public safety and education, despite its immense\npotential. The majority of researchers choose to publish their work on Arxiv.\nThe effectiveness and originality of future work depend on the ability to\ndetect AI components in such contributions. To address this need, this study\nwill analyze a method that can see purposely manufactured content that academic\norganizations use to post on Arxiv. For this study, a dataset was created using\nphysics, mathematics, and computer science articles. Using the newly built\ndataset, the following step is to put originality.ai through its paces. The\nstatistical analysis shows that Originality.ai is very accurate, with a rate of\n98%.", "published": "2024-02-09 17:20:48", "link": "http://arxiv.org/abs/2403.13812v1", "categories": ["cs.DL", "cs.AI", "cs.CL", "cs.CY", "cs.LG", "stat.OT", "62P25", "I.7; G.1; G.3"], "primary_category": "cs.DL"}
{"title": "Data-driven Joint Detection and Localization of Acoustic Reflectors", "abstract": "Room geometry inference algorithms rely on the localization of acoustic\nreflectors to identify boundary surfaces of an enclosure. Rooms with highly\nabsorptive walls or walls at large distances from the measurement setup pose\nchallenges for such algorithms. As it is not always possible to localize all\nwalls, we present a data-driven method to jointly detect and localize acoustic\nreflectors that correspond to nearby and/or reflective walls. A multi-branch\nconvolutional recurrent neural network is employed for this purpose. The\nnetwork's input consists of a time-domain acoustic beamforming map, obtained\nvia Radon transform from multi-channel room impulse responses. A modified loss\nfunction is proposed that forces the network to pay more attention to walls\nthat can be estimated with a small error. Simulation results show that the\nproposed method can detect nearby and/or reflective walls and improve the\nlocalization performance for the detected walls.", "published": "2024-02-09 08:40:33", "link": "http://arxiv.org/abs/2402.06246v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Transversal Study of Fundamental Frequency Contours in Parkinsonian\n  Voices", "abstract": "A transversal study of the pitch variability of parkinsonian voices in read\nspeech is presented. 30 patients suffering from Parkinson's disease (PD) and 32\nhealthy speakers were recorded while reading a text without voiceless phonemes.\nThe fundamental frequency contours were calculated from the recordings, and the\nfollowing measures were used for describing them: mean, minimum, maximum, and\nstandard deviation of the estimated fundamental frequencies. Results based on\nthese measures indicate that the influence of PD on some aspects of intonation\ncan be masked by the effects of aging, especially for male voices. However,\nsome parameters such as the relative fundamental frequency range exhibit lower\ncorrelations with age than with PD stage, as evaluated using the Hoehn and Yahr\nscale. These correlations between relative fundamental frequency range and PD\nstage reach moderate-to-high values in the case of women. Additionally, three\nparameters describing the form of the fundamental frequency modulation spectrum\nwere investigated for correlation with age and PD stage. The study of this\nmodulation spectrum provides some insight into the ability of the speakers to\nplan the intonation of full phrases. For both male and female populations,\nsignificant correlations were found between parameters obtained from the\nmodulation spectrum of fundamental frequency and the PD stage. Nevertheless,\nthe quantitative assessment of the performance of regression models built from\nthese modulation parameters and fundamental frequency range suggests that such\nmeasures are likely to be of limited value in the early diagnosis of PD due to\ninter-speaker variability.", "published": "2024-02-09 13:08:58", "link": "http://arxiv.org/abs/2402.06387v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MusicMagus: Zero-Shot Text-to-Music Editing via Diffusion Models", "abstract": "Recent advances in text-to-music generation models have opened new avenues in\nmusical creativity. However, music generation usually involves iterative\nrefinements, and how to edit the generated music remains a significant\nchallenge. This paper introduces a novel approach to the editing of music\ngenerated by such models, enabling the modification of specific attributes,\nsuch as genre, mood and instrument, while maintaining other aspects unchanged.\nOur method transforms text editing to \\textit{latent space manipulation} while\nadding an extra constraint to enforce consistency. It seamlessly integrates\nwith existing pretrained text-to-music diffusion models without requiring\nadditional training. Experimental results demonstrate superior performance over\nboth zero-shot and certain supervised baselines in style and timbre transfer\nevaluations. Additionally, we showcase the practical applicability of our\napproach in real-world music editing scenarios.", "published": "2024-02-09 04:34:08", "link": "http://arxiv.org/abs/2402.06178v3", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A New Approach to Voice Authenticity", "abstract": "Voice faking, driven primarily by recent advances in text-to-speech (TTS)\nsynthesis technology, poses significant societal challenges. Currently, the\nprevailing assumption is that unaltered human speech can be considered genuine,\nwhile fake speech comes from TTS synthesis. We argue that this binary\ndistinction is oversimplified. For instance, altered playback speeds can be\nused for malicious purposes, like in the 'Drunken Nancy Pelosi' incident.\nSimilarly, editing of audio clips can be done ethically, e.g., for brevity or\nsummarization in news reporting or podcasts, but editing can also create\nmisleading narratives. In this paper, we propose a conceptual shift away from\nthe binary paradigm of audio being either 'fake' or 'real'. Instead, our focus\nis on pinpointing 'voice edits', which encompass traditional modifications like\nfilters and cuts, as well as TTS synthesis and VC systems. We delineate 6\ncategories and curate a new challenge dataset rooted in the M-AILABS corpus,\nfor which we present baseline detection systems. And most importantly, we argue\nthat merely categorizing audio as fake or real is a dangerous\nover-simplification that will fail to move the field of speech technology\nforward.", "published": "2024-02-09 10:34:01", "link": "http://arxiv.org/abs/2402.06304v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploiting spatial diversity for increasing the robustness of sound\n  source localization systems against reverberation", "abstract": "Acoustic reverberation is one of the most relevant factors that hampers the\nlocalization of a sound source inside a room. To date, several approaches have\nbeen proposed to deal with it, but have not always been evaluated under\nrealistic conditions. This paper proposes exploiting spatial diversity as an\nalternative approach to achieve robustness against reverberation. The\ntheoretical arguments supporting this approach are first presented and later\nconfirmed by means of simulation results and real measurements. Simulations are\nrun for reverberation times up to 2 s, thus providing results with a wider\nrange of validity than in other previous research works. It is concluded that\nthe use of systems consisting of several, sufficiently separated, small arrays\nleads to the best results in reverberant environments. Some recommendations are\ngiven regarding the choice of the array sizes, the separation among them, and\nthe way to combine SRP-PHAT maps obtained from diverse arrays.", "published": "2024-02-09 13:57:02", "link": "http://arxiv.org/abs/2402.06411v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Analytical model for the relation between signal bandwidth and spatial\n  resolution in Steered-Response Power Phase Transform (SRP-PHAT) maps", "abstract": "An analysis of the relationship between the bandwidth of acoustic signals and\nthe required resolution of steered-response power phase transform (SRP-PHAT)\nmaps used for sound source localization is presented. This relationship does\nnot rely on the far-field assumption, nor does it depend on any specific array\ntopology. The proposed analysis considers the computation of a SRP map as a\nprocess of sampling a set of generalized cross-correlation (GCC) functions,\neach one corresponding to a different microphone pair. From this approach, we\nderive a rule that relates GCC bandwidth with inter-microphone distance,\nresolution of the SRP map, and the potential position of the sound source\nrelative to the array position. This rule is a sufficient condition for an\naliasing-free calculation of the specified SRP-PHAT map. Simulation results\nshow that limiting the bandwidth of the GCC according to such rule leads to\nsignificant reductions in sound source localization errors when sources are not\nin the immediate vicinity of the microphone array. These error reductions are\nmore relevant for coarser resolutions of the SRP map, and they happen in both\nanechoic and reverberant environments.", "published": "2024-02-09 18:05:49", "link": "http://arxiv.org/abs/2402.06586v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Capturing Cancer as Music: Cancer Mechanisms Expressed through\n  Musification", "abstract": "The development of cancer is difficult to express on a simple and intuitive\nlevel due to its complexity. Since cancer is so widespread, raising public\nawareness about its mechanisms can help those affected cope with its realities,\nas well as inspire others to make lifestyle adjustments and screen for the\ndisease. Unfortunately, studies have shown that cancer literature is too\ntechnical for the general public to understand. We found that musification, the\nprocess of turning data into music, remains an unexplored avenue for conveying\nthis information. We explore the pedagogical effectiveness of musification\nthrough the use of an algorithm that manipulates a piece of music in a manner\nanalogous to the development of cancer. We conducted two lab studies and found\nthat our approach is marginally more effective at promoting cancer literacy\nwhen accompanied by a text-based article than text-based articles alone.", "published": "2024-02-09 20:48:39", "link": "http://arxiv.org/abs/2402.06777v1", "categories": ["cs.HC", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "GLA-Grad: A Griffin-Lim Extended Waveform Generation Diffusion Model", "abstract": "Diffusion models are receiving a growing interest for a variety of signal\ngeneration tasks such as speech or music synthesis. WaveGrad, for example, is a\nsuccessful diffusion model that conditionally uses the mel spectrogram to guide\na diffusion process for the generation of high-fidelity audio. However, such\nmodels face important challenges concerning the noise diffusion process for\ntraining and inference, and they have difficulty generating high-quality speech\nfor speakers that were not seen during training. With the aim of minimizing the\nconditioning error and increasing the efficiency of the noise diffusion\nprocess, we propose in this paper a new scheme called GLA-Grad, which consists\nin introducing a phase recovery algorithm such as the Griffin-Lim algorithm\n(GLA) at each step of the regular diffusion process. Furthermore, it can be\ndirectly applied to an already-trained waveform generation model, without\nadditional training or fine-tuning. We show that our algorithm outperforms\nstate-of-the-art diffusion models for speech generation, especially when\ngenerating speech for a previously unseen target speaker.", "published": "2024-02-09 12:12:52", "link": "http://arxiv.org/abs/2402.15516v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Evaluating Co-Creativity using Total Information Flow", "abstract": "Co-creativity in music refers to two or more musicians or musical agents\ninteracting with one another by composing or improvising music. However, this\nis a very subjective process and each musician has their own preference as to\nwhich improvisation is better for some context. In this paper, we aim to create\na measure based on total information flow to quantitatively evaluate the\nco-creativity process in music. In other words, our measure is an indication of\nhow \"good\" a creative musical process is. Our main hypothesis is that a good\nmusical creation would maximize information flow between the participants\ncaptured by music voices recorded in separate tracks. We propose a method to\ncompute the information flow using pre-trained generative models as entropy\nestimators. We demonstrate how our method matches with human perception using a\nqualitative study.", "published": "2024-02-09 22:15:39", "link": "http://arxiv.org/abs/2402.06810v1", "categories": ["cs.SD", "cs.AI", "cs.HC", "cs.IT", "cs.LG", "eess.AS", "math.IT"], "primary_category": "cs.SD"}
