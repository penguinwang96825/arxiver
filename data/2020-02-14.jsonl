{"title": "HULK: An Energy Efficiency Benchmark Platform for Responsible Natural\n  Language Processing", "abstract": "Computation-intensive pretrained models have been taking the lead of many\nnatural language processing benchmarks such as GLUE. However, energy efficiency\nin the process of model training and inference becomes a critical bottleneck.\nWe introduce HULK, a multi-task energy efficiency benchmarking platform for\nresponsible natural language processing. With HULK, we compare pretrained\nmodels' energy efficiency from the perspectives of time and cost. Baseline\nbenchmarking results are provided for further analysis. The fine-tuning\nefficiency of different pretrained models can differ a lot among different\ntasks and fewer parameter number does not necessarily imply better efficiency.\nWe analyzed such phenomenon and demonstrate the method of comparing the\nmulti-task efficiency of pretrained models. Our platform is available at\nhttps://sites.engineering.ucsb.edu/~xiyou/hulk/.", "published": "2020-02-14 01:04:19", "link": "http://arxiv.org/abs/2002.05829v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding patient complaint characteristics using contextual\n  clinical BERT embeddings", "abstract": "In clinical conversational applications, extracted entities tend to capture\nthe main subject of a patient's complaint, namely symptoms or diseases.\nHowever, they mostly fail to recognize the characterizations of a complaint\nsuch as the time, the onset, and the severity. For example, if the input is \"I\nhave a headache and it is extreme\", state-of-the-art models only recognize the\nmain symptom entity - headache, but ignore the severity factor of \"extreme\",\nthat characterizes headache. In this paper, we design a two-stage approach to\ndetect the characterizations of entities like symptoms presented by general\nusers in contexts where they would describe their symptoms to a clinician. We\nuse Word2Vec and BERT to encode clinical text given by the patients. We\ntransform the output and re-frame the task as multi-label classification\nproblem. Finally, we combine the processed encodings with the Linear\nDiscriminant Analysis (LDA) algorithm to classify the characterizations of the\nmain entity. Experimental results demonstrate that our method achieves 40-50%\nimprovement on the accuracy over the state-of-the-art models.", "published": "2020-02-14 07:45:33", "link": "http://arxiv.org/abs/2002.05902v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Transformers as Soft Reasoners over Language", "abstract": "Beginning with McCarthy's Advice Taker (1959), AI has pursued the goal of\nproviding a system with explicit, general knowledge and having the system\nreason over that knowledge. However, expressing the knowledge in a formal\n(logical or probabilistic) representation has been a major obstacle to this\nresearch. This paper investigates a modern approach to this problem where the\nfacts and rules are provided as natural language sentences, thus bypassing a\nformal representation. We train transformers to reason (or emulate reasoning)\nover these sentences using synthetically generated data. Our models, that we\ncall RuleTakers, provide the first empirical demonstration that this kind of\nsoft reasoning over language is learnable, can achieve high (99%) accuracy, and\ngeneralizes to test data requiring substantially deeper chaining than seen\nduring training (95%+ scores). We also demonstrate that the models transfer\nwell to two hand-authored rulebases, and to rulebases paraphrased into more\nnatural language. These findings are significant as it suggests a new role for\ntransformers, namely as limited \"soft theorem provers\" operating over explicit\ntheories in language. This in turn suggests new possibilities for\nexplainability, correctability, and counterfactual reasoning in\nquestion-answering.", "published": "2020-02-14 04:23:28", "link": "http://arxiv.org/abs/2002.05867v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Zero-Resource Cross-Domain Named Entity Recognition", "abstract": "Existing models for cross-domain named entity recognition (NER) rely on\nnumerous unlabeled corpus or labeled NER training data in target domains.\nHowever, collecting data for low-resource target domains is not only expensive\nbut also time-consuming. Hence, we propose a cross-domain NER model that does\nnot use any external resources. We first introduce a Multi-Task Learning (MTL)\nby adding a new objective function to detect whether tokens are named entities\nor not. We then introduce a framework called Mixture of Entity Experts (MoEE)\nto improve the robustness for zero-resource domain adaptation. Finally,\nexperimental results show that our model outperforms strong unsupervised\ncross-domain sequence labeling models, and the performance of our model is\nclose to that of the state-of-the-art model which leverages extensive\nresources.", "published": "2020-02-14 09:04:18", "link": "http://arxiv.org/abs/2002.05923v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transformer on a Diet", "abstract": "Transformer has been widely used thanks to its ability to capture sequence\ninformation in an efficient way. However, recent developments, such as BERT and\nGPT-2, deliver only heavy architectures with a focus on effectiveness. In this\npaper, we explore three carefully-designed light Transformer architectures to\nfigure out whether the Transformer with less computations could produce\ncompetitive results. Experimental results on language model benchmark datasets\nhint that such trade-off is promising, and the light Transformer reduces 70%\nparameters at best, while obtains competitive perplexity compared to standard\nTransformer. The source code is publicly available.", "published": "2020-02-14 18:41:58", "link": "http://arxiv.org/abs/2002.06170v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Trends of digitalization and adoption of big data & analytics among UK\n  SMEs: Analysis and lessons drawn from a case study of 53 SMEs", "abstract": "Small and Medium Enterprises (SMEs) now generate digital data at an\nunprecedented rate from online transactions, social media marketing and\nassociated customer interactions, online product or service reviews and\nfeedback, clinical diagnosis, Internet of Things (IoT) sensors, and production\nprocesses. All these forms of data can be transformed into monetary value if\nput into a proper data value chain. This requires both skills and IT\ninvestments for the long-term benefit of businesses. However, such spending is\nbeyond the capacity of most SMEs due to their limited resources and restricted\naccess to finances. This paper presents lessons learned from a case study of 53\nUK SMEs, mostly from the West Midlands region of England, supported as part of\na 3-year ERDF project, Big Data Corridor, in the areas of big data management,\nanalytics and related IT issues. Based on our study's sample companies, several\nperspectives including the digital technology trends, challenges facing the UK\nSMEs, and the state of their adoption in data analytics and big data, are\npresented in the paper.", "published": "2020-02-14 09:51:24", "link": "http://arxiv.org/abs/2002.11623v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "A Dataset Independent Set of Baselines for Relation Prediction in\n  Argument Mining", "abstract": "Argument Mining is the research area which aims at extracting argument\ncomponents and predicting argumentative relations (i.e.,support and attack)\nfrom text. In particular, numerous approaches have been proposed in the\nliterature to predict the relations holding between the arguments, and\napplication-specific annotated resources were built for this purpose. Despite\nthe fact that these resources have been created to experiment on the same task,\nthe definition of a single relation prediction method to be successfully\napplied to a significant portion of these datasets is an open research problem\nin Argument Mining. This means that none of the methods proposed in the\nliterature can be easily ported from one resource to another. In this paper, we\naddress this problem by proposing a set of dataset independent strong neural\nbaselines which obtain homogeneous results on all the datasets proposed in the\nliterature for the argumentative relation prediction task. Thus, our baselines\ncan be employed by the Argument Mining community to compare more effectively\nhow well a method performs on the argumentative relation prediction task.", "published": "2020-02-14 12:38:18", "link": "http://arxiv.org/abs/2003.04970v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Data Efficient End-To-End Spoken Language Understanding Architecture", "abstract": "End-to-end architectures have been recently proposed for spoken language\nunderstanding (SLU) and semantic parsing. Based on a large amount of data,\nthose models learn jointly acoustic and linguistic-sequential features. Such\narchitectures give very good results in the context of domain, intent and slot\ndetection, their application in a more complex semantic chunking and tagging\ntask is less easy. For that, in many cases, models are combined with an\nexternal language model to enhance their performance.\n  In this paper we introduce a data efficient system which is trained\nend-to-end, with no additional, pre-trained external module. One key feature of\nour approach is an incremental training procedure where acoustic, language and\nsemantic models are trained sequentially one after the other. The proposed\nmodel has a reasonable size and achieves competitive results with respect to\nstate-of-the-art while using a small training dataset. In particular, we reach\n24.02% Concept Error Rate (CER) on MEDIA/test while training on MEDIA/train\nwithout any additional data.", "published": "2020-02-14 10:24:42", "link": "http://arxiv.org/abs/2002.05955v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Integrating Discrete and Neural Features via Mixed-feature\n  Trans-dimensional Random Field Language Models", "abstract": "There has been a long recognition that discrete features (n-gram features)\nand neural network based features have complementary strengths for language\nmodels (LMs). Improved performance can be obtained by model interpolation,\nwhich is, however, a suboptimal two-step integration of discrete and neural\nfeatures. The trans-dimensional random field (TRF) framework has the potential\nadvantage of being able to flexibly integrate a richer set of features.\nHowever, either discrete or neural features are used alone in previous TRF LMs.\nThis paper develops a mixed-feature TRF LM and demonstrates its advantage in\nintegrating discrete and neural features. Various LMs are trained over PTB and\nGoogle one-billion-word datasets, and evaluated in N-best list rescoring\nexperiments for speech recognition. Among all single LMs (i.e. without model\ninterpolation), the mixed-feature TRF LMs perform the best, improving over both\ndiscrete TRF LMs and neural TRF LMs alone, and also being significantly better\nthan LSTM LMs. Compared to interpolating two separately trained models with\ndiscrete and neural features respectively, the performance of mixed-feature TRF\nLMs matches the best interpolated model, and with simplified one-step training\nprocess and reduced training time.", "published": "2020-02-14 11:05:11", "link": "http://arxiv.org/abs/2002.05967v2", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Query2box: Reasoning over Knowledge Graphs in Vector Space using Box\n  Embeddings", "abstract": "Answering complex logical queries on large-scale incomplete knowledge graphs\n(KGs) is a fundamental yet challenging task. Recently, a promising approach to\nthis problem has been to embed KG entities as well as the query into a vector\nspace such that entities that answer the query are embedded close to the query.\nHowever, prior work models queries as single points in the vector space, which\nis problematic because a complex query represents a potentially large set of\nits answer entities, but it is unclear how such a set can be represented as a\nsingle point. Furthermore, prior work can only handle queries that use\nconjunctions ($\\wedge$) and existential quantifiers ($\\exists$). Handling\nqueries with logical disjunctions ($\\vee$) remains an open problem. Here we\npropose query2box, an embedding-based framework for reasoning over arbitrary\nqueries with $\\wedge$, $\\vee$, and $\\exists$ operators in massive and\nincomplete KGs. Our main insight is that queries can be embedded as boxes\n(i.e., hyper-rectangles), where a set of points inside the box corresponds to a\nset of answer entities of the query. We show that conjunctions can be naturally\nrepresented as intersections of boxes and also prove a negative result that\nhandling disjunctions would require embedding with dimension proportional to\nthe number of KG entities. However, we show that by transforming queries into a\nDisjunctive Normal Form, query2box is capable of handling arbitrary logical\nqueries with $\\wedge$, $\\vee$, $\\exists$ in a scalable manner. We demonstrate\nthe effectiveness of query2box on three large KGs and show that query2box\nachieves up to 25% relative improvement over the state of the art.", "published": "2020-02-14 11:20:10", "link": "http://arxiv.org/abs/2002.05969v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Dialogue history integration into end-to-end signal-to-concept spoken\n  language understanding systems", "abstract": "This work investigates the embeddings for representing dialog history in\nspoken language understanding (SLU) systems. We focus on the scenario when the\nsemantic information is extracted directly from the speech signal by means of a\nsingle end-to-end neural network model. We proposed to integrate dialogue\nhistory into an end-to-end signal-to-concept SLU system. The dialog history is\nrepresented in the form of dialog history embedding vectors (so-called\nh-vectors) and is provided as an additional information to end-to-end SLU\nmodels in order to improve the system performance. Three following types of\nh-vectors are proposed and experimentally evaluated in this paper: (1)\nsupervised-all embeddings predicting bag-of-concepts expected in the answer of\nthe user from the last dialog system response; (2) supervised-freq embeddings\nfocusing on predicting only a selected set of semantic concept (corresponding\nto the most frequent errors in our experiments); and (3) unsupervised\nembeddings. Experiments on the MEDIA corpus for the semantic slot filling task\ndemonstrate that the proposed h-vectors improve the model performance.", "published": "2020-02-14 13:09:11", "link": "http://arxiv.org/abs/2002.06012v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Deep Speaker Embeddings for Far-Field Speaker Recognition on Short\n  Utterances", "abstract": "Speaker recognition systems based on deep speaker embeddings have achieved\nsignificant performance in controlled conditions according to the results\nobtained for early NIST SRE (Speaker Recognition Evaluation) datasets. From the\npractical point of view, taking into account the increased interest in virtual\nassistants (such as Amazon Alexa, Google Home, AppleSiri, etc.), speaker\nverification on short utterances in uncontrolled noisy environment conditions\nis one of the most challenging and highly demanded tasks. This paper presents\napproaches aimed to achieve two goals: a) improve the quality of far-field\nspeaker verification systems in the presence of environmental noise,\nreverberation and b) reduce the system qualitydegradation for short utterances.\nFor these purposes, we considered deep neural network architectures based on\nTDNN (TimeDelay Neural Network) and ResNet (Residual Neural Network) blocks. We\nexperimented with state-of-the-art embedding extractors and their training\nprocedures. Obtained results confirm that ResNet architectures outperform the\nstandard x-vector approach in terms of speaker verification quality for both\nlong-duration and short-duration utterances. We also investigate the impact of\nspeech activity detector, different scoring models, adaptation and score\nnormalization techniques. The experimental results are presented for publicly\navailable data and verification protocols for the VoxCeleb1, VoxCeleb2, and\nVOiCES datasets.", "published": "2020-02-14 13:34:33", "link": "http://arxiv.org/abs/2002.06033v1", "categories": ["cs.SD", "cs.CL", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "FQuAD: French Question Answering Dataset", "abstract": "Recent advances in the field of language modeling have improved\nstate-of-the-art results on many Natural Language Processing tasks. Among them,\nReading Comprehension has made significant progress over the past few years.\nHowever, most results are reported in English since labeled resources available\nin other languages, such as French, remain scarce. In the present work, we\nintroduce the French Question Answering Dataset (FQuAD). FQuAD is a French\nNative Reading Comprehension dataset of questions and answers on a set of\nWikipedia articles that consists of 25,000+ samples for the 1.0 version and\n60,000+ samples for the 1.1 version. We train a baseline model which achieves\nan F1 score of 92.2 and an exact match ratio of 82.1 on the test set. In order\nto track the progress of French Question Answering models we propose a\nleader-board and we have made the 1.0 version of our dataset freely available\nat https://illuin-tech.github.io/FQuAD-explorer/.", "published": "2020-02-14 15:23:38", "link": "http://arxiv.org/abs/2002.06071v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scalable Neural Methods for Reasoning With a Symbolic Knowledge Base", "abstract": "We describe a novel way of representing a symbolic knowledge base (KB) called\na sparse-matrix reified KB. This representation enables neural modules that are\nfully differentiable, faithful to the original semantics of the KB, expressive\nenough to model multi-hop inferences, and scalable enough to use with\nrealistically large KBs. The sparse-matrix reified KB can be distributed across\nmultiple GPUs, can scale to tens of millions of entities and facts, and is\norders of magnitude faster than naive sparse-matrix implementations. The\nreified KB enables very simple end-to-end architectures to obtain competitive\nperformance on several benchmarks representing two families of tasks: KB\ncompletion, and learning semantic parsers from denotations.", "published": "2020-02-14 16:32:19", "link": "http://arxiv.org/abs/2002.06115v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Combining Visual and Textual Features for Semantic Segmentation of\n  Historical Newspapers", "abstract": "The massive amounts of digitized historical documents acquired over the last\ndecades naturally lend themselves to automatic processing and exploration.\nResearch work seeking to automatically process facsimiles and extract\ninformation thereby are multiplying with, as a first essential step, document\nlayout analysis. If the identification and categorization of segments of\ninterest in document images have seen significant progress over the last years\nthanks to deep learning techniques, many challenges remain with, among others,\nthe use of finer-grained segmentation typologies and the consideration of\ncomplex, heterogeneous documents such as historical newspapers. Besides, most\napproaches consider visual features only, ignoring textual signal. In this\ncontext, we introduce a multimodal approach for the semantic segmentation of\nhistorical newspapers that combines visual and textual features. Based on a\nseries of experiments on diachronic Swiss and Luxembourgish newspapers, we\ninvestigate, among others, the predictive power of visual and textual features\nand their capacity to generalize across time and sources. Results show\nconsistent improvement of multimodal models in comparison to a strong visual\nbaseline, as well as better robustness to high material variance.", "published": "2020-02-14 17:56:18", "link": "http://arxiv.org/abs/2002.06144v4", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Unsupervised Speaker Adaptation using Attention-based Speaker Memory for\n  End-to-End ASR", "abstract": "We propose an unsupervised speaker adaptation method inspired by the neural\nTuring machine for end-to-end (E2E) automatic speech recognition (ASR). The\nproposed model contains a memory block that holds speaker i-vectors extracted\nfrom the training data and reads relevant i-vectors from the memory through an\nattention mechanism. The resulting memory vector (M-vector) is concatenated to\nthe acoustic features or to the hidden layer activations of an E2E neural\nnetwork model. The E2E ASR system is based on the joint connectionist temporal\nclassification and attention-based encoder-decoder architecture. M-vector and\ni-vector results are compared for inserting them at different layers of the\nencoder neural network using the WSJ and TED-LIUM2 ASR benchmarks. We show that\nM-vectors, which do not require an auxiliary speaker embedding extraction\nsystem at test time, achieve similar word error rates (WERs) compared to\ni-vectors for single speaker utterances and significantly lower WERs for\nutterances in which there are speaker changes.", "published": "2020-02-14 18:31:31", "link": "http://arxiv.org/abs/2002.06165v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Semantic Relatedness and Taxonomic Word Embeddings", "abstract": "This paper connects a series of papers dealing with taxonomic word\nembeddings. It begins by noting that there are different types of semantic\nrelatedness and that different lexical representations encode different forms\nof relatedness. A particularly important distinction within semantic\nrelatedness is that of thematic versus taxonomic relatedness. Next, we present\na number of experiments that analyse taxonomic embeddings that have been\ntrained on a synthetic corpus that has been generated via a random walk over a\ntaxonomy. These experiments demonstrate how the properties of the synthetic\ncorpus, such as the percentage of rare words, are affected by the shape of the\nknowledge graph the corpus is generated from. Finally, we explore the\ninteractions between the relative sizes of natural and synthetic corpora on the\nperformance of embeddings when taxonomic and thematic embeddings are combined.", "published": "2020-02-14 20:02:11", "link": "http://arxiv.org/abs/2002.06235v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Reliability of Latent Dirichlet Allocation by Assessing Its\n  Stability Using Clustering Techniques on Replicated Runs", "abstract": "For organizing large text corpora topic modeling provides useful tools. A\nwidely used method is Latent Dirichlet Allocation (LDA), a generative\nprobabilistic model which models single texts in a collection of texts as\nmixtures of latent topics. The assignments of words to topics rely on initial\nvalues such that generally the outcome of LDA is not fully reproducible. In\naddition, the reassignment via Gibbs Sampling is based on conditional\ndistributions, leading to different results in replicated runs on the same text\ndata. This fact is often neglected in everyday practice. We aim to improve the\nreliability of LDA results. Therefore, we study the stability of LDA by\ncomparing assignments from replicated runs. We propose to quantify the\nsimilarity of two generated topics by a modified Jaccard coefficient. Using\nsuch similarities, topics can be clustered. A new pruning algorithm for\nhierarchical clustering results based on the idea that two LDA runs create\npairs of similar topics is proposed. This approach leads to the new measure\nS-CLOP ({\\bf S}imilarity of multiple sets by {\\bf C}lustering with {\\bf LO}cal\n{\\bf P}runing) for quantifying the stability of LDA models. We discuss some\ncharacteristics of this measure and illustrate it with an application to real\ndata consisting of newspaper articles from \\textit{USA Today}. Our results show\nthat the measure S-CLOP is useful for assessing the stability of LDA models or\nany other topic modeling procedure that characterize its topics by word\ndistributions. Based on the newly proposed measure for LDA stability, we\npropose a method to increase the reliability and hence to improve the\nreproducibility of empirical findings based on topic modeling. This increase in\nreliability is obtained by running the LDA several times and taking as\nprototype the most representative run, that is the LDA run with highest average\nsimilarity to all other runs.", "published": "2020-02-14 07:10:18", "link": "http://arxiv.org/abs/2003.04980v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Consistency-aware multi-channel speech enhancement using deep neural\n  networks", "abstract": "This paper proposes a deep neural network (DNN)-based multi-channel speech\nenhancement system in which a DNN is trained to maximize the quality of the\nenhanced time-domain signal. DNN-based multi-channel speech enhancement is\noften conducted in the time-frequency (T-F) domain because spatial filtering\ncan be efficiently implemented in the T-F domain. In such a case, ordinary\nobjective functions are computed on the estimated T-F mask or spectrogram.\nHowever, the estimated spectrogram is often inconsistent, and its amplitude and\nphase may change when the spectrogram is converted back to the time-domain.\nThat is, the objective function does not evaluate the enhanced time-domain\nsignal properly. To address this problem, we propose to use an objective\nfunction defined on the reconstructed time-domain signal. Specifically, speech\nenhancement is conducted by multi-channel Wiener filtering in the T-F domain,\nand its result is converted back to the time-domain. We propose two objective\nfunctions computed on the reconstructed signal where the first one is defined\nin the time-domain, and the other one is defined in the T-F domain. Our\nexperiment demonstrates the effectiveness of the proposed system comparing to\nT-F masking and mask-based beamforming.", "published": "2020-02-14 01:08:28", "link": "http://arxiv.org/abs/2002.05831v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Phase reconstruction based on recurrent phase unwrapping with deep\n  neural networks", "abstract": "Phase reconstruction, which estimates phase from a given amplitude\nspectrogram, is an active research field in acoustical signal processing with\nmany applications including audio synthesis. To take advantage of rich\nknowledge from data, several studies presented deep neural network (DNN)--based\nphase reconstruction methods. However, the training of a DNN for phase\nreconstruction is not an easy task because phase is sensitive to the shift of a\nwaveform. To overcome this problem, we propose a DNN-based two-stage phase\nreconstruction method. In the proposed method, DNNs estimate phase derivatives\ninstead of phase itself, which allows us to avoid the sensitivity problem.\nThen, phase is recursively estimated based on the estimated derivatives, which\nis named recurrent phase unwrapping (RPU). The experimental results confirm\nthat the proposed method outperformed the direct phase estimation by a DNN.", "published": "2020-02-14 01:10:06", "link": "http://arxiv.org/abs/2002.05832v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Real-time speech enhancement using equilibriated RNN", "abstract": "We propose a speech enhancement method using a causal deep neural\nnetwork~(DNN) for real-time applications. DNN has been widely used for\nestimating a time-frequency~(T-F) mask which enhances a speech signal. One\npopular DNN structure for that is a recurrent neural network~(RNN) owing to its\ncapability of effectively modelling time-sequential data like speech. In\nparticular, the long short-term memory (LSTM) is often used to alleviate the\nvanishing/exploding gradient problem which makes the training of an RNN\ndifficult. However, the number of parameters of LSTM is increased as the price\nof mitigating the difficulty of training, which requires more computational\nresources. For real-time speech enhancement, it is preferable to use a smaller\nnetwork without losing the performance. In this paper, we propose to use the\nequilibriated recurrent neural network~(ERNN) for avoiding the\nvanishing/exploding gradient problem without increasing the number of\nparameters. The proposed structure is causal, which requires only the\ninformation from the past, in order to apply it in real-time. Compared to the\nuni- and bi-directional LSTM networks, the proposed method achieved the similar\nperformance with much fewer parameters.", "published": "2020-02-14 02:00:13", "link": "http://arxiv.org/abs/2002.05843v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Sound Event Detection by Multitask Learning of Sound Events and Scenes\n  with Soft Scene Labels", "abstract": "Sound event detection (SED) and acoustic scene classification (ASC) are major\ntasks in environmental sound analysis. Considering that sound events and scenes\nare closely related to each other, some works have addressed joint analyses of\nsound events and acoustic scenes based on multitask learning (MTL), in which\nthe knowledge of sound events and scenes can help in estimating them mutually.\nThe conventional MTL-based methods utilize one-hot scene labels to train the\nrelationship between sound events and scenes; thus, the conventional methods\ncannot model the extent to which sound events and scenes are related. However,\nin the real environment, common sound events may occur in some acoustic scenes;\non the other hand, some sound events occur only in a limited acoustic scene. In\nthis paper, we thus propose a new method for SED based on MTL of SED and ASC\nusing the soft labels of acoustic scenes, which enable us to model the extent\nto which sound events and scenes are related. Experiments conducted using TUT\nSound Events 2016/2017 and TUT Acoustic Scenes 2016 datasets show that the\nproposed method improves the SED performance by 3.80% in F-score compared with\nconventional MTL-based SED.", "published": "2020-02-14 02:24:06", "link": "http://arxiv.org/abs/2002.05848v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Sequence Matching Network for Polyphonic Sound Event Localization and\n  Detection", "abstract": "Polyphonic sound event detection and direction-of-arrival estimation require\ndifferent input features from audio signals. While sound event detection mainly\nrelies on time-frequency patterns, direction-of-arrival estimation relies on\nmagnitude or phase differences between microphones. Previous approaches use the\nsame input features for sound event detection and direction-of-arrival\nestimation, and train the two tasks jointly or in a two-stage transfer-learning\nmanner. We propose a two-step approach that decouples the learning of the sound\nevent detection and directional-of-arrival estimation systems. In the first\nstep, we detect the sound events and estimate the directions-of-arrival\nseparately to optimize the performance of each system. In the second step, we\ntrain a deep neural network to match the two output sequences of the event\ndetector and the direction-of-arrival estimator. This modular and hierarchical\napproach allows the flexibility in the system design, and increase the\nperformance of the whole sound event localization and detection system. The\nexperimental results using the DCASE 2019 sound event localization and\ndetection dataset show an improved performance compared to the previous\nstate-of-the-art solutions.", "published": "2020-02-14 04:08:01", "link": "http://arxiv.org/abs/2002.05865v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Sound Event Localization based on Sound Intensity Vector Refined By\n  DNN-Based Denoising and Source Separation", "abstract": "We propose a direction-of-arrival (DOA) estimation method for Sound Event\nLocalization and Detection (SELD). Direct estimation of DOA using a deep neural\nnetwork (DNN), i.e. completely-datadriven approach, achieves high accuracy.\nHowever, there is a gap in the accuracy between DOA estimation for single and\noverlapping sources because they cannot incorporate physical knowledge.\nMeanwhile, although the accuracy of physics-based approaches is inferior to\nDNN-based approaches, it is robust for overlapping source. In this study, we\nconsider a combination of physics-based and DNN-based approaches; the sound\nintensity vectors (IVs) for physics-based DOA estimation is refined based on\nDNN-based denoising and source separation. This method enables the accurate DOA\nestimation for both single and overlapping sources using a spherical microphone\narray. Experimental results show that the proposed method achieves\nstate-of-the-art DOA estimation accuracy on an open dataset of the SELD.", "published": "2020-02-14 12:20:44", "link": "http://arxiv.org/abs/2002.05994v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Adaptive X-vector Model for Text-independent Speaker Verification", "abstract": "In this paper, adaptive mechanisms are applied in deep neural network (DNN)\ntraining for x-vector-based text-independent speaker verification. First,\nadaptive convolutional neural networks (ACNNs) are employed in frame-level\nembedding layers, where the parameters of the convolution filters are adjusted\nbased on the input features. Compared with conventional CNNs, ACNNs have more\nflexibility in capturing speaker information. Moreover, we replace conventional\nbatch normalization (BN) with adaptive batch normalization (ABN). By\ndynamically generating the scaling and shifting parameters in BN, ABN adapts\nmodels to the acoustic variability arising from various factors such as channel\nand environmental noises. Finally, we incorporate these two methods to further\nimprove performance. Experiments are carried out on the speaker in the wild\n(SITW) and VOiCES databases. The results demonstrate that the proposed methods\nsignificantly outperform the original x-vector approach.", "published": "2020-02-14 14:28:23", "link": "http://arxiv.org/abs/2002.06049v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Speaker Diarization with Region Proposal Network", "abstract": "Speaker diarization is an important pre-processing step for many speech\napplications, and it aims to solve the \"who spoke when\" problem. Although the\nstandard diarization systems can achieve satisfactory results in various\nscenarios, they are composed of several independently-optimized modules and\ncannot deal with the overlapped speech. In this paper, we propose a novel\nspeaker diarization method: Region Proposal Network based Speaker Diarization\n(RPNSD). In this method, a neural network generates overlapped speech segment\nproposals, and compute their speaker embeddings at the same time. Compared with\nstandard diarization systems, RPNSD has a shorter pipeline and can handle the\noverlapped speech. Experimental results on three diarization datasets reveal\nthat RPNSD achieves remarkable improvements over the state-of-the-art x-vector\nbaseline.", "published": "2020-02-14 19:17:25", "link": "http://arxiv.org/abs/2002.06220v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Comparison of Pooling Methods on LSTM Models for Rare Acoustic Event\n  Classification", "abstract": "Acoustic event classification (AEC) and acoustic event detection (AED) refer\nto the task of detecting whether specific target events occur in audios. As\nlong short-term memory (LSTM) leads to state-of-the-art results in various\nspeech related tasks, it is employed as a popular solution for AEC as well.\nThis paper focuses on investigating the dynamics of LSTM model on AEC tasks. It\nincludes a detailed analysis on LSTM memory retaining, and a benchmarking of\nnine different pooling methods on LSTM models using 1.7M generated mixture\nclips of multiple events with different signal-to-noise ratios. This paper\nfocuses on understanding: 1) utterance-level classification accuracy; 2)\nsensitivity to event position within an utterance. The analysis is done on the\ndataset for the detection of rare sound events from DCASE 2017 Challenge. We\nfind max pooling on the prediction level to perform the best among the nine\npooling approaches in terms of classification accuracy and insensitivity to\nevent position within an utterance. To authors' best knowledge, this is the\nfirst kind of such work focused on LSTM dynamics for AEC tasks.", "published": "2020-02-14 22:56:41", "link": "http://arxiv.org/abs/2002.06279v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Enhancement using Self-Adaptation and Multi-Head Self-Attention", "abstract": "This paper investigates a self-adaptation method for speech enhancement using\nauxiliary speaker-aware features; we extract a speaker representation used for\nadaptation directly from the test utterance. Conventional studies of deep\nneural network (DNN)--based speech enhancement mainly focus on building a\nspeaker independent model. Meanwhile, in speech applications including speech\nrecognition and synthesis, it is known that model adaptation to the target\nspeaker improves the accuracy. Our research question is whether a DNN for\nspeech enhancement can be adopted to unknown speakers without any auxiliary\nguidance signal in test-phase. To achieve this, we adopt multi-task learning of\nspeech enhancement and speaker identification, and use the output of the final\nhidden layer of speaker identification branch as an auxiliary feature. In\naddition, we use multi-head self-attention for capturing long-term dependencies\nin the speech and noise. Experimental results on a public dataset show that our\nstrategy achieves the state-of-the-art performance and also outperform\nconventional methods in terms of subjective quality.", "published": "2020-02-14 05:05:36", "link": "http://arxiv.org/abs/2002.05873v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Stable Training of DNN for Speech Enhancement based on\n  Perceptually-Motivated Black-Box Cost Function", "abstract": "Improving subjective sound quality of enhanced signals is one of the most\nimportant missions in speech enhancement. For evaluating the subjective\nquality, several methods related to perceptually-motivated objective sound\nquality assessment (OSQA) have been proposed such as PESQ (perceptual\nevaluation of speech quality). However, direct use of such measures for\ntraining deep neural network (DNN) is not allowed in most cases because popular\nOSQAs are non-differentiable with respect to DNN parameters. Therefore, the\nprevious study has proposed to approximate the score of OSQAs by an auxiliary\nDNN so that its gradient can be used for training the primary DNN. One problem\nwith this approach is instability of the training caused by the approximation\nerror of the score. To overcome this problem, we propose to use stabilization\ntechniques borrowed from reinforcement learning. The experiments, aimed to\nincrease the score of PESQ as an example, show that the proposed method (i) can\nstably train a DNN to increase PESQ, (ii) achieved the state-of-the-art PESQ\nscore on a public dataset, and (iii) resulted in better sound quality than\nconventional methods based on subjective evaluation.", "published": "2020-02-14 05:44:17", "link": "http://arxiv.org/abs/2002.05879v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Boosted Locality Sensitive Hashing: Discriminative Binary Codes for\n  Source Separation", "abstract": "Speech enhancement tasks have seen significant improvements with the advance\nof deep learning technology, but with the cost of increased computational\ncomplexity. In this study, we propose an adaptive boosting approach to learning\nlocality sensitive hash codes, which represent audio spectra efficiently. We\nuse the learned hash codes for single-channel speech denoising tasks as an\nalternative to a complex machine learning model, particularly to address the\nresource-constrained environments. Our adaptive boosting algorithm learns\nsimple logistic regressors as the weak learners. Once trained, their binary\nclassification results transform each spectrum of test noisy speech into a bit\nstring. Simple bitwise operations calculate Hamming distance to find the\nK-nearest matching frames in the dictionary of training noisy speech spectra,\nwhose associated ideal binary masks are averaged to estimate the denoising mask\nfor that test mixture. Our proposed learning algorithm differs from AdaBoost in\nthe sense that the projections are trained to minimize the distances between\nthe self-similarity matrix of the hash codes and that of the original spectra,\nrather than the misclassification rate. We evaluate our discriminative hash\ncodes on the TIMIT corpus with various noise types, and show comparative\nperformance to deep learning methods in terms of denoising performance and\ncomplexity.", "published": "2020-02-14 20:10:00", "link": "http://arxiv.org/abs/2002.06239v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Acoustic Scene Classification Using Bilinear Pooling on Time-liked and\n  Frequency-liked Convolution Neural Network", "abstract": "The current methodology in tackling Acoustic Scene Classification (ASC) task\ncan be described in two steps, preprocessing of the audio waveform into log-mel\nspectrogram and then using it as the input representation for Convolutional\nNeural Network (CNN). This paradigm shift occurs after DCASE 2016 where this\nframework model achieves the state-of-the-art result in ASC tasks on the\n(ESC-50) dataset and achieved an accuracy of 64.5%, which constitute to 20.5%\nimprovement over the baseline model, and DCASE 2016 dataset with an accuracy of\n90.0% (development) and 86.2% (evaluation), which constitute a 6.4% and 9%\nimprovements with respect to the baseline system. In this paper, we explored\nthe use of harmonic and percussive source separation (HPSS) to split the audio\ninto harmonic audio and percussive audio, which has received popularity in the\nfield of music information retrieval (MIR). Although works have been done in\nusing HPSS as input representation for CNN model in ASC task, this paper\nfurther investigate the possibility on leveraging the separated harmonic\ncomponent and percussive component by curating 2 CNNs which tries to understand\nharmonic audio and percussive audio in their natural form, one specialized in\nextracting deep features in time biased domain and another specialized in\nextracting deep features in frequency biased domain, respectively. The deep\nfeatures extracted from these 2 CNNs will then be combined using bilinear\npooling. Hence, presenting a two-stream time and frequency CNN architecture\napproach in classifying acoustic scene. The model is being evaluated on DCASE\n2019 sub task 1a dataset and scored an average of 65% on development dataset,\nKaggle Leadership Private and Public board.", "published": "2020-02-14 04:06:32", "link": "http://arxiv.org/abs/2002.07065v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
