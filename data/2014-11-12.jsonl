{"title": "Distributed Representations for Compositional Semantics", "abstract": "The mathematical representation of semantics is a key issue for Natural\nLanguage Processing (NLP). A lot of research has been devoted to finding ways\nof representing the semantics of individual words in vector spaces.\nDistributional approaches --- meaning distributed representations that exploit\nco-occurrence statistics of large corpora --- have proved popular and\nsuccessful across a number of tasks. However, natural language usually comes in\nstructures beyond the word level, with meaning arising not only from the\nindividual words but also the structure they are contained in at the phrasal or\nsentential level. Modelling the compositional process by which the meaning of\nan utterance arises from the meaning of its parts is an equally fundamental\ntask of NLP.\n  This dissertation explores methods for learning distributed semantic\nrepresentations and models for composing these into representations for larger\nlinguistic units. Our underlying hypothesis is that neural models are a\nsuitable vehicle for learning semantically rich representations and that such\nrepresentations in turn are suitable vehicles for solving important tasks in\nnatural language processing. The contribution of this thesis is a thorough\nevaluation of our hypothesis, as part of which we introduce several new\napproaches to representation learning and compositional semantics, as well as\nmultiple state-of-the-art models which apply distributed semantic\nrepresentations to various tasks in NLP.", "published": "2014-11-12 11:26:51", "link": "http://arxiv.org/abs/1411.3146v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Statistically Significant Detection of Linguistic Change", "abstract": "We propose a new computational approach for tracking and detecting\nstatistically significant linguistic shifts in the meaning and usage of words.\nSuch linguistic shifts are especially prevalent on the Internet, where the\nrapid exchange of ideas can quickly change a word's meaning. Our meta-analysis\napproach constructs property time series of word usage, and then uses\nstatistically sound change point detection algorithms to identify significant\nlinguistic shifts.\n  We consider and analyze three approaches of increasing complexity to generate\nsuch linguistic property time series, the culmination of which uses\ndistributional characteristics inferred from word co-occurrences. Using\nrecently proposed deep neural language models, we first train vector\nrepresentations of words for each time period. Second, we warp the vector\nspaces into one unified coordinate system. Finally, we construct a\ndistance-based distributional time series for each word to track it's\nlinguistic displacement over time.\n  We demonstrate that our approach is scalable by tracking linguistic change\nacross years of micro-blogging using Twitter, a decade of product reviews using\na corpus of movie reviews from Amazon, and a century of written books using the\nGoogle Book-ngrams. Our analysis reveals interesting patterns of language usage\nchange commensurate with each medium.", "published": "2014-11-12 20:37:08", "link": "http://arxiv.org/abs/1411.3315v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "H.3.3; I.2.6"], "primary_category": "cs.CL"}
