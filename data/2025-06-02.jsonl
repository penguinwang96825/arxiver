{"title": "Something Just Like TRuST : Toxicity Recognition of Span and Target", "abstract": "Toxicity in online content, including content generated by language models,\nhas become a critical concern due to its potential for negative psychological\nand social impact. This paper introduces TRuST, a comprehensive dataset\ndesigned to improve toxicity detection that merges existing datasets, and has\nlabels for toxicity, target social group, and toxic spans. It includes a\ndiverse range of target groups such as ethnicity, gender, religion, disability,\nand politics, with both human/machine-annotated and human machine-generated\ndata. We benchmark state-of-the-art large language models (LLMs) on toxicity\ndetection, target group identification, and toxic span extraction. We find that\nfine-tuned models consistently outperform zero-shot and few-shot prompting,\nthough performance remains low for certain social groups. Further, reasoning\ncapabilities do not significantly improve performance, indicating that LLMs\nhave weak social reasoning skills.", "published": "2025-06-02 23:48:16", "link": "http://arxiv.org/abs/2506.02326v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Two-stage Stochastic Assignment Games", "abstract": "In this paper, we study a two-stage stochastic version of the assignment\ngame, which is a fundamental cooperative game. Given an initial setting, the\nset of players may change in the second stage according to some probability\ndistribution, and the goal is to find core solutions that are minimally\nmodified.\n  When the probability distribution is given explicitly, we observe that the\nproblem is polynomial time solvable, as it can be modeled as an LP. More\ninterestingly, we prove that the underlying polyhedron is integral, and exploit\nthis in two ways.\n  First, integrality of the polyhedron allows us to show that the problem can\nbe well approximated when the distribution is unknown, which is a hard setting.\n  Second, we can establish an intimate connection to the well-studied\nmultistage vertex cover problem. Here, it is known that the problem is NP-hard\neven when there are only 2 stages and the graph in each stage is bipartite. As\na byproduct of our result, we can prove that the problem is polynomial-time\nsolvable if the bipartition is the same in each stage.", "published": "2025-06-02 10:17:26", "link": "http://arxiv.org/abs/2506.01509v1", "categories": ["cs.DM"], "primary_category": "cs.DM"}
{"title": "Feature-aware Hypergraph Generation via Next-Scale Prediction", "abstract": "Hypergraphs generalize traditional graphs by allowing hyperedges to connect\nmultiple nodes, making them well-suited for modeling complex structures with\nhigher-order relationships, such as 3D meshes, molecular systems, and\nelectronic circuits. While topology is central to hypergraph structure, many\nreal-world applications also require node and hyperedge features. Existing\nhypergraph generation methods focus solely on topology, often overlooking\nfeature modeling. In this work, we introduce FAHNES (feature-aware hypergraph\ngeneration via next-scale prediction), a hierarchical approach that jointly\ngenerates hypergraph topology and features. FAHNES builds a multi-scale\nrepresentation through node coarsening, then learns to reconstruct finer levels\nvia localized expansion and refinement, guided by a new node budget mechanism\nthat controls cluster splitting. We evaluate FAHNES on synthetic hypergraphs,\n3D meshes, and molecular datasets. FAHNES achieves competitive results in\nreconstructing topology and features, establishing a foundation for future\nresearch in featured hypergraph generative modeling.", "published": "2025-06-02 09:24:08", "link": "http://arxiv.org/abs/2506.01467v1", "categories": ["cs.LG", "cs.DM"], "primary_category": "cs.LG"}
{"title": "The random $k$-SAT Gibbs uniqueness threshold revisited", "abstract": "We prove that for any $k\\geq3$ for clause/variable ratios up to the Gibbs\nuniqueness threshold of the corresponding Galton-Watson tree, the number of\nsatisfying assignments of random $k$-SAT formulas is given by the `replica\nsymmetric solution' predicted by physics methods [Monasson, Zecchina: Phys.\nRev. Lett. (1996)]. Furthermore, while the Gibbs uniqueness threshold is still\nnot known precisely for any $k\\geq3$, we derive new lower bounds on this\nthreshold that improve over prior work [Montanari and Shah: SODA (2007)].The\nimprovement is significant particularly for small $k$.", "published": "2025-06-02 06:32:12", "link": "http://arxiv.org/abs/2506.01359v1", "categories": ["cs.DM", "math.CO", "math.PR", "68Q87, 60C05, 68R07"], "primary_category": "cs.DM"}
{"title": "Reweighted Spectral Partitioning Works: Bounds for Special Graph Classes", "abstract": "Spectral partitioning is a method that can be used to compute small sparse\ncuts or small edge-separators in a wide variety of graph classes, by computing\nthe second-smallest eigenvalue (and eigenvector) of the Laplacian matrix. Upper\nbounds on this eigenvalue for certain graph classes imply that the method\nobtains small edge-separators for these classes, usually with a sub-optimal\ndependence on the maximum degree. In this work, we show that a related method,\ncalled reweighted spectral partitioning, guarantees near-optimal sparse\nvertex-cuts and vertex-separators in a wide variety of graph classes. In many\ncases, this involves little-to-no necessary dependence on maximum degree.\n  We also obtain a new proof of the planar separator theorem, a strengthened\neigenvalue bound for bounded-genus graphs, and a refined form of the recent\nCheeger-style inequality for vertex expansion via a specialized\ndimension-reduction step.", "published": "2025-06-02 00:40:30", "link": "http://arxiv.org/abs/2506.01228v1", "categories": ["cs.DS", "cs.CG", "cs.DM"], "primary_category": "cs.DS"}
{"title": "Entity Image and Mixed-Modal Image Retrieval Datasets", "abstract": "Despite advances in multimodal learning, challenging benchmarks for\nmixed-modal image retrieval that combines visual and textual information are\nlacking. This paper introduces a novel benchmark to rigorously evaluate image\nretrieval that demands deep cross-modal contextual understanding. We present\ntwo new datasets: the Entity Image Dataset (EI), providing canonical images for\nWikipedia entities, and the Mixed-Modal Image Retrieval Dataset (MMIR), derived\nfrom the WIT dataset. The MMIR benchmark features two challenging query types\nrequiring models to ground textual descriptions in the context of provided\nvisual entities: single entity-image queries (one entity image with descriptive\ntext) and multi-entity-image queries (multiple entity images with relational\ntext). We empirically validate the benchmark's utility as both a training\ncorpus and an evaluation set for mixed-modal retrieval. The quality of both\ndatasets is further affirmed through crowd-sourced human annotations. The\ndatasets are accessible through the GitHub page:\nhttps://github.com/google-research-datasets/wit-retrieval.", "published": "2025-06-02 22:04:06", "link": "http://arxiv.org/abs/2506.02291v1", "categories": ["cs.CV", "cs.IR"], "primary_category": "cs.CV"}
{"title": "TransAct V2: Lifelong User Action Sequence Modeling on Pinterest Recommendation", "abstract": "Modeling user action sequences has become a popular focus in industrial\nrecommendation system research, particularly for Click-Through Rate (CTR)\nprediction tasks. However, industry-scale CTR models often rely on short user\nsequences, limiting their ability to capture long-term behavior. Additionally,\nthese models typically lack an integrated action-prediction task within a\npoint-wise ranking framework, reducing their predictive power. They also rarely\naddress the infrastructure challenges involved in efficiently serving\nlarge-scale sequential models. In this paper, we introduce TransAct V2, a\nproduction model for Pinterest's Homefeed ranking system, featuring three key\ninnovations: (1) leveraging very long user sequences to improve CTR\npredictions, (2) integrating a Next Action Loss function for enhanced user\naction forecasting, and (3) employing scalable, low-latency deployment\nsolutions tailored to handle the computational demands of extended user action\nsequences.", "published": "2025-06-02 21:15:20", "link": "http://arxiv.org/abs/2506.02267v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "Towards Human-like Preference Profiling in Sequential Recommendation", "abstract": "Sequential recommendation systems aspire to profile users by interpreting\ntheir interaction histories, echoing how humans make decisions by weighing\nexperience, relative preference strength, and situational relevance. Yet,\nexisting large language model (LLM)-based recommenders often fall short of\nmimicking the flexible, context-aware decision strategies humans exhibit,\nneglecting the structured, dynamic, and context-aware mechanisms fundamental to\nhuman behaviors. To bridge this gap, we propose RecPO, a preference\noptimization framework that models structured feedback and contextual delay to\nemulate human-like prioritization in sequential recommendation RecPO exploits\nadaptive reward margins based on inferred preference hierarchies and temporal\nsignals, enabling the model to favor immediately relevant items and to\ndistinguish between varying degrees of preference and aversion. Extensive\nexperiments across five real-world datasets demonstrate that RecPO not only\nyields performance gains over state-of-the-art baselines, but also mirrors key\ncharacteristics of human decision-making: favoring timely satisfaction,\nmaintaining coherent preferences, and exercising discernment under shifting\ncontexts.", "published": "2025-06-02 21:09:29", "link": "http://arxiv.org/abs/2506.02261v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "A Dynamic Framework for Semantic Grouping of Common Data Elements (CDE) Using Embeddings and Clustering", "abstract": "This research aims to develop a dynamic and scalable framework to facilitate\nharmonization of Common Data Elements (CDEs) across heterogeneous biomedical\ndatasets by addressing challenges such as semantic heterogeneity, structural\nvariability, and context dependence to streamline integration, enhance\ninteroperability, and accelerate scientific discovery. Our methodology\nleverages Large Language Models (LLMs) for context-aware text embeddings that\nconvert CDEs into dense vectors capturing semantic relationships and patterns.\nThese embeddings are clustered using Hierarchical Density-Based Spatial\nClustering of Applications with Noise (HDBSCAN) to group semantically similar\nCDEs. The framework incorporates four key steps: (1) LLM-based text embedding\nto mathematically represent semantic context, (2) unsupervised clustering of\nembeddings via HDBSCAN, (3) automated labeling using LLM summarization, and (4)\nsupervised learning to train a classifier assigning new or unclustered CDEs to\nlabeled clusters. Evaluated on the NIH NLM CDE Repository with over 24,000\nCDEs, the system identified 118 meaningful clusters at an optimized minimum\ncluster size of 20. The classifier achieved 90.46 percent overall accuracy,\nperforming best in larger categories. External validation against Gravity\nProjects Social Determinants of Health domains showed strong agreement\n(Adjusted Rand Index 0.52, Normalized Mutual Information 0.78), indicating that\nembeddings effectively capture cluster characteristics. This adaptable and\nscalable approach offers a practical solution to CDE harmonization, improving\nselection efficiency and supporting ongoing data interoperability.", "published": "2025-06-02 18:43:37", "link": "http://arxiv.org/abs/2506.02160v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "GLoSS: Generative Language Models with Semantic Search for Sequential Recommendation", "abstract": "We propose Generative Low-rank language model with Semantic Search (GLoSS), a\ngenerative recommendation framework that combines large language models with\ndense retrieval for sequential recommendation. Unlike prior methods such as\nGPT4Rec, which rely on lexical matching via BM25, GLoSS uses semantic search to\nretrieve relevant items beyond lexical matching. For query generation, we\nemploy 4-bit quantized LlaMA-3 models fine-tuned with low-rank adaptation\n(LoRA), enabling efficient training and inference on modest hardware. We\nevaluate GLoSS on three real-world Amazon review datasets: Beauty, Toys, and\nSports, and find that it achieves state-of-the-art performance. Compared to\ntraditional ID-based baselines, GLoSS improves Recall@5 by 33.3%, 52.8%, and\n15.2%, and NDCG@5 by 30.0%, 42.6%, and 16.1%, respectively. It also outperforms\nLLM-based recommenders such as P5, GPT4Rec, LlamaRec and E4SRec with Recall@5\ngains of 4.3%, 22.8%, and 29.5%. Additionally, user segment evaluations show\nthat GLoSS performs particularly well for cold-start users in the Amazon Toys\nand Sports datasets, and benefits from longer user histories in Amazon Beauty\ndataset, demonstrating robustness across different levels of interaction\nlengths.", "published": "2025-06-02 17:31:42", "link": "http://arxiv.org/abs/2506.01910v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Getting almost all the bits from a quantum random access code", "abstract": "A quantum random access code (QRAC) is a map $x\\mapsto\\rho_x$ that encodes\n$n$-bit strings $x$ into $m$-qubit quantum states $\\rho_x$, in a way that\nallows us to recover any one bit of $x$ with success probability $\\geq p$. The\nmeasurement on $\\rho_x$ that is used to recover, say, $x_1$ may destroy all the\ninformation about the other bits; this is in fact what happens in the\nwell-known QRAC that encodes $n=2$ bits into $m=1$ qubits. Does this generalize\nto large $n$, i.e., could there exist QRACs that are so \"obfuscated\" that one\ncannot get much more than one bit out of them? Here we show that this is not\nthe case: for every QRAC there exists a measurement that (with high\nprobability) recovers the full $n$-bit string $x$ up to small Hamming distance,\neven for the worst-case $x$.", "published": "2025-06-02 17:24:30", "link": "http://arxiv.org/abs/2506.01903v1", "categories": ["quant-ph", "cs.IR"], "primary_category": "quant-ph"}
{"title": "When Should Dense Retrievers Be Updated in Evolving Corpora? Detecting Out-of-Distribution Corpora Using GradNormIR", "abstract": "Dense retrievers encode texts into embeddings to efficiently retrieve\nrelevant documents from large databases in response to user queries. However,\nreal-world corpora continually evolve, leading to a shift from the original\ntraining distribution of the retriever. Without timely updates or retraining,\nindexing newly emerging documents can degrade retrieval performance for future\nqueries. Thus, identifying when a dense retriever requires an update is\ncritical for maintaining robust retrieval systems. In this paper, we propose a\nnovel task of predicting whether a corpus is out-of-distribution (OOD) relative\nto a dense retriever before indexing. Addressing this task allows us to\nproactively manage retriever updates, preventing potential retrieval failures.\nWe introduce GradNormIR, an unsupervised approach that leverages gradient norms\nto detect OOD corpora effectively. Experiments on the BEIR benchmark\ndemonstrate that GradNormIR enables timely updates of dense retrievers in\nevolving document collections, significantly enhancing retrieval robustness and\nefficiency.", "published": "2025-06-02 17:06:35", "link": "http://arxiv.org/abs/2506.01877v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "CiteEval: Principle-Driven Citation Evaluation for Source Attribution", "abstract": "Citation quality is crucial in information-seeking systems, directly\ninfluencing trust and the effectiveness of information access. Current\nevaluation frameworks, both human and automatic, mainly rely on Natural\nLanguage Inference (NLI) to assess binary or ternary supportiveness from cited\nsources, which we argue is a suboptimal proxy for citation evaluation. In this\nwork we introduce CiteEval, a citation evaluation framework driven by\nprinciples focusing on fine-grained citation assessment within a broad context,\nencompassing not only the cited sources but the full retrieval context, user\nquery, and generated text. Guided by the proposed framework, we construct\nCiteBench, a multi-domain benchmark with high-quality human annotations on\ncitation quality. To enable efficient evaluation, we further develop\nCiteEval-Auto, a suite of model-based metrics that exhibit strong correlation\nwith human judgments. Experiments across diverse systems demonstrate\nCiteEval-Auto's superior ability to capture the multifaceted nature of\ncitations compared to existing metrics, offering a principled and scalable\napproach to evaluate and improve model-generated citations.", "published": "2025-06-02 16:15:34", "link": "http://arxiv.org/abs/2506.01829v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "SPOT-Trip: Dual-Preference Driven Out-of-Town Trip Recommendation", "abstract": "Out-of-town trip recommendation aims to generate a sequence of Points of\nInterest (POIs) for users traveling from their hometowns to previously\nunvisited regions based on personalized itineraries, e.g., origin, destination,\nand trip duration. Modeling the complex user preferences--which often exhibit a\ntwo-fold nature of static and dynamic interests--is critical for effective\nrecommendations. However, the sparsity of out-of-town check-in data presents\nsignificant challenges in capturing such user preferences. Meanwhile, existing\nmethods often conflate the static and dynamic preferences, resulting in\nsuboptimal performance. In this paper, we for the first time systematically\nstudy the problem of out-of-town trip recommendation. A novel framework\nSPOT-Trip is proposed to explicitly learns the dual static-dynamic user\npreferences. Specifically, to handle scarce data, we construct a POI attribute\nknowledge graph to enrich the semantic modeling of users' hometown and\nout-of-town check-ins, enabling the static preference modeling through\nattribute relation-aware aggregation. Then, we employ neural ordinary\ndifferential equations (ODEs) to capture the continuous evolution of latent\ndynamic user preferences and innovatively combine a temporal point process to\ndescribe the instantaneous probability of each preference behavior. Further, a\nstatic-dynamic fusion module is proposed to merge the learned static and\ndynamic user preferences. Extensive experiments on real data offer insight into\nthe effectiveness of the proposed solutions, showing that SPOT-Trip achieves\nperformance improvement by up to 17.01%.", "published": "2025-06-02 14:11:21", "link": "http://arxiv.org/abs/2506.01705v2", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "GRAM: Generative Recommendation via Semantic-aware Multi-granular Late Fusion", "abstract": "Generative recommendation is an emerging paradigm that leverages the\nextensive knowledge of large language models by formulating recommendations\ninto a text-to-text generation task. However, existing studies face two key\nlimitations in (i) incorporating implicit item relationships and (ii) utilizing\nrich yet lengthy item information. To address these challenges, we propose a\nGenerative Recommender via semantic-Aware Multi-granular late fusion (GRAM),\nintroducing two synergistic innovations. First, we design semantic-to-lexical\ntranslation to encode implicit hierarchical and collaborative item\nrelationships into the vocabulary space of LLMs. Second, we present\nmulti-granular late fusion to integrate rich semantics efficiently with minimal\ninformation loss. It employs separate encoders for multi-granular prompts,\ndelaying the fusion until the decoding stage. Experiments on four benchmark\ndatasets show that GRAM outperforms eight state-of-the-art generative\nrecommendation models, achieving significant improvements of 11.5-16.0% in\nRecall@5 and 5.3-13.6% in NDCG@5. The source code is available at\nhttps://github.com/skleee/GRAM.", "published": "2025-06-02 13:42:46", "link": "http://arxiv.org/abs/2506.01673v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Small Stickers, Big Meanings: A Multilingual Sticker Semantic Understanding Dataset with a Gamified Approach", "abstract": "Stickers, though small, are a highly condensed form of visual expression,\nubiquitous across messaging platforms and embraced by diverse cultures,\ngenders, and age groups. Despite their popularity, sticker retrieval remains an\nunderexplored task due to the significant human effort and subjectivity\ninvolved in constructing high-quality sticker query datasets. Although large\nlanguage models (LLMs) excel at general NLP tasks, they falter when confronted\nwith the nuanced, intangible, and highly specific nature of sticker query\ngeneration.\n  To address this challenge, we propose a threefold solution. First, we\nintroduce Sticktionary, a gamified annotation framework designed to gather\ndiverse, high-quality, and contextually resonant sticker queries. Second, we\npresent StickerQueries, a multilingual sticker query dataset containing 1,115\nEnglish and 615 Chinese queries, annotated by over 60 contributors across 60+\nhours. Lastly, Through extensive quantitative and qualitative evaluation, we\ndemonstrate that our approach significantly enhances query generation quality,\nretrieval accuracy, and semantic understanding in the sticker domain. To\nsupport future research, we publicly release our multilingual dataset along\nwith two fine-tuned query generation models.", "published": "2025-06-02 13:38:45", "link": "http://arxiv.org/abs/2506.01668v1", "categories": ["cs.MM", "cs.IR"], "primary_category": "cs.MM"}
{"title": "Engram Memory Encoding and Retrieval: A Neurocomputational Perspective", "abstract": "Despite substantial research into the biological basis of memory, the precise\nmechanisms by which experiences are encoded, stored, and retrieved in the brain\nremain incompletely understood. A growing body of evidence supports the engram\ntheory, which posits that sparse populations of neurons undergo lasting\nphysical and biochemical changes to support long-term memory. Yet, a\ncomprehensive computational framework that integrates biological findings with\nmechanistic models remains elusive. This work synthesizes insights from\ncellular neuroscience and computational modeling to address key challenges in\nengram research: how engram neurons are identified and manipulated; how\nsynaptic plasticity mechanisms contribute to stable memory traces; and how\nsparsity promotes efficient, interference-resistant representations. Relevant\ncomputational approaches -- such as sparse regularization, engram gating, and\nbiologically inspired architectures like Sparse Distributed Memory and spiking\nneural networks -- are also examined. Together, these findings suggest that\nmemory efficiency, capacity, and stability emerge from the interaction of\nplasticity and sparsity constraints. By integrating neurobiological and\ncomputational perspectives, this paper provides a comprehensive theoretical\nfoundation for engram research and proposes a roadmap for future inquiry into\nthe mechanisms underlying memory, with implications for the diagnosis and\ntreatment of memory-related disorders.", "published": "2025-06-02 13:30:39", "link": "http://arxiv.org/abs/2506.01659v1", "categories": ["cs.NE", "cs.AI", "cs.IR", "cs.LG", "q-bio.NC", "I.2.0; I.2.4; I.2.6; I.2.m; E.1; E.2; E.4; H.3; J.3; J.4"], "primary_category": "cs.NE"}
{"title": "Argument-Centric Causal Intervention Method for Mitigating Bias in Cross-Document Event Coreference Resolution", "abstract": "Cross-document Event Coreference Resolution (CD-ECR) is a fundamental task in\nnatural language processing (NLP) that seeks to determine whether event\nmentions across multiple documents refer to the same real-world occurrence.\nHowever, current CD-ECR approaches predominantly rely on trigger features\nwithin input mention pairs, which induce spurious correlations between\nsurface-level lexical features and coreference relationships, impairing the\noverall performance of the models. To address this issue, we propose a novel\ncross-document event coreference resolution method based on Argument-Centric\nCausal Intervention (ACCI). Specifically, we construct a structural causal\ngraph to uncover confounding dependencies between lexical triggers and\ncoreference labels, and introduce backdoor-adjusted interventions to isolate\nthe true causal effect of argument semantics. To further mitigate spurious\ncorrelations, ACCI integrates a counterfactual reasoning module that quantifies\nthe causal influence of trigger word perturbations, and an argument-aware\nenhancement module to promote greater sensitivity to semantically grounded\ninformation. In contrast to prior methods that depend on costly data\naugmentation or heuristic-based filtering, ACCI enables effective debiasing in\na unified end-to-end framework without altering the underlying training\nprocedure. Extensive experiments demonstrate that ACCI achieves CoNLL F1 of\n88.4% on ECB+ and 85.2% on GVC, achieving state-of-the-art performance. The\nimplementation and materials are available at https://github.com/era211/ACCI.", "published": "2025-06-02 09:46:59", "link": "http://arxiv.org/abs/2506.01488v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Building Entity Association Mining Framework for Knowledge Discovery", "abstract": "Extracting useful signals or pattern to support important business decisions\nfor example analyzing investment product traction and discovering customer\npreference, risk monitoring etc. from unstructured text is a challenging task.\nCapturing interaction of entities or concepts and association mining is a\ncrucial component in text mining, enabling information extraction and reasoning\nover and knowledge discovery from text. Furthermore, it can be used to enrich\nor filter knowledge graphs to guide exploration processes, descriptive\nanalytics and uncover hidden stories in the text. In this paper, we introduce a\ndomain independent pipeline i.e., generalized framework to enable document\nfiltering, entity extraction using various sources (or techniques) as plug-ins\nand association mining to build any text mining business use-case and\nquantitatively define a scoring metric for ranking purpose. The proposed\nframework has three major components a) Document filtering: filtering\ndocuments/text of interest from massive amount of texts b) Configurable entity\nextraction pipeline: include entity extraction techniques i.e., i) DBpedia\nSpotlight, ii) Spacy NER, iii) Custom Entity Matcher, iv) Phrase extraction (or\ndictionary) based c) Association Relationship Mining: To generates\nco-occurrence graph to analyse potential relationships among entities,\nconcepts. Further, co-occurrence count based frequency statistics provide a\nholistic window to observe association trends or buzz rate in specific business\ncontext. The paper demonstrates the usage of framework as fundamental building\nbox in two financial use-cases namely brand product discovery and vendor risk\nmonitoring. We aim that such framework will remove duplicated effort, minimize\nthe development effort, and encourage reusability and rapid prototyping in\nassociation mining business applications for institutions.", "published": "2025-06-02 09:08:38", "link": "http://arxiv.org/abs/2506.01451v1", "categories": ["cs.CL", "cs.IR", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Generative Next POI Recommendation with Semantic ID", "abstract": "Point-of-interest (POI) recommendation systems aim to predict the next\ndestinations of user based on their preferences and historical check-ins.\nExisting generative POI recommendation methods usually employ random numeric\nIDs for POIs, limiting the ability to model semantic relationships between\nsimilar locations. In this paper, we propose Generative Next POI Recommendation\nwith Semantic ID (GNPR-SID), an LLM-based POI recommendation model with a novel\nsemantic POI ID (SID) representation method that enhances the semantic\nunderstanding of POI modeling. There are two key components in our GNPR-SID:\n(1) a Semantic ID Construction module that generates semantically rich POI IDs\nbased on semantic and collaborative features, and (2) a Generative POI\nRecommendation module that fine-tunes LLMs to predict the next POI using these\nsemantic IDs. By incorporating user interaction patterns and POI semantic\nfeatures into the semantic ID generation, our method improves the\nrecommendation accuracy and generalization of the model. To construct\nsemantically related SIDs, we propose a POI quantization method based on\nresidual quantized variational autoencoder, which maps POIs into a discrete\nsemantic space. We also propose a diversity loss to ensure that SIDs are\nuniformly distributed across the semantic space. Extensive experiments on three\nbenchmark datasets demonstrate that GNPR-SID substantially outperforms\nstate-of-the-art methods, achieving up to 16% improvement in recommendation\naccuracy.", "published": "2025-06-02 07:04:16", "link": "http://arxiv.org/abs/2506.01375v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "TimeGraph: Synthetic Benchmark Datasets for Robust Time-Series Causal Discovery", "abstract": "Robust causal discovery in time series datasets depends on reliable benchmark\ndatasets with known ground-truth causal relationships. However, such datasets\nremain scarce, and existing synthetic alternatives often overlook critical\ntemporal properties inherent in real-world data, including nonstationarity\ndriven by trends and seasonality, irregular sampling intervals, and the\npresence of unobserved confounders. To address these challenges, we introduce\nTimeGraph, a comprehensive suite of synthetic time-series benchmark datasets\nthat systematically incorporates both linear and nonlinear dependencies while\nmodeling key temporal characteristics such as trends, seasonal effects, and\nheterogeneous noise patterns. Each dataset is accompanied by a fully specified\ncausal graph featuring varying densities and diverse noise distributions and is\nprovided in two versions: one including unobserved confounders and one without,\nthereby offering extensive coverage of real-world complexity while preserving\nmethodological neutrality. We further demonstrate the utility of TimeGraph\nthrough systematic evaluations of state-of-the-art causal discovery algorithms\nincluding PCMCI+, LPCMCI, and FGES across a diverse array of configurations and\nmetrics. Our experiments reveal significant variations in algorithmic\nperformance under realistic temporal conditions, underscoring the need for\nrobust synthetic benchmarks in the fair and transparent assessment of causal\ndiscovery methods. The complete TimeGraph suite, including dataset generation\nscripts, evaluation metrics, and recommended experimental protocols, is freely\navailable to facilitate reproducible research and foster community-driven\nadvancements in time-series causal discovery.", "published": "2025-06-02 06:34:11", "link": "http://arxiv.org/abs/2506.01361v1", "categories": ["cs.LG", "cs.IR", "stat.ML", "62H12, 62P10, 68T05", "I.2.6; I.5.1; G.3"], "primary_category": "cs.LG"}
{"title": "A Platform for Investigating Public Health Content with Efficient Concern Classification", "abstract": "A recent rise in online content expressing concerns with public health\ninitiatives has contributed to already stalled uptake of preemptive measures\nglobally. Future public health efforts must attempt to understand such content,\nwhat concerns it may raise among readers, and how to effectively respond to it.\nTo this end, we present ConcernScope, a platform that uses a teacher-student\nframework for knowledge transfer between large language models and light-weight\nclassifiers to quickly and effectively identify the health concerns raised in a\ntext corpus. The platform allows uploading massive files directly,\nautomatically scraping specific URLs, and direct text editing. ConcernScope is\nbuilt on top of a taxonomy of public health concerns. Intended for public\nhealth officials, we demonstrate several applications of this platform: guided\ndata exploration to find useful examples of common concerns found in online\ncommunity datasets, identification of trends in concerns through an example\ntime series analysis of 186,000 samples, and finding trends in topic frequency\nbefore and after significant events.", "published": "2025-06-02 04:36:13", "link": "http://arxiv.org/abs/2506.01308v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Experimental Covert Communication Using Software-Defined Radio", "abstract": "The fundamental information-theoretic limits of covert, or low probability of\ndetection (LPD), communication have been extensively studied for over a decade,\nresulting in the square root law (SRL): only $L\\sqrt{n}$ covert bits can be\nreliably transmitted over time-bandwidth product $n$, for constant $L>0$.\nTransmitting more either results in detection or decoding errors. The SRL\nimposes significant constraints on hardware realization of provably-secure\ncovert communication. Thus, experimental validation of covert communication is\nunderexplored: to date, only two experimental studies of SRL-based covert\ncommunication are available, both focusing on optical channels. Here, we report\nour initial results demonstrating the provably-secure covert radio-frequency\n(RF) communication using software-defined radios (SDRs). These validate\ntheoretical predictions, open practical avenues for implementing covert\ncommunication systems, as well as raise future research questions.", "published": "2025-06-02 22:30:54", "link": "http://arxiv.org/abs/2506.02297v1", "categories": ["cs.NI", "cs.IT", "cs.SY", "eess.SY", "math.IT"], "primary_category": "cs.NI"}
{"title": "MLorc: Momentum Low-rank Compression for Large Language Model Adaptation", "abstract": "With increasing size of large language models (LLMs), full-parameter\nfine-tuning imposes substantial memory demands. To alleviate this, we propose a\nnovel memory-efficient training paradigm called Momentum Low-rank compression\n(MLorc). By directly compressing and reconstructing momentum rather than\ngradients, MLorc avoids imposing a fixed-rank constraint on weight update\nmatrices and better preserves the training dynamics of full-parameter\nfine-tuning, in contrast to existing low-rank approaches such as LoRA and\nGaLore. Empirically, MLorc consistently outperforms other memory-efficient\ntraining methods, matches or even exceeds the performance of full fine-tuning\nwith a small rank (e.g., $r=4$), and generalizes well across different\noptimizers -- all while not compromising time or memory efficiency.\nFurthermore, we provide a theoretical guarantee for its convergence under\nreasonable assumptions.", "published": "2025-06-02 17:21:10", "link": "http://arxiv.org/abs/2506.01897v2", "categories": ["cs.LG", "cs.IT", "math.IT", "math.OC"], "primary_category": "cs.LG"}
{"title": "Trade-offs in Data Memorization via Strong Data Processing Inequalities", "abstract": "Recent research demonstrated that training large language models involves\nmemorization of a significant fraction of training data. Such memorization can\nlead to privacy violations when training on sensitive user data and thus\nmotivates the study of data memorization's role in learning. In this work, we\ndevelop a general approach for proving lower bounds on excess data\nmemorization, that relies on a new connection between strong data processing\ninequalities and data memorization. We then demonstrate that several simple and\nnatural binary classification problems exhibit a trade-off between the number\nof samples available to a learning algorithm, and the amount of information\nabout the training data that a learning algorithm needs to memorize to be\naccurate. In particular, $\\Omega(d)$ bits of information about the training\ndata need to be memorized when $O(1)$ $d$-dimensional examples are available,\nwhich then decays as the number of examples grows at a problem-specific rate.\nFurther, our lower bounds are generally matched (up to logarithmic factors) by\nsimple learning algorithms. We also extend our lower bounds to more general\nmixture-of-clusters models. Our definitions and results build on the work of\nBrown et al. (2021) and address several limitations of the lower bounds in\ntheir work.", "published": "2025-06-02 16:41:49", "link": "http://arxiv.org/abs/2506.01855v1", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Practical Short-Length Coding Schemes for Binary Distributed Hypothesis Testing", "abstract": "This paper addresses the design of practical shortlength coding schemes for\nDistributed Hypothesis Testing (DHT). While most prior work on DHT has focused\non informationtheoretic analyses, deriving bounds on Type-II error exponents\nvia achievability schemes based on quantization and quantizebinning, the\npractical implementation of DHT coding schemes has remained largely unexplored.\nMoreover, existing practical coding solutions for quantization and\nquantize-binning approaches were developed for source reconstruction tasks\nconsidering very long code length, and they are not directly applicable to DHT.\nIn this context, this paper introduces efficient shortlength implementations of\nquantization and quantize-binning schemes for DHT, constructed from short\nbinary linear block codes. Numerical results show the efficiency of the\nproposed coding schemes compared to uncoded cases and to existing schemes\ninitially developed for data reconstruction. In addition to practical code\ndesign, the paper derives exact analytical expressions for the Type-I and\nType-II error probabilities associated with each proposed scheme. The provided\nanalytical expressions are shown to predict accurately the practical\nperformance measured from Monte-Carlo simulations of the proposed schemes.\nThese theoretical results are novel and offer a useful framework for optimizing\nand comparing practical DHT schemes across a wide range of source and code\nparameters.", "published": "2025-06-02 14:53:56", "link": "http://arxiv.org/abs/2506.01747v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Bayes optimal learning of attention-indexed models", "abstract": "We introduce the attention-indexed model (AIM), a theoretical framework for\nanalyzing learning in deep attention layers. Inspired by multi-index models,\nAIM captures how token-level outputs emerge from layered bilinear interactions\nover high-dimensional embeddings. Unlike prior tractable attention models, AIM\nallows full-width key and query matrices, aligning more closely with practical\ntransformers. Using tools from statistical mechanics and random matrix theory,\nwe derive closed-form predictions for Bayes-optimal generalization error and\nidentify sharp phase transitions as a function of sample complexity, model\nwidth, and sequence length. We propose a matching approximate message passing\nalgorithm and show that gradient descent can reach optimal performance. AIM\noffers a solvable playground for understanding learning in modern attention\narchitectures.", "published": "2025-06-02 12:11:26", "link": "http://arxiv.org/abs/2506.01582v1", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.IT", "math.IT", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Self-supervised Latent Space Optimization with Nebula Variational Coding", "abstract": "Deep learning approaches process data in a layer-by-layer way with\nintermediate (or latent) features. We aim at designing a general solution to\noptimize the latent manifolds to improve the performance on classification,\nsegmentation, completion and/or reconstruction through probabilistic models.\nThis paper proposes a variational inference model which leads to a clustered\nembedding. We introduce additional variables in the latent space, called\n\\textbf{nebula anchors}, that guide the latent variables to form clusters\nduring training. To prevent the anchors from clustering among themselves, we\nemploy the variational constraint that enforces the latent features within an\nanchor to form a Gaussian distribution, resulting in a generative model we\nrefer as Nebula Variational Coding (NVC). Since each latent feature can be\nlabeled with the closest anchor, we also propose to apply metric learning in a\nself-supervised way to make the separation between clusters more explicit. As a\nconsequence, the latent variables of our variational coder form clusters which\nadapt to the generated semantic of the training data, \\textit{e.g.} the\ncategorical labels of each sample. We demonstrate experimentally that it can be\nused within different architectures designed to solve different problems\nincluding text sequence, images, 3D point clouds and volumetric data,\nvalidating the advantage of our proposed method.", "published": "2025-06-02 08:13:32", "link": "http://arxiv.org/abs/2506.01414v1", "categories": ["cs.LG", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "Formal Security Analysis of SPV Clients Versus Home-Based Full Nodes in Bitcoin-Derived Systems", "abstract": "This paper presents a mathematically rigorous formal analysis of Simplified\nPayment Verification (SPV) clients, as specified in Section 8 of the original\nBitcoin white paper, versus non-mining full nodes operated by home users. It\ndefines security as resistance to divergence from global consensus and models\ntransaction acceptance, enforcement capability, and divergence probability\nunder adversarial conditions. The results demonstrate that SPV clients, despite\nomitting script verification, are cryptographically sufficient under\nhonest-majority assumptions and topologically less vulnerable to attack than\nstructurally passive, non-enforcing full nodes. The paper introduces new axioms\non behavioral divergence and communication topology, proving that home-based\nfull nodes increase systemic entropy without contributing to consensus\nintegrity. Using a series of formally defined lemmas, propositions, and Monte\nCarlo simulation results, it is shown that SPV clients represent the rational\nequilibrium strategy for non-mining participants. This challenges the\nprevailing narrative that home validators enhance network security, providing\nformal and operational justifications for the sufficiency of SPV models.", "published": "2025-06-02 07:20:25", "link": "http://arxiv.org/abs/2506.01384v1", "categories": ["cs.CR", "cs.DC", "cs.GT", "cs.IT", "math.IT", "68M10, 68Q85, 91A40, 94A60", "F.1.2; F.2.2; K.6.5"], "primary_category": "cs.CR"}
{"title": "Rydberg Atomic Quantum MIMO Receivers for The Multi-User Uplink", "abstract": "Rydberg atomic quantum receivers (RAQRs) have emerged as a promising solution\nfor evolving wireless receivers from the classical to the quantum domain. To\nfurther unleash their great potential in wireless communications, we propose a\nflexible architecture for Rydberg atomic quantum multiple-input multiple-output\n(RAQ-MIMO) receivers in the multi-user uplink. Then the corresponding signal\nmodel of the RAQ-MIMO system is constructed by paving the way from quantum\nphysics to classical wireless communications. Explicitly, we outline the\nassociated operating principles and transmission flow. We also validate the\nlinearity of our model and its feasible region. Based on our model, we derive\nclosed-form asymptotic formulas for the ergodic achievable rate (EAR) of both\nthe maximum-ratio combining (MRC) and zero-forcing (ZF) receivers operating in\nuncorrelated fading channels (UFC) and the correlated fading channels (CFC),\nrespectively. Furthermore, we theoretically characterize the EAR difference\nboth between the UFC and CFC scenarios, as well as MRC and ZF schemes. More\nparticularly, we quantify the superiority of RAQ-MIMO receivers over the\nclassical massive MIMO (M-MIMO) receivers, specifying an increase of $\\log_{2}\n\\Pi$ of the EAR per user, $\\Pi$-fold reduction of the users' transmit power,\nand $\\sqrt[\\nu]{\\Pi}$-fold increase of the transmission distance, respectively,\nwhere $\\Pi = \\text{ReceiverGainRatio} / \\text{ReceiverNoisePowerRatio}$ of the\nsingle-sensor receivers and $\\nu$ is the path-loss exponent. Our simulation\nresults reveal that, compared to classical M-MIMO receivers, our RAQ-MIMO\nscheme can either realize $\\sim 12$ bits/s/Hz/user ($\\sim 8$ bits/s/Hz/user)\nhigher EAR, or $\\sim 10000$-fold ($\\sim 500$-fold) lower transmit power, or\nalternatively, $\\sim 100$-fold ($\\sim 21$-fold) longer distance in free-space\ntransmissions, in the standard quantum limit (photon shot limit).", "published": "2025-06-02 06:16:54", "link": "http://arxiv.org/abs/2506.01355v1", "categories": ["eess.SP", "cs.IT", "math.IT", "quant-ph"], "primary_category": "eess.SP"}
{"title": "Near-Optimal Clustering in Mixture of Markov Chains", "abstract": "We study the problem of clustering $T$ trajectories of length $H$, each\ngenerated by one of $K$ unknown ergodic Markov chains over a finite state space\nof size $S$. The goal is to accurately group trajectories according to their\nunderlying generative model. We begin by deriving an instance-dependent,\nhigh-probability lower bound on the clustering error rate, governed by the\nweighted KL divergence between the transition kernels of the chains. We then\npresent a novel two-stage clustering algorithm. In Stage~I, we apply spectral\nclustering using a new injective Euclidean embedding for ergodic Markov chains\n-- a contribution of independent interest that enables sharp concentration\nresults. Stage~II refines the initial clusters via a single step of\nlikelihood-based reassignment. Our method achieves a near-optimal clustering\nerror with high probability, under the conditions $H =\n\\tilde{\\Omega}(\\gamma_{\\mathrm{ps}}^{-1} (S^2 \\vee \\pi_{\\min}^{-1}))$ and $TH =\n\\tilde{\\Omega}(\\gamma_{\\mathrm{ps}}^{-1} S^2 )$, where $\\pi_{\\min}$ is the\nminimum stationary probability of a state across the $K$ chains and\n$\\gamma_{\\mathrm{ps}}$ is the minimum pseudo-spectral gap. These requirements\nprovide significant improvements, if not at least comparable, to the\nstate-of-the-art guarantee (Kausik et al., 2023), and moreover, our algorithm\noffers a key practical advantage: unlike existing approach, it requires no\nprior knowledge of model-specific quantities (e.g., separation between kernels\nor visitation probabilities). We conclude by discussing the inherent gap\nbetween our upper and lower bounds, providing insights into the unique\nstructure of this clustering problem.", "published": "2025-06-02 05:10:40", "link": "http://arxiv.org/abs/2506.01324v1", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT", "math.PR"], "primary_category": "stat.ML"}
{"title": "Region-of-Interest-Guided Deep Joint Source-Channel Coding for Image Transmission", "abstract": "Deep joint source-channel coding (deepJSCC) and semantic communication have\nshown promising improvements in communication performance over wireless\nnetworks. However, current approaches primarily focus on enhancing average\nperformance metrics, such as overall image reconstruction quality or task\naccuracy, which may not fully align with users' actual experience -- often\ndriven by the quality of specific regions of interest (ROI). Motivated by this,\nwe propose ROI-guided joint source-channel coding (ROI-JSCC), a novel deepJSCC\nframework that prioritizes high-quality transmission of ROI. The ROI-JSCC\nconsists of four key components: (1) ROI embedding and feature map extraction,\n(2) ROI-guided split processing, (3) ROI-based loss function design, and (4)\nROI-adaptive bandwidth allocation. Together, these components enable ROI-JSCC\nto selectively improve the reconstruction quality of varying ROI while\npreserving overall image quality without increasing computational burden.\nExperimental results under diverse communication environments demonstrate that\nROI-JSCC significantly improves ROI reconstruction quality while maintaining\ncompetitive average image quality compared to recent state-of-the-art methods.\nAll codes are available at https://github.com/hansung-choi/ROI-JSCC.", "published": "2025-06-02 02:42:50", "link": "http://arxiv.org/abs/2506.01269v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Construction of DNA codes using $\u03b8$-skew cyclic codes over $\\mathbb{F}_4 + v \\mathbb{F}_4$", "abstract": "In this paper, we investigate $\\theta$-skew cyclic codes over the ring $R=\n\\mathbb{F}_4 + v \\mathbb{F}_4$, where $v^2=v$ and $\\theta$ is a non-trivial\nautomorphism over $\\mathbb{F}_4 + v \\mathbb{F}_4$. This allows us to describe\nDNA code over this ring by characterizing $\\theta$-skew cyclic reversible DNA\ncodes and $\\theta$-skew cyclic reversible complement DNA codes. We also explore\nthe Gray images of $\\theta$-skew cyclic codes.", "published": "2025-06-02 01:23:43", "link": "http://arxiv.org/abs/2506.01236v1", "categories": ["cs.IT", "math.IT", "94B05, 94B60, 11T71"], "primary_category": "cs.IT"}
{"title": "Fairly Wired: Towards Leximin-Optimal Division of Electricity", "abstract": "In many parts of the world - particularly in developing countries - the\ndemand for electricity exceeds the available supply. In such cases, it is\nimpossible to provide electricity to all households simultaneously. This raises\na fundamental question: how should electricity be allocated fairly? In this\npaper, we explore this question through the lens of egalitarianism - a\nprinciple that emphasizes equality by prioritizing the welfare of the worst-off\nhouseholds. One natural rule that aligns with this principle is to maximize the\negalitarian welfare - the smallest utility across all households. We show that\ncomputing such an allocation is NP-hard, even under strong simplifying\nassumptions. Leximin is a stronger fairness notion that generalizes the\negalitarian welfare: it also requires to maximize the smallest utility, but\nthen, subject to that, the second-smallest, then the third, and so on. The\nhardness results extends directly to leximin as well. Despite this, we present\na Fully Polynomial-Time Approximation Scheme (FPTAS) for leximin in the special\ncase where the network connectivity graph is a tree. This means that we can\nefficiently approximate leximin - and, in particular, the egalitarian welfare -\nto any desired level of accuracy.", "published": "2025-06-02 19:30:03", "link": "http://arxiv.org/abs/2506.02193v1", "categories": ["cs.GT", "cs.DS", "cs.MA"], "primary_category": "cs.GT"}
{"title": "Online Competitive Information Gathering for Partially Observable Trajectory Games", "abstract": "Game-theoretic agents must make plans that optimally gather information about\ntheir opponents. These problems are modeled by partially observable stochastic\ngames (POSGs), but planning in fully continuous POSGs is intractable without\nheavy offline computation or assumptions on the order of belief maintained by\neach player. We formulate a finite history/horizon refinement of POSGs which\nadmits competitive information gathering behavior in trajectory space, and\nthrough a series of approximations, we present an online method for computing\nrational trajectory plans in these games which leverages particle-based\nestimations of the joint state space and performs stochastic gradient play. We\nalso provide the necessary adjustments required to deploy this method on\nindividual agents. The method is tested in continuous pursuit-evasion and\nwarehouse-pickup scenarios (alongside extensions to $N > 2$ players and to more\ncomplex environments with visual and physical obstacles), demonstrating\nevidence of active information gathering and outperforming passive competitors.", "published": "2025-06-02 17:45:58", "link": "http://arxiv.org/abs/2506.01927v1", "categories": ["cs.GT", "cs.AI", "cs.MA", "cs.RO"], "primary_category": "cs.GT"}
{"title": "Beyond Static Responses: Multi-Agent LLM Systems as a New Paradigm for Social Science Research", "abstract": "As large language models (LLMs) transition from static tools to fully agentic\nsystems, their potential for transforming social science research has become\nincreasingly evident. This paper introduces a structured framework for\nunderstanding the diverse applications of LLM-based agents, ranging from simple\ndata processors to complex, multi-agent systems capable of simulating emergent\nsocial dynamics. By mapping this developmental continuum across six levels, the\npaper clarifies the technical and methodological boundaries between different\nagentic architectures, providing a comprehensive overview of current\ncapabilities and future potential. It highlights how lower-tier systems\nstreamline conventional tasks like text classification and data annotation,\nwhile higher-tier systems enable novel forms of inquiry, including the study of\ngroup dynamics, norm formation, and large-scale social processes. However,\nthese advancements also introduce significant challenges, including issues of\nreproducibility, ethical oversight, and the risk of emergent biases. The paper\ncritically examines these concerns, emphasizing the need for robust validation\nprotocols, interdisciplinary collaboration, and standardized evaluation\nmetrics. It argues that while LLM-based agents hold transformative potential\nfor the social sciences, realizing this promise will require careful,\ncontext-sensitive deployment and ongoing methodological refinement. The paper\nconcludes with a call for future research that balances technical innovation\nwith ethical responsibility, encouraging the development of agentic systems\nthat not only replicate but also extend the frontiers of social science,\noffering new insights into the complexities of human behavior.", "published": "2025-06-02 16:27:29", "link": "http://arxiv.org/abs/2506.01839v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "Agentic AI and Multiagentic: Are We Reinventing the Wheel?", "abstract": "The terms Agentic AI and Multiagentic AI have recently gained popularity in\ndiscussions on generative artificial intelligence, often used to describe\nautonomous software agents and systems composed of such agents. However, the\nuse of these terms confuses these buzzwords with well-established concepts in\nAI literature: intelligent agents and multi-agent systems. This article offers\na critical analysis of this conceptual misuse. We review the theoretical\norigins of \"agentic\" in the social sciences (Bandura, 1986) and philosophical\nnotions of intentionality (Dennett, 1971), and then summarise foundational\nworks on intelligent agents and multi-agent systems by Wooldridge, Jennings and\nothers. We examine classic agent architectures, from simple reactive agents to\nBelief-Desire-Intention (BDI) models, and highlight key properties (autonomy,\nreactivity, proactivity, social capability) that define agency in AI. We then\ndiscuss recent developments in large language models (LLMs) and agent platforms\nbased on LLMs, including the emergence of LLM-powered AI agents and open-source\nmulti-agent orchestration frameworks. We argue that the term AI Agentic is\noften used as a buzzword for what are essentially AI agents, and AI\nMultiagentic for what are multi-agent systems. This confusion overlooks decades\nof research in the field of autonomous agents and multi-agent systems. The\narticle advocates for scientific and technological rigour and the use of\nestablished terminology from the state of the art in AI, incorporating the\nwealth of existing knowledge, including standards for multi-agent system\nplatforms, communication languages and coordination and cooperation algorithms,\nagreement technologies (automated negotiation, argumentation, virtual\norganisations, trust, reputation, etc.), into the new and promising wave of\nLLM-based AI agents, so as not to end up reinventing the wheel.", "published": "2025-06-02 09:19:11", "link": "http://arxiv.org/abs/2506.01463v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "Quantitative Error Feedback for Quantization Noise Reduction of Filtering over Graphs", "abstract": "This paper introduces an innovative error feedback framework designed to\nmitigate quantization noise in distributed graph filtering, where\ncommunications are constrained to quantized messages. It comes from error\nspectrum shaping techniques from state-space digital filters, and therefore\nestablishes connections between quantized filtering processes over different\ndomains. In contrast to existing error compensation methods, our framework\nquantitatively feeds back the quantization noise for exact compensation. We\nexamine the framework under three key scenarios: (i) deterministic graph\nfiltering, (ii) graph filtering over random graphs, and (iii) graph filtering\nwith random node-asynchronous updates. Rigorous theoretical analysis\ndemonstrates that the proposed framework significantly reduces the effect of\nquantization noise, and we provide closed-form solutions for the optimal error\nfeedback coefficients. Moreover, this quantitative error feedback mechanism can\nbe seamlessly integrated into communication-efficient decentralized\noptimization frameworks, enabling lower error floors. Numerical experiments\nvalidate the theoretical results, consistently showing that our method\noutperforms conventional quantization strategies in terms of both accuracy and\nrobustness.", "published": "2025-06-02 07:57:04", "link": "http://arxiv.org/abs/2506.01404v1", "categories": ["cs.LG", "cs.MA", "cs.SY", "eess.SY"], "primary_category": "cs.LG"}
{"title": "An Empirical Study of Group Conformity in Multi-Agent Systems", "abstract": "Recent advances in Large Language Models (LLMs) have enabled multi-agent\nsystems that simulate real-world interactions with near-human reasoning. While\nprevious studies have extensively examined biases related to protected\nattributes such as race, the emergence and propagation of biases on socially\ncontentious issues in multi-agent LLM interactions remain underexplored. This\nstudy explores how LLM agents shape public opinion through debates on five\ncontentious topics. By simulating over 2,500 debates, we analyze how initially\nneutral agents, assigned a centrist disposition, adopt specific stances over\ntime. Statistical analyses reveal significant group conformity mirroring human\nbehavior; LLM agents tend to align with numerically dominant groups or more\nintelligent agents, exerting a greater influence. These findings underscore the\ncrucial role of agent intelligence in shaping discourse and highlight the risks\nof bias amplification in online interactions. Our results emphasize the need\nfor policy measures that promote diversity and transparency in LLM-generated\ndiscussions to mitigate the risks of bias propagation within anonymous online\nenvironments.", "published": "2025-06-02 05:22:29", "link": "http://arxiv.org/abs/2506.01332v1", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Greedy recursion parameter selection for the One-Way Navier-Stokes (OWNS) equations", "abstract": "The One-Way Navier-Stokes (OWNS) equations use recursive filtering to\nconstruct efficient, well-posed one-way approximations to linear hyperbolic\nsystems. The recursion parameters are critical to the accuracy and stability of\nthe method, and have previously been chosen based on heuristic estimates of key\neigenvalues (or their branches), which converges slowly and requires\ntrial-and-error tuning for new systems. We review the projection and recursive\nOWNS formulations (OWNS-P and OWNS-R) for inhomogeneous equations and propose a\ngreedy algorithm for parameter selection. We show that it converges faster and\nleads to a net decrease in computational cost.", "published": "2025-06-02 23:15:41", "link": "http://arxiv.org/abs/2506.02320v1", "categories": ["math.NA", "cs.NA", "physics.comp-ph", "physics.flu-dyn"], "primary_category": "math.NA"}
{"title": "Second-order AAA algorithms for structured data-driven modeling", "abstract": "The data-driven modeling of dynamical systems has become an essential tool\nfor the construction of accurate computational models from real-world data. In\nthis process, the inherent differential structures underlying the considered\nphysical phenomena are often neglected making the reinterpretation of the\nlearned models in a physically meaningful sense very challenging. In this work,\nwe present three data-driven modeling approaches for the construction of\ndynamical systems with second-order differential structure directly from\nfrequency domain data. Based on the second-order structured barycentric form,\nwe extend the well-known Adaptive Antoulas-Anderson algorithm to the case of\nsecond-order systems. Depending on the available computational resources, we\npropose variations of the proposed method that prioritize either higher\ncomputation speed or greater modeling accuracy, and we present a theoretical\nanalysis for the expected accuracy and performance of the proposed methods.\nThree numerical examples demonstrate the effectiveness of our new structured\napproaches in comparison to classical unstructured data-driven modeling.", "published": "2025-06-02 20:34:18", "link": "http://arxiv.org/abs/2506.02241v1", "categories": ["math.NA", "cs.LG", "cs.NA", "cs.SY", "eess.SY", "math.DS", "math.OC", "41A20, 65D15, 93B15, 93C05, 93C80"], "primary_category": "math.NA"}
{"title": "On a spherically lifted spin model at finite temperature", "abstract": "We investigate an \\(n\\)-vector model over \\(k\\) sites with generic pairwise\ninteractions and spherical constraints. The model is a lifting of the Ising\nmodel whereby the support of the spin is lifted to a hypersphere. We show that\nthe \\(n\\)-vector model converges to a limiting distribution at a rate of\n\\(n^{-1/2 + o(1)}\\). We show that the limiting distribution for \\(n \\to\n\\infty\\) is determined by the solution of an equality-constrained maximization\ntask over positive definite matrices. We prove that the obtained maximal value\nand maximizer, respectively, give rise to the free energy and correlation\nfunction of the limiting distribution. In the finite temperature regime, the\nmaximization task is a log-determinant regularization of the semidefinite\nprogram (SDP) in the Goemans-Williamson algorithm. Moreover, the inverse\ntemperature determines the regularization strength, with the zero temperature\nlimit converging to the SDP in Goemans-Williamson. Our derivation draws a\ncurious connection between the semidefinite relaxation of integer programming\nand the spherical lifting of sampling on a hypercube. To the authors' best\nknowledge, this work is the first to solve the setting of fixed \\(k\\) and\ninfinite \\(n\\) under unstructured pairwise interactions.", "published": "2025-06-02 20:02:49", "link": "http://arxiv.org/abs/2506.02220v1", "categories": ["math.PR", "cs.NA", "math.NA"], "primary_category": "math.PR"}
{"title": "Introduction to the theory of generalized locally Toeplitz sequences and its applications", "abstract": "The theory of generalized locally Toeplitz (GLT) sequences was conceived as\nan apparatus for computing the spectral distribution of matrices arising from\nthe numerical discretization of differential equations (DEs). The purpose of\nthis review is to introduce the reader to the theory of GLT sequences and to\npresent some of its applications to the computation of the spectral\ndistribution of DE discretization matrices. We mainly focus on the\napplications, whereas the theory is presented in a self-contained tool-kit\nfashion, without entering into technical details. The exposition is supposed to\nbe understandable to master's degree students in mathematics. It also discloses\nnew more efficient approaches to the spectral analysis of DE discretization\nmatrices as well as a novel spectral analysis tool that has not been considered\nin the GLT literature heretofore, i.e., the modulus of integral continuity.", "published": "2025-06-02 18:30:49", "link": "http://arxiv.org/abs/2506.02151v1", "categories": ["math.NA", "cs.NA", "15B05, 15A18, 47B06, 65N06, 65N30"], "primary_category": "math.NA"}
{"title": "An adaptive data sampling strategy for stabilizing dynamical systems via controller inference", "abstract": "Learning stabilizing controllers from data is an important task in\nengineering applications; however, collecting informative data is challenging\nbecause unstable systems often lead to rapidly growing or erratic trajectories.\nIn this work, we propose an adaptive sampling scheme that generates data while\nsimultaneously stabilizing the system to avoid instabilities during the data\ncollection. Under mild assumptions, the approach provably generates data sets\nthat are informative for stabilization and have minimal size. The numerical\nexperiments demonstrate that controller inference with the novel adaptive\nsampling approach learns controllers with up to one order of magnitude fewer\ndata samples than unguided data generation. The results show that the proposed\napproach opens the door to stabilizing systems in edge cases and limit states\nwhere instabilities often occur and data collection is inherently difficult.", "published": "2025-06-02 15:56:17", "link": "http://arxiv.org/abs/2506.01816v1", "categories": ["math.OC", "cs.LG", "cs.NA", "math.DS", "math.NA", "37N35, 65F55, 90C22, 90C59, 93B52"], "primary_category": "math.OC"}
{"title": "Quantum Circuit Encodings of Polynomial Chaos Expansions", "abstract": "This work investigates the expressive power of quantum circuits in\napproximating high-dimensional, real-valued functions. We focus on\ncountably-parametric holomorphic maps $u:U\\to \\mathbb{R}$, where the parameter\ndomain is $U=[-1,1]^{\\mathbb{N}}$. We establish dimension-independent quantum\ncircuit approximation rates via the best $n$-term truncations of generalized\npolynomial chaos (gPC) expansions of these parametric maps, demonstrating that\nthese rates depend solely on the summability exponent of the gPC expansion\ncoefficients. The key to our findings is based on the fact that so-called\n``$(\\boldsymbol{b},\\epsilon)$-holomorphic'' functions, where $\\boldsymbol{b}\\in\n(0,1]^\\mathbb N \\cap \\ell^p(\\mathbb N)$ for some $p\\in(0,1)$, permit structured\nand sparse gPC expansions. Then, $n$-term truncated gPC expansions are known to\nadmit approximation rates of order $n^{-1/p + 1/2}$ in the $L^2$ norm and of\norder $n^{-1/p + 1}$ in the $L^\\infty$ norm. We show the existence of\nparameterized quantum circuit (PQC) encodings of these $n$-term truncated gPC\nexpansions, and bound PQC depth and width via (i) tensorization of univariate\nPQCs that encode Cheby\\v{s}ev-polynomials in $[-1,1]$ and (ii) linear\ncombination of unitaries (LCU) to build PQC emulations of $n$-term truncated\ngPC expansions. The results provide a rigorous mathematical foundation for the\nuse of quantum algorithms in high-dimensional function approximation. As\ncountably-parametric holomorphic maps naturally arise in parametric PDE models\nand uncertainty quantification (UQ), our results have implications for\nquantum-enhanced algorithms for a wide range of maps in applications.", "published": "2025-06-02 15:53:36", "link": "http://arxiv.org/abs/2506.01811v2", "categories": ["math.NA", "cs.NA", "quant-ph"], "primary_category": "math.NA"}
{"title": "Tight Convergence Rates in Gradient Mapping for the Difference-of-Convex Algorithm", "abstract": "We establish new theoretical convergence guarantees for the\ndifference-of-convex algorithm (DCA), where the second function is allowed to\nbe weakly-convex, measuring progress via composite gradient mapping. Based on a\ntight analysis of two iterations of DCA, we identify six parameter regimes\nleading to sublinear convergence rates toward critical points and establish\nthose rates by proving adapted descent lemmas. We recover existing rates for\nthe standard difference-of-convex decompositions of nonconvex-nonconcave\nfunctions, while for all other curvature settings our results are new,\ncomplementing recently obtained rates on the gradient residual. Three of our\nsublinear rates are tight for any number of DCA iterations, while for the other\nthree regimes we conjecture exact rates, using insights from the tight analysis\nof gradient descent and numerical validation using the performance estimation\nmethodology. Finally, we show how the equivalence between proximal gradient\ndescent (PGD) and DCA allows the derivation of exact PGD rates for any constant\nstepsize.", "published": "2025-06-02 15:32:43", "link": "http://arxiv.org/abs/2506.01791v1", "categories": ["math.OC", "cs.NA", "math.NA"], "primary_category": "math.OC"}
{"title": "Multicontinuum splitting schemes for multiscale wave problems", "abstract": "In this work, we propose multicontinuum splitting schemes for the wave\nequation with a high-contrast coefficient, extending our previous research on\nmultiscale flow problems. The proposed approach consists of two main parts:\ndecomposing the solution space into distinct components, and designing tailored\ntime discretization schemes to enhance computational efficiency. To achieve the\ndecomposition, we employ a multicontinuum homogenization method to introduce\nphysically meaningful macroscopic variables and to separate fast and slow\ndynamics, effectively isolating contrast effects in high-contrast cases. This\ndecomposition enables the design of schemes where the fast-dynamics\n(contrast-dependent) component is treated implicitly, while the slow-dynamics\n(contrast-independent) component is handled explicitly. The idea of discrete\nenergy conservation is applied to derive the stability conditions, which are\ncontrast-independent with appropriately chosen continua. We further discuss\nstrategies for optimizing the space decomposition. These include a Rayleigh\nquotient problem involving tensors, and an alternative generalized eigenvalue\ndecomposition to reduce computational effort. Finally, various numerical\nexamples are presented to validate the accuracy and stability of our proposed\nmethod.", "published": "2025-06-02 13:40:46", "link": "http://arxiv.org/abs/2506.01670v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "A Budgeted Multi-Level Monte Carlo Method for Full Field Estimates of Multi-PDE Problems", "abstract": "We present a high-performance budgeted multi-level Monte Carlo method for\nestimates on the entire spatial domain of multi-PDE problems with random input\ndata. The method is designed to operate optimally within memory and CPU-time\nconstraints and eliminates the need for a priori knowledge of the problem's\nregularity and the algorithm's potential memory demand. To achieve this, we\nbuild on the budgeted multi-level Monte Carlo framework and enhance it with a\nsparse multi-index update algorithm operating on a dynamically assembled\nparallel data structure to enable estimates of the full field solution. We\ndemonstrate numerically and provide mathematical proof that this update\nalgorithm allows computing the full spatial domain estimates at the same\nCPU-time cost as a single quantity of interest, and that the maximum memory\nusage is similar to the memory demands of the deterministic formulation of the\nproblem despite solving the stochastic formulation in parallel. We apply the\nmethod to a sequence of interlinked PDE problems, ranging from a stochastic\npartial differential equation for sampling random fields that serve as the\ndiffusion coefficient in an elliptic subsurface flow problem, to a hyperbolic\nPDE describing mass transport in the resulting flux field.", "published": "2025-06-02 13:17:23", "link": "http://arxiv.org/abs/2506.01644v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Maximum volume coordinates for Grassmann interpolation: Lagrange, Hermite, and errors", "abstract": "We present a novel approach to Riemannian interpolation on the Grassmann\nmanifold. Instead of relying on the Riemannian normal coordinates, i.e. the\nRiemannian exponential and logarithm maps, we approach the interpolation\nproblem with an alternative set of local coordinates and corresponding\nparameterizations. A special property of these coordinates is that their\ncalculation does not require any matrix decompositions. This is a numerical\nadvantage over Riemann normal coordinates and many other retractions on the\nGrassmann manifold, especially when derivative data are to be treated. To\nestimate the interpolation error, we examine the conditioning of these mappings\nand state explicit bounds. It turns out that the parameterizations are\nwell-conditioned, but the coordinate mappings are generally not. As a remedy,\nwe introduce maximum-volume coordinates that are based on a search for\nsubblocks of column-orthogonal matrices of large absolute determinant. We show\nthat the order of magnitude of the asymptotic interpolation error on $\\Gr(n,p)$\nis the same as in the Euclidean space. Two numerical experiments are conducted.\nThe first is an academic one, where we interpolate a parametric orthogonal\nprojector $QQ^T$, where the $Q$--factor stems from a parametric compact\nQR--decomposition. The second experiment is in the context of parametric model\nreduction of dynamical systems, where we interpolate reduced subspaces that are\nobtained by proper orthogonal decomposition.", "published": "2025-06-02 11:59:51", "link": "http://arxiv.org/abs/2506.01574v1", "categories": ["math.NA", "cs.NA", "15B10 15B57 65F99 53C30 53C80"], "primary_category": "math.NA"}
{"title": "A semi-smooth Newton method for magnetic field problems with hysteresis", "abstract": "Ferromagnetic materials exhibit anisotropy, saturation, and hysteresis. We\nhere study the incorporation of an incremental vector hysteresis model\nrepresenting such complex behavior into nonlinear magnetic field problems both,\nfrom a theoretical and a numerical point of view. We show that the hysteresis\noperators, relating magnetic fields and fluxes at every material point, are\nstrongly monotone and Lipschitz continuous. This allows to ensure\nwell-posedness of the corresponding magnetic field problems and appropriate\nfinite element discretizations thereof. We further show that the hysteresis\noperators are semi-smooth, derive a candidate for their generalized Jacobians,\nand establish global linear and local superlinear convergence of a the\nsemi-smooth Newton method with line search applied to the iterative solution of\nthe discretized nonlinear field problems. The results are proven in detail for\na hysteresis model involving a single pinning force and the scalar potential\nformulation of magnetostatics. The extension to multiple pinning forces and the\nvector potential formulation is possible and briefly outlined. The theoretical\nresults are further illustrated by numerical tests.", "published": "2025-06-02 10:04:59", "link": "http://arxiv.org/abs/2506.01499v1", "categories": ["math.NA", "cs.NA", "65K10, 65N30, 49M15, 65K15 46N10"], "primary_category": "math.NA"}
{"title": "Point Jacobi-type preconditioning and parameter tuning for Calderon-preconditioned Burton-Miller method in transmission problems", "abstract": "It was recently demonstrated that the boundary element method based on the\nBurton-Miller formulation (BM-BEM), widely used for solving exterior problems,\ncan be adapted to solve transmission problems efficiently. This approach\nutilises Calderon's identities to improve the spectral properties of the\nunderlying integral operator. Consequently, most eigenvalues of the squared BEM\ncoefficient matrix, i.e. the collocation-discretised version of the operator,\ncluster at a few points in the complex plane. When these clustering points are\nclosely packed, the resulting linear system is well-conditioned and can be\nsolved efficiently using the generalised minimal residual method with only a\nfew iterations. However, when multiple materials with significantly different\nmaterial constants are involved, some eigenvalues become separated,\ndeteriorating the conditioning. To address this, we propose an enhanced\nCalderon-preconditioned BM-BEM with two strategies. First, we apply a\npreconditioning scheme inspired by the point Jacobi method. Second, we tune the\nBurton-Miller parameters to minimise the condition number of the coefficient\nmatrix. Both strategies leverage a newly derived analytical expression for the\neigenvalue clustering points of the relevant operator. Numerical experiments\ndemonstrate that the proposed method, combining both strategies, is\nparticularly effective for solving scattering problems involving composite\npenetrable materials with high contrast in material properties.", "published": "2025-06-02 08:53:52", "link": "http://arxiv.org/abs/2506.01440v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Exact operator inference with minimal data", "abstract": "This work introduces a novel method to generate snapshot data for operator\ninference that guarantees the exact reconstruction of intrusive\nprojection-based reduced-order models (ROMs). To ensure exact reconstruction,\nthe operator inference least squares matrix must have full rank, without\nregularization. Existing works have achieved this full rank using heuristic\nstrategies to generate snapshot data and a-posteriori checks on full rank, but\nwithout a guarantee of success. Our novel snapshot data generation method\nprovides this guarantee thanks to two key ingredients: first we identify ROM\nstates that induce full rank, then we generate snapshots corresponding to\nexactly these states by simulating multiple trajectories for only a single time\nstep. This way, the number of required snapshots is minimal and orders of\nmagnitude lower than typically reported with existing methods. The method\navoids non-Markovian terms and does not require re-projection. Since the number\nof snapshots is minimal, the least squares problem simplifies to a linear\nsystem that is numerically more stable. In addition, because the inferred\noperators are exact, properties of the intrusive ROM operators such as symmetry\nor skew-symmetry are preserved. Numerical results for differential equations\ninvolving 2nd, 3rd and 8th order polynomials demonstrate that the novel\nsnapshot data generation method leads to exact reconstruction of the intrusive\nreduced order models.", "published": "2025-06-02 01:44:33", "link": "http://arxiv.org/abs/2506.01244v1", "categories": ["math.NA", "cs.NA", "65Y99, 65F22, 35R30, 65D05"], "primary_category": "math.NA"}
{"title": "Learning collective variables that preserve transition rates", "abstract": "Collective variables (CVs) play a crucial role in capturing rare events in\nhigh-dimensional systems, motivating the continual search for principled\napproaches to their design. In this work, we revisit the framework of\nquantitative coarse graining and identify the orthogonality condition from\nLegoll and Lelievre (2010) as a key criterion for constructing CVs that\naccurately preserve the statistical properties of the original process. We\nestablish that satisfaction of the orthogonality condition enables error\nestimates for both relative entropy and pathwise distance to scale\nproportionally with the degree of scale separation. Building on this\nfoundation, we introduce a general numerical method for designing neural\nnetwork-based CVs that integrates tools from manifold learning with\ngroup-invariant featurization. To demonstrate the efficacy of our approach, we\nconstruct CVs for butane and achieve a CV that reproduces the anti-gauche\ntransition rate with less than ten percent relative error. Additionally, we\nprovide empirical evidence challenging the necessity of uniform positive\ndefiniteness in diffusion tensors for transition rate reproduction and\nhighlight the critical role of light atoms in CV design for molecular dynamics.", "published": "2025-06-02 00:18:16", "link": "http://arxiv.org/abs/2506.01222v1", "categories": ["math.NA", "cs.NA", "physics.chem-ph", "stat.ML", "70-08, 60G25, 58-08, 68T07"], "primary_category": "math.NA"}
{"title": "CACTI: Leveraging Copy Masking and Contextual Information to Improve Tabular Data Imputation", "abstract": "We present CACTI, a masked autoencoding approach for imputing tabular data\nthat leverages the structure in missingness patterns and contextual\ninformation. Our approach employs a novel median truncated copy masking\ntraining strategy that encourages the model to learn from empirical patterns of\nmissingness while incorporating semantic relationships between features -\ncaptured by column names and text descriptions - to better represent feature\ndependence. These dual sources of inductive bias enable CACTI to outperform\nstate-of-the-art methods - an average $R^2$ gain of 7.8% over the next best\nmethod (13.4%, 6.1%, and 5.3% under missing not at random, at random and\ncompletely at random, respectively) - across a diverse range of datasets and\nmissingness conditions. Our results highlight the value of leveraging\ndataset-specific contextual information and missingness patterns to enhance\nimputation performance.", "published": "2025-06-02 22:50:22", "link": "http://arxiv.org/abs/2506.02306v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Latent Stochastic Interpolants", "abstract": "Stochastic Interpolants (SI) are a powerful framework for generative\nmodeling, capable of flexibly transforming between two probability\ndistributions. However, their use in jointly optimized latent variable models\nremains unexplored as they require direct access to the samples from the two\ndistributions. This work presents Latent Stochastic Interpolants (LSI) enabling\njoint learning in a latent space with end-to-end optimized encoder, decoder and\nlatent SI models. We achieve this by developing a principled Evidence Lower\nBound (ELBO) objective derived directly in continuous time. The joint\noptimization allows LSI to learn effective latent representations along with a\ngenerative process that transforms an arbitrary prior distribution into the\nencoder-defined aggregated posterior. LSI sidesteps the simple priors of the\nnormal diffusion models and mitigates the computational demands of applying SI\ndirectly in high-dimensional observation spaces, while preserving the\ngenerative flexibility of the SI framework. We demonstrate the efficacy of LSI\nthrough comprehensive experiments on the standard large scale ImageNet\ngeneration benchmark.", "published": "2025-06-02 21:34:50", "link": "http://arxiv.org/abs/2506.02276v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "MoCA: Multi-modal Cross-masked Autoencoder for Digital Health Measurements", "abstract": "The growing prevalence of digital health technologies has led to the\ngeneration of complex multi-modal data, such as physical activity measurements\nsimultaneously collected from various sensors of mobile and wearable devices.\nThese data hold immense potential for advancing health studies, but current\nmethods predominantly rely on supervised learning, requiring extensive labeled\ndatasets that are often expensive or impractical to obtain, especially in\nclinical studies. To address this limitation, we propose a self-supervised\nlearning framework called Multi-modal Cross-masked Autoencoder (MoCA) that\nleverages cross-modality masking and the Transformer autoencoder architecture\nto utilize both temporal correlations within modalities and cross-modal\ncorrelations between data streams. We also provide theoretical guarantees to\nsupport the effectiveness of the cross-modality masking scheme in MoCA.\nComprehensive experiments and ablation studies demonstrate that our method\noutperforms existing approaches in both reconstruction and downstream tasks. We\nrelease open-source code for data processing, pre-training, and downstream\ntasks in the supplementary materials. This work highlights the transformative\npotential of self-supervised learning in digital health and multi-modal data.", "published": "2025-06-02 21:07:25", "link": "http://arxiv.org/abs/2506.02260v1", "categories": ["stat.ML", "cs.LG", "stat.AP"], "primary_category": "stat.ML"}
{"title": "Assumption-free stability for ranking problems", "abstract": "In this work, we consider ranking problems among a finite set of candidates:\nfor instance, selecting the top-$k$ items among a larger list of candidates or\nobtaining the full ranking of all items in the set. These problems are often\nunstable, in the sense that estimating a ranking from noisy data can exhibit\nhigh sensitivity to small perturbations. Concretely, if we use data to provide\na score for each item (say, by aggregating preference data over a sample of\nusers), then for two items with similar scores, small fluctuations in the data\ncan alter the relative ranking of those items. Many existing theoretical\nresults for ranking problems assume a separation condition to avoid this\nchallenge, but real-world data often contains items whose scores are\napproximately tied, limiting the applicability of existing theory. To address\nthis gap, we develop a new algorithmic stability framework for ranking\nproblems, and propose two novel ranking operators for achieving stable ranking:\nthe \\emph{inflated top-$k$} for the top-$k$ selection problem and the\n\\emph{inflated full ranking} for ranking the full list. To enable stability,\neach method allows for expressing some uncertainty in the output. For both of\nthese two problems, our proposed methods provide guaranteed stability, with no\nassumptions on data distributions and no dependence on the total number of\ncandidates to be ranked. Experiments on real-world data confirm that the\nproposed methods offer stability without compromising the informativeness of\nthe output.", "published": "2025-06-02 21:02:13", "link": "http://arxiv.org/abs/2506.02257v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "primary_category": "stat.ML"}
{"title": "Enabling Probabilistic Learning on Manifolds through Double Diffusion Maps", "abstract": "We present a generative learning framework for probabilistic sampling based\non an extension of the Probabilistic Learning on Manifolds (PLoM) approach,\nwhich is designed to generate statistically consistent realizations of a random\nvector in a finite-dimensional Euclidean space, informed by a limited (yet\nrepresentative) set of observations. In its original form, PLoM constructs a\nreduced-order probabilistic model by combining three main components: (a)\nkernel density estimation to approximate the underlying probability measure,\n(b) Diffusion Maps to uncover the intrinsic low-dimensional manifold structure,\nand (c) a reduced-order Ito Stochastic Differential Equation (ISDE) to sample\nfrom the learned distribution. A key challenge arises, however, when the number\nof available data points N is small and the dimensionality of the diffusion-map\nbasis approaches N, resulting in overfitting and loss of generalization. To\novercome this limitation, we propose an enabling extension that implements a\nsynthesis of Double Diffusion Maps -- a technique capable of capturing\nmultiscale geometric features of the data -- with Geometric Harmonics (GH), a\nnonparametric reconstruction method that allows smooth nonlinear interpolation\nin high-dimensional ambient spaces. This approach enables us to solve a\nfull-order ISDE directly in the latent space, preserving the full dynamical\ncomplexity of the system, while leveraging its reduced geometric\nrepresentation. The effectiveness and robustness of the proposed method are\nillustrated through two numerical studies: one based on data generated from\ntwo-dimensional Hermite polynomial functions and another based on high-fidelity\nsimulations of a detonation wave in a reactive flow.", "published": "2025-06-02 20:58:49", "link": "http://arxiv.org/abs/2506.02254v1", "categories": ["stat.ML", "cs.LG", "math.PR"], "primary_category": "stat.ML"}
{"title": "Constrained Sliced Wasserstein Embedding", "abstract": "Sliced Wasserstein (SW) distances offer an efficient method for comparing\nhigh-dimensional probability measures by projecting them onto multiple\n1-dimensional probability distributions. However, identifying informative\nslicing directions has proven challenging, often necessitating a large number\nof slices to achieve desirable performance and thereby increasing computational\ncomplexity. We introduce a constrained learning approach to optimize the\nslicing directions for SW distances. Specifically, we constrain the 1D\ntransport plans to approximate the optimal plan in the original space, ensuring\nmeaningful slicing directions. By leveraging continuous relaxations of these\ntransport plans, we enable a gradient-based primal-dual approach to train the\nslicer parameters, alongside the remaining model parameters. We demonstrate how\nthis constrained slicing approach can be applied to pool high-dimensional\nembeddings into fixed-length permutation-invariant representations. Numerical\nresults on foundation models trained on images, point clouds, and protein\nsequences showcase the efficacy of the proposed constrained learning approach\nin learning more informative slicing directions. Our implementation code can be\nfound at https://github.com/Stranja572/constrainedswe.", "published": "2025-06-02 19:43:40", "link": "http://arxiv.org/abs/2506.02203v1", "categories": ["cs.LG", "cs.AI", "math.OC", "q-bio.QM", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Asymptotically exact variational flows via involutive MCMC kernels", "abstract": "Most expressive variational families -- such as normalizing flows -- lack\npractical convergence guarantees, as their theoretical assurances typically\nhold only at the intractable global optimum. In this work, we present a general\nrecipe for constructing tuning-free, asymptotically exact variational flows\nfrom involutive MCMC kernels. The core methodological component is a novel\nrepresentation of general involutive MCMC kernels as invertible,\nmeasure-preserving iterated random function systems, which act as the flow maps\nof our variational flows. This leads to three new variational families with\nprovable total variation convergence. Our framework resolves key practical\nlimitations of existing variational families with similar guarantees (e.g.,\nMixFlows), while requiring substantially weaker theoretical assumptions.\nFinally, we demonstrate the competitive performance of our flows across tasks\nincluding posterior approximation, Monte Carlo estimates, and normalization\nconstant estimation, outperforming or matching No-U-Turn sampler (NUTS) and\nblack-box normalizing flows.", "published": "2025-06-02 18:44:35", "link": "http://arxiv.org/abs/2506.02162v1", "categories": ["stat.CO", "stat.ML"], "primary_category": "stat.CO"}
{"title": "Generalized Gradient Norm Clipping & Non-Euclidean $(L_0,L_1)$-Smoothness", "abstract": "This work introduces a hybrid non-Euclidean optimization method which\ngeneralizes gradient norm clipping by combining steepest descent and\nconditional gradient approaches. The method achieves the best of both worlds by\nestablishing a descent property under a generalized notion of\n($L_0$,$L_1$)-smoothness. Weight decay is incorporated in a principled manner\nby identifying a connection to the Frank-Wolfe short step. In the stochastic\ncase, we show an order optimal $O(n^{-1/4})$ convergence rate by leveraging a\nmomentum based gradient estimator. We discuss how to instantiate the algorithms\nfor deep learning and demonstrate their properties on image classification and\nlanguage modeling.", "published": "2025-06-02 17:34:29", "link": "http://arxiv.org/abs/2506.01913v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "SMOTE-DP: Improving Privacy-Utility Tradeoff with Synthetic Data", "abstract": "Privacy-preserving data publication, including synthetic data sharing, often\nexperiences trade-offs between privacy and utility. Synthetic data is generally\nmore effective than data anonymization in balancing this trade-off, however,\nnot without its own challenges. Synthetic data produced by generative models\ntrained on source data may inadvertently reveal information about outliers.\nTechniques specifically designed for preserving privacy, such as introducing\nnoise to satisfy differential privacy, often incur unpredictable and\nsignificant losses in utility. In this work we show that, with the right\nmechanism of synthetic data generation, we can achieve strong privacy\nprotection without significant utility loss. Synthetic data generators\nproducing contracting data patterns, such as Synthetic Minority Over-sampling\nTechnique (SMOTE), can enhance a differentially private data generator,\nleveraging the strengths of both. We prove in theory and through empirical\ndemonstration that this SMOTE-DP technique can produce synthetic data that not\nonly ensures robust privacy protection but maintains utility in downstream\nlearning tasks.", "published": "2025-06-02 17:27:10", "link": "http://arxiv.org/abs/2506.01907v1", "categories": ["cs.LG", "cs.CR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Machine-Learned Sampling of Conditioned Path Measures", "abstract": "We propose algorithms for sampling from posterior path measures $P(C([0, T],\n\\mathbb{R}^d))$ under a general prior process. This leverages ideas from (1)\ncontrolled equilibrium dynamics, which gradually transport between two path\nmeasures, and (2) optimization in $\\infty$-dimensional probability space\nendowed with a Wasserstein metric, which can be used to evolve a density curve\nunder the specified likelihood. The resulting algorithms are theoretically\ngrounded and can be integrated seamlessly with neural networks for learning the\ntarget trajectory ensembles, without access to data.", "published": "2025-06-02 17:25:03", "link": "http://arxiv.org/abs/2506.01904v1", "categories": ["stat.ML", "cs.LG", "stat.CO"], "primary_category": "stat.ML"}
{"title": "Variational Inference for Latent Variable Models in High Dimensions", "abstract": "Variational inference (VI) is a popular method for approximating intractable\nposterior distributions in Bayesian inference and probabilistic machine\nlearning. In this paper, we introduce a general framework for quantifying the\nstatistical accuracy of mean-field variational inference (MFVI) for posterior\napproximation in Bayesian latent variable models with categorical local latent\nvariables. Utilizing our general framework, we capture the exact asymptotic\nregime where MFVI `works' for the celebrated latent Dirichlet allocation (LDA)\nmodel. Focusing on the mixed membership stochastic blockmodel (MMSB), we show\nthat the vanilla fully factorized MFVI, often used in the literature, is\nsuboptimal. We propose a partially grouped VI algorithm for this model and show\nthat it works, and derive its exact asymptotic performance. We further\nillustrate that our bounds are tight for both the above models.", "published": "2025-06-02 17:19:58", "link": "http://arxiv.org/abs/2506.01893v1", "categories": ["math.ST", "math.PR", "stat.ML", "stat.TH", "62F15, 62C10, 60F10"], "primary_category": "math.ST"}
{"title": "Agnostic Reinforcement Learning: Foundations and Algorithms", "abstract": "Reinforcement Learning (RL) has demonstrated tremendous empirical success\nacross numerous challenging domains. However, we lack a strong theoretical\nunderstanding of the statistical complexity of RL in environments with large\nstate spaces, where function approximation is required for sample-efficient\nlearning. This thesis addresses this gap by rigorously examining the\nstatistical complexity of RL with function approximation from a learning\ntheoretic perspective. Departing from a long history of prior work, we consider\nthe weakest form of function approximation, called agnostic policy learning, in\nwhich the learner seeks to find the best policy in a given class $\\Pi$, with no\nguarantee that $\\Pi$ contains an optimal policy for the underlying task.\n  We systematically explore agnostic policy learning along three key axes:\nenvironment access -- how a learner collects data from the environment;\ncoverage conditions -- intrinsic properties of the underlying MDP measuring the\nexpansiveness of state-occupancy measures for policies in the class $\\Pi$, and\nrepresentational conditions -- structural assumptions on the class $\\Pi$\nitself. Within this comprehensive framework, we (1) design new learning\nalgorithms with theoretical guarantees and (2) characterize fundamental\nperformance bounds of any algorithm. Our results reveal significant statistical\nseparations that highlight the power and limitations of agnostic policy\nlearning.", "published": "2025-06-02 17:12:24", "link": "http://arxiv.org/abs/2506.01884v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Towards Better Generalization and Interpretability in Unsupervised Concept-Based Models", "abstract": "To increase the trustworthiness of deep neural networks, it is critical to\nimprove the understanding of how they make decisions. This paper introduces a\nnovel unsupervised concept-based model for image classification, named\nLearnable Concept-Based Model (LCBM) which models concepts as random variables\nwithin a Bernoulli latent space. Unlike traditional methods that either require\nextensive human supervision or suffer from limited scalability, our approach\nemploys a reduced number of concepts without sacrificing performance. We\ndemonstrate that LCBM surpasses existing unsupervised concept-based models in\ngeneralization capability and nearly matches the performance of black-box\nmodels. The proposed concept representation enhances information retention and\naligns more closely with human understanding. A user study demonstrates the\ndiscovered concepts are also more intuitive for humans to interpret. Finally,\ndespite the use of concept embeddings, we maintain model interpretability by\nmeans of a local linear combination of concepts.", "published": "2025-06-02 16:26:41", "link": "http://arxiv.org/abs/2506.02092v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "When Lower-Order Terms Dominate: Adaptive Expert Algorithms for Heavy-Tailed Losses", "abstract": "We consider the problem setting of prediction with expert advice with\npossibly heavy-tailed losses, i.e.\\ the only assumption on the losses is an\nupper bound on their second moments, denoted by $\\theta$. We develop adaptive\nalgorithms that do not require any prior knowledge about the range or the\nsecond moment of the losses. Existing adaptive algorithms have what is\ntypically considered a lower-order term in their regret guarantees. We show\nthat this lower-order term, which is often the maximum of the losses, can\nactually dominate the regret bound in our setting. Specifically, we show that\neven with small constant $\\theta$, this lower-order term can scale as\n$\\sqrt{KT}$, where $K$ is the number of experts and $T$ is the time horizon. We\npropose adaptive algorithms with improved regret bounds that avoid the\ndependence on such a lower-order term and guarantee $\\mathcal{O}(\\sqrt{\\theta\nT\\log(K)})$ regret in the worst case, and $\\mathcal{O}(\\theta\n\\log(KT)/\\Delta_{\\min})$ regret when the losses are sampled i.i.d.\\ from some\nfixed distribution, where $\\Delta_{\\min}$ is the difference between the mean\nlosses of the second best expert and the best expert. Additionally, when the\nloss function is the squared loss, our algorithm also guarantees improved\nregret bounds over prior results.", "published": "2025-06-02 14:29:05", "link": "http://arxiv.org/abs/2506.01722v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Signature Maximum Mean Discrepancy Two-Sample Statistical Tests", "abstract": "Maximum Mean Discrepancy (MMD) is a widely used concept in machine learning\nresearch which has gained popularity in recent years as a highly effective tool\nfor comparing (finite-dimensional) distributions. Since it is designed as a\nkernel-based method, the MMD can be extended to path space valued distributions\nusing the signature kernel. The resulting signature MMD (sig-MMD) can be used\nto define a metric between distributions on path space. Similarly to the\noriginal use case of the MMD as a test statistic within a two-sample testing\nframework, the sig-MMD can be applied to determine if two sets of paths are\ndrawn from the same stochastic process. This work is dedicated to understanding\nthe possibilities and challenges associated with applying the sig-MMD as a\nstatistical tool in practice. We introduce and explain the sig-MMD, and provide\neasily accessible and verifiable examples for its practical use. We present\nexamples that can lead to Type 2 errors in the hypothesis test, falsely\nindicating that samples have been drawn from the same underlying process (which\ngenerally occurs in a limited data setting). We then present techniques to\nmitigate the occurrence of this type of error.", "published": "2025-06-02 14:26:58", "link": "http://arxiv.org/abs/2506.01718v1", "categories": ["stat.ML", "cs.LG", "math.DS"], "primary_category": "stat.ML"}
{"title": "Missing Data in Signal Processing and Machine Learning: Models, Methods and Modern Approaches", "abstract": "This tutorial aims to provide signal processing (SP) and machine learning\n(ML) practitioners with vital tools, in an accessible way, to answer the\nquestion: How to deal with missing data? There are many strategies to handle\nincomplete signals. In this paper, we propose to group these strategies based\non three common tasks: i) missing-data imputation, ii) estimation with missing\nvalues and iii) prediction with missing values. We focus on methodological and\nexperimental results through specific case studies on real-world applications.\nPromising and future research directions, including a better integration of\ninformative missingness, are also discussed. We hope that the proposed\nconceptual framework and the presentation of recent missing-data problems\nrelated will encourage researchers of the SP and ML communities to develop\noriginal methods and to efficiently deal with new applications involving\nmissing data.", "published": "2025-06-02 13:58:36", "link": "http://arxiv.org/abs/2506.01696v2", "categories": ["eess.SP", "stat.ML"], "primary_category": "eess.SP"}
{"title": "Mixture of Experts Provably Detect and Learn the Latent Cluster Structure in Gradient-Based Learning", "abstract": "Mixture of Experts (MoE), an ensemble of specialized models equipped with a\nrouter that dynamically distributes each input to appropriate experts, has\nachieved successful results in the field of machine learning. However,\ntheoretical understanding of this architecture is falling behind due to its\ninherent complexity. In this paper, we theoretically study the sample and\nruntime complexity of MoE following the stochastic gradient descent (SGD) when\nlearning a regression task with an underlying cluster structure of single index\nmodels. On the one hand, we prove that a vanilla neural network fails in\ndetecting such a latent organization as it can only process the problem as a\nwhole. This is intrinsically related to the concept of information exponent\nwhich is low for each cluster, but increases when we consider the entire task.\nOn the other hand, we show that a MoE succeeds in dividing this problem into\neasier subproblems by leveraging the ability of each expert to weakly recover\nthe simpler function corresponding to an individual cluster. To the best of our\nknowledge, this work is among the first to explore the benefits of the MoE\nframework by examining its SGD dynamics in the context of nonlinear regression.", "published": "2025-06-02 13:26:44", "link": "http://arxiv.org/abs/2506.01656v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "General agents need world models", "abstract": "Are world models a necessary ingredient for flexible, goal-directed\nbehaviour, or is model-free learning sufficient? We provide a formal answer to\nthis question, showing that any agent capable of generalizing to multi-step\ngoal-directed tasks must have learned a predictive model of its environment. We\nshow that this model can be extracted from the agent's policy, and that\nincreasing the agents performance or the complexity of the goals it can achieve\nrequires learning increasingly accurate world models. This has a number of\nconsequences: from developing safe and general agents, to bounding agent\ncapabilities in complex environments, and providing new algorithms for\neliciting world models from agents.", "published": "2025-06-02 13:01:13", "link": "http://arxiv.org/abs/2506.01622v1", "categories": ["cs.AI", "cs.LG", "cs.RO", "stat.ML"], "primary_category": "cs.AI"}
{"title": "MMD-Sense-Analysis: Word Sense Detection Leveraging Maximum Mean Discrepancy", "abstract": "Word sense analysis is an essential analysis work for interpreting the\nlinguistic and social backgrounds. The word sense change detection is a task of\nidentifying and interpreting shifts in word meanings over time. This paper\nproposes MMD-Sense-Analysis, a novel approach that leverages Maximum Mean\nDiscrepancy (MMD) to select semantically meaningful variables and quantify\nchanges across time periods. This method enables both the identification of\nwords undergoing sense shifts and the explanation of their evolution over\nmultiple historical periods. To my knowledge, this is the first application of\nMMD to word sense change detection. Empirical assessment results demonstrate\nthe effectiveness of the proposed approach.", "published": "2025-06-02 12:40:46", "link": "http://arxiv.org/abs/2506.01602v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Unpacking Softmax: How Temperature Drives Representation Collapse, Compression, and Generalization", "abstract": "The softmax function is a fundamental building block of deep neural networks,\ncommonly used to define output distributions in classification tasks or\nattention weights in transformer architectures. Despite its widespread use and\nproven effectiveness, its influence on learning dynamics and learned\nrepresentations remains poorly understood, limiting our ability to optimize\nmodel behavior. In this paper, we study the pivotal role of the softmax\nfunction in shaping the model's representation. We introduce the concept of\nrank deficit bias - a phenomenon in which softmax-based deep networks find\nsolutions of rank much lower than the number of classes. This bias depends on\nthe softmax function's logits norm, which is implicitly influenced by\nhyperparameters or directly modified by softmax temperature. Furthermore, we\ndemonstrate how to exploit the softmax dynamics to learn compressed\nrepresentations or to enhance their performance on out-of-distribution data. We\nvalidate our findings across diverse architectures and real-world datasets,\nhighlighting the broad applicability of temperature tuning in improving model\nperformance. Our work provides new insights into the mechanisms of softmax,\nenabling better control over representation learning in deep neural networks.", "published": "2025-06-02 11:38:10", "link": "http://arxiv.org/abs/2506.01562v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "To Each Metric Its Decoding: Post-Hoc Optimal Decision Rules of Probabilistic Hierarchical Classifiers", "abstract": "Hierarchical classification offers an approach to incorporate the concept of\nmistake severity by leveraging a structured, labeled hierarchy. However,\ndecoding in such settings frequently relies on heuristic decision rules, which\nmay not align with task-specific evaluation metrics. In this work, we propose a\nframework for the optimal decoding of an output probability distribution with\nrespect to a target metric. We derive optimal decision rules for increasingly\ncomplex prediction settings, providing universal algorithms when candidates are\nlimited to the set of nodes. In the most general case of predicting a subset of\nnodes, we focus on rules dedicated to the hierarchical $hF_{\\beta}$ scores,\ntailored to hierarchical settings. To demonstrate the practical utility of our\napproach, we conduct extensive empirical evaluations, showcasing the\nsuperiority of our proposed optimal strategies, particularly in underdetermined\nscenarios. These results highlight the potential of our methods to enhance the\nperformance and reliability of hierarchical classifiers in real-world\napplications. The code is available at\nhttps://github.com/RomanPlaud/hierarchical_decision_rules", "published": "2025-06-02 11:29:40", "link": "http://arxiv.org/abs/2506.01552v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Adaptive Destruction Processes for Diffusion Samplers", "abstract": "This paper explores the challenges and benefits of a trainable destruction\nprocess in diffusion samplers -- diffusion-based generative models trained to\nsample an unnormalised density without access to data samples. Contrary to the\nmajority of work that views diffusion samplers as approximations to an\nunderlying continuous-time model, we view diffusion models as discrete-time\npolicies trained to produce samples in very few generation steps. We propose to\ntrade some of the elegance of the underlying theory for flexibility in the\ndefinition of the generative and destruction policies. In particular, we\ndecouple the generation and destruction variances, enabling both transition\nkernels to be learned as unconstrained Gaussian densities. We show that, when\nthe number of steps is limited, training both generation and destruction\nprocesses results in faster convergence and improved sampling quality on\nvarious benchmarks. Through a robust ablation study, we investigate the design\nchoices necessary to facilitate stable training. Finally, we show the\nscalability of our approach through experiments on GAN latent space sampling\nfor conditional image generation.", "published": "2025-06-02 11:07:27", "link": "http://arxiv.org/abs/2506.01541v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Temporal Causal-based Simulation for Realistic Time-series Generation", "abstract": "Causal Discovery plays a pivotal role in revealing relationships among\nobserved variables, particularly in the temporal setup. While the majority of\nCD methods rely on synthetic data for evaluation, and recently for training,\nthese fall short in accurately mirroring real-world scenarios; an effect even\nmore evident in temporal data. Generation techniques depending on simplified\nassumptions on causal structure, effects and time, limit the quality and\ndiversity of the simulated data. In this work, we introduce Temporal\nCausal-based Simulation (TCS), a robust framework for generating realistic\ntime-series data and their associated temporal causal graphs. The approach is\nstructured in three phases: estimating the true lagged causal structure of the\ndata, approximating the functional dependencies between variables and learning\nthe noise distribution of the corresponding causal model, each part of which\ncan be explicitly tailored based on data assumptions and characteristics.\nThrough an extensive evaluation process, we highlight that single detection\nmethods for generated data discrimination prove inadequate, accentuating it as\na multifaceted challenge. For this, we detail a Min-max optimization phase that\ndraws on AutoML techniques. Our contributions include a flexible,\nmodel-agnostic pipeline for generating realistic temporal causal data, a\nthorough evaluation setup which enhances the validity of the generated datasets\nand insights into the challenges posed by realistic data generation. Through\nexperiments involving not only real but also semi-synthetic and purely\nsynthetic datasets, we demonstrate that while sampling realistic causal data\nremains a complex task, our method enriches the domain of generating sensible\ncausal-based temporal data.", "published": "2025-06-02 10:59:48", "link": "http://arxiv.org/abs/2506.02084v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Alignment as Distribution Learning: Your Preference Model is Explicitly a Language Model", "abstract": "Alignment via reinforcement learning from human feedback (RLHF) has become\nthe dominant paradigm for controlling the quality of outputs from large\nlanguage models (LLMs). However, when viewed as `loss + regularization,' the\nstandard RLHF objective lacks theoretical justification and incentivizes\ndegenerate, deterministic solutions, an issue that variants such as Direct\nPolicy Optimization (DPO) also inherit. In this paper, we rethink alignment by\nframing it as \\emph{distribution learning} from pairwise preference feedback by\nexplicitly modeling how information about the target language model bleeds\nthrough the preference data. This explicit modeling leads us to propose three\nprincipled learning objectives: preference maximum likelihood estimation,\npreference distillation, and reverse KL minimization. We theoretically show\nthat all three approaches enjoy strong non-asymptotic $O(1/n)$ convergence to\nthe target language model, naturally avoiding degeneracy and reward\noverfitting. Finally, we empirically demonstrate that our distribution learning\nframework, especially preference distillation, consistently outperforms or\nmatches the performances of RLHF and DPO across various tasks and models.", "published": "2025-06-02 10:36:31", "link": "http://arxiv.org/abs/2506.01523v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Beyond Diagonal Covariance: Flexible Posterior VAEs via Free-Form Injective Flows", "abstract": "Variational Autoencoders (VAEs) are powerful generative models widely used\nfor learning interpretable latent spaces, quantifying uncertainty, and\ncompressing data for downstream generative tasks. VAEs typically rely on\ndiagonal Gaussian posteriors due to computational constraints. Using arguments\ngrounded in differential geometry, we demonstrate inherent limitations in the\nrepresentational capacity of diagonal covariance VAEs, as illustrated by\nexplicit low-dimensional examples. In response, we show that a regularized\nvariant of the recently introduced Free-form Injective Flow (FIF) can be\ninterpreted as a VAE featuring a highly flexible, implicitly defined posterior.\nCrucially, this regularization yields a posterior equivalent to a full Gaussian\ncovariance distribution, yet maintains computational costs comparable to\nstandard diagonal covariance VAEs. Experiments on image datasets validate our\napproach, demonstrating that incorporating full covariance substantially\nimproves model likelihood.", "published": "2025-06-02 10:36:27", "link": "http://arxiv.org/abs/2506.01522v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Learning of Population Dynamics: Inverse Optimization Meets JKO Scheme", "abstract": "Learning population dynamics involves recovering the underlying process that\ngoverns particle evolution, given evolutionary snapshots of samples at discrete\ntime points. Recent methods frame this as an energy minimization problem in\nprobability space and leverage the celebrated JKO scheme for efficient time\ndiscretization. In this work, we introduce $\\texttt{iJKOnet}$, an approach that\ncombines the JKO framework with inverse optimization techniques to learn\npopulation dynamics. Our method relies on a conventional $\\textit{end-to-end}$\nadversarial training procedure and does not require restrictive architectural\nchoices, e.g., input-convex neural networks. We establish theoretical\nguarantees for our methodology and demonstrate improved performance over prior\nJKO-based methods.", "published": "2025-06-02 10:08:03", "link": "http://arxiv.org/abs/2506.01502v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Robust Federated Learning against Noisy Clients via Masked Optimization", "abstract": "In recent years, federated learning (FL) has made significant advance in\nprivacy-sensitive applications. However, it can be hard to ensure that FL\nparticipants provide well-annotated data for training. The corresponding\nannotations from different clients often contain complex label noise at varying\nlevels. This label noise issue has a substantial impact on the performance of\nthe trained models, and clients with greater noise levels can be largely\nattributed for this degradation. To this end, it is necessary to develop an\neffective optimization strategy to alleviate the adverse effects of these noisy\nclients.In this study, we present a two-stage optimization framework,\nMaskedOptim, to address this intricate label noise problem. The first stage is\ndesigned to facilitate the detection of noisy clients with higher label noise\nrates. The second stage focuses on rectifying the labels of the noisy clients'\ndata through an end-to-end label correction mechanism, aiming to mitigate the\nnegative impacts caused by misinformation within datasets. This is achieved by\nlearning the potential ground-truth labels of the noisy clients' datasets via\nbackpropagation. To further enhance the training robustness, we apply the\ngeometric median based model aggregation instead of the commonly-used vanilla\naveraged model aggregation. We implement sixteen related methods and conduct\nevaluations on three image datasets and one text dataset with diverse label\nnoise patterns for a comprehensive comparison. Extensive experimental results\nindicate that our proposed framework shows its robustness in different\nscenarios. Additionally, our label correction framework effectively enhances\nthe data quality of the detected noisy clients' local datasets. % Our codes\nwill be open-sourced to facilitate related research communities. Our codes are\navailable via https://github.com/Sprinter1999/MaskedOptim .", "published": "2025-06-02 09:35:42", "link": "http://arxiv.org/abs/2506.02079v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A meaningful prediction of functional decline in amyotrophic lateral sclerosis based on multi-event survival analysis", "abstract": "Amyotrophic lateral sclerosis (ALS) is a degenerative disorder of motor\nneurons that causes progressive paralysis in patients. Current treatment\noptions aim to prolong survival and improve quality of life; however, due to\nthe heterogeneity of the disease, it is often difficult to determine the\noptimal time for potential therapies or medical interventions. In this study,\nwe propose a novel method to predict the time until a patient with ALS\nexperiences significant functional impairment (ALSFRS-R<=2) with respect to\nfive common functions: speaking, swallowing, handwriting, walking and\nbreathing. We formulate this task as a multi-event survival problem and\nvalidate our approach in the PRO-ACT dataset by training five covariate-based\nsurvival models to estimate the probability of an event over a 500-day period\nafter a baseline visit. We then predict five event-specific individual survival\ndistributions (ISDs) for each patient, each providing an interpretable and\nmeaningful estimate of when that event will likely take place in the future.\nThe results show that covariate-based models are superior to the Kaplan-Meier\nestimator at predicting time-to-event outcomes. Additionally, our method\nenables practitioners to make individual counterfactual predictions, where\ncertain features (covariates) can be changed to see their effect on the\npredicted outcome. In this regard, we find that Riluzole has little to no\nimpact on predicted functional decline. However, for patients with bulbar-onset\nALS, our method predicts considerably shorter counterfactual time-to-event\nestimates for tasks related to speech and swallowing compared to limb-onset\nALS. The proposed method can be applied to current clinical examination data to\nassess the risk of functional decline and thus allow more personalized\ntreatment planning.", "published": "2025-06-02 09:04:59", "link": "http://arxiv.org/abs/2506.02076v1", "categories": ["q-bio.QM", "cs.LG", "stat.ML"], "primary_category": "q-bio.QM"}
{"title": "Mitigating Disparate Impact of Differentially Private Learning through Bounded Adaptive Clipping", "abstract": "Differential privacy (DP) has become an essential framework for\nprivacy-preserving machine learning. Existing DP learning methods, however,\noften have disparate impacts on model predictions, e.g., for minority groups.\nGradient clipping, which is often used in DP learning, can suppress larger\ngradients from challenging samples. We show that this problem is amplified by\nadaptive clipping, which will often shrink the clipping bound to tiny values to\nmatch a well-fitting majority, while significantly reducing the accuracy for\nothers. We propose bounded adaptive clipping, which introduces a tunable lower\nbound to prevent excessive gradient suppression. Our method improves the\naccuracy of the worst-performing class on average over 10 percentage points on\nskewed MNIST and Fashion MNIST compared to the unbounded adaptive clipping, and\nover 5 percentage points over constant clipping.", "published": "2025-06-02 07:44:17", "link": "http://arxiv.org/abs/2506.01396v1", "categories": ["cs.LG", "cs.CR", "stat.ML", "I.2.6; K.4.2"], "primary_category": "cs.LG"}
{"title": "Improved Regret Bounds for Gaussian Process Upper Confidence Bound in Bayesian Optimization", "abstract": "This paper addresses the Bayesian optimization problem (also referred to as\nthe Bayesian setting of the Gaussian process bandit), where the learner seeks\nto minimize the regret under a function drawn from a known Gaussian process\n(GP). Under a Mat\\'ern kernel with a certain degree of smoothness, we show that\nthe Gaussian process upper confidence bound (GP-UCB) algorithm achieves\n$\\tilde{O}(\\sqrt{T})$ cumulative regret with high probability. Furthermore, our\nanalysis yields $O(\\sqrt{T \\ln^4 T})$ regret under a squared exponential\nkernel. These results fill the gap between the existing regret upper bound for\nGP-UCB and the best-known bound provided by Scarlett (2018). The key idea in\nour proof is to capture the concentration behavior of the input sequence\nrealized by GP-UCB, enabling a more refined analysis of the GP's information\ngain.", "published": "2025-06-02 07:38:58", "link": "http://arxiv.org/abs/2506.01393v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "MMD-Flagger: Leveraging Maximum Mean Discrepancy to Detect Hallucinations", "abstract": "Large language models (LLMs) have become pervasive in our everyday life. Yet,\na fundamental obstacle prevents their use in many critical applications: their\npropensity to generate fluent, human-quality content that is not grounded in\nreality. The detection of such hallucinations is thus of the highest\nimportance. In this work, we propose a new method to flag hallucinated content,\nMMD-Flagger. It relies on Maximum Mean Discrepancy (MMD), a non-parametric\ndistance between distributions. On a high-level perspective, MMD-Flagger tracks\nthe MMD between the generated documents and documents generated with various\ntemperature parameters. We show empirically that inspecting the shape of this\ntrajectory is sufficient to detect most hallucinations. This novel method is\nbenchmarked on two machine translation datasets, on which it outperforms\nnatural competitors.", "published": "2025-06-02 06:50:58", "link": "http://arxiv.org/abs/2506.01367v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Distributionally Robust Learning in Survival Analysis", "abstract": "We introduce an innovative approach that incorporates a Distributionally\nRobust Learning (DRL) approach into Cox regression to enhance the robustness\nand accuracy of survival predictions. By formulating a DRL framework with a\nWasserstein distance-based ambiguity set, we develop a variant Cox model that\nis less sensitive to assumptions about the underlying data distribution and\nmore resilient to model misspecification and data perturbations. By leveraging\nWasserstein duality, we reformulate the original min-max DRL problem into a\ntractable regularized empirical risk minimization problem, which can be\ncomputed by exponential conic programming. We provide guarantees on the finite\nsample behavior of our DRL-Cox model. Moreover, through extensive simulations\nand real world case studies, we demonstrate that our regression model achieves\nsuperior performance in terms of prediction accuracy and robustness compared\nwith traditional methods.", "published": "2025-06-02 06:11:22", "link": "http://arxiv.org/abs/2506.01348v1", "categories": ["cs.LG", "math.OC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Adversarial learning for nonparametric regression: Minimax rate and adaptive estimation", "abstract": "Despite tremendous advancements of machine learning models and algorithms in\nvarious application domains, they are known to be vulnerable to subtle, natural\nor intentionally crafted perturbations in future input data, known as\nadversarial attacks. While numerous adversarial learning methods have been\nproposed, fundamental questions about their statistical optimality in robust\nloss remain largely unanswered. In particular, the minimax rate of convergence\nand the construction of rate-optimal estimators under future $X$-attacks are\nyet to be worked out.\n  In this paper, we address this issue in the context of nonparametric\nregression, under suitable assumptions on the smoothness of the regression\nfunction and the geometric structure of the input perturbation set. We first\nestablish the minimax rate of convergence under adversarial $L_q$-risks with $1\n\\leq q \\leq \\infty$ and propose a piecewise local polynomial estimator that\nachieves the minimax optimality. The established minimax rate elucidates how\nthe smoothness level and perturbation magnitude affect the fundamental limit of\nadversarial learning under future $X$-attacks. Furthermore, we construct a\ndata-driven adaptive estimator that is shown to achieve, within a logarithmic\nfactor, the optimal rate across a broad scale of nonparametric and adversarial\nclasses.", "published": "2025-06-02 02:38:47", "link": "http://arxiv.org/abs/2506.01267v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "primary_category": "stat.ML"}
{"title": "Neural Variance-aware Dueling Bandits with Deep Representation and Shallow Exploration", "abstract": "In this paper, we address the contextual dueling bandit problem by proposing\nvariance-aware algorithms that leverage neural networks to approximate\nnonlinear utility functions. Our approach employs a \\textit{variance-aware\nexploration strategy}, which adaptively accounts for uncertainty in pairwise\ncomparisons while relying only on the gradients with respect to the learnable\nparameters of the last layer. This design effectively balances the\nexploration--exploitation tradeoff under both the Upper Confidence Bound (UCB)\nand Thompson Sampling (TS) frameworks. As a result, under standard assumptions,\nwe establish theoretical guarantees showing that our algorithms achieve\nsublinear cumulative average regret of order $\\bigol\\lt(d \\sqrt{\\sum_{t=1}^T\n\\sigma_t^2} + \\sqrt{dT}\\rt),$ for sufficiently wide neural networks, where $ d\n$ is the contextual dimension, $ \\sigma_t^2 $ the variance of comparisons at\nround $ t $, and $ T $ the total number of rounds. We also empirically validate\nthat our approach offers reasonable computational efficiency and achieves\nsublinear regret on both synthetic tasks with nonlinear utilities and\nreal-world tasks, outperforming existing methods.", "published": "2025-06-02 01:58:48", "link": "http://arxiv.org/abs/2506.01250v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Sounding Like a Winner? Prosodic Differences in Post-Match Interviews", "abstract": "This study examines the prosodic characteristics associated with winning and\nlosing in post-match tennis interviews. Additionally, this research explores\nthe potential to classify match outcomes solely based on post-match interview\nrecordings using prosodic features and self-supervised learning (SSL)\nrepresentations. By analyzing prosodic elements such as pitch and intensity,\nalongside SSL models like Wav2Vec 2.0 and HuBERT, the aim is to determine\nwhether an athlete has won or lost their match. Traditional acoustic features\nand deep speech representations are extracted from the data, and machine\nlearning classifiers are employed to distinguish between winning and losing\nplayers. Results indicate that SSL representations effectively differentiate\nbetween winning and losing outcomes, capturing subtle speech patterns linked to\nemotional states. At the same time, prosodic cues -- such as pitch variability\n-- remain strong indicators of victory.", "published": "2025-06-02 21:45:39", "link": "http://arxiv.org/abs/2506.02283v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Are Mamba-based Audio Foundation Models the Best Fit for Non-Verbal Emotion Recognition?", "abstract": "In this work, we focus on non-verbal vocal sounds emotion recognition (NVER).\nWe investigate mamba-based audio foundation models (MAFMs) for the first time\nfor NVER and hypothesize that MAFMs will outperform attention-based audio\nfoundation models (AAFMs) for NVER by leveraging its state-space modeling to\ncapture intrinsic emotional structures more effectively. Unlike AAFMs, which\nmay amplify irrelevant patterns due to their attention mechanisms, MAFMs will\nextract more stable and context-aware representations, enabling better\ndifferentiation of subtle non-verbal emotional cues. Our experiments with\nstate-of-the-art (SOTA) AAFMs and MAFMs validates our hypothesis. Further,\nmotivated from related research such as speech emotion recognition, synthetic\nspeech detection, where fusion of foundation models (FMs) have showed improved\nperformance, we also explore fusion of FMs for NVER. To this end, we propose,\nRENO, that uses renyi-divergence as a novel loss function for effective\nalignment of the FMs. It also makes use of self-attention for better\nintra-representation interaction of the FMs. With RENO, through the\nheterogeneous fusion of MAFMs and AAFMs, we show the topmost performance in\ncomparison to individual FMs, its fusion and also setting SOTA in comparison to\nprevious SOTA work.", "published": "2025-06-02 21:04:29", "link": "http://arxiv.org/abs/2506.02258v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Investigating the Impact of Word Informativeness on Speech Emotion Recognition", "abstract": "In emotion recognition from speech, a key challenge lies in identifying\nspeech signal segments that carry the most relevant acoustic variations for\ndiscerning specific emotions. Traditional approaches compute functionals for\nfeatures such as energy and F0 over entire sentences or longer speech portions,\npotentially missing essential fine-grained variation in the long-form\nstatistics. This research investigates the use of word informativeness, derived\nfrom a pre-trained language model, to identify semantically important segments.\nAcoustic features are then computed exclusively for these identified segments,\nenhancing emotion recognition accuracy. The methodology utilizes standard\nacoustic prosodic features, their functionals, and self-supervised\nrepresentations. Results indicate a notable improvement in recognition\nperformance when features are computed on segments selected based on word\ninformativeness, underscoring the effectiveness of this approach.", "published": "2025-06-02 20:30:48", "link": "http://arxiv.org/abs/2506.02239v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Investigating the Reasonable Effectiveness of Speaker Pre-Trained Models and their Synergistic Power for SingMOS Prediction", "abstract": "In this study, we focus on Singing Voice Mean Opinion Score (SingMOS)\nprediction. Previous research have shown the performance benefit with the use\nof state-of-the-art (SOTA) pre-trained models (PTMs). However, they haven't\nexplored speaker recognition speech PTMs (SPTMs) such as x-vector, ECAPA and we\nhypothesize that it will be the most effective for SingMOS prediction. We\nbelieve that due to their speaker recognition pre-training, it equips them to\ncapture fine-grained vocal features (e.g., pitch, tone, intensity) from\nsynthesized singing voices in a much more better way than other PTMs. Our\nexperiments with SOTA PTMs including SPTMs and music PTMs validates the\nhypothesis. Additionally, we introduce a novel fusion framework, BATCH that\nuses Bhattacharya Distance for fusion of PTMs. Through BATCH with the fusion of\nspeaker recognition SPTMs, we report the topmost performance comparison to all\nthe individual PTMs and baseline fusion techniques as well as setting SOTA.", "published": "2025-06-02 20:22:59", "link": "http://arxiv.org/abs/2506.02232v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Machine Unlearning for Paralinguistic Speech Processing", "abstract": "In this work, we pioneer the study of Machine Unlearning (MU) for\nParalinguistic Speech Processing (PSP). We focus on two key PSP tasks: Speech\nEmotion Recognition (SER) and Depression Detection (DD). To this end, we\npropose, SISA++, a novel extension to previous state-of-the-art (SOTA) MU\nmethod, SISA by merging models trained on different shards with\nweight-averaging. With such modifications, we show that SISA++ preserves\nperformance more in comparison to SISA after unlearning in benchmark SER\n(CREMA-D) and DD (E-DAIC) datasets. Also, to guide future research for easier\nadoption of MU for PSP, we present ``cookbook recipes'' - actionable\nrecommendations for selecting optimal feature representations and downstream\narchitectures that can mitigate performance degradation after the unlearning\nprocess.", "published": "2025-06-02 20:14:22", "link": "http://arxiv.org/abs/2506.02230v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dhvani: A Weakly-supervised Phonemic Error Detection and Personalized Feedback System for Hindi", "abstract": "Computer-Assisted Pronunciation Training (CAPT) has been extensively studied\nfor English. However, there remains a critical gap in its application to Indian\nlanguages with a base of 1.5 billion speakers. Pronunciation tools tailored to\nIndian languages are strikingly lacking despite the fact that millions learn\nthem every year. With over 600 million speakers and being the fourth\nmost-spoken language worldwide, improving Hindi pronunciation is a vital first\nstep toward addressing this gap. This paper proposes 1) Dhvani -- a novel CAPT\nsystem for Hindi, 2) synthetic speech generation for Hindi mispronunciations,\nand 3) a novel methodology for providing personalized feedback to learners.\nWhile the system often interacts with learners using Devanagari graphemes, its\ncore analysis targets phonemic distinctions, leveraging Hindi's highly phonetic\northography to analyze mispronounced speech and provide targeted feedback.", "published": "2025-06-02 18:45:52", "link": "http://arxiv.org/abs/2506.02166v1", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
{"title": "HENT-SRT: Hierarchical Efficient Neural Transducer with Self-Distillation for Joint Speech Recognition and Translation", "abstract": "Neural transducers (NT) provide an effective framework for speech streaming,\ndemonstrating strong performance in automatic speech recognition (ASR).\nHowever, the application of NT to speech translation (ST) remains challenging,\nas existing approaches struggle with word reordering and performance\ndegradation when jointly modeling ASR and ST, resulting in a gap with\nattention-based encoder-decoder (AED) models. Existing NT-based ST approaches\nalso suffer from high computational training costs. To address these issues, we\npropose HENT-SRT (Hierarchical Efficient Neural Transducer for Speech\nRecognition and Translation), a novel framework that factorizes ASR and\ntranslation tasks to better handle reordering. To ensure robust ST while\npreserving ASR performance, we use self-distillation with CTC consistency\nregularization. Moreover, we improve computational efficiency by incorporating\nbest practices from ASR transducers, including a down-sampled hierarchical\nencoder, a stateless predictor, and a pruned transducer loss to reduce training\ncomplexity. Finally, we introduce a blank penalty during decoding, reducing\ndeletions and improving translation quality. Our approach is evaluated on three\nconversational datasets Arabic, Spanish, and Mandarin achieving new\nstate-of-the-art performance among NT models and substantially narrowing the\ngap with AED-based systems.", "published": "2025-06-02 18:37:50", "link": "http://arxiv.org/abs/2506.02157v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "DNCASR: End-to-End Training for Speaker-Attributed ASR", "abstract": "This paper introduces DNCASR, a novel end-to-end trainable system designed\nfor joint neural speaker clustering and automatic speech recognition (ASR),\nenabling speaker-attributed transcription of long multi-party meetings. DNCASR\nuses two separate encoders to independently encode global speaker\ncharacteristics and local waveform information, along with two linked decoders\nto generate speaker-attributed transcriptions. The use of linked decoders\nallows the entire system to be jointly trained under a unified loss function.\nBy employing a serialised training approach, DNCASR effectively addresses\noverlapping speech in real-world meetings, where the link improves the\nprediction of speaker indices in overlapping segments. Experiments on the\nAMI-MDM meeting corpus demonstrate that the jointly trained DNCASR outperforms\na parallel system that does not have links between the speaker and ASR\ndecoders. Using cpWER to measure the speaker-attributed word error rate, DNCASR\nachieves a 9.0% relative reduction on the AMI-MDM Eval set.", "published": "2025-06-02 17:36:57", "link": "http://arxiv.org/abs/2506.01916v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "On-device Streaming Discrete Speech Units", "abstract": "Discrete speech units (DSUs) are derived from clustering the features of\nself-supervised speech models (S3Ms). DSUs offer significant advantages for\non-device streaming speech applications due to their rich phonetic information,\nhigh transmission efficiency, and seamless integration with large language\nmodels. However, conventional DSU-based approaches are impractical as they\nrequire full-length speech input and computationally expensive S3Ms. In this\nwork, we reduce both the attention window and the model size while preserving\nthe effectiveness of DSUs. Our results demonstrate that we can reduce\nfloating-point operations (FLOPs) by 50% with only a relative increase of 6.5%\nin character error rate (CER) on the ML-SUPERB 1h dataset. These findings\nhighlight the potential of DSUs for real-time speech processing in\nresource-constrained environments.", "published": "2025-06-02 16:30:38", "link": "http://arxiv.org/abs/2506.01845v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and Accountability", "abstract": "High-quality datasets are fundamental to training and evaluating machine\nlearning models, yet their creation-especially with accurate human\nannotations-remains a significant challenge. Many dataset paper submissions\nlack originality, diversity, or rigorous quality control, and these\nshortcomings are often overlooked during peer review. Submissions also\nfrequently omit essential details about dataset construction and properties.\nWhile existing tools such as datasheets aim to promote transparency, they are\nlargely descriptive and do not provide standardized, measurable methods for\nevaluating data quality. Similarly, metadata requirements at conferences\npromote accountability but are inconsistently enforced. To address these\nlimitations, this position paper advocates for the integration of systematic,\nrubric-based evaluation metrics into the dataset review process-particularly as\nsubmission volumes continue to grow. We also explore scalable, cost-effective\nmethods for synthetic data generation, including dedicated tools and\nLLM-as-a-judge approaches, to support more efficient evaluation. As a call to\naction, we introduce DataRubrics, a structured framework for assessing the\nquality of both human- and model-generated datasets. Leveraging recent advances\nin LLM-based evaluation, DataRubrics offers a reproducible, scalable, and\nactionable solution for dataset quality assessment, enabling both authors and\nreviewers to uphold higher standards in data-centric research. We also release\ncode to support reproducibility of LLM-based evaluations at\nhttps://github.com/datarubrics/datarubrics.", "published": "2025-06-02 15:31:52", "link": "http://arxiv.org/abs/2506.01789v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Comparison of spectrogram scaling in multi-label Music Genre Recognition", "abstract": "As the accessibility and ease-of-use of digital audio workstations increases,\nso does the quantity of music available to the average listener; additionally,\ndifferences between genres are not always well defined and can be abstract,\nwith widely varying combinations of genres across individual records. In this\narticle, multiple preprocessing methods and approaches to model training are\ndescribed and compared, accounting for the eclectic nature of today's albums. A\ncustom, manually labeled dataset of more than 18000 entries has been used to\nperform the experiments.", "published": "2025-06-02 15:11:36", "link": "http://arxiv.org/abs/2506.02091v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Benchmarking Neural Speech Codec Intelligibility with SITool", "abstract": "Speech intelligibility assessment is essential for evaluating neural speech\ncodecs, yet most evaluation efforts focus on overall quality rather than\nintelligibility. Only a few publicly available tools exist for conducting\nstandardized intelligibility tests, like the Diagnostic Rhyme Test (DRT) and\nModified Rhyme Test (MRT). We introduce the Speech Intelligibility Toolkit for\nSubjective Evaluation (SITool), a Flask-based web application for conducting\nDRT and MRT in laboratory and crowdsourcing settings. We use SITool to\nbenchmark 13 neural and traditional speech codecs, analyzing phoneme-level\ndegradations and comparing subjective DRT results with objective\nintelligibility metrics. Our findings show that, while neural speech codecs can\noutperform traditional ones in subjective intelligibility, only STOI and ESTOI\n- not WER - significantly correlate with subjective results, although they\nstruggle to capture gender and wordlist-specific variations observed in\nsubjective evaluations.", "published": "2025-06-02 14:42:50", "link": "http://arxiv.org/abs/2506.01731v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Self-Supervised Speech Quality Assessment (S3QA): Leveraging Speech Foundation Models for a Scalable Speech Quality Metric", "abstract": "Methods for automatically assessing speech quality are critical for many\nhuman language technologies. Behavioral ratings provided by human raters (e.g.,\nmean opinion scores; MOS) are considered the gold standard, but they are\nsusceptible to variability between individual raters, cannot easily be\ngeneralized across corpora, and are labor-intensive to collect, thus limiting\nthe acoustic challenges they can quantify. Here, we present a new, scalable\nmethod for automatically assessing speech quality: the self-supervised speech\nquality assessment (S3QA) model. First, we processed high quality utterances\nfrom multiple speech corpora, using a wide range of acoustic manipulations\nintended to emulate common sources of quality degradation in the real-world:\nfrequency filtering, reverberation, background noise, and digital compression.\nSecond, we leveraged an existing, pre-trained speech foundation model, WavLM,\nto computationally derive a self-supervised training target for the level of\nsignal degradation by calculating the cosine distances between the clean and\ndegraded versions of each utterance in the embedding space. Next, we trained a\ntransformer-based model to predict the cosine distance, or degradation index,\ngiven only the degraded versions of these utterances. Finally, the trained\nmodel was evaluated on unseen test corpora of synthetic mixtures, NISQA, and\nVOiCES. We show that the S3QA model trained on this task performs well and is\naligned with both behavioral ratings (MOS), speech technology performance\n(automatic speech recognition) and other important features of the held-out\ndata (e.g., microphone distances). This approach provides an automated,\nscalable method for assessing speech quality across a wide range of acoustic\nchallenges, and could easily be adapted to other use cases where acoustic\nsimulations are available.", "published": "2025-06-02 13:26:07", "link": "http://arxiv.org/abs/2506.01655v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Rhythm and Voice Conversion to Improve ASR on Dysarthric Speech", "abstract": "Automatic speech recognition (ASR) systems struggle with dysarthric speech\ndue to high inter-speaker variability and slow speaking rates. To address this,\nwe explore dysarthric-to-healthy speech conversion for improved ASR\nperformance. Our approach extends the Rhythm and Voice (RnV) conversion\nframework by introducing a syllable-based rhythm modeling method suited for\ndysarthric speech. We assess its impact on ASR by training LF-MMI models and\nfine-tuning Whisper on converted speech. Experiments on the Torgo corpus reveal\nthat LF-MMI achieves significant word error rate reductions, especially for\nmore severe cases of dysarthria, while fine-tuning Whisper on converted data\nhas minimal effect on its performance. These results highlight the potential of\nunsupervised rhythm and voice conversion for dysarthric ASR. Code available at:\nhttps://github.com/idiap/RnV", "published": "2025-06-02 12:57:36", "link": "http://arxiv.org/abs/2506.01618v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Lessons Learned from the URGENT 2024 Speech Enhancement Challenge", "abstract": "The URGENT 2024 Challenge aims to foster speech enhancement (SE) techniques\nwith great universality, robustness, and generalizability, featuring a broader\ntask definition, large-scale multi-domain data, and comprehensive evaluation\nmetrics. Nourished by the challenge outcomes, this paper presents an in-depth\nanalysis of two key, yet understudied, issues in SE system development: data\ncleaning and evaluation metrics. We highlight several overlooked problems in\ntraditional SE pipelines: (1) mismatches between declared and effective audio\nbandwidths, along with label noise even in various \"high-quality\" speech\ncorpora; (2) lack of both effective SE systems to conquer the hardest\nconditions (e.g., speech overlap, strong noise / reverberation) and reliable\nmeasure of speech sample difficulty; (3) importance of combining multifaceted\nmetrics for a comprehensive evaluation correlating well with human judgment. We\nhope that this endeavor can inspire improved SE pipeline designs in the future.", "published": "2025-06-02 12:50:37", "link": "http://arxiv.org/abs/2506.01611v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Unveiling Audio Deepfake Origins: A Deep Metric learning And Conformer Network Approach With Ensemble Fusion", "abstract": "Audio deepfakes are acquiring an unprecedented level of realism with advanced\nAI. While current research focuses on discerning real speech from spoofed\nspeech, tracing the source system is equally crucial. This work proposes a\nnovel audio source tracing system combining deep metric multi-class N-pair loss\nwith Real Emphasis and Fake Dispersion framework, a Conformer classification\nnetwork, and ensemble score-embedding fusion. The N-pair loss improves\ndiscriminative ability, while Real Emphasis and Fake Dispersion enhance\nrobustness by focusing on differentiating real and fake speech patterns. The\nConformer network captures both global and local dependencies in the audio\nsignal, crucial for source tracing. The proposed ensemble score-embedding\nfusion shows an optimal trade-off between in-domain and out-of-domain source\ntracing scenarios. We evaluate our method using Frechet Distance and standard\nmetrics, demonstrating superior performance in source tracing over the baseline\nsystem.", "published": "2025-06-02 12:42:09", "link": "http://arxiv.org/abs/2506.02085v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Silence is Golden: Leveraging Adversarial Examples to Nullify Audio Control in LDM-based Talking-Head Generation", "abstract": "Advances in talking-head animation based on Latent Diffusion Models (LDM)\nenable the creation of highly realistic, synchronized videos. These fabricated\nvideos are indistinguishable from real ones, increasing the risk of potential\nmisuse for scams, political manipulation, and misinformation. Hence, addressing\nthese ethical concerns has become a pressing issue in AI security. Recent\nproactive defense studies focused on countering LDM-based models by adding\nperturbations to portraits. However, these methods are ineffective at\nprotecting reference portraits from advanced image-to-video animation. The\nlimitations are twofold: 1) they fail to prevent images from being manipulated\nby audio signals, and 2) diffusion-based purification techniques can\neffectively eliminate protective perturbations. To address these challenges, we\npropose Silencer, a two-stage method designed to proactively protect the\nprivacy of portraits. First, a nullifying loss is proposed to ignore audio\ncontrol in talking-head generation. Second, we apply anti-purification loss in\nLDM to optimize the inverted latent feature to generate robust perturbations.\nExtensive experiments demonstrate the effectiveness of Silencer in proactively\nprotecting portrait privacy. We hope this work will raise awareness among the\nAI security community regarding critical ethical issues related to talking-head\ngeneration techniques. Code: https://github.com/yuangan/Silencer.", "published": "2025-06-02 12:26:46", "link": "http://arxiv.org/abs/2506.01591v1", "categories": ["cs.GR", "cs.CR", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.GR"}
{"title": "Learning Perceptually Relevant Temporal Envelope Morphing", "abstract": "Temporal envelope morphing, the process of interpolating between the\namplitude dynamics of two audio signals, is an emerging problem in generative\naudio systems that lacks sufficient perceptual grounding. Morphing of temporal\nenvelopes in a perceptually intuitive manner should enable new methods for\nsound blending in creative media and for probing perceptual organization in\npsychoacoustics. However, existing audio morphing techniques often fail to\nproduce intermediate temporal envelopes when input sounds have distinct\ntemporal structures; many morphers effectively overlay both temporal\nstructures, leading to perceptually unnatural results. In this paper, we\nintroduce a novel workflow for learning envelope morphing with perceptual\nguidance: we first derive perceptually grounded morphing principles through\nhuman listening studies, then synthesize large-scale datasets encoding these\nprinciples, and finally train machine learning models to create perceptually\nintermediate morphs. Specifically, we present: (1) perceptual principles that\nguide envelope morphing, derived from our listening studies, (2) a supervised\nframework to learn these principles, (3) an autoencoder that learns to compress\ntemporal envelope structures into latent representations, and (4) benchmarks\nfor evaluating audio envelope morphs, using both synthetic and naturalistic\ndata, and show that our approach outperforms existing methods in producing\ntemporally intermediate morphs. All code, models, and datasets will be made\npublicly available upon publication.", "published": "2025-06-02 12:20:51", "link": "http://arxiv.org/abs/2506.01588v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "LinearVC: Linear transformations of self-supervised features through the lens of voice conversion", "abstract": "We introduce LinearVC, a simple voice conversion method that sheds light on\nthe structure of self-supervised representations. First, we show that simple\nlinear transformations of self-supervised features effectively convert voices.\nNext, we probe the geometry of the feature space by constraining the set of\nallowed transformations. We find that just rotating the features is sufficient\nfor high-quality voice conversion. This suggests that content information is\nembedded in a low-dimensional subspace which can be linearly transformed to\nproduce a target voice. To validate this hypothesis, we finally propose a\nmethod that explicitly factorizes content and speaker information using\nsingular value decomposition; the resulting linear projection with a rank of\njust 100 gives competitive conversion results. Our work has implications for\nboth practical voice conversion and a broader understanding of self-supervised\nspeech representations. Samples and code: https://www.kamperh.com/linearvc/.", "published": "2025-06-02 10:18:02", "link": "http://arxiv.org/abs/2506.01510v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Analyzing the Importance of Blank for CTC-Based Knowledge Distillation", "abstract": "With the rise of large pre-trained foundation models for automatic speech\nrecognition new challenges appear. While the performance of these models is\ngood, runtime and cost of inference increases. One approach to make use of\ntheir strength while retaining efficiency is to distill their knowledge to\nsmaller models during training. In this work, we explore different CTC-based\ndistillation variants, focusing on blank token handling. We show that common\napproaches like blank elimination do not always work off the shelf. We explore\nnew blank selection patterns as a potential sweet spot between standard\nknowledge distillation and blank elimination mechanisms. Through the\nintroduction of a symmetric selection method, we are able to remove the CTC\nloss during knowledge distillation with minimal to no performance degradation.\nWith this, we make the training independent from target labels, potentially\nallowing for distillation on untranscribed audio data.", "published": "2025-06-02 10:08:38", "link": "http://arxiv.org/abs/2506.01503v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Continual Speech Learning with Fused Speech Features", "abstract": "Rapid growth in speech data demands adaptive models, as traditional static\nmethods fail to keep pace with dynamic and diverse speech information. We\nintroduce continuous speech learning, a new set-up targeting at bridging the\nadaptation gap in current speech models. We use the encoder-decoder Whisper\nmodel to standardize speech tasks into a generative format. We integrate a\nlearnable gated-fusion layer on the top of the encoder to dynamically select\ntask-specific features for downstream tasks. Our approach improves accuracy\nsignificantly over traditional methods in six speech processing tasks,\ndemonstrating gains in adapting to new speech tasks without full retraining.", "published": "2025-06-02 09:59:35", "link": "http://arxiv.org/abs/2506.01496v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Enhancing GOP in CTC-Based Mispronunciation Detection with Phonological Knowledge", "abstract": "Computer-Assisted Pronunciation Training (CAPT) systems employ automatic\nmeasures of pronunciation quality, such as the goodness of pronunciation (GOP)\nmetric. GOP relies on forced alignments, which are prone to labeling and\nsegmentation errors due to acoustic variability. While alignment-free methods\naddress these challenges, they are computationally expensive and scale poorly\nwith phoneme sequence length and inventory size. To enhance efficiency, we\nintroduce a substitution-aware alignment-free GOP that restricts phoneme\nsubstitutions based on phoneme clusters and common learner errors. We evaluated\nour GOP on two L2 English speech datasets, one with child speech, My\nPronunciation Coach (MPC), and SpeechOcean762, which includes child and adult\nspeech. We compared RPS (restricted phoneme substitutions) and UPS\n(unrestricted phoneme substitutions) setups within alignment-free methods,\nwhich outperformed the baseline. We discuss our results and outline avenues for\nfuture research.", "published": "2025-06-02 09:45:29", "link": "http://arxiv.org/abs/2506.02080v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Inter-Speaker Relative Cues for Text-Guided Target Speech Extraction", "abstract": "We propose a novel approach that utilize inter-speaker relative cues for\ndistinguishing target speakers and extracting their voices from mixtures.\nContinuous cues (e.g., temporal order, age, pitch level) are grouped by\nrelative differences, while discrete cues (e.g., language, gender, emotion)\nretain their categories. Relative cues offers greater flexibility than fixed\nspeech attribute classification, facilitating much easier expansion of\ntext-guided target speech extraction datasets. Our experiments show that\ncombining all relative cues yields better performance than random subsets, with\ngender and temporal order being the most robust across languages and\nreverberant conditions. Additional cues like pitch level, loudness, distance,\nspeaking duration, language, and pitch range also demonstrate notable benefit\nin complex scenarios. Fine-tuning pre-trained WavLM Base+ CNN encoders improves\noverall performance over the baseline of using only a Conv1d encoder.", "published": "2025-06-02 09:43:43", "link": "http://arxiv.org/abs/2506.01483v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Automatic Stage Lighting Control: Is it a Rule-Driven Process or Generative Task?", "abstract": "Stage lighting plays an essential role in live music performances,\ninfluencing the engaging experience of both musicians and audiences. Given the\nhigh costs associated with hiring or training professional lighting engineers,\nAutomatic Stage Lighting Control (ASLC) has gained increasing attention.\nHowever, most existing approaches only classify music into limited categories\nand map them to predefined light patterns, resulting in formulaic and\nmonotonous outcomes that lack rationality. To address this issue, this paper\npresents an end-to-end solution that directly learns from experienced lighting\nengineers -- Skip-BART. To the best of our knowledge, this is the first work to\nconceptualize ASLC as a generative task rather than merely a classification\nproblem. Our method modifies the BART model to take audio music as input and\nproduce light hue and value (intensity) as output, incorporating a novel skip\nconnection mechanism to enhance the relationship between music and light within\nthe frame grid.We validate our method through both quantitative analysis and an\nhuman evaluation, demonstrating that Skip-BART outperforms conventional\nrule-based methods across all evaluation metrics and shows only a limited gap\ncompared to real lighting engineers.Specifically, our method yields a p-value\nof 0.72 in a statistical comparison based on human evaluations with human\nlighting engineers, suggesting that the proposed approach closely matches human\nlighting engineering performance. To support further research, we have made our\nself-collected dataset, code, and trained model parameters available at\nhttps://github.com/RS2002/Skip-BART .", "published": "2025-06-02 09:42:36", "link": "http://arxiv.org/abs/2506.01482v1", "categories": ["cs.LG", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Evaluating the Effectiveness of Pre-Trained Audio Embeddings for Classification of Parkinson's Disease Speech Data", "abstract": "Speech impairments are prevalent biomarkers for Parkinson's Disease (PD),\nmotivating the development of diagnostic techniques using speech data for\nclinical applications. Although deep acoustic features have shown promise for\nPD classification, their effectiveness often varies due to individual speaker\ndifferences, a factor that has not been thoroughly explored in the existing\nliterature. This study investigates the effectiveness of three pre-trained\naudio embeddings (OpenL3, VGGish and Wav2Vec2.0 models) for PD classification.\nUsing the NeuroVoz dataset, OpenL3 outperforms others in diadochokinesis (DDK)\nand listen and repeat (LR) tasks, capturing critical acoustic features for PD\ndetection. Only Wav2Vec2.0 shows significant gender bias, achieving more\nfavorable results for male speakers, in DDK tasks. The misclassified cases\nreveal challenges with atypical speech patterns, highlighting the need for\nimproved feature extraction and model robustness in PD detection.", "published": "2025-06-02 09:32:54", "link": "http://arxiv.org/abs/2506.02078v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Few-step Adversarial Schr\u00f6dinger Bridge for Generative Speech Enhancement", "abstract": "Deep generative models have recently been employed for speech enhancement to\ngenerate perceptually valid clean speech on large-scale datasets. Several\ndiffusion models have been proposed, and more recently, a tractable\nSchr\\\"odinger Bridge has been introduced to transport between the clean and\nnoisy speech distributions. However, these models often suffer from an\niterative reverse process and require a large number of sampling steps -- more\nthan 50. Our investigation reveals that the performance of baseline models\nsignificantly degrades when the number of sampling steps is reduced,\nparticularly under low-SNR conditions. We propose integrating Schr\\\"odinger\nBridge with GANs to effectively mitigate this issue, achieving high-quality\noutputs on full-band datasets while substantially reducing the required\nsampling steps. Experimental results demonstrate that our proposed model\noutperforms existing baselines, even with a single inference step, in both\ndenoising and dereverberation tasks.", "published": "2025-06-02 09:17:35", "link": "http://arxiv.org/abs/2506.01460v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TalTech Systems for the Interspeech 2025 ML-SUPERB 2.0 Challenge", "abstract": "This paper describes the language identification and multilingual speech\nrecognition system developed at Tallinn University of Technology for the\nInterspeech 2025 ML-SUPERB 2.0 Challenge. A hybrid language identification\nsystem is used, consisting of a pretrained language embedding model and a\nlight-weight speech recognition model with a shared encoder across languages\nand language-specific bigram language models. For speech recognition, three\nmodels are used, where only a single model is applied for each language,\ndepending on the training data availability and performance on held-out data.\nThe model set consists of a finetuned version of SeamlessM4T, MMS-1B-all with\ncustom language adapters and MMS-zeroshot. The system obtained the top overall\nscore in the challenge.", "published": "2025-06-02 09:16:09", "link": "http://arxiv.org/abs/2506.01458v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Universal Preference-Score-based Pairwise Speech Quality Assessment", "abstract": "To compare the performance of two speech generation systems, one of the most\neffective approaches is estimating the preference score between their generated\nspeech. This paper proposes a novel universal preference-score-based pairwise\nspeech quality assessment (UPPSQA) model, aimed at predicting the preference\nscore between paired speech samples to determine which one has better quality.\nThe model first predicts the absolute mean opinion score (MOS) for the two\nspeech samples separately, and then aggregates them into a relative preference\nscore using a preference function. To address the scarcity of preference data,\nwe also construct a new pairwise speech dataset based on a MOS dataset for\nexperiments. Experimental results confirm that, whether in training scenarios\nwith different data types and label conditions, or in both in-domain and\nout-of-domain test scenarios, the prediction accuracy of UPP-SQA outperforms\nthat of the baseline models, demonstrating its universality.", "published": "2025-06-02 09:12:50", "link": "http://arxiv.org/abs/2506.01455v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Whale: Large-Scale multilingual ASR model with w2v-BERT and E-Branchformer with large speech data", "abstract": "This paper reports on the development of a large-scale speech recognition\nmodel, Whale. Similar to models such as Whisper and OWSM, Whale leverages both\na large model size and a diverse, extensive dataset. Whale's architecture\nintegrates w2v-BERT self-supervised model, an encoder-decoder backbone built on\nE-Branchformer, and a joint CTC-attention decoding strategy. The training\ncorpus comprises varied speech data, of not only public corpora but also\nin-house data, thereby enhancing the model's robustness to different speaking\nstyles and acoustic conditions. Through evaluations on multiple benchmarks,\nWhale achieved comparable performance to existing models. In particular, it\nachieves a word error rate of 2.4% on the Librispeech test-clean set and a\ncharacter error rate of 3.4% on the CSJ eval3 set, outperforming Whisper\nlarge-v3 and OWSM v3.1.", "published": "2025-06-02 08:52:50", "link": "http://arxiv.org/abs/2506.01439v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Attention Is Not Always the Answer: Optimizing Voice Activity Detection with Simple Feature Fusion", "abstract": "Voice Activity Detection (VAD) plays a key role in speech processing, often\nutilizing hand-crafted or neural features. This study examines the\neffectiveness of Mel-Frequency Cepstral Coefficients (MFCCs) and pre-trained\nmodel (PTM) features, including wav2vec 2.0, HuBERT, WavLM, UniSpeech, MMS, and\nWhisper. We propose FusionVAD, a unified framework that combines both feature\ntypes using three fusion strategies: concatenation, addition, and\ncross-attention (CA). Experimental results reveal that simple fusion\ntechniques, particularly addition, outperform CA in both accuracy and\nefficiency. Fusion-based models consistently surpass single-feature models,\nhighlighting the complementary nature of MFCCs and PTM features. Notably, our\nbest-performing fusion model exceeds the state-of-the-art Pyannote across\nmultiple datasets, achieving an absolute average improvement of 2.04%. These\nresults confirm that simple feature fusion enhances VAD robustness while\nmaintaining computational efficiency.", "published": "2025-06-02 06:47:42", "link": "http://arxiv.org/abs/2506.01365v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Zero-Shot Text-to-Speech for Vietnamese", "abstract": "This paper introduces PhoAudiobook, a newly curated dataset comprising 941\nhours of high-quality audio for Vietnamese text-to-speech. Using PhoAudiobook,\nwe conduct experiments on three leading zero-shot TTS models: VALL-E,\nVoiceCraft, and XTTS-V2. Our findings demonstrate that PhoAudiobook\nconsistently enhances model performance across various metrics. Moreover,\nVALL-E and VoiceCraft exhibit superior performance in synthesizing short\nsentences, highlighting their robustness in handling diverse linguistic\ncontexts. We publicly release PhoAudiobook to facilitate further research and\ndevelopment in Vietnamese text-to-speech.", "published": "2025-06-02 05:07:06", "link": "http://arxiv.org/abs/2506.01322v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Learning Sparsity for Effective and Efficient Music Performance Question Answering", "abstract": "Music performances, characterized by dense and continuous audio as well as\nseamless audio-visual integration, present unique challenges for multimodal\nscene understanding and reasoning. Recent Music Performance Audio-Visual\nQuestion Answering (Music AVQA) datasets have been proposed to reflect these\nchallenges, highlighting the continued need for more effective integration of\naudio-visual representations in complex question answering. However, existing\nMusic AVQA methods often rely on dense and unoptimized representations, leading\nto inefficiencies in the isolation of key information, the reduction of\nredundancy, and the prioritization of critical samples. To address these\nchallenges, we introduce Sparsify, a sparse learning framework specifically\ndesigned for Music AVQA. It integrates three sparsification strategies into an\nend-to-end pipeline and achieves state-of-the-art performance on the Music AVQA\ndatasets. In addition, it reduces training time by 28.32% compared to its fully\ntrained dense counterpart while maintaining accuracy, demonstrating clear\nefficiency gains. To further improve data efficiency, we propose a key-subset\nselection algorithm that selects and uses approximately 25% of MUSIC-AVQA v2.0\ntraining data and retains 70-80% of full-data performance across models.", "published": "2025-06-02 05:02:03", "link": "http://arxiv.org/abs/2506.01319v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Online Audio-Visual Autoregressive Speaker Extraction", "abstract": "This paper proposes a novel online audio-visual speaker extraction model. In\nthe streaming regime, most studies optimize the audio network only, leaving the\nvisual frontend less explored. We first propose a lightweight visual frontend\nbased on depth-wise separable convolution. Then, we propose a lightweight\nautoregressive acoustic encoder to serve as the second cue, to actively explore\nthe information in the separated speech signal from past steps. Scenario-wise,\nfor the first time, we study how the algorithm performs when there is a change\nin focus of attention, i.e., the target speaker. Experimental results on LRS3\ndatasets show that our visual frontend performs comparably to the previous\nstate-of-the-art on both SkiM and ConvTasNet audio backbones with only 0.1\nmillion network parameters and 2.1 MACs per second of processing. The\nautoregressive acoustic encoder provides an additional 0.9 dB gain in terms of\nSI-SNRi, and its momentum is robust against the change in attention.", "published": "2025-06-02 02:47:53", "link": "http://arxiv.org/abs/2506.01270v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "WCTC-Biasing: Retraining-free Contextual Biasing ASR with Wildcard CTC-based Keyword Spotting and Inter-layer Biasing", "abstract": "Despite recent advances in end-to-end speech recognition methods, the output\ntends to be biased to the training data's vocabulary, resulting in inaccurate\nrecognition of proper nouns and other unknown terms. To address this issue, we\npropose a method to improve recognition accuracy of such rare words in\nCTC-based models without additional training or text-to-speech systems.\nSpecifically, keyword spotting is performed using acoustic features of\nintermediate layers during inference, and a bias is applied to the subsequent\nlayers of the acoustic model for detected keywords. For keyword detection, we\nadopt a wildcard CTC that is both fast and tolerant of ambiguous matches,\nallowing flexible handling of words that are difficult to match strictly. Since\nthis method does not require retraining of existing models, it can be easily\napplied to even large-scale models. In experiments on Japanese speech\nrecognition, the proposed method achieved a 29% improvement in the F1 score for\nunknown words.", "published": "2025-06-02 02:30:26", "link": "http://arxiv.org/abs/2506.01263v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Confidence intervals for forced alignment boundaries using model ensembles", "abstract": "Forced alignment is a common tool to align audio with orthographic and\nphonetic transcriptions. Most forced alignment tools provide only a single\nestimate of a boundary. The present project introduces a method of deriving\nconfidence intervals for these boundaries using a neural network ensemble\ntechnique. Ten different segment classifier neural networks were previously\ntrained, and the alignment process is repeated with each model. The alignment\nensemble is then used to place the boundary at the median of the boundaries in\nthe ensemble, and 97.85% confidence intervals are constructed using order\nstatistics. On the Buckeye and TIMIT corpora, the ensemble boundaries show a\nslight improvement over using just a single model. The confidence intervals are\nincorporated into Praat TextGrids using a point tier, and they are also output\nas a table for researchers to analyze separately as diagnostics or to\nincorporate uncertainty into their analyses.", "published": "2025-06-02 02:12:28", "link": "http://arxiv.org/abs/2506.01256v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Sensitivity-Aware Density Estimation in Multiple Dimensions", "abstract": "We formulate an optimization problem to estimate probability densities in the\ncontext of multidimensional problems that are sampled with uneven probability.\nIt considers detector sensitivity as an heterogeneous density and takes\nadvantage of the computational speed and flexible boundary conditions offered\nby splines on a grid. We choose to regularize the Hessian of the spline via the\nnuclear norm to promote sparsity. As a result, the method is spatially adaptive\nand stable against the choice of the regularization parameter, which plays the\nrole of the bandwidth. We test our computational pipeline on standard densities\nand provide software. We also present a new approach to PET rebinning as an\napplication of our framework.", "published": "2025-06-02 23:28:49", "link": "http://arxiv.org/abs/2506.02323v1", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.DS", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Absorb and Converge: Provable Convergence Guarantee for Absorbing Discrete Diffusion Models", "abstract": "Discrete state space diffusion models have shown significant advantages in\napplications involving discrete data, such as text and image generation. It has\nalso been observed that their performance is highly sensitive to the choice of\nrate matrices, particularly between uniform and absorbing rate matrices. While\nempirical results suggest that absorbing rate matrices often yield better\ngeneration quality compared to uniform rate matrices, existing theoretical\nworks have largely focused on the uniform rate matrices case. Notably,\nconvergence guarantees and error analyses for absorbing diffusion models are\nstill missing. In this work, we provide the first finite-time error bounds and\nconvergence rate analysis for discrete diffusion models using absorbing rate\nmatrices. We begin by deriving an upper bound on the KL divergence of the\nforward process, introducing a surrogate initialization distribution to address\nthe challenge posed by the absorbing stationary distribution, which is a\nsingleton and causes the KL divergence to be ill-defined. We then establish the\nfirst convergence guarantees for both the $\\tau$-leaping and uniformization\nsamplers under absorbing rate matrices, demonstrating improved rates over their\ncounterparts using uniform rate matrices. Furthermore, under suitable\nassumptions, we provide convergence guarantees without early stopping. Our\nanalysis introduces several new technical tools to address challenges unique to\nabsorbing rate matrices. These include a Jensen-type argument for bounding\nforward process convergence, novel techniques for bounding absorbing score\nfunctions, and a non-divergent upper bound on the score near initialization\nthat removes the need of early-stopping.", "published": "2025-06-02 23:14:35", "link": "http://arxiv.org/abs/2506.02318v1", "categories": ["cs.LG", "eess.SP", "math.ST", "stat.TH"], "primary_category": "cs.LG"}
{"title": "Sums of Mixed Independent Positive Random Variables: A Unified Framework", "abstract": "This paper proposes a comprehensive and unprecedented framework that\nstreamlines the derivation of exact, compact -- yet tractable -- solutions for\nthe probability density function (PDF) and cumulative distribution function\n(CDF) of the sum of a broad spectrum of mixed independent positive random\nvariables (RVs). To showcase the framework's potential and extensive\napplicability, we tackle the enduring challenge of obtaining these statistics\nfor the sum of fading variates in an exact, manageable, and unified manner.\nSpecifically, we derive novel, tractable expressions for the PDF and CDF of the\nsum of Gaussian-class and non-Gaussian-class fading distributions, thereby\ncovering a plethora of conventional, generalized, and recently introduced\nfading models. The proposed framework accommodates independent and identically\ndistributed (i.i.d.) sums, independent but not necessarily identically\ndistributed (i.n.i.d.) sums, and mixed-type sums. Moreover, we introduce the\nstrikingly novel $\\alpha$-$\\mu$ mixture distribution that unifies all\nGaussian-class fading models.", "published": "2025-06-02 19:20:12", "link": "http://arxiv.org/abs/2506.02186v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Tomographic Foundation Model -- FORCE: Flow-Oriented Reconstruction Conditioning Engine", "abstract": "Computed tomography (CT) is a major medical imaging modality. Clinical CT\nscenarios, such as low-dose screening, sparse-view scanning, and metal\nimplants, often lead to severe noise and artifacts in reconstructed images,\nrequiring improved reconstruction techniques. The introduction of deep learning\nhas significantly advanced CT image reconstruction. However, obtaining paired\ntraining data remains rather challenging due to patient motion and other\nconstraints. Although deep learning methods can still perform well with\napproximately paired data, they inherently carry the risk of hallucination due\nto data inconsistencies and model instability. In this paper, we integrate the\ndata fidelity with the state-of-the-art generative AI model, referred to as the\nPoisson flow generative model (PFGM) with a generalized version PFGM++, and\npropose a novel CT framework: Flow-Oriented Reconstruction Conditioning Engine\n(FORCE). In our experiments, the proposed method shows superior performance in\nvarious CT imaging tasks, outperforming existing unsupervised reconstruction\napproaches.", "published": "2025-06-02 18:25:12", "link": "http://arxiv.org/abs/2506.02149v1", "categories": ["eess.IV", "cs.LG", "eess.SP"], "primary_category": "eess.IV"}
{"title": "Characterization of the Combined Effective Radiation Pattern of UAV-Mounted Antennas and Ground Station", "abstract": "An Unmanned Aerial Vehicle (UAV)-based communication typically involves a\nlink between a UAV-mounted antenna and a ground station. The radiation pattern\nof both antennas is influenced by nearby reflecting surfaces and scatterers,\nsuch as the UAV body and the ground. Experimentally characterizing the\neffective radiation patterns of both antennas is challenging, as the received\npower depends on their interaction. In this study, we learn a combined\nradiation pattern from experimental UAV flight data, assuming the UAV travels\nwith a fixed orientation (constant yaw angle and zero pitch/roll). We validate\nthe characterized radiation pattern by cross-referencing it with experiments\ninvolving different UAV trajectories, all conducted under identical ground\nstation and UAV orientation conditions. Experimental results show that the\nlearned combined radiation pattern reduces received power estimation error by\nup to 10 dB, compared to traditional anechoic chamber radiation patterns that\nneglect the effects of the UAV body and surrounding objects.", "published": "2025-06-02 17:44:05", "link": "http://arxiv.org/abs/2506.01925v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "EEG Foundation Models for BCI Learn Diverse Features of Electrophysiology", "abstract": "Brain computer interface (BCI) research, as well as increasing portions of\nthe field of neuroscience, have found success deploying large-scale artificial\nintelligence (AI) pre-training methods in conjunction with vast public\nrepositories of data. This approach of pre-training foundation models using\nlabel-free, self-supervised objectives offers the potential to learn robust\nrepresentations of neurophysiology, potentially addressing longstanding\nchallenges in neural decoding. However, to date, much of this work has focused\nexplicitly on standard BCI benchmarks and tasks, which likely overlooks the\nmultitude of features these powerful methods might learn about brain function\nas well as other electrophysiological information. We introduce a new method\nfor self-supervised BCI foundation model pre-training for EEG inspired by a\ntransformer-based approach adapted from the HuBERT framework originally\ndeveloped for speech processing. Our pipeline is specifically focused on\nlow-profile, real-time usage, involving minimally pre-processed data and just\neight EEG channels on the scalp. We show that our foundation model learned a\nrepresentation of EEG that supports standard BCI tasks (P300, motor imagery),\nbut also that this model learns features of neural data related to individual\nvariability, and other salient electrophysiological components (e.g., alpha\nrhythms). In addition to describing and evaluating a novel approach to\npre-training BCI models and neural decoding, this work opens the aperture for\nwhat kind of tasks and use-cases might exist for neural data in concert with\npowerful AI methods.", "published": "2025-06-02 16:55:26", "link": "http://arxiv.org/abs/2506.01867v1", "categories": ["q-bio.NC", "eess.SP"], "primary_category": "q-bio.NC"}
{"title": "Efficient Learning of Balanced Signed Graphs via Sparse Linear Programming", "abstract": "Signed graphs are equipped with both positive and negative edge weights,\nencoding pairwise correlations as well as anti-correlations in data. A balanced\nsigned graph is a signed graph with no cycles containing an odd number of\nnegative edges. Laplacian of a balanced signed graph has eigenvectors that map\nvia a simple linear transform to ones in a corresponding positive graph\nLaplacian, thus enabling reuse of spectral filtering tools designed for\npositive graphs. We propose an efficient method to learn a balanced signed\ngraph Laplacian directly from data. Specifically, extending a previous linear\nprogramming (LP) based sparse inverse covariance estimation method called\nCLIME, we formulate a new LP problem for each Laplacian column $i$, where the\nlinear constraints restrict weight signs of edges stemming from node $i$, so\nthat nodes of same / different polarities are connected by positive / negative\nedges. Towards optimal model selection, we derive a suitable CLIME parameter\n$\\rho$ based on a combination of the Hannan-Quinn information criterion and a\nminimum feasibility criterion. We solve the LP problem efficiently by tailoring\na sparse LP method based on ADMM. We theoretically prove local solution\nconvergence of our proposed iterative algorithm. Extensive experimental results\non synthetic and real-world datasets show that our balanced graph learning\nmethod outperforms competing methods and enables reuse of spectral filters,\nwavelets, and graph convolutional nets (GCN) constructed for positive graphs.", "published": "2025-06-02 16:09:51", "link": "http://arxiv.org/abs/2506.01826v1", "categories": ["cs.LG", "eess.SP"], "primary_category": "cs.LG"}
{"title": "A New 5 bit/2D-symbol Modulation Format for Relative Intensity Noise-dominated IM-DD Systems", "abstract": "We propose a novel 5-bit/2D-symbol modulation format based on PAM-6 optimized\nfor IM-DD systems dominated by relative intensity noise. The proposed\nmodulation scheme improves SNR by 0.94 dB compared to conventional PAM-6 and\nachieves near-optimal BER performance.", "published": "2025-06-02 15:10:01", "link": "http://arxiv.org/abs/2506.01761v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "The Promise of Spiking Neural Networks for Ubiquitous Computing: A Survey and New Perspectives", "abstract": "Spiking neural networks (SNNs) have emerged as a class of bio -inspired\nnetworks that leverage sparse, event-driven signaling to achieve low-power\ncomputation while inherently modeling temporal dynamics. Such characteristics\nalign closely with the demands of ubiquitous computing systems, which often\noperate on resource-constrained devices while continuously monitoring and\nprocessing time-series sensor data. Despite their unique and promising\nfeatures, SNNs have received limited attention and remain underexplored (or at\nleast, under-adopted) within the ubiquitous computing community. To address\nthis gap, this paper first introduces the core components of SNNs, both in\nterms of models and training mechanisms. It then presents a systematic survey\nof 76 SNN-based studies focused on time-series data analysis, categorizing them\ninto six key application domains. For each domain, we summarize relevant works\nand subsequent advancements, distill core insights, and highlight key takeaways\nfor researchers and practitioners. To facilitate hands-on experimentation, we\nalso provide a comprehensive review of current software frameworks and\nneuromorphic hardware platforms, detailing their capabilities and\nspecifications, and then offering tailored recommendations for selecting\ndevelopment tools based on specific application needs. Finally, we identify\nprevailing challenges within each application domain and propose future\nresearch directions that need be explored in ubiquitous community. Our survey\nhighlights the transformative potential of SNNs in enabling energy-efficient\nubiquitous sensing across diverse application domains, while also serving as an\nessential introduction for researchers looking to enter this emerging field.", "published": "2025-06-02 14:47:48", "link": "http://arxiv.org/abs/2506.01737v1", "categories": ["cs.NE", "eess.SP", "I.2"], "primary_category": "cs.NE"}
{"title": "Local Ambiguity Shaping for Doppler-Resilient Sequences Under Spectral and PAPR Constraints", "abstract": "This paper focuses on designing Doppler-resilient sequences with low local\nAmbiguity Function (AF) sidelobes, subject to certain spectral and\nPeak-to-Average Power Ratio (PAPR) constraints. To achieve this, we propose two\ndistinctoptimization algorithms: (i) an Alternating Minimization (AM) algorithm\nfor superior Weighted Peak Sidelobe Level (WPSL) minimization, and (ii) a\nlow-complexity Augmented Lagrangian-assisted Majorization Minimization (ALaMM)\nalgorithm with effective WPSL suppression. The proposed schemes hold great\npotential for sequence design in future 6G and integrated sensing and\ncommunication applications, supporting robust sensing under spectral\ncoexistence constraints in high-mobility scenarios.", "published": "2025-06-02 13:14:55", "link": "http://arxiv.org/abs/2506.01637v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Network Digital Twin for 6G and Beyond: An End-to-End View Across Multi-Domain Network Ecosystems", "abstract": "With the rapid development of technology, the number of smart mobile users is\nincreasing, accompanied by growing demands from applications such as\nvirtual/augmented reality (VR/XR), remote surgery, autonomous vehicles, and\nreal-time holographic communications, all of which require high transmission\nrates and ultra-low latency in 6G and beyond networks (6G+). This poses\nenormous challenges in efficiently deploying large-scale networks, including\nnetwork design, planning, troubleshooting, optimization, and maintenance,\nwithout affecting the user experience. Network Digital Twin (NDT) has emerged\nas a potential solution, enabling the creation of a virtual model that reflects\nthe actual network, supporting the simulation of various network designs,\napplying diverse operating policies, and reproducing complex fault scenarios\nunder real-world conditions. This motivate us for this study, where we provide\na comprehensive survey of NDT in the context of 6G+, covering areas such as\nradio access networks (RAN), transport networks, 5G core networks and beyond\n(5GCORE+), cloud/edge computing, applications (blockchain, health system,\nmanufacturing, security, and vehicular networks), non-terrestrial networks\n(NTNs), and quantum networks, from both academic and industrial perspectives.\nIn particular, we are the first to provide an in-depth guide and usage of RAN\nand 5GCORE+ for NDT. Then, we provide an extensive review of foundation\ntechnologies such as transport networks, cloud/edge computing, applications,\nNTNs, and quantum networks in NDT. Finally, we discuss the key challenges, open\nissues, and future research directions for NDT in the context of 6G+.", "published": "2025-06-02 12:49:51", "link": "http://arxiv.org/abs/2506.01609v1", "categories": ["cs.NI", "eess.SP"], "primary_category": "cs.NI"}
{"title": "Unified Interference-Aware Water-Filling for QoS-Constrained Communication, Sensing, and JRC", "abstract": "Water-filling (WF) algorithms are pivotal in maximizing capacity and spectral\nefficiency in multiple-input and multiple-output (MIMO) systems. However,\ntraditional WF approaches cater solely to communication requirements,\nneglecting the emerging heterogeneity of 6G, including sensing and joint\nradar-communication (JRC). As these diverse demands grow in importance and have\ndifferent Quality of Service (QoS) constraints, traditional WF becomes\ninadequate. Therefore, in this paper, we propose a unified interference-aware\nand QoS-constrained WF algorithm for systems with communication, sensing, and\nJRC. The proposed algorithm enables power allocation for multi-user MIMO\nsystems, effectively addressing interference and balancing the support for\nheterogeneous user requirements.", "published": "2025-06-02 07:51:37", "link": "http://arxiv.org/abs/2506.01400v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Enabling Scalable Distributed Beamforming via Networked LEO Satellites Towards 6G", "abstract": "In this paper, we propose scalable distributed beamforming schemes over low\nEarth orbit (LEO) satellite networks that rely solely on statistical channel\nstate information for downlink orthogonal frequency division multiplexing\nsystems. We begin by introducing the system model and presenting a pragmatic\nyet effective analog beamformer and user-scheduling design. We then derive a\nclosed-form lower bound on the ergodic sum rate, based on the hardening bound,\nfor the digital beamformer design. Next, we formulate a per-satellite\npower-constrained sum-rate maximization problem, whose centralized solution,\nobtained via the weighted minimum mean squared error (WMMSE) framework,\nestablishes performance limits and motivates decentralized strategies. We\nsubsequently introduce two decentralized optimization schemes, based on\napproximating the hardening bound and decentralizing the WMMSE framework, for\nrepresentative inter-satellite link topologies. In the Ring scheme, satellites\nupdate beamformers locally and exchange intermediate parameters sequentially.\nIn the Star scheme, edge satellites update beamformers locally and in parallel,\nachieving consensus on intermediate parameters at a central satellite using a\npenalty-dual decomposition framework. Extensive simulations demonstrate that\nour distributed designs achieve near-centralized performance with superior\nscalability, substantially outperforming simple closed-form beamformers and\nsingle-satellite baselines in sum rate. Additionally, the delay-overhead\ntrade-off between the two topologies is revealed.", "published": "2025-06-02 07:18:50", "link": "http://arxiv.org/abs/2506.01382v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "From Turbulence to Tranquility: AI-Driven Low-Altitude Network", "abstract": "Low Altitude Economy (LAE) networks own transformative potential in urban\nmobility, emergency response, and aerial logistics. However, these networks\nface significant challenges in spectrum management, interference mitigation,\nand real-time coordination across dynamic and resource-constrained\nenvironments. After addressing these challenges, this study explores three core\nelements for enabling intelligent LAE networks as follows machine\nlearning-based spectrum sensing and coexistence, artificial intelligence\n(AI)-optimized resource allocation and trajectory planning, and testbed-driven\nvalidation and standardization. We highlight how federated and reinforcement\nlearning techniques support decentralized, adaptive decision-making under\nmobility and energy constraints. In addition, we discuss the role of real-world\nplatforms such as AERPAW in bridging the gap between simulation and deployment\nand enabling iterative system refinement under realistic conditions. This study\naims to provide a forward-looking roadmap toward developing efficient and\ninteroperable AI-driven LAE ecosystems.", "published": "2025-06-02 07:12:44", "link": "http://arxiv.org/abs/2506.01378v1", "categories": ["eess.SP", "cs.LG"], "primary_category": "eess.SP"}
{"title": "Energy-Efficient Integrated Communication and Computation via Non-Terrestrial Networks with Uncertainty Awareness", "abstract": "Non-terrestrial network (NTN)-based integrated communication and computation\nempowers various emerging applications with global coverage. Yet this vision is\nseverely challenged by the energy issue given the limited energy supply of NTN\nnodes and the energy-consuming nature of communication and computation. In this\npaper, we investigate the energy-efficient integrated communication and\ncomputation for the ground node data through a NTN, incorporating an unmanned\naerial vehicle (UAV) and a satellite. We jointly consider ground data\noffloading to the UAV, edge processing on the UAV, and the forwarding of\nresults from UAV to satellite, where we particularly address the uncertainties\nof the UAV-satellite links due to the large distance and high dynamics therein.\nAccordingly, we propose to minimize the weighted energy consumption due to data\noffloading, UAV computation, UAV transmission, and UAV propulsion, in the\npresence of angular uncertainties under Gaussian distribution within the\nUAV-satellite channels. The formulated problem with probabilistic constraints\ndue to uncertainties is converted into a deterministic form by exploiting the\nBernstein-type inequality, which is then solved using a block coordinate\ndescent framework with algorithm design. Simulation results are provided to\ndemonstrate the performance superiority of our proposal in terms of energy\nsustainability, along with the robustness against uncertain non-terrestrial\nenvironments.", "published": "2025-06-02 01:43:18", "link": "http://arxiv.org/abs/2506.01243v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
