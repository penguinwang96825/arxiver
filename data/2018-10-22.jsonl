{"title": "Labeling Gaps Between Words: Recognizing Overlapping Mentions with\n  Mention Separators", "abstract": "In this paper, we propose a new model that is capable of recognizing\noverlapping mentions. We introduce a novel notion of mention separators that\ncan be effectively used to capture how mentions overlap with one another. On\ntop of a novel multigraph representation that we introduce, we show that\nefficient and exact inference can still be performed. We present some\ntheoretical analysis on the differences between our model and a recently\nproposed model for recognizing overlapping mentions, and discuss the possible\nimplications of the differences. Through extensive empirical analysis on\nstandard datasets, we demonstrate the effectiveness of our approach.", "published": "2018-10-22 03:38:23", "link": "http://arxiv.org/abs/1810.09073v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Dual-Attention Hierarchical Recurrent Neural Network for Dialogue Act\n  Classification", "abstract": "Recognising dialogue acts (DA) is important for many natural language\nprocessing tasks such as dialogue generation and intention recognition. In this\npaper, we propose a dual-attention hierarchical recurrent neural network for DA\nclassification. Our model is partially inspired by the observation that\nconversational utterances are normally associated with both a DA and a topic,\nwhere the former captures the social act and the latter describes the subject\nmatter. However, such a dependency between DAs and topics has not been utilised\nby most existing systems for DA classification. With a novel dual task-specific\nattention mechanism, our model is able, for utterances, to capture information\nabout both DAs and topics, as well as information about the interactions\nbetween them. Experimental results show that by modelling topic as an auxiliary\ntask, our model can significantly improve DA classification, yielding better or\ncomparable performance to the state-of-the-art method on three public datasets.", "published": "2018-10-22 09:45:52", "link": "http://arxiv.org/abs/1810.09154v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Named Entity Disambiguation using Deep Learning on Graphs", "abstract": "We tackle \\ac{NED} by comparing entities in short sentences with \\wikidata{}\ngraphs. Creating a context vector from graphs through deep learning is a\nchallenging problem that has never been applied to \\ac{NED}. Our main\ncontribution is to present an experimental study of recent neural techniques,\nas well as a discussion about which graph features are most important for the\ndisambiguation task. In addition, a new dataset (\\wikidatadisamb{}) is created\nto allow a clean and scalable evaluation of \\ac{NED} with \\wikidata{} entries,\nand to be used as a reference in future research. In the end our results show\nthat a \\ac{Bi-LSTM} encoding of the graph triplets performs best, improving\nupon the baseline models and scoring an \\rm{F1} value of $91.6\\%$ on the\n\\wikidatadisamb{} test set", "published": "2018-10-22 10:16:07", "link": "http://arxiv.org/abs/1810.09164v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predictive Linguistic Features of Schizophrenia", "abstract": "Schizophrenia is one of the most disabling and difficult to treat of all\nhuman medical/health conditions, ranking in the top ten causes of disability\nworldwide. It has been a puzzle in part due to difficulty in identifying its\nbasic, fundamental components. Several studies have shown that some\nmanifestations of schizophrenia (e.g., the negative symptoms that include\nblunting of speech prosody, as well as the disorganization symptoms that lead\nto disordered language) can be understood from the perspective of linguistics.\nHowever, schizophrenia research has not kept pace with technologies in\ncomputational linguistics, especially in semantics and pragmatics. As such, we\nexamine the writings of schizophrenia patients analyzing their syntax,\nsemantics and pragmatics. In addition, we analyze tweets of (self pro-claimed)\nschizophrenia patients who publicly discuss their diagnoses. For writing\nsamples dataset, syntactic features are found to be the most successful in\nclassification whereas for the less structured Twitter dataset, a combination\nof features performed the best.", "published": "2018-10-22 15:48:52", "link": "http://arxiv.org/abs/1810.09377v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistic Legal Concept Extraction in Portuguese", "abstract": "This work investigates legal concepts and their expression in Portuguese,\nconcentrating on the \"Order of Attorneys of Brazil\" Bar exam. Using a corpus\nformed by a collection of multiple-choice questions, three norms related to the\nEthics part of the OAB exam, language resources (Princeton WordNet and\nOpenWordNet-PT) and tools (AntConc and Freeling), we began to investigate the\nconcepts and words missing from our repertory of concepts and words in\nPortuguese, the knowledge base OpenWordNet-PT. We add these concepts and words\nto OpenWordNet-PT and hence obtain a representation of these texts that is\n\"contained\" in the lexical knowledge base.", "published": "2018-10-22 15:58:57", "link": "http://arxiv.org/abs/1810.09379v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatically Detecting Self-Reported Birth Defect Outcomes on Twitter\n  for Large-scale Epidemiological Research", "abstract": "In recent work, we identified and studied a small cohort of Twitter users\nwhose pregnancies with birth defect outcomes could be observed via their\npublicly available tweets. Exploiting social media's large-scale potential to\ncomplement the limited methods for studying birth defects, the leading cause of\ninfant mortality, depends on the further development of automatic methods. The\nprimary objective of this study was to take the first step towards scaling the\nuse of social media for observing pregnancies with birth defect outcomes,\nnamely, developing methods for automatically detecting tweets by users\nreporting their birth defect outcomes. We annotated and pre-processed\napproximately 23,000 tweets that mention birth defects in order to train and\nevaluate supervised machine learning algorithms, including feature-engineered\nand deep learning-based classifiers. We also experimented with various\nunder-sampling and over-sampling approaches to address the class imbalance. A\nSupport Vector Machine (SVM) classifier trained on the original, imbalanced\ndata set, with n-grams, word clusters, and structural features, achieved the\nbest baseline performance for the positive classes: an F1-score of 0.65 for the\n\"defect\" class and 0.51 for the \"possible defect\" class. Our contributions\ninclude (i) natural language processing (NLP) and supervised machine learning\nmethods for automatically detecting tweets by users reporting their birth\ndefect outcomes, (ii) a comparison of feature-engineered and deep\nlearning-based classifiers trained on imbalanced, under-sampled, and\nover-sampled data, and (iii) an error analysis that could inform classification\nimprovements using our publicly available corpus. Future work will focus on\nautomating user-level analyses for cohort inclusion.", "published": "2018-10-22 19:00:46", "link": "http://arxiv.org/abs/1810.09506v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Universal Dialogue State Tracking", "abstract": "Dialogue state tracking is the core part of a spoken dialogue system. It\nestimates the beliefs of possible user's goals at every dialogue turn. However,\nfor most current approaches, it's difficult to scale to large dialogue domains.\nThey have one or more of following limitations: (a) Some models don't work in\nthe situation where slot values in ontology changes dynamically; (b) The number\nof model parameters is proportional to the number of slots; (c) Some models\nextract features based on hand-crafted lexicons. To tackle these challenges, we\npropose StateNet, a universal dialogue state tracker. It is independent of the\nnumber of values, shares parameters across all slots, and uses pre-trained word\nvectors instead of explicit semantic dictionaries. Our experiments on two\ndatasets show that our approach not only overcomes the limitations, but also\nsignificantly outperforms the performance of state-of-the-art approaches.", "published": "2018-10-22 22:47:48", "link": "http://arxiv.org/abs/1810.09587v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ruuh: A Deep Learning Based Conversational Social Agent", "abstract": "Dialogue systems and conversational agents are becoming increasingly popular\nin the modern society but building an agent capable of holding intelligent\nconversation with its users is a challenging problem for artificial\nintelligence. In this demo, we demonstrate a deep learning based conversational\nsocial agent called \"Ruuh\" (facebook.com/Ruuh) designed by a team at Microsoft\nIndia to converse on a wide range of topics. Ruuh needs to think beyond the\nutilitarian notion of merely generating \"relevant\" responses and meet a wider\nrange of user social needs, like expressing happiness when user's favorite team\nwins, sharing a cute comment on showing the pictures of the user's pet and so\non. The agent also needs to detect and respond to abusive language, sensitive\ntopics and trolling behavior of the users. Many of these problems pose\nsignificant research challenges which will be demonstrated in our demo. Our\nagent has interacted with over 2 million real world users till date which has\ngenerated over 150 million user conversations.", "published": "2018-10-22 14:26:13", "link": "http://arxiv.org/abs/1810.12097v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Exploration of Dropout with RNNs for Natural Language Inference", "abstract": "Dropout is a crucial regularization technique for the Recurrent Neural\nNetwork (RNN) models of Natural Language Inference (NLI). However, dropout has\nnot been evaluated for the effectiveness at different layers and dropout rates\nin NLI models. In this paper, we propose a novel RNN model for NLI and\nempirically evaluate the effect of applying dropout at different layers in the\nmodel. We also investigate the impact of varying dropout rates at these layers.\nOur empirical evaluation on a large (Stanford Natural Language Inference\n(SNLI)) and a small (SciTail) dataset suggest that dropout at each feed-forward\nconnection severely affects the model accuracy at increasing dropout rates. We\nalso show that regularizing the embedding layer is efficient for SNLI whereas\nregularizing the recurrent layer improves the accuracy for SciTail. Our model\nachieved an accuracy 86.14% on the SNLI dataset and 77.05% on SciTail.", "published": "2018-10-22 17:48:21", "link": "http://arxiv.org/abs/1810.08606v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Proactive Security: Embedded AI Solution for Violent and Abusive Speech\n  Recognition", "abstract": "Violence is an epidemic in Brazil and a problem on the rise world-wide.\nMobile devices provide communication technologies which can be used to monitor\nand alert about violent situations. However, current solutions, like panic\nbuttons or safe words, might increase the loss of life in violent situations.\nWe propose an embedded artificial intelligence solution, using natural language\nand speech processing technology, to silently alert someone who can help in\nthis situation. The corpus used contains 400 positive phrases and 800 negative\nphrases, totaling 1,200 sentences which are classified using two well-known\nextraction methods for natural language processing tasks: bag-of-words and word\nembeddings and classified with a support vector machine. We describe the\nproof-of-concept product in development with promising results, indicating a\npath towards a commercial product. More importantly we show that model\nimprovements via word embeddings and data augmentation techniques provide an\nintrinsically robust model. The final embedded solution also has a small\nfootprint of less than 10 MB.", "published": "2018-10-22 17:56:08", "link": "http://arxiv.org/abs/1810.09431v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ordered Neurons: Integrating Tree Structures into Recurrent Neural\n  Networks", "abstract": "Natural language is hierarchically structured: smaller units (e.g., phrases)\nare nested within larger units (e.g., clauses). When a larger constituent ends,\nall of the smaller constituents that are nested within it must also be closed.\nWhile the standard LSTM architecture allows different neurons to track\ninformation at different time scales, it does not have an explicit bias towards\nmodeling a hierarchy of constituents. This paper proposes to add such an\ninductive bias by ordering the neurons; a vector of master input and forget\ngates ensures that when a given neuron is updated, all the neurons that follow\nit in the ordering are also updated. Our novel recurrent architecture, ordered\nneurons LSTM (ON-LSTM), achieves good performance on four different tasks:\nlanguage modeling, unsupervised parsing, targeted syntactic evaluation, and\nlogical inference.", "published": "2018-10-22 20:37:46", "link": "http://arxiv.org/abs/1810.09536v6", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Fully Attention-Based Information Retriever", "abstract": "Recurrent neural networks are now the state-of-the-art in natural language\nprocessing because they can build rich contextual representations and process\ntexts of arbitrary length. However, recent developments on attention mechanisms\nhave equipped feedforward networks with similar capabilities, hence enabling\nfaster computations due to the increase in the number of operations that can be\nparallelized. We explore this new type of architecture in the domain of\nquestion-answering and propose a novel approach that we call Fully Attention\nBased Information Retriever (FABIR). We show that FABIR achieves competitive\nresults in the Stanford Question Answering Dataset (SQuAD) while having fewer\nparameters and being faster at both learning and inference than rival methods.", "published": "2018-10-22 22:10:46", "link": "http://arxiv.org/abs/1810.09580v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "BioSentVec: creating sentence embeddings for biomedical texts", "abstract": "Sentence embeddings have become an essential part of today's natural language\nprocessing (NLP) systems, especially together advanced deep learning methods.\nAlthough pre-trained sentence encoders are available in the general domain,\nnone exists for biomedical texts to date. In this work, we introduce\nBioSentVec: the first open set of sentence embeddings trained with over 30\nmillion documents from both scholarly articles in PubMed and clinical notes in\nthe MIMIC-III Clinical Database. We evaluate BioSentVec embeddings in two\nsentence pair similarity tasks in different text genres. Our benchmarking\nresults demonstrate that the BioSentVec embeddings can better capture sentence\nsemantics compared to the other competitive alternatives and achieve\nstate-of-the-art performance in both tasks. We expect BioSentVec to facilitate\nthe research and development in biomedical text mining and to complement the\nexisting resources in biomedical word embeddings. BioSentVec is publicly\navailable at https://github.com/ncbi-nlp/BioSentVec", "published": "2018-10-22 14:10:01", "link": "http://arxiv.org/abs/1810.09302v6", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MiME: Multilevel Medical Embedding of Electronic Health Records for\n  Predictive Healthcare", "abstract": "Deep learning models exhibit state-of-the-art performance for many predictive\nhealthcare tasks using electronic health records (EHR) data, but these models\ntypically require training data volume that exceeds the capacity of most\nhealthcare systems. External resources such as medical ontologies are used to\nbridge the data volume constraint, but this approach is often not directly\napplicable or useful because of inconsistencies with terminology. To solve the\ndata insufficiency challenge, we leverage the inherent multilevel structure of\nEHR data and, in particular, the encoded relationships among medical codes. We\npropose Multilevel Medical Embedding (MiME) which learns the multilevel\nembedding of EHR data while jointly performing auxiliary prediction tasks that\nrely on this inherent EHR structure without the need for external labels. We\nconducted two prediction tasks, heart failure prediction and sequential disease\nprediction, where MiME outperformed baseline methods in diverse evaluation\nsettings. In particular, MiME consistently outperformed all baselines when\npredicting heart failure on datasets of different volumes, especially\ndemonstrating the greatest performance improvement (15% relative gain in PR-AUC\nover the best baseline) on the smallest dataset, demonstrating its ability to\neffectively model the multilevel structure of EHR data.", "published": "2018-10-22 23:19:43", "link": "http://arxiv.org/abs/1810.09593v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Biomedical Document Clustering and Visualization based on the Concepts\n  of Diseases", "abstract": "Document clustering is a text mining technique used to provide better\ndocument search and browsing in digital libraries or online corpora. A lot of\nresearch has been done on biomedical document clustering that is based on using\nexisting ontology. But, associations and co-occurrences of the medical concepts\nare not well represented by using ontology. In this research, a vector\nrepresentation of concepts of diseases and similarity measurement between\nconcepts are proposed. They identify the closest concepts of diseases in the\ncontext of a corpus. Each document is represented by using the vector space\nmodel. A weight scheme is proposed to consider both local content and\nassociations between concepts. A Self-Organizing Map is used as document\nclustering algorithm. The vector projection and visualization features of SOM\nenable visualization and analysis of the clusters distributions and\nrelationships on the two dimensional space. The experimental results show that\nthe proposed document clustering framework generates meaningful clusters and\nfacilitate visualization of the clusters based on the concepts of diseases.", "published": "2018-10-22 23:37:31", "link": "http://arxiv.org/abs/1810.09597v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LAMVI-2: A Visual Tool for Comparing and Tuning Word Embedding Models", "abstract": "Tuning machine learning models, particularly deep learning architectures, is\na complex process. Automated hyperparameter tuning algorithms often depend on\nspecific optimization metrics. However, in many situations, a developer trades\none metric against another: accuracy versus overfitting, precision versus\nrecall, smaller models and accuracy, etc. With deep learning, not only are the\nmodel's representations opaque, the model's behavior when parameters \"knobs\"\nare changed may also be unpredictable. Thus, picking the \"best\" model often\nrequires time-consuming model comparison. In this work, we introduce LAMVI-2, a\nvisual analytics system to support a developer in comparing hyperparameter\nsettings and outcomes. By focusing on word-embedding models (\"deep learning for\ntext\") we integrate views to compare both high-level statistics as well as\ninternal model behaviors (e.g., comparing word 'distances'). We demonstrate how\ndevelopers can work with LAMVI-2 to more quickly and accurately narrow down an\nappropriate and effective application-specific model.", "published": "2018-10-22 20:05:42", "link": "http://arxiv.org/abs/1810.11367v1", "categories": ["cs.CL", "cs.HC", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Comparison of Five Multiple Instance Learning Pooling Functions for\n  Sound Event Detection with Weak Labeling", "abstract": "Sound event detection (SED) entails two subtasks: recognizing what types of\nsound events are present in an audio stream (audio tagging), and pinpointing\ntheir onset and offset times (localization). In the popular multiple instance\nlearning (MIL) framework for SED with weak labeling, an important component is\nthe pooling function. This paper compares five types of pooling functions both\ntheoretically and experimentally, with special focus on their performance of\nlocalization. Although the attention pooling function is currently receiving\nthe most attention, we find the linear softmax pooling function to perform the\nbest among the five. Using this pooling function, we build a neural network\ncalled TALNet. It is the first system to reach state-of-the-art audio tagging\nperformance on Audio Set, while exhibiting strong localization performance on\nthe DCASE 2017 challenge at the same time.", "published": "2018-10-22 01:34:29", "link": "http://arxiv.org/abs/1810.09050v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Connectionist Temporal Localization for Sound Event Detection with\n  Sequential Labeling", "abstract": "Research on sound event detection (SED) with weak labeling has mostly focused\non presence/absence labeling, which provides no temporal information at all\nabout the event occurrences. In this paper, we consider SED with sequential\nlabeling, which specifies the temporal order of the event boundaries. The\nconventional connectionist temporal classification (CTC) framework, when\napplied to SED with sequential labeling, does not localize long events well due\nto a \"peak clustering\" problem. We adapt the CTC framework and propose\nconnectionist temporal localization (CTL), which successfully solves the\nproblem. Evaluation on a subset of Audio Set shows that CTL closes a third of\nthe gap between presence/ absence labeling and strong labeling, demonstrating\nthe usefulness of the extra temporal information in sequential labeling. CTL\nalso makes it easy to combine sequential labeling with presence/absence\nlabeling and strong labeling.", "published": "2018-10-22 01:41:45", "link": "http://arxiv.org/abs/1810.09052v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automatic acoustic identification of individual animals: Improving\n  generalisation across species and recording conditions", "abstract": "Many animals emit vocal sounds which, independently from the sounds'\nfunction, embed some individually-distinctive signature. Thus the automatic\nrecognition of individuals by sound is a potentially powerful tool for zoology\nand ecology research and practical monitoring. Here we present a general\nautomatic identification method, that can work across multiple animal species\nwith various levels of complexity in their communication systems. We further\nintroduce new analysis techniques based on dataset manipulations that can\nevaluate the robustness and generality of a classifier. By using these\ntechniques we confirmed the presence of experimental confounds in situations\nresembling those from past studies. We introduce data manipulations that can\nreduce the impact of these confounds, compatible with any classifier. We\nsuggest that assessment of confounds should become a standard part of future\nstudies to ensure they do not report over-optimistic results. We provide\nannotated recordings used for analyses along with this study and we call for\ndataset sharing to be a common practice to enhance development of methods and\ncomparisons of results.", "published": "2018-10-22 13:39:23", "link": "http://arxiv.org/abs/1810.09273v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Investigation of Monaural Front-End Processing for Robust ASR without\n  Retraining or Joint-Training", "abstract": "In recent years, monaural speech separation has been formulated as a\nsupervised learning problem, which has been systematically researched and shown\nthe dramatical improvement of speech intelligibility and quality for human\nlisteners. However, it has not been well investigated whether the methods can\nbe employed as the front-end processing and directly improve the performance of\na machine listener, i.e., an automatic speech recognizer, without retraining or\njoint-training the acoustic model. In this paper, we explore the effectiveness\nof the independent front-end processing for the multi-conditional trained ASR\non the CHiME-3 challenge. We find that directly feeding the enhanced features\nto ASR can make 36.40% and 11.78% relative WER reduction for the GMM-based and\nDNN-based ASR respectively. We also investigate the affect of noisy phase and\ngeneralization ability under unmatched noise condition.", "published": "2018-10-22 03:17:42", "link": "http://arxiv.org/abs/1810.09067v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Our Practice Of Using Machine Learning To Recognize Species By Voice", "abstract": "As the technology is advancing, audio recognition in machine learning is\nimproved as well. Research in audio recognition has traditionally focused on\nspeech. Living creatures (especially the small ones) are part of the whole\necosystem, monitoring as well as maintaining them are important tasks. Species\nsuch as animals and birds are tending to change their activities as well as\ntheir habitats due to the adverse effects on the environment or due to other\nnatural or man-made calamities. For those in far deserted areas, we will not\nhave any idea about their existence until we can continuously monitor them.\nContinuous monitoring will take a lot of hard work and labor. If there is no\ncontinuous monitoring, then there might be instances where endangered species\nmay encounter dangerous situations. The best way to monitor those species are\nthrough audio recognition. Classifying sound can be a difficult task even for\nhumans. Powerful audio signals and their processing techniques make it possible\nto detect audio of various species. There might be many ways wherein audio\nrecognition can be done. We can train machines either by pre-recorded audio\nfiles or by recording them live and detecting them. The audio of species can be\ndetected by removing all the background noise and echoes. Smallest sound is\nconsidered as a syllable. Extracting various syllables is the process we are\nfocusing on which is known as audio recognition in terms of Machine Learning\n(ML).", "published": "2018-10-22 04:23:17", "link": "http://arxiv.org/abs/1810.09078v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Unsupervised Detection of Anomalous Sound based on Deep Learning and the\n  Neyman-Pearson Lemma", "abstract": "This paper proposes a novel optimization principle and its implementation for\nunsupervised anomaly detection in sound (ADS) using an autoencoder (AE). The\ngoal of unsupervised-ADS is to detect unknown anomalous sound without training\ndata of anomalous sound. Use of an AE as a normal model is a state-of-the-art\ntechnique for unsupervised-ADS. To decrease the false positive rate (FPR), the\nAE is trained to minimize the reconstruction error of normal sounds and the\nanomaly score is calculated as the reconstruction error of the observed sound.\nUnfortunately, since this training procedure does not take into account the\nanomaly score for anomalous sounds, the true positive rate (TPR) does not\nnecessarily increase. In this study, we define an objective function based on\nthe Neyman-Pearson lemma by considering ADS as a statistical hypothesis test.\nThe proposed objective function trains the AE to maximize the TPR under an\narbitrary low FPR condition. To calculate the TPR in the objective function, we\nconsider that the set of anomalous sounds is the complementary set of normal\nsounds and simulate anomalous sounds by using a rejection sampling algorithm.\nThrough experiments using synthetic data, we found that the proposed method\nimproved the performance measures of ADS under low FPR conditions. In addition,\nwe confirmed that the proposed method could detect anomalous sounds in real\nenvironments.", "published": "2018-10-22 08:20:59", "link": "http://arxiv.org/abs/1810.09133v1", "categories": ["stat.ML", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "stat.ML"}
{"title": "DNN-based Source Enhancement to Increase Objective Sound Quality\n  Assessment Score", "abstract": "We propose a training method for deep neural network (DNN)-based source\nenhancement to increase objective sound quality assessment (OSQA) scores such\nas the perceptual evaluation of speech quality (PESQ). In many conventional\nstudies, DNNs have been used as a mapping function to estimate time-frequency\nmasks and trained to minimize an analytically tractable objective function such\nas the mean squared error (MSE). Since OSQA scores have been used widely for\nsound-quality evaluation, constructing DNNs to increase OSQA scores would be\nbetter than using the minimum-MSE to create high-quality output signals.\nHowever, since most OSQA scores are not analytically tractable, \\textit{i.e.},\nthey are black boxes, the gradient of the objective function cannot be\ncalculated by simply applying back-propagation. To calculate the gradient of\nthe OSQA-based objective function, we formulated a DNN optimization scheme on\nthe basis of \\textit{black-box optimization}, which is used for training a\ncomputer that plays a game. For a black-box-optimization scheme, we adopt the\npolicy gradient method for calculating the gradient on the basis of a sampling\nalgorithm. To simulate output signals using the sampling algorithm, DNNs are\nused to estimate the probability density function of the output signals that\nmaximize OSQA scores. The OSQA scores are calculated from the simulated output\nsignals, and the DNNs are trained to increase the probability of generating the\nsimulated output signals that achieve high OSQA scores. Through several\nexperiments, we found that OSQA scores significantly increased by applying the\nproposed method, even though the MSE was not minimized.", "published": "2018-10-22 08:34:04", "link": "http://arxiv.org/abs/1810.09137v1", "categories": ["stat.ML", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "stat.ML"}
{"title": "Chord Recognition in Symbolic Music: A Segmental CRF Model,\n  Segment-Level Features, and Comparative Evaluations on Classical and Popular\n  Music", "abstract": "We present a new approach to harmonic analysis that is trained to segment\nmusic into a sequence of chord spans tagged with chord labels. Formulated as a\nsemi-Markov Conditional Random Field (semi-CRF), this joint segmentation and\nlabeling approach enables the use of a rich set of segment-level features, such\nas segment purity and chord coverage, that capture the extent to which the\nevents in an entire segment of music are compatible with a candidate chord\nlabel. The new chord recognition model is evaluated extensively on three\ncorpora of classical music and a newly created corpus of rock music.\nExperimental results show that the semi-CRF model performs substantially better\nthan previous approaches when trained on a sufficient number of labeled\nexamples and remains competitive when the amount of training data is limited.", "published": "2018-10-22 22:55:17", "link": "http://arxiv.org/abs/1810.10002v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
