{"title": "EfficientQA : a RoBERTa Based Phrase-Indexed Question-Answering System", "abstract": "State-of-the-art extractive question-answering models achieve superhuman\nperformances on the SQuAD benchmark. Yet, they are unreasonably heavy and need\nexpensive GPU computing to answer questions in a reasonable time. Thus, they\ncannot be used in the open-domain question-answering paradigm for real-world\nqueries on hundreds of thousands of documents. In this paper, we explore the\npossibility of transferring the natural language understanding of language\nmodels into dense vectors representing questions and answer candidates to make\nquestion-answering compatible with a simple nearest neighbor search task. This\nnew model, which we call EfficientQA, takes advantage of the pair of sequences\nkind of input of BERT-based models to build meaningful, dense representations\nof candidate answers. These latter are extracted from the context in a\nquestion-agnostic fashion. Our model achieves state-of-the-art results in\nPhrase-Indexed Question Answering (PIQA), beating the previous state-of-art by\n1.3 points in exact-match and 1.4 points in f1-score. These results show that\ndense vectors can embed rich semantic representations of sequences, although\nthese were built from language models not originally trained for the use case.\nThus, to build more resource-efficient NLP systems in the future, training\nlanguage models better adapted to build dense representations of phrases is one\nof the possibilities.", "published": "2021-01-06 17:46:05", "link": "http://arxiv.org/abs/2101.02157v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit\n  Reasoning Strategies", "abstract": "A key limitation in current datasets for multi-hop reasoning is that the\nrequired steps for answering the question are mentioned in it explicitly. In\nthis work, we introduce StrategyQA, a question answering (QA) benchmark where\nthe required reasoning steps are implicit in the question, and should be\ninferred using a strategy. A fundamental challenge in this setup is how to\nelicit such creative questions from crowdsourcing workers, while covering a\nbroad range of potential strategies. We propose a data collection procedure\nthat combines term-based priming to inspire annotators, careful control over\nthe annotator population, and adversarial filtering for eliminating reasoning\nshortcuts. Moreover, we annotate each question with (1) a decomposition into\nreasoning steps for answering it, and (2) Wikipedia paragraphs that contain the\nanswers to each step. Overall, StrategyQA includes 2,780 examples, each\nconsisting of a strategy question, its decomposition, and evidence paragraphs.\nAnalysis shows that questions in StrategyQA are short, topic-diverse, and cover\na wide range of strategies. Empirically, we show that humans perform well (87%)\non this task, while our best baseline reaches an accuracy of $\\sim$66%.", "published": "2021-01-06 19:14:23", "link": "http://arxiv.org/abs/2101.02235v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can RNNs learn Recursive Nested Subject-Verb Agreements?", "abstract": "One of the fundamental principles of contemporary linguistics states that\nlanguage processing requires the ability to extract recursively nested tree\nstructures. However, it remains unclear whether and how this code could be\nimplemented in neural circuits. Recent advances in Recurrent Neural Networks\n(RNNs), which achieve near-human performance in some language tasks, provide a\ncompelling model to address such questions. Here, we present a new framework to\nstudy recursive processing in RNNs, using subject-verb agreement as a probe\ninto the representations of the neural network. We trained six distinct types\nof RNNs on a simplified probabilistic context-free grammar designed to\nindependently manipulate the length of a sentence and the depth of its\nsyntactic tree. All RNNs generalized to subject-verb dependencies longer than\nthose seen during training. However, none systematically generalized to deeper\ntree structures, even those with a structural bias towards learning nested tree\n(i.e., stack-RNNs). In addition, our analyses revealed primacy and recency\neffects in the generalization patterns of LSTM-based models, showing that these\nmodels tend to perform well on the outer- and innermost parts of a\ncenter-embedded tree structure, but poorly on its middle levels. Finally,\nprobing the internal states of the model during the processing of sentences\nwith nested tree structures, we found a complex encoding of grammatical\nagreement information (e.g. grammatical number), in which all the information\nfor multiple words nouns was carried by a single unit. Taken together, these\nresults indicate how neural networks may extract bounded nested tree\nstructures, without learning a systematic recursive rule.", "published": "2021-01-06 20:47:02", "link": "http://arxiv.org/abs/2101.02258v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On-Device Document Classification using multimodal features", "abstract": "From small screenshots to large videos, documents take up a bulk of space in\na modern smartphone. Documents in a phone can accumulate from various sources,\nand with the high storage capacity of mobiles, hundreds of documents are\naccumulated in a short period. However, searching or managing documents remains\nan onerous task, since most search methods depend on meta-information or only\ntext in a document. In this paper, we showcase that a single modality is\ninsufficient for classification and present a novel pipeline to classify\ndocuments on-device, thus preventing any private user data transfer to server.\nFor this task, we integrate an open-source library for Optical Character\nRecognition (OCR) and our novel model architecture in the pipeline. We optimise\nthe model for size, a necessary metric for on-device inference. We benchmark\nour classification model with a standard multimodal dataset FOOD-101 and\nshowcase competitive results with the previous State of the Art with 30% model\ncompression.", "published": "2021-01-06 05:36:58", "link": "http://arxiv.org/abs/2101.01880v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Deep Neural Network Based Relation Extraction: An Overview", "abstract": "Knowledge is a formal way of understanding the world, providing a human-level\ncognition and intelligence for the next-generation artificial intelligence\n(AI). One of the representations of knowledge is semantic relations between\nentities. An effective way to automatically acquire this important knowledge,\ncalled Relation Extraction (RE), a sub-task of information extraction, plays a\nvital role in Natural Language Processing (NLP). Its purpose is to identify\nsemantic relations between entities from natural language text. To date, there\nare several studies for RE in previous works, which have documented these\ntechniques based on Deep Neural Networks (DNNs) become a prevailing technique\nin this research. Especially, the supervised and distant supervision methods\nbased on DNNs are the most popular and reliable solutions for RE. This article\n1) introduces some general concepts, and further 2) gives a comprehensive\noverview of DNNs in RE from two points of view: supervised RE, which attempts\nto improve the standard RE systems, and distant supervision RE, which adopts\nDNNs to design sentence encoder and de-noise method. We further 3) cover some\nnovel methods and recent trends as well as discuss possible future research\ndirections for this task.", "published": "2021-01-06 07:53:05", "link": "http://arxiv.org/abs/2101.01907v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SF-QA: Simple and Fair Evaluation Library for Open-domain Question\n  Answering", "abstract": "Although open-domain question answering (QA) draws great attention in recent\nyears, it requires large amounts of resources for building the full system and\nis often difficult to reproduce previous results due to complex configurations.\nIn this paper, we introduce SF-QA: simple and fair evaluation framework for\nopen-domain QA. SF-QA framework modularizes the pipeline open-domain QA system,\nwhich makes the task itself easily accessible and reproducible to research\ngroups without enough computing resources. The proposed evaluation framework is\npublicly available and anyone can contribute to the code and evaluations.", "published": "2021-01-06 08:02:41", "link": "http://arxiv.org/abs/2101.01910v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Order Embeddings from Merged Ontologies using Sketching", "abstract": "We give a simple, low resource method to produce order embeddings from\nontologies. Such embeddings map words to vectors so that order relations on the\nwords, such as hypernymy/hyponymy, are represented in a direct way. Our method\nuses sketching techniques, in particular countsketch, for dimensionality\nreduction. We also study methods to merge ontologies, in particular those in\nmedical domains, so that order relations are preserved. We give computational\nresults for medical ontologies and for wordnet, showing that our merging\ntechniques are effective and our embedding yields an accurate representation in\nboth generic and specialised domains.", "published": "2021-01-06 17:50:55", "link": "http://arxiv.org/abs/2101.02158v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hypothesis Stitcher for End-to-End Speaker-attributed ASR on Long-form\n  Multi-talker Recordings", "abstract": "An end-to-end (E2E) speaker-attributed automatic speech recognition (SA-ASR)\nmodel was proposed recently to jointly perform speaker counting, speech\nrecognition and speaker identification. The model achieved a low\nspeaker-attributed word error rate (SA-WER) for monaural overlapped speech\ncomprising an unknown number of speakers. However, the E2E modeling approach is\nsusceptible to the mismatch between the training and testing conditions. It has\nyet to be investigated whether the E2E SA-ASR model works well for recordings\nthat are much longer than samples seen during training. In this work, we first\napply a known decoding technique that was developed to perform single-speaker\nASR for long-form audio to our E2E SA-ASR task. Then, we propose a novel method\nusing a sequence-to-sequence model, called hypothesis stitcher. The model takes\nmultiple hypotheses obtained from short audio segments that are extracted from\nthe original long-form input, and it then outputs a fused single hypothesis. We\npropose several architectural variations of the hypothesis stitcher model and\ncompare them with the conventional decoding methods. Experiments using\nLibriSpeech and LibriCSS corpora show that the proposed method significantly\nimproves SA-WER especially for long-form multi-talker recordings.", "published": "2021-01-06 03:36:09", "link": "http://arxiv.org/abs/2101.01853v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Taxonomy Completion via Triplet Matching Network", "abstract": "Automatically constructing taxonomy finds many applications in e-commerce and\nweb search. One critical challenge is as data and business scope grow in real\napplications, new concepts are emerging and needed to be added to the existing\ntaxonomy. Previous approaches focus on the taxonomy expansion, i.e. finding an\nappropriate hypernym concept from the taxonomy for a new query concept. In this\npaper, we formulate a new task, \"taxonomy completion\", by discovering both the\nhypernym and hyponym concepts for a query. We propose Triplet Matching Network\n(TMN), to find the appropriate <hypernym, hyponym> pairs for a given query\nconcept. TMN consists of one primal scorer and multiple auxiliary scorers.\nThese auxiliary scorers capture various fine-grained signals (e.g., query to\nhypernym or query to hyponym semantics), and the primal scorer makes a holistic\nprediction on <query, hypernym, hyponym> triplet based on the internal feature\nrepresentations of all auxiliary scorers. Also, an innovative channel-wise\ngating mechanism that retains task-specific information in concept\nrepresentations is introduced to further boost model performance. Experiments\non four real-world large-scale datasets show that TMN achieves the best\nperformance on both taxonomy completion task and the previous taxonomy\nexpansion task, outperforming existing methods.", "published": "2021-01-06 07:19:55", "link": "http://arxiv.org/abs/2101.01896v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Curriculum-Meta Learning for Order-Robust Continual Relation Extraction", "abstract": "Continual relation extraction is an important task that focuses on extracting\nnew facts incrementally from unstructured text. Given the sequential arrival\norder of the relations, this task is prone to two serious challenges, namely\ncatastrophic forgetting and order-sensitivity. We propose a novel\ncurriculum-meta learning method to tackle the above two challenges in continual\nrelation extraction. We combine meta learning and curriculum learning to\nquickly adapt model parameters to a new task and to reduce interference of\npreviously seen tasks on the current task. We design a novel relation\nrepresentation learning method through the distribution of domain and range\ntypes of relations. Such representations are utilized to quantify the\ndifficulty of tasks for the construction of curricula. Moreover, we also\npresent novel difficulty-based metrics to quantitatively measure the extent of\norder-sensitivity of a given model, suggesting new ways to evaluate model\nrobustness. Our comprehensive experiments on three benchmark datasets show that\nour proposed method outperforms the state-of-the-art techniques. The code is\navailable at the anonymous GitHub repository:\nhttps://github.com/wutong8023/AAAI_CML.", "published": "2021-01-06 08:52:34", "link": "http://arxiv.org/abs/2101.01926v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "User Ex Machina : Simulation as a Design Probe in Human-in-the-Loop Text\n  Analytics", "abstract": "Topic models are widely used analysis techniques for clustering documents and\nsurfacing thematic elements of text corpora. These models remain challenging to\noptimize and often require a \"human-in-the-loop\" approach where domain experts\nuse their knowledge to steer and adjust. However, the fragility,\nincompleteness, and opacity of these models means even minor changes could\ninduce large and potentially undesirable changes in resulting model. In this\npaper we conduct a simulation-based analysis of human-centered interactions\nwith topic models, with the objective of measuring the sensitivity of topic\nmodels to common classes of user actions. We find that user interactions have\nimpacts that differ in magnitude but often negatively affect the quality of the\nresulting modelling in a way that can be difficult for the user to evaluate. We\nsuggest the incorporation of sensitivity and \"multiverse\" analyses to topic\nmodel interfaces to surface and overcome these deficiencies.", "published": "2021-01-06 19:44:11", "link": "http://arxiv.org/abs/2101.02244v1", "categories": ["cs.HC", "cs.CL", "cs.LG", "68U15", "H.5.0"], "primary_category": "cs.HC"}
{"title": "The 2020 Personalized Voice Trigger Challenge: Open Database, Evaluation\n  Metrics and the Baseline Systems", "abstract": "The 2020 Personalized Voice Trigger Challenge (PVTC2020) addresses two\ndifferent research problems a unified setup: joint wake-up word detection with\nspeaker verification on close-talking single microphone data and far-field\nmulti-channel microphone array data. Specially, the second task poses an\nadditional cross-channel matching challenge on top of the far-field condition.\nTo simulate the real-life application scenario, the enrollment utterances are\nrecorded from close-talking cell-phone only, while the test utterances are\nrecorded from both the close-talking cell-phone and the far-field microphone\narrays. This paper introduces our challenge setup and the released database as\nwell as the evaluation metrics. In addition, we present a joint end-to-end\nneural network baseline system trained with the proposed database for\nspeaker-dependent wake-up word detection. Results show that the cost calculated\nfrom the miss rate and the false alarm rate, can reach 0.37 in the\nclose-talking single microphone task and 0.31 in the far-field microphone array\ntask. The official website and the open-source baseline system have been\nreleased.", "published": "2021-01-06 09:26:21", "link": "http://arxiv.org/abs/2101.01935v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Multichannel CRNN for Speaker Counting: an Analysis of Performance", "abstract": "Speaker counting is the task of estimating the number of people that are\nsimultaneously speaking in an audio recording. For several audio processing\ntasks such as speaker diarization, separation, localization and tracking,\nknowing the number of speakers at each timestep is a prerequisite, or at least\nit can be a strong advantage, in addition to enabling a low latency processing.\nIn a previous work, we addressed the speaker counting problem with a\nmultichannel convolutional recurrent neural network which produces an\nestimation at a short-term frame resolution. In this work, we show that, for a\ngiven frame, there is an optimal position in the input sequence for best\nprediction accuracy. We empirically demonstrate the link between that optimal\nposition, the length of the input sequence and the size of the convolutional\nfilters.", "published": "2021-01-06 11:29:28", "link": "http://arxiv.org/abs/2101.01977v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Environment Transfer for Distributed Systems", "abstract": "Collecting sufficient amount of data that can represent various acoustic\nenvironmental attributes is a critical problem for distributed acoustic machine\nlearning. Several audio data augmentation techniques have been introduced to\naddress this problem but they tend to remain in simple manipulation of existing\ndata and are insufficient to cover the variability of the environments. We\npropose a method to extend a technique that has been used for transferring\nacoustic style textures between audio data. The method transfers audio\nsignatures between environments for distributed acoustic data augmentation.\nThis paper devises metrics to evaluate the generated acoustic data, based on\nclassification accuracy and content preservation. A series of experiments were\nconducted using UrbanSound8K dataset and the results show that the proposed\nmethod generates better audio data with transferred environmental features\nwhile preserving content features.", "published": "2021-01-06 04:27:24", "link": "http://arxiv.org/abs/2101.01863v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Interspeech 2021 Deep Noise Suppression Challenge", "abstract": "The Deep Noise Suppression (DNS) challenge is designed to foster innovation\nin the area of noise suppression to achieve superior perceptual speech quality.\nWe recently organized a DNS challenge special session at INTERSPEECH and ICASSP\n2020. We open-sourced training and test datasets for the wideband scenario. We\nalso open-sourced a subjective evaluation framework based on ITU-T standard\nP.808, which was also used to evaluate participants of the challenge. Many\nresearchers from academia and industry made significant contributions to push\nthe field forward, yet even the best noise suppressor was far from achieving\nsuperior speech quality in challenging scenarios. In this version of the\nchallenge organized at INTERSPEECH 2021, we are expanding both our training and\ntest datasets to accommodate full band scenarios. The two tracks in this\nchallenge will focus on real-time denoising for (i) wide band, and(ii) full\nband scenarios. We are also making available a reliable non-intrusive objective\nspeech quality metric called DNSMOS for the participants to use during their\ndevelopment phase.", "published": "2021-01-06 07:46:25", "link": "http://arxiv.org/abs/2101.01902v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Investigating the efficacy of music version retrieval systems for\n  setlist identification", "abstract": "The setlist identification (SLI) task addresses a music recognition use case\nwhere the goal is to retrieve the metadata and timestamps for all the tracks\nplayed in live music events. Due to various musical and non-musical changes in\nlive performances, developing automatic SLI systems is still a challenging task\nthat, despite its industrial relevance, has been under-explored in the academic\nliterature. In this paper, we propose an end-to-end workflow that identifies\nrelevant metadata and timestamps of live music performances using a version\nidentification system. We compare 3 of such systems to investigate their\nsuitability for this particular task. For developing and evaluating SLI\nsystems, we also contribute a new dataset that contains 99.5h of concerts with\nannotated metadata and timestamps, along with the corresponding reference set.\nThe dataset is categorized by audio qualities and genres to analyze the\nperformance of SLI systems in different use cases. Our approach can identify\n68% of the annotated segments, with values ranging from 35% to 77% based on the\ngenre. Finally, we evaluate our approach against a database of 56.8k songs to\nillustrate the effect of expanding the reference set, where we can still\nidentify 56% of the annotated segments.", "published": "2021-01-06 15:41:12", "link": "http://arxiv.org/abs/2101.02098v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
