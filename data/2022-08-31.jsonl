{"title": "The Glass Ceiling of Automatic Evaluation in Natural Language Generation", "abstract": "Automatic evaluation metrics capable of replacing human judgments are\ncritical to allowing fast development of new methods. Thus, numerous research\nefforts have focused on crafting such metrics. In this work, we take a step\nback and analyze recent progress by comparing the body of existing automatic\nmetrics and human metrics altogether. As metrics are used based on how they\nrank systems, we compare metrics in the space of system rankings. Our extensive\nstatistical analysis reveals surprising findings: automatic metrics -- old and\nnew -- are much more similar to each other than to humans. Automatic metrics\nare not complementary and rank systems similarly. Strikingly, human metrics\npredict each other much better than the combination of all automatic metrics\nused to predict a human metric. It is surprising because human metrics are\noften designed to be independent, to capture different aspects of quality, e.g.\ncontent fidelity or readability. We provide a discussion of these findings and\nrecommendations for future work in the field of evaluation.", "published": "2022-08-31 01:13:46", "link": "http://arxiv.org/abs/2208.14585v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unified Knowledge Prompt Pre-training for Customer Service Dialogues", "abstract": "Dialogue bots have been widely applied in customer service scenarios to\nprovide timely and user-friendly experience. These bots must classify the\nappropriate domain of a dialogue, understand the intent of users, and generate\nproper responses. Existing dialogue pre-training models are designed only for\nseveral dialogue tasks and ignore weakly-supervised expert knowledge in\ncustomer service dialogues. In this paper, we propose a novel unified knowledge\nprompt pre-training framework, UFA (\\textbf{U}nified Model \\textbf{F}or\n\\textbf{A}ll Tasks), for customer service dialogues. We formulate all the tasks\nof customer service dialogues as a unified text-to-text generation task and\nintroduce a knowledge-driven prompt strategy to jointly learn from a mixture of\ndistinct dialogue tasks. We pre-train UFA on a large-scale Chinese customer\nservice corpus collected from practical scenarios and get significant\nimprovements on both natural language understanding (NLU) and natural language\ngeneration (NLG) benchmarks.", "published": "2022-08-31 06:23:53", "link": "http://arxiv.org/abs/2208.14652v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-Shot Learning for Clinical Natural Language Processing Using Siamese\n  Neural Networks", "abstract": "Clinical Natural Language Processing (NLP) has become an emerging technology\nin healthcare that leverages a large amount of free-text data in electronic\nhealth records (EHRs) to improve patient care, support clinical decisions, and\nfacilitate clinical and translational science research. Recently, deep learning\nhas achieved state-of-the-art performance in many clinical NLP tasks. However,\ntraining deep learning models usually requires large annotated datasets, which\nare normally not publicly available and can be time-consuming to build in\nclinical domains. Working with smaller annotated datasets is typical in\nclinical NLP and therefore, ensuring that deep learning models perform well is\ncrucial for the models to be used in real-world applications. A widely adopted\napproach is fine-tuning existing Pre-trained Language Models (PLMs), but these\nattempts fall short when the training dataset contains only a few annotated\nsamples. Few-Shot Learning (FSL) has recently been investigated to tackle this\nproblem. Siamese Neural Network (SNN) has been widely utilized as an FSL\napproach in computer vision, but has not been studied well in NLP. Furthermore,\nthe literature on its applications in clinical domains is scarce. In this\npaper, we propose two SNN-based FSL approaches for clinical NLP, including\nPre-Trained SNN (PT-SNN) and SNN with Second-Order Embeddings (SOE-SNN). We\nevaluated the proposed approaches on two clinical tasks, namely clinical text\nclassification and clinical named entity recognition. We tested three few-shot\nsettings including 4-shot, 8-shot, and 16-shot learning. Both clinical NLP\ntasks were benchmarked using three PLMs, including BERT,BioBERT, and\nBioClinicalBERT. The experimental results verified the effectiveness of the\nproposed SNN-based FSL approaches in both NLP tasks.", "published": "2022-08-31 15:36:27", "link": "http://arxiv.org/abs/2208.14923v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Methods for Natural Language Processing: A Survey", "abstract": "Recent work in natural language processing (NLP) has yielded appealing\nresults from scaling model parameters and training data; however, using only\nscale to improve performance means that resource consumption also grows. Such\nresources include data, time, storage, or energy, all of which are naturally\nlimited and unevenly distributed. This motivates research into efficient\nmethods that require fewer resources to achieve similar results. This survey\nsynthesizes and relates current methods and findings in efficient NLP. We aim\nto provide both guidance for conducting NLP under limited resources, and point\ntowards promising research directions for developing more efficient methods.", "published": "2022-08-31 20:32:35", "link": "http://arxiv.org/abs/2209.00099v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tradeoffs in Resampling and Filtering for Imbalanced Classification", "abstract": "Imbalanced classification problems are extremely common in natural language\nprocessing and are solved using a variety of resampling and filtering\ntechniques, which often involve making decisions on how to select training data\nor decide which test examples should be labeled by the model. We examine the\ntradeoffs in model performance involved in choices of training sample and\nfilter training and test data in heavily imbalanced token classification task\nand examine the relationship between the magnitude of these tradeoffs and the\nbase rate of the phenomenon of interest. In experiments on sequence tagging to\ndetect rare phenomena in English and Arabic texts, we find that different\nmethods of selecting training data bring tradeoffs in effectiveness and\nefficiency. We also see that in highly imbalanced cases, filtering test data\nusing first-pass retrieval models is as important for model performance as\nselecting training data. The base rate of a rare positive class has a clear\neffect on the magnitude of the changes in performance caused by the selection\nof training or test data. As the base rate increases, the differences brought\nabout by those choices decreases.", "published": "2022-08-31 21:40:47", "link": "http://arxiv.org/abs/2209.00127v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Fellowship of the Authors: Disambiguating Names from Social Network\n  Context", "abstract": "Most NLP approaches to entity linking and coreference resolution focus on\nretrieving similar mentions using sparse or dense text representations. The\ncommon \"Wikification\" task, for instance, retrieves candidate Wikipedia\narticles for each entity mention. For many domains, such as bibliographic\ncitations, authority lists with extensive textual descriptions for each entity\nare lacking and ambiguous named entities mostly occur in the context of other\nnamed entities. Unlike prior work, therefore, we seek to leverage the\ninformation that can be gained from looking at association networks of\nindividuals derived from textual evidence in order to disambiguate names. We\ncombine BERT-based mention representations with a variety of graph induction\nstrategies and experiment with supervised and unsupervised cluster inference\nmethods. We experiment with data consisting of lists of names from two domains:\nbibliographic citations from CrossRef and chains of transmission (isnads) from\nclassical Arabic histories. We find that in-domain language model pretraining\ncan significantly improve mention representations, especially for larger\ncorpora, and that the availability of bibliographic information, such as\npublication venue or title, can also increase performance on this task. We also\npresent a novel supervised cluster inference model which gives competitive\nperformance for little computational effort, making it ideal for situations\nwhere individuals must be identified without relying on an exhaustive authority\nlist.", "published": "2022-08-31 21:51:55", "link": "http://arxiv.org/abs/2209.00133v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextualized language models for semantic change detection: lessons\n  learned", "abstract": "We present a qualitative analysis of the (potentially erroneous) outputs of\ncontextualized embedding-based methods for detecting diachronic semantic\nchange. First, we introduce an ensemble method outperforming previously\ndescribed contextualized approaches. This method is used as a basis for an\nin-depth analysis of the degrees of semantic change predicted for English words\nacross 5 decades. Our findings show that contextualized methods can often\npredict high change scores for words which are not undergoing any real\ndiachronic semantic shift in the lexicographic sense of the term (or at least\nthe status of these shifts is questionable). Such challenging cases are\ndiscussed in detail with examples, and their linguistic categorization is\nproposed. Our conclusion is that pre-trained contextualized language models are\nprone to confound changes in lexicographic senses and changes in contextual\nvariance, which naturally stem from their distributional nature, but is\ndifferent from the types of issues observed in methods based on static\nembeddings. Additionally, they often merge together syntactic and semantic\naspects of lexical entities. We propose a range of possible future solutions to\nthese issues.", "published": "2022-08-31 23:35:24", "link": "http://arxiv.org/abs/2209.00154v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Application of Data Encryption in Chinese Named Entity Recognition", "abstract": "Recently, with the continuous development of deep learning, the performance\nof named entity recognition tasks has been dramatically improved. However, the\nprivacy and the confidentiality of data in some specific fields, such as\nbiomedical and military, cause insufficient data to support the training of\ndeep neural networks. In this paper, we propose an encryption learning\nframework to address the problems of data leakage and inconvenient disclosure\nof sensitive data in certain domains. We introduce multiple encryption\nalgorithms to encrypt training data in the named entity recognition task for\nthe first time. In other words, we train the deep neural network using the\nencrypted data. We conduct experiments on six Chinese datasets, three of which\nare constructed by ourselves. The experimental results show that the encryption\nmethod achieves satisfactory results. The performance of some models trained\nwith encrypted data even exceeds the performance of the unencrypted method,\nwhich verifies the effectiveness of the introduced encryption method and solves\nthe problem of data leakage to a certain extent.", "published": "2022-08-31 04:20:37", "link": "http://arxiv.org/abs/2208.14627v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Generating Intermediate Steps for NLI with Next-Step Supervision", "abstract": "The Natural Language Inference (NLI) task often requires reasoning over\nmultiple steps to reach the conclusion. While the necessity of generating such\nintermediate steps (instead of a summary explanation) has gained popular\nsupport, it is unclear how to generate such steps without complete end-to-end\nsupervision and how such generated steps can be further utilized. In this work,\nwe train a sequence-to-sequence model to generate only the next step given an\nNLI premise and hypothesis pair (and previous steps); then enhance it with\nexternal knowledge and symbolic search to generate intermediate steps with only\nnext-step supervision. We show the correctness of such generated steps through\nautomated and human verification. Furthermore, we show that such generated\nsteps can help improve end-to-end NLI task performance using simple data\naugmentation strategies, across multiple public NLI datasets.", "published": "2022-08-31 05:25:33", "link": "http://arxiv.org/abs/2208.14641v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GRILLBot: An Assistant for Real-World Tasks with Neural Semantic Parsing\n  and Graph-Based Representations", "abstract": "GRILLBot is the winning system in the 2022 Alexa Prize TaskBot Challenge,\nmoving towards the next generation of multimodal task assistants. It is a voice\nassistant to guide users through complex real-world tasks in the domains of\ncooking and home improvement. These are long-running and complex tasks that\nrequire flexible adjustment and adaptation. The demo highlights the core\naspects, including a novel Neural Decision Parser for contextualized semantic\nparsing, a new \"TaskGraph\" state representation that supports conditional\nexecution, knowledge-grounded chit-chat, and automatic enrichment of tasks with\nimages and videos.", "published": "2022-08-31 14:24:35", "link": "http://arxiv.org/abs/2208.14884v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Incorporating Task-specific Concept Knowledge into Script Learning", "abstract": "In this paper, we present Tetris, a new task of Goal-Oriented Script\nCompletion. Unlike previous work, it considers a more realistic and general\nsetting, where the input includes not only the goal but also additional user\ncontext, including preferences and history. To address this problem, we propose\na novel approach, which uses two techniques to improve performance: (1) concept\nprompting, and (2) script-oriented contrastive learning that addresses step\nrepetition and hallucination problems. On our WikiHow-based dataset, we find\nthat both methods improve performance. The dataset, repository, and models will\nbe publicly available to facilitate further research on this new task.", "published": "2022-08-31 18:55:22", "link": "http://arxiv.org/abs/2209.00068v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Intelligent Traffic Monitoring with Hybrid AI", "abstract": "Challenges in Intelligent Traffic Monitoring (ITMo) are exacerbated by the\nlarge quantity and modalities of data and the need for the utilization of\nstate-of-the-art (SOTA) reasoners. We formulate the problem of ITMo and\nintroduce HANS, a neuro-symbolic architecture for multi-modal context\nunderstanding, and its application to ITMo. HANS utilizes knowledge graph\ntechnology to serve as a backbone for SOTA reasoning in the traffic domain.\nThrough case studies, we show how HANS addresses the challenges associated with\ntraffic monitoring while being able to integrate with a wide range of reasoning\nmethods", "published": "2022-08-31 17:47:22", "link": "http://arxiv.org/abs/2209.00448v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Efficient Sparsely Activated Transformers", "abstract": "Transformer-based neural networks have achieved state-of-the-art task\nperformance in a number of machine learning domains including natural language\nprocessing and computer vision. To further improve their accuracy, recent work\nhas explored the integration of dynamic behavior into these networks in the\nform of mixture-of-expert (MoE) layers. In this paper, we explore the\nintroduction of MoE layers to optimize a different metric: inference latency.\nWe introduce a novel system named PLANER that takes an existing\nTransformer-based network and a user-defined latency target and produces an\noptimized, sparsely-activated version of the original network that tries to\nmeet the latency target while maintaining baseline accuracy. We evaluate PLANER\non two real-world language modeling tasks using the Transformer-XL network and\nachieve inference latency reductions of over 2x at iso-accuracy.", "published": "2022-08-31 00:44:27", "link": "http://arxiv.org/abs/2208.14580v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Continuous QA Learning with Structured Prompts", "abstract": "QA models with lifelong learning (LL) abilities are important for practical\nQA applications, and architecture-based LL methods are reported to be an\neffective implementation for these models. However, it is non-trivial to extend\nprevious approaches to QA tasks since they either require access to task\nidentities in the testing phase or do not explicitly model samples from unseen\ntasks. In this paper, we propose Diana: a dynamic architecture-based lifelong\nQA model that tries to learn a sequence of QA tasks with a prompt enhanced\nlanguage model. Four types of hierarchically organized prompts are used in\nDiana to capture QA knowledge from different granularities. Specifically, we\ndedicate task-level prompts to capture task-specific knowledge to retain high\nLL performances and maintain instance-level prompts to learn knowledge shared\nacross different input samples to improve the model's generalization\nperformance. Moreover, we dedicate separate prompts to explicitly model unseen\ntasks and introduce a set of prompt key vectors to facilitate knowledge sharing\nbetween tasks. Extensive experiments demonstrate that Diana outperforms\nstate-of-the-art lifelong QA models, especially in handling unseen tasks.", "published": "2022-08-31 02:38:16", "link": "http://arxiv.org/abs/2208.14602v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "When Variable-Length Codes Meet the Field of Error Detection", "abstract": "Given a finite alphabet $A$ and a binary relation $\\tau\\subseteq A^*\\times\nA^*$, a set $X$ is $\\tau$-{\\it independent} if $ \\tau(X)\\cap X=\\emptyset$.\nGiven a quasi-metric $d$ over $A^*$ (in the meaning of \\cite{W31}) and $k\\ge\n1$, we associate the relation $\\tau_{d,k}$ defined by $(x,y)\\in\\tau_{d,k}$ if,\nand only if, $d(x,y)\\le k$ \\cite{CP02}.In the spirit of \\cite{JK97,N21}, the\nerror detection-correction capability of variable-length codes can be expressed\nin term of conditions over $\\tau_{d,k}$. With respect to the prefix metric, the\nfactor one, and every quasi-metric associated to (anti-)automorphisms of the\nfree monoid, we examine whether those conditions are decidable for a given\nregular code.", "published": "2022-08-31 08:14:28", "link": "http://arxiv.org/abs/2208.14681v2", "categories": ["cs.IT", "cs.CL", "cs.DM", "math.IT"], "primary_category": "cs.IT"}
{"title": "Hierarchical Local-Global Transformer for Temporal Sentence Grounding", "abstract": "This paper studies the multimedia problem of temporal sentence grounding\n(TSG), which aims to accurately determine the specific video segment in an\nuntrimmed video according to a given sentence query. Traditional TSG methods\nmainly follow the top-down or bottom-up framework and are not end-to-end. They\nseverely rely on time-consuming post-processing to refine the grounding\nresults. Recently, some transformer-based approaches are proposed to\nefficiently and effectively model the fine-grained semantic alignment between\nvideo and query. Although these methods achieve significant performance to some\nextent, they equally take frames of the video and words of the query as\ntransformer input for correlating, failing to capture their different levels of\ngranularity with distinct semantics. To address this issue, in this paper, we\npropose a novel Hierarchical Local-Global Transformer (HLGT) to leverage this\nhierarchy information and model the interactions between different levels of\ngranularity and different modalities for learning more fine-grained multi-modal\nrepresentations. Specifically, we first split the video and query into\nindividual clips and phrases to learn their local context (adjacent dependency)\nand global correlation (long-range dependency) via a temporal transformer.\nThen, a global-local transformer is introduced to learn the interactions\nbetween the local-level and global-level semantics for better multi-modal\nreasoning. Besides, we develop a new cross-modal cycle-consistency loss to\nenforce interaction between two modalities and encourage the semantic alignment\nbetween them. Finally, we design a brand-new cross-modal parallel transformer\ndecoder to integrate the encoded visual and textual features for final\ngrounding. Extensive experiments on three challenging datasets show that our\nproposed HLGT achieves a new state-of-the-art performance.", "published": "2022-08-31 14:16:56", "link": "http://arxiv.org/abs/2208.14882v1", "categories": ["cs.MM", "cs.CL", "cs.CV", "cs.IR"], "primary_category": "cs.MM"}
{"title": "Singing Beat Tracking With Self-supervised Front-end and Linear\n  Transformers", "abstract": "Tracking beats of singing voices without the presence of musical\naccompaniment can find many applications in music production, automatic song\narrangement, and social media interaction. Its main challenge is the lack of\nstrong rhythmic and harmonic patterns that are important for music rhythmic\nanalysis in general. Even for human listeners, this can be a challenging task.\nAs a result, existing music beat tracking systems fail to deliver satisfactory\nperformance on singing voices. In this paper, we propose singing beat tracking\nas a novel task, and propose the first approach to solving this task. Our\napproach leverages semantic information of singing voices by employing\npre-trained self-supervised WavLM and DistilHuBERT speech representations as\nthe front-end and uses a self-attention encoder layer to predict beats. To\ntrain and test the system, we obtain separated singing voices and their beat\nannotations using source separation and beat tracking on complete songs,\nfollowed by manual corrections. Experiments on the 741 separated vocal tracks\nof the GTZAN dataset show that the proposed system outperforms several\nstate-of-the-art music beat tracking methods by a large margin in terms of beat\ntracking accuracy. Ablation studies also confirm the advantages of pre-trained\nself-supervised speech representations over generic spectral features.", "published": "2022-08-31 00:29:39", "link": "http://arxiv.org/abs/2208.14578v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Domain Shift-oriented Machine Anomalous Sound Detection Model Based on\n  Self-Supervised Learning", "abstract": "Thanks to the development of deep learning, research on machine anomalous\nsound detection based on self-supervised learning has made remarkable\nachievements. However, there are differences in the acoustic characteristics of\nthe test set and the training set under different operating conditions of the\nsame machine (domain shifts). It is challenging for the existing detection\nmethods to learn the domain shifts features stably with low computation\noverhead. To address these problems, we propose a domain shift-oriented machine\nanomalous sound detection model based on self-supervised learning\n(TranSelf-DyGCN) in this paper. Firstly, we design a time-frequency domain\nfeature modeling network to capture global and local spatial and time-domain\nfeatures, thus improving the stability of machine anomalous sound detection\nstability under domain shifts. Then, we adopt a Dynamic Graph Convolutional\nNetwork (DyGCN) to model the inter-dependence relationship between domain\nshifts features, enabling the model to perceive domain shifts features\nefficiently. Finally, we use a Domain Adaptive Network (DAN) to compensate for\nthe performance decrease caused by domain shifts, making the model adapt to\nanomalous sound better in the self-supervised environment. The performance of\nthe suggested model is validated on DCASE 2020 task 2 and DCASE 2022 task 2.", "published": "2022-08-31 12:34:39", "link": "http://arxiv.org/abs/2208.14812v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Real-Time Tempo and Meter Tracking System for Rhythmic Improvis", "abstract": "Music is a form of expression that often requires interaction between\nplayers. If one wishes to interact in such a musical way with a computer, it is\nnecessary for the machine to be able to interpret the input given by the human\nto find its musical meaning. In this work, we propose a system capable of\ndetecting basic rhythmic features that can allow an application to synchronize\nits output with the rhythm given by the user, without having any prior\nagreement or requirement on the possible input. The system is described in\ndetail and an evaluation is given through simulation using quantitative\nmetrics. The evaluation shows that the system can detect tempo and meter\nconsistently under certain settings, and could be a solid base for further\ndevelopments leading to a system robust to rhythmically changing inputs.", "published": "2022-08-31 09:25:39", "link": "http://arxiv.org/abs/2208.14717v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Open Challenges in Musical Metacreation", "abstract": "Musical Metacreation tries to obtain creative behaviors from computers\nalgorithms composing music. In this paper I briefly analyze how this field\nevolved from algorithmic composition to be focused on the search for\ncreativity, and I point out some issues in pursuing this goal. Finally, I argue\nthat hybridization of algorithms can be a useful direction for research.", "published": "2022-08-31 09:34:27", "link": "http://arxiv.org/abs/2208.14734v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A New Corpus for Computational Music Research and A Novel Method for\n  Musical Structure Analysis", "abstract": "Computational models of music, while providing good descriptions of melodic\ndevelopment, still cannot fully grasp the general structure comprised of\nrepetitions, transpositions, and reuse of melodic material.\n  We present a corpus of strongly structured baroque allemandes, and describe a\ntop-down approach to abstract the shared structure of their musical content\nusing tree representations produced from pairwise differences between the\nSchenkerian-inspired analyses of each piece, thereby providing a rich\nhierarchical description of the corpus.", "published": "2022-08-31 09:49:10", "link": "http://arxiv.org/abs/2208.14747v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Harmonization and Evaluation; Tweaking the Parameters on Human Listeners", "abstract": "Kansei models were used to study the connotative meaning of music. In\nmultimedia and mixed reality, automatically generated melodies are increasingly\nbeing used. It is important to consider whether and what feelings are\ncommunicated by this music. Evaluation of computer-generated melodies is not a\ntrivial task. Considered the difficulty of defining useful quantitative metrics\nof the quality of a generated musical piece, researchers often resort to human\nevaluation. In these evaluations, often the judges are required to evaluate a\nset of generated pieces along with some benchmark pieces. The latter are often\ncomposed by humans. While this kind of evaluation is relatively common, it is\nknown that care should be taken when designing the experiment, as humans can be\ninfluenced by a variety of factors. In this paper, we examine the impact of the\npresence of harmony in audio files that judges must evaluate, to see whether\nhaving an accompaniment can change the evaluation of generated melodies. To do\nso, we generate melodies with two different algorithms and harmonize them with\nan automatic tool that we designed for this experiment, and ask more than sixty\nparticipants to evaluate the melodies. By using statistical analyses, we show\nharmonization does impact the evaluation process, by emphasizing the\ndifferences among judgements.", "published": "2022-08-31 09:54:06", "link": "http://arxiv.org/abs/2208.14750v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cadence Detection in Symbolic Classical Music using Graph Neural\n  Networks", "abstract": "Cadences are complex structures that have been driving music from the\nbeginning of contrapuntal polyphony until today. Detecting such structures is\nvital for numerous MIR tasks such as musicological analysis, key detection, or\nmusic segmentation. However, automatic cadence detection remains challenging\nmainly because it involves a combination of high-level musical elements like\nharmony, voice leading, and rhythm. In this work, we present a graph\nrepresentation of symbolic scores as an intermediate means to solve the cadence\ndetection task. We approach cadence detection as an imbalanced node\nclassification problem using a Graph Convolutional Network. We obtain results\nthat are roughly on par with the state of the art, and we present a model\ncapable of making predictions at multiple levels of granularity, from\nindividual notes to beats, thanks to the fine-grained, note-by-note\nrepresentation. Moreover, our experiments suggest that graph convolution can\nlearn non-local features that assist in cadence detection, freeing us from the\nneed of having to devise specialized features that encode non-local context. We\nargue that this general approach to modeling musical scores and classification\ntasks has a number of potential advantages, beyond the specific recognition\ntask presented here.", "published": "2022-08-31 12:39:57", "link": "http://arxiv.org/abs/2208.14819v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sketching the Expression: Flexible Rendering of Expressive Piano\n  Performance with Self-Supervised Learning", "abstract": "We propose a system for rendering a symbolic piano performance with flexible\nmusical expression. It is necessary to actively control musical expression for\ncreating a new music performance that conveys various emotions or nuances.\nHowever, previous approaches were limited to following the composer's\nguidelines of musical expression or dealing with only a part of the musical\nattributes. We aim to disentangle the entire musical expression and structural\nattribute of piano performance using a conditional VAE framework. It\nstochastically generates expressive parameters from latent representations and\ngiven note structures. In addition, we employ self-supervised approaches that\nforce the latent variables to represent target attributes. Finally, we leverage\na two-step encoder and decoder that learn hierarchical dependency to enhance\nthe naturalness of the output. Experimental results show that our system can\nstably generate performance parameters relevant to the given musical scores,\nlearn disentangled representations, and control musical attributes\nindependently of each other.", "published": "2022-08-31 13:48:28", "link": "http://arxiv.org/abs/2208.14867v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Evaluating generative audio systems and their metrics", "abstract": "Recent years have seen considerable advances in audio synthesis with deep\ngenerative models. However, the state-of-the-art is very difficult to quantify;\ndifferent studies often use different evaluation methodologies and different\nmetrics when reporting results, making a direct comparison to other systems\ndifficult if not impossible. Furthermore, the perceptual relevance and meaning\nof the reported metrics in most cases unknown, prohibiting any conclusive\ninsights with respect to practical usability and audio quality. This paper\npresents a study that investigates state-of-the-art approaches side-by-side\nwith (i) a set of previously proposed objective metrics for audio\nreconstruction, and with (ii) a listening study. The results indicate that\ncurrently used objective metrics are insufficient to describe the perceptual\nquality of current systems.", "published": "2022-08-31 21:48:34", "link": "http://arxiv.org/abs/2209.00130v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
