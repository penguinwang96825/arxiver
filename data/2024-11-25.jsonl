{"title": "Deep Learning-Based Electricity Price Forecast for Virtual Bidding in Wholesale Electricity Market", "abstract": "Virtual bidding plays an important role in two-settlement electric power\nmarkets, as it can reduce discrepancies between day-ahead and real-time\nmarkets. Renewable energy penetration increases volatility in electricity\nprices, making accurate forecasting critical for virtual bidders, reducing\nuncertainty and maximizing profits. This study presents a Transformer-based\ndeep learning model to forecast the price spread between real-time and\nday-ahead electricity prices in the ERCOT (Electric Reliability Council of\nTexas) market. The proposed model leverages various time-series features,\nincluding load forecasts, solar and wind generation forecasts, and temporal\nattributes. The model is trained under realistic constraints and validated\nusing a walk-forward approach by updating the model every week. Based on the\nprice spread prediction results, several trading strategies are proposed and\nthe most effective strategy for maximizing cumulative profit under realistic\nmarket conditions is identified through backtesting. The results show that the\nstrategy of trading only at the peak hour with a precision score of over 50%\nproduces nearly consistent profit over the test period. The proposed method\nunderscores the importance of an accurate electricity price forecasting model\nand introduces a new method of evaluating the price forecast model from a\nvirtual bidder's perspective, providing valuable insights for future research.", "published": "2024-11-25 20:04:16", "link": "http://arxiv.org/abs/2412.00062v1", "categories": ["cs.LG", "q-fin.CP"], "primary_category": "cs.LG"}
{"title": "Pricing Multi-strike Quanto Call Options on Multiple Assets with Stochastic Volatility, Correlation, and Exchange Rates", "abstract": "Quanto options allow the buyer to exchange the foreign currency payoff into\nthe domestic currency at a fixed exchange rate. We investigate quanto options\nwith multiple underlying assets valued in different foreign currencies each\nwith a different strike price in the payoff function. We carry out a\ncomparative performance analysis of different stochastic volatility (SV),\nstochastic correlation (SC), and stochastic exchange rate (SER) models to\ndetermine the best combination of these models for Monte Carlo (MC) simulation\npricing. In addition, we test the performance of all model variants with\nconstant correlation as a benchmark. We find that a combination of GARCH-Jump\nSV, Weibull SC, and Ornstein Uhlenbeck (OU) SER performs best. In addition, we\nanalyze different discretization schemes and their results. In our simulations,\nthe Milstein scheme yields the best balance between execution times and lower\nstandard deviations of price estimates. Furthermore, we find that incorporating\nmean reversion into stochastic correlation and stochastic FX rate modeling is\nbeneficial for MC simulation pricing. We improve the accuracy of our\nsimulations by implementing antithetic variates variance reduction. Finally, we\nderive the correlation risk parameters Cora and Gora in our framework so that\ncorrelation hedging of quanto options can be performed.", "published": "2024-11-25 17:56:10", "link": "http://arxiv.org/abs/2411.16617v1", "categories": ["q-fin.PR", "q-fin.CP", "q-fin.MF"], "primary_category": "q-fin.PR"}
{"title": "Predictive Power of LLMs in Financial Markets", "abstract": "Predicting the movement of the stock market and other assets has been\nvaluable over the past few decades. Knowing how the value of a certain sector\nmarket may move in the future provides much information for investors, as they\nuse that information to develop strategies to maximize profit or minimize risk.\nHowever, market data are quite noisy, and it is challenging to choose the right\ndata or the right model to create such predictions. With the rise of large\nlanguage models, there are ways to analyze certain data much more efficiently\nthan before.\n  Our goal is to determine whether the GPT model provides more useful\ninformation compared to other traditional transformer models, such as the BERT\nmodel. We shall use data from the Federal Reserve Beige Book, which provides\nsummaries of economic conditions in different districts in the US. Using such\ndata, we then employ the LLM's to make predictions on the correlations. Using\nthese correlations, we then compare the results with well-known strategies and\ndetermine whether knowing the economic conditions improves investment\ndecisions. We conclude that the Beige Book does contain information regarding\ncorrelations amongst different assets, yet the GPT model has too much\nlook-ahead bias and that traditional models still triumph.", "published": "2024-11-25 16:53:22", "link": "http://arxiv.org/abs/2411.16569v1", "categories": ["q-fin.PM", "q-fin.CP"], "primary_category": "q-fin.PM"}
{"title": "Do Activists Align with Larger Mutual Funds?", "abstract": "This paper demonstrates that hedge funds tend to design their activist\ncampaigns to align with the preferences and ideologies of institutions holding\nlarge stakes in the target company. I estimate these preferences by analyzing\nthe institutions' previous proxy voting behavior. The results reveal that\nactivists benefit from this approach. Campaigns with a stronger positive\ncorrelation between the preferences of larger institutions and activist\ncommunications attract more shareholder attention, receive more votes, and are\nmore likely to succeed.", "published": "2024-11-25 16:38:17", "link": "http://arxiv.org/abs/2411.16553v1", "categories": ["q-fin.CP", "q-fin.GN"], "primary_category": "q-fin.CP"}
{"title": "FinML-Chain: A Blockchain-Integrated Dataset for Enhanced Financial Machine Learning", "abstract": "Machine learning is critical for innovation and efficiency in financial\nmarkets, offering predictive models and data-driven decision-making. However,\nchallenges such as missing data, lack of transparency, untimely updates,\ninsecurity, and incompatible data sources limit its effectiveness. Blockchain\ntechnology, with its transparency, immutability, and real-time updates,\naddresses these challenges. We present a framework for integrating\nhigh-frequency on-chain data with low-frequency off-chain data, providing a\nbenchmark for addressing novel research questions in economic mechanism design.\nThis framework generates modular, extensible datasets for analyzing economic\nmechanisms such as the Transaction Fee Mechanism, enabling multi-modal insights\nand fairness-driven evaluations. Using four machine learning techniques,\nincluding linear regression, deep neural networks, XGBoost, and LSTM models, we\ndemonstrate the framework's ability to produce datasets that advance financial\nresearch and improve understanding of blockchain-driven systems. Our\ncontributions include: (1) proposing a research scenario for the Transaction\nFee Mechanism and demonstrating how the framework addresses previously\nunexplored questions in economic mechanism design; (2) providing a benchmark\nfor financial machine learning by open-sourcing a sample dataset generated by\nthe framework and the code for the pipeline, enabling continuous dataset\nexpansion; and (3) promoting reproducibility, transparency, and collaboration\nby fully open-sourcing the framework and its outputs. This initiative supports\nresearchers in extending our work and developing innovative financial\nmachine-learning models, fostering advancements at the intersection of machine\nlearning, blockchain, and economics.", "published": "2024-11-25 10:55:11", "link": "http://arxiv.org/abs/2411.16277v1", "categories": ["econ.GN", "cs.CE", "cs.CR", "q-fin.CP", "q-fin.EC", "stat.ML"], "primary_category": "econ.GN"}
{"title": "AD-HOC: A C++ Expression Template package for high-order derivatives backpropagation", "abstract": "This document presents a new C++ Automatic Differentiation (AD) tool, AD-HOC\n(Automatic Differentiation for High-Order Calculations). This tool aims to have\nthe following features: -Calculation of user specified derivatives of arbitrary\norder -To be able to run with similar speeds as handwritten code -All\nderivatives calculations are computed in a single backpropagation tree pass -No\nsource code generation is used, relying heavily on the C++ compiler to\nstatically build the computation tree before runtime -A simple interface -The\nability to be used \\textit{in conjunction} with other established,\ngeneral-purpose dynamic AD tools -Header-only library, with no external\ndependencies -Open source, with a business-friendly license", "published": "2024-11-25 10:24:29", "link": "http://arxiv.org/abs/2412.05300v2", "categories": ["cs.MS", "q-fin.CP"], "primary_category": "cs.MS"}
{"title": "CatNet: Effective FDR Control in LSTM with Gaussian Mirrors and SHAP Feature Importance", "abstract": "We introduce CatNet, an algorithm that effectively controls False Discovery\nRate (FDR) and selects significant features in LSTM with the Gaussian Mirror\n(GM) method. To evaluate the feature importance of LSTM in time series, we\nintroduce a vector of the derivative of the SHapley Additive exPlanations\n(SHAP) to measure feature importance. We also propose a new kernel-based\ndependence measure to avoid multicollinearity in the GM algorithm, to make a\nrobust feature selection with controlled FDR. We use simulated data to evaluate\nCatNet's performance in both linear models and LSTM models with different link\nfunctions. The algorithm effectively controls the FDR while maintaining a high\nstatistical power in all cases. We also evaluate the algorithm's performance in\ndifferent low-dimensional and high-dimensional cases, demonstrating its\nrobustness in various input dimensions. To evaluate CatNet's performance in\nreal world applications, we construct a multi-factor investment portfolio to\nforecast the prices of S\\&P 500 index components. The results demonstrate that\nour model achieves superior predictive accuracy compared to traditional LSTM\nmodels without feature selection and FDR control. Additionally, CatNet\neffectively captures common market-driving features, which helps informed\ndecision-making in financial markets by enhancing the interpretability of\npredictions. Our study integrates of the Gaussian Mirror algorithm with LSTM\nmodels for the first time, and introduces SHAP values as a new feature\nimportance metric for FDR control methods, marking a significant advancement in\nfeature selection and error control for neural networks.", "published": "2024-11-25 18:53:37", "link": "http://arxiv.org/abs/2411.16666v2", "categories": ["stat.ML", "cs.AI", "cs.LG", "q-fin.ST"], "primary_category": "stat.ML"}
{"title": "What events matter for exchange rate volatility ?", "abstract": "This paper expands on stochastic volatility models by proposing a data-driven\nmethod to select the macroeconomic events most likely to impact volatility. The\npaper identifies and quantifies the effects of macroeconomic events across\nmultiple countries on exchange rate volatility using high-frequency currency\nreturns, while accounting for persistent stochastic volatility effects and\nseasonal components capturing time-of-day patterns. Given the hundreds of\nmacroeconomic announcements and their lags, we rely on sparsity-based methods\nto select relevant events for the model. We contribute to the exchange rate\nliterature in four ways: First, we identify the macroeconomic events that drive\ncurrency volatility, estimate their effects and connect them to macroeconomic\nfundamentals. Second, we find a link between intraday seasonality, trading\nvolume, and the opening hours of major markets across the globe. We provide a\nsimple labor-based explanation for this observed pattern. Third, we show that\nincluding macroeconomic events and seasonal components is crucial for\nforecasting exchange rate volatility. Fourth, our proposed model yields the\nlowest volatility and highest Sharpe ratio in portfolio allocations when\ncompared to standard SV and GARCH models.", "published": "2024-11-25 10:01:07", "link": "http://arxiv.org/abs/2411.16244v1", "categories": ["q-fin.ST", "econ.EM"], "primary_category": "q-fin.ST"}
{"title": "MarketGPT: Developing a Pre-trained transformer (GPT) for Modeling Financial Time Series", "abstract": "This work presents a generative pre-trained transformer (GPT) designed for\nmodeling financial time series. The GPT functions as an order generation engine\nwithin a discrete event simulator, enabling realistic replication of limit\norder book dynamics. Our model leverages recent advancements in large language\nmodels to produce long sequences of order messages in a steaming manner. Our\nresults demonstrate that the model successfully reproduces key features of\norder flow data, even when the initial order flow prompt is no longer present\nwithin the model's context window. Moreover, evaluations reveal that the model\ncaptures several statistical properties, or 'stylized facts', characteristic of\nreal financial markets and broader macro-scale data distributions.\nCollectively, this work marks a significant step toward creating high-fidelity,\ninteractive market simulations.", "published": "2024-11-25 17:15:28", "link": "http://arxiv.org/abs/2411.16585v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "TransCompressor: LLM-Powered Multimodal Data Compression for Smart\n  Transportation", "abstract": "The incorporation of Large Language Models (LLMs) into smart transportation\nsystems has paved the way for improving data management and operational\nefficiency. This study introduces TransCompressor, a novel framework that\nleverages LLMs for efficient compression and decompression of multimodal\ntransportation sensor data. TransCompressor has undergone thorough evaluation\nwith diverse sensor data types, including barometer, speed, and altitude\nmeasurements, across various transportation modes like buses, taxis, and MTRs.\nComprehensive evaluation illustrates the effectiveness of TransCompressor in\nreconstructing transportation sensor data at different compression ratios. The\nresults highlight that, with well-crafted prompts, LLMs can utilize their vast\nknowledge base to contribute to data compression processes, enhancing data\nstorage, analysis, and retrieval in smart transportation settings.", "published": "2024-11-25 00:32:20", "link": "http://arxiv.org/abs/2411.16020v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MH-MoE: Multi-Head Mixture-of-Experts", "abstract": "Multi-Head Mixture-of-Experts (MH-MoE) demonstrates superior performance by\nusing the multi-head mechanism to collectively attend to information from\nvarious representation spaces within different experts. In this paper, we\npresent a novel implementation of MH-MoE that maintains both FLOPs and\nparameter parity with sparse Mixture of Experts models. Experimental results on\nlanguage models show that the new implementation yields quality improvements\nover both vanilla MoE and fine-grained MoE models. Additionally, our\nexperiments demonstrate that MH-MoE is compatible with 1-bit Large Language\nModels (LLMs) such as BitNet.", "published": "2024-11-25 09:05:36", "link": "http://arxiv.org/abs/2411.16205v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NormXLogit: The Head-on-Top Never Lies", "abstract": "The Transformer architecture has emerged as the dominant choice for building\nlarge language models (LLMs). However, with new LLMs emerging on a frequent\nbasis, it is important to consider the potential value of architecture-agnostic\napproaches that can provide interpretability across a variety of architectures.\nDespite recent successes in the interpretability of LLMs, many existing\napproaches rely on complex methods that are often tied to a specific model\ndesign and come with a significant computational cost. To address these\nlimitations, we propose a novel technique, called NormXLogit, for assessing the\nsignificance of individual input tokens. This method operates based on the\ninput and output representations associated with each token. First, we\ndemonstrate that during the pre-training of LLMs, the norms of word embeddings\ncapture the importance of input tokens. Second, we reveal a significant\nrelationship between a token's importance and the extent to which its\nrepresentation can resemble the model's final prediction. Through extensive\nanalysis, we show that our approach consistently outperforms existing\ngradient-based methods in terms of faithfulness. Additionally, our method\nachieves better performance in layer-wise explanations compared to the most\nprominent architecture-specific methods.", "published": "2024-11-25 10:12:27", "link": "http://arxiv.org/abs/2411.16252v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Preference Optimization for Reasoning with Pseudo Feedback", "abstract": "Preference optimization techniques, such as Direct Preference Optimization\n(DPO), are frequently employed to enhance the reasoning capabilities of large\nlanguage models (LLMs) in domains like mathematical reasoning and coding,\ntypically following supervised fine-tuning. These methods rely on high-quality\nlabels for reasoning tasks to generate preference pairs; however, the\navailability of reasoning datasets with human-verified labels is limited. In\nthis study, we introduce a novel approach to generate pseudo feedback for\nreasoning tasks by framing the labeling of solutions to reason problems as an\nevaluation against associated test cases. We explore two forms of pseudo\nfeedback based on test cases: one generated by frontier LLMs and the other by\nextending self-consistency to multi-test-case. We conduct experiments on both\nmathematical reasoning and coding tasks using pseudo feedback for preference\noptimization, and observe improvements across both tasks. Specifically, using\nMathstral-7B as our base model, we improve MATH results from 58.3 to 68.6,\nsurpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and\nCollege Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3,\nrespectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.6 on\nLiveCodeBench (from 21.1), surpassing Claude-3-Haiku.", "published": "2024-11-25 12:44:02", "link": "http://arxiv.org/abs/2411.16345v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-modal Retrieval Augmented Multi-modal Generation: Datasets,\n  Evaluation Metrics and Strong Baselines", "abstract": "We present a systematic investigation of Multi-modal Retrieval Augmented\nMulti-modal Generation (M$^2$RAG), a novel task that enables foundation models\nto process multi-modal web content and generate multi-modal responses, which\nexhibits better information density and readability. Despite its potential\nimpact, M$^2$RAG remains understudied, lacking comprehensive analysis and\nhigh-quality data resources. To address this gap, we establish a comprehensive\nbenchmark through a rigorous data curation pipeline, and employ text-modal\nmetrics and multi-modal metrics based on foundation models for evaluation. We\nfurther propose several strategies for foundation models to process M$^2$RAG\neffectively and construct a training set by filtering high-quality samples\nusing designed metrics. Our extensive experiments demonstrate the reliability\nof our proposed metrics, a landscape of model performance within our designed\nstrategies, and show that our fine-tuned 7B-8B models outperform the\nstate-of-the-art GPT-4o model. Additionally, we perform fine-grained analyses\nacross diverse domains and validate the effectiveness of our designs in data\ncuration pipeline. All resources, including codes, datasets, and model weights,\nwill be publicly released.", "published": "2024-11-25 13:20:19", "link": "http://arxiv.org/abs/2411.16365v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finding Structure in Language Models", "abstract": "When we speak, write or listen, we continuously make predictions based on our\nknowledge of a language's grammar. Remarkably, children acquire this\ngrammatical knowledge within just a few years, enabling them to understand and\ngeneralise to novel constructions that have never been uttered before. Language\nmodels are powerful tools that create representations of language by\nincrementally predicting the next word in a sentence, and they have had a\ntremendous societal impact in recent years. The central research question of\nthis thesis is whether these models possess a deep understanding of grammatical\nstructure similar to that of humans. This question lies at the intersection of\nnatural language processing, linguistics, and interpretability. To address it,\nwe will develop novel interpretability techniques that enhance our\nunderstanding of the complex nature of large-scale language models. We approach\nour research question from three directions. First, we explore the presence of\nabstract linguistic information through structural priming, a key paradigm in\npsycholinguistics for uncovering grammatical structure in human language\nprocessing. Next, we examine various linguistic phenomena, such as adjective\norder and negative polarity items, and connect a model's comprehension of these\nphenomena to the data distribution on which it was trained. Finally, we\nintroduce a controlled testbed for studying hierarchical structure in language\nmodels using various synthetic languages of increasing complexity and examine\nthe role of feature interactions in modelling this structure. Our findings\noffer a detailed account of the grammatical knowledge embedded in language\nmodel representations and provide several directions for investigating\nfundamental linguistic questions using computational methods.", "published": "2024-11-25 14:37:24", "link": "http://arxiv.org/abs/2411.16433v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning by Analogy: Enhancing Few-Shot Prompting for Math Word Problem\n  Solving with Computational Graph-Based Retrieval", "abstract": "Large language models (LLMs) are known to struggle with complicated reasoning\ntasks such as math word problems (MWPs). In this paper, we present how analogy\nfrom similarly structured questions can improve LLMs' problem-solving\ncapabilities for MWPs. Specifically, we rely on the retrieval of problems with\nsimilar computational graphs to the given question to serve as exemplars in the\nprompt, providing the correct reasoning path for the generation model to refer\nto. Empirical results across six math word problem datasets demonstrate the\neffectiveness of our proposed method, which achieves a significant improvement\nof up to 6.7 percent on average in absolute value, compared to baseline\nmethods. These results highlight our method's potential in addressing the\nreasoning challenges in current LLMs.", "published": "2024-11-25 15:01:25", "link": "http://arxiv.org/abs/2411.16454v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous\n  Knowledge Reasoning", "abstract": "Despite the outstanding capabilities of large language models (LLMs),\nknowledge-intensive reasoning still remains a challenging task due to LLMs'\nlimitations in compositional reasoning and the hallucination problem. A\nprevalent solution is to employ chain-of-thought (CoT) with retrieval-augmented\ngeneration (RAG), which first formulates a reasoning plan by decomposing\ncomplex questions into simpler sub-questions, and then applies iterative RAG at\neach sub-question. However, prior works exhibit two crucial problems:\ninadequate reasoning planning and poor incorporation of heterogeneous\nknowledge. In this paper, we introduce AtomR, a framework for LLMs to conduct\naccurate heterogeneous knowledge reasoning at the atomic level. Inspired by how\nknowledge graph query languages model compositional reasoning through combining\npredefined operations, we propose three atomic knowledge operators, a unified\nset of operators for LLMs to retrieve and manipulate knowledge from\nheterogeneous sources. First, in the reasoning planning stage, AtomR decomposes\na complex question into a reasoning tree where each leaf node corresponds to an\natomic knowledge operator, achieving question decomposition that is highly\nfine-grained and orthogonal. Subsequently, in the reasoning execution stage,\nAtomR executes each atomic knowledge operator, which flexibly selects,\nretrieves, and operates atomic level knowledge from heterogeneous sources. We\nalso introduce BlendQA, a challenging benchmark specially tailored for\nheterogeneous knowledge reasoning. Experiments on three single-source and two\nmulti-source datasets show that AtomR outperforms state-of-the-art baselines by\na large margin, with F1 score improvements of 9.4% on 2WikiMultihop and 9.5% on\nBlendQA. We release our code and datasets.", "published": "2024-11-25 15:35:51", "link": "http://arxiv.org/abs/2411.16495v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Profiling Bias in LLMs: Stereotype Dimensions in Contextual Word\n  Embeddings", "abstract": "Large language models (LLMs) are the foundation of the current successes of\nartificial intelligence (AI), however, they are unavoidably biased. To\neffectively communicate the risks and encourage mitigation efforts these models\nneed adequate and intuitive descriptions of their discriminatory properties,\nappropriate for all audiences of AI. We suggest bias profiles with respect to\nstereotype dimensions based on dictionaries from social psychology research.\nAlong these dimensions we investigate gender bias in contextual embeddings,\nacross contexts and layers, and generate stereotype profiles for twelve\ndifferent LLMs, demonstrating their intuition and use case for exposing and\nvisualizing bias.", "published": "2024-11-25 16:14:45", "link": "http://arxiv.org/abs/2411.16527v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recent Trends in Linear Text Segmentation: a Survey", "abstract": "Linear Text Segmentation is the task of automatically tagging text documents\nwith topic shifts, i.e. the places in the text where the topics change. A\nwell-established area of research in Natural Language Processing, drawing from\nwell-understood concepts in linguistic and computational linguistic research,\nthe field has recently seen a lot of interest as a result of the surge of text,\nvideo, and audio available on the web, which in turn require ways of\nsummarising and categorizing the mole of content for which linear text\nsegmentation is a fundamental step. In this survey, we provide an extensive\noverview of current advances in linear text segmentation, describing the state\nof the art in terms of resources and approaches for the task. Finally, we\nhighlight the limitations of available resources and of the task itself, while\nindicating ways forward based on the most recent literature and under-explored\nresearch directions.", "published": "2024-11-25 17:48:59", "link": "http://arxiv.org/abs/2411.16613v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "StructFormer: Document Structure-based Masked Attention and its Impact\n  on Language Model Pre-Training", "abstract": "Most state-of-the-art techniques for Language Models (LMs) today rely on\ntransformer-based architectures and their ubiquitous attention mechanism.\nHowever, the exponential growth in computational requirements with longer input\nsequences confines Transformers to handling short passages. Recent efforts have\naimed to address this limitation by introducing selective attention mechanisms,\nnotably local and global attention. While sparse attention mechanisms, akin to\nfull attention in being Turing-complete, have been theoretically established,\ntheir practical impact on pre-training remains unexplored. This study focuses\non empirically assessing the influence of global attention on BERT\npre-training. The primary steps involve creating an extensive corpus of\nstructure-aware text through arXiv data, alongside a text-only counterpart. We\ncarry out pre-training on these two datasets, investigate shifts in attention\npatterns, and assess their implications for downstream tasks. Our analysis\nunderscores the significance of incorporating document structure into LM\nmodels, demonstrating their capacity to excel in more abstract tasks, such as\ndocument understanding.", "published": "2024-11-25 17:57:52", "link": "http://arxiv.org/abs/2411.16618v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Large Language Models Perform Latent Multi-Hop Reasoning without\n  Exploiting Shortcuts?", "abstract": "We evaluate how well Large Language Models (LLMs) latently recall and compose\nfacts to answer multi-hop queries like \"In the year Scarlett Johansson was\nborn, the Summer Olympics were hosted in the country of\". One major challenge\nin evaluating this ability is that LLMs may have developed shortcuts by\nencounters of the head entity \"Scarlett Johansson\" and the answer entity\n\"United States\" in the same training sequences or merely guess the answer based\non frequency-based priors. To prevent shortcuts, we exclude test queries where\nthe head and answer entities co-appear in pretraining corpora. Through careful\nselection of relations and facts and systematic removal of cases where models\nmight guess answers or exploit partial matches, we construct an evaluation\ndataset SOCRATES (ShOrtCut-fRee lATent rEaSoning). We observe that LLMs\ndemonstrate promising latent multi-hop reasoning abilities without exploiting\nshortcuts, but only for certain types of queries. For queries requiring latent\nrecall of countries as the intermediate answer, the best models achieve 80%\nlatent composability, but this drops to just 5% for the recall of years.\nComparisons with Chain-of-Thought composability highlight a significant gap\nbetween the ability of models to reason latently versus explicitly. Analysis\nreveals that latent representations of the intermediate answer are constructed\nmore often in queries with higher latent composability, and shows the emergence\nof latent multi-hop reasoning during pretraining.", "published": "2024-11-25 18:59:30", "link": "http://arxiv.org/abs/2411.16679v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Self-Distillation via Previous Mini-batches for Fine-tuning\n  Small Language Models", "abstract": "Knowledge distillation (KD) has become a widely adopted approach for\ncompressing large language models (LLMs) to reduce computational costs and\nmemory footprints. However, the availability of complex teacher models is a\nprerequisite for running most KD pipelines. Thus, the traditional KD procedure\ncan be unachievable or budget-unfriendly, particularly when relying on\ncommercial LLMs like GPT4. In this regard, Self-distillation (SelfD) emerges as\nan advisable alternative, enabling student models to learn without teachers'\nguidance. Nonetheless, existing SelfD approaches for LMs often involve\narchitectural modifications, assuming the models are open-source, which may not\nalways be practical. In this work, we introduce a model-agnostic and\ntask-agnostic method named dynamic SelfD from the previous minibatch (DynSDPB),\nwhich realizes current iterations' distillation from the last ones' generated\nlogits. Additionally, to address prediction inaccuracies during the early\niterations, we dynamically adjust the distillation influence and temperature\nvalues to enhance the adaptability of fine-tuning. Furthermore, DynSDPB is a\nnovel fine-tuning policy that facilitates the seamless integration of existing\nself-correction and self-training techniques for small language models (SLMs)\nbecause they all require updating SLMs' parameters. We demonstrate the superior\nperformance of DynSDPB on both encoder-only LMs (e.g., BERT model families) and\ndecoder-only LMs (e.g., LLaMA model families), validating its effectiveness\nacross natural language understanding (NLU) and natural language generation\n(NLG) benchmarks.", "published": "2024-11-25 23:37:48", "link": "http://arxiv.org/abs/2411.16991v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tree Transformers are an Ineffective Model of Syntactic Constituency", "abstract": "Linguists have long held that a key aspect of natural language syntax is the\nrecursive organization of language units into constituent structures, and\nresearch has suggested that current state-of-the-art language models lack an\ninherent bias towards this feature. A number of alternative models have been\nproposed to provide inductive biases towards constituency, including the Tree\nTransformer, which utilizes a modified attention mechanism to organize tokens\ninto constituents.\n  We investigate Tree Transformers to study whether they utilize meaningful\nand/or useful constituent structures. We pretrain a large Tree Transformer on\nlanguage modeling in order to investigate the learned constituent tree\nrepresentations of sentences, finding little evidence for meaningful\nstructures. Next, we evaluate Tree Transformers with similar transformer models\non error detection tasks requiring constituent structure. We find that while\nthe Tree Transformer models may slightly outperform at these tasks, there is\nlittle evidence to suggest a meaningful improvement. In general, we conclude\nthat there is little evidence to support Tree Transformer as an effective model\nof syntactic constituency.", "published": "2024-11-25 23:53:46", "link": "http://arxiv.org/abs/2411.16993v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Emergent Capabilities by Finetuning", "abstract": "A fundamental open challenge in modern LLM scaling is the lack of\nunderstanding around emergent capabilities. In particular, language model\npretraining loss is known to be highly predictable as a function of compute.\nHowever, downstream capabilities are far less predictable -- sometimes even\nexhibiting emergent jumps -- which makes it challenging to anticipate the\ncapabilities of future models. In this work, we first pose the task of\nemergence prediction: given access to current LLMs that have random few-shot\naccuracy on a task, can we predict whether future models (GPT-N+1) will have\nnon-trivial accuracy on that task? We then discover a simple insight for this\nproblem: finetuning LLMs on a given task can shift the point in scaling at\nwhich emergence occurs towards less capable models. To operationalize this\ninsight, we can finetune LLMs with varying amounts of data and fit a parametric\nfunction that predicts when emergence will occur (i.e., \"emergence laws\"). We\nvalidate this approach using four standard NLP benchmarks where large-scale\nopen-source LLMs already demonstrate emergence (MMLU, GSM8K, CommonsenseQA, and\nCoLA). Using only small-scale LLMs, we find that, in some cases, we can\naccurately predict whether models trained with up to 4x more compute have\nemerged. Finally, we present a case study of two realistic uses for emergence\nprediction.", "published": "2024-11-25 01:48:09", "link": "http://arxiv.org/abs/2411.16035v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SAGEval: The frontiers of Satisfactory Agent based NLG Evaluation for\n  reference-free open-ended text", "abstract": "Large Language Model (LLM) integrations into applications like Microsoft365\nsuite and Google Workspace for creating/processing documents, emails,\npresentations, etc. has led to considerable enhancements in productivity and\ntime savings. But as these integrations become more more complex, it is\nparamount to ensure that the quality of output from the LLM-integrated\napplications are relevant and appropriate for use. Identifying the need to\ndevelop robust evaluation approaches for natural language generation, wherein\nreferences/ground labels doesn't exist or isn't amply available, this paper\nintroduces a novel framework called \"SAGEval\" which utilizes a critiquing Agent\nto provide feedback on scores generated by LLM evaluators. We show that the\ncritiquing Agent is able to rectify scores from LLM evaluators, in absence of\nreferences/ground-truth labels, thereby reducing the need for labeled data even\nfor complex NLG evaluation scenarios, like the generation of JSON-structured\nforms/surveys with responses in different styles like multiple choice, likert\nratings, single choice questions, etc.", "published": "2024-11-25 04:07:16", "link": "http://arxiv.org/abs/2411.16077v1", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL"}
{"title": "LLM Augmentations to support Analytical Reasoning over Multiple\n  Documents", "abstract": "Building on their demonstrated ability to perform a variety of tasks, we\ninvestigate the application of large language models (LLMs) to enhance in-depth\nanalytical reasoning within the context of intelligence analysis. Intelligence\nanalysts typically work with massive dossiers to draw connections between\nseemingly unrelated entities, and uncover adversaries' plans and motives. We\nexplore if and how LLMs can be helpful to analysts for this task and develop an\narchitecture to augment the capabilities of an LLM with a memory module called\ndynamic evidence trees (DETs) to develop and track multiple investigation\nthreads. Through extensive experiments on multiple datasets, we highlight how\nLLMs, as-is, are still inadequate to support intelligence analysts and offer\nrecommendations to improve LLMs for such intricate reasoning applications.", "published": "2024-11-25 06:00:42", "link": "http://arxiv.org/abs/2411.16116v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DoubleCCA: Improving Foundation Model Group Robustness with Random\n  Sentence Embeddings", "abstract": "This paper presents a novel method to improve the robustness of foundation\nmodels to group-based biases. We propose a simple yet effective method, called\nDoubleCCA, that leverages random sentences and Canonical Correlation Analysis\n(CCA) to enrich the text embeddings of the foundation model. First, we generate\nvarious random sentences that augment the original prompts, which extends the\noriginal prompts with random words or character sequences. Second, we use an\nadditional sentence embedding model to generate different text embeddings with\nrespect to these random sentences. We then use CCA double twice to align the\nrepresentations and reconstruct them back to the original representation space.\nWe demonstrate the effectiveness of our method on a variety of tasks and\ndatasets, showing that it outperforms existing methods in terms of both\nperformance and robustness. Our method is simple to implement and can be easily\nintegrated into existing models, making it a practical solution for improving\nthe robustness of foundation models to group-based biases.", "published": "2024-11-25 09:52:28", "link": "http://arxiv.org/abs/2411.16236v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Transparent Neighborhood Approximation for Text Classifier Explanation", "abstract": "Recent literature highlights the critical role of neighborhood construction\nin deriving model-agnostic explanations, with a growing trend toward deploying\ngenerative models to improve synthetic instance quality, especially for\nexplaining text classifiers. These approaches overcome the challenges in\nneighborhood construction posed by the unstructured nature of texts, thereby\nimproving the quality of explanations. However, the deployed generators are\nusually implemented via neural networks and lack inherent explainability,\nsparking arguments over the transparency of the explanation process itself. To\naddress this limitation while preserving neighborhood quality, this paper\nintroduces a probability-based editing method as an alternative to black-box\ntext generators. This approach generates neighboring texts by implementing\nmanipulations based on in-text contexts. Substituting the generator-based\nconstruction process with recursive probability-based editing, the resultant\nexplanation method, XPROB (explainer with probability-based editing), exhibits\ncompetitive performance according to the evaluation conducted on two real-world\ndatasets. Additionally, XPROB's fully transparent and more controllable\nconstruction process leads to superior stability compared to the\ngenerator-based explainers.", "published": "2024-11-25 10:10:09", "link": "http://arxiv.org/abs/2411.16251v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unraveling Arithmetic in Large Language Models: The Role of Algebraic\n  Structures", "abstract": "The reasoning abilities of large language models (LLMs) have improved with\nchain-of-thought (CoT) prompting, allowing models to solve complex tasks\nstepwise. However, training CoT capabilities requires detailed reasoning data,\nwhich is often scarce. The self-taught reasoner (STaR) framework addresses this\nby using reinforcement learning to automatically generate reasoning steps,\nreducing reliance on human-labeled data. Although STaR and its variants have\ndemonstrated empirical success, a theoretical foundation explaining these\nimprovements is lacking. Large language models (LLMs) have demonstrated\nremarkable mathematical capabilities, largely driven by chain-of-thought (CoT)\nprompting, which decomposes complex reasoning into step-by-step solutions.\nHowever, the mechanisms underlying LLMs' ability to perform arithmetic in a\nsingle step of CoT remain poorly understood. In this work, we propose that LLMs\nlearn arithmetic by capturing algebraic structures, such as commutativity and\nidentity properties. Since these structures are observable through input-output\nrelationships, they can generalize to unseen data. We empirically demonstrate\nthat LLMs can learn algebraic structures using a custom dataset of arithmetic\nproblems, as well as providing theoretical evidence showing that, under\nspecific configurations of weights and biases, the transformer-based LLMs can\ngenerate embeddings that remain invariant to both permutations of input tokens\nand the presence of identity elements. Our findings indicate that leveraging\nalgebraic structures can enhance the LLMs' arithmetic capabilities, offering\ninsights into improving their arithmetic performance.", "published": "2024-11-25 10:23:11", "link": "http://arxiv.org/abs/2411.16260v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "BayLing 2: A Multilingual Large Language Model with Efficient Language\n  Alignment", "abstract": "Large language models (LLMs), with their powerful generative capabilities and\nvast knowledge, empower various tasks in everyday life. However, these\nabilities are primarily concentrated in high-resource languages, leaving\nlow-resource languages with weaker generative capabilities and relatively\nlimited knowledge. Enhancing the multilingual capabilities of LLMs is therefore\ncrucial for serving over 100 linguistic communities worldwide. An intuitive\napproach to enhance the multilingual capabilities would be to construct\ninstruction data for various languages, but constructing instruction data for\nover 100 languages is prohibitively costly. In this paper, we introduce BayLing\n2, which efficiently transfers generative capabilities and knowledge from\nhigh-resource languages to low-resource languages through language alignment.\nTo achieve this, we constructed a dataset of 3.2 million instructions,\ncomprising high-resource language instructions (Chinese and English) and\ncross-lingual instructions for 100+ languages and performed instruction tuning\nbased on the dataset to facilitate the capability transfer between languages.\nUsing Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B,\nand BayLing-2-8B, and conducted a comprehensive evaluation of BayLing. For\nmultilingual translation across 100+ languages, BayLing shows superior\nperformance compared to open-source models of similar scale. For multilingual\nknowledge and understanding benchmarks, BayLing achieves significant\nimprovements across over 20 low-resource languages, demonstrating its\ncapability of effective knowledge transfer from high-resource to low-resource\nlanguages. Furthermore, results on English benchmarks indicate that BayLing\nmaintains high performance in highresource languages while enhancing the\nperformance in low-resource languages. Demo, homepage, code and models of\nBayLing are available.", "published": "2024-11-25 11:35:08", "link": "http://arxiv.org/abs/2411.16300v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Two-Hop Curse: LLMs trained on A$\\rightarrow$B, B$\\rightarrow$C fail\n  to learn A$\\rightarrow$C", "abstract": "[Notice: This version is outdated. Recent research contradicts some key\nclaims; we are working on a major revision with more nuanced analysis. Please\nwait for the updated version.]\n  While LLMs excel at multi-hop questions (e.g. \"Who is the spouse of the\nperformer of Imagine?\") when using chain-of-thought reasoning (CoT), they\nstruggle when forced to reason internally (without CoT). Previous work on the\nsize and nature of this gap produced mixed evidence with inconclusive results.\nIn this paper, we introduce a controlled setting for investigating two-hop\nreasoning in LLMs, where the above-chance performance constitutes undeniable\nevidence for latent reasoning. We fine-tune LLMs (including Llama 3 8B Instruct\nand GPT-4o) on fictional facts and confirm that they generalize to answering\ntwo-hop questions about them using CoT. We find that models can perform latent\nreasoning when facts appear together during training or in the prompt. However,\nto our surprise, models completely fail at two-hop reasoning without CoT when\nlearned facts only appear in different documents, achieving chance-level\naccuracy and chance-level test loss. We call this complete failure to compose\nseparately learned facts the Two-Hop Curse. Moreover, we evaluate 9 frontier\nLLMs on real-world facts, finding that models completely fail at two-hop no-CoT\nreasoning for over half of question categories while maintaining partial\nsuccess with CoT across most categories. These results suggest that LLMs lack a\ngeneral capability for latent multi-hop reasoning independent of the question\ntype.", "published": "2024-11-25 13:04:28", "link": "http://arxiv.org/abs/2411.16353v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FineWeb-zhtw: Scalable Curation of Traditional Chinese Text Data from\n  the Web", "abstract": "The quality and size of a pretraining dataset significantly influence the\nperformance of large language models (LLMs). While there have been numerous\nefforts in the curation of such a dataset for English users, there is a\nrelative lack of similar initiatives for Traditional Chinese. Building upon\nthis foundation of FineWeb, we introduce FineWeb-zhtw, a dataset tailored\nspecifically for Traditional Chinese users. We came up with multiple stages of\nmeticulously designed filters to cater to the linguistic difference between\nEnglish and Traditional Chinese, to ensure comprehensiveness and quality. We\ndetermined effectiveness from querying dataset samples with three main\nobjectives. Our code and datasets are publicly available.", "published": "2024-11-25 13:49:45", "link": "http://arxiv.org/abs/2411.16387v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Human-Calibrated Automated Testing and Validation of Generative Language\n  Models", "abstract": "This paper introduces a comprehensive framework for the evaluation and\nvalidation of generative language models (GLMs), with a focus on\nRetrieval-Augmented Generation (RAG) systems deployed in high-stakes domains\nsuch as banking. GLM evaluation is challenging due to open-ended outputs and\nsubjective quality assessments. Leveraging the structured nature of RAG\nsystems, where generated responses are grounded in a predefined document\ncollection, we propose the Human-Calibrated Automated Testing (HCAT) framework.\nHCAT integrates a) automated test generation using stratified sampling, b)\nembedding-based metrics for explainable assessment of functionality, risk and\nsafety attributes, and c) a two-stage calibration approach that aligns\nmachine-generated evaluations with human judgments through probability\ncalibration and conformal prediction.\n  In addition, the framework includes robustness testing to evaluate model\nperformance against adversarial, out-of-distribution, and varied input\nconditions, as well as targeted weakness identification using marginal and\nbivariate analysis to pinpoint specific areas for improvement. This\nhuman-calibrated, multi-layered evaluation framework offers a scalable,\ntransparent, and interpretable approach to GLM assessment, providing a\npractical and reliable solution for deploying GLMs in applications where\naccuracy, transparency, and regulatory compliance are paramount.", "published": "2024-11-25 13:53:36", "link": "http://arxiv.org/abs/2411.16391v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adapter-based Approaches to Knowledge-enhanced Language Models -- A\n  Survey", "abstract": "Knowledge-enhanced language models (KELMs) have emerged as promising tools to\nbridge the gap between large-scale language models and domain-specific\nknowledge. KELMs can achieve higher factual accuracy and mitigate\nhallucinations by leveraging knowledge graphs (KGs). They are frequently\ncombined with adapter modules to reduce the computational load and risk of\ncatastrophic forgetting. In this paper, we conduct a systematic literature\nreview (SLR) on adapter-based approaches to KELMs. We provide a structured\noverview of existing methodologies in the field through quantitative and\nqualitative analysis and explore the strengths and potential shortcomings of\nindividual approaches. We show that general knowledge and domain-specific\napproaches have been frequently explored along with various adapter\narchitectures and downstream tasks. We particularly focused on the popular\nbiomedical domain, where we provided an insightful performance comparison of\nexisting KELMs. We outline the main trends and propose promising future\ndirections.", "published": "2024-11-25 14:10:24", "link": "http://arxiv.org/abs/2411.16403v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "When Babies Teach Babies: Can student knowledge sharing outperform\n  Teacher-Guided Distillation on small datasets?", "abstract": "We present our submission to the BabyLM challenge, aiming to push the\nboundaries of data-efficient language model pretraining. Our method builds upon\ndeep mutual learning, introducing a student model search for diverse\ninitialization. We address the limitation of treating students equally by\nformulating weighted mutual learning as a bi-level optimization problem. The\ninner loop learns compact students through online distillation, while the outer\nloop optimizes weights for better knowledge distillation from diverse students.\nThis dynamic weighting strategy eliminates the need for a teacher model,\nreducing computational requirements. Our evaluations show that teacher-less\nmethods can match or surpass teacher-supervised approaches.", "published": "2024-11-25 15:25:31", "link": "http://arxiv.org/abs/2411.16487v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple\n  Distillation, Big Progress or Bitter Lesson?", "abstract": "This paper presents a critical examination of current approaches to\nreplicating OpenAI's O1 model capabilities, with particular focus on the\nwidespread but often undisclosed use of knowledge distillation techniques.\nWhile our previous work explored the fundamental technical path to O1\nreplication, this study reveals how simple distillation from O1's API, combined\nwith supervised fine-tuning, can achieve superior performance on complex\nmathematical reasoning tasks. Through extensive experiments, we show that a\nbase model fine-tuned on simply tens of thousands of samples O1-distilled\nlong-thought chains outperforms O1-preview on the American Invitational\nMathematics Examination (AIME) with minimal technical complexity. Moreover, our\ninvestigation extends beyond mathematical reasoning to explore the\ngeneralization capabilities of O1-distilled models across diverse tasks:\nhallucination, safety and open-domain QA. Notably, despite training only on\nmathematical problem-solving data, our models demonstrated strong\ngeneralization to open-ended QA tasks and became significantly less susceptible\nto sycophancy after fine-tuning. We deliberately make this finding public to\npromote transparency in AI research and to challenge the current trend of\nobscured technical claims in the field. Our work includes: (1) A detailed\ntechnical exposition of the distillation process and its effectiveness, (2) A\ncomprehensive benchmark framework for evaluating and categorizing O1\nreplication attempts based on their technical transparency and reproducibility,\n(3) A critical discussion of the limitations and potential risks of\nover-relying on distillation approaches, our analysis culminates in a crucial\nbitter lesson: while the pursuit of more capable AI systems is important, the\ndevelopment of researchers grounded in first-principles thinking is paramount.", "published": "2024-11-25 15:31:27", "link": "http://arxiv.org/abs/2411.16489v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "All Languages Matter: Evaluating LMMs on Culturally Diverse 100\n  Languages", "abstract": "Existing Large Multimodal Models (LMMs) generally focus on only a few regions\nand languages. As LMMs continue to improve, it is increasingly important to\nensure they understand cultural contexts, respect local sensitivities, and\nsupport low-resource languages, all while effectively integrating corresponding\nvisual cues. In pursuit of culturally diverse global multimodal models, our\nproposed All Languages Matter Benchmark (ALM-bench) represents the largest and\nmost comprehensive effort to date for evaluating LMMs across 100 languages.\nALM-bench challenges existing models by testing their ability to understand and\nreason about culturally diverse images paired with text in various languages,\nincluding many low-resource languages traditionally underrepresented in LMM\nresearch. The benchmark offers a robust and nuanced evaluation framework\nfeaturing various question formats, including true/false, multiple choice, and\nopen-ended questions, which are further divided into short and long-answer\ncategories. ALM-bench design ensures a comprehensive assessment of a model's\nability to handle varied levels of difficulty in visual and linguistic\nreasoning. To capture the rich tapestry of global cultures, ALM-bench carefully\ncurates content from 13 distinct cultural aspects, ranging from traditions and\nrituals to famous personalities and celebrations. Through this, ALM-bench not\nonly provides a rigorous testing ground for state-of-the-art open and\nclosed-source LMMs but also highlights the importance of cultural and\nlinguistic inclusivity, encouraging the development of models that can serve\ndiverse global populations effectively. Our benchmark is publicly available.", "published": "2024-11-25 15:44:42", "link": "http://arxiv.org/abs/2411.16508v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "LaB-RAG: Label Boosted Retrieval Augmented Generation for Radiology\n  Report Generation", "abstract": "In the current paradigm of image captioning, deep learning models are trained\nto generate text from image embeddings of latent features. We challenge the\nassumption that these latent features ought to be high-dimensional vectors\nwhich require model fine tuning to handle. Here we propose Label Boosted\nRetrieval Augmented Generation (LaB-RAG), a text-based approach to image\ncaptioning that leverages image descriptors in the form of categorical labels\nto boost standard retrieval augmented generation (RAG) with pretrained large\nlanguage models (LLMs). We study our method in the context of radiology report\ngeneration (RRG), where the task is to generate a clinician's report detailing\ntheir observations from a set of radiological images, such as X-rays. We argue\nthat simple linear classifiers over extracted image embeddings can effectively\ntransform X-rays into text-space as radiology-specific labels. In combination\nwith standard RAG, we show that these derived text labels can be used with\ngeneral-domain LLMs to generate radiology reports. Without ever training our\ngenerative language model or image feature encoder models, and without ever\ndirectly \"showing\" the LLM an X-ray, we demonstrate that LaB-RAG achieves\nbetter results across natural language and radiology language metrics compared\nwith other retrieval-based RRG methods, while attaining competitive results\ncompared to other fine-tuned vision-language RRG models. We further present\nresults of our experiments with various components of LaB-RAG to better\nunderstand our method. Finally, we critique the use of a popular RRG metric,\narguing it is possible to artificially inflate its results without true\ndata-leakage.", "published": "2024-11-25 16:10:05", "link": "http://arxiv.org/abs/2411.16523v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "EnStack: An Ensemble Stacking Framework of Large Language Models for\n  Enhanced Vulnerability Detection in Source Code", "abstract": "Automated detection of software vulnerabilities is critical for enhancing\nsecurity, yet existing methods often struggle with the complexity and diversity\nof modern codebases. In this paper, we introduce EnStack, a novel ensemble\nstacking framework that enhances vulnerability detection using natural language\nprocessing (NLP) techniques. Our approach synergizes multiple pre-trained large\nlanguage models (LLMs) specialized in code understanding CodeBERT for semantic\nanalysis, GraphCodeBERT for structural representation, and UniXcoder for\ncross-modal capabilities. By fine-tuning these models on the Draper VDISC\ndataset and integrating their outputs through meta-classifiers such as Logistic\nRegression, Support Vector Machines (SVM), Random Forest, and XGBoost, EnStack\neffectively captures intricate code patterns and vulnerabilities that\nindividual models may overlook. The meta-classifiers consolidate the strengths\nof each LLM, resulting in a comprehensive model that excels in detecting subtle\nand complex vulnerabilities across diverse programming contexts. Experimental\nresults demonstrate that EnStack significantly outperforms existing methods,\nachieving notable improvements in accuracy, precision, recall, and F1-score.\nThis work highlights the potential of ensemble LLM approaches in code analysis\ntasks and offers valuable insights into applying NLP techniques for advancing\nautomated vulnerability detection.", "published": "2024-11-25 16:47:10", "link": "http://arxiv.org/abs/2411.16561v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "From Generation to Judgment: Opportunities and Challenges of\n  LLM-as-a-judge", "abstract": "Assessment and evaluation have long been critical challenges in artificial\nintelligence (AI) and natural language processing (NLP). However, traditional\nmethods, whether matching-based or embedding-based, often fall short of judging\nsubtle attributes and delivering satisfactory results. Recent advancements in\nLarge Language Models (LLMs) inspire the \"LLM-as-a-judge\" paradigm, where LLMs\nare leveraged to perform scoring, ranking, or selection across various tasks\nand applications. This paper provides a comprehensive survey of LLM-based\njudgment and assessment, offering an in-depth overview to advance this emerging\nfield. We begin by giving detailed definitions from both input and output\nperspectives. Then we introduce a comprehensive taxonomy to explore\nLLM-as-a-judge from three dimensions: what to judge, how to judge and where to\njudge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and\nhighlight key challenges and promising directions, aiming to provide valuable\ninsights and inspire future research in this promising research area. Paper\nlist and more resources about LLM-as-a-judge can be found at\nhttps://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge and\nhttps://llm-as-a-judge.github.io.", "published": "2024-11-25 17:28:44", "link": "http://arxiv.org/abs/2411.16594v6", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Do Automatic Factuality Metrics Measure Factuality? A Critical\n  Evaluation", "abstract": "Modern LLMs can now produce highly readable abstractive summaries, to the\npoint where traditional automated metrics for evaluating summary quality, such\nas ROUGE, have become saturated. However, LLMs still sometimes introduce\nunwanted content into summaries, i.e., information inconsistent with or\nunsupported by their source. Measuring the occurrence of these often subtle\n``hallucinations'' automatically has proved to be challenging. This in turn has\nmotivated development of a variety of metrics intended to measure the factual\nconsistency of generated summaries against their source. But are these\napproaches measuring what they purport to do? In this work, we stress-test\nautomatic factuality metrics. Specifically, we investigate whether and to what\ndegree superficial attributes of summary texts suffice to predict\n``factuality'', finding that a (supervised) model using only such shallow\nfeatures is reasonably competitive with SOTA factuality scoring methods. We\nthen evaluate how factuality metrics respond to factual corrections in\ninconsistent summaries and find that only a few show meaningful improvements.\nIn contrast, some metrics are more sensitive to benign, non-factual edits.\nMotivated by these insights, we show that one can ``game'' (most) automatic\nfactuality metrics, i.e., reliably inflate ``factuality'' scores by appending\ninnocuous sentences to generated summaries. Taken together, our results raise\nquestions about the degree to which we should rely on existing automated\nfactuality metrics and what exactly we want ``factuality metrics'' to measure.", "published": "2024-11-25 18:15:15", "link": "http://arxiv.org/abs/2411.16638v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Preventing Jailbreak Prompts as Malicious Tools for Cybercriminals: A\n  Cyber Defense Perspective", "abstract": "Jailbreak prompts pose a significant threat in AI and cybersecurity, as they\nare crafted to bypass ethical safeguards in large language models, potentially\nenabling misuse by cybercriminals. This paper analyzes jailbreak prompts from a\ncyber defense perspective, exploring techniques like prompt injection and\ncontext manipulation that allow harmful content generation, content filter\nevasion, and sensitive information extraction. We assess the impact of\nsuccessful jailbreaks, from misinformation and automated social engineering to\nhazardous content creation, including bioweapons and explosives. To address\nthese threats, we propose strategies involving advanced prompt analysis,\ndynamic safety protocols, and continuous model fine-tuning to strengthen AI\nresilience. Additionally, we highlight the need for collaboration among AI\nresearchers, cybersecurity experts, and policymakers to set standards for\nprotecting AI systems. Through case studies, we illustrate these cyber defense\napproaches, promoting responsible AI practices to maintain system integrity and\npublic trust. \\textbf{\\color{red}Warning: This paper contains content which the\nreader may find offensive.}", "published": "2024-11-25 18:23:58", "link": "http://arxiv.org/abs/2411.16642v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "SHuBERT: Self-Supervised Sign Language Representation Learning via\n  Multi-Stream Cluster Prediction", "abstract": "Sign language processing has traditionally relied on task-specific\nmodels,limiting the potential for transfer learning across tasks. We introduce\nSHuBERT (Sign Hidden-Unit BERT), a self-supervised transformer encoder that\nlearns strong representations from approximately 1,000 hours of American Sign\nLanguage (ASL) video content. Inspired by the success of the HuBERT speech\nrepresentation model, SHuBERT adapts masked prediction for multi-stream visual\nsign language input, learning to predict multiple targets for corresponding to\nclustered hand, face, and body pose streams. SHuBERT achieves state-of-the-art\nperformance across multiple benchmarks. On sign language translation, it\noutperforms prior methods trained on publicly available data on the How2Sign\n(+0.7 BLEU), OpenASL (+10.0 BLEU), and FLEURS-ASL (+0.3 BLEU) benchmarks.\nSimilarly for isolated sign language recognition, SHuBERT's accuracy surpasses\nthat of specialized models on ASL-Citizen (+5\\%) and SEM-LEX (+20.6\\%), while\ncoming close to them on WLASL2000 (-3\\%). Ablation studies confirm the\ncontribution of each component of the approach.", "published": "2024-11-25 03:13:08", "link": "http://arxiv.org/abs/2411.16765v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Contrastive Multi-graph Learning with Neighbor Hierarchical Sifting for\n  Semi-supervised Text Classification", "abstract": "Graph contrastive learning has been successfully applied in text\nclassification due to its remarkable ability for self-supervised node\nrepresentation learning. However, explicit graph augmentations may lead to a\nloss of semantics in the contrastive views. Secondly, existing methods tend to\noverlook edge features and the varying significance of node features during\nmulti-graph learning. Moreover, the contrastive loss suffer from false\nnegatives. To address these limitations, we propose a novel method of\ncontrastive multi-graph learning with neighbor hierarchical sifting for\nsemi-supervised text classification, namely ConNHS. Specifically, we exploit\ncore features to form a multi-relational text graph, enhancing semantic\nconnections among texts. By separating text graphs, we provide diverse views\nfor contrastive learning. Our approach ensures optimal preservation of the\ngraph information, minimizing data loss and distortion. Then, we separately\nexecute relation-aware propagation and cross-graph attention propagation, which\neffectively leverages the varying correlations between nodes and edge features\nwhile harmonising the information fusion across graphs. Subsequently, we\npresent the neighbor hierarchical sifting loss (NHS) to refine the negative\nselection. For one thing, following the homophily assumption, NHS masks\nfirst-order neighbors of the anchor and positives from being negatives. For\nanother, NHS excludes the high-order neighbors analogous to the anchor based on\ntheir similarities. Consequently, it effectively reduces the occurrence of\nfalse negatives, preventing the expansion of the distance between similar\nsamples in the embedding space. Our experiments on ThuCNews, SogouNews, 20\nNewsgroups, and Ohsumed datasets achieved 95.86\\%, 97.52\\%, 87.43\\%, and\n70.65\\%, which demonstrates competitive results in semi-supervised text\nclassification.", "published": "2024-11-25 08:35:55", "link": "http://arxiv.org/abs/2411.16787v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Leveraging the Power of MLLMs for Gloss-Free Sign Language Translation", "abstract": "Sign language translation (SLT) is a challenging task that involves\ntranslating sign language images into spoken language. For SLT models to\nperform this task successfully, they must bridge the modality gap and identify\nsubtle variations in sign language components to understand their meanings\naccurately. To address these challenges, we propose a novel gloss-free SLT\nframework called Multimodal Sign Language Translation (MMSLT), which leverages\nthe representational capabilities of off-the-shelf multimodal large language\nmodels (MLLMs). Specifically, we generate detailed textual descriptions of sign\nlanguage components using MLLMs. Then, through our proposed multimodal-language\npre-training module, we integrate these description features with sign video\nfeatures to align them within the spoken sentence space. Our approach achieves\nstate-of-the-art performance on benchmark datasets PHOENIX14T and CSL-Daily,\nhighlighting the potential of MLLMs to be effectively utilized in SLT.", "published": "2024-11-25 09:01:41", "link": "http://arxiv.org/abs/2411.16789v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Enhancing Answer Reliability Through Inter-Model Consensus of Large\n  Language Models", "abstract": "We propose a collaborative framework in which multiple large language models\n-- including GPT-4-0125-preview, Meta-LLaMA-3-70B-Instruct, Claude-3-Opus, and\nGemini-1.5-Flash -- generate and answer complex, PhD-level statistical\nquestions when definitive ground truth is unavailable. Our study examines how\ninter-model consensus improves both response reliability and identifies the\nquality of the generated questions. Employing chi-square tests, Fleiss' Kappa,\nand confidence interval analysis, we quantify consensus rates and inter-rater\nagreement to assess both response precision and question quality. Key results\nindicate that Claude and GPT-4 produce well-structured, less ambiguous\nquestions with a higher inter-rater agreement, as shown by narrower confidence\nintervals and greater alignment with question-generating models. In contrast,\nGemini and LLaMA exhibit greater variability and lower reliability in question\nformulation. These findings demonstrate that collaborative interactions among\nlarge language models enhance response reliability and provide valuable\ninsights for optimizing AI-driven collaborative reasoning systems.", "published": "2024-11-25 10:18:17", "link": "http://arxiv.org/abs/2411.16797v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning LLMs with Noisy Data for Political Argument Generation and\n  Post Guidance", "abstract": "The incivility in social media discourse complicates the deployment of\nautomated text generation models for politically sensitive content. Fine-tuning\nand prompting strategies are critical, but underexplored, solutions to mitigate\ntoxicity in such contexts. This study investigates the fine-tuning and\nprompting effects on GPT-3.5 Turbo using subsets of the CLAPTON dataset of\npolitical discussion posts, comprising Twitter and Reddit data labeled for\ntheir justification, reciprocity and incivility. Fine-tuned models on Reddit\ndata scored highest on discussion quality, while combined noisy data led to\npersistent toxicity. Prompting strategies reduced specific toxic traits, such\nas personal attacks, but had limited broader impact. The findings emphasize\nthat high-quality data and well-crafted prompts are essential to reduce\nincivility and improve rhetorical quality in automated political discourse\ngeneration.", "published": "2024-11-25 15:28:11", "link": "http://arxiv.org/abs/2411.16813v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KL-geodesics flow matching with a novel sampling scheme", "abstract": "Non-autoregressive language models generate all tokens simultaneously,\noffering potential speed advantages over traditional autoregressive models, but\nthey face challenges in modeling the complex dependencies inherent in text\ndata. In this work, we investigate a conditional flow matching approach for\ntext generation. We represent tokens as one-hot vectors in a \\(V\\)-dimensional\nsimplex and utilize geodesics under the Kullback-Leibler (KL) divergence, which\ncorrespond to linear interpolation in logit space. We provide a theoretical\njustification that maximizing the conditional likelihood \\(P_{\\theta}(x_1 \\mid\nx_t, t)\\) yields the exact flow matching velocity under logit interpolation. To\naddress the suboptimal performance of basic inference, we propose a novel\nempirical sampling scheme that iteratively samples from the conditional\ndistribution and introduces additional noise, significantly improving results\ndespite lacking full theoretical underpinnings. Furthermore, we propose a\nhybrid inference method that combines the basic approach with the sampling\nscheme. This method demonstrates superior performance on both conditional and\nunconditional text generation experiments compared to previous SOTA method for\ndiscrete flow matching.", "published": "2024-11-25 17:15:41", "link": "http://arxiv.org/abs/2411.16821v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Boundless Socratic Learning with Language Games", "abstract": "An agent trained within a closed system can master any desired capability, as\nlong as the following three conditions hold: (a) it receives sufficiently\ninformative and aligned feedback, (b) its coverage of experience/data is broad\nenough, and (c) it has sufficient capacity and resource. In this position\npaper, we justify these conditions, and consider what limitations arise from\n(a) and (b) in closed systems, when assuming that (c) is not a bottleneck.\nConsidering the special case of agents with matching input and output spaces\n(namely, language), we argue that such pure recursive self-improvement, dubbed\n\"Socratic learning\", can boost performance vastly beyond what is present in its\ninitial data or knowledge, and is only limited by time, as well as gradual\nmisalignment concerns. Furthermore, we propose a constructive framework to\nimplement it, based on the notion of language games.", "published": "2024-11-25 20:16:16", "link": "http://arxiv.org/abs/2411.16905v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Harnessing LLMs for Educational Content-Driven Italian Crossword\n  Generation", "abstract": "In this work, we unveil a novel tool for generating Italian crossword puzzles\nfrom text, utilizing advanced language models such as GPT-4o,\nMistral-7B-Instruct-v0.3, and Llama3-8b-Instruct. Crafted specifically for\neducational applications, this cutting-edge generator makes use of the\ncomprehensive Italian-Clue-Instruct dataset, which comprises over 30,000\nentries including diverse text, solutions, and types of clues. This carefully\nassembled dataset is designed to facilitate the creation of contextually\nrelevant clues in various styles associated with specific texts and keywords.\nThe study delves into four distinctive styles of crossword clues: those without\nformat constraints, those formed as definite determiner phrases, copular\nsentences, and bare noun phrases. Each style introduces unique linguistic\nstructures to diversify clue presentation. Given the lack of sophisticated\neducational tools tailored to the Italian language, this project seeks to\nenhance learning experiences and cognitive development through an engaging,\ninteractive platform. By meshing state-of-the-art AI with contemporary\neducational strategies, our tool can dynamically generate crossword puzzles\nfrom Italian educational materials, thereby providing an enjoyable and\ninteractive learning environment. This technological advancement not only\nredefines educational paradigms but also sets a new benchmark for interactive\nand cognitive language learning solutions.", "published": "2024-11-25 21:13:25", "link": "http://arxiv.org/abs/2411.16936v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Teaching Smaller Language Models To Generalise To Unseen Compositional\n  Questions (Full Thesis)", "abstract": "Pretrained large Language Models (LLMs) are able to answer questions that are\nunlikely to have been encountered during training. However a diversity of\npotential applications exist in the broad domain of reasoning systems and\nconsiderations such as latency, cost, available compute resource and internet\nconnectivity are relevant in determining an appropriate approach. We consider\nthe setting where some local compute capacity is available at inference time\nbut internet connectivity is not.\n  Similar to a general-purpose LLM, we assume that our much smaller Reasoning\nModels may be asked arbitrary questions from unknown distributions, so we focus\non evaluation in an unseen setting. We train our models to answer diverse\nquestions by instilling an ability to reason over a retrieved context. We\nacquire context from two knowledge sources; a Wikipedia corpus queried using a\nmulti-hop dense retrieval system with novel extensions, and from rationales\ngenerated from a larger Language Model optimised to run in a lower resource\nenvironment.\n  Our main contributions: We propose novel methods to show that our model is\ncapable of answering contextualised questions without memorisation. We\nestablish a comprehensive set of baseline results on unseen evaluation\ndatasets. We show that the addition of novel retrieval-augmented training\ndatasets (RATD) to the training regime of the Reasoning Model significantly\nimproves results. We demonstrate further significant improvement through the\napplication of methods for combining knowledge from two sources. The first\nmethod (RR) involves training a novel Rationale Ranking model to score both\ngenerated rationales and retrieved contexts with respect to relevance and\ntruthfulness. We use the scores to derive combined contexts. We also show that\nutilising the RATD datasets enables our model to become proficient at utilising\ncombined noisy contexts.", "published": "2024-11-25 23:25:34", "link": "http://arxiv.org/abs/2411.16985v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adaptive Circuit Behavior and Generalization in Mechanistic\n  Interpretability", "abstract": "Mechanistic interpretability aims to understand the inner workings of large\nneural networks by identifying circuits, or minimal subgraphs within the model\nthat implement algorithms responsible for performing specific tasks. These\ncircuits are typically discovered and analyzed using a narrowly defined prompt\nformat. However, given the abilities of large language models (LLMs) to\ngeneralize across various prompt formats for the same task, it remains unclear\nhow well these circuits generalize. For instance, it is unclear whether the\nmodels generalization results from reusing the same circuit components, the\ncomponents behaving differently, or the use of entirely different components.\nIn this paper, we investigate the generality of the indirect object\nidentification (IOI) circuit in GPT-2 small, which is well-studied and believed\nto implement a simple, interpretable algorithm. We evaluate its performance on\nprompt variants that challenge the assumptions of this algorithm. Our findings\nreveal that the circuit generalizes surprisingly well, reusing all of its\ncomponents and mechanisms while only adding additional input edges. Notably,\nthe circuit generalizes even to prompt variants where the original algorithm\nshould fail; we discover a mechanism that explains this which we term S2\nHacking. Our findings indicate that circuits within LLMs may be more flexible\nand general than previously recognized, underscoring the importance of studying\ncircuit generalization to better understand the broader capabilities of these\nmodels.", "published": "2024-11-25 05:32:34", "link": "http://arxiv.org/abs/2411.16105v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Enhancing Multi-Agent Consensus through Third-Party LLM Integration:\n  Analyzing Uncertainty and Mitigating Hallucinations in Large Language Models", "abstract": "Large Language Models (LLMs) still face challenges when dealing with complex\nreasoning tasks, often resulting in hallucinations, which limit the practical\napplication of LLMs. To alleviate this issue, this paper proposes a new method\nthat integrates different LLMs to expand the knowledge boundary, reduce\ndependence on a single model, and promote in-depth debate among agents. The\nmain contributions include: 1) Introducing third-party LLMs to adjust the\nattention weights of agents through uncertainty estimation and confidence\nanalysis, optimizing consensus formation in multi-agent systems; 2) Experiments\non arithmetic datasets have validated the effectiveness of the method,\nsurpassing traditional multi-agent baselines. This research provides a new\nperspective for large models to alleviate hallucination phenomena when dealing\nwith complex tasks.", "published": "2024-11-25 08:42:33", "link": "http://arxiv.org/abs/2411.16189v1", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Video-Text Dataset Construction from Multi-AI Feedback: Promoting\n  Weak-to-Strong Preference Learning for Video Large Language Models", "abstract": "High-quality video-text preference data is crucial for Multimodal Large\nLanguage Models (MLLMs) alignment. However, existing preference data is very\nscarce. Obtaining VQA preference data for preference training is costly, and\nmanually annotating responses is highly unreliable, which could result in\nlow-quality pairs. Meanwhile, AI-generated responses controlled by temperature\nadjustment lack diversity. To address these issues, we propose a high-quality\nVQA preference dataset, called \\textit{\\textbf{M}ultiple \\textbf{M}ultimodal\n\\textbf{A}rtificial \\textbf{I}ntelligence \\textbf{P}reference Datasets in\n\\textbf{V}QA} (\\textbf{MMAIP-V}), which is constructed by sampling from the\nresponse distribution set and using an external scoring function for response\nevaluation. Furthermore, to fully leverage the preference knowledge in MMAIP-V\nand ensure sufficient optimization, we propose \\textit{\\textbf{Iter}ative\n\\textbf{W}eak-to-\\textbf{S}trong \\textbf{R}einforcement \\textbf{L}earning from\n\\textbf{AI} \\textbf{F}eedback for video MLLMs} (\\textbf{Iter-W2S-RLAIF}), a\nframework that gradually enhances MLLMs' alignment capabilities by iteratively\nupdating the reference model and performing parameter extrapolation. Finally,\nwe propose an unbiased and information-complete evaluation scheme in VQA\nevaluation. Experiments demonstrate that MMAIP-V is beneficial for MLLMs in\npreference learning and Iter-W2S-RLAIF fully exploits the alignment information\nin MMAIP-V. We believe that the proposed automatic VQA preference data\ngeneration pipeline based on AI feedback can greatly promote future work in the\nMLLMs alignment. \\textbf{Code and dataset are available}\n\\href{https://anonymous.4open.science/r/MMAIP-V_Iter-W2S-RLAIF-702F}{MMAIP-V\\_Iter-W2S-RLAIF-702F}.", "published": "2024-11-25 08:59:39", "link": "http://arxiv.org/abs/2411.16201v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Learning from Relevant Subgoals in Successful Dialogs using Iterative\n  Training for Task-oriented Dialog Systems", "abstract": "Task-oriented Dialog (ToD) systems have to solve multiple subgoals to\naccomplish user goals, whereas feedback is often obtained only at the end of\nthe dialog. In this work, we propose SUIT (SUbgoal-aware ITerative Training),\nan iterative training approach for improving ToD systems. We sample dialogs\nfrom the model we aim to improve and determine subgoals that contribute to\ndialog success using distant supervision to obtain high quality training\nsamples. We show how this data improves supervised fine-tuning or,\nalternatively, preference learning results. SUIT is able to iteratively\ngenerate more data instead of relying on fixed static sets. SUIT reaches new\nstate-of-the-art performance on a popular ToD benchmark.", "published": "2024-11-25 11:47:31", "link": "http://arxiv.org/abs/2411.16305v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can AI grade your essays? A comparative analysis of large language\n  models and teacher ratings in multidimensional essay scoring", "abstract": "The manual assessment and grading of student writing is a time-consuming yet\ncritical task for teachers. Recent developments in generative AI, such as large\nlanguage models, offer potential solutions to facilitate essay-scoring tasks\nfor teachers. In our study, we evaluate the performance and reliability of both\nopen-source and closed-source LLMs in assessing German student essays,\ncomparing their evaluations to those of 37 teachers across 10 pre-defined\ncriteria (i.e., plot logic, expression). A corpus of 20 real-world essays from\nYear 7 and 8 students was analyzed using five LLMs: GPT-3.5, GPT-4, o1, LLaMA\n3-70B, and Mixtral 8x7B, aiming to provide in-depth insights into LLMs' scoring\ncapabilities. Closed-source GPT models outperform open-source models in both\ninternal consistency and alignment with human ratings, particularly excelling\nin language-related criteria. The novel o1 model outperforms all other LLMs,\nachieving Spearman's $r = .74$ with human assessments in the overall score, and\nan internal consistency of $ICC=.80$. These findings indicate that LLM-based\nassessment can be a useful tool to reduce teacher workload by supporting the\nevaluation of essays, especially with regard to language-related criteria.\nHowever, due to their tendency for higher scores, the models require further\nrefinement to better capture aspects of content quality.", "published": "2024-11-25 12:33:14", "link": "http://arxiv.org/abs/2411.16337v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Fundamental Limits of Prompt Tuning Transformers: Universality, Capacity\n  and Efficiency", "abstract": "We investigate the statistical and computational limits of prompt tuning for\ntransformer-based foundation models. Our key contributions are prompt tuning on\n\\textit{single-head} transformers with only a \\textit{single} self-attention\nlayer: (i) is universal, and (ii) supports efficient (even almost-linear time)\nalgorithms under the Strong Exponential Time Hypothesis (SETH). Statistically,\nwe prove that prompt tuning on such simplest possible transformers are\nuniversal approximators for sequence-to-sequence Lipschitz functions. In\naddition, we provide an exponential-in-$dL$ and -in-$(1/\\epsilon)$ lower bound\non the required soft-prompt tokens for prompt tuning to memorize any dataset\nwith 1-layer, 1-head transformers. Computationally, we identify a phase\ntransition in the efficiency of prompt tuning, determined by the norm of the\n\\textit{soft-prompt-induced} keys and queries, and provide an upper bound\ncriterion. Beyond this criterion, no sub-quadratic (efficient) algorithm for\nprompt tuning exists under SETH. Within this criterion, we showcase our theory\nby proving the existence of almost-linear time prompt tuning inference\nalgorithms. These fundamental limits provide important necessary conditions for\ndesigning expressive and efficient prompt tuning methods for practitioners.", "published": "2024-11-25 16:12:17", "link": "http://arxiv.org/abs/2411.16525v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language\n  Models for Robotics", "abstract": "Spatial understanding is a crucial capability that enables robots to perceive\ntheir surroundings, reason about their environment, and interact with it\nmeaningfully. In modern robotics, these capabilities are increasingly provided\nby vision-language models. However, these models face significant challenges in\nspatial reasoning tasks, as their training data are based on general-purpose\nimage datasets that often lack sophisticated spatial understanding. For\nexample, datasets frequently do not capture reference frame comprehension, yet\neffective spatial reasoning requires understanding whether to reason from ego-,\nworld-, or object-centric perspectives. To address this issue, we introduce\nRoboSpatial, a large-scale dataset for spatial understanding in robotics. It\nconsists of real indoor and tabletop scenes, captured as 3D scans and\negocentric images, and annotated with rich spatial information relevant to\nrobotics. The dataset includes 1M images, 5k 3D scans, and 3M annotated spatial\nrelationships, and the pairing of 2D egocentric images with 3D scans makes it\nboth 2D- and 3D- ready. Our experiments show that models trained with\nRoboSpatial outperform baselines on downstream tasks such as spatial affordance\nprediction, spatial relationship prediction, and robot manipulation.", "published": "2024-11-25 16:21:34", "link": "http://arxiv.org/abs/2411.16537v4", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Enhancing LLM Reasoning via Critique Models with Test-Time and\n  Training-Time Supervision", "abstract": "Training large language models (LLMs) to spend more time thinking and\nreflection before responding is crucial for effectively solving complex\nreasoning tasks in fields such as science, coding, and mathematics. However,\nthe effectiveness of mechanisms like self-reflection and self-correction\ndepends on the model's capacity to accurately assess its own performance, which\ncan be limited by factors such as initial accuracy, question difficulty, and\nthe lack of external feedback. In this paper, we delve into a two-player\nparadigm that separates the roles of reasoning and critique models, where the\ncritique model provides step-level feedback to supervise the reasoning (actor)\nmodel during both test-time and train-time. We first propose AutoMathCritique,\nan automated and scalable framework for collecting critique data, resulting in\na dataset of $76,321$ responses paired with step-level feedback. Fine-tuning\nlanguage models with this dataset enables them to generate natural language\nfeedback for mathematical reasoning. We demonstrate that the critique models\nconsistently improve the actor's performance on difficult queries at test-time,\nespecially when scaling up inference-time computation. Motivated by these\nfindings, we introduce the critique-based supervision to the actor's\nself-training process, and propose a critique-in-the-loop self-improvement\nmethod. Experiments show that the method improves the actor's exploration\nefficiency and solution diversity, especially on challenging queries, leading\nto a stronger reasoning model. Lastly, we take the preliminary step to explore\ntraining self-talk reasoning models via critique supervision and showcase its\npotential. Our code and datasets are at\n\\href{https://mathcritique.github.io/}{https://mathcritique.github.io/}.", "published": "2024-11-25 17:11:54", "link": "http://arxiv.org/abs/2411.16579v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Generated Critiques Boost Reward Modeling for Language Models", "abstract": "Reward modeling is crucial for aligning large language models (LLMs) with\nhuman preferences, especially in reinforcement learning from human feedback\n(RLHF). However, current reward models mainly produce scalar scores and\nstruggle to incorporate critiques in a natural language format. We hypothesize\nthat predicting both critiques and the scalar reward would improve reward\nmodeling ability. Motivated by this, we propose Critic-RM, a framework that\nimproves reward models using self-generated critiques without extra\nsupervision. Critic-RM employs a two-stage process: generating and filtering\nhigh-quality critiques, followed by joint fine-tuning on reward prediction and\ncritique generation. Experiments across benchmarks show that Critic-RM improves\nreward modeling accuracy by 3.7%-7.3% compared to standard reward models and\nLLM judges, demonstrating strong performance and data efficiency. Additional\nstudies further validate the effectiveness of generated critiques in rectifying\nflawed reasoning steps with 2.5%-3.2% gains in improving reasoning accuracy.", "published": "2024-11-25 18:28:26", "link": "http://arxiv.org/abs/2411.16646v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DreamRunner: Fine-Grained Compositional Story-to-Video Generation with\n  Retrieval-Augmented Motion Adaptation", "abstract": "Storytelling video generation (SVG) aims to produce coherent and visually\nrich multi-scene videos that follow a structured narrative. Existing methods\nprimarily employ LLM for high-level planning to decompose a story into\nscene-level descriptions, which are then independently generated and stitched\ntogether. However, these approaches struggle with generating high-quality\nvideos aligned with the complex single-scene description, as visualizing such\ncomplex description involves coherent composition of multiple characters and\nevents, complex motion synthesis and muti-character customization. To address\nthese challenges, we propose DreamRunner, a novel story-to-video generation\nmethod: First, we structure the input script using a large language model (LLM)\nto facilitate both coarse-grained scene planning as well as fine-grained\nobject-level layout and motion planning. Next, DreamRunner presents\nretrieval-augmented test-time adaptation to capture target motion priors for\nobjects in each scene, supporting diverse motion customization based on\nretrieved videos, thus facilitating the generation of new videos with complex,\nscripted motions. Lastly, we propose a novel spatial-temporal region-based 3D\nattention and prior injection module SR3AI for fine-grained object-motion\nbinding and frame-by-frame semantic control. We compare DreamRunner with\nvarious SVG baselines, demonstrating state-of-the-art performance in character\nconsistency, text alignment, and smooth transitions. Additionally, DreamRunner\nexhibits strong fine-grained condition-following ability in compositional\ntext-to-video generation, significantly outperforming baselines on\nT2V-ComBench. Finally, we validate DreamRunner's robust ability to generate\nmulti-object interactions with qualitative examples.", "published": "2024-11-25 18:41:56", "link": "http://arxiv.org/abs/2411.16657v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "In-Context Experience Replay Facilitates Safety Red-Teaming of\n  Text-to-Image Diffusion Models", "abstract": "Text-to-image (T2I) models have shown remarkable progress, but their\npotential to generate harmful content remains a critical concern in the ML\ncommunity. While various safety mechanisms have been developed, the field lacks\nsystematic tools for evaluating their effectiveness against real-world misuse\nscenarios. In this work, we propose ICER, a novel red-teaming framework that\nleverages Large Language Models (LLMs) and a bandit optimization-based\nalgorithm to generate interpretable and semantic meaningful problematic prompts\nby learning from past successful red-teaming attempts. Our ICER efficiently\nprobes safety mechanisms across different T2I models without requiring internal\naccess or additional training, making it broadly applicable to deployed\nsystems. Through extensive experiments, we demonstrate that ICER significantly\noutperforms existing prompt attack methods in identifying model vulnerabilities\nwhile maintaining high semantic similarity with intended content. By uncovering\nthat successful jailbreaking instances can systematically facilitate the\ndiscovery of new vulnerabilities, our work provides crucial insights for\ndeveloping more robust safety mechanisms in T2I systems.", "published": "2024-11-25 04:17:24", "link": "http://arxiv.org/abs/2411.16769v2", "categories": ["cs.LG", "cs.CL", "cs.CR", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Parameter Efficient Instruction Tuning: An Empirical Study", "abstract": "Instruction tuning has become an important step for finetuning pretrained\nlanguage models to better follow human instructions and generalize on various\ntasks. Nowadays, pretrained language models become increasingly larger, and\nfull parameter finetuning is overwhelmingly costly. Therefore, Parameter\nEfficient Finetuning (PEFT) has arisen as a cost-effective practice for\ninstruction tuning because of significantly smaller computational, memory, and\nstorage cost compared to full finetuning. Despite their widespread adaptations,\nthe vast hyperparameter spaces, the number of PEFT methods, the different focus\nof instruction tuning capabilities make disentangling the impact of each aspect\ndifficult. This study systematically investigates several representative PEFT\nmethods, surveying the effect of hyperparameter choices including training\nhyperparameters and PEFT-specific hyperparameters, how different models sizes\nand the number of instruction tasks affect the performance,\nin-task-distribution memorization and open instruction following capability.\nOur empirical study shows that only LoRA and adapter can get close to full\nfinetuning with ideal training settings. The ideal training setting includes an\nappropriate learning rate, largest LoRA rank or adapter size allowed and\ndiverse training tasks. On the other hand, LoRA and adapter suffer from\ntraining instability if such an ideal training condition is not met.\nAdditionally, LoRA requires a greater number of tasks for effective unseen task\ngeneralization, exhibit slower learning speed. Moreover, LoRA has weaker\ntask-level memorization. Lastly, LoRA and adapter fall short in complex\nreasoning, coding and long-form generation compared to finetuning in open\ninstruction tuning settings but it shows stronger capabilities compared to\nadapter.", "published": "2024-11-25 07:06:09", "link": "http://arxiv.org/abs/2411.16775v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What can LLM tell us about cities?", "abstract": "This study explores the capabilities of large language models (LLMs) in\nproviding knowledge about cities and regions on a global scale. We employ two\nmethods: directly querying the LLM for target variable values and extracting\nexplicit and implicit features from the LLM correlated with the target\nvariable. Our experiments reveal that LLMs embed a broad but varying degree of\nknowledge across global cities, with ML models trained on LLM-derived features\nconsistently leading to improved predictive accuracy. Additionally, we observe\nthat LLMs demonstrate a certain level of knowledge across global cities on all\ncontinents, but it is evident when they lack knowledge, as they tend to\ngenerate generic or random outputs for unfamiliar tasks. These findings suggest\nthat LLMs can offer new opportunities for data-driven decision-making in the\nstudy of cities.", "published": "2024-11-25 09:07:56", "link": "http://arxiv.org/abs/2411.16791v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Towards Efficient Model-Heterogeneity Federated Learning for Large\n  Models", "abstract": "As demand grows for complex tasks and high-performance applications in edge\ncomputing, the deployment of large models in federated learning has become\nincreasingly urgent, given their superior representational power and\ngeneralization capabilities. However, the resource constraints and\nheterogeneity among clients present significant challenges to this deployment.\nTo tackle these challenges, we introduce HeteroTune, an innovative fine-tuning\nframework tailored for model-heterogeneity federated learning (MHFL). In\nparticular, we propose a novel parameter-efficient fine-tuning (PEFT)\nstructure, called FedAdapter, which employs a multi-branch cross-model\naggregator to enable efficient knowledge aggregation across diverse models.\nBenefiting from the lightweight FedAdapter, our approach significantly reduces\nboth the computational and communication overhead. Finally, our approach is\nsimple yet effective, making it applicable to a wide range of large model\nfine-tuning tasks. Extensive experiments on computer vision (CV) and natural\nlanguage processing (NLP) tasks demonstrate that our method achieves\nstate-of-the-art results, seamlessly integrating efficiency and performance.", "published": "2024-11-25 09:58:51", "link": "http://arxiv.org/abs/2411.16796v1", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.DC", "68T07", "I.2.11"], "primary_category": "cs.LG"}
{"title": "Enhancing In-Hospital Mortality Prediction Using Multi-Representational\n  Learning with LLM-Generated Expert Summaries", "abstract": "In-hospital mortality (IHM) prediction for ICU patients is critical for\ntimely interventions and efficient resource allocation. While structured\nphysiological data provides quantitative insights, clinical notes offer\nunstructured, context-rich narratives. This study integrates these modalities\nwith Large Language Model (LLM)-generated expert summaries to improve IHM\nprediction accuracy. Using the MIMIC-III database, we analyzed time-series\nphysiological data and clinical notes from the first 48 hours of ICU admission.\nClinical notes were concatenated chronologically for each patient and\ntransformed into expert summaries using Med42-v2 70B. A multi-representational\nlearning framework was developed to integrate these data sources, leveraging\nLLMs to enhance textual data while mitigating direct reliance on LLM\npredictions, which can introduce challenges in uncertainty quantification and\ninterpretability. The proposed model achieved an AUPRC of 0.6156 (+36.41%) and\nan AUROC of 0.8955 (+7.64%) compared to a time-series-only baseline. Expert\nsummaries outperformed clinical notes or time-series data alone, demonstrating\nthe value of LLM-generated knowledge. Performance gains were consistent across\ndemographic groups, with notable improvements in underrepresented populations,\nunderscoring the framework's equitable application potential. By integrating\nLLM-generated summaries with structured and unstructured data, the framework\ncaptures complementary patient information, significantly improving predictive\nperformance. This approach showcases the potential of LLMs to augment critical\ncare prediction models, emphasizing the need for domain-specific validation and\nadvanced integration strategies for broader clinical adoption.", "published": "2024-11-25 16:36:38", "link": "http://arxiv.org/abs/2411.16818v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Augmenting Multimodal LLMs with Self-Reflective Tokens for\n  Knowledge-based Visual Question Answering", "abstract": "Multimodal LLMs (MLLMs) are the natural extension of large language models to\nhandle multimodal inputs, combining text and image data. They have recently\ngarnered attention due to their capability to address complex tasks involving\nboth modalities. However, their effectiveness is limited to the knowledge\nacquired during training, which restricts their practical utility. In this\nwork, we introduce a novel method to enhance the adaptability of MLLMs by\nintegrating external knowledge sources. Our proposed model, Reflective LLaVA\n(ReflectiVA), utilizes reflective tokens to dynamically determine the need for\nexternal knowledge and predict the relevance of information retrieved from an\nexternal database. Tokens are trained following a two-stage two-model training\nrecipe. This ultimately enables the MLLM to manage external knowledge while\npreserving fluency and performance on tasks where external knowledge is not\nneeded. Through our experiments, we demonstrate the efficacy of ReflectiVA for\nknowledge-based visual question answering, highlighting its superior\nperformance compared to existing methods. Source code and trained models are\npublicly available at https://aimagelab.github.io/ReflectiVA.", "published": "2024-11-25 19:01:03", "link": "http://arxiv.org/abs/2411.16863v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "A Cross-Corpus Speech Emotion Recognition Method Based on Supervised\n  Contrastive Learning", "abstract": "Research on Speech Emotion Recognition (SER) often faces challenges such as\nthe lack of large-scale public datasets and limited generalization capability\nwhen dealing with data from different distributions. To solve this problem,\nthis paper proposes a cross-corpus speech emotion recognition method based on\nsupervised contrast learning. The method employs a two-stage fine-tuning\nprocess: first, the self-supervised speech representation model is fine-tuned\nusing supervised contrastive learning on multiple speech emotion datasets;\nthen, the classifier is fine-tuned on the target dataset. The experimental\nresults show that the WavLM-based model achieved unweighted accuracy (UA) of\n77.41% on the IEMOCAP dataset and 96.49% on the CASIA dataset, outperforming\nthe state-of-the-art results on the two datasets.", "published": "2024-11-25 07:03:31", "link": "http://arxiv.org/abs/2411.19803v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speculative Decoding with CTC-based Draft Model for LLM Inference\n  Acceleration", "abstract": "Inference acceleration of large language models (LLMs) has been put forward\nin many application scenarios and speculative decoding has shown its advantage\nin addressing inference acceleration. Speculative decoding usually introduces a\ndraft model to assist the base LLM where the draft model produces drafts and\nthe base LLM verifies the draft for acceptance or rejection. In this framework,\nthe final inference speed is decided by the decoding speed of the draft model\nand the acceptance rate of the draft provided by the draft model. Currently the\nwidely used draft models usually generate draft tokens for the next several\npositions in a non-autoregressive way without considering the correlations\nbetween draft tokens. Therefore, it has a high decoding speed but an\nunsatisfactory acceptance rate. In this paper, we focus on how to improve the\nperformance of the draft model and aim to accelerate inference via a high\nacceptance rate. To this end, we propose a CTC-based draft model which\nstrengthens the correlations between draft tokens during the draft phase,\nthereby generating higher-quality draft candidate sequences. Experiment results\nshow that compared to strong baselines, the proposed method can achieve a\nhigher acceptance rate and hence a faster inference speed.", "published": "2024-11-25 14:10:21", "link": "http://arxiv.org/abs/2412.00061v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Specifications: The missing link to making the development of LLM\n  systems an engineering discipline", "abstract": "Despite the significant strides made by generative AI in just a few short\nyears, its future progress is constrained by the challenge of building modular\nand robust systems. This capability has been a cornerstone of past\ntechnological revolutions, which relied on combining components to create\nincreasingly sophisticated and reliable systems. Cars, airplanes, computers,\nand software consist of components-such as engines, wheels, CPUs, and\nlibraries-that can be assembled, debugged, and replaced. A key tool for\nbuilding such reliable and modular systems is specification: the precise\ndescription of the expected behavior, inputs, and outputs of each component.\nHowever, the generality of LLMs and the inherent ambiguity of natural language\nmake defining specifications for LLM-based components (e.g., agents) both a\nchallenging and urgent problem. In this paper, we discuss the progress the\nfield has made so far-through advances like structured outputs, process\nsupervision, and test-time compute-and outline several future directions for\nresearch to enable the development of modular and reliable LLM-based systems\nthrough improved specifications.", "published": "2024-11-25 07:48:31", "link": "http://arxiv.org/abs/2412.05299v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "DocEDA: Automated Extraction and Design of Analog Circuits from\n  Documents with Large Language Model", "abstract": "Efficient and accurate extraction of electrical parameters from circuit\ndatasheets and design documents is critical for accelerating circuit design in\nElectronic Design Automation (EDA). Traditional workflows often rely on\nengineers manually searching and extracting these parameters, which is\ntime-consuming, and prone to human error. To address these challenges, we\nintroduce DocEDA, an automated system that leverages advanced computer vision\ntechniques and Large Language Models (LLMs) to extract electrical parameters\nseamlessly from documents. The layout analysis model specifically designed for\ndatasheet is proposed to classify documents into circuit-related parts.\nUtilizing the inherent Chain-of-Thought reasoning capabilities of LLMs, DocEDA\nautomates the extraction of electronic component parameters from documents. For\ncircuit diagrams parsing, an improved GAM-YOLO model is hybrid with topology\nidentification to transform diagrams into circuit netlists. Then, a space\nmapping enhanced optimization framework is evoked for optimization the layout\nin the document. Experimental evaluations demonstrate that DocEDA significantly\nenhances the efficiency of processing circuit design documents and the accuracy\nof electrical parameter extraction. It exhibits adaptability to various circuit\ndesign scenarios and document formats, offering a novel solution for EDA with\nthe potential to transform traditional methodologies.", "published": "2024-11-25 15:41:43", "link": "http://arxiv.org/abs/2412.05301v1", "categories": ["cs.AR", "cs.AI", "cs.CL"], "primary_category": "cs.AR"}
{"title": "Cautious Optimizers: Improving Training with One Line of Code", "abstract": "AdamW has been the default optimizer for transformer pretraining. For many\nyears, our community searched for faster and more stable optimizers with only\nconstrained positive outcomes. In this work, we propose a single-line\nmodification in Pytorch to any momentum-based optimizer, which we rename\ncautious optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that\nthis modification preserves Adam's Hamiltonian function and it does not break\nthe convergence guarantee under the Lyapunov analysis. In addition, a whole new\nfamily of optimizers is revealed by our theoretical insight. Among them, we\npick the simplest one for empirical experiments, showing not only speed-up on\nLlama and MAE pretraining up to $1.47$ times, but also better results in LLM\npost-training tasks. Code is available at\nhttps://github.com/kyleliang919/C-Optim.", "published": "2024-11-25 04:36:01", "link": "http://arxiv.org/abs/2411.16085v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.DM"], "primary_category": "cs.LG"}
{"title": "Feasibility of Mental Health Triage Call Priority Prediction Using\n  Machine Learning", "abstract": "Ensuring accurate call prioritisation is essential for optimising the\nefficiency and responsiveness of mental health helplines. Currently, call\noperators rely entirely on the caller's statements to determine the priority of\nthe calls. It has been shown that entirely subjective assessment can lead to\nerrors. Furthermore, it is a missed opportunity not to utilise the voice\nproperties readily available during the call to aid in the evaluation.\nIncorrect prioritisation can result in delayed assistance for high-risk\nindividuals, resource misallocation, increased mental health deterioration,\nloss of trust, and potential legal consequences. It is vital to address these\nrisks to guarantee the reliability and effectiveness of mental health services.\nThis study delves into the potential of using machine learning, a branch of\nArtificial Intelligence, to estimate call priority from the callers' voices for\nusers of mental health phone helplines. After analysing 459 call records from a\nmental health helpline, we achieved a balanced accuracy of 92\\%, showing\npromise in aiding the call operators' efficiency in call handling processes and\nimproving customer satisfaction.", "published": "2024-11-25 02:19:57", "link": "http://arxiv.org/abs/2412.00057v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SKQVC: One-Shot Voice Conversion by K-Means Quantization with\n  Self-Supervised Speech Representations", "abstract": "One-shot voice conversion (VC) is a method that enables the transformation\nbetween any two speakers using only a single target speaker utterance. Existing\nmethods often rely on complex architectures and pre-trained speaker\nverification (SV) models to improve the fidelity of converted speech. Recent\nworks utilizing K-means quantization (KQ) with self-supervised learning (SSL)\nfeatures have proven capable of capturing content information from speech.\nHowever, they often struggle to preserve speaking variation, such as prosodic\ndetail and phonetic variation, particularly with smaller codebooks. In this\nwork, we propose a simple yet effective one-shot VC model that utilizes the\ncharacteristics of SSL features and speech attributes. Our approach addresses\nthe issue of losing speaking variation, enabling high-fidelity voice conversion\ntrained with only reconstruction losses, without requiring external speaker\nembeddings. We demonstrate the performance of our model across 6 evaluation\nmetrics, with results highlighting the benefits of the speaking variation\ncompensation method.", "published": "2024-11-25 07:14:26", "link": "http://arxiv.org/abs/2411.16147v1", "categories": ["cs.SD", "cs.AI", "eess.AS", "68T07"], "primary_category": "cs.SD"}
{"title": "The SVASR System for Text-dependent Speaker Verification (TdSV) AAIC\n  Challenge 2024", "abstract": "This paper introduces an efficient and accurate pipeline for text-dependent\nspeaker verification (TDSV), designed to address the need for high-performance\nbiometric systems. The proposed system incorporates a Fast-Conformer-based ASR\nmodule to validate speech content, filtering out Target-Wrong (TW) and\nImpostor-Wrong (IW) trials. For speaker verification, we propose a feature\nfusion approach that combines speaker embeddings extracted from wav2vec-BERT\nand ReDimNet models to create a unified speaker representation. This system\nachieves competitive results on the TDSV 2024 Challenge test set, with a\nnormalized min-DCF of 0.0452 (rank 2), highlighting its effectiveness in\nbalancing accuracy and robustness.", "published": "2024-11-25 10:53:45", "link": "http://arxiv.org/abs/2411.16276v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sonic: Shifting Focus to Global Audio Perception in Portrait Animation", "abstract": "The study of talking face generation mainly explores the intricacies of\nsynchronizing facial movements and crafting visually appealing,\ntemporally-coherent animations. However, due to the limited exploration of\nglobal audio perception, current approaches predominantly employ auxiliary\nvisual and spatial knowledge to stabilize the movements, which often results in\nthe deterioration of the naturalness and temporal inconsistencies.Considering\nthe essence of audio-driven animation, the audio signal serves as the ideal and\nunique priors to adjust facial expressions and lip movements, without resorting\nto interference of any visual signals. Based on this motivation, we propose a\nnovel paradigm, dubbed as Sonic, to {s}hift f{o}cus on the exploration of\nglobal audio per{c}ept{i}o{n}.To effectively leverage global audio knowledge,\nwe disentangle it into intra- and inter-clip audio perception and collaborate\nwith both aspects to enhance overall perception.For the intra-clip audio\nperception, 1). \\textbf{Context-enhanced audio learning}, in which long-range\nintra-clip temporal audio knowledge is extracted to provide facial expression\nand lip motion priors implicitly expressed as the tone and speed of speech. 2).\n\\textbf{Motion-decoupled controller}, in which the motion of the head and\nexpression movement are disentangled and independently controlled by\nintra-audio clips. Most importantly, for inter-clip audio perception, as a\nbridge to connect the intra-clips to achieve the global perception,\n\\textbf{Time-aware position shift fusion}, in which the global inter-clip audio\ninformation is considered and fused for long-audio inference via through\nconsecutively time-aware shifted windows. Extensive experiments demonstrate\nthat the novel audio-driven paradigm outperform existing SOTA methodologies in\nterms of video quality, temporally consistency, lip synchronization precision,\nand motion diversity.", "published": "2024-11-25 12:24:52", "link": "http://arxiv.org/abs/2411.16331v1", "categories": ["cs.MM", "cs.CV", "cs.GR", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
