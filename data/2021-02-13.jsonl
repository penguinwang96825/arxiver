{"title": "Generating Diversified Comments via Reader-Aware Topic Modeling and\n  Saliency Detection", "abstract": "Automatic comment generation is a special and challenging task to verify the\nmodel ability on news content comprehension and language generation. Comments\nnot only convey salient and interesting information in news articles, but also\nimply various and different reader characteristics which we treat as the\nessential clues for diversity. However, most of the comment generation\napproaches only focus on saliency information extraction, while the\nreader-aware factors implied by comments are neglected. To address this issue,\nwe propose a unified reader-aware topic modeling and saliency information\ndetection framework to enhance the quality of generated comments. For\nreader-aware topic modeling, we design a variational generative clustering\nalgorithm for latent semantic learning and topic mining from reader comments.\nFor saliency information detection, we introduce Bernoulli distribution\nestimating on news content to select saliency information. The obtained topic\nrepresentations as well as the selected saliency information are incorporated\ninto the decoder to generate diversified and informative comments. Experimental\nresults on three datasets show that our framework outperforms existing baseline\nmethods in terms of both automatic metrics and human evaluation. The potential\nethical issues are also discussed in detail.", "published": "2021-02-13 03:50:31", "link": "http://arxiv.org/abs/2102.06856v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Capturing Label Distribution: A Case Study in NLI", "abstract": "We study estimating inherent human disagreement (annotation label\ndistribution) in natural language inference task. Post-hoc smoothing of the\npredicted label distribution to match the expected label entropy is very\neffective. Such simple manipulation can reduce KL divergence by almost half,\nyet will not improve majority label prediction accuracy or learn label\ndistributions. To this end, we introduce a small amount of examples with\nmultiple references into training. We depart from the standard practice of\ncollecting a single reference per each training example, and find that\ncollecting multiple references can achieve better accuracy under the fixed\nannotation budget. Lastly, we provide rich analyses comparing these two methods\nfor improving label distribution estimation.", "published": "2021-02-13 04:14:31", "link": "http://arxiv.org/abs/2102.06859v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The first large scale collection of diverse Hausa language datasets", "abstract": "Hausa language belongs to the Afroasiatic phylum, and with more\nfirst-language speakers than any other sub-Saharan African language. With a\nmajority of its speakers residing in the Northern and Southern areas of Nigeria\nand the Republic of Niger, respectively, it is estimated that over 100 million\npeople speak the language. Hence, making it one of the most spoken Chadic\nlanguage. While Hausa is considered well-studied and documented language among\nthe sub-Saharan African languages, it is viewed as a low resource language from\nthe perspective of natural language processing (NLP) due to limited resources\nto utilise in NLP-related tasks. This is common to most languages in Africa;\nthus, it is crucial to enrich such languages with resources that will support\nand speed the pace of conducting various downstream tasks to meet the demand of\nthe modern society. While there exist useful datasets, notably from news sites\nand religious texts, more diversity is needed in the corpus.\n  We provide an expansive collection of curated datasets consisting of both\nformal and informal forms of the language from refutable websites and online\nsocial media networks, respectively. The collection is large and more diverse\nthan the existing corpora by providing the first and largest set of Hausa\nsocial media data posts to capture the peculiarities in the language. The\ncollection also consists of a parallel dataset, which can be used for tasks\nsuch as machine translation with applications in areas such as the detection of\nspurious or inciteful online content. We describe the curation process -- from\nthe collection, preprocessing and how to obtain the data -- and proffer some\nresearch problems that could be addressed using the data.", "published": "2021-02-13 19:34:20", "link": "http://arxiv.org/abs/2102.06991v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interactive Learning from Activity Description", "abstract": "We present a novel interactive learning protocol that enables training\nrequest-fulfilling agents by verbally describing their activities. Unlike\nimitation learning (IL), our protocol allows the teaching agent to provide\nfeedback in a language that is most appropriate for them. Compared with reward\nin reinforcement learning (RL), the description feedback is richer and allows\nfor improved sample complexity. We develop a probabilistic framework and an\nalgorithm that practically implements our protocol. Empirical results in two\nchallenging request-fulfilling problems demonstrate the strengths of our\napproach: compared with RL baselines, it is more sample-efficient; compared\nwith IL baselines, it achieves competitive success rates without requiring the\nteaching agent to be able to demonstrate the desired behavior using the\nlearning agent's actions. Apart from empirical evaluation, we also provide\ntheoretical guarantees for our algorithm under certain assumptions about the\nteacher and the environment.", "published": "2021-02-13 22:51:11", "link": "http://arxiv.org/abs/2102.07024v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PAQ: 65 Million Probably-Asked Questions and What You Can Do With Them", "abstract": "Open-domain Question Answering models which directly leverage question-answer\n(QA) pairs, such as closed-book QA (CBQA) models and QA-pair retrievers, show\npromise in terms of speed and memory compared to conventional models which\nretrieve and read from text corpora. QA-pair retrievers also offer\ninterpretable answers, a high degree of control, and are trivial to update at\ntest time with new knowledge. However, these models lack the accuracy of\nretrieve-and-read systems, as substantially less knowledge is covered by the\navailable QA-pairs relative to text corpora like Wikipedia. To facilitate\nimproved QA-pair models, we introduce Probably Asked Questions (PAQ), a very\nlarge resource of 65M automatically-generated QA-pairs. We introduce a new\nQA-pair retriever, RePAQ, to complement PAQ. We find that PAQ preempts and\ncaches test questions, enabling RePAQ to match the accuracy of recent\nretrieve-and-read models, whilst being significantly faster. Using PAQ, we\ntrain CBQA models which outperform comparable baselines by 5%, but trail RePAQ\nby over 15%, indicating the effectiveness of explicit retrieval. RePAQ can be\nconfigured for size (under 500MB) or speed (over 1K questions per second)\nwhilst retaining high accuracy. Lastly, we demonstrate RePAQ's strength at\nselective QA, abstaining from answering when it is likely to be incorrect. This\nenables RePAQ to ``back-off\" to a more expensive state-of-the-art model,\nleading to a combined system which is both more accurate and 2x faster than the\nstate-of-the-art model alone.", "published": "2021-02-13 23:43:45", "link": "http://arxiv.org/abs/2102.07033v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-Channel Speech Enhancement using Graph Neural Networks", "abstract": "Multi-channel speech enhancement aims to extract clean speech from a noisy\nmixture using signals captured from multiple microphones. Recently proposed\nmethods tackle this problem by incorporating deep neural network models with\nspatial filtering techniques such as the minimum variance distortionless\nresponse (MVDR) beamformer. In this paper, we introduce a different research\ndirection by viewing each audio channel as a node lying in a non-Euclidean\nspace and, specifically, a graph. This formulation allows us to apply graph\nneural networks (GNN) to find spatial correlations among the different channels\n(nodes). We utilize graph convolution networks (GCN) by incorporating them in\nthe embedding space of a U-Net architecture. We use LibriSpeech dataset and\nsimulate room acoustics data to extensively experiment with our approach using\ndifferent array types, and number of microphones. Results indicate the\nsuperiority of our approach when compared to prior state-of-the-art method.", "published": "2021-02-13 14:20:40", "link": "http://arxiv.org/abs/2102.06934v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Convolutional and Recurrent Networks for Polyphonic Instrument\n  Classification from Monophonic Raw Audio Waveforms", "abstract": "Sound Event Detection and Audio Classification tasks are traditionally\naddressed through time-frequency representations of audio signals such as\nspectrograms. However, the emergence of deep neural networks as efficient\nfeature extractors has enabled the direct use of audio signals for\nclassification purposes. In this paper, we attempt to recognize musical\ninstruments in polyphonic audio by only feeding their raw waveforms into deep\nlearning models. Various recurrent and convolutional architectures\nincorporating residual connections are examined and parameterized in order to\nbuild end-to-end classi-fiers with low computational cost and only minimal\npreprocessing. We obtain competitive classification scores and useful\ninstrument-wise insight through the IRMAS test set, utilizing a parallel\nCNN-BiGRU model with multiple residual connections, while maintaining a\nsignificantly reduced number of trainable parameters.", "published": "2021-02-13 13:44:46", "link": "http://arxiv.org/abs/2102.06930v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
