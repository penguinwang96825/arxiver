{"title": "Toward a full-scale neural machine translation in production: the\n  Booking.com use case", "abstract": "While some remarkable progress has been made in neural machine translation\n(NMT) research, there have not been many reports on its development and\nevaluation in practice. This paper tries to fill this gap by presenting some of\nour findings from building an in-house travel domain NMT system in a large\nscale E-commerce setting. The three major topics that we cover are optimization\nand training (including different optimization strategies and corpus sizes),\nhandling real-world content and evaluating results.", "published": "2017-09-18 08:46:20", "link": "http://arxiv.org/abs/1709.05820v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Limitations of Cross-Lingual Learning from Image Search", "abstract": "Cross-lingual representation learning is an important step in making NLP\nscale to all the world's languages. Recent work on bilingual lexicon induction\nsuggests that it is possible to learn cross-lingual representations of words\nbased on similarities between images associated with these words. However, that\nwork focused on the translation of selected nouns only. In our work, we\ninvestigate whether the meaning of other parts-of-speech, in particular\nadjectives and verbs, can be learned in the same way. We also experiment with\ncombining the representations learned from visual data with embeddings learned\nfrom textual data. Our experiments across five language pairs indicate that\nprevious work does not scale to the problem of learning cross-lingual\nrepresentations beyond simple nouns.", "published": "2017-09-18 13:28:40", "link": "http://arxiv.org/abs/1709.05914v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequence to Sequence Learning for Event Prediction", "abstract": "This paper presents an approach to the task of predicting an event\ndescription from a preceding sentence in a text. Our approach explores\nsequence-to-sequence learning using a bidirectional multi-layer recurrent\nneural network. Our approach substantially outperforms previous work in terms\nof the BLEU score on two datasets derived from WikiHow and DeScript\nrespectively. Since the BLEU score is not easy to interpret as a measure of\nevent prediction, we complement our study with a second evaluation that\nexploits the rich linguistic annotation of gold paraphrase sets of events.", "published": "2017-09-18 16:34:18", "link": "http://arxiv.org/abs/1709.06033v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Iterative Policy Learning in End-to-End Trainable Task-Oriented Neural\n  Dialog Models", "abstract": "In this paper, we present a deep reinforcement learning (RL) framework for\niterative dialog policy optimization in end-to-end task-oriented dialog\nsystems. Popular approaches in learning dialog policy with RL include letting a\ndialog agent to learn against a user simulator. Building a reliable user\nsimulator, however, is not trivial, often as difficult as building a good\ndialog agent. We address this challenge by jointly optimizing the dialog agent\nand the user simulator with deep RL by simulating dialogs between the two\nagents. We first bootstrap a basic dialog agent and a basic user simulator by\nlearning directly from dialog corpora with supervised training. We then improve\nthem further by letting the two agents to conduct task-oriented dialogs and\niteratively optimizing their policies with deep RL. Both the dialog agent and\nthe user simulator are designed with neural network models that can be trained\nend-to-end. Our experiment results show that the proposed method leads to\npromising improvements on task success rate and total task reward comparing to\nsupervised training and single-agent RL training baseline models.", "published": "2017-09-18 19:45:51", "link": "http://arxiv.org/abs/1709.06136v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Paraphrasing verbal metonymy through computational methods", "abstract": "Verbal metonymy has received relatively scarce attention in the field of\ncomputational linguistics despite the fact that a model to accurately\nparaphrase metonymy has applications both in academia and the technology\nsector. The method described in this paper makes use of data from the British\nNational Corpus in order to create word vectors, find instances of verbal\nmetonymy and generate potential paraphrases. Two different ways of creating\nword vectors are evaluated in this study: Continuous bag of words and\nSkip-grams. Skip-grams are found to outperform the Continuous bag of words\napproach. Furthermore, the Skip-gram model is found to operate with\nbetter-than-chance accuracy and there is a strong positive relationship (phi\ncoefficient = 0.61) between the model's classification and human judgement of\nthe ranked paraphrases. This study lends credence to the viability of modelling\nverbal metonymy through computational methods based on distributional\nsemantics.", "published": "2017-09-18 20:40:11", "link": "http://arxiv.org/abs/1709.06162v1", "categories": ["cs.CL", "68T50", "H.3.1; I.2.7"], "primary_category": "cs.CL"}
{"title": "Flexible Computing Services for Comparisons and Analyses of Classical\n  Chinese Poetry", "abstract": "We collect nine corpora of representative Chinese poetry for the time span of\n1046 BCE and 1644 CE for studying the history of Chinese words, collocations,\nand patterns. By flexibly integrating our own tools, we are able to provide new\nperspectives for approaching our goals. We illustrate the ideas with two\nexamples. The first example show a new way to compare word preferences of\npoets, and the second example demonstrates how we can utilize our corpora in\nhistorical studies of the Chinese words. We show the viability of the tools for\nacademic research, and we wish to make it helpful for enriching existing\nChinese dictionary as well.", "published": "2017-09-18 00:01:07", "link": "http://arxiv.org/abs/1709.05729v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Towards Building a Knowledge Base of Monetary Transactions from a News\n  Collection", "abstract": "We address the problem of extracting structured representations of economic\nevents from a large corpus of news articles, using a combination of natural\nlanguage processing and machine learning techniques. The developed techniques\nallow for semi-automatic population of a financial knowledge base, which, in\nturn, may be used to support a range of data mining and exploration tasks. The\nkey challenge we face in this domain is that the same event is often reported\nmultiple times, with varying correctness of details. We address this challenge\nby first collecting all information pertinent to a given event from the entire\ncorpus, then considering all possible representations of the event, and\nfinally, using a supervised learning method, to rank these representations by\nthe associated confidence scores. A main innovative element of our approach is\nthat it jointly extracts and stores all attributes of the event as a single\nrepresentation (quintuple). Using a purpose-built test set we demonstrate that\nour supervised learning approach can achieve 25% improvement in F1-score over\nbaseline methods that consider the earliest, the latest or the most frequent\nreporting of the event.", "published": "2017-09-18 02:09:08", "link": "http://arxiv.org/abs/1709.05743v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Word Vector Enrichment of Low Frequency Words in the Bag-of-Words Model\n  for Short Text Multi-class Classification Problems", "abstract": "The bag-of-words model is a standard representation of text for many linear\nclassifier learners. In many problem domains, linear classifiers are preferred\nover more complex models due to their efficiency, robustness and\ninterpretability, and the bag-of-words text representation can capture\nsufficient information for linear classifiers to make highly accurate\npredictions. However in settings where there is a large vocabulary, large\nvariance in the frequency of terms in the training corpus, many classes and\nvery short text (e.g., single sentences or document titles) the bag-of-words\nrepresentation becomes extremely sparse, and this can reduce the accuracy of\nclassifiers. A particular issue in such settings is that short texts tend to\ncontain infrequently occurring or rare terms which lack class-conditional\nevidence. In this work we introduce a method for enriching the bag-of-words\nmodel by complementing such rare term information with related terms from both\ngeneral and domain-specific Word Vector models. By reducing sparseness in the\nbag-of-words models, our enrichment approach achieves improved classification\nover several baseline classifiers in a variety of text classification problems.\nOur approach is also efficient because it requires no change to the linear\nclassifier before or during training, since bag-of-words enrichment applies\nonly to text being classified.", "published": "2017-09-18 05:00:34", "link": "http://arxiv.org/abs/1709.05778v1", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
