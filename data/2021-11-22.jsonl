{"title": "Can depth-adaptive BERT perform better on binary classification tasks", "abstract": "In light of the success of transferring language models into NLP tasks, we\nask whether the full BERT model is always the best and does it exist a simple\nbut effective method to find the winning ticket in state-of-the-art deep neural\nnetworks without complex calculations. We construct a series of BERT-based\nmodels with different size and compare their predictions on 8 binary\nclassification tasks. The results show there truly exist smaller sub-networks\nperforming better than the full model. Then we present a further study and\npropose a simple method to shrink BERT appropriately before fine-tuning. Some\nextended experiments indicate that our method could save time and storage\noverhead extraordinarily with little even no accuracy loss.", "published": "2021-11-22 02:22:47", "link": "http://arxiv.org/abs/2111.10951v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reinforcement Learning for Few-Shot Text Generation Adaptation", "abstract": "Controlling the generative model to adapt a new domain with limited samples\nis a difficult challenge and it is receiving increasing attention. Recently,\nmethods based on meta-learning have shown promising results for few-shot domain\nadaptation. However, meta-learning-based methods usually suffer from the\nproblem of overfitting, which results in a lack of diversity in the generated\ntexts. To avoid this problem, in this study, a novel framework based on\nreinforcement learning (RL) is proposed. In this framework, to increase the\nsample utilization of RL and decrease its sample requirement, maximum\nlikelihood estimation learning is incorporated into the RL process. When there\nare only a few in-domain samples available, experimental results on five target\ndomains in two few-shot configurations show that this framework performs better\nthan baselines.", "published": "2021-11-22 07:33:40", "link": "http://arxiv.org/abs/2111.11030v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Cross-Linguistic Gender Bias in Hindi-English Across\n  Domains", "abstract": "Measuring, evaluating and reducing Gender Bias has come to the forefront with\nnewer and improved language embeddings being released every few months. But\ncould this bias vary from domain to domain? We see a lot of work to study these\nbiases in various embedding models but limited work has been done to debias\nIndic languages. We aim to measure and study this bias in Hindi language, which\nis a higher-order language (gendered) with reference to English, a lower-order\nlanguage. To achieve this, we study the variations across domains to quantify\nif domain embeddings allow us some insight into Gender bias for this pair of\nHindi-English model. We will generate embeddings in four different corpora and\ncompare results by implementing different metrics like with pre-trained State\nof the Art Indic-English translation model, which has performed better at many\nNLP tasks than existing models.", "published": "2021-11-22 12:55:36", "link": "http://arxiv.org/abs/2111.11159v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Human-Machine Interaction Speech Corpus from the ROBIN project", "abstract": "This paper introduces a new Romanian speech corpus from the ROBIN project,\ncalled ROBIN Technical Acquisition Speech Corpus (ROBINTASC). Its main purpose\nwas to improve the behaviour of a conversational agent, allowing human-machine\ninteraction in the context of purchasing technical equipment. The paper\ncontains a detailed description of the acquisition process, corpus statistics\nas well as an evaluation of the corpus influence on a low-latency ASR system as\nwell as a dialogue component.", "published": "2021-11-22 13:10:41", "link": "http://arxiv.org/abs/2111.11170v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Namesakes: Ambiguously Named Entities from Wikipedia and News", "abstract": "We present Namesakes, a dataset of ambiguously named entities obtained from\nEnglish-language Wikipedia and news articles. It consists of 58862 mentions of\n4148 unique entities and their namesakes: 1000 mentions from news, 28843 from\nWikipedia articles about the entity, and 29019 Wikipedia backlink mentions.\nNamesakes should be helpful in establishing challenging benchmarks for the task\nof named entity linking (NEL).", "published": "2021-11-22 17:29:20", "link": "http://arxiv.org/abs/2111.11372v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Visual Sentiment Analysis: A Natural DisasterUse-case Task at MediaEval\n  2021", "abstract": "The Visual Sentiment Analysis task is being offered for the first time at\nMediaEval. The main purpose of the task is to predict the emotional response to\nimages of natural disasters shared on social media. Disaster-related images are\ngenerally complex and often evoke an emotional response, making them an ideal\nuse case of visual sentiment analysis. We believe being able to perform\nmeaningful analysis of natural disaster-related data could be of great societal\nimportance, and a joint effort in this regard can open several interesting\ndirections for future research. The task is composed of three sub-tasks, each\naiming to explore a different aspect of the challenge. In this paper, we\nprovide a detailed overview of the task, the general motivation of the task,\nand an overview of the dataset and the metrics to be used for the evaluation of\nthe proposed solutions.", "published": "2021-11-22 19:11:52", "link": "http://arxiv.org/abs/2111.11471v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning", "abstract": "Despite the recent success of multi-task learning and transfer learning for\nnatural language processing (NLP), few works have systematically studied the\neffect of scaling up the number of tasks during pre-training. Towards this\ngoal, this paper introduces ExMix (Extreme Mixture): a massive collection of\n107 supervised NLP tasks across diverse domains and task-families. Using ExMix,\nwe study the effect of multi-task pre-training at the largest scale to date,\nand analyze co-training transfer amongst common families of tasks. Through this\nanalysis, we show that manually curating an ideal set of tasks for multi-task\npre-training is not straightforward, and that multi-task scaling can vastly\nimprove models on its own. Finally, we propose ExT5: a model pre-trained using\na multi-task objective of self-supervised span denoising and supervised ExMix.\nVia extensive experiments, we show that ExT5 outperforms strong T5 baselines on\nSuperGLUE, GEM, Rainbow, Closed-Book QA tasks, and several tasks outside of\nExMix. ExT5 also significantly improves sample efficiency while pre-training.", "published": "2021-11-22 02:34:46", "link": "http://arxiv.org/abs/2111.10952v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hierarchical Knowledge Distillation for Dialogue Sequence Labeling", "abstract": "This paper presents a novel knowledge distillation method for dialogue\nsequence labeling. Dialogue sequence labeling is a supervised learning task\nthat estimates labels for each utterance in the target dialogue document, and\nis useful for many applications such as dialogue act estimation. Accurate\nlabeling is often realized by a hierarchically-structured large model\nconsisting of utterance-level and dialogue-level networks that capture the\ncontexts within an utterance and between utterances, respectively. However, due\nto its large model size, such a model cannot be deployed on\nresource-constrained devices. To overcome this difficulty, we focus on\nknowledge distillation which trains a small model by distilling the knowledge\nof a large and high performance teacher model. Our key idea is to distill the\nknowledge while keeping the complex contexts captured by the teacher model. To\nthis end, the proposed method, hierarchical knowledge distillation, trains the\nsmall model by distilling not only the probability distribution of the label\nclassification, but also the knowledge of utterance-level and dialogue-level\ncontexts trained in the teacher model by training the model to mimic the\nteacher model's output in each level. Experiments on dialogue act estimation\nand call scene segmentation demonstrate the effectiveness of the proposed\nmethod.", "published": "2021-11-22 02:45:23", "link": "http://arxiv.org/abs/2111.10957v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Multilingual Language Model with Massive Multilingual\n  Knowledge Triples", "abstract": "Knowledge-enhanced language representation learning has shown promising\nresults across various knowledge-intensive NLP tasks. However, prior methods\nare limited in efficient utilization of multilingual knowledge graph (KG) data\nfor language model (LM) pretraining. They often train LMs with KGs in indirect\nways, relying on extra entity/relation embeddings to facilitate knowledge\ninjection. In this work, we explore methods to make better use of the\nmultilingual annotation and language agnostic property of KG triples, and\npresent novel knowledge based multilingual language models (KMLMs) trained\ndirectly on the knowledge triples. We first generate a large amount of\nmultilingual synthetic sentences using the Wikidata KG triples. Then based on\nthe intra- and inter-sentence structures of the generated data, we design\npretraining tasks to enable the LMs to not only memorize the factual knowledge\nbut also learn useful logical patterns. Our pretrained KMLMs demonstrate\nsignificant performance improvements on a wide range of knowledge-intensive\ncross-lingual tasks, including named entity recognition (NER), factual\nknowledge retrieval, relation classification, and a newly designed logical\nreasoning task.", "published": "2021-11-22 02:56:04", "link": "http://arxiv.org/abs/2111.10962v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hierarchical Text Classification As Sub-Hierarchy Sequence Generation", "abstract": "Hierarchical text classification (HTC) is essential for various real\napplications. However, HTC models are challenging to develop because they often\nrequire processing a large volume of documents and labels with hierarchical\ntaxonomy. Recent HTC models based on deep learning have attempted to\nincorporate hierarchy information into a model structure. Consequently, these\nmodels are challenging to implement when the model parameters increase for a\nlarge-scale hierarchy because the model structure depends on the hierarchy\nsize. To solve this problem, we formulate HTC as a sub-hierarchy sequence\ngeneration to incorporate hierarchy information into a target label sequence\ninstead of the model structure. Subsequently, we propose the Hierarchy DECoder\n(HiDEC), which decodes a text sequence into a sub-hierarchy sequence using\nrecursive hierarchy decoding, classifying all parents at the same level into\nchildren at once. In addition, HiDEC is trained to use hierarchical path\ninformation from a root to each leaf in a sub-hierarchy composed of the labels\nof a target document via an attention mechanism and hierarchy-aware masking.\nHiDEC achieved state-of-the-art performance with significantly fewer model\nparameters than existing models on benchmark datasets, such as RCV1-v2, NYT,\nand EURLEX57K.", "published": "2021-11-22 10:50:39", "link": "http://arxiv.org/abs/2111.11104v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Vector Space Semantics for Lambek Calculus with Soft Subexponentials", "abstract": "We develop a vector space semantics for Lambek Calculus with Soft\nSubexponentials, apply the calculus to construct compositional vector\ninterpretations for parasitic gap noun phrases and discourse units with\nanaphora and ellipsis, and experiment with the constructions in a\ndistributional sentence similarity task. As opposed to previous work, which\nused Lambek Calculus with a Relevant Modality the calculus used in this paper\nuses a bounded version of the modality and is decidable. The vector space\nsemantics of this new modality allows us to meaningfully define contraction as\nprojection and provide a linear theory behind what we could previously only\nachieve via nonlinear maps.", "published": "2021-11-22 16:39:30", "link": "http://arxiv.org/abs/2111.11331v3", "categories": ["cs.LO", "cs.CL"], "primary_category": "cs.LO"}
{"title": "DLVGen: A Dual Latent Variable Approach to Personalized Dialogue\n  Generation", "abstract": "The generation of personalized dialogue is vital to natural and human-like\nconversation. Typically, personalized dialogue generation models involve\nconditioning the generated response on the dialogue history and a\nrepresentation of the persona/personality of the interlocutor. As it is\nimpractical to obtain the persona/personality representations for every\ninterlocutor, recent works have explored the possibility of generating\npersonalized dialogue by finetuning the model with dialogue examples\ncorresponding to a given persona instead. However, in real-world\nimplementations, a sufficient number of corresponding dialogue examples are\nalso rarely available. Hence, in this paper, we propose a Dual Latent Variable\nGenerator (DLVGen) capable of generating personalized dialogue in the absence\nof any persona/personality information or any corresponding dialogue examples.\nUnlike prior work, DLVGen models the latent distribution over potential\nresponses as well as the latent distribution over the agent's potential\npersona. During inference, latent variables are sampled from both distributions\nand fed into the decoder. Empirical results show that DLVGen is capable of\ngenerating diverse responses which accurately incorporate the agent's persona.", "published": "2021-11-22 17:21:21", "link": "http://arxiv.org/abs/2111.11363v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RedCaps: web-curated image-text data created by the people, for the\n  people", "abstract": "Large datasets of paired images and text have become increasingly popular for\nlearning generic representations for vision and vision-and-language tasks. Such\ndatasets have been built by querying search engines or collecting HTML alt-text\n-- since web data is noisy, they require complex filtering pipelines to\nmaintain quality. We explore alternate data sources to collect high quality\ndata with minimal filtering. We introduce RedCaps -- a large-scale dataset of\n12M image-text pairs collected from Reddit. Images and captions from Reddit\ndepict and describe a wide variety of objects and scenes. We collect data from\na manually curated set of subreddits, which give coarse image labels and allow\nus to steer the dataset composition without labeling individual instances. We\nshow that captioning models trained on RedCaps produce rich and varied captions\npreferred by humans, and learn visual representations that transfer to many\ndownstream tasks.", "published": "2021-11-22 18:59:34", "link": "http://arxiv.org/abs/2111.11431v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Many Heads but One Brain: Fusion Brain -- a Competition and a Single\n  Multimodal Multitask Architecture", "abstract": "Supporting the current trend in the AI community, we present the AI Journey\n2021 Challenge called Fusion Brain, the first competition which is targeted to\nmake the universal architecture which could process different modalities (in\nthis case, images, texts, and code) and solve multiple tasks for vision and\nlanguage. The Fusion Brain Challenge combines the following specific tasks:\nCode2code Translation, Handwritten Text recognition, Zero-shot Object\nDetection, and Visual Question Answering. We have created datasets for each\ntask to test the participants' submissions on it. Moreover, we have collected\nand made publicly available a new handwritten dataset in both English and\nRussian, which consists of 94,128 pairs of images and texts. We also propose a\nmultimodal and multitask architecture - a baseline solution, in the center of\nwhich is a frozen foundation model and which has been trained in Fusion mode\nalong with Single-task mode. The proposed Fusion approach proves to be\ncompetitive and more energy-efficient compared to the task-specific one.", "published": "2021-11-22 03:46:52", "link": "http://arxiv.org/abs/2111.10974v4", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "L-Verse: Bidirectional Generation Between Image and Text", "abstract": "Far beyond learning long-range interactions of natural language, transformers\nare becoming the de-facto standard for many vision tasks with their power and\nscalability. Especially with cross-modal tasks between image and text, vector\nquantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB\nimage into a sequence of feature vectors. To better leverage the correlation\nbetween image and text, we propose L-Verse, a novel architecture consisting of\nfeature-augmented variational autoencoder (AugVAE) and bidirectional\nauto-regressive transformer (BiART) for image-to-text and text-to-image\ngeneration. Our AugVAE shows the state-of-the-art reconstruction performance on\nImageNet1K validation set, along with the robustness to unseen images in the\nwild. Unlike other models, BiART can distinguish between image (or text) as a\nconditional reference and a generation target. L-Verse can be directly used for\nimage-to-text or text-to-image generation without any finetuning or extra\nobject detection framework. In quantitative and qualitative experiments,\nL-Verse shows impressive results against previous methods in both image-to-text\nand text-to-image generation on MS-COCO Captions. We furthermore assess the\nscalability of L-Verse architecture on Conceptual Captions and present the\ninitial result of bidirectional vision-language representation learning on\ngeneral domain.", "published": "2021-11-22 11:48:26", "link": "http://arxiv.org/abs/2111.11133v11", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Zero-Shot Open-Book Question Answering", "abstract": "Open book question answering is a subset of question answering tasks where\nthe system aims to find answers in a given set of documents (open-book) and\ncommon knowledge about a topic. This article proposes a solution for answering\nnatural language questions from a corpus of Amazon Web Services (AWS) technical\ndocuments with no domain-specific labeled data (zero-shot). These questions can\nhave yes-no-none answers, short answers, long answers, or any combination of\nthe above. This solution comprises a two-step architecture in which a retriever\nfinds the right document and an extractor finds the answers in the retrieved\ndocument. We are introducing a new test dataset for open-book QA based on real\ncustomer questions on AWS technical documentation. After experimenting with\nseveral information retrieval systems and extractor models based on extractive\nlanguage models, the solution attempts to find the yes-no-none answers and text\nanswers in the same pass. The model is trained on the The Stanford Question\nAnswering Dataset - SQuAD (Rajpurkaret al., 2016) and Natural Questions\n(Kwiatkowski et al., 2019) datasets. We were able to achieve 49% F1 and 39%\nexact match score (EM) end-to-end with no domain-specific training.", "published": "2021-11-22 20:38:41", "link": "http://arxiv.org/abs/2111.11520v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Building Goal-Oriented Dialogue Systems with Situated Visual Context", "abstract": "Most popular goal-oriented dialogue agents are capable of understanding the\nconversational context. However, with the surge of virtual assistants with\nscreen, the next generation of agents are required to also understand screen\ncontext in order to provide a proper interactive experience, and better\nunderstand users' goals. In this paper, we propose a novel multimodal\nconversational framework, where the dialogue agent's next action and their\narguments are derived jointly conditioned both on the conversational and the\nvisual context. Specifically, we propose a new model, that can reason over the\nvisual context within a conversation and populate API arguments with visual\nentities given the user query. Our model can recognize visual features such as\ncolor and shape as well as the metadata based features such as price or star\nrating associated with a visual entity. In order to train our model, due to a\nlack of suitable multimodal conversational datasets, we also propose a novel\nmultimodal dialog simulator to generate synthetic data and also collect\nrealistic user data from MTurk to improve model robustness. The proposed model\nachieves a reasonable 85% model accuracy, without high inference latency. We\nalso demonstrate the proposed approach in a prototypical furniture shopping\nexperience for a multimodal virtual assistant.", "published": "2021-11-22 23:30:52", "link": "http://arxiv.org/abs/2111.11576v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Keyword Assisted Embedded Topic Model", "abstract": "By illuminating latent structures in a corpus of text, topic models are an\nessential tool for categorizing, summarizing, and exploring large collections\nof documents. Probabilistic topic models, such as latent Dirichlet allocation\n(LDA), describe how words in documents are generated via a set of latent\ndistributions called topics. Recently, the Embedded Topic Model (ETM) has\nextended LDA to utilize the semantic information in word embeddings to derive\nsemantically richer topics. As LDA and its extensions are unsupervised models,\nthey aren't defined to make efficient use of a user's prior knowledge of the\ndomain. To this end, we propose the Keyword Assisted Embedded Topic Model\n(KeyETM), which equips ETM with the ability to incorporate user knowledge in\nthe form of informative topic-level priors over the vocabulary. Using both\nquantitative metrics and human responses on a topic intrusion task, we\ndemonstrate that KeyETM produces better topics than other guided, generative\nmodels in the literature.", "published": "2021-11-22 07:27:17", "link": "http://arxiv.org/abs/2112.03101v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "HTMOT : Hierarchical Topic Modelling Over Time", "abstract": "Over the years, topic models have provided an efficient way of extracting\ninsights from text. However, while many models have been proposed, none are\nable to model topic temporality and hierarchy jointly. Modelling time provide\nmore precise topics by separating lexically close but temporally distinct\ntopics while modelling hierarchy provides a more detailed view of the content\nof a document corpus. In this study, we therefore propose a novel method,\nHTMOT, to perform Hierarchical Topic Modelling Over Time. We train HTMOT using\na new implementation of Gibbs sampling, which is more efficient. Specifically,\nwe show that only applying time modelling to deep sub-topics provides a way to\nextract specific stories or events while high level topics extract larger\nthemes in the corpus. Our results show that our training procedure is fast and\ncan extract accurate high-level topics and temporally precise sub-topics. We\nmeasured our model's performance using the Word Intrusion task and outlined\nsome limitations of this evaluation method, especially for hierarchical models.\nAs a case study, we focused on the various developments in the space industry\nin 2020.", "published": "2021-11-22 11:02:35", "link": "http://arxiv.org/abs/2112.03104v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Multi-Channel Multi-Speaker ASR Using 3D Spatial Feature", "abstract": "Automatic speech recognition (ASR) of multi-channel multi-speaker overlapped\nspeech remains one of the most challenging tasks to the speech community. In\nthis paper, we look into this challenge by utilizing the location information\nof target speakers in the 3D space for the first time. To explore the strength\nof proposed the 3D spatial feature, two paradigms are investigated. 1) a\npipelined system with a multi-channel speech separation module followed by the\nstate-of-the-art single-channel ASR module; 2) a \"All-In-One\" model where the\n3D spatial feature is directly used as an input to ASR system without explicit\nseparation modules. Both of them are fully differentiable and can be\nback-propagated end-to-end. We test them on simulated overlapped speech and\nreal recordings. Experimental results show that 1) the proposed ALL-In-One\nmodel achieved a comparable error rate to the pipelined system while reducing\nthe inference time by half; 2) the proposed 3D spatial feature significantly\noutperformed (31\\% CERR) all previous works of using the 1D directional\ninformation in both paradigms.", "published": "2021-11-22 07:19:12", "link": "http://arxiv.org/abs/2111.11023v1", "categories": ["cs.SD", "cs.CL", "cs.HC", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sound Field Reproduction With Weighted Mode Matching and\n  Infinite-Dimensional Harmonic Analysis: An Experimental Evaluation", "abstract": "Sound field reproduction methods based on numerical optimization, which aim\nto minimize the error between synthesized and desired sound fields, are useful\nin many practical scenarios because of their flexibility in the array geometry\nof loudspeakers. However, the reproduction performance of these methods in a\npractical environment has not been sufficiently investigated. We evaluate\nweighted mode matching, which is a sound field reproduction method based on the\nspherical wavefunction expansion of the sound field, in comparison with\nconventional pressure matching. We also introduce a method of\ninfinite-dimensional harmonic analysis for estimating the expansion\ncoefficients of the sound field from microphone measurements. Experimental\nresults indicated that weighted mode matching using the expansion coefficients\nof the transfer functions estimated by the infinite-dimensional harmonic\nanalysis outperforms conventional pressure matching, especially when the number\nof microphones is small.", "published": "2021-11-22 08:21:52", "link": "http://arxiv.org/abs/2111.11045v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Comparing the Accuracy of Deep Neural Networks (DNN) and Convolutional\n  Neural Network (CNN) in Music Genre Recognition (MGR): Experiments on Kurdish\n  Music", "abstract": "Musicologists use various labels to classify similar music styles under a\nshared title. But, non-specialists may categorize music differently. That could\nbe through finding patterns in harmony, instruments, and form of the music.\nPeople usually identify a music genre solely by listening, but now computers\nand Artificial Intelligence (AI) can automate this process. The work on\napplying AI in the classification of types of music has been growing recently,\nbut there is no evidence of such research on the Kurdish music genres. In this\nresearch, we developed a dataset that contains 880 samples from eight different\nKurdish music genres. We evaluated two machine learning approaches, a Deep\nNeural Network (DNN) and a Convolutional Neural Network (CNN), to recognize the\ngenres. The results showed that the CNN model outperformed the DNN by achieving\n92% versus 90% accuracy.", "published": "2021-11-22 09:21:48", "link": "http://arxiv.org/abs/2111.11063v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Longitudinal Speech Biomarkers for Automated Alzheimer's Detection", "abstract": "We introduce a novel audio processing architecture, the Open Voice Brain\nModel (OVBM), improving detection accuracy for Alzheimer's (AD) longitudinal\ndiscrimination from spontaneous speech. We also outline the OVBM design\nmethodology leading us to such architecture, which in general can incorporate\nmultimodal biomarkers and target simultaneously several diseases and other AI\ntasks. Key in our methodology is the use of multiple biomarkers complementing\neach other, and when two of them uniquely identify different subjects in a\ntarget disease we say they are orthogonal. We illustrate the methodology by\nintroducing 16 biomarkers, three of which are orthogonal, demonstrating\nsimultaneous above state-of-the-art discrimination for apparently unrelated\ndiseases such as AD and COVID-19. Inspired by research conducted at the MIT\nCenter for Brain Minds and Machines, OVBM combines biomarker implementations of\nthe four modules of intelligence: The brain OS chunks and overlaps audio\nsamples and aggregates biomarker features from the sensory stream and cognitive\ncore creating a multi-modal graph neural network of symbolic compositional\nmodels for the target task. We apply it to AD, achieving above state-of-the-art\naccuracy of 93.8% on raw audio, while extracting a subject saliency map that\nlongitudinally tracks relative disease progression using multiple biomarkers,\n16 in the reported AD task. The ultimate aim is to help medical practice by\ndetecting onset and treatment impact so that intervention options can be\nlongitudinally tested. Using the OBVM design methodology, we introduce a novel\nlung and respiratory tract biomarker created using 200,000+ cough samples to\npre-train a model discriminating cough cultural origin. This cough dataset sets\na new benchmark as the largest audio health dataset with 30,000+ subjects\nparticipating in April 2020, demonstrating for the first-time cough cultural\nbias.", "published": "2021-11-22 18:38:14", "link": "http://arxiv.org/abs/2111.11859v1", "categories": ["cs.SD", "eess.AS", "q-bio.QM", "I.2.0; I.2.m"], "primary_category": "cs.SD"}
