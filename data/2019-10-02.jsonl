{"title": "Abstractive Dialog Summarization with Semantic Scaffolds", "abstract": "The demand for abstractive dialog summary is growing in real-world\napplications. For example, customer service center or hospitals would like to\nsummarize customer service interaction and doctor-patient interaction. However,\nfew researchers explored abstractive summarization on dialogs due to the lack\nof suitable datasets. We propose an abstractive dialog summarization dataset\nbased on MultiWOZ. If we directly apply previous state-of-the-art document\nsummarization methods on dialogs, there are two significant drawbacks: the\ninformative entities such as restaurant names are difficult to preserve, and\nthe contents from different dialog domains are sometimes mismatched. To address\nthese two drawbacks, we propose Scaffold Pointer Network (SPNet)to utilize the\nexisting annotation on speaker role, semantic slot and dialog domain. SPNet\nincorporates these semantic scaffolds for dialog summarization. Since ROUGE\ncannot capture the two drawbacks mentioned, we also propose a new evaluation\nmetric that considers critical informative entities in the text. On MultiWOZ,\nour proposed SPNet outperforms state-of-the-art abstractive summarization\nmethods on all the automatic and human evaluation metrics.", "published": "2019-10-02 08:22:03", "link": "http://arxiv.org/abs/1910.00825v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BookQA: Stories of Challenges and Opportunities", "abstract": "We present a system for answering questions based on the full text of books\n(BookQA), which first selects book passages given a question at hand, and then\nuses a memory network to reason and predict an answer. To improve\ngeneralization, we pretrain our memory network using artificial questions\ngenerated from book sentences. We experiment with the recently published\nNarrativeQA corpus, on the subset of Who questions, which expect book\ncharacters as answers. We experimentally show that BERT-based retrieval and\npretraining improve over baseline results significantly. At the same time, we\nconfirm that NarrativeQA is a highly challenging data set, and that there is\nneed for novel research in order to achieve high-precision BookQA results. We\nanalyze some of the bottlenecks of the current approach, and we argue that more\nresearch is needed on text representation, retrieval of relevant passages, and\nreasoning, including commonsense knowledge.", "published": "2019-10-02 10:05:44", "link": "http://arxiv.org/abs/1910.00856v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting BERT for End-to-End Aspect-based Sentiment Analysis", "abstract": "In this paper, we investigate the modeling power of contextualized embeddings\nfrom pre-trained language models, e.g. BERT, on the E2E-ABSA task.\nSpecifically, we build a series of simple yet insightful neural baselines to\ndeal with E2E-ABSA. The experimental results show that even with a simple\nlinear classification layer, our BERT-based architecture can outperform\nstate-of-the-art works. Besides, we also standardize the comparative study by\nconsistently utilizing a hold-out validation dataset for model selection, which\nis largely ignored by previous works. Therefore, our work can serve as a\nBERT-based benchmark for E2E-ABSA.", "published": "2019-10-02 11:34:58", "link": "http://arxiv.org/abs/1910.00883v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Multi-Task Natural Language Understanding for Cross-domain\n  Conversational AI: HERMIT NLU", "abstract": "We present a new neural architecture for wide-coverage Natural Language\nUnderstanding in Spoken Dialogue Systems. We develop a hierarchical multi-task\narchitecture, which delivers a multi-layer representation of sentence meaning\n(i.e., Dialogue Acts and Frame-like structures). The architecture is a\nhierarchy of self-attention mechanisms and BiLSTM encoders followed by CRF\ntagging layers. We describe a variety of experiments, showing that our approach\nobtains promising results on a dataset annotated with Dialogue Acts and Frame\nSemantics. Moreover, we demonstrate its applicability to a different, publicly\navailable NLU dataset annotated with domain-specific intents and corresponding\nsemantic roles, providing overall performance higher than state-of-the-art\ntools such as RASA, Dialogflow, LUIS, and Watson. For example, we show an\naverage 4.45% improvement in entity tagging F-score over Rasa, Dialogflow and\nLUIS.", "published": "2019-10-02 12:39:53", "link": "http://arxiv.org/abs/1910.00912v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A CCG-based Compositional Semantics and Inference System for\n  Comparatives", "abstract": "Comparative constructions play an important role in natural language\ninference. However, attempts to study semantic representations and logical\ninferences for comparatives from the computational perspective are not well\ndeveloped, due to the complexity of their syntactic structures and inference\npatterns. In this study, using a framework based on Combinatory Categorial\nGrammar (CCG), we present a compositional semantics that maps various\ncomparative constructions in English to semantic representations and introduces\nan inference system that effectively handles logical inference with\ncomparatives, including those involving numeral adjectives, antonyms, and\nquantification. We evaluate the performance of our system on the FraCaS test\nsuite and show that the system can handle a variety of complex logical\ninferences with comparatives.", "published": "2019-10-02 13:05:48", "link": "http://arxiv.org/abs/1910.00930v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Word Decomposition Models for Abusive Language Detection", "abstract": "User generated text on social media often suffers from a lot of undesired\ncharacteristics including hatespeech, abusive language, insults etc. that are\ntargeted to attack or abuse a specific group of people. Often such text is\nwritten differently compared to traditional text such as news involving either\nexplicit mention of abusive words, obfuscated words and typological errors or\nimplicit abuse i.e., indicating or targeting negative stereotypes. Thus,\nprocessing this text poses several robustness challenges when we apply natural\nlanguage processing techniques developed for traditional text. For example,\nusing word or token based models to process such text can treat two spelling\nvariants of a word as two different words. Following recent work, we analyze\nhow character, subword and byte pair encoding (BPE) models can be aid some of\nthe challenges posed by user generated text. In our work, we analyze the\neffectiveness of each of the above techniques, compare and contrast various\nword decomposition techniques when used in combination with others. We\nexperiment with finetuning large pretrained language models, and demonstrate\ntheir robustness to domain shift by studying Wikipedia attack, toxicity and\nTwitter hatespeech datasets", "published": "2019-10-02 16:00:42", "link": "http://arxiv.org/abs/1910.01043v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and\n  lighter", "abstract": "As Transfer Learning from large-scale pre-trained models becomes more\nprevalent in Natural Language Processing (NLP), operating these large models in\non-the-edge and/or under constrained computational training or inference\nbudgets remains challenging. In this work, we propose a method to pre-train a\nsmaller general-purpose language representation model, called DistilBERT, which\ncan then be fine-tuned with good performances on a wide range of tasks like its\nlarger counterparts. While most prior work investigated the use of distillation\nfor building task-specific models, we leverage knowledge distillation during\nthe pre-training phase and show that it is possible to reduce the size of a\nBERT model by 40%, while retaining 97% of its language understanding\ncapabilities and being 60% faster. To leverage the inductive biases learned by\nlarger models during pre-training, we introduce a triple loss combining\nlanguage modeling, distillation and cosine-distance losses. Our smaller, faster\nand lighter model is cheaper to pre-train and we demonstrate its capabilities\nfor on-device computations in a proof-of-concept experiment and a comparative\non-device study.", "published": "2019-10-02 17:56:28", "link": "http://arxiv.org/abs/1910.01108v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cracking the Contextual Commonsense Code: Understanding Commonsense\n  Reasoning Aptitude of Deep Contextual Representations", "abstract": "Pretrained deep contextual representations have advanced the state-of-the-art\non various commonsense NLP tasks, but we lack a concrete understanding of the\ncapability of these models. Thus, we investigate and challenge several aspects\nof BERT's commonsense representation abilities. First, we probe BERT's ability\nto classify various object attributes, demonstrating that BERT shows a strong\nability in encoding various commonsense features in its embedding space, but is\nstill deficient in many areas. Next, we show that, by augmenting BERT's\npretraining data with additional data related to the deficient attributes, we\nare able to improve performance on a downstream commonsense reasoning task\nwhile using a minimal amount of data. Finally, we develop a method of\nfine-tuning knowledge graphs embeddings alongside BERT and show the continued\nimportance of explicit knowledge graphs.", "published": "2019-10-02 18:35:40", "link": "http://arxiv.org/abs/1910.01157v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clinical Text Generation through Leveraging Medical Concept and\n  Relations", "abstract": "With a neural sequence generation model, this study aims to develop a method\nof writing the patient clinical texts given a brief medical history. As a\nproof-of-a-concept, we have demonstrated that it can be workable to use medical\nconcept embedding in clinical text generation. Our model was based on the\nSequence-to-Sequence architecture and trained with a large set of de-identified\nclinical text data. The quantitative result shows that our concept embedding\nmethod decreased the perplexity of the baseline architecture. Also, we discuss\nthe analyzed results from a human evaluation performed by medical doctors.", "published": "2019-10-02 10:17:28", "link": "http://arxiv.org/abs/1910.00861v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The merits of Universal Language Model Fine-tuning for Small Datasets --\n  a case with Dutch book reviews", "abstract": "We evaluated the effectiveness of using language models, that were\npre-trained in one domain, as the basis for a classification model in another\ndomain: Dutch book reviews. Pre-trained language models have opened up new\npossibilities for classification tasks with limited labelled data, because\nrepresentation can be learned in an unsupervised fashion. In our experiments we\nhave studied the effects of training set size (100-1600 items) on the\nprediction accuracy of a ULMFiT classifier, based on a language models that we\npre-trained on the Dutch Wikipedia. We also compared ULMFiT to Support Vector\nMachines, which is traditionally considered suitable for small collections. We\nfound that ULMFiT outperforms SVM for all training set sizes and that\nsatisfactory results (~90%) can be achieved using training sets that can be\nmanually annotated within a few hours. We deliver both our new benchmark\ncollection of Dutch book reviews for sentiment classification as well as the\npre-trained Dutch language model to the community.", "published": "2019-10-02 12:02:46", "link": "http://arxiv.org/abs/1910.00896v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Identifying Nuances in Fake News vs. Satire: Using Semantic and\n  Linguistic Cues", "abstract": "The blurry line between nefarious fake news and protected-speech satire has\nbeen a notorious struggle for social media platforms. Further to the efforts of\nreducing exposure to misinformation on social media, purveyors of fake news\nhave begun to masquerade as satire sites to avoid being demoted. In this work,\nwe address the challenge of automatically classifying fake news versus satire.\nPrevious work have studied whether fake news and satire can be distinguished\nbased on language differences. Contrary to fake news, satire stories are\nusually humorous and carry some political or social message. We hypothesize\nthat these nuances could be identified using semantic and linguistic cues.\nConsequently, we train a machine learning method using semantic representation,\nwith a state-of-the-art contextual language model, and with linguistic features\nbased on textual coherence metrics. Empirical evaluation attests to the merits\nof our approach compared to the language-based baseline and sheds light on the\nnuances between fake news and satire. As avenues for future work, we consider\nstudying additional linguistic features related to the humor aspect, and\nenriching the data with current news events, to help identify a political or\nsocial message.", "published": "2019-10-02 18:47:17", "link": "http://arxiv.org/abs/1910.01160v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Linking artificial and human neural representations of language", "abstract": "What information from an act of sentence understanding is robustly\nrepresented in the human brain? We investigate this question by comparing\nsentence encoding models on a brain decoding task, where the sentence that an\nexperimental participant has seen must be predicted from the fMRI signal evoked\nby the sentence. We take a pre-trained BERT architecture as a baseline sentence\nencoding model and fine-tune it on a variety of natural language understanding\n(NLU) tasks, asking which lead to improvements in brain-decoding performance.\n  We find that none of the sentence encoding tasks tested yield significant\nincreases in brain decoding performance. Through further task ablations and\nrepresentational analyses, we find that tasks which produce syntax-light\nrepresentations yield significant improvements in brain decoding performance.\nOur results constrain the space of NLU models that could best account for human\nneural representations of language, but also suggest limits on the possibility\nof decoding fine-grained syntactic information from fMRI human neuroimaging.", "published": "2019-10-02 22:36:51", "link": "http://arxiv.org/abs/1910.01244v1", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Tree-Structured Semantic Encoder with Knowledge Sharing for Domain\n  Adaptation in Natural Language Generation", "abstract": "Domain adaptation in natural language generation (NLG) remains challenging\nbecause of the high complexity of input semantics across domains and limited\ndata of a target domain. This is particularly the case for dialogue systems,\nwhere we want to be able to seamlessly include new domains into the\nconversation. Therefore, it is crucial for generation models to share knowledge\nacross domains for the effective adaptation from one domain to another. In this\nstudy, we exploit a tree-structured semantic encoder to capture the internal\nstructure of complex semantic representations required for multi-domain\ndialogues in order to facilitate knowledge sharing across domains. In addition,\na layer-wise attention mechanism between the tree encoder and the decoder is\nadopted to further improve the model's capability. The automatic evaluation\nresults show that our model outperforms previous methods in terms of the BLEU\nscore and the slot error rate, in particular when the adaptation data is\nlimited. In subjective evaluation, human judges tend to prefer the sentences\ngenerated by our model, rating them more highly on informativeness and\nnaturalness than other systems.", "published": "2019-10-02 14:27:11", "link": "http://arxiv.org/abs/1910.06719v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Word Embedding Factorization for Compression Using Distilled\n  Nonlinear Neural Decomposition", "abstract": "Word-embeddings are vital components of Natural Language Processing (NLP)\nmodels and have been extensively explored. However, they consume a lot of\nmemory which poses a challenge for edge deployment. Embedding matrices,\ntypically, contain most of the parameters for language models and about a third\nfor machine translation systems. In this paper, we propose Distilled Embedding,\nan (input/output) embedding compression method based on low-rank matrix\ndecomposition and knowledge distillation. First, we initialize the weights of\nour decomposed matrices by learning to reconstruct the full pre-trained\nword-embedding and then fine-tune end-to-end, employing knowledge distillation\non the factorized embedding. We conduct extensive experiments with various\ncompression rates on machine translation and language modeling, using different\ndata-sets with a shared word-embedding matrix for both embedding and vocabulary\nprojection matrices. We show that the proposed technique is simple to\nreplicate, with one fixed parameter controlling compression size, has higher\nBLEU score on translation and lower perplexity on language modeling compared to\ncomplex, difficult to tune state-of-the-art methods.", "published": "2019-10-02 16:40:03", "link": "http://arxiv.org/abs/1910.06720v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Deep Factorization of Style and Structure in Fonts", "abstract": "We propose a deep factorization model for typographic analysis that\ndisentangles content from style. Specifically, a variational inference\nprocedure factors each training glyph into the combination of a\ncharacter-specific content embedding and a latent font-specific style variable.\nThe underlying generative model combines these factors through an asymmetric\ntranspose convolutional process to generate the image of the glyph itself. When\ntrained on corpora of fonts, our model learns a manifold over font styles that\ncan be used to analyze or reconstruct new, unseen fonts. On the task of\nreconstructing missing glyphs from an unknown font given only a small number of\nobservations, our model outperforms both a strong nearest neighbors baseline\nand a state-of-the-art discriminative model from prior work.", "published": "2019-10-02 02:24:12", "link": "http://arxiv.org/abs/1910.00748v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Speech-to-speech Translation between Untranscribed Unknown Languages", "abstract": "In this paper, we explore a method for training speech-to-speech translation\ntasks without any transcription or linguistic supervision. Our proposed method\nconsists of two steps: First, we train and generate discrete representation\nwith unsupervised term discovery with a discrete quantized autoencoder. Second,\nwe train a sequence-to-sequence model that directly maps the source language\nspeech to the target language's discrete representation. Our proposed method\ncan directly generate target speech without any auxiliary or pre-training steps\nwith a source or target transcription. To the best of our knowledge, this is\nthe first work that performed pure speech-to-speech translation between\nuntranscribed unknown languages.", "published": "2019-10-02 06:42:57", "link": "http://arxiv.org/abs/1910.00795v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SummAE: Zero-Shot Abstractive Text Summarization using Length-Agnostic\n  Auto-Encoders", "abstract": "We propose an end-to-end neural model for zero-shot abstractive text\nsummarization of paragraphs, and introduce a benchmark task, ROCSumm, based on\nROCStories, a subset for which we collected human summaries. In this task,\nfive-sentence stories (paragraphs) are summarized with one sentence, using\nhuman summaries only for evaluation. We show results for extractive and human\nbaselines to demonstrate a large abstractive gap in performance. Our model,\nSummAE, consists of a denoising auto-encoder that embeds sentences and\nparagraphs in a common space, from which either can be decoded. Summaries for\nparagraphs are generated by decoding a sentence from the paragraph\nrepresentations. We find that traditional sequence-to-sequence auto-encoders\nfail to produce good summaries and describe how specific architectural choices\nand pre-training techniques can significantly improve performance,\noutperforming extractive baselines. The data, training, evaluation code, and\nbest model weights are open-sourced.", "published": "2019-10-02 14:57:55", "link": "http://arxiv.org/abs/1910.00998v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Language is Power: Representing States Using Natural Language in\n  Reinforcement Learning", "abstract": "Recent advances in reinforcement learning have shown its potential to tackle\ncomplex real-life tasks. However, as the dimensionality of the task increases,\nreinforcement learning methods tend to struggle. To overcome this, we explore\nmethods for representing the semantic information embedded in the state. While\nprevious methods focused on information in its raw form (e.g., raw visual\ninput), we propose to represent the state using natural language. Language can\nrepresent complex scenarios and concepts, making it a favorable candidate for\nrepresentation. Empirical evidence, within the domain of ViZDoom, suggests that\nnatural language based agents are more robust, converge faster and perform\nbetter than vision based agents, showing the benefit of using natural language\nrepresentations for reinforcement learning.", "published": "2019-10-02 11:06:17", "link": "http://arxiv.org/abs/1910.02789v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Emergence of Writing Systems Through Multi-Agent Cooperation", "abstract": "Learning to communicate is considered an essential task to develop a general\nAI. While recent literature in language evolution has studied emergent language\nthrough discrete or continuous message symbols, there has been little work in\nthe emergence of writing systems in artificial agents. In this paper, we\npresent a referential game setup with two agents, where the mode of\ncommunication is a written language system that emerges during the play. We\nshow that the agents can learn to coordinate successfully using this mode of\ncommunication. Further, we study how the game rules affect the writing system\ntaxonomy by proposing a consistency metric.", "published": "2019-10-02 01:35:14", "link": "http://arxiv.org/abs/1910.00741v1", "categories": ["cs.MA", "cs.CL", "cs.CV", "cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.MA"}
{"title": "Animating Face using Disentangled Audio Representations", "abstract": "All previous methods for audio-driven talking head generation assume the\ninput audio to be clean with a neutral tone. As we show empirically, one can\neasily break these systems by simply adding certain background noise to the\nutterance or changing its emotional tone (to such as sad). To make talking head\ngeneration robust to such variations, we propose an explicit audio\nrepresentation learning framework that disentangles audio sequences into\nvarious factors such as phonetic content, emotional tone, background noise and\nothers. We conduct experiments to validate that conditioned on disentangled\ncontent representation, the generated mouth movement by our model is\nsignificantly more accurate than previous approaches (without disentangled\nlearning) in the presence of noise and emotional variations. We further\ndemonstrate that our framework is compatible with current state-of-the-art\napproaches by replacing their original audio learning component with ours. To\nour best knowledge, this is the first work which improves the performance of\ntalking head generation from disentangled audio representation perspective,\nwhich is important for many real-world applications.", "published": "2019-10-02 00:47:19", "link": "http://arxiv.org/abs/1910.00726v1", "categories": ["cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.CV"}
{"title": "From Senones to Chenones: Tied Context-Dependent Graphemes for Hybrid\n  Speech Recognition", "abstract": "There is an implicit assumption that traditional hybrid approaches for\nautomatic speech recognition (ASR) cannot directly model graphemes and need to\nrely on phonetic lexicons to get competitive performance, especially on English\nwhich has poor grapheme-phoneme correspondence. In this work, we show for the\nfirst time that, on English, hybrid ASR systems can in fact model graphemes\neffectively by leveraging tied context-dependent graphemes, i.e., chenones. Our\nchenone-based systems significantly outperform equivalent senone baselines by\n4.5% to 11.1% relative on three different English datasets. Our results on\nLibrispeech are state-of-the-art compared to other hybrid approaches and\ncompetitive with previously published end-to-end numbers. Further analysis\nshows that chenones can better utilize powerful acoustic models and large\ntraining data, and require context- and position-dependent modeling to work\nwell. Chenone-based systems also outperform senone baselines on proper noun and\nrare word recognition, an area where the latter is traditionally thought to\nhave an advantage. Our work provides an alternative for end-to-end ASR and\nestablishes that hybrid systems can be improved by dropping the reliance on\nphonetic knowledge.", "published": "2019-10-02 04:17:46", "link": "http://arxiv.org/abs/1910.01493v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
