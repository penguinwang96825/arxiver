{"title": "Efficient Pre-training of Masked Language Model via Concept-based\n  Curriculum Masking", "abstract": "Masked language modeling (MLM) has been widely used for pre-training\neffective bidirectional representations, but incurs substantial training costs.\nIn this paper, we propose a novel concept-based curriculum masking (CCM) method\nto efficiently pre-train a language model. CCM has two key differences from\nexisting curriculum learning approaches to effectively reflect the nature of\nMLM. First, we introduce a carefully-designed linguistic difficulty criterion\nthat evaluates the MLM difficulty of each token. Second, we construct a\ncurriculum that gradually masks words related to the previously masked words by\nretrieving a knowledge graph. Experimental results show that CCM significantly\nimproves pre-training efficiency. Specifically, the model trained with CCM\nshows comparative performance with the original BERT on the General Language\nUnderstanding Evaluation benchmark at half of the training cost.", "published": "2022-12-15 05:01:59", "link": "http://arxiv.org/abs/2212.07617v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gradient-based Intra-attention Pruning on Pre-trained Language Models", "abstract": "Pre-trained language models achieve superior performance but are\ncomputationally expensive. Techniques such as pruning and knowledge\ndistillation have been developed to reduce their sizes and latencies. In this\nwork, we propose a structured pruning method GRAIN (Gradient-based\nIntra-attention pruning), which performs task-specific pruning with knowledge\ndistillation and yields highly effective models. Different from common\napproaches that prune each attention head as a whole, GRAIN inspects and prunes\nintra-attention structures, which greatly expands the structure search space\nand enables more flexible models. We also propose a gradient separation\nstrategy that reduces the interference of distillation on pruning for a better\ncombination of the two approaches. Experiments on GLUE, SQuAD, and CoNLL 2003\nshow that GRAIN notably outperforms other methods, especially in the high\nsparsity regime, and achieves $6\\sim7\\times$ speedups while maintaining\n$93\\%\\sim99\\%$ performance. Under extreme compression where only $3\\%$\ntransformer weights remain, the pruned model is still competitive compared to\nlarger models.", "published": "2022-12-15 06:52:31", "link": "http://arxiv.org/abs/2212.07634v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improve Text Classification Accuracy with Intent Information", "abstract": "Text classification, a core component of task-oriented dialogue systems,\nattracts continuous research from both the research and industry community, and\nhas resulted in tremendous progress. However, existing method does not consider\nthe use of label information, which may weaken the performance of text\nclassification systems in some token-aware scenarios. To address the problem,\nin this paper, we introduce the use of label information as label embedding for\nthe task of text classification and achieve remarkable performance on benchmark\ndataset.", "published": "2022-12-15 08:15:32", "link": "http://arxiv.org/abs/2212.07649v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advancing Multilingual Pre-training: TRIP Triangular Document-level\n  Pre-training for Multilingual Language Models", "abstract": "Despite the success of multilingual sequence-to-sequence pre-training, most\nexisting approaches rely on document-level monolingual corpora in many\ndifferent languages, sentence-level bilingual corpora,\\footnote{In this paper,\nwe use `bilingual corpora' to denote parallel corpora with `bilingual\ntranslation pairs' in many different language pairs, each consisting of two\nsentences/documents with the same meaning written in different languages. We\nuse `trilingual corpora' to denote parallel corpora with `trilingual\ntranslation pairs' in many different language combinations, each consisting of\nthree sentences/documents.} and sometimes synthetic document-level bilingual\ncorpora. This hampers the performance with cross-lingual document-level tasks\nsuch as document-level translation. Therefore, we propose to mine and leverage\ndocument-level trilingual parallel corpora to improve sequence-to-sequence\nmultilingual pre-training. We present \\textbf{Tri}angular Document-level\n\\textbf{P}re-training (\\textbf{TRIP}), which is the first in the field to\naccelerate the conventional monolingual and bilingual objectives into a\ntrilingual objective with a novel method called Grafting. Experiments show that\nTRIP achieves several strong state-of-the-art (SOTA) scores on three\nmultilingual document-level machine translation benchmarks and one\ncross-lingual abstractive summarization benchmark, including consistent\nimprovements by up to 3.11 d-BLEU points and 8.9 ROUGE-L points.", "published": "2022-12-15 12:14:25", "link": "http://arxiv.org/abs/2212.07752v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention as a Guide for Simultaneous Speech Translation", "abstract": "The study of the attention mechanism has sparked interest in many fields,\nsuch as language modeling and machine translation. Although its patterns have\nbeen exploited to perform different tasks, from neural network understanding to\ntextual alignment, no previous work has analysed the encoder-decoder attention\nbehavior in speech translation (ST) nor used it to improve ST on a specific\ntask. In this paper, we fill this gap by proposing an attention-based policy\n(EDAtt) for simultaneous ST (SimulST) that is motivated by an analysis of the\nexisting attention relations between audio input and textual output. Its goal\nis to leverage the encoder-decoder attention scores to guide inference in real\ntime. Results on en->{de, es} show that the EDAtt policy achieves overall\nbetter results compared to the SimulST state of the art, especially in terms of\ncomputational-aware latency.", "published": "2022-12-15 14:18:53", "link": "http://arxiv.org/abs/2212.07850v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Effects of In-domain Corpus Size on pre-training BERT", "abstract": "Many prior language modeling efforts have shown that pre-training on an\nin-domain corpus can significantly improve performance on downstream\ndomain-specific NLP tasks. However, the difficulties associated with collecting\nenough in-domain data might discourage researchers from approaching this\npre-training task. In this paper, we conducted a series of experiments by\npre-training Bidirectional Encoder Representations from Transformers (BERT)\nwith different sizes of biomedical corpora. The results demonstrate that\npre-training on a relatively small amount of in-domain data (4GB) with limited\ntraining steps, can lead to better performance on downstream domain-specific\nNLP tasks compared with fine-tuning models pre-trained on general corpora.", "published": "2022-12-15 15:49:27", "link": "http://arxiv.org/abs/2212.07914v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Visually-augmented pretrained language models for NLP tasks without\n  images", "abstract": "Although pre-trained language models~(PLMs) have shown impressive performance\nby text-only self-supervised training, they are found lack of visual semantics\nor commonsense. Existing solutions often rely on explicit images for visual\nknowledge augmentation (requiring time-consuming retrieval or generation), and\nthey also conduct the augmentation for the whole input text, without\nconsidering whether it is actually needed in specific inputs or tasks. To\naddress these issues, we propose a novel \\textbf{V}isually-\\textbf{A}ugmented\nfine-tuning approach that can be generally applied to various PLMs or NLP\ntasks, \\textbf{W}ithout using any retrieved or generated \\textbf{I}mages,\nnamely \\textbf{VAWI}. Experimental results show that our approach can\nconsistently improve the performance of BERT, RoBERTa, BART, and T5 at\ndifferent scales, and outperform several competitive baselines on ten tasks.\nOur codes and data are publicly available\nat~\\url{https://github.com/RUCAIBox/VAWI}.", "published": "2022-12-15 16:13:25", "link": "http://arxiv.org/abs/2212.07937v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting the Gold Standard: Grounding Summarization Evaluation with\n  Robust Human Evaluation", "abstract": "Human evaluation is the foundation upon which the evaluation of both\nsummarization systems and automatic metrics rests. However, existing human\nevaluation studies for summarization either exhibit a low inter-annotator\nagreement or have insufficient scale, and an in-depth analysis of human\nevaluation is lacking. Therefore, we address the shortcomings of existing\nsummarization evaluation along the following axes: (1) We propose a modified\nsummarization salience protocol, Atomic Content Units (ACUs), which is based on\nfine-grained semantic units and allows for a high inter-annotator agreement.\n(2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large\nhuman evaluation dataset consisting of 22,000 summary-level annotations over 28\ntop-performing systems on three datasets. (3) We conduct a comparative study of\nfour human evaluation protocols, underscoring potential confounding factors in\nevaluation setups. (4) We evaluate 50 automatic metrics and their variants\nusing the collected human annotations across evaluation protocols and\ndemonstrate how our benchmark leads to more statistically stable and\nsignificant results. The metrics we benchmarked include recent methods based on\nlarge language models (LLMs), GPTScore and G-Eval. Furthermore, our findings\nhave important implications for evaluating LLMs, as we show that LLMs adjusted\nby human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation,\nwhich is affected by the annotators' prior, input-agnostic preferences, calling\nfor more robust, targeted evaluation methods.", "published": "2022-12-15 17:26:05", "link": "http://arxiv.org/abs/2212.07981v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-VALUE: A Framework for Cross-Dialectal English NLP", "abstract": "Dialect differences caused by regional, social, and economic factors cause\nperformance discrepancies for many groups of language technology users.\nInclusive and equitable language technology must critically be dialect\ninvariant, meaning that performance remains constant over dialectal shifts.\nCurrent systems often fall short of this ideal since they are designed and\ntested on a single dialect: Standard American English (SAE). We introduce a\nsuite of resources for evaluating and achieving English dialect invariance. The\nresource is called Multi-VALUE, a controllable rule-based translation system\nspanning 50 English dialects and 189 unique linguistic features. Multi-VALUE\nmaps SAE to synthetic forms of each dialect. First, we use this system to\nstress tests question answering, machine translation, and semantic parsing.\nStress tests reveal significant performance disparities for leading models on\nnon-standard dialects. Second, we use this system as a data augmentation\ntechnique to improve the dialect robustness of existing systems. Finally, we\npartner with native speakers of Chicano and Indian English to release new\ngold-standard variants of the popular CoQA task. To execute the transformation\ncode, run model checkpoints, and download both synthetic and gold-standard\ndialectal benchmark datasets, see http://value-nlp.org.", "published": "2022-12-15 18:17:01", "link": "http://arxiv.org/abs/2212.08011v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attributed Question Answering: Evaluation and Modeling for Attributed\n  Large Language Models", "abstract": "Large language models (LLMs) have shown impressive results while requiring\nlittle or no direct supervision. Further, there is mounting evidence that LLMs\nmay have potential in information-seeking scenarios. We believe the ability of\nan LLM to attribute the text that it generates is likely to be crucial in this\nsetting. We formulate and study Attributed QA as a key first step in the\ndevelopment of attributed LLMs. We propose a reproducible evaluation framework\nfor the task and benchmark a broad set of architectures. We take human\nannotations as a gold standard and show that a correlated automatic metric is\nsuitable for development. Our experimental work gives concrete answers to two\nkey questions (How to measure attribution?, and How well do current\nstate-of-the-art methods perform on attribution?), and give some hints as to\nhow to address a third (How to build LLMs with attribution?).", "published": "2022-12-15 18:45:29", "link": "http://arxiv.org/abs/2212.08037v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in\n  Zero-Shot Reasoning", "abstract": "Generating a Chain of Thought (CoT) has been shown to consistently improve\nlarge language model (LLM) performance on a wide range of NLP tasks. However,\nprior work has mainly focused on logical reasoning tasks (e.g. arithmetic,\ncommonsense QA); it remains unclear whether improvements hold for more diverse\ntypes of reasoning, especially in socially situated contexts. Concretely, we\nperform a controlled evaluation of zero-shot CoT across two socially sensitive\ndomains: harmful questions and stereotype benchmarks. We find that zero-shot\nCoT reasoning in sensitive domains significantly increases a model's likelihood\nto produce harmful or undesirable output, with trends holding across different\nprompt formats and model variants. Furthermore, we show that harmful CoTs\nincrease with model size, but decrease with improved instruction following. Our\nwork suggests that zero-shot CoT should be used with caution on socially\nimportant tasks, especially when marginalized groups or sensitive topics are\ninvolved.", "published": "2022-12-15 18:59:32", "link": "http://arxiv.org/abs/2212.08061v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Chess Commentaries by Combining Language Models with Symbolic\n  Reasoning Engines", "abstract": "Despite many recent advancements in language modeling, state-of-the-art\nlanguage models lack grounding in the real world and struggle with tasks\ninvolving complex reasoning. Meanwhile, advances in the symbolic reasoning\ncapabilities of AI have led to systems that outperform humans in games like\nchess and Go (Silver et al., 2018). Chess commentary provides an interesting\ndomain for bridging these two fields of research, as it requires reasoning over\na complex board state and providing analyses in natural language. In this work\nwe demonstrate how to combine symbolic reasoning engines with controllable\nlanguage models to generate chess commentaries. We conduct experiments to\ndemonstrate that our approach generates commentaries that are preferred by\nhuman judges over previous baselines.", "published": "2022-12-15 23:38:31", "link": "http://arxiv.org/abs/2212.08195v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Saved You A Click: Automatically Answering Clickbait Titles", "abstract": "Often clickbait articles have a title that is phrased as a question or vague\nteaser that entices the user to click on the link and read the article to find\nthe explanation. We developed a system that will automatically find the answer\nor explanation of the clickbait hook from the website text so that the user\ndoes not need to read through the text themselves. We fine-tune an extractive\nquestion and answering model (RoBERTa) and an abstractive one (T5), using data\nscraped from the 'StopClickbait' Facebook pages and Reddit's 'SavedYouAClick'\nsubforum. We find that both extractive and abstractive models improve\nsignificantly after finetuning. We find that the extractive model performs\nslightly better according to ROUGE scores, while the abstractive one has a\nslight edge in terms of BERTscores.", "published": "2022-12-15 23:41:20", "link": "http://arxiv.org/abs/2212.08196v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fixing MoE Over-Fitting on Low-Resource Languages in Multilingual\n  Machine Translation", "abstract": "Sparsely gated Mixture of Experts (MoE) models have been shown to be a\ncompute-efficient method to scale model capacity for multilingual machine\ntranslation. However, for low-resource tasks, MoE models severely over-fit. We\nshow effective regularization strategies, namely dropout techniques for MoE\nlayers in EOM and FOM, Conditional MoE Routing and Curriculum Learning methods\nthat prevent over-fitting and improve the performance of MoE models on\nlow-resource tasks without adversely affecting high-resource tasks. On a\nmassively multilingual machine translation benchmark, our strategies result in\nabout +1 chrF++ improvement in very low resource language pairs. We perform an\nextensive analysis of the learned MoE routing to better understand the impact\nof our regularization methods and how we can improve them.", "published": "2022-12-15 01:06:55", "link": "http://arxiv.org/abs/2212.07571v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using Two Losses and Two Datasets Simultaneously to Improve TempoWiC\n  Accuracy", "abstract": "WSD (Word Sense Disambiguation) is the task of identifying which sense of a\nword is meant in a sentence or other segment of text. Researchers have worked\non this task (e.g. Pustejovsky, 2002) for years but it's still a challenging\none even for SOTA (state-of-the-art) LMs (language models). The new dataset,\nTempoWiC introduced by Loureiro et al. (2022b) focuses on the fact that words\nchange over time. Their best baseline achieves 70.33% macro-F1. In this work,\nwe use two different losses simultaneously to train RoBERTa-based\nclassification models. We also improve our model by using another similar\ndataset to generalize better. Our best configuration beats their best baseline\nby 4.23% and reaches 74.56% macroF1.", "published": "2022-12-15 08:57:30", "link": "http://arxiv.org/abs/2212.07669v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Summary-Oriented Vision Modeling for Multimodal Abstractive\n  Summarization", "abstract": "Multimodal abstractive summarization (MAS) aims to produce a concise summary\ngiven the multimodal data (text and vision). Existing studies mainly focus on\nhow to effectively use the visual features from the perspective of an article,\nhaving achieved impressive success on the high-resource English dataset.\nHowever, less attention has been paid to the visual features from the\nperspective of the summary, which may limit the model performance, especially\nin the low- and zero-resource scenarios. In this paper, we propose to improve\nthe summary quality through summary-oriented visual features. To this end, we\ndevise two auxiliary tasks including vision to summary task and masked image\nmodeling task. Together with the main summarization task, we optimize the MAS\nmodel via the training objectives of all these tasks. By these means, the MAS\nmodel can be enhanced by capturing the summary-oriented visual features,\nthereby yielding more accurate summaries. Experiments on 44 languages, covering\nmid-high-, low-, and zero-resource scenarios, verify the effectiveness and\nsuperiority of the proposed approach, which achieves state-of-the-art\nperformance under all scenarios. Additionally, we will contribute a large-scale\nmultilingual multimodal abstractive summarization (MM-Sum) dataset.", "published": "2022-12-15 09:05:26", "link": "http://arxiv.org/abs/2212.07672v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "FreCDo: A Large Corpus for French Cross-Domain Dialect Identification", "abstract": "We present a novel corpus for French dialect identification comprising\n413,522 French text samples collected from public news websites in Belgium,\nCanada, France and Switzerland. To ensure an accurate estimation of the dialect\nidentification performance of models, we designed the corpus to eliminate\npotential biases related to topic, writing style, and publication source. More\nprecisely, the training, validation and test splits are collected from\ndifferent news websites, while searching for different keywords (topics). This\nleads to a French cross-domain (FreCDo) dialect identification task. We conduct\nexperiments with four competitive baselines, a fine-tuned CamemBERT model, an\nXGBoost based on fine-tuned CamemBERT features, a Support Vector Machines (SVM)\nclassifier based on fine-tuned CamemBERT features, and an SVM based on word\nn-grams. Aside from presenting quantitative results, we also make an analysis\nof the most discriminative features learned by CamemBERT. Our corpus is\navailable at https://github.com/MihaelaGaman/FreCDo.", "published": "2022-12-15 10:32:29", "link": "http://arxiv.org/abs/2212.07707v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "COLA: Improving Conversational Recommender Systems by Collaborative\n  Augmentation", "abstract": "Conversational recommender systems (CRS) aim to employ natural language\nconversations to suggest suitable products to users. Understanding user\npreferences for prospective items and learning efficient item representations\nare crucial for CRS. Despite various attempts, earlier studies mostly learned\nitem representations based on individual conversations, ignoring item\npopularity embodied among all others. Besides, they still need support in\nefficiently capturing user preferences since the information reflected in a\nsingle conversation is limited. Inspired by collaborative filtering, we propose\na collaborative augmentation (COLA) method to simultaneously improve both item\nrepresentation learning and user preference modeling to address these issues.\nWe construct an interactive user-item graph from all conversations, which\naugments item representations with user-aware information, i.e., item\npopularity. To improve user preference modeling, we retrieve similar\nconversations from the training corpus, where the involved items and attributes\nthat reflect the user's potential interests are used to augment the user\nrepresentation through gate control. Extensive experiments on two benchmark\ndatasets demonstrate the effectiveness of our method. Our code and data are\navailable at https://github.com/DongdingLin/COLA.", "published": "2022-12-15 12:37:28", "link": "http://arxiv.org/abs/2212.07767v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "MASTER: Multi-task Pre-trained Bottlenecked Masked Autoencoders are\n  Better Dense Retrievers", "abstract": "Pre-trained Transformers (\\eg BERT) have been commonly used in existing dense\nretrieval methods for parameter initialization, and recent studies are\nexploring more effective pre-training tasks for further improving the quality\nof dense vectors. Although various novel and effective tasks have been\nproposed, their different input formats and learning objectives make them hard\nto be integrated for jointly improving the model performance. In this work, we\naim to unify a variety of pre-training tasks into the bottlenecked masked\nautoencoder manner, and integrate them into a multi-task pre-trained model,\nnamely MASTER. Concretely, MASTER utilizes a shared-encoder multi-decoder\narchitecture that can construct a representation bottleneck to compress the\nabundant semantic information across tasks into dense vectors. Based on it, we\nintegrate three types of representative pre-training tasks: corrupted passages\nrecovering, related passages recovering and PLMs outputs recovering, to\ncharacterize the inner-passage information, inter-passage relations and PLMs\nknowledge. Extensive experiments have shown that our approach outperforms\ncompetitive dense retrieval methods. Our code and data are publicly released in\n\\url{https://github.com/microsoft/SimXNS}.", "published": "2022-12-15 13:57:07", "link": "http://arxiv.org/abs/2212.07841v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "The effects of gender bias in word embeddings on depression prediction", "abstract": "Word embeddings are extensively used in various NLP problems as a\nstate-of-the-art semantic feature vector representation. Despite their success\non various tasks and domains, they might exhibit an undesired bias for\nstereotypical categories due to statistical and societal biases that exist in\nthe dataset they are trained on. In this study, we analyze the gender bias in\nfour different pre-trained word embeddings specifically for the depression\ncategory in the mental disorder domain. We use contextual and non-contextual\nembeddings that are trained on domain-independent as well as clinical\ndomain-specific data. We observe that embeddings carry bias for depression\ntowards different gender groups depending on the type of embeddings. Moreover,\nwe demonstrate that these undesired correlations are transferred to the\ndownstream task for depression phenotype recognition. We find that data\naugmentation by simply swapping gender words mitigates the bias significantly\nin the downstream task.", "published": "2022-12-15 14:19:33", "link": "http://arxiv.org/abs/2212.07852v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning", "abstract": "Large language models show improved downstream task performance when prompted\nto generate step-by-step reasoning to justify their final answers. These\nreasoning steps greatly improve model interpretability and verification, but\nobjectively studying their correctness (independent of the final answer) is\ndifficult without reliable methods for automatic evaluation. We simply do not\nknow how often the stated reasoning steps actually support the final end task\npredictions. In this work, we present ROSCOE, a suite of interpretable,\nunsupervised automatic scores that improve and extend previous text generation\nevaluation metrics. To evaluate ROSCOE against baseline metrics, we design a\ntypology of reasoning errors and collect synthetic and human evaluation scores\non commonly used reasoning datasets. In contrast with existing metrics, ROSCOE\ncan measure semantic consistency, logicality, informativeness, fluency, and\nfactuality - among other traits - by leveraging properties of step-by-step\nrationales. We empirically verify the strength of our metrics on five human\nannotated and six programmatically perturbed diagnostics datasets - covering a\ndiverse set of tasks that require reasoning skills and show that ROSCOE can\nconsistently outperform baseline metrics.", "published": "2022-12-15 15:52:39", "link": "http://arxiv.org/abs/2212.07919v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue", "abstract": "Modern virtual assistants use internal semantic parsing engines to convert\nuser utterances to actionable commands. However, prior work has demonstrated\nthat semantic parsing is a difficult multilingual transfer task with low\ntransfer efficiency compared to other tasks. In global markets such as India\nand Latin America, this is a critical issue as switching between languages is\nprevalent for bilingual users. In this work we dramatically improve the\nzero-shot performance of a multilingual and codeswitched semantic parsing\nsystem using two stages of multilingual alignment. First, we show that\nconstrastive alignment pretraining improves both English performance and\ntransfer efficiency. We then introduce a constrained optimization approach for\nhyperparameter-free adversarial alignment during finetuning. Our Doubly Aligned\nMultilingual Parser (DAMP) improves mBERT transfer performance by 3x, 6x, and\n81x on the Spanglish, Hinglish and Multilingual Task Oriented Parsing\nbenchmarks respectively and outperforms XLM-R and mT5-Large using 3.2x fewer\nparameters.", "published": "2022-12-15 18:58:07", "link": "http://arxiv.org/abs/2212.08054v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Constitutional AI: Harmlessness from AI Feedback", "abstract": "As AI systems become more capable, we would like to enlist their help to\nsupervise other AIs. We experiment with methods for training a harmless AI\nassistant through self-improvement, without any human labels identifying\nharmful outputs. The only human oversight is provided through a list of rules\nor principles, and so we refer to the method as 'Constitutional AI'. The\nprocess involves both a supervised learning and a reinforcement learning phase.\nIn the supervised phase we sample from an initial model, then generate\nself-critiques and revisions, and then finetune the original model on revised\nresponses. In the RL phase, we sample from the finetuned model, use a model to\nevaluate which of the two samples is better, and then train a preference model\nfrom this dataset of AI preferences. We then train with RL using the preference\nmodel as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a\nresult we are able to train a harmless but non-evasive AI assistant that\nengages with harmful queries by explaining its objections to them. Both the SL\nand RL methods can leverage chain-of-thought style reasoning to improve the\nhuman-judged performance and transparency of AI decision making. These methods\nmake it possible to control AI behavior more precisely and with far fewer human\nlabels.", "published": "2022-12-15 06:19:23", "link": "http://arxiv.org/abs/2212.08073v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Joint processing of linguistic properties in brains and language models", "abstract": "Language models have been shown to be very effective in predicting brain\nrecordings of subjects experiencing complex language stimuli. For a deeper\nunderstanding of this alignment, it is important to understand the\ncorrespondence between the detailed processing of linguistic information by the\nhuman brain versus language models. We investigate this correspondence via a\ndirect approach, in which we eliminate information related to specific\nlinguistic properties in the language model representations and observe how\nthis intervention affects the alignment with fMRI brain recordings obtained\nwhile participants listened to a story. We investigate a range of linguistic\nproperties (surface, syntactic, and semantic) and find that the elimination of\neach one results in a significant decrease in brain alignment. Specifically, we\nfind that syntactic properties (i.e. Top Constituents and Tree Depth) have the\nlargest effect on the trend of brain alignment across model layers. These\nfindings provide clear evidence for the role of specific linguistic information\nin the alignment between brain and language models, and open new avenues for\nmapping the joint information processing in both systems. We make the code\npublicly available\n[https://github.com/subbareddy248/linguistic-properties-brain-alignment].", "published": "2022-12-15 19:13:42", "link": "http://arxiv.org/abs/2212.08094v2", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Injecting Domain Knowledge in Language Models for Task-Oriented Dialogue\n  Systems", "abstract": "Pre-trained language models (PLM) have advanced the state-of-the-art across\nNLP applications, but lack domain-specific knowledge that does not naturally\noccur in pre-training data. Previous studies augmented PLMs with symbolic\nknowledge for different downstream NLP tasks. However, knowledge bases (KBs)\nutilized in these studies are usually large-scale and static, in contrast to\nsmall, domain-specific, and modifiable knowledge bases that are prominent in\nreal-world task-oriented dialogue (TOD) systems. In this paper, we showcase the\nadvantages of injecting domain-specific knowledge prior to fine-tuning on TOD\ntasks. To this end, we utilize light-weight adapters that can be easily\nintegrated with PLMs and serve as a repository for facts learned from different\nKBs. To measure the efficacy of proposed knowledge injection methods, we\nintroduce Knowledge Probing using Response Selection (KPRS) -- a probe designed\nspecifically for TOD models. Experiments on KPRS and the response generation\ntask show improvements of knowledge injection with adapters over strong\nbaselines.", "published": "2022-12-15 20:15:05", "link": "http://arxiv.org/abs/2212.08120v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Long Sequence Modeling via State Space Augmented Transformer", "abstract": "Transformer models have achieved superior performance in various natural\nlanguage processing tasks. However, the quadratic computational cost of the\nattention mechanism limits its practicality for long sequences. There are\nexisting attention variants that improve the computational efficiency, but they\nhave limited ability to effectively compute global information. In parallel to\nTransformer models, state space models (SSMs) are tailored for long sequences,\nbut they are not flexible enough to capture complicated local information. We\npropose SPADE, short for $\\underline{\\textbf{S}}$tate\ns$\\underline{\\textbf{P}}$ace\n$\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$\nTransform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the\nbottom layer of SPADE, and we employ efficient local attention methods for the\nother layers. The SSM augments global information, which complements the lack\nof long-range dependency issue in local attention methods. Experimental results\non the Long Range Arena benchmark and language modeling tasks demonstrate the\neffectiveness of the proposed method. To further demonstrate the scalability of\nSPADE, we pre-train large encoder-decoder models and present fine-tuning\nresults on natural language understanding and natural language generation\ntasks.", "published": "2022-12-15 20:51:27", "link": "http://arxiv.org/abs/2212.08136v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MM-SHAP: A Performance-agnostic Metric for Measuring Multimodal\n  Contributions in Vision and Language Models & Tasks", "abstract": "Vision and language models (VL) are known to exploit unrobust indicators in\nindividual modalities (e.g., introduced by distributional biases) instead of\nfocusing on relevant information in each modality. That a unimodal model\nachieves similar accuracy on a VL task to a multimodal one, indicates that\nso-called unimodal collapse occurred. However, accuracy-based tests fail to\ndetect e.g., when the model prediction is wrong, while the model used relevant\ninformation from a modality. Instead, we propose MM-SHAP, a\nperformance-agnostic multimodality score based on Shapley values that reliably\nquantifies in which proportions a multimodal model uses individual modalities.\nWe apply MM-SHAP in two ways: (1) to compare models for their average degree of\nmultimodality, and (2) to measure for individual models the contribution of\nindividual modalities for different tasks and datasets. Experiments with six VL\nmodels -- LXMERT, CLIP and four ALBEF variants -- on four VL tasks highlight\nthat unimodal collapse can occur to different degrees and in different\ndirections, contradicting the wide-spread assumption that unimodal collapse is\none-sided. Based on our results, we recommend MM-SHAP for analysing multimodal\ntasks, to diagnose and guide progress towards multimodal integration. Code\navailable at \\url{https://github.com/Heidelberg-NLP/MM-SHAP}.", "published": "2022-12-15 21:41:06", "link": "http://arxiv.org/abs/2212.08158v3", "categories": ["cs.CV", "cs.CL", "68Txx", "I.2.7; I.2.10"], "primary_category": "cs.CV"}
{"title": "Reliable Measures of Spread in High Dimensional Latent Spaces", "abstract": "Understanding geometric properties of natural language processing models'\nlatent spaces allows the manipulation of these properties for improved\nperformance on downstream tasks. One such property is the amount of data spread\nin a model's latent space, or how fully the available latent space is being\nused. In this work, we define data spread and demonstrate that the commonly\nused measures of data spread, Average Cosine Similarity and a partition\nfunction min/max ratio I(V), do not provide reliable metrics to compare the use\nof latent space across models. We propose and examine eight alternative\nmeasures of data spread, all but one of which improve over these current\nmetrics when applied to seven synthetic data distributions. Of our proposed\nmeasures, we recommend one principal component-based measure and one\nentropy-based measure that provide reliable, relative measures of spread and\ncan be used to compare models of different sizes and dimensionalities.", "published": "2022-12-15 22:15:11", "link": "http://arxiv.org/abs/2212.08172v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources\n  in Natural Language Understanding Systems", "abstract": "Many state-of-the-art natural language understanding (NLU) models are based\non pretrained neural language models. These models often make inferences using\ninformation from multiple sources. An important class of such inferences are\nthose that require both background knowledge, presumably contained in a model's\npretrained parameters, and instance-specific information that is supplied at\ninference time. However, the integration and reasoning abilities of NLU models\nin the presence of multiple knowledge sources have been largely understudied.\nIn this work, we propose a test suite of coreference resolution subtasks that\nrequire reasoning over multiple facts. These subtasks differ in terms of which\nknowledge sources contain the relevant facts. We also introduce subtasks where\nknowledge is present only at inference time using fictional knowledge. We\nevaluate state-of-the-art coreference resolution models on our dataset. Our\nresults indicate that several models struggle to reason on-the-fly over\nknowledge observed both at pretrain time and at inference time. However, with\ntask-specific training, a subset of models demonstrates the ability to\nintegrate certain knowledge types from multiple sources. Still, even the best\nperforming models seem to have difficulties with reliably integrating knowledge\npresented only at inference time.", "published": "2022-12-15 23:26:54", "link": "http://arxiv.org/abs/2212.08192v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Measuring Annotator Agreement Generally across Complex Structured,\n  Multi-object, and Free-text Annotation Tasks", "abstract": "When annotators label data, a key metric for quality assurance is\ninter-annotator agreement (IAA): the extent to which annotators agree on their\nlabels. Though many IAA measures exist for simple categorical and ordinal\nlabeling tasks, relatively little work has considered more complex labeling\ntasks, such as structured, multi-object, and free-text annotations.\nKrippendorff's alpha, best known for use with simpler labeling tasks, does have\na distance-based formulation with broader applicability, but little work has\nstudied its efficacy and consistency across complex annotation tasks.\n  We investigate the design and evaluation of IAA measures for complex\nannotation tasks, with evaluation spanning seven diverse tasks: image bounding\nboxes, image keypoints, text sequence tagging, ranked lists, free text\ntranslations, numeric vectors, and syntax trees. We identify the difficulty of\ninterpretability and the complexity of choosing a distance function as key\nobstacles in applying Krippendorff's alpha generally across these tasks. We\npropose two novel, more interpretable measures, showing they yield more\nconsistent IAA measures across tasks and annotation distance functions.", "published": "2022-12-15 20:12:48", "link": "http://arxiv.org/abs/2212.09503v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transformers learn in-context by gradient descent", "abstract": "At present, the mechanisms of in-context learning in Transformers are not\nwell understood and remain mostly an intuition. In this paper, we suggest that\ntraining Transformers on auto-regressive objectives is closely related to\ngradient-based meta-learning formulations. We start by providing a simple\nweight construction that shows the equivalence of data transformations induced\nby 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a\nregression loss. Motivated by that construction, we show empirically that when\ntraining self-attention-only Transformers on simple regression tasks either the\nmodels learned by GD and Transformers show great similarity or, remarkably, the\nweights found by optimization match the construction. Thus we show how trained\nTransformers become mesa-optimizers i.e. learn models by gradient descent in\ntheir forward pass. This allows us, at least in the domain of regression\nproblems, to mechanistically understand the inner workings of in-context\nlearning in optimized Transformers. Building on this insight, we furthermore\nidentify how Transformers surpass the performance of plain gradient descent by\nlearning an iterative curvature correction and learn linear models on deep data\nrepresentations to solve non-linear regression tasks. Finally, we discuss\nintriguing parallels to a mechanism identified to be crucial for in-context\nlearning termed induction-head (Olsson et al., 2022) and show how it could be\nunderstood as a specific case of in-context learning by gradient descent\nlearning within Transformers. Code to reproduce the experiments can be found at\nhttps://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd .", "published": "2022-12-15 09:21:21", "link": "http://arxiv.org/abs/2212.07677v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Retrieval-based Disentangled Representation Learning with Natural\n  Language Supervision", "abstract": "Disentangled representation learning remains challenging as the underlying\nfactors of variation in the data do not naturally exist. The inherent\ncomplexity of real-world data makes it unfeasible to exhaustively enumerate and\nencapsulate all its variations within a finite set of factors. However, it is\nworth noting that most real-world data have linguistic equivalents, typically\nin the form of textual descriptions. These linguistic counterparts can\nrepresent the data and effortlessly decomposed into distinct tokens. In light\nof this, we present Vocabulary Disentangled Retrieval (VDR), a retrieval-based\nframework that harnesses natural language as proxies of the underlying data\nvariation to drive disentangled representation learning. Our approach employ a\nbi-encoder model to represent both data and natural language in a vocabulary\nspace, enabling the model to distinguish dimensions that capture intrinsic\ncharacteristics within data through its natural language counterpart, thus\nfacilitating disentanglement. We extensively assess the performance of VDR\nacross 15 retrieval benchmark datasets, covering text-to-text and cross-modal\nretrieval scenarios, as well as human evaluation. Our experimental results\ncompellingly demonstrate the superiority of VDR over previous bi-encoder\nretrievers with comparable model size and training costs, achieving an\nimpressive 8.7% improvement in NDCG@10 on the BEIR benchmark, a 5.3% increase\non MS COCO, and a 6.0% increase on Flickr30k in terms of mean recall in the\nzero-shot setting. Moreover, The results from human evaluation indicate that\ninterpretability of our method is on par with SOTA captioning models.", "published": "2022-12-15 10:20:42", "link": "http://arxiv.org/abs/2212.07699v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "CLAM: Selective Clarification for Ambiguous Questions with Generative\n  Language Models", "abstract": "Users often ask dialogue systems ambiguous questions that require\nclarification. We show that current language models rarely ask users to clarify\nambiguous questions and instead provide incorrect answers. To address this, we\nintroduce CLAM: a framework for getting language models to selectively ask for\nclarification about ambiguous user questions. In particular, we show that we\ncan prompt language models to detect whether a given question is ambiguous,\ngenerate an appropriate clarifying question to ask the user, and give a final\nanswer after receiving clarification. We also show that we can simulate users\nby providing language models with privileged information. This lets us\nautomatically evaluate multi-turn clarification dialogues. Finally, CLAM\nsignificantly improves language models' accuracy on mixed ambiguous and\nunambiguous questions relative to SotA.", "published": "2022-12-15 12:47:18", "link": "http://arxiv.org/abs/2212.07769v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TeTIm-Eval: a novel curated evaluation data set for comparing\n  text-to-image models", "abstract": "Evaluating and comparing text-to-image models is a challenging problem.\nSignificant advances in the field have recently been made, piquing interest of\nvarious industrial sectors. As a consequence, a gold standard in the field\nshould cover a variety of tasks and application contexts. In this paper a novel\nevaluation approach is experimented, on the basis of: (i) a curated data set,\nmade by high-quality royalty-free image-text pairs, divided into ten\ncategories; (ii) a quantitative metric, the CLIP-score, (iii) a human\nevaluation task to distinguish, for a given text, the real and the generated\nimages. The proposed method has been applied to the most recent models, i.e.,\nDALLE2, Latent Diffusion, Stable Diffusion, GLIDE and Craiyon. Early\nexperimental results show that the accuracy of the human judgement is fully\ncoherent with the CLIP-score. The dataset has been made available to the\npublic.", "published": "2022-12-15 13:52:03", "link": "http://arxiv.org/abs/2212.07839v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "RWEN-TTS: Relation-aware Word Encoding Network for Natural\n  Text-to-Speech Synthesis", "abstract": "With the advent of deep learning, a huge number of text-to-speech (TTS)\nmodels which produce human-like speech have emerged. Recently, by introducing\nsyntactic and semantic information w.r.t the input text, various approaches\nhave been proposed to enrich the naturalness and expressiveness of TTS models.\nAlthough these strategies showed impressive results, they still have some\nlimitations in utilizing language information. First, most approaches only use\ngraph networks to utilize syntactic and semantic information without\nconsidering linguistic features. Second, most previous works do not explicitly\nconsider adjacent words when encoding syntactic and semantic information, even\nthough it is obvious that adjacent words are usually meaningful when encoding\nthe current word. To address these issues, we propose Relation-aware Word\nEncoding Network (RWEN), which effectively allows syntactic and semantic\ninformation based on two modules (i.e., Semantic-level Relation Encoding and\nAdjacent Word Relation Encoding). Experimental results show substantial\nimprovements compared to previous works.", "published": "2022-12-15 16:17:03", "link": "http://arxiv.org/abs/2212.07939v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units", "abstract": "Direct speech-to-speech translation (S2ST), in which all components can be\noptimized jointly, is advantageous over cascaded approaches to achieve fast\ninference with a simplified pipeline. We present a novel two-pass direct S2ST\narchitecture, UnitY, which first generates textual representations and predicts\ndiscrete acoustic units subsequently. We enhance the model performance by\nsubword prediction in the first-pass decoder, advanced two-pass decoder\narchitecture design and search strategy, and better training regularization. To\nleverage large amounts of unlabeled text data, we pre-train the first-pass text\ndecoder based on the self-supervised denoising auto-encoding task. Experimental\nevaluations on benchmark datasets at various data scales demonstrate that UnitY\noutperforms a single-pass speech-to-unit translation model by 2.5-4.2 ASR-BLEU\nwith 2.83x decoding speed-up. We show that the proposed methods boost the\nperformance even when predicting spectrogram in the second pass. However,\npredicting discrete units achieves 2.51x decoding speed-up compared to that\ncase.", "published": "2022-12-15 18:58:28", "link": "http://arxiv.org/abs/2212.08055v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "FiDO: Fusion-in-Decoder optimized for stronger performance and faster\n  inference", "abstract": "Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that\nsets the state-of-the-art on many knowledge-intensive NLP tasks. However, the\narchitecture used for FiD was chosen by making minimal modifications to a\nstandard T5 model, which our analysis shows to be highly suboptimal for a\nretrieval-augmented model. In particular, FiD allocates the bulk of FLOPs to\nthe encoder, while the majority of inference time results from memory bandwidth\nconstraints in the decoder. We propose two simple changes to the FiD\narchitecture to alleviate memory bandwidth constraints, and speed up inference\nby 7x. This allows us to use a much larger decoder at modest cost. We denote\nFiD with the above modifications as FiDO, and show that it strongly improves\nperformance over existing FiD models for a wide range of inference budgets. For\nexample, FiDO-Large-XXL performs faster inference than FiD-Base and achieves\nbetter performance than FiD-Large.", "published": "2022-12-15 21:35:46", "link": "http://arxiv.org/abs/2212.08153v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NBC-Softmax : Darkweb Author fingerprinting and migration tracking", "abstract": "Metric learning aims to learn distances from the data, which enhances the\nperformance of similarity-based algorithms. An author style detection task is a\nmetric learning problem, where learning style features with small intra-class\nvariations and larger inter-class differences is of great importance to achieve\nbetter performance. Recently, metric learning based on softmax loss has been\nused successfully for style detection. While softmax loss can produce separable\nrepresentations, its discriminative power is relatively poor. In this work, we\npropose NBC-Softmax, a contrastive loss based clustering technique for softmax\nloss, which is more intuitive and able to achieve superior performance. Our\ntechnique meets the criterion for larger number of samples, thus achieving\nblock contrastiveness, which is proven to outperform pair-wise losses. It uses\nmini-batch sampling effectively and is scalable. Experiments on 4 darkweb\nsocial forums, with NBCSAuthor that uses the proposed NBC-Softmax for author\nand sybil detection, shows that our negative block contrastive approach\nconstantly outperforms state-of-the-art methods using the same network\narchitecture.\n  Our code is publicly available at : https://github.com/gayanku/NBC-Softmax", "published": "2022-12-15 23:00:33", "link": "http://arxiv.org/abs/2212.08184v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Ring That Bell: A Corpus and Method for Multimodal Metaphor Detection in\n  Videos", "abstract": "We present the first openly available multimodal metaphor annotated corpus.\nThe corpus consists of videos including audio and subtitles that have been\nannotated by experts. Furthermore, we present a method for detecting metaphors\nin the new dataset based on the textual content of the videos. The method\nachieves a high F1-score (62\\%) for metaphorical labels. We also experiment\nwith other modalities and multimodal methods; however, these methods did not\nout-perform the text-based model. In our error analysis, we do identify that\nthere are cases where video could help in disambiguating metaphors, however,\nthe visual cues are too subtle for our model to capture. The data is available\non Zenodo.", "published": "2022-12-15 17:11:35", "link": "http://arxiv.org/abs/2301.01134v1", "categories": ["cs.MM", "cs.CL", "cs.CV"], "primary_category": "cs.MM"}
{"title": "Vision Transformers are Parameter-Efficient Audio-Visual Learners", "abstract": "Vision transformers (ViTs) have achieved impressive results on various\ncomputer vision tasks in the last several years. In this work, we study the\ncapability of frozen ViTs, pretrained only on visual data, to generalize to\naudio-visual data without finetuning any of its original parameters. To do so,\nwe propose a latent audio-visual hybrid (LAVISH) adapter that adapts pretrained\nViTs to audio-visual tasks by injecting a small number of trainable parameters\ninto every layer of a frozen ViT. To efficiently fuse visual and audio cues,\nour LAVISH adapter uses a small set of latent tokens, which form an attention\nbottleneck, thus, eliminating the quadratic cost of standard cross-attention.\nCompared to the existing modality-specific audio-visual methods, our approach\nachieves competitive or even better performance on various audio-visual tasks\nwhile using fewer tunable parameters and without relying on costly audio\npretraining or external audio encoders. Our code is available at\nhttps://genjib.github.io/project_page/LAVISH/", "published": "2022-12-15 17:31:54", "link": "http://arxiv.org/abs/2212.07983v2", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Best-Answer Prediction in Q&A Sites Using User Information", "abstract": "Community Question Answering (CQA) sites have spread and multiplied\nsignificantly in recent years. Sites like Reddit, Quora, and Stack Exchange are\nbecoming popular amongst people interested in finding answers to diverse\nquestions. One practical way of finding such answers is automatically\npredicting the best candidate given existing answers and comments. Many studies\nwere conducted on answer prediction in CQA but with limited focus on using the\nbackground information of the questionnaires. We address this limitation using\na novel method for predicting the best answers using the questioner's\nbackground information and other features, such as the textual content or the\nrelationships with other participants. Our answer classification model was\ntrained using the Stack Exchange dataset and validated using the Area Under the\nCurve (AUC) metric. The experimental results show that the proposed method\ncomplements previous methods by pointing out the importance of the\nrelationships between users, particularly throughout the level of involvement\nin different communities on Stack Exchange. Furthermore, we point out that\nthere is little overlap between user-relation information and the information\nrepresented by the shallow text features and the meta-features, such as time\ndifferences.", "published": "2022-12-15 02:28:52", "link": "http://arxiv.org/abs/2212.08475v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.SI", "I.2.7; I.2.1"], "primary_category": "cs.CL"}
{"title": "Improving Fast-slow Encoder based Transducer with Streaming Deliberation", "abstract": "This paper introduces a fast-slow encoder based transducer with streaming\ndeliberation for end-to-end automatic speech recognition. We aim to improve the\nrecognition accuracy of the fast-slow encoder based transducer while keeping\nits latency low by integrating a streaming deliberation model. Specifically,\nthe deliberation model leverages partial hypotheses from the streaming fast\nencoder and implicitly learns to correct recognition errors. We modify the\nparallel beam search algorithm for fast-slow encoder based transducer to be\nefficient and compatible with the deliberation model. In addition, the\ndeliberation model is designed to process streaming data. To further improve\nthe deliberation performance, a simple text augmentation approach is explored.\nWe also compare LSTM and Conformer models for encoding partial hypotheses.\nExperiments on Librispeech and in-house data show relative WER reductions\n(WERRs) from 3% to 5% with a slight increase in model size and negligible extra\ntoken emission latency compared with fast-slow encoder based transducer.\nCompared with vanilla neural transducers, the proposed deliberation model\ntogether with fast-slow encoder based transducer obtains relative 10-11% WERRs\non Librispeech and around relative 6% WERR on in-house data with smaller\nemission delays.", "published": "2022-12-15 08:16:46", "link": "http://arxiv.org/abs/2212.07650v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "DeFT-AN: Dense Frequency-Time Attentive Network for Multichannel Speech\n  Enhancement", "abstract": "In this study, we propose a dense frequency-time attentive network (DeFT-AN)\nfor multichannel speech enhancement. DeFT-AN is a mask estimation network that\npredicts a complex spectral masking pattern for suppressing the noise and\nreverberation embedded in the short-time Fourier transform (STFT) of an input\nsignal. The proposed mask estimation network incorporates three different types\nof blocks for aggregating information in the spatial, spectral, and temporal\ndimensions. It utilizes a spectral transformer with a modified feed-forward\nnetwork and a temporal conformer with sequential dilated convolutions. The use\nof dense blocks and transformers dedicated to the three different\ncharacteristics of audio signals enables more comprehensive enhancement in\nnoisy and reverberant environments. The remarkable performance of DeFT-AN over\nstate-of-the-art multichannel models is demonstrated based on two popular noisy\nand reverberant datasets in terms of various metrics for speech quality and\nintelligibility.", "published": "2022-12-15 01:03:18", "link": "http://arxiv.org/abs/2212.07570v5", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A large-scale and PCR-referenced vocal audio dataset for COVID-19", "abstract": "The UK COVID-19 Vocal Audio Dataset is designed for the training and\nevaluation of machine learning models that classify SARS-CoV-2 infection status\nor associated respiratory symptoms using vocal audio. The UK Health Security\nAgency recruited voluntary participants through the national Test and Trace\nprogramme and the REACT-1 survey in England from March 2021 to March 2022,\nduring dominant transmission of the Alpha and Delta SARS-CoV-2 variants and\nsome Omicron variant sublineages. Audio recordings of volitional coughs,\nexhalations, and speech were collected in the 'Speak up to help beat\ncoronavirus' digital survey alongside demographic, self-reported symptom and\nrespiratory condition data, and linked to SARS-CoV-2 test results. The UK\nCOVID-19 Vocal Audio Dataset represents the largest collection of SARS-CoV-2\nPCR-referenced audio recordings to date. PCR results were linked to 70,794 of\n72,999 participants and 24,155 of 25,776 positive cases. Respiratory symptoms\nwere reported by 45.62% of participants. This dataset has additional potential\nuses for bioacoustics research, with 11.30% participants reporting asthma, and\n27.20% with linked influenza PCR test results.", "published": "2022-12-15 11:40:40", "link": "http://arxiv.org/abs/2212.07738v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MAViL: Masked Audio-Video Learners", "abstract": "We present Masked Audio-Video Learners (MAViL) to train audio-visual\nrepresentations. Our approach learns with three complementary forms of\nself-supervision: (1) reconstruction of masked audio and video input data, (2)\nintra- and inter-modal contrastive learning with masking, and (3) self-training\nby reconstructing joint audio-video contextualized features learned from the\nfirst two objectives. Pre-training with MAViL not only enables the model to\nperform well in audio-visual classification and retrieval tasks but also\nimproves representations of each modality in isolation, without using\ninformation from the other modality for fine-tuning or inference. Empirically,\nMAViL sets a new state-of-the-art on AudioSet (53.1 mAP) and VGGSound (67.1%\naccuracy). For the first time, a self-supervised audio-visual model outperforms\nones that use external supervision on these benchmarks.", "published": "2022-12-15 18:59:59", "link": "http://arxiv.org/abs/2212.08071v2", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Audio-based AI classifiers show no evidence of improved COVID-19\n  screening over simple symptoms checkers", "abstract": "Recent work has reported that AI classifiers trained on audio recordings can\naccurately predict severe acute respiratory syndrome coronavirus 2 (SARSCoV2)\ninfection status. Here, we undertake a large scale study of audio-based deep\nlearning classifiers, as part of the UK governments pandemic response. We\ncollect and analyse a dataset of audio recordings from 67,842 individuals with\nlinked metadata, including reverse transcription polymerase chain reaction\n(PCR) test outcomes, of whom 23,514 tested positive for SARS CoV 2. Subjects\nwere recruited via the UK governments National Health Service Test-and-Trace\nprogramme and the REal-time Assessment of Community Transmission (REACT)\nrandomised surveillance survey. In an unadjusted analysis of our dataset AI\nclassifiers predict SARS-CoV-2 infection status with high accuracy (Receiver\nOperating Characteristic Area Under the Curve (ROCAUC) 0.846 [0.838, 0.854])\nconsistent with the findings of previous studies. However, after matching on\nmeasured confounders, such as age, gender, and self reported symptoms, our\nclassifiers performance is much weaker (ROC-AUC 0.619 [0.594, 0.644]). Upon\nquantifying the utility of audio based classifiers in practical settings, we\nfind them to be outperformed by simple predictive scores based on user reported\nsymptoms.", "published": "2022-12-15 15:44:02", "link": "http://arxiv.org/abs/2212.08570v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Statistical Design and Analysis for Robust Machine Learning: A Case\n  Study from COVID-19", "abstract": "Since early in the coronavirus disease 2019 (COVID-19) pandemic, there has\nbeen interest in using artificial intelligence methods to predict COVID-19\ninfection status based on vocal audio signals, for example cough recordings.\nHowever, existing studies have limitations in terms of data collection and of\nthe assessment of the performances of the proposed predictive models. This\npaper rigorously assesses state-of-the-art machine learning techniques used to\npredict COVID-19 infection status based on vocal audio signals, using a dataset\ncollected by the UK Health Security Agency. This dataset includes acoustic\nrecordings and extensive study participant meta-data. We provide guidelines on\ntesting the performance of methods to classify COVID-19 infection status based\non acoustic features and we discuss how these can be extended more generally to\nthe development and assessment of predictive methods based on public health\ndatasets.", "published": "2022-12-15 13:50:13", "link": "http://arxiv.org/abs/2212.08571v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.AP"], "primary_category": "cs.SD"}
