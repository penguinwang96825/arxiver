{"title": "Nearest Neighbour Few-Shot Learning for Cross-lingual Classification", "abstract": "Even though large pre-trained multilingual models (e.g. mBERT, XLM-R) have\nled to significant performance gains on a wide range of cross-lingual NLP\ntasks, success on many downstream tasks still relies on the availability of\nsufficient annotated data. Traditional fine-tuning of pre-trained models using\nonly a few target samples can cause over-fitting. This can be quite limiting as\nmost languages in the world are under-resourced. In this work, we investigate\ncross-lingual adaptation using a simple nearest neighbor few-shot (<15 samples)\ninference technique for classification tasks. We experiment using a total of 16\ndistinct languages across two NLP tasks- XNLI and PAWS-X. Our approach\nconsistently improves traditional fine-tuning using only a handful of labeled\nsamples in target locales. We also demonstrate its generalization capability\nacross tasks.", "published": "2021-09-06 03:18:23", "link": "http://arxiv.org/abs/2109.02221v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sent2Span: Span Detection for PICO Extraction in the Biomedical Text\n  without Span Annotations", "abstract": "The rapid growth in published clinical trials makes it difficult to maintain\nup-to-date systematic reviews, which requires finding all relevant trials. This\nleads to policy and practice decisions based on out-of-date, incomplete, and\nbiased subsets of available clinical evidence. Extracting and then normalising\nPopulation, Intervention, Comparator, and Outcome (PICO) information from\nclinical trial articles may be an effective way to automatically assign trials\nto systematic reviews and avoid searching and screening - the two most\ntime-consuming systematic review processes. We propose and test a novel\napproach to PICO span detection. The major difference between our proposed\nmethod and previous approaches comes from detecting spans without needing\nannotated span data and using only crowdsourced sentence-level annotations.\nExperiments on two datasets show that PICO span detection results achieve much\nhigher results for recall when compared to fully supervised methods with PICO\nsentence detection at least as good as human annotations. By removing the\nreliance on expert annotations for span detection, this work could be used in\nhuman-machine pipeline for turning low-quality crowdsourced, and sentence-level\nPICO annotations into structured information that can be used to quickly assign\ntrials to relevant systematic reviews.", "published": "2021-09-06 06:14:49", "link": "http://arxiv.org/abs/2109.02254v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncertainty-Aware Balancing for Multilingual and Multi-Domain Neural\n  Machine Translation Training", "abstract": "Learning multilingual and multi-domain translation model is challenging as\nthe heterogeneous and imbalanced data make the model converge inconsistently\nover different corpora in real world. One common practice is to adjust the\nshare of each corpus in the training, so that the learning process is balanced\nand low-resource cases can benefit from the high resource ones. However,\nautomatic balancing methods usually depend on the intra- and inter-dataset\ncharacteristics, which is usually agnostic or requires human priors. In this\nwork, we propose an approach, MultiUAT, that dynamically adjusts the training\ndata usage based on the model's uncertainty on a small set of trusted clean\ndata for multi-corpus machine translation. We experiments with two classes of\nuncertainty measures on multilingual (16 languages with 4 settings) and\nmulti-domain settings (4 for in-domain and 2 for out-of-domain on\nEnglish-German translation) and demonstrate our approach MultiUAT substantially\noutperforms its baselines, including both static and dynamic strategies. We\nanalyze the cross-domain transfer and show the deficiency of static and\nsimilarity based methods.", "published": "2021-09-06 08:30:33", "link": "http://arxiv.org/abs/2109.02284v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MyProfessors: Mining Turkish Student Reviews", "abstract": "We introduce Hocalarim (MyProfessors), the largest student review dataset\navailable for the Turkish language. It consists of over 5000 professor reviews\nleft online by students, with different aspects of education rated on a scale\nof 1 to 5 stars. We investigate the properties of the dataset and present its\nstatistics. We examine the impact of students' institution type on their\nratings and the correlation of students' bias to give positive or negative\nfeedback.", "published": "2021-09-06 09:55:58", "link": "http://arxiv.org/abs/2109.02325v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Vision Guided Generative Pre-trained Language Models for Multimodal\n  Abstractive Summarization", "abstract": "Multimodal abstractive summarization (MAS) models that summarize videos\n(vision modality) and their corresponding transcripts (text modality) are able\nto extract the essential information from massive multimodal data on the\nInternet. Recently, large-scale generative pre-trained language models (GPLMs)\nhave been shown to be effective in text generation tasks. However, existing MAS\nmodels cannot leverage GPLMs' powerful generation ability. To fill this\nresearch gap, we aim to study two research questions: 1) how to inject visual\ninformation into GPLMs without hurting their generation ability; and 2) where\nis the optimal place in GPLMs to inject the visual information? In this paper,\nwe present a simple yet effective method to construct vision guided (VG) GPLMs\nfor the MAS task using attention-based add-on layers to incorporate visual\ninformation while maintaining their original text generation ability. Results\nshow that our best model significantly surpasses the prior state-of-the-art\nmodel by 5.7 ROUGE-1, 5.3 ROUGE-2, and 5.1 ROUGE-L scores on the How2 dataset,\nand our visual guidance method contributes 83.6% of the overall improvement.\nFurthermore, we conduct thorough ablation studies to analyze the effectiveness\nof various modality fusion methods and fusion locations.", "published": "2021-09-06 12:31:21", "link": "http://arxiv.org/abs/2109.02401v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Eliminating Sentiment Bias for Aspect-Level Sentiment Classification\n  with Unsupervised Opinion Extraction", "abstract": "Aspect-level sentiment classification (ALSC) aims at identifying the\nsentiment polarity of a specified aspect in a sentence. ALSC is a practical\nsetting in aspect-based sentiment analysis due to no opinion term labeling\nneeded, but it fails to interpret why a sentiment polarity is derived for the\naspect. To address this problem, recent works fine-tune pre-trained Transformer\nencoders for ALSC to extract an aspect-centric dependency tree that can locate\nthe opinion words. However, the induced opinion words only provide an intuitive\ncue far below human-level interpretability. Besides, the pre-trained encoder\ntends to internalize an aspect's intrinsic sentiment, causing sentiment bias\nand thus affecting model performance. In this paper, we propose a span-based\nanti-bias aspect representation learning framework. It first eliminates the\nsentiment bias in the aspect embedding by adversarial learning against aspects'\nprior sentiment. Then, it aligns the distilled opinion candidates with the\naspect by span-based dependency modeling to highlight the interpretable opinion\nterms. Our method achieves new state-of-the-art performance on five benchmarks,\nwith the capability of unsupervised opinion extraction.", "published": "2021-09-06 12:32:42", "link": "http://arxiv.org/abs/2109.02403v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multitask Balanced and Recalibrated Network for Medical Code Prediction", "abstract": "Human coders assign standardized medical codes to clinical documents\ngenerated during patients' hospitalization, which is error-prone and\nlabor-intensive. Automated medical coding approaches have been developed using\nmachine learning methods such as deep neural networks. Nevertheless, automated\nmedical coding is still challenging because of the imbalanced class problem,\ncomplex code association, and noise in lengthy documents. To solve these\nissues, we propose a novel neural network called Multitask Balanced and\nRecalibrated Neural Network. Significantly, the multitask learning scheme\nshares the relationship knowledge between different code branches to capture\nthe code association. A recalibrated aggregation module is developed by\ncascading convolutional blocks to extract high-level semantic features that\nmitigate the impact of noise in documents. Also, the cascaded structure of the\nrecalibrated module can benefit the learning from lengthy notes. To solve the\nclass imbalanced problem, we deploy the focal loss to redistribute the\nattention of low and high-frequency medical codes. Experimental results show\nthat our proposed model outperforms competitive baselines on a real-world\nclinical dataset MIMIC-III.", "published": "2021-09-06 12:58:25", "link": "http://arxiv.org/abs/2109.02418v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Length Divergence Bias in Textual Matching Models", "abstract": "Despite the remarkable success deep models have achieved in Textual Matching\n(TM) tasks, it still remains unclear whether they truly understand language or\nmeasure the semantic similarity of texts by exploiting statistical bias in\ndatasets. In this work, we provide a new perspective to study this issue -- via\nthe length divergence bias. We find the length divergence heuristic widely\nexists in prevalent TM datasets, providing direct cues for prediction. To\ndetermine whether TM models have adopted such heuristic, we introduce an\nadversarial evaluation scheme which invalidates the heuristic. In this\nadversarial setting, all TM models perform worse, indicating they have indeed\nadopted this heuristic. Through a well-designed probing experiment, we\nempirically validate that the bias of TM models can be attributed in part to\nextracting the text length information during training. To alleviate the length\ndivergence bias, we propose an adversarial training method. The results\ndemonstrate we successfully improve the robustness and generalization ability\nof models at the same time.", "published": "2021-09-06 13:12:06", "link": "http://arxiv.org/abs/2109.02431v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Mind-Map Generation via Sequence-to-Graph and Reinforced Graph\n  Refinement", "abstract": "A mind-map is a diagram that represents the central concept and key ideas in\na hierarchical way. Converting plain text into a mind-map will reveal its key\nsemantic structure and be easier to understand. Given a document, the existing\nautomatic mind-map generation method extracts the relationships of every\nsentence pair to generate the directed semantic graph for this document. The\ncomputation complexity increases exponentially with the length of the document.\nMoreover, it is difficult to capture the overall semantics. To deal with the\nabove challenges, we propose an efficient mind-map generation network that\nconverts a document into a graph via sequence-to-graph. To guarantee a\nmeaningful mind-map, we design a graph refinement module to adjust the relation\ngraph in a reinforcement learning manner. Extensive experimental results\ndemonstrate that the proposed approach is more effective and efficient than the\nexisting methods. The inference time is reduced by thousands of times compared\nwith the existing methods. The case studies verify that the generated mind-maps\nbetter reveal the underlying semantic structures of the document.", "published": "2021-09-06 13:41:19", "link": "http://arxiv.org/abs/2109.02457v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DialogLM: Pre-trained Model for Long Dialogue Understanding and\n  Summarization", "abstract": "Dialogue is an essential part of human communication and cooperation.\nExisting research mainly focuses on short dialogue scenarios in a one-on-one\nfashion. However, multi-person interactions in the real world, such as meetings\nor interviews, are frequently over a few thousand words. There is still a lack\nof corresponding research and powerful tools to understand and process such\nlong dialogues. Therefore, in this work, we present a pre-training framework\nfor long dialogue understanding and summarization. Considering the nature of\nlong conversations, we propose a window-based denoising approach for generative\npre-training. For a dialogue, it corrupts a window of text with\ndialogue-inspired noise, and guides the model to reconstruct this window based\non the content of the remaining conversation. Furthermore, to process longer\ninput, we augment the model with sparse attention which is combined with\nconventional attention in a hybrid manner. We conduct extensive experiments on\nfive datasets of long dialogues, covering tasks of dialogue summarization,\nabstractive question answering and topic segmentation. Experimentally, we show\nthat our pre-trained model DialogLM significantly surpasses the\nstate-of-the-art models across datasets and tasks. Source code and all the\npre-trained models are available on our GitHub repository\n(https://github.com/microsoft/DialogLM).", "published": "2021-09-06 13:55:03", "link": "http://arxiv.org/abs/2109.02492v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "You should evaluate your language model on marginal likelihood over\n  tokenisations", "abstract": "Neural language models typically tokenise input text into sub-word units to\nachieve an open vocabulary. The standard approach is to use a single canonical\ntokenisation at both train and test time. We suggest that this approach is\nunsatisfactory and may bottleneck our evaluation of language model performance.\nUsing only the one-best tokenisation ignores tokeniser uncertainty over\nalternative tokenisations, which may hurt model out-of-domain performance.\n  In this paper, we argue that instead, language models should be evaluated on\ntheir marginal likelihood over tokenisations. We compare different estimators\nfor the marginal likelihood based on sampling, and show that it is feasible to\nestimate the marginal likelihood with a manageable number of samples. We then\nevaluate pretrained English and German language models on both the\none-best-tokenisation and marginal perplexities, and show that the marginal\nperplexity can be significantly better than the one best, especially on\nout-of-domain data. We link this difference in perplexity to the tokeniser\nuncertainty as measured by tokeniser entropy. We discuss some implications of\nour results for language model training and evaluation, particularly with\nregard to tokenisation robustness.", "published": "2021-09-06 15:37:02", "link": "http://arxiv.org/abs/2109.02550v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Natural Language Representation with Large-Scale Out-of-Domain\n  Commonsense", "abstract": "We study how to enhance text representation via textual commonsense. We point\nout that commonsense has the nature of domain discrepancy. Namely, commonsense\nhas different data formats and is domain-independent from the downstream task.\nThis nature brings challenges to introducing commonsense in general text\nunderstanding tasks. A typical method of introducing textual knowledge is\ncontinuing pre-training over the commonsense corpus. However, it will cause\ncatastrophic forgetting to the downstream task due to the domain discrepancy.\nIn addition, previous methods of directly using textual descriptions as extra\ninput information cannot apply to large-scale commonsense.\n  In this paper, we propose to use large-scale out-of-domain commonsense to\nenhance text representation. In order to effectively incorporate the\ncommonsense, we proposed OK-Transformer (\\underline{O}ut-of-domain\n\\underline{K}nowledge enhanced \\underline{Transformer}). OK-Transformer\neffectively integrates commonsense descriptions and enhances them to the target\ntext representation. In addition, OK-Transformer can adapt to the\nTransformer-based language models (e.g. BERT, RoBERTa) for free, without\npre-training on large-scale unsupervised corpora. We have verified the\neffectiveness of OK-Transformer in multiple applications such as commonsense\nreasoning, general text classification, and low-resource commonsense settings.", "published": "2021-09-06 16:16:10", "link": "http://arxiv.org/abs/2109.02572v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finding needles in a haystack: Sampling Structurally-diverse Training\n  Sets from Synthetic Data for Compositional Generalization", "abstract": "Modern semantic parsers suffer from two principal limitations. First,\ntraining requires expensive collection of utterance-program pairs. Second,\nsemantic parsers fail to generalize at test time to new compositions/structures\nthat have not been observed during training. Recent research has shown that\nautomatic generation of synthetic utterance-program pairs can alleviate the\nfirst problem, but its potential for the second has thus far been\nunder-explored. In this work, we investigate automatic generation of synthetic\nutterance-program pairs for improving compositional generalization in semantic\nparsing. Given a small training set of annotated examples and an \"infinite\"\npool of synthetic examples, we select a subset of synthetic examples that are\nstructurally-diverse and use them to improve compositional generalization. We\nevaluate our approach on a new split of the schema2QA dataset, and show that it\nleads to dramatic improvements in compositional generalization as well as\nmoderate improvements in the traditional i.i.d setup. Moreover,\nstructurally-diverse sampling achieves these improvements with as few as 5K\nexamples, compared to 1M examples when sampling uniformly at random -- a 200x\nimprovement in data efficiency.", "published": "2021-09-06 16:20:47", "link": "http://arxiv.org/abs/2109.02575v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text-to-Table: A New Way of Information Extraction", "abstract": "We study a new problem setting of information extraction (IE), referred to as\ntext-to-table. In text-to-table, given a text, one creates a table or several\ntables expressing the main content of the text, while the model is learned from\ntext-table pair data. The problem setting differs from those of the existing\nmethods for IE. First, the extraction can be carried out from long texts to\nlarge tables with complex structures. Second, the extraction is entirely\ndata-driven, and there is no need to explicitly define the schemas. As far as\nwe know, there has been no previous work that studies the problem. In this\nwork, we formalize text-to-table as a sequence-to-sequence (seq2seq) problem.\nWe first employ a seq2seq model fine-tuned from a pre-trained language model to\nperform the task. We also develop a new method within the seq2seq approach,\nexploiting two additional techniques in table generation: table constraint and\ntable relation embeddings. We consider text-to-table as an inverse problem of\nthe well-studied table-to-text, and make use of four existing table-to-text\ndatasets in our experiments on text-to-table. Experimental results show that\nthe vanilla seq2seq model can outperform the baseline methods of using relation\nextraction and named entity extraction. The results also show that our method\ncan further boost the performances of the vanilla seq2seq model. We further\ndiscuss the main challenges of the proposed task. The code and data are\navailable at https://github.com/shirley-wu/text_to_table.", "published": "2021-09-06 19:35:46", "link": "http://arxiv.org/abs/2109.02707v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Inspiring Content on Social Media", "abstract": "Inspiration moves a person to see new possibilities and transforms the way\nthey perceive their own potential. Inspiration has received little attention in\npsychology, and has not been researched before in the NLP community. To the\nbest of our knowledge, this work is the first to study inspiration through\nmachine learning methods. We aim to automatically detect inspiring content from\nsocial media data. To this end, we analyze social media posts to tease out what\nmakes a post inspiring and what topics are inspiring. We release a dataset of\n5,800 inspiring and 5,800 non-inspiring English-language public post unique ids\ncollected from a dump of Reddit public posts made available by a third party\nand use linguistic heuristics to automatically detect which social media\nEnglish-language posts are inspiring.", "published": "2021-09-06 20:57:32", "link": "http://arxiv.org/abs/2109.02734v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does BERT Learn as Humans Perceive? Understanding Linguistic Styles\n  through Lexica", "abstract": "People convey their intention and attitude through linguistic styles of the\ntext that they write. In this study, we investigate lexicon usages across\nstyles throughout two lenses: human perception and machine word importance,\nsince words differ in the strength of the stylistic cues that they provide. To\ncollect labels of human perception, we curate a new dataset, Hummingbird, on\ntop of benchmarking style datasets. We have crowd workers highlight the\nrepresentative words in the text that makes them think the text has the\nfollowing styles: politeness, sentiment, offensiveness, and five emotion types.\nWe then compare these human word labels with word importance derived from a\npopular fine-tuned style classifier like BERT. Our results show that the BERT\noften finds content words not relevant to the target style as important words\nused in style prediction, but humans do not perceive the same way even though\nfor some styles (e.g., positive sentiment and joy) human- and\nmachine-identified words share significant overlap for some styles.", "published": "2021-09-06 21:01:17", "link": "http://arxiv.org/abs/2109.02738v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-end Neural Information Status Classification", "abstract": "Most previous studies on information status (IS) classification and bridging\nanaphora recognition assume that the gold mention or syntactic tree information\nis given (Hou et al., 2013; Roesiger et al., 2018; Hou, 2020; Yu and Poesio,\n2020). In this paper, we propose an end-to-end neural approach for information\nstatus classification. Our approach consists of a mention extraction component\nand an information status assignment component. During the inference time, our\nsystem takes a raw text as the input and generates mentions together with their\ninformation status. On the ISNotes corpus (Markert et al., 2012), we show that\nour information status assignment component achieves new state-of-the-art\nresults on fine-grained IS classification based on gold mentions. Furthermore,\nour system performs significantly better than other baselines for both mention\nextraction and fine-grained IS classification in the end-to-end setting.\nFinally, we apply our system on BASHI (Roesiger, 2018) and SciCorp (Roesiger,\n2016) to recognize referential bridging anaphora. We find that our end-to-end\nsystem trained on ISNotes achieves competitive results on bridging anaphora\nrecognition compared to the previous state-of-the-art system that relies on\nsyntactic information and is trained on the in-domain datasets (Yu and Poesio,\n2020).", "published": "2021-09-06 21:44:11", "link": "http://arxiv.org/abs/2109.02753v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Combinatorial Optimization for Word-level Adversarial Textual\n  Attack", "abstract": "Over the past few years, various word-level textual attack approaches have\nbeen proposed to reveal the vulnerability of deep neural networks used in\nnatural language processing. Typically, these approaches involve an important\noptimization step to determine which substitute to be used for each word in the\noriginal input. However, current research on this step is still rather limited,\nfrom the perspectives of both problem-understanding and problem-solving. In\nthis paper, we address these issues by uncovering the theoretical properties of\nthe problem and proposing an efficient local search algorithm (LS) to solve it.\nWe establish the first provable approximation guarantee on solving the problem\nin general cases.Extensive experiments involving 5 NLP tasks, 8 datasets and 26\nNLP models show that LS can largely reduce the number of queries usually by an\norder of magnitude to achieve high attack success rates. Further experiments\nshow that the adversarial examples crafted by LS usually have higher quality,\nexhibit better transferability, and can bring more robustness improvement to\nvictim models by adversarial training.", "published": "2021-09-06 03:44:43", "link": "http://arxiv.org/abs/2109.02229v3", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "BERT might be Overkill: A Tiny but Effective Biomedical Entity Linker\n  based on Residual Convolutional Neural Networks", "abstract": "Biomedical entity linking is the task of linking entity mentions in a\nbiomedical document to referent entities in a knowledge base. Recently, many\nBERT-based models have been introduced for the task. While these models have\nachieved competitive results on many datasets, they are computationally\nexpensive and contain about 110M parameters. Little is known about the factors\ncontributing to their impressive performance and whether the\nover-parameterization is needed. In this work, we shed some light on the inner\nworking mechanisms of these large BERT-based models. Through a set of probing\nexperiments, we have found that the entity linking performance only changes\nslightly when the input word order is shuffled or when the attention scope is\nlimited to a fixed window size. From these observations, we propose an\nefficient convolutional neural network with residual connections for biomedical\nentity linking. Because of the sparse connectivity and weight sharing\nproperties, our model has a small number of parameters and is highly efficient.\nOn five public datasets, our model achieves comparable or even better linking\naccuracy than the state-of-the-art BERT-based models while having about 60\ntimes fewer parameters.", "published": "2021-09-06 04:25:47", "link": "http://arxiv.org/abs/2109.02237v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "STaCK: Sentence Ordering with Temporal Commonsense Knowledge", "abstract": "Sentence order prediction is the task of finding the correct order of\nsentences in a randomly ordered document. Correctly ordering the sentences\nrequires an understanding of coherence with respect to the chronological\nsequence of events described in the text. Document-level contextual\nunderstanding and commonsense knowledge centered around these events are often\nessential in uncovering this coherence and predicting the exact chronological\norder. In this paper, we introduce STaCK -- a framework based on graph neural\nnetworks and temporal commonsense knowledge to model global information and\npredict the relative order of sentences. Our graph network accumulates temporal\nevidence using knowledge of `past' and `future' and formulates sentence\nordering as a constrained edge classification problem. We report results on\nfive different datasets, and empirically show that the proposed method is\nnaturally suitable for order prediction. The implementation of this work is\npublicly available at: https://github.com/declare-lab/sentence-ordering.", "published": "2021-09-06 05:29:48", "link": "http://arxiv.org/abs/2109.02247v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Numerical Reasoning Skills in the Modular Approach for Complex\n  Question Answering on Text", "abstract": "Numerical reasoning skills are essential for complex question answering (CQA)\nover text. It requires opertaions including counting, comparison, addition and\nsubtraction. A successful approach to CQA on text, Neural Module Networks\n(NMNs), follows the programmer-interpreter paradigm and leverages specialised\nmodules to perform compositional reasoning. However, the NMNs framework does\nnot consider the relationship between numbers and entities in both questions\nand paragraphs. We propose effective techniques to improve NMNs' numerical\nreasoning capabilities by making the interpreter question-aware and capturing\nthe relationship between entities and numbers. On the same subset of the DROP\ndataset for CQA on text, experimental results show that our additions\noutperform the original NMNs by 3.0 points for the overall F1 score.", "published": "2021-09-06 08:34:31", "link": "http://arxiv.org/abs/2109.02289v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Visual Dialog Questioner with Entity-based Strategy Learning\n  and Augmented Guesser", "abstract": "Considering the importance of building a good Visual Dialog (VD) Questioner,\nmany researchers study the topic under a Q-Bot-A-Bot image-guessing game\nsetting, where the Questioner needs to raise a series of questions to collect\ninformation of an undisclosed image. Despite progress has been made in\nSupervised Learning (SL) and Reinforcement Learning (RL), issues still exist.\nFirstly, previous methods do not provide explicit and effective guidance for\nQuestioner to generate visually related and informative questions. Secondly,\nthe effect of RL is hampered by an incompetent component, i.e., the Guesser,\nwho makes image predictions based on the generated dialogs and assigns rewards\naccordingly. To enhance VD Questioner: 1) we propose a Related entity enhanced\nQuestioner (ReeQ) that generates questions under the guidance of related\nentities and learns entity-based questioning strategy from human dialogs; 2) we\npropose an Augmented Guesser (AugG) that is strong and is optimized for the VD\nsetting especially. Experimental results on the VisDial v1.0 dataset show that\nour approach achieves state-of-theart performance on both image-guessing task\nand question diversity. Human study further proves that our model generates\nmore visually related, informative and coherent questions.", "published": "2021-09-06 08:58:43", "link": "http://arxiv.org/abs/2109.02297v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LightTag: Text Annotation Platform", "abstract": "Text annotation tools assume that their user's goal is to create a labeled\ncorpus. However, users view annotation as a necessary evil on the way to\ndeliver business value through NLP. Thus an annotation tool should optimize for\nthe throughput of the global NLP process, not only the productivity of\nindividual annotators. LightTag is a text annotation tool designed and built on\nthat principle. This paper shares our design rationale, data modeling choices,\nand user interface decisions then illustrates how those choices serve the full\nNLP lifecycle.", "published": "2021-09-06 09:41:48", "link": "http://arxiv.org/abs/2109.02320v1", "categories": ["cs.CL", "cs.AI", "I.7.2"], "primary_category": "cs.CL"}
{"title": "From Alignment to Assignment: Frustratingly Simple Unsupervised Entity\n  Alignment", "abstract": "Cross-lingual entity alignment (EA) aims to find the equivalent entities\nbetween crosslingual KGs, which is a crucial step for integrating KGs.\nRecently, many GNN-based EA methods are proposed and show decent performance\nimprovements on several public datasets. Meanwhile, existing GNN-based EA\nmethods inevitably inherit poor interpretability and low efficiency from neural\nnetworks. Motivated by the isomorphic assumption of GNNbased methods, we\nsuccessfully transform the cross-lingual EA problem into the assignment\nproblem. Based on this finding, we propose a frustratingly Simple but Effective\nUnsupervised entity alignment method (SEU) without neural networks. Extensive\nexperiments show that our proposed unsupervised method even beats advanced\nsupervised methods across all public datasets and has high efficiency,\ninterpretability, and stability.", "published": "2021-09-06 11:02:26", "link": "http://arxiv.org/abs/2109.02363v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PermuteFormer: Efficient Relative Position Encoding for Long Sequences", "abstract": "A recent variation of Transformer, Performer, scales Transformer to longer\nsequences with a linear attention mechanism. However, it is not compatible with\nrelative position encoding, which has advantages over absolute position\nencoding. In this paper, we discuss possible ways to add relative position\nencoding to Performer. Based on the analysis, we propose PermuteFormer, a\nPerformer-based model with relative position encoding that scales linearly on\nlong sequences. PermuteFormer applies position-dependent transformation on\nqueries and keys to encode positional information into the attention module.\nThis transformation is carefully crafted so that the final output of\nself-attention is not affected by absolute positions of tokens. PermuteFormer\nintroduces negligible computational overhead by design that it runs as fast as\nPerformer. We evaluate PermuteFormer on Long-Range Arena, a dataset for long\nsequences, as well as WikiText-103, a language modeling dataset. The\nexperiments show that PermuteFormer uniformly improves the performance of\nPerformer with almost no computational overhead and outperforms vanilla\nTransformer on most of the tasks.", "published": "2021-09-06 11:49:22", "link": "http://arxiv.org/abs/2109.02377v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Proto: A Neural Cocktail for Generating Appealing Conversations", "abstract": "In this paper, we present our Alexa Prize Grand Challenge 4 socialbot: Proto.\nLeveraging diverse sources of world knowledge, and powered by a suite of neural\nand rule-based natural language understanding modules, state-of-the-art neural\ngenerators, novel state-based deterministic generators, an ensemble of neural\nre-rankers, a robust post-processing algorithm, and an efficient overall\nconversation strategy, Proto strives to be able to converse coherently about a\ndiverse range of topics of interest to humans, and provide a memorable\nexperience to the user. In this paper we dissect and analyze the different\ncomponents and conversation strategies implemented by our socialbot, which\nenables us to generate colloquial, empathetic, engaging, self-rectifying,\nfactually correct, and on-topic response, which has helped us achieve\nconsistent scores throughout the competition.", "published": "2021-09-06 14:46:04", "link": "http://arxiv.org/abs/2109.02513v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "General-Purpose Question-Answering with Macaw", "abstract": "Despite the successes of pretrained language models, there are still few\nhigh-quality, general-purpose QA systems that are freely available. In\nresponse, we present Macaw, a versatile, generative question-answering (QA)\nsystem that we are making available to the community. Macaw is built on\nUnifiedQA, itself built on T5, and exhibits strong performance, zero-shot, on a\nwide variety of topics, including outperforming GPT-3 by over 10% (absolute) on\nChallenge300, a suite of 300 challenge questions, despite being an order of\nmagnitude smaller (11 billion vs. 175 billion parameters). In addition, Macaw\nallows different permutations (\"angles\") of its inputs and outputs to be used,\nfor example Macaw can take a question and produce an answer; or take an answer\nand produce a question; or take an answer and question, and produce\nmultiple-choice options. We describe the system, and illustrate a variety of\nquestion types where it produces surprisingly good answers, well outside the\ntraining setup. We also identify question classes where it still appears to\nstruggle, offering insights into the limitations of pretrained language models.\nMacaw is freely available, and we hope that it proves useful to the community.\nMacaw is available at https://github.com/allenai/macaw", "published": "2021-09-06 16:37:46", "link": "http://arxiv.org/abs/2109.02593v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WhyAct: Identifying Action Reasons in Lifestyle Vlogs", "abstract": "We aim to automatically identify human action reasons in online videos. We\nfocus on the widespread genre of lifestyle vlogs, in which people perform\nactions while verbally describing them. We introduce and make publicly\navailable the WhyAct dataset, consisting of 1,077 visual actions manually\nannotated with their reasons. We describe a multimodal model that leverages\nvisual and textual information to automatically infer the reasons corresponding\nto an action presented in the video.", "published": "2021-09-06 21:26:47", "link": "http://arxiv.org/abs/2109.02747v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "External knowledge transfer deployment inside a simple double agent\n  Viterbi algorithm", "abstract": "We consider in this paper deploying external knowledge transfer inside a\nsimple double agent Viterbi algorithm which is an algorithm firstly introduced\nby the author in his preprint \"Hidden Markov Based Mathematical Model dedicated\nto Extract Ingredients from Recipe Text\". The key challenge of this work lies\nin discovering the reason why our old model does have bad performances when it\nis confronted with estimating ingredient state for unknown words and see if\ndeploying external knowledge transfer directly on calculating state matrix\ncould be the solution instead of deploying it only on back propagating step.", "published": "2021-09-06 07:24:36", "link": "http://arxiv.org/abs/2110.00433v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data Science Kitchen at GermEval 2021: A Fine Selection of Hand-Picked\n  Features, Delivered Fresh from the Oven", "abstract": "This paper presents the contribution of the Data Science Kitchen at GermEval\n2021 shared task on the identification of toxic, engaging, and fact-claiming\ncomments. The task aims at extending the identification of offensive language,\nby including additional subtasks that identify comments which should be\nprioritized for fact-checking by moderators and community managers. Our\ncontribution focuses on a feature-engineering approach with a conventional\nclassification backend. We combine semantic and writing style embeddings\nderived from pre-trained deep neural networks with additional numerical\nfeatures, specifically designed for this task. Classifier ensembles are used to\nderive predictions for each subtask via a majority voting scheme. Our best\nsubmission achieved macro-averaged F1-scores of 66.8\\%,\\,69.9\\% and 72.5\\% for\nthe identification of toxic, engaging, and fact-claiming comments.", "published": "2021-09-06 12:00:29", "link": "http://arxiv.org/abs/2109.02383v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GPT-3 Models are Poor Few-Shot Learners in the Biomedical Domain", "abstract": "Deep neural language models have set new breakthroughs in many tasks of\nNatural Language Processing (NLP). Recent work has shown that deep transformer\nlanguage models (pretrained on large amounts of texts) can achieve high levels\nof task-specific few-shot performance comparable to state-of-the-art models.\nHowever, the ability of these large language models in few-shot transfer\nlearning has not yet been explored in the biomedical domain. We investigated\nthe performance of two powerful transformer language models, i.e. GPT-3 and\nBioBERT, in few-shot settings on various biomedical NLP tasks. The experimental\nresults showed that, to a great extent, both the models underperform a language\nmodel fine-tuned on the full training data. Although GPT-3 had already achieved\nnear state-of-the-art results in few-shot knowledge transfer on open-domain NLP\ntasks, it could not perform as effectively as BioBERT, which is orders of\nmagnitude smaller than GPT-3. Regarding that BioBERT was already pretrained on\nlarge biomedical text corpora, our study suggests that language models may\nlargely benefit from in-domain pretraining in task-specific few-shot learning.\nHowever, in-domain pretraining seems not to be sufficient; novel pretraining\nand few-shot learning strategies are required in the biomedical NLP domain.", "published": "2021-09-06 15:50:37", "link": "http://arxiv.org/abs/2109.02555v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Enhanced Event Extraction in Financial Documents", "abstract": "Event extraction is a classic task in natural language processing with wide\nuse in handling large amount of yet rapidly growing financial, legal, medical,\nand government documents which often contain multiple events with their\nelements scattered and mixed across the documents, making the problem much more\ndifficult. Though the underlying relations between event elements to be\nextracted provide helpful contextual information, they are somehow overlooked\nin prior studies. We showcase the enhancement to this task brought by utilizing\nthe knowledge graph that captures entity relations and their attributes. We\npropose a first event extraction framework that embeds a knowledge graph\nthrough a Graph Neural Network and integrates the embedding with regular\nfeatures, all at document-level. Specifically, for extracting events from\nChinese financial announcements, our method outperforms the state-of-the-art\nmethod by 5.3% in F1-score.", "published": "2021-09-06 16:35:15", "link": "http://arxiv.org/abs/2109.02592v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SS-BERT: Mitigating Identity Terms Bias in Toxic Comment Classification\n  by Utilising the Notion of \"Subjectivity\" and \"Identity Terms\"", "abstract": "Toxic comment classification models are often found biased toward identity\nterms which are terms characterizing a specific group of people such as\n\"Muslim\" and \"black\". Such bias is commonly reflected in false-positive\npredictions, i.e. non-toxic comments with identity terms. In this work, we\npropose a novel approach to tackle such bias in toxic comment classification,\nleveraging the notion of subjectivity level of a comment and the presence of\nidentity terms. We hypothesize that when a comment is made about a group of\npeople that is characterized by an identity term, the likelihood of that\ncomment being toxic is associated with the subjectivity level of the comment,\ni.e. the extent to which the comment conveys personal feelings and opinions.\nBuilding upon the BERT model, we propose a new structure that is able to\nleverage these features, and thoroughly evaluate our model on 4 datasets of\nvarying sizes and representing different social media platforms. The results\nshow that our model can consistently outperform BERT and a SOTA model devised\nto address identity term bias in a different way, with a maximum improvement in\nF1 of 2.43% and 1.91% respectively.", "published": "2021-09-06 18:40:06", "link": "http://arxiv.org/abs/2109.02691v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Question Answering via SPARQL Silhouette Generation", "abstract": "Knowledge Graph Question Answering (KGQA) has become a prominent area in\nnatural language processing due to the emergence of large-scale Knowledge\nGraphs (KGs). Recently Neural Machine Translation based approaches are gaining\nmomentum that translates natural language queries to structured query languages\nthereby solving the KGQA task. However, most of these methods struggle with\nout-of-vocabulary words where test entities and relations are not seen during\ntraining time. In this work, we propose a modular two-stage neural architecture\nto solve the KGQA task.\n  The first stage generates a sketch of the target SPARQL called SPARQL\nsilhouette for the input question. This comprises of (1) Noise simulator to\nfacilitate out-of-vocabulary words and to reduce vocabulary size (2) seq2seq\nmodel for text to SPARQL silhouette generation. The second stage is a Neural\nGraph Search Module. SPARQL silhouette generated in the first stage is\ndistilled in the second stage by substituting precise relation in the predicted\nstructure. We simulate ideal and realistic scenarios by designing a noise\nsimulator. Experimental results show that the quality of generated SPARQL\nsilhouette in the first stage is outstanding for the ideal scenarios but for\nrealistic scenarios (i.e. noisy linker), the quality of the resulting SPARQL\nsilhouette drops drastically. However, our neural graph search module recovers\nit considerably. We show that our method can achieve reasonable performance\nimproving the state-of-art by a margin of 3.72% F1 for the LC-QuAD-1 dataset.\nWe believe, our proposed approach is novel and will lead to dynamic KGQA\nsolutions that are suited for practical applications.", "published": "2021-09-06 14:55:37", "link": "http://arxiv.org/abs/2109.09475v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Audio-based Musical Version Identification: Elements and Challenges", "abstract": "In this article, we aim to provide a review of the key ideas and approaches\nproposed in 20 years of scientific literature around musical version\nidentification (VI) research and connect them to current practice. For more\nthan a decade, VI systems suffered from the accuracy-scalability trade-off,\nwith attempts to increase accuracy that typically resulted in cumbersome,\nnon-scalable systems. Recent years, however, have witnessed the rise of deep\nlearning-based approaches that take a step toward bridging the\naccuracy-scalability gap, yielding systems that can realistically be deployed\nin industrial applications. Although this trend positively influences the\nnumber of researchers and institutions working on VI, it may also result in\nobscuring the literature before the deep learning era. To appreciate two\ndecades of novel ideas in VI research and to facilitate building better\nsystems, we now review some of the successful concepts and applications\nproposed in the literature and study their evolution throughout the years.", "published": "2021-09-06 13:45:11", "link": "http://arxiv.org/abs/2109.02472v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "XMUSPEECH System for VoxCeleb Speaker Recognition Challenge 2021", "abstract": "This paper describes the XMUSPEECH speaker recognition and diarisation\nsystems for the VoxCeleb Speaker Recognition Challenge 2021. For track 2, we\nevaluate two systems including ResNet34-SE and ECAPA-TDNN. For track 4, an\nimportant part of our system is VAD module which greatly improves the\nperformance. Our best submission on the track 4 obtained on the evaluation set\nDER 5.54% and JER 27.11%, while the performance on the development set is DER\n2.92% and JER 20.84%.", "published": "2021-09-06 15:36:09", "link": "http://arxiv.org/abs/2109.02549v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Speaker Identification for Shared Devices by Adapting\n  Embeddings to Speaker Subsets", "abstract": "Speaker identification typically involves three stages. First, a front-end\nspeaker embedding model is trained to embed utterance and speaker profiles.\nSecond, a scoring function is applied between a runtime utterance and each\nspeaker profile. Finally, the speaker is identified using nearest neighbor\naccording to the scoring metric. To better distinguish speakers sharing a\ndevice within the same household, we propose a household-adapted nonlinear\nmapping to a low dimensional space to complement the global scoring metric. The\ncombined scoring function is optimized on labeled or pseudo-labeled speaker\nutterances. With input dropout, the proposed scoring model reduces EER by\n45-71% in simulated households with 2 to 7 hard-to-discriminate speakers per\nhousehold. On real-world internal data, the EER reduction is 49.2%. From t-SNE\nvisualization, we also show that clusters formed by household-adapted speaker\nembeddings are more compact and uniformly distributed, compared to clusters\nformed by global embeddings before adaptation.", "published": "2021-09-06 16:22:03", "link": "http://arxiv.org/abs/2109.02576v1", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Machine Learning: Challenges, Limitations, and Compatibility for Audio\n  Restoration Processes", "abstract": "In this paper machine learning networks are explored for their use in\nrestoring degraded and compressed speech audio. The project intent is to build\na new trained model from voice data to learn features of compression\nartifacting distortion introduced by data loss from lossy compression and\nresolution loss with an existing algorithm presented in SEGAN: Speech\nEnhancement Generative Adversarial Network. The resulting generator from the\nmodel was then to be used to restore degraded speech audio. This paper details\nan examination of the subsequent compatibility and operational issues presented\nby working with deprecated code, which obstructed the trained model from\nsuccessfully being developed. This paper further serves as an examination of\nthe challenges, limitations, and compatibility in the current state of machine\nlearning.", "published": "2021-09-06 18:40:51", "link": "http://arxiv.org/abs/2109.02692v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Binaural SoundNet: Predicting Semantics, Depth and Motion with Binaural\n  Sounds", "abstract": "Humans can robustly recognize and localize objects by using visual and/or\nauditory cues. While machines are able to do the same with visual data already,\nless work has been done with sounds. This work develops an approach for scene\nunderstanding purely based on binaural sounds. The considered tasks include\npredicting the semantic masks of sound-making objects, the motion of\nsound-making objects, and the depth map of the scene. To this aim, we propose a\nnovel sensor setup and record a new audio-visual dataset of street scenes with\neight professional binaural microphones and a 360-degree camera. The\nco-existence of visual and audio cues is leveraged for supervision transfer. In\nparticular, we employ a cross-modal distillation framework that consists of\nmultiple vision teacher methods and a sound student method -- the student\nmethod is trained to generate the same results as the teacher methods do. This\nway, the auditory system can be trained without using human annotations. To\nfurther boost the performance, we propose another novel auxiliary task, coined\nSpatial Sound Super-Resolution, to increase the directional resolution of\nsounds. We then formulate the four tasks into one end-to-end trainable\nmulti-tasking network aiming to boost the overall performance. Experimental\nresults show that 1) our method achieves good results for all four tasks, 2)\nthe four tasks are mutually beneficial -- training them together achieves the\nbest performance, 3) the number and orientation of microphones are both\nimportant, and 4) features learned from the standard spectrogram and features\nobtained by the classic signal processing pipeline are complementary for\nauditory perception tasks. The data and code are released.", "published": "2021-09-06 22:24:00", "link": "http://arxiv.org/abs/2109.02763v2", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Complementing Handcrafted Features with Raw Waveform Using a\n  Light-weight Auxiliary Model", "abstract": "An emerging trend in audio processing is capturing low-level speech\nrepresentations from raw waveforms. These representations have shown promising\nresults on a variety of tasks, such as speech recognition and speech\nseparation. Compared to handcrafted features, learning speech features via\nbackpropagation provides the model greater flexibility in how it represents\ndata for different tasks theoretically. However, results from empirical study\nshows that, in some tasks, such as voice spoof detection, handcrafted features\nare more competitive than learned features. Instead of evaluating handcrafted\nfeatures and raw waveforms independently, this paper proposes an Auxiliary\nRawnet model to complement handcrafted features with features learned from raw\nwaveforms. A key benefit of the approach is that it can improve accuracy at a\nrelatively low computational cost. The proposed Auxiliary Rawnet model is\ntested using the ASVspoof 2019 dataset and the results from this dataset\nindicate that a light-weight waveform encoder can potentially boost the\nperformance of handcrafted-features-based encoders in exchange for a small\namount of additional computational work.", "published": "2021-09-06 23:32:10", "link": "http://arxiv.org/abs/2109.02773v1", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FastAudio: A Learnable Audio Front-End for Spoof Speech Detection", "abstract": "Voice assistants, such as smart speakers, have exploded in popularity. It is\ncurrently estimated that the smart speaker adoption rate has exceeded 35% in\nthe US adult population. Manufacturers have integrated speaker identification\ntechnology, which attempts to determine the identity of the person speaking, to\nprovide personalized services to different members of the same family. Speaker\nidentification can also play an important role in controlling how the smart\nspeaker is used. For example, it is not critical to correctly identify the user\nwhen playing music. However, when reading the user's email out loud, it is\ncritical to correctly verify the speaker that making the request is the\nauthorized user. Speaker verification systems, which authenticate the speaker\nidentity, are therefore needed as a gatekeeper to protect against various\nspoofing attacks that aim to impersonate the enrolled user. This paper compares\npopular learnable front-ends which learn the representations of audio by joint\ntraining with downstream tasks (End-to-End). We categorize the front-ends by\ndefining two generic architectures and then analyze the filtering stages of\nboth types in terms of learning constraints. We propose replacing fixed\nfilterbanks with a learnable layer that can better adapt to anti-spoofing\ntasks. The proposed FastAudio front-end is then tested with two popular\nback-ends to measure the performance on the LA track of the ASVspoof 2019\ndataset. The FastAudio front-end achieves a relative improvement of 27% when\ncompared with fixed front-ends, outperforming all other learnable front-ends on\nthis task.", "published": "2021-09-06 23:32:10", "link": "http://arxiv.org/abs/2109.02774v1", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fruit-CoV: An Efficient Vision-based Framework for Speedy Detection and\n  Diagnosis of SARS-CoV-2 Infections Through Recorded Cough Sounds", "abstract": "SARS-CoV-2 is colloquially known as COVID-19 that had an initial outbreak in\nDecember 2019. The deadly virus has spread across the world, taking part in the\nglobal pandemic disease since March 2020. In addition, a recent variant of\nSARS-CoV-2 named Delta is intractably contagious and responsible for more than\nfour million deaths over the world. Therefore, it is vital to possess a\nself-testing service of SARS-CoV-2 at home. In this study, we introduce\nFruit-CoV, a two-stage vision framework, which is capable of detecting\nSARS-CoV-2 infections through recorded cough sounds. Specifically, we convert\nsounds into Log-Mel Spectrograms and use the EfficientNet-V2 network to extract\nits visual features in the first stage. In the second stage, we use 14\nconvolutional layers extracted from the large-scale Pretrained Audio Neural\nNetworks for audio pattern recognition (PANNs) and the Wavegram-Log-Mel-CNN to\naggregate feature representations of the Log-Mel Spectrograms. Finally, we use\nthe combined features to train a binary classifier. In this study, we use a\ndataset provided by the AICovidVN 115M Challenge, which includes a total of\n7371 recorded cough sounds collected throughout Vietnam, India, and\nSwitzerland. Experimental results show that our proposed model achieves an AUC\nscore of 92.8% and ranks the 1st place on the leaderboard of the AICovidVN\nChallenge. More importantly, our proposed framework can be integrated into a\ncall center or a VoIP system to speed up detecting SARS-CoV-2 infections\nthrough online/recorded cough sounds.", "published": "2021-09-06 07:56:02", "link": "http://arxiv.org/abs/2109.03219v1", "categories": ["cs.SD", "cs.LG", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
