{"title": "A Multilingual FrameNet-based Grammar and Lexicon for Controlled Natural\n  Language", "abstract": "Berkeley FrameNet is a lexico-semantic resource for English based on the\ntheory of frame semantics. It has been exploited in a range of natural language\nprocessing applications and has inspired the development of framenets for many\nlanguages. We present a methodological approach to the extraction and\ngeneration of a computational multilingual FrameNet-based grammar and lexicon.\nThe approach leverages FrameNet-annotated corpora to automatically extract a\nset of cross-lingual semantico-syntactic valence patterns. Based on data from\nBerkeley FrameNet and Swedish FrameNet, the proposed approach has been\nimplemented in Grammatical Framework (GF), a categorial grammar formalism\nspecialized for multilingual grammars. The implementation of the grammar and\nlexicon is supported by the design of FrameNet, providing a frame semantic\nabstraction layer, an interlingual semantic API (application programming\ninterface), over the interlingual syntactic API already provided by GF Resource\nGrammar Library. The evaluation of the acquired grammar and lexicon shows the\nfeasibility of the approach. Additionally, we illustrate how the FrameNet-based\ngrammar and lexicon are exploited in two distinct multilingual controlled\nnatural language applications. The produced resources are available under an\nopen source license.", "published": "2015-11-12 15:23:37", "link": "http://arxiv.org/abs/1511.03924v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Skip-gram Using Convolutional Pseudowords", "abstract": "This work studies the representational mapping across multimodal data such\nthat given a piece of the raw data in one modality the corresponding semantic\ndescription in terms of the raw data in another modality is immediately\nobtained. Such a representational mapping can be found in a wide spectrum of\nreal-world applications including image/video retrieval, object recognition,\naction/behavior recognition, and event understanding and prediction. To that\nend, we introduce a simplified training objective for learning multimodal\nembeddings using the skip-gram architecture by introducing convolutional\n\"pseudowords:\" embeddings composed of the additive combination of distributed\nword representations and image features from convolutional neural networks\nprojected into the multimodal space. We present extensive results of the\nrepresentational properties of these embeddings on various word similarity\nbenchmarks to show the promise of this approach.", "published": "2015-11-12 19:32:08", "link": "http://arxiv.org/abs/1511.04024v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "LSTM-based Deep Learning Models for Non-factoid Answer Selection", "abstract": "In this paper, we apply a general deep learning (DL) framework for the answer\nselection task, which does not depend on manually defined features or\nlinguistic tools. The basic framework is to build the embeddings of questions\nand answers based on bidirectional long short-term memory (biLSTM) models, and\nmeasure their closeness by cosine similarity. We further extend this basic\nmodel in two directions. One direction is to define a more composite\nrepresentation for questions and answers by combining convolutional neural\nnetwork with the basic framework. The other direction is to utilize a simple\nbut efficient attention mechanism in order to generate the answer\nrepresentation according to the question context. Several variations of models\nare provided. The models are examined by two datasets, including TREC-QA and\nInsuranceQA. Experimental results demonstrate that the proposed models\nsubstantially outperform several strong baselines.", "published": "2015-11-12 22:01:54", "link": "http://arxiv.org/abs/1511.04108v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Grounding of Textual Phrases in Images by Reconstruction", "abstract": "Grounding (i.e. localizing) arbitrary, free-form textual phrases in visual\ncontent is a challenging problem with many applications for human-computer\ninteraction and image-text reference resolution. Few datasets provide the\nground truth spatial localization of phrases, thus it is desirable to learn\nfrom data with no or little grounding supervision. We propose a novel approach\nwhich learns grounding by reconstructing a given phrase using an attention\nmechanism, which can be either latent or optimized directly. During training\nour approach encodes the phrase using a recurrent network language model and\nthen learns to attend to the relevant image region in order to reconstruct the\ninput phrase. At test time, the correct attention, i.e., the grounding, is\nevaluated. If grounding supervision is available it can be directly applied via\na loss over the attention mechanism. We demonstrate the effectiveness of our\napproach on the Flickr 30k Entities and ReferItGame datasets with different\nlevels of supervision, ranging from no supervision over partial supervision to\nfull supervision. Our supervised variant improves by a large margin over the\nstate-of-the-art on both datasets.", "published": "2015-11-12 01:13:47", "link": "http://arxiv.org/abs/1511.03745v4", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Document Context Language Models", "abstract": "Text documents are structured on multiple levels of detail: individual words\nare related by syntax, but larger units of text are related by discourse\nstructure. Existing language models generally fail to account for discourse\nstructure, but it is crucial if we are to have language models that reward\ncoherence and generate coherent texts. We present and empirically evaluate a\nset of multi-level recurrent neural network language models, called\nDocument-Context Language Models (DCLM), which incorporate contextual\ninformation both within and beyond the sentence. In comparison with word-level\nrecurrent neural network language models, the DCLM models obtain slightly\nbetter predictive likelihoods, and considerably better assessments of document\ncoherence.", "published": "2015-11-12 16:53:50", "link": "http://arxiv.org/abs/1511.03962v4", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
