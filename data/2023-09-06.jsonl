{"title": "Zero-Resource Hallucination Prevention for Large Language Models", "abstract": "The prevalent use of large language models (LLMs) in various domains has\ndrawn attention to the issue of \"hallucination,\" which refers to instances\nwhere LLMs generate factually inaccurate or ungrounded information. Existing\ntechniques for hallucination detection in language assistants rely on intricate\nfuzzy, specific free-language-based chain of thought (CoT) techniques or\nparameter-based methods that suffer from interpretability issues. Additionally,\nthe methods that identify hallucinations post-generation could not prevent\ntheir occurrence and suffer from inconsistent performance due to the influence\nof the instruction format and model style. In this paper, we introduce a novel\npre-detection self-evaluation technique, referred to as SELF-FAMILIARITY, which\nfocuses on evaluating the model's familiarity with the concepts present in the\ninput instruction and withholding the generation of response in case of\nunfamiliar concepts. This approach emulates the human ability to refrain from\nresponding to unfamiliar topics, thus reducing hallucinations. We validate\nSELF-FAMILIARITY across four different large language models, demonstrating\nconsistently superior performance compared to existing techniques. Our findings\npropose a significant shift towards preemptive strategies for hallucination\nmitigation in LLM assistants, promising improvements in reliability,\napplicability, and interpretability.", "published": "2023-09-06 01:57:36", "link": "http://arxiv.org/abs/2309.02654v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models", "abstract": "Large language models (LLMs) trained on massive corpora demonstrate\nimpressive capabilities in a wide range of tasks. While there are ongoing\nefforts to adapt these models to languages beyond English, the attention given\nto their evaluation methodologies remains limited. Current multilingual\nbenchmarks often rely on back translations or re-implementations of English\ntests, limiting their capacity to capture unique cultural and linguistic\nnuances. To bridge this gap for the Korean language, we introduce the HAE-RAE\nBench, a dataset curated to challenge models lacking Korean cultural and\ncontextual depth. The dataset encompasses six downstream tasks across four\ndomains: vocabulary, history, general knowledge, and reading comprehension.\nUnlike traditional evaluation suites focused on token and sequence\nclassification or mathematical and logical reasoning, the HAE-RAE Bench\nemphasizes a model's aptitude for recalling Korean-specific knowledge and\ncultural contexts. Comparative analysis with prior Korean benchmarks indicates\nthat the HAE-RAE Bench presents a greater challenge to non-Korean models by\ndisturbing abilities and knowledge learned from English being transferred.", "published": "2023-09-06 04:38:16", "link": "http://arxiv.org/abs/2309.02706v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Agent-based simulation of pedestrians' earthquake evacuation;\n  application to Beirut, Lebanon", "abstract": "Most seismic risk assessment methods focus on estimating the damages to the\nbuilt environment and the consequent socioeconomic losses without fully taking\ninto account the social aspect of risk. Yet, human behaviour is a key element\nin predicting the human impact of an earthquake, therefore, it is important to\ninclude it in quantitative risk assessment studies. In this study, an\ninterdisciplinary approach simulating pedestrians' evacuation during\nearthquakes at the city scale is developed using an agent-based model. The\nmodel integrates the seismic hazard, the physical vulnerability as well as\nindividuals' behaviours and mobility. The simulator is applied to the case of\nBeirut, Lebanon. Lebanon is at the heart of the Levant fault system that has\ngenerated several Mw>7 earthquakes, the latest being in 1759. It is one of the\ncountries with the highest seismic risk in the Mediterranean region. This is\ndue to the high seismic vulnerability of the buildings due to the absence of\nmandatory seismic regulation until 2012, the high level of urbanization, and\nthe lack of adequate spatial planning and risk prevention policies. Beirut as\nthe main residential, economic and institutional hub of Lebanon is densely\npopulated. To accommodate the growing need for urban development, constructions\nhave almost taken over all of the green areas of the city; squares and gardens\nare disappearing to give place to skyscrapers. However, open spaces are safe\nplaces to shelter, away from debris, and therefore play an essential role in\nearthquake evacuation. Despite the massive urbanization, there are a few open\nspaces but locked gates and other types of anthropogenic barriers often limit\ntheir access. To simulate this complex context, pedestrians' evacuation\nsimulations are run in a highly realistic spatial environment implemented in\nGAMA [1]. Previous data concerning soil and buildings in Beirut [2, 3] are\ncomplemented by new geographic data extracted from high-resolution Pleiades\nsatellite images. The seismic loading is defined as a peak ground acceleration\nof 0.3g, as stated in Lebanese seismic regulations. Building damages are\nestimated using an artificial neural network trained to predict the mean damage\n[4] based on the seismic loading as well as the soil and building vibrational\nproperties [5]. Moreover, the quantity and the footprint of the generated\ndebris around each building are also estimated and included in the model. We\nsimulate how topography, buildings, debris, and access to open spaces, affect\nindividuals' mobility. Two city configurations are implemented: 1. Open spaces\nare accessible without any barriers; 2. Access to some open spaces is blocked.\nThe first simulation results show that while 52% of the population is able to\narrive to an open space within 5 minutes after an earthquake, this number is\nreduced to 39% when one of the open spaces is locked. These results show that\nthe presence of accessible open spaces in a city and their proximity to the\nresidential buildings is a crucial factor for ensuring people's safety when an\nearthquake occurs.", "published": "2023-09-06 08:00:17", "link": "http://arxiv.org/abs/2309.02812v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aligning Large Language Models for Clinical Tasks", "abstract": "Large Language Models (LLMs) have demonstrated remarkable adaptability,\nshowcasing their capacity to excel in tasks for which they were not explicitly\ntrained. However, despite their impressive natural language processing (NLP)\ncapabilities, effective alignment of LLMs remains a crucial challenge when\ndeploying them for specific clinical applications. The ability to generate\nresponses with factually accurate content and to engage in non-trivial\nreasoning steps are crucial for the LLMs to be eligible for applications in\nclinical medicine. Employing a combination of techniques including\ninstruction-tuning and in-prompt strategies like few-shot and chain-of-thought\nprompting has significantly enhanced the performance of LLMs. Our proposed\nalignment strategy for medical question-answering, known as\n'expand-guess-refine', offers a parameter and data-efficient solution. A\npreliminary analysis of this method demonstrated outstanding performance,\nachieving a score of 70.63% on a subset of questions sourced from the USMLE\ndataset.", "published": "2023-09-06 10:20:06", "link": "http://arxiv.org/abs/2309.02884v2", "categories": ["cs.CL", "I.2, I.7, J.3"], "primary_category": "cs.CL"}
{"title": "ViCGCN: Graph Convolutional Network with Contextualized Language Models\n  for Social Media Mining in Vietnamese", "abstract": "Social media processing is a fundamental task in natural language processing\nwith numerous applications. As Vietnamese social media and information science\nhave grown rapidly, the necessity of information-based mining on Vietnamese\nsocial media has become crucial. However, state-of-the-art research faces\nseveral significant drawbacks, including imbalanced data and noisy data on\nsocial media platforms. Imbalanced and noisy are two essential issues that need\nto be addressed in Vietnamese social media texts. Graph Convolutional Networks\ncan address the problems of imbalanced and noisy data in text classification on\nsocial media by taking advantage of the graph structure of the data. This study\npresents a novel approach based on contextualized language model (PhoBERT) and\ngraph-based method (Graph Convolutional Networks). In particular, the proposed\napproach, ViCGCN, jointly trained the power of Contextualized embeddings with\nthe ability of Graph Convolutional Networks, GCN, to capture more syntactic and\nsemantic dependencies to address those drawbacks. Extensive experiments on\nvarious Vietnamese benchmark datasets were conducted to verify our approach.\nThe observation shows that applying GCN to BERTology models as the final layer\nsignificantly improves performance. Moreover, the experiments demonstrate that\nViCGCN outperforms 13 powerful baseline models, including BERTology models,\nfusion BERTology and GCN models, other baselines, and SOTA on three benchmark\nsocial media datasets. Our proposed ViCGCN approach demonstrates a significant\nimprovement of up to 6.21%, 4.61%, and 2.63% over the best Contextualized\nLanguage Models, including multilingual and monolingual, on three benchmark\ndatasets, UIT-VSMEC, UIT-ViCTSD, and UIT-VSFC, respectively. Additionally, our\nintegrated model ViCGCN achieves the best performance compared to other\nBERTology integrated with GCN models.", "published": "2023-09-06 10:51:34", "link": "http://arxiv.org/abs/2309.02902v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leave no Place Behind: Improved Geolocation in Humanitarian Documents", "abstract": "Geographical location is a crucial element of humanitarian response,\noutlining vulnerable populations, ongoing events, and available resources.\nLatest developments in Natural Language Processing may help in extracting vital\ninformation from the deluge of reports and documents produced by the\nhumanitarian sector. However, the performance and biases of existing\nstate-of-the-art information extraction tools are unknown. In this work, we\ndevelop annotated resources to fine-tune the popular Named Entity Recognition\n(NER) tools Spacy and roBERTa to perform geotagging of humanitarian texts. We\nthen propose a geocoding method FeatureRank which links the candidate locations\nto the GeoNames database. We find that not only does the humanitarian-domain\ndata improves the performance of the classifiers (up to F1 = 0.92), but it also\nalleviates some of the bias of the existing tools, which erroneously favor\nlocations in the Western countries. Thus, we conclude that more resources from\nnon-Western documents are necessary to ensure that off-the-shelf NER systems\nare suitable for the deployment in the humanitarian sector.", "published": "2023-09-06 11:20:02", "link": "http://arxiv.org/abs/2309.02914v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from\n  Knowledge Graphs", "abstract": "Large language models (LLMs), such as ChatGPT and GPT-4, are versatile and\ncan solve different tasks due to their emergent ability and generalizability.\nHowever, LLMs sometimes lack domain-specific knowledge to perform tasks, which\nwould also cause hallucination during inference. In some previous works,\nadditional modules like graph neural networks (GNNs) are trained on retrieved\nknowledge from external knowledge bases, aiming to mitigate the problem of\nlacking domain-specific knowledge. However, incorporating additional modules:\n1) would need retraining additional modules when encountering novel domains; 2)\nwould become a bottleneck since LLMs' strong abilities are not fully utilized\nfor retrieval. In this paper, we propose a paradigm, termed Knowledge Solver\n(KSL), to teach LLMs to search for essential knowledge from external knowledge\nbases by harnessing their own strong generalizability. Specifically, we design\na simple yet effective prompt to transform retrieval into a multi-hop decision\nsequence, which empowers LLMs with searching knowledge ability in zero-shot\nmanner. Additionally, KSL is able to provide complete retrieval paths and\ntherefore increase explainability of LLMs' reasoning processes. We conduct\nexperiments on three datasets: CommonsenseQA, OpenbookQA, and MedQA-USMLE, and\nfound that our approach improves LLM baseline performance by a relatively large\nmargin.", "published": "2023-09-06 15:55:01", "link": "http://arxiv.org/abs/2309.03118v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Everyone Deserves A Reward: Learning Customized Human Preferences", "abstract": "Reward models (RMs) are essential for aligning large language models (LLMs)\nwith human preferences to improve interaction quality. However, the real world\nis pluralistic, which leads to diversified human preferences with respect to\ndifferent religions, politics, cultures, etc. Moreover, each individual can\nhave their unique preferences on various topics. Neglecting the diversity of\nhuman preferences, current human feedback aligning methods only consider a\ngeneral reward model, which is below satisfaction for customized or\npersonalized application scenarios. To explore customized preference learning,\nwe collect a domain-specific preference (DSP) dataset, which includes preferred\nresponses for each given query from four practical domains. Besides, from the\nperspective of data efficiency, we propose a three-stage customized RM learning\nscheme, then empirically verify its effectiveness on both general preference\ndatasets and our DSP set. Furthermore, we test multiple training and data\nstrategies on the three learning stages. We find several ways to better\npreserve the general preferring ability while training the customized RMs,\nespecially general preference enrichment, and customized preference imitation\nlearning. The DSP dataset and code are available at\nhttps://github.com/Linear95/DSP.", "published": "2023-09-06 16:03:59", "link": "http://arxiv.org/abs/2309.03126v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gender-specific Machine Translation with Large Language Models", "abstract": "While machine translation (MT) systems have seen significant improvements, it\nis still common for translations to reflect societal biases, such as gender\nbias. Decoder-only Large Language Models (LLMs) have demonstrated potential in\nMT, albeit with performance slightly lagging behind traditional encoder-decoder\nNeural Machine Translation (NMT) systems. However, LLMs offer a unique\nadvantage: the ability to control the properties of the output through prompts.\nIn this study, we leverage this flexibility to explore LLaMa's capability to\nproduce gender-specific translations. Our results indicate that LLaMa can\ngenerate gender-specific translations with translation accuracy and gender bias\ncomparable to NLLB, a state-of-the-art multilingual NMT system. Furthermore,\nour experiments reveal that LLaMa's gender-specific translations rely on\ncoreference resolution to determine gender, showing higher gender variance in\ngender-ambiguous datasets but maintaining consistency in less ambiguous\ncontexts. This research investigates the potential and challenges of using LLMs\nfor gender-specific translations as an instance of the controllability of\noutputs offered by LLMs.", "published": "2023-09-06 17:24:06", "link": "http://arxiv.org/abs/2309.03175v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Narrative as a Dynamical System", "abstract": "There is increasing evidence that human activity in general, and narrative in\nparticular, can be treated as a dynamical system in the physics sense; a system\nwhose evolution is described by an action integral, such that the average of\nall possible paths from point A to point B is given by the extremum of the\naction. We create by construction three such paths by averaging about 500\ndifferent narratives, and we show that the average path is consistent with an\naction principle.", "published": "2023-09-06 16:56:33", "link": "http://arxiv.org/abs/2309.06600v2", "categories": ["cs.CL", "J.5; I.2.7"], "primary_category": "cs.CL"}
{"title": "Epi-Curriculum: Episodic Curriculum Learning for Low-Resource Domain\n  Adaptation in Neural Machine Translation", "abstract": "Neural Machine Translation (NMT) models have become successful, but their\nperformance remains poor when translating on new domains with a limited number\nof data. In this paper, we present a novel approach Epi-Curriculum to address\nlow-resource domain adaptation (DA), which contains a new episodic training\nframework along with denoised curriculum learning. Our episodic training\nframework enhances the model's robustness to domain shift by episodically\nexposing the encoder/decoder to an inexperienced decoder/encoder. The denoised\ncurriculum learning filters the noised data and further improves the model's\nadaptability by gradually guiding the learning process from easy to more\ndifficult tasks. Experiments on English-German and English-Romanian translation\nshow that: (i) Epi-Curriculum improves both model's robustness and adaptability\nin seen and unseen domains; (ii) Our episodic training framework enhances the\nencoder and decoder's robustness to domain shift.", "published": "2023-09-06 00:59:27", "link": "http://arxiv.org/abs/2309.02640v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Joint Study of Phrase Grounding and Task Performance in Vision and\n  Language Models", "abstract": "Key to tasks that require reasoning about natural language in visual contexts\nis grounding words and phrases to image regions. However, observing this\ngrounding in contemporary models is complex, even if it is generally expected\nto take place if the task is addressed in a way that is conductive to\ngeneralization. We propose a framework to jointly study task performance and\nphrase grounding, and propose three benchmarks to study the relation between\nthe two. Our results show that contemporary models demonstrate inconsistency\nbetween their ability to ground phrases and solve tasks. We show how this can\nbe addressed through brute-force training on ground phrasing annotations, and\nanalyze the dynamics it creates. Code and at available at\nhttps://github.com/lil-lab/phrase_grounding.", "published": "2023-09-06 03:54:57", "link": "http://arxiv.org/abs/2309.02691v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Automated Open-domain Scientific Hypotheses\n  Discovery", "abstract": "Hypothetical induction is recognized as the main reasoning type when\nscientists make observations about the world and try to propose hypotheses to\nexplain those observations. Past research on hypothetical induction is under a\nconstrained setting: (1) the observation annotations in the dataset are\ncarefully manually handpicked sentences (resulting in a close-domain setting);\nand (2) the ground truth hypotheses are mostly commonsense knowledge, making\nthe task less challenging. In this work, we tackle these problems by proposing\nthe first dataset for social science academic hypotheses discovery, with the\nfinal goal to create systems that automatically generate valid, novel, and\nhelpful scientific hypotheses, given only a pile of raw web corpus. Unlike\nprevious settings, the new dataset requires (1) using open-domain data (raw web\ncorpus) as observations; and (2) proposing hypotheses even new to humanity. A\nmulti-module framework is developed for the task, including three different\nfeedback mechanisms to boost performance, which exhibits superior performance\nin terms of both GPT-4 based and expert-based evaluation. To the best of our\nknowledge, this is the first work showing that LLMs are able to generate novel\n(''not existing in literature'') and valid (''reflecting reality'') scientific\nhypotheses.", "published": "2023-09-06 05:19:41", "link": "http://arxiv.org/abs/2309.02726v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HC3 Plus: A Semantic-Invariant Human ChatGPT Comparison Corpus", "abstract": "ChatGPT has garnered significant interest due to its impressive performance;\nhowever, there is growing concern about its potential risks, particularly in\nthe detection of AI-generated content (AIGC), which is often challenging for\nuntrained individuals to identify. Current datasets used for detecting\nChatGPT-generated text primarily focus on question-answering tasks, often\noverlooking tasks with semantic-invariant properties, such as summarization,\ntranslation, and paraphrasing. In this paper, we demonstrate that detecting\nmodel-generated text in semantic-invariant tasks is more challenging. To\naddress this gap, we introduce a more extensive and comprehensive dataset that\nincorporates a wider range of tasks than previous work, including those with\nsemantic-invariant properties. In addition, instruction fine-tuning has\ndemonstrated superior performance across various tasks. In this paper, we\nexplore the use of instruction fine-tuning models for detecting text generated\nby ChatGPT.", "published": "2023-09-06 05:33:57", "link": "http://arxiv.org/abs/2309.02731v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rubric-Specific Approach to Automated Essay Scoring with Augmentation\n  Training", "abstract": "Neural based approaches to automatic evaluation of subjective responses have\nshown superior performance and efficiency compared to traditional rule-based\nand feature engineering oriented solutions. However, it remains unclear whether\nthe suggested neural solutions are sufficient replacements of human raters as\nwe find recent works do not properly account for rubric items that are\nessential for automated essay scoring during model training and validation. In\nthis paper, we propose a series of data augmentation operations that train and\ntest an automated scoring model to learn features and functions overlooked by\nprevious works while still achieving state-of-the-art performance in the\nAutomated Student Assessment Prize dataset.", "published": "2023-09-06 05:51:19", "link": "http://arxiv.org/abs/2309.02740v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Hot or Cold? Adaptive Temperature Sampling for Code Generation with\n  Large Language Models", "abstract": "Recently, Large Language Models (LLMs) have shown impressive abilities in\ncode generation. However, existing LLMs' decoding strategies are designed for\nNatural Language (NL) generation, overlooking the differences between NL and\nprogramming languages (PL). Due to this oversight, a better decoding strategy\nfor code generation remains an open question. In this paper, we conduct the\nfirst systematic study to explore a decoding strategy specialized in code\ngeneration. With an analysis of loss distributions of code tokens, we find that\ncode tokens can be divided into two categories: challenging tokens that are\ndifficult to predict and confident tokens that can be easily inferred. Among\nthem, the challenging tokens mainly appear at the beginning of a code block.\nInspired by the above findings, we propose a simple yet effective method:\nAdaptive Temperature (AdapT) sampling, which dynamically adjusts the\ntemperature coefficient when decoding different tokens. We apply a larger\ntemperature when sampling for challenging tokens, allowing LLMs to explore\ndiverse choices. We employ a smaller temperature for confident tokens avoiding\nthe influence of tail randomness noises. We apply AdapT sampling to LLMs with\ndifferent sizes and conduct evaluations on two popular datasets. Results show\nthat AdapT sampling significantly outperforms state-of-the-art decoding\nstrategy.", "published": "2023-09-06 06:27:33", "link": "http://arxiv.org/abs/2309.02772v3", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Promoting Open-domain Dialogue Generation through Learning Pattern\n  Information between Contexts and Responses", "abstract": "Recently, utilizing deep neural networks to build the opendomain dialogue\nmodels has become a hot topic. However, the responses generated by these models\nsuffer from many problems such as responses not being contextualized and tend\nto generate generic responses that lack information content, damaging the\nuser's experience seriously. Therefore, many studies try introducing more\ninformation into the dialogue models to make the generated responses more vivid\nand informative. Unlike them, this paper improves the quality of generated\nresponses by learning the implicit pattern information between contexts and\nresponses in the training samples. In this paper, we first build an open-domain\ndialogue model based on the pre-trained language model (i.e., GPT-2). And then,\nan improved scheduled sampling method is proposed for pre-trained models, by\nwhich the responses can be used to guide the response generation in the\ntraining phase while avoiding the exposure bias problem. More importantly, we\ndesign a response-aware mechanism for mining the implicit pattern information\nbetween contexts and responses so that the generated replies are more diverse\nand approximate to human replies. Finally, we evaluate the proposed model (RAD)\non the Persona-Chat and DailyDialog datasets; and the experimental results show\nthat our model outperforms the baselines on most automatic and manual metrics.", "published": "2023-09-06 08:11:39", "link": "http://arxiv.org/abs/2309.02823v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A deep Natural Language Inference predictor without language-specific\n  training data", "abstract": "In this paper we present a technique of NLP to tackle the problem of\ninference relation (NLI) between pairs of sentences in a target language of\nchoice without a language-specific training dataset. We exploit a generic\ntranslation dataset, manually translated, along with two instances of the same\npre-trained model - the first to generate sentence embeddings for the source\nlanguage, and the second fine-tuned over the target language to mimic the\nfirst. This technique is known as Knowledge Distillation. The model has been\nevaluated over machine translated Stanford NLI test dataset, machine translated\nMulti-Genre NLI test dataset, and manually translated RTE3-ITA test dataset. We\nalso test the proposed architecture over different tasks to empirically\ndemonstrate the generality of the NLI task. The model has been evaluated over\nthe native Italian ABSITA dataset, on the tasks of Sentiment Analysis,\nAspect-Based Sentiment Analysis, and Topic Recognition. We emphasise the\ngenerality and exploitability of the Knowledge Distillation technique that\noutperforms other methodologies based on machine translation, even though the\nformer was not directly trained on the data it was tested over.", "published": "2023-09-06 10:20:59", "link": "http://arxiv.org/abs/2309.02887v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Challenges of Building Datasets for Hate Speech Detection", "abstract": "Detection of hate speech has been formulated as a standalone application of\nNLP and different approaches have been adopted for identifying the target\ngroups, obtaining raw data, defining the labeling process, choosing the\ndetection algorithm, and evaluating the performance in the desired setting.\nHowever, unlike other downstream tasks, hate speech suffers from the lack of\nlarge-sized, carefully curated, generalizable datasets owing to the highly\nsubjective nature of the task. In this paper, we first analyze the issues\nsurrounding hate speech detection through a data-centric lens. We then outline\na holistic framework to encapsulate the data creation pipeline across seven\nbroad dimensions by taking the specific example of hate speech towards sexual\nminorities. We posit that practitioners would benefit from following this\nframework as a form of best practice when creating hate speech datasets in the\nfuture.", "published": "2023-09-06 11:15:47", "link": "http://arxiv.org/abs/2309.02912v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Persona-aware Generative Model for Code-mixed Language", "abstract": "Code-mixing and script-mixing are prevalent across online social networks and\nmultilingual societies. However, a user's preference toward code-mixing depends\non the socioeconomic status, demographics of the user, and the local context,\nwhich existing generative models mostly ignore while generating code-mixed\ntexts. In this work, we make a pioneering attempt to develop a persona-aware\ngenerative model to generate texts resembling real-life code-mixed texts of\nindividuals. We propose a Persona-aware Generative Model for Code-mixed\nGeneration, PARADOX, a novel Transformer-based encoder-decoder model that\nencodes an utterance conditioned on a user's persona and generates code-mixed\ntexts without monolingual reference data. We propose an alignment module that\nre-calibrates the generated sequence to resemble real-life code-mixed texts.\nPARADOX generates code-mixed texts that are semantically more meaningful and\nlinguistically more valid. To evaluate the personification capabilities of\nPARADOX, we propose four new metrics -- CM BLEU, CM Rouge-1, CM Rouge-L and CM\nKS. On average, PARADOX achieves 1.6 points better CM BLEU, 47% better\nperplexity and 32% better semantic coherence than the non-persona-based\ncounterparts.", "published": "2023-09-06 11:20:41", "link": "http://arxiv.org/abs/2309.02915v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ContrastWSD: Enhancing Metaphor Detection with Word Sense Disambiguation\n  Following the Metaphor Identification Procedure", "abstract": "This paper presents ContrastWSD, a RoBERTa-based metaphor detection model\nthat integrates the Metaphor Identification Procedure (MIP) and Word Sense\nDisambiguation (WSD) to extract and contrast the contextual meaning with the\nbasic meaning of a word to determine whether it is used metaphorically in a\nsentence. By utilizing the word senses derived from a WSD model, our model\nenhances the metaphor detection process and outperforms other methods that rely\nsolely on contextual embeddings or integrate only the basic definitions and\nother external knowledge. We evaluate our approach on various benchmark\ndatasets and compare it with strong baselines, indicating the effectiveness in\nadvancing metaphor detection.", "published": "2023-09-06 15:41:38", "link": "http://arxiv.org/abs/2309.03103v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "J-Guard: Journalism Guided Adversarially Robust Detection of\n  AI-generated News", "abstract": "The rapid proliferation of AI-generated text online is profoundly reshaping\nthe information landscape. Among various types of AI-generated text,\nAI-generated news presents a significant threat as it can be a prominent source\nof misinformation online. While several recent efforts have focused on\ndetecting AI-generated text in general, these methods require enhanced\nreliability, given concerns about their vulnerability to simple adversarial\nattacks. Furthermore, due to the eccentricities of news writing, applying these\ndetection methods for AI-generated news can produce false positives,\npotentially damaging the reputation of news organizations. To address these\nchallenges, we leverage the expertise of an interdisciplinary team to develop a\nframework, J-Guard, capable of steering existing supervised AI text detectors\nfor detecting AI-generated news while boosting adversarial robustness. By\nincorporating stylistic cues inspired by the unique journalistic attributes,\nJ-Guard effectively distinguishes between real-world journalism and\nAI-generated news articles. Our experiments on news articles generated by a\nvast array of AI models, including ChatGPT (GPT3.5), demonstrate the\neffectiveness of J-Guard in enhancing detection capabilities while maintaining\nan average performance decrease of as low as 7% when faced with adversarial\nattacks.", "published": "2023-09-06 17:06:31", "link": "http://arxiv.org/abs/2309.03164v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Framework-Based Qualitative Analysis of Free Responses of Large Language\n  Models: Algorithmic Fidelity", "abstract": "Today, using Large-scale generative Language Models (LLMs) it is possible to\nsimulate free responses to interview questions like those traditionally\nanalyzed using qualitative research methods. Qualitative methodology\nencompasses a broad family of techniques involving manual analysis of\nopen-ended interviews or conversations conducted freely in natural language.\nHere we consider whether artificial \"silicon participants\" generated by LLMs\nmay be productively studied using qualitative methods aiming to produce\ninsights that could generalize to real human populations. The key concept in\nour analysis is algorithmic fidelity, a term introduced by Argyle et al. (2023)\ncapturing the degree to which LLM-generated outputs mirror human\nsub-populations' beliefs and attitudes. By definition, high algorithmic\nfidelity suggests latent beliefs elicited from LLMs may generalize to real\nhumans, whereas low algorithmic fidelity renders such research invalid. Here we\nused an LLM to generate interviews with silicon participants matching specific\ndemographic characteristics one-for-one with a set of human participants. Using\nframework-based qualitative analysis, we showed the key themes obtained from\nboth human and silicon participants were strikingly similar. However, when we\nanalyzed the structure and tone of the interviews we found even more striking\ndifferences. We also found evidence of the hyper-accuracy distortion described\nby Aher et al. (2023). We conclude that the LLM we tested (GPT-3.5) does not\nhave sufficient algorithmic fidelity to expect research on it to generalize to\nhuman populations. However, the rapid pace of LLM research makes it plausible\nthis could change in the future. Thus we stress the need to establish epistemic\nnorms now around how to assess validity of LLM-based qualitative research,\nespecially concerning the need to ensure representation of heterogeneous lived\nexperiences.", "published": "2023-09-06 15:00:44", "link": "http://arxiv.org/abs/2309.06364v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Synthetic Text Generation using Hypergraph Representations", "abstract": "Generating synthetic variants of a document is often posed as text-to-text\ntransformation. We propose an alternate LLM based method that first decomposes\na document into semantic frames and then generates text using this interim\nsparse format. The frames are modeled using a hypergraph, which allows\nperturbing the frame contents in a principled manner. Specifically, new\nhyperedges are mined through topological analysis and complex polyadic\nrelationships including hierarchy and temporal dynamics are accommodated. We\nshow that our solution generates documents that are diverse, coherent and vary\nin style, sentiment, format, composition and facts.", "published": "2023-09-06 14:14:37", "link": "http://arxiv.org/abs/2309.06550v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Certifying LLM Safety against Adversarial Prompting", "abstract": "Large language models (LLMs) are vulnerable to adversarial attacks that add\nmalicious tokens to an input prompt to bypass the safety guardrails of an LLM\nand cause it to produce harmful content. In this work, we introduce\nerase-and-check, the first framework for defending against adversarial prompts\nwith certifiable safety guarantees. Given a prompt, our procedure erases tokens\nindividually and inspects the resulting subsequences using a safety filter. Our\nsafety certificate guarantees that harmful prompts are not mislabeled as safe\ndue to an adversarial attack up to a certain size. We implement the safety\nfilter in two ways, using Llama 2 and DistilBERT, and compare the performance\nof erase-and-check for the two cases. We defend against three attack modes: i)\nadversarial suffix, where an adversarial sequence is appended at the end of a\nharmful prompt; ii) adversarial insertion, where the adversarial sequence is\ninserted anywhere in the middle of the prompt; and iii) adversarial infusion,\nwhere adversarial tokens are inserted at arbitrary positions in the prompt, not\nnecessarily as a contiguous block. Our experimental results demonstrate that\nthis procedure can obtain strong certified safety guarantees on harmful prompts\nwhile maintaining good empirical performance on safe prompts. Additionally, we\npropose three efficient empirical defenses: i) RandEC, a randomized subsampling\nversion of erase-and-check; ii) GreedyEC, which greedily erases tokens that\nmaximize the softmax score of the harmful class; and iii) GradEC, which uses\ngradient information to optimize tokens to erase. We demonstrate their\neffectiveness against adversarial prompts generated by the Greedy Coordinate\nGradient (GCG) attack algorithm. The code for our experiments is available at\nhttps://github.com/aounon/certified-llm-safety.", "published": "2023-09-06 04:37:20", "link": "http://arxiv.org/abs/2309.02705v4", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Offensive Hebrew Corpus and Detection using BERT", "abstract": "Offensive language detection has been well studied in many languages, but it\nis lagging behind in low-resource languages, such as Hebrew. In this paper, we\npresent a new offensive language corpus in Hebrew. A total of 15,881 tweets\nwere retrieved from Twitter. Each was labeled with one or more of five classes\n(abusive, hate, violence, pornographic, or none offensive) by Arabic-Hebrew\nbilingual speakers. The annotation process was challenging as each annotator is\nexpected to be familiar with the Israeli culture, politics, and practices to\nunderstand the context of each tweet. We fine-tuned two Hebrew BERT models,\nHeBERT and AlephBERT, using our proposed dataset and another published dataset.\nWe observed that our data boosts HeBERT performance by 2% when combined with\nD_OLaH. Fine-tuning AlephBERT on our data and testing on D_OLaH yields 69%\naccuracy, while fine-tuning on D_OLaH and testing on our data yields 57%\naccuracy, which may be an indication to the generalizability our data offers.\nOur dataset and fine-tuned models are available on GitHub and Huggingface.", "published": "2023-09-06 05:18:43", "link": "http://arxiv.org/abs/2309.02724v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.1; I.2.6; I.2.7; I.5.1"], "primary_category": "cs.CL"}
{"title": "GRASS: Unified Generation Model for Speech-to-Semantic Tasks", "abstract": "This paper explores the instruction fine-tuning technique for\nspeech-to-semantic tasks by introducing a unified end-to-end (E2E) framework\nthat generates target text conditioned on a task-related prompt for audio data.\nWe pre-train the model using large and diverse data, where instruction-speech\npairs are constructed via a text-to-speech (TTS) system. Extensive experiments\ndemonstrate that our proposed model achieves state-of-the-art (SOTA) results on\nmany benchmarks covering speech named entity recognition, speech sentiment\nanalysis, speech question answering, and more, after fine-tuning. Furthermore,\nthe proposed model achieves competitive performance in zero-shot and few-shot\nscenarios. To facilitate future work on instruction fine-tuning for\nspeech-to-semantic tasks, we release our instruction dataset and code.", "published": "2023-09-06 06:44:26", "link": "http://arxiv.org/abs/2309.02780v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Norm Tweaking: High-performance Low-bit Quantization of Large Language\n  Models", "abstract": "As the size of large language models (LLMs) continues to grow, model\ncompression without sacrificing accuracy has become a crucial challenge for\ndeployment. While some quantization methods, such as GPTQ, have made progress\nin achieving acceptable 4-bit weight-only quantization, attempts at lower-bit\nquantization often result in severe performance degradation. In this paper, we\nintroduce a technique called norm tweaking, which can be used as a plugin in\ncurrent PTQ methods to achieve high precision while being cost-efficient. Our\napproach is inspired by the observation that rectifying the quantized\nactivation distribution to match its float counterpart can readily restore\naccuracy for LLMs. To achieve this, we carefully design a tweaking strategy\nthat includes calibration data generation and channel-wise distance constraint\nto update the weights of normalization layers for better generalization. We\nconduct extensive experiments on various datasets using several open-sourced\nLLMs. Our method demonstrates significant improvements in both weight-only\nquantization and joint quantization of weights and activations, surpassing\nexisting PTQ methods. On GLM-130B and OPT-66B, our method even achieves the\nsame level of accuracy at 2-bit quantization as their float ones. Our simple\nand effective approach makes it more practical for real-world applications.", "published": "2023-09-06 06:51:15", "link": "http://arxiv.org/abs/2309.02784v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Multimodal Analysis of Influencer Content on Twitter", "abstract": "Influencer marketing involves a wide range of strategies in which brands\ncollaborate with popular content creators (i.e., influencers) to leverage their\nreach, trust, and impact on their audience to promote and endorse products or\nservices. Because followers of influencers are more likely to buy a product\nafter receiving an authentic product endorsement rather than an explicit direct\nproduct promotion, the line between personal opinions and commercial content\npromotion is frequently blurred. This makes automatic detection of regulatory\ncompliance breaches related to influencer advertising (e.g., misleading\nadvertising or hidden sponsorships) particularly difficult. In this work, we\n(1) introduce a new Twitter (now X) dataset consisting of 15,998 influencer\nposts mapped into commercial and non-commercial categories for assisting in the\nautomatic detection of commercial influencer content; (2) experiment with an\nextensive set of predictive models that combine text and visual information\nshowing that our proposed cross-attention approach outperforms state-of-the-art\nmultimodal models; and (3) conduct a thorough analysis of strengths and\nlimitations of our models. We show that multimodal modeling is useful for\nidentifying commercial posts, reducing the amount of false positives, and\ncapturing relevant context that aids in the discovery of undisclosed commercial\nposts.", "published": "2023-09-06 15:07:23", "link": "http://arxiv.org/abs/2309.03064v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "GPT-InvestAR: Enhancing Stock Investment Strategies through Annual\n  Report Analysis with Large Language Models", "abstract": "Annual Reports of publicly listed companies contain vital information about\ntheir financial health which can help assess the potential impact on Stock\nprice of the firm. These reports are comprehensive in nature, going up to, and\nsometimes exceeding, 100 pages. Analysing these reports is cumbersome even for\na single firm, let alone the whole universe of firms that exist. Over the\nyears, financial experts have become proficient in extracting valuable\ninformation from these documents relatively quickly. However, this requires\nyears of practice and experience. This paper aims to simplify the process of\nassessing Annual Reports of all the firms by leveraging the capabilities of\nLarge Language Models (LLMs). The insights generated by the LLM are compiled in\na Quant styled dataset and augmented by historical stock price data. A Machine\nLearning model is then trained with LLM outputs as features. The walkforward\ntest results show promising outperformance wrt S&P500 returns. This paper\nintends to provide a framework for future work in this direction. To facilitate\nthis, the code has been released as open source.", "published": "2023-09-06 17:18:55", "link": "http://arxiv.org/abs/2309.03079v1", "categories": ["q-fin.ST", "cs.CL", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "Implicit Design Choices and Their Impact on Emotion Recognition Model\n  Development and Evaluation", "abstract": "Emotion recognition is a complex task due to the inherent subjectivity in\nboth the perception and production of emotions. The subjectivity of emotions\nposes significant challenges in developing accurate and robust computational\nmodels. This thesis examines critical facets of emotion recognition, beginning\nwith the collection of diverse datasets that account for psychological factors\nin emotion production.\n  To handle the challenge of non-representative training data, this work\ncollects the Multimodal Stressed Emotion dataset, which introduces controlled\nstressors during data collection to better represent real-world influences on\nemotion production. To address issues with label subjectivity, this research\ncomprehensively analyzes how data augmentation techniques and annotation\nschemes impact emotion perception and annotator labels. It further handles\nnatural confounding variables and variations by employing adversarial networks\nto isolate key factors like stress from learned emotion representations during\nmodel training. For tackling concerns about leakage of sensitive demographic\nvariables, this work leverages adversarial learning to strip sensitive\ndemographic information from multimodal encodings. Additionally, it proposes\noptimized sociological evaluation metrics aligned with cost-effective,\nreal-world needs for model testing.\n  This research advances robust, practical emotion recognition through\nmultifaceted studies of challenges in datasets, labels, modeling, demographic\nand membership variable encoding in representations, and evaluation. The\ngroundwork has been laid for cost-effective, generalizable emotion recognition\nmodels that are less likely to encode sensitive demographic information.", "published": "2023-09-06 02:45:42", "link": "http://arxiv.org/abs/2309.03238v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "GPT Can Solve Mathematical Problems Without a Calculator", "abstract": "Previous studies have typically assumed that large language models are unable\nto accurately perform arithmetic operations, particularly multiplication of >8\ndigits, and operations involving decimals and fractions, without the use of\ncalculator tools. This paper aims to challenge this misconception. With\nsufficient training data, a 2 billion-parameter language model can accurately\nperform multi-digit arithmetic operations with almost 100% accuracy without\ndata leakage, significantly surpassing GPT-4 (whose multi-digit multiplication\naccuracy is only 4.3%). We also demonstrate that our MathGLM, fine-tuned from\nGLM-10B on a dataset with additional multi-step arithmetic operations and math\nproblems described in text, achieves similar performance to GPT-4 on a\n5,000-samples Chinese math problem test set. Our code and data are public at\nhttps://github.com/THUDM/MathGLM.", "published": "2023-09-06 06:18:16", "link": "http://arxiv.org/abs/2309.03241v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Parameter Efficient Audio Captioning With Faithful Guidance Using\n  Audio-text Shared Latent Representation", "abstract": "There has been significant research on developing pretrained transformer\narchitectures for multimodal-to-text generation tasks. Albeit performance\nimprovements, such models are frequently overparameterized, hence suffer from\nhallucination and large memory footprint making them challenging to deploy on\nedge devices. In this paper, we address both these issues for the application\nof automated audio captioning. First, we propose a data augmentation technique\nfor generating hallucinated audio captions and show that similarity based on an\naudio-text shared latent space is suitable for detecting hallucination. Then,\nwe propose a parameter efficient inference time faithful decoding algorithm\nthat enables smaller audio captioning models with performance equivalent to\nlarger models trained with more data. During the beam decoding step, the\nsmaller model utilizes an audio-text shared latent representation to\nsemantically align the generated text with corresponding input audio. Faithful\nguidance is introduced into the beam probability by incorporating the cosine\nsimilarity between latent representation projections of greedy rolled out\nintermediate beams and audio clip. We show the efficacy of our algorithm on\nbenchmark datasets and evaluate the proposed scheme against baselines using\nconventional audio captioning and semantic similarity metrics while\nillustrating tradeoffs between performance and complexity.", "published": "2023-09-06 19:42:52", "link": "http://arxiv.org/abs/2309.03340v1", "categories": ["cs.CL", "cs.MM", "cs.SD"], "primary_category": "cs.CL"}
{"title": "RoDia: A New Dataset for Romanian Dialect Identification from Speech", "abstract": "We introduce RoDia, the first dataset for Romanian dialect identification\nfrom speech. The RoDia dataset includes a varied compilation of speech samples\nfrom five distinct regions of Romania, covering both urban and rural\nenvironments, totaling 2 hours of manually annotated speech data. Along with\nour dataset, we introduce a set of competitive models to be used as baselines\nfor future research. The top scoring model achieves a macro F1 score of 59.83%\nand a micro F1 score of 62.08%, indicating that the task is challenging. We\nthus believe that RoDia is a valuable resource that will stimulate research\naiming to address the challenges of Romanian dialect identification. We release\nour dataset at https://github.com/codrut2/RoDia.", "published": "2023-09-06 21:56:24", "link": "http://arxiv.org/abs/2309.03378v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Addressing the Blind Spots in Spoken Language Processing", "abstract": "This paper explores the critical but often overlooked role of non-verbal\ncues, including co-speech gestures and facial expressions, in human\ncommunication and their implications for Natural Language Processing (NLP). We\nargue that understanding human communication requires a more holistic approach\nthat goes beyond textual or spoken words to include non-verbal elements.\nBorrowing from advances in sign language processing, we propose the development\nof universal automatic gesture segmentation and transcription models to\ntranscribe these non-verbal cues into textual form. Such a methodology aims to\nbridge the blind spots in spoken language understanding, enhancing the scope\nand applicability of NLP models. Through motivating examples, we demonstrate\nthe limitations of relying solely on text-based models. We propose a\ncomputationally efficient and flexible approach for incorporating non-verbal\ncues, which can seamlessly integrate with existing NLP pipelines. We conclude\nby calling upon the research community to contribute to the development of\nuniversal transcription methods and to validate their effectiveness in\ncapturing the complexities of real-world, multi-modal interactions.", "published": "2023-09-06 10:29:25", "link": "http://arxiv.org/abs/2309.06572v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Leveraging ASR Pretrained Conformers for Speaker Verification through\n  Transfer Learning and Knowledge Distillation", "abstract": "This paper explores the use of ASR-pretrained Conformers for speaker\nverification, leveraging their strengths in modeling speech signals. We\nintroduce three strategies: (1) Transfer learning to initialize the speaker\nembedding network, improving generalization and reducing overfitting. (2)\nKnowledge distillation to train a more flexible speaker verification model,\nincorporating frame-level ASR loss as an auxiliary task. (3) A lightweight\nspeaker adaptor for efficient feature conversion without altering the original\nASR Conformer, allowing parallel ASR and speaker verification. Experiments on\nVoxCeleb show significant improvements: transfer learning yields a 0.48% EER,\nknowledge distillation results in a 0.43% EER, and the speaker adaptor\napproach, with just an added 4.92M parameters to a 130.94M-parameter model,\nachieves a 0.57% EER. Overall, our methods effectively transfer ASR\ncapabilities to speaker verification tasks.", "published": "2023-09-06 14:02:50", "link": "http://arxiv.org/abs/2309.03019v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "MuLanTTS: The Microsoft Speech Synthesis System for Blizzard Challenge\n  2023", "abstract": "In this paper, we present MuLanTTS, the Microsoft end-to-end neural\ntext-to-speech (TTS) system designed for the Blizzard Challenge 2023. About 50\nhours of audiobook corpus for French TTS as hub task and another 2 hours of\nspeaker adaptation as spoke task are released to build synthesized voices for\ndifferent test purposes including sentences, paragraphs, homographs, lists,\netc. Building upon DelightfulTTS, we adopt contextual and emotion encoders to\nadapt the audiobook data to enrich beyond sentences for long-form prosody and\ndialogue expressiveness. Regarding the recording quality, we also apply denoise\nalgorithms and long audio processing for both corpora. For the hub task, only\nthe 50-hour single speaker data is used for building the TTS system, while for\nthe spoke task, a multi-speaker source model is used for target speaker fine\ntuning. MuLanTTS achieves mean scores of quality assessment 4.3 and 4.5 in the\nrespective tasks, statistically comparable with natural speech while keeping\ngood similarity according to similarity assessment. The excellent and\nsimilarity in this year's new and dense statistical evaluation show the\neffectiveness of our proposed system in both tasks.", "published": "2023-09-06 06:03:07", "link": "http://arxiv.org/abs/2309.02743v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Simultaneous Measurement of Multiple Acoustic Attributes Using\n  Structured Periodic Test Signals Including Music and Other Sound Materials", "abstract": "We introduce a general framework for measuring acoustic properties such as\nliner time-invariant (LTI) response, signal-dependent time-invariant (SDTI)\ncomponent, and random and time-varying (RTV) component simultaneously using\nstructured periodic test signals. The framework also enables music pieces and\nother sound materials as test signals by \"safeguarding\" them by adding slight\ndeterministic \"noise.\" Measurement using swept-sin, MLS (Maxim Length\nSequence), and their variants are special cases of the proposed framework. We\nimplemented interactive and real-time measuring tools based on this framework\nand made them open-source. Furthermore, we applied this framework to assess\npitch extractors objectively.", "published": "2023-09-06 06:21:10", "link": "http://arxiv.org/abs/2309.02767v1", "categories": ["cs.SD", "eess.AS", "68-04", "J.2"], "primary_category": "cs.SD"}
{"title": "Self-Supervised Disentanglement of Harmonic and Rhythmic Features in\n  Music Audio Signals", "abstract": "The aim of latent variable disentanglement is to infer the multiple\ninformative latent representations that lie behind a data generation process\nand is a key factor in controllable data generation. In this paper, we propose\na deep neural network-based self-supervised learning method to infer the\ndisentangled rhythmic and harmonic representations behind music audio\ngeneration. We train a variational autoencoder that generates an audio\nmel-spectrogram from two latent features representing the rhythmic and harmonic\ncontent. In the training phase, the variational autoencoder is trained to\nreconstruct the input mel-spectrogram given its pitch-shifted version. At each\nforward computation in the training phase, a vector rotation operation is\napplied to one of the latent features, assuming that the dimensions of the\nfeature vectors are related to pitch intervals. Therefore, in the trained\nvariational autoencoder, the rotated latent feature represents the\npitch-related information of the mel-spectrogram, and the unrotated latent\nfeature represents the pitch-invariant information, i.e., the rhythmic content.\nThe proposed method was evaluated using a predictor-based disentanglement\nmetric on the learned features. Furthermore, we demonstrate its application to\nthe automatic generation of music remixes.", "published": "2023-09-06 07:30:15", "link": "http://arxiv.org/abs/2309.02796v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Real-time auralization for performers on virtual stages", "abstract": "This article presents an interactive system for stage acoustics\nexperimentation including considerations for hearing one's own and others'\ninstruments. The quality of real-time auralization systems for psychophysical\nexperiments on music performance depends on the system's calibration and\nlatency, among other factors (e.g. visuals, simulation methods, haptics, etc).\nThe presented system focuses on the acoustic considerations for laboratory\nimplementations. The calibration is implemented as a set of filters accounting\nfor the microphone-instrument distances and the directivity factors, as well as\nthe transducers' frequency responses. Moreover, sources of errors are\ncharacterized using both state-of-the-art information and derivations from the\nmathematical definition of the calibration filter. In order to compensate for\nhardware latency without cropping parts of the simulated impulse responses, the\nvirtual direct sound of musicians hearing themselves is skipped from the\nsimulation and addressed by letting the actual direct sound reach the listener\nthrough open headphones. The required latency compensation of the interactive\npart (i.e. hearing others) meets the minimum distance requirement between\nmusicians, which is 2 m for the implemented system. Finally, a proof of concept\nis provided that includes objective and subjective experiments, which give\nsupport to the feasibility of the proposed setup.", "published": "2023-09-06 16:44:50", "link": "http://arxiv.org/abs/2309.03149v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Leveraging Geometrical Acoustic Simulations of Spatial Room Impulse\n  Responses for Improved Sound Event Detection and Localization", "abstract": "As deeper and more complex models are developed for the task of sound event\nlocalization and detection (SELD), the demand for annotated spatial audio data\ncontinues to increase. Annotating field recordings with 360$^{\\circ}$ video\ntakes many hours from trained annotators, while recording events within\nmotion-tracked laboratories are bounded by cost and expertise. Because of this,\nlocalization models rely on a relatively limited amount of spatial audio data\nin the form of spatial room impulse response (SRIR) datasets, which limits the\nprogress of increasingly deep neural network based approaches. In this work, we\ndemonstrate that simulated geometrical acoustics can provide an appealing\nsolution to this problem. We use simulated geometrical acoustics to generate a\nnovel SRIR dataset that can train a SELD model to provide similar performance\nto that of a real SRIR dataset. Furthermore, we demonstrate using simulated\ndata to augment existing datasets, improving on benchmarks set by state of the\nart SELD models. We explore the potential and limitations of geometric acoustic\nsimulation for localization and event detection. We also propose further\nstudies to verify the limitations of this method, as well as further methods to\ngenerate synthetic data for SELD tasks without the need to record more data.", "published": "2023-09-06 19:34:30", "link": "http://arxiv.org/abs/2309.03337v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Highly Controllable Diffusion-based Any-to-Any Voice Conversion Model\n  with Frame-level Prosody Feature", "abstract": "We propose a highly controllable voice manipulation system that can perform\nany-to-any voice conversion (VC) and prosody modulation simultaneously.\nState-of-the-art VC systems can transfer sentence-level characteristics such as\nspeaker, emotion, and speaking style. However, manipulating the frame-level\nprosody, such as pitch, energy and speaking rate, still remains challenging.\nOur proposed model utilizes a frame-level prosody feature to effectively\ntransfer such properties. Specifically, pitch and energy trajectories are\nintegrated in a prosody conditioning module and then fed alongside speaker and\ncontents embeddings to a diffusion-based decoder generating a converted speech\nmel-spectrogram. To adjust the speaking rate, our system includes a\nself-supervised model based post-processing step which allows improved\ncontrollability. The proposed model showed comparable speech quality and\nimproved intelligibility compared to a SOTA approach. It can cover a varying\nrange of fundamental frequency (F0), energy and speed modulation while\nmaintaining converted speech quality.", "published": "2023-09-06 21:16:58", "link": "http://arxiv.org/abs/2309.03364v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Stylebook: Content-Dependent Speaking Style Modeling for Any-to-Any\n  Voice Conversion using Only Speech Data", "abstract": "While many recent any-to-any voice conversion models succeed in transferring\nsome target speech's style information to the converted speech, they still lack\nthe ability to faithfully reproduce the speaking style of the target speaker.\nIn this work, we propose a novel method to extract rich style information from\ntarget utterances and to efficiently transfer it to source speech content\nwithout requiring text transcriptions or speaker labeling. Our proposed\napproach introduces an attention mechanism utilizing a self-supervised learning\n(SSL) model to collect the speaking styles of a target speaker each\ncorresponding to the different phonetic content. The styles are represented\nwith a set of embeddings called stylebook. In the next step, the stylebook is\nattended with the source speech's phonetic content to determine the final\ntarget style for each source content. Finally, content information extracted\nfrom the source speech and content-dependent target style embeddings are fed\ninto a diffusion-based decoder to generate the converted speech\nmel-spectrogram. Experiment results show that our proposed method combined with\na diffusion-based generative model can achieve better speaker similarity in\nany-to-any voice conversion tasks when compared to baseline models, while the\nincrease in computational complexity with longer utterances is suppressed.", "published": "2023-09-06 05:33:54", "link": "http://arxiv.org/abs/2309.02730v3", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "BigVSAN: Enhancing GAN-based Neural Vocoders with Slicing Adversarial\n  Network", "abstract": "Generative adversarial network (GAN)-based vocoders have been intensively\nstudied because they can synthesize high-fidelity audio waveforms faster than\nreal-time. However, it has been reported that most GANs fail to obtain the\noptimal projection for discriminating between real and fake data in the feature\nspace. In the literature, it has been demonstrated that slicing adversarial\nnetwork (SAN), an improved GAN training framework that can find the optimal\nprojection, is effective in the image generation task. In this paper, we\ninvestigate the effectiveness of SAN in the vocoding task. For this purpose, we\npropose a scheme to modify least-squares GAN, which most GAN-based vocoders\nadopt, so that their loss functions satisfy the requirements of SAN. Through\nour experiments, we demonstrate that SAN can improve the performance of\nGAN-based vocoders, including BigVGAN, with small modifications. Our code is\navailable at https://github.com/sony/bigvsan.", "published": "2023-09-06 08:48:03", "link": "http://arxiv.org/abs/2309.02836v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LuViRA Dataset Validation and Discussion: Comparing Vision, Radio, and\n  Audio Sensors for Indoor Localization", "abstract": "We present a unique comparative analysis, and evaluation of vision, radio,\nand audio based localization algorithms. We create the first baseline for the\naforementioned sensors using the recently published Lund University Vision,\nRadio, and Audio (LuViRA) dataset, where all the sensors are synchronized and\nmeasured in the same environment. Some of the challenges of using each specific\nsensor for indoor localization tasks are highlighted. Each sensor is paired\nwith a current state-of-the-art localization algorithm and evaluated for\ndifferent aspects: localization accuracy, reliability and sensitivity to\nenvironment changes, calibration requirements, and potential system complexity.\nSpecifically, the evaluation covers the ORB-SLAM3 algorithm for vision-based\nlocalization with an RGB-D camera, a machine-learning algorithm for radio-based\nlocalization with massive MIMO technology, and the SFS2 algorithm for\naudio-based localization with distributed microphones. The results can serve as\na guideline and basis for further development of robust and high-precision\nmulti-sensory localization systems, e.g., through sensor fusion, context, and\nenvironment-aware adaptation.", "published": "2023-09-06 12:57:00", "link": "http://arxiv.org/abs/2309.02961v2", "categories": ["eess.SP", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "An Efficient Temporary Deepfake Location Approach Based Embeddings for\n  Partially Spoofed Audio Detection", "abstract": "Partially spoofed audio detection is a challenging task, lying in the need to\naccurately locate the authenticity of audio at the frame level. To address this\nissue, we propose a fine-grained partially spoofed audio detection method,\nnamely Temporal Deepfake Location (TDL), which can effectively capture\ninformation of both features and locations. Specifically, our approach involves\ntwo novel parts: embedding similarity module and temporal convolution\noperation. To enhance the identification between the real and fake features,\nthe embedding similarity module is designed to generate an embedding space that\ncan separate the real frames from fake frames. To effectively concentrate on\nthe position information, temporal convolution operation is proposed to\ncalculate the frame-specific similarities among neighboring frames, and\ndynamically select informative neighbors to convolution. Extensive experiments\nshow that our method outperform baseline models in ASVspoof2019 Partial Spoof\ndataset and demonstrate superior performance even in the crossdataset scenario.", "published": "2023-09-06 14:29:29", "link": "http://arxiv.org/abs/2309.03036v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Matcha-TTS: A fast TTS architecture with conditional flow matching", "abstract": "We introduce Matcha-TTS, a new encoder-decoder architecture for speedy TTS\nacoustic modelling, trained using optimal-transport conditional flow matching\n(OT-CFM). This yields an ODE-based decoder capable of high output quality in\nfewer synthesis steps than models trained using score matching. Careful design\nchoices additionally ensure each synthesis step is fast to run. The method is\nprobabilistic, non-autoregressive, and learns to speak from scratch without\nexternal alignments. Compared to strong pre-trained baseline models, the\nMatcha-TTS system has the smallest memory footprint, rivals the speed of the\nfastest models on long utterances, and attains the highest mean opinion score\nin a listening test. Please see https://shivammehta25.github.io/Matcha-TTS/ for\naudio examples, code, and pre-trained models.", "published": "2023-09-06 17:59:57", "link": "http://arxiv.org/abs/2309.03199v2", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.SD", "68T07", "I.2.7; I.2.6; H.5.5"], "primary_category": "eess.AS"}
{"title": "Presenting the SWTC: A Symbolic Corpus of Themes from John Williams'\n  Star Wars Episodes I-IX", "abstract": "This paper presents a new symbolic corpus of musical themes from the complete\nStar Wars trilogies (Episodes I-IX) by John Williams. The corpus files are made\navailable in multiple formats (.krn, .sib, and .musicxml) and include melodic,\nharmonic, and formal information. The Star Wars Thematic Corpus (SWTC) contains\na total of 64 distinctive, recurring, and symbolically meaningful themes and\nmotifs, commonly referred to as leitmotifs. Through this corpus we also\nintroduce a new humdrum standard for non-functional harmony encodings, **harte,\nbased on Harte (2005, 2010). This report details the motivation, describes the\ntranscription and encoding processes, and provides some brief summary\nstatistics. While relatively small in scale, the SWTC represents a unified\ncollection from one of the most prolific and influential composers of the 20th\ncentury, and the under-studied subset of film and multimedia musical material\nin general. We hope the SWTC will provide insights into John Williams'\ncompositional style, as well as prove useful in comparisons against other\nthematic corpora from film and beyond.", "published": "2023-09-06 18:21:55", "link": "http://arxiv.org/abs/2309.03298v1", "categories": ["cs.SD", "cs.SC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Role of Communication and Reference Songs in the Mixing Process:\n  Insights from Professional Mix Engineers", "abstract": "Effective music mixing requires technical and creative finesse, but clear\ncommunication with the client is crucial. The mixing engineer must grasp the\nclient's expectations, and preferences, and collaborate to achieve the desired\nsound. The tacit agreement for the desired sound of the mix is often\nestablished using guides like reference songs and demo mixes exchanged between\nthe artist and the engineer and sometimes verbalised using semantic terms. This\npaper presents the findings of a two-phased exploratory study aimed at\nunderstanding how professional mixing engineers interact with clients and use\ntheir feedback to guide the mixing process. For phase one, semi-structured\ninterviews were conducted with five mixing engineers with the aim of gathering\ninsights about their communication strategies, creative processes, and\ndecision-making criteria. Based on the inferences from these interviews, an\nonline questionnaire was designed and administered to a larger group of 22\nmixing engineers during the second phase. The results of this study shed light\non the importance of collaboration, empathy, and intention in the mixing\nprocess, and can inform the development of smart multi-track mixing systems\nthat better support these practices. By highlighting the significance of these\nfindings, this paper contributes to the growing body of research on the\ncollaborative nature of music production and provides actionable\nrecommendations for the design and implementation of innovative mixing tools.", "published": "2023-09-06 23:45:09", "link": "http://arxiv.org/abs/2309.03404v3", "categories": ["cs.HC", "cs.AI", "eess.AS"], "primary_category": "cs.HC"}
