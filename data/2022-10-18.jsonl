{"title": "Systematicity in GPT-3's Interpretation of Novel English Noun Compounds", "abstract": "Levin et al. (2019) show experimentally that the interpretations of novel\nEnglish noun compounds (e.g., stew skillet), while not fully compositional, are\nhighly predictable based on whether the modifier and head refer to artifacts or\nnatural kinds. Is the large language model GPT-3 governed by the same\ninterpretive principles? To address this question, we first compare Levin et\nal.'s experimental data with GPT-3 generations, finding a high degree of\nsimilarity. However, this evidence is consistent with GPT3 reasoning only about\nspecific lexical items rather than the more abstract conceptual categories of\nLevin et al.'s theory. To probe more deeply, we construct prompts that require\nthe relevant kind of conceptual reasoning. Here, we fail to find convincing\nevidence that GPT-3 is reasoning about more than just individual lexical items.\nThese results highlight the importance of controlling for low-level\ndistributional regularities when assessing whether a large language model\nlatently encodes a deeper theory.", "published": "2022-10-18 00:25:24", "link": "http://arxiv.org/abs/2210.09492v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Less is More: A Lightweight and Robust Neural Architecture for Discourse\n  Parsing", "abstract": "Complex feature extractors are widely employed for text representation\nbuilding. However, these complex feature extractors make the NLP systems prone\nto overfitting especially when the downstream training datasets are relatively\nsmall, which is the case for several discourse parsing tasks. Thus, we propose\nan alternative lightweight neural architecture that removes multiple complex\nfeature extractors and only utilizes learnable self-attention modules to\nindirectly exploit pretrained neural language models, in order to maximally\npreserve the generalizability of pre-trained language models. Experiments on\nthree common discourse parsing tasks show that powered by recent pretrained\nlanguage models, the lightweight architecture consisting of only two\nself-attention layers obtains much better generalizability and robustness.\nMeanwhile, it achieves comparable or even better system performance with fewer\nlearnable parameters and less processing time.", "published": "2022-10-18 02:07:09", "link": "http://arxiv.org/abs/2210.09537v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing Cross-modal Semantics Alignment Capability from the Textual\n  Perspective", "abstract": "In recent years, vision and language pre-training (VLP) models have advanced\nthe state-of-the-art results in a variety of cross-modal downstream tasks.\nAligning cross-modal semantics is claimed to be one of the essential\ncapabilities of VLP models. However, it still remains unclear about the inner\nworking mechanism of alignment in VLP models. In this paper, we propose a new\nprobing method that is based on image captioning to first empirically study the\ncross-modal semantics alignment of VLP models. Our probing method is built upon\nthe fact that given an image-caption pair, the VLP models will give a score,\nindicating how well two modalities are aligned; maximizing such scores will\ngenerate sentences that VLP models believe are of good alignment. Analyzing\nthese sentences thus will reveal in what way different modalities are aligned\nand how well these alignments are in VLP models. We apply our probing method to\nfive popular VLP models, including UNITER, ROSITA, ViLBERT, CLIP, and LXMERT,\nand provide a comprehensive analysis of the generated captions guided by these\nmodels. Our results show that VLP models (1) focus more on just aligning\nobjects with visual words, while neglecting global semantics; (2) prefer fixed\nsentence patterns, thus ignoring more important textual information including\nfluency and grammar; and (3) deem the captions with more visual words are\nbetter aligned with images. These findings indicate that VLP models still have\nweaknesses in cross-modal semantics alignment and we hope this work will draw\nresearchers' attention to such problems when designing a new VLP model.", "published": "2022-10-18 02:55:58", "link": "http://arxiv.org/abs/2210.09550v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DisCup: Discriminator Cooperative Unlikelihood Prompt-tuning for\n  Controllable Text Generation", "abstract": "Prompt learning with immensely large Casual Language Models (CLMs) has been\nshown promising for attribute-controllable text generation (CTG). However,\nvanilla prompt tuning tends to imitate training corpus characteristics beyond\nthe control attributes, resulting in a poor generalization ability. Moreover,\nit is less able to capture the relationship between different attributes,\nfurther limiting the control performance. In this paper, we propose a new CTG\napproach, namely DisCup, which incorporates the attribute knowledge of\ndiscriminator to optimize the control-prompts, steering a frozen CLM to produce\nattribute-specific texts. Specifically, the frozen CLM model, capable of\nproducing multitudinous texts, is first used to generate the next-token\ncandidates based on the context, so as to ensure the diversity of tokens to be\npredicted. Then, we leverage an attribute-discriminator to select\ndesired/undesired tokens from those candidates, providing the inter-attribute\nknowledge. Finally, we bridge the above two traits by an unlikelihood objective\nfor prompt-tuning. Extensive experimental results show that DisCup can achieve\na new state-of-the-art control performance while maintaining an efficient and\nhigh-quality text generation, only relying on around 10 virtual tokens.", "published": "2022-10-18 02:59:06", "link": "http://arxiv.org/abs/2210.09551v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Inference of Data-Driven Discourse Structures using a Tree\n  Auto-Encoder", "abstract": "With a growing need for robust and general discourse structures in many\ndownstream tasks and real-world applications, the current lack of high-quality,\nhigh-quantity discourse trees poses a severe shortcoming. In order the\nalleviate this limitation, we propose a new strategy to generate tree\nstructures in a task-agnostic, unsupervised fashion by extending a latent tree\ninduction framework with an auto-encoding objective. The proposed approach can\nbe applied to any tree-structured objective, such as syntactic parsing,\ndiscourse parsing and others. However, due to the especially difficult\nannotation process to generate discourse trees, we initially develop such\nmethod to complement task-specific models in generating much larger and more\ndiverse discourse treebanks.", "published": "2022-10-18 03:28:39", "link": "http://arxiv.org/abs/2210.09559v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Domain-Independent Supervised Discourse Parsing Through Gradient\n  Boosting", "abstract": "Discourse analysis and discourse parsing have shown great impact on many\nimportant problems in the field of Natural Language Processing (NLP). Given the\ndirect impact of discourse annotations on model performance and\ninterpretability, robustly extracting discourse structures from arbitrary\ndocuments is a key task to further improve computational models in NLP. To this\nend, we present a new, supervised paradigm directly tackling the domain\nadaptation issue in discourse parsing. Specifically, we introduce the first\nfully supervised discourse parser designed to alleviate the domain dependency\nthrough a staged model of weak classifiers by introducing the gradient boosting\nframework.", "published": "2022-10-18 03:44:27", "link": "http://arxiv.org/abs/2210.09565v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NADI 2022: The Third Nuanced Arabic Dialect Identification Shared Task", "abstract": "We describe findings of the third Nuanced Arabic Dialect Identification\nShared Task (NADI 2022). NADI aims at advancing state of the art Arabic NLP,\nincluding on Arabic dialects. It does so by affording diverse datasets and\nmodeling opportunities in a standardized context where meaningful comparisons\nbetween models and approaches are possible. NADI 2022 targeted both dialect\nidentification (Subtask 1) and dialectal sentiment analysis (Subtask 2) at the\ncountry level. A total of 41 unique teams registered for the shared task, of\nwhom 21 teams have actually participated (with 105 valid submissions). Among\nthese, 19 teams participated in Subtask 1 and 10 participated in Subtask 2. The\nwinning team achieved 27.06 F1 on Subtask 1 and F1=75.16 on Subtask 2,\nreflecting that the two subtasks remain challenging and motivating future work\nin this area. We describe methods employed by participating teams and offer an\noutlook for NADI.", "published": "2022-10-18 04:31:05", "link": "http://arxiv.org/abs/2210.09582v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Summary Workbench: Unifying Application and Evaluation of Text\n  Summarization Models", "abstract": "This paper presents Summary Workbench, a new tool for developing and\nevaluating text summarization models. New models and evaluation measures can be\neasily integrated as Docker-based plugins, allowing to examine the quality of\ntheir summaries against any input and to evaluate them using various evaluation\nmeasures. Visual analyses combining multiple measures provide insights into the\nmodels' strengths and weaknesses. The tool is hosted at\n\\url{https://tldr.demo.webis.de} and also supports local deployment for private\nresources.", "published": "2022-10-18 04:47:25", "link": "http://arxiv.org/abs/2210.09587v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Synergy with Translation Artifacts for Training and Inference in\n  Multilingual Tasks", "abstract": "Translation has played a crucial role in improving the performance on\nmultilingual tasks: (1) to generate the target language data from the source\nlanguage data for training and (2) to generate the source language data from\nthe target language data for inference. However, prior works have not\nconsidered the use of both translations simultaneously. This paper shows that\ncombining them can synergize the results on various multilingual sentence\nclassification tasks. We empirically find that translation artifacts stylized\nby translators are the main factor of the performance gain. Based on this\nanalysis, we adopt two training methods, SupCon and MixUp, considering\ntranslation artifacts. Furthermore, we propose a cross-lingual fine-tuning\nalgorithm called MUSC, which uses SupCon and MixUp jointly and improves the\nperformance. Our code is available at https://github.com/jongwooko/MUSC.", "published": "2022-10-18 04:55:24", "link": "http://arxiv.org/abs/2210.09588v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Denoising Enhanced Distantly Supervised Ultrafine Entity Typing", "abstract": "Recently, the task of distantly supervised (DS) ultra-fine entity typing has\nreceived significant attention. However, DS data is noisy and often suffers\nfrom missing or wrong labeling issues resulting in low precision and low\nrecall. This paper proposes a novel ultra-fine entity typing model with\ndenoising capability. Specifically, we build a noise model to estimate the\nunknown labeling noise distribution over input contexts and noisy type labels.\nWith the noise model, more trustworthy labels can be recovered by subtracting\nthe estimated noise from the input. Furthermore, we propose an entity typing\nmodel, which adopts a bi-encoder architecture, is trained on the denoised data.\nFinally, the noise model and entity typing model are trained iteratively to\nenhance each other. We conduct extensive experiments on the Ultra-Fine entity\ntyping dataset as well as OntoNotes dataset and demonstrate that our approach\nsignificantly outperforms other baseline methods.", "published": "2022-10-18 05:20:16", "link": "http://arxiv.org/abs/2210.09599v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving robustness of spontaneous speech synthesis with linguistic\n  speech regularization and pseudo-filled-pause insertion", "abstract": "We present a training method with linguistic speech regularization that\nimproves the robustness of spontaneous speech synthesis methods with filled\npause (FP) insertion. Spontaneous speech synthesis is aimed at producing speech\nwith human-like disfluencies, such as FPs. Because modeling the complex data\ndistribution of spontaneous speech with a rich FP vocabulary is challenging,\nthe quality of FP-inserted synthetic speech is often limited. To address this\nissue, we present a method for synthesizing spontaneous speech that improves\nrobustness to diverse FP insertions. Regularization is used to stabilize the\nsynthesis of the linguistic speech (i.e., non-FP) elements. To further improve\nrobustness to diverse FP insertions, it utilizes pseudo-FPs sampled using an FP\nword prediction model as well as ground-truth FPs. Our experiments demonstrated\nthat the proposed method improves the naturalness of synthetic speech with\nground-truth and predicted FPs by 0.24 and 0.26, respectively.", "published": "2022-10-18 12:49:55", "link": "http://arxiv.org/abs/2210.09815v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Mid-attribute speaker generation using optimal-transport-based\n  interpolation of Gaussian mixture models", "abstract": "In this paper, we propose a method for intermediating multiple speakers'\nattributes and diversifying their voice characteristics in ``speaker\ngeneration,'' an emerging task that aims to synthesize a nonexistent speaker's\nnaturally sounding voice. The conventional TacoSpawn-based speaker generation\nmethod represents the distributions of speaker embeddings by Gaussian mixture\nmodels (GMMs) conditioned with speaker attributes. Although this method enables\nthe sampling of various speakers from the speaker-attribute-aware GMMs, it is\nnot yet clear whether the learned distributions can represent speakers with an\nintermediate attribute (i.e., mid-attribute). To this end, we propose an\noptimal-transport-based method that interpolates the learned GMMs to generate\nnonexistent speakers with mid-attribute (e.g., gender-neutral) voices. We\nempirically validate our method and evaluate the naturalness of synthetic\nspeech and the controllability of two speaker attributes: gender and language\nfluency. The evaluation results show that our method can control the generated\nspeakers' attributes by a continuous scalar value without statistically\nsignificant degradation of speech naturalness.", "published": "2022-10-18 14:59:25", "link": "http://arxiv.org/abs/2210.09916v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "HMM vs. CTC for Automatic Speech Recognition: Comparison Based on\n  Full-Sum Training from Scratch", "abstract": "In this work, we compare from-scratch sequence-level cross-entropy (full-sum)\ntraining of Hidden Markov Model (HMM) and Connectionist Temporal Classification\n(CTC) topologies for automatic speech recognition (ASR). Besides accuracy, we\nfurther analyze their capability for generating high-quality time alignment\nbetween the speech signal and the transcription, which can be crucial for many\nsubsequent applications. Moreover, we propose several methods to improve\nconvergence of from-scratch full-sum training by addressing the alignment\nmodeling issue. Systematic comparison is conducted on both Switchboard and\nLibriSpeech corpora across CTC, posterior HMM with and w/o transition\nprobabilities, and standard hybrid HMM. We also provide a detailed analysis of\nboth Viterbi forced-alignment and Baum-Welch full-sum occupation probabilities.", "published": "2022-10-18 16:03:27", "link": "http://arxiv.org/abs/2210.09951v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Optimizing Temporal Resolution Of Convolutional Recurrent Neural\n  Networks For Sound Event Detection", "abstract": "In this technical report, the systems we submitted for subtask 4 of the DCASE\n2021 challenge, regarding sound event detection, are described in detail. These\nmodels are closely related to the baseline provided for this problem, as they\nare essentially convolutional recurrent neural networks trained in a mean\nteacher setting to deal with the heterogeneous annotation of the supplied data.\nHowever, the time resolution of the predictions was adapted to deal with the\nfact that these systems are evaluated using two intersection-based metrics\ninvolving different needs in terms of temporal localization. This was done by\noptimizing the pooling operations.\n  For the first of the defined evaluation scenarios, imposing relatively strict\nrequirements on the temporal localization accuracy, our best model achieved a\nPSDS score of 0.3609 on the validation data. This is only marginally better\nthan the performance obtained by the baseline system (0.342): The amount of\npooling in the baseline network already turned out to be optimal, and thus, no\nsubstantial changes were made, explaining this result.\n  For the second evaluation scenario, imposing relatively lax restrictions on\nthe localization accuracy, our best-performing system achieved a PSDS score of\n0.7312 on the validation data. This is significantly better than the\nperformance obtained by the baseline model (0.527), which can effectively be\nattributed to the changes that were applied to the pooling operations of the\nnetwork.", "published": "2022-10-18 23:25:11", "link": "http://arxiv.org/abs/2210.10208v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Personalization of CTC Speech Recognition Models with Contextual\n  Adapters and Adaptive Boosting", "abstract": "End-to-end speech recognition models trained using joint Connectionist\nTemporal Classification (CTC)-Attention loss have gained popularity recently.\nIn these models, a non-autoregressive CTC decoder is often used at inference\ntime due to its speed and simplicity. However, such models are hard to\npersonalize because of their conditional independence assumption that prevents\noutput tokens from previous time steps to influence future predictions. To\ntackle this, we propose a novel two-way approach that first biases the encoder\nwith attention over a predefined list of rare long-tail and out-of-vocabulary\n(OOV) words and then uses dynamic boosting and phone alignment network during\ndecoding to further bias the subword predictions. We evaluate our approach on\nopen-source VoxPopuli and in-house medical datasets to showcase a 60%\nimprovement in F1 score on domain-specific rare words over a strong CTC\nbaseline.", "published": "2022-10-18 01:08:21", "link": "http://arxiv.org/abs/2210.09510v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SVLDL: Improved Speaker Age Estimation Using Selective Variance Label\n  Distribution Learning", "abstract": "Estimating age from a single speech is a classic and challenging topic.\nAlthough Label Distribution Learning (LDL) can represent adjacent\nindistinguishable ages well, the uncertainty of the age estimate for each\nutterance varies from person to person, i.e., the variance of the age\ndistribution is different. To address this issue, we propose selective variance\nlabel distribution learning (SVLDL) method to adapt the variance of different\nage distributions. Furthermore, the model uses WavLM as the speech feature\nextractor and adds the auxiliary task of gender recognition to further improve\nthe performance. Two tricks are applied on the loss function to enhance the\nrobustness of the age estimation and improve the quality of the fitted age\ndistribution. Extensive experiments show that the model achieves\nstate-of-the-art performance on all aspects of the NIST SRE08-10 and a\nreal-world datasets.", "published": "2022-10-18 01:34:31", "link": "http://arxiv.org/abs/2210.09524v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Hybrid System of Sound Event Detection Transformer and Frame-wise\n  Model for DCASE 2022 Task 4", "abstract": "In this paper, we describe in detail our system for DCASE 2022 Task4. The\nsystem combines two considerably different models: an end-to-end Sound Event\nDetection Transformer (SEDT) and a frame-wise model, Metric Learning and Focal\nLoss CNN (MLFL-CNN). The former is an event-wise model which learns event-level\nrepresentations and predicts sound event categories and boundaries directly,\nwhile the latter is based on the widely adopted frame-classification scheme,\nunder which each frame is classified into event categories and event boundaries\nare obtained by post-processing such as thresholding and smoothing. For SEDT,\nself-supervised pre-training using unlabeled data is applied, and\nsemi-supervised learning is adopted by using an online teacher, which is\nupdated from the student model using the Exponential Moving Average (EMA)\nstrategy and generates reliable pseudo labels for weakly-labeled and unlabeled\ndata. For the frame-wise model, the ICT-TOSHIBA system of DCASE 2021 Task 4 is\nused. Experimental results show that the hybrid system considerably outperforms\neither individual model and achieves psds1 of 0.420 and psds2 of 0.783 on the\nvalidation set without external data. The code is available at\nhttps://github.com/965694547/Hybrid-system-of-frame-wise-model-and-SEDT.", "published": "2022-10-18 01:47:05", "link": "http://arxiv.org/abs/2210.09529v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Discrete Cross-Modal Alignment Enables Zero-Shot Speech Translation", "abstract": "End-to-end Speech Translation (ST) aims at translating the source language\nspeech into target language text without generating the intermediate\ntranscriptions. However, the training of end-to-end methods relies on parallel\nST data, which are difficult and expensive to obtain. Fortunately, the\nsupervised data for automatic speech recognition (ASR) and machine translation\n(MT) are usually more accessible, making zero-shot speech translation a\npotential direction. Existing zero-shot methods fail to align the two\nmodalities of speech and text into a shared semantic space, resulting in much\nworse performance compared to the supervised ST methods. In order to enable\nzero-shot ST, we propose a novel Discrete Cross-Modal Alignment (DCMA) method\nthat employs a shared discrete vocabulary space to accommodate and match both\nmodalities of speech and text. Specifically, we introduce a vector quantization\nmodule to discretize the continuous representations of speech and text into a\nfinite set of virtual tokens, and use ASR data to map corresponding speech and\ntext to the same virtual token in a shared codebook. This way, source language\nspeech can be embedded in the same semantic space as the source language text,\nwhich can be then transformed into target language text with an MT module.\nExperiments on multiple language pairs demonstrate that our zero-shot ST method\nsignificantly improves the SOTA, and even performers on par with the strong\nsupervised ST baselines.", "published": "2022-10-18 03:06:47", "link": "http://arxiv.org/abs/2210.09556v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Risk of re-identification for shared clinical speech recordings", "abstract": "Large, curated datasets are required to leverage speech-based tools in\nhealthcare. These are costly to produce, resulting in increased interest in\ndata sharing. As speech can potentially identify speakers (i.e., voiceprints),\nsharing recordings raises privacy concerns. We examine the re-identification\nrisk for speech recordings, without reference to demographic or metadata, using\na state-of-the-art speaker recognition system. We demonstrate that the risk is\ninversely related to the number of comparisons an adversary must consider,\ni.e., the search space. Risk is high for a small search space but drops as the\nsearch space grows ($precision >0.85$ for $<1*10^{6}$ comparisons, $precision\n<0.5$ for $>3*10^{6}$ comparisons). Next, we show that the nature of a speech\nrecording influences re-identification risk, with non-connected speech (e.g.,\nvowel prolongation) being harder to identify. Our findings suggest that speaker\nrecognition systems can be used to re-identify participants in specific\ncircumstances, but in practice, the re-identification risk appears low.", "published": "2022-10-18 16:38:32", "link": "http://arxiv.org/abs/2210.09975v2", "categories": ["eess.AS", "cs.CR", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Maestro-U: Leveraging joint speech-text representation learning for zero\n  supervised speech ASR", "abstract": "Training state-of-the-art Automated Speech Recognition (ASR) models typically\nrequires a substantial amount of transcribed speech. In this work, we\ndemonstrate that a modality-matched joint speech and text model can be\nleveraged to train a massively multilingual ASR model without any supervised\n(manually transcribed) speech for some languages. This paper explores the use\nof jointly learnt speech and text representations in a massively multilingual,\nzero supervised speech, real-world setting to expand the set of languages\ncovered by ASR with only unlabeled speech and text in the target languages.\nUsing the FLEURS dataset, we define the task to cover $102$ languages, where\ntranscribed speech is available in $52$ of these languages and can be used to\nimprove end-to-end ASR quality on the remaining $50$. First, we show that by\ncombining speech representations with byte-level text representations and use\nof language embeddings, we can dramatically reduce the Character Error Rate\n(CER) on languages with no supervised speech from 64.8\\% to 30.8\\%, a relative\nreduction of 53\\%. Second, using a subset of South Asian languages we show that\nMaestro-U can promote knowledge transfer from languages with supervised speech\neven when there is limited to no graphemic overlap. Overall, Maestro-U closes\nthe gap to oracle performance by 68.5\\% relative and reduces the CER of 19\nlanguages below 15\\%.", "published": "2022-10-18 17:50:31", "link": "http://arxiv.org/abs/2210.10027v2", "categories": ["cs.CL", "cs.SD", "eess.AS", "68T10", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Simple and Effective Unsupervised Speech Translation", "abstract": "The amount of labeled data to train models for speech tasks is limited for\nmost languages, however, the data scarcity is exacerbated for speech\ntranslation which requires labeled data covering two different languages. To\naddress this issue, we study a simple and effective approach to build speech\ntranslation systems without labeled data by leveraging recent advances in\nunsupervised speech recognition, machine translation and speech synthesis,\neither in a pipeline approach, or to generate pseudo-labels for training\nend-to-end speech translation models. Furthermore, we present an unsupervised\ndomain adaptation technique for pre-trained speech models which improves the\nperformance of downstream unsupervised speech recognition, especially for\nlow-resource settings. Experiments show that unsupervised speech-to-text\ntranslation outperforms the previous unsupervised state of the art by 3.2 BLEU\non the Libri-Trans benchmark, on CoVoST 2, our best systems outperform the best\nsupervised end-to-end models (without pre-training) from only two years ago by\nan average of 5.0 BLEU over five X-En directions. We also report competitive\nresults on MuST-C and CVSS benchmarks.", "published": "2022-10-18 22:26:13", "link": "http://arxiv.org/abs/2210.10191v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "BirdSoundsDenoising: Deep Visual Audio Denoising for Bird Sounds", "abstract": "Audio denoising has been explored for decades using both traditional and deep\nlearning-based methods. However, these methods are still limited to either\nmanually added artificial noise or lower denoised audio quality. To overcome\nthese challenges, we collect a large-scale natural noise bird sound dataset. We\nare the first to transfer the audio denoising problem into an image\nsegmentation problem and propose a deep visual audio denoising (DVAD) model.\nWith a total of 14,120 audio images, we develop an audio ImageMask tool and\npropose to use a few-shot generalization strategy to label these images.\nExtensive experimental results demonstrate that the proposed model achieves\nstate-of-the-art performance. We also show that our method can be easily\ngeneralized to speech denoising, audio separation, audio enhancement, and noise\nestimation.", "published": "2022-10-18 22:37:25", "link": "http://arxiv.org/abs/2210.10196v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Source Transformer Architectures for Audiovisual Scene\n  Classification", "abstract": "In this technical report, the systems we submitted for subtask 1B of the\nDCASE 2021 challenge, regarding audiovisual scene classification, are described\nin detail. They are essentially multi-source transformers employing a\ncombination of auditory and visual features to make predictions. These models\nare evaluated utilizing the macro-averaged multi-class cross-entropy and\naccuracy metrics.\n  In terms of the macro-averaged multi-class cross-entropy, our best model\nachieved a score of 0.620 on the validation data. This is slightly better than\nthe performance of the baseline system (0.658).\n  With regard to the accuracy measure, our best model achieved a score of\n77.1\\% on the validation data, which is about the same as the performance\nobtained by the baseline system (77.0\\%).", "published": "2022-10-18 23:42:42", "link": "http://arxiv.org/abs/2210.10212v1", "categories": ["eess.AS", "cs.CV", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
