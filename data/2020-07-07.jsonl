{"title": "Research on Annotation Rules and Recognition Algorithm Based on Phrase\n  Window", "abstract": "At present, most Natural Language Processing technology is based on the\nresults of Word Segmentation for Dependency Parsing, which mainly uses an\nend-to-end method based on supervised learning. There are two main problems\nwith this method: firstly, the la-beling rules are complex and the data is too\ndifficult to label, the workload of which is large; secondly, the algorithm\ncannot recognize the multi-granularity and diversity of language components. In\norder to solve these two problems, we propose labeling rules based on phrase\nwindows, and designed corresponding phrase recognition algorithms. The labeling\nrule uses phrases as the minimum unit, di-vides sentences into 7 types of\nnestable phrase types, and marks the grammatical dependencies between phrases.\nThe corresponding algorithm, drawing on the idea of identifying the target area\nin the image field, can find the start and end positions of various phrases in\nthe sentence, and realize the synchronous recognition of nested phrases and\ngrammatical dependencies. The results of the experiment shows that the labeling\nrule is convenient and easy to use, and there is no ambiguity; the algorithm is\nmore grammatically multi-granular and diverse than the end-to-end algorithm.\nExperiments on the CPWD dataset improve the accuracy of the end-to-end method\nby about 1 point. The corresponding method was applied to the CCL2018\ncompetition, and the first place in the Chinese Metaphor Sentiment Analysis\nTask.", "published": "2020-07-07 00:19:47", "link": "http://arxiv.org/abs/2007.03140v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "scb-mt-en-th-2020: A Large English-Thai Parallel Corpus", "abstract": "The primary objective of our work is to build a large-scale English-Thai\ndataset for machine translation. We construct an English-Thai machine\ntranslation dataset with over 1 million segment pairs, curated from various\nsources, namely news, Wikipedia articles, SMS messages, task-based dialogs,\nweb-crawled data and government documents. Methodology for gathering data,\nbuilding parallel texts and removing noisy sentence pairs are presented in a\nreproducible manner. We train machine translation models based on this dataset.\nOur models' performance are comparable to that of Google Translation API (as of\nMay 2020) for Thai-English and outperform Google when the Open Parallel Corpus\n(OPUS) is included in the training data for both Thai-English and English-Thai\ntranslation. The dataset, pre-trained models, and source code to reproduce our\nwork are available for public use.", "published": "2020-07-07 15:14:32", "link": "http://arxiv.org/abs/2007.03541v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating German Transformer Language Models with Syntactic Agreement\n  Tests", "abstract": "Pre-trained transformer language models (TLMs) have recently refashioned\nnatural language processing (NLP): Most state-of-the-art NLP models now operate\non top of TLMs to benefit from contextualization and knowledge induction. To\nexplain their success, the scientific community conducted numerous analyses.\nBesides other methods, syntactic agreement tests were utilized to analyse TLMs.\nMost of the studies were conducted for the English language, however. In this\nwork, we analyse German TLMs. To this end, we design numerous agreement tasks,\nsome of which consider peculiarities of the German language. Our experimental\nresults show that state-of-the-art German TLMs generally perform well on\nagreement tasks, but we also identify and discuss syntactic structures that\npush them to their limits.", "published": "2020-07-07 20:01:42", "link": "http://arxiv.org/abs/2007.03765v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Targeting the Benchmark: On Methodology in Current Natural Language\n  Processing Research", "abstract": "It has become a common pattern in our field: One group introduces a language\ntask, exemplified by a dataset, which they argue is challenging enough to serve\nas a benchmark. They also provide a baseline model for it, which then soon is\nimproved upon by other groups. Often, research efforts then move on, and the\npattern repeats itself. What is typically left implicit is the argumentation\nfor why this constitutes progress, and progress towards what. In this paper, we\ntry to step back for a moment from this pattern and work out possible\nargumentations and their parts.", "published": "2020-07-07 19:28:18", "link": "http://arxiv.org/abs/2007.04792v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DAM: Deliberation, Abandon and Memory Networks for Generating Detailed\n  and Non-repetitive Responses in Visual Dialogue", "abstract": "Visual Dialogue task requires an agent to be engaged in a conversation with\nhuman about an image. The ability of generating detailed and non-repetitive\nresponses is crucial for the agent to achieve human-like conversation. In this\npaper, we propose a novel generative decoding architecture to generate\nhigh-quality responses, which moves away from decoding the whole encoded\nsemantics towards the design that advocates both transparency and flexibility.\nIn this architecture, word generation is decomposed into a series of\nattention-based information selection steps, performed by the novel recurrent\nDeliberation, Abandon and Memory (DAM) module. Each DAM module performs an\nadaptive combination of the response-level semantics captured from the encoder\nand the word-level semantics specifically selected for generating each word.\nTherefore, the responses contain more detailed and non-repetitive descriptions\nwhile maintaining the semantic accuracy. Furthermore, DAM is flexible to\ncooperate with existing visual dialogue encoders and adaptive to the encoder\nstructures by constraining the information selection mode in DAM. We apply DAM\nto three typical encoders and verify the performance on the VisDial v1.0\ndataset. Experimental results show that the proposed models achieve new\nstate-of-the-art performance with high-quality responses. The code is available\nat https://github.com/JXZe/DAM.", "published": "2020-07-07 09:49:47", "link": "http://arxiv.org/abs/2007.03310v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Continual BERT: Continual Learning for Adaptive Extractive Summarization\n  of COVID-19 Literature", "abstract": "The scientific community continues to publish an overwhelming amount of new\nresearch related to COVID-19 on a daily basis, leading to much literature\nwithout little to no attention. To aid the community in understanding the\nrapidly flowing array of COVID-19 literature, we propose a novel BERT\narchitecture that provides a brief yet original summarization of lengthy\npapers. The model continually learns on new data in online fashion while\nminimizing catastrophic forgetting, thus fitting to the need of the community.\nBenchmark and manual examination of its performance show that the model provide\na sound summary of new scientific literature.", "published": "2020-07-07 13:16:19", "link": "http://arxiv.org/abs/2007.03405v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Go Transformer: Natural Language Modeling for Game Play", "abstract": "This work applies natural language modeling to generate plausible strategic\nmoves in the ancient game of Go. We train the Generative Pretrained Transformer\n(GPT-2) to mimic the style of Go champions as archived in Smart Game Format\n(SGF), which offers a text description of move sequences. The trained model\nfurther generates valid but previously unseen strategies for Go. Because GPT-2\npreserves punctuation and spacing, the raw output of the text generator\nprovides inputs to game visualization and creative patterns, such as the Sabaki\nproject's game engine using auto-replays. Results demonstrate that language\nmodeling can capture both the sequencing format of championship Go games and\ntheir strategic formations. Compared to random game boards, the GPT-2\nfine-tuning shows efficient opening move sequences favoring corner play over\nless advantageous center and side play. Game generation as a language modeling\ntask offers novel approaches to more than 40 other board games where historical\ntext annotation provides training data (e.g., Amazons & Connect 4/6).", "published": "2020-07-07 14:37:27", "link": "http://arxiv.org/abs/2007.03500v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Inductive Transfer to Detect Offensive Language", "abstract": "With the growing use of social media and its availability, many instances of\nthe use of offensive language have been observed across multiple languages and\ndomains. This phenomenon has given rise to the growing need to detect the\noffensive language used in social media cross-lingually. In OffensEval 2020,\nthe organizers have released the \\textit{multilingual Offensive Language\nIdentification Dataset} (mOLID), which contains tweets in five different\nlanguages, to detect offensive language. In this work, we introduce a\ncross-lingual inductive approach to identify the offensive language in tweets\nusing the contextual word embedding \\textit{XLM-RoBERTa} (XLM-R). We show that\nour model performs competitively on all five languages, obtaining the fourth\nposition in the English task with an F1-score of $0.919$ and eighth position in\nthe Turkish task with an F1-score of $0.781$. Further experimentation proves\nthat our model works competitively in a zero-shot learning environment, and is\nextensible to other languages.", "published": "2020-07-07 20:10:31", "link": "http://arxiv.org/abs/2007.03771v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Pre-Trained Models for Heterogeneous Information Networks", "abstract": "In network representation learning we learn how to represent heterogeneous\ninformation networks in a low-dimensional space so as to facilitate effective\nsearch, classification, and prediction solutions. Previous network\nrepresentation learning methods typically require sufficient task-specific\nlabeled data to address domain-specific problems. The trained model usually\ncannot be transferred to out-of-domain datasets. We propose a self-supervised\npre-training and fine-tuning framework, PF-HIN, to capture the features of a\nheterogeneous information network. Unlike traditional network representation\nlearning models that have to train the entire model all over again for every\ndownstream task and dataset, PF-HIN only needs to fine-tune the model and a\nsmall number of extra task-specific parameters, thus improving model efficiency\nand effectiveness. During pre-training, we first transform the neighborhood of\na given node into a sequence. PF-HIN is pre-trained based on two\nself-supervised tasks, masked node modeling and adjacent node prediction. We\nadopt deep bi-directional transformer encoders to train the model, and leverage\nfactorized embedding parameterization and cross-layer parameter sharing to\nreduce the parameters. In the fine-tuning stage, we choose four benchmark\ndownstream tasks, i.e., link prediction, similarity search, node\nclassification, and node clustering. PF-HIN consistently and significantly\noutperforms state-of-the-art alternatives on each of these tasks, on four\ndatasets.", "published": "2020-07-07 03:36:28", "link": "http://arxiv.org/abs/2007.03184v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Structured (De)composable Representations Trained with Neural Networks", "abstract": "The paper proposes a novel technique for representing templates and instances\nof concept classes. A template representation refers to the generic\nrepresentation that captures the characteristics of an entire class. The\nproposed technique uses end-to-end deep learning to learn structured and\ncomposable representations from input images and discrete labels. The obtained\nrepresentations are based on distance estimates between the distributions given\nby the class label and those given by contextual information, which are modeled\nas environments. We prove that the representations have a clear structure\nallowing to decompose the representation into factors that represent classes\nand environments. We evaluate our novel technique on classification and\nretrieval tasks involving different modalities (visual and language data).", "published": "2020-07-07 10:20:31", "link": "http://arxiv.org/abs/2007.03325v1", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Diverse and Styled Image Captioning Using SVD-Based Mixture of Recurrent\n  Experts", "abstract": "With great advances in vision and natural language processing, the generation\nof image captions becomes a need. In a recent paper, Mathews, Xie and He [1],\nextended a new model to generate styled captions by separating semantics and\nstyle. In continuation of this work, here a new captioning model is developed\nincluding an image encoder to extract the features, a mixture of recurrent\nnetworks to embed the set of extracted features to a set of words, and a\nsentence generator that combines the obtained words as a stylized sentence. The\nresulted system that entitled as Mixture of Recurrent Experts (MoRE), uses a\nnew training algorithm that derives singular value decomposition (SVD) from\nweighting matrices of Recurrent Neural Networks (RNNs) to increase the\ndiversity of captions. Each decomposition step depends on a distinctive factor\nbased on the number of RNNs in MoRE. Since the used sentence generator gives a\nstylized language corpus without paired images, our captioning model can do the\nsame. Besides, the styled and diverse captions are extracted without training\non a densely labeled or styled dataset. To validate this captioning model, we\nuse Microsoft COCO which is a standard factual image caption dataset. We show\nthat the proposed captioning model can generate a diverse and stylized image\ncaptions without the necessity of extra-labeling. The results also show better\ndescriptions in terms of content accuracy.", "published": "2020-07-07 11:00:27", "link": "http://arxiv.org/abs/2007.03338v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Do Transformers Need Deep Long-Range Memory", "abstract": "Deep attention models have advanced the modelling of sequential data across\nmany domains. For language modelling in particular, the Transformer-XL -- a\nTransformer augmented with a long-range memory of past activations -- has been\nshown to be state-of-the-art across a variety of well-studied benchmarks. The\nTransformer-XL incorporates a long-range memory at every layer of the network,\nwhich renders its state to be thousands of times larger than RNN predecessors.\nHowever it is unclear whether this is necessary. We perform a set of\ninterventions to show that comparable performance can be obtained with 6X fewer\nlong range memories and better performance can be obtained by limiting the\nrange of attention in lower layers of the network.", "published": "2020-07-07 11:48:49", "link": "http://arxiv.org/abs/2007.03356v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Cultural Convergence: Insights into the behavior of misinformation\n  networks on Twitter", "abstract": "How can the birth and evolution of ideas and communities in a network be\nstudied over time? We use a multimodal pipeline, consisting of network mapping,\ntopic modeling, bridging centrality, and divergence to analyze Twitter data\nsurrounding the COVID-19 pandemic. We use network mapping to detect accounts\ncreating content surrounding COVID-19, then Latent Dirichlet Allocation to\nextract topics, and bridging centrality to identify topical and non-topical\nbridges, before examining the distribution of each topic and bridge over time\nand applying Jensen-Shannon divergence of topic distributions to show\ncommunities that are converging in their topical narratives.", "published": "2020-07-07 13:50:24", "link": "http://arxiv.org/abs/2007.03443v1", "categories": ["cs.SI", "cs.CL", "physics.soc-ph", "H.1.2; H.4.3; I.2.1; I.2.6; I.2.7; I.5"], "primary_category": "cs.SI"}
{"title": "An Emergency Medical Services Clinical Audit System driven by Named\n  Entity Recognition from Deep Learning", "abstract": "Clinical performance audits are routinely performed in Emergency Medical\nServices (EMS) to ensure adherence to treatment protocols, to identify\nindividual areas of weakness for remediation, and to discover systemic\ndeficiencies to guide the development of the training syllabus. At present,\nthese audits are performed by manual chart review which is time-consuming and\nlaborious. In this paper, we present an automatic audit system based on both\nthe structured and unstructured ambulance case records and clinical notes with\na deep neural network-based named entities recognition model. The dataset used\nin this study contained 58,898 unlabelled ambulance incidents encountered by\nthe Singapore Civil Defence Force from 1st April 2019 to 30th June 2019. A\nweakly-supervised training approach was adopted to label the sentences. Later\non, we trained three different models to perform the NER task. All three models\nachieve F1 scores of around 0.981 under entity type matching evaluation and\naround 0.976 under strict evaluation, while the BiLSTM-CRF model is 1~2 orders\nof magnitude lighter and faster than our BERT-based models. Overall, our\napproach yielded a named entity recognition model that could reliably identify\nclinical entities from unstructured paramedic free-text reports. Our proposed\nsystem may improve the efficiency of clinical performance audits and can also\nhelp with EMS database research.", "published": "2020-07-07 16:32:44", "link": "http://arxiv.org/abs/2007.03596v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What Gives the Answer Away? Question Answering Bias Analysis on Video QA\n  Datasets", "abstract": "Question answering biases in video QA datasets can mislead multimodal model\nto overfit to QA artifacts and jeopardize the model's ability to generalize.\nUnderstanding how strong these QA biases are and where they come from helps the\ncommunity measure progress more accurately and provide researchers insights to\ndebug their models. In this paper, we analyze QA biases in popular video\nquestion answering datasets and discover pretrained language models can answer\n37-48% questions correctly without using any multimodal context information,\nfar exceeding the 20% random guess baseline for 5-choose-1 multiple-choice\nquestions. Our ablation study shows biases can come from annotators and type of\nquestions. Specifically, annotators that have been seen during training are\nbetter predicted by the model and reasoning, abstract questions incur more\nbiases than factual, direct questions. We also show empirically that using\nannotator-non-overlapping train-test splits can reduce QA biases for video QA\ndatasets.", "published": "2020-07-07 17:00:11", "link": "http://arxiv.org/abs/2007.03626v1", "categories": ["cs.CL", "cs.CV", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "The curious case of developmental BERTology: On sparsity, transfer\n  learning, generalization and the brain", "abstract": "In this essay, we explore a point of intersection between deep learning and\nneuroscience, through the lens of large language models, transfer learning and\nnetwork compression. Just like perceptual and cognitive neurophysiology has\ninspired effective deep neural network architectures which in turn make a\nuseful model for understanding the brain, here we explore how biological neural\ndevelopment might inspire efficient and robust optimization procedures which in\nturn serve as a useful model for the maturation and aging of the brain.", "published": "2020-07-07 20:16:30", "link": "http://arxiv.org/abs/2007.03774v1", "categories": ["cs.CL", "cs.LG", "q-bio.NC", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Placepedia: Comprehensive Place Understanding with Multi-Faceted\n  Annotations", "abstract": "Place is an important element in visual understanding. Given a photo of a\nbuilding, people can often tell its functionality, e.g. a restaurant or a shop,\nits cultural style, e.g. Asian or European, as well as its economic type, e.g.\nindustry oriented or tourism oriented. While place recognition has been widely\nstudied in previous work, there remains a long way towards comprehensive place\nunderstanding, which is far beyond categorizing a place with an image and\nrequires information of multiple aspects. In this work, we contribute\nPlacepedia, a large-scale place dataset with more than 35M photos from 240K\nunique places. Besides the photos, each place also comes with massive\nmulti-faceted information, e.g. GDP, population, etc., and labels at multiple\nlevels, including function, city, country, etc.. This dataset, with its large\namount of data and rich annotations, allows various studies to be conducted.\nParticularly, in our studies, we develop 1) PlaceNet, a unified framework for\nmulti-level place recognition, and 2) a method for city embedding, which can\nproduce a vector representation for a city that captures both visual and\nmulti-faceted side information. Such studies not only reveal key challenges in\nplace understanding, but also establish connections between visual observations\nand underlying socioeconomic/cultural implications.", "published": "2020-07-07 20:17:01", "link": "http://arxiv.org/abs/2007.03777v4", "categories": ["cs.CV", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
{"title": "ISA: An Intelligent Shopping Assistant", "abstract": "Despite the growth of e-commerce, brick-and-mortar stores are still the\npreferred destinations for many people. In this paper, we present ISA, a\nmobile-based intelligent shopping assistant that is designed to improve\nshopping experience in physical stores. ISA assists users by leveraging\nadvanced techniques in computer vision, speech processing, and natural language\nprocessing. An in-store user only needs to take a picture or scan the barcode\nof the product of interest, and then the user can talk to the assistant about\nthe product. The assistant can also guide the user through the purchase process\nor recommend other similar products to the user. We take a data-driven approach\nin building the engines of ISA's natural language processing component, and the\nengines achieve good performance.", "published": "2020-07-07 21:57:34", "link": "http://arxiv.org/abs/2007.03805v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Expressive Interviewing: A Conversational System for Coping with\n  COVID-19", "abstract": "The ongoing COVID-19 pandemic has raised concerns for many regarding personal\nand public health implications, financial security and economic stability.\nAlongside many other unprecedented challenges, there are increasing concerns\nover social isolation and mental health. We introduce \\textit{Expressive\nInterviewing}--an interview-style conversational system that draws on ideas\nfrom motivational interviewing and expressive writing. Expressive Interviewing\nseeks to encourage users to express their thoughts and feelings through writing\nby asking them questions about how COVID-19 has impacted their lives. We\npresent relevant aspects of the system's design and implementation as well as\nquantitative and qualitative analyses of user interactions with the system. In\naddition, we conduct a comparative evaluation with a general purpose dialogue\nsystem for mental health that shows our system potential in helping users to\ncope with COVID-19 issues.", "published": "2020-07-07 22:52:14", "link": "http://arxiv.org/abs/2007.03819v1", "categories": ["cs.HC", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "Predicting Afrobeats Hit Songs Using Spotify Data", "abstract": "This study approached the Hit Song Science problem with the aim of predicting\nwhich songs in the Afrobeats genre will become popular among Spotify listeners.\nA dataset of 2063 songs was generated through the Spotify Web API, with the\nprovided audio features. Random Forest and Gradient Boosting algorithms proved\nto be successful with approximately F1 scores of 86%.", "published": "2020-07-07 00:14:30", "link": "http://arxiv.org/abs/2007.03137v2", "categories": ["cs.IR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "Multi-Tones' Phase Coding (MTPC) of Interaural Time Difference by\n  Spiking Neural Network", "abstract": "Inspired by the mammal's auditory localization pathway, in this paper we\npropose a pure spiking neural network (SNN) based computational model for\nprecise sound localization in the noisy real-world environment, and implement\nthis algorithm in a real-time robotic system with a microphone array. The key\nof this model relies on the MTPC scheme, which encodes the interaural time\ndifference (ITD) cues into spike patterns. This scheme naturally follows the\nfunctional structures of the human auditory localization system, rather than\nartificially computing of time difference of arrival. Besides, it highlights\nthe advantages of SNN, such as event-driven and power efficiency. The MTPC is\npipelined with two different SNN architectures, the convolutional SNN and\nrecurrent SNN, by which it shows the applicability to various SNNs. This\nproposal is evaluated by the microphone collected location-dependent acoustic\ndata, in a real-world environment with noise, obstruction, reflection, or other\naffects. The experiment results show a mean error azimuth of 1~3 degrees, which\nsurpasses the accuracy of the other biologically plausible neuromorphic\napproach for sound source localization.", "published": "2020-07-07 08:22:56", "link": "http://arxiv.org/abs/2007.03274v1", "categories": ["eess.AS", "cs.NE", "cs.SD"], "primary_category": "eess.AS"}
{"title": "X-vectors: New Quantitative Biomarkers for Early Parkinson's Disease\n  Detection from Speech", "abstract": "Many articles have used voice analysis to detect Parkinson's disease (PD),\nbut few have focused on the early stages of the disease and the gender effect.\nIn this article, we have adapted the latest speaker recognition system, called\nx-vectors, in order to detect an early stage of PD from voice analysis.\nX-vectors are embeddings extracted from a deep neural network, which provide\nrobust speaker representations and improve speaker recognition when large\namounts of training data are used. Our goal was to assess whether, in the\ncontext of early PD detection, this technique would outperform the more\nstandard classifier MFCC-GMM (Mel-Frequency Cepstral Coefficients - Gaussian\nMixture Model) and, if so, under which conditions. We recorded 221 French\nspeakers (including recently diagnosed PD subjects and healthy controls) with a\nhigh-quality microphone and with their own telephone. Men and women were\nanalyzed separately in order to have more precise models and to assess a\npossible gender effect. Several experimental and methodological aspects were\ntested in order to analyze their impacts on classification performance. We\nassessed the impact of audio segment duration, data augmentation, type of\ndataset used for the neural network training, kind of speech tasks, and\nback-end analyses. X-vectors technique provided better classification\nperformances than MFCC-GMM for text-independent tasks, and seemed to be\nparticularly suited for the early detection of PD in women (7 to 15%\nimprovement). This result was observed for both recording types (high-quality\nmicrophone and telephone).", "published": "2020-07-07 16:34:51", "link": "http://arxiv.org/abs/2007.03599v1", "categories": ["eess.AS", "cs.SD", "q-bio.QM"], "primary_category": "eess.AS"}
