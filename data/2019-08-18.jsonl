{"title": "Understanding Undesirable Word Embedding Associations", "abstract": "Word embeddings are often criticized for capturing undesirable word\nassociations such as gender stereotypes. However, methods for measuring and\nremoving such biases remain poorly understood. We show that for any embedding\nmodel that implicitly does matrix factorization, debiasing vectors post hoc\nusing subspace projection (Bolukbasi et al., 2016) is, under certain\nconditions, equivalent to training on an unbiased corpus. We also prove that\nWEAT, the most common association test for word embeddings, systematically\noverestimates bias. Given that the subspace projection method is provably\neffective, we use it to derive a new measure of association called the\n$\\textit{relational inner product association}$ (RIPA). Experiments with RIPA\nreveal that, on average, skipgram with negative sampling (SGNS) does not make\nmost words any more gendered than they are in the training corpus. However, for\ngender-stereotyped words, SGNS actually amplifies the gender association in the\ncorpus.", "published": "2019-08-18 01:28:45", "link": "http://arxiv.org/abs/1908.06361v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Concurrent Parsing of Constituency and Dependency", "abstract": "Constituent and dependency representation for syntactic structure share a lot\nof linguistic and computational characteristics, this paper thus makes the\nfirst attempt by introducing a new model that is capable of parsing constituent\nand dependency at the same time, so that lets either of the parsers enhance\neach other. Especially, we evaluate the effect of different shared network\ncomponents and empirically verify that dependency parsing may be much more\nbeneficial from constituent parsing structure.\n  The proposed parser achieves new state-of-the-art performance for both\nparsing tasks, constituent and dependency on PTB and CTB benchmarks.", "published": "2019-08-18 05:10:59", "link": "http://arxiv.org/abs/1908.06379v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TDAM: a Topic-Dependent Attention Model for Sentiment Analysis", "abstract": "We propose a topic-dependent attention model for sentiment classification and\ntopic extraction. Our model assumes that a global topic embedding is shared\nacross documents and employs an attention mechanism to derive local topic\nembedding for words and sentences. These are subsequently incorporated in a\nmodified Gated Recurrent Unit (GRU) for sentiment classification and extraction\nof topics bearing different sentiment polarities. Those topics emerge from the\nwords' local topic embeddings learned by the internal attention of the GRU\ncells in the context of a multi-task learning framework. In this paper, we\npresent the hierarchical architecture, the new GRU unit and the experiments\nconducted on users' reviews which demonstrate classification performance on a\npar with the state-of-the-art methodologies for sentiment classification and\ntopic coherence outperforming the current approaches for supervised topic\nextraction. In addition, our model is able to extract coherent aspect-sentiment\nclusters despite using no aspect-level annotations for training.", "published": "2019-08-18 12:50:47", "link": "http://arxiv.org/abs/1908.06435v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TwistBytes -- Hierarchical Classification at GermEval 2019: walking the\n  fine line (of recall and precision)", "abstract": "We present here our approach to the GermEval 2019 Task 1 - Shared Task on\nhierarchical classification of German blurbs. We achieved first place in the\nhierarchical subtask B and second place on the root node, flat classification\nsubtask A. In subtask A, we applied a simple multi-feature TF-IDF extraction\nmethod using different n-gram range and stopword removal, on each feature\nextraction module. The classifier on top was a standard linear SVM. For the\nhierarchical classification, we used a local approach, which was more\nlight-weighted but was similar to the one used in subtask A. The key point of\nour approach was the application of a post-processing to cope with the\nmulti-label aspect of the task, increasing the recall but not surpassing the\nprecision measure score.", "published": "2019-08-18 18:09:19", "link": "http://arxiv.org/abs/1908.06493v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Modeling Islamist Extremist Communications on Social Media using\n  Contextual Dimensions: Religion, Ideology, and Hate", "abstract": "Terror attacks have been linked in part to online extremist content. Although\ntens of thousands of Islamist extremism supporters consume such content, they\nare a small fraction relative to peaceful Muslims. The efforts to contain the\never-evolving extremism on social media platforms have remained inadequate and\nmostly ineffective. Divergent extremist and mainstream contexts challenge\nmachine interpretation, with a particular threat to the precision of\nclassification algorithms. Our context-aware computational approach to the\nanalysis of extremist content on Twitter breaks down this persuasion process\ninto building blocks that acknowledge inherent ambiguity and sparsity that\nlikely challenge both manual and automated classification. We model this\nprocess using a combination of three contextual dimensions -- religion,\nideology, and hate -- each elucidating a degree of radicalization and\nhighlighting independent features to render them computationally accessible. We\nutilize domain-specific knowledge resources for each of these contextual\ndimensions such as Qur'an for religion, the books of extremist ideologues and\npreachers for political ideology and a social media hate speech corpus for\nhate. Our study makes three contributions to reliable analysis: (i) Development\nof a computational approach rooted in the contextual dimensions of religion,\nideology, and hate that reflects strategies employed by online Islamist\nextremist groups, (ii) An in-depth analysis of relevant tweet datasets with\nrespect to these dimensions to exclude likely mislabeled users, and (iii) A\nframework for understanding online radicalization as a process to assist\ncounter-programming. Given the potentially significant social impact, we\nevaluate the performance of our algorithms to minimize mislabeling, where our\napproach outperforms a competitive baseline by 10.2% in precision.", "published": "2019-08-18 21:46:19", "link": "http://arxiv.org/abs/1908.06520v3", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "RefNet: A Reference-aware Network for Background Based Conversation", "abstract": "Existing conversational systems tend to generate generic responses. Recently,\nBackground Based Conversations (BBCs) have been introduced to address this\nissue. Here, the generated responses are grounded in some background\ninformation. The proposed methods for BBCs are able to generate more\ninformative responses, they either cannot generate natural responses or have\ndifficulty in locating the right background information. In this paper, we\npropose a Reference-aware Network (RefNet) to address the two issues. Unlike\nexisting methods that generate responses token by token, RefNet incorporates a\nnovel reference decoder that provides an alternative way to learn to directly\ncite a semantic unit (e.g., a span containing complete semantic information)\nfrom the background. Experimental results show that RefNet significantly\noutperforms state-of-the-art methods in terms of both automatic and human\nevaluations, indicating that RefNet can generate more appropriate and\nhuman-like responses.", "published": "2019-08-18 14:49:16", "link": "http://arxiv.org/abs/1908.06449v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Multi-level Neural Network for Implicit Causality Detection in Web\n  Texts", "abstract": "Mining causality from text is a complex and crucial natural language\nunderstanding task corresponding to the human cognition. Existing studies at\nits solution can be grouped into two primary categories: feature engineering\nbased and neural model based methods. In this paper, we find that the former\nhas incomplete coverage and inherent errors but provide prior knowledge; while\nthe latter leverages context information but causal inference of which is\ninsufficiency. To handle the limitations, we propose a novel causality\ndetection model named MCDN to explicitly model causal reasoning process, and\nfurthermore, to exploit the advantages of both methods. Specifically, we adopt\nmulti-head self-attention to acquire semantic feature at word level and develop\nthe SCRN to infer causality at segment level. To the best of our knowledge,\nwith regards to the causality tasks, this is the first time that the Relation\nNetwork is applied. The experimental results show that: 1) the proposed\napproach performs prominent performance on causality detection; 2) further\nanalysis manifests the effectiveness and robustness of MCDN.", "published": "2019-08-18 10:34:59", "link": "http://arxiv.org/abs/1908.07822v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Parsimonious Morpheme Segmentation with an Application to Enriching Word\n  Embeddings", "abstract": "Traditionally, many text-mining tasks treat individual word-tokens as the\nfinest meaningful semantic granularity. However, in many languages and\nspecialized corpora, words are composed by concatenating semantically\nmeaningful subword structures. Word-level analysis cannot leverage the semantic\ninformation present in such subword structures. With regard to word embedding\ntechniques, this leads to not only poor embeddings for infrequent words in\nlong-tailed text corpora but also weak capabilities for handling\nout-of-vocabulary words. In this paper we propose MorphMine for unsupervised\nmorpheme segmentation. MorphMine applies a parsimony criterion to\nhierarchically segment words into the fewest number of morphemes at each level\nof the hierarchy. This leads to longer shared morphemes at each level of\nsegmentation. Experiments show that MorphMine segments words in a variety of\nlanguages into human-verified morphemes. Additionally, we experimentally\ndemonstrate that utilizing MorphMine morphemes to enrich word embeddings\nconsistently improves embedding quality on a variety of of embedding\nevaluations and a downstream language modeling task.", "published": "2019-08-18 00:45:16", "link": "http://arxiv.org/abs/1908.07832v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Dual-Staged Context Aggregation Method Towards Efficient End-To-End\n  Speech Enhancement", "abstract": "In speech enhancement, an end-to-end deep neural network converts a noisy\nspeech signal to a clean speech directly in time domain without time-frequency\ntransformation or mask estimation. However, aggregating contextual information\nfrom a high-resolution time domain signal with an affordable model complexity\nstill remains challenging. In this paper, we propose a densely connected\nconvolutional and recurrent network (DCCRN), a hybrid architecture, to enable\ndual-staged temporal context aggregation. With the dense connectivity and\ncross-component identical shortcut, DCCRN consistently outperforms competing\nconvolutional baselines with an average STOI improvement of 0.23 and PESQ of\n1.38 at three SNR levels. The proposed method is computationally efficient with\nonly 1.38 million parameters. The generalizability performance on the unseen\nnoise types is still decent considering its low complexity, although it is\nrelatively weaker comparing to Wave-U-Net with 7.25 times more parameters.", "published": "2019-08-18 15:53:09", "link": "http://arxiv.org/abs/1908.06468v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Musical Rhythm Transcription Based on Bayesian Piece-Specific Score\n  Models Capturing Repetitions", "abstract": "Most work on musical score models (a.k.a. musical language models) for music\ntranscription has focused on describing the local sequential dependence of\nnotes in musical scores and failed to capture their global repetitive\nstructure, which can be a useful guide for transcribing music. Focusing on\nrhythm, we formulate several classes of Bayesian Markov models of musical\nscores that describe repetitions indirectly using the sparse transition\nprobabilities of notes or note patterns. This enables us to construct\npiece-specific models for unseen scores with an unfixed repetitive structure\nand to derive tractable inference algorithms. Moreover, to describe approximate\nrepetitions, we explicitly incorporate a process for modifying the repeated\nnotes/note patterns. We apply these models as prior musical score models for\nrhythm transcription, where piece-specific score models are inferred from\nperformed MIDI data by Bayesian learning, in contrast to the conventional\nsupervised construction of score models. Evaluations using the vocal melodies\nof popular music showed that the Bayesian models improved the transcription\naccuracy for most of the tested model types, indicating the universal efficacy\nof the proposed approach. Moreover, we found an effective data representation\nfor modelling rhythms that maximizes the transcription accuracy and\ncomputational efficiency.", "published": "2019-08-18 14:26:59", "link": "http://arxiv.org/abs/1908.06969v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
