{"title": "Constructing Colloquial Dataset for Persian Sentiment Analysis of Social\n  Microblogs", "abstract": "Introduction: Microblogging websites have massed rich data sources for\nsentiment analysis and opinion mining. In this regard, sentiment classification\nhas frequently proven inefficient because microblog posts typically lack\nsyntactically consistent terms and representatives since users on these social\nnetworks do not like to write lengthy statements. Also, there are some\nlimitations to low-resource languages. The Persian language has exceptional\ncharacteristics and demands unique annotated data and models for the sentiment\nanalysis task, which are distinctive from text features within the English\ndialect. Method: This paper first constructs a user opinion dataset called\nITRC-Opinion in a collaborative environment and insource way. Our dataset\ncontains 60,000 informal and colloquial Persian texts from social microblogs\nsuch as Twitter and Instagram. Second, this study proposes a new architecture\nbased on the convolutional neural network (CNN) model for more effective\nsentiment analysis of colloquial text in social microblog posts. The\nconstructed datasets are used to evaluate the presented architecture.\nFurthermore, some models, such as LSTM, CNN-RNN, BiLSTM, and BiGRU with\ndifferent word embeddings, including Fasttext, Glove, and Word2vec,\ninvestigated our dataset and evaluated the results. Results: The results\ndemonstrate the benefit of our dataset and the proposed model (72% accuracy),\ndisplaying meaningful improvement in sentiment classification performance.", "published": "2023-06-22 05:51:22", "link": "http://arxiv.org/abs/2306.12679v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Neural Machine Translation System for Indic to Indic\n  Languages", "abstract": "This paper gives an Indic-to-Indic (IL-IL) MNMT baseline model for 11 ILs\nimplemented on the Samanantar corpus and analyzed on the Flores-200 corpus. All\nthe models are evaluated using the BLEU score. In addition, the languages are\nclassified under three groups namely East Indo- Aryan (EI), Dravidian (DR), and\nWest Indo-Aryan (WI). The effect of language relatedness on MNMT model\nefficiency is studied. Owing to the presence of large corpora from English (EN)\nto ILs, MNMT IL-IL models using EN as a pivot are also built and examined. To\nachieve this, English- Indic (EN-IL) models are also developed, with and\nwithout the usage of related languages. Results reveal that using related\nlanguages is beneficial for the WI group only, while it is detrimental for the\nEI group and shows an inconclusive effect on the DR group, but it is useful for\nEN-IL models. Thus, related language groups are used to develop pivot MNMT\nmodels. Furthermore, the IL corpora are transliterated from the corresponding\nscripts to a modified ITRANS script, and the best MNMT models from the previous\napproaches are built on the transliterated corpus. It is observed that the\nusage of pivot models greatly improves MNMT baselines with AS-TA achieving the\nminimum BLEU score and PA-HI achieving the maximum score. Among languages, AS,\nML, and TA achieve the lowest BLEU score, whereas HI, PA, and GU perform the\nbest. Transliteration also helps the models with few exceptions. The best\nincrement of scores is observed in ML, TA, and BN and the worst average\nincrement is observed in KN, HI, and PA, across all languages. The best model\nobtained is the PA-HI language pair trained on PAWI transliterated corpus which\ngives 24.29 BLEU.", "published": "2023-06-22 06:44:09", "link": "http://arxiv.org/abs/2306.12693v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Generation for Advertising: A Survey", "abstract": "Natural language generation methods have emerged as effective tools to help\nadvertisers increase the number of online advertisements they produce. This\nsurvey entails a review of the research trends on this topic over the past\ndecade, from template-based to extractive and abstractive approaches using\nneural networks. Additionally, key challenges and directions revealed through\nthe survey, including metric optimization, faithfulness, diversity,\nmultimodality, and the development of benchmark datasets, are discussed.", "published": "2023-06-22 07:52:34", "link": "http://arxiv.org/abs/2306.12719v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generative Multimodal Entity Linking", "abstract": "Multimodal Entity Linking (MEL) is the task of mapping mentions with\nmultimodal contexts to the referent entities from a knowledge base. Existing\nMEL methods mainly focus on designing complex multimodal interaction mechanisms\nand require fine-tuning all model parameters, which can be prohibitively costly\nand difficult to scale in the era of Large Language Models (LLMs). In this\nwork, we propose GEMEL, a Generative Multimodal Entity Linking framework based\non LLMs, which directly generates target entity names. We keep the vision and\nlanguage model frozen and only train a feature mapper to enable cross-modality\ninteractions. To adapt LLMs to the MEL task, we leverage the in-context\nlearning capability of LLMs by retrieving multimodal instances as\ndemonstrations. Extensive experiments show that, with only ~0.3% of the model\nparameters fine-tuned, GEMEL achieves state-of-the-art results on two\nwell-established MEL datasets (7.7% accuracy gains on WikiDiverse and 8.8%\naccuracy gains on WikiMEL). The performance gain stems from mitigating the\npopularity bias of LLM predictions and disambiguating less common entities\neffectively. Further analysis verifies the generality and scalability of GEMEL.\nOur framework is compatible with any off-the-shelf language model, paving the\nway towards an efficient and general solution for utilizing LLMs in the MEL\ntask. Our code is available at https://github.com/HITsz-TMG/GEMEL.", "published": "2023-06-22 07:57:19", "link": "http://arxiv.org/abs/2306.12725v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mapping and Cleaning Open Commonsense Knowledge Bases with Generative\n  Translation", "abstract": "Structured knowledge bases (KBs) are the backbone of many\nknow\\-ledge-intensive applications, and their automated construction has\nreceived considerable attention. In particular, open information extraction\n(OpenIE) is often used to induce structure from a text. However, although it\nallows high recall, the extracted knowledge tends to inherit noise from the\nsources and the OpenIE algorithm. Besides, OpenIE tuples contain an open-ended,\nnon-canonicalized set of relations, making the extracted knowledge's downstream\nexploitation harder. In this paper, we study the problem of mapping an open KB\ninto the fixed schema of an existing KB, specifically for the case of\ncommonsense knowledge. We propose approaching the problem by generative\ntranslation, i.e., by training a language model to generate fixed-schema\nassertions from open ones. Experiments show that this approach occupies a sweet\nspot between traditional manual, rule-based, or classification-based\ncanonicalization and purely generative KB construction like COMET. Moreover, it\nproduces higher mapping accuracy than the former while avoiding the\nassociation-based noise of the latter.", "published": "2023-06-22 09:42:54", "link": "http://arxiv.org/abs/2306.12766v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overview of Robust and Multilingual Automatic Evaluation Metrics for\n  Open-Domain Dialogue Systems at DSTC 11 Track 4", "abstract": "The advent and fast development of neural networks have revolutionized the\nresearch on dialogue systems and subsequently have triggered various challenges\nregarding their automatic evaluation. Automatic evaluation of open-domain\ndialogue systems as an open challenge has been the center of the attention of\nmany researchers. Despite the consistent efforts to improve automatic metrics'\ncorrelations with human evaluation, there have been very few attempts to assess\ntheir robustness over multiple domains and dimensions. Also, their focus is\nmainly on the English language. All of these challenges prompt the development\nof automatic evaluation metrics that are reliable in various domains,\ndimensions, and languages. This track in the 11th Dialogue System Technology\nChallenge (DSTC11) is part of the ongoing effort to promote robust and\nmultilingual automatic evaluation metrics. This article describes the datasets\nand baselines provided to participants and discusses the submission and result\ndetails of the two proposed subtasks.", "published": "2023-06-22 10:50:23", "link": "http://arxiv.org/abs/2306.12794v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "xSIM++: An Improved Proxy to Bitext Mining Performance for Low-Resource\n  Languages", "abstract": "We introduce a new proxy score for evaluating bitext mining based on\nsimilarity in a multilingual embedding space: xSIM++. In comparison to xSIM,\nthis improved proxy leverages rule-based approaches to extend English sentences\nin any evaluation set with synthetic, hard-to-distinguish examples which more\nclosely mirror the scenarios we encounter during large-scale mining. We\nvalidate this proxy by running a significant number of bitext mining\nexperiments for a set of low-resource languages, and subsequently train NMT\nsystems on the mined data. In comparison to xSIM, we show that xSIM++ is better\ncorrelated with the downstream BLEU scores of translation systems trained on\nmined bitexts, providing a reliable proxy of bitext mining performance without\nneeding to run expensive bitext mining pipelines. xSIM++ also reports\nperformance for different error types, offering more fine-grained feedback for\nmodel development.", "published": "2023-06-22 14:20:15", "link": "http://arxiv.org/abs/2306.12907v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Cross-temporal Summarization: Dataset, Models, Evaluation", "abstract": "While summarization has been extensively researched in natural language\nprocessing (NLP), cross-lingual cross-temporal summarization (CLCTS) is a\nlargely unexplored area that has the potential to improve cross-cultural\naccessibility and understanding. This paper comprehensively addresses the CLCTS\ntask, including dataset creation, modeling, and evaluation. We (1) build the\nfirst CLCTS corpus with 328 instances for hDe-En (extended version with 455\ninstances) and 289 for hEn-De (extended version with 501 instances), leveraging\nhistorical fiction texts and Wikipedia summaries in English and German; (2)\nexamine the effectiveness of popular transformer end-to-end models with\ndifferent intermediate finetuning tasks; (3) explore the potential of GPT-3.5\nas a summarizer; (4) report evaluations from humans, GPT-4, and several recent\nautomatic evaluation metrics. Our results indicate that intermediate task\nfinetuned end-to-end models generate bad to moderate quality summaries while\nGPT-3.5, as a zero-shot summarizer, provides moderate to good quality outputs.\nGPT-3.5 also seems very adept at normalizing historical text. To assess data\ncontamination in GPT-3.5, we design an adversarial attack scheme in which we\nfind that GPT-3.5 performs slightly worse for unseen source documents compared\nto seen documents. Moreover, it sometimes hallucinates when the source\nsentences are inverted against its prior knowledge with a summarization\naccuracy of 0.67 for plot omission, 0.71 for entity swap, and 0.53 for plot\nnegation. Overall, our regression results of model performances suggest that\nlonger, older, and more complex source texts (all of which are more\ncharacteristic for historical language variants) are harder to summarize for\nall models, indicating the difficulty of the CLCTS task.", "published": "2023-06-22 14:31:18", "link": "http://arxiv.org/abs/2306.12916v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Public Attitudes Toward ChatGPT on Twitter: Sentiments, Topics, and\n  Occupations", "abstract": "ChatGPT sets a new record with the fastest-growing user base, as a chatbot\npowered by a large language model (LLM). While it demonstrates state-of-the-art\ncapabilities in a variety of language-generation tasks, it also raises\nwidespread public concerns regarding its societal impact. In this paper, we\ninvestigated public attitudes towards ChatGPT by applying natural language\nprocessing techniques such as sentiment analysis and topic modeling to Twitter\ndata from December 5, 2022 to June 10, 2023. Our sentiment analysis result\nindicates that the overall sentiment was largely neutral to positive, and\nnegative sentiments were decreasing over time. Our topic model reveals that the\nmost popular topics discussed were Education, Bard, Search Engines, OpenAI,\nMarketing, and Cybersecurity, but the ranking varies by month. We also analyzed\nthe occupations of Twitter users and found that those with occupations in arts\nand entertainment tweeted aboutChatGPT most frequently. Additionally, people\ntended to tweet about topics relevant to their occupation. For instance,\nCybersecurity is the most discussed topic among those with occupations related\nto computer and math, and Education is the most discussed topic among those in\nacademic and research. Overall, our exploratory study provides insights into\nthe public perception of ChatGPT, which could be valuable to both the general\npublic and developers of this technology.", "published": "2023-06-22 15:10:18", "link": "http://arxiv.org/abs/2306.12951v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conversation Derailment Forecasting with Graph Convolutional Networks", "abstract": "Online conversations are particularly susceptible to derailment, which can\nmanifest itself in the form of toxic communication patterns like disrespectful\ncomments or verbal abuse. Forecasting conversation derailment predicts signs of\nderailment in advance enabling proactive moderation of conversations. Current\nstate-of-the-art approaches to address this problem rely on sequence models\nthat treat dialogues as text streams. We propose a novel model based on a graph\nconvolutional neural network that considers dialogue user dynamics and the\ninfluence of public perception on conversation utterances. Through empirical\nevaluation, we show that our model effectively captures conversation dynamics\nand outperforms the state-of-the-art models on the CGA and CMV benchmark\ndatasets by 1.5\\% and 1.7\\%, respectively.", "published": "2023-06-22 15:40:59", "link": "http://arxiv.org/abs/2306.12982v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speech Emotion Diarization: Which Emotion Appears When?", "abstract": "Speech Emotion Recognition (SER) typically relies on utterance-level\nsolutions. However, emotions conveyed through speech should be considered as\ndiscrete speech events with definite temporal boundaries, rather than\nattributes of the entire utterance. To reflect the fine-grained nature of\nspeech emotions, we propose a new task: Speech Emotion Diarization (SED). Just\nas Speaker Diarization answers the question of \"Who speaks when?\", Speech\nEmotion Diarization answers the question of \"Which emotion appears when?\". To\nfacilitate the evaluation of the performance and establish a common benchmark\nfor researchers, we introduce the Zaion Emotion Dataset (ZED), an openly\naccessible speech emotion dataset that includes non-acted emotions recorded in\nreal-life conditions, along with manually-annotated boundaries of emotion\nsegments within the utterance. We provide competitive baselines and open-source\nthe code and the pre-trained models.", "published": "2023-06-22 15:47:36", "link": "http://arxiv.org/abs/2306.12991v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysis of the Cambridge Multiple-Choice Questions Reading Dataset with\n  a Focus on Candidate Response Distribution", "abstract": "Multiple choice exams are widely used to assess candidates across a diverse\nrange of domains and tasks. To moderate question quality, newly proposed\nquestions often pass through pre-test evaluation stages before being deployed\ninto real-world exams. Currently, this evaluation process is manually\nintensive, which can lead to time lags in the question development cycle.\nStreamlining this process via automation can significantly enhance efficiency,\nhowever, there's a current lack of datasets with adequate pre-test analysis\ninformation. In this paper we analyse a subset of the public Cambridge\nMultiple-Choice Questions Reading Database released by Cambridge University\nPress & Assessment; a multiple-choice comprehension dataset of questions at\ndifferent target levels, with corresponding candidate selection distributions.\nWe introduce the task of candidate distribution matching, propose several\nevaluation metrics for the task, and demonstrate that automatic systems trained\non RACE++ can be leveraged as baselines for our task. We further demonstrate\nthat these automatic systems can be used for practical pre-test evaluation\ntasks such as detecting underperforming distractors, where our detection\nsystems can automatically identify poor distractors that few candidates select.", "published": "2023-06-22 17:13:08", "link": "http://arxiv.org/abs/2306.13047v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Named entity recognition in resumes", "abstract": "Named entity recognition (NER) is used to extract information from various\ndocuments and texts such as names and dates. It is important to extract\neducation and work experience information from resumes in order to filter them.\nConsidering the fact that all information in a resume has to be entered to the\ncompanys system manually, automatizing this process will save time of the\ncompanies. In this study, a deep learning-based semi-automatic named entity\nrecognition system has been implemented with a focus on resumes in the field of\nIT. Firstly, resumes of employees from five different IT related fields has\nbeen annotated. Six transformer based pre-trained models have been adapted to\nnamed entity recognition problem using the annotated data. These models have\nbeen selected among popular models in the natural language processing field.\nThe obtained system can recognize eight different entity types which are city,\ndate, degree, diploma major, job title, language, country and skill. Models\nused in the experiments are compared using micro, macro and weighted F1 scores\nand the performance of the methods was evaluated. Taking these scores into\naccount for test set the best micro and weighted F1 score is obtained by\nRoBERTa and the best macro F1 score is obtained by Electra model.", "published": "2023-06-22 17:30:37", "link": "http://arxiv.org/abs/2306.13062v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of\n  Confidence Elicitation in LLMs", "abstract": "Empowering large language models to accurately express confidence in their\nanswers is essential for trustworthy decision-making. Previous confidence\nelicitation methods, which primarily rely on white-box access to internal model\ninformation or model fine-tuning, have become less suitable for LLMs,\nespecially closed-source commercial APIs. This leads to a growing need to\nexplore the untapped area of black-box approaches for LLM uncertainty\nestimation. To better break down the problem, we define a systematic framework\nwith three components: prompting strategies for eliciting verbalized\nconfidence, sampling methods for generating multiple responses, and aggregation\ntechniques for computing consistency. We then benchmark these methods on two\nkey tasks-confidence calibration and failure prediction-across five types of\ndatasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs\nincluding GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights:\n1) LLMs, when verbalizing their confidence, tend to be overconfident,\npotentially imitating human patterns of expressing confidence. 2) As model\ncapability scales up, both calibration and failure prediction performance\nimprove. 3) Employing our proposed strategies, such as human-inspired prompts,\nconsistency among multiple responses, and better aggregation strategies can\nhelp mitigate this overconfidence from various perspectives. 4) Comparisons\nwith white-box methods indicate that while white-box methods perform better,\nthe gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements,\nnone of these techniques consistently outperform others, and all investigated\nmethods struggle in challenging tasks, such as those requiring professional\nknowledge, indicating significant scope for improvement. We believe this study\ncan serve as a strong baseline and provide insights for eliciting confidence in\nblack-box LLMs.", "published": "2023-06-22 17:31:44", "link": "http://arxiv.org/abs/2306.13063v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Noor-Ghateh: A Benchmark Dataset for Evaluating Arabic Word Segmenters\n  in Hadith Domain", "abstract": "There are numerous complex and rich morphological features in the Arabic\nlanguage, which are highly useful when analyzing traditional Arabic textbooks,\nespecially in the literary and religious contexts, and help in understanding\nthe meaning of the textbooks. Vocabulary separation means separating the word\ninto different components, such as the root and affixes. In the morphological\ndatasets, the variety of markers and the number of data samples help to\nevaluate the morphological techniques. In this paper, we present a standard\ndataset for analyzing the Arabic segmentation tools, which includes\napproximately 223,690 words from the \"Shariat al-Islam\" book, labeled by human\nexperts. In terms of volume and word variety, this dataset is superior to the\nother Hadith Arabic datasets, to the best of our knowledge. To estimate the\ndataset, we applied different methods, including Farasa, Camel, and ALP, and\nreported the annotation quality and analyzed the benchmark specifications as\nwell. This be", "published": "2023-06-22 16:50:40", "link": "http://arxiv.org/abs/2307.09630v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying and Extracting Rare Disease Phenotypes with Large Language\n  Models", "abstract": "Rare diseases (RDs) are collectively common and affect 300 million people\nworldwide. Accurate phenotyping is critical for informing diagnosis and\ntreatment, but RD phenotypes are often embedded in unstructured text and\ntime-consuming to extract manually. While natural language processing (NLP)\nmodels can perform named entity recognition (NER) to automate extraction, a\nmajor bottleneck is the development of a large, annotated corpus for model\ntraining. Recently, prompt learning emerged as an NLP paradigm that can lead to\nmore generalizable results without any (zero-shot) or few labeled samples\n(few-shot). Despite growing interest in ChatGPT, a revolutionary large language\nmodel capable of following complex human prompts and generating high-quality\nresponses, none have studied its NER performance for RDs in the zero- and\nfew-shot settings. To this end, we engineered novel prompts aimed at extracting\nRD phenotypes and, to the best of our knowledge, are the first the establish a\nbenchmark for evaluating ChatGPT's performance in these settings. We compared\nits performance to the traditional fine-tuning approach and conducted an\nin-depth error analysis. Overall, fine-tuning BioClinicalBERT resulted in\nhigher performance (F1 of 0.689) than ChatGPT (F1 of 0.472 and 0.591 in the\nzero- and few-shot settings, respectively). Despite this, ChatGPT achieved\nsimilar or higher accuracy for certain entities (i.e., rare diseases and signs)\nin the one-shot setting (F1 of 0.776 and 0.725). This suggests that with\nappropriate prompt engineering, ChatGPT has the potential to match or\noutperform fine-tuned language models for certain entity types with just one\nlabeled sample. While the proliferation of large language models may provide\nopportunities for supporting RD diagnosis and treatment, researchers and\nclinicians should critically evaluate model outputs and be well-informed of\ntheir limitations.", "published": "2023-06-22 03:52:12", "link": "http://arxiv.org/abs/2306.12656v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Natural Language Processing in Electronic Health Records in Relation to\n  Healthcare Decision-making: A Systematic Review", "abstract": "Background: Natural Language Processing (NLP) is widely used to extract\nclinical insights from Electronic Health Records (EHRs). However, the lack of\nannotated data, automated tools, and other challenges hinder the full\nutilisation of NLP for EHRs. Various Machine Learning (ML), Deep Learning (DL)\nand NLP techniques are studied and compared to understand the limitations and\nopportunities in this space comprehensively.\n  Methodology: After screening 261 articles from 11 databases, we included 127\npapers for full-text review covering seven categories of articles: 1) medical\nnote classification, 2) clinical entity recognition, 3) text summarisation, 4)\ndeep learning (DL) and transfer learning architecture, 5) information\nextraction, 6) Medical language translation and 7) other NLP applications. This\nstudy follows the Preferred Reporting Items for Systematic Reviews and\nMeta-Analyses (PRISMA) guidelines.\n  Result and Discussion: EHR was the most commonly used data type among the\nselected articles, and the datasets were primarily unstructured. Various ML and\nDL methods were used, with prediction or classification being the most common\napplication of ML or DL. The most common use cases were: the International\nClassification of Diseases, Ninth Revision (ICD-9) classification, clinical\nnote analysis, and named entity recognition (NER) for clinical descriptions and\nresearch on psychiatric disorders.\n  Conclusion: We find that the adopted ML models were not adequately assessed.\nIn addition, the data imbalance problem is quite important, yet we must find\ntechniques to address this underlining problem. Future studies should address\nkey limitations in studies, primarily identifying Lupus Nephritis, Suicide\nAttempts, perinatal self-harmed and ICD-9 classification.", "published": "2023-06-22 12:10:41", "link": "http://arxiv.org/abs/2306.12834v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Unveiling Global Narratives: A Multilingual Twitter Dataset of News\n  Media on the Russo-Ukrainian Conflict", "abstract": "The ongoing Russo-Ukrainian conflict has been a subject of intense media\ncoverage worldwide. Understanding the global narrative surrounding this topic\nis crucial for researchers that aim to gain insights into its multifaceted\ndimensions. In this paper, we present a novel multimedia dataset that focuses\non this topic by collecting and processing tweets posted by news or media\ncompanies on social media across the globe. We collected tweets from February\n2022 to May 2023 to acquire approximately 1.5 million tweets in 60 different\nlanguages along with their images. Each entry in the dataset is accompanied by\nprocessed tags, allowing for the identification of entities, stances, textual\nor visual concepts, and sentiment. The availability of this multimedia dataset\nserves as a valuable resource for researchers aiming to investigate the global\nnarrative surrounding the ongoing conflict from various aspects such as who are\nthe prominent entities involved, what stances are taken, where do these stances\noriginate from, how are the different textual and visual concepts related to\nthe event portrayed.", "published": "2023-06-22 13:52:31", "link": "http://arxiv.org/abs/2306.12886v2", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Semi-automated extraction of research topics and trends from NCI funding\n  in radiological sciences from 2000-2020", "abstract": "Investigators, funders, and the public desire knowledge on topics and trends\nin publicly funded research but current efforts in manual categorization are\nlimited in scale and understanding. We developed a semi-automated approach to\nextract and name research topics, and applied this to \\$1.9B of NCI funding\nover 21 years in the radiological sciences to determine micro- and macro-scale\nresearch topics and funding trends. Our method relies on sequential clustering\nof existing biomedical-based word embeddings, naming using subject matter\nexperts, and visualization to discover trends at a macroscopic scale above\nindividual topics. We present results using 15 and 60 cluster topics, where we\nfound that 2D projection of grant embeddings reveals two dominant axes:\nphysics-biology and therapeutic-diagnostic. For our dataset, we found that\nfunding for therapeutics- and physics-based research have outpaced diagnostics-\nand biology-based research, respectively. We hope these results may (1) give\ninsight to funders on the appropriateness of their funding allocation, (2)\nassist investigators in contextualizing their work and explore neighboring\nresearch domains, and (3) allow the public to review where their tax dollars\nare being allocated.", "published": "2023-06-22 17:47:42", "link": "http://arxiv.org/abs/2306.13075v1", "categories": ["cs.CL", "cs.CV", "68T50 (Primary), 68T10 (Secondary)", "I.2.7; I.5.3; J.3"], "primary_category": "cs.CL"}
{"title": "Prompt to GPT-3: Step-by-Step Thinking Instructions for Humor Generation", "abstract": "Artificial intelligence has made significant progress in natural language\nprocessing, with models like GPT-3 demonstrating impressive capabilities.\nHowever, these models still have limitations when it comes to complex tasks\nthat require an understanding of the user, such as mastering human comedy\nwriting strategies. This paper explores humor generation using GPT-3 by\nmodeling human comedy writing theory and leveraging step-by-step thinking\ninstructions. In addition, we explore the role of cognitive distance in\ncreating humor.", "published": "2023-06-22 20:38:52", "link": "http://arxiv.org/abs/2306.13195v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DiversiGATE: A Comprehensive Framework for Reliable Large Language\n  Models", "abstract": "In this paper, we introduce DiversiGATE, a unified framework that\nconsolidates diverse methodologies for LLM verification. The proposed framework\ncomprises two main components: Diversification and Aggregation which provide a\nholistic perspective on existing verification approaches, such as\nSelf-Consistency, Math Prompter and WebGPT. Furthermore, we propose a novel\n`SelfLearner' model that conforms to the DiversiGATE framework which can learn\nfrom its own outputs and refine its performance over time, leading to improved\naccuracy. To evaluate the effectiveness of SelfLearner, we conducted a rigorous\nseries of experiments, including tests on synthetic data as well as on popular\narithmetic reasoning benchmarks such as GSM8K. Our results demonstrate that our\napproach outperforms traditional LLMs, achieving a considerable 54.8% -> 61.8%\nimprovement on the GSM8K benchmark.", "published": "2023-06-22 22:29:40", "link": "http://arxiv.org/abs/2306.13230v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Class-Incremental Learning based on Label Generation", "abstract": "Despite the great success of pre-trained language models, it is still a\nchallenge to use these models for continual learning, especially for the\nclass-incremental learning (CIL) setting due to catastrophic forgetting (CF).\nThis paper reports our finding that if we formulate CIL as a continual label\ngeneration problem, CF is drastically reduced and the generalizable\nrepresentations of pre-trained models can be better retained. We thus propose a\nnew CIL method (VAG) that also leverages the sparsity of vocabulary to focus\nthe generation and creates pseudo-replay samples by using label semantics.\nExperimental results show that VAG outperforms baselines by a large margin.", "published": "2023-06-22 01:14:47", "link": "http://arxiv.org/abs/2306.12619v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of\n  General-Purpose Large Language Models", "abstract": "Sentiment analysis is a vital tool for uncovering insights from financial\narticles, news, and social media, shaping our understanding of market\nmovements. Despite the impressive capabilities of large language models (LLMs)\nin financial natural language processing (NLP), they still struggle with\naccurately interpreting numerical values and grasping financial context,\nlimiting their effectiveness in predicting financial sentiment. In this paper,\nwe introduce a simple yet effective instruction tuning approach to address\nthese issues. By transforming a small portion of supervised financial sentiment\nanalysis data into instruction data and fine-tuning a general-purpose LLM with\nthis method, we achieve remarkable advancements in financial sentiment\nanalysis. In the experiment, our approach outperforms state-of-the-art\nsupervised sentiment analysis models, as well as widely used LLMs like ChatGPT\nand LLaMAs, particularly in scenarios where numerical understanding and\ncontextual comprehension are vital.", "published": "2023-06-22 03:56:38", "link": "http://arxiv.org/abs/2306.12659v1", "categories": ["cs.CL", "cs.LG", "q-fin.ST", "q-fin.TR"], "primary_category": "cs.CL"}
{"title": "From Word Models to World Models: Translating from Natural Language to\n  the Probabilistic Language of Thought", "abstract": "How does language inform our downstream thinking? In particular, how do\nhumans make meaning from language--and how can we leverage a theory of\nlinguistic meaning to build machines that think in more human-like ways? In\nthis paper, we propose rational meaning construction, a computational framework\nfor language-informed thinking that combines neural language models with\nprobabilistic models for rational inference. We frame linguistic meaning as a\ncontext-sensitive mapping from natural language into a probabilistic language\nof thought (PLoT)--a general-purpose symbolic substrate for generative world\nmodeling. Our architecture integrates two computational tools that have not\npreviously come together: we model thinking with probabilistic programs, an\nexpressive representation for commonsense reasoning; and we model meaning\nconstruction with large language models (LLMs), which support broad-coverage\ntranslation from natural language utterances to code expressions in a\nprobabilistic programming language. We illustrate our framework through\nexamples covering four core domains from cognitive science: probabilistic\nreasoning, logical and relational reasoning, visual and physical reasoning, and\nsocial reasoning. In each, we show that LLMs can generate context-sensitive\ntranslations that capture pragmatically-appropriate linguistic meanings, while\nBayesian inference with the generated programs supports coherent and robust\ncommonsense reasoning. We extend our framework to integrate\ncognitively-motivated symbolic modules (physics simulators, graphics engines,\nand planning algorithms) to provide a unified commonsense thinking interface\nfrom language. Finally, we explore how language can drive the construction of\nworld models themselves. We hope this work will provide a roadmap towards\ncognitive models and AI systems that synthesize the insights of both modern and\nclassical computational perspectives.", "published": "2023-06-22 05:14:00", "link": "http://arxiv.org/abs/2306.12672v2", "categories": ["cs.CL", "cs.AI", "cs.SC"], "primary_category": "cs.CL"}
{"title": "Vec2Vec: A Compact Neural Network Approach for Transforming Text\n  Embeddings with High Fidelity", "abstract": "Vector embeddings have become ubiquitous tools for many language-related\ntasks. A leading embedding model is OpenAI's text-ada-002 which can embed\napproximately 6,000 words into a 1,536-dimensional vector. While powerful,\ntext-ada-002 is not open source and is only available via API. We trained a\nsimple neural network to convert open-source 768-dimensional MPNet embeddings\ninto text-ada-002 embeddings. We compiled a subset of 50,000 online food\nreviews. We calculated MPNet and text-ada-002 embeddings for each review and\ntrained a simple neural network to for 75 epochs. The neural network was\ndesigned to predict the corresponding text-ada-002 embedding for a given MPNET\nembedding. Our model achieved an average cosine similarity of 0.932 on 10,000\nunseen reviews in our held-out test dataset. We manually assessed the quality\nof our predicted embeddings for vector search over text-ada-002-embedded\nreviews. While not as good as real text-ada-002 embeddings, predicted\nembeddings were able to retrieve highly relevant reviews. Our final model,\nVec2Vec, is lightweight (<80 MB) and fast. Future steps include training a\nneural network with a more sophisticated architecture and a larger dataset of\npaired embeddings to achieve greater performance. The ability to convert\nbetween and align embedding spaces may be helpful for interoperability,\nlimiting dependence on proprietary models, protecting data privacy, reducing\ncosts, and offline operations.", "published": "2023-06-22 06:23:31", "link": "http://arxiv.org/abs/2306.12689v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "I.2.7; D.2.12"], "primary_category": "cs.CL"}
{"title": "On the Robustness of Generative Retrieval Models: An Out-of-Distribution\n  Perspective", "abstract": "Recently, we have witnessed generative retrieval increasingly gaining\nattention in the information retrieval (IR) field, which retrieves documents by\ndirectly generating their identifiers. So far, much effort has been devoted to\ndeveloping effective generative retrieval models. There has been less attention\npaid to the robustness perspective. When a new retrieval paradigm enters into\nthe real-world application, it is also critical to measure the\nout-of-distribution (OOD) generalization, i.e., how would generative retrieval\nmodels generalize to new distributions. To answer this question, firstly, we\ndefine OOD robustness from three perspectives in retrieval problems: 1) The\nquery variations; 2) The unforeseen query types; and 3) The unforeseen tasks.\nBased on this taxonomy, we conduct empirical studies to analyze the OOD\nrobustness of several representative generative retrieval models against dense\nretrieval models. The empirical results indicate that the OOD robustness of\ngenerative retrieval models requires enhancement. We hope studying the OOD\nrobustness of generative retrieval models would be advantageous to the IR\ncommunity.", "published": "2023-06-22 09:18:52", "link": "http://arxiv.org/abs/2306.12756v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Implicit spoken language diarization", "abstract": "Spoken language diarization (LD) and related tasks are mostly explored using\nthe phonotactic approach. Phonotactic approaches mostly use explicit way of\nlanguage modeling, hence requiring intermediate phoneme modeling and\ntranscribed data. Alternatively, the ability of deep learning approaches to\nmodel temporal dynamics may help for the implicit modeling of language\ninformation through deep embedding vectors. Hence this work initially explores\nthe available speaker diarization frameworks that capture speaker information\nimplicitly to perform LD tasks. The performance of the LD system on synthetic\ncode-switch data using the end-to-end x-vector approach is 6.78% and 7.06%, and\nfor practical data is 22.50% and 60.38%, in terms of diarization error rate and\nJaccard error rate (JER), respectively. The performance degradation is due to\nthe data imbalance and resolved to some extent by using pre-trained wave2vec\nembeddings that provide a relative improvement of 30.74% in terms of JER.", "published": "2023-06-22 14:29:53", "link": "http://arxiv.org/abs/2306.12913v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Quantizable Transformers: Removing Outliers by Helping Attention Heads\n  Do Nothing", "abstract": "Transformer models have been widely adopted in various domains over the last\nyears, and especially large language models have advanced the field of AI\nsignificantly. Due to their size, the capability of these networks has\nincreased tremendously, but this has come at the cost of a significant increase\nin necessary compute. Quantization is one of the most effective ways to reduce\nthe computational time and memory consumption of neural networks. Many studies\nhave shown, however, that modern transformer models tend to learn strong\noutliers in their activations, making them difficult to quantize. To retain\nacceptable performance, the existence of these outliers requires activations to\nbe in higher bitwidth or the use of different numeric formats, extra\nfine-tuning, or other workarounds. We show that strong outliers are related to\nvery specific behavior of attention heads that try to learn a \"no-op\" or just a\npartial update of the residual. To achieve the exact zeros needed in the\nattention matrix for a no-update, the input to the softmax is pushed to be\nlarger and larger during training, causing outliers in other parts of the\nnetwork. Based on these observations, we propose two simple (independent)\nmodifications to the attention mechanism - clipped softmax and gated attention.\nWe empirically show that models pre-trained using our methods learn\nsignificantly smaller outliers while maintaining and sometimes even improving\nthe floating-point task performance. This enables us to quantize transformers\nto full INT8 quantization of the activations without any additional effort. We\ndemonstrate the effectiveness of our methods on both language models (BERT,\nOPT) and vision transformers.", "published": "2023-06-22 14:39:04", "link": "http://arxiv.org/abs/2306.12929v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Apolitical Intelligence? Auditing Delphi's responses on controversial\n  political issues in the US", "abstract": "As generative language models are deployed in ever-wider contexts, concerns\nabout their political values have come to the forefront with critique from all\nparts of the political spectrum that the models are biased and lack neutrality.\nHowever, the question of what neutrality is and whether it is desirable remains\nunderexplored. In this paper, I examine neutrality through an audit of Delphi\n[arXiv:2110.07574], a large language model designed for crowdsourced ethics. I\nanalyse how Delphi responds to politically controversial questions compared to\ndifferent US political subgroups. I find that Delphi is poorly calibrated with\nrespect to confidence and exhibits a significant political skew. Based on these\nresults, I examine the question of neutrality from a data-feminist lens, in\nterms of how notions of neutrality shift power and further marginalise unheard\nvoices. These findings can hopefully contribute to a more reflexive debate\nabout the normative questions of alignment and what role we want generative\nmodels to play in society.", "published": "2023-06-22 15:56:50", "link": "http://arxiv.org/abs/2306.13000v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Towards Explainable Evaluation Metrics for Machine Translation", "abstract": "Unlike classical lexical overlap metrics such as BLEU, most current\nevaluation metrics for machine translation (for example, COMET or BERTScore)\nare based on black-box large language models. They often achieve strong\ncorrelations with human judgments, but recent research indicates that the\nlower-quality classical metrics remain dominant, one of the potential reasons\nbeing that their decision processes are more transparent. To foster more\nwidespread acceptance of novel high-quality metrics, explainability thus\nbecomes crucial. In this concept paper, we identify key properties as well as\nkey goals of explainable machine translation metrics and provide a\ncomprehensive synthesis of recent techniques, relating them to our established\ngoals and properties. In this context, we also discuss the latest\nstate-of-the-art approaches to explainable metrics based on generative models\nsuch as ChatGPT and GPT4. Finally, we contribute a vision of next-generation\napproaches, including natural language explanations. We hope that our work can\nhelp catalyze and guide future research on explainable evaluation metrics and,\nmediately, also contribute to better and more transparent machine translation\nsystems.", "published": "2023-06-22 17:07:57", "link": "http://arxiv.org/abs/2306.13041v2", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Visual Adversarial Examples Jailbreak Aligned Large Language Models", "abstract": "Recently, there has been a surge of interest in integrating vision into Large\nLanguage Models (LLMs), exemplified by Visual Language Models (VLMs) such as\nFlamingo and GPT-4. This paper sheds light on the security and safety\nimplications of this trend. First, we underscore that the continuous and\nhigh-dimensional nature of the visual input makes it a weak link against\nadversarial attacks, representing an expanded attack surface of\nvision-integrated LLMs. Second, we highlight that the versatility of LLMs also\npresents visual attackers with a wider array of achievable adversarial\nobjectives, extending the implications of security failures beyond mere\nmisclassification. As an illustration, we present a case study in which we\nexploit visual adversarial examples to circumvent the safety guardrail of\naligned LLMs with integrated vision. Intriguingly, we discover that a single\nvisual adversarial example can universally jailbreak an aligned LLM, compelling\nit to heed a wide range of harmful instructions that it otherwise would not)\nand generate harmful content that transcends the narrow scope of a `few-shot'\nderogatory corpus initially employed to optimize the adversarial example. Our\nstudy underscores the escalating adversarial risks associated with the pursuit\nof multimodality. Our findings also connect the long-studied adversarial\nvulnerabilities of neural networks to the nascent field of AI alignment. The\npresented attack suggests a fundamental adversarial challenge for AI alignment,\nespecially in light of the emerging trend toward multimodality in frontier\nfoundation models.", "published": "2023-06-22 22:13:03", "link": "http://arxiv.org/abs/2306.13213v2", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Predictive Patentomics: Forecasting Innovation Success and Valuation\n  with ChatGPT", "abstract": "Analysis of innovation has been fundamentally limited by conventional\napproaches to broad, structural variables. This paper pushes the boundaries,\ntaking an LLM approach to patent analysis with the groundbreaking ChatGPT\ntechnology. OpenAI's state-of-the-art textual embedding accesses complex\ninformation about the quality and impact of each invention to power deep\nlearning predictive models. The nuanced embedding drives a 24% incremental\nimprovement in R-squared predicting patent value and clearly isolates the worst\nand best applications. These models enable a revision of the contemporary\nKogan, Papanikolaou, Seru, and Stoffman (2017) valuation of patents by a median\ndeviation of 1.5 times, accounting for potential institutional predictions.\nFurthermore, the market fails to incorporate timely information about\napplications; a long-short portfolio based on predicted acceptance rates\nachieves significant abnormal returns of 3.3% annually. The models provide an\nopportunity to revolutionize startup and small-firm corporate policy vis-a-vis\npatenting.", "published": "2023-06-22 13:21:20", "link": "http://arxiv.org/abs/2307.01202v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DL"], "primary_category": "cs.LG"}
{"title": "AudioPaLM: A Large Language Model That Can Speak and Listen", "abstract": "We introduce AudioPaLM, a large language model for speech understanding and\ngeneration. AudioPaLM fuses text-based and speech-based language models, PaLM-2\n[Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified\nmultimodal architecture that can process and generate text and speech with\napplications including speech recognition and speech-to-speech translation.\nAudioPaLM inherits the capability to preserve paralinguistic information such\nas speaker identity and intonation from AudioLM and the linguistic knowledge\npresent only in text large language models such as PaLM-2. We demonstrate that\ninitializing AudioPaLM with the weights of a text-only large language model\nimproves speech processing, successfully leveraging the larger quantity of text\ntraining data used in pretraining to assist with the speech tasks. The\nresulting model significantly outperforms existing systems for speech\ntranslation tasks and has the ability to perform zero-shot speech-to-text\ntranslation for many languages for which input/target language combinations\nwere not seen in training. AudioPaLM also demonstrates features of audio\nlanguage models, such as transferring a voice across languages based on a short\nspoken prompt. We release examples of our method at\nhttps://google-research.github.io/seanet/audiopalm/examples", "published": "2023-06-22 14:37:54", "link": "http://arxiv.org/abs/2306.12925v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Russian assimilatory palatalization is incomplete neutralization", "abstract": "Incomplete neutralization refers to phonetic traces of underlying contrasts\nin phonologically neutralizing contexts. The present study examines one such\ncontext: Russian assimilatory palatalization in C+j sequences. Russian\ncontrasts plain and palatalized consonants, with the plain consonants having a\nsecondary articulation involving retraction of the tongue dorsum\n(velarization/uvularization). However, Russian also has stop-glide sequences\nthat form near-minimal pairs with palatalized stops. In the environment\npreceding palatal glides, the contrast between palatalized and plain consonants\nis neutralized, due to the palatalization of the plain stop (assimilatory\npalatalization). The purpose of the study is to explore whether the\nneutralization is complete. To do so, we conducted an electromagnetic\narticulography (EMA) experiment examining temporal coordination and the spatial\nposition of the tongue body in underlyingly palatalized consonants and those\nderived from assimilatory palatalization. Articulatory results from four native\nspeakers of Russian revealed that gestures in both conditions are coordinated\nas complex segments, i.e., they are palatalized consonants. However, there are\ndifferences across conditions consistent with the residual presence of a tongue\ndorsum retraction gesture in the plain obstruents. We conclude that\nneutralization of the plain-palatal contrast in Russian is incomplete;\nconsonants in the assimilatory palatalization condition exhibit inter-gestural\ncoordination characteristic of palatalized consonants along with residual\nevidence of an underlying tongue dorsum retraction (velarization/uvularization)\ngesture.", "published": "2023-06-22 10:43:38", "link": "http://arxiv.org/abs/2306.12789v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NoisyILRMA: Diffuse-Noise-Aware Independent Low-Rank Matrix Analysis for\n  Fast Blind Source Extraction", "abstract": "In this paper, we address the multichannel blind source extraction (BSE) of a\nsingle source in diffuse noise environments. To solve this problem even faster\nthan by fast multichannel nonnegative matrix factorization (FastMNMF) and its\nvariant, we propose a BSE method called NoisyILRMA, which is a modification of\nindependent low-rank matrix analysis (ILRMA) to account for diffuse noise.\nNoisyILRMA can achieve considerably fast BSE by incorporating an algorithm\ndeveloped for independent vector extraction. In addition, to improve the BSE\nperformance of NoisyILRMA, we propose a mechanism to switch the source model\nwith ILRMA-like nonnegative matrix factorization to a more expressive source\nmodel during optimization. In the experiment, we show that NoisyILRMA runs\nfaster than a FastMNMF algorithm while maintaining the BSE performance. We also\nconfirm that the switching mechanism improves the BSE performance of\nNoisyILRMA.", "published": "2023-06-22 11:35:49", "link": "http://arxiv.org/abs/2306.12820v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Toward Leveraging Pre-Trained Self-Supervised Frontends for Automatic\n  Singing Voice Understanding Tasks: Three Case Studies", "abstract": "Automatic singing voice understanding tasks, such as singer identification,\nsinging voice transcription, and singing technique classification, benefit from\ndata-driven approaches that utilize deep learning techniques. These approaches\nwork well even under the rich diversity of vocal and noisy samples owing to\ntheir representation ability. However, the limited availability of labeled data\nremains a significant obstacle to achieving satisfactory performance. In recent\nyears, self-supervised learning models (SSL models) have been trained using\nlarge amounts of unlabeled data in the field of speech processing and music\nclassification. By fine-tuning these models for the target tasks, comparable\nperformance to conventional supervised learning can be achieved with limited\ntraining data. Therefore, in this paper, we investigate the effectiveness of\nSSL models for various singing voice recognition tasks. We report the results\nof experiments comparing SSL models for three different tasks (i.e., singer\nidentification, singing voice transcription, and singing technique\nclassification) as initial exploration and aim to discuss these findings.\nExperimental results show that each SSL model achieves comparable performance\nand sometimes outperforms compared to state-of-the-art methods on each task. We\nalso conducted a layer-wise analysis to further understand the behavior of the\nSSL models.", "published": "2023-06-22 07:47:18", "link": "http://arxiv.org/abs/2306.12714v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MFCCGAN: A Novel MFCC-Based Speech Synthesizer Using Adversarial\n  Learning", "abstract": "In this paper, we introduce MFCCGAN as a novel speech synthesizer based on\nadversarial learning that adopts MFCCs as input and generates raw speech\nwaveforms. Benefiting the GAN model capabilities, it produces speech with\nhigher intelligibility than a rule-based MFCC-based speech synthesizer WORLD.\nWe evaluated the model based on a popular intrusive objective speech\nintelligibility measure (STOI) and quality (NISQA score). Experimental results\nshow that our proposed system outperforms Librosa MFCC- inversion (by an\nincrease of about 26% up to 53% in STOI and 16% up to 78% in NISQA score) and a\nrise of about 10% in intelligibility and about 4% in naturalness in comparison\nwith conventional rule-based vocoder WORLD that used in the CycleGAN-VC family.\nHowever, WORLD needs additional data like F0. Finally, using perceptual loss in\ndiscriminators based on STOI could improve the quality more. WebMUSHRA-based\nsubjective tests also show the quality of the proposed approach.", "published": "2023-06-22 10:29:24", "link": "http://arxiv.org/abs/2306.12785v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Wind Noise Reduction with a Diffusion-based Stochastic Regeneration\n  Model", "abstract": "In this paper we present a method for single-channel wind noise reduction\nusing our previously proposed diffusion-based stochastic regeneration model\ncombining predictive and generative modelling. We introduce a non-additive\nspeech in noise model to account for the non-linear deformation of the membrane\ncaused by the wind flow and possible clipping. We show that our stochastic\nregeneration model outperforms other neural-network-based wind noise reduction\nmethods as well as purely predictive and generative models, on a dataset using\nsimulated and real-recorded wind noise. We further show that the proposed\nmethod generalizes well by testing on an unseen dataset with real-recorded wind\nnoise. Audio samples, data generation scripts and code for the proposed methods\ncan be found online (https://uhh.de/inf-sp-storm-wind).", "published": "2023-06-22 13:25:57", "link": "http://arxiv.org/abs/2306.12867v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Siamese SIREN: Audio Compression with Implicit Neural Representations", "abstract": "Implicit Neural Representations (INRs) have emerged as a promising method for\nrepresenting diverse data modalities, including 3D shapes, images, and audio.\nWhile recent research has demonstrated successful applications of INRs in image\nand 3D shape compression, their potential for audio compression remains largely\nunexplored. Motivated by this, we present a preliminary investigation into the\nuse of INRs for audio compression. Our study introduces Siamese SIREN, a novel\napproach based on the popular SIREN architecture. Our experimental results\nindicate that Siamese SIREN achieves superior audio reconstruction fidelity\nwhile utilizing fewer network parameters compared to previous INR\narchitectures.", "published": "2023-06-22 15:16:06", "link": "http://arxiv.org/abs/2306.12957v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
