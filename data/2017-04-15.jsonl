{"title": "Neural Paraphrase Identification of Questions with Noisy Pretraining", "abstract": "We present a solution to the problem of paraphrase identification of\nquestions. We focus on a recent dataset of question pairs annotated with binary\nparaphrase labels and show that a variant of the decomposable attention model\n(Parikh et al., 2016) results in accurate performance on this task, while being\nfar simpler than many competing neural architectures. Furthermore, when the\nmodel is pretrained on a noisy dataset of automatically collected question\nparaphrases, it obtains the best reported performance on the dataset.", "published": "2017-04-15 02:09:31", "link": "http://arxiv.org/abs/1704.04565v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MUSE: Modularizing Unsupervised Sense Embeddings", "abstract": "This paper proposes to address the word sense ambiguity issue in an\nunsupervised manner, where word sense representations are learned along a word\nsense selection mechanism given contexts. Prior work focused on designing a\nsingle model to deliver both mechanisms, and thus suffered from either\ncoarse-grained representation learning or inefficient sense selection. The\nproposed modular approach, MUSE, implements flexible modules to optimize\ndistinct mechanisms, achieving the first purely sense-level representation\nlearning system with linear-time sense selection. We leverage reinforcement\nlearning to enable joint training on the proposed modules, and introduce\nvarious exploration techniques on sense selection for better robustness. The\nexperiments on benchmark data show that the proposed approach achieves the\nstate-of-the-art performance on synonym selection as well as on contextual word\nsimilarities in terms of MaxSimC.", "published": "2017-04-15 07:36:49", "link": "http://arxiv.org/abs/1704.04601v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph Convolutional Encoders for Syntax-aware Neural Machine Translation", "abstract": "We present a simple and effective approach to incorporating syntactic\nstructure into neural attention-based encoder-decoder models for machine\ntranslation. We rely on graph-convolutional networks (GCNs), a recent class of\nneural networks developed for modeling graph-structured data. Our GCNs use\npredicted syntactic dependency trees of source sentences to produce\nrepresentations of words (i.e. hidden states of the encoder) that are sensitive\nto their syntactic neighborhoods. GCNs take word representations as input and\nproduce word representations as output, so they can easily be incorporated as\nlayers into standard encoders (e.g., on top of bidirectional RNNs or\nconvolutional neural networks). We evaluate their effectiveness with\nEnglish-German and English-Czech translation experiments for different types of\nencoders and observe substantial improvements over their syntax-agnostic\nversions in all the considered setups.", "published": "2017-04-15 19:04:59", "link": "http://arxiv.org/abs/1704.04675v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Online Spatial Concept and Lexical Acquisition with Simultaneous\n  Localization and Mapping", "abstract": "In this paper, we propose an online learning algorithm based on a\nRao-Blackwellized particle filter for spatial concept acquisition and mapping.\nWe have proposed a nonparametric Bayesian spatial concept acquisition model\n(SpCoA). We propose a novel method (SpCoSLAM) integrating SpCoA and FastSLAM in\nthe theoretical framework of the Bayesian generative model. The proposed method\ncan simultaneously learn place categories and lexicons while incrementally\ngenerating an environmental map. Furthermore, the proposed method has scene\nimage features and a language model added to SpCoA. In the experiments, we\ntested online learning of spatial concepts and environmental maps in a novel\nenvironment of which the robot did not have a map. Then, we evaluated the\nresults of online learning of spatial concepts and lexical acquisition. The\nexperimental results demonstrated that the robot was able to more accurately\nlearn the relationships between words and the place in the environmental map\nincrementally by using the proposed method.", "published": "2017-04-15 17:18:11", "link": "http://arxiv.org/abs/1704.04664v2", "categories": ["cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.AI"}
{"title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations", "abstract": "We present RACE, a new dataset for benchmark evaluation of methods in the\nreading comprehension task. Collected from the English exams for middle and\nhigh school Chinese students in the age range between 12 to 18, RACE consists\nof near 28,000 passages and near 100,000 questions generated by human experts\n(English instructors), and covers a variety of topics which are carefully\ndesigned for evaluating the students' ability in understanding and reasoning.\nIn particular, the proportion of questions that requires reasoning is much\nlarger in RACE than that in other benchmark datasets for reading comprehension,\nand there is a significant gap between the performance of the state-of-the-art\nmodels (43%) and the ceiling human performance (95%). We hope this new dataset\ncan serve as a valuable resource for research and evaluation in machine\ncomprehension. The dataset is freely available at\nhttp://www.cs.cmu.edu/~glai1/data/race/ and the code is available at\nhttps://github.com/qizhex/RACE_AR_baselines.", "published": "2017-04-15 19:31:41", "link": "http://arxiv.org/abs/1704.04683v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
