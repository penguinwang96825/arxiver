{"title": "Machine Translation of Low-Resource Indo-European Languages", "abstract": "In this work, we investigate methods for the challenging task of translating\nbetween low-resource language pairs that exhibit some level of similarity. In\nparticular, we consider the utility of transfer learning for translating\nbetween several Indo-European low-resource languages from the Germanic and\nRomance language families. In particular, we build two main classes of\ntransfer-based systems to study how relatedness can benefit the translation\nperformance. The primary system fine-tunes a model pre-trained on a related\nlanguage pair and the contrastive system fine-tunes one pre-trained on an\nunrelated language pair. Our experiments show that although relatedness is not\nnecessary for transfer learning to work, it does benefit model performance.", "published": "2021-08-08 21:41:08", "link": "http://arxiv.org/abs/2108.03739v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Model Evaluation in Open-ended Text Generation", "abstract": "Although current state-of-the-art language models have achieved impressive\nresults in numerous natural language processing tasks, still they could not\nsolve the problem of producing repetitive, dull and sometimes inconsistent text\nin open-ended text generation. Studies often attribute this problem to the\nmaximum likelihood training objective, and propose alternative approaches by\nusing stochastic decoding methods or altering the training objective. However,\nthere is still a lack of consistent evaluation metrics to directly compare the\nefficacy of these solutions. In this work, we study different evaluation\nmetrics that have been proposed to evaluate quality, diversity and consistency\nof machine-generated text. From there, we propose a practical pipeline to\nevaluate language models in open-ended generation task, and research on how to\nimprove the model's performance in all dimensions by leveraging different\nauxiliary training objectives.", "published": "2021-08-08 06:16:02", "link": "http://arxiv.org/abs/2108.03578v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "#StayHome or #Marathon? Social Media Enhanced Pandemic Surveillance on\n  Spatial-temporal Dynamic Graphs", "abstract": "COVID-19 has caused lasting damage to almost every domain in public health,\nsociety, and economy. To monitor the pandemic trend, existing studies rely on\nthe aggregation of traditional statistical models and epidemic spread theory.\nIn other words, historical statistics of COVID-19, as well as the population\nmobility data, become the essential knowledge for monitoring the pandemic\ntrend. However, these solutions can barely provide precise prediction and\nsatisfactory explanations on the long-term disease surveillance while the\nubiquitous social media resources can be the key enabler for solving this\nproblem. For example, serious discussions may occur on social media before and\nafter some breaking events take place. These events, such as marathon and\nparade, may impact the spread of the virus. To take advantage of the social\nmedia data, we propose a novel framework, Social Media enhAnced pandemic\nsuRveillance Technique (SMART), which is composed of two modules: (i)\ninformation extraction module to construct heterogeneous knowledge graphs based\non the extracted events and relationships among them; (ii) time series\nprediction module to provide both short-term and long-term forecasts of the\nconfirmed cases and fatality at the state-level in the United States and to\ndiscover risk factors for COVID-19 interventions. Extensive experiments show\nthat our method largely outperforms the state-of-the-art baselines by 7.3% and\n7.4% in confirmed case/fatality prediction, respectively.", "published": "2021-08-08 15:46:05", "link": "http://arxiv.org/abs/2108.03670v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Leveraging Commonsense Knowledge on Classifying False News and\n  Determining Checkworthiness of Claims", "abstract": "Widespread and rapid dissemination of false news has made fact-checking an\nindispensable requirement. Given its time-consuming and labor-intensive nature,\nthe task calls for an automated support to meet the demand. In this paper, we\npropose to leverage commonsense knowledge for the tasks of false news\nclassification and check-worthy claim detection. Arguing that commonsense\nknowledge is a factor in human believability, we fine-tune the BERT language\nmodel with a commonsense question answering task and the aforementioned tasks\nin a multi-task learning environment. For predicting fine-grained false news\ntypes, we compare the proposed fine-tuned model's performance with the false\nnews classification models on a public dataset as well as a newly collected\ndataset. We compare the model's performance with the single-task BERT model and\na state-of-the-art check-worthy claim detection tool to evaluate the\ncheck-worthy claim detection. Our experimental analysis demonstrates that\ncommonsense knowledge can improve performance in both tasks.", "published": "2021-08-08 20:52:45", "link": "http://arxiv.org/abs/2108.03731v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficacy of BERT embeddings on predicting disaster from Twitter data", "abstract": "Social media like Twitter provide a common platform to share and communicate\npersonal experiences with other people. People often post their life\nexperiences, local news, and events on social media to inform others. Many\nrescue agencies monitor this type of data regularly to identify disasters and\nreduce the risk of lives. However, it is impossible for humans to manually\ncheck the mass amount of data and identify disasters in real-time. For this\npurpose, many research works have been proposed to present words in\nmachine-understandable representations and apply machine learning methods on\nthe word representations to identify the sentiment of a text. The previous\nresearch methods provide a single representation or embedding of a word from a\ngiven document. However, the recent advanced contextual embedding method (BERT)\nconstructs different vectors for the same word in different contexts. BERT\nembeddings have been successfully used in different natural language processing\n(NLP) tasks, yet there is no concrete analysis of how these representations are\nhelpful in disaster-type tweet analysis. In this research work, we explore the\nefficacy of BERT embeddings on predicting disaster from Twitter data and\ncompare these to traditional context-free word embedding methods (GloVe,\nSkip-gram, and FastText). We use both traditional machine learning methods and\ndeep learning methods for this purpose. We provide both quantitative and\nqualitative results for this study. The results show that the BERT embeddings\nhave the best results in disaster prediction task than the traditional word\nembeddings. Our codes are made freely accessible to the research community.", "published": "2021-08-08 17:44:29", "link": "http://arxiv.org/abs/2108.10698v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep Single Shot Musical Instrument Identification using Scalograms", "abstract": "Musical Instrument Identification has for long had a reputation of being one\nof the most ill-posed problems in the field of Musical Information\nRetrieval(MIR). Despite several robust attempts to solve the problem, a\ntimeline spanning over the last five odd decades, the problem remains an open\nconundrum. In this work, the authors take on a further complex version of the\ntraditional problem statement. They attempt to solve the problem with minimal\ndata available - one audio excerpt per class. We propose to use a convolutional\nSiamese network and a residual variant of the same to identify musical\ninstruments based on the corresponding scalograms of their audio excerpts. Our\nexperiments and corresponding results obtained on two publicly available\ndatasets validate the superiority of our algorithm by $\\approx$ 3\\% over the\nexisting synonymous algorithms in present-day literature.", "published": "2021-08-08 05:11:07", "link": "http://arxiv.org/abs/2108.03569v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "BeatNet: CRNN and Particle Filtering for Online Joint Beat Downbeat and\n  Meter Tracking", "abstract": "The online estimation of rhythmic information, such as beat positions,\ndownbeat positions, and meter, is critical for many real-time music\napplications. Musical rhythm comprises complex hierarchical relationships\nacross time, rendering its analysis intrinsically challenging and at times\nsubjective. Furthermore, systems which attempt to estimate rhythmic information\nin real-time must be causal and must produce estimates quickly and efficiently.\nIn this work, we introduce an online system for joint beat, downbeat, and meter\ntracking, which utilizes causal convolutional and recurrent layers, followed by\na pair of sequential Monte Carlo particle filters applied during inference. The\nproposed system does not need to be primed with a time signature in order to\nperform downbeat tracking, and is instead able to estimate meter and adjust the\npredictions over time. Additionally, we propose an information gate strategy to\nsignificantly decrease the computational cost of particle filtering during the\ninference step, making the system much faster than previous sampling-based\nmethods. Experiments on the GTZAN dataset, which is unseen during training,\nshow that the system outperforms various online beat and downbeat tracking\nsystems and achieves comparable performance to a baseline offline joint method.", "published": "2021-08-08 06:07:59", "link": "http://arxiv.org/abs/2108.03576v1", "categories": ["eess.AS", "cs.AI", "cs.IR", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
