{"title": "GNOME: Generating Negotiations through Open-Domain Mapping of Exchanges", "abstract": "Language Models have previously shown strong negotiation capabilities in\nclosed domains where the negotiation strategy prediction scope is constrained\nto a specific setup. In this paper, we first show that these models are not\ngeneralizable beyond their original training domain despite their wide-scale\npretraining. Following this, we propose an automated framework called GNOME,\nwhich processes existing human-annotated, closed-domain datasets using Large\nLanguage Models and produces synthetic open-domain dialogues for negotiation.\nGNOME improves the generalizability of negotiation systems while reducing the\nexpensive and subjective task of manual data curation. Through our experimental\nsetup, we create a benchmark comparing encoder and decoder models trained on\nexisting datasets against datasets created through GNOME. Our results show that\nmodels trained on our dataset not only perform better than previous state of\nthe art models on domain specific strategy prediction, but also generalize\nbetter to previously unseen domains.", "published": "2024-06-16 00:26:17", "link": "http://arxiv.org/abs/2406.10764v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained\n  Language Model for Knowledge Editing and Fine-tuning", "abstract": "Pre-trained language models, trained on large-scale corpora, demonstrate\nstrong generalizability across various NLP tasks. Fine-tuning these models for\nspecific tasks typically involves updating all parameters, which is\nresource-intensive. Parameter-efficient fine-tuning (PEFT) methods, such as the\npopular LoRA family, introduce low-rank matrices to learn only a few parameters\nefficiently. However, during inference, the product of these matrices updates\nall pre-trained parameters, complicating tasks like knowledge editing that\nrequire selective updates. We propose a novel PEFT method, which conducts\n\\textbf{r}ow and c\\textbf{o}lumn-wise spar\\textbf{se}\n\\textbf{lo}w-\\textbf{r}ank \\textbf{a}daptation (RoseLoRA), to address this\nchallenge. RoseLoRA identifies and updates only the most important parameters\nfor a specific task, maintaining efficiency while preserving other model\nknowledge. By adding a sparsity constraint on the product of low-rank matrices\nand converting it to row and column-wise sparsity, we ensure efficient and\nprecise model updates. Our theoretical analysis guarantees the lower bound of\nthe sparsity with respective to the matrix product. Extensive experiments on\nfive benchmarks across twenty datasets demonstrate that RoseLoRA outperforms\nbaselines in both general fine-tuning and knowledge editing tasks.", "published": "2024-06-16 02:08:49", "link": "http://arxiv.org/abs/2406.10777v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Understanding Jailbreak Attacks in LLMs: A Representation Space\n  Analysis", "abstract": "Large language models (LLMs) are susceptible to a type of attack known as\njailbreaking, which misleads LLMs to output harmful contents. Although there\nare diverse jailbreak attack strategies, there is no unified understanding on\nwhy some methods succeed and others fail. This paper explores the behavior of\nharmful and harmless prompts in the LLM's representation space to investigate\nthe intrinsic properties of successful jailbreak attacks. We hypothesize that\nsuccessful attacks share some similar properties: They are effective in moving\nthe representation of the harmful prompt towards the direction to the harmless\nprompts. We leverage hidden representations into the objective of existing\njailbreak attacks to move the attacks along the acceptance direction, and\nconduct experiments to validate the above hypothesis using the proposed\nobjective. We hope this study provides new insights into understanding how LLMs\nunderstand harmfulness information.", "published": "2024-06-16 03:38:48", "link": "http://arxiv.org/abs/2406.10794v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Evolution Fine-Tuning for Policy Optimization", "abstract": "The alignment of large language models (LLMs) is crucial not only for\nunlocking their potential in specific tasks but also for ensuring that\nresponses meet human expectations and adhere to safety and ethical principles.\nCurrent alignment methodologies face considerable challenges. For instance,\nsupervised fine-tuning (SFT) requires extensive, high-quality annotated\nsamples, while reinforcement learning from human feedback (RLHF) is complex and\noften unstable. In this paper, we introduce self-evolution fine-tuning (SEFT)\nfor policy optimization, with the aim of eliminating the need for annotated\nsamples while retaining the stability and efficiency of SFT. SEFT first trains\nan adaptive reviser to elevate low-quality responses while maintaining\nhigh-quality ones. The reviser then gradually guides the policy's optimization\nby fine-tuning it with enhanced responses. One of the prominent features of\nthis method is its ability to leverage unlimited amounts of unannotated data\nfor policy optimization through supervised fine-tuning. Our experiments on\nAlpacaEval 2.0 and MT-Bench demonstrate the effectiveness of SEFT. We also\nprovide a comprehensive analysis of its advantages over existing alignment\ntechniques.", "published": "2024-06-16 06:38:02", "link": "http://arxiv.org/abs/2406.10813v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Citation-Based Summarization of Landmark Judgments", "abstract": "Landmark judgments are of prime importance in the Common Law System because\nof their exceptional jurisprudence and frequent references in other judgments.\nIn this work, we leverage contextual references available in citing judgments\nto create an extractive summary of the target judgment. We evaluate the\nproposed algorithm on two datasets curated from the judgments of Indian Courts\nand find the results promising.", "published": "2024-06-16 07:27:16", "link": "http://arxiv.org/abs/2406.10824v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Survey of Scientific Large Language Models and Their\n  Applications in Scientific Discovery", "abstract": "In many scientific fields, large language models (LLMs) have revolutionized\nthe way text and other modalities of data (e.g., molecules and proteins) are\nhandled, achieving superior performance in various applications and augmenting\nthe scientific discovery process. Nevertheless, previous surveys on scientific\nLLMs often concentrate on one or two fields or a single modality. In this\npaper, we aim to provide a more holistic view of the research landscape by\nunveiling cross-field and cross-modal connections between scientific LLMs\nregarding their architectures and pre-training techniques. To this end, we\ncomprehensively survey over 260 scientific LLMs, discuss their commonalities\nand differences, as well as summarize pre-training datasets and evaluation\ntasks for each field and modality. Moreover, we investigate how LLMs have been\ndeployed to benefit scientific discovery. Resources related to this survey are\navailable at https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models.", "published": "2024-06-16 08:03:24", "link": "http://arxiv.org/abs/2406.10833v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leading Whitespaces of Language Models' Subword Vocabulary Pose a\n  Confound for Calculating Word Probabilities", "abstract": "Predictions of word-by-word conditional probabilities from Transformer-based\nlanguage models are often evaluated to model the incremental processing\ndifficulty of human readers. In this paper, we argue that there is a confound\nposed by the most common method of aggregating subword probabilities of such\nlanguage models into word probabilities. This is due to the fact that tokens in\nthe subword vocabulary of most language models have leading whitespaces and\ntherefore do not naturally define stop probabilities of words. We first prove\nthat this can result in distributions over word probabilities that sum to more\nthan one, thereby violating the axiom that $\\mathsf{P}(\\Omega) = 1$. This\nproperty results in a misallocation of word-by-word surprisal, where the\nunacceptability of the end of the current word is incorrectly carried over to\nthe next word. Additionally, this implicit prediction of word boundaries\nincorrectly models psycholinguistic experiments where human subjects directly\nobserve upcoming word boundaries. We present a simple decoding technique to\nreaccount the probability of the trailing whitespace into that of the current\nword, which resolves this confound. Experiments show that this correction\nreveals lower estimates of garden-path effects in transitive/intransitive\nsentences and poorer fits to naturalistic reading times.", "published": "2024-06-16 08:44:56", "link": "http://arxiv.org/abs/2406.10851v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Query-Relevant Neurons in Large Language Models for\n  Long-Form Texts", "abstract": "Large Language Models (LLMs) possess vast amounts of knowledge within their\nparameters, prompting research into methods for locating and editing this\nknowledge. Previous work has largely focused on locating entity-related (often\nsingle-token) facts in smaller models. However, several key questions remain\nunanswered: (1) How can we effectively locate query-relevant neurons in\ndecoder-only LLMs, such as Llama and Mistral? (2) How can we address the\nchallenge of long-form (or free-form) text generation? (3) Are there localized\nknowledge regions in LLMs? In this study, we introduce Query-Relevant Neuron\nCluster Attribution (QRNCA), a novel architecture-agnostic framework capable of\nidentifying query-relevant neurons in LLMs. QRNCA allows for the examination of\nlong-form answers beyond triplet facts by employing the proxy task of\nmulti-choice question answering. To evaluate the effectiveness of our detected\nneurons, we build two multi-choice QA datasets spanning diverse domains and\nlanguages. Empirical evaluations demonstrate that our method outperforms\nbaseline methods significantly. Further, analysis of neuron distributions\nreveals the presence of visible localized regions, particularly within\ndifferent domains. Finally, we show potential applications of our detected\nneurons in knowledge editing and neuron-based prediction.", "published": "2024-06-16 09:36:32", "link": "http://arxiv.org/abs/2406.10868v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COOL: Comprehensive Knowledge Enhanced Prompt Learning for Domain\n  Adaptive Few-shot Fake News Detection", "abstract": "Most Fake News Detection (FND) methods often struggle with data scarcity for\nemerging news domain. Recently, prompt learning based on Pre-trained Language\nModels (PLM) has emerged as a promising approach in domain adaptive few-shot\nlearning, since it greatly reduces the need for labeled data by bridging the\ngap between pre-training and downstream task. Furthermore, external knowledge\nis also helpful in verifying emerging news, as emerging news often involves\ntimely knowledge that may not be contained in the PLM's outdated prior\nknowledge. To this end, we propose COOL, a Comprehensive knOwledge enhanced\nprOmpt Learning method for domain adaptive few-shot FND. Specifically, we\npropose a comprehensive knowledge extraction module to extract both structured\nand unstructured knowledge that are positively or negatively correlated with\nnews from external sources, and adopt an adversarial contrastive enhanced\nhybrid prompt learning strategy to model the domain-invariant news-knowledge\ninteraction pattern for FND. Experimental results demonstrate the superiority\nof COOL over various state-of-the-arts.", "published": "2024-06-16 09:41:25", "link": "http://arxiv.org/abs/2406.10870v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Potential of Multimodal LLM with Knowledge-Intensive\n  Multimodal ASR", "abstract": "Recent advancements in multimodal large language models (MLLMs) have made\nsignificant progress in integrating information across various modalities, yet\nreal-world applications in educational and scientific domains remain\nchallenging. This paper introduces the Multimodal Scientific ASR (MS-ASR) task,\nwhich focuses on transcribing scientific conference videos by leveraging visual\ninformation from slides to enhance the accuracy of technical terminologies.\nRealized that traditional metrics like WER fall short in assessing performance\naccurately, prompting the proposal of severity-aware WER (SWER) that considers\nthe content type and severity of ASR errors. We propose the Scientific Vision\nAugmented ASR (SciVASR) framework as a baseline method, enabling MLLMs to\nimprove transcript quality through post-editing. Evaluations of\nstate-of-the-art MLLMs, including GPT-4o, show a 45% improvement over\nspeech-only baselines, highlighting the importance of multimodal information\nintegration.", "published": "2024-06-16 10:04:19", "link": "http://arxiv.org/abs/2406.10880v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Teaching Large Language Models to Express Knowledge Boundary from Their\n  Own Signals", "abstract": "Large language models (LLMs) have achieved great success, but their\noccasional content fabrication, or hallucination, limits their practical\napplication. Hallucination arises because LLMs struggle to admit ignorance due\nto inadequate training on knowledge boundaries. We call it a limitation of LLMs\nthat they can not accurately express their knowledge boundary, answering\nquestions they know while admitting ignorance to questions they do not know. In\nthis paper, we aim to teach LLMs to recognize and express their knowledge\nboundary, so they can reduce hallucinations caused by fabricating when they do\nnot know. We propose CoKE, which first probes LLMs' knowledge boundary via\ninternal confidence given a set of questions, and then leverages the probing\nresults to elicit the expression of the knowledge boundary. Extensive\nexperiments show CoKE helps LLMs express knowledge boundaries, answering known\nquestions while declining unknown ones, significantly improving in-domain and\nout-of-domain performance.", "published": "2024-06-16 10:07:20", "link": "http://arxiv.org/abs/2406.10881v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SCAR: Efficient Instruction-Tuning for Large Language Models via Style\n  Consistency-Aware Response Ranking", "abstract": "Recent studies emphasize that manually ensuring a consistent response style\nand maintaining high data quality in training sets can significantly improve\nthe performance of fine-tuned Large Language Models (LLMs) while reducing the\nnumber of training examples needed. However, the precise definition of style\nand the relationship between style, data quality, and LLM performance remains\nunclear. This research identifies two key stylistic elements in responses:\nlinguistic form and instructional surprisal. We find that, among training data\nof comparable quality, higher consistency in these response elements leads to\nbetter LLM performance. Inspired by this, we introduce Style Consistency-Aware\nResponse Ranking (SCAR), which automatically prioritizes instruction-response\npairs in the training set based on their response stylistic consistency. By\nselecting the most style-consistent examples, sometimes as few as 0.7% of the\nfull dataset, the fine-tuned LLMs can match or even surpass the performance of\nmodels trained on the entire dataset in coding and open-ended\nquestion-answering benchmarks. Code and data are available at\nhttps://github.com/zhuang-li/SCAR .", "published": "2024-06-16 10:10:37", "link": "http://arxiv.org/abs/2406.10882v7", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Role of Entity and Event Level Conceptualization in Generalizable\n  Reasoning: A Survey of Tasks, Methods, Applications, and Future Directions", "abstract": "Entity- and event-level conceptualization, as fundamental elements of human\ncognition, plays a pivotal role in generalizable reasoning. This process\ninvolves abstracting specific instances into higher-level concepts and forming\nabstract knowledge that can be applied in unfamiliar or novel situations, which\ncan enhance models' inferential capabilities and support the effective transfer\nof knowledge across various domains. Despite its significance, there is\ncurrently a lack of a systematic overview that comprehensively examines\nexisting works in the definition, execution, and application of\nconceptualization to enhance reasoning tasks. In this paper, we address this\ngap by presenting the first comprehensive survey of 150+ papers, categorizing\nvarious definitions, resources, methods, and downstream applications related to\nconceptualization into a unified taxonomy, with a focus on the entity and event\nlevels. Furthermore, we shed light on potential future directions in this field\nand hope to garner more attention from the community.", "published": "2024-06-16 10:32:41", "link": "http://arxiv.org/abs/2406.10885v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Logit Separability-Driven Samples and Multiple Class-Related Words\n  Selection for Advancing In-Context Learning", "abstract": "Effective organization of in-context learning (ICL) demonstrations is key to\nimproving the quality of large language model (LLM) responses. To create better\nsample-label pairs that instruct LLM understanding, we introduce logit\nseparability, a criterion to assess the clarity of both samples and\nclass-related words at the logit level. This facilitates the optimization of\nsample and label selection, enhancing the precision of information provided in\nICL demonstrations. Additionally, we find that incorporating multiple\nclass-related words for each sample, rather than relying on a single class\nname, improves performance by offering a broader range of label information.\nBuilding on these insights, we propose LICL, a logit separability-based method\nthat jointly organizes samples and integrates multiple class-related words into\neach sample-label pair. Evaluations across seven classification datasets show\nthat this approach significantly improves ICL performance by providing clearer\ninstructions and richer label information.", "published": "2024-06-16 12:11:46", "link": "http://arxiv.org/abs/2406.10908v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "E-Bench: Towards Evaluating the Ease-of-Use of Large Language Models", "abstract": "Most large language models (LLMs) are sensitive to prompts, and another\nsynonymous expression or a typo may lead to unexpected results for the model.\nComposing an optimal prompt for a specific demand lacks theoretical support and\nrelies entirely on human experimentation, which poses a considerable obstacle\nto popularizing generative artificial intelligence. However, there is no\nsystematic analysis of the stability of LLMs in resisting prompt perturbations\nin real-world scenarios. In this work, we propose to evaluate the ease-of-use\nof LLMs and construct E-Bench, simulating the actual situation of human use\nfrom synonymous perturbation (including paraphrasing, simplification, and\ncolloquialism) and typographical perturbation (such as typing). On this basis,\nwe also discuss the combination of these two types of perturbation and analyze\nthe main reasons for performance degradation. Experimental results indicate\nthat with the increase of model size, although the ease-of-use are\nsignificantly improved, there is still a long way to go to build a sufficiently\nuser-friendly model.", "published": "2024-06-16 14:08:30", "link": "http://arxiv.org/abs/2406.10950v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Avoiding Copyright Infringement via Large Language Model Unlearning", "abstract": "Pre-trained Large Language Models (LLMs) have demonstrated remarkable\ncapabilities but also pose risks by learning and generating copyrighted\nmaterial, leading to significant legal and ethical concerns. In real-world\nscenarios, model owners need to continuously address copyright infringement as\nnew requests for content removal emerge at different time points. This leads to\nthe need for sequential unlearning, where copyrighted content is removed\nsequentially as new requests arise. Despite its practical relevance, sequential\nunlearning in the context of copyright infringement has not been rigorously\nexplored in existing literature. To address this gap, we propose Stable\nSequential Unlearning (SSU), a novel framework designed to unlearn copyrighted\ncontent from LLMs over multiple time steps. Our approach works by identifying\nand removing specific weight updates in the model's parameters that correspond\nto copyrighted content. We improve unlearning efficacy by introducing random\nlabeling loss and ensuring the model retains its general-purpose knowledge by\nadjusting targeted parameters. Experimental results show that SSU achieves an\neffective trade-off between unlearning efficacy and general-purpose language\nabilities, outperforming existing baselines.", "published": "2024-06-16 14:12:37", "link": "http://arxiv.org/abs/2406.10952v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Eliminating Biased Length Reliance of Direct Preference Optimization via\n  Down-Sampled KL Divergence", "abstract": "Direct Preference Optimization (DPO) has emerged as a prominent algorithm for\nthe direct and robust alignment of Large Language Models (LLMs) with human\npreferences, offering a more straightforward alternative to the complex\nReinforcement Learning from Human Feedback (RLHF). Despite its promising\nefficacy, DPO faces a notable drawback: \"verbosity\", a common over-optimization\nphenomenon also observed in RLHF. While previous studies mainly attributed\nverbosity to biased labels within the data, we propose that the issue also\nstems from an inherent algorithmic length reliance in DPO. Specifically, we\nsuggest that the discrepancy between sequence-level Kullback-Leibler (KL)\ndivergences between chosen and rejected sequences, used in DPO, results in\noverestimated or underestimated rewards due to varying token lengths.\nEmpirically, we utilize datasets with different label lengths to demonstrate\nthe presence of biased rewards. We then introduce an effective downsampling\napproach, named SamPO, to eliminate potential length reliance. Our experimental\nevaluations, conducted across three LLMs of varying scales and a diverse array\nof conditional and open-ended benchmarks, highlight the efficacy of SamPO in\nmitigating verbosity, achieving improvements of 5% to 12% over DPO through\ndebaised rewards. Our codes can be accessed at:\nhttps://github.com/LuJunru/SamPO/.", "published": "2024-06-16 14:24:30", "link": "http://arxiv.org/abs/2406.10957v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ESCoT: Towards Interpretable Emotional Support Dialogue Systems", "abstract": "Understanding the reason for emotional support response is crucial for\nestablishing connections between users and emotional support dialogue systems.\nPrevious works mostly focus on generating better responses but ignore\ninterpretability, which is extremely important for constructing reliable\ndialogue systems. To empower the system with better interpretability, we\npropose an emotional support response generation scheme, named\n$\\textbf{E}$motion-Focused and $\\textbf{S}$trategy-Driven\n$\\textbf{C}$hain-$\\textbf{o}$f-$\\textbf{T}$hought ($\\textbf{ESCoT}$), mimicking\nthe process of $\\textit{identifying}$, $\\textit{understanding}$, and\n$\\textit{regulating}$ emotions. Specially, we construct a new dataset with\nESCoT in two steps: (1) $\\textit{Dialogue Generation}$ where we first generate\ndiverse conversation situations, then enhance dialogue generation using richer\nemotional support strategies based on these situations; (2) $\\textit{Chain\nSupplement}$ where we focus on supplementing selected dialogues with elements\nsuch as emotion, stimuli, appraisal, and strategy reason, forming the manually\nverified chains. Additionally, we further develop a model to generate dialogue\nresponses with better interpretability. We also conduct extensive experiments\nand human evaluations to validate the effectiveness of the proposed ESCoT and\ngenerated dialogue responses. Our data and code are available at\n$\\href{https://github.com/TeigenZhang/ESCoT}{https://github.com/TeigenZhang/ESCoT}$.", "published": "2024-06-16 14:37:17", "link": "http://arxiv.org/abs/2406.10960v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Cosine Similarity via Normalized ICA-transformed Embeddings", "abstract": "Cosine similarity is widely used to measure the similarity between two\nembeddings, while interpretations based on angle and correlation coefficient\nare common. In this study, we focus on the interpretable axes of embeddings\ntransformed by Independent Component Analysis (ICA), and propose a novel\ninterpretation of cosine similarity as the sum of semantic similarities over\naxes. The normalized ICA-transformed embeddings exhibit sparsity, enhancing the\ninterpretability of each axis, and the semantic similarity defined by the\nproduct of the components represents the shared meaning between the two\nembeddings along each axis. The effectiveness of this approach is demonstrated\nthrough intuitive numerical examples and thorough numerical experiments. By\nderiving the probability distributions that govern each component and the\nproduct of components, we propose a method for selecting statistically\nsignificant axes.", "published": "2024-06-16 15:44:37", "link": "http://arxiv.org/abs/2406.10984v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Taking a Deep Breath: Enhancing Language Modeling of Large Language\n  Models with Sentinel Tokens", "abstract": "Large language models (LLMs) have shown promising efficacy across various\ntasks, becoming powerful tools in numerous aspects of human life. However,\nTransformer-based LLMs suffer a performance degradation when modeling long-term\ncontexts due to they discard some information to reduce computational overhead.\nIn this work, we propose a simple yet effective method to enable LLMs to take a\ndeep breath, encouraging them to summarize information contained within\ndiscrete text chunks. Specifically, we segment the text into multiple chunks\nand insert special token <SR> at the end of each chunk. We then modify the\nattention mask to integrate the chunk's information into the corresponding <SR>\ntoken. This facilitates LLMs to interpret information not only from historical\nindividual tokens but also from the <SR> token, aggregating the chunk's\nsemantic information. Experiments on language modeling and out-of-domain\ndownstream tasks validate the superiority of our approach.", "published": "2024-06-16 15:50:10", "link": "http://arxiv.org/abs/2406.10985v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Query Rewriting: Aligning Rewriters through Marginal\n  Probability of Conversational Answers", "abstract": "Query rewriting is a crucial technique for passage retrieval in open-domain\nconversational question answering (CQA). It decontexualizes conversational\nqueries into self-contained questions suitable for off-the-shelf retrievers.\nExisting methods attempt to incorporate retriever's preference during the\ntraining of rewriting models. However, these approaches typically rely on\nextensive annotations such as in-domain rewrites and/or relevant passage\nlabels, limiting the models' generalization and adaptation capabilities. In\nthis paper, we introduce AdaQR ($\\textbf{Ada}$ptive $\\textbf{Q}$uery\n$\\textbf{R}$ewriting), a framework for training query rewriting models with\nlimited rewrite annotations from seed datasets and completely no passage label.\nOur approach begins by fine-tuning compact large language models using only\n~$10\\%$ of rewrite annotations from the seed dataset training split. The models\nare then utilized to generate rewrite candidates for each query instance. A\nnovel approach is then proposed to assess retriever's preference for these\ncandidates by the probability of answers conditioned on the conversational\nquery by marginalizing the Top-$K$ passages. This serves as the reward for\noptimizing the rewriter further using Direct Preference Optimization (DPO), a\nprocess free of rewrite and retrieval annotations. Experimental results on four\nopen-domain CQA datasets demonstrate that AdaQR not only enhances the in-domain\ncapabilities of the rewriter with limited annotation requirement, but also\nadapts effectively to out-of-domain datasets.", "published": "2024-06-16 16:09:05", "link": "http://arxiv.org/abs/2406.10991v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Lifelong Dialogue Agents via Timeline-based Memory Management", "abstract": "To achieve lifelong human-agent interaction, dialogue agents need to\nconstantly memorize perceived information and properly retrieve it for response\ngeneration (RG). While prior studies focus on getting rid of outdated memories\nto improve retrieval quality, we argue that such memories provide rich,\nimportant contextual cues for RG (e.g., changes in user behaviors) in long-term\nconversations. We present THEANINE, a framework for LLM-based lifelong dialogue\nagents. THEANINE discards memory removal and manages large-scale memories by\nlinking them based on their temporal and cause-effect relation. Enabled by this\nlinking structure, THEANINE augments RG with memory timelines - series of\nmemories representing the evolution or causality of relevant past events. Along\nwith THEANINE, we introduce TeaFarm, a counterfactual-driven evaluation scheme,\naddressing the limitation of G-Eval and human efforts when assessing agent\nperformance in integrating past memories into RG. A supplementary video for\nTHEANINE and data for TeaFarm are at\nhttps://huggingface.co/spaces/ResearcherScholar/Theanine.", "published": "2024-06-16 16:17:46", "link": "http://arxiv.org/abs/2406.10996v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RUPBench: Benchmarking Reasoning Under Perturbations for Robustness\n  Evaluation in Large Language Models", "abstract": "With the increasing use of large language models (LLMs), ensuring reliable\nperformance in diverse, real-world environments is essential. Despite their\nremarkable achievements, LLMs often struggle with adversarial inputs,\nsignificantly impacting their effectiveness in practical applications. To\nsystematically understand the robustness of LLMs, we present RUPBench, a\ncomprehensive benchmark designed to evaluate LLM robustness across diverse\nreasoning tasks. Our benchmark incorporates 15 reasoning datasets, categorized\ninto commonsense, arithmetic, logical, and knowledge-intensive reasoning, and\nintroduces nine types of textual perturbations at lexical, syntactic, and\nsemantic levels. By examining the performance of state-of-the-art LLMs such as\nGPT-4o, Llama3, Phi-3, and Gemma on both original and perturbed datasets, we\nprovide a detailed analysis of their robustness and error patterns. Our\nfindings highlight that larger models tend to exhibit greater robustness to\nperturbations. Additionally, common error types are identified through manual\ninspection, revealing specific challenges faced by LLMs in different reasoning\ncontexts. This work provides insights into areas where LLMs need further\nimprovement to handle diverse and noisy inputs effectively.", "published": "2024-06-16 17:26:44", "link": "http://arxiv.org/abs/2406.11020v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FoodieQA: A Multimodal Dataset for Fine-Grained Understanding of Chinese\n  Food Culture", "abstract": "Food is a rich and varied dimension of cultural heritage, crucial to both\nindividuals and social groups. To bridge the gap in the literature on the\noften-overlooked regional diversity in this domain, we introduce FoodieQA, a\nmanually curated, fine-grained image-text dataset capturing the intricate\nfeatures of food cultures across various regions in China. We evaluate\nvision-language Models (VLMs) and large language models (LLMs) on newly\ncollected, unseen food images and corresponding questions. FoodieQA comprises\nthree multiple-choice question-answering tasks where models need to answer\nquestions based on multiple images, a single image, and text-only descriptions,\nrespectively. While LLMs excel at text-based question answering, surpassing\nhuman accuracy, the open-sourced VLMs still fall short by 41% on multi-image\nand 21% on single-image VQA tasks, although closed-weights models perform\ncloser to human levels (within 10%). Our findings highlight that understanding\nfood and its cultural implications remains a challenging and under-explored\ndirection.", "published": "2024-06-16 17:59:32", "link": "http://arxiv.org/abs/2406.11030v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive\n  Declarative Grammars", "abstract": "Logical reasoning remains a challenge for natural language processing, but it\ncan be improved by training language models to mimic theorem provers on\nprocedurally generated problems. Previous work used domain-specific proof\ngeneration algorithms, which biases reasoning toward specific proof traces and\nlimits auditability and extensibility. We present a simpler and more general\ndeclarative framework with flexible context-sensitive rules binding multiple\nlanguages (specifically, simplified English and the TPTP theorem-proving\nlanguage). We construct first-order logic problems by selecting up to 32\npremises and one hypothesis. We demonstrate that using semantic constraints\nduring generation and careful English verbalization of predicates enhances\nlogical reasoning without hurting natural English tasks. We use relatively\nsmall DeBERTa-v3 models to achieve state-of-the-art accuracy on the FOLIO\nhuman-authored logic dataset, surpassing GPT-4 in accuracy with or without an\nexternal solver by 12%.", "published": "2024-06-16 18:10:49", "link": "http://arxiv.org/abs/2406.11035v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Reconsidering Sentence-Level Sign Language Translation", "abstract": "Historically, sign language machine translation has been posed as a\nsentence-level task: datasets consisting of continuous narratives are chopped\nup and presented to the model as isolated clips. In this work, we explore the\nlimitations of this task framing. First, we survey a number of linguistic\nphenomena in sign languages that depend on discourse-level context. Then as a\ncase study, we perform the first human baseline for sign language translation\nthat actually substitutes a human into the machine learning task framing,\nrather than provide the human with the entire document as context. This human\nbaseline -- for ASL to English translation on the How2Sign dataset -- shows\nthat for 33% of sentences in our sample, our fluent Deaf signer annotators were\nonly able to understand key parts of the clip in light of additional\ndiscourse-level context. These results underscore the importance of\nunderstanding and sanity checking examples when adapting machine learning to\nnew domains.", "published": "2024-06-16 19:19:54", "link": "http://arxiv.org/abs/2406.11049v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can LLMs Understand the Implication of Emphasized Sentences in Dialogue?", "abstract": "Emphasis is a crucial component in human communication, which indicates the\nspeaker's intention and implication beyond pure text in dialogue. While Large\nLanguage Models (LLMs) have revolutionized natural language processing, their\nability to understand emphasis in dialogue remains unclear. This paper\nintroduces Emphasized-Talk, a benchmark with emphasis-annotated dialogue\nsamples capturing the implications of emphasis. We evaluate various LLMs, both\nopen-source and commercial, to measure their performance in understanding\nemphasis. Additionally, we propose an automatic evaluation pipeline using\nGPT-4, which achieves a high correlation with human rating. Our findings reveal\nthat although commercial LLMs generally perform better, there is still\nsignificant room for improvement in comprehending emphasized sentences.", "published": "2024-06-16 20:41:44", "link": "http://arxiv.org/abs/2406.11065v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Limitations of Detecting Machine-Generated Text", "abstract": "Recent improvements in the quality of the generations by large language\nmodels have spurred research into identifying machine-generated text. Such work\noften presents high-performing detectors. However, humans and machines can\nproduce text in different styles and domains, yet the performance impact of\nsuch on machine generated text detection systems remains unclear. In this\npaper, we audit the classification performance for detecting machine-generated\ntext by evaluating on texts with varying writing styles. We find that\nclassifiers are highly sensitive to stylistic changes and differences in text\ncomplexity, and in some cases degrade entirely to random classifiers. We\nfurther find that detection systems are particularly susceptible to misclassify\neasy-to-read texts while they have high performance for complex texts, leading\nto concerns about the reliability of detection systems. We recommend that\nfuture work attends to stylistic factors and reading difficulty levels of\nhuman-written and machine-generated text.", "published": "2024-06-16 21:02:02", "link": "http://arxiv.org/abs/2406.11073v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multiple Sources are Better Than One: Incorporating External Knowledge\n  in Low-Resource Glossing", "abstract": "In this paper, we address the data scarcity problem in automatic data-driven\nglossing for low-resource languages by coordinating multiple sources of\nlinguistic expertise. We supplement models with translations at both the token\nand sentence level as well as leverage the extensive linguistic capability of\nmodern LLMs. Our enhancements lead to an average absolute improvement of\n5%-points in word-level accuracy over the previous state of the art on a\ntypologically diverse dataset spanning six low-resource languages. The\nimprovements are particularly noticeable for the lowest-resourced language\nGitksan, where we achieve a 10%-point improvement. Furthermore, in a simulated\nultra-low resource setting for the same six languages, training on fewer than\n100 glossed sentences, we establish an average 10%-point improvement in\nword-level accuracy over the previous state-of-the-art system.", "published": "2024-06-16 22:01:15", "link": "http://arxiv.org/abs/2406.11085v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation\n  Detection Using In-Context Learning based on Emotional Information", "abstract": "Misinformation is prevalent in various fields such as education, politics,\nhealth, etc., causing significant harm to society. However, current methods for\ncross-domain misinformation detection rely on time and resources consuming\nfine-tuning and complex model structures. With the outstanding performance of\nLLMs, many studies have employed them for misinformation detection.\nUnfortunately, they focus on in-domain tasks and do not incorporate significant\nsentiment and emotion features (which we jointly call affect). In this paper,\nwe propose RAEmoLLM, the first retrieval augmented (RAG) LLMs framework to\naddress cross-domain misinformation detection using in-context learning based\non affective information. It accomplishes this by applying an emotion-aware LLM\nto construct a retrieval database of affective embeddings. This database is\nused by our retrieval module to obtain source-domain samples, which are\nsubsequently used for the inference module's in-context few-shot learning to\ndetect target domain misinformation. We evaluate our framework on three\nmisinformation benchmarks. Results show that RAEmoLLM achieves significant\nimprovements compared to the zero-shot method on three datasets, with the\nhighest increases of 20.69%, 23.94%, and 39.11% respectively. This work will be\nreleased on https://github.com/lzw108/RAEmoLLM.", "published": "2024-06-16 22:49:11", "link": "http://arxiv.org/abs/2406.11093v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Potential and Challenges of Evaluating Attitudes, Opinions, and\n  Values in Large Language Models", "abstract": "Recent advances in Large Language Models (LLMs) have sparked wide interest in\nvalidating and comprehending the human-like cognitive-behavioral traits LLMs\nmay capture and convey. These cognitive-behavioral traits include typically\nAttitudes, Opinions, Values (AOVs). However, measuring AOVs embedded within\nLLMs remains opaque, and different evaluation methods may yield different\nresults. This has led to a lack of clarity on how different studies are related\nto each other and how they can be interpreted. This paper aims to bridge this\ngap by providing a comprehensive overview of recent works on the evaluation of\nAOVs in LLMs. Moreover, we survey related approaches in different stages of the\nevaluation pipeline in these works. By doing so, we address the potential and\nchallenges with respect to understanding the model, human-AI alignment, and\ndownstream application in social sciences. Finally, we provide practical\ninsights into evaluation methods, model enhancement, and interdisciplinary\ncollaboration, thereby contributing to the evolving landscape of evaluating\nAOVs in LLMs.", "published": "2024-06-16 22:59:18", "link": "http://arxiv.org/abs/2406.11096v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Regulated Data-Free Knowledge Amalgamation for Text Classification", "abstract": "Recently, there has been a growing availability of pre-trained text models on\nvarious model repositories. These models greatly reduce the cost of training\nnew models from scratch as they can be fine-tuned for specific tasks or trained\non large datasets. However, these datasets may not be publicly accessible due\nto the privacy, security, or intellectual property issues. In this paper, we\naim to develop a lightweight student network that can learn from multiple\nteacher models without accessing their original training data. Hence, we\ninvestigate Data-Free Knowledge Amalgamation (DFKA), a knowledge-transfer task\nthat combines insights from multiple pre-trained teacher models and transfers\nthem effectively to a compact student network. To accomplish this, we propose\nSTRATANET, a modeling framework comprising: (a) a steerable data generator that\nproduces text data tailored to each teacher and (b) an amalgamation module that\nimplements a self-regulative strategy using confidence estimates from the\nteachers' different layers to selectively integrate their knowledge and train a\nversatile student. We evaluate our method on three benchmark text\nclassification datasets with varying labels or domains. Empirically, we\ndemonstrate that the student model learned using our STRATANET outperforms\nseveral baselines significantly under data-driven and data-free constraints.", "published": "2024-06-16 21:13:30", "link": "http://arxiv.org/abs/2406.15476v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantifying Generative Media Bias with a Corpus of Real-world and\n  Generated News Articles", "abstract": "Large language models (LLMs) are increasingly being utilised across a range\nof tasks and domains, with a burgeoning interest in their application within\nthe field of journalism. This trend raises concerns due to our limited\nunderstanding of LLM behaviour in this domain, especially with respect to\npolitical bias. Existing studies predominantly focus on LLMs undertaking\npolitical questionnaires, which offers only limited insights into their biases\nand operational nuances. To address this gap, our study establishes a new\ncurated dataset that contains 2,100 human-written articles and utilises their\ndescriptions to generate 56,700 synthetic articles using nine LLMs. This\nenables us to analyse shifts in properties between human-authored and\nmachine-generated articles, with this study focusing on political bias,\ndetecting it using both supervised models and LLMs. Our findings reveal\nsignificant disparities between base and instruction-tuned LLMs, with\ninstruction-tuned models exhibiting consistent political bias. Furthermore, we\nare able to study how LLMs behave as classifiers, observing their display of\npolitical bias even in this role. Overall, for the first time within the\njournalistic domain, this study outlines a framework and provides a structured\ndataset for quantifiable experiments, serving as a foundation for further\nresearch into LLM political bias and its implications.", "published": "2024-06-16 01:32:04", "link": "http://arxiv.org/abs/2406.10773v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference", "abstract": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest .", "published": "2024-06-16 01:33:02", "link": "http://arxiv.org/abs/2406.10774v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ShareLoRA: Parameter Efficient and Robust Large Language Model\n  Fine-tuning via Shared Low-Rank Adaptation", "abstract": "This study introduces an approach to optimize Parameter Efficient Fine Tuning\n(PEFT) for Pretrained Language Models (PLMs) by implementing a Shared Low Rank\nAdaptation (ShareLoRA). By strategically deploying ShareLoRA across different\nlayers and adapting it for the Query, Key, and Value components of\nself-attention layers, we achieve a substantial reduction in the number of\ntraining parameters and memory usage. Importantly, ShareLoRA not only maintains\nmodel performance but also exhibits robustness in both classification and\ngeneration tasks across a variety of models, including RoBERTa, GPT-2, LLaMA\nand LLaMA2. It demonstrates superior transfer learning capabilities compared to\nstandard LoRA applications and mitigates overfitting by sharing weights across\nlayers. Our findings affirm that ShareLoRA effectively boosts parameter\nefficiency while ensuring scalable and high-quality performance across\ndifferent language model architectures.", "published": "2024-06-16 02:52:28", "link": "http://arxiv.org/abs/2406.10785v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring the Zero-Shot Capabilities of LLMs Handling Multiple Problems\n  at once", "abstract": "Recent studies have proposed placing multiple problems in a single prompt to\nimprove input token utilization for a more efficient LLM inference. We call\nthis MPP, in contrast to conventional SPP that prompts an LLM with a single\nproblem at a time. While MPP has been shown to work comparably well or even\nbetter than SPP under few-shot settings, its zero-shot performance is\nunderexplored, which better reveals the innate multiple problem handling\ncapabilities of LLMs. To address that, we study the zero-shot MPP performance\nof various LLMs on 6 classification and 12 reasoning benchmarks and confirm\nthat LLMs are competent zero-shot multi-problem solvers. We also examine the\nconditions of effectiveness of zero-shot MPP and explore several model-level\nfactors that may enable MPP. We observe that LLMs consistently perform worse\nwith selecting indices of texts of a given class label and with multiple\nmixed-source reasoning problems, indicating a lack of true understanding. We\nalso find that instruction tuning is an important factor than enhances MPP.", "published": "2024-06-16 02:52:32", "link": "http://arxiv.org/abs/2406.10786v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "KGPA: Robustness Evaluation for Large Language Models via Cross-Domain\n  Knowledge Graphs", "abstract": "Existing frameworks for assessing robustness of large language models (LLMs)\noverly depend on specific benchmarks, increasing costs and failing to evaluate\nperformance of LLMs in professional domains due to dataset limitations. This\npaper proposes a framework that systematically evaluates the robustness of LLMs\nunder adversarial attack scenarios by leveraging knowledge graphs (KGs). Our\nframework generates original prompts from the triplets of knowledge graphs and\ncreates adversarial prompts by poisoning, assessing the robustness of LLMs\nthrough the results of these adversarial attacks. We systematically evaluate\nthe effectiveness of this framework and its modules. Experiments show that\nadversarial robustness of the ChatGPT family ranks as GPT-4-turbo > GPT-4o >\nGPT-3.5-turbo, and the robustness of large language models is influenced by the\nprofessional domains in which they operate.", "published": "2024-06-16 04:48:43", "link": "http://arxiv.org/abs/2406.10802v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Post-hoc Utterance Refining Method by Entity Mining for Faithful\n  Knowledge Grounded Conversations", "abstract": "Despite the striking advances in recent language generation performance,\nmodel-generated responses have suffered from the chronic problem of\nhallucinations that are either untrue or unfaithful to a given source.\nEspecially in the task of knowledge grounded conversation, the models are\nrequired to generate informative responses, but hallucinated utterances lead to\nmiscommunication. In particular, entity-level hallucination that causes\ncritical misinformation and undesirable conversation is one of the major\nconcerns. To address this issue, we propose a post-hoc refinement method called\nREM. It aims to enhance the quality and faithfulness of hallucinated utterances\nby refining them based on the source knowledge. If the generated utterance has\na low source-faithfulness score with the given knowledge, REM mines the key\nentities in the knowledge and implicitly uses them for refining the utterances.\nWe verify that our method reduces entity hallucination in the utterance. Also,\nwe show the adaptability and efficacy of REM with extensive experiments and\ngenerative results. Our code is available at https://github.com/YOONNAJANG/REM.", "published": "2024-06-16 06:12:47", "link": "http://arxiv.org/abs/2406.10809v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reminding Multimodal Large Language Models of Object-aware Knowledge\n  with Retrieved Tags", "abstract": "Despite recent advances in the general visual instruction-following ability\nof Multimodal Large Language Models (MLLMs), they still struggle with critical\nproblems when required to provide a precise and detailed response to a visual\ninstruction: (1) failure to identify novel objects or entities, (2) mention of\nnon-existent objects, and (3) neglect of object's attributed details. Intuitive\nsolutions include improving the size and quality of data or using larger\nfoundation models. They show effectiveness in mitigating these issues, but at\nan expensive cost of collecting a vast amount of new data and introducing a\nsignificantly larger model. Standing at the intersection of these approaches,\nwe examine the three object-oriented problems from the perspective of the\nimage-to-text mapping process by the multimodal connector. In this paper, we\nfirst identify the limitations of multimodal connectors stemming from\ninsufficient training data. Driven by this, we propose to enhance the mapping\nwith retrieval-augmented tag tokens, which contain rich object-aware\ninformation such as object names and attributes. With our Tag-grounded visual\ninstruction tuning with retrieval Augmentation (TUNA), we outperform baselines\nthat share the same language model and training data on 12 benchmarks.\nFurthermore, we show the zero-shot capability of TUNA when provided with\nspecific datastores.", "published": "2024-06-16 08:20:12", "link": "http://arxiv.org/abs/2406.10839v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Step-level Value Preference Optimization for Mathematical Reasoning", "abstract": "Direct Preference Optimization (DPO) using an implicit reward model has\nproven to be an effective alternative to reinforcement learning from human\nfeedback (RLHF) for fine-tuning preference aligned large language models\n(LLMs). However, the overall preference annotations of responses do not fully\ncapture the fine-grained quality of model outputs in complex multi-step\nreasoning tasks, such as mathematical reasoning. To address this limitation, we\nintroduce a novel algorithm called Step-level Value Preference Optimization\n(SVPO). Our approach employs Monte Carlo Tree Search (MCTS) to automatically\nannotate step-level preferences for multi-step reasoning. Furthermore, from the\nperspective of learning-to-rank, we train an explicit value model to replicate\nthe behavior of the implicit reward model, complementing standard preference\noptimization. This value model enables the LLM to generate higher reward\nresponses with minimal cost during inference. Experimental results demonstrate\nthat our method achieves state-of-the-art performance on both in-domain and\nout-of-domain mathematical reasoning benchmarks. Our code is available at\n\\url{https://github.com/MARIO-Math-Reasoning/Super_MARIO}.", "published": "2024-06-16 09:06:17", "link": "http://arxiv.org/abs/2406.10858v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Demonstration Notebook: Finding the Most Suited In-Context Learning\n  Example from Interactions", "abstract": "Large language models (LLMs) benefit greatly from prompt engineering, with\nin-context learning standing as a pivital technique. While former approaches\nhave provided various ways to construct the demonstrations used for in-context\nlearning, they often ignore the inherent heterogeneity within datasets,\napplying the same demonstrations to all reasoning questions. We observed that\nthe effectiveness of demonstrations varies depending on the specific question.\nThis motivates our exploration of using prompt engineering to select\nappropriate demonstrations. To address the challenge of automatically creating\nand choosing demonstrations tailored to each question, we propose a novel\nprompt engineering workflow built around a novel object called the\n\"demonstration notebook.\" This notebook helps identify the most suitable\nin-context learning example for a question by gathering and reusing information\nfrom the LLM's past interactions. Our experiments show that this approach\noutperforms all existing methods for automatic demonstration construction and\nselection (as far as we know), achieving state-of-the-art results on serveral\nreasoning benchmarks. The method's versatility is further demonstrated by its\nsuccess in text summarization and prompt compression tasks. Additionally, we\ncontribute a rigorous analysis method to reveal the \"demonstrative regime\" of a\ndemonstration, providing valuable insights into how demonstrations relate to\ndifferent question types within a dataset.", "published": "2024-06-16 10:02:20", "link": "http://arxiv.org/abs/2406.10878v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Distilling Opinions at Scale: Incremental Opinion Summarization using\n  XL-OPSUMM", "abstract": "Opinion summarization in e-commerce encapsulates the collective views of\nnumerous users about a product based on their reviews. Typically, a product on\nan e-commerce platform has thousands of reviews, each review comprising around\n10-15 words. While Large Language Models (LLMs) have shown proficiency in\nsummarization tasks, they struggle to handle such a large volume of reviews due\nto context limitations. To mitigate, we propose a scalable framework called\nXl-OpSumm that generates summaries incrementally. However, the existing test\nset, AMASUM has only 560 reviews per product on average. Due to the lack of a\ntest set with thousands of reviews, we created a new test set called\nXl-Flipkart by gathering data from the Flipkart website and generating\nsummaries using GPT-4. Through various automatic evaluations and extensive\nanalysis, we evaluated the framework's efficiency on two datasets, AMASUM and\nXl-Flipkart. Experimental results show that our framework, Xl-OpSumm powered by\nLlama-3-8B-8k, achieves an average ROUGE-1 F1 gain of 4.38% and a ROUGE-L F1\ngain of 3.70% over the next best-performing model.", "published": "2024-06-16 10:36:41", "link": "http://arxiv.org/abs/2406.10886v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AutoHallusion: Automatic Generation of Hallucination Benchmarks for\n  Vision-Language Models", "abstract": "Large vision-language models (LVLMs) are prone to hallucinations, where\ncertain contextual cues in an image can trigger the language module to produce\noverconfident and incorrect reasoning about abnormal or hypothetical objects.\nWhile some benchmarks have been developed to investigate LVLM hallucinations,\nthey often rely on hand-crafted corner cases whose failure patterns may not\ngeneralize well. Additionally, fine-tuning on these examples could undermine\ntheir validity. To address this, we aim to scale up the number of cases through\nan automated approach, reducing human bias in crafting such corner cases. This\nmotivates the development of AutoHallusion, the first automated benchmark\ngeneration approach that employs several key strategies to create a diverse\nrange of hallucination examples. Our generated visual-question pairs pose\nsignificant challenges to LVLMs, requiring them to overcome contextual biases\nand distractions to arrive at correct answers. AutoHallusion enables us to\ncreate new benchmarks at the minimum cost and thus overcomes the fragility of\nhand-crafted benchmarks. It also reveals common failure patterns and reasons,\nproviding key insights to detect, avoid, or control hallucinations.\nComprehensive evaluations of top-tier LVLMs, e.g., GPT-4V(ision), Gemini Pro\nVision, Claude 3, and LLaVA-1.5, show a 97.7% and 98.7% success rate of\nhallucination induction on synthetic and real-world datasets of AutoHallusion,\npaving the way for a long battle against hallucinations. The codebase and data\ncan be accessed at https://github.com/wuxiyang1996/AutoHallusion.", "published": "2024-06-16 11:44:43", "link": "http://arxiv.org/abs/2406.10900v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Light Up the Shadows: Enhance Long-Tailed Entity Grounding with\n  Concept-Guided Vision-Language Models", "abstract": "Multi-Modal Knowledge Graphs (MMKGs) have proven valuable for various\ndownstream tasks. However, scaling them up is challenging because building\nlarge-scale MMKGs often introduces mismatched images (i.e., noise). Most\nentities in KGs belong to the long tail, meaning there are few images of them\navailable online. This scarcity makes it difficult to determine whether a found\nimage matches the entity. To address this, we draw on the Triangle of Reference\nTheory and suggest enhancing vision-language models with concept guidance.\nSpecifically, we introduce COG, a two-stage framework with COncept-Guided\nvision-language models. The framework comprises a Concept Integration module,\nwhich effectively identifies image-text pairs of long-tailed entities, and an\nEvidence Fusion module, which offers explainability and enables human\nverification. To demonstrate the effectiveness of COG, we create a dataset of\n25k image-text pairs of long-tailed entities. Our comprehensive experiments\nshow that COG not only improves the accuracy of recognizing long-tailed\nimage-text pairs compared to baselines but also offers flexibility and\nexplainability.", "published": "2024-06-16 11:49:00", "link": "http://arxiv.org/abs/2406.10902v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Breaking the Attention Bottleneck", "abstract": "Attention-based transformers have become the standard architecture in many\ndeep learning fields, primarily due to their ability to model long-range\ndependencies and handle variable-length input sequences. However, the attention\nmechanism with its quadratic complexity is a significant bottleneck in the\ntransformer architecture. This algorithm is only uni-directional in the decoder\nand converges to a static pattern in over-parametrized decoder-only models. I\naddress this issue by developing a generative function as attention or\nactivation replacement. It still has the auto-regressive character by comparing\neach token with the previous one. In my test setting with nanoGPT this yields a\nsmaller loss while having a smaller model. The loss further drops by\nincorporating an average context vector. This concept of attention replacement\nis distributed under the GNU AGPL v3 license at\nhttps://gitlab.com/Bachstelze/causal_generation.", "published": "2024-06-16 12:06:58", "link": "http://arxiv.org/abs/2406.10906v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "DocNet: Semantic Structure in Inductive Bias Detection Models", "abstract": "News will be biased so long as people have opinions. As social media becomes\nthe primary entry point for news and partisan differences increase, it is\nincreasingly important for informed citizens to be able to recognize bias. If\npeople are aware of the biases of the news they consume, they will be able to\ntake action to avoid polarizing echo chambers. In this paper, we explore an\noften overlooked aspect of bias detection in media: the semantic structure of\nnews articles. We present DocNet, a novel, inductive, and low-resource document\nembedding and political bias detection model. We also demonstrate that the\nsemantic structure of news articles from opposing political sides, as\nrepresented in document-level graph embeddings, have significant similarities.\nDocNet bypasses the need for pre-trained language models, reducing resource\ndependency while achieving comparable performance. It can be used to advance\npolitical bias detection in low-resource environments. Our code and data are\nmade available at: https://anonymous.4open.science/r/DocNet/", "published": "2024-06-16 14:51:12", "link": "http://arxiv.org/abs/2406.10965v3", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Towards Supporting Legal Argumentation with NLP: Is More Data Really All\n  You Need?", "abstract": "Modeling legal reasoning and argumentation justifying decisions in cases has\nalways been central to AI & Law, yet contemporary developments in legal NLP\nhave increasingly focused on statistically classifying legal conclusions from\ntext. While conceptually simpler, these approaches often fall short in\nproviding usable justifications connecting to appropriate legal concepts. This\npaper reviews both traditional symbolic works in AI & Law and recent advances\nin legal NLP, and distills possibilities of integrating expert-informed\nknowledge to strike a balance between scalability and explanation in symbolic\nvs. data-driven approaches. We identify open challenges and discuss the\npotential of modern NLP models and methods that integrate", "published": "2024-06-16 15:15:44", "link": "http://arxiv.org/abs/2406.10974v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Toward Optimal LLM Alignments Using Two-Player Games", "abstract": "The standard Reinforcement Learning from Human Feedback (RLHF) framework\nprimarily focuses on optimizing the performance of large language models using\npre-collected prompts. However, collecting prompts that provide comprehensive\ncoverage is both tedious and challenging, and often fails to include scenarios\nthat LLMs need to improve on the most. In this paper, we investigate alignment\nthrough the lens of two-agent games, involving iterative interactions between\nan adversarial and a defensive agent. The adversarial agent's task at each step\nis to generate prompts that expose the weakness of the defensive agent. In\nreturn, the defensive agent seeks to improve its responses to these newly\nidentified prompts it struggled with, based on feedback from the reward model.\nWe theoretically demonstrate that this iterative reinforcement learning\noptimization converges to a Nash Equilibrium for the game induced by the\nagents. Experimental results in safety scenarios demonstrate that learning in\nsuch a competitive environment not only fully trains agents but also leads to\npolicies with enhanced generalization capabilities for both adversarial and\ndefensive agents.", "published": "2024-06-16 15:24:50", "link": "http://arxiv.org/abs/2406.10977v1", "categories": ["cs.CL", "cs.AI", "68"], "primary_category": "cs.CL"}
{"title": "Balancing Rigor and Utility: Mitigating Cognitive Biases in Large\n  Language Models for Multiple-Choice Questions", "abstract": "This paper examines the role of cognitive biases in the decision-making\nprocesses of large language models (LLMs), challenging the conventional goal of\neliminating all biases. When properly balanced, we show that certain cognitive\nbiases can enhance decision-making efficiency through rational deviations and\nheuristic shortcuts. By introducing heuristic moderation and an abstention\noption, which allows LLMs to withhold responses when uncertain, we reduce error\nrates, improve decision accuracy, and optimize decision rates. Using the\nBalance Rigor and Utility (BRU) dataset, developed through expert\ncollaboration, our findings demonstrate that targeted inspection of cognitive\nbiases aligns LLM decisions more closely with human reasoning, enhancing\nreliability and suggesting strategies for future improvements. This approach\noffers a novel way to leverage cognitive biases to improve the practical\nutility of LLMs across various applications.", "published": "2024-06-16 16:25:22", "link": "http://arxiv.org/abs/2406.10999v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs\n  Using the New York Times Connections Word Game", "abstract": "The New York Times Connections game has emerged as a popular and challenging\npursuit for word puzzle enthusiasts. We collect 438 Connections games to\nevaluate the performance of state-of-the-art large language models (LLMs)\nagainst expert and novice human players. Our results show that even the best\nperforming LLM, Claude 3.5 Sonnet, which has otherwise shown impressive\nreasoning abilities on a wide variety of benchmarks, can only fully solve 18%\nof the games. Novice and expert players perform better than Claude 3.5 Sonnet,\nwith expert human players significantly outperforming it. We create a taxonomy\nof the knowledge types required to successfully cluster and categorize words in\nthe Connections game. We find that while LLMs perform relatively well on\ncategorizing words based on semantic relations they struggle with other types\nof knowledge such as Encyclopedic Knowledge, Multiword Expressions or knowledge\nthat combines both Word Form and Meaning. Our results establish the New York\nTimes Connections game as a challenging benchmark for evaluating abstract\nreasoning capabilities in AI systems.", "published": "2024-06-16 17:10:32", "link": "http://arxiv.org/abs/2406.11012v7", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Optimized Speculative Sampling for GPU Hardware Accelerators", "abstract": "In this work, we optimize speculative sampling for parallel hardware\naccelerators to improve sampling speed. We notice that substantial portions of\nthe intermediate matrices necessary for speculative sampling can be computed\nconcurrently. This allows us to distribute the workload across multiple GPU\nthreads, enabling simultaneous operations on matrix segments within thread\nblocks. This results in profiling time improvements ranging from 6% to 13%\nrelative to the baseline implementation, without compromising accuracy. To\nfurther accelerate speculative sampling, probability distributions\nparameterized by softmax are approximated by sigmoid. This approximation\napproach results in significantly greater relative improvements in profiling\ntime, ranging from 37% to 94%, with a minor decline in accuracy. We conduct\nextensive experiments on both automatic speech recognition and summarization\ntasks to validate the effectiveness of our optimization methods.", "published": "2024-06-16 17:19:23", "link": "http://arxiv.org/abs/2406.11016v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Universal Cross-Lingual Text Classification", "abstract": "Text classification, an integral task in natural language processing,\ninvolves the automatic categorization of text into predefined classes. Creating\nsupervised labeled datasets for low-resource languages poses a considerable\nchallenge. Unlocking the language potential of low-resource languages requires\nrobust datasets with supervised labels. However, such datasets are scarce, and\nthe label space is often limited. In our pursuit to address this gap, we aim to\noptimize existing labels/datasets in different languages. This research\nproposes a novel perspective on Universal Cross-Lingual Text Classification,\nleveraging a unified model across languages. Our approach involves blending\nsupervised data from different languages during training to create a universal\nmodel. The supervised data for a target classification task might come from\ndifferent languages covering different labels. The primary goal is to enhance\nlabel and language coverage, aiming for a label set that represents a union of\nlabels from various languages. We propose the usage of a strong multilingual\nSBERT as our base model, making our novel training strategy feasible. This\nstrategy contributes to the adaptability and effectiveness of the model in\ncross-lingual language transfer scenarios, where it can categorize text in\nlanguages not encountered during training. Thus, the paper delves into the\nintricacies of cross-lingual text classification, with a particular focus on\nits application for low-resource languages, exploring methodologies and\nimplications for the development of a robust and adaptable universal\ncross-lingual model.", "published": "2024-06-16 17:58:29", "link": "http://arxiv.org/abs/2406.11028v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Curating Stopwords in Marathi: A TF-IDF Approach for Improved Text\n  Analysis and Information Retrieval", "abstract": "Stopwords are commonly used words in a language that are often considered to\nbe of little value in determining the meaning or significance of a document.\nThese words occur frequently in most texts and don't provide much useful\ninformation for tasks like sentiment analysis and text classification. English,\nwhich is a high-resource language, takes advantage of the availability of\nstopwords, whereas low-resource Indian languages like Marathi are very limited,\nstandardized, and can be used in available packages, but the number of\navailable words in those packages is low. Our work targets the curation of\nstopwords in the Marathi language using the MahaCorpus, with 24.8 million\nsentences. We make use of the TF-IDF approach coupled with human evaluation to\ncurate a strong stopword list of 400 words. We apply the stop word removal to\nthe text classification task and show its efficacy. The work also presents a\nsimple recipe for stopword curation in a low-resource language. The stopwords\nare integrated into the mahaNLP library and publicly available on\nhttps://github.com/l3cube-pune/MarathiNLP .", "published": "2024-06-16 17:59:05", "link": "http://arxiv.org/abs/2406.11029v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "garak: A Framework for Security Probing Large Language Models", "abstract": "As Large Language Models (LLMs) are deployed and integrated into thousands of\napplications, the need for scalable evaluation of how models respond to\nadversarial attacks grows rapidly. However, LLM security is a moving target:\nmodels produce unpredictable output, are constantly updated, and the potential\nadversary is highly diverse: anyone with access to the internet and a decent\ncommand of natural language. Further, what constitutes a security weak in one\ncontext may not be an issue in a different context; one-fits-all guardrails\nremain theoretical. In this paper, we argue that it is time to rethink what\nconstitutes ``LLM security'', and pursue a holistic approach to LLM security\nevaluation, where exploration and discovery of issues are central. To this end,\nthis paper introduces garak (Generative AI Red-teaming and Assessment Kit), a\nframework which can be used to discover and identify vulnerabilities in a\ntarget LLM or dialog system. garak probes an LLM in a structured fashion to\ndiscover potential vulnerabilities. The outputs of the framework describe a\ntarget model's weaknesses, contribute to an informed discussion of what\ncomposes vulnerabilities in unique contexts, and can inform alignment and\npolicy discussions for LLM deployment.", "published": "2024-06-16 18:18:43", "link": "http://arxiv.org/abs/2406.11036v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "A Peek into Token Bias: Large Language Models Are Not Yet Genuine\n  Reasoners", "abstract": "This study introduces a hypothesis-testing framework to assess whether large\nlanguage models (LLMs) possess genuine reasoning abilities or primarily depend\non token bias. We go beyond evaluating LLMs on accuracy; rather, we aim to\ninvestigate their token bias in solving logical reasoning tasks. Specifically,\nwe develop carefully controlled synthetic datasets, featuring conjunction\nfallacy and syllogistic problems. Our framework outlines a list of hypotheses\nwhere token biases are readily identifiable, with all null hypotheses assuming\ngenuine reasoning capabilities of LLMs. The findings in this study suggest,\nwith statistical guarantee, that most LLMs still struggle with logical\nreasoning. While they may perform well on classic problems, their success\nlargely depends on recognizing superficial patterns with strong token bias,\nthereby raising concerns about their actual reasoning and generalization\nabilities. Codes and data are open-sourced at\nhttps://github.com/bowen-upenn/llm_token_bias.", "published": "2024-06-16 19:22:53", "link": "http://arxiv.org/abs/2406.11050v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "InstructCMP: Length Control in Sentence Compression through\n  Instruction-based Large Language Models", "abstract": "Extractive summarization can produce faithful summaries but often requires\nadditional constraints such as a desired summary length. Traditional sentence\ncompression models do not typically consider the constraints because of their\nrestricted model abilities, which require model modifications for coping with\nthem. To bridge this gap, we propose Instruction-based Compression\n(InstructCMP), an approach to the sentence compression task that can consider\nthe length constraint through instructions by leveraging the zero-shot\ntask-solving abilities of Large Language Models (LLMs). For this purpose, we\ncreated new evaluation datasets by transforming traditional sentence\ncompression datasets into an instruction format. By using the datasets, we\nfirst reveal that the current LLMs still face challenges in accurately\ncontrolling the length for a compressed text. To address this issue, we propose\nan approach named \"length priming,\" that incorporates additional length\ninformation into the instructions without external resources. While the length\npriming effectively works in a zero-shot setting, a training dataset with the\ninstructions would further improve the ability of length control. Thus, we\nadditionally created a training dataset in an instruction format to fine-tune\nthe model on it. Experimental results and analysis show that applying the\nlength priming significantly improves performances of InstructCMP in both\nzero-shot and fine-tuning settings without the need of any model modifications.", "published": "2024-06-16 23:00:47", "link": "http://arxiv.org/abs/2406.11097v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Grading Massive Open Online Courses Using Large Language Models", "abstract": "Massive open online courses (MOOCs) offer free education globally. Despite\nthis democratization of learning, the massive enrollment in these courses makes\nit impractical for an instructor to assess every student's writing assignment.\nAs a result, peer grading, often guided by a straightforward rubric, is the\nmethod of choice. While convenient, peer grading often falls short in terms of\nreliability and validity. In this study, we explore the feasibility of using\nlarge language models (LLMs) to replace peer grading in MOOCs. To this end, we\nadapt the zero-shot chain-of-thought (ZCoT) prompting technique to automate the\nfeedback process once the LLM assigns a score to an assignment. Specifically,\nto instruct LLMs for grading, we use three distinct prompts based on ZCoT: (1)\nZCoT with instructor-provided correct answers, (2) ZCoT with both\ninstructor-provided correct answers and rubrics, and (3) ZCoT with\ninstructor-provided correct answers and LLM-generated rubrics. We tested these\nprompts in 18 different scenarios using two LLMs, GPT-4 and GPT-3.5, across\nthree MOOCs: Introductory Astronomy, Astrobiology, and the History and\nPhilosophy of Astronomy. Our results show that ZCoT, when augmented with\ninstructor-provided correct answers and rubrics, produces grades that are more\naligned with those assigned by instructors compared to peer grading. Finally,\nour findings indicate a promising potential for automated grading systems in\nMOOCs, especially in subjects with well-defined rubrics, to improve the\nlearning experience for millions of online learners worldwide.", "published": "2024-06-16 23:42:11", "link": "http://arxiv.org/abs/2406.11102v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for\n  Multi-label Social Media Text Classification in Disaster Informatics", "abstract": "In the field of crisis/disaster informatics, social media is increasingly\nbeing used for improving situational awareness to inform response and relief\nefforts. Efficient and accurate text classification tools have been a focal\narea of investigation in crisis informatics. However, current methods mostly\nrely on single-label text classification models, which fails to capture\ndifferent insights embedded in dynamic and multifaceted disaster-related social\nmedia data. This study introduces a novel approach to disaster text\nclassification by enhancing a pre-trained Large Language Model (LLM) through\ninstruction fine-tuning targeted for multi-label classification of\ndisaster-related tweets. Our methodology involves creating a comprehensive\ninstruction dataset from disaster-related tweets, which is then used to\nfine-tune an open-source LLM, thereby embedding it with disaster-specific\nknowledge. This fine-tuned model can classify multiple aspects of\ndisaster-related information simultaneously, such as the type of event,\ninformativeness, and involvement of human aid, significantly improving the\nutility of social media data for situational awareness in disasters. The\nresults demonstrate that this approach enhances the categorization of critical\ninformation from social media posts, thereby facilitating a more effective\ndeployment for situational awareness during emergencies. This research paves\nthe way for more advanced, adaptable, and robust disaster management tools,\nleveraging the capabilities of LLMs to improve real-time situational awareness\nand response strategies in disaster scenarios.", "published": "2024-06-16 23:01:10", "link": "http://arxiv.org/abs/2406.15477v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HiddenTables & PyQTax: A Cooperative Game and Dataset For TableQA to\n  Ensure Scale and Data Privacy Across a Myriad of Taxonomies", "abstract": "A myriad of different Large Language Models (LLMs) face a common challenge in\ncontextually analyzing table question-answering tasks. These challenges are\nengendered from (1) finite context windows for large tables, (2) multi-faceted\ndiscrepancies amongst tokenization patterns against cell boundaries, and (3)\nvarious limitations stemming from data confidentiality in the process of using\nexternal models such as gpt-3.5-turbo. We propose a cooperative game dubbed\n\"HiddenTables\" as a potential resolution to this challenge. In essence,\n\"HiddenTables\" is played between the code-generating LLM \"Solver\" and the\n\"Oracle\" which evaluates the ability of the LLM agents to solve Table QA tasks.\nThis game is based on natural language schemas and importantly, ensures the\nsecurity of the underlying data. We provide evidential experiments on a diverse\nset of tables that demonstrate an LLM's collective inability to generalize and\nperform on complex queries, handle compositional dependencies, and align\nnatural language to programmatic commands when concrete table schemas are\nprovided. Unlike encoder-based models, we have pushed the boundaries of\n\"HiddenTables\" to not be limited by the number of rows - therefore we exhibit\nimproved efficiency in prompt and completion tokens. Our infrastructure has\nspawned a new dataset \"PyQTax\" that spans across 116,671 question-table-answer\ntriplets and provides additional fine-grained breakdowns & labels for varying\nquestion taxonomies. Therefore, in tandem with our academic contributions\nregarding LLMs' deficiency in TableQA tasks, \"HiddenTables\" is a tactile\nmanifestation of how LLMs can interact with massive datasets while ensuring\ndata security and minimizing generation costs.", "published": "2024-06-16 04:53:29", "link": "http://arxiv.org/abs/2406.10803v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "ptt5-v2: A Closer Look at Continued Pretraining of T5 Models for the\n  Portuguese Language", "abstract": "Despite advancements in Natural Language Processing (NLP) and the growing\navailability of pretrained models, the English language remains the primary\nfocus of model development. Continued pretraining on language-specific corpora\nprovides a practical solution for adapting models to other languages. However,\nthe impact of different pretraining settings on downstream tasks remains\nunderexplored. This work introduces $\\texttt{ptt5-v2}$, investigating the\ncontinued pretraining of T5 models for Portuguese. We first develop a baseline\nset of settings and pretrain models with sizes up to 3B parameters. Finetuning\non three Portuguese downstream tasks (assin2 STS, assin2 RTE, and TweetSentBR)\nyields SOTA results on the latter two. We then explore the effects of different\npretraining configurations, including pretraining data quality, optimization\nstrategies, and multi-epoch pretraining. Perhaps surprisingly, their impact\nremains subtle compared to our baseline. We release $\\texttt{ptt5-v2}$\npretrained checkpoints and their MonoT5-based finetuned $\\texttt{MonoPTT5}$\nrerankers on HuggingFace in their respective collections at\n\\url{https://huggingface.co/unicamp-dl}.", "published": "2024-06-16 05:17:56", "link": "http://arxiv.org/abs/2406.10806v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "LLMFactor: Extracting Profitable Factors through Prompts for Explainable\n  Stock Movement Prediction", "abstract": "Recently, Large Language Models (LLMs) have attracted significant attention\nfor their exceptional performance across a broad range of tasks, particularly\nin text analysis. However, the finance sector presents a distinct challenge due\nto its dependence on time-series data for complex forecasting tasks. In this\nstudy, we introduce a novel framework called LLMFactor, which employs\nSequential Knowledge-Guided Prompting (SKGP) to identify factors that influence\nstock movements using LLMs. Unlike previous methods that relied on keyphrases\nor sentiment analysis, this approach focuses on extracting factors more\ndirectly related to stock market dynamics, providing clear explanations for\ncomplex temporal changes. Our framework directs the LLMs to create background\nknowledge through a fill-in-the-blank strategy and then discerns potential\nfactors affecting stock prices from related news. Guided by background\nknowledge and identified factors, we leverage historical stock prices in\ntextual format to predict stock movement. An extensive evaluation of the\nLLMFactor framework across four benchmark datasets from both the U.S. and\nChinese stock markets demonstrates its superiority over existing\nstate-of-the-art methods and its effectiveness in financial time-series\nforecasting.", "published": "2024-06-16 06:20:50", "link": "http://arxiv.org/abs/2406.10811v1", "categories": ["cs.CL", "cs.AI", "cs.CE"], "primary_category": "cs.CL"}
{"title": "GUI-World: A Video Benchmark and Dataset for Multimodal GUI-oriented\n  Understanding", "abstract": "Recently, Multimodal Large Language Models (MLLMs) have been used as agents\nto control keyboard and mouse inputs by directly perceiving the Graphical User\nInterface (GUI) and generating corresponding commands. However, current agents\nprimarily demonstrate strong understanding capabilities in static environments\nand are mainly applied to relatively simple domains, such as Web or mobile\ninterfaces. We argue that a robust GUI agent should be capable of perceiving\ntemporal information on the GUI, including dynamic Web content and multi-step\ntasks. Additionally, it should possess a comprehensive understanding of various\nGUI scenarios, including desktop software and multi-window interactions. To\nthis end, this paper introduces a new dataset, termed GUI-World, which features\nmeticulously crafted Human-MLLM annotations, extensively covering six GUI\nscenarios and eight types of GUI-oriented questions in three formats. We\nevaluate the capabilities of current state-of-the-art MLLMs, including Image\nLLMs and Video LLMs, in understanding various types of GUI content, especially\ndynamic and sequential content. Our findings reveal that current models\nstruggle with dynamic GUI content without manually annotated keyframes or\noperation history. On the other hand, Video LLMs fall short in all GUI-oriented\ntasks given the sparse GUI video dataset. Therefore, we take the initial step\nof leveraging a fine-tuned Video LLM, GUI-Vid, as a GUI-oriented assistant,\ndemonstrating an improved understanding of various GUI tasks. However, due to\nthe limitations in the performance of base LLMs, we conclude that using video\nLLMs as GUI agents remains a significant challenge. We believe our work\nprovides valuable insights for future research in dynamic GUI content\nunderstanding. All the dataset and code are publicly available at:\nhttps://gui-world.github.io.", "published": "2024-06-16 06:56:53", "link": "http://arxiv.org/abs/2406.10819v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Exposing the Achilles' Heel: Evaluating LLMs Ability to Handle Mistakes\n  in Mathematical Reasoning", "abstract": "Large Language Models (LLMs) have been applied to Math Word Problems (MWPs)\nwith transformative impacts, revolutionizing how these complex problems are\napproached and solved in various domains including educational settings.\nHowever, the evaluation of these models often prioritizes final accuracy,\noverlooking the crucial aspect of reasoning capabilities. This work addresses\nthis gap by focusing on the ability of LLMs to detect and correct reasoning\nmistakes. We introduce a novel dataset MWP-MISTAKE, incorporating MWPs with\nboth correct and incorrect reasoning steps generated through rule-based methods\nand smaller language models. Our comprehensive benchmarking reveals significant\ninsights into the strengths and weaknesses of state-of-the-art models, such as\nGPT-4o, GPT-4, GPT-3.5Turbo, and others. We highlight GPT-$o's superior\nperformance in mistake detection and rectification and the persistent\nchallenges faced by smaller models. Additionally, we identify issues related to\ndata contamination and memorization, impacting the reliability of LLMs in\nreal-world applications. Our findings emphasize the importance of rigorous\nevaluation of reasoning processes and propose future directions to enhance the\ngeneralization and robustness of LLMs in mathematical problem-solving.", "published": "2024-06-16 08:06:05", "link": "http://arxiv.org/abs/2406.10834v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Automatic Milestone Detection in Group\n  Discussions", "abstract": "Large language models like GPT have proven widely successful on natural\nlanguage understanding tasks based on written text documents. In this paper, we\ninvestigate an LLM's performance on recordings of a group oral communication\ntask in which utterances are often truncated or not well-formed. We propose a\nnew group task experiment involving a puzzle with several milestones that can\nbe achieved in any order. We investigate methods for processing transcripts to\ndetect if, when, and by whom a milestone has been completed. We demonstrate\nthat iteratively prompting GPT with transcription chunks outperforms semantic\nsimilarity search methods using text embeddings, and further discuss the\nquality and randomness of GPT responses under different context window sizes.", "published": "2024-06-16 08:32:22", "link": "http://arxiv.org/abs/2406.10842v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "TorchOpera: A Compound AI System for LLM Safety", "abstract": "We introduce TorchOpera, a compound AI system for enhancing the safety and\nquality of prompts and responses for Large Language Models. TorchOpera ensures\nthat all user prompts are safe, contextually grounded, and effectively\nprocessed, while enhancing LLM responses to be relevant and high quality.\nTorchOpera utilizes the vector database for contextual grounding, rule-based\nwrappers for flexible modifications, and specialized mechanisms for detecting\nand adjusting unsafe or incorrect content. We also provide a view of the\ncompound AI system to reduce the computational cost. Extensive experiments show\nthat TorchOpera ensures the safety, reliability, and applicability of LLMs in\nreal-world settings while maintaining the efficiency of LLM responses.", "published": "2024-06-16 08:39:19", "link": "http://arxiv.org/abs/2406.10847v2", "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Optimizing Automatic Speech Assessment: W-RankSim Regularization and\n  Hybrid Feature Fusion Strategies", "abstract": "Automatic Speech Assessment (ASA) has seen notable advancements with the\nutilization of self-supervised features (SSL) in recent research. However, a\nkey challenge in ASA lies in the imbalanced distribution of data, particularly\nevident in English test datasets. To address this challenge, we approach ASA as\nan ordinal classification task, introducing Weighted Vectors Ranking Similarity\n(W-RankSim) as a novel regularization technique. W-RankSim encourages closer\nproximity of weighted vectors in the output layer for similar classes, implying\nthat feature vectors with similar labels would be gradually nudged closer to\neach other as they converge towards corresponding weighted vectors. Extensive\nexperimental evaluations confirm the effectiveness of our approach in improving\nordinal classification performance for ASA. Furthermore, we propose a hybrid\nmodel that combines SSL and handcrafted features, showcasing how the inclusion\nof handcrafted features enhances performance in an ASA system.", "published": "2024-06-16 09:55:21", "link": "http://arxiv.org/abs/2406.10873v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language\n  Models", "abstract": "Large language models (LLMs) inevitably memorize sensitive, copyrighted, and\nharmful knowledge from the training corpus; therefore, it is crucial to erase\nthis knowledge from the models. Machine unlearning is a promising solution for\nefficiently removing specific knowledge by post hoc modifying models. In this\npaper, we propose a Real-World Knowledge Unlearning benchmark (RWKU) for LLM\nunlearning. RWKU is designed based on the following three key factors: (1) For\nthe task setting, we consider a more practical and challenging unlearning\nsetting, where neither the forget corpus nor the retain corpus is accessible.\n(2) For the knowledge source, we choose 200 real-world famous people as the\nunlearning targets and show that such popular knowledge is widely present in\nvarious LLMs. (3) For the evaluation framework, we design the forget set and\nthe retain set to evaluate the model's capabilities across various real-world\napplications. Regarding the forget set, we provide four four membership\ninference attack (MIA) methods and nine kinds of adversarial attack probes to\nrigorously test unlearning efficacy. Regarding the retain set, we assess\nlocality and utility in terms of neighbor perturbation, general ability,\nreasoning ability, truthfulness, factuality, and fluency. We conduct extensive\nexperiments across two unlearning scenarios, two models and six baseline\nmethods and obtain some meaningful findings. We release our benchmark and code\npublicly at http://rwku-bench.github.io for future work.", "published": "2024-06-16 10:47:21", "link": "http://arxiv.org/abs/2406.10890v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "New Solutions on LLM Acceleration, Optimization, and Application", "abstract": "Large Language Models (LLMs) have become extremely potent instruments with\nexceptional capacities for comprehending and producing human-like text in a\nwide range of applications. However, the increasing size and complexity of LLMs\npresent significant challenges in both training and deployment, leading to\nsubstantial computational and storage costs as well as heightened energy\nconsumption. In this paper, we provide a review of recent advancements and\nresearch directions aimed at addressing these challenges and enhancing the\nefficiency of LLM-based systems. We begin by discussing algorithm-level\nacceleration techniques focused on optimizing LLM inference speed and resource\nutilization. We also explore LLM-hardware co-design strategies with a vision to\nimprove system efficiency by tailoring hardware architectures to LLM\nrequirements. Further, we delve into LLM-to-accelerator compilation approaches,\nwhich involve customizing hardware accelerators for efficient LLM deployment.\nFinally, as a case study to leverage LLMs for assisting circuit design, we\nexamine LLM-aided design methodologies for an important task: High-Level\nSynthesis (HLS) functional verification, by creating a new dataset that\ncontains a large number of buggy and bug-free codes, which can be essential for\ntraining LLMs to specialize on HLS verification and debugging. For each aspect\nmentioned above, we begin with a detailed background study, followed by the\npresentation of several novel solutions proposed to overcome specific\nchallenges. We then outline future research directions to drive further\nadvancements. Through these efforts, we aim to pave the way for more efficient\nand scalable deployment of LLMs across a diverse range of applications.", "published": "2024-06-16 11:56:50", "link": "http://arxiv.org/abs/2406.10903v1", "categories": ["cs.LG", "cs.CL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "Multi-LLM QA with Embodied Exploration", "abstract": "Large language models (LLMs) have grown in popularity due to their natural\nlanguage interface and pre trained knowledge, leading to rapidly increasing\nsuccess in question-answering (QA) tasks. More recently, multi-agent systems\nwith LLM-based agents (Multi-LLM) have been utilized increasingly more for QA.\nIn these scenarios, the models may each answer the question and reach a\nconsensus or each model is specialized to answer different domain questions.\nHowever, most prior work dealing with Multi-LLM QA has focused on scenarios\nwhere the models are asked in a zero-shot manner or are given information\nsources to extract the answer. For question answering of an unknown\nenvironment, embodied exploration of the environment is first needed to answer\nthe question. This skill is necessary for personalizing embodied AI to\nenvironments such as households. There is a lack of insight into whether a\nMulti-LLM system can handle question-answering based on observations from\nembodied exploration. In this work, we address this gap by investigating the\nuse of Multi-Embodied LLM Explorers (MELE) for QA in an unknown environment.\nMultiple LLM-based agents independently explore and then answer queries about a\nhousehold environment. We analyze different aggregation methods to generate a\nsingle, final answer for each query: debating, majority voting, and training a\ncentral answer module (CAM). Using CAM, we observe a $46\\%$ higher accuracy\ncompared against the other non-learning-based aggregation methods. We provide\ncode and the query dataset for further research.", "published": "2024-06-16 12:46:40", "link": "http://arxiv.org/abs/2406.10918v5", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Generating Tables from the Parametric Knowledge of Language Models", "abstract": "We explore generating factual and accurate tables from the parametric\nknowledge of large language models (LLMs). While LLMs have demonstrated\nimpressive capabilities in recreating knowledge bases and generating free-form\ntext, we focus on generating structured tabular data, which is crucial in\ndomains like finance and healthcare. We examine the table generation abilities\nof four state-of-the-art LLMs: GPT-3.5, GPT-4, Llama2-13B, and Llama2-70B,\nusing three prompting methods for table generation: (a) full-table, (b)\nrow-by-row; (c) cell-by-cell. For evaluation, we introduce a novel benchmark,\nWikiTabGen which contains 100 curated Wikipedia tables. Tables are further\nprocessed to ensure their factual correctness and manually annotated with short\nnatural language descriptions. Our findings reveal that table generation\nremains a challenge, with GPT-4 reaching the highest accuracy at 19.6%. Our\ndetailed analysis sheds light on how various table properties, such as size,\ntable popularity, and numerical content, influence generation performance. This\nwork highlights the unique challenges in LLM-based table generation and\nprovides a solid evaluation framework for future research. Our code, prompts\nand data are all publicly available:\nhttps://github.com/analysis-bots/WikiTabGen", "published": "2024-06-16 12:55:55", "link": "http://arxiv.org/abs/2406.10922v1", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Investigating Video Reasoning Capability of Large Language Models with\n  Tropes in Movies", "abstract": "Large Language Models (LLMs) have demonstrated effectiveness not only in\nlanguage tasks but also in video reasoning. This paper introduces a novel\ndataset, Tropes in Movies (TiM), designed as a testbed for exploring two\ncritical yet previously overlooked video reasoning skills: (1) Abstract\nPerception: understanding and tokenizing abstract concepts in videos, and (2)\nLong-range Compositional Reasoning: planning and integrating intermediate\nreasoning steps for understanding long-range videos with numerous frames.\nUtilizing tropes from movie storytelling, TiM evaluates the reasoning\ncapabilities of state-of-the-art LLM-based approaches. Our experiments show\nthat current methods, including Captioner-Reasoner, Large Multimodal Model\nInstruction Fine-tuning, and Visual Programming, only marginally outperform a\nrandom baseline when tackling the challenges of Abstract Perception and\nLong-range Compositional Reasoning. To address these deficiencies, we propose\nFace-Enhanced Viper of Role Interactions (FEVoRI) and Context Query Reduction\n(ConQueR), which enhance Visual Programming by fostering role interaction\nawareness and progressively refining movie contexts and trope queries during\nreasoning processes, significantly improving performance by 15 F1 points.\nHowever, this performance still lags behind human levels (40 vs. 65 F1).\nAdditionally, we introduce a new protocol to evaluate the necessity of Abstract\nPerception and Long-range Compositional Reasoning for task resolution. This is\ndone by analyzing the code generated through Visual Programming using an\nAbstract Syntax Tree (AST), thereby confirming the increased complexity of TiM.\nThe dataset and code are available at: https://ander1119.github.io/TiM", "published": "2024-06-16 12:58:31", "link": "http://arxiv.org/abs/2406.10923v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Understanding Understanding: A Pragmatic Framework Motivated by Large\n  Language Models", "abstract": "Motivated by the rapid ascent of Large Language Models (LLMs) and debates\nabout the extent to which they possess human-level qualities, we propose a\nframework for testing whether any agent (be it a machine or a human)\nunderstands a subject matter. In Turing-test fashion, the framework is based\nsolely on the agent's performance, and specifically on how well it answers\nquestions. Elements of the framework include circumscribing the set of\nquestions (the \"scope of understanding\"), requiring general competence\n(\"passing grade\"), avoiding \"ridiculous answers\", but still allowing wrong and\n\"I don't know\" answers to some questions. Reaching certainty about these\nconditions requires exhaustive testing of the questions which is impossible for\nnontrivial scopes, but we show how high confidence can be achieved via random\nsampling and the application of probabilistic confidence bounds. We also show\nthat accompanying answers with explanations can improve the sample complexity\nrequired to achieve acceptable bounds, because an explanation of an answer\nimplies the ability to answer many similar questions. According to our\nframework, current LLMs cannot be said to understand nontrivial domains, but as\nthe framework provides a practical recipe for testing understanding, it thus\nalso constitutes a tool for building AI agents that do understand.", "published": "2024-06-16 13:37:08", "link": "http://arxiv.org/abs/2406.10937v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "City-LEO: Toward Transparent City Management Using LLM with End-to-End\n  Optimization", "abstract": "Existing operations research (OR) models and tools play indispensable roles\nin smart-city operations, yet their practical implementation is limited by the\ncomplexity of modeling and deficiencies in optimization proficiency. To\ngenerate more relevant and accurate solutions to users' requirements, we\npropose a large language model (LLM)-based agent (\"City-LEO\") that enhances the\nefficiency and transparency of city management through conversational\ninteractions. Specifically, to accommodate diverse users' requirements and\nenhance computational tractability, City-LEO leverages LLM's logical reasoning\ncapabilities on prior knowledge to scope down large-scale optimization problems\nefficiently. In the human-like decision process, City-LEO also incorporates\nEnd-to-end (E2E) model to synergize the prediction and optimization. The E2E\nframework be conducive to coping with environmental uncertainties and involving\nmore query-relevant features, and then facilitates transparent and\ninterpretable decision-making process. In case study, we employ City-LEO in the\noperations management of e-bike sharing (EBS) system. The numerical results\ndemonstrate that City-LEO has superior performance when benchmarks against the\nfull-scale optimization problem. With less computational time, City-LEO\ngenerates more satisfactory and relevant solutions to the users' requirements,\nand achieves lower global suboptimality without significantly compromising\naccuracy. In a broader sense, our proposed agent offers promise to develop\nLLM-embedded OR tools for smart-city operations management.", "published": "2024-06-16 14:25:08", "link": "http://arxiv.org/abs/2406.10958v2", "categories": ["math.OC", "cs.CL", "cs.MA"], "primary_category": "math.OC"}
{"title": "Promoting Data and Model Privacy in Federated Learning through Quantized\n  LoRA", "abstract": "Conventional federated learning primarily aims to secure the privacy of data\ndistributed across multiple edge devices, with the global model dispatched to\nedge devices for parameter updates during the learning process. However, the\ndevelopment of large language models (LLMs) requires substantial data and\ncomputational resources, rendering them valuable intellectual properties for\ntheir developers and owners. To establish a mechanism that protects both data\nand model privacy in a federated learning context, we introduce a method that\njust needs to distribute a quantized version of the model's parameters during\ntraining. This method enables accurate gradient estimations for parameter\nupdates while preventing clients from accessing a model whose performance is\ncomparable to the centrally hosted one. Moreover, we combine this quantization\nstrategy with LoRA, a popular and parameter-efficient fine-tuning method, to\nsignificantly reduce communication costs in federated learning. The proposed\nframework, named \\textsc{FedLPP}, successfully ensures both data and model\nprivacy in the federated learning context. Additionally, the learned central\nmodel exhibits good generalization and can be trained in a resource-efficient\nmanner.", "published": "2024-06-16 15:23:07", "link": "http://arxiv.org/abs/2406.10976v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "CoSTA: Code-Switched Speech Translation using Aligned Speech-Text\n  Interleaving", "abstract": "Code-switching is a widely prevalent linguistic phenomenon in multilingual\nsocieties like India. Building speech-to-text models for code-switched speech\nis challenging due to limited availability of datasets. In this work, we focus\non the problem of spoken translation (ST) of code-switched speech in Indian\nlanguages to English text. We present a new end-to-end model architecture COSTA\nthat scaffolds on pretrained automatic speech recognition (ASR) and machine\ntranslation (MT) modules (that are more widely available for many languages).\nSpeech and ASR text representations are fused using an aligned interleaving\nscheme and are fed further as input to a pretrained MT module; the whole\npipeline is then trained end-to-end for spoken translation using synthetically\ncreated ST data. We also release a new evaluation benchmark for code-switched\nBengali-English, Hindi-English, Marathi-English and Telugu- English speech to\nEnglish text. COSTA significantly outperforms many competitive cascaded and\nend-to-end multimodal baselines by up to 3.5 BLEU points.", "published": "2024-06-16 16:10:51", "link": "http://arxiv.org/abs/2406.10993v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Data Shapley in One Training Run", "abstract": "Data Shapley provides a principled framework for attributing data's\ncontribution within machine learning contexts. However, existing approaches\nrequire re-training models on different data subsets, which is computationally\nintensive, foreclosing their application to large-scale models. Furthermore,\nthey produce the same attribution score for any models produced by running the\nlearning algorithm, meaning they cannot perform targeted attribution towards a\nspecific model obtained from a single run of the algorithm. This paper\nintroduces In-Run Data Shapley, which addresses these limitations by offering\nscalable data attribution for a target model of interest. In its most efficient\nimplementation, our technique incurs negligible additional runtime compared to\nstandard model training. This dramatic efficiency improvement makes it possible\nto perform data attribution for the foundation model pretraining stage for the\nfirst time. We present several case studies that offer fresh insights into\npretraining data's contribution and discuss their implications for copyright in\ngenerative AI and pretraining data curation.", "published": "2024-06-16 17:09:24", "link": "http://arxiv.org/abs/2406.11011v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Large Language Models for Dysfluency Detection in Stuttered Speech", "abstract": "Accurately detecting dysfluencies in spoken language can help to improve the\nperformance of automatic speech and language processing components and support\nthe development of more inclusive speech and language technologies. Inspired by\nthe recent trend towards the deployment of large language models (LLMs) as\nuniversal learners and processors of non-lexical inputs, such as audio and\nvideo, we approach the task of multi-label dysfluency detection as a language\nmodeling problem. We present hypotheses candidates generated with an automatic\nspeech recognition system and acoustic representations extracted from an audio\nencoder model to an LLM, and finetune the system to predict dysfluency labels\non three datasets containing English and German stuttered speech. The\nexperimental results show that our system effectively combines acoustic and\nlexical information and achieves competitive results on the multi-label\nstuttering detection task.", "published": "2024-06-16 17:51:22", "link": "http://arxiv.org/abs/2406.11025v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Evaluating the Performance of Large Language Models via Debates", "abstract": "Large Language Models (LLMs) are rapidly evolving and impacting various\nfields, necessitating the development of effective methods to evaluate and\ncompare their performance. Most current approaches for performance evaluation\nare either based on fixed, domain-specific questions that lack the flexibility\nrequired in many real-world applications, or rely on human input, making them\nunscalable. To address these issues, we propose an automated benchmarking\nframework based on debates between LLMs, judged by another LLM. This method\nassesses not only domain knowledge, but also skills such as argumentative\nreasoning and inconsistency recognition. We evaluate the performance of various\nstate-of-the-art LLMs using the debate framework and achieve rankings that\nalign closely with popular rankings based on human input, eliminating the need\nfor costly human crowdsourcing.", "published": "2024-06-16 19:02:31", "link": "http://arxiv.org/abs/2406.11044v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WildVision: Evaluating Vision-Language Models in the Wild with Human\n  Preferences", "abstract": "Recent breakthroughs in vision-language models (VLMs) emphasize the necessity\nof benchmarking human preferences in real-world multimodal interactions. To\naddress this gap, we launched WildVision-Arena (WV-Arena), an online platform\nthat collects human preferences to evaluate VLMs. We curated WV-Bench by\nselecting 500 high-quality samples from 8,000 user submissions in WV-Arena.\nWV-Bench uses GPT-4 as the judge to compare each VLM with Claude-3-Sonnet,\nachieving a Spearman correlation of 0.94 with the WV-Arena Elo. This\nsignificantly outperforms other benchmarks like MMVet, MMMU, and MMStar.\n  Our comprehensive analysis of 20K real-world interactions reveals important\ninsights into the failure cases of top-performing VLMs. For example, we find\nthat although GPT-4V surpasses many other models like Reka-Flash, Opus, and\nYi-VL-Plus in simple visual recognition and reasoning tasks, it still faces\nchallenges with subtle contextual cues, spatial reasoning, visual imagination,\nand expert domain knowledge. Additionally, current VLMs exhibit issues with\nhallucinations and safety when intentionally provoked. We are releasing our\nchat and feedback data to further advance research in the field of VLMs.", "published": "2024-06-16 20:53:25", "link": "http://arxiv.org/abs/2406.11069v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "DP-MemArc: Differential Privacy Transfer Learning for Memory Efficient\n  Language Models", "abstract": "Large language models have repeatedly shown outstanding performance across\ndiverse applications. However, deploying these models can inadvertently risk\nuser privacy. The significant memory demands during training pose a major\nchallenge in terms of resource consumption. This substantial size places a\nheavy load on memory resources, raising considerable practical concerns. In\nthis paper, we introduce DP-MemArc, a novel training framework aimed at\nreducing the memory costs of large language models while emphasizing the\nprotection of user data privacy. DP-MemArc incorporates side network or\nreversible network designs to support a variety of differential privacy\nmemory-efficient fine-tuning schemes. Our approach not only achieves about 2.5\ntimes in memory optimization but also ensures robust privacy protection,\nkeeping user data secure and confidential. Extensive experiments have\ndemonstrated that DP-MemArc effectively provides differential privacy-efficient\nfine-tuning across different task scenarios.", "published": "2024-06-16 22:11:41", "link": "http://arxiv.org/abs/2406.11087v5", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "A Notion of Complexity for Theory of Mind via Discrete World Models", "abstract": "Theory of Mind (ToM) can be used to assess the capabilities of Large Language\nModels (LLMs) in complex scenarios where social reasoning is required. While\nthe research community has proposed many ToM benchmarks, their hardness varies\ngreatly, and their complexity is not well defined. This work proposes a\nframework inspired by cognitive load theory to measure the complexity of ToM\ntasks. We quantify a problem's complexity as the number of states necessary to\nsolve it correctly. Our complexity measure also accounts for spurious states of\na ToM problem designed to make it apparently harder. We use our method to\nassess the complexity of five widely adopted ToM benchmarks. On top of this\nframework, we design a prompting technique that augments the information\navailable to a model with a description of how the environment changes with the\nagents' interactions. We name this technique Discrete World Models (DWM) and\nshow how it elicits superior performance on ToM tasks.", "published": "2024-06-16 16:46:55", "link": "http://arxiv.org/abs/2406.11911v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Automatic Speech Recognition for Biomedical Data in Bengali Language", "abstract": "This paper presents the development of a prototype Automatic Speech\nRecognition (ASR) system specifically designed for Bengali biomedical data.\nRecent advancements in Bengali ASR are encouraging, but a lack of\ndomain-specific data limits the creation of practical healthcare ASR models.\nThis project bridges this gap by developing an ASR system tailored for Bengali\nmedical terms like symptoms, severity levels, and diseases, encompassing two\nmajor dialects: Bengali and Sylheti. We train and evaluate two popular ASR\nframeworks on a comprehensive 46-hour Bengali medical corpus. Our core\nobjective is to create deployable health-domain ASR systems for digital health\napplications, ultimately increasing accessibility for non-technical users in\nthe healthcare sector.", "published": "2024-06-16 10:49:21", "link": "http://arxiv.org/abs/2406.12931v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "WundtGPT: Shaping Large Language Models To Be An Empathetic, Proactive\n  Psychologist", "abstract": "Large language models (LLMs) are raging over the medical domain, and their\nmomentum has carried over into the mental health domain, leading to the\nemergence of few mental health LLMs. Although such mental health LLMs could\nprovide reasonable suggestions for psychological counseling, how to develop an\nauthentic and effective doctor-patient relationship (DPR) through LLMs is still\nan important problem. To fill this gap, we dissect DPR into two key attributes,\ni.e., the psychologist's empathy and proactive guidance. We thus present\nWundtGPT, an empathetic and proactive mental health large language model that\nis acquired by fine-tuning it with instruction and real conversation between\npsychologists and patients. It is designed to assist psychologists in diagnosis\nand help patients who are reluctant to communicate face-to-face understand\ntheir psychological conditions. Its uniqueness lies in that it could not only\npose purposeful questions to guide patients in detailing their symptoms but\nalso offer warm emotional reassurance. In particular, WundtGPT incorporates\nCollection of Questions, Chain of Psychodiagnosis, and Empathy Constraints into\na comprehensive prompt for eliciting LLMs' questions and diagnoses.\nAdditionally, WundtGPT proposes a reward model to promote alignment with\nempathetic mental health professionals, which encompasses two key factors:\ncognitive empathy and emotional empathy. We offer a comprehensive evaluation of\nour proposed model. Based on these outcomes, we further conduct the manual\nevaluation based on proactivity, effectiveness, professionalism and coherence.\nWe notice that WundtGPT can offer professional and effective consultation. The\nmodel is available at huggingface.", "published": "2024-06-16 16:06:38", "link": "http://arxiv.org/abs/2406.15474v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "People will agree what I think: Investigating LLM's False Consensus\n  Effect", "abstract": "Large Language Models (LLMs) have been recently adopted in interactive\nsystems requiring communication. As the false belief in a model can harm the\nusability of such systems, LLMs should not have cognitive biases that humans\nhave. Psychologists especially focus on the False Consensus Effect (FCE), a\ncognitive bias where individuals overestimate the extent to which others share\ntheir beliefs or behaviors, because FCE can distract smooth communication by\nposing false beliefs. However, previous studies have less examined FCE in LLMs\nthoroughly, which needs more consideration of confounding biases, general\nsituations, and prompt changes. Therefore, in this paper, we conduct two\nstudies to examine the FCE phenomenon in LLMs. In Study 1, we investigate\nwhether LLMs have FCE. In Study 2, we explore how various prompting styles\naffect the demonstration of FCE. As a result of these studies, we identified\nthat popular LLMs have FCE. Also, the result specifies the conditions when FCE\nbecomes more or less prevalent compared to normal usage.", "published": "2024-06-16 03:29:28", "link": "http://arxiv.org/abs/2407.12007v2", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Revisiting and Improving Scoring Fusion for Spoofing-aware Speaker\n  Verification Using Compositional Data Analysis", "abstract": "Fusing outputs from automatic speaker verification (ASV) and spoofing\ncountermeasure (CM) is expected to make an integrated system robust to\nzero-effort imposters and synthesized spoofing attacks. Many score-level fusion\nmethods have been proposed, but many remain heuristic. This paper revisits\nscore-level fusion using tools from decision theory and presents three main\nfindings. First, fusion by summing the ASV and CM scores can be interpreted on\nthe basis of compositional data analysis, and score calibration before fusion\nis essential. Second, the interpretation leads to an improved fusion method\nthat linearly combines the log-likelihood ratios of ASV and CM. However, as the\nthird finding reveals, this linear combination is inferior to a non-linear one\nin making optimal decisions. The outcomes of these findings, namely, the score\ncalibration before fusion, improved linear fusion, and better non-linear\nfusion, were found to be effective on the SASV challenge database.", "published": "2024-06-16 08:10:23", "link": "http://arxiv.org/abs/2406.10836v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-Scale Accent Modeling and Disentangling for Multi-Speaker\n  Multi-Accent Text-to-Speech Synthesis", "abstract": "Generating speech across different accents while preserving speaker identity\nis crucial for various real-world applications. However, accurately and\nindependently modeling both speaker and accent characteristics in\ntext-to-speech (TTS) systems is challenging due to the complex variations of\naccents and the inherent entanglement between speaker and accent identities. In\nthis paper, we propose a novel approach for multi-speaker multi-accent TTS\nsynthesis that aims to synthesize speech for multiple speakers, each with\nvarious accents. Our approach employs a multi-scale accent modeling strategy to\naddress accent variations on different levels. Specifically, we introduce both\nglobal (utterance level) and local (phoneme level) accent modeling to capture\noverall accent characteristics within an utterance and fine-grained accent\nvariations across phonemes, respectively. To enable independent control of\nspeakers and accents, we use the speaker embedding to represent speaker\nidentity and achieve speaker-independent accent control through speaker\ndisentanglement within the multi-scale accent modeling. Additionally, we\npresent a local accent prediction model that enables our system to generate\naccented speech directly from phoneme inputs. We conduct extensive experiments\non an English accented speech corpus. Experimental results demonstrate that our\nproposed system outperforms baseline systems in terms of speech quality and\naccent rendering for generating multi-speaker multi-accent speech. Ablation\nstudies further validate the effectiveness of different components in our\nproposed system.", "published": "2024-06-16 08:34:22", "link": "http://arxiv.org/abs/2406.10844v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SingMOS: An extensive Open-Source Singing Voice Dataset for MOS\n  Prediction", "abstract": "In speech generation tasks, human subjective ratings, usually referred to as\nthe opinion score, are considered the \"gold standard\" for speech quality\nevaluation, with the mean opinion score (MOS) serving as the primary evaluation\nmetric. Due to the high cost of human annotation, several MOS prediction\nsystems have emerged in the speech domain, demonstrating good performance.\nThese MOS prediction models are trained using annotations from previous\nspeech-related challenges. However, compared to the speech domain, the singing\ndomain faces data scarcity and stricter copyright protections, leading to a\nlack of high-quality MOS-annotated datasets for singing. To address this, we\npropose SingMOS, a high-quality and diverse MOS dataset for singing, covering a\nrange of Chinese and Japanese datasets. These synthesized vocals are generated\nusing state-of-the-art models in singing synthesis, conversion, or resynthesis\ntasks and are rated by professional annotators alongside real vocals. Data\nanalysis demonstrates the diversity and reliability of our dataset.\nAdditionally, we conduct further exploration on SingMOS, providing insights for\nsinging MOS prediction and guidance for the continued expansion of SingMOS.", "published": "2024-06-16 12:15:28", "link": "http://arxiv.org/abs/2406.10911v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Joint Audio and Symbolic Conditioning for Temporally Controlled\n  Text-to-Music Generation", "abstract": "We present JASCO, a temporally controlled text-to-music generation model\nutilizing both symbolic and audio-based conditions. JASCO can generate\nhigh-quality music samples conditioned on global text descriptions along with\nfine-grained local controls. JASCO is based on the Flow Matching modeling\nparadigm together with a novel conditioning method. This allows music\ngeneration controlled both locally (e.g., chords) and globally (text\ndescription). Specifically, we apply information bottleneck layers in\nconjunction with temporal blurring to extract relevant information with respect\nto specific controls. This allows the incorporation of both symbolic and\naudio-based conditions in the same text-to-music model. We experiment with\nvarious symbolic control signals (e.g., chords, melody), as well as with audio\nrepresentations (e.g., separated drum tracks, full-mix). We evaluate JASCO\nconsidering both generation quality and condition adherence, using both\nobjective metrics and human studies. Results suggest that JASCO is comparable\nto the evaluated baselines considering generation quality while allowing\nsignificantly better and more versatile controls over the generated music.\nSamples are available on our demo page\nhttps://pages.cs.huji.ac.il/adiyoss-lab/JASCO.", "published": "2024-06-16 15:06:06", "link": "http://arxiv.org/abs/2406.10970v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Outlier Reduction with Gated Attention for Improved Post-training\n  Quantization in Large Sequence-to-sequence Speech Foundation Models", "abstract": "This paper explores the improvement of post-training quantization (PTQ) after\nknowledge distillation in the Whisper speech foundation model family. We\naddress the challenge of outliers in weights and activation tensors, known to\nimpede quantization quality in transformer-based language and vision models.\nExtending this observation to Whisper, we demonstrate that these outliers are\nalso present when transformer-based models are trained to perform automatic\nspeech recognition, necessitating mitigation strategies for PTQ. We show that\noutliers can be reduced by a recently proposed gating mechanism in the\nattention blocks of the student model, enabling effective 8-bit quantization,\nand lower word error rates compared to student models without the gating\nmechanism in place.", "published": "2024-06-16 17:32:45", "link": "http://arxiv.org/abs/2406.11022v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NAST: Noise Aware Speech Tokenization for Speech Language Models", "abstract": "Speech tokenization is the task of representing speech signals as a sequence\nof discrete units. Such representations can be later used for various\ndownstream tasks including automatic speech recognition, text-to-speech, etc.\nMore relevant to this study, such representation serves as the basis of Speech\nLanguage Models. In this work, we tackle the task of speech tokenization under\nthe noisy setup and present NAST: Noise Aware Speech Tokenization for Speech\nLanguage Models. NAST is composed of three main components: (i) a predictor;\n(ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of\nNAST considering several spoken language modeling tasks and show that NAST is\nsuperior to the evaluated baselines across all setups. Lastly, we analyze NAST\nand show its disentanglement properties and robustness to signal variations in\nthe form of noise, reverberation, pitch-shift, and time-stretch. Code and\npre-trained models are available at https://github.com/ShovalMessica/NAST.", "published": "2024-06-16 18:20:45", "link": "http://arxiv.org/abs/2406.11037v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Continual Test-time Adaptation for End-to-end Speech Recognition on\n  Noisy Speech", "abstract": "Deep Learning-based end-to-end Automatic Speech Recognition (ASR) has made\nsignificant strides but still struggles with performance on out-of-domain\nsamples due to domain shifts in real-world scenarios. Test-Time Adaptation\n(TTA) methods address this issue by adapting models using test samples at\ninference time. However, current ASR TTA methods have largely focused on\nnon-continual TTA, which limits cross-sample knowledge learning compared to\ncontinual TTA. In this work, we first propose a Fast-slow TTA framework for ASR\nthat leverages the advantage of continual and non-continual TTA. Following this\nframework, we introduce Dynamic SUTA (DSUTA), an entropy-minimization-based\ncontinual TTA method for ASR. To enhance DSUTA robustness for time-varying\ndata, we design a dynamic reset strategy to automatically detect domain shifts\nand reset the model, making it more effective at handling multi-domain data.\nOur method demonstrates superior performance on various noisy ASR datasets,\noutperforming both non-continual and continual TTA baselines while maintaining\nrobustness to domain changes without requiring domain boundary information.", "published": "2024-06-16 20:41:03", "link": "http://arxiv.org/abs/2406.11064v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Imperceptible Rhythm Backdoor Attacks: Exploring Rhythm Transformation\n  for Embedding Undetectable Vulnerabilities on Speech Recognition", "abstract": "Speech recognition is an essential start ring of human-computer interaction,\nand recently, deep learning models have achieved excellent success in this\ntask. However, when the model training and private data provider are always\nseparated, some security threats that make deep neural networks (DNNs) abnormal\ndeserve to be researched. In recent years, the typical backdoor attacks have\nbeen researched in speech recognition systems. The existing backdoor methods\nare based on data poisoning. The attacker adds some incorporated changes to\nbenign speech spectrograms or changes the speech components, such as pitch and\ntimbre. As a result, the poisoned data can be detected by human hearing or\nautomatic deep algorithms. To improve the stealthiness of data poisoning, we\npropose a non-neural and fast algorithm called Random Spectrogram Rhythm\nTransformation (RSRT) in this paper. The algorithm combines four steps to\ngenerate stealthy poisoned utterances. From the perspective of rhythm component\ntransformation, our proposed trigger stretches or squeezes the mel spectrograms\nand recovers them back to signals. The operation keeps timbre and content\nunchanged for good stealthiness. Our experiments are conducted on two kinds of\nspeech recognition tasks, including testing the stealthiness of poisoned\nsamples by speaker verification and automatic speech recognition. The results\nshow that our method has excellent effectiveness and stealthiness. The rhythm\ntrigger needs a low poisoning rate and gets a very high attack success rate.", "published": "2024-06-16 13:29:21", "link": "http://arxiv.org/abs/2406.10932v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Robust Channel Learning for Large-Scale Radio Speaker Verification", "abstract": "Recent research in speaker verification has increasingly focused on achieving\nrobust and reliable recognition under challenging channel conditions and noisy\nenvironments. Identifying speakers in radio communications is particularly\ndifficult due to inherent limitations such as constrained bandwidth and\npervasive noise interference. To address this issue, we present a Channel\nRobust Speaker Learning (CRSL) framework that enhances the robustness of the\ncurrent speaker verification pipeline, considering data source, data\naugmentation, and the efficiency of model transfer processes. Our framework\nintroduces an augmentation module that mitigates bandwidth variations in radio\nspeech datasets by manipulating the bandwidth of training inputs. It also\naddresses unknown noise by introducing noise within the manifold space.\nAdditionally, we propose an efficient fine-tuning method that reduces the need\nfor extensive additional training time and large amounts of data. Moreover, we\ndevelop a toolkit for assembling a large-scale radio speech corpus and\nestablish a benchmark specifically tailored for radio scenario speaker\nverification studies. Experimental results demonstrate that our proposed\nmethodology effectively enhances performance and mitigates degradation caused\nby radio transmission in speaker verification tasks. The code will be available\non Github.", "published": "2024-06-16 14:17:57", "link": "http://arxiv.org/abs/2406.10956v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SPEAR: Receiver-to-Receiver Acoustic Neural Warping Field", "abstract": "We present SPEAR, a continuous receiver-to-receiver acoustic neural warping\nfield for spatial acoustic effects prediction in an acoustic 3D space with a\nsingle stationary audio source. Unlike traditional source-to-receiver modelling\nmethods that require prior space acoustic properties knowledge to rigorously\nmodel audio propagation from source to receiver, we propose to predict by\nwarping the spatial acoustic effects from one reference receiver position to\nanother target receiver position, so that the warped audio essentially\naccommodates all spatial acoustic effects belonging to the target position.\nSPEAR can be trained in a data much more readily accessible manner, in which we\nsimply ask two robots to independently record spatial audio at different\npositions. We further theoretically prove the universal existence of the\nwarping field if and only if one audio source presents. Three physical\nprinciples are incorporated to guide SPEAR network design, leading to the\nlearned warping field physically meaningful. We demonstrate SPEAR superiority\non both synthetic, photo-realistic and real-world dataset, showing the huge\npotential of SPEAR to various down-stream robotic tasks.", "published": "2024-06-16 16:40:26", "link": "http://arxiv.org/abs/2406.11006v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
