{"title": "A Fully Differentiable Beam Search Decoder", "abstract": "We introduce a new beam search decoder that is fully differentiable, making\nit possible to optimize at training time through the inference procedure. Our\ndecoder allows us to combine models which operate at different granularities\n(e.g. acoustic and language models). It can be used when target sequences are\nnot aligned to input sequences by considering all possible alignments between\nthe two. We demonstrate our approach scales by applying it to speech\nrecognition, jointly training acoustic and word-level language models. The\nsystem is end-to-end, with gradients flowing through the whole architecture\nfrom the word-level transcriptions. Recent research efforts have shown that\ndeep neural networks with attention-based mechanisms are powerful enough to\nsuccessfully train an acoustic model from the final transcription, while\nimplicitly learning a language model. Instead, we show that it is possible to\ndiscriminatively train an acoustic model jointly with an explicit and possibly\npre-trained language model.", "published": "2019-02-16 01:28:12", "link": "http://arxiv.org/abs/1902.06022v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Language Similarities with Dimensionality Reduction Technique", "abstract": "In recent years several novel models were developed to process natural\nlanguage, development of accurate language translation systems have helped us\novercome geographical barriers and communicate ideas effectively. These models\nare developed mostly for a few languages that are widely used while other\nlanguages are ignored. Most of the languages that are spoken share lexical,\nsyntactic and sematic similarity with several other languages and knowing this\ncan help us leverage the existing model to build more specific and accurate\nmodels that can be used for other languages, so here I have explored the idea\nof representing several known popular languages in a lower dimension such that\ntheir similarities can be visualized using simple 2 dimensional plots. This can\neven help us understand newly discovered languages that may not share its\nvocabulary with any of the existing languages.", "published": "2019-02-16 11:27:21", "link": "http://arxiv.org/abs/1902.06092v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CruzAffect at AffCon 2019 Shared Task: A feature-rich approach to\n  characterize happiness", "abstract": "We present our system, CruzAffect, for the CL-Aff Shared Task 2019.\nCruzAffect consists of several types of robust and efficient models for\naffective classification tasks. We utilize both traditional classifiers, such\nas XGBoosted Forest, as well as a deep learning Convolutional Neural Networks\n(CNN) classifier. We explore rich feature sets such as syntactic features,\nemotional features, and profile features, and utilize several sentiment\nlexicons, to discover essential indicators of social involvement and control\nthat a subject might exercise in their happy moments, as described in textual\nsnippets from the HappyDB database. The data comes with a labeled set (10K),\nand a larger unlabeled set (70K). We therefore use supervised methods on the\n10K dataset, and a bootstrapped semi-supervised approach for the 70K. We\nevaluate these models for binary classification of agency and social labels\n(Task 1), as well as multi-class prediction for concepts labels (Task 2). We\nobtain promising results on the held-out data, suggesting that the proposed\nfeature sets effectively represent the data for affective classification tasks.\nWe also build concepts models that discover general themes recurring in happy\nmoments. Our results indicate that generic characteristics are shared between\nthe classes of agency, social and concepts, suggesting it should be possible to\nbuild general models for affective classification tasks.", "published": "2019-02-16 01:54:47", "link": "http://arxiv.org/abs/1902.06024v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "TopicEq: A Joint Topic and Mathematical Equation Model for Scientific\n  Texts", "abstract": "Scientific documents rely on both mathematics and text to communicate ideas.\nInspired by the topical correspondence between mathematical equations and word\ncontexts observed in scientific texts, we propose a novel topic model that\njointly generates mathematical equations and their surrounding text (TopicEq).\nUsing an extension of the correlated topic model, the context is generated from\na mixture of latent topics, and the equation is generated by an RNN that\ndepends on the latent topic activations. To experiment with this model, we\ncreate a corpus of 400K equation-context pairs extracted from a range of\nscientific articles from arXiv, and fit the model using a variational\nautoencoder approach. Experimental results show that this joint model\nsignificantly outperforms existing topic models and equation models for\nscientific texts. Moreover, we qualitatively show that the model effectively\ncaptures the relationship between topics and mathematics, enabling novel\napplications such as topic-aware equation generation, equation topic inference,\nand topic-aware alignment of mathematical symbols and words.", "published": "2019-02-16 03:39:51", "link": "http://arxiv.org/abs/1902.06034v3", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Combination of Domain Knowledge and Deep Learning for Sentiment Analysis\n  of Short and Informal Messages on Social Media", "abstract": "Sentiment analysis has been emerging recently as one of the major natural\nlanguage processing (NLP) tasks in many applications. Especially, as social\nmedia channels (e.g. social networks or forums) have become significant sources\nfor brands to observe user opinions about their products, this task is thus\nincreasingly crucial. However, when applied with real data obtained from social\nmedia, we notice that there is a high volume of short and informal messages\nposted by users on those channels. This kind of data makes the existing works\nsuffer from many difficulties to handle, especially ones using deep learning\napproaches. In this paper, we propose an approach to handle this problem. This\nwork is extended from our previous work, in which we proposed to combine the\ntypical deep learning technique of Convolutional Neural Networks with domain\nknowledge. The combination is used for acquiring additional training data\naugmentation and a more reasonable loss function. In this work, we further\nimprove our architecture by various substantial enhancements, including\nnegation-based data augmentation, transfer learning for word embeddings, the\ncombination of word-level embeddings and character-level embeddings, and using\nmultitask learning technique for attaching domain knowledge rules in the\nlearning process. Those enhancements, specifically aiming to handle short and\ninformal messages, help us to enjoy significant improvement in performance once\nexperimenting on real datasets.", "published": "2019-02-16 06:03:57", "link": "http://arxiv.org/abs/1902.06050v2", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
