{"title": "Deep Active Learning for Named Entity Recognition", "abstract": "Deep learning has yielded state-of-the-art performance on many natural\nlanguage processing tasks including named entity recognition (NER). However,\nthis typically requires large amounts of labeled data. In this work, we\ndemonstrate that the amount of labeled training data can be drastically reduced\nwhen deep learning is combined with active learning. While active learning is\nsample-efficient, it can be computationally expensive since it requires\niterative retraining. To speed this up, we introduce a lightweight architecture\nfor NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and\nword encoders and a long short term memory (LSTM) tag decoder. The model\nachieves nearly state-of-the-art performance on standard datasets for the task\nwhile being computationally much more efficient than best performing models. We\ncarry out incremental active learning, during the training process, and are\nable to nearly match state-of-the-art performance with just 25\\% of the\noriginal training data.", "published": "2017-07-19 03:18:40", "link": "http://arxiv.org/abs/1707.05928v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Thematic Fit with Distributional Feature Overlap", "abstract": "In this paper, we introduce a new distributional method for modeling\npredicate-argument thematic fit judgments. We use a syntax-based DSM to build a\nprototypical representation of verb-specific roles: for every verb, we extract\nthe most salient second order contexts for each of its roles (i.e. the most\nsalient dimensions of typical role fillers), and then we compute thematic fit\nas a weighted overlap between the top features of candidate fillers and role\nprototypes. Our experiments show that our method consistently outperforms a\nbaseline re-implementing a state-of-the-art system, and achieves better or\ncomparable results to those reported in the literature for the other\nunsupervised systems. Moreover, it provides an explicit representation of the\nfeatures characterizing verb-specific semantic roles.", "published": "2017-07-19 07:51:05", "link": "http://arxiv.org/abs/1707.05967v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Argotario: Computational Argumentation Meets Serious Games", "abstract": "An important skill in critical thinking and argumentation is the ability to\nspot and recognize fallacies. Fallacious arguments, omnipresent in\nargumentative discourse, can be deceptive, manipulative, or simply leading to\n`wrong moves' in a discussion. Despite their importance, argumentation scholars\nand NLP researchers with focus on argumentation quality have not yet\ninvestigated fallacies empirically. The nonexistence of resources dealing with\nfallacious argumentation calls for scalable approaches to data acquisition and\nannotation, for which the serious games methodology offers an appealing, yet\nunexplored, alternative. We present Argotario, a serious game that deals with\nfallacies in everyday argumentation. Argotario is a multilingual, open-source,\nplatform-independent application with strong educational aspects, accessible at\nwww.argotario.net.", "published": "2017-07-19 10:08:51", "link": "http://arxiv.org/abs/1707.06002v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Target-Side Inflection in Neural Machine Translation", "abstract": "NMT systems have problems with large vocabulary sizes. Byte-pair encoding\n(BPE) is a popular approach to solving this problem, but while BPE allows the\nsystem to generate any target-side word, it does not enable effective\ngeneralization over the rich vocabulary in morphologically rich languages with\nstrong inflectional phenomena. We introduce a simple approach to overcome this\nproblem by training a system to produce the lemma of a word and its\nmorphologically rich POS tag, which is then followed by a deterministic\ngeneration step. We apply this strategy for English-Czech and English-German\ntranslation scenarios, obtaining improvements in both settings. We furthermore\nshow that the improvement is not due to only adding explicit morphological\ninformation.", "published": "2017-07-19 10:47:22", "link": "http://arxiv.org/abs/1707.06012v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Language Modeling using Densely Connected Recurrent Neural\n  Networks", "abstract": "In this paper, we introduce the novel concept of densely connected layers\ninto recurrent neural networks. We evaluate our proposed architecture on the\nPenn Treebank language modeling task. We show that we can obtain similar\nperplexity scores with six times fewer parameters compared to a standard\nstacked 2-layer LSTM model trained with dropout (Zaremba et al. 2014). In\ncontrast with the current usage of skip connections, we show that densely\nconnecting only a few stacked layers with skip connections already yields\nsignificant perplexity reductions.", "published": "2017-07-19 14:49:35", "link": "http://arxiv.org/abs/1707.06130v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Expect the unexpected: Harnessing Sentence Completion for Sarcasm\n  Detection", "abstract": "The trigram `I love being' is expected to be followed by positive words such\nas `happy'. In a sarcastic sentence, however, the word `ignored' may be\nobserved. The expected and the observed words are, thus, incongruous. We model\nsarcasm detection as the task of detecting incongruity between an observed and\nan expected word. In order to obtain the expected word, we use Context2Vec, a\nsentence completion library based on Bidirectional LSTM. However, since the\nexact word where such an incongruity occurs may not be known in advance, we\npresent two approaches: an All-words approach (which consults sentence\ncompletion for every content word) and an Incongruous words-only approach\n(which consults sentence completion for the 50% most incongruous content\nwords). The approaches outperform reported values for tweets but not for\ndiscussion forum posts. This is likely to be because of redundant consultation\nof sentence completion for discussion forum posts. Therefore, we consider an\noracle case where the exact incongruous word is manually labeled in a corpus\nreported in past work. In this case, the performance is higher than the\nall-words approach. This sets up the promise for using sentence completion for\nsarcasm detection.", "published": "2017-07-19 15:24:20", "link": "http://arxiv.org/abs/1707.06151v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence-level quality estimation by predicting HTER as a\n  multi-component metric", "abstract": "This submission investigates alternative machine learning models for\npredicting the HTER score on the sentence level. Instead of directly predicting\nthe HTER score, we suggest a model that jointly predicts the amount of the 4\ndistinct post-editing operations, which are then used to calculate the HTER\nscore. This also gives the possibility to correct invalid (e.g. negative)\npredicted values prior to the calculation of the HTER score. Without any\nfeature exploration, a multi-layer perceptron with 4 outputs yields small but\nsignificant improvements over the baseline.", "published": "2017-07-19 15:48:27", "link": "http://arxiv.org/abs/1707.06167v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fast and Accurate OOV Decoder on High-Level Features", "abstract": "This work proposes a novel approach to out-of-vocabulary (OOV) keyword search\n(KWS) task. The proposed approach is based on using high-level features from an\nautomatic speech recognition (ASR) system, so called phoneme posterior based\n(PPB) features, for decoding. These features are obtained by calculating\ntime-dependent phoneme posterior probabilities from word lattices, followed by\ntheir smoothing. For the PPB features we developed a special novel very fast,\nsimple and efficient OOV decoder. Experimental results are presented on the\nGeorgian language from the IARPA Babel Program, which was the test language in\nthe OpenKWS 2016 evaluation campaign. The results show that in terms of maximum\nterm weighted value (MTWV) metric and computational speed, for single ASR\nsystems, the proposed approach significantly outperforms the state-of-the-art\napproach based on using in-vocabulary proxies for OOV keywords in the indexed\ndatabase. The comparison of the two OOV KWS approaches on the fusion results of\nthe nine different ASR systems demonstrates that the proposed OOV decoder\noutperforms the proxy-based approach in terms of MTWV metric given the\ncomparable processing speed. Other important advantages of the OOV decoder\ninclude extremely low memory consumption and simplicity of its implementation\nand parameter optimization.", "published": "2017-07-19 17:03:34", "link": "http://arxiv.org/abs/1707.06195v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling in\n  Speech Recognition", "abstract": "Layer normalization is a recently introduced technique for normalizing the\nactivities of neurons in deep neural networks to improve the training speed and\nstability. In this paper, we introduce a new layer normalization technique\ncalled Dynamic Layer Normalization (DLN) for adaptive neural acoustic modeling\nin speech recognition. By dynamically generating the scaling and shifting\nparameters in layer normalization, DLN adapts neural acoustic models to the\nacoustic variability arising from various factors such as speakers, channel\nnoises, and environments. Unlike other adaptive acoustic models, our proposed\napproach does not require additional adaptation data or speaker information\nsuch as i-vectors. Moreover, the model size is fixed as it dynamically\ngenerates adaptation parameters. We apply our proposed DLN to deep\nbidirectional LSTM acoustic models and evaluate them on two benchmark datasets\nfor large vocabulary ASR experiments: WSJ and TED-LIUM release 2. The\nexperimental results show that our DLN improves neural acoustic models in terms\nof transcription accuracy by dynamically adapting to various speakers and\nenvironments.", "published": "2017-07-19 13:04:09", "link": "http://arxiv.org/abs/1707.06065v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Domain Adaptation for Robust Speech Recognition via\n  Variational Autoencoder-Based Data Augmentation", "abstract": "Domain mismatch between training and testing can lead to significant\ndegradation in performance in many machine learning scenarios. Unfortunately,\nthis is not a rare situation for automatic speech recognition deployments in\nreal-world applications. Research on robust speech recognition can be regarded\nas trying to overcome this domain mismatch issue. In this paper, we address the\nunsupervised domain adaptation problem for robust speech recognition, where\nboth source and target domain speech are presented, but word transcripts are\nonly available for the source domain speech. We present novel\naugmentation-based methods that transform speech in a way that does not change\nthe transcripts. Specifically, we first train a variational autoencoder on both\nsource and target domain data (without supervision) to learn a latent\nrepresentation of speech. We then transform nuisance attributes of speech that\nare irrelevant to recognition by modifying the latent representations, in order\nto augment labeled training data with additional data whose distribution is\nmore similar to the target domain. The proposed method is evaluated on the\nCHiME-4 dataset and reduces the absolute word error rate (WER) by as much as\n35% compared to the non-adapted baseline.", "published": "2017-07-19 19:10:44", "link": "http://arxiv.org/abs/1707.06265v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reward-Balancing for Statistical Spoken Dialogue Systems using\n  Multi-objective Reinforcement Learning", "abstract": "Reinforcement learning is widely used for dialogue policy optimization where\nthe reward function often consists of more than one component, e.g., the\ndialogue success and the dialogue length. In this work, we propose a structured\nmethod for finding a good balance between these components by searching for the\noptimal reward component weighting. To render this search feasible, we use\nmulti-objective reinforcement learning to significantly reduce the number of\ntraining dialogues required. We apply our proposed method to find optimized\ncomponent weights for six domains and compare them to a default baseline.", "published": "2017-07-19 21:21:03", "link": "http://arxiv.org/abs/1707.06299v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Learning Visually Grounded Sentence Representations", "abstract": "We introduce a variety of models, trained on a supervised image captioning\ncorpus to predict the image features for a given caption, to perform sentence\nrepresentation grounding. We train a grounded sentence encoder that achieves\ngood performance on COCO caption and image retrieval and subsequently show that\nthis encoder can successfully be transferred to various NLP tasks, with\nimproved performance over text-only models. Lastly, we analyze the contribution\nof grounding, and show that word embeddings learned by this system outperform\nnon-grounded ones.", "published": "2017-07-19 23:12:57", "link": "http://arxiv.org/abs/1707.06320v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Language Transfer of Audio Word2Vec: Learning Audio Segment\n  Representations without Target Language Data", "abstract": "Audio Word2Vec offers vector representations of fixed dimensionality for\nvariable-length audio segments using Sequence-to-sequence Autoencoder (SA).\nThese vector representations are shown to describe the sequential phonetic\nstructures of the audio segments to a good degree, with real world applications\nsuch as query-by-example Spoken Term Detection (STD). This paper examines the\ncapability of language transfer of Audio Word2Vec. We train SA from one\nlanguage (source language) and use it to extract the vector representation of\nthe audio segments of another language (target language). We found that SA can\nstill catch phonetic structure from the audio segments of the target language\nif the source and target languages are similar. In query-by-example STD, we\nobtain the vector representations from the SA learned from a large amount of\nsource language data, and found them surpass the representations from naive\nencoder and SA directly learned from a small amount of target language data.\nThe result shows that it is possible to learn Audio Word2Vec model from\nhigh-resource languages and use it on low-resource languages. This further\nexpands the usability of Audio Word2Vec.", "published": "2017-07-19 10:54:00", "link": "http://arxiv.org/abs/1707.06519v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Metrical-accent Aware Vocal Onset Detection in Polyphonic Audio", "abstract": "The goal of this study is the automatic detection of onsets of the singing\nvoice in polyphonic audio recordings. Starting with a hypothesis that the\nknowledge of the current position in a metrical cycle (i.e. metrical accent)\ncan improve the accuracy of vocal note onset detection, we propose a novel\nprobabilistic model to jointly track beats and vocal note onsets. The proposed\nmodel extends a state of the art model for beat and meter tracking, in which\na-priori probability of a note at a specific metrical accent interacts with the\nprobability of observing a vocal note onset. We carry out an evaluation on a\nvaried collection of multi-instrument datasets from two music traditions\n(English popular music and Turkish makam) with different types of metrical\ncycles and singing styles. Results confirm that the proposed model reasonably\nimproves vocal note onset detection accuracy compared to a baseline model that\ndoes not take metrical position into account.", "published": "2017-07-19 15:39:09", "link": "http://arxiv.org/abs/1707.06163v1", "categories": ["cs.SD", "cs.CL", "cs.MM"], "primary_category": "cs.SD"}
{"title": "Crowdsourcing Multiple Choice Science Questions", "abstract": "We present a novel method for obtaining high-quality, domain-targeted\nmultiple choice questions from crowd workers. Generating these questions can be\ndifficult without trading away originality, relevance or diversity in the\nanswer options. Our method addresses these problems by leveraging a large\ncorpus of domain-specific text and a small set of existing questions. It\nproduces model suggestions for document selection and answer distractor choice\nwhich aid the human question generation process. With this method we have\nassembled SciQ, a dataset of 13.7K multiple choice science exam questions\n(Dataset available at http://allenai.org/data.html). We demonstrate that the\nmethod produces in-domain questions by providing an analysis of this new\ndataset and by showing that humans cannot distinguish the crowdsourced\nquestions from original questions. When using SciQ as additional training data\nto existing questions, we observe accuracy improvements on real science exams.", "published": "2017-07-19 17:28:46", "link": "http://arxiv.org/abs/1707.06209v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.HC"}
{"title": "The Role of Conversation Context for Sarcasm Detection in Online\n  Interactions", "abstract": "Computational models for sarcasm detection have often relied on the content\nof utterances in isolation. However, speaker's sarcastic intent is not always\nobvious without additional context. Focusing on social media discussions, we\ninvestigate two issues: (1) does modeling of conversation context help in\nsarcasm detection and (2) can we understand what part of conversation context\ntriggered the sarcastic reply. To address the first issue, we investigate\nseveral types of Long Short-Term Memory (LSTM) networks that can model both the\nconversation context and the sarcastic response. We show that the conditional\nLSTM network (Rocktaschel et al., 2015) and LSTM networks with sentence level\nattention on context and response outperform the LSTM model that reads only the\nresponse. To address the second issue, we present a qualitative analysis of\nattention weights produced by the LSTM models with attention and discuss the\nresults compared with human performance on the task.", "published": "2017-07-19 01:21:26", "link": "http://arxiv.org/abs/1707.06226v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Single-Channel Multi-talker Speech Recognition with Permutation\n  Invariant Training", "abstract": "Although great progresses have been made in automatic speech recognition\n(ASR), significant performance degradation is still observed when recognizing\nmulti-talker mixed speech. In this paper, we propose and evaluate several\narchitectures to address this problem under the assumption that only a single\nchannel of mixed signal is available. Our technique extends permutation\ninvariant training (PIT) by introducing the front-end feature separation module\nwith the minimum mean square error (MSE) criterion and the back-end recognition\nmodule with the minimum cross entropy (CE) criterion. More specifically, during\ntraining we compute the average MSE or CE over the whole utterance for each\npossible utterance-level output-target assignment, pick the one with the\nminimum MSE or CE, and optimize for that assignment. This strategy elegantly\nsolves the label permutation problem observed in the deep learning based\nmulti-talker mixed speech separation and recognition systems. The proposed\narchitectures are evaluated and compared on an artificially mixed AMI dataset\nwith both two- and three-talker mixed speech. The experimental results indicate\nthat our proposed architectures can cut the word error rate (WER) by 45.0% and\n25.0% relatively against the state-of-the-art single-talker speech recognition\nsystem across all speakers when their energies are comparable, for two- and\nthree-talker mixed speech, respectively. To our knowledge, this is the first\nwork on the multi-talker mixed speech recognition on the challenging\nspeaker-independent spontaneous large vocabulary continuous speech task.", "published": "2017-07-19 03:48:54", "link": "http://arxiv.org/abs/1707.06527v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS", "I.2.7"], "primary_category": "cs.SD"}
{"title": "Stock Prediction: a method based on extraction of news features and\n  recurrent neural networks", "abstract": "This paper proposed a method for stock prediction. In terms of feature\nextraction, we extract the features of stock-related news besides stock prices.\nWe first select some seed words based on experience which are the symbols of\ngood news and bad news. Then we propose an optimization method and calculate\nthe positive polar of all words. After that, we construct the features of news\nbased on the positive polar of their words. In consideration of sequential\nstock prices and continuous news effects, we propose a recurrent neural network\nmodel to help predict stock prices. Compared to SVM classifier with price\nfeatures, we find our proposed method has an over 5% improvement on stock\nprediction accuracy in experiments.", "published": "2017-07-19 13:40:06", "link": "http://arxiv.org/abs/1707.07585v1", "categories": ["q-fin.ST", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "q-fin.ST"}
