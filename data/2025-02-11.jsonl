{"title": "Efficient Sparsification of Simplicial Complexes via Local Densities of States", "abstract": "Simplicial complexes (SCs), a generalization of graph models for relational\ndata that account for higher-order relations between data items, have become a\npopular abstraction for analyzing complex data using tools from topological\ndata analysis or topological signal processing. However, the analysis of many\nreal-world datasets leads to dense SCs with a large number of higher-order\ninteractions. Unfortunately, analyzing such large SCs often has a prohibitive\ncost in terms of computation time and memory consumption. The sparsification of\nsuch complexes, i.e., the approximation of an original SC with a sparser\nsimplicial complex with only a log-linear number of high-order simplices while\nmaintaining a spectrum close to the original SC, is of broad interest.\n  In this work, we develop a novel method for a probabilistic sparsifaction of\nSCs. At its core lies the efficient computation of sparsifying sampling\nprobability through local densities of states as functional descriptors of the\nspectral information. To avoid pathological structures in the spectrum of the\ncorresponding Hodge Laplacian operators, we suggest a \"kernel-ignoring\"\ndecomposition for approximating the sampling probability; additionally, we\nexploit error estimates to show asymptotically prevailing algorithmic\ncomplexity of the developed method. The performance of the framework is\ndemonstrated on the family of Vietoris--Rips filtered simplicial complexes.", "published": "2025-02-11 13:51:42", "link": "http://arxiv.org/abs/2502.07558v1", "categories": ["stat.ML", "cs.CG", "cs.DM", "cs.NA", "cs.SI", "math.NA"], "primary_category": "stat.ML"}
{"title": "Minimal Shortfall Strategies for Liquidation of a Basket of Stocks using Reinforcement Learning", "abstract": "This paper studies the ubiquitous problem of liquidating large quantities of\nhighly correlated stocks, a task frequently encountered by institutional\ninvestors and proprietary trading firms. Traditional methods in this setting\nsuffer from the curse of dimensionality, making them impractical for\nhigh-dimensional problems. In this work, we propose a novel method based on\nstochastic optimal control to optimally tackle this complex multidimensional\nproblem. The proposed method minimizes the overall execution shortfall of\nhighly correlated stocks using a reinforcement learning approach. We rigorously\nestablish the convergence of our optimal trading strategy and present an\nimplementation of our algorithm using intra-day market data.", "published": "2025-02-11 18:55:14", "link": "http://arxiv.org/abs/2502.07868v1", "categories": ["q-fin.TR", "math.OC", "q-fin.CP"], "primary_category": "q-fin.TR"}
{"title": "Integrating the implied regularity into implied volatility models: A study on free arbitrage model", "abstract": "Implied volatility IV is a key metric in financial markets, reflecting market\nexpectations of future price fluctuations. Research has explored IV's\nrelationship with moneyness, focusing on its connection to the implied Hurst\nexponent H. Our study reveals that H approaches 1/2 when moneyness equals 1,\nmarking a critical point in market efficiency expectations. We developed an IV\nmodel that integrates H to capture these dynamics more effectively. This model\nconsiders the interaction between H and the underlying-to-strike price ratio\nS/K, crucial for capturing IV variations based on moneyness. Using Optuna\noptimization across multiple indexes, the model outperformed SABR and fSABR in\naccuracy. This approach provides a more detailed representation of market\nexpectations and IV-H dynamics, improving options pricing and volatility\nforecasting while enhancing theoretical and pratcical financial analysis.", "published": "2025-02-11 12:44:45", "link": "http://arxiv.org/abs/2502.07518v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "Intraday order transition dynamics in high, medium, and low market cap stocks: A Markov chain approach", "abstract": "An empirical stochastic analysis of high-frequency, tick-by-tick order data\nof NASDAQ100 listed stocks is conducted using a first-order discrete-time\nMarkov chain model to explore intraday order transition dynamics. This analysis\nfocuses on three market cap categories: High, Medium, and Low. Time-homogeneous\ntransition probability matrices are estimated and compared across time-zones\nand market cap categories, and we found that limit orders exhibit higher degree\nof inertia (DoI), i.e., the probability of placing consecutive limit order is\nhigher, during the opening hour. However, in the subsequent hour, the DoI of\nlimit order decreases, while that of market order increases. Limit order\nadjustments via additions and deletions of limit orders increases significantly\nafter the opening hour. All the order transitions then stabilize during\nmid-hours. As the closing hour approaches, consecutive order executions surge,\nwith decreased placement of buy and sell limit orders following sell and buy\nexecutions, respectively. In terms of the differences in order transitions\nbetween stocks of different market cap, DoI of orders is stronger in high and\nmedium market cap stocks. On the other hand, lower market cap stocks show a\nhigher probability of limit order modifications and greater likelihood of\nsubmitting sell/buy limit orders after buy/sell executions. Further, order\ntransitions are clustered across all stocks, except during opening and closing\nhours. The findings of this study may be useful in understanding intraday order\nplacement dynamics across stocks of varying market cap, thus aiding market\nparticipants in making informed order placements at different times of trading\nhour.", "published": "2025-02-11 15:13:10", "link": "http://arxiv.org/abs/2502.07625v1", "categories": ["q-fin.ST", "q-fin.TR"], "primary_category": "q-fin.ST"}
{"title": "FinRL-DeepSeek: LLM-Infused Risk-Sensitive Reinforcement Learning for Trading Agents", "abstract": "This paper presents a novel risk-sensitive trading agent combining\nreinforcement learning and large language models (LLMs). We extend the\nConditional Value-at-Risk Proximal Policy Optimization (CPPO) algorithm, by\nadding risk assessment and trading recommendation signals generated by a LLM\nfrom financial news. Our approach is backtested on the Nasdaq-100 index\nbenchmark, using financial news data from the FNSPID dataset and the DeepSeek\nV3, Qwen 2.5 and Llama 3.3 language models. The code, data, and trading agents\nare available at: https://github.com/benstaf/FinRL_DeepSeek", "published": "2025-02-11 09:23:14", "link": "http://arxiv.org/abs/2502.07393v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "Ask Patients with Patience: Enabling LLMs for Human-Centric Medical\n  Dialogue with Grounded Reasoning", "abstract": "Accurate and efficient diagnosis in online medical consultations remains a\nchallenge for current large language models. These models often rely on\nsingle-turn interactions and lack the ability to refine their predictions\nthrough follow-up questions. Additionally, their responses frequently contain\ncomplex medical terminology, making them less accessible to non-medical users\nand creating barriers to effective communication. In this paper, we introduce\nAsk Patients with Patience (APP), the first multi-turn dialogue that enables\nLLMs to iteratively refine diagnoses based on grounded reasoning. By\nintegrating medical guidelines and entropy minimization, APP improves both\ndiagnostic accuracy and efficiency. Furthermore, it features human-centric\ncommunication that bridges the gap between user comprehension and medical\nterminology, significantly enhancing user accessibility and engagement. We\nevaluated APP using a subset of the ReMeDi dataset, comparing it with\nsingle-turn and traditional multi-turn LLM baselines. APP achieved higher\nsimilarity scores in diagnosis predictions, demonstrating better alignment with\nground truth diagnoses. Entropy analysis showed that APP reduces diagnostic\nuncertainty more rapidly across iterations, increasing confidence in its\npredictions. APP also excels in user accessibility and empathy, further\nbridging the gap between complex medical language and user understanding. Code\nwill be released at: https://github.com/SuperMedIntel/AskPatients.", "published": "2025-02-11 00:13:52", "link": "http://arxiv.org/abs/2502.07143v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Large-Scale Benchmark for Vietnamese Sentence Paraphrases", "abstract": "This paper presents ViSP, a high-quality Vietnamese dataset for sentence\nparaphrasing, consisting of 1.2M original-paraphrase pairs collected from\nvarious domains. The dataset was constructed using a hybrid approach that\ncombines automatic paraphrase generation with manual evaluation to ensure high\nquality. We conducted experiments using methods such as back-translation, EDA,\nand baseline models like BART and T5, as well as large language models (LLMs),\nincluding GPT-4o, Gemini-1.5, Aya, Qwen-2.5, and Meta-Llama-3.1 variants. To\nthe best of our knowledge, this is the first large-scale study on Vietnamese\nparaphrasing. We hope that our dataset and findings will serve as a valuable\nfoundation for future research and applications in Vietnamese paraphrase tasks.", "published": "2025-02-11 02:30:21", "link": "http://arxiv.org/abs/2502.07188v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph RAG-Tool Fusion", "abstract": "Recent developments in retrieval-augmented generation (RAG) for selecting\nrelevant tools from a tool knowledge base enable LLM agents to scale their\ncomplex tool calling capabilities to hundreds or thousands of external tools,\nAPIs, or agents-as-tools. However, traditional RAG-based tool retrieval fails\nto capture structured dependencies between tools, limiting the retrieval\naccuracy of a retrieved tool's dependencies. For example, among a vector\ndatabase of tools, a \"get stock price\" API requires a \"stock ticker\" parameter\nfrom a \"get stock ticker\" API, and both depend on OS-level internet\nconnectivity tools. In this paper, we address this limitation by introducing\nGraph RAG-Tool Fusion, a novel plug-and-play approach that combines the\nstrengths of vector-based retrieval with efficient graph traversal to capture\nall relevant tools (nodes) along with any nested dependencies (edges) within\nthe predefined tool knowledge graph. We also present ToolLinkOS, a new tool\nselection benchmark of 573 fictional tools, spanning over 15 industries, each\nwith an average of 6.3 tool dependencies. We demonstrate that Graph RAG-Tool\nFusion achieves absolute improvements of 71.7% and 22.1% over na\\\"ive RAG on\nToolLinkOS and ToolSandbox benchmarks, respectively (mAP@10). ToolLinkOS\ndataset is available at\nhttps://github.com/EliasLumer/Graph-RAG-Tool-Fusion-ToolLinkOS", "published": "2025-02-11 03:32:34", "link": "http://arxiv.org/abs/2502.07223v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large\n  Language Models", "abstract": "Previous multilingual benchmarks focus primarily on simple understanding\ntasks, but for large language models(LLMs), we emphasize proficiency in\ninstruction following, reasoning, long context understanding, code generation,\nand so on. However, measuring these advanced capabilities across languages is\nunderexplored. To address the disparity, we introduce BenchMAX, a multi-way\nmultilingual evaluation benchmark that allows for fair comparisons of these\nimportant abilities across languages. To maintain high quality, three distinct\nnative-speaking annotators independently annotate each sample within all tasks\nafter the data was machine-translated from English into 16 other languages.\nAdditionally, we present a novel translation challenge stemming from dataset\nconstruction. Extensive experiments on BenchMAX reveal varying effectiveness of\ncore capabilities across languages, highlighting performance gaps that cannot\nbe bridged by simply scaling up model size. BenchMAX serves as a comprehensive\nmultilingual evaluation platform, providing a promising test bed to promote the\ndevelopment of multilingual language models. The dataset and code are publicly\naccessible.", "published": "2025-02-11 08:17:19", "link": "http://arxiv.org/abs/2502.07346v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Target-Augmented Shared Fusion-based Multimodal Sarcasm Explanation\n  Generation", "abstract": "Sarcasm is a linguistic phenomenon that intends to ridicule a target (e.g.,\nentity, event, or person) in an inherent way. Multimodal Sarcasm Explanation\n(MuSE) aims at revealing the intended irony in a sarcastic post using a natural\nlanguage explanation. Though important, existing systems overlooked the\nsignificance of the target of sarcasm in generating explanations. In this\npaper, we propose a Target-aUgmented shaRed fusion-Based sarcasm explanatiOn\nmodel, aka. TURBO. We design a novel shared-fusion mechanism to leverage the\ninter-modality relationships between an image and its caption. TURBO assumes\nthe target of the sarcasm and guides the multimodal shared fusion mechanism in\nlearning intricacies of the intended irony for explanations. We evaluate our\nproposed TURBO model on the MORE+ dataset. Comparison against multiple\nbaselines and state-of-the-art models signifies the performance improvement of\nTURBO by an average margin of $+3.3\\%$. Moreover, we explore LLMs in zero and\none-shot settings for our task and observe that LLM-generated explanation,\nthough remarkable, often fails to capture the critical nuances of the sarcasm.\nFurthermore, we supplement our study with extensive human evaluation on TURBO's\ngenerated explanations and find them out to be comparatively better than other\nsystems.", "published": "2025-02-11 09:19:46", "link": "http://arxiv.org/abs/2502.07391v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity Linking using LLMs for Automated Product Carbon Footprint\n  Estimation", "abstract": "Growing concerns about climate change and sustainability are driving\nmanufacturers to take significant steps toward reducing their carbon\nfootprints. For these manufacturers, a first step towards this goal is to\nidentify the environmental impact of the individual components of their\nproducts. We propose a system leveraging large language models (LLMs) to\nautomatically map components from manufacturer Bills of Materials (BOMs) to\nLife Cycle Assessment (LCA) database entries by using LLMs to expand on\navailable component information. Our approach reduces the need for manual data\nprocessing, paving the way for more accessible sustainability practices.", "published": "2025-02-11 09:54:39", "link": "http://arxiv.org/abs/2502.07418v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Agent Collaboration for Multilingual Code Instruction Tuning", "abstract": "Recent advancement in code understanding and generation demonstrates that\ncode LLMs fine-tuned on a high-quality instruction dataset can gain powerful\ncapabilities to address wide-ranging code-related tasks. However, most previous\nexisting methods mainly view each programming language in isolation and ignore\nthe knowledge transfer among different programming languages. To bridge the gap\namong different programming languages, we introduce a novel multi-agent\ncollaboration framework to enhance multilingual instruction tuning for code\nLLMs, where multiple language-specific intelligent agent components with\ngeneration memory work together to transfer knowledge from one language to\nanother efficiently and effectively. Specifically, we first generate the\nlanguage-specific instruction data from the code snippets and then provide the\ngenerated data as the seed data for language-specific agents. Multiple\nlanguage-specific agents discuss and collaborate to formulate a new instruction\nand its corresponding solution (A new programming language or existing\nprogramming language), To further encourage the cross-lingual transfer, each\nagent stores its generation history as memory and then summarizes its merits\nand faults. Finally, the high-quality multilingual instruction data is used to\nencourage knowledge transfer among different programming languages to train\nQwen2.5-xCoder. Experimental results on multilingual programming benchmarks\ndemonstrate the superior performance of Qwen2.5-xCoder in sharing common\nknowledge, highlighting its potential to reduce the cross-lingual gap.", "published": "2025-02-11 11:46:38", "link": "http://arxiv.org/abs/2502.07487v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Corporate Greenwashing Detection in Text -- a Survey", "abstract": "Greenwashing is an effort to mislead the public about the environmental\nimpact of an entity, such as a state or company. We provide a comprehensive\nsurvey of the scientific literature addressing natural language processing\nmethods to identify potentially misleading climate-related corporate\ncommunications, indicative of greenwashing. We break the detection of\ngreenwashing into intermediate tasks, and review the state-of-the-art\napproaches for each of them. We discuss datasets, methods, and results, as well\nas limitations and open challenges. We also provide an overview of how far the\nfield has come as a whole, and point out future research directions.", "published": "2025-02-11 13:28:56", "link": "http://arxiv.org/abs/2502.07541v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grammar Control in Dialogue Response Generation for Language Learning\n  Chatbots", "abstract": "Chatbots based on large language models offer cheap conversation practice\nopportunities for language learners. However, they are hard to control for\nlinguistic forms that correspond to learners' current needs, such as grammar.\nWe control grammar in chatbot conversation practice by grounding a dialogue\nresponse generation model in a pedagogical repository of grammar skills. We\nalso explore how this control helps learners to produce specific grammar. We\ncomprehensively evaluate prompting, fine-tuning, and decoding strategies for\ngrammar-controlled dialogue response generation. Strategically decoding Llama3\noutperforms GPT-3.5 when tolerating minor response quality losses. Our\nsimulation predicts grammar-controlled responses to support grammar acquisition\nadapted to learner proficiency. Existing language learning chatbots and\nresearch on second language acquisition benefit from these affordances. Code\navailable on GitHub.", "published": "2025-02-11 13:30:41", "link": "http://arxiv.org/abs/2502.07544v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "O1 Embedder: Let Retrievers Think Before Action", "abstract": "The growing power of large language models (LLMs) has revolutionized how\npeople access and utilize information. Notably, the LLMs excel at performing\nfine-grained data representation, which facilitates precise retrieval of\ninformation. They also generate high-quality answers based on external\nreferences, enabling the production of useful knowledge. The recent\nintroduction of reasoning models, like OpenAI O1 and DeepSeek R1, marks another\nleap forward, highlighting LLMs' ability to think progressively before\ndelivering final answers. This breakthrough significantly improves the ability\nto address complex tasks, e.g., coding and math proofs.\n  Inspired by this progress, we aim to develop similar capabilities for\nretrieval models, which hold great promise for tackling critical challenges in\nthe field, including multi-task retrieval, zero-shot retrieval, and tasks\nrequiring intensive reasoning of complex relationships. With this motivation,\nwe propose a novel approach called O1 Embedder, which generates useful thoughts\nfor the input query before making retrieval for the target documents. To\nrealize this objective, we conquer two technical difficulties. First, we design\na data synthesis workflow, creating training signals for O1 Embedder by\ngenerating initial thoughts from an LLM-expert and subsequently refining them\nusing a retrieval committee. Second, we optimize the training process, enabling\na pre-trained model to be jointly fine-tuned to generate retrieval thoughts via\nbehavior cloning and perform dense retrieval through contrastive learning. Our\napproach is evaluated by comprehensive experiments, where substantial\nimprovements are achieved across 12 popular datasets, spanning both in-domain\nand out-of-domain scenarios. These results highlight O1 Embedder's remarkable\naccuracy and generalizability, paving the way for the development of\nnext-generation IR foundation models.", "published": "2025-02-11 13:48:10", "link": "http://arxiv.org/abs/2502.07555v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DPO-Shift: Shifting the Distribution of Direct Preference Optimization", "abstract": "Direct Preference Optimization (DPO) and its variants have become\nincreasingly popular for aligning language models with human preferences. These\nmethods aim to teach models to better distinguish between chosen (or preferred)\nand rejected (or dispreferred) responses. However, prior research has\nidentified that the probability of chosen responses often decreases during\ntraining, and this phenomenon is known as likelihood displacement. To tackle\nthis challenge, in this work we introduce \\method to controllably shift the\ndistribution of the chosen probability. Then, we show that \\method exhibits a\nfundamental trade-off between improving the chosen probability and sacrificing\nthe reward margin, as supported by both theoretical analysis and experimental\nvalidation. Furthermore, we demonstrate the superiority of \\method over DPO on\ndownstream tasks such as MT-Bench and a designed win rate experiment. We\nbelieve this study shows that the likelihood displacement issue of DPO can be\neffectively mitigated with a simple, theoretically grounded solution. Our code\nis available at https://github.com/Meaquadddd/DPO-Shift.", "published": "2025-02-11 14:49:44", "link": "http://arxiv.org/abs/2502.07599v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lexical categories of stem-forming roots in Mapud\u00fcngun verb forms", "abstract": "After developing a computational system for morphological analysis of the\nMapuche language, and evaluating it with texts from various authors and styles,\nit became necessary to verify the linguistic assumptions of the source used as\nthe basis for implementing this tool.\n  In the present work, the primary focus is on the lexical category\nclassification of Mapud\\\"ungun roots recognised as verbal in the source\nutilised for the development of the morphological analysis system.\n  The results of this lexical category revision directly benefit the\ncomputational analyser, as they are implemented as soon as they are verified.\nAdditionally, it is hoped that these results will help clarify some\nuncertainties about lexical categories in the Mapuche language.\n  This work addresses a preliminary task to identify the valency of true verbal\nroots, the results of which will be presented in a subsequent work that\ncomplements this article.", "published": "2025-02-11 15:10:23", "link": "http://arxiv.org/abs/2502.07623v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BiaSWE: An Expert Annotated Dataset for Misogyny Detection in Swedish", "abstract": "In this study, we introduce the process for creating BiaSWE, an\nexpert-annotated dataset tailored for misogyny detection in the Swedish\nlanguage. To address the cultural and linguistic specificity of misogyny in\nSwedish, we collaborated with experts from the social sciences and humanities.\nOur interdisciplinary team developed a rigorous annotation process,\nincorporating both domain knowledge and language expertise, to capture the\nnuances of misogyny in a Swedish context. This methodology ensures that the\ndataset is not only culturally relevant but also aligned with broader efforts\nin bias detection for low-resource languages. The dataset, along with the\nannotation guidelines, is publicly available for further research.", "published": "2025-02-11 15:25:10", "link": "http://arxiv.org/abs/2502.07637v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Auto-Drafting Police Reports from Noisy ASR Outputs: A Trust-Centered\n  LLM Approach", "abstract": "Achieving a delicate balance between fostering trust in law enforcement and\nprotecting the rights of both officers and civilians continues to emerge as a\npressing research and product challenge in the world today. In the pursuit of\nfairness and transparency, this study presents an innovative AI-driven system\ndesigned to generate police report drafts from complex, noisy, and multi-role\ndialogue data. Our approach intelligently extracts key elements of law\nenforcement interactions and includes them in the draft, producing structured\nnarratives that are not only high in quality but also reinforce accountability\nand procedural clarity. This framework holds the potential to transform the\nreporting process, ensuring greater oversight, consistency, and fairness in\nfuture policing practices. A demonstration video of our system can be accessed\nat\nhttps://drive.google.com/file/d/1kBrsGGR8e3B5xPSblrchRGj-Y-kpCHNO/view?usp=sharing", "published": "2025-02-11 16:27:28", "link": "http://arxiv.org/abs/2502.07677v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models as Proxies for Theories of Human Linguistic\n  Cognition", "abstract": "We consider the possible role of current large language models (LLMs) in the\nstudy of human linguistic cognition. We focus on the use of such models as\nproxies for theories of cognition that are relatively linguistically-neutral in\ntheir representations and learning but differ from current LLMs in key ways. We\nillustrate this potential use of LLMs as proxies for theories of cognition in\nthe context of two kinds of questions: (a) whether the target theory accounts\nfor the acquisition of a given pattern from a given corpus; and (b) whether the\ntarget theory makes a given typologically-attested pattern easier to acquire\nthan another, typologically-unattested pattern. For each of the two questions\nwe show, building on recent literature, how current LLMs can potentially be of\nhelp, but we note that at present this help is quite limited.", "published": "2025-02-11 16:38:16", "link": "http://arxiv.org/abs/2502.07687v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Making Language Models Robust Against Negation", "abstract": "Negation has been a long-standing challenge for language models. Previous\nstudies have shown that they struggle with negation in many natural language\nunderstanding tasks. In this work, we propose a self-supervised method to make\nlanguage models more robust against negation. We introduce a novel task, Next\nSentence Polarity Prediction (NSPP), and a variation of the Next Sentence\nPrediction (NSP) task. We show that BERT and RoBERTa further pre-trained on our\ntasks outperform the off-the-shelf versions on nine negation-related\nbenchmarks. Most notably, our pre-training tasks yield between 1.8% and 9.1%\nimprovement on CondaQA, a large question-answering corpus requiring reasoning\nover negation.", "published": "2025-02-11 17:18:47", "link": "http://arxiv.org/abs/2502.07717v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intelligent Legal Assistant: An Interactive Clarification System for\n  Legal Question Answering", "abstract": "The rise of large language models has opened new avenues for users seeking\nlegal advice. However, users often lack professional legal knowledge, which can\nlead to questions that omit critical information. This deficiency makes it\nchallenging for traditional legal question-answering systems to accurately\nidentify users' actual needs, often resulting in imprecise or generalized\nadvice. In this work, we develop a legal question-answering system called\nIntelligent Legal Assistant, which interacts with users to precisely capture\ntheir needs. When a user poses a question, the system requests that the user\nselect their geographical location to pinpoint the applicable laws. It then\ngenerates clarifying questions and options based on the key information missing\nfrom the user's initial question. This allows the user to select and provide\nthe necessary details. Once all necessary information is provided, the system\nproduces an in-depth legal analysis encompassing three aspects: overall\nconclusion, jurisprudential analysis, and resolution suggestions.", "published": "2025-02-11 19:19:08", "link": "http://arxiv.org/abs/2502.07904v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Elevating Legal LLM Responses: Harnessing Trainable Logical Structures\n  and Semantic Knowledge with Legal Reasoning", "abstract": "Large Language Models (LLMs) have achieved impressive results across numerous\ndomains, yet they experience notable deficiencies in legal question-answering\ntasks. LLMs often generate generalized responses that lack the logical\nspecificity required for expert legal advice and are prone to hallucination,\nproviding answers that appear correct but are unreliable. Retrieval-Augmented\nGeneration (RAG) techniques offer partial solutions to address this challenge,\nbut existing approaches typically focus only on semantic similarity, neglecting\nthe logical structure essential to legal reasoning. In this paper, we propose\nthe Logical-Semantic Integration Model (LSIM), a novel supervised framework\nthat bridges semantic and logical coherence. LSIM comprises three components:\nreinforcement learning predicts a structured fact-rule chain for each question,\na trainable Deep Structured Semantic Model (DSSM) retrieves the most relevant\ncandidate questions by integrating semantic and logical features, and\nin-context learning generates the final answer using the retrieved content. Our\nexperiments on a real-world legal QA dataset-validated through both automated\nmetrics and human evaluation-demonstrate that LSIM significantly enhances\naccuracy and reliability compared to existing methods.", "published": "2025-02-11 19:33:07", "link": "http://arxiv.org/abs/2502.07912v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting Multilingual Embedding Models to Historical Luxembourgish", "abstract": "The growing volume of digitized historical texts requires effective semantic\nsearch using text embeddings. However, pre-trained multilingual models face\nchallenges with historical content due to OCR noise and outdated spellings.\nThis study examines multilingual embeddings for cross-lingual semantic search\nin historical Luxembourgish (LB), a low-resource language. We collect\nhistorical Luxembourgish news articles from various periods and use GPT-4o for\nsentence segmentation and translation, generating 20,000 parallel training\nsentences per language pair. Additionally, we create a semantic search\n(Historical LB Bitext Mining) evaluation set and find that existing models\nperform poorly on cross-lingual search for historical Luxembourgish. Using our\nhistorical and additional modern parallel training data, we adapt several\nmultilingual embedding models through contrastive learning or knowledge\ndistillation and increase accuracy significantly for all models. We release our\nadapted models and historical Luxembourgish-German/French/English bitexts to\nsupport further research.", "published": "2025-02-11 20:35:29", "link": "http://arxiv.org/abs/2502.07938v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Geometry of Prompting: Unveiling Distinct Mechanisms of Task\n  Adaptation in Language Models", "abstract": "Decoder-only language models have the ability to dynamically switch between\nvarious computational tasks based on input prompts. Despite many successful\napplications of prompting, there is very limited understanding of the internal\nmechanism behind such flexibility. In this work, we investigate how different\nprompting methods affect the geometry of representations in these models.\nEmploying a framework grounded in statistical physics, we reveal that various\nprompting techniques, while achieving similar performance, operate through\ndistinct representational mechanisms for task adaptation. Our analysis\nhighlights the critical role of input distribution samples and label semantics\nin few-shot in-context learning. We also demonstrate evidence of synergistic\nand interfering interactions between different tasks on the representational\nlevel. Our work contributes to the theoretical understanding of large language\nmodels and lays the groundwork for developing more effective,\nrepresentation-aware prompting strategies.", "published": "2025-02-11 23:09:50", "link": "http://arxiv.org/abs/2502.08009v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language-TPP: Integrating Temporal Point Processes with Language Models\n  for Event Analysis", "abstract": "Temporal Point Processes (TPPs) have been widely used for event sequence\nmodeling, but they often struggle to incorporate rich textual event\ndescriptions effectively. Conversely, while Large Language Models (LLMs) have\nbeen shown remarkable capabilities in processing textual data, they lack\nmechanisms for handling temporal dynamics. To bridge this gap, we introduce\nLanguage-TPP, a unified framework that integrates TPPs with LLMs for enhanced\nevent sequence modeling. Language-TPP introduces a novel temporal encoding\nmechanism that converts continuous time intervals into specialized byte-tokens,\nenabling seamless integration with standard LLM architectures. This approach\nallows Language-TPP to achieve state-of-the-art performance across multiple TPP\ntasks, including event time prediction, type prediction, and intensity\nestimation, on five datasets. Additionally, we demonstrate that incorporating\ntemporal information significantly improves the quality of generated event\ndescriptions.", "published": "2025-02-11 00:09:45", "link": "http://arxiv.org/abs/2502.07139v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Don't Just Demo, Teach Me the Principles: A Principle-Based Multi-Agent\n  Prompting Strategy for Text Classification", "abstract": "We present PRINCIPLE-BASED PROMPTING, a simple but effective multi-agent\nprompting strategy for text classification. It first asks multiple LLM agents\nto independently generate candidate principles based on analysis of\ndemonstration samples with or without labels, consolidates them into final\nprinciples via a finalizer agent, and then sends them to a classifier agent to\nperform downstream classification tasks. Extensive experiments on binary and\nmulti-class classification datasets with different sizes of LLMs show that our\napproach not only achieves substantial performance gains (1.55% - 19.37%) over\nzero-shot prompting on macro-F1 score but also outperforms other strong\nbaselines (CoT and stepback prompting). Principles generated by our approach\nhelp LLMs perform better on classification tasks than human crafted principles\non two private datasets. Our multi-agent PRINCIPLE-BASED PROMPTING approach\nalso shows on-par or better performance compared to demonstration-based\nfew-shot prompting approaches, yet with substantially lower inference costs.\nAblation studies show that label information and the multi-agent cooperative\nLLM framework play an important role in generating high-quality principles to\nfacilitate downstream classification tasks.", "published": "2025-02-11 01:10:13", "link": "http://arxiv.org/abs/2502.07165v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Refine Knowledge of Large Language Models via Adaptive Contrastive\n  Learning", "abstract": "How to alleviate the hallucinations of Large Language Models (LLMs) has\nalways been the fundamental goal pursued by the LLMs research community.\nLooking through numerous hallucination-related studies, a mainstream category\nof methods is to reduce hallucinations by optimizing the knowledge\nrepresentation of LLMs to change their output. Considering that the core focus\nof these works is the knowledge acquired by models, and knowledge has long been\na central theme in human societal progress, we believe that the process of\nmodels refining knowledge can greatly benefit from the way humans learn. In our\nwork, by imitating the human learning process, we design an Adaptive\nContrastive Learning strategy. Our method flexibly constructs different\npositive and negative samples for contrastive learning based on LLMs' actual\nmastery of knowledge. This strategy helps LLMs consolidate the correct\nknowledge they already possess, deepen their understanding of the correct\nknowledge they have encountered but not fully grasped, forget the incorrect\nknowledge they previously learned, and honestly acknowledge the knowledge they\nlack. Extensive experiments and detailed analyses on widely used datasets\ndemonstrate the effectiveness of our method.", "published": "2025-02-11 02:19:13", "link": "http://arxiv.org/abs/2502.07184v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Perceived Confidence Scoring for Data Annotation with Zero-Shot LLMs", "abstract": "Zero-shot LLMs are now also used for textual classification tasks, e.g.,\nsentiment/emotion detection of a given input as a sentence/article. However,\ntheir performance can be suboptimal in such data annotation tasks. We introduce\na novel technique Perceived Confidence Scoring (PCS) that evaluates LLM's\nconfidence for its classification of an input by leveraging Metamorphic\nRelations (MRs). The MRs generate semantically equivalent yet textually mutated\nversions of the input. Following the principles of Metamorphic Testing (MT),\nthe mutated versions are expected to have annotation labels similar to the\ninput. By analyzing the consistency of LLM responses across these variations,\nPCS computes a confidence score based on the frequency of predicted labels. PCS\ncan be used both for single LLM and multiple LLM settings (e.g., majority\nvoting). We introduce an algorithm Perceived Differential Evolution (PDE) that\ndetermines the optimal weights assigned to the MRs and the LLMs for a\nclassification task. Empirical evaluation shows PCS significantly improves\nzero-shot accuracy for Llama-3-8B-Instruct (4.96%) and Mistral-7B-Instruct-v0.3\n(10.52%), with Gemma-2-9b-it showing a 9.39% gain. When combining all three\nmodels, PCS significantly outperforms majority voting by 7.75%.", "published": "2025-02-11 02:25:44", "link": "http://arxiv.org/abs/2502.07186v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GENERator: A Long-Context Generative Genomic Foundation Model", "abstract": "Advancements in DNA sequencing technologies have significantly improved our\nability to decode genomic sequences. However, the prediction and interpretation\nof these sequences remain challenging due to the intricate nature of genetic\nmaterial. Large language models (LLMs) have introduced new opportunities for\nbiological sequence analysis. Recent developments in genomic language models\nhave underscored the potential of LLMs in deciphering DNA sequences.\nNonetheless, existing models often face limitations in robustness and\napplication scope, primarily due to constraints in model structure and training\ndata scale. To address these limitations, we present GENERator, a generative\ngenomic foundation model featuring a context length of 98k base pairs (bp) and\n1.2B parameters. Trained on an expansive dataset comprising 386B bp of\neukaryotic DNA, the GENERator demonstrates state-of-the-art performance across\nboth established and newly proposed benchmarks. The model adheres to the\ncentral dogma of molecular biology, accurately generating protein-coding\nsequences that translate into proteins structurally analogous to known\nfamilies. It also shows significant promise in sequence optimization,\nparticularly through the prompt-responsive generation of enhancer sequences\nwith specific activity profiles. These capabilities position the GENERator as a\npivotal tool for genomic research and biotechnological advancement, enhancing\nour ability to interpret and predict complex biological systems and enabling\nprecise genomic interventions. Implementation details and supplementary\nresources are available at https://github.com/GenerTeam/GENERator.", "published": "2025-02-11 05:39:49", "link": "http://arxiv.org/abs/2502.07272v3", "categories": ["cs.CL", "q-bio.GN"], "primary_category": "cs.CL"}
{"title": "Small Language Model Makes an Effective Long Text Extractor", "abstract": "Named Entity Recognition (NER) is a fundamental problem in natural language\nprocessing (NLP). However, the task of extracting longer entity spans (e.g.,\nawards) from extended texts (e.g., homepages) is barely explored. Current NER\nmethods predominantly fall into two categories: span-based methods and\ngeneration-based methods. Span-based methods require the enumeration of all\npossible token-pair spans, followed by classification on each span, resulting\nin substantial redundant computations and excessive GPU memory usage. In\ncontrast, generation-based methods involve prompting or fine-tuning large\nlanguage models (LLMs) to adapt to downstream NER tasks. However, these methods\nstruggle with the accurate generation of longer spans and often incur\nsignificant time costs for effective fine-tuning. To address these challenges,\nthis paper introduces a lightweight span-based NER method called SeNER, which\nincorporates a bidirectional arrow attention mechanism coupled with\nLogN-Scaling on the [CLS] token to embed long texts effectively, and comprises\na novel bidirectional sliding-window plus-shaped attention (BiSPA) mechanism to\nreduce redundant candidate token-pair spans significantly and model\ninteractions between token-pair spans simultaneously. Extensive experiments\ndemonstrate that our method achieves state-of-the-art extraction accuracy on\nthree long NER datasets and is capable of extracting entities from long texts\nin a GPU-memory-friendly manner. Code:\nhttps://github.com/THUDM/scholar-profiling/tree/main/sener", "published": "2025-02-11 06:06:25", "link": "http://arxiv.org/abs/2502.07286v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction", "abstract": "Reasoning is a fundamental capability of Large Language Models. While prior\nresearch predominantly focuses on enhancing narrow skills like math or code\ngeneration, improving performance on many other reasoning tasks remains\nchallenging due to sparse and fragmented training data. To address this issue,\nwe propose CodeI/O, a novel approach that systematically condenses diverse\nreasoning patterns inherently embedded in contextually-grounded codes, through\ntransforming the original code into a code input-output prediction format. By\ntraining models to predict inputs/outputs given code and test cases entirely in\nnatural language as Chain-of-Thought (CoT) rationales, we expose them to\nuniversal reasoning primitives -- like logic flow planning, state-space\nsearching, decision tree traversal, and modular decomposition -- while\ndecoupling structured reasoning from code-specific syntax and preserving\nprocedural rigor. Experimental results demonstrate CodeI/O leads to consistent\nimprovements across symbolic, scientific, logic, math & numerical, and\ncommonsense reasoning tasks. By matching the existing ground-truth outputs or\nre-executing the code with predicted inputs, we can verify each prediction and\nfurther enhance the CoTs through multi-turn revision, resulting in CodeI/O++\nand achieving higher performance. Our data and models are available at\nhttps://github.com/hkust-nlp/CodeIO.", "published": "2025-02-11 07:26:50", "link": "http://arxiv.org/abs/2502.07316v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MEMIT-Merge: Addressing MEMIT's Key-Value Conflicts in Same-Subject\n  Batch Editing for LLMs", "abstract": "As large language models continue to scale up, knowledge editing techniques\nthat modify models' internal knowledge without full retraining have gained\nsignificant attention. MEMIT, a prominent batch editing algorithm, stands out\nfor its capability to perform mass knowledge modifications. However, we uncover\na critical limitation that MEMIT's editing efficacy significantly deteriorates\nwhen processing batches containing multiple edits sharing the same subject. Our\nanalysis reveals that the root cause lies in MEMIT's key value modeling\nframework: When multiple facts with the same subject in a batch are modeled\nthrough MEMIT's key value mechanism, identical keys (derived from the shared\nsubject) are forced to represent different values (corresponding to different\nknowledge), resulting in updates conflicts during editing. Addressing this\nissue, we propose MEMIT-Merge, an enhanced approach that merges value\ncomputation processes for facts sharing the same subject, effectively resolving\nthe performance degradation in same-subject batch editing scenarios.\nExperimental results demonstrate that when MEMIT's edit success rate drops to\naround 50% at larger batch sizes, MEMIT-Merge maintains a success rate\nexceeding 90%, showcasing remarkable robustness to subject entity collisions.", "published": "2025-02-11 07:42:09", "link": "http://arxiv.org/abs/2502.07322v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Aligning Large Language Models to Follow Instructions and Hallucinate\n  Less via Effective Data Filtering", "abstract": "Training LLMs on data containing unfamiliar knowledge during the instruction\ntuning stage can encourage hallucinations. To address this challenge, we\nintroduce NOVA, a novel framework designed to identify high-quality data that\naligns well with the LLM's learned knowledge to reduce hallucinations. NOVA\nincludes Internal Consistency Probing (ICP) and Semantic Equivalence\nIdentification (SEI) to measure how familiar the LLM is with instruction data.\nSpecifically, ICP evaluates the LLM's understanding of the given instruction by\ncalculating the tailored consistency among multiple self-generated responses.\nSEI further assesses the familiarity of the LLM with the target response by\ncomparing it to the generated responses, using the proposed semantic clustering\nand well-designed voting strategy. Finally, to ensure the quality of selected\nsamples, we introduce an expert-aligned reward model, considering\ncharacteristics beyond just familiarity. By considering data quality and\navoiding unfamiliar data, we can utilize the selected data to effectively align\nLLMs to follow instructions and hallucinate less.", "published": "2025-02-11 08:05:56", "link": "http://arxiv.org/abs/2502.07340v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LongReD: Mitigating Short-Text Degradation of Long-Context Large\n  Language Models via Restoration Distillation", "abstract": "Large language models (LLMs) have gained extended context windows through\nscaling positional encodings and lightweight continual pre-training. However,\nthis often leads to degraded performance on short-text tasks, while the reasons\nfor this degradation remain insufficiently explored. In this work, we identify\ntwo primary factors contributing to this issue: distribution drift in hidden\nstates and attention scores, and catastrophic forgetting during continual\npre-training. To address these challenges, we propose Long Context Pre-training\nwith Restoration Distillation (LongReD), a novel approach designed to mitigate\nshort-text performance degradation through minimizing the distribution\ndiscrepancy between the extended and original models. Besides training on long\ntexts, LongReD distills the hidden state of selected layers from the original\nmodel on short texts. Additionally, LongReD also introduces a short-to-long\ndistillation, aligning the output distribution on short texts with that on long\ntexts by leveraging skipped positional indices. Experiments on common text\nbenchmarks demonstrate that LongReD effectively preserves the model's\nshort-text performance while maintaining comparable or even better capacity to\nhandle long texts than baselines. Our code is available at\nhttps://github.com/RUCAIBox/LongReD.", "published": "2025-02-11 08:37:16", "link": "http://arxiv.org/abs/2502.07365v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Parametric type design in the era of variable and color fonts", "abstract": "Parametric fonts are programatically defined fonts with variable parameters,\npioneered by Donald Kunth with his MetaFont technology in the 1980s. While\nDonald Knuth's ideas in MetaFont and subsequently in MetaPost are often seen as\nlegacy techniques from the pre-graphical user interface (GUI) era of type\ndesign, recent trends like variable fonts suggest a resurgence of certain\nprinciples. This paper explores a modern type design process built on\nparametric design principles, specifically using MetaPost. The author created\ntwo variable fonts with this method and released them under a free, open-source\nlicense. The paper details the methodology, workflow, and insights gained from\nthis process.", "published": "2025-02-11 09:12:05", "link": "http://arxiv.org/abs/2502.07386v1", "categories": ["cs.CL", "cs.GR"], "primary_category": "cs.CL"}
{"title": "RomanLens: The Role Of Latent Romanization In Multilinguality In LLMs", "abstract": "Large Language Models (LLMs) exhibit remarkable multilingual generalization\ndespite being predominantly trained on English-centric corpora. A fundamental\nquestion arises: how do LLMs achieve such robust multilingual capabilities? We\ntake the case of non-Roman script languages, we investigate the role of\nRomanization - the representation of non-Roman scripts using Roman characters -\nas a bridge in multilingual processing. Using mechanistic interpretability\ntechniques, we analyze next-token generation and find that intermediate layers\nfrequently represent target words in Romanized form before transitioning to\nnative script, a phenomenon we term Latent Romanization. Further, through\nactivation patching experiments, we demonstrate that LLMs encode semantic\nconcepts similarly across native and Romanized scripts, suggesting a shared\nunderlying representation. Additionally, for translation into non-Roman script\nlanguages, our findings reveal that when the target language is in Romanized\nform, its representations emerge earlier in the model's layers compared to\nnative script. These insights contribute to a deeper understanding of\nmultilingual representation in LLMs and highlight the implicit role of\nRomanization in facilitating language transfer.", "published": "2025-02-11 10:10:26", "link": "http://arxiv.org/abs/2502.07424v2", "categories": ["cs.CL", "cs.AI", "I.2.7", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Hierarchical Document Parsing via Large Margin Feature Matching and\n  Heuristics", "abstract": "We present our solution to the AAAI-25 VRD-IU challenge, achieving first\nplace in the competition. Our approach integrates large margin loss for\nimproved feature discrimination and employs heuristic rules to refine\nhierarchical relationships. By combining a deep learning-based matching\nstrategy with greedy algorithms, we achieve a significant boost in accuracy\nwhile maintaining computational efficiency. Our method attains an accuracy of\n0.98904 on the private leaderboard, demonstrating its effectiveness in document\nstructure parsing. Source codes are publicly available at\nhttps://github.com/ffyyytt/VRUID-AAAI-DAKiet", "published": "2025-02-11 10:37:01", "link": "http://arxiv.org/abs/2502.07442v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn\n  More", "abstract": "Large Language Models (LLMs) are discovered to suffer from accurately\nretrieving key information. To address this, we propose Mask-Enhanced\nAutoregressive Prediction (MEAP), a simple yet effective training paradigm that\nseamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction\n(NTP) to enhance the latter's in-context retrieval capabilities. Specifically,\nMEAP first randomly masks a small fraction of input tokens and then directly\nperforms the standard next-token prediction autoregressive using a decoder-only\nTransformer. MEAP eliminates the need for bidirectional attention or\nencoder-decoder architectures for MLM, incurring no additional computational\noverhead during pre-training or inference. Intensive experiments demonstrate\nthat MEAP substantially outperforms NTP on key information retrieval and\nlong-context reasoning tasks, while performing on par or better on commonsense\nreasoning tasks. The benefits of MEAP also extend to supervised fine-tuning,\nwhere it shows remarkable advantages in lost-in-the-middle scenarios,\noutperforming NTP by 11.77 percentage points. Our analysis indicates that\nMEAP's effectiveness arises from its ability to promote more distinguishable\nattention scores by concentrating on a reduced set of non-masked tokens. This\nmechanism improves the model's focus on task-relevant signals while mitigating\nthe influence of peripheral context. These findings position MEAP as a\npromising training paradigm for large language models.", "published": "2025-02-11 11:49:03", "link": "http://arxiv.org/abs/2502.07490v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Translation of Emergent Communication", "abstract": "Emergent Communication (EC) provides a unique window into the language\nsystems that emerge autonomously when agents are trained to jointly achieve\nshared goals. However, it is difficult to interpret EC and evaluate its\nrelationship with natural languages (NL). This study employs unsupervised\nneural machine translation (UNMT) techniques to decipher ECs formed during\nreferential games with varying task complexities, influenced by the semantic\ndiversity of the environment. Our findings demonstrate UNMT's potential to\ntranslate EC, illustrating that task complexity characterized by semantic\ndiversity enhances EC translatability, while higher task complexity with\nconstrained semantic variability exhibits pragmatic EC, which, although\nchallenging to interpret, remains suitable for translation. This research marks\nthe first attempt, to our knowledge, to translate EC without the aid of\nparallel data.", "published": "2025-02-11 13:41:06", "link": "http://arxiv.org/abs/2502.07552v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Efficient and Multifaceted Computer-assisted Pronunciation\n  Training Leveraging Hierarchical Selective State Space Model and Decoupled\n  Cross-entropy Loss", "abstract": "Prior efforts in building computer-assisted pronunciation training (CAPT)\nsystems often treat automatic pronunciation assessment (APA) and\nmispronunciation detection and diagnosis (MDD) as separate fronts: the former\naims to provide multiple pronunciation aspect scores across diverse linguistic\nlevels, while the latter focuses instead on pinpointing the precise phonetic\npronunciation errors made by non-native language learners. However, it is\ngenerally expected that a full-fledged CAPT system should perform both\nfunctionalities simultaneously and efficiently. In response to this surging\ndemand, we in this work first propose HMamba, a novel CAPT approach that\nseamlessly integrates APA and MDD tasks in parallel. In addition, we introduce\na novel loss function, decoupled cross-entropy loss (deXent), specifically\ntailored for MDD to facilitate better-supervised learning for detecting\nmispronounced phones, thereby enhancing overall performance. A comprehensive\nset of empirical results on the speechocean762 benchmark dataset demonstrates\nthe effectiveness of our approach on APA. Notably, our proposed approach also\nyields a considerable improvement in MDD performance over a strong baseline,\nachieving an F1-score of 63.85%. Our codes are made available at\nhttps://github.com/Fuann/hmamba", "published": "2025-02-11 14:17:29", "link": "http://arxiv.org/abs/2502.07575v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "We Can't Understand AI Using our Existing Vocabulary", "abstract": "This position paper argues that, in order to understand AI, we cannot rely on\nour existing vocabulary of human words. Instead, we should strive to develop\nneologisms: new words that represent precise human concepts that we want to\nteach machines, or machine concepts that we need to learn. We start from the\npremise that humans and machines have differing concepts. This means\ninterpretability can be framed as a communication problem: humans must be able\nto reference and control machine concepts, and communicate human concepts to\nmachines. Creating a shared human-machine language through developing\nneologisms, we believe, could solve this communication problem. Successful\nneologisms achieve a useful amount of abstraction: not too detailed, so they're\nreusable in many contexts, and not too high-level, so they convey precise\ninformation. As a proof of concept, we demonstrate how a \"length neologism\"\nenables controlling LLM response length, while a \"diversity neologism\" allows\nsampling more variable responses. Taken together, we argue that we cannot\nunderstand AI using our existing vocabulary, and expanding it through\nneologisms creates opportunities for both controlling and understanding\nmachines better.", "published": "2025-02-11 14:34:05", "link": "http://arxiv.org/abs/2502.07586v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large\n  Language Models", "abstract": "Zero-Shot Anomaly Detection (ZSAD) is an emerging AD paradigm. Unlike the\ntraditional unsupervised AD setting that requires a large number of normal\nsamples to train a model, ZSAD is more practical for handling data-restricted\nreal-world scenarios. Recently, Multimodal Large Language Models (MLLMs) have\nshown revolutionary reasoning capabilities in various vision tasks. However,\nthe reasoning of image abnormalities remains underexplored due to the lack of\ncorresponding datasets and benchmarks. To facilitate research in AD &\nreasoning, we establish the first visual instruction tuning dataset,\nAnomaly-Instruct-125k, and the evaluation benchmark, VisA-D&R. Through\ninvestigation with our benchmark, we reveal that current MLLMs like GPT-4o\ncannot accurately detect and describe fine-grained anomalous details in images.\nTo address this, we propose Anomaly-OneVision (Anomaly-OV), the first\nspecialist visual assistant for ZSAD and reasoning. Inspired by human behavior\nin visual inspection, Anomaly-OV leverages a Look-Twice Feature Matching (LTFM)\nmechanism to adaptively select and emphasize abnormal visual tokens. Extensive\nexperiments demonstrate that Anomaly-OV achieves significant improvements over\nadvanced generalist models in both detection and reasoning. Extensions to\nmedical and 3D AD are provided for future study. The link to our project page:\nhttps://xujiacong.github.io/Anomaly-OV/", "published": "2025-02-11 14:50:43", "link": "http://arxiv.org/abs/2502.07601v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Tractable Transformers for Flexible Conditional Generation", "abstract": "Non-autoregressive (NAR) generative models are valuable because they can\nhandle diverse conditional generation tasks in a more principled way than their\nautoregressive (AR) counterparts, which are constrained by sequential\ndependency requirements. Recent advancements in NAR models, such as diffusion\nlanguage models, have demonstrated superior performance in unconditional\ngeneration compared to AR models (e.g., GPTs) of similar sizes. However, such\nimprovements do not always lead to improved conditional generation performance.\nWe show that a key reason for this gap is the difficulty in generalizing to\nconditional probability queries unseen during training. As a result, strong\nunconditional generation performance does not guarantee high-quality\nconditional generation. This paper proposes Tractable Transformers\n(Tracformer), a Transformer-based generative model that is more robust to\ndifferent conditional generation tasks. Unlike existing models that rely solely\non global contextual features derived from full inputs, Tracformers incorporate\na sparse Transformer encoder to capture both local and global contextual\ninformation. This information is routed through a decoder for conditional\ngeneration. Empirical results demonstrate that Tracformers achieve\nstate-of-the-art conditional generation performance on text modeling compared\nto recent diffusion and AR model baselines.", "published": "2025-02-11 15:05:26", "link": "http://arxiv.org/abs/2502.07616v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Mobile Touch Interaction with Large Language Models", "abstract": "Interacting with Large Language Models (LLMs) for text editing on mobile\ndevices currently requires users to break out of their writing environment and\nswitch to a conversational AI interface. In this paper, we propose to control\nthe LLM via touch gestures performed directly on the text. We first chart a\ndesign space that covers fundamental touch input and text transformations. In\nthis space, we then concretely explore two control mappings: spread-to-generate\nand pinch-to-shorten, with visual feedback loops. We evaluate this concept in a\nuser study (N=14) that compares three feedback designs: no visualisation, text\nlength indicator, and length + word indicator. The results demonstrate that\ntouch-based control of LLMs is both feasible and user-friendly, with the length\n+ word indicator proving most effective for managing text generation. This work\nlays the foundation for further research into gesture-based interaction with\nLLMs on touch devices.", "published": "2025-02-11 15:17:00", "link": "http://arxiv.org/abs/2502.07629v1", "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "primary_category": "cs.HC"}
{"title": "FoQA: A Faroese Question-Answering Dataset", "abstract": "We present FoQA, a Faroese extractive question-answering (QA) dataset with\n2,000 samples, created using a semi-automated approach combining Large Language\nModels (LLMs) and human validation. The dataset was generated from Faroese\nWikipedia articles using GPT-4-turbo for initial QA generation, followed by\nquestion rephrasing to increase complexity and native speaker validation to\nensure quality. We provide baseline performance metrics for FoQA across\nmultiple models, including LLMs and BERT, demonstrating its effectiveness in\nevaluating Faroese QA performance. The dataset is released in three versions: a\nvalidated set of 2,000 samples, a complete set of all 10,001 generated samples,\nand a set of 2,395 rejected samples for error analysis.", "published": "2025-02-11 15:33:17", "link": "http://arxiv.org/abs/2502.07642v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "exHarmony: Authorship and Citations for Benchmarking the Reviewer\n  Assignment Problem", "abstract": "The peer review process is crucial for ensuring the quality and reliability\nof scholarly work, yet assigning suitable reviewers remains a significant\nchallenge. Traditional manual methods are labor-intensive and often\nineffective, leading to nonconstructive or biased reviews. This paper\nintroduces the exHarmony (eHarmony but for connecting experts to manuscripts)\nbenchmark, designed to address these challenges by re-imagining the Reviewer\nAssignment Problem (RAP) as a retrieval task. Utilizing the extensive data from\nOpenAlex, we propose a novel approach that considers a host of signals from the\nauthors, most similar experts, and the citation relations as potential\nindicators for a suitable reviewer for a manuscript. This approach allows us to\ndevelop a standard benchmark dataset for evaluating the reviewer assignment\nproblem without needing explicit labels. We benchmark various methods,\nincluding traditional lexical matching, static neural embeddings, and\ncontextualized neural embeddings, and introduce evaluation metrics that assess\nboth relevance and diversity in the context of RAP. Our results indicate that\nwhile traditional methods perform reasonably well, contextualized embeddings\ntrained on scholarly literature show the best performance. The findings\nunderscore the importance of further research to enhance the diversity and\neffectiveness of reviewer assignments.", "published": "2025-02-11 16:35:04", "link": "http://arxiv.org/abs/2502.07683v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "WHODUNIT: Evaluation benchmark for culprit detection in mystery stories", "abstract": "We present a novel data set, WhoDunIt, to assess the deductive reasoning\ncapabilities of large language models (LLM) within narrative contexts.\nConstructed from open domain mystery novels and short stories, the dataset\nchallenges LLMs to identify the perpetrator after reading and comprehending the\nstory. To evaluate model robustness, we apply a range of character-level name\naugmentations, including original names, name swaps, and substitutions with\nwell-known real and/or fictional entities from popular discourse. We further\nuse various prompting styles to investigate the influence of prompting on\ndeductive reasoning accuracy.\n  We conduct evaluation study with state-of-the-art models, specifically\nGPT-4o, GPT-4-turbo, and GPT-4o-mini, evaluated through multiple trials with\nmajority response selection to ensure reliability. The results demonstrate that\nwhile LLMs perform reliably on unaltered texts, accuracy diminishes with\ncertain name substitutions, particularly those with wide recognition. This\ndataset is publicly available here.", "published": "2025-02-11 18:14:44", "link": "http://arxiv.org/abs/2502.07747v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Advanced NLP Framework for Automated Medical Diagnosis with DeBERTa\n  and Dynamic Contextual Positional Gating", "abstract": "This paper presents a novel Natural Language Processing (NLP) framework for\nenhancing medical diagnosis through the integration of advanced techniques in\ndata augmentation, feature extraction, and classification. The proposed\napproach employs back-translation to generate diverse paraphrased datasets,\nimproving robustness and mitigating overfitting in classification tasks.\nLeveraging Decoding-enhanced BERT with Disentangled Attention (DeBERTa) with\nDynamic Contextual Positional Gating (DCPG), the model captures fine-grained\ncontextual and positional relationships, dynamically adjusting the influence of\npositional information based on semantic context to produce high-quality text\nembeddings. For classification, an Attention-Based Feedforward Neural Network\n(ABFNN) is utilized, effectively focusing on the most relevant features to\nimprove decision-making accuracy. Applied to the classification of symptoms,\nclinical notes, and other medical texts, this architecture demonstrates its\nability to address the complexities of medical data. The combination of data\naugmentation, contextual embedding generation, and advanced classification\nmechanisms offers a robust and accurate diagnostic tool, with potential\napplications in automated medical diagnosis and clinical decision support. This\nmethod demonstrates the effectiveness of the proposed NLP framework for medical\ndiagnosis, achieving remarkable results with an accuracy of 99.78%, recall of\n99.72%, precision of 99.79%, and an F1-score of 99.75%. These metrics not only\nunderscore the model's robust performance in classifying medical texts with\nexceptional precision and reliability but also highlight its superiority over\nexisting methods, making it a highly promising tool for automated diagnostic\nsystems.", "published": "2025-02-11 18:32:24", "link": "http://arxiv.org/abs/2502.07755v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DarwinLM: Evolutionary Structured Pruning of Large Language Models", "abstract": "Large Language Models (LLMs) have achieved significant success across various\nNLP tasks. However, their massive computational costs limit their widespread\nuse, particularly in real-time applications. Structured pruning offers an\neffective solution by compressing models and directly providing end-to-end\nspeed improvements, regardless of the hardware environment. Meanwhile,\ndifferent components of the model exhibit varying sensitivities towards\npruning, calling for non-uniform model compression. However, a pruning method\nshould not only identify a capable substructure, but also account for\npost-compression training. To this end, we propose DarwinLM, a method for\ntraining-aware structured pruning. DarwinLM builds upon an evolutionary search\nprocess, generating multiple offspring models in each generation through\nmutation, and selecting the fittest for survival. To assess the effect of\npost-training, we incorporate a lightweight, multistep training process within\nthe offspring population, progressively increasing the number of tokens and\neliminating poorly performing models in each selection stage. We validate our\nmethod through extensive experiments on Llama-2-7B, Llama-3.1-8B and\nQwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured\npruning. For instance, DarwinLM surpasses ShearedLlama while requiring 5x less\ntraining data during post-compression training. Code is at:\nhttps://github.com/IST-DASLab/DarwinLM", "published": "2025-02-11 18:59:35", "link": "http://arxiv.org/abs/2502.07780v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature?", "abstract": "Medical research faces well-documented challenges in translating novel\ntreatments into clinical practice. Publishing incentives encourage researchers\nto present \"positive\" findings, even when empirical results are equivocal.\nConsequently, it is well-documented that authors often spin study results,\nespecially in article abstracts. Such spin can influence clinician\ninterpretation of evidence and may affect patient care decisions. In this\nstudy, we ask whether the interpretation of trial results offered by Large\nLanguage Models (LLMs) is similarly affected by spin. This is important since\nLLMs are increasingly being used to trawl through and synthesize published\nmedical evidence. We evaluated 22 LLMs and found that they are across the board\nmore susceptible to spin than humans. They might also propagate spin into their\noutputs: We find evidence, e.g., that LLMs implicitly incorporate spin into\nplain language summaries that they generate. We also find, however, that LLMs\nare generally capable of recognizing spin, and can be prompted in a way to\nmitigate spin's impact on LLM outputs.", "published": "2025-02-11 21:21:05", "link": "http://arxiv.org/abs/2502.07963v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MetaSC: Test-Time Safety Specification Optimization for Language Models", "abstract": "We propose a novel dynamic safety framework that optimizes language model\n(LM) safety reasoning at inference time without modifying model weights.\nBuilding on recent advances in self-critique methods, our approach leverages a\nmeta-critique mechanism that iteratively updates safety prompts-termed\nspecifications-to drive the critique and revision process adaptively. This\ntest-time optimization not only improves performance against adversarial\njailbreak requests but also in diverse general safety-related tasks, such as\navoiding moral harm or pursuing honest responses. Our empirical evaluations\nacross several language models demonstrate that dynamically optimized safety\nprompts yield significantly higher safety scores compared to fixed system\nprompts and static self-critique defenses. Code released at\nhttps://github.com/vicgalle/meta-self-critique.git .", "published": "2025-02-11 22:06:25", "link": "http://arxiv.org/abs/2502.07985v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Speculate, then Collaborate: Fusing Knowledge of Language Models during\n  Decoding", "abstract": "Large Language Models (LLMs) often excel in specific domains but fall short\nin others due to the limitations of their training. Thus, enabling LLMs to\nsolve problems collaboratively by integrating their complementary knowledge\npromises to improve their performance across domains. To realize this\npotential, we introduce a novel Collaborative Speculative Decoding (CoSD)\nalgorithm that enables efficient LLM knowledge fusion at test time without\nrequiring additional model training. CoSD employs a draft model to generate\ninitial sequences and an easy-to-learn rule or decision tree to decide when to\ninvoke an assistant model to improve these drafts. CoSD not only enhances\nknowledge fusion but also improves inference efficiency, is transferable across\ndomains and models, and offers greater explainability. Experimental results\ndemonstrate that CoSD improves accuracy by up to 10\\% across benchmarks\ncompared to existing methods, providing a scalable and effective solution for\nLLM-based applications", "published": "2025-02-11 23:40:53", "link": "http://arxiv.org/abs/2502.08020v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hallucination, Monofacts, and Miscalibration: An Empirical Investigation", "abstract": "Recent theoretical work by [Kalai and Vempala 2024] proves that a particular\nnotion of hallucination rate in LLMs must be lower bounded by the training data\nmonofact rate (related to the classical Good-Turing missing mass estimator)\nminus model miscalibration. Through systematic experiments with n-gram models\nand in-context learning with LLMs, we empirically investigate and validate this\ntheory by examining how different underlying data distributions affect the\nmonofact rate and a model's tendency to hallucinate. We then vary model\nmiscalibration through controlled upweighting of training samples while holding\nmonofact rates constant, allowing us to isolate miscalibration's reduction\neffect on hallucination. These findings suggest that both the distribution of\nfact frequencies in training data and the calibration-hallucination trade-off\nare inherent to probabilistic language generation. Our results also suggest\nthat current practices of aggressive deduplication in training data may need to\nbe reconsidered, as selective duplication could serve as a principled mechanism\nfor reducing hallucination.", "published": "2025-02-11 18:46:00", "link": "http://arxiv.org/abs/2502.08666v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unveiling Simplicities of Attention: Adaptive Long-Context Head\n  Identification", "abstract": "The ability to process long contexts is crucial for many natural language\nprocessing tasks, yet it remains a significant challenge. While substantial\nprogress has been made in enhancing the efficiency of attention mechanisms,\nthere is still a gap in understanding how attention heads function in\nlong-context settings. In this paper, we observe that while certain heads\nconsistently attend to local information only, others swing between attending\nto local and long-context information depending on the query. This raises the\nquestion: can we identify which heads require long-context information to\npredict the next token accurately? We demonstrate that it's possible to predict\nwhich heads are crucial for long-context processing using only local keys. The\ncore idea here is to exploit a simple model for the long-context scores via\nsecond moment approximations. These findings unveil simple properties of\nattention in the context of long sequences, and open the door to potentially\nsignificant gains in efficiency.", "published": "2025-02-11 00:04:32", "link": "http://arxiv.org/abs/2502.09647v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "UKTA: Unified Korean Text Analyzer", "abstract": "Evaluating writing quality is complex and time-consuming often delaying\nfeedback to learners. While automated writing evaluation tools are effective\nfor English, Korean automated writing evaluation tools face challenges due to\ntheir inability to address multi-view analysis, error propagation, and\nevaluation explainability. To overcome these challenges, we introduce UKTA\n(Unified Korean Text Analyzer), a comprehensive Korea text analysis and writing\nevaluation system. UKTA provides accurate low-level morpheme analysis, key\nlexical features for mid-level explainability, and transparent high-level\nrubric-based writing scores. Our approach enhances accuracy and quadratic\nweighted kappa over existing baseline, positioning UKTA as a leading\nmulti-perspective tool for Korean text analysis and writing evaluation.", "published": "2025-02-11 13:30:56", "link": "http://arxiv.org/abs/2502.09648v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AI-VERDE: A Gateway for Egalitarian Access to Large Language Model-Based\n  Resources For Educational Institutions", "abstract": "We present AI-VERDE, a unified LLM-as-a-platform service designed to\nfacilitate seamless integration of commercial, cloud-hosted, and on-premise\nopen LLMs in academic settings. AI-VERDE streamlines access management for\ninstructional and research groups by providing features such as robust access\ncontrol, privacy-preserving mechanisms, native Retrieval-Augmented Generation\n(RAG) support, budget management for third-party LLM services, and both a\nconversational web interface and API access. In a pilot deployment at a large\npublic university, AI-VERDE demonstrated significant engagement across diverse\neducational and research groups, enabling activities that would typically\nrequire substantial budgets for commercial LLM services with limited user and\nteam management capabilities. To the best of our knowledge, AI-Verde is the\nfirst platform to address both academic and research needs for LLMs within an\nhigher education institutional framework.", "published": "2025-02-11 18:11:22", "link": "http://arxiv.org/abs/2502.09651v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Towards a Robust Framework for Multimodal Hate Detection: A Study on\n  Video vs. Image-based Content", "abstract": "Social media platforms enable the propagation of hateful content across\ndifferent modalities such as textual, auditory, and visual, necessitating\neffective detection methods. While recent approaches have shown promise in\nhandling individual modalities, their effectiveness across different modality\ncombinations remains unexplored. This paper presents a systematic analysis of\nfusion-based approaches for multimodal hate detection, focusing on their\nperformance across video and image-based content. Our comprehensive evaluation\nreveals significant modality-specific limitations: while simple embedding\nfusion achieves state-of-the-art performance on video content (HateMM dataset)\nwith a 9.9% points F1-score improvement, it struggles with complex image-text\nrelationships in memes (Hateful Memes dataset). Through detailed ablation\nstudies and error analysis, we demonstrate how current fusion approaches fail\nto capture nuanced cross-modal interactions, particularly in cases involving\nbenign confounders. Our findings provide crucial insights for developing more\nrobust hate detection systems and highlight the need for modality-specific\narchitectural considerations. The code is available at\nhttps://github.com/gak97/Video-vs-Meme-Hate.", "published": "2025-02-11 00:07:40", "link": "http://arxiv.org/abs/2502.07138v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "I.4.10; I.5.1; I.2.7"], "primary_category": "cs.CV"}
{"title": "Does Training on Synthetic Data Make Models Less Robust?", "abstract": "An increasingly common practice is to train large language models (LLMs)\nusing synthetic data. Often this synthetic data is produced by the same or\nsimilar LLMs as those it is being used to train. This raises the question of\nwhether the synthetic data might in fact exacerbate certain \"blindspots\" by\nreinforcing heuristics that the LLM already encodes. In this paper, we conduct\nsimulated experiments on the natural language inference (NLI) task with\nLlama-2-7B-hf models. We use MultiNLI as the general task and HANS, a targeted\nevaluation set designed to measure the presence of specific heuristic\nstrategies for NLI, as our \"blindspot\" task. Our goal is to determine whether\nperformance disparities between the general and blind spot tasks emerge. Our\nresults indicate that synthetic data does not reinforce blindspots in the way\nwe expected. Specifically, we see that, while fine-tuning with synthetic data\ndoesn't necessarily reduce the use of the heuristic, it also does not make it\nworse as we hypothesized.", "published": "2025-02-11 01:03:33", "link": "http://arxiv.org/abs/2502.07164v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DrugImproverGPT: A Large Language Model for Drug Optimization with\n  Fine-Tuning via Structured Policy Optimization", "abstract": "Finetuning a Large Language Model (LLM) is crucial for generating results\ntowards specific objectives. This research delves into the realm of drug\noptimization and introduce a novel reinforcement learning algorithm to finetune\na drug optimization LLM-based generative model, enhancing the original drug\nacross target objectives, while retains the beneficial chemical properties of\nthe original drug. This work is comprised of two primary components: (1)\nDrugImprover: A framework tailored for improving robustness and efficiency in\ndrug optimization. It includes a LLM designed for drug optimization and a novel\nStructured Policy Optimization (SPO) algorithm, which is theoretically\ngrounded. This algorithm offers a unique perspective for fine-tuning the\nLLM-based generative model by aligning the improvement of the generated\nmolecule with the input molecule under desired objectives. (2) A dataset of 1\nmillion compounds, each with OEDOCK docking scores on 5 human proteins\nassociated with cancer cells and 24 binding sites from SARS-CoV-2 virus. We\nconduct a comprehensive evaluation of SPO and demonstrate its effectiveness in\nimproving the original drug across target properties. Our code and dataset will\nbe publicly available at: https://github.com/xuefeng-cs/DrugImproverGPT.", "published": "2025-02-11 04:00:21", "link": "http://arxiv.org/abs/2502.07237v1", "categories": ["cs.LG", "cs.CL", "q-bio.BM", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Hidden Division of Labor in Scientific Teams Revealed Through 1.6\n  Million LaTeX Files", "abstract": "Recognition of individual contributions is fundamental to the scientific\nreward system, yet coauthored papers obscure who did what. Traditional\nproxies-author order and career stage-reinforce biases, while contribution\nstatements remain self-reported and limited to select journals. We construct\nthe first large-scale dataset on writing contributions by analyzing\nauthor-specific macros in LaTeX files from 1.6 million papers (1991-2023) by 2\nmillion scientists. Validation against self-reported statements (precision =\n0.87), author order patterns, field-specific norms, and Overleaf records\n(Spearman's rho = 0.6, p < 0.05) confirms the reliability of the created data.\nUsing explicit section information, we reveal a hidden division of labor within\nscientific teams: some authors primarily contribute to conceptual sections\n(e.g., Introduction and Discussion), while others focus on technical sections\n(e.g., Methods and Experiments). These findings provide the first large-scale\nevidence of implicit labor division in scientific teams, challenging\nconventional authorship practices and informing institutional policies on\ncredit allocation.", "published": "2025-02-11 05:07:36", "link": "http://arxiv.org/abs/2502.07263v1", "categories": ["cs.SI", "cs.CL", "cs.DL"], "primary_category": "cs.SI"}
{"title": "When More is Less: Understanding Chain-of-Thought Length in LLMs", "abstract": "Chain-of-thought (CoT) reasoning enhances the multi-step reasoning\ncapabilities of large language models (LLMs) by breaking complex tasks into\nsmaller, manageable sub-tasks. Researchers have been exploring ways to guide\nmodels to generate more complex CoT processes to improve the reasoning ability\nof LLMs, such as long CoT and the test-time scaling law. However, for most\nmodels and tasks, does an increase in CoT length consistently lead to improved\nreasoning accuracy? In this paper, we observe a nuanced relationship: as the\nnumber of reasoning steps increases, performance initially improves but\neventually decreases. To understand this phenomenon, we provide a piece of\nevidence that longer reasoning processes are increasingly susceptible to noise.\nWe theoretically prove the existence of an optimal CoT length and derive a\nscaling law for this optimal length based on model capability and task\ndifficulty. Inspired by our theory, we conduct experiments on both synthetic\nand real world datasets and propose Length-filtered Vote to alleviate the\neffects of excessively long or short CoTs. Our findings highlight the critical\nneed to calibrate CoT length to align with model capabilities and task demands,\noffering a principled framework for optimizing multi-step reasoning in LLMs.", "published": "2025-02-11 05:28:59", "link": "http://arxiv.org/abs/2502.07266v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification", "abstract": "The interactions between DNA, RNA, and proteins are fundamental to biological\nprocesses, as illustrated by the central dogma of molecular biology. While\nmodern biological pre-trained models have achieved great success in analyzing\nthese macromolecules individually, their interconnected nature remains\nunder-explored. In this paper, we follow the guidance of the central dogma to\nredesign both the data and model pipeline and offer a comprehensive framework,\nLife-Code, that spans different biological functions. As for data flow, we\npropose a unified pipeline to integrate multi-omics data by\nreverse-transcribing RNA and reverse-translating amino acids into\nnucleotide-based sequences. As for the model, we design a codon tokenizer and a\nhybrid long-sequence architecture to encode the interactions of both coding and\nnon-coding regions with masked modeling pre-training. To model the translation\nand folding process with coding sequences, Life-Code learns protein structures\nof the corresponding amino acids by knowledge distillation from off-the-shelf\nprotein language models. Such designs enable Life-Code to capture complex\ninteractions within genetic sequences, providing a more comprehensive\nunderstanding of multi-omics with the central dogma. Extensive Experiments show\nthat Life-Code achieves state-of-the-art performance on various tasks across\nthree omics, highlighting its potential for advancing multi-omics analysis and\ninterpretation.", "published": "2025-02-11 06:53:59", "link": "http://arxiv.org/abs/2502.07299v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "q-bio.GN"], "primary_category": "cs.LG"}
{"title": "Bridging the Evaluation Gap: Leveraging Large Language Models for Topic\n  Model Evaluation", "abstract": "This study presents a framework for automated evaluation of dynamically\nevolving topic taxonomies in scientific literature using Large Language Models\n(LLMs). In digital library systems, topic modeling plays a crucial role in\nefficiently organizing and retrieving scholarly content, guiding researchers\nthrough complex knowledge landscapes. As research domains proliferate and\nshift, traditional human centric and static evaluation methods struggle to\nmaintain relevance. The proposed approach harnesses LLMs to measure key quality\ndimensions, such as coherence, repetitiveness, diversity, and topic-document\nalignment, without heavy reliance on expert annotators or narrow statistical\nmetrics. Tailored prompts guide LLM assessments, ensuring consistent and\ninterpretable evaluations across various datasets and modeling techniques.\nExperiments on benchmark corpora demonstrate the method's robustness,\nscalability, and adaptability, underscoring its value as a more holistic and\ndynamic alternative to conventional evaluation strategies.", "published": "2025-02-11 08:23:56", "link": "http://arxiv.org/abs/2502.07352v1", "categories": ["cs.CL", "cs.AI", "cs.DL"], "primary_category": "cs.CL"}
{"title": "EvoFlow: Evolving Diverse Agentic Workflows On The Fly", "abstract": "The past two years have witnessed the evolution of large language model\n(LLM)-based multi-agent systems from labor-intensive manual design to partial\nautomation (\\textit{e.g.}, prompt engineering, communication topology) and\neventually to fully automated design. However, existing agentic automation\npipelines often lack LLM heterogeneity and focus on single-objective\nperformance optimization, limiting their potential to combine weaker models for\nmore customized and cost-effective solutions. To address this challenge, we\npropose EvoFlow, a niching evolutionary algorithm-based framework to\nautomatically search a population of heterogeneous and complexity-adaptive\nagentic workflows, rather than a single homogeneous, complex workflow.\nTechnically, EvoFlow performs \\textit{(1) tag-based retrieval} to extract\nparent workflows from an agentic population, evolves new workflows through\n\\textit{(2) crossover} and \\textit{(3) mutation}, and employs \\textit{(4)\nniching-based selection} to maintain population diversity and quality.\nExtensive evaluations across seven benchmarks demonstrate that EvoFlow is:\n\\textbf{(I) diverse}, evolving a population of workflows ranging from simple\nI/O tasks to complex multi-turn interactions; \\textbf{(II) high-performing},\noutperforming previous handcrafted and automated workflows by\n$1.23\\%\\sim29.86\\%$; \\textbf{(III) economical}, surpassing powerful\n\\llmname{o1-preview} at $12.4\\%$ of its inference cost using weaker open-source\nmodels.", "published": "2025-02-11 08:48:46", "link": "http://arxiv.org/abs/2502.07373v1", "categories": ["cs.LG", "cs.CL", "cs.MA", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Forget What You Know about LLMs Evaluations -- LLMs are Like a Chameleon", "abstract": "Large language models (LLMs) often appear to excel on public benchmarks, but\nthese high scores may mask an overreliance on dataset-specific surface cues\nrather than true language understanding. We introduce the Chameleon Benchmark\nOverfit Detector (C-BOD), a meta-evaluation framework that systematically\ndistorts benchmark prompts via a parametric transformation and detects\noverfitting of LLMs. By rephrasing inputs while preserving their semantic\ncontent and labels, C-BOD exposes whether a model's performance is driven by\nmemorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our\nmethod reveals an average performance degradation of 2.15% under modest\nperturbations, with 20 out of 26 models exhibiting statistically significant\ndifferences. Notably, models with higher baseline accuracy exhibit larger\nperformance differences under perturbation, and larger LLMs tend to be more\nsensitive to rephrasings indicating that both cases may overrely on fixed\nprompt patterns. In contrast, the Llama family and models with lower baseline\naccuracy show insignificant degradation, suggesting reduced dependency on\nsuperficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows\neasy integration into training pipelines to promote more robust language\nunderstanding. Our findings challenge the community to look beyond leaderboard\nscores and prioritize resilience and generalization in LLM evaluation.", "published": "2025-02-11 10:43:36", "link": "http://arxiv.org/abs/2502.07445v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RusCode: Russian Cultural Code Benchmark for Text-to-Image Generation", "abstract": "Text-to-image generation models have gained popularity among users around the\nworld. However, many of these models exhibit a strong bias toward\nEnglish-speaking cultures, ignoring or misrepresenting the unique\ncharacteristics of other language groups, countries, and nationalities. The\nlack of cultural awareness can reduce the generation quality and lead to\nundesirable consequences such as unintentional insult, and the spread of\nprejudice. In contrast to the field of natural language processing, cultural\nawareness in computer vision has not been explored as extensively. In this\npaper, we strive to reduce this gap. We propose a RusCode benchmark for\nevaluating the quality of text-to-image generation containing elements of the\nRussian cultural code. To do this, we form a list of 19 categories that best\nrepresent the features of Russian visual culture. Our final dataset consists of\n1250 text prompts in Russian and their translations into English. The prompts\ncover a wide range of topics, including complex concepts from art, popular\nculture, folk traditions, famous people's names, natural objects, scientific\nachievements, etc. We present the results of a human evaluation of the\nside-by-side comparison of Russian visual concepts representations using\npopular generative models.", "published": "2025-02-11 10:57:12", "link": "http://arxiv.org/abs/2502.07455v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "PerCul: A Story-Driven Cultural Evaluation of LLMs in Persian", "abstract": "Large language models predominantly reflect Western cultures, largely due to\nthe dominance of English-centric training data. This imbalance presents a\nsignificant challenge, as LLMs are increasingly used across diverse contexts\nwithout adequate evaluation of their cultural competence in non-English\nlanguages, including Persian. To address this gap, we introduce PerCul, a\ncarefully constructed dataset designed to assess the sensitivity of LLMs toward\nPersian culture. PerCul features story-based, multiple-choice questions that\ncapture culturally nuanced scenarios. Unlike existing benchmarks, PerCul is\ncurated with input from native Persian annotators to ensure authenticity and to\nprevent the use of translation as a shortcut. We evaluate several\nstate-of-the-art multilingual and Persian-specific LLMs, establishing a\nfoundation for future research in cross-cultural NLP evaluation. Our\nexperiments demonstrate a 11.3% gap between best closed source model and\nlayperson baseline while the gap increases to 21.3% by using the best\nopen-weight model. You can access the dataset from here:\nhttps://huggingface.co/datasets/teias-ai/percul", "published": "2025-02-11 11:07:44", "link": "http://arxiv.org/abs/2502.07459v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7"], "primary_category": "cs.CL"}
{"title": "LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its\n  Hybrid", "abstract": "Linear sequence modeling approaches, such as linear attention, provide\nadvantages like linear-time training and constant-memory inference over\nsequence lengths. However, existing sequence parallelism (SP) methods are\neither not optimized for the right-product-first feature of linear attention or\nuse a ring-style communication strategy, which results in lower computation\nparallelism, limits their scalability for longer sequences in distributed\nsystems. In this paper, we introduce LASP-2, a new SP method to enhance both\ncommunication and computation parallelism when training linear attention\ntransformer models with very-long input sequences. Compared to previous work\nLASP, LASP-2 rethinks the minimal communication requirement for SP on linear\nattention layers, reorganizes the whole communication-computation workflow of\nLASP. In this way, only one single AllGather collective communication is needed\non intermediate memory states, whose sizes are independent of the sequence\nlength, leading to significant improvements of both communication and\ncomputation parallelism, as well as their overlap. Additionally, we extend\nLASP-2 to LASP-2H by applying similar communication redesign to standard\nattention modules, offering an efficient SP solution for hybrid models that\nblend linear and standard attention layers. Our evaluation on a Linear-Llama3\nmodel, a variant of Llama3 with linear attention replacing standard attention,\ndemonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2\nachieves training speed improvements of 15.2% over LASP and 36.6% over Ring\nAttention, with a sequence length of 2048K across 64 GPUs. The Code is released\nas a part of: https://github.com/OpenSparseLLMs/Linear-MoE.", "published": "2025-02-11 14:01:39", "link": "http://arxiv.org/abs/2502.07563v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Automated Capability Discovery via Model Self-Exploration", "abstract": "Foundation models have become general-purpose assistants, exhibiting diverse\ncapabilities across numerous domains through training on web-scale data. It\nremains challenging to precisely characterize even a fraction of the full\nspectrum of capabilities and potential risks in any new model. Existing\nevaluation approaches often require significant human effort, and it is taking\nincreasing effort to design ever harder challenges for more capable models. We\nintroduce Automated Capability Discovery (ACD), a framework that designates one\nfoundation model as a scientist to systematically propose open-ended tasks\nprobing the abilities of a subject model (potentially itself). By combining\nfrontier models with ideas from the field of open-endedness, ACD automatically\nand systematically uncovers both surprising capabilities and failures in the\nsubject model. We demonstrate ACD across a range of foundation models\n(including the GPT, Claude, and Llama series), showing that it automatically\nreveals thousands of capabilities that would be challenging for any single team\nto uncover. We further validate our method's automated scoring with extensive\nhuman surveys, observing high agreement between model-generated and human\nevaluations. By leveraging foundation models' ability to both create tasks and\nself-evaluate, ACD is a significant step toward scalable, automated evaluation\nof novel AI systems. All code and evaluation logs are open-sourced at\nhttps://github.com/conglu1997/ACD.", "published": "2025-02-11 14:23:13", "link": "http://arxiv.org/abs/2502.07577v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Human Decision-making is Susceptible to AI-driven Manipulation", "abstract": "Artificial Intelligence (AI) systems are increasingly intertwined with daily\nlife, assisting users in executing various tasks and providing guidance on\ndecision-making. This integration introduces risks of AI-driven manipulation,\nwhere such systems may exploit users' cognitive biases and emotional\nvulnerabilities to steer them toward harmful outcomes. Through a randomized\ncontrolled trial with 233 participants, we examined human susceptibility to\nsuch manipulation in financial (e.g., purchases) and emotional (e.g., conflict\nresolution) decision-making contexts. Participants interacted with one of three\nAI agents: a neutral agent (NA) optimizing for user benefit without explicit\ninfluence, a manipulative agent (MA) designed to covertly influence beliefs and\nbehaviors, or a strategy-enhanced manipulative agent (SEMA) employing explicit\npsychological tactics to reach its hidden objectives. By analyzing\nparticipants' decision patterns and shifts in their preference ratings\npost-interaction, we found significant susceptibility to AI-driven\nmanipulation. Particularly, across both decision-making domains, participants\ninteracting with the manipulative agents shifted toward harmful options at\nsubstantially higher rates (financial, MA: 62.3%, SEMA: 59.6%; emotional, MA:\n42.3%, SEMA: 41.5%) compared to the NA group (financial, 35.8%; emotional,\n12.8%). Notably, our findings reveal that even subtle manipulative objectives\n(MA) can be as effective as employing explicit psychological strategies (SEMA)\nin swaying human decision-making. By revealing the potential for covert AI\ninfluence, this study highlights a critical vulnerability in human-AI\ninteractions, emphasizing the need for ethical safeguards and regulatory\nframeworks to ensure responsible deployment of AI technologies and protect\nhuman autonomy.", "published": "2025-02-11 15:56:22", "link": "http://arxiv.org/abs/2502.07663v2", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Breaking Down Bias: On The Limits of Generalizable Pruning Strategies", "abstract": "We employ model pruning to examine how LLMs conceptualize racial biases, and\nwhether a generalizable mitigation strategy for such biases appears feasible.\nOur analysis yields several novel insights. We find that pruning can be an\neffective method to reduce bias without significantly increasing anomalous\nmodel behavior. Neuron-based pruning strategies generally yield better results\nthan approaches pruning entire attention heads. However, our results also show\nthat the effectiveness of either approach quickly deteriorates as pruning\nstrategies become more generalized. For instance, a model that is trained on\nremoving racial biases in the context of financial decision-making poorly\ngeneralizes to biases in commercial transactions. Overall, our analysis\nsuggests that racial biases are only partially represented as a general concept\nwithin language models. The other part of these biases is highly\ncontext-specific, suggesting that generalizable mitigation strategies may be of\nlimited effectiveness. Our findings have important implications for legal\nframeworks surrounding AI. In particular, they suggest that an effective\nmitigation strategy should include the allocation of legal responsibility on\nthose that deploy models in a specific use case.", "published": "2025-02-11 18:55:57", "link": "http://arxiv.org/abs/2502.07771v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Auditing Prompt Caching in Language Model APIs", "abstract": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.", "published": "2025-02-11 18:58:04", "link": "http://arxiv.org/abs/2502.07776v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Vision-Language Models for Edge Networks: A Comprehensive Survey", "abstract": "Vision Large Language Models (VLMs) combine visual understanding with natural\nlanguage processing, enabling tasks like image captioning, visual question\nanswering, and video analysis. While VLMs show impressive capabilities across\ndomains such as autonomous vehicles, smart surveillance, and healthcare, their\ndeployment on resource-constrained edge devices remains challenging due to\nprocessing power, memory, and energy limitations. This survey explores recent\nadvancements in optimizing VLMs for edge environments, focusing on model\ncompression techniques, including pruning, quantization, knowledge\ndistillation, and specialized hardware solutions that enhance efficiency. We\nprovide a detailed discussion of efficient training and fine-tuning methods,\nedge deployment challenges, and privacy considerations. Additionally, we\ndiscuss the diverse applications of lightweight VLMs across healthcare,\nenvironmental monitoring, and autonomous systems, illustrating their growing\nimpact. By highlighting key design strategies, current challenges, and offering\nrecommendations for future directions, this survey aims to inspire further\nresearch into the practical deployment of VLMs, ultimately making advanced AI\naccessible in resource-limited settings.", "published": "2025-02-11 14:04:43", "link": "http://arxiv.org/abs/2502.07855v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Training Sparse Mixture Of Experts Text Embedding Models", "abstract": "Transformer-based text embedding models have improved their performance on\nbenchmarks like MIRACL and BEIR by increasing their parameter counts. However,\nthis scaling approach introduces significant deployment challenges, including\nincreased inference latency and memory usage. These challenges are particularly\nsevere in retrieval-augmented generation (RAG) applications, where large\nmodels' increased memory requirements constrain dataset ingestion capacity, and\ntheir higher latency directly impacts query-time performance. While causal\nlanguage models have addressed similar efficiency challenges using Mixture of\nExperts (MoE) architectures, this approach hasn't been successfully adapted to\nthe general text embedding setting. In this paper, we introduce Nomic Embed v2,\nthe first general purpose MoE text embedding model. Our model outperforms\nmodels in the same parameter class on both monolingual and multilingual\nbenchmarks while also maintaining competitive performance with models twice its\nsize. We open-source all code, models, and evaluation data to ensure full\nreproducibility of our training pipeline at\n\\href{https://github.com/nomic-ai/contrastors}{https://github.com/nomic-ai/contrastors}.", "published": "2025-02-11 21:36:31", "link": "http://arxiv.org/abs/2502.07972v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Principled Data Selection for Alignment: The Hidden Risks of Difficult\n  Examples", "abstract": "The alignment of large language models (LLMs) often assumes that using more\nclean data yields better outcomes, overlooking the match between model capacity\nand example difficulty. Challenging this, we propose a new principle:\nPreference data vary in difficulty, and overly difficult examples hinder\nalignment, by exceeding the model's capacity. Through systematic\nexperimentation, we validate this principle with three key findings: (1)\npreference examples vary in difficulty, as evidenced by consistent learning\norders across alignment runs; (2) overly difficult examples significantly\ndegrade performance across four LLMs and two datasets; and (3) the capacity of\na model dictates its threshold for handling difficult examples, underscoring a\ncritical relationship between data selection and model capacity. Building on\nthis principle, we introduce Selective DPO, which filters out overly difficult\nexamples. This simple adjustment improves alignment performance by 9-16% in win\nrates on the AlpacaEval 2 benchmark compared to the DPO baseline, suppressing a\nseries of DPO variants with different algorithmic adjustments. Together, these\nresults illuminate the importance of aligning data difficulty with model\ncapacity, offering a transformative perspective for improving alignment\nstrategies in LLMs. Code is available at\nhttps://github.com/glorgao/SelectiveDPO.", "published": "2025-02-11 17:01:11", "link": "http://arxiv.org/abs/2502.09650v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MoHAVE: Mixture of Hierarchical Audio-Visual Experts for Robust Speech\n  Recognition", "abstract": "Audio-visual speech recognition (AVSR) has become critical for enhancing\nspeech recognition in noisy environments by integrating both auditory and\nvisual modalities. However, existing AVSR systems struggle to scale up without\ncompromising computational efficiency. In this study, we introduce MoHAVE\n(Mixture of Hierarchical Audio-Visual Experts), a novel robust AVSR framework\ndesigned to address these scalability constraints. By leveraging a\nMixture-of-Experts (MoE) architecture, MoHAVE activates modality-specific\nexpert groups, ensuring dynamic adaptation to various audio-visual inputs with\nminimal computational overhead. Key contributions of MoHAVE include: (1) a\nsparse MoE framework that efficiently scales AVSR model capacity, (2) a\nhierarchical gating mechanism that dynamically utilizes the expert groups based\non input context, enhancing adaptability and robustness, and (3) remarkable\nperformance across robust AVSR benchmarks, including LRS3 and MuAViC\ntranscription and translation tasks, setting a new standard for scalable speech\nrecognition systems.", "published": "2025-02-11 11:01:05", "link": "http://arxiv.org/abs/2502.10447v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Trustworthy AI on Safety, Bias, and Privacy: A Survey", "abstract": "The capabilities of artificial intelligence systems have been advancing to a\ngreat extent, but these systems still struggle with failure modes,\nvulnerabilities, and biases. In this paper, we study the current state of the\nfield, and present promising insights and perspectives regarding concerns that\nchallenge the trustworthiness of AI models. In particular, this paper\ninvestigates the issues regarding three thrusts: safety, privacy, and bias,\nwhich hurt models' trustworthiness. For safety, we discuss safety alignment in\nthe context of large language models, preventing them from generating toxic or\nharmful content. For bias, we focus on spurious biases that can mislead a\nnetwork. Lastly, for privacy, we cover membership inference attacks in deep\nneural networks. The discussions addressed in this paper reflect our own\nexperiments and observations.", "published": "2025-02-11 20:08:42", "link": "http://arxiv.org/abs/2502.10450v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Bridging Brain Signals and Language: A Deep Learning Approach to\n  EEG-to-Text Decoding", "abstract": "Brain activity translation into human language delivers the capability to\nrevolutionize machine-human interaction while providing communication support\nto people with speech disability. Electronic decoding reaches a certain level\nof achievement yet current EEG-to-text decoding methods fail to reach open\nvocabularies and depth of meaning and individual brain-specific variables. We\nintroduce a special framework which changes conventional closed-vocabulary\nEEG-to-text decoding approaches by integrating subject-specific learning models\nwith natural language processing methods to resolve detection obstacles. This\nmethod applies a deep representation learning approach to extract important EEG\nfeatures which allow training of neural networks to create elaborate sentences\nthat extend beyond original data content. The ZuCo dataset analysis\ndemonstrates that research findings achieve higher BLEU, ROUGE and BERTScore\nperformance when compared to current methods. The research proves how this\nframework functions as an effective approach to generate meaningful and correct\ntexts while understanding individual brain variations. The proposed research\naims to create a connection between open-vocabulary Text generation systems and\nhuman brain signal interpretation for developing efficacious brain-to-text\nsystems. The research produces interdisciplinary effects through innovative\nassistive technology development and personalized communication systems which\nextend possibilities for human-computer interaction in various settings.", "published": "2025-02-11 14:43:14", "link": "http://arxiv.org/abs/2502.17465v1", "categories": ["eess.SP", "cs.CL", "cs.LG"], "primary_category": "eess.SP"}
{"title": "MiniF2F in Rocq: Automatic Translation Between Proof Assistants -- A\n  Case Study", "abstract": "In this work, we conduct an experiment using state-of-the-art LLMs to\ntranslate MiniF2F into Rocq. The translation task focuses on generating a Rocq\ntheorem based on three sources: a natural language description, the Lean\nformalization, and the Isabelle formalization. We conducted our experiment in 3\nstages of increasing complexity, from basic one-shot prompting to multi-turn\nconversations that incorporate feedback from unsuccessful attempts. At each\nstage, we perform multiple rounds of translation using increasingly advanced\nmodels: GPT-4o mini, Claude 3.5 Sonnet, o1 mini, and o1. We successfully\ntranslated 478 out of 488 theorems. The dataset is opensource:\nhttps://github.com/LLM4Rocq/miniF2F-rocq.", "published": "2025-02-11 09:32:55", "link": "http://arxiv.org/abs/2503.04763v1", "categories": ["cs.LO", "cs.CL", "cs.LG", "cs.PL"], "primary_category": "cs.LO"}
{"title": "TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language\n  Navigation", "abstract": "In this work, we propose a modular approach for the Vision-Language\nNavigation (VLN) task by decomposing the problem into four sub-modules that use\nstate-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs)\nin a zero-shot setting. Given navigation instruction in natural language, we\nfirst prompt LLM to extract the landmarks and the order in which they are\nvisited. Assuming the known model of the environment, we retrieve the top-k\nlocations of the last landmark and generate $k$ path hypotheses from the\nstarting location to the last landmark using the shortest path algorithm on the\ntopological map of the environment. Each path hypothesis is represented by a\nsequence of panoramas. We then use dynamic programming to compute the alignment\nscore between the sequence of panoramas and the sequence of landmark names,\nwhich match scores obtained from VLM. Finally, we compute the nDTW metric\nbetween the hypothesis that yields the highest alignment score to evaluate the\npath fidelity. We demonstrate superior performance compared to other approaches\nthat use joint semantic maps like VLMaps \\cite{vlmaps} on the complex\nR2R-Habitat \\cite{r2r} instruction dataset and quantify in detail the effect of\nvisual grounding on navigation performance.", "published": "2025-02-11 07:09:37", "link": "http://arxiv.org/abs/2502.07306v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Music for All: Exploring Multicultural Representations in Music\n  Generation Models", "abstract": "The advent of Music-Language Models has greatly enhanced the automatic music\ngeneration capability of AI systems, but they are also limited in their\ncoverage of the musical genres and cultures of the world. We present a study of\nthe datasets and research papers for music generation and quantify the bias and\nunder-representation of genres. We find that only 5.7% of the total hours of\nexisting music datasets come from non-Western genres, which naturally leads to\ndisparate performance of the models across genres. We then investigate the\nefficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating\nthis bias. Our experiments with two popular models -- MusicGen and Mustango,\nfor two underrepresented non-Western music traditions -- Hindustani Classical\nand Turkish Makam music, highlight the promises as well as the non-triviality\nof cross-genre adaptation of music through small datasets, implying the need\nfor more equitable baseline music-language models that are designed for\ncross-cultural transfer learning.", "published": "2025-02-11 07:46:29", "link": "http://arxiv.org/abs/2502.07328v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.SD"}
{"title": "Which Economic Tasks are Performed with AI? Evidence from Millions of\n  Claude Conversations", "abstract": "Despite widespread speculation about artificial intelligence's impact on the\nfuture of work, we lack systematic empirical evidence about how these systems\nare actually being used for different tasks. Here, we present a novel framework\nfor measuring AI usage patterns across the economy. We leverage a recent\nprivacy-preserving system to analyze over four million Claude.ai conversations\nthrough the lens of tasks and occupations in the U.S. Department of Labor's\nO*NET Database. Our analysis reveals that AI usage primarily concentrates in\nsoftware development and writing tasks, which together account for nearly half\nof all total usage. However, usage of AI extends more broadly across the\neconomy, with approximately 36% of occupations using AI for at least a quarter\nof their associated tasks. We also analyze how AI is being used for tasks,\nfinding 57% of usage suggests augmentation of human capabilities (e.g.,\nlearning or iterating on an output) while 43% suggests automation (e.g.,\nfulfilling a request with minimal human involvement). While our data and\nmethods face important limitations and only paint a picture of AI usage on a\nsingle platform, they provide an automated, granular approach for tracking AI's\nevolving role in the economy and identifying leading indicators of future\nimpact as these technologies continue to advance.", "published": "2025-02-11 00:46:43", "link": "http://arxiv.org/abs/2503.04761v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Economics of Sourcing Human Data", "abstract": "Progress in AI has relied on human-generated data, from annotator\nmarketplaces to the wider Internet. However, the widespread use of large\nlanguage models now threatens the quality and integrity of human-generated data\non these very platforms. We argue that this issue goes beyond the immediate\nchallenge of filtering AI-generated content--it reveals deeper flaws in how\ndata collection systems are designed. Existing systems often prioritize speed,\nscale, and efficiency at the cost of intrinsic human motivation, leading to\ndeclining engagement and data quality. We propose that rethinking data\ncollection systems to align with contributors' intrinsic motivations--rather\nthan relying solely on external incentives--can help sustain high-quality data\nsourcing at scale while maintaining contributor trust and long-term\nparticipation.", "published": "2025-02-11 17:51:52", "link": "http://arxiv.org/abs/2502.07732v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CV", "cs.HC", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Towards Understanding of Frequency Dependence on Sound Event Detection", "abstract": "In this work, various analysis methods are conducted on frequency-dependent\nmethods on SED to further delve into their detailed characteristics and\nbehaviors on SED. While SED has been rapidly advancing through the adoption of\nvarious deep learning techniques from other pattern recognition fields, these\ntechniques are often not suitable for SED. To address this issue, two\nfrequency-dependent SED methods were previously proposed: FilterAugment, a data\naugmentation randomly weighting frequency bands, and frequency dynamic\nconvolution (FDY Conv), an architecture applying frequency adaptive convolution\nkernels. These methods have demonstrated superior performance in SED, and we\naim to further analyze their detailed effectiveness and characteristics in SED.\nWe compare class-wise performance to find out specific pros and cons of\nFilterAugment and FDY Conv. We apply Gradient-weighted Class Activation Mapping\n(Grad-CAM), which highlights time-frequency region that is more inferred by the\nmodel, on SED models with and without frequency masking and two types of\nFilterAugment to observe their detailed characteristics. We propose simpler\nfrequency dependent convolution methods and compare them with FDY Conv to\nfurther understand which components of FDY Conv affects SED performance.\nLastly, we apply PCA to show how FDY Conv adapts dynamic kernel across\nfrequency dimensions on different sound event classes. The results and\ndiscussions demonstrate that frequency dependency plays a significant role in\nsound event detection and further confirms the effectiveness of frequency\ndependent methods on SED.", "published": "2025-02-11 03:07:03", "link": "http://arxiv.org/abs/2502.07208v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Advanced Zero-Shot Text-to-Speech for Background Removal and\n  Preservation with Controllable Masked Speech Prediction", "abstract": "The acoustic background plays a crucial role in natural conversation. It\nprovides context and helps listeners understand the environment, but a strong\nbackground makes it difficult for listeners to understand spoken words. The\nappropriate handling of these backgrounds is situation-dependent: Although it\nmay be necessary to remove background to ensure speech clarity, preserving the\nbackground is sometimes crucial to maintaining the contextual integrity of the\nspeech. Despite recent advancements in zero-shot Text-to-Speech technologies,\ncurrent systems often struggle with speech prompts containing backgrounds. To\naddress these challenges, we propose a Controllable Masked Speech Prediction\nstrategy coupled with a dual-speaker encoder, utilizing a task-related control\nsignal to guide the prediction of dual background removal and preservation\ntargets. Experimental results demonstrate that our approach enables precise\ncontrol over the removal or preservation of background across various acoustic\nconditions and exhibits strong generalization capabilities in unseen scenarios.", "published": "2025-02-11 08:17:07", "link": "http://arxiv.org/abs/2502.07345v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RenderBox: Expressive Performance Rendering with Text Control", "abstract": "Expressive music performance rendering involves interpreting symbolic scores\nwith variations in timing, dynamics, articulation, and instrument-specific\ntechniques, resulting in performances that capture musical can emotional\nintent. We introduce RenderBox, a unified framework for text-and-score\ncontrolled audio performance generation across multiple instruments, applying\ncoarse-level controls through natural language descriptions and granular-level\ncontrols using music scores. Based on a diffusion transformer architecture and\ncross-attention joint conditioning, we propose a curriculum-based paradigm that\ntrains from plain synthesis to expressive performance, gradually incorporating\ncontrollable factors such as speed, mistakes, and style diversity.\n  RenderBox achieves high performance compared to baseline models across key\nmetrics such as FAD and CLAP, and also tempo and pitch accuracy under different\nprompting tasks. Subjective evaluation further demonstrates that RenderBox is\nable to generate controllable expressive performances that sound natural and\nmusically engaging, aligning well with prompts and intent.", "published": "2025-02-11 17:10:31", "link": "http://arxiv.org/abs/2502.07711v1", "categories": ["eess.AS", "cs.MM"], "primary_category": "eess.AS"}
{"title": "VINP: Variational Bayesian Inference with Neural Speech Prior for Joint\n  ASR-Effective Speech Dereverberation and Blind RIR Identification", "abstract": "Reverberant speech, denoting the speech signal degraded by the process of\nreverberation, contains crucial knowledge of both anechoic source speech and\nroom impulse response (RIR). This work proposes a variational Bayesian\ninference (VBI) framework with neural speech prior (VINP) for joint speech\ndereverberation and blind RIR identification. In VINP, a probabilistic signal\nmodel is constructed in the time-frequency (T-F) domain based on convolution\ntransfer function (CTF) approximation. For the first time, we propose using an\narbitrary discriminative dereverberation deep neural network (DNN) to predict\nthe prior distribution of anechoic speech within a probabilistic model. By\nintegrating both reverberant speech and the anechoic speech prior, VINP yields\nthe maximum a posteriori (MAP) and maximum likelihood (ML) estimations of the\nanechoic speech spectrum and CTF filter, respectively. After simple\ntransformations, the waveforms of anechoic speech and RIR are estimated.\nMoreover, VINP is effective for automatic speech recognition (ASR) systems,\nwhich sets it apart from most deep learning (DL)-based single-channel\ndereverberation approaches. Experiments on single-channel speech\ndereverberation demonstrate that VINP reaches an advanced level in most metrics\nrelated to human perception and displays unquestionable state-of-the-art (SOTA)\nperformance in ASR-related metrics. For blind RIR identification, experiments\nindicate that VINP attains the SOTA level in blind estimation of reverberation\ntime at 60 dB (RT60) and direct-to-reverberation ratio (DRR). Codes and audio\nsamples are available online.", "published": "2025-02-11 02:54:28", "link": "http://arxiv.org/abs/2502.07205v2", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Vevo: Controllable Zero-Shot Voice Imitation with Self-Supervised\n  Disentanglement", "abstract": "The imitation of voice, targeted on specific speech attributes such as timbre\nand speaking style, is crucial in speech generation. However, existing methods\nrely heavily on annotated data, and struggle with effectively disentangling\ntimbre and style, leading to challenges in achieving controllable generation,\nespecially in zero-shot scenarios. To address these issues, we propose Vevo, a\nversatile zero-shot voice imitation framework with controllable timbre and\nstyle. Vevo operates in two core stages: (1) Content-Style Modeling: Given\neither text or speech's content tokens as input, we utilize an autoregressive\ntransformer to generate the content-style tokens, which is prompted by a style\nreference; (2) Acoustic Modeling: Given the content-style tokens as input, we\nemploy a flow-matching transformer to produce acoustic representations, which\nis prompted by a timbre reference. To obtain the content and content-style\ntokens of speech, we design a fully self-supervised approach that progressively\ndecouples the timbre, style, and linguistic content of speech. Specifically, we\nadopt VQ-VAE as the tokenizer for the continuous hidden features of HuBERT. We\ntreat the vocabulary size of the VQ-VAE codebook as the information bottleneck,\nand adjust it carefully to obtain the disentangled speech representations.\nSolely self-supervised trained on 60K hours of audiobook speech data, without\nany fine-tuning on style-specific corpora, Vevo matches or surpasses existing\nmethods in accent and emotion conversion tasks. Additionally, Vevo's\neffectiveness in zero-shot voice conversion and text-to-speech tasks further\ndemonstrates its strong generalization and versatility. Audio samples are\navailable at https://versavoice.github.io.", "published": "2025-02-11 04:18:33", "link": "http://arxiv.org/abs/2502.07243v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Visual-based spatial audio generation system for multi-speaker\n  environments", "abstract": "In multimedia applications such as films and video games, spatial audio\ntechniques are widely employed to enhance user experiences by simulating 3D\nsound: transforming mono audio into binaural formats. However, this process is\noften complex and labor-intensive for sound designers, requiring precise\nsynchronization of audio with the spatial positions of visual components. To\naddress these challenges, we propose a visual-based spatial audio generation\nsystem - an automated system that integrates face detection YOLOv8 for object\ndetection, monocular depth estimation, and spatial audio techniques. Notably,\nthe system operates without requiring additional binaural dataset training. The\nproposed system is evaluated against existing Spatial Audio generation system\nusing objective metrics. Experimental results demonstrate that our method\nsignificantly improves spatial consistency between audio and video, enhances\nspeech quality, and performs robustly in multi-speaker scenarios. By\nstreamlining the audio-visual alignment process, the proposed system enables\nsound engineers to achieve high-quality results efficiently, making it a\nvaluable tool for professionals in multimedia production.", "published": "2025-02-11 13:24:38", "link": "http://arxiv.org/abs/2502.07538v2", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "LoRP-TTS: Low-Rank Personalized Text-To-Speech", "abstract": "Speech synthesis models convert written text into natural-sounding audio.\nWhile earlier models were limited to a single speaker, recent advancements have\nled to the development of zero-shot systems that generate realistic speech from\na wide range of speakers using their voices as additional prompts. However,\nthey still struggle with imitating non-studio-quality samples that differ\nsignificantly from the training datasets. In this work, we demonstrate that\nutilizing Low-Rank Adaptation (LoRA) allows us to successfully use even single\nrecordings of spontaneous speech in noisy environments as prompts. This\napproach enhances speaker similarity by up to $30pp$ while preserving content\nand naturalness. It represents a significant step toward creating truly diverse\nspeech corpora, that is crucial in all speech-related tasks.", "published": "2025-02-11 14:00:12", "link": "http://arxiv.org/abs/2502.07562v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
