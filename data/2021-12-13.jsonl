{"title": "Sparse Structure Learning via Graph Neural Networks for Inductive\n  Document Classification", "abstract": "Recently, graph neural networks (GNNs) have been widely used for document\nclassification. However, most existing methods are based on static word\nco-occurrence graphs without sentence-level information, which poses three\nchallenges:(1) word ambiguity, (2) word synonymity, and (3) dynamic contextual\ndependency. To address these challenges, we propose a novel GNN-based sparse\nstructure learning model for inductive document classification. Specifically, a\ndocument-level graph is initially generated by a disjoint union of\nsentence-level word co-occurrence graphs. Our model collects a set of trainable\nedges connecting disjoint words between sentences and employs structure\nlearning to sparsely select edges with dynamic contextual dependencies. Graphs\nwith sparse structures can jointly exploit local and global contextual\ninformation in documents through GNNs. For inductive learning, the refined\ndocument graph is further fed into a general readout function for graph-level\nclassification and optimization in an end-to-end manner. Extensive experiments\non several real-world datasets demonstrate that the proposed model outperforms\nmost state-of-the-art results, and reveal the necessity to learn sparse\nstructures for each document.", "published": "2021-12-13 02:36:04", "link": "http://arxiv.org/abs/2112.06386v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Plurality and Quantification in Graph Representation of Meaning", "abstract": "In this thesis we present a semantic representation formalism based on\ndirected graphs and explore its linguistic adequacy and explanatory benefits in\nthe semantics of plurality and quantification. Our graph language covers the\nessentials of natural language semantics using only monadic second-order\nvariables. We define its model-theoretical interpretation in terms of graph\ntraversal, where the relative scope of variables arises from their order of\nvaluation. We present a unification-based mechanism for constructing semantic\ngraphs at a simple syntax-semantics interface, where syntax as a partition\nfunction on discourse referents is implemented with categorial grammars by\nestablishing a partly deterministic relation between semantics and syntactic\ndistribution. This mechanism is automated to facilitate future exploration. The\npresent graph formalism is applied to linguistic issues in distributive\npredication, cross-categorial conjunction, and scope permutation of\nquantificational expressions, including the exceptional scoping behaviors of\nindefinites.", "published": "2021-12-13 07:04:41", "link": "http://arxiv.org/abs/2112.06448v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting User Code-Switching Level from Sociological and Psychological\n  Profiles", "abstract": "Multilingual speakers tend to alternate between languages within a\nconversation, a phenomenon referred to as \"code-switching\" (CS). CS is a\ncomplex phenomenon that not only encompasses linguistic challenges, but also\ncontains a great deal of complexity in terms of its dynamic behaviour across\nspeakers. This dynamic behaviour has been studied by sociologists and\npsychologists, identifying factors affecting CS. In this paper, we provide an\nempirical user study on Arabic-English CS, where we show the correlation\nbetween users' CS frequency and character traits. We use machine learning (ML)\nto validate the findings, informing and confirming existing theories. The\npredictive models were able to predict users' CS frequency with an accuracy\nhigher than 55%, where travel experiences and personality traits played the\nbiggest role in the modeling process.", "published": "2021-12-13 07:36:02", "link": "http://arxiv.org/abs/2112.06462v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ITA: Image-Text Alignments for Multi-Modal Named Entity Recognition", "abstract": "Recently, Multi-modal Named Entity Recognition (MNER) has attracted a lot of\nattention. Most of the work utilizes image information through region-level\nvisual representations obtained from a pretrained object detector and relies on\nan attention mechanism to model the interactions between image and text\nrepresentations. However, it is difficult to model such interactions as image\nand text representations are trained separately on the data of their respective\nmodality and are not aligned in the same space. As text representations take\nthe most important role in MNER, in this paper, we propose {\\bf I}mage-{\\bf\nt}ext {\\bf A}lignments (ITA) to align image features into the textual space, so\nthat the attention mechanism in transformer-based pretrained textual embeddings\ncan be better utilized. ITA first aligns the image into regional object tags,\nimage-level captions and optical characters as visual contexts, concatenates\nthem with the input texts as a new cross-modal input, and then feeds it into a\npretrained textual embedding model. This makes it easier for the attention\nmodule of a pretrained textual embedding model to model the interaction between\nthe two modalities since they are both represented in the textual space. ITA\nfurther aligns the output distributions predicted from the cross-modal input\nand textual input views so that the MNER model can be more practical in dealing\nwith text-only inputs and robust to noises from images. In our experiments, we\nshow that ITA models can achieve state-of-the-art accuracy on multi-modal Named\nEntity Recognition datasets, even without image information.", "published": "2021-12-13 08:29:43", "link": "http://arxiv.org/abs/2112.06482v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Native Chinese Reader: A Dataset Towards Native-Level Chinese Machine\n  Reading Comprehension", "abstract": "We present Native Chinese Reader (NCR), a new machine reading comprehension\n(MRC) dataset with particularly long articles in both modern and classical\nChinese. NCR is collected from the exam questions for the Chinese course in\nChina's high schools, which are designed to evaluate the language proficiency\nof native Chinese youth. Existing Chinese MRC datasets are either\ndomain-specific or focusing on short contexts of a few hundreds of characters\nin modern Chinese only. By contrast, NCR contains 8390 documents with an\naverage length of 1024 characters covering a wide range of Chinese writing\nstyles, including modern articles, classical literature and classical poetry. A\ntotal of 20477 questions on these documents also require strong reasoning\nabilities and common sense to figure out the correct answers. We implemented\nmultiple baseline models using popular Chinese pre-trained models and\nadditionally launched an online competition using our dataset to examine the\nlimit of current methods. The best model achieves 59% test accuracy while human\nevaluation shows an average accuracy of 79%, which indicates a significant\nperformance gap between current MRC models and native Chinese speakers. We\nrelease the dataset at https://sites.google.com/view/native-chinese-reader/.", "published": "2021-12-13 09:11:38", "link": "http://arxiv.org/abs/2112.06494v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Evidence Collection for Fake News Detection", "abstract": "Fake news, misinformation, and unverifiable facts on social media platforms\npropagate disharmony and affect society, especially when dealing with an\nepidemic like COVID-19. The task of Fake News Detection aims to tackle the\neffects of such misinformation by classifying news items as fake or real. In\nthis paper, we propose a novel approach that improves over the current\nautomatic fake news detection approaches by automatically gathering evidence\nfor each claim. Our approach extracts supporting evidence from the web articles\nand then selects appropriate text to be treated as evidence sets. We use a\npre-trained summarizer on these evidence sets and then use the extracted\nsummary as supporting evidence to aid the classification task. Our experiments,\nusing both machine learning and deep learning-based methods, help perform an\nextensive evaluation of our approach. The results show that our approach\noutperforms the state-of-the-art methods in fake news detection to achieve an\nF1-score of 99.25 over the dataset provided for the CONSTRAINT-2021 Shared\nTask. We also release the augmented dataset, our code and models for any\nfurther research.", "published": "2021-12-13 09:38:41", "link": "http://arxiv.org/abs/2112.06507v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Data-based Curricula Work?", "abstract": "Current state-of-the-art NLP systems use large neural networks that require\nlots of computational resources for training. Inspired by human knowledge\nacquisition, researchers have proposed curriculum learning, - sequencing of\ntasks (task-based curricula) or ordering and sampling of the datasets\n(data-based curricula) that facilitate training. This work investigates the\nbenefits of data-based curriculum learning for large modern language models\nsuch as BERT and T5. We experiment with various curricula based on a range of\ncomplexity measures and different sampling strategies. Extensive experiments on\ndifferent NLP tasks show that curricula based on various complexity measures\nrarely has any benefits while random sampling performs either as well or better\nthan curricula.", "published": "2021-12-13 09:42:32", "link": "http://arxiv.org/abs/2112.06510v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WECHSEL: Effective initialization of subword embeddings for\n  cross-lingual transfer of monolingual language models", "abstract": "Large pretrained language models (LMs) have become the central building block\nof many NLP applications. Training these models requires ever more\ncomputational resources and most of the existing models are trained on English\ntext only. It is exceedingly expensive to train these models in other\nlanguages. To alleviate this problem, we introduce a novel method -- called\nWECHSEL -- to efficiently and effectively transfer pretrained LMs to new\nlanguages. WECHSEL can be applied to any model which uses subword-based\ntokenization and learns an embedding for each subword. The tokenizer of the\nsource model (in English) is replaced with a tokenizer in the target language\nand token embeddings are initialized such that they are semantically similar to\nthe English tokens by utilizing multilingual static word embeddings covering\nEnglish and the target language. We use WECHSEL to transfer the English RoBERTa\nand GPT-2 models to four languages (French, German, Chinese and Swahili). We\nalso study the benefits of our method on very low-resource languages. WECHSEL\nimproves over proposed methods for cross-lingual parameter transfer and\noutperforms models of comparable size trained from scratch with up to 64x less\ntraining effort. Our method makes training large language models for new\nlanguages more accessible and less damaging to the environment. We make our\ncode and models publicly available.", "published": "2021-12-13 12:26:02", "link": "http://arxiv.org/abs/2112.06598v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ANEA: Automated (Named) Entity Annotation for German Domain-Specific\n  Texts", "abstract": "Named entity recognition (NER) is an important task that aims to resolve\nuniversal categories of named entities, e.g., persons, locations,\norganizations, and times. Despite its common and viable use in many use cases,\nNER is barely applicable in domains where general categories are suboptimal,\nsuch as engineering or medicine. To facilitate NER of domain-specific types, we\npropose ANEA, an automated (named) entity annotator to assist human annotators\nin creating domain-specific NER corpora for German text collections when given\na set of domain-specific texts. In our evaluation, we find that ANEA\nautomatically identifies terms that best represent the texts' content,\nidentifies groups of coherent terms, and extracts and assigns descriptive\nlabels to these groups, i.e., annotates text datasets into the domain (named)\nentities.", "published": "2021-12-13 15:09:06", "link": "http://arxiv.org/abs/2112.06724v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Context-Word Biases in Lexical Semantic Datasets", "abstract": "State-of-the-art pretrained contextualized models (PCM) eg. BERT use tasks\nsuch as WiC and WSD to evaluate their word-in-context representations. This\ninherently assumes that performance in these tasks reflect how well a model\nrepresents the coupled word and context semantics. We question this assumption\nby presenting the first quantitative analysis on the context-word interaction\nbeing tested in major contextual lexical semantic tasks. To achieve this, we\nrun probing baselines on masked input, and propose measures to calculate and\nvisualize the degree of context or word biases in existing datasets. The\nanalysis was performed on both models and humans. Our findings demonstrate that\nmodels are usually not being tested for word-in-context semantics in the same\nway as humans are in these tasks, which helps us better understand the\nmodel-human gap. Specifically, to PCMs, most existing datasets fall into the\nextreme ends (the retrieval-based tasks exhibit strong target word bias while\nWiC-style tasks and WSD show strong context bias); In comparison, humans are\nless biased and achieve much better performance when both word and context are\navailable than with masked input. We recommend our framework for understanding\nand controlling these biases for model interpretation and future task design.", "published": "2021-12-13 15:37:05", "link": "http://arxiv.org/abs/2112.06733v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Roof-Transformer: Divided and Joined Understanding with Knowledge\n  Enhancement", "abstract": "Recent work on enhancing BERT-based language representation models with\nknowledge graphs (KGs) and knowledge bases (KBs) has yielded promising results\non multiple NLP tasks. State-of-the-art approaches typically integrate the\noriginal input sentences with KG triples and feed the combined representation\ninto a BERT model. However, as the sequence length of a BERT model is limited,\nsuch a framework supports little knowledge other than the original input\nsentences and is thus forced to discard some knowledge. This problem is\nespecially severe for downstream tasks for which the input is a long paragraph\nor even a document, such as QA or reading comprehension tasks. We address this\nproblem with Roof-Transformer, a model with two underlying BERTs and a fusion\nlayer on top. One underlying BERT encodes the knowledge resources and the other\none encodes the original input sentences, and the fusion layer integrates the\ntwo resultant encodings. Experimental results on a QA task and the GLUE\nbenchmark attest the effectiveness of the proposed model.", "published": "2021-12-13 15:40:54", "link": "http://arxiv.org/abs/2112.06736v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Khmer Text Classification Using Word Embedding and Neural Networks", "abstract": "Text classification is one of the fundamental tasks in natural language\nprocessing to label an open-ended text and is useful for various applications\nsuch as sentiment analysis. In this paper, we discuss various classification\napproaches for Khmer text, ranging from a classical TF-IDF algorithm with\nsupport vector machine classifier to modern word embedding-based neural network\nclassifiers including linear layer model, recurrent neural network and\nconvolutional neural network. A Khmer word embedding model is trained on a\n30-million-Khmer-word corpus to construct word vector representations that are\nused to train three different neural network classifiers. We evaluate the\nperformance of different approaches on a news article dataset for both\nmulti-class and multi-label text classification tasks. The result suggests that\nneural network classifiers using a word embedding model consistently outperform\nthe traditional classifier using TF-IDF. The recurrent neural network\nclassifier provides a slightly better result compared to the convolutional\nnetwork and the linear layer network.", "published": "2021-12-13 15:57:32", "link": "http://arxiv.org/abs/2112.06748v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Keyphrase Generation Beyond the Boundaries of Title and Abstract", "abstract": "Keyphrase generation aims at generating important phrases (keyphrases) that\nbest describe a given document. In scholarly domains, current approaches have\nlargely used only the title and abstract of the articles to generate\nkeyphrases. In this paper, we comprehensively explore whether the integration\nof additional information from the full text of a given article or from\nsemantically similar articles can be helpful for a neural keyphrase generation\nmodel or not. We discover that adding sentences from the full text,\nparticularly in the form of the extractive summary of the article can\nsignificantly improve the generation of both types of keyphrases that are\neither present or absent from the text. Experimental results with three widely\nused models for keyphrase generation along with one of the latest transformer\nmodels suitable for longer documents, Longformer Encoder-Decoder (LED) validate\nthe observation. We also present a new large-scale scholarly dataset FullTextKP\nfor keyphrase generation. Unlike prior large-scale datasets, FullTextKP\nincludes the full text of the articles along with the title and abstract. We\nrelease the source code at https://github.com/kgarg8/FullTextKP.", "published": "2021-12-13 16:33:01", "link": "http://arxiv.org/abs/2112.06776v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A cognitively driven weighted-entropy model for embedding semantic\n  categories in hyperbolic geometry", "abstract": "In this paper, an unsupervised and cognitively driven weighted-entropy method\nfor embedding semantic categories in hyperbolic geometry is proposed. The model\nis driven by two fields of research in cognitive linguistics: the first is the\nstatistical learning theory of language acquisition and the proposal of using\nhigh-dimensional networks to represent semantic knowledge in cognition, and the\nsecond is the domain-specific approach to semantic communication. Weighted\nconditional entropy of word co-occurrence is proposed as the embedding metric,\nand the two weighting parameters are collocation diversity and conditional\nprobability ranking in the corresponding statistical distribution. The\nBoltzmann distribution is then used on the weighted-entropy metric and embedded\ninto a hyperbolic Poincare disk model. Testing has been in particular performed\nin the domains of basic color and kinship words, which belong to the classes\nthat domain-specificity focused research in cognitive semantics has most\nintensively investigated. Results show that this new approach can successfully\nmodel and map the semantic relationships of popularity and similarity for most\nof the basic color and kinship words in English and have potential to be\ngeneralized to other semantic domains and different languages. Generally, this\npaper contributes to both computational cognitive semantics and the research on\nnetwork and geometry-driven language embedding in computational linguistics and\nNLP.", "published": "2021-12-13 18:33:45", "link": "http://arxiv.org/abs/2112.06876v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts", "abstract": "Scaling language models with more data, compute and parameters has driven\nsignificant progress in natural language processing. For example, thanks to\nscaling, GPT-3 was able to achieve strong results on in-context learning tasks.\nHowever, training these large dense models requires significant amounts of\ncomputing resources. In this paper, we propose and develop a family of language\nmodels named GLaM (Generalist Language Model), which uses a sparsely activated\nmixture-of-experts architecture to scale the model capacity while also\nincurring substantially less training cost compared to dense variants. The\nlargest GLaM has 1.2 trillion parameters, which is approximately 7x larger than\nGPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half\nof the computation flops for inference, while still achieving better overall\nzero-shot and one-shot performance across 29 NLP tasks.", "published": "2021-12-13 18:58:19", "link": "http://arxiv.org/abs/2112.06905v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "English2Gbe: A multilingual machine translation model for {Fon/Ewe}Gbe", "abstract": "Language is an essential factor of emancipation. Unfortunately, most of the\nmore than 2,000 African languages are low-resourced. The community has recently\nused machine translation to revive and strengthen several African languages.\nHowever, the trained models are often bilingual, resulting in a potentially\nexponential number of models to train and maintain to cover all possible\ntranslation directions. Additionally, bilingual models do not leverage the\nsimilarity between some of the languages. Consequently, multilingual neural\nmachine translation (NMT) is gaining considerable interest, especially for\nlow-resourced languages. Nevertheless, its adoption by the community is still\nlimited. This paper introduces English2Gbe, a multilingual NMT model capable of\ntranslating from English to Ewe or Fon. Using the BLEU, CHRF, and TER scores\ncomputed with the Sacrebleu (Post, 2018) package for reproducibility, we show\nthat English2Gbe outperforms bilingual models (English to Ewe and English to\nFon) and gives state-of-the-art results on the JW300 benchmark for Fon\nestablished by Nekoto et al. (2020). We hope this work will contribute to the\nmassive adoption of Multilingual models inside the community. Our code is made\naccessible from Github.", "published": "2021-12-13 10:35:09", "link": "http://arxiv.org/abs/2112.11482v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Surfer100: Generating Surveys From Web Resources, Wikipedia-style", "abstract": "Fast-developing fields such as Artificial Intelligence (AI) often outpace the\nefforts of encyclopedic sources such as Wikipedia, which either do not\ncompletely cover recently-introduced topics or lack such content entirely. As a\nresult, methods for automatically producing content are valuable tools to\naddress this information overload. We show that recent advances in pretrained\nlanguage modeling can be combined for a two-stage extractive and abstractive\napproach for Wikipedia lead paragraph generation. We extend this approach to\ngenerate longer Wikipedia-style summaries with sections and examine how such\nmethods struggle in this application through detailed studies with 100\nreference human-collected surveys. This is the first study on utilizing web\nresources for long Wikipedia-style summaries to the best of our knowledge.", "published": "2021-12-13 02:18:01", "link": "http://arxiv.org/abs/2112.06377v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey of Toxic Comment Classification Methods", "abstract": "While in real life everyone behaves themselves at least to some extent, it is\nmuch more difficult to expect people to behave themselves on the internet,\nbecause there are few checks or consequences for posting something toxic to\nothers. Yet, for people on the other side, toxic texts often lead to serious\npsychological consequences. Detecting such toxic texts is challenging. In this\npaper, we attempt to build a toxicity detector using machine learning methods\nincluding CNN, Naive Bayes model, as well as LSTM. While there has been\nnumerous groundwork laid by others, we aim to build models that provide higher\naccuracy than the predecessors. We produced very high accuracy models using\nLSTM and CNN, and compared them to the go-to solutions in language processing,\nthe Naive Bayes model. A word embedding approach is also applied to empower the\naccuracy of our models.", "published": "2021-12-13 04:17:20", "link": "http://arxiv.org/abs/2112.06412v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Study on Token Pruning for ColBERT", "abstract": "The ColBERT model has recently been proposed as an effective BERT based\nranker. By adopting a late interaction mechanism, a major advantage of ColBERT\nis that document representations can be precomputed in advance. However, the\nbig downside of the model is the index size, which scales linearly with the\nnumber of tokens in the collection. In this paper, we study various designs for\nColBERT models in order to attack this problem. While compression techniques\nhave been explored to reduce the index size, in this paper we study token\npruning techniques for ColBERT. We compare simple heuristics, as well as a\nsingle layer of attention mechanism to select the tokens to keep at indexing\ntime. Our experiments show that ColBERT indexes can be pruned up to 30\\% on the\nMS MARCO passage collection without a significant drop in performance. Finally,\nwe experiment on MS MARCO documents, which reveal several challenges for such\nmechanism.", "published": "2021-12-13 10:24:54", "link": "http://arxiv.org/abs/2112.06540v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Understanding and Improving the Exemplar-based Generation for\n  Open-domain Conversation", "abstract": "Exemplar-based generative models for open-domain conversation produce\nresponses based on the exemplars provided by the retriever, taking advantage of\ngenerative models and retrieval models. However, they often ignore the\nretrieved exemplars while generating responses or produce responses over-fitted\nto the retrieved exemplars. In this paper, we argue that these drawbacks are\nderived from the one-to-many problem of the open-domain conversation. When the\nretrieved exemplar is relevant to the given context yet significantly different\nfrom the gold response, the exemplar-based generative models are trained to\nignore the exemplar since the exemplar is not helpful for generating the gold\nresponse. On the other hand, when the retrieved exemplar is lexically similar\nto the gold response, the generative models are trained to rely on the exemplar\nhighly. Therefore, we propose a training method selecting exemplars that are\nsemantically relevant to the gold response but lexically distanced from the\ngold response to mitigate the above disadvantages. In the training phase, our\nproposed training method first uses the gold response instead of dialogue\ncontext as a query to select exemplars that are semantically relevant to the\ngold response. And then, it eliminates the exemplars that lexically resemble\nthe gold responses to alleviate the dependency of the generative models on that\nexemplars. The remaining exemplars could be irrelevant to the given context\nsince they are searched depending on the gold response. Thus, our proposed\ntraining method further utilizes the relevance scores between the given context\nand the exemplars to penalize the irrelevant exemplars. Extensive experiments\ndemonstrate that our proposed training method alleviates the drawbacks of the\nexisting exemplar-based generative models and significantly improves the\nperformance in terms of appropriateness and informativeness.", "published": "2021-12-13 15:06:09", "link": "http://arxiv.org/abs/2112.06723v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Attentive Contextual Carryover for Multi-Turn End-to-End Spoken Language\n  Understanding", "abstract": "Recent years have seen significant advances in end-to-end (E2E) spoken\nlanguage understanding (SLU) systems, which directly predict intents and slots\nfrom spoken audio. While dialogue history has been exploited to improve\nconventional text-based natural language understanding systems, current E2E SLU\napproaches have not yet incorporated such critical contextual signals in\nmulti-turn and task-oriented dialogues. In this work, we propose a contextual\nE2E SLU model architecture that uses a multi-head attention mechanism over\nencoded previous utterances and dialogue acts (actions taken by the voice\nassistant) of a multi-turn dialogue. We detail alternative methods to integrate\nthese contexts into the state-ofthe-art recurrent and transformer-based models.\nWhen applied to a large de-identified dataset of utterances collected by a\nvoice assistant, our method reduces average word and semantic error rates by\n10.8% and 12.6%, respectively. We also present results on a publicly available\ndataset and show that our method significantly improves performance over a\nnoncontextual baseline", "published": "2021-12-13 15:49:36", "link": "http://arxiv.org/abs/2112.06743v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Step-unrolled Denoising Autoencoders for Text Generation", "abstract": "In this paper we propose a new generative model of text, Step-unrolled\nDenoising Autoencoder (SUNDAE), that does not rely on autoregressive models.\nSimilarly to denoising diffusion techniques, SUNDAE is repeatedly applied on a\nsequence of tokens, starting from random inputs and improving them each time\nuntil convergence. We present a simple new improvement operator that converges\nin fewer iterations than diffusion methods, while qualitatively producing\nbetter samples on natural language datasets. SUNDAE achieves state-of-the-art\nresults (among non-autoregressive methods) on the WMT'14 English-to-German\ntranslation task and good qualitative results on unconditional language\nmodeling on the Colossal Cleaned Common Crawl dataset and a dataset of Python\ncode from GitHub. The non-autoregressive nature of SUNDAE opens up\npossibilities beyond left-to-right prompted generation, by filling in arbitrary\nblank patterns in a template.", "published": "2021-12-13 16:00:33", "link": "http://arxiv.org/abs/2112.06749v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sparse Interventions in Language Models with Differentiable Masking", "abstract": "There has been a lot of interest in understanding what information is\ncaptured by hidden representations of language models (LMs). Typically,\ninterpretation methods i) do not guarantee that the model actually uses the\nencoded information, and ii) do not discover small subsets of neurons\nresponsible for a considered phenomenon. Inspired by causal mediation analysis,\nwe propose a method that discovers within a neural LM a small subset of neurons\nresponsible for a particular linguistic phenomenon, i.e., subsets causing a\nchange in the corresponding token emission probabilities. We use a\ndifferentiable relaxation to approximately search through the combinatorial\nspace. An $L_0$ regularization term ensures that the search converges to\ndiscrete and sparse solutions. We apply our method to analyze subject-verb\nnumber agreement and gender bias detection in LSTMs. We observe that it is fast\nand finds better solutions than the alternative (REINFORCE). Our experiments\nconfirm that each of these phenomenons is mediated through a small subset of\nneurons that do not play any other discernible role.", "published": "2021-12-13 17:49:16", "link": "http://arxiv.org/abs/2112.06837v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generating Fluent Fact Checking Explanations with Unsupervised\n  Post-Editing", "abstract": "Fact-checking systems have become important tools to verify fake and\nmisguiding news. These systems become more trustworthy when human-readable\nexplanations accompany the veracity labels. However, manual collection of such\nexplanations is expensive and time-consuming. Recent works frame explanation\ngeneration as extractive summarization, and propose to automatically select a\nsufficient subset of the most important facts from the ruling comments (RCs) of\na professional journalist to obtain fact-checking explanations. However, these\nexplanations lack fluency and sentence coherence. In this work, we present an\niterative edit-based algorithm that uses only phrase-level edits to perform\nunsupervised post-editing of disconnected RCs. To regulate our editing\nalgorithm, we use a scoring function with components including fluency and\nsemantic preservation. In addition, we show the applicability of our approach\nin a completely unsupervised setting. We experiment with two benchmark\ndatasets, LIAR-PLUS and PubHealth. We show that our model generates\nexplanations that are fluent, readable, non-redundant, and cover important\ninformation for the fact check.", "published": "2021-12-13 15:31:07", "link": "http://arxiv.org/abs/2112.06924v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Framework para Caracterizar Fake News en Terminos de Emociones", "abstract": "Social networks have become one of the main information channels for human\nbeings due to the immediate and social interactivity they offer, allowing in\nsome cases to publish what each user considers relevant. This has brought with\nit the generation of false news or Fake News, publications that only seek to\ngenerate uncertainty, misinformation or skew the opinion of readers. It has\nbeen shown that the human being is not capable of fully identifying whether an\narticle is really a fact or a Fake News, due to this it is that models arise\nthat seek to characterize and identify articles based on data mining and\nmachine learning. This article proposes a three-layer framework, the main\nobjective of which is to characterize the emotions present in Fake News and to\nbe a tool for future work that identifies the emotional state and intentional\nstate of the public.", "published": "2021-12-13 21:46:21", "link": "http://arxiv.org/abs/2112.07035v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Large Language Models are not Models of Natural Language: they are\n  Corpus Models", "abstract": "Natural Language Processing (NLP) has become one of the leading application\nareas in the current Artificial Intelligence boom. Transfer learning has\nenabled large deep learning neural networks trained on the language modeling\ntask to vastly improve performance in almost all downstream language tasks.\nInterestingly, when the language models are trained with data that includes\nsoftware code, they demonstrate remarkable abilities in generating functioning\ncomputer code from natural language specifications. We argue that this creates\na conundrum for the claim that eliminative neural models are a radical\nrestructuring in our understanding of cognition in that they eliminate the need\nfor symbolic abstractions like generative phrase structure grammars. Because\nthe syntax of programming languages is by design determined by phrase structure\ngrammars, neural models that produce syntactic code are apparently\nuninformative about the theoretical foundations of programming languages. The\ndemonstration that neural models perform well on tasks that involve clearly\nsymbolic systems, proves that they cannot be used as an argument that language\nand other cognitive systems are not symbolic. Finally, we argue as a corollary\nthat the term language model is misleading and propose the adoption of the\nworking term corpus model instead, which better reflects the genesis and\ncontents of the model.", "published": "2021-12-13 22:39:46", "link": "http://arxiv.org/abs/2112.07055v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The King is Naked: on the Notion of Robustness for Natural Language\n  Processing", "abstract": "There is growing evidence that the classical notion of adversarial robustness\noriginally introduced for images has been adopted as a de facto standard by a\nlarge part of the NLP research community. We show that this notion is\nproblematic in the context of NLP as it considers a narrow spectrum of\nlinguistic phenomena. In this paper, we argue for semantic robustness, which is\nbetter aligned with the human concept of linguistic fidelity. We characterize\nsemantic robustness in terms of biases that it is expected to induce in a\nmodel. We study semantic robustness of a range of vanilla and robustly trained\narchitectures using a template-based generative test bed. We complement the\nanalysis with empirical evidence that, despite being harder to implement,\nsemantic robustness can improve performance %gives guarantees for on complex\nlinguistic phenomena where models robust in the classical sense fail.", "published": "2021-12-13 16:19:48", "link": "http://arxiv.org/abs/2112.07605v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Compression of Natural Language Models", "abstract": "Deep neural networks are effective feature extractors but they are\nprohibitively large for deployment scenarios. Due to the huge number of\nparameters, interpretability of parameters in different layers is not\nstraight-forward. This is why neural networks are sometimes considered black\nboxes. Although simpler models are easier to explain, finding them is not easy.\nIf found, a sparse network that can fit to a data from scratch would help to\ninterpret parameters of a neural network. To this end, lottery ticket\nhypothesis states that typical dense neural networks contain a small sparse\nsub-network that can be trained to a reach similar test accuracy in an equal\nnumber of steps. The goal of this work is to assess whether such a trainable\nsubnetwork exists for natural language models (NLM)s. To achieve this goal we\nwill review state-of-the-art compression techniques such as quantization,\nknowledge distillation, and pruning.", "published": "2021-12-13 08:14:21", "link": "http://arxiv.org/abs/2112.11480v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Translating Human Mobility Forecasting through Natural Language\n  Generation", "abstract": "Existing human mobility forecasting models follow the standard design of the\ntime-series prediction model which takes a series of numerical values as input\nto generate a numerical value as a prediction. Although treating this as a\nregression problem seems straightforward, incorporating various contextual\ninformation such as the semantic category information of each Place-of-Interest\n(POI) is a necessary step, and often the bottleneck, in designing an effective\nmobility prediction model. As opposed to the typical approach, we treat\nforecasting as a translation problem and propose a novel forecasting through a\nlanguage generation pipeline. The paper aims to address the human mobility\nforecasting problem as a language translation task in a sequence-to-sequence\nmanner. A mobility-to-language template is first introduced to describe the\nnumerical mobility data as natural language sentences. The core intuition of\nthe human mobility forecasting translation task is to convert the input\nmobility description sentences into a future mobility description from which\nthe prediction target can be obtained. Under this pipeline, a two-branch\nnetwork, SHIFT (Translating Human Mobility Forecasting), is designed.\nSpecifically, it consists of one main branch for language generation and one\nauxiliary branch to directly learn mobility patterns. During the training, we\ndevelop a momentum mode for better connecting and training the two branches.\nExtensive experiments on three real-world datasets demonstrate that the\nproposed SHIFT is effective and presents a new revolutionary approach to\nforecasting human mobility.", "published": "2021-12-13 09:56:27", "link": "http://arxiv.org/abs/2112.11481v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dependency Learning for Legal Judgment Prediction with a Unified\n  Text-to-Text Transformer", "abstract": "Given the fact of a case, Legal Judgment Prediction (LJP) involves a series\nof sub-tasks such as predicting violated law articles, charges and term of\npenalty. We propose leveraging a unified text-to-text Transformer for LJP,\nwhere the dependencies among sub-tasks can be naturally established within the\nauto-regressive decoder. Compared with previous works, it has three advantages:\n(1) it fits in the pretraining pattern of masked language models, and thereby\ncan benefit from the semantic prompts of each sub-task rather than treating\nthem as atomic labels, (2) it utilizes a single unified architecture, enabling\nfull parameter sharing across all sub-tasks, and (3) it can incorporate both\nclassification and generative sub-tasks. We show that this unified transformer,\nalbeit pretrained on general-domain text, outperforms pretrained models\ntailored specifically for the legal domain. Through an extensive set of\nexperiments, we find that the best order to capture dependencies is different\nfrom human intuitions, and the most reasonable logical order for humans can be\nsub-optimal for the model. We further include two more auxiliary tasks: court\nview generation and article content prediction, showing they can not only\nimprove the prediction accuracy, but also provide interpretable explanations\nfor model outputs even when an error is made. With the best configuration, our\nmodel outperforms both previous SOTA and a single-tasked version of the unified\ntransformer by a large margin.", "published": "2021-12-13 01:38:37", "link": "http://arxiv.org/abs/2112.06370v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Detecting Emotion Carriers by Combining Acoustic and Lexical\n  Representations", "abstract": "Personal narratives (PN) - spoken or written - are recollections of facts,\npeople, events, and thoughts from one's own experience. Emotion recognition and\nsentiment analysis tasks are usually defined at the utterance or document\nlevel. However, in this work, we focus on Emotion Carriers (EC) defined as the\nsegments (speech or text) that best explain the emotional state of the narrator\n(\"loss of father\", \"made me choose\"). Once extracted, such EC can provide a\nricher representation of the user state to improve natural language\nunderstanding and dialogue modeling. In previous work, it has been shown that\nEC can be identified using lexical features. However, spoken narratives should\nprovide a richer description of the context and the users' emotional state. In\nthis paper, we leverage word-based acoustic and textual embeddings as well as\nearly and late fusion techniques for the detection of ECs in spoken narratives.\nFor the acoustic word-level representations, we use Residual Neural Networks\n(ResNet) pretrained on separate speech emotion corpora and fine-tuned to detect\nEC. Experiments with different fusion and system combination strategies show\nthat late fusion leads to significant improvements for this task.", "published": "2021-12-13 12:39:53", "link": "http://arxiv.org/abs/2112.06603v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "PM-MMUT: Boosted Phone-Mask Data Augmentation using Multi-Modeling Unit\n  Training for Phonetic-Reduction-Robust E2E Speech Recognition", "abstract": "Consonant and vowel reduction are often encountered in speech, which might\ncause performance degradation in automatic speech recognition (ASR). Our\nrecently proposed learning strategy based on masking, Phone Masking Training\n(PMT), alleviates the impact of such phenomenon in Uyghur ASR. Although PMT\nachieves remarkably improvements, there still exists room for further gains due\nto the granularity mismatch between the masking unit of PMT (phoneme) and the\nmodeling unit (word-piece). To boost the performance of PMT, we propose\nmulti-modeling unit training (MMUT) architecture fusion with PMT (PM-MMUT). The\nidea of MMUT framework is to split the Encoder into two parts including\nacoustic feature sequences to phoneme-level representation (AF-to-PLR) and\nphoneme-level representation to word-piece-level representation (PLR-to-WPLR).\nIt allows AF-to-PLR to be optimized by an intermediate phoneme-based CTC loss\nto learn the rich phoneme-level context information brought by PMT.\nExperimental results on Uyghur ASR show that the proposed approaches outperform\nobviously the pure PMT. We also conduct experiments on the 960-hour Librispeech\nbenchmark using ESPnet1, which achieves about 10% relative WER reduction on all\nthe test set without LM fusion comparing with the latest official ESPnet1\npre-trained model.", "published": "2021-12-13 15:04:33", "link": "http://arxiv.org/abs/2112.06721v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VL-Adapter: Parameter-Efficient Transfer Learning for\n  Vision-and-Language Tasks", "abstract": "Recently, fine-tuning language models pre-trained on large text corpora have\nprovided huge improvements on vision-and-language (V&L) tasks as well as on\npure language tasks. However, fine-tuning the entire parameter set of\npre-trained models becomes impractical since the model size is growing rapidly.\nHence, in this paper, we introduce adapter-based parameter-efficient transfer\nlearning techniques to V&L models such as VL-BART and VLT5. We evaluate our\nmethods in a unified multi-task setup on both image-text and video-text\nbenchmarks. For the image-text tasks, we use four diverse V&L datasets: VQAv2,\nGQA, NLVR2 , and MSCOCO image captioning. For video-text tasks, we use TVQA,\nHow2QA, TVC, and YC2C. With careful training and thorough experiments, we\nbenchmark three popular adapter-based methods (Adapter, Hyperformer, Compacter)\nagainst the standard full fine-tuning and the recently proposed prompt-tuning\napproach. We also enhance the efficiency and performance of adapters by sharing\ntheir weights to attain knowledge across tasks. Our results demonstrate that\ntraining the adapter with the weight-sharing technique (4.18% of total\nparameters for image-text tasks and 3.39% for video-text tasks) can match the\nperformance of fine-tuning the entire model. Lastly, we present a comprehensive\nanalysis including the combination of adapter and task-specific prompts and the\nimpact of V&L pre-training on adapters. Our code is available at:\nhttps://github.com/ylsung/VL_adapter.", "published": "2021-12-13 17:35:26", "link": "http://arxiv.org/abs/2112.06825v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Improving and Diagnosing Knowledge-Based Visual Question Answering via\n  Entity Enhanced Knowledge Injection", "abstract": "Knowledge-Based Visual Question Answering (KBVQA) is a bi-modal task\nrequiring external world knowledge in order to correctly answer a text question\nand associated image. Recent single modality text work has shown knowledge\ninjection into pre-trained language models, specifically entity enhanced\nknowledge graph embeddings, can improve performance on downstream\nentity-centric tasks. In this work, we empirically study how and whether such\nmethods, applied in a bi-modal setting, can improve an existing VQA system's\nperformance on the KBVQA task. We experiment with two large publicly available\nVQA datasets, (1) KVQA which contains mostly rare Wikipedia entities and (2)\nOKVQA which is less entity-centric and more aligned with common sense\nreasoning. Both lack explicit entity spans and we study the effect of different\nweakly supervised and manual methods for obtaining them. Additionally we\nanalyze how recently proposed bi-modal and single modal attention explanations\nare affected by the incorporation of such entity enhanced representations. Our\nresults show substantial improved performance on the KBVQA task without the\nneed for additional costly pre-training and we provide insights for when entity\nknowledge injection helps improve a model's understanding. We provide code and\nenhanced datasets for reproducibility.", "published": "2021-12-13 18:45:42", "link": "http://arxiv.org/abs/2112.06888v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Controlled Cue Generation for Play Scripts", "abstract": "In this paper, we use a large-scale play scripts dataset to propose the novel\ntask of theatrical cue generation from dialogues. Using over one million lines\nof dialogue and cues, we approach the problem of cue generation as a controlled\ntext generation task, and show how cues can be used to enhance the impact of\ndialogue using a language model conditioned on a dialogue/cue discriminator. In\naddition, we explore the use of topic keywords and emotions for controlled text\ngeneration. Extensive quantitative and qualitative experiments show that\nlanguage models can be successfully used to generate plausible and\nattribute-controlled texts in highly specialised domains such as play scripts.\nSupporting materials can be found at: https://catlab-team.github.io/cuegen.", "published": "2021-12-13 19:00:17", "link": "http://arxiv.org/abs/2112.06953v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Event Based Time-Vectors for auditory features extraction: a\n  neuromorphic approach for low power audio recognition", "abstract": "In recent years tremendous efforts have been done to advance the state of the\nart for Natural Language Processing (NLP) and audio recognition. However, these\nefforts often translated in increased power consumption and memory requirements\nfor bigger and more complex models. These solutions falls short of the\nconstraints of IoT devices which need low power, low memory efficient\ncomputation, and therefore they fail to meet the growing demand of efficient\nedge computing. Neuromorphic systems have proved to be excellent candidates for\nlow-power low-latency computation in a multitude of applications. For this\nreason we present a neuromorphic architecture, capable of unsupervised auditory\nfeature recognition. We then validate the network on a subset of Google's\nSpeech Commands dataset.", "published": "2021-12-13 21:08:04", "link": "http://arxiv.org/abs/2112.07011v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ISEEQ: Information Seeking Question Generation using Dynamic\n  Meta-Information Retrieval and Knowledge Graphs", "abstract": "Conversational Information Seeking (CIS) is a relatively new research area\nwithin conversational AI that attempts to seek information from end-users in\norder to understand and satisfy users' needs. If realized, such a system has\nfar-reaching benefits in the real world; for example, a CIS system can assist\nclinicians in pre-screening or triaging patients in healthcare. A key open\nsub-problem in CIS that remains unaddressed in the literature is generating\nInformation Seeking Questions (ISQs) based on a short initial query from the\nend-user. To address this open problem, we propose Information SEEking Question\ngenerator (ISEEQ), a novel approach for generating ISQs from just a short user\nquery, given a large text corpus relevant to the user query. Firstly, ISEEQ\nuses a knowledge graph to enrich the user query. Secondly, ISEEQ uses the\nknowledge-enriched query to retrieve relevant context passages to ask coherent\nISQs adhering to a conceptual flow. Thirdly, ISEEQ introduces a new deep\ngenerative-adversarial reinforcement learning-based approach for generating\nISQs. We show that ISEEQ can generate high-quality ISQs to promote the\ndevelopment of CIS agents. ISEEQ significantly outperforms comparable baselines\non five ISQ evaluation metrics across four datasets having user queries from\ndiverse domains. Further, we argue that ISEEQ is transferable across domains\nfor generating ISQs, as it shows the acceptable performance when trained and\ntested on different pairs of domains. The qualitative human evaluation confirms\nISEEQ-generated ISQs are comparable in quality to human-generated questions and\noutperform the best comparable baseline.", "published": "2021-12-13 04:02:13", "link": "http://arxiv.org/abs/2112.07622v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Detecting Audio Adversarial Examples with Logit Noising", "abstract": "Automatic speech recognition (ASR) systems are vulnerable to audio\nadversarial examples that attempt to deceive ASR systems by adding\nperturbations to benign speech signals. Although an adversarial example and the\noriginal benign wave are indistinguishable to humans, the former is transcribed\nas a malicious target sentence by ASR systems. Several methods have been\nproposed to generate audio adversarial examples and feed them directly into the\nASR system (over-line). Furthermore, many researchers have demonstrated the\nfeasibility of robust physical audio adversarial examples(over-air). To defend\nagainst the attacks, several studies have been proposed. However, deploying\nthem in a real-world situation is difficult because of accuracy drop or time\noverhead. In this paper, we propose a novel method to detect audio adversarial\nexamples by adding noise to the logits before feeding them into the decoder of\nthe ASR. We show that carefully selected noise can significantly impact the\ntranscription results of the audio adversarial examples, whereas it has minimal\nimpact on the transcription results of benign audio waves. Based on this\ncharacteristic, we detect audio adversarial examples by comparing the\ntranscription altered by logit noising with its original transcription. The\nproposed method can be easily applied to ASR systems without any structural\nchanges or additional training. The experimental results show that the proposed\nmethod is robust to over-line audio adversarial examples as well as over-air\naudio adversarial examples compared with state-of-the-art detection methods.", "published": "2021-12-13 06:57:34", "link": "http://arxiv.org/abs/2112.06443v1", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "Computational bioacoustics with deep learning: a review and roadmap", "abstract": "Animal vocalisations and natural soundscapes are fascinating objects of\nstudy, and contain valuable evidence about animal behaviours, populations and\necosystems. They are studied in bioacoustics and ecoacoustics, with signal\nprocessing and analysis an important component. Computational bioacoustics has\naccelerated in recent decades due to the growth of affordable digital sound\nrecording devices, and to huge progress in informatics such as big data, signal\nprocessing and machine learning. Methods are inherited from the wider field of\ndeep learning, including speech and image processing. However, the tasks,\ndemands and data characteristics are often different from those addressed in\nspeech or music analysis. There remain unsolved problems, and tasks for which\nevidence is surely present in many acoustic signals, but not yet realised. In\nthis paper I perform a review of the state of the art in deep learning for\ncomputational bioacoustics, aiming to clarify key concepts and identify and\nanalyse knowledge gaps. Based on this, I offer a subjective but principled\nroadmap for computational bioacoustics with deep learning: topics that the\ncommunity should aim to address, in order to make the most of future\ndevelopments in AI and informatics, and to use audio data in answering\nzoological and ecological questions.", "published": "2021-12-13 15:10:01", "link": "http://arxiv.org/abs/2112.06725v1", "categories": ["cs.SD", "eess.AS", "q-bio.QM"], "primary_category": "cs.SD"}
{"title": "Decoding High-level Imagined Speech using Attention-based Deep Neural\n  Networks", "abstract": "Brain-computer interface (BCI) is the technology that enables the\ncommunication between humans and devices by reflecting status and intentions of\nhumans. When conducting imagined speech, the users imagine the pronunciation as\nif actually speaking. In the case of decoding imagined speech-based EEG\nsignals, complex task can be conducted more intuitively, but decoding\nperformance is lower than that of other BCI paradigms. We modified our previous\nmodel for decoding imagined speech-based EEG signals. Ten subjects participated\nin the experiment. The average accuracy of our proposed method was 0.5648 for\nclassifying four words. In other words, our proposed method has significant\nstrength in learning local features. Hence, we demonstrated the feasibility of\ndecoding imagined speech-based EEG signals with robust performance.", "published": "2021-12-13 08:33:04", "link": "http://arxiv.org/abs/2112.06922v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
