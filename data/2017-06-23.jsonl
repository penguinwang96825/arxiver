{"title": "Named Entity Recognition with stack residual LSTM and trainable bias\n  decoding", "abstract": "Recurrent Neural Network models are the state-of-the-art for Named Entity\nRecognition (NER). We present two innovations to improve the performance of\nthese models. The first innovation is the introduction of residual connections\nbetween the Stacked Recurrent Neural Network model to address the degradation\nproblem of deep neural networks. The second innovation is a bias decoding\nmechanism that allows the trained system to adapt to non-differentiable and\nexternally computed objectives, such as the entity-based F-measure. Our work\nimproves the state-of-the-art results for both Spanish and English languages on\nthe standard train/development/test split of the CoNLL 2003 Shared Task NER\ndataset.", "published": "2017-06-23 08:33:38", "link": "http://arxiv.org/abs/1706.07598v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparison of Modified Kneser-Ney and Witten-Bell Smoothing Techniques\n  in Statistical Language Model of Bahasa Indonesia", "abstract": "Smoothing is one technique to overcome data sparsity in statistical language\nmodel. Although in its mathematical definition there is no explicit dependency\nupon specific natural language, different natures of natural languages result\nin different effects of smoothing techniques. This is true for Russian language\nas shown by Whittaker (1998). In this paper, We compared Modified Kneser-Ney\nand Witten-Bell smoothing techniques in statistical language model of Bahasa\nIndonesia. We used train sets of totally 22M words that we extracted from\nIndonesian version of Wikipedia. As far as we know, this is the largest train\nset used to build statistical language model for Bahasa Indonesia. The\nexperiments with 3-gram, 5-gram, and 7-gram showed that Modified Kneser-Ney\nconsistently outperforms Witten-Bell smoothing technique in term of perplexity\nvalues. It is interesting to note that our experiments showed 5-gram model for\nModified Kneser-Ney smoothing technique outperforms that of 7-gram. Meanwhile,\nWitten-Bell smoothing is consistently improving over the increase of n-gram\norder.", "published": "2017-06-23 17:43:20", "link": "http://arxiv.org/abs/1706.07786v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
