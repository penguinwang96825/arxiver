{"title": "Controlling Multimodal LLMs via Reward-guided Decoding", "abstract": "As Multimodal Large Language Models (MLLMs) gain widespread applicability, it\nis becoming increasingly desirable to adapt them for diverse user needs. In\nthis paper, we study the adaptation of MLLMs through controlled decoding. To\nachieve this, we introduce the first method for reward-guided decoding of MLLMs\nand demonstrate its application in improving their visual grounding. Our method\ninvolves building reward models for visual grounding and using them to guide\nthe MLLM's decoding process. Concretely, we build two separate reward models to\nindependently control the degree of object precision and recall in the model's\noutput. Our approach enables on-the-fly controllability of an MLLM's inference\nprocess in two ways: first, by giving control over the relative importance of\neach reward function during decoding, allowing a user to dynamically trade off\nobject precision for recall in image captioning tasks; second, by giving\ncontrol over the breadth of the search during decoding, allowing the user to\ncontrol the trade-off between the amount of test-time compute and the degree of\nvisual grounding. We evaluate our method on standard object hallucination\nbenchmarks, showing that it provides significant controllability over MLLM\ninference, while consistently outperforming existing hallucination mitigation\nmethods.", "published": "2025-08-15 17:29:06", "link": "http://arxiv.org/abs/2508.11616v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "TinyTim: A Family of Language Models for Divergent Generation", "abstract": "This work introduces TinyTim, a family of large language models fine-tuned on\nJames Joyce's `Finnegans Wake'. Through quantitative evaluation against\nbaseline models, we demonstrate that TinyTim V1 produces a statistically\ndistinct generative profile characterized by high lexical diversity and low\nsemantic coherence. These findings are interpreted through theories of\ncreativity and complex problem-solving, arguing that such specialized models\ncan function as divergent knowledge sources within more extensive creative\narchitectures, powering automated discovery mechanisms in diverse settings.", "published": "2025-08-15 17:14:29", "link": "http://arxiv.org/abs/2508.11607v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dataset Creation for Visual Entailment using Generative AI", "abstract": "In this paper we present and validate a new synthetic dataset for training\nvisual entailment models. Existing datasets for visual entailment are small and\nsparse compared to datasets for textual entailment. Manually creating datasets\nis labor-intensive. We base our synthetic dataset on the SNLI dataset for\ntextual entailment. We take the premise text from SNLI as input prompts in a\ngenerative image model, Stable Diffusion, creating an image to replace each\ntextual premise. We evaluate our dataset both intrinsically and extrinsically.\nFor extrinsic evaluation, we evaluate the validity of the generated images by\nusing them as training data for a visual entailment classifier based on CLIP\nfeature vectors. We find that synthetic training data only leads to a slight\ndrop in quality on SNLI-VE, with an F-score 0.686 compared to 0.703 when\ntrained on real data. We also compare the quality of our generated training\ndata to original training data on another dataset: SICK-VTE. Again, there is\nonly a slight drop in F-score: from 0.400 to 0.384. These results indicate that\nin settings with data sparsity, synthetic data can be a promising solution for\ntraining visual entailment models.", "published": "2025-08-15 17:13:41", "link": "http://arxiv.org/abs/2508.11605v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Representing Speech Through Autoregressive Prediction of Cochlear Tokens", "abstract": "We introduce AuriStream, a biologically inspired model for encoding speech\nvia a two-stage framework inspired by the human auditory processing hierarchy.\nThe first stage transforms raw audio into a time-frequency representation based\non the human cochlea, from which we extract discrete \\textbf{cochlear tokens}.\nThe second stage applies an autoregressive sequence model over the cochlear\ntokens. AuriStream learns meaningful phoneme and word representations, and\nstate-of-the-art lexical semantics. AuriStream shows competitive performance on\ndiverse downstream SUPERB speech tasks. Complementing AuriStream's strong\nrepresentational capabilities, it generates continuations of audio which can be\nvisualized in a spectrogram space and decoded back into audio, providing\ninsights into the model's predictions. In summary, we present a two-stage\nframework for speech representation learning to advance the development of more\nhuman-like models that efficiently handle a range of speech-based tasks.", "published": "2025-08-15 17:06:04", "link": "http://arxiv.org/abs/2508.11598v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models", "abstract": "Recent advancements in large language models (LLMs) have greatly improved\ntheir capabilities on complex reasoning tasks through Long Chain-of-Thought\n(CoT). However, this approach often results in substantial redundancy,\nimpairing computational efficiency and causing significant delays in real-time\napplications. To improve the efficiency, current methods often rely on\nhuman-defined difficulty priors, which do not align with the LLM's self-awared\ndifficulty, leading to inefficiencies. In this paper, we introduce the Dynamic\nReasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to\ndynamically assess and adjust their reasoning depth in response to problem\ncomplexity. DR. SAF integrates three key components: Boundary Self-Awareness\nAlignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.\nThese components allow models to optimize their reasoning processes, balancing\nefficiency and accuracy without compromising performance. Our experimental\nresults demonstrate that DR. SAF achieves a 49.27% reduction in total response\ntokens with minimal loss in accuracy. The framework also delivers a 6.59x gain\nin token efficiency and a 5x reduction in training time, making it well-suited\nto resource-limited settings. During extreme training, DR. SAF can even surpass\ntraditional instruction-based models in token efficiency with more than 16%\naccuracy improvement.", "published": "2025-08-15 16:40:29", "link": "http://arxiv.org/abs/2508.11582v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AgentMental: An Interactive Multi-Agent Framework for Explainable and Adaptive Mental Health Assessment", "abstract": "Mental health assessment is crucial for early intervention and effective\ntreatment, yet traditional clinician-based approaches are limited by the\nshortage of qualified professionals. Recent advances in artificial intelligence\nhave sparked growing interest in automated psychological assessment, yet most\nexisting approaches are constrained by their reliance on static text analysis,\nlimiting their ability to capture deeper and more informative insights that\nemerge through dynamic interaction and iterative questioning. Therefore, in\nthis paper, we propose a multi-agent framework for mental health evaluation\nthat simulates clinical doctor-patient dialogues, with specialized agents\nassigned to questioning, adequacy evaluation, scoring, and updating. We\nintroduce an adaptive questioning mechanism in which an evaluation agent\nassesses the adequacy of user responses to determine the necessity of\ngenerating targeted follow-up queries to address ambiguity and missing\ninformation. Additionally, we employ a tree-structured memory in which the root\nnode encodes the user's basic information, while child nodes (e.g., topic and\nstatement) organize key information according to distinct symptom categories\nand interaction turns. This memory is dynamically updated throughout the\ninteraction to reduce redundant questioning and further enhance the information\nextraction and contextual tracking capabilities. Experimental results on the\nDAIC-WOZ dataset illustrate the effectiveness of our proposed method, which\nachieves better performance than existing approaches.", "published": "2025-08-15 16:20:45", "link": "http://arxiv.org/abs/2508.11567v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emphasis Sensitivity in Speech Representations", "abstract": "This work investigates whether modern speech models are sensitive to prosodic\nemphasis - whether they encode emphasized and neutral words in systematically\ndifferent ways. Prior work typically relies on isolated acoustic correlates\n(e.g., pitch, duration) or label prediction, both of which miss the relational\nstructure of emphasis. This paper proposes a residual-based framework, defining\nemphasis as the difference between paired neutral and emphasized word\nrepresentations. Analysis on self-supervised speech models shows that these\nresiduals correlate strongly with duration changes and perform poorly at word\nidentity prediction, indicating a structured, relational encoding of prosodic\nemphasis. In ASR fine-tuned models, residuals occupy a subspace up to 50% more\ncompact than in pre-trained models, further suggesting that emphasis is encoded\nas a consistent, low-dimensional transformation that becomes more structured\nwith task-specific learning.", "published": "2025-08-15 16:18:47", "link": "http://arxiv.org/abs/2508.11566v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Language models align with brain regions that represent concepts across modalities", "abstract": "Cognitive science and neuroscience have long faced the challenge of\ndisentangling representations of language from representations of conceptual\nmeaning. As the same problem arises in today's language models (LMs), we\ninvestigate the relationship between LM--brain alignment and two neural\nmetrics: (1) the level of brain activation during processing of sentences,\ntargeting linguistic processing, and (2) a novel measure of meaning consistency\nacross input modalities, which quantifies how consistently a brain region\nresponds to the same concept across paradigms (sentence, word cloud, image)\nusing an fMRI dataset (Pereira et al., 2018). Our experiments show that both\nlanguage-only and language-vision models predict the signal better in more\nmeaning-consistent areas of the brain, even when these areas are not strongly\nsensitive to language processing, suggesting that LMs might internally\nrepresent cross-modal conceptual meaning.", "published": "2025-08-15 15:32:19", "link": "http://arxiv.org/abs/2508.11536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models", "abstract": "As large language models (LLMs) become more widely deployed, it is crucial to\nexamine their ethical tendencies. Building on research on fairness and\ndiscrimination in AI, we investigate whether LLMs exhibit speciesist bias --\ndiscrimination based on species membership -- and how they value non-human\nanimals. We systematically examine this issue across three paradigms: (1)\nSpeciesismBench, a 1,003-item benchmark assessing recognition and moral\nevaluation of speciesist statements; (2) established psychological measures\ncomparing model responses with those of human participants; (3) text-generation\ntasks probing elaboration on, or resistance to, speciesist rationalizations. In\nour benchmark, LLMs reliably detected speciesist statements but rarely\ncondemned them, often treating speciesist attitudes as morally acceptable. On\npsychological measures, results were mixed: LLMs expressed slightly lower\nexplicit speciesism than people, yet in direct trade-offs they more often chose\nto save one human over multiple animals. A tentative interpretation is that\nLLMs may weight cognitive capacity rather than species per se: when capacities\nwere equal, they showed no species preference, and when an animal was described\nas more capable, they tended to prioritize it over a less capable human. In\nopen-ended text generation tasks, LLMs frequently normalized or rationalized\nharm toward farmed animals while refusing to do so for non-farmed animals.\nThese findings suggest that while LLMs reflect a mixture of progressive and\nmainstream human views, they nonetheless reproduce entrenched cultural norms\naround animal exploitation. We argue that expanding AI fairness and alignment\nframeworks to explicitly include non-human moral patients is essential for\nreducing these biases and preventing the entrenchment of speciesist attitudes\nin AI systems and the societies they influence.", "published": "2025-08-15 15:22:00", "link": "http://arxiv.org/abs/2508.11534v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Reference Points in LLM Sentiment Analysis: The Role of Structured Context", "abstract": "Large language models (LLMs) are now widely used across many fields,\nincluding marketing research. Sentiment analysis, in particular, helps firms\nunderstand consumer preferences. While most NLP studies classify sentiment from\nreview text alone, marketing theories, such as prospect theory and\nexpectation--disconfirmation theory, point out that customer evaluations are\nshaped not only by the actual experience but also by additional reference\npoints. This study therefore investigates how the content and format of such\nsupplementary information affect sentiment analysis using LLMs. We compare\nnatural language (NL) and JSON-formatted prompts using a lightweight 3B\nparameter model suitable for practical marketing applications. Experiments on\ntwo Yelp categories (Restaurant and Nightlife) show that the JSON prompt with\nadditional information outperforms all baselines without fine-tuning: Macro-F1\nrises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it\ndeployable in resource-constrained edge devices. Furthermore, a follow-up\nanalysis confirms that performance gains stem from genuine contextual reasoning\nrather than label proxying. This work demonstrates that structured prompting\ncan enable smaller models to achieve competitive performance, offering a\npractical alternative to large-scale model deployment.", "published": "2025-08-15 13:04:32", "link": "http://arxiv.org/abs/2508.11454v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps", "abstract": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave ushered in a new era of AI capabilities, demonstrating near-human-level\nperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)\nand leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the\ndevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourced\ngeneral-domain prompts, often falling short of reflecting performance in\nreal-world applications. To bridge this critical gap, we present Inclusion\nArena, a live leaderboard that ranks models based on human feedback collected\ndirectly from AI-powered applications. Our platform integrates pairwise model\ncomparisons into natural user interactions, ensuring evaluations reflect\npractical usage scenarios. For robust model ranking, we employ the\nBradley-Terry model augmented with two key innovations: (1) Placement Matches,\na cold-start mechanism to quickly estimate initial ratings for newly integrated\nmodels, and (2) Proximity Sampling, an intelligent comparison strategy that\nprioritizes battles between models of similar capabilities to maximize\ninformation gain and enhance rating stability. Extensive empirical analyses and\nsimulations demonstrate that Inclusion Arena yields reliable and stable\nrankings, exhibits higher data transitivity compared to general crowdsourced\ndatasets, and significantly mitigates the risk of malicious manipulation. By\nfostering an open alliance between foundation models and real-world\napplications, Inclusion Arena aims to accelerate the development of LLMs and\nMLLMs truly optimized for practical, user-centric deployments. The platform is\npublicly accessible at https://doraemon.alipay.com/model-ranking.", "published": "2025-08-15 13:00:07", "link": "http://arxiv.org/abs/2508.11452v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity", "abstract": "Learning unified text embeddings that excel across diverse downstream tasks\nis a central goal in representation learning, yet negative transfer remains a\npersistent obstacle. This challenge is particularly pronounced when jointly\ntraining a single encoder for Information Retrieval (IR) and Semantic Textual\nSimilarity (STS), two essential but fundamentally disparate tasks for which\nnaive co-training typically yields steep performance trade-offs. We argue that\nresolving this conflict requires systematically decoupling task-specific\nlearning signals throughout the training pipeline. To this end, we introduce\nCoDiEmb, a unified framework that reconciles the divergent requirements of IR\nand STS in a collaborative yet distinct manner. CoDiEmb integrates three key\ninnovations for effective joint optimization: (1) Task-specialized objectives\npaired with a dynamic sampler that forms single-task batches and balances\nper-task updates, thereby preventing gradient interference. For IR, we employ a\ncontrastive loss with multiple positives and hard negatives, augmented by\ncross-device sampling. For STS, we adopt order-aware objectives that directly\noptimize correlation and ranking consistency. (2) A delta-guided model fusion\nstrategy that computes fine-grained merging weights for checkpoints by\nanalyzing each parameter's deviation from its pre-trained initialization,\nproving more effective than traditional Model Soups. (3) An efficient,\nsingle-stage training pipeline that is simple to implement and converges\nstably. Extensive experiments on 15 standard IR and STS benchmarks across three\nbase encoders validate CoDiEmb. Our results and analysis demonstrate that the\nframework not only mitigates cross-task trade-offs but also measurably improves\nthe geometric properties of the embedding space.", "published": "2025-08-15 12:46:35", "link": "http://arxiv.org/abs/2508.11442v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political Discourse", "abstract": "Anti-sexist speech, i.e., public expressions that challenge or resist\ngendered abuse and sexism, plays a vital role in shaping democratic debate\nonline. Yet automated content moderation systems, increasingly powered by large\nlanguage models (LLMs), may struggle to distinguish such resistance from the\nsexism it opposes. This study examines how five LLMs classify sexist,\nanti-sexist, and neutral political tweets from the UK, focusing on\nhigh-salience trigger events involving female Members of Parliament in the year\n2022. Our analysis show that models frequently misclassify anti-sexist speech\nas harmful, particularly during politically charged events where rhetorical\nstyles of harm and resistance converge. These errors risk silencing those who\nchallenge sexism, with disproportionate consequences for marginalised voices.\nWe argue that moderation design must move beyond binary harmful/not-harmful\nschemas, integrate human-in-the-loop review during sensitive events, and\nexplicitly include counter-speech in training data. By linking feminist\nscholarship, event-based analysis, and model evaluation, this work highlights\nthe sociotechnical challenges of safeguarding resistance speech in digital\npolitical spaces.", "published": "2025-08-15 12:24:22", "link": "http://arxiv.org/abs/2508.11434v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor", "abstract": "Automated humor generation with Large Language Models (LLMs) often yields\njokes that feel generic, repetitive, or tone-deaf because humor is deeply\nsituated and hinges on the listener's cultural background, mindset, and\nimmediate context. We introduce HumorPlanSearch, a modular pipeline that\nexplicitly models context through: (1) Plan-Search for diverse, topic-tailored\nstrategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and\nstylistic reasoning; (3) a Knowledge Graph to retrieve and adapt\nhigh-performing historical strategies; (4) novelty filtering via semantic\nembeddings; and (5) an iterative judge-driven revision loop. To evaluate\ncontext sensitivity and comedic quality, we propose the Humor Generation Score\n(HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates,\nand topic relevance. In experiments across nine topics with feedback from 13\nhuman judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent\n(p < 0.05) over a strong baseline. By foregrounding context at every stage from\nstrategy planning to multi-signal evaluation, HumorPlanSearch advances\nAI-driven humor toward more coherent, adaptive, and culturally attuned comedy.", "published": "2025-08-15 12:07:56", "link": "http://arxiv.org/abs/2508.11429v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions", "abstract": "Large language models implicitly encode preferences over human values, yet\nsteering them often requires large training data. In this work, we investigate\na simple approach: Can we reliably modify a model's value system in downstream\nbehavior by training it to answer value survey questions accordingly? We first\nconstruct value profiles of several open-source LLMs by asking them to rate a\nseries of value-related descriptions spanning 20 distinct human values, which\nwe use as a baseline for subsequent experiments. We then investigate whether\nthe value system of a model can be governed by fine-tuning on the value\nsurveys. We evaluate the effect of finetuning on the model's behavior in two\nways; first, we assess how answers change on in-domain, held-out survey\nquestions. Second, we evaluate whether the model's behavior changes in\nout-of-domain settings (situational scenarios). To this end, we construct a\ncontextualized moral judgment dataset based on Reddit posts and evaluate\nchanges in the model's behavior in text-based adventure games. We demonstrate\nthat our simple approach can not only change the model's answers to in-domain\nsurvey questions, but also produces substantial shifts (value alignment) in\nimplicit downstream task behavior.", "published": "2025-08-15 11:36:17", "link": "http://arxiv.org/abs/2508.11414v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training", "abstract": "We propose an end-to-end differentiable training paradigm for stable training\nof a rationalized transformer classifier. Our approach results in a single\nmodel that simultaneously classifies a sample and scores input tokens based on\ntheir relevance to the classification. To this end, we build on the widely-used\nthree-player-game for training rationalized models, which typically relies on\ntraining a rationale selector, a classifier and a complement classifier. We\nsimplify this approach by making a single model fulfill all three roles,\nleading to a more efficient training paradigm that is not susceptible to the\ncommon training instabilities that plague existing approaches. Further, we\nextend this paradigm to produce class-wise rationales while incorporating\nrecent advances in parameterizing and regularizing the resulting rationales,\nthus leading to substantially improved and state-of-the-art alignment with\nhuman annotations without any explicit supervision.", "published": "2025-08-15 10:51:58", "link": "http://arxiv.org/abs/2508.11393v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Model Interpretability and Rationale Extraction by Input Mask Optimization", "abstract": "Concurrent to the rapid progress in the development of neural-network based\nmodels in areas like natural language processing and computer vision, the need\nfor creating explanations for the predictions of these black-box models has\nrisen steadily. We propose a new method to generate extractive explanations for\npredictions made by neural networks, that is based on masking parts of the\ninput which the model does not consider to be indicative of the respective\nclass. The masking is done using gradient-based optimization combined with a\nnew regularization scheme that enforces sufficiency, comprehensiveness and\ncompactness of the generated explanation, three properties that are known to be\ndesirable from the related field of rationale extraction in natural language\nprocessing. In this way, we bridge the gap between model interpretability and\nrationale extraction, thereby proving that the latter of which can be performed\nwithout training a specialized model, only on the basis of a trained\nclassifier. We further apply the same method to image inputs and obtain high\nquality explanations for image classifications, which indicates that the\nconditions proposed for rationale extraction in natural language processing are\nmore broadly applicable to different input types.", "published": "2025-08-15 10:41:09", "link": "http://arxiv.org/abs/2508.11388v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Retrieval-augmented reasoning with lean language models", "abstract": "This technical report details a novel approach to combining reasoning and\nretrieval augmented generation (RAG) within a single, lean language model\narchitecture. While existing RAG systems typically rely on large-scale models\nand external APIs, our work addresses the increasing demand for performant and\nprivacy-preserving solutions deployable in resource-constrained or secure\nenvironments. Building on recent developments in test-time scaling and\nsmall-scale reasoning models, we develop a retrieval augmented conversational\nagent capable of interpreting complex, domain-specific queries using a\nlightweight backbone model. Our system integrates a dense retriever with\nfine-tuned Qwen2.5-Instruct models, using synthetic query generation and\nreasoning traces derived from frontier models (e.g., DeepSeek-R1) over a\ncurated corpus, in this case, the NHS A-to-Z condition pages. We explore the\nimpact of summarisation-based document compression, synthetic data design, and\nreasoning-aware fine-tuning on model performance. Evaluation against both\nnon-reasoning and general-purpose lean models demonstrates that our\ndomain-specific fine-tuning approach yields substantial gains in answer\naccuracy and consistency, approaching frontier-level performance while\nremaining feasible for local deployment. All implementation details and code\nare publicly released to support reproducibility and adaptation across domains.", "published": "2025-08-15 10:38:15", "link": "http://arxiv.org/abs/2508.11386v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs", "abstract": "Large Language Models (LLMs) are highly sensitive to subtle, non-semantic\nvariations in prompt phrasing and formatting. In this work, we present the\nfirst systematic evaluation of 5 methods for improving prompt robustness within\na unified experimental framework. We benchmark these techniques on 8 models\nfrom Llama, Qwen and Gemma families across 52 tasks from Natural Instructions\ndataset. Our evaluation covers robustness methods from both fine-tuned and\nin-context learning paradigms, and tests their generalization against multiple\ntypes of distribution shifts. Finally, we extend our analysis to GPT-4.1 and\nDeepSeek V3 to assess frontier models' current robustness to format\nperturbations. Our findings offer actionable insights into the relative\neffectiveness of these robustness methods, enabling practitioners to make\ninformed decisions when aiming for stable and reliable LLM performance in\nreal-world applications. Code:\nhttps://github.com/AIRI-Institute/when-punctuation-matters.", "published": "2025-08-15 10:32:50", "link": "http://arxiv.org/abs/2508.11383v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Feedback Indicators: The Alignment between Llama and a Teacher in Language Learning", "abstract": "Automated feedback generation has the potential to enhance students' learning\nprogress by providing timely and targeted feedback. Moreover, it can assist\nteachers in optimizing their time, allowing them to focus on more strategic and\npersonalized aspects of teaching. To generate high-quality, information-rich\nformative feedback, it is essential first to extract relevant indicators, as\nthese serve as the foundation upon which the feedback is constructed. Teachers\noften employ feedback criteria grids composed of various indicators that they\nevaluate systematically. This study examines the initial phase of extracting\nsuch indicators from students' submissions of a language learning course using\nthe large language model Llama 3.1. Accordingly, the alignment between\nindicators generated by the LLM and human ratings across various feedback\ncriteria is investigated. The findings demonstrate statistically significant\nstrong correlations, even in cases involving unanticipated combinations of\nindicators and criteria. The methodology employed in this paper offers a\npromising foundation for extracting indicators from students' submissions using\nLLMs. Such indicators can potentially be utilized to auto-generate explainable\nand transparent formative feedback in future research.", "published": "2025-08-15 09:59:22", "link": "http://arxiv.org/abs/2508.11364v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis", "abstract": "The proliferation of high-quality text from Large Language Models (LLMs)\ndemands reliable and efficient detection methods. While existing training-free\napproaches show promise, they often rely on surface-level statistics and\noverlook fundamental signal properties of the text generation process. In this\nwork, we reframe detection as a signal processing problem, introducing a novel\nparadigm that analyzes the sequence of token log-probabilities in the frequency\ndomain. By systematically analyzing the signal's spectral properties using the\nglobal Discrete Fourier Transform (DFT) and the local Short-Time Fourier\nTransform (STFT), we find that human-written text consistently exhibits\nsignificantly higher spectral energy. This higher energy reflects the\nlarger-amplitude fluctuations inherent in human writing compared to the\nsuppressed dynamics of LLM-generated text. Based on this key insight, we\nconstruct SpecDetect, a detector built on a single, robust feature from the\nglobal DFT: DFT total energy. We also propose an enhanced version,\nSpecDetect++, which incorporates a sampling discrepancy mechanism to further\nboost robustness. Extensive experiments demonstrate that our approach\noutperforms the state-of-the-art model while running in nearly half the time.\nOur work introduces a new, efficient, and interpretable pathway for\nLLM-generated text detection, showing that classical signal processing\ntechniques offer a surprisingly powerful solution to this modern challenge.", "published": "2025-08-15 09:13:42", "link": "http://arxiv.org/abs/2508.11343v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning", "abstract": "Graph ``pre-training and prompt-tuning'' aligns downstream tasks with\npre-trained objectives to enable efficient knowledge transfer under limited\nsupervision. However, existing methods rely on homophily-based low-frequency\nknowledge, failing to handle diverse spectral distributions in real-world\ngraphs with varying homophily. Our theoretical analysis reveals a spectral\nspecificity principle: optimal knowledge transfer requires alignment between\npre-trained spectral filters and the intrinsic spectrum of downstream graphs.\nUnder limited supervision, large spectral gaps between pre-training and\ndownstream tasks impede effective adaptation. To bridge this gap, we propose\nthe HS-GPPT model, a novel framework that ensures spectral alignment throughout\nboth pre-training and prompt-tuning. We utilize a hybrid spectral filter\nbackbone and local-global contrastive learning to acquire abundant spectral\nknowledge. Then we design prompt graphs to align the spectral distribution with\npretexts, facilitating spectral knowledge transfer across homophily and\nheterophily. Extensive experiments validate the effectiveness under both\ntransductive and inductive learning settings. Our code is available at\nhttps://anonymous.4open.science/r/HS-GPPT-62D2/.", "published": "2025-08-15 08:55:57", "link": "http://arxiv.org/abs/2508.11328v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LLM Compression: How Far Can We Go in Balancing Size and Performance?", "abstract": "Quantization is an essential and popular technique for improving the\naccessibility of large language models (LLMs) by reducing memory usage and\ncomputational costs while maintaining performance. In this study, we apply\n4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer\nQuantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their\nimpact across multiple NLP tasks. We benchmark these models on MS MARCO\n(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K\n(Mathematical Reasoning) datasets, assessing both accuracy and efficiency\nacross various tasks. The study measures the trade-offs between model\ncompression and task performance, analyzing key evaluation metrics, namely\naccuracy, inference latency, and throughput (total output tokens generated per\nsecond), providing insights into the suitability of low-bit quantization for\nreal-world deployment. Using the results, users can then make suitable\ndecisions based on the specifications that need to be met. We discuss the pros\nand cons of GSQ and GPTQ techniques on models of different sizes, which also\nserve as a benchmark for future experiments.", "published": "2025-08-15 08:41:20", "link": "http://arxiv.org/abs/2508.11318v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems", "abstract": "The growing interest in automatic survey generation (ASG), a task that\ntraditionally required considerable time and effort, has been spurred by recent\nadvances in large language models (LLMs). With advancements in\nretrieval-augmented generation (RAG) and the rising popularity of multi-agent\nsystems (MASs), synthesizing academic surveys using LLMs has become a viable\napproach, thereby elevating the need for robust evaluation methods in this\ndomain. However, existing evaluation methods suffer from several limitations,\nincluding biased metrics, a lack of human preference, and an over-reliance on\nLLMs-as-judges. To address these challenges, we propose SGSimEval, a\ncomprehensive benchmark for Survey Generation with Similarity-Enhanced\nEvaluation that evaluates automatic survey generation systems by integrating\nassessments of the outline, content, and references, and also combines\nLLM-based scoring with quantitative metrics to provide a multifaceted\nevaluation framework. In SGSimEval, we also introduce human preference metrics\nthat emphasize both inherent quality and similarity to humans. Extensive\nexperiments reveal that current ASG systems demonstrate human-comparable\nsuperiority in outline generation, while showing significant room for\nimprovement in content and reference generation, and our evaluation metrics\nmaintain strong consistency with human assessments.", "published": "2025-08-15 08:27:58", "link": "http://arxiv.org/abs/2508.11310v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory", "abstract": "LLMs increasingly exhibit over-refusal behavior, where safety mechanisms\ncause models to reject benign instructions that superficially resemble harmful\ncontent. This phenomena diminishes utility in production applications that\nrepeatedly rely on common prompt templates or applications that frequently rely\non LLMs for specific tasks (e.g. sentiment analysis, language translation).\nThrough comprehensive evaluation, we demonstrate that LLMs still tend to refuse\nresponses to harmful instructions when those instructions are reframed to\nappear as benign tasks. Our mechanistic analysis reveal that LLMs follow\ndistinct \"constellation\" patterns in embedding space as representations\ntraverse layers, with each task maintaining consistent trajectories that shift\npredictably between refusal and non-refusal cases. We introduce\nSafeConstellations, an inference-time trajectory-shifting approach that tracks\ntask-specific trajectory patterns and guides representations toward non-refusal\npathways. By selectively guiding model behavior only on tasks prone to\nover-refusal, and by preserving general model behavior, our method reduces\nover-refusal rates by up to 73% with minimal impact on utility-offering a\nprincipled approach to mitigating over-refusals.", "published": "2025-08-15 07:54:42", "link": "http://arxiv.org/abs/2508.11290v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries", "abstract": "Depression, anxiety, and stress are widespread mental health concerns that\nincreasingly drive individuals to seek information from Large Language Models\n(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini\nPro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty\npragmatic questions about depression, anxiety, and stress when those questions\nare framed for six user profiles (baseline, woman, man, young, old, and\nuniversity student). The models generated 2,880 answers, which we scored for\nsentiment and emotions using state-of-the-art tools. Our analysis revealed that\noptimism, fear, and sadness dominated the emotional landscape across all\noutputs, with neutral sentiment maintaining consistently high values.\nGratitude, joy, and trust appeared at moderate levels, while emotions such as\nanger, disgust, and love were rarely expressed. The choice of LLM significantly\ninfluenced emotional expression patterns. Mixtral exhibited the highest levels\nof negative emotions including disapproval, annoyance, and sadness, while Llama\ndemonstrated the most optimistic and joyful responses. The type of mental\nhealth condition dramatically shaped emotional responses: anxiety prompts\nelicited extraordinarily high fear scores (0.974), depression prompts generated\nelevated sadness (0.686) and the highest negative sentiment, while\nstress-related queries produced the most optimistic responses (0.755) with\nelevated joy and trust. In contrast, demographic framing of queries produced\nonly marginal variations in emotional tone. Statistical analyses confirmed\nsignificant model-specific and condition-specific differences, while\ndemographic influences remained minimal. These findings highlight the critical\nimportance of model selection in mental health applications, as each LLM\nexhibits a distinct emotional signature that could significantly impact user\nexperience and outcomes.", "published": "2025-08-15 07:47:10", "link": "http://arxiv.org/abs/2508.11285v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection", "abstract": "Detecting toxic content using language models is crucial yet challenging.\nWhile substantial progress has been made in English, toxicity detection in\nFrench remains underdeveloped, primarily due to the lack of culturally\nrelevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new\npublic benchmark of 53,622 French online comments, constructed via a\nsemi-automated annotation pipeline that reduces manual labeling to only 10%\nthrough high-confidence LLM-based pre-annotation and human verification. Then,\nwe benchmark a broad range of models and uncover a counterintuitive insight:\nSmall Language Models (SLMs) outperform many larger models in robustness and\ngeneralization under the toxicity detection task. Motivated by this finding, we\npropose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic\nweighted loss that progressively emphasizes the model's final decision,\nsignificantly improving faithfulness. Our fine-tuned 4B model achieves\nstate-of-the-art performance, improving its F1 score by 13% over its baseline\nand outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a\ncross-lingual toxicity benchmark demonstrates strong multilingual ability,\nsuggesting that our methodology can be effectively extended to other languages\nand safety-critical classification tasks.", "published": "2025-08-15 07:40:41", "link": "http://arxiv.org/abs/2508.11281v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought", "abstract": "Evaluating large language models (LLMs) in specific domain like tourism\nremains challenging due to the prohibitive cost of annotated benchmarks and\npersistent issues like hallucinations. We propose $\\textbf{L}$able-Free\n$\\textbf{E}$valuation of LLM on $\\textbf{T}$ourism using Expert\n$\\textbf{T}$ree-$\\textbf{o}$f-$\\textbf{T}$hought (LETToT), a framework that\nleverages expert-derived reasoning structures-instead of labeled data-to access\nLLMs in tourism. First, we iteratively refine and validate hierarchical ToT\ncomponents through alignment with generic quality dimensions and expert\nfeedback. Results demonstrate the effectiveness of our systematically optimized\nexpert ToT with 4.99-14.15\\% relative quality gains over baselines. Second, we\napply LETToT's optimized expert ToT to evaluate models of varying scales\n(32B-671B parameters), revealing: (1) Scaling laws persist in specialized\ndomains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,\nDeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit\nreasoning architectures outperform counterparts in accuracy and conciseness\n($p<0.05$). Our work established a scalable, label-free paradigm for\ndomain-specific LLM evaluation, offering a robust alternative to conventional\nannotated benchmarks.", "published": "2025-08-15 07:37:12", "link": "http://arxiv.org/abs/2508.11280v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?", "abstract": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages.", "published": "2025-08-15 06:53:28", "link": "http://arxiv.org/abs/2508.11260v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing", "abstract": "Instruction fine-tuned large language models (LLMs) enable a simple zero-shot\nor few-shot prompting paradigm, also known as in-context learning, for building\nprediction models. This convenience, combined with continued advances in LLM\ncapability, has the potential to drive their adoption across a broad range of\ndomains, including high-stakes applications where group fairness -- preventing\ndisparate impacts across demographic groups -- is essential. The majority of\nexisting approaches to enforcing group fairness on LLM-based classifiers rely\non traditional fair algorithms applied via model fine-tuning or head-tuning on\nfinal-layer embeddings, but they are no longer applicable to closed-weight LLMs\nunder the in-context learning setting, which include some of the most capable\ncommercial models today, such as GPT-4, Gemini, and Claude. In this paper, we\npropose a framework for deriving fair classifiers from closed-weight LLMs via\nprompting: the LLM is treated as a feature extractor, and features are elicited\nfrom its probabilistic predictions (e.g., token log probabilities) using\nprompts strategically designed for the specified fairness criterion to obtain\nsufficient statistics for fair classification; a fair algorithm is then applied\nto these features to train a lightweight fair classifier in a post-hoc manner.\nExperiments on five datasets, including three tabular ones, demonstrate strong\naccuracy-fairness tradeoffs for the classifiers derived by our framework from\nboth open-weight and closed-weight LLMs; in particular, our framework is\ndata-efficient and outperforms fair classifiers trained on LLM embeddings\n(i.e., head-tuning) or from scratch on raw tabular features.", "published": "2025-08-15 06:50:29", "link": "http://arxiv.org/abs/2508.11258v1", "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information", "abstract": "Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving\nabilities in mathematics, as evaluated by existing benchmarks exclusively on\nwell-defined problems. However, such evaluation setup constitutes a critical\ngap, since a genuine intelligent agent should not only solve problems (as a\nmath quiz solver), but also be able~to ask for information when the problems\nlack sufficient information, enabling proactivity in responding users'\nrequests. To bridge such gap, we proposes a new dataset consisting of two types\nof incomplete problems with diverse contexts. Based on the dataset, our\nsystematical evaluation of LRMs reveals their inability in proactively asking\nfor information. In addition, we uncover the behaviors related to overthinking\nand hallucination of LRMs, and highlight the potential and challenges of\nsupervised fine-tuning in learning such ability. We hope to provide new\ninsights in developing LRMs with genuine intelligence, rather than just solving\nproblems.", "published": "2025-08-15 06:42:00", "link": "http://arxiv.org/abs/2508.11252v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering", "abstract": "Multi-hop question answering (MHQA) requires integrating knowledge scattered\nacross multiple passages to derive the correct answer. Traditional\nretrieval-augmented generation (RAG) methods primarily focus on coarse-grained\ntextual semantic similarity and ignore structural associations among dispersed\nknowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods\naddress this by leveraging knowledge graphs (KGs) to capture structural\nassociations, but they tend to overly rely on structural information and\nfine-grained word- or phrase-level retrieval, resulting in an underutilization\nof textual semantics. In this paper, we propose a novel RAG approach called\nHGRAG for MHQA that achieves cross-granularity integration of structural and\nsemantic information via hypergraphs. Structurally, we construct an entity\nhypergraph where fine-grained entities serve as nodes and coarse-grained\npassages as hyperedges, and establish knowledge association through shared\nentities. Semantically, we design a hypergraph retrieval method that integrates\nfine-grained entity similarity and coarse-grained passage similarity via\nhypergraph diffusion. Finally, we employ a retrieval enhancement module, which\nfurther refines the retrieved results both semantically and structurally, to\nobtain the most relevant passages as context for answer generation with the\nLLM. Experimental results on benchmark datasets demonstrate that our approach\noutperforms state-of-the-art methods in QA performance, and achieves a\n6$\\times$ speedup in retrieval efficiency.", "published": "2025-08-15 06:36:13", "link": "http://arxiv.org/abs/2508.11247v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Benchmarking Prosody Encoding in Discrete Speech Tokens", "abstract": "Recently, discrete tokens derived from self-supervised learning (SSL) models\nvia k-means clustering have been actively studied as pseudo-text in speech\nlanguage models and as efficient intermediate representations for various\ntasks. However, these discrete tokens are typically learned in advance,\nseparately from the training of language models or downstream tasks. As a\nresult, choices related to discretization, such as the SSL model used or the\nnumber of clusters, must be made heuristically. In particular, speech language\nmodels are expected to understand and generate responses that reflect not only\nthe semantic content but also prosodic features. Yet, there has been limited\nresearch on the ability of discrete tokens to capture prosodic information. To\naddress this gap, this study conducts a comprehensive analysis focusing on\nprosodic encoding based on their sensitivity to the artificially modified\nprosody, aiming to provide practical guidelines for designing discrete tokens.", "published": "2025-08-15 05:11:16", "link": "http://arxiv.org/abs/2508.11224v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ORFuzz: Fuzzing the \"Other Side\" of LLM Safety -- Testing Over-Refusal", "abstract": "Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously\nrejecting benign queries due to overly conservative safety measures - a\ncritical functional flaw that undermines their reliability and usability.\nCurrent methods for testing this behavior are demonstrably inadequate,\nsuffering from flawed benchmarks and limited test generation capabilities, as\nhighlighted by our empirical user study. To the best of our knowledge, this\npaper introduces the first evolutionary testing framework, ORFuzz, for the\nsystematic detection and analysis of LLM over-refusals. ORFuzz uniquely\nintegrates three core components: (1) safety category-aware seed selection for\ncomprehensive test coverage, (2) adaptive mutator optimization using reasoning\nLLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge\nmodel validated to accurately reflect user perception of toxicity and refusal.\nOur extensive evaluations demonstrate that ORFuzz generates diverse, validated\nover-refusal instances at a rate (6.98% average) more than double that of\nleading baselines, effectively uncovering vulnerabilities. Furthermore,\nORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly\ntransferable test cases that achieves a superior 63.56% average over-refusal\nrate across 10 diverse LLMs, significantly outperforming existing datasets.\nORFuzz and ORFuzzSet provide a robust automated testing framework and a\nvaluable community resource, paving the way for developing more reliable and\ntrustworthy LLM-based software systems.", "published": "2025-08-15 05:03:26", "link": "http://arxiv.org/abs/2508.11222v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.SE"}
{"title": "How Causal Abstraction Underpins Computational Explanation", "abstract": "Explanations of cognitive behavior often appeal to computations over\nrepresentations. What does it take for a system to implement a given\ncomputation over suitable representational vehicles within that system? We\nargue that the language of causality -- and specifically the theory of causal\nabstraction -- provides a fruitful lens on this topic. Drawing on current\ndiscussions in deep learning with artificial neural networks, we illustrate how\nclassical themes in the philosophy of computation and cognition resurface in\ncontemporary machine learning. We offer an account of computational\nimplementation grounded in causal abstraction, and examine the role for\nrepresentation in the resulting picture. We argue that these issues are most\nprofitably explored in connection with generalization and prediction.", "published": "2025-08-15 04:46:02", "link": "http://arxiv.org/abs/2508.11214v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection", "abstract": "Detecting multimodal misinformation on social media remains challenging due\nto inconsistencies between modalities, changes in temporal patterns, and\nsubstantial class imbalance. Many existing methods treat posts independently\nand fail to capture the event-level structure that connects them across time\nand modality. We propose E-CaTCH, an interpretable and scalable framework for\nrobustly detecting misinformation. If needed, E-CaTCH clusters posts into\npseudo-events based on textual similarity and temporal proximity, then\nprocesses each event independently. Within each event, textual and visual\nfeatures are extracted using pre-trained BERT and ResNet encoders, refined via\nintra-modal self-attention, and aligned through bidirectional cross-modal\nattention. A soft gating mechanism fuses these representations to form\ncontextualized, content-aware embeddings of each post. To model temporal\nevolution, E-CaTCH segments events into overlapping time windows and uses a\ntrend-aware LSTM, enhanced with semantic shift and momentum signals, to encode\nnarrative progression over time. Classification is performed at the event\nlevel, enabling better alignment with real-world misinformation dynamics. To\naddress class imbalance and promote stable learning, the model integrates\nadaptive class weighting, temporal consistency regularization, and hard-example\nmining. The total loss is aggregated across all events. Extensive experiments\non Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH\nconsistently outperforms state-of-the-art baselines. Cross-dataset evaluations\nfurther demonstrate its robustness, generalizability, and practical\napplicability across diverse misinformation scenarios.", "published": "2025-08-15 04:13:23", "link": "http://arxiv.org/abs/2508.11197v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Novel Parasitic Dual-Scale Modeling for Efficient and Accurate Multilingual Speech Translation", "abstract": "Recent advancements in speech-to-text translation have led to the development\nof multilingual models capable of handling multiple language pairs\nsimultaneously. However, these unified models often suffer from large parameter\nsizes, making it challenging to balance inference efficiency and performance,\nparticularly in local deployment scenarios. We propose an innovative Parasitic\nDual-Scale Approach, which combines an enhanced speculative sampling method\nwith model compression and knowledge distillation techniques. Building on the\nWhisper Medium model, we enhance it for multilingual speech translation into\nwhisperM2M, and integrate our novel KVSPN module, achieving state-of-the-art\n(SOTA) performance across six popular languages with improved inference\nefficiency. KVSPN enables a 40\\% speedup with no BLEU score degradation.\nCombined with distillation methods, it represents a 2.6$\\times$ speedup over\nthe original Whisper Medium with superior performance.", "published": "2025-08-15 03:46:46", "link": "http://arxiv.org/abs/2508.11189v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Expressive Speech Retrieval using Natural Language Descriptions of Speaking Style", "abstract": "We introduce the task of expressive speech retrieval, where the goal is to\nretrieve speech utterances spoken in a given style based on a natural language\ndescription of that style. While prior work has primarily focused on performing\nspeech retrieval based on what was said in an utterance, we aim to do so based\non how something was said. We train speech and text encoders to embed speech\nand text descriptions of speaking styles into a joint latent space, which\nenables using free-form text prompts describing emotions or styles as queries\nto retrieve matching expressive speech segments. We perform detailed analyses\nof various aspects of our proposed framework, including encoder architectures,\ntraining criteria for effective cross-modal alignment, and prompt augmentation\nfor improved generalization to arbitrary text queries. Experiments on multiple\ndatasets encompassing 22 speaking styles demonstrate that our approach achieves\nstrong retrieval performance as measured by Recall@k.", "published": "2025-08-15 03:38:21", "link": "http://arxiv.org/abs/2508.11187v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction", "abstract": "Distractors, incorrect but plausible answer choices in multiple-choice\nquestions (MCQs), play a critical role in educational assessment by diagnosing\nstudent misconceptions. Recent work has leveraged large language models (LLMs)\nto generate shared, group-level distractors by learning common error patterns\nacross large student populations. However, such distractors often fail to\ncapture the diverse reasoning errors of individual students, limiting their\ndiagnostic effectiveness. To address this limitation, we introduce the task of\npersonalized distractor generation, which aims to generate tailored distractors\nbased on individual misconceptions inferred from each student's past\nquestion-answering (QA) records, ensuring every student receives options that\neffectively exposes their specific reasoning errors. While promising, this task\nis challenging because each student typically has only a few QA records, which\noften lack the student's underlying reasoning processes, making training-based\ngroup-level approaches infeasible. To overcome this, we propose a training-free\ntwo-stage framework. In the first stage, we construct a student-specific\nmisconception prototype by applying Monte Carlo Tree Search (MCTS) to recover\nthe student's reasoning trajectories from past incorrect answers. In the second\nstage, this prototype guides the simulation of the student's reasoning on new\nquestions, enabling the generation of personalized distractors that align with\nthe student's recurring misconceptions. Experiments show that our approach\nachieves the best performance in generating plausible, personalized distractors\nfor 140 students, and also effectively generalizes to group-level settings,\nhighlighting its robustness and adaptability.", "published": "2025-08-15 03:20:37", "link": "http://arxiv.org/abs/2508.11184v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation for OffensiveLanguage Identification", "abstract": "Tulu, a low-resource Dravidian language predominantly spoken in southern\nIndia, has limited computational resources despite its growing digital\npresence. This study presents the first benchmark dataset for Offensive\nLanguage Identification (OLI) in code-mixed Tulu social media content,\ncollected from YouTube comments across various domains. The dataset, annotated\nwith high inter-annotator agreement (Krippendorff's alpha = 0.984), includes\n3,845 comments categorized into four classes: Not Offensive, Not Tulu,\nOffensive Untargeted, and Offensive Targeted. We evaluate a suite of deep\nlearning models, including GRU, LSTM, BiGRU, BiLSTM, CNN, and attention-based\nvariants, alongside transformer architectures (mBERT, XLM-RoBERTa). The BiGRU\nmodel with self-attention achieves the best performance with 82% accuracy and a\n0.81 macro F1-score. Transformer models underperform, highlighting the\nlimitations of multilingual pretraining in code-mixed, under-resourced\ncontexts. This work lays the foundation for further NLP research in Tulu and\nsimilar low-resource, code-mixed languages.", "published": "2025-08-15 02:34:22", "link": "http://arxiv.org/abs/2508.11166v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering", "abstract": "This paper presents MobQA, a benchmark dataset designed to evaluate the\nsemantic understanding capabilities of large language models (LLMs) for human\nmobility data through natural language question answering.\n  While existing models excel at predicting human movement patterns, it remains\nunobvious how much they can interpret the underlying reasons or semantic\nmeaning of those patterns. MobQA provides a comprehensive evaluation framework\nfor LLMs to answer questions about diverse human GPS trajectories spanning\ndaily to weekly granularities. It comprises 5,800 high-quality question-answer\npairs across three complementary question types: factual retrieval (precise\ndata extraction), multiple-choice reasoning (semantic inference), and free-form\nexplanation (interpretive description), which all require spatial, temporal,\nand semantic reasoning. Our evaluation of major LLMs reveals strong performance\non factual retrieval but significant limitations in semantic reasoning and\nexplanation question answering, with trajectory length substantially impacting\nmodel effectiveness. These findings demonstrate the achievements and\nlimitations of state-of-the-art LLMs for semantic mobility\nunderstanding.\\footnote{MobQA dataset is available at\nhttps://github.com/CyberAgentAILab/mobqa.}", "published": "2025-08-15 02:30:20", "link": "http://arxiv.org/abs/2508.11163v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations", "abstract": "Existing rumor detection methods often neglect the content within images as\nwell as the inherent relationships between contexts and images across different\nvisual scales, thereby resulting in the loss of critical information pertinent\nto rumor identification. To address these issues, this paper presents a novel\ncross-modal rumor detection scheme based on contrastive learning, namely the\nMulti-scale Image and Context Correlation exploration algorithm (MICC).\nSpecifically, we design an SCLIP encoder to generate unified semantic\nembeddings for text and multi-scale image patches through contrastive\npretraining, enabling their relevance to be measured via dot-product\nsimilarity. Building upon this, a Cross-Modal Multi-Scale Alignment module is\nintroduced to identify image regions most relevant to the textual semantics,\nguided by mutual information maximization and the information bottleneck\nprinciple, through a Top-K selection strategy based on a cross-modal relevance\nmatrix constructed between the text and multi-scale image patches. Moreover, a\nscale-aware fusion network is designed to integrate the highly correlated\nmulti-scale image features with global text features by assigning adaptive\nweights to image regions based on their semantic importance and cross-modal\nrelevance. The proposed methodology has been extensively evaluated on two\nreal-world datasets. The experimental results demonstrate that it achieves a\nsubstantial performance improvement over existing state-of-the-art approaches\nin rumor detection, highlighting its effectiveness and potential for practical\napplications.", "published": "2025-08-15 01:13:50", "link": "http://arxiv.org/abs/2508.11141v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents", "abstract": "Large language models (LLMs) are emerging as a go-to tool for querying\ninformation. However, current LLM benchmarks rarely feature natural questions\nthat are both information-seeking as well as genuinely time-consuming for\nhumans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural\nand complex questions that require dozens, and at times hundreds, of\nintermediate steps to solve -- far more than any existing QA benchmark. To\nbuild MoNaCo, we developed a decomposed annotation pipeline to elicit and\nmanually answer natural time-consuming questions at scale. Frontier LLMs\nevaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and\nhallucinations. Our results underscore the need for reasoning models that\nbetter handle the complexity and sheer breadth of real-world\ninformation-seeking questions -- with MoNaCo providing an effective resource\nfor tracking such progress. The MONACO benchmark, codebase, prompts and models\npredictions are publicly available at: https://tomerwolgithub.github.io/monaco", "published": "2025-08-15 00:58:10", "link": "http://arxiv.org/abs/2508.11133v1", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Is ChatGPT-5 Ready for Mammogram VQA?", "abstract": "Mammogram visual question answering (VQA) integrates image interpretation\nwith clinical reasoning and has potential to support breast cancer screening.\nWe systematically evaluated the GPT-5 family and GPT-4o model on four public\nmammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment,\nabnormality detection, and malignancy classification tasks. GPT-5 consistently\nwas the best performing model but lagged behind both human experts and\ndomain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scores\namong GPT variants in density (56.8%), distortion (52.5%), mass (64.5%),\ncalcification (63.5%), and malignancy (52.8%) classification. On InBreast, it\nattained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0%\nmalignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detection\nand 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADS\naccuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Compared\nwith human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) and\nspecificity (52.3%). While GPT-5 exhibits promising capabilities for screening\ntasks, its performance remains insufficient for high-stakes clinical imaging\napplications without targeted domain adaptation and optimization. However, the\ntremendous improvements in performance from GPT-4o to GPT-5 show a promising\ntrend in the potential for general large language models (LLMs) to assist with\nmammography VQA tasks.", "published": "2025-08-15 17:56:24", "link": "http://arxiv.org/abs/2508.11628v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Pretrained Conformers for Audio Fingerprinting and Retrieval", "abstract": "Conformers have shown great results in speech processing due to their ability\nto capture both local and global interactions. In this work, we utilize a\nself-supervised contrastive learning framework to train conformer-based\nencoders that are capable of generating unique embeddings for small segments of\naudio, generalizing well to previously unseen data. We achieve state-of-the-art\nresults for audio retrieval tasks while using only 3 seconds of audio to\ngenerate embeddings. Our models are almost completely immune to temporal\nmisalignments and achieve state-of-the-art results in cases of other audio\ndistortions such as noise, reverb or extreme temporal stretching. Code and\nmodels are made publicly available and the results are easy to reproduce as we\ntrain and test using popular and freely available datasets of different sizes.", "published": "2025-08-15 17:19:09", "link": "http://arxiv.org/abs/2508.11609v1", "categories": ["cs.SD", "cs.AI", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CryptoScope: Utilizing Large Language Models for Automated Cryptographic Logic Vulnerability Detection", "abstract": "Cryptographic algorithms are fundamental to modern security, yet their\nimplementations frequently harbor subtle logic flaws that are hard to detect.\nWe introduce CryptoScope, a novel framework for automated cryptographic\nvulnerability detection powered by Large Language Models (LLMs). CryptoScope\ncombines Chain-of-Thought (CoT) prompting with Retrieval-Augmented Generation\n(RAG), guided by a curated cryptographic knowledge base containing over 12,000\nentries. We evaluate CryptoScope on LLM-CLVA, a benchmark of 92 cases primarily\nderived from real-world CVE vulnerabilities, complemented by cryptographic\nchallenges from major Capture The Flag (CTF) competitions and synthetic\nexamples across 11 programming languages. CryptoScope consistently improves\nperformance over strong LLM baselines, boosting DeepSeek-V3 by 11.62%,\nGPT-4o-mini by 20.28%, and GLM-4-Flash by 28.69%. Additionally, it identifies 9\npreviously undisclosed flaws in widely used open-source cryptographic projects.", "published": "2025-08-15 17:07:54", "link": "http://arxiv.org/abs/2508.11599v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks", "abstract": "Deploying multiple machine learning models on resource-constrained robotic\nplatforms for different perception tasks often results in redundant\ncomputations, large memory footprints, and complex integration challenges. In\nresponse, this work presents Visual Perception Engine (VPEngine), a modular\nframework designed to enable efficient GPU usage for visual multitasking while\nmaintaining extensibility and developer accessibility. Our framework\narchitecture leverages a shared foundation model backbone that extracts image\nrepresentations, which are efficiently shared, without any unnecessary GPU-CPU\nmemory transfers, across multiple specialized task-specific model heads running\nin parallel. This design eliminates the computational redundancy inherent in\nfeature extraction component when deploying traditional sequential models while\nenabling dynamic task prioritization based on application demands. We\ndemonstrate our framework's capabilities through an example implementation\nusing DINOv2 as the foundation model with multiple task (depth, object\ndetection and semantic segmentation) heads, achieving up to 3x speedup compared\nto sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine\noffers efficient GPU utilization and maintains a constant memory footprint\nwhile allowing per-task inference frequencies to be adjusted dynamically during\nruntime. The framework is written in Python and is open source with ROS2 C++\n(Humble) bindings for ease of use by the robotics community across diverse\nrobotic platforms. Our example implementation demonstrates end-to-end real-time\nperformance at $\\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized\nmodels.", "published": "2025-08-15 16:42:23", "link": "http://arxiv.org/abs/2508.11584v1", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "ADMIRE-BayesOpt: Accelerated Data MIxture RE-weighting for Language Models with Bayesian Optimization", "abstract": "Determining the optimal data mixture for large language model training\nremains a challenging problem with an outsized impact on performance. In\npractice, language model developers continue to rely on heuristic exploration\nsince no learning-based approach has emerged as a reliable solution. In this\nwork, we propose to view the selection of training data mixtures as a black-box\nhyperparameter optimization problem, for which Bayesian Optimization is a\nwell-established class of appropriate algorithms. Firstly, we cast data mixture\nlearning as a sequential decision-making problem, in which we aim to find a\nsuitable trade-off between the computational cost of training exploratory\n(proxy-) models and final mixture performance. Secondly, we systematically\nexplore the properties of transferring mixtures learned at a small scale to\nlarger-scale experiments, providing insights and highlighting opportunities for\nresearch at a modest scale. By proposing Multi-fidelity Bayesian Optimization\nas a suitable method in this common scenario, we introduce a natural framework\nto balance experiment cost with model fit, avoiding the risks of overfitting to\nsmaller scales while minimizing the number of experiments at high cost. We\npresent results for pre-training and instruction finetuning across models\nranging from 1 million to 7 billion parameters, varying from simple\narchitectures to state-of-the-art models and benchmarks spanning dozens of\ndatasets. We demonstrate consistently strong results relative to a wide range\nof benchmarks, showingspeed-ups of over 500% in determining the best data\nmixture on our largest experiments relative to recent baselines. In addition,\nwe broaden access to research by sharing ADMIRE IFT Runs, a dataset of 460 full\ntraining & evaluation runs across various model sizes worth over 13,000 GPU\nhours, greatly reducing the cost of conducting research in this area.", "published": "2025-08-15 15:53:09", "link": "http://arxiv.org/abs/2508.11551v1", "categories": ["stat.ML", "cs.AI", "cs.LG"], "primary_category": "stat.ML"}
{"title": "A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow", "abstract": "Artificial intelligence is reshaping science and industry, yet many users\nstill regard its models as opaque \"black boxes\". Conventional explainable\nartificial-intelligence methods clarify individual predictions but overlook the\nupstream decisions and downstream quality checks that determine whether\ninsights can be trusted. In this work, we present Holistic Explainable\nArtificial Intelligence (HXAI), a user-centric framework that embeds\nexplanation into every stage of the data-analysis workflow and tailors those\nexplanations to users. HXAI unifies six components (data, analysis set-up,\nlearning process, model output, model quality, communication channel) into a\nsingle taxonomy and aligns each component with the needs of domain experts,\ndata analysts and data scientists. A 112-item question bank covers these needs;\nour survey of contemporary tools highlights critical coverage gaps. Grounded in\ntheories of human explanation, principles from human-computer interaction and\nfindings from empirical user studies, HXAI identifies the characteristics that\nmake explanations clear, actionable and cognitively manageable. A comprehensive\ntaxonomy operationalises these insights, reducing terminological ambiguity and\nenabling rigorous coverage analysis of existing toolchains. We further\ndemonstrate how AI agents that embed large-language models can orchestrate\ndiverse explanation techniques, translating technical artifacts into\nstakeholder-specific narratives that bridge the gap between AI developers and\ndomain experts. Departing from traditional surveys or perspective articles,\nthis work melds concepts from multiple disciplines, lessons from real-world\nprojects and a critical synthesis of the literature to advance a novel,\nend-to-end viewpoint on transparency, trustworthiness and responsible AI\ndeployment.", "published": "2025-08-15 15:15:25", "link": "http://arxiv.org/abs/2508.11529v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models", "abstract": "Addressing large-scale planning problems has become one of the central\nchallenges in the planning community, deriving from the state-space explosion\ncaused by growing objects and actions. Recently, researchers have explored the\neffectiveness of leveraging Large Language Models (LLMs) to generate helpful\nactions and states to prune the search space. However, prior works have largely\noverlooked integrating LLMs with domain-specific knowledge to ensure valid\nplans. In this paper, we propose a novel LLM-assisted planner integrated with\nproblem decomposition, which first decomposes large planning problems into\nmultiple simpler sub-tasks. Then we explore two novel paradigms to utilize\nLLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where\nLLM4Inspire provides heuristic guidance according to general knowledge and\nLLM4Predict employs domain-specific knowledge to infer intermediate conditions.\nWe empirically validate the effectiveness of our planner across multiple\ndomains, demonstrating the ability of search space partition when solving\nlarge-scale planning problems. The experimental results show that LLMs\neffectively locate feasible solutions when pruning the search space, where\ninfusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds\nparticular promise compared with LLM4Inspire, which offers general knowledge\nwithin LLMs.", "published": "2025-08-15 15:08:07", "link": "http://arxiv.org/abs/2508.11524v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Weighted First Order Model Counting for Two-variable Logic with Axioms on Two Relations", "abstract": "The Weighted First-Order Model Counting Problem (WFOMC) asks to compute the\nweighted sum of models of a given first-order logic sentence over a given\ndomain. The boundary between fragments for which WFOMC can be computed in\npolynomial time relative to the domain size lies between the two-variable\nfragment ($\\text{FO}^2$) and the three-variable fragment ($\\text{FO}^3$). It is\nknown that WFOMC for \\FOthree{} is $\\mathsf{\\#P_1}$-hard while polynomial-time\nalgorithms exist for computing WFOMC for $\\text{FO}^2$ and $\\text{C}^2$,\npossibly extended by certain axioms such as the linear order axiom, the\nacyclicity axiom, and the connectedness axiom. All existing research has\nconcentrated on extending the fragment with axioms on a single distinguished\nrelation, leaving a gap in understanding the complexity boundary of axioms on\nmultiple relations. In this study, we explore the extension of the two-variable\nfragment by axioms on two relations, presenting both negative and positive\nresults. We show that WFOMC for $\\text{FO}^2$ with two linear order relations\nand $\\text{FO}^2$ with two acyclic relations are $\\mathsf{\\#P_1}$-hard.\nConversely, we provide an algorithm in time polynomial in the domain size for\nWFOMC of $\\text{C}^2$ with a linear order relation, its successor relation and\nanother successor relation.", "published": "2025-08-15 14:54:17", "link": "http://arxiv.org/abs/2508.11515v1", "categories": ["cs.LO", "cs.AI", "03C13, 68T27", "F.4.0"], "primary_category": "cs.LO"}
{"title": "Towards Faithful Class-level Self-explainability in Graph Neural Networks by Subgraph Dependencies", "abstract": "Enhancing the interpretability of graph neural networks (GNNs) is crucial to\nensure their safe and fair deployment. Recent work has introduced\nself-explainable GNNs that generate explanations as part of training, improving\nboth faithfulness and efficiency. Some of these models, such as ProtGNN and\nPGIB, learn class-specific prototypes, offering a potential pathway toward\nclass-level explanations. However, their evaluations focus solely on\ninstance-level explanations, leaving open the question of whether these\nprototypes meaningfully generalize across instances of the same class. In this\npaper, we introduce GraphOracle, a novel self-explainable GNN framework\ndesigned to generate and evaluate class-level explanations for GNNs. Our model\njointly learns a GNN classifier and a set of structured, sparse subgraphs that\nare discriminative for each class. We propose a novel integrated training that\ncaptures graph$\\unicode{x2013}$subgraph$\\unicode{x2013}$prediction dependencies\nefficiently and faithfully, validated through a masking-based evaluation\nstrategy. This strategy enables us to retroactively assess whether prior\nmethods like ProtGNN and PGIB deliver effective class-level explanations. Our\nresults show that they do not. In contrast, GraphOracle achieves superior\nfidelity, explainability, and scalability across a range of graph\nclassification tasks. We further demonstrate that GraphOracle avoids the\ncomputational bottlenecks of previous methods$\\unicode{x2014}$like Monte Carlo\nTree Search$\\unicode{x2014}$by using entropy-regularized subgraph selection and\nlightweight random walk extraction, enabling faster and more scalable training.\nThese findings position GraphOracle as a practical and principled solution for\nfaithful class-level self-explainability in GNNs.", "published": "2025-08-15 14:44:11", "link": "http://arxiv.org/abs/2508.11513v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media", "abstract": "Reliable autonomous navigation across the unstructured terrains of distant\nplanetary surfaces is a critical enabler for future space exploration. However,\nthe deployment of learning-based controllers is hindered by the inherent\nsim-to-real gap, particularly for the complex dynamics of wheel interactions\nwith granular media. This work presents a complete sim-to-real framework for\ndeveloping and validating robust control policies for dynamic waypoint tracking\non such challenging surfaces. We leverage massively parallel simulation to\ntrain reinforcement learning agents across a vast distribution of procedurally\ngenerated environments with randomized physics. These policies are then\ntransferred zero-shot to a physical wheeled rover operating in a lunar-analogue\nfacility. Our experiments systematically compare multiple reinforcement\nlearning algorithms and action smoothing filters to identify the most effective\ncombinations for real-world deployment. Crucially, we provide strong empirical\nevidence that agents trained with procedural diversity achieve superior\nzero-shot performance compared to those trained on static scenarios. We also\nanalyze the trade-offs of fine-tuning with high-fidelity particle physics,\nwhich offers minor gains in low-speed precision at a significant computational\ncost. Together, these contributions establish a validated workflow for creating\nreliable learning-based navigation systems, marking a critical step towards\ndeploying autonomous robots in the final frontier.", "published": "2025-08-15 14:30:07", "link": "http://arxiv.org/abs/2508.11503v1", "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Handwritten Text Recognition of Historical Manuscripts Using Transformer-Based Models", "abstract": "Historical handwritten text recognition (HTR) is essential for unlocking the\ncultural and scholarly value of archival documents, yet digitization is often\nhindered by scarce transcriptions, linguistic variation, and highly diverse\nhandwriting styles. In this study, we apply TrOCR, a state-of-the-art\ntransformer-based HTR model, to 16th-century Latin manuscripts authored by\nRudolf Gwalther. We investigate targeted image preprocessing and a broad suite\nof data augmentation techniques, introducing four novel augmentation methods\ndesigned specifically for historical handwriting characteristics. We also\nevaluate ensemble learning approaches to leverage the complementary strengths\nof augmentation-trained models. On the Gwalther dataset, our best single-model\naugmentation (Elastic) achieves a Character Error Rate (CER) of 1.86, while a\ntop-5 voting ensemble achieves a CER of 1.60 - representing a 50% relative\nimprovement over the best reported TrOCR_BASE result and a 42% improvement over\nthe previous state of the art. These results highlight the impact of\ndomain-specific augmentations and ensemble strategies in advancing HTR\nperformance for historical manuscripts.", "published": "2025-08-15 14:20:58", "link": "http://arxiv.org/abs/2508.11499v1", "categories": ["cs.CV", "cs.AI", "cs.DL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Landmark-Assisted Monte Carlo Planning", "abstract": "Landmarks$\\unicode{x2013}$conditions that must be satisfied at some point in\nevery solution plan$\\unicode{x2013}$have contributed to major advancements in\nclassical planning, but they have seldom been used in stochastic domains. We\nformalize probabilistic landmarks and adapt the UCT algorithm to leverage them\nas subgoals to decompose MDPs; core to the adaptation is balancing between\ngreedy landmark achievement and final goal achievement. Our results in\nbenchmark domains show that well-chosen landmarks can significantly improve the\nperformance of UCT in online probabilistic planning, while the best balance of\ngreedy versus long-term goal achievement is problem-dependent. The results\nsuggest that landmarks can provide helpful guidance for anytime algorithms\nsolving MDPs.", "published": "2025-08-15 14:16:14", "link": "http://arxiv.org/abs/2508.11493v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "RMSL: Weakly-Supervised Insider Threat Detection with Robust Multi-sphere Learning", "abstract": "Insider threat detection aims to identify malicious user behavior by\nanalyzing logs that record user interactions. Due to the lack of fine-grained\nbehavior-level annotations, detecting specific behavior-level anomalies within\nuser behavior sequences is challenging. Unsupervised methods face high false\npositive rates and miss rates due to the inherent ambiguity between normal and\nanomalous behaviors. In this work, we instead introduce weak labels of behavior\nsequences, which have lower annotation costs, i.e., the training labels\n(anomalous or normal) are at sequence-level instead of behavior-level, to\nenhance the detection capability for behavior-level anomalies by learning\ndiscriminative features. To achieve this, we propose a novel framework called\nRobust Multi-sphere Learning (RMSL). RMSL uses multiple hyper-spheres to\nrepresent the normal patterns of behaviors. Initially, a one-class classifier\nis constructed as a good anomaly-supervision-free starting point. Building on\nthis, using multiple instance learning and adaptive behavior-level\nself-training debiasing based on model prediction confidence, the framework\nfurther refines hyper-spheres and feature representations using weak\nsequence-level labels. This approach enhances the model's ability to\ndistinguish between normal and anomalous behaviors. Extensive experiments\ndemonstrate that RMSL significantly improves the performance of behavior-level\ninsider threat detection.", "published": "2025-08-15 13:36:03", "link": "http://arxiv.org/abs/2508.11472v1", "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation", "abstract": "Indoor navigation is a difficult task, as it generally comes with poor GPS\naccess, forcing solutions to rely on other sources of information. While\nsignificant progress continues to be made in this area, deployment to\nproduction applications is still lacking, given the complexity and additional\nrequirements of current solutions. Here, we introduce an efficient, real-time\nand easily deployable deep learning approach, based on visual input only, that\ncan predict the direction towards a target from images captured by a mobile\ndevice. Our technical approach, based on a novel graph-based path generation\nmethod, combined with explainable data augmentation and curriculum learning,\nincludes contributions that make the process of data collection, annotation and\ntraining, as automatic as possible, efficient and robust. On the practical\nside, we introduce a novel largescale dataset, with video footage inside a\nrelatively large shopping mall, in which each frame is annotated with the\ncorrect next direction towards different specific target destinations.\nDifferent from current methods, ours relies solely on vision, avoiding the need\nof special sensors, additional markers placed along the path, knowledge of the\nscene map or internet access. We also created an easy to use application for\nAndroid, which we plan to make publicly available. We make all our data and\ncode available along with visual demos on our project site", "published": "2025-08-15 12:54:13", "link": "http://arxiv.org/abs/2508.11446v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Informative Post-Hoc Explanations Only Exist for Simple Functions", "abstract": "Many researchers have suggested that local post-hoc explanation algorithms\ncan be used to gain insights into the behavior of complex machine learning\nmodels. However, theoretical guarantees about such algorithms only exist for\nsimple decision functions, and it is unclear whether and under which\nassumptions similar results might exist for complex models. In this paper, we\nintroduce a general, learning-theory-based framework for what it means for an\nexplanation to provide information about a decision function. We call an\nexplanation informative if it serves to reduce the complexity of the space of\nplausible decision functions. With this approach, we show that many popular\nexplanation algorithms are not informative when applied to complex decision\nfunctions, providing a rigorous mathematical rejection of the idea that it\nshould be possible to explain any model. We then derive conditions under which\ndifferent explanation algorithms become informative. These are often stronger\nthan what one might expect. For example, gradient explanations and\ncounterfactual explanations are non-informative with respect to the space of\ndifferentiable functions, and SHAP and anchor explanations are not informative\nwith respect to the space of decision trees. Based on these results, we discuss\nhow explanation algorithms can be modified to become informative. While the\nproposed analysis of explanation algorithms is mathematical, we argue that it\nholds strong implications for the practical applicability of these algorithms,\nparticularly for auditing, regulation, and high-risk applications of AI.", "published": "2025-08-15 12:46:18", "link": "http://arxiv.org/abs/2508.11441v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager", "abstract": "Recent advances in mathematical reasoning and the long-term planning\ncapabilities of large language models (LLMs) have precipitated the development\nof agents, which are being increasingly leveraged in business operations\nprocesses. Decision models to optimize inventory levels are one of the core\nelements of operations management. However, the capabilities of the LLM agent\nin making inventory decisions in uncertain contexts, as well as the\ndecision-making biases (e.g. framing effect, etc.) of the agent, remain largely\nunexplored. This prompts concerns regarding the capacity of LLM agents to\neffectively address real-world problems, as well as the potential implications\nof biases that may be present. To address this gap, we introduce AIM-Bench, a\nnovel benchmark designed to assess the decision-making behaviour of LLM agents\nin uncertain supply chain management scenarios through a diverse series of\ninventory replenishment experiments. Our results reveal that different LLMs\ntypically exhibit varying degrees of decision bias that are similar to those\nobserved in human beings. In addition, we explored strategies to mitigate the\npull-to-centre effect and the bullwhip effect, namely cognitive reflection and\nimplementation of information sharing. These findings underscore the need for\ncareful consideration of the potential biases in deploying LLMs in Inventory\ndecision-making scenarios. We hope that these insights will pave the way for\nmitigating human decision bias and developing human-centred decision support\nsystems for supply chains.", "published": "2025-08-15 11:38:19", "link": "http://arxiv.org/abs/2508.11416v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting", "abstract": "Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two\nprominent post-training paradigms for refining the capabilities and aligning\nthe behavior of Large Language Models (LLMs). Existing approaches that\nintegrate SFT and RL often face the risk of disrupting established model\npatterns and inducing overfitting to expert data. To address this, we present a\nnovel investigation into the unified view of SFT and RL through an off-policy\nversus on-policy lens. We propose CHORD, a framework for the Controllable\nHarmonization of On- and Off-Policy Reinforcement Learning via Dynamic\nWeighting, which reframes SFT not as a separate stage but as a dynamically\nweighted auxiliary objective within the on-policy RL process. Based on an\nanalysis of off-policy expert data's influence at both holistic and granular\nlevels, we incorporate a dual-control mechanism in CHORD. Specifically, the\nframework first employs a global coefficient to holistically guide the\ntransition from off-policy imitation to on-policy exploration, and then applies\na token-wise weighting function that enables granular learning from expert\ntokens, which preserves on-policy exploration and mitigates disruption from\noff-policy data. We conduct extensive experiments on widely used benchmarks,\nproviding empirical evidence that CHORD achieves a stable and efficient\nlearning process. By effectively harmonizing off-policy expert data with\non-policy exploration, CHORD demonstrates significant improvements over\nbaselines. We release the implementation at\nhttps://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to\ninspire further research.", "published": "2025-08-15 11:20:03", "link": "http://arxiv.org/abs/2508.11408v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Open, Reproducible and Trustworthy Robot-Based Experiments with Virtual Labs and Digital-Twin-Based Execution Tracing", "abstract": "We envision a future in which autonomous robots conduct scientific\nexperiments in ways that are not only precise and repeatable, but also open,\ntrustworthy, and transparent. To realize this vision, we present two key\ncontributions: a semantic execution tracing framework that logs sensor data\ntogether with semantically annotated robot belief states, ensuring that\nautomated experimentation is transparent and replicable; and the AICOR Virtual\nResearch Building (VRB), a cloud-based platform for sharing, replicating, and\nvalidating robot task executions at scale. Together, these tools enable\nreproducible, robot-driven science by integrating deterministic execution,\nsemantic memory, and open knowledge representation, laying the foundation for\nautonomous systems to participate in scientific discovery.", "published": "2025-08-15 11:16:06", "link": "http://arxiv.org/abs/2508.11406v1", "categories": ["cs.RO", "cs.AI", "68T40", "I.2.9"], "primary_category": "cs.RO"}
{"title": "An Exploratory Study on Crack Detection in Concrete through Human-Robot Collaboration", "abstract": "Structural inspection in nuclear facilities is vital for maintaining\noperational safety and integrity. Traditional methods of manual inspection pose\nsignificant challenges, including safety risks, high cognitive demands, and\npotential inaccuracies due to human limitations. Recent advancements in\nArtificial Intelligence (AI) and robotic technologies have opened new\npossibilities for safer, more efficient, and accurate inspection methodologies.\nSpecifically, Human-Robot Collaboration (HRC), leveraging robotic platforms\nequipped with advanced detection algorithms, promises significant improvements\nin inspection outcomes and reductions in human workload. This study explores\nthe effectiveness of AI-assisted visual crack detection integrated into a\nmobile Jackal robot platform. The experiment results indicate that HRC enhances\ninspection accuracy and reduces operator workload, resulting in potential\nsuperior performance outcomes compared to traditional manual methods.", "published": "2025-08-15 11:13:07", "link": "http://arxiv.org/abs/2508.11404v1", "categories": ["cs.RO", "cs.AI", "cs.HC"], "primary_category": "cs.RO"}
{"title": "Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling and Explainable Mental Disorder Diagnosis", "abstract": "LLM-based agents have emerged as transformative tools capable of executing\ncomplex tasks through iterative planning and action, achieving significant\nadvancements in understanding and addressing user needs. Yet, their\neffectiveness remains limited in specialized domains such as mental health\ndiagnosis, where they underperform compared to general applications. Current\napproaches to integrating diagnostic capabilities into LLMs rely on scarce,\nhighly sensitive mental health datasets, which are challenging to acquire.\nThese methods also fail to emulate clinicians' proactive inquiry skills, lack\nmulti-turn conversational comprehension, and struggle to align outputs with\nexpert clinical reasoning. To address these gaps, we propose DSM5AgentFlow, the\nfirst LLM-based agent workflow designed to autonomously generate DSM-5 Level-1\ndiagnostic questionnaires. By simulating therapist-client dialogues with\nspecific client profiles, the framework delivers transparent, step-by-step\ndisorder predictions, producing explainable and trustworthy results. This\nworkflow serves as a complementary tool for mental health diagnosis, ensuring\nadherence to ethical and legal standards. Through comprehensive experiments, we\nevaluate leading LLMs across three critical dimensions: conversational realism,\ndiagnostic accuracy, and explainability. Our datasets and implementations are\nfully open-sourced.", "published": "2025-08-15 11:08:32", "link": "http://arxiv.org/abs/2508.11398v1", "categories": ["cs.HC", "cs.AI", "cs.IR"], "primary_category": "cs.HC"}
{"title": "G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration", "abstract": "We introduce G-CUT3R, a novel feed-forward approach for guided 3D scene\nreconstruction that enhances the CUT3R model by integrating prior information.\nUnlike existing feed-forward methods that rely solely on input images, our\nmethod leverages auxiliary data, such as depth, camera calibrations, or camera\npositions, commonly available in real-world scenarios. We propose a lightweight\nmodification to CUT3R, incorporating a dedicated encoder for each modality to\nextract features, which are fused with RGB image tokens via zero convolution.\nThis flexible design enables seamless integration of any combination of prior\ninformation during inference. Evaluated across multiple benchmarks, including\n3D reconstruction and other multi-view tasks, our approach demonstrates\nsignificant performance improvements, showing its ability to effectively\nutilize available priors while maintaining compatibility with varying input\nmodalities.", "published": "2025-08-15 10:25:58", "link": "http://arxiv.org/abs/2508.11379v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Does the Skeleton-Recall Loss Really Work?", "abstract": "Image segmentation is an important and widely performed task in computer\nvision. Accomplishing effective image segmentation in diverse settings often\nrequires custom model architectures and loss functions. A set of models that\nspecialize in segmenting thin tubular structures are topology\npreservation-based loss functions. These models often utilize a pixel\nskeletonization process claimed to generate more precise segmentation masks of\nthin tubes and better capture the structures that other models often miss. One\nsuch model, Skeleton Recall Loss (SRL) proposed by Kirchhoff et al.~\\cite\n{kirchhoff2024srl}, was stated to produce state-of-the-art results on benchmark\ntubular datasets. In this work, we performed a theoretical analysis of the\ngradients for the SRL loss. Upon comparing the performance of the proposed\nmethod on some of the tubular datasets (used in the original work, along with\nsome additional datasets), we found that the performance of SRL-based\nsegmentation models did not exceed traditional baseline models. By providing\nboth a theoretical explanation and empirical evidence, this work critically\nevaluates the limitations of topology-based loss functions, offering valuable\ninsights for researchers aiming to develop more effective segmentation models\nfor complex tubular structures.", "published": "2025-08-15 10:16:34", "link": "http://arxiv.org/abs/2508.11374v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Minimizing Surrogate Losses for Decision-Focused Learning using Differentiable Optimization", "abstract": "Decision-focused learning (DFL) trains a machine learning (ML) model to\npredict parameters of an optimization problem, to directly minimize decision\nregret, i.e., maximize decision quality. Gradient-based DFL requires computing\nthe derivative of the solution to the optimization problem with respect to the\npredicted parameters. However, for many optimization problems, such as linear\nprograms (LPs), the gradient of the regret with respect to the predicted\nparameters is zero almost everywhere. Existing gradient-based DFL approaches\nfor LPs try to circumvent this issue in one of two ways: (a) smoothing the LP\ninto a differentiable optimization problem by adding a quadratic regularizer\nand then minimizing the regret directly or (b) minimizing surrogate losses that\nhave informative (sub)gradients. In this paper, we show that the former\napproach still results in zero gradients, because even after smoothing the\nregret remains constant across large regions of the parameter space. To address\nthis, we propose minimizing surrogate losses -- even when a differentiable\noptimization layer is used and regret can be minimized directly. Our\nexperiments demonstrate that minimizing surrogate losses allows differentiable\noptimization layers to achieve regret comparable to or better than\nsurrogate-loss based DFL methods. Further, we demonstrate that this also holds\nfor DYS-Net, a recently proposed differentiable optimization technique for LPs,\nthat computes approximate solutions and gradients through operations that can\nbe performed using feedforward neural network layers. Because DYS-Net executes\nthe forward and the backward pass very efficiently, by minimizing surrogate\nlosses using DYS-Net, we are able to attain regret on par with the\nstate-of-the-art while reducing training time by a significant margin.", "published": "2025-08-15 09:59:56", "link": "http://arxiv.org/abs/2508.11365v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks", "abstract": "As autonomous agents become adept at understanding and interacting with\ngraphical user interface (GUI) environments, a new era of automated task\nexecution is emerging. Recent studies have demonstrated that Reinforcement\nLearning (RL) can effectively enhance agents' performance in dynamic\ninteractive GUI environments. However, these methods face two key limitations:\n(1) they overlook the significant variation in difficulty across different GUI\ntasks by treating the entire training data as a uniform set, which hampers the\nagent's ability to adapt its learning process; and (2) most approaches collapse\ntask-specific nuances into a single, coarse reward, leaving the agent with a\nuniform signal that yields inefficient policy updates. To address these\nlimitations, we propose CRAFT-GUI, a curriculum learning framework based on\nGroup Relative Policy Optimization (GRPO) that explicitly accounts for the\nvarying difficulty across trajectories. To enable more fine-grained policy\noptimization, we design a reward function that combines simple rule-based\nsignals with model-judged evaluation, providing richer and more nuanced\nfeedback during training. Experimental results demonstrate that our method\nachieves significant improvements over previous state-of-the-art approaches,\noutperforming them by 5.6% on public benchmarks Android Control and 10.3% on\nour internal online benchmarks, respectively. These findings empirically\nvalidate the effectiveness of integrating reinforcement learning with\ncurriculum learning in GUI interaction tasks.", "published": "2025-08-15 09:55:02", "link": "http://arxiv.org/abs/2508.11360v1", "categories": ["cs.AI", "cs.HC"], "primary_category": "cs.AI"}
{"title": "PTSM: Physiology-aware and Task-invariant Spatio-temporal Modeling for Cross-Subject EEG Decoding", "abstract": "Cross-subject electroencephalography (EEG) decoding remains a fundamental\nchallenge in brain-computer interface (BCI) research due to substantial\ninter-subject variability and the scarcity of subject-invariant\nrepresentations. This paper proposed PTSM (Physiology-aware and Task-invariant\nSpatio-temporal Modeling), a novel framework for interpretable and robust EEG\ndecoding across unseen subjects. PTSM employs a dual-branch masking mechanism\nthat independently learns personalized and shared spatio-temporal patterns,\nenabling the model to preserve individual-specific neural characteristics while\nextracting task-relevant, population-shared features. The masks are factorized\nacross temporal and spatial dimensions, allowing fine-grained modulation of\ndynamic EEG patterns with low computational overhead. To further address\nrepresentational entanglement, PTSM enforces information-theoretic constraints\nthat decompose latent embeddings into orthogonal task-related and\nsubject-related subspaces. The model is trained end-to-end via a\nmulti-objective loss integrating classification, contrastive, and\ndisentanglement objectives. Extensive experiments on cross-subject motor\nimagery datasets demonstrate that PTSM achieves strong zero-shot\ngeneralization, outperforming state-of-the-art baselines without\nsubject-specific calibration. Results highlight the efficacy of disentangled\nneural representations for achieving both personalized and transferable\ndecoding in non-stationary neurophysiological settings.", "published": "2025-08-15 09:51:14", "link": "http://arxiv.org/abs/2508.11357v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism", "abstract": "Recent advancements in Large Language Models have yielded significant\nimprovements in complex reasoning tasks such as mathematics and programming.\nHowever, these models remain heavily dependent on annotated data and exhibit\nlimited adaptability in unsupervised scenarios. To address these limitations,\ntest-time reinforcement learning (TTRL) has been proposed, which enables\nself-optimization by leveraging model-generated pseudo-labels. Despite its\npromise, TTRL faces several key challenges, including high inference costs due\nto parallel rollouts and early-stage estimation bias that fosters\noverconfidence, reducing output diversity and causing performance plateaus. To\naddress these challenges, we introduce an entropy-based mechanism to enhance\nthe exploration-exploitation balance in test-time reinforcement learning\nthrough two strategies: Entropy-fork Tree Majority Rollout (ETMR) and\nEntropy-based Advantage Reshaping (EAR). Compared with the baseline, our\napproach enables Llama3.1-8B to achieve a 68 percent relative improvement in\nPass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent of\nthe rollout tokens budget. This highlights our method's ability to effectively\noptimize the trade-off between inference efficiency, diversity, and estimation\nrobustness, thereby advancing unsupervised reinforcement learning for\nopen-domain reasoning tasks.", "published": "2025-08-15 09:49:14", "link": "http://arxiv.org/abs/2508.11356v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Leveraging the RETFound foundation model for optic disc segmentation in retinal images", "abstract": "RETFound is a well-known foundation model (FM) developed for fundus camera\nand optical coherence tomography images. It has shown promising performance\nacross multiple datasets in diagnosing diseases, both eye-specific and\nsystemic, from retinal images. However, to our best knowledge, it has not been\nused for other tasks. We present the first adaptation of RETFound for optic\ndisc segmentation, a ubiquitous and foundational task in retinal image\nanalysis. The resulting segmentation system outperforms state-of-the-art,\nsegmentation-specific baseline networks after training a head with only a very\nmodest number of task-specific examples. We report and discuss results with\nfour public datasets, IDRID, Drishti-GS, RIM-ONE-r3, and REFUGE, and a private\ndataset, GoDARTS, achieving about 96% Dice consistently across all datasets.\nOverall, our method obtains excellent performance in internal verification,\ndomain generalization and domain adaptation, and exceeds most of the\nstate-of-the-art baseline results. We discuss the results in the framework of\nthe debate about FMs as alternatives to task-specific architectures. The code\nis available at: [link to be added after the paper is accepted]", "published": "2025-08-15 09:43:49", "link": "http://arxiv.org/abs/2508.11354v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models", "abstract": "With the growing incorporation of deep neural network (DNN) models into\nmodern software systems, the prohibitive construction costs have become a\nsignificant challenge. Model reuse has been widely applied to reduce training\ncosts, but indiscriminately reusing entire models may incur significant\ninference overhead. Consequently, DNN modularization has gained attention,\nenabling module reuse by decomposing DNN models. The emerging\nmodularizing-while-training (MwT) paradigm, which incorporates modularization\ninto training, outperforms modularizing-after-training approaches. However,\nexisting MwT methods focus on small-scale CNN models at the convolutional\nkernel level and struggle with diverse DNNs and large-scale models,\nparticularly Transformer-based models. To address these limitations, we propose\nNeMo, a scalable and generalizable MwT approach. NeMo operates at the neuron\nlevel fundamental component common to all DNNs-ensuring applicability to\nTransformers and various architectures. We design a contrastive learning-based\nmodular training method with an effective composite loss function, enabling\nscalability to large-scale models. Comprehensive experiments on two\nTransformer-based models and four CNN models across two classification datasets\ndemonstrate NeMo's superiority over state-of-the-art MwT methods. Results show\naverage gains of 1.72% in module classification accuracy and 58.10% reduction\nin module size, demonstrating efficacy across both CNN and large-scale\nTransformer-based models. A case study on open-source projects shows NeMo's\npotential benefits in practical scenarios, offering a promising approach for\nscalable and generalizable DNN modularization.", "published": "2025-08-15 09:25:40", "link": "http://arxiv.org/abs/2508.11348v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "SAGE: Scale-Aware Gradual Evolution for Continual Knowledge Graph Embedding", "abstract": "Traditional knowledge graph (KG) embedding methods aim to represent entities\nand relations in a low-dimensional space, primarily focusing on static graphs.\nHowever, real-world KGs are dynamically evolving with the constant addition of\nentities, relations and facts. To address such dynamic nature of KGs, several\ncontinual knowledge graph embedding (CKGE) methods have been developed to\nefficiently update KG embeddings to accommodate new facts while maintaining\nlearned knowledge. As KGs grow at different rates and scales in real-world\nscenarios, existing CKGE methods often fail to consider the varying scales of\nupdates and lack systematic evaluation throughout the entire update process. In\nthis paper, we propose SAGE, a scale-aware gradual evolution framework for\nCKGE. Specifically, SAGE firstly determine the embedding dimensions based on\nthe update scales and expand the embedding space accordingly. The Dynamic\nDistillation mechanism is further employed to balance the preservation of\nlearned knowledge and the incorporation of new facts. We conduct extensive\nexperiments on seven benchmarks, and the results show that SAGE consistently\noutperforms existing baselines, with a notable improvement of 1.38% in MRR,\n1.25% in H@1 and 1.6% in H@10. Furthermore, experiments comparing SAGE with\nmethods using fixed embedding dimensions show that SAGE achieves optimal\nperformance on every snapshot, demonstrating the importance of adaptive\nembedding dimensions in CKGE. The codes of SAGE are publicly available at:\nhttps://github.com/lyfxjtu/Dynamic-Embedding.", "published": "2025-08-15 09:23:23", "link": "http://arxiv.org/abs/2508.11347v1", "categories": ["cs.AI", "cs.LG", "I.2.4; I.2.6; H.2.8"], "primary_category": "cs.AI"}
{"title": "RegimeNAS: Regime-Aware Differentiable Architecture Search With Theoretical Guarantees for Financial Trading", "abstract": "We introduce RegimeNAS, a novel differentiable architecture search framework\nspecifically designed to enhance cryptocurrency trading performance by\nexplicitly integrating market regime awareness. Addressing the limitations of\nstatic deep learning models in highly dynamic financial environments, RegimeNAS\nfeatures three core innovations: (1) a theoretically grounded Bayesian search\nspace optimizing architectures with provable convergence properties; (2)\nspecialized, dynamically activated neural modules (Volatility, Trend, and Range\nblocks) tailored for distinct market conditions; and (3) a multi-objective loss\nfunction incorporating market-specific penalties (e.g., volatility matching,\ntransition smoothness) alongside mathematically enforced Lipschitz stability\nconstraints. Regime identification leverages multi-head attention across\nmultiple timeframes for improved accuracy and uncertainty estimation. Rigorous\nempirical evaluation on extensive real-world cryptocurrency data demonstrates\nthat RegimeNAS significantly outperforms state-of-the-art benchmarks, achieving\nan 80.3% Mean Absolute Error reduction compared to the best traditional\nrecurrent baseline and converging substantially faster (9 vs. 50+ epochs).\nAblation studies and regime-specific analysis confirm the critical contribution\nof each component, particularly the regime-aware adaptation mechanism. This\nwork underscores the imperative of embedding domain-specific knowledge, such as\nmarket regimes, directly within the NAS process to develop robust and adaptive\nmodels for challenging financial applications.", "published": "2025-08-15 09:09:54", "link": "http://arxiv.org/abs/2508.11338v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless Edge-Device Networks", "abstract": "The integration of wireless communications and Large Language Models (LLMs)\nis poised to unlock ubiquitous intelligent services, yet deploying them in\nwireless edge-device collaborative environments presents a critical trade-off\nbetween inference quality and end-to-end latency. A fundamental mismatch exists\nbetween task complexity and resource allocation: offloading simple queries\ninvites prohibitive latency, while on-device models lack the capacity for\ndemanding computations. To address this challenge, we propose a dynamic,\nquality-latency aware routing framework that orchestrates inference between a\nlightweight model on the mobile device and a powerful model on the edge server.\nOur framework employs two distinct cost models: for single-turn queries, it\nfuses a BERT-predicted semantic score with communication and computation\noverheads; for multi-turn dialogues, it further quantifies context-aware costs\narising from model switching and KV-cache management. While maintaining full\ninference quality, extensive experiments demonstrate that our framework cuts\naverage response latency by 5-15% and reduces large model invocations by 10-20%\nagainst competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks.", "published": "2025-08-15 07:55:05", "link": "http://arxiv.org/abs/2508.11291v1", "categories": ["cs.IT", "cs.AI", "cs.LG", "math.IT"], "primary_category": "cs.IT"}
{"title": "CSGO: Generalized Optimization for Cold Start in Wireless Collaborative Edge LLM Systems", "abstract": "While deploying large language models on edge devices promises low-latency\nand privacy-preserving AI services, it is hindered by limited device resources.\nAlthough pipeline parallelism facilitates distributed inference, existing\napproaches often ignore the cold-start latency caused by on-demand model\nloading. In this paper, we propose a latency-aware scheduling framework that\noverlaps model loading with computation and communication to minimize total\ninference latency. Based on device and model parameters, the framework\ndynamically adjusts layer partitioning and allocation to effectively hide\nloading time, thereby eliminating as many idle periods as possible. We\nformulate the problem as a Mixed-Integer Non-Linear Program and design an\nefficient dynamic programming algorithm to optimize model partitioning and\ndevice assignment. Experimental results show that the proposed method\nsignificantly reduces cold-start latency compared to baseline strategies.", "published": "2025-08-15 07:49:22", "link": "http://arxiv.org/abs/2508.11287v1", "categories": ["cs.IT", "cs.AI", "cs.LG", "math.IT"], "primary_category": "cs.IT"}
{"title": "Scene Graph-Guided Proactive Replanning for Failure-Resilient Embodied Agent", "abstract": "When humans perform everyday tasks, we naturally adjust our actions based on\nthe current state of the environment. For instance, if we intend to put\nsomething into a drawer but notice it is closed, we open it first. However,\nmany autonomous robots lack this adaptive awareness. They often follow\npre-planned actions that may overlook subtle yet critical changes in the scene,\nwhich can result in actions being executed under outdated assumptions and\neventual failure. While replanning is critical for robust autonomy, most\nexisting methods respond only after failures occur, when recovery may be\ninefficient or infeasible. While proactive replanning holds promise for\npreventing failures in advance, current solutions often rely on manually\ndesigned rules and extensive supervision. In this work, we present a proactive\nreplanning framework that detects and corrects failures at subtask boundaries\nby comparing scene graphs constructed from current RGB-D observations against\nreference graphs extracted from successful demonstrations. When the current\nscene fails to align with reference trajectories, a lightweight reasoning\nmodule is activated to diagnose the mismatch and adjust the plan. Experiments\nin the AI2-THOR simulator demonstrate that our approach detects semantic and\nspatial mismatches before execution failures occur, significantly improving\ntask success and robustness.", "published": "2025-08-15 07:48:51", "link": "http://arxiv.org/abs/2508.11286v1", "categories": ["cs.RO", "cs.AI", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Is General-Purpose AI Reasoning Sensitive to Data-Induced Cognitive Biases? Dynamic Benchmarking on Typical Software Engineering Dilemmas", "abstract": "Human cognitive biases in software engineering can lead to costly errors.\nWhile general-purpose AI (GPAI) systems may help mitigate these biases due to\ntheir non-human nature, their training on human-generated data raises a\ncritical question: Do GPAI systems themselves exhibit cognitive biases?\n  To investigate this, we present the first dynamic benchmarking framework to\nevaluate data-induced cognitive biases in GPAI within software engineering\nworkflows. Starting with a seed set of 16 hand-crafted realistic tasks, each\nfeaturing one of 8 cognitive biases (e.g., anchoring, framing) and\ncorresponding unbiased variants, we test whether bias-inducing linguistic cues\nunrelated to task logic can lead GPAI systems from correct to incorrect\nconclusions.\n  To scale the benchmark and ensure realism, we develop an on-demand\naugmentation pipeline relying on GPAI systems to generate task variants that\npreserve bias-inducing cues while varying surface details. This pipeline\nensures correctness (88--99% on average, according to human evaluation),\npromotes diversity, and controls reasoning complexity by leveraging\nProlog-based reasoning and LLM-as-a-judge validation. It also verifies that the\nembedded biases are both harmful and undetectable by logic-based, unbiased\nreasoners.\n  We evaluate leading GPAI systems (GPT, LLaMA, DeepSeek) and find a consistent\ntendency to rely on shallow linguistic heuristics over deep reasoning. All\nsystems exhibit cognitive biases (ranging from 5.9% to 35% across types), with\nbias sensitivity increasing sharply with task complexity (up to 49%),\nhighlighting critical risks in real-world software engineering deployments.", "published": "2025-08-15 07:29:46", "link": "http://arxiv.org/abs/2508.11278v1", "categories": ["cs.HC", "cs.AI", "cs.SE"], "primary_category": "cs.HC"}
{"title": "Enhancing Supervised Composed Image Retrieval via Reasoning-Augmented Representation Engineering", "abstract": "Composed Image Retrieval (CIR) presents a significant challenge as it\nrequires jointly understanding a reference image and a modified textual\ninstruction to find relevant target images. Some existing methods attempt to\nuse a two-stage approach to further refine retrieval results. However, this\noften requires additional training of a ranking model. Despite the success of\nChain-of-Thought (CoT) techniques in reducing training costs for language\nmodels, their application in CIR tasks remains limited -- compressing visual\ninformation into text or relying on elaborate prompt designs. Besides, existing\nworks only utilize it for zero-shot CIR, as it is challenging to achieve\nsatisfactory results in supervised CIR with a well-trained model. In this work,\nwe proposed a framework that includes the Pyramid Matching Model with\nTraining-Free Refinement (PMTFR) to address these challenges. Through a simple\nbut effective module called Pyramid Patcher, we enhanced the Pyramid Matching\nModel's understanding of visual information at different granularities.\nInspired by representation engineering, we extracted representations from COT\ndata and injected them into the LVLMs. This approach allowed us to obtain\nrefined retrieval scores in the Training-Free Refinement paradigm without\nrelying on explicit textual reasoning, further enhancing performance. Extensive\nexperiments on CIR benchmarks demonstrate that PMTFR surpasses state-of-the-art\nmethods in supervised CIR tasks. The code will be made public.", "published": "2025-08-15 07:10:10", "link": "http://arxiv.org/abs/2508.11272v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Vision-Language Models display a strong gender bias", "abstract": "Vision-language models (VLM) align images and text in a shared representation\nspace that is useful for retrieval and zero-shot transfer. Yet, this alignment\ncan encode and amplify social stereotypes in subtle ways that are not obvious\nfrom standard accuracy metrics. In this study, we test whether the contrastive\nvision-language encoder exhibits gender-linked associations when it places\nembeddings of face images near embeddings of short phrases that describe\noccupations and activities. We assemble a dataset of 220 face photographs split\nby perceived binary gender and a set of 150 unique statements distributed\nacross six categories covering emotional labor, cognitive labor, domestic\nlabor, technical labor, professional roles, and physical labor. We compute\nunit-norm image embeddings for every face and unit-norm text embeddings for\nevery statement, then define a statement-level association score as the\ndifference between the mean cosine similarity to the male set and the mean\ncosine similarity to the female set, where positive values indicate stronger\nassociation with the male set and negative values indicate stronger association\nwith the female set. We attach bootstrap confidence intervals by resampling\nimages within each gender group, aggregate by category with a separate\nbootstrap over statements, and run a label-swap null model that estimates the\nlevel of mean absolute association we would expect if no gender structure were\npresent. The outcome is a statement-wise and category-wise map of gender\nassociations in a contrastive vision-language space, accompanied by\nuncertainty, simple sanity checks, and a robust gender bias evaluation\nframework.", "published": "2025-08-15 06:57:26", "link": "http://arxiv.org/abs/2508.11262v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Hallucination in LLM-Based Code Generation: An Automotive Case Study", "abstract": "Large Language Models (LLMs) have shown significant potential in automating\ncode generation tasks offering new opportunities across software engineering\ndomains. However, their practical application remains limited due to\nhallucinations - outputs that appear plausible but are factually incorrect,\nunverifiable or nonsensical. This paper investigates hallucination phenomena in\nthe context of code generation with a specific focus on the automotive domain.\nA case study is presented that evaluates multiple code LLMs for three different\nprompting complexities ranging from a minimal one-liner prompt to a prompt with\nCovesa Vehicle Signal Specifications (VSS) as additional context and finally to\na prompt with an additional code skeleton. The evaluation reveals a high\nfrequency of syntax violations, invalid reference errors and API knowledge\nconflicts in state-of-the-art models GPT-4.1, Codex and GPT-4o. Among the\nevaluated models, only GPT-4.1 and GPT-4o were able to produce a correct\nsolution when given the most context-rich prompt. Simpler prompting strategies\nfailed to yield a working result, even after multiple refinement iterations.\nThese findings highlight the need for effective mitigation techniques to ensure\nthe safe and reliable use of LLM generated code, especially in safety-critical\ndomains such as automotive software systems.", "published": "2025-08-15 06:46:50", "link": "http://arxiv.org/abs/2508.11257v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE"}
{"title": "Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception", "abstract": "Dense visual perception tasks have been constrained by their reliance on\npredefined categories, limiting their applicability in real-world scenarios\nwhere visual concepts are unbounded. While Vision-Language Models (VLMs) like\nCLIP have shown promise in open-vocabulary tasks, their direct application to\ndense perception often leads to suboptimal performance due to limitations in\nlocal feature representation. In this work, we present our observation that\nCLIP's image tokens struggle to effectively aggregate information from\nspatially or semantically related regions, resulting in features that lack\nlocal discriminability and spatial consistency. To address this issue, we\npropose DeCLIP, a novel framework that enhances CLIP by decoupling the\nself-attention module to obtain ``content'' and ``context'' features\nrespectively. \\revise{The context features are enhanced by jointly distilling\nsemantic correlations from Vision Foundation Models (VFMs) and object integrity\ncues from diffusion models, thereby enhancing spatial consistency. In parallel,\nthe content features are aligned with image crop representations and\nconstrained by region correlations from VFMs to improve local discriminability.\nExtensive experiments demonstrate that DeCLIP establishes a solid foundation\nfor open-vocabulary dense perception, consistently achieving state-of-the-art\nperformance across a broad spectrum of tasks, including 2D detection and\nsegmentation, 3D instance segmentation, video instance segmentation, and 6D\nobject pose estimation.} Code is available at\nhttps://github.com/xiaomoguhz/DeCLIP", "published": "2025-08-15 06:43:51", "link": "http://arxiv.org/abs/2508.11256v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Graph Neural Diffusion via Generalized Opinion Dynamics", "abstract": "There has been a growing interest in developing diffusion-based Graph Neural\nNetworks (GNNs), building on the connections between message passing mechanisms\nin GNNs and physical diffusion processes. However, existing methods suffer from\nthree critical limitations: (1) they rely on homogeneous diffusion with static\ndynamics, limiting adaptability to diverse graph structures; (2) their depth is\nconstrained by computational overhead and diminishing interpretability; and (3)\ntheoretical understanding of their convergence behavior remains limited. To\naddress these challenges, we propose GODNF, a Generalized Opinion Dynamics\nNeural Framework, which unifies multiple opinion dynamics models into a\nprincipled, trainable diffusion mechanism. Our framework captures heterogeneous\ndiffusion patterns and temporal dynamics via node-specific behavior modeling\nand dynamic neighborhood influence, while ensuring efficient and interpretable\nmessage propagation even at deep layers. We provide a rigorous theoretical\nanalysis demonstrating GODNF's ability to model diverse convergence\nconfigurations. Extensive empirical evaluations of node classification and\ninfluence estimation tasks confirm GODNF's superiority over state-of-the-art\nGNNs.", "published": "2025-08-15 06:36:57", "link": "http://arxiv.org/abs/2508.11249v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation", "abstract": "Sampling efficiency is critical for deploying visuomotor learning in\nreal-world robotic manipulation. While task symmetry has emerged as a promising\ninductive bias to improve efficiency, most prior work is limited to isometric\nsymmetries -- applying the same group transformation to all task objects across\nall timesteps. In this work, we explore non-isometric symmetries, applying\nmultiple independent group transformations across spatial and temporal\ndimensions to relax these constraints. We introduce a novel formulation of the\npartially observable Markov decision process (POMDP) that incorporates the\nnon-isometric symmetry structures, and propose a simple yet effective data\naugmentation method, Multi-Group Equivariance Augmentation (MEA). We integrate\nMEA with offline reinforcement learning to enhance sampling efficiency, and\nintroduce a voxel-based visual representation that preserves translational\nequivariance. Extensive simulation and real-robot experiments across two\nmanipulation domains demonstrate the effectiveness of our approach.", "published": "2025-08-15 04:30:01", "link": "http://arxiv.org/abs/2508.11204v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation", "abstract": "We introduce StyleMM, a novel framework that can construct a stylized 3D\nMorphable Model (3DMM) based on user-defined text descriptions specifying a\ntarget style. Building upon a pre-trained mesh deformation network and a\ntexture generator for original 3DMM-based realistic human faces, our approach\nfine-tunes these models using stylized facial images generated via text-guided\nimage-to-image (i2i) translation with a diffusion model, which serve as\nstylization targets for the rendered mesh. To prevent undesired changes in\nidentity, facial alignment, or expressions during i2i translation, we introduce\na stylization method that explicitly preserves the facial attributes of the\nsource image. By maintaining these critical attributes during image\nstylization, the proposed approach ensures consistent 3D style transfer across\nthe 3DMM parameter space through image-based training. Once trained, StyleMM\nenables feed-forward generation of stylized face meshes with explicit control\nover shape, expression, and texture parameters, producing meshes with\nconsistent vertex connectivity and animatability. Quantitative and qualitative\nevaluations demonstrate that our approach outperforms state-of-the-art methods\nin terms of identity-level facial diversity and stylization capability. The\ncode and videos are available at\n[kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).", "published": "2025-08-15 04:29:46", "link": "http://arxiv.org/abs/2508.11203v1", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.MM", "51-04", "I.3.8; I.4.9"], "primary_category": "cs.GR"}
{"title": "Visuomotor Grasping with World Models for Surgical Robots", "abstract": "Grasping is a fundamental task in robot-assisted surgery (RAS), and\nautomating it can reduce surgeon workload while enhancing efficiency, safety,\nand consistency beyond teleoperated systems. Most prior approaches rely on\nexplicit object pose tracking or handcrafted visual features, limiting their\ngeneralization to novel objects, robustness to visual disturbances, and the\nability to handle deformable objects. Visuomotor learning offers a promising\nalternative, but deploying it in RAS presents unique challenges, such as low\nsignal-to-noise ratio in visual observations, demands for high safety and\nmillimeter-level precision, as well as the complex surgical environment. This\npaper addresses three key challenges: (i) sim-to-real transfer of visuomotor\npolicies to ex vivo surgical scenes, (ii) visuomotor learning using only a\nsingle stereo camera pair -- the standard RAS setup, and (iii) object-agnostic\ngrasping with a single policy that generalizes to diverse, unseen surgical\nobjects without retraining or task-specific models. We introduce Grasp Anything\nfor Surgery V2 (GASv2), a visuomotor learning framework for surgical grasping.\nGASv2 leverages a world-model-based architecture and a surgical perception\npipeline for visual observations, combined with a hybrid control system for\nsafe execution. We train the policy in simulation using domain randomization\nfor sim-to-real transfer and deploy it on a real robot in both phantom-based\nand ex vivo surgical settings, using only a single pair of endoscopic cameras.\nExtensive experiments show our policy achieves a 65% success rate in both\nsettings, generalizes to unseen objects and grippers, and adapts to diverse\ndisturbances, demonstrating strong performance, generality, and robustness.", "published": "2025-08-15 04:23:07", "link": "http://arxiv.org/abs/2508.11200v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "Quantum-Boosted High-Fidelity Deep Learning", "abstract": "A fundamental limitation of probabilistic deep learning is its predominant\nreliance on Gaussian priors. This simplistic assumption prevents models from\naccurately capturing the complex, non-Gaussian landscapes of natural data,\nparticularly in demanding domains like complex biological data, severely\nhindering the fidelity of the model for scientific discovery. The\nphysically-grounded Boltzmann distribution offers a more expressive\nalternative, but it is computationally intractable on classical computers. To\ndate, quantum approaches have been hampered by the insufficient qubit scale and\noperational stability required for the iterative demands of deep learning.\nHere, we bridge this gap by introducing the Quantum Boltzmann\nMachine-Variational Autoencoder (QBM-VAE), a large-scale and long-time stable\nhybrid quantum-classical architecture. Our framework leverages a quantum\nprocessor for efficient sampling from the Boltzmann distribution, enabling its\nuse as a powerful prior within a deep generative model. Applied to\nmillion-scale single-cell datasets from multiple sources, the QBM-VAE generates\na latent space that better preserves complex biological structures,\nconsistently outperforming conventional Gaussian-based deep learning models\nlike VAE and SCVI in essential tasks such as omics data integration, cell-type\nclassification, and trajectory inference. It also provides a typical example of\nintroducing a physics priori into deep learning to drive the model to acquire\nscientific discovery capabilities that breaks through data limitations. This\nwork provides the demonstration of a practical quantum advantage in deep\nlearning on a large-scale scientific problem and offers a transferable\nblueprint for developing hybrid quantum AI models.", "published": "2025-08-15 03:51:20", "link": "http://arxiv.org/abs/2508.11190v1", "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "primary_category": "cs.LG"}
{"title": "On Strong and Weak Admissibility in Non-Flat Assumption-Based Argumentation", "abstract": "In this work, we broaden the investigation of admissibility notions in the\ncontext of assumption-based argumentation (ABA). More specifically, we study\ntwo prominent alternatives to the standard notion of admissibility from\nabstract argumentation, namely strong and weak admissibility, and introduce the\nrespective preferred, complete and grounded semantics for general (sometimes\ncalled non-flat) ABA. To do so, we use abstract bipolar set-based argumentation\nframeworks (BSAFs) as formal playground since they concisely capture the\nrelations between assumptions and are expressive enough to represent general\nnon-flat ABA frameworks, as recently shown. While weak admissibility has been\nrecently investigated for a restricted fragment of ABA in which assumptions\ncannot be derived (flat ABA), strong admissibility has not been investigated\nfor ABA so far. We introduce strong admissibility for ABA and investigate\ndesirable properties. We furthermore extend the recent investigations of weak\nadmissibility in the flat ABA fragment to the non-flat case. We show that the\ncentral modularization property is maintained under classical, strong, and weak\nadmissibility. We also show that strong and weakly admissible semantics in\nnon-flat ABA share some of the shortcomings of standard admissible semantics\nand discuss ways to address these.", "published": "2025-08-15 03:13:07", "link": "http://arxiv.org/abs/2508.11182v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "A Semi-supervised Generative Model for Incomplete Multi-view Data Integration with Missing Labels", "abstract": "Multi-view learning is widely applied to real-life datasets, such as multiple\nomics biological data, but it often suffers from both missing views and missing\nlabels. Prior probabilistic approaches addressed the missing view problem by\nusing a product-of-experts scheme to aggregate representations from present\nviews and achieved superior performance over deterministic classifiers, using\nthe information bottleneck (IB) principle. However, the IB framework is\ninherently fully supervised and cannot leverage unlabeled data. In this work,\nwe propose a semi-supervised generative model that utilizes both labeled and\nunlabeled samples in a unified framework. Our method maximizes the likelihood\nof unlabeled samples to learn a latent space shared with the IB on labeled\ndata. We also perform cross-view mutual information maximization in the latent\nspace to enhance the extraction of shared information across views. Compared to\nexisting approaches, our model achieves better predictive and imputation\nperformance on both image and multi-omics data with missing views and limited\nlabeled samples.", "published": "2025-08-15 03:10:18", "link": "http://arxiv.org/abs/2508.11180v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Better Supervised Fine-tuning for VQA: Integer-Only Loss", "abstract": "With the rapid advancement of vision language models(VLM), their ability to\nassess visual content based on specific criteria and dimensions has become\nincreasingly critical for applications such as video-theme consistency\nassessment and visual quality scoring. However, existing methods often suffer\nfrom imprecise results and inefficient loss calculation, which limit the focus\nof the model on key evaluation indicators. To address this, we propose\nIOVQA(Integer-only VQA), a novel fine-tuning approach tailored for VLMs to\nenhance their performance in video quality assessment tasks. The key innovation\nof IOVQA lies in its label construction and its targeted loss calculation\nmechanism. Specifically, during dataset curation, we constrain the model's\noutput to integers within the range of [10,50], ensuring numerical stability,\nand convert decimal Overall_MOS to integer before using them as labels. We also\nintroduce a target-mask strategy: when computing the loss, only the first\ntwo-digit-integer of the label is unmasked, forcing the model to learn the\ncritical components of the numerical evaluation. After fine-tuning the\nQwen2.5-VL model using the constructed dataset, experimental results\ndemonstrate that the proposed method significantly improves the model's\naccuracy and consistency in the VQA task, ranking 3rd in VQualA 2025\nGenAI-Bench AIGC Video Quality Assessment Challenge -- Track I. Our work\nhighlights the effectiveness of merely leaving integer labels during\nfine-tuning, providing an effective idea for optimizing VLMs in quantitative\nevaluation scenarios.", "published": "2025-08-15 02:40:43", "link": "http://arxiv.org/abs/2508.11170v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Role-Augmented Intent-Driven Generative Search Engine Optimization", "abstract": "Generative Search Engines (GSEs), powered by Large Language Models (LLMs) and\nRetrieval-Augmented Generation (RAG), are reshaping information retrieval.\nWhile commercial systems (e.g., BingChat, Perplexity.ai) demonstrate impressive\nsemantic synthesis capabilities, their black-box nature fundamentally\nundermines established Search Engine Optimization (SEO) practices. Content\ncreators face a critical challenge: their optimization strategies, effective in\ntraditional search engines, are misaligned with generative retrieval contexts,\nresulting in diminished visibility. To bridge this gap, we propose a\nRole-Augmented Intent-Driven Generative Search Engine Optimization (G-SEO)\nmethod, providing a structured optimization pathway tailored for GSE scenarios.\nOur method models search intent through reflective refinement across diverse\ninformational roles, enabling targeted content enhancement. To better evaluate\nthe method under realistic settings, we address the benchmarking limitations of\nprior work by: (1) extending the GEO dataset with diversified query variations\nreflecting real-world search scenarios and (2) introducing G-Eval 2.0, a\n6-level LLM-augmented evaluation rubric for fine-grained human-aligned\nassessment. Experimental results demonstrate that search intent serves as an\neffective signal for guiding content optimization, yielding significant\nimprovements over single-aspect baseline approaches in both subjective\nimpressions and objective content visibility within GSE responses.", "published": "2025-08-15 02:08:55", "link": "http://arxiv.org/abs/2508.11158v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "AlphaAgents: Large Language Model based Multi-Agents for Equity Portfolio Constructions", "abstract": "The field of artificial intelligence (AI) agents is evolving rapidly, driven\nby the capabilities of Large Language Models (LLMs) to autonomously perform and\nrefine tasks with human-like efficiency and adaptability. In this context,\nmulti-agent collaboration has emerged as a promising approach, enabling\nmultiple AI agents to work together to solve complex challenges. This study\ninvestigates the application of role-based multi-agent systems to support stock\nselection in equity research and portfolio management. We present a\ncomprehensive analysis performed by a team of specialized agents and evaluate\ntheir stock-picking performance against established benchmarks under varying\nlevels of risk tolerance. Furthermore, we examine the advantages and\nlimitations of employing multi-agent frameworks in equity analysis, offering\ncritical insights into their practical efficacy and implementation challenges.", "published": "2025-08-15 01:49:56", "link": "http://arxiv.org/abs/2508.11152v1", "categories": ["q-fin.ST", "cs.AI"], "primary_category": "q-fin.ST"}
{"title": "Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward", "abstract": "Existing reinforcement learning (RL) methods struggle with long-horizon\nrobotic manipulation tasks, particularly those involving sparse rewards. While\naction chunking is a promising paradigm for robotic manipulation, using RL to\ndirectly learn continuous action chunks in a stable and data-efficient manner\nremains a critical challenge. This paper introduces AC3 (Actor-Critic for\nContinuous Chunks), a novel RL framework that learns to generate\nhigh-dimensional, continuous action sequences. To make this learning process\nstable and data-efficient, AC3 incorporates targeted stabilization mechanisms\nfor both the actor and the critic. First, to ensure reliable policy\nimprovement, the actor is trained with an asymmetric update rule, learning\nexclusively from successful trajectories. Second, to enable effective value\nlearning despite sparse rewards, the critic's update is stabilized using\nintra-chunk $n$-step returns and further enriched by a self-supervised module\nproviding intrinsic rewards at anchor points aligned with each action chunk. We\nconducted extensive experiments on 25 tasks from the BiGym and RLBench\nbenchmarks. Results show that by using only a few demonstrations and a simple\nmodel architecture, AC3 achieves superior success rates on most tasks,\nvalidating its effective design.", "published": "2025-08-15 01:27:15", "link": "http://arxiv.org/abs/2508.11143v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "Thyme: Think Beyond Images", "abstract": "Following OpenAI's introduction of the ``thinking with images'' concept,\nrecent efforts have explored stimulating the use of visual information in the\nreasoning process to enhance model performance in perception and reasoning\ntasks. However, to the best of our knowledge, no open-source work currently\noffers a feature set as rich as proprietary models (O3), which can perform\ndiverse image manipulations and simultaneously enhance logical reasoning\ncapabilities through code. In this paper, we make a preliminary attempt in this\ndirection by introducing Thyme (Think Beyond Images), a novel paradigm for\nenabling MLLMs to transcend existing ``think with images'' approaches by\nautonomously generating and executing diverse image processing and\ncomputational operations via executable code. This approach not only\nfacilitates a rich, on-the-fly set of image manipulations (e.g., cropping,\nrotation, contrast enhancement) but also allows for mathematical computations,\nall while maintaining high autonomy in deciding when and how to apply these\noperations. We activate this capability through a two-stage training strategy:\nan initial SFT on a curated dataset of 500K samples to teach code generation,\nfollowed by a RL phase to refine decision-making. For the RL stage, we manually\ncollect and design high-resolution question-answer pairs to increase the\nlearning difficulty, and we propose GRPO-ATS (Group Relative Policy\nOptimization with Adaptive Temperature Sampling), an algorithm that applies\ndistinct temperatures to text and code generation to balance reasoning\nexploration with code execution precision. We conduct extensive experimental\nanalysis and ablation studies. Comprehensive evaluations on nearly 20\nbenchmarks show that Thyme yields significant and consistent performance gains,\nparticularly in challenging high-resolution perception and complex reasoning\ntasks.", "published": "2025-08-15 17:59:49", "link": "http://arxiv.org/abs/2508.11630v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "LoRAtorio: An intrinsic approach to LoRA Skill Composition", "abstract": "Low-Rank Adaptation (LoRA) has become a widely adopted technique in\ntext-to-image diffusion models, enabling the personalisation of visual concepts\nsuch as characters, styles, and objects. However, existing approaches struggle\nto effectively compose multiple LoRA adapters, particularly in open-ended\nsettings where the number and nature of required skills are not known in\nadvance. In this work, we present LoRAtorio, a novel train-free framework for\nmulti-LoRA composition that leverages intrinsic model behaviour. Our method is\nmotivated by two key observations: (1) LoRA adapters trained on narrow domains\nproduce denoised outputs that diverge from the base model, and (2) when\noperating out-of-distribution, LoRA outputs show behaviour closer to the base\nmodel than when conditioned in distribution. The balance between these two\nobservations allows for exceptional performance in the single LoRA scenario,\nwhich nevertheless deteriorates when multiple LoRAs are loaded. Our method\noperates in the latent space by dividing it into spatial patches and computing\ncosine similarity between each patch's predicted noise and that of the base\nmodel. These similarities are used to construct a spatially-aware weight\nmatrix, which guides a weighted aggregation of LoRA outputs. To address domain\ndrift, we further propose a modification to classifier-free guidance that\nincorporates the base model's unconditional score into the composition. We\nextend this formulation to a dynamic module selection setting, enabling\ninference-time selection of relevant LoRA adapters from a large pool. LoRAtorio\nachieves state-of-the-art performance, showing up to a 1.3% improvement in\nClipScore and a 72.43% win rate in GPT-4V pairwise evaluations, and generalises\neffectively to multiple latent diffusion models.", "published": "2025-08-15 17:52:56", "link": "http://arxiv.org/abs/2508.11624v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "CoreEditor: Consistent 3D Editing via Correspondence-constrained Diffusion", "abstract": "Text-driven 3D editing seeks to modify 3D scenes according to textual\ndescriptions, and most existing approaches tackle this by adapting pre-trained\n2D image editors to multi-view inputs. However, without explicit control over\nmulti-view information exchange, they often fail to maintain cross-view\nconsistency, leading to insufficient edits and blurry details. We introduce\nCoreEditor, a novel framework for consistent text-to-3D editing. The key\ninnovation is a correspondence-constrained attention mechanism that enforces\nprecise interactions between pixels expected to remain consistent throughout\nthe diffusion denoising process. Beyond relying solely on geometric alignment,\nwe further incorporate semantic similarity estimated during denoising, enabling\nmore reliable correspondence modeling and robust multi-view editing. In\naddition, we design a selective editing pipeline that allows users to choose\npreferred results from multiple candidates, offering greater flexibility and\nuser control. Extensive experiments show that CoreEditor produces high-quality,\n3D-consistent edits with sharper details, significantly outperforming prior\nmethods.", "published": "2025-08-15 17:13:11", "link": "http://arxiv.org/abs/2508.11603v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "DashCam Video: A complementary low-cost data stream for on-demand forest-infrastructure system monitoring", "abstract": "Our study introduces a novel, low-cost, and reproducible framework for\nreal-time, object-level structural assessment and geolocation of roadside\nvegetation and infrastructure with commonly available but underutilized\ndashboard camera (dashcam) video data. We developed an end-to-end pipeline that\ncombines monocular depth estimation, depth error correction, and geometric\ntriangulation to generate accurate spatial and structural data from\nstreet-level video streams from vehicle-mounted dashcams. Depth maps were first\nestimated using a state-of-the-art monocular depth model, then refined via a\ngradient-boosted regression framework to correct underestimations, particularly\nfor distant objects. The depth correction model achieved strong predictive\nperformance (R2 = 0.92, MAE = 0.31 on transformed scale), significantly\nreducing bias beyond 15 m. Further, object locations were estimated using\nGPS-based triangulation, while object heights were calculated using pin hole\ncamera geometry. Our method was evaluated under varying conditions of camera\nplacement and vehicle speed. Low-speed vehicle with inside camera gave the\nhighest accuracy, with mean geolocation error of 2.83 m, and mean absolute\nerror (MAE) in height estimation of 2.09 m for trees and 0.88 m for poles. To\nthe best of our knowledge, it is the first framework to combine monocular depth\nmodeling, triangulated GPS-based geolocation, and real-time structural\nassessment for urban vegetation and infrastructure using consumer-grade video\ndata. Our approach complements conventional RS methods, such as LiDAR and image\nby offering a fast, real-time, and cost-effective solution for object-level\nmonitoring of vegetation risks and infrastructure exposure, making it\nespecially valuable for utility companies, and urban planners aiming for\nscalable and frequent assessments in dynamic urban environments.", "published": "2025-08-15 16:55:12", "link": "http://arxiv.org/abs/2508.11591v1", "categories": ["cs.CV", "cs.ET"], "primary_category": "cs.CV"}
{"title": "Causality Matters: How Temporal Information Emerges in Video Language Models", "abstract": "Video language models (VideoLMs) have made significant progress in multimodal\nunderstanding. However, temporal understanding, which involves identifying\nevent order, duration, and relationships across time, still remains a core\nchallenge. Prior works emphasize positional encodings (PEs) as a key mechanism\nfor encoding temporal structure. Surprisingly, we find that removing or\nmodifying PEs in video inputs yields minimal degradation in the performance of\ntemporal understanding. In contrast, reversing the frame sequence while\npreserving the original PEs causes a substantial drop. To explain this\nbehavior, we conduct substantial analysis experiments to trace how temporal\ninformation is integrated within the model. We uncover a causal information\npathway: temporal cues are progressively synthesized through inter-frame\nattention, aggregated in the final frame, and subsequently integrated into the\nquery tokens. This emergent mechanism shows that temporal reasoning emerges\nfrom inter-visual token interactions under the constraints of causal attention,\nwhich implicitly encodes temporal structure. Based on these insights, we\npropose two efficiency-oriented strategies: staged cross-modal attention and a\ntemporal exit mechanism for early token truncation. Experiments on two\nbenchmarks validate the effectiveness of both approaches. To the best of our\nknowledge, this is the first work to systematically investigate video temporal\nunderstanding in VideoLMs, offering insights for future model improvement.", "published": "2025-08-15 16:33:14", "link": "http://arxiv.org/abs/2508.11576v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "TrajSV: A Trajectory-based Model for Sports Video Representations and Applications", "abstract": "Sports analytics has received significant attention from both academia and\nindustry in recent years. Despite the growing interest and efforts in this\nfield, several issues remain unresolved, including (1) data unavailability, (2)\nlack of an effective trajectory-based framework, and (3) requirement for\nsufficient supervision labels. In this paper, we present TrajSV, a\ntrajectory-based framework that addresses various issues in existing studies.\nTrajSV comprises three components: data preprocessing, Clip Representation\nNetwork (CRNet), and Video Representation Network (VRNet). The data\npreprocessing module extracts player and ball trajectories from sports\nbroadcast videos. CRNet utilizes a trajectory-enhanced Transformer module to\nlearn clip representations based on these trajectories. Additionally, VRNet\nlearns video representations by aggregating clip representations and visual\nfeatures with an encoder-decoder architecture. Finally, a triple contrastive\nloss is introduced to optimize both video and clip representations in an\nunsupervised manner. The experiments are conducted on three broadcast video\ndatasets to verify the effectiveness of TrajSV for three types of sports (i.e.,\nsoccer, basketball, and volleyball) with three downstream applications (i.e.,\nsports video retrieval, action spotting, and video captioning). The results\ndemonstrate that TrajSV achieves state-of-the-art performance in sports video\nretrieval, showcasing a nearly 70% improvement. It outperforms baselines in\naction spotting, achieving state-of-the-art results in 9 out of 17 action\ncategories, and demonstrates a nearly 20% improvement in video captioning.\nAdditionally, we introduce a deployed system along with the three applications\nbased on TrajSV.", "published": "2025-08-15 16:23:36", "link": "http://arxiv.org/abs/2508.11569v1", "categories": ["cs.CV", "cs.IR"], "primary_category": "cs.CV"}
{"title": "Training-Free Anomaly Generation via Dual-Attention Enhancement in Diffusion Model", "abstract": "Industrial anomaly detection (AD) plays a significant role in manufacturing\nwhere a long-standing challenge is data scarcity. A growing body of works have\nemerged to address insufficient anomaly data via anomaly generation. However,\nthese anomaly generation methods suffer from lack of fidelity or need to be\ntrained with extra data. To this end, we propose a training-free anomaly\ngeneration framework dubbed AAG, which is based on Stable Diffusion (SD)'s\nstrong generation ability for effective anomaly image generation. Given a\nnormal image, mask and a simple text prompt, AAG can generate realistic and\nnatural anomalies in the specific regions and simultaneously keep contents in\nother regions unchanged. In particular, we propose Cross-Attention Enhancement\n(CAE) to re-engineer the cross-attention mechanism within Stable Diffusion\nbased on the given mask. CAE increases the similarity between visual tokens in\nspecific regions and text embeddings, which guides these generated visual\ntokens in accordance with the text description. Besides, generated anomalies\nneed to be more natural and plausible with object in given image. We propose\nSelf-Attention Enhancement (SAE) which improves similarity between each normal\nvisual token and anomaly visual tokens. SAE ensures that generated anomalies\nare coherent with original pattern. Extensive experiments on MVTec AD and VisA\ndatasets demonstrate effectiveness of AAG in anomaly generation and its\nutility. Furthermore, anomaly images generated by AAG can bolster performance\nof various downstream anomaly inspection tasks.", "published": "2025-08-15 15:52:02", "link": "http://arxiv.org/abs/2508.11550v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Reinforcing Video Reasoning Segmentation to Think Before It Segments", "abstract": "Video reasoning segmentation (VRS) endeavors to delineate referred objects in\nvideos guided by implicit instructions that encapsulate human intent and\ntemporal logic. Previous approaches leverage large vision language models\n(LVLMs) to encode object semantics into <SEG> tokens for mask prediction.\nHowever, this paradigm suffers from limited interpretability during inference\nand suboptimal performance due to inadequate spatiotemporal reasoning. Drawing\ninspiration from seminal breakthroughs in reinforcement learning, we introduce\nVeason-R1, a specialized LVLM for VRS that emphasizes structured reasoning in\nsegmentation. Veason-R1 is trained through Group Relative Policy Optimization\n(GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, we\ncurate high-quality CoT training data to instill structured reasoning\ntrajectories, bridging video-level semantics and frame-level spatial grounding,\nyielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPO\nfine-tuning encourages efficient exploration of the reasoning space by\noptimizing reasoning chains. To this end, we incorporate a holistic reward\nmechanism that synergistically enhances spatial alignment and temporal\nconsistency, bolstering keyframe localization and fine-grained grounding.\nComprehensive empirical evaluations demonstrate that Veason-R1 achieves\nstate-of-the-art performance on multiple benchmarks, surpassing prior art by\nsignificant margins (e.g., +1.3 J &F in ReVOS and +10.0 J &F in ReasonVOS),\nwhile exhibiting robustness to hallucinations (+8.8 R). Our code and model\nweights will be available at Veason-R1.", "published": "2025-08-15 15:34:56", "link": "http://arxiv.org/abs/2508.11538v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "An Efficient Medical Image Classification Method Based on a Lightweight Improved ConvNeXt-Tiny Architecture", "abstract": "Intelligent analysis of medical imaging plays a crucial role in assisting\nclinical diagnosis. However, achieving efficient and high-accuracy image\nclassification in resource-constrained computational environments remains\nchallenging. This study proposes a medical image classification method based on\nan improved ConvNeXt-Tiny architecture. Through structural optimization and\nloss function design, the proposed method enhances feature extraction\ncapability and classification performance while reducing computational\ncomplexity. Specifically, the method introduces a dual global pooling (Global\nAverage Pooling and Global Max Pooling) feature fusion strategy into the\nConvNeXt-Tiny backbone to simultaneously preserve global statistical features\nand salient response information. A lightweight channel attention module,\ntermed Squeeze-and-Excitation Vector (SEVector), is designed to improve the\nadaptive allocation of channel weights while minimizing parameter overhead.\nAdditionally, a Feature Smoothing Loss is incorporated into the loss function\nto enhance intra-class feature consistency and suppress intra-class variance.\nUnder CPU-only conditions (8 threads), the method achieves a maximum\nclassification accuracy of 89.10% on the test set within 10 training epochs,\nexhibiting a stable convergence trend in loss values. Experimental results\ndemonstrate that the proposed method effectively improves medical image\nclassification performance in resource-limited settings, providing a feasible\nand efficient solution for the deployment and promotion of medical imaging\nanalysis models.", "published": "2025-08-15 15:20:25", "link": "http://arxiv.org/abs/2508.11532v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Multi-State Tracker: Enhancing Efficient Object Tracking via Multi-State Specialization and Interaction", "abstract": "Efficient trackers achieve faster runtime by reducing computational\ncomplexity and model parameters. However, this efficiency often compromises the\nexpense of weakened feature representation capacity, thus limiting their\nability to accurately capture target states using single-layer features. To\novercome this limitation, we propose Multi-State Tracker (MST), which utilizes\nhighly lightweight state-specific enhancement (SSE) to perform specialized\nenhancement on multi-state features produced by multi-state generation (MSG)\nand aggregates them in an interactive and adaptive manner using cross-state\ninteraction (CSI). This design greatly enhances feature representation while\nincurring minimal computational overhead, leading to improved tracking\nrobustness in complex environments. Specifically, the MSG generates multiple\nstate representations at multiple stages during feature extraction, while SSE\nrefines them to highlight target-specific features. The CSI module facilitates\ninformation exchange between these states and ensures the integration of\ncomplementary features. Notably, the introduced SSE and CSI modules adopt a\nhighly lightweight hidden state adaptation-based state space duality (HSA-SSD)\ndesign, incurring only 0.1 GFLOPs in computation and 0.66 M in parameters.\nExperimental results demonstrate that MST outperforms all previous efficient\ntrackers across multiple datasets, significantly improving tracking accuracy\nand robustness. In particular, it shows excellent runtime performance, with an\nAO score improvement of 4.5\\% over the previous SOTA efficient tracker HCAT on\nthe GOT-10K dataset. The code is available at https://github.com/wsumel/MST.", "published": "2025-08-15 15:19:39", "link": "http://arxiv.org/abs/2508.11531v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "A Real-time Concrete Crack Detection and Segmentation Model Based on YOLOv11", "abstract": "Accelerated aging of transportation infrastructure in the rapidly developing\nYangtze River Delta region necessitates efficient concrete crack detection, as\ncrack deterioration critically compromises structural integrity and regional\neconomic growth. To overcome the limitations of inefficient manual inspection\nand the suboptimal performance of existing deep learning models, particularly\nfor small-target crack detection within complex backgrounds, this paper\nproposes YOLOv11-KW-TA-FP, a multi-task concrete crack detection and\nsegmentation model based on the YOLOv11n architecture. The proposed model\nintegrates a three-stage optimization framework: (1) Embedding dynamic\nKernelWarehouse convolution (KWConv) within the backbone network to enhance\nfeature representation through a dynamic kernel sharing mechanism; (2)\nIncorporating a triple attention mechanism (TA) into the feature pyramid to\nstrengthen channel-spatial interaction modeling; and (3) Designing an FP-IoU\nloss function to facilitate adaptive bounding box regression penalization.\nExperimental validation demonstrates that the enhanced model achieves\nsignificant performance improvements over the baseline, attaining 91.3%\nprecision, 76.6% recall, and 86.4% mAP@50. Ablation studies confirm the\nsynergistic efficacy of the proposed modules. Furthermore, robustness tests\nindicate stable performance under conditions of data scarcity and noise\ninterference. This research delivers an efficient computer vision solution for\nautomated infrastructure inspection, exhibiting substantial practical\nengineering value.", "published": "2025-08-15 14:57:00", "link": "http://arxiv.org/abs/2508.11517v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Semi-Supervised Learning with Online Knowledge Distillation for Skin Lesion Classification", "abstract": "Deep Learning has emerged as a promising approach for skin lesion analysis.\nHowever, existing methods mostly rely on fully supervised learning, requiring\nextensive labeled data, which is challenging and costly to obtain. To alleviate\nthis annotation burden, this study introduces a novel semi-supervised deep\nlearning approach that integrates ensemble learning with online knowledge\ndistillation for enhanced skin lesion classification. Our methodology involves\ntraining an ensemble of convolutional neural network models, using online\nknowledge distillation to transfer insights from the ensemble to its members.\nThis process aims to enhance the performance of each model within the ensemble,\nthereby elevating the overall performance of the ensemble itself.\nPost-training, any individual model within the ensemble can be deployed at test\ntime, as each member is trained to deliver comparable performance to the\nensemble. This is particularly beneficial in resource-constrained environments.\nExperimental results demonstrate that the knowledge-distilled individual model\nperforms better than independently trained models. Our approach demonstrates\nsuperior performance on both the \\emph{International Skin Imaging\nCollaboration} 2018 and 2019 public benchmark datasets, surpassing current\nstate-of-the-art results. By leveraging ensemble learning and online knowledge\ndistillation, our method reduces the need for extensive labeled data while\nproviding a more resource-efficient solution for skin lesion classification in\nreal-world scenarios.", "published": "2025-08-15 14:40:48", "link": "http://arxiv.org/abs/2508.11511v1", "categories": ["eess.IV", "cs.CV", "cs.LG"], "primary_category": "eess.IV"}
{"title": "AIM: Amending Inherent Interpretability via Self-Supervised Masking", "abstract": "It has been observed that deep neural networks (DNNs) often use both genuine\nas well as spurious features. In this work, we propose \"Amending Inherent\nInterpretability via Self-Supervised Masking\" (AIM), a simple yet interestingly\neffective method that promotes the network's utilization of genuine features\nover spurious alternatives without requiring additional annotations. In\nparticular, AIM uses features at multiple encoding stages to guide a\nself-supervised, sample-specific feature-masking process. As a result, AIM\nenables the training of well-performing and inherently interpretable models\nthat faithfully summarize the decision process. We validate AIM across a\ndiverse range of challenging datasets that test both out-of-distribution\ngeneralization and fine-grained visual understanding. These include\ngeneral-purpose classification benchmarks such as ImageNet100, HardImageNet,\nand ImageWoof, as well as fine-grained classification datasets such as\nWaterbirds, TravelingBirds, and CUB-200. AIM demonstrates significant dual\nbenefits: interpretability improvements, as measured by the Energy Pointing\nGame (EPG) score, and accuracy gains over strong baselines. These consistent\ngains across domains and architectures provide compelling evidence that AIM\npromotes the use of genuine and meaningful features that directly contribute to\nimproved generalization and human-aligned interpretability.", "published": "2025-08-15 14:29:59", "link": "http://arxiv.org/abs/2508.11502v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Hierarchical Graph Feature Enhancement with Adaptive Frequency Modulation for Visual Recognition", "abstract": "Convolutional neural networks (CNNs) have\n  demonstrated strong performance in visual recognition tasks,\n  but their inherent reliance on regular grid structures limits\n  their capacity to model complex topological relationships and\n  non-local semantics within images. To address this limita tion, we propose\nthe hierarchical graph feature enhancement\n  (HGFE), a novel framework that integrates graph-based rea soning into CNNs to\nenhance both structural awareness and\n  feature representation. HGFE builds two complementary levels\n  of graph structures: intra-window graph convolution to cap ture local spatial\ndependencies and inter-window supernode\n  interactions to model global semantic relationships. Moreover,\n  we introduce an adaptive frequency modulation module that\n  dynamically balances low-frequency and high-frequency signal\n  propagation, preserving critical edge and texture information\n  while mitigating over-smoothing. The proposed HGFE module\n  is lightweight, end-to-end trainable, and can be seamlessly\n  integrated into standard CNN backbone networks. Extensive\n  experiments on CIFAR-100 (classification), PASCAL VOC,\n  and VisDrone (detection), as well as CrackSeg and CarParts\n  (segmentation), validated the effectiveness of the HGFE in\n  improving structural representation and enhancing overall\n  recognition performance.", "published": "2025-08-15 14:19:50", "link": "http://arxiv.org/abs/2508.11497v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Relative Position Matters: Trajectory Prediction and Planning with Polar Representation", "abstract": "Trajectory prediction and planning in autonomous driving are highly\nchallenging due to the complexity of predicting surrounding agents' movements\nand planning the ego agent's actions in dynamic environments. Existing methods\nencode map and agent positions and decode future trajectories in Cartesian\ncoordinates. However, modeling the relationships between the ego vehicle and\nsurrounding traffic elements in Cartesian space can be suboptimal, as it does\nnot naturally capture the varying influence of different elements based on\ntheir relative distances and directions. To address this limitation, we adopt\nthe Polar coordinate system, where positions are represented by radius and\nangle. This representation provides a more intuitive and effective way to model\nspatial changes and relative relationships, especially in terms of distance and\ndirectional influence. Based on this insight, we propose Polaris, a novel\nmethod that operates entirely in Polar coordinates, distinguishing itself from\nconventional Cartesian-based approaches. By leveraging the Polar\nrepresentation, this method explicitly models distance and direction variations\nand captures relative relationships through dedicated encoding and refinement\nmodules, enabling more structured and spatially aware trajectory prediction and\nplanning. Extensive experiments on the challenging prediction (Argoverse 2) and\nplanning benchmarks (nuPlan) demonstrate that Polaris achieves state-of-the-art\nperformance.", "published": "2025-08-15 14:15:11", "link": "http://arxiv.org/abs/2508.11492v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Perception in Plan: Coupled Perception and Planning for End-to-End Autonomous Driving", "abstract": "End-to-end autonomous driving has achieved remarkable advancements in recent\nyears. Existing methods primarily follow a perception-planning paradigm, where\nperception and planning are executed sequentially within a fully differentiable\nframework for planning-oriented optimization. We further advance this paradigm\nthrough a perception-in-plan framework design, which integrates perception into\nthe planning process. This design facilitates targeted perception guided by\nevolving planning objectives over time, ultimately enhancing planning\nperformance. Building on this insight, we introduce VeteranAD, a coupled\nperception and planning framework for end-to-end autonomous driving. By\nincorporating multi-mode anchored trajectories as planning priors, the\nperception module is specifically designed to gather traffic elements along\nthese trajectories, enabling comprehensive and targeted perception. Planning\ntrajectories are then generated based on both the perception results and the\nplanning priors. To make perception fully serve planning, we adopt an\nautoregressive strategy that progressively predicts future trajectories while\nfocusing on relevant regions for targeted perception at each step. With this\nsimple yet effective design, VeteranAD fully unleashes the potential of\nplanning-oriented end-to-end methods, leading to more accurate and reliable\ndriving behavior. Extensive experiments on the NAVSIM and Bench2Drive datasets\ndemonstrate that our VeteranAD achieves state-of-the-art performance.", "published": "2025-08-15 14:05:57", "link": "http://arxiv.org/abs/2508.11488v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Automated Building Heritage Assessment Using Street-Level Imagery", "abstract": "Detailed data is required to quantify energy conservation measures in\nbuildings, such as envelop retrofits, without compromising cultural heritage.\nNovel artificial intelligence tools may improve efficiency in identifying\nheritage values in buildings compared to costly and time-consuming traditional\ninventories. In this study, the large language model GPT was used to detect\nvarious aspects of cultural heritage value in fa\\c{c}ade images. Using this\ndata and building register data as features, machine learning models were\ntrained to classify multi-family and non-residential buildings in Stockholm,\nSweden. Validation against an expert-created inventory shows a macro F1-score\nof 0.71 using a combination of register data and features retrieved from GPT,\nand a score of 0.60 using only GPT-derived data. The presented methodology can\ncontribute to a higher-quality database and thus support careful energy\nefficiency measures and integrated consideration of heritage value in\nlarge-scale energetic refurbishment scenarios.", "published": "2025-08-15 13:59:24", "link": "http://arxiv.org/abs/2508.11486v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models", "abstract": "Despite significant advances in video synthesis, research into multi-shot\nvideo generation remains in its infancy. Even with scaled-up models and massive\ndatasets, the shot transition capabilities remain rudimentary and unstable,\nlargely confining generated videos to single-shot sequences. In this work, we\nintroduce CineTrans, a novel framework for generating coherent multi-shot\nvideos with cinematic, film-style transitions. To facilitate insights into the\nfilm editing style, we construct a multi-shot video-text dataset Cine250K with\ndetailed shot annotations. Furthermore, our analysis of existing video\ndiffusion models uncovers a correspondence between attention maps in the\ndiffusion model and shot boundaries, which we leverage to design a mask-based\ncontrol mechanism that enables transitions at arbitrary positions and transfers\neffectively in a training-free setting. After fine-tuning on our dataset with\nthe mask mechanism, CineTrans produces cinematic multi-shot sequences while\nadhering to the film editing style, avoiding unstable transitions or naive\nconcatenations. Finally, we propose specialized evaluation metrics for\ntransition control, temporal consistency and overall quality, and demonstrate\nthrough extensive experiments that CineTrans significantly outperforms existing\nbaselines across all criteria.", "published": "2025-08-15 13:58:22", "link": "http://arxiv.org/abs/2508.11484v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "OpenConstruction: A Systematic Synthesis of Open Visual Datasets for Data-Centric Artificial Intelligence in Construction Monitoring", "abstract": "The construction industry increasingly relies on visual data to support\nArtificial Intelligence (AI) and Machine Learning (ML) applications for site\nmonitoring. High-quality, domain-specific datasets, comprising images, videos,\nand point clouds, capture site geometry and spatiotemporal dynamics, including\nthe location and interaction of objects, workers, and materials. However,\ndespite growing interest in leveraging visual datasets, existing resources vary\nwidely in sizes, data modalities, annotation quality, and representativeness of\nreal-world construction conditions. A systematic review to categorize their\ndata characteristics and application contexts is still lacking, limiting the\ncommunity's ability to fully understand the dataset landscape, identify\ncritical gaps, and guide future directions toward more effective, reliable, and\nscalable AI applications in construction. To address this gap, this study\nconducts an extensive search of academic databases and open-data platforms,\nyielding 51 publicly available visual datasets that span the 2005-2024 period.\nThese datasets are categorized using a structured data schema covering (i) data\nfundamentals (e.g., size and license), (ii) data modalities (e.g., RGB and\npoint cloud), (iii) annotation frameworks (e.g., bounding boxes), and (iv)\ndownstream application domains (e.g., progress tracking). This study\nsynthesizes these findings into an open-source catalog, OpenConstruction,\nsupporting data-driven method development. Furthermore, the study discusses\nseveral critical limitations in the existing construction dataset landscape and\npresents a roadmap for future data infrastructure anchored in the Findability,\nAccessibility, Interoperability, and Reusability (FAIR) principles. By\nreviewing the current landscape and outlining strategic priorities, this study\nsupports the advancement of data-centric solutions in the construction sector.", "published": "2025-08-15 13:56:21", "link": "http://arxiv.org/abs/2508.11482v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "TACR-YOLO: A Real-time Detection Framework for Abnormal Human Behaviors Enhanced with Coordinate and Task-Aware Representations", "abstract": "Abnormal Human Behavior Detection (AHBD) under special scenarios is becoming\nincreasingly crucial. While YOLO-based detection methods excel in real-time\ntasks, they remain hindered by challenges including small objects, task\nconflicts, and multi-scale fusion in AHBD. To tackle them, we propose\nTACR-YOLO, a new real-time framework for AHBD. We introduce a Coordinate\nAttention Module to enhance small object detection, a Task-Aware Attention\nModule to deal with classification-regression conflicts, and a Strengthen Neck\nNetwork for refined multi-scale fusion, respectively. In addition, we optimize\nAnchor Box sizes using K-means clustering and deploy DIoU-Loss to improve\nbounding box regression. The Personnel Anomalous Behavior Detection (PABD)\ndataset, which includes 8,529 samples across four behavior categories, is also\npresented. Extensive experimental results indicate that TACR-YOLO achieves\n91.92% mAP on PABD, with competitive speed and robustness. Ablation studies\nhighlight the contribution of each improvement. This work provides new insights\nfor abnormal behavior detection under special scenarios, advancing its\nprogress.", "published": "2025-08-15 13:45:21", "link": "http://arxiv.org/abs/2508.11478v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SPG: Style-Prompting Guidance for Style-Specific Content Creation", "abstract": "Although recent text-to-image (T2I) diffusion models excel at aligning\ngenerated images with textual prompts, controlling the visual style of the\noutput remains a challenging task. In this work, we propose Style-Prompting\nGuidance (SPG), a novel sampling strategy for style-specific image generation.\nSPG constructs a style noise vector and leverages its directional deviation\nfrom unconditional noise to guide the diffusion process toward the target style\ndistribution. By integrating SPG with Classifier-Free Guidance (CFG), our\nmethod achieves both semantic fidelity and style consistency. SPG is simple,\nrobust, and compatible with controllable frameworks like ControlNet and\nIPAdapter, making it practical and widely applicable. Extensive experiments\ndemonstrate the effectiveness and generality of our approach compared to\nstate-of-the-art methods. Code is available at\nhttps://github.com/Rumbling281441/SPG.", "published": "2025-08-15 13:44:56", "link": "http://arxiv.org/abs/2508.11476v1", "categories": ["cs.GR", "cs.CV"], "primary_category": "cs.GR"}
{"title": "CoFi: A Fast Coarse-to-Fine Few-Shot Pipeline for Glomerular Basement Membrane Segmentation", "abstract": "Accurate segmentation of the glomerular basement membrane (GBM) in electron\nmicroscopy (EM) images is fundamental for quantifying membrane thickness and\nsupporting the diagnosis of various kidney diseases. While supervised deep\nlearning approaches achieve high segmentation accuracy, their reliance on\nextensive pixel-level annotation renders them impractical for clinical\nworkflows. Few-shot learning can reduce this annotation burden but often\nstruggles to capture the fine structural details necessary for GBM analysis. In\nthis study, we introduce CoFi, a fast and efficient coarse-to-fine few-shot\nsegmentation pipeline designed for GBM delineation in EM images. CoFi first\ntrains a lightweight neural network using only three annotated images to\nproduce an initial coarse segmentation mask. This mask is then automatically\nprocessed to generate high-quality point prompts with morphology-aware pruning,\nwhich are subsequently used to guide SAM in refining the segmentation. The\nproposed method achieved exceptional GBM segmentation performance, with a Dice\ncoefficient of 74.54% and an inference speed of 1.9 FPS. We demonstrate that\nCoFi not only alleviates the annotation and computational burdens associated\nwith conventional methods, but also achieves accurate and reliable segmentation\nresults. The pipeline's speed and annotation efficiency make it well-suited for\nresearch and hold strong potential for clinical applications in renal\npathology. The pipeline is publicly available at:\nhttps://github.com/ddrrnn123/CoFi.", "published": "2025-08-15 13:34:24", "link": "http://arxiv.org/abs/2508.11469v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Data-Driven Deepfake Image Detection Method -- The 2024 Global Deepfake Image Detection Challenge", "abstract": "With the rapid development of technology in the field of AI, deepfake\ntechnology has emerged as a double-edged sword. It has not only created a large\namount of AI-generated content but also posed unprecedented challenges to\ndigital security. The task of the competition is to determine whether a face\nimage is a Deepfake image and output its probability score of being a Deepfake\nimage. In the image track competition, our approach is based on the Swin\nTransformer V2-B classification network. And online data augmentation and\noffline sample generation methods are employed to enrich the diversity of\ntraining samples and increase the generalization ability of the model. Finally,\nwe got the award of excellence in Deepfake image detection.", "published": "2025-08-15 13:24:47", "link": "http://arxiv.org/abs/2508.11464v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Subcortical Masks Generation in CT Images via Ensemble-Based Cross-Domain Label Transfer", "abstract": "Subcortical segmentation in neuroimages plays an important role in\nunderstanding brain anatomy and facilitating computer-aided diagnosis of\ntraumatic brain injuries and neurodegenerative disorders. However, training\naccurate automatic models requires large amounts of labelled data. Despite the\navailability of publicly available subcortical segmentation datasets for\nMagnetic Resonance Imaging (MRI), a significant gap exists for Computed\nTomography (CT). This paper proposes an automatic ensemble framework to\ngenerate high-quality subcortical segmentation labels for CT scans by\nleveraging existing MRI-based models. We introduce a robust ensembling pipeline\nto integrate them and apply it to unannotated paired MRI-CT data, resulting in\na comprehensive CT subcortical segmentation dataset. Extensive experiments on\nmultiple public datasets demonstrate the superior performance of our proposed\nframework. Furthermore, using our generated CT dataset, we train segmentation\nmodels that achieve improved performance on related segmentation tasks. To\nfacilitate future research, we make our source code, generated dataset, and\ntrained models publicly available at\nhttps://github.com/SCSE-Biomedical-Computing-Group/CT-Subcortical-Segmentation,\nmarking the first open-source release for CT subcortical segmentation to the\nbest of our knowledge.", "published": "2025-08-15 12:57:35", "link": "http://arxiv.org/abs/2508.11450v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation", "abstract": "Multimodal Large Language Models (MLLMs) with unified architectures excel\nacross a wide range of vision-language tasks, yet aligning them with\npersonalized image generation remains a significant challenge. Existing methods\nfor MLLMs are frequently subject-specific, demanding a data-intensive\nfine-tuning process for every new subject, which limits their scalability. In\nthis paper, we introduce MM-R1, a framework that integrates a cross-modal\nChain-of-Thought (X-CoT) reasoning strategy to unlock the inherent potential of\nunified MLLMs for personalized image generation. Specifically, we structure\npersonalization as an integrated visual reasoning and generation process: (1)\ngrounding subject concepts by interpreting and understanding user-provided\nimages and contextual cues, and (2) generating personalized images conditioned\non both the extracted subject representations and user prompts. To further\nenhance the reasoning capability, we adopt Grouped Reward Proximal Policy\nOptimization (GRPO) to explicitly align the generation. Experiments demonstrate\nthat MM-R1 unleashes the personalization capability of unified MLLMs to\ngenerate images with high subject fidelity and strong text alignment in a\nzero-shot manner.", "published": "2025-08-15 12:20:27", "link": "http://arxiv.org/abs/2508.11433v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Robust Convolution Neural ODEs via Contractivity-promoting regularization", "abstract": "Neural networks can be fragile to input noise and adversarial attacks.\n  In this work, we consider Convolutional Neural Ordinary Differential\nEquations (NODEs), a family of continuous-depth neural networks represented by\ndynamical systems, and propose to use contraction theory to improve their\nrobustness.\n  For a contractive dynamical system two trajectories starting from different\ninitial conditions converge to each other exponentially fast.\n  Contractive Convolutional NODEs can enjoy increased robustness as slight\nperturbations of the features do not cause a significant change in the output.\n  Contractivity can be induced during training by using a regularization term\ninvolving the Jacobian of the system dynamics.\n  To reduce the computational burden, we show that it can also be promoted\nusing carefully selected weight regularization terms for a class of NODEs with\nslope-restricted activation functions.\n  The performance of the proposed regularizers is illustrated through benchmark\nimage classification tasks on MNIST and FashionMNIST datasets, where images are\ncorrupted by different kinds of noise and attacks.", "published": "2025-08-15 12:18:44", "link": "http://arxiv.org/abs/2508.11432v1", "categories": ["cs.LG", "cs.CV", "cs.SY", "eess.SY"], "primary_category": "cs.LG"}
{"title": "Remove360: Benchmarking Residuals After Object Removal in 3D Gaussian Splatting", "abstract": "Understanding what semantic information persists after object removal is\ncritical for privacy-preserving 3D reconstruction and editable scene\nrepresentations. In this work, we introduce a novel benchmark and evaluation\nframework to measure semantic residuals, the unintended semantic traces left\nbehind, after object removal in 3D Gaussian Splatting. We conduct experiments\nacross a diverse set of indoor and outdoor scenes, showing that current methods\ncan preserve semantic information despite the absence of visual geometry. We\nalso release Remove360, a dataset of pre/post-removal RGB images and\nobject-level masks captured in real-world environments. While prior datasets\nhave focused on isolated object instances, Remove360 covers a broader and more\ncomplex range of indoor and outdoor scenes, enabling evaluation of object\nremoval in the context of full-scene representations. Given ground truth images\nof a scene before and after object removal, we assess whether we can truly\neliminate semantic presence, and if downstream models can still infer what was\nremoved. Our findings reveal critical limitations in current 3D object removal\ntechniques and underscore the need for more robust solutions capable of\nhandling real-world complexity. The evaluation framework is available at\ngithub.com/spatial-intelligence-ai/Remove360.git. Data are available at\nhuggingface.co/datasets/simkoc/Remove360.", "published": "2025-08-15 12:15:06", "link": "http://arxiv.org/abs/2508.11431v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving", "abstract": "Autonomous driving requires rich contextual comprehension and precise\npredictive reasoning to navigate dynamic and complex environments safely.\nVision-Language Models (VLMs) and Driving World Models (DWMs) have\nindependently emerged as powerful recipes addressing different aspects of this\nchallenge. VLMs provide interpretability and robust action prediction through\ntheir ability to understand multi-modal context, while DWMs excel in generating\ndetailed and plausible future driving scenarios essential for proactive\nplanning. Integrating VLMs with DWMs is an intuitive, promising, yet\nunderstudied strategy to exploit the complementary strengths of accurate\nbehavioral prediction and realistic scene generation. Nevertheless, this\nintegration presents notable challenges, particularly in effectively connecting\naction-level decisions with high-fidelity pixel-level predictions and\nmaintaining computational efficiency. In this paper, we propose ImagiDrive, a\nnovel end-to-end autonomous driving framework that integrates a VLM-based\ndriving agent with a DWM-based scene imaginer to form a unified\nimagination-and-planning loop. The driving agent predicts initial driving\ntrajectories based on multi-modal inputs, guiding the scene imaginer to\ngenerate corresponding future scenarios. These imagined scenarios are\nsubsequently utilized to iteratively refine the driving agent's planning\ndecisions. To address efficiency and predictive accuracy challenges inherent in\nthis integration, we introduce an early stopping mechanism and a trajectory\nselection strategy. Extensive experimental validation on the nuScenes and\nNAVSIM datasets demonstrates the robustness and superiority of ImagiDrive over\nprevious alternatives under both open-loop and closed-loop conditions.", "published": "2025-08-15 12:06:55", "link": "http://arxiv.org/abs/2508.11428v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Training-free Dimensionality Reduction via Feature Truncation: Enhancing Efficiency in Privacy-preserving Multi-Biometric Systems", "abstract": "Biometric recognition is widely used, making the privacy and security of\nextracted templates a critical concern. Biometric Template Protection schemes,\nespecially those utilizing Homomorphic Encryption, introduce significant\ncomputational challenges due to increased workload. Recent advances in deep\nneural networks have enabled state-of-the-art feature extraction for face,\nfingerprint, and iris modalities. The ubiquity and affordability of biometric\nsensors further facilitate multi-modal fusion, which can enhance security by\ncombining features from different modalities. This work investigates the\nbiometric performance of reduced multi-biometric template sizes. Experiments\nare conducted on an in-house virtual multi-biometric database, derived from\nDNN-extracted features for face, fingerprint, and iris, using the FRGC, MCYT,\nand CASIA databases. The evaluated approaches are (i) explainable and\nstraightforward to implement under encryption, (ii) training-free, and (iii)\ncapable of generalization. Dimensionality reduction of feature vectors leads to\nfewer operations in the Homomorphic Encryption (HE) domain, enabling more\nefficient encrypted processing while maintaining biometric accuracy and\nsecurity at a level equivalent to or exceeding single-biometric recognition.\nOur results demonstrate that, by fusing feature vectors from multiple\nmodalities, template size can be reduced by 67 % with no loss in Equal Error\nRate (EER) compared to the best-performing single modality.", "published": "2025-08-15 11:49:19", "link": "http://arxiv.org/abs/2508.11419v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SelfAdapt: Unsupervised Domain Adaptation of Cell Segmentation Models", "abstract": "Deep neural networks have become the go-to method for biomedical instance\nsegmentation. Generalist models like Cellpose demonstrate state-of-the-art\nperformance across diverse cellular data, though their effectiveness often\ndegrades on domains that differ from their training data. While supervised\nfine-tuning can address this limitation, it requires annotated data that may\nnot be readily available. We propose SelfAdapt, a method that enables the\nadaptation of pre-trained cell segmentation models without the need for labels.\nOur approach builds upon student-teacher augmentation consistency training,\nintroducing L2-SP regularization and label-free stopping criteria. We evaluate\nour method on the LiveCell and TissueNet datasets, demonstrating relative\nimprovements in AP0.5 of up to 29.64% over baseline Cellpose. Additionally, we\nshow that our unsupervised adaptation can further improve models that were\npreviously fine-tuned with supervision. We release SelfAdapt as an easy-to-use\nextension of the Cellpose framework. The code for our method is publicly\navailable at https: //github.com/Kainmueller-Lab/self_adapt.", "published": "2025-08-15 11:31:48", "link": "http://arxiv.org/abs/2508.11411v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator", "abstract": "Atmospheric turbulence severely degrades video quality by introducing\ndistortions such as geometric warping, blur, and temporal flickering, posing\nsignificant challenges to both visual clarity and temporal consistency. Current\nstate-of-the-art methods are based on transformer and 3D architectures and\nrequire multi-frame input, but their large computational cost and memory usage\nlimit real-time deployment, especially in resource-constrained scenarios. In\nthis work, we propose RMFAT: Recurrent Multi-scale Feature Atmospheric\nTurbulence Mitigator, designed for efficient and temporally consistent video\nrestoration under AT conditions. RMFAT adopts a lightweight recurrent framework\nthat restores each frame using only two inputs at a time, significantly\nreducing temporal window size and computational burden. It further integrates\nmulti-scale feature encoding and decoding with temporal warping modules at both\nencoder and decoder stages to enhance spatial detail and temporal coherence.\nExtensive experiments on synthetic and real-world atmospheric turbulence\ndatasets demonstrate that RMFAT not only outperforms existing methods in terms\nof clarity restoration (with nearly a 9\\% improvement in SSIM) but also\nachieves significantly improved inference speed (more than a fourfold reduction\nin runtime), making it particularly suitable for real-time atmospheric\nturbulence suppression tasks.", "published": "2025-08-15 11:20:18", "link": "http://arxiv.org/abs/2508.11409v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "LKFMixer: Exploring Large Kernel Feature For Efficient Image Super-Resolution", "abstract": "The success of self-attention (SA) in Transformer demonstrates the importance\nof non-local information to image super-resolution (SR), but the huge computing\npower required makes it difficult to implement lightweight models. To solve\nthis problem, we propose a pure convolutional neural network (CNN) model,\nLKFMixer, which utilizes large convolutional kernel to simulate the ability of\nself-attention to capture non-local features. Specifically, we increase the\nkernel size to 31 to obtain the larger receptive field as possible, and reduce\nthe parameters and computations by coordinate decomposition. Meanwhile, a\nspatial feature modulation block (SFMB) is designed to enhance the focus of\nfeature information on both spatial and channel dimension. In addition, by\nintroducing feature selection block (FSB), the model can adaptively adjust the\nweights between local features and non-local features. Extensive experiments\nshow that the proposed LKFMixer family outperform other state-of-the-art (SOTA)\nmethods in terms of SR performance and reconstruction quality. In particular,\ncompared with SwinIR-light on Manga109 dataset, LKFMixer-L achieves 0.6dB PSNR\nimprovement at $\\times$4 scale, while the inference speed is $\\times$5 times\nfaster. The code is available at https://github.com/Supereeeee/LKFMixer.", "published": "2025-08-15 10:50:38", "link": "http://arxiv.org/abs/2508.11391v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Unified Knowledge Distillation Framework: Fine-Grained Alignment and Geometric Relationship Preservation for Deep Face Recognition", "abstract": "Knowledge Distillation is crucial for optimizing face recognition models for\ndeployment in computationally limited settings, such as edge devices.\nTraditional KD methods, such as Raw L2 Feature Distillation or Feature\nConsistency loss, often fail to capture both fine-grained instance-level\ndetails and complex relational structures, leading to suboptimal performance.\nWe propose a unified approach that integrates two novel loss functions,\nInstance-Level Embedding Distillation and Relation-Based Pairwise Similarity\nDistillation. Instance-Level Embedding Distillation focuses on aligning\nindividual feature embeddings by leveraging a dynamic hard mining strategy,\nthereby enhancing learning from challenging examples. Relation-Based Pairwise\nSimilarity Distillation captures relational information through pairwise\nsimilarity relationships, employing a memory bank mechanism and a sample mining\nstrategy. This unified framework ensures both effective instance-level\nalignment and preservation of geometric relationships between samples, leading\nto a more comprehensive distillation process. Our unified framework outperforms\nstate-of-the-art distillation methods across multiple benchmark face\nrecognition datasets, as demonstrated by extensive experimental evaluations.\nInterestingly, when using strong teacher networks compared to the student, our\nunified KD enables the student to even surpass the teacher's accuracy.", "published": "2025-08-15 10:20:29", "link": "http://arxiv.org/abs/2508.11376v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "AnatoMaskGAN: GNN-Driven Slice Feature Fusion and Noise Augmentation for Medical Semantic Image Synthesis", "abstract": "Medical semantic-mask synthesis boosts data augmentation and analysis, yet\nmost GAN-based approaches still produce one-to-one images and lack spatial\nconsistency in complex scans. To address this, we propose AnatoMaskGAN, a novel\nsynthesis framework that embeds slice-related spatial features to precisely\naggregate inter-slice contextual dependencies, introduces diverse\nimage-augmentation strategies, and optimizes deep feature learning to improve\nperformance on complex medical images. Specifically, we design a GNN-based\nstrongly correlated slice-feature fusion module to model spatial relationships\nbetween slices and integrate contextual information from neighboring slices,\nthereby capturing anatomical details more comprehensively; we introduce a\nthree-dimensional spatial noise-injection strategy that weights and fuses\nspatial features with noise to enhance modeling of structural diversity; and we\nincorporate a grayscale-texture classifier to optimize grayscale distribution\nand texture representation during generation. Extensive experiments on the\npublic L2R-OASIS and L2R-Abdomen CT datasets show that AnatoMaskGAN raises PSNR\non L2R-OASIS to 26.50 dB (0.43 dB higher than the current state of the art) and\nachieves an SSIM of 0.8602 on L2R-Abdomen CT--a 0.48 percentage-point gain over\nthe best model, demonstrating its superiority in reconstruction accuracy and\nperceptual quality. Ablation studies that successively remove the slice-feature\nfusion module, spatial 3D noise-injection strategy, and grayscale-texture\nclassifier reveal that each component contributes significantly to PSNR, SSIM,\nand LPIPS, further confirming the independent value of each core design in\nenhancing reconstruction accuracy and perceptual quality.", "published": "2025-08-15 10:19:38", "link": "http://arxiv.org/abs/2508.11375v1", "categories": ["eess.IV", "cs.CV", "I.4.9"], "primary_category": "eess.IV"}
{"title": "HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model", "abstract": "Understanding and recognizing human-object interaction (HOI) is a pivotal\napplication in AR/VR and robotics. Recent open-vocabulary HOI detection\napproaches depend exclusively on large language models for richer textual\nprompts, neglecting their inherent 3D spatial understanding capabilities. To\naddress this shortcoming, we introduce HOID-R1, the first HOI detection\nframework that integrates chain-of-thought (CoT) guided supervised fine-tuning\n(SFT) with group relative policy optimization (GRPO) within a reinforcement\nlearning (RL) paradigm. Specifically, we initially apply SFT to imbue the model\nwith essential reasoning capabilities, forcing the model to articulate its\nthought process in the output. Subsequently, we integrate GRPO to leverage\nmulti-reward signals for policy optimization, thereby enhancing alignment\nacross diverse modalities. To mitigate hallucinations in the CoT reasoning, we\nintroduce an \"MLLM-as-a-judge\" mechanism that supervises the CoT outputs,\nfurther improving generalization. Extensive experiments show that HOID-R1\nachieves state-of-the-art performance on HOI detection benchmarks and\noutperforms existing methods in open-world generalization to novel scenarios.", "published": "2025-08-15 09:28:57", "link": "http://arxiv.org/abs/2508.11350v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Semantically Guided Adversarial Testing of Vision Models Using Language Models", "abstract": "In targeted adversarial attacks on vision models, the selection of the target\nlabel is a critical yet often overlooked determinant of attack success. This\ntarget label corresponds to the class that the attacker aims to force the model\nto predict. Now, existing strategies typically rely on randomness, model\npredictions, or static semantic resources, limiting interpretability,\nreproducibility, or flexibility. This paper then proposes a semantics-guided\nframework for adversarial target selection using the cross-modal knowledge\ntransfer from pretrained language and vision-language models. We evaluate\nseveral state-of-the-art models (BERT, TinyLLAMA, and CLIP) as similarity\nsources to select the most and least semantically related labels with respect\nto the ground truth, forming best- and worst-case adversarial scenarios. Our\nexperiments on three vision models and five attack methods reveal that these\nmodels consistently render practical adversarial targets and surpass static\nlexical databases, such as WordNet, particularly for distant class\nrelationships. We also observe that static testing of target labels offers a\npreliminary assessment of the effectiveness of similarity sources, \\textit{a\npriori} testing. Our results corroborate the suitability of pretrained models\nfor constructing interpretable, standardized, and scalable adversarial\nbenchmarks across architectures and datasets.", "published": "2025-08-15 09:11:22", "link": "http://arxiv.org/abs/2508.11341v1", "categories": ["cs.CV", "cs.CR", "cs.LG", "68T45, 68T01, 68T07, 68T10, 68M25", "I.2.10; I.5.4; I.2.6; I.2.7; K.6.5"], "primary_category": "cs.CV"}
{"title": "Cost-Effective Active Labeling for Data-Efficient Cervical Cell Classification", "abstract": "Information on the number and category of cervical cells is crucial for the\ndiagnosis of cervical cancer. However, existing classification methods capable\nof automatically measuring this information require the training dataset to be\nrepresentative, which consumes an expensive or even unaffordable human cost. We\nherein propose active labeling that enables us to construct a representative\ntraining dataset using a much smaller human cost for data-efficient cervical\ncell classification. This cost-effective method efficiently leverages the\nclassifier's uncertainty on the unlabeled cervical cell images to accurately\nselect images that are most beneficial to label. With a fast estimation of the\nuncertainty, this new algorithm exhibits its validity and effectiveness in\nenhancing the representative ability of the constructed training dataset. The\nextensive empirical results confirm its efficacy again in navigating the usage\nof human cost, opening the avenue for data-efficient cervical cell\nclassification.", "published": "2025-08-15 09:11:15", "link": "http://arxiv.org/abs/2508.11340v1", "categories": ["cs.CV", "q-bio.TO"], "primary_category": "cs.CV"}
{"title": "Index-Aligned Query Distillation for Transformer-based Incremental Object Detection", "abstract": "Incremental object detection (IOD) aims to continuously expand the capability\nof a model to detect novel categories while preserving its performance on\npreviously learned ones. When adopting a transformer-based detection model to\nperform IOD, catastrophic knowledge forgetting may inevitably occur, meaning\nthe detection performance on previously learned categories may severely\ndegenerate. Previous typical methods mainly rely on knowledge distillation (KD)\nto mitigate the catastrophic knowledge forgetting of transformer-based\ndetection models. Specifically, they utilize Hungarian Matching to build a\ncorrespondence between the queries of the last-phase and current-phase\ndetection models and align the classifier and regressor outputs between matched\nqueries to avoid knowledge forgetting. However, we observe that in IOD task,\nHungarian Matching is not a good choice. With Hungarian Matching, the query of\nthe current-phase model may match different queries of the last-phase model at\ndifferent iterations during KD. As a result, the knowledge encoded in each\nquery may be reshaped towards new categories, leading to the forgetting of\npreviously encoded knowledge of old categories. Based on our observations, we\npropose a new distillation approach named Index-Aligned Query Distillation\n(IAQD) for transformer-based IOD. Beyond using Hungarian Matching, IAQD\nestablishes a correspondence between queries of the previous and current phase\nmodels that have the same index. Moreover, we perform index-aligned\ndistillation only on partial queries which are critical for the detection of\nprevious categories. In this way, IAQD largely preserves the previous semantic\nand spatial encoding capabilities without interfering with the learning of new\ncategories. Extensive experiments on representative benchmarks demonstrate that\nIAQD effectively mitigates knowledge forgetting, achieving new state-of-the-art\nperformance.", "published": "2025-08-15 09:10:05", "link": "http://arxiv.org/abs/2508.11339v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "GANDiff FR: Hybrid GAN Diffusion Synthesis for Causal Bias Attribution in Face Recognition", "abstract": "We introduce GANDiff FR, the first synthetic framework that precisely\ncontrols demographic and environmental factors to measure, explain, and reduce\nbias with reproducible rigor. GANDiff FR unifies StyleGAN3-based\nidentity-preserving generation with diffusion-based attribute control, enabling\nfine-grained manipulation of pose around 30 degrees, illumination (four\ndirections), and expression (five levels) under ceteris paribus conditions. We\nsynthesize 10,000 demographically balanced faces across five cohorts validated\nfor realism via automated detection (98.2%) and human review (89%) to isolate\nand quantify bias drivers. Benchmarking ArcFace, CosFace, and AdaFace under\nmatched operating points shows AdaFace reduces inter-group TPR disparity by 60%\n(2.5% vs. 6.3%), with illumination accounting for 42% of residual bias.\nCross-dataset evaluation on RFW, BUPT, and CASIA WebFace confirms strong\nsynthetic-to-real transfer (r 0.85). Despite around 20% computational overhead\nrelative to pure GANs, GANDiff FR yields three times more attribute-conditioned\nvariants, establishing a reproducible, regulation-aligned (EU AI Act) standard\nfor fairness auditing. Code and data are released to support transparent,\nscalable bias evaluation.", "published": "2025-08-15 09:05:57", "link": "http://arxiv.org/abs/2508.11334v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Guiding WaveMamba with Frequency Maps for Image Debanding", "abstract": "Compression at low bitrates in modern codecs often introduces banding\nartifacts, especially in smooth regions such as skies. These artifacts degrade\nvisual quality and are common in user-generated content due to repeated\ntranscoding. We propose a banding restoration method that employs the Wavelet\nState Space Model and a frequency masking map to preserve high-frequency\ndetails. Furthermore, we provide a benchmark of open-source banding restoration\nmethods and evaluate their performance on two public banding image datasets.\nExperimentation on the available datasets suggests that the proposed\npost-processing approach effectively suppresses banding compared to the\nstate-of-the-art method (a DBI value of 0.082 on BAND-2k) while preserving\nimage textures. Visual inspections of the results confirm this. Code and\nsupplementary material are available at:\nhttps://github.com/xinyiW915/Debanding-PCS2025.", "published": "2025-08-15 09:03:40", "link": "http://arxiv.org/abs/2508.11331v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Noise Matters: Optimizing Matching Noise for Diffusion Classifiers", "abstract": "Although today's pretrained discriminative vision-language models (e.g.,\nCLIP) have demonstrated strong perception abilities, such as zero-shot image\nclassification, they also suffer from the bag-of-words problem and spurious\nbias. To mitigate these problems, some pioneering studies leverage powerful\ngenerative models (e.g., pretrained diffusion models) to realize generalizable\nimage classification, dubbed Diffusion Classifier (DC). Specifically, by\nrandomly sampling a Gaussian noise, DC utilizes the differences of denoising\neffects with different category conditions to classify categories.\nUnfortunately, an inherent and notorious weakness of existing DCs is noise\ninstability: different random sampled noises lead to significant performance\nchanges. To achieve stable classification performance, existing DCs always\nensemble the results of hundreds of sampled noises, which significantly reduces\nthe classification speed. To this end, we firstly explore the role of noise in\nDC, and conclude that: there are some ``good noises'' that can relieve the\ninstability. Meanwhile, we argue that these good noises should meet two\nprinciples: Frequency Matching and Spatial Matching. Regarding both principles,\nwe propose a novel Noise Optimization method to learn matching (i.e., good)\nnoise for DCs: NoOp. For frequency matching, NoOp first optimizes a\ndataset-specific noise: Given a dataset and a timestep t, optimize one randomly\ninitialized parameterized noise. For Spatial Matching, NoOp trains a\nMeta-Network that adopts an image as input and outputs image-specific noise\noffset. The sum of optimized noise and noise offset will be used in DC to\nreplace random noise. Extensive ablations on various datasets demonstrated the\neffectiveness of NoOp.", "published": "2025-08-15 09:01:03", "link": "http://arxiv.org/abs/2508.11330v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking", "abstract": "3D multi-object tracking is a critical and challenging task in the field of\nautonomous driving. A common paradigm relies on modeling individual object\nmotion, e.g., Kalman filters, to predict trajectories. While effective in\nsimple scenarios, this approach often struggles in crowded environments or with\ninaccurate detections, as it overlooks the rich geometric relationships between\nobjects. This highlights the need to leverage spatial cues. However, existing\ngeometry-aware methods can be susceptible to interference from irrelevant\nobjects, leading to ambiguous features and incorrect associations. To address\nthis, we propose focusing on cue-consistency: identifying and matching stable\nspatial patterns over time. We introduce the Dynamic Scene Cue-Consistency\nTracker (DSC-Track) to implement this principle. Firstly, we design a unified\nspatiotemporal encoder using Point Pair Features (PPF) to learn discriminative\ntrajectory embeddings while suppressing interference. Secondly, our\ncue-consistency transformer module explicitly aligns consistent feature\nrepresentations between historical tracks and current detections. Finally, a\ndynamic update mechanism preserves salient spatiotemporal information for\nstable online tracking. Extensive experiments on the nuScenes and Waymo Open\nDatasets validate the effectiveness and robustness of our approach. On the\nnuScenes benchmark, for instance, our method achieves state-of-the-art\nperformance, reaching 73.2% and 70.3% AMOTA on the validation and test sets,\nrespectively.", "published": "2025-08-15 08:48:13", "link": "http://arxiv.org/abs/2508.11323v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models", "abstract": "Vision-Language Models (VLMs), exemplified by CLIP, have emerged as\nfoundational for multimodal intelligence. However, their capacity for logical\nunderstanding remains significantly underexplored, resulting in critical\n''logical blindspots'' that limit their reliability in practical applications.\nTo systematically diagnose this, we introduce LogicBench, a comprehensive\nbenchmark with over 50,000 vision-language pairs across 9 logical categories\nand 4 diverse scenarios: images, videos, anomaly detection, and medical\ndiagnostics. Our evaluation reveals that existing VLMs, even the\nstate-of-the-art ones, fall at over 40 accuracy points below human performance,\nparticularly in challenging tasks like Causality and Conditionality,\nhighlighting their reliance on surface semantics over critical logical\nstructures. To bridge this gap, we propose LogicCLIP, a novel training\nframework designed to boost VLMs' logical sensitivity through advancements in\nboth data generation and optimization objectives. LogicCLIP utilizes\nlogic-aware data generation and a contrastive learning strategy that combines\ncoarse-grained alignment, a fine-grained multiple-choice objective, and a novel\nlogical structure-aware objective. Extensive experiments demonstrate\nLogicCLIP's substantial improvements in logical comprehension across all\nLogicBench domains, significantly outperforming baselines. Moreover, LogicCLIP\nretains, and often surpasses, competitive performance on general\nvision-language benchmarks, demonstrating that the enhanced logical\nunderstanding does not come at the expense of general alignment. We believe\nthat LogicBench and LogicCLIP will be important resources for advancing VLM\nlogical capabilities.", "published": "2025-08-15 08:40:13", "link": "http://arxiv.org/abs/2508.11317v1", "categories": ["cs.CV", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Denoise-then-Retrieve: Text-Conditioned Video Denoising for Video Moment Retrieval", "abstract": "Current text-driven Video Moment Retrieval (VMR) methods encode all video\nclips, including irrelevant ones, disrupting multimodal alignment and hindering\noptimization. To this end, we propose a denoise-then-retrieve paradigm that\nexplicitly filters text-irrelevant clips from videos and then retrieves the\ntarget moment using purified multimodal representations. Following this\nparadigm, we introduce the Denoise-then-Retrieve Network (DRNet), comprising\nText-Conditioned Denoising (TCD) and Text-Reconstruction Feedback (TRF)\nmodules. TCD integrates cross-attention and structured state space blocks to\ndynamically identify noisy clips and produce a noise mask to purify multimodal\nvideo representations. TRF further distills a single query embedding from\npurified video representations and aligns it with the text embedding, serving\nas auxiliary supervision for denoising during training. Finally, we perform\nconditional retrieval using text embeddings on purified video representations\nfor accurate VMR. Experiments on Charades-STA and QVHighlights demonstrate that\nour approach surpasses state-of-the-art methods on all metrics. Furthermore,\nour denoise-then-retrieve paradigm is adaptable and can be seamlessly\nintegrated into advanced VMR models to boost performance.", "published": "2025-08-15 08:34:05", "link": "http://arxiv.org/abs/2508.11313v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Hyperspectral vs. RGB for Pedestrian Segmentation in Urban Driving Scenes: A Comparative Study", "abstract": "Pedestrian segmentation in automotive perception systems faces critical\nsafety challenges due to metamerism in RGB imaging, where pedestrians and\nbackgrounds appear visually indistinguishable.. This study investigates the\npotential of hyperspectral imaging (HSI) for enhanced pedestrian segmentation\nin urban driving scenarios using the Hyperspectral City v2 (H-City) dataset. We\ncompared standard RGB against two dimensionality-reduction approaches by\nconverting 128-channel HSI data into three-channel representations: Principal\nComponent Analysis (PCA) and optimal band selection using Contrast\nSignal-to-Noise Ratio with Joint Mutual Information Maximization (CSNR-JMIM).\nThree semantic segmentation models were evaluated: U-Net, DeepLabV3+, and\nSegFormer. CSNR-JMIM consistently outperformed RGB with an average improvements\nof 1.44% in Intersection over Union (IoU) and 2.18% in F1-score for pedestrian\nsegmentation. Rider segmentation showed similar gains with 1.43% IoU and 2.25%\nF1-score improvements. These improved performance results from enhanced\nspectral discrimination of optimally selected HSI bands effectively reducing\nfalse positives. This study demonstrates robust pedestrian segmentation through\noptimal HSI band selection, showing significant potential for safety-critical\nautomotive applications.", "published": "2025-08-15 08:10:19", "link": "http://arxiv.org/abs/2508.11301v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Allen: Rethinking MAS Design through Step-Level Policy Autonomy", "abstract": "We introduce a new Multi-Agent System (MAS) - Allen, designed to address two\ncore challenges in current MAS design: (1) improve system's policy autonomy,\nempowering agents to dynamically adapt their behavioral strategies, and (2)\nachieving the trade-off between collaborative efficiency, task supervision, and\nhuman oversight in complex network topologies.\n  Our core insight is to redefine the basic execution unit in the MAS, allowing\nagents to autonomously form different patterns by combining these units. We\nhave constructed a four-tier state architecture (Task, Stage, Agent, Step) to\nconstrain system behavior from both task-oriented and execution-oriented\nperspectives. This achieves a unification of topological optimization and\ncontrollable progress.\n  Allen grants unprecedented Policy Autonomy, while making a trade-off for the\ncontrollability of the collaborative structure. The project code has been open\nsource at: https://github.com/motern88/Allen", "published": "2025-08-15 08:02:34", "link": "http://arxiv.org/abs/2508.11294v1", "categories": ["cs.MA", "cs.CV"], "primary_category": "cs.MA"}
{"title": "TimeMachine: Fine-Grained Facial Age Editing with Identity Preservation", "abstract": "With the advancement of generative models, facial image editing has made\nsignificant progress. However, achieving fine-grained age editing while\npreserving personal identity remains a challenging task.In this paper, we\npropose TimeMachine, a novel diffusion-based framework that achieves accurate\nage editing while keeping identity features unchanged. To enable fine-grained\nage editing, we inject high-precision age information into the multi-cross\nattention module, which explicitly separates age-related and identity-related\nfeatures. This design facilitates more accurate disentanglement of age\nattributes, thereby allowing precise and controllable manipulation of facial\naging.Furthermore, we propose an Age Classifier Guidance (ACG) module that\npredicts age directly in the latent space, instead of performing denoising\nimage reconstruction during training. By employing a lightweight module to\nincorporate age constraints, this design enhances age editing accuracy by\nmodest increasing training cost. Additionally, to address the lack of\nlarge-scale, high-quality facial age datasets, we construct a HFFA dataset\n(High-quality Fine-grained Facial-Age dataset) which contains one million\nhigh-resolution images labeled with identity and facial attributes.\nExperimental results demonstrate that TimeMachine achieves state-of-the-art\nperformance in fine-grained age editing while preserving identity consistency.", "published": "2025-08-15 07:46:37", "link": "http://arxiv.org/abs/2508.11284v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Unifying Scale-Aware Depth Prediction and Perceptual Priors for Monocular Endoscope Pose Estimation and Tissue Reconstruction", "abstract": "Accurate endoscope pose estimation and 3D tissue surface reconstruction\nsignificantly enhances monocular minimally invasive surgical procedures by\nenabling accurate navigation and improved spatial awareness. However, monocular\nendoscope pose estimation and tissue reconstruction face persistent challenges,\nincluding depth ambiguity, physiological tissue deformation, inconsistent\nendoscope motion, limited texture fidelity, and a restricted field of view. To\novercome these limitations, a unified framework for monocular endoscopic tissue\nreconstruction that integrates scale-aware depth prediction with\ntemporally-constrained perceptual refinement is presented. This framework\nincorporates a novel MAPIS-Depth module, which leverages Depth Pro for robust\ninitialisation and Depth Anything for efficient per-frame depth prediction, in\nconjunction with L-BFGS-B optimisation, to generate pseudo-metric depth\nestimates. These estimates are temporally refined by computing pixel\ncorrespondences using RAFT and adaptively blending flow-warped frames based on\nLPIPS perceptual similarity, thereby reducing artefacts arising from\nphysiological tissue deformation and motion. To ensure accurate registration of\nthe synthesised pseudo-RGBD frames from MAPIS-Depth, a novel WEMA-RTDL module\nis integrated, optimising both rotation and translation. Finally, truncated\nsigned distance function-based volumetric fusion and marching cubes are applied\nto extract a comprehensive 3D surface mesh. Evaluations on HEVD and SCARED,\nwith ablation and comparative analyses, demonstrate the framework's robustness\nand superiority over state-of-the-art methods.", "published": "2025-08-15 07:41:17", "link": "http://arxiv.org/abs/2508.11282v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Boosting the Robustness-Accuracy Trade-off of SNNs by Robust Temporal Self-Ensemble", "abstract": "Spiking Neural Networks (SNNs) offer a promising direction for\nenergy-efficient and brain-inspired computing, yet their vulnerability to\nadversarial perturbations remains poorly understood. In this work, we revisit\nthe adversarial robustness of SNNs through the lens of temporal ensembling,\ntreating the network as a collection of evolving sub-networks across discrete\ntimesteps. This formulation uncovers two critical but underexplored\nchallenges-the fragility of individual temporal sub-networks and the tendency\nfor adversarial vulnerabilities to transfer across time. To overcome these\nlimitations, we propose Robust Temporal self-Ensemble (RTE), a training\nframework that improves the robustness of each sub-network while reducing the\ntemporal transferability of adversarial perturbations. RTE integrates both\nobjectives into a unified loss and employs a stochastic sampling strategy for\nefficient optimization. Extensive experiments across multiple benchmarks\ndemonstrate that RTE consistently outperforms existing training methods in\nrobust-accuracy trade-off. Additional analyses reveal that RTE reshapes the\ninternal robustness landscape of SNNs, leading to more resilient and temporally\ndiversified decision boundaries. Our study highlights the importance of\ntemporal structure in adversarial learning and offers a principled foundation\nfor building robust spiking models.", "published": "2025-08-15 07:34:06", "link": "http://arxiv.org/abs/2508.11279v1", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Probing the Representational Power of Sparse Autoencoders in Vision Models", "abstract": "Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting\nthe hidden states of large language models (LLMs). By learning to reconstruct\nactivations from a sparse bottleneck layer, SAEs discover interpretable\nfeatures from the high-dimensional internal representations of LLMs. Despite\ntheir popularity with language models, SAEs remain understudied in the visual\ndomain. In this work, we provide an extensive evaluation the representational\npower of SAEs for vision models using a broad range of image-based tasks. Our\nexperimental results demonstrate that SAE features are semantically meaningful,\nimprove out-of-distribution generalization, and enable controllable generation\nacross three vision model architectures: vision embedding models, multi-modal\nLMMs and diffusion models. In vision embedding models, we find that learned SAE\nfeatures can be used for OOD detection and provide evidence that they recover\nthe ontological structure of the underlying model. For diffusion models, we\ndemonstrate that SAEs enable semantic steering through text encoder\nmanipulation and develop an automated pipeline for discovering\nhuman-interpretable attributes. Finally, we conduct exploratory experiments on\nmulti-modal LLMs, finding evidence that SAE features reveal shared\nrepresentations across vision and language modalities. Our study provides a\nfoundation for SAE evaluation in vision models, highlighting their strong\npotential improving interpretability, generalization, and steerability in the\nvisual domain.", "published": "2025-08-15 07:29:42", "link": "http://arxiv.org/abs/2508.11277v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Domain-aware Category-level Geometry Learning Segmentation for 3D Point Clouds", "abstract": "Domain generalization in 3D segmentation is a critical challenge in deploying\nmodels to unseen environments. Current methods mitigate the domain shift by\naugmenting the data distribution of point clouds. However, the model learns\nglobal geometric patterns in point clouds while ignoring the category-level\ndistribution and alignment. In this paper, a category-level geometry learning\nframework is proposed to explore the domain-invariant geometric features for\ndomain generalized 3D semantic segmentation. Specifically, Category-level\nGeometry Embedding (CGE) is proposed to perceive the fine-grained geometric\nproperties of point cloud features, which constructs the geometric properties\nof each class and couples geometric embedding to semantic learning. Secondly,\nGeometric Consistent Learning (GCL) is proposed to simulate the latent 3D\ndistribution and align the category-level geometric embeddings, allowing the\nmodel to focus on the geometric invariant information to improve\ngeneralization. Experimental results verify the effectiveness of the proposed\nmethod, which has very competitive segmentation accuracy compared with the\nstate-of-the-art domain generalized point cloud methods.", "published": "2025-08-15 07:02:08", "link": "http://arxiv.org/abs/2508.11265v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Temporally-Similar Structure-Aware Spatiotemporal Fusion of Satellite Images", "abstract": "This paper proposes a novel spatiotemporal (ST) fusion framework for\nsatellite images, named Temporally-Similar Structure-Aware ST fusion (TSSTF).\nST fusion is a promising approach to address the trade-off between the spatial\nand temporal resolution of satellite images. In real-world scenarios, observed\nsatellite images are severely degraded by noise due to measurement equipment\nand environmental conditions. Consequently, some recent studies have focused on\nenhancing the robustness of ST fusion methods against noise. However, existing\nnoise-robust ST fusion approaches often fail to capture fine spatial structure,\nleading to oversmoothing and artifacts. To address this issue, TSSTF introduces\ntwo key mechanisms: Temporally-Guided Total Variation (TGTV) and\nTemporally-Guided Edge Constraint (TGEC). TGTV is a novel regularization\nfunction that promotes spatial piecewise smoothness while preserving structural\ndetails, guided by a reference high spatial resolution image acquired on a\nnearby date. TGEC enforces consistency in edge locations between two temporally\nadjacent images, while allowing for spectral variations. We formulate the ST\nfusion task as a constrained optimization problem incorporating TGTV and TGEC,\nand develop an efficient algorithm based on a preconditioned primal-dual\nsplitting method. Experimental results demonstrate that TSSTF performs\ncomparably to state-of-the-art methods under noise-free conditions and\noutperforms them under noisy conditions. Additionally, we provide a\ncomprehensive set of recommended parameter values that consistently yield high\nperformance across diverse target regions and noise conditions, aiming to\nenhance reproducibility and practical utility.", "published": "2025-08-15 06:50:34", "link": "http://arxiv.org/abs/2508.11259v1", "categories": ["eess.SP", "cs.CV"], "primary_category": "eess.SP"}
{"title": "FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation", "abstract": "Recent advances in audio-driven portrait animation have demonstrated\nimpressive capabilities. However, existing methods struggle to align with\nfine-grained human preferences across multiple dimensions, such as motion\nnaturalness, lip-sync accuracy, and visual quality. This is due to the\ndifficulty of optimizing among competing preference objectives, which often\nconflict with one another, and the scarcity of large-scale, high-quality\ndatasets with multidimensional preference annotations. To address these, we\nfirst introduce Talking-Critic, a multimodal reward model that learns\nhuman-aligned reward functions to quantify how well generated videos satisfy\nmultidimensional expectations. Leveraging this model, we curate Talking-NSQ, a\nlarge-scale multidimensional human preference dataset containing 410K\npreference pairs. Finally, we propose Timestep-Layer adaptive multi-expert\nPreference Optimization (TLPO), a novel framework for aligning diffusion-based\nportrait animation models with fine-grained, multidimensional preferences. TLPO\ndecouples preferences into specialized expert modules, which are then fused\nacross timesteps and network layers, enabling comprehensive, fine-grained\nenhancement across all dimensions without mutual interference. Experiments\ndemonstrate that Talking-Critic significantly outperforms existing methods in\naligning with human preference ratings. Meanwhile, TLPO achieves substantial\nimprovements over baseline models in lip-sync accuracy, motion naturalness, and\nvisual quality, exhibiting superior performance in both qualitative and\nquantitative evaluations. Ours project page:\nhttps://fantasy-amap.github.io/fantasy-talking2/", "published": "2025-08-15 06:43:46", "link": "http://arxiv.org/abs/2508.11255v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian Re-Identification in Autonomous Driving", "abstract": "Re-Identification (ReID) is a critical technology in intelligent perception\nsystems, especially within autonomous driving, where onboard cameras must\nidentify pedestrians across views and time in real-time to support safe\nnavigation and trajectory prediction. However, the presence of uncertain or\nmissing input modalities--such as RGB, infrared, sketches, or textual\ndescriptions--poses significant challenges to conventional ReID approaches.\nWhile large-scale pre-trained models offer strong multimodal semantic modeling\ncapabilities, their computational overhead limits practical deployment in\nresource-constrained environments. To address these challenges, we propose a\nlightweight Uncertainty Modal Modeling (UMM) framework, which integrates a\nmultimodal token mapper, synthetic modality augmentation strategy, and\ncross-modal cue interactive learner. Together, these components enable unified\nfeature representation, mitigate the impact of missing modalities, and extract\ncomplementary information across different data types. Additionally, UMM\nleverages CLIP's vision-language alignment ability to fuse multimodal inputs\nefficiently without extensive finetuning. Experimental results demonstrate that\nUMM achieves strong robustness, generalization, and computational efficiency\nunder uncertain modality conditions, offering a scalable and practical solution\nfor pedestrian re-identification in autonomous driving scenarios.", "published": "2025-08-15 04:50:27", "link": "http://arxiv.org/abs/2508.11218v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Fluid Dynamics and Domain Reconstruction from Noisy Flow Images Using Physics-Informed Neural Networks and Quasi-Conformal Mapping", "abstract": "Blood flow imaging provides important information for hemodynamic behavior\nwithin the vascular system and plays an essential role in medical diagnosis and\ntreatment planning. However, obtaining high-quality flow images remains a\nsignificant challenge. In this work, we address the problem of denoising flow\nimages that may suffer from artifacts due to short acquisition times or\ndevice-induced errors. We formulate this task as an optimization problem, where\nthe objective is to minimize the discrepancy between the modeled velocity\nfield, constrained to satisfy the Navier-Stokes equations, and the observed\nnoisy velocity data. To solve this problem, we decompose it into two\nsubproblems: a fluid subproblem and a geometry subproblem. The fluid subproblem\nleverages a Physics-Informed Neural Network to reconstruct the velocity field\nfrom noisy observations, assuming a fixed domain. The geometry subproblem aims\nto infer the underlying flow region by optimizing a quasi-conformal mapping\nthat deforms a reference domain. These two subproblems are solved in an\nalternating Gauss-Seidel fashion, iteratively refining both the velocity field\nand the domain. Upon convergence, the framework yields a high-quality\nreconstruction of the flow image. We validate the proposed method through\nexperiments on synthetic flow data in a converging channel geometry under\nvarying levels of Gaussian noise, and on real-like flow data in an aortic\ngeometry with signal-dependent noise. The results demonstrate the effectiveness\nand robustness of the approach. Additionally, ablation studies are conducted to\nassess the influence of key hyperparameters.", "published": "2025-08-15 04:49:07", "link": "http://arxiv.org/abs/2508.11216v1", "categories": ["math.NA", "cs.CV", "cs.NA"], "primary_category": "math.NA"}
{"title": "A Coarse-to-Fine Human Pose Estimation Method based on Two-stage Distillation and Progressive Graph Neural Network", "abstract": "Human pose estimation has been widely applied in the human-centric\nunderstanding and generation, but most existing state-of-the-art human pose\nestimation methods require heavy computational resources for accurate\npredictions. In order to obtain an accurate, robust yet lightweight human pose\nestimator, one feasible way is to transfer pose knowledge from a powerful\nteacher model to a less-parameterized student model by knowledge distillation.\nHowever, the traditional knowledge distillation framework does not fully\nexplore the contextual information among human joints. Thus, in this paper, we\npropose a novel coarse-to-fine two-stage knowledge distillation framework for\nhuman pose estimation. In the first-stage distillation, we introduce the human\njoints structure loss to mine the structural information among human joints so\nas to transfer high-level semantic knowledge from the teacher model to the\nstudent model. In the second-stage distillation, we utilize an Image-Guided\nProgressive Graph Convolutional Network (IGP-GCN) to refine the initial human\npose obtained from the first-stage distillation and supervise the training of\nthe IGP-GCN in the progressive way by the final output pose of teacher model.\nThe extensive experiments on the benchmark dataset: COCO keypoint and CrowdPose\ndatasets, show that our proposed method performs favorably against lots of the\nexisting state-of-the-art human pose estimation methods, especially for the\nmore complex CrowdPose dataset, the performance improvement of our model is\nmore significant.", "published": "2025-08-15 04:41:49", "link": "http://arxiv.org/abs/2508.11212v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Efficient Image-to-Image Schr\u00f6dinger Bridge for CT Field of View Extension", "abstract": "Computed tomography (CT) is a cornerstone imaging modality for non-invasive,\nhigh-resolution visualization of internal anatomical structures. However, when\nthe scanned object exceeds the scanner's field of view (FOV), projection data\nare truncated, resulting in incomplete reconstructions and pronounced artifacts\nnear FOV boundaries. Conventional reconstruction algorithms struggle to recover\naccurate anatomy from such data, limiting clinical reliability. Deep learning\napproaches have been explored for FOV extension, with diffusion generative\nmodels representing the latest advances in image synthesis. Yet, conventional\ndiffusion models are computationally demanding and slow at inference due to\ntheir iterative sampling process. To address these limitations, we propose an\nefficient CT FOV extension framework based on the image-to-image Schr\\\"odinger\nBridge (I$^2$SB) diffusion model. Unlike traditional diffusion models that\nsynthesize images from pure Gaussian noise, I$^2$SB learns a direct stochastic\nmapping between paired limited-FOV and extended-FOV images. This direct\ncorrespondence yields a more interpretable and traceable generative process,\nenhancing anatomical consistency and structural fidelity in reconstructions.\nI$^2$SB achieves superior quantitative performance, with root-mean-square error\n(RMSE) values of 49.8\\,HU on simulated noisy data and 152.0HU on real data,\noutperforming state-of-the-art diffusion models such as conditional denoising\ndiffusion probabilistic models (cDDPM) and patch-based diffusion methods.\nMoreover, its one-step inference enables reconstruction in just 0.19s per 2D\nslice, representing over a 700-fold speedup compared to cDDPM (135s) and\nsurpassing diffusionGAN (0.58s), the second fastest. This combination of\naccuracy and efficiency makes I$^2$SB highly suitable for real-time or clinical\ndeployment.", "published": "2025-08-15 04:41:05", "link": "http://arxiv.org/abs/2508.11211v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning", "abstract": "Recent advances in vision-language models (VLMs) have demonstrated strong\ngeneralization in natural image tasks. However, their performance often\ndegrades on unmanned aerial vehicle (UAV)-based aerial imagery, which features\nhigh resolution, complex spatial semantics, and strict real-time constraints.\nThese challenges limit the applicability of general-purpose VLMs to structured\naerial reasoning tasks. To address these challenges, we propose UAV-VL-R1, a\nlightweight VLM explicitly designed for aerial visual reasoning. It is trained\nusing a hybrid method that combines supervised fine-tuning (SFT) and\nmulti-stage reinforcement learning (RL). We leverage the group relative policy\noptimization (GRPO) algorithm to promote structured and interpretable reasoning\nthrough rule-guided rewards and intra-group policy alignment. To support model\ntraining and evaluation, we introduce a high-resolution visual question\nanswering dataset named HRVQA-VL, which consists of 50,019 annotated samples\ncovering eight UAV-relevant reasoning tasks, including object counting,\ntransportation recognition, and spatial scene inference. Experimental results\nshow that UAV-VL-R1 achieves a 48.17% higher zero-shot accuracy than the\nQwen2-VL-2B-Instruct baseline and even outperforms its 72B-scale variant, which\nis 36x larger, on multiple tasks. Ablation studies reveal that while SFT\nimproves semantic alignment, it may reduce reasoning diversity in mathematical\ntasks. GRPO-based RL compensates for this limitation by enhancing logical\nflexibility and the robustness of inference. Additionally, UAV-VL-R1 requires\nonly 3.9GB of memory under FP16 inference and can be quantized to 2.5GB with\nINT8, supporting real-time deployment on resource-constrained UAV platforms.", "published": "2025-08-15 04:06:40", "link": "http://arxiv.org/abs/2508.11196v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Generating Dialogues from Egocentric Instructional Videos for Task Assistance: Dataset, Method and Benchmark", "abstract": "Many everyday tasks ranging from fixing appliances, cooking recipes to car\nmaintenance require expert knowledge, especially when tasks are complex and\nmulti-step. Despite growing interest in AI agents, there is a scarcity of\ndialogue-video datasets grounded for real world task assistance. In this paper,\nwe propose a simple yet effective approach that transforms single-person\ninstructional videos into task-guidance two-person dialogues, aligned with fine\ngrained steps and video-clips. Our fully automatic approach, powered by large\nlanguage models, offers an efficient alternative to the substantial cost and\neffort required for human-assisted data collection. Using this technique, we\nbuild HowToDIV, a large-scale dataset containing 507 conversations, 6636\nquestion-answer pairs and 24 hours of videoclips across diverse tasks in\ncooking, mechanics, and planting. Each session includes multi-turn conversation\nwhere an expert teaches a novice user how to perform a task step by step, while\nobserving user's surrounding through a camera and microphone equipped wearable\ndevice. We establish the baseline benchmark performance on HowToDIV dataset\nthrough Gemma-3 model for future research on this new task of dialogues for\nprocedural-task assistance.", "published": "2025-08-15 03:57:20", "link": "http://arxiv.org/abs/2508.11192v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector", "abstract": "Monocular 3D object detectors, while effective on data from one ego camera\nheight, struggle with unseen or out-of-distribution camera heights. Existing\nmethods often rely on Plucker embeddings, image transformations or data\naugmentation. This paper takes a step towards this understudied problem by\nfirst investigating the impact of camera height variations on state-of-the-art\n(SoTA) Mono3D models. With a systematic analysis on the extended CARLA dataset\nwith multiple camera heights, we observe that depth estimation is a primary\nfactor influencing performance under height variations. We mathematically prove\nand also empirically observe consistent negative and positive trends in mean\ndepth error of regressed and ground-based depth models, respectively, under\ncamera height changes. To mitigate this, we propose Camera Height Robust\nMonocular 3D Detector (CHARM3R), which averages both depth estimates within the\nmodel. CHARM3R improves generalization to unseen camera heights by more than\n$45\\%$, achieving SoTA performance on the CARLA dataset. Codes and Models at\nhttps://github.com/abhi1kumar/CHARM3R", "published": "2025-08-15 03:27:17", "link": "http://arxiv.org/abs/2508.11185v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Versatile Video Tokenization with Generative 2D Gaussian Splatting", "abstract": "Video tokenization procedure is critical for a wide range of video processing\ntasks. Most existing approaches directly transform video into fixed-grid and\npatch-wise tokens, which exhibit limited versatility. Spatially, uniformly\nallocating a fixed number of tokens often leads to over-encoding in\nlow-information regions. Temporally, reducing redundancy remains challenging\nwithout explicitly distinguishing between static and dynamic content. In this\nwork, we propose the Gaussian Video Transformer (GVT), a versatile video\ntokenizer built upon a generative 2D Gaussian Splatting (2DGS) strategy. We\nfirst extract latent rigid features from a video clip and represent them with a\nset of 2D Gaussians generated by our proposed Spatio-Temporal Gaussian\nEmbedding (STGE) mechanism in a feed-forward manner. Such generative 2D\nGaussians not only enhance spatial adaptability by assigning higher (resp.,\nlower) rendering weights to regions with higher (resp., lower) information\ncontent during rasterization, but also improve generalization by avoiding\nper-video optimization.To enhance the temporal versatility, we introduce a\nGaussian Set Partitioning (GSP) strategy that separates the 2D Gaussians into\nstatic and dynamic sets, which explicitly model static content shared across\ndifferent time-steps and dynamic content specific to each time-step, enabling a\ncompact representation.We primarily evaluate GVT on the video reconstruction,\nwhile also assessing its performance on action recognition and compression\nusing the UCF101, Kinetics, and DAVIS datasets. Extensive experiments\ndemonstrate that GVT achieves a state-of-the-art video reconstruction quality,\noutperforms the baseline MAGVIT-v2 in action recognition, and delivers\ncomparable compression performance.", "published": "2025-08-15 03:16:45", "link": "http://arxiv.org/abs/2508.11183v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "HistoViT: Vision Transformer for Accurate and Scalable Histopathological Cancer Diagnosis", "abstract": "Accurate and scalable cancer diagnosis remains a critical challenge in modern\npathology, particularly for malignancies such as breast, prostate, bone, and\ncervical, which exhibit complex histological variability. In this study, we\npropose a transformer-based deep learning framework for multi-class tumor\nclassification in histopathological images. Leveraging a fine-tuned Vision\nTransformer (ViT) architecture, our method addresses key limitations of\nconventional convolutional neural networks, offering improved performance,\nreduced preprocessing requirements, and enhanced scalability across tissue\ntypes. To adapt the model for histopathological cancer images, we implement a\nstreamlined preprocessing pipeline that converts tiled whole-slide images into\nPyTorch tensors and standardizes them through data normalization. This ensures\ncompatibility with the ViT architecture and enhances both convergence stability\nand overall classification performance. We evaluate our model on four benchmark\ndatasets: ICIAR2018 (breast), SICAPv2 (prostate), UT-Osteosarcoma (bone), and\nSipakMed (cervical) dataset -- demonstrating consistent outperformance over\nexisting deep learning methods. Our approach achieves classification accuracies\nof 99.32%, 96.92%, 95.28%, and 96.94% for breast, prostate, bone, and cervical\ncancers respectively, with area under the ROC curve (AUC) scores exceeding 99%\nacross all datasets. These results confirm the robustness, generalizability,\nand clinical potential of transformer-based architectures in digital pathology.\nOur work represents a significant advancement toward reliable, automated, and\ninterpretable cancer diagnosis systems that can alleviate diagnostic burdens\nand improve healthcare outcomes.", "published": "2025-08-15 03:10:52", "link": "http://arxiv.org/abs/2508.11181v1", "categories": ["eess.IV", "cs.CV", "cs.LG"], "primary_category": "eess.IV"}
{"title": "Fine-Grained VLM Fine-tuning via Latent Hierarchical Adapter Learning", "abstract": "Adapter-based approaches have garnered attention for fine-tuning pre-trained\nVision-Language Models (VLMs) on few-shot classification tasks. These methods\nstrive to develop a lightweight module that better aligns visual and (category)\ntextual representations, thereby enhancing performance on downstream few-shot\nlearning tasks. However, existing adapters generally learn/align (category)\ntextual-visual modalities via explicit spatial proximity in the underlying\nembedding space, which i) fails to capture the inherent one-to-many\nassociations between categories and image samples and ii) struggles to\nestablish accurate associations between the unknown categories and images. To\naddress these issues, inspired by recent works on hyperbolic learning, we\ndevelop a novel Latent Hierarchical Adapter (LatHAdapter) for fine-tuning VLMs\non downstream few-shot classification tasks. The core of LatHAdapter is to\nexploit the latent semantic hierarchy of downstream training data and employ it\nto provide richer, fine-grained guidance for the adapter learning process.\nSpecifically, LatHAdapter first introduces some learnable `attribute' prompts\nas the bridge to align categories and images. Then, it projects the categories,\nattribute prompts, and images within each batch in a hyperbolic space, and\nemploys hierarchical regularization to learn the latent semantic hierarchy of\nthem, thereby fully modeling the inherent one-to-many associations among\ncategories, learnable attributes, and image samples. Extensive experiments on\nfour challenging few-shot tasks show that the proposed LatHAdapter consistently\noutperforms many other fine-tuning approaches, particularly in adapting known\nclasses and generalizing to unknown classes.", "published": "2025-08-15 03:02:36", "link": "http://arxiv.org/abs/2508.11176v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Exploring the Tradeoff Between Diversity and Discrimination for Continuous Category Discovery", "abstract": "Continuous category discovery (CCD) aims to automatically discover novel\ncategories in continuously arriving unlabeled data. This is a challenging\nproblem considering that there is no number of categories and labels in the\nnewly arrived data, while also needing to mitigate catastrophic forgetting.\nMost CCD methods cannot handle the contradiction between novel class discovery\nand classification well. They are also prone to accumulate errors in the\nprocess of gradually discovering novel classes. Moreover, most of them use\nknowledge distillation and data replay to prevent forgetting, occupying more\nstorage space. To address these limitations, we propose Independence-based\nDiversity and Orthogonality-based Discrimination (IDOD). IDOD mainly includes\nindependent enrichment of diversity module, joint discovery of novelty module,\nand continuous increment by orthogonality module. In independent enrichment,\nthe backbone is trained separately using contrastive loss to avoid it focusing\nonly on features for classification. Joint discovery transforms multi-stage\nnovel class discovery into single-stage, reducing error accumulation impact.\nContinuous increment by orthogonality module generates mutually orthogonal\nprototypes for classification and prevents forgetting with lower space overhead\nvia representative representation replay. Experimental results show that on\nchallenging fine-grained datasets, our method outperforms the state-of-the-art\nmethods.", "published": "2025-08-15 02:51:30", "link": "http://arxiv.org/abs/2508.11173v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "VFM-Guided Semi-Supervised Detection Transformer for Source-Free Object Detection in Remote Sensing Images", "abstract": "Unsupervised domain adaptation methods have been widely explored to bridge\ndomain gaps. However, in real-world remote-sensing scenarios, privacy and\ntransmission constraints often preclude access to source domain data, which\nlimits their practical applicability. Recently, Source-Free Object Detection\n(SFOD) has emerged as a promising alternative, aiming at cross-domain\nadaptation without relying on source data, primarily through a self-training\nparadigm. Despite its potential, SFOD frequently suffers from training collapse\ncaused by noisy pseudo-labels, especially in remote sensing imagery with dense\nobjects and complex backgrounds. Considering that limited target domain\nannotations are often feasible in practice, we propose a Vision\nfoundation-Guided DEtection TRansformer (VG-DETR), built upon a semi-supervised\nframework for SFOD in remote sensing images. VG-DETR integrates a Vision\nFoundation Model (VFM) into the training pipeline in a \"free lunch\" manner,\nleveraging a small amount of labeled target data to mitigate pseudo-label noise\nwhile improving the detector's feature-extraction capability. Specifically, we\nintroduce a VFM-guided pseudo-label mining strategy that leverages the VFM's\nsemantic priors to further assess the reliability of the generated\npseudo-labels. By recovering potentially correct predictions from\nlow-confidence outputs, our strategy improves pseudo-label quality and\nquantity. In addition, a dual-level VFM-guided alignment method is proposed,\nwhich aligns detector features with VFM embeddings at both the instance and\nimage levels. Through contrastive learning among fine-grained prototypes and\nsimilarity matching between feature maps, this dual-level alignment further\nenhances the robustness of feature representations against domain gaps.\nExtensive experiments demonstrate that VG-DETR achieves superior performance in\nsource-free remote sensing detection tasks.", "published": "2025-08-15 02:35:56", "link": "http://arxiv.org/abs/2508.11167v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Semi-supervised Image Dehazing via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models", "abstract": "Existing dehazing methods deal with real-world haze images with difficulty,\nespecially scenes with thick haze. One of the main reasons is the lack of\nreal-world paired data and robust priors. To avoid the costly collection of\npaired hazy and clear images, we propose an efficient semi-supervised image\ndehazing method via Expectation-Maximization and Bidirectional Brownian Bridge\nDiffusion Models (EM-B3DM) with a two-stage learning scheme. In the first\nstage, we employ the EM algorithm to decouple the joint distribution of paired\nhazy and clear images into two conditional distributions, which are then\nmodeled using a unified Brownian Bridge diffusion model to directly capture the\nstructural and content-related correlations between hazy and clear images. In\nthe second stage, we leverage the pre-trained model and large-scale unpaired\nhazy and clear images to further improve the performance of image dehazing.\nAdditionally, we introduce a detail-enhanced Residual Difference Convolution\nblock (RDC) to capture gradient-level information, significantly enhancing the\nmodel's representation capability. Extensive experiments demonstrate that our\nEM-B3DM achieves superior or at least comparable performance to\nstate-of-the-art methods on both synthetic and real-world datasets.", "published": "2025-08-15 02:33:44", "link": "http://arxiv.org/abs/2508.11165v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "LEARN: A Story-Driven Layout-to-Image Generation Framework for STEM Instruction", "abstract": "LEARN is a layout-aware diffusion framework designed to generate\npedagogically aligned illustrations for STEM education. It leverages a curated\nBookCover dataset that provides narrative layouts and structured visual cues,\nenabling the model to depict abstract and sequential scientific concepts with\nstrong semantic alignment. Through layout-conditioned generation, contrastive\nvisual-semantic training, and prompt modulation, LEARN produces coherent visual\nsequences that support mid-to-high-level reasoning in line with Bloom's\ntaxonomy while reducing extraneous cognitive load as emphasized by Cognitive\nLoad Theory. By fostering spatially organized and story-driven narratives, the\nframework counters fragmented attention often induced by short-form media and\npromotes sustained conceptual focus. Beyond static diagrams, LEARN demonstrates\npotential for integration with multimodal systems and curriculum-linked\nknowledge graphs to create adaptive, exploratory educational content. As the\nfirst generative approach to unify layout-based storytelling, semantic\nstructure learning, and cognitive scaffolding, LEARN represents a novel\ndirection for generative AI in education. The code and dataset will be released\nto facilitate future research and practical deployment.", "published": "2025-08-15 01:49:58", "link": "http://arxiv.org/abs/2508.11153v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Residual-based Efficient Bidirectional Diffusion Model for Image Dehazing and Haze Generation", "abstract": "Current deep dehazing methods only focus on removing haze from hazy images,\nlacking the capability to translate between hazy and haze-free images. To\naddress this issue, we propose a residual-based efficient bidirectional\ndiffusion model (RBDM) that can model the conditional distributions for both\ndehazing and haze generation. Firstly, we devise dual Markov chains that can\neffectively shift the residuals and facilitate bidirectional smooth transitions\nbetween them. Secondly, the RBDM perturbs the hazy and haze-free images at\nindividual timesteps and predicts the noise in the perturbed data to\nsimultaneously learn the conditional distributions. Finally, to enhance\nperformance on relatively small datasets and reduce computational costs, our\nmethod introduces a unified score function learned on image patches instead of\nentire images. Our RBDM successfully implements size-agnostic bidirectional\ntransitions between haze-free and hazy images with only 15 sampling steps.\nExtensive experiments demonstrate that the proposed method achieves superior or\nat least comparable performance to state-of-the-art methods on both synthetic\nand real-world datasets.", "published": "2025-08-15 01:00:15", "link": "http://arxiv.org/abs/2508.11134v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Lower Bounds for Induced-Universal Graphs", "abstract": "We give a series of new lower bounds on the minimum number of vertices\nrequired by a graph to contain every graph of a given family as induced\nsubgraph. In particular, we show that this induced-universal graph for\n$n$-vertex planar graphs must have at least $10.52n$ vertices. We also show\nthat the number of conflicting graphs to consider in order to beat this lower\nbound is at least $137$. In other words, any family of less than $137$ planar\ngraphs of $n$ vertices has an induced-universal graph with less than $10.52n$\nvertices, stressing the difficulty in beating such lower bounds. Similar\nresults are developed for other graph families, including but not limited to,\ntrees, outerplanar graphs, series-parallel graphs, $K_{3,3}$-minor free graphs.\nAs a byproduct, we show that any family of $t$ graphs of $n$ vertices having\nsmall chromatic number and sublinear pathwidth, like any proper minor-closed\nfamily, has an induced-universal graph with less than $\\frac{15}{7} \\sqrt{t}\n\\cdot n$ vertices. This is achieved by making a bridge between equitable\ncolorings, combinatorial designs, and path-decompositions.", "published": "2025-08-15 16:44:22", "link": "http://arxiv.org/abs/2508.11585v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "Temporal Network Analysis of Microservice Architectural Degradation", "abstract": "Microservice architecture can be modeled as a network of microservices making\ncalls to each other, commonly known as the service dependency graph. Network\nScience can provide methods to study such networks. In particular, temporal\nnetwork analysis is a branch of Network Science that analyzes networks evolving\nwith time. In microservice systems, temporal networks can arise if we examine\nthe architecture of the system across releases or monitor a deployed system\nusing tracing.\n  In this research summary paper, I discuss the challenges in obtaining\ntemporal networks from microservice systems and analyzing them with the\ntemporal network methods. In particular, the most complete temporal network\nthat we could obtain contains 7 time instances and 42 microservices, which\nlimits the potential analysis that could be applied.", "published": "2025-08-15 16:26:20", "link": "http://arxiv.org/abs/2508.11571v1", "categories": ["cs.SE", "cs.DM"], "primary_category": "cs.SE"}
{"title": "Sampling tree-weighted partitions without sampling trees", "abstract": "This paper gives a new algorithm for sampling tree-weighted partitions of a\nlarge class of planar graphs. Formally, the tree-weighted distribution on\n$k$-partitions of a graph weights $k$-partitions proportional to the product of\nthe number of spanning trees of each partition class. Recent work on problems\nin computational redistricting analysis has driven special interest in the\nconditional distribution where all partition classes have the same size\n(balanced partitions). One class of Markov chains in wide use aims to sample\nfrom balanced tree-weighted $k$-partitions using a sampler for balanced\ntree-weighted 2-partitions. Previous implementations of this 2-partition\nsampler would draw a random spanning tree and check whether it contains an edge\nwhose removal produces a balanced 2-component forest; if it does, this\n2-partition is accepted, otherwise the algorithm rejects and repeats. In\npractice, this is a significant computational bottleneck.\n  We show that in fact it is possible to sample from the balanced tree-weighted\n2-partition distribution directly, without first sampling a spanning tree; the\nacceptance and rejection rates are the same as in previous samplers. We prove\nthat on a wide class of planar graphs encompassing network structures typically\narising from the geographic data used in computational redistricting, our\nalgorithm takes expected linear time $O(n)$. Notably, this is asymptotically\nfaster than the best known method to generate random trees, which is $O(n\n\\log^2 n)$ for approximate sampling and $O(n^{1 + \\log \\log \\log n / \\log \\log\nn})$ for exact sampling. Additionally, we show that a variant of our algorithm\nalso gives a speedup to $O(n \\log n)$ for exact sampling of uniformly random\ntrees on these families of graphs, improving the bounds for both exact and\napproximate sampling.", "published": "2025-08-15 00:36:22", "link": "http://arxiv.org/abs/2508.11130v1", "categories": ["cs.DS", "cs.DM", "math.CO"], "primary_category": "cs.DS"}
{"title": "INFNet: A Task-aware Information Flow Network for Large-Scale Recommendation Systems", "abstract": "Feature interaction has long been a cornerstone of ranking models in\nlarge-scale recommender systems due to its proven effectiveness in capturing\ncomplex dependencies among features. However, existing feature interaction\nstrategies face two critical challenges in industrial applications: (1) The\nvast number of categorical and sequential features makes exhaustive interaction\ncomputationally prohibitive, often resulting in optimization difficulties. (2)\nReal-world recommender systems typically involve multiple prediction\nobjectives, yet most current approaches apply feature interaction modules prior\nto the multi-task learning layers. This late-fusion design overlooks\ntask-specific feature dependencies and inherently limits the capacity of\nmulti-task modeling. To address these limitations, we propose the Information\nFlow Network (INFNet), a task-aware architecture designed for large-scale\nrecommendation scenarios. INFNet distinguishes features into three token types,\ncategorical tokens, sequence tokens, and task tokens, and introduces a novel\ndual-flow design comprising heterogeneous and homogeneous alternating\ninformation blocks. For heterogeneous information flow, we employ a\ncross-attention mechanism with proxy that facilitates efficient cross-modal\ntoken interaction with balanced computational cost. For homogeneous flow, we\ndesign type-specific Proxy Gated Units (PGUs) to enable fine-grained intra-type\nfeature processing. Extensive experiments on multiple offline benchmarks\nconfirm that INFNet achieves state-of-the-art performance. Moreover, INFNet has\nbeen successfully deployed in a commercial online advertising system, yielding\nsignificant gains of +1.587% in Revenue (REV) and +1.155% in Click-Through Rate\n(CTR).", "published": "2025-08-15 16:18:32", "link": "http://arxiv.org/abs/2508.11565v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "When Algorithms Mirror Minds: A Confirmation-Aware Social Dynamic Model of Echo Chamber and Homogenization Traps", "abstract": "Recommender systems increasingly suffer from echo chambers and user\nhomogenization, systemic distortions arising from the dynamic interplay between\nalgorithmic recommendations and human behavior. While prior work has studied\nthese phenomena through the lens of algorithmic bias or social network\nstructure, we argue that the psychological mechanisms of users and the\nclosed-loop interaction between users and recommenders are critical yet\nunderstudied drivers of these emergent effects. To bridge this gap, we propose\nthe Confirmation-Aware Social Dynamic Model which incorporates user psychology\nand social relationships to simulate the actual user and recommender\ninteraction process. Our theoretical analysis proves that echo chambers and\nhomogenization traps, defined respectively as reduced recommendation diversity\nand homogenized user representations, will inevitably occur. We also conduct\nextensive empirical simulations on two real-world datasets and one synthetic\ndataset with five well-designed metrics, exploring the root factors influencing\nthe aforementioned phenomena from three level perspectives: the stochasticity\nand social integration degree of recommender (system-level), the psychological\nmechanisms of users (user-level), and the dataset scale (platform-level).\nFurthermore, we demonstrate four practical mitigation strategies that help\nalleviate echo chambers and user homogenization at the cost of some\nrecommendation accuracy. Our findings provide both theoretical and empirical\ninsights into the emergence and drivers of echo chambers and user\nhomogenization, as well as actionable guidelines for human-centered recommender\ndesign.", "published": "2025-08-15 14:55:55", "link": "http://arxiv.org/abs/2508.11516v1", "categories": ["cs.SI", "cs.IR"], "primary_category": "cs.SI"}
{"title": "RAG for Geoscience: What We Expect, Gaps and Opportunities", "abstract": "Retrieval-Augmented Generation (RAG) enhances language models by combining\nretrieval with generation. However, its current workflow remains largely\ntext-centric, limiting its applicability in geoscience. Many geoscientific\ntasks are inherently evidence-hungry. Typical examples involve imputing missing\nobservations using analog scenes, retrieving equations and parameters to\ncalibrate models, geolocating field photos based on visual cues, or surfacing\nhistorical case studies to support policy analyses. A simple\n``retrieve-then-generate'' pipeline is insufficient for these needs. We\nenvision Geo-RAG, a next-generation paradigm that reimagines RAG as a modular\nretrieve $\\rightarrow$ reason $\\rightarrow$ generate $\\rightarrow$ verify loop.\nGeo-RAG supports four core capabilities: (i) retrieval of multi-modal Earth\ndata; (ii) reasoning under physical and domain constraints; (iii) generation of\nscience-grade artifacts; and (iv) verification of generated hypotheses against\nnumerical models, ground measurements, and expert assessments. This shift opens\nnew opportunities for more trustworthy and transparent geoscience workflows.", "published": "2025-08-15 06:33:27", "link": "http://arxiv.org/abs/2508.11246v1", "categories": ["cs.ET", "cs.IR"], "primary_category": "cs.ET"}
{"title": "Mitigating Filter Bubble from the Perspective of Community Detection: A Universal Framework", "abstract": "In recent years, recommender systems have primarily focused on improving\naccuracy at the expense of diversity, which exacerbates the well-known filter\nbubble effect. This paper proposes a universal framework called CD-CGCN to\naddress the filter bubble issue in recommender systems from a community\ndetection perspective. By analyzing user-item interaction histories with a\ncommunity detection algorithm, we reveal that state-of-the-art recommendations\noften focus on intra-community items, worsening the filter bubble effect.\nCD-CGCN, a model-agnostic framework, integrates a Conditional Discriminator and\na Community-reweighted Graph Convolutional Network which can be plugged into\nmost recommender models. Using adversarial learning based on community labels,\nit counteracts the extracted community attributes and incorporates an inference\nstrategy tailored to the user's specific filter bubble state. Extensive\nexperiments on real-world datasets with multiple base models validate its\neffectiveness in mitigating filter bubbles while preserving recommendation\nquality. Additionally, by applying community debiasing to the original test set\nto construct an unbiased test set, we observe that CD-CGCN demonstrates\nsuperior performance in capturing users' inter-community preferences.", "published": "2025-08-15 05:57:38", "link": "http://arxiv.org/abs/2508.11239v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Representation Quantization for Collaborative Filtering Augmentation", "abstract": "As the core algorithm in recommendation systems, collaborative filtering (CF)\nalgorithms inevitably face the problem of data sparsity. Since CF captures\nsimilar users and items for recommendations, it is effective to augment the\nlacking user-user and item-item homogeneous linkages. However, existing methods\nare typically limited to connecting through overlapping interacted neighbors or\nthrough similar attributes and contents. These approaches are constrained by\ncoarse-grained, sparse attributes and fail to effectively extract behavioral\ncharacteristics jointly from interaction sequences and attributes. To address\nthese challenges, we propose a novel two-stage collaborative recommendation\nalgorithm, DQRec: Decomposition-based Quantized Variational AutoEncoder\n(DQ-VAE) for Recommendation. DQRec augments features and homogeneous linkages\nby extracting the behavior characteristics jointly from interaction sequences\nand attributes, namely patterns, such as user multi-aspect interests. Inspired\nby vector quantization (VQ) technology, we propose a new VQ algorithm, DQ-VAE,\nwhich decomposes the pre-trained representation embeddings into distinct\ndimensions, and quantize them to generates semantic IDs. We utilize the\ngenerated semantic IDs as the extracted patterns mentioned above. By\nintegrating these semantic ID patterns into the recommendation process through\nfeature and linkage augmentation, the system enriches both latent and explicit\nuser and item features, identifies pattern-similar neighbors, and thereby\nimproves the efficiency of information diffusion. Experimental comparisons with\nbaselines across multiple datasets demonstrate the superior performance of the\nproposed DQRec method.", "published": "2025-08-15 04:00:50", "link": "http://arxiv.org/abs/2508.11194v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Optimizing Rate-CRB Performance for Beyond Diagonal Reconfigurable Intelligent Surface Enabled ISAC", "abstract": "This letter considers a beyond diagonal reconfigurable intelligent surface\n(BD-RIS) aided integrated sensing and communication (ISAC) system, where the\nBD-RIS can help a multi-antenna base station (BS) serve multiple user\nequipments (UEs) and localize a target simultaneously. We formulate an\noptimization problem that designs the BS beamforming matrix and the BD-RIS\nscattering matrix to maximize UEs' sum rate subject to a localization\nCramer-Rao bound (CRB) constraint and an additional unitary matrix constraint\nfor the scattering matrix. Because unitary matrices form a manifold, our\nproblem belongs to constrained manifold optimization. This letter proposes a\nlog-barrier based Riemannian steepest ascent method to solve this problem\neffectively. Numerical results verify the effectiveness of our algorithm and\nthe performance gain of the BD-RIS aided ISAC systems over the conventional RIS\naided ISAC systems.", "published": "2025-08-15 08:03:31", "link": "http://arxiv.org/abs/2508.11295v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Beyond Diagonal Reconfigurable Intelligent Surface Enabled Sensing: Cramer-Rao Bound Optimization", "abstract": "Recently, beyond diagonal reconfigurable intelligent surface (BD-RIS) has\nemerged as a more flexible solution to engineer the wireless propagation\nchannels, thanks to its non-diagonal reflecting matrix. Although the gain of\nthe BD-RIS over the conventional RIS in communication has been revealed in many\nworks, its gain in 6G sensing is still unknown. This motivates us to study the\nBD-RIS assisted sensing in this letter. Specifically, we derive the Cramer-Rao\nbound (CRB) for estimating the angle-of-arrival (AOA) from the target to the\nBD-RIS under the constraint that the BD-RIS scattering matrix is unitary. To\nminimize the CRB, we develop an optimization scheme based on an adaptive\nRiemannian steepest ascent algorithm that can satisfy the non-convex unitary\nconstraint. Numerical results demonstrate that the proposed BD-RIS-assisted\ntarget localization method achieves superior sensing performance.", "published": "2025-08-15 07:55:23", "link": "http://arxiv.org/abs/2508.11292v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Multi-Satellite Cooperative MIMO Transmission: Statistical CSI-Aware RSMA Precoding Design", "abstract": "We investigate inter-satellite cooperative transmission in a multiple\nlow-Earth orbit (LEO) satellite communication system to enhance spectral\nefficiency. Specifically, we design multiple-input multipleoutput (MIMO)\nprecoding at LEO satellites for cooperative rate-splitting multiple access\n(RSMA). Given the difficulty of acquiring instantaneous channel state\ninformation (iCSI) due to long delays and Doppler effects, we formulate an\nergodic max-min fairness rate (MMFR) maximization problem based on statistical\nCSI (sCSI). To address the challenge of ergodic rate evaluation, we approximate\nthe problem using closed-form upper bounds and develop a weighted minimum mean\nsquared error-based algorithm to obtain a stationary point. Simulation results\ndemonstrate that the proposed sCSI-based RSMA scheme approaches iCSI-based\nperformance and significantly outperforms conventional space-division multiple\naccess.", "published": "2025-08-15 00:41:25", "link": "http://arxiv.org/abs/2508.11132v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective", "abstract": "Carbon capture and storage (CCS) projects typically involve a diverse array\nof stakeholders or players from public, private, and regulatory sectors, each\nwith different objectives and responsibilities. Given the complexity, scale,\nand long-term nature of CCS operations, determining whether individual\nstakeholders can independently maximize their interests or whether\ncollaborative coalition agreements are needed remains a central question for\neffective CCS project planning and management. CCS projects are often\nimplemented in geologically connected sites, where shared geological features\nsuch as pressure space and reservoir pore capacity can lead to competitive\nbehavior among stakeholders. Furthermore, CO2 storage sites are often located\nin geologically mature basins that previously served as sites for hydrocarbon\nextraction or wastewater disposal in order to leverage existing\ninfrastructures, which makes unilateral optimization even more complicated and\nunrealistic.\n  In this work, we propose a paradigm based on Markov games to quantitatively\ninvestigate how different coalition structures affect the goals of\nstakeholders. We frame this multi-stakeholder multi-site problem as a\nmulti-agent reinforcement learning problem with safety constraints. Our\napproach enables agents to learn optimal strategies while compliant with safety\nregulations. We present an example where multiple operators are injecting CO2\ninto their respective project areas in a geologically connected basin. To\naddress the high computational cost of repeated simulations of high-fidelity\nmodels, a previously developed surrogate model based on the Embed-to-Control\n(E2C) framework is employed. Our results demonstrate the effectiveness of the\nproposed framework in addressing optimal management of CO2 storage when\nmultiple stakeholders with various objectives and goals are involved.", "published": "2025-08-15 17:36:25", "link": "http://arxiv.org/abs/2508.11618v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Nonparametric learning of stochastic differential equations from sparse and noisy data", "abstract": "The paper proposes a systematic framework for building data-driven stochastic\ndifferential equation (SDE) models from sparse, noisy observations. Unlike\ntraditional parametric approaches, which assume a known functional form for the\ndrift, our goal here is to learn the entire drift function directly from data\nwithout strong structural assumptions, making it especially relevant in\nscientific disciplines where system dynamics are partially understood or highly\ncomplex. We cast the estimation problem as minimization of the penalized\nnegative log-likelihood functional over a reproducing kernel Hilbert space\n(RKHS). In the sparse observation regime, the presence of unobserved trajectory\nsegments makes the SDE likelihood intractable. To address this, we develop an\nExpectation-Maximization (EM) algorithm that employs a novel Sequential Monte\nCarlo (SMC) method to approximate the filtering distribution and generate Monte\nCarlo estimates of the E-step objective. The M-step then reduces to a penalized\nempirical risk minimization problem in the RKHS, whose minimizer is given by a\nfinite linear combination of kernel functions via a generalized representer\ntheorem. To control model complexity across EM iterations, we also develop a\nhybrid Bayesian variant of the algorithm that uses shrinkage priors to identify\nsignificant coefficients in the kernel expansion. We establish important\ntheoretical convergence results for both the exact and approximate EM\nsequences. The resulting EM-SMC-RKHS procedure enables accurate estimation of\nthe drift function of stochastic dynamical systems in low-data regimes and is\nbroadly applicable across domains requiring continuous-time modeling under\nobservational constraints. We demonstrate the effectiveness of our method\nthrough a series of numerical experiments.", "published": "2025-08-15 17:01:59", "link": "http://arxiv.org/abs/2508.11597v1", "categories": ["stat.ML", "cs.LG", "math.PR", "stat.ME", "62G05, 62M05, 60H10, 60J60, 46E22, 65C05, 65C35"], "primary_category": "stat.ML"}
{"title": "Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation", "abstract": "Effective and efficient agricultural manipulation and harvesting depend on\naccurately understanding the current state of the grasp. The agricultural\nenvironment presents unique challenges due to its complexity, clutter, and\nocclusion. Additionally, fruit is physically attached to the plant, requiring\nprecise separation during harvesting. Selecting appropriate sensors and\nmodeling techniques is critical for obtaining reliable feedback and correctly\nidentifying grasp states. This work investigates a set of key sensors, namely\ninertial measurement units (IMUs), infrared (IR) reflectance, tension, tactile\nsensors, and RGB cameras, integrated into a compliant gripper to classify grasp\nstates. We evaluate the individual contribution of each sensor and compare the\nperformance of two widely used classification models: Random Forest and Long\nShort-Term Memory (LSTM) networks. Our results demonstrate that a Random Forest\nclassifier, trained in a controlled lab environment and tested on real cherry\ntomato plants, achieved 100% accuracy in identifying slip, grasp failure, and\nsuccessful picks, marking a substantial improvement over baseline performance.\nFurthermore, we identify a minimal viable sensor combination, namely IMU and\ntension sensors that effectively classifies grasp states. This classifier\nenables the planning of corrective actions based on real-time feedback, thereby\nenhancing the efficiency and reliability of fruit harvesting operations.", "published": "2025-08-15 16:47:42", "link": "http://arxiv.org/abs/2508.11588v1", "categories": ["cs.RO", "cs.LG"], "primary_category": "cs.RO"}
{"title": "SeamlessFlow: A Trainer Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling", "abstract": "We introduce SeamlessFlow, a server based reinforcement learning (RL)\nframework that addresses two core challenges in industrial scale RL: (1)\ndecoupling RL training from the complex execution flow of agents; (2)\nmaximizing GPU utilization with minimal idle time while preserving the\nstability and scalability required for large-scale deployments. First,\nSeamlessFlow introduces a data plane that decouples the RL trainer from\ndiverse, complex agent implementations while sustaining high throughput. A\ncentral trajectory manager maintains complete interaction histories and\nsupports partial rollout, allowing rollout to pause for weight updates and\nresume seamlessly, keeping agents unaware of service interruptions. Second, we\npropose a tag driven scheduling paradigm that abstracts hardware into\ncapability tagged resources, unifying colocated and disaggregated\narchitectures. Based on this, SeamlessFlow introduces a spatiotemporal\nmultiplexing pipeline that dynamically reassigns idle training nodes to rollout\nin a train rollout separated setup, eliminating pipeline bubbles and fully\nexploiting heterogeneous cluster resources. By combining these innovations,\nSeamlessFlow delivers both stability and high performance, making it well\nsuited for multi agent, long horizon, and other complex RL tasks.", "published": "2025-08-15 15:55:37", "link": "http://arxiv.org/abs/2508.11553v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Nested Operator Inference for Adaptive Data-Driven Learning of Reduced-order Models", "abstract": "This paper presents a data-driven, nested Operator Inference (OpInf) approach\nfor learning physics-informed reduced-order models (ROMs) from snapshot data of\nhigh-dimensional dynamical systems. The approach exploits the inherent\nhierarchy within the reduced space to iteratively construct initial guesses for\nthe OpInf learning problem that prioritize the interactions of the dominant\nmodes. The initial guess computed for any target reduced dimension corresponds\nto a ROM with provably smaller or equal snapshot reconstruction error than with\nstandard OpInf. Moreover, our nested OpInf algorithm can be warm-started from\npreviously learned models, enabling versatile application scenarios involving\ndynamic basis and model form updates. We demonstrate the performance of our\nalgorithm on a cubic heat conduction problem, with nested OpInf achieving a\nfour times smaller error than standard OpInf at a comparable offline time.\nFurther, we apply nested OpInf to a large-scale, parameterized model of the\nGreenland ice sheet where, despite model form approximation errors, it learns a\nROM with, on average, 3% error and computational speed-up factor above 19,000.", "published": "2025-08-15 15:38:52", "link": "http://arxiv.org/abs/2508.11542v1", "categories": ["cs.LG", "cs.CE", "cs.NA", "math.NA"], "primary_category": "cs.LG"}
{"title": "DFed-SST: Building Semantic- and Structure-aware Topologies for Decentralized Federated Graph Learning", "abstract": "Decentralized Federated Learning (DFL) has emerged as a robust distributed\nparadigm that circumvents the single-point-of-failure and communication\nbottleneck risks of centralized architectures. However, a significant challenge\narises as existing DFL optimization strategies, primarily designed for tasks\nsuch as computer vision, fail to address the unique topological information\ninherent in the local subgraph. Notably, while Federated Graph Learning (FGL)\nis tailored for graph data, it is predominantly implemented in a centralized\nserver-client model, failing to leverage the benefits of decentralization.To\nbridge this gap, we propose DFed-SST, a decentralized federated graph learning\nframework with adaptive communication. The core of our method is a\ndual-topology adaptive communication mechanism that leverages the unique\ntopological features of each client's local subgraph to dynamically construct\nand optimize the inter-client communication topology. This allows our framework\nto guide model aggregation efficiently in the face of heterogeneity. Extensive\nexperiments on eight real-world datasets consistently demonstrate the\nsuperiority of DFed-SST, achieving 3.26% improvement in average accuracy over\nbaseline methods.", "published": "2025-08-15 15:15:54", "link": "http://arxiv.org/abs/2508.11530v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Physics-Informed Diffusion Models for Unsupervised Anomaly Detection in Multivariate Time Series", "abstract": "We propose an unsupervised anomaly detection approach based on a\nphysics-informed diffusion model for multivariate time series data. Over the\npast years, diffusion model has demonstrated its effectiveness in forecasting,\nimputation, generation, and anomaly detection in the time series domain. In\nthis paper, we present a new approach for learning the physics-dependent\ntemporal distribution of multivariate time series data using a weighted\nphysics-informed loss during diffusion model training. A weighted\nphysics-informed loss is constructed using a static weight schedule. This\napproach enables a diffusion model to accurately approximate underlying data\ndistribution, which can influence the unsupervised anomaly detection\nperformance. Our experiments on synthetic and real-world datasets show that\nphysics-informed training improves the F1 score in anomaly detection; it\ngenerates better data diversity and log-likelihood. Our model outperforms\nbaseline approaches, additionally, it surpasses prior physics-informed work and\npurely data-driven diffusion models on a synthetic dataset and one real-world\ndataset while remaining competitive on others.", "published": "2025-08-15 15:13:32", "link": "http://arxiv.org/abs/2508.11528v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Finite-Width Neural Tangent Kernels from Feynman Diagrams", "abstract": "Neural tangent kernels (NTKs) are a powerful tool for analyzing deep,\nnon-linear neural networks. In the infinite-width limit, NTKs can easily be\ncomputed for most common architectures, yielding full analytic control over the\ntraining dynamics. However, at infinite width, important properties of training\nsuch as NTK evolution or feature learning are absent. Nevertheless, finite\nwidth effects can be included by computing corrections to the Gaussian\nstatistics at infinite width. We introduce Feynman diagrams for computing\nfinite-width corrections to NTK statistics. These dramatically simplify the\nnecessary algebraic manipulations and enable the computation of layer-wise\nrecursive relations for arbitrary statistics involving preactivations, NTKs and\ncertain higher-derivative tensors (dNTK and ddNTK) required to predict the\ntraining dynamics at leading order. We demonstrate the feasibility of our\nframework by extending stability results for deep networks from preactivations\nto NTKs and proving the absence of finite-width corrections for scale-invariant\nnonlinearities such as ReLU on the diagonal of the Gram matrix of the NTK. We\nvalidate our results with numerical experiments.", "published": "2025-08-15 15:02:40", "link": "http://arxiv.org/abs/2508.11522v1", "categories": ["cs.LG", "hep-th"], "primary_category": "cs.LG"}
{"title": "DiCriTest: Testing Scenario Generation for Decision-Making Agents Considering Diversity and Criticality", "abstract": "The growing deployment of decision-making agents in dynamic environments\nincreases the demand for safety verification. While critical testing scenario\ngeneration has emerged as an appealing verification methodology, effectively\nbalancing diversity and criticality remains a key challenge for existing\nmethods, particularly due to local optima entrapment in high-dimensional\nscenario spaces. To address this limitation, we propose a dual-space guided\ntesting framework that coordinates scenario parameter space and agent behavior\nspace, aiming to generate testing scenarios considering diversity and\ncriticality. Specifically, in the scenario parameter space, a hierarchical\nrepresentation framework combines dimensionality reduction and\nmulti-dimensional subspace evaluation to efficiently localize diverse and\ncritical subspaces. This guides dynamic coordination between two generation\nmodes: local perturbation and global exploration, optimizing critical scenario\nquantity and diversity. Complementarily, in the agent behavior space,\nagent-environment interaction data are leveraged to quantify behavioral\ncriticality/diversity and adaptively support generation mode switching, forming\na closed feedback loop that continuously enhances scenario characterization and\nexploration within the parameter space. Experiments show our framework improves\ncritical scenario generation by an average of 56.23\\% and demonstrates greater\ndiversity under novel parameter-behavior co-driven metrics when tested on five\ndecision-making agents, outperforming state-of-the-art baselines.", "published": "2025-08-15 14:51:45", "link": "http://arxiv.org/abs/2508.11514v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Predicting and Explaining Traffic Crash Severity Through Crash Feature Selection", "abstract": "Motor vehicle crashes remain a leading cause of injury and death worldwide,\nnecessitating data-driven approaches to understand and mitigate crash severity.\nThis study introduces a curated dataset of more than 3 million people involved\nin accidents in Ohio over six years (2017-2022), aggregated to more than 2.3\nmillion vehicle-level records for predictive analysis. The primary contribution\nis a transparent and reproducible methodology that combines Automated Machine\nLearning (AutoML) and explainable artificial intelligence (AI) to identify and\ninterpret key risk factors associated with severe crashes. Using the JADBio\nAutoML platform, predictive models were constructed to distinguish between\nsevere and non-severe crash outcomes. The models underwent rigorous feature\nselection across stratified training subsets, and their outputs were\ninterpreted using SHapley Additive exPlanations (SHAP) to quantify the\ncontribution of individual features. A final Ridge Logistic Regression model\nachieved an AUC-ROC of 85.6% on the training set and 84.9% on a hold-out test\nset, with 17 features consistently identified as the most influential\npredictors. Key features spanned demographic, environmental, vehicle, human,\nand operational categories, including location type, posted speed, minimum\noccupant age, and pre-crash action. Notably, certain traditionally emphasized\nfactors, such as alcohol or drug impairment, were less influential in the final\nmodel compared to environmental and contextual variables. Emphasizing\nmethodological rigor and interpretability over mere predictive performance,\nthis study offers a scalable framework to support Vision Zero with aligned\ninterventions and advanced data-informed traffic safety policy.", "published": "2025-08-15 14:31:26", "link": "http://arxiv.org/abs/2508.11504v1", "categories": ["cs.LG", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Calibrated and uncertain? Evaluating uncertainty estimates in binary classification models", "abstract": "Rigorous statistical methods, including parameter estimation with\naccompanying uncertainties, underpin the validity of scientific discovery,\nespecially in the natural sciences. With increasingly complex data models such\nas deep learning techniques, uncertainty quantification has become exceedingly\ndifficult and a plethora of techniques have been proposed. In this case study,\nwe use the unifying framework of approximate Bayesian inference combined with\nempirical tests on carefully created synthetic classification datasets to\ninvestigate qualitative properties of six different probabilistic machine\nlearning algorithms for class probability and uncertainty estimation: (i) a\nneural network ensemble, (ii) neural network ensemble with conflictual loss,\n(iii) evidential deep learning, (iv) a single neural network with Monte Carlo\nDropout, (v) Gaussian process classification and (vi) a Dirichlet process\nmixture model. We check if the algorithms produce uncertainty estimates which\nreflect commonly desired properties, such as being well calibrated and\nexhibiting an increase in uncertainty for out-of-distribution data points. Our\nresults indicate that all algorithms are well calibrated, but none of the deep\nlearning based algorithms provide uncertainties that consistently reflect lack\nof experimental evidence for out-of-distribution data points. We hope our study\nmay serve as a clarifying example for researchers developing new methods of\nuncertainty estimation for scientific data-driven modeling.", "published": "2025-08-15 13:17:32", "link": "http://arxiv.org/abs/2508.11460v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Multi-Sensory Cognitive Computing for Learning Population-level Brain Connectivity", "abstract": "The generation of connectional brain templates (CBTs) has recently garnered\nsignificant attention for its potential to identify unique connectivity\npatterns shared across individuals. However, existing methods for CBT learning\nsuch as conventional machine learning and graph neural networks (GNNs) are\nhindered by several limitations. These include: (i) poor interpretability due\nto their black-box nature, (ii) high computational cost, and (iii) an exclusive\nfocus on structure and topology, overlooking the cognitive capacity of the\ngenerated CBT. To address these challenges, we introduce mCOCO (multi-sensory\nCOgnitive COmputing), a novel framework that leverages Reservoir Computing (RC)\nto learn population-level functional CBT from BOLD\n(Blood-Oxygen-level-Dependent) signals. RC's dynamic system properties allow\nfor tracking state changes over time, enhancing interpretability and enabling\nthe modeling of brain-like dynamics, as demonstrated in prior literature. By\nintegrating multi-sensory inputs (e.g., text, audio, and visual data), mCOCO\ncaptures not only structure and topology but also how brain regions process\ninformation and adapt to cognitive tasks such as sensory processing, all in a\ncomputationally efficient manner. Our mCOCO framework consists of two phases:\n(1) mapping BOLD signals into the reservoir to derive individual functional\nconnectomes, which are then aggregated into a group-level CBT - an approach, to\nthe best of our knowledge, not previously explored in functional connectivity\nstudies - and (2) incorporating multi-sensory inputs through a cognitive\nreservoir, endowing the CBT with cognitive traits. Extensive evaluations show\nthat our mCOCO-based template significantly outperforms GNN-based CBT in terms\nof centeredness, discriminativeness, topological soundness, and multi-sensory\nmemory retention. Our source code is available at\nhttps://github.com/basiralab/mCOCO.", "published": "2025-08-15 12:38:39", "link": "http://arxiv.org/abs/2508.11436v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Generative Co-Design of Antibody Sequences and Structures via Black-Box Guidance in a Shared Latent Space", "abstract": "Advancements in deep generative models have enabled the joint modeling of\nantibody sequence and structure, given the antigen-antibody complex as context.\nHowever, existing approaches for optimizing complementarity-determining regions\n(CDRs) to improve developability properties operate in the raw data space,\nleading to excessively costly evaluations due to the inefficient search\nprocess. To address this, we propose LatEnt blAck-box Design (LEAD), a\nsequence-structure co-design framework that optimizes both sequence and\nstructure within their shared latent space. Optimizing shared latent codes can\nnot only break through the limitations of existing methods, but also ensure\nsynchronization of different modality designs. Particularly, we design a\nblack-box guidance strategy to accommodate real-world scenarios where many\nproperty evaluators are non-differentiable. Experimental results demonstrate\nthat our LEAD achieves superior optimization performance for both single and\nmulti-property objectives. Notably, LEAD reduces query consumption by a half\nwhile surpassing baseline methods in property optimization. The code is\navailable at https://github.com/EvaFlower/LatEnt-blAck-box-Design.", "published": "2025-08-15 11:59:13", "link": "http://arxiv.org/abs/2508.11424v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "A Remedy for Over-Squashing in Graph Learning via Forman-Ricci Curvature based Graph-to-Hypergraph Structural Lifting", "abstract": "Graph Neural Networks are highly effective at learning from relational data,\nleveraging node and edge features while maintaining the symmetries inherent to\ngraph structures. However, many real-world systems, such as social or\nbiological networks, exhibit complex interactions that are more naturally\nrepresented by higher-order topological domains. The emerging field of\nGeometric and Topological Deep Learning addresses this challenge by introducing\nmethods that utilize and benefit from higher-order structures. Central to TDL\nis the concept of lifting, which transforms data representations from basic\ngraph forms to more expressive topologies before the application of GNN models\nfor learning. In this work, we propose a structural lifting strategy using\nForman-Ricci curvature, which defines an edge-based network characteristic\nbased on Riemannian geometry. Curvature reveals local and global properties of\na graph, such as a network's backbones, i.e. coarse, structure-preserving graph\ngeometries that form connections between major communities - most suitably\nrepresented as hyperedges to model information flows between clusters across\nlarge distances in the network. To this end, our approach provides a remedy to\nthe problem of information distortion in message passing across long distances\nand graph bottlenecks - a phenomenon known in graph learning as over-squashing.", "published": "2025-08-15 10:46:27", "link": "http://arxiv.org/abs/2508.11390v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Fusing Rewards and Preferences in Reinforcement Learning", "abstract": "We present Dual-Feedback Actor (DFA), a reinforcement learning algorithm that\nfuses both individual rewards and pairwise preferences (if available) into a\nsingle update rule. DFA uses the policy's log-probabilities directly to model\nthe preference probability, avoiding a separate reward-modeling step.\nPreferences can be provided by human-annotators (at state-level or\ntrajectory-level) or be synthesized online from Q-values stored in an\noff-policy replay buffer. Under a Bradley-Terry model, we prove that minimizing\nDFA's preference loss recovers the entropy-regularized Soft Actor-Critic (SAC)\npolicy. Our simulation results show that DFA trained on generated preferences\nmatches or exceeds SAC on six control environments and demonstrates a more\nstable training process. With only a semi-synthetic preference dataset under\nBradley-Terry model, our algorithm outperforms reward-modeling reinforcement\nlearning from human feedback (RLHF) baselines in a stochastic GridWorld and\napproaches the performance of an oracle with true rewards.", "published": "2025-08-15 09:56:03", "link": "http://arxiv.org/abs/2508.11363v1", "categories": ["cs.LG", "I.2.6"], "primary_category": "cs.LG"}
{"title": "Harmonized Gradient Descent for Class Imbalanced Data Stream Online Learning", "abstract": "Many real-world data are sequentially collected over time and often exhibit\nskewed class distributions, resulting in imbalanced data streams. While\nexisting approaches have explored several strategies, such as resampling and\nreweighting, for imbalanced data stream learning, our work distinguishes itself\nby addressing the imbalance problem through training modification, particularly\nfocusing on gradient descent techniques. We introduce the harmonized gradient\ndescent (HGD) algorithm, which aims to equalize the norms of gradients across\ndifferent classes. By ensuring the gradient norm balance, HGD mitigates\nunder-fitting for minor classes and achieves balanced online learning. Notably,\nHGD operates in a streamlined implementation process, requiring no data-buffer,\nextra parameters, or prior knowledge, making it applicable to any learning\nmodels utilizing gradient descent for optimization. Theoretical analysis, based\non a few common and mild assumptions, shows that HGD achieves a satisfied\nsub-linear regret bound. The proposed algorithm are compared with the commonly\nused online imbalance learning methods under several imbalanced data stream\nscenarios. Extensive experimental evaluations demonstrate the efficiency and\neffectiveness of HGD in learning imbalanced data streams.", "published": "2025-08-15 09:35:13", "link": "http://arxiv.org/abs/2508.11353v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "A Global Dataset of Location Data Integrity-Assessed Reforestation Efforts", "abstract": "Afforestation and reforestation are popular strategies for mitigating climate\nchange by enhancing carbon sequestration. However, the effectiveness of these\nefforts is often self-reported by project developers, or certified through\nprocesses with limited external validation. This leads to concerns about data\nreliability and project integrity. In response to increasing scrutiny of\nvoluntary carbon markets, this study presents a dataset on global afforestation\nand reforestation efforts compiled from primary (meta-)information and\naugmented with time-series satellite imagery and other secondary data. Our\ndataset covers 1,289,068 planting sites from 45,628 projects spanning 33 years.\nSince any remote sensing-based validation effort relies on the integrity of a\nplanting site's geographic boundary, this dataset introduces a standardized\nassessment of the provided site-level location information, which we summarize\nin one easy-to-communicate key indicator: LDIS -- the Location Data Integrity\nScore. We find that approximately 79\\% of the georeferenced planting sites\nmonitored fail on at least 1 out of 10 LDIS indicators, while 15\\% of the\nmonitored projects lack machine-readable georeferenced data in the first place.\nIn addition to enhancing accountability in the voluntary carbon market, the\npresented dataset also holds value as training data for e.g. computer\nvision-related tasks with millions of linked Sentinel-2 and Planetscope\nsatellite images.", "published": "2025-08-15 09:28:31", "link": "http://arxiv.org/abs/2508.11349v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Conformal Prediction Meets Long-tail Classification", "abstract": "Conformal Prediction (CP) is a popular method for uncertainty quantification\nthat converts a pretrained model's point prediction into a prediction set, with\nthe set size reflecting the model's confidence. Although existing CP methods\nare guaranteed to achieve marginal coverage, they often exhibit imbalanced\ncoverage across classes under long-tail label distributions, tending to over\ncover the head classes at the expense of under covering the remaining tail\nclasses. This under coverage is particularly concerning, as it undermines the\nreliability of the prediction sets for minority classes, even with coverage\nensured on average. In this paper, we propose the Tail-Aware Conformal\nPrediction (TACP) method to mitigate the under coverage of the tail classes by\nutilizing the long-tail structure and narrowing the head-tail coverage gap.\nTheoretical analysis shows that it consistently achieves a smaller head-tail\ncoverage gap than standard methods. To further improve coverage balance across\nall classes, we introduce an extension of TACP: soft TACP (sTACP) via a\nreweighting mechanism. The proposed framework can be combined with various\nnon-conformity scores, and experiments on multiple long-tail benchmark datasets\ndemonstrate the effectiveness of our methods.", "published": "2025-08-15 09:14:46", "link": "http://arxiv.org/abs/2508.11345v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Repetitive TMS-based Identification of Methamphetamine-Dependent Individuals Using EEG Spectra", "abstract": "The impact of repetitive transcranial magnetic stimulation (rTMS) on\nmethamphetamine (METH) users' craving levels is often assessed using\nquestionnaires. This study explores the feasibility of using neural signals to\nobtain more objective results. EEG signals recorded from 20 METH-addicted\nparticipants Before and After rTMS (MBT and MAT) and from 20 healthy\nparticipants (HC) are analyzed. In each EEG paradigm, participants are shown 15\nMETH-related and 15 neutral pictures randomly, and the relative band power\n(RBP) of each EEG sub-band frequency is derived. The average RBP across all 31\nchannels, as well as individual brain regions, is analyzed. Statistically,\nMAT's alpha, beta, and gamma RBPs are more like those of HC compared to MBT, as\nindicated by the power topographies. Utilizing a random forest (RF), the gamma\nRBP is identified as the optimal frequency band for distinguishing between MBT\nand HC with a 90% accuracy. The performance of classifying MAT versus HC is\nlower than that of MBT versus HC, suggesting that the efficacy of rTMS can be\nvalidated using RF with gamma RBP. Furthermore, the gamma RBP recorded by the\nTP10 and CP2 channels dominates the classification task of MBT versus HC when\nreceiving METH-related image cues. The gamma RBP during exposure to\nMETH-related cues can serve as a biomarker for distinguishing between MBT and\nHC and for evaluating the effectiveness of rTMS. Therefore, real-time\nmonitoring of gamma RBP variations holds promise as a parameter for\nimplementing a customized closed-loop neuromodulation system for treating METH\naddiction.", "published": "2025-08-15 08:31:10", "link": "http://arxiv.org/abs/2508.11312v1", "categories": ["q-bio.NC", "cs.LG", "eess.SP"], "primary_category": "q-bio.NC"}
{"title": "Approximating the universal thermal climate index using sparse regression with orthogonal polynomials", "abstract": "This article explores novel data-driven modeling approaches for analyzing and\napproximating the Universal Thermal Climate Index (UTCI), a\nphysiologically-based metric integrating multiple atmospheric variables to\nassess thermal comfort. Given the nonlinear, multivariate structure of UTCI, we\ninvestigate symbolic and sparse regression techniques as tools for\ninterpretable and efficient function approximation. In particular, we highlight\nthe benefits of using orthogonal polynomial bases-such as Legendre\npolynomials-in sparse regression frameworks, demonstrating their advantages in\nstability, convergence, and hierarchical interpretability compared to standard\npolynomial expansions. We demonstrate that our models achieve significantly\nlower root-mean squared losses than the widely used sixth-degree polynomial\nbenchmark-while using the same or fewer parameters. By leveraging Legendre\npolynomial bases, we construct models that efficiently populate a Pareto front\nof accuracy versus complexity and exhibit stable, hierarchical coefficient\nstructures across varying model capacities. Training on just 20% of the data,\nour models generalize robustly to the remaining 80%, with consistent\nperformance under bootstrapping. The decomposition effectively approximates the\nUTCI as a Fourier-like expansion in an orthogonal basis, yielding results near\nthe theoretical optimum in the L2 (least squares) sense. We also connect these\nfindings to the broader context of equation discovery in environmental\nmodeling, referencing probabilistic grammar-based methods that enforce domain\nconsistency and compactness in symbolic expressions. Taken together, these\nresults illustrate how combining sparsity, orthogonality, and symbolic\nstructure enables robust, interpretable modeling of complex environmental\nindices like UTCI - and significantly outperforms the state-of-the-art\napproximation in both accuracy and efficiency.", "published": "2025-08-15 08:22:01", "link": "http://arxiv.org/abs/2508.11307v1", "categories": ["physics.ao-ph", "cs.LG", "physics.data-an"], "primary_category": "physics.ao-ph"}
{"title": "Uniform convergence for Gaussian kernel ridge regression", "abstract": "This paper establishes the first polynomial convergence rates for Gaussian\nkernel ridge regression (KRR) with a fixed hyperparameter in both the uniform\nand the $L^{2}$-norm. The uniform convergence result closes a gap in the\ntheoretical understanding of KRR with the Gaussian kernel, where no such rates\nwere previously known. In addition, we prove a polynomial $L^{2}$-convergence\nrate in the case, where the Gaussian kernel's width parameter is fixed. This\nalso contributes to the broader understanding of smooth kernels, for which\npreviously only sub-polynomial $L^{2}$-rates were known in similar settings.\nTogether, these results provide new theoretical justification for the use of\nGaussian KRR with fixed hyperparameters in nonparametric regression.", "published": "2025-08-15 07:20:31", "link": "http://arxiv.org/abs/2508.11274v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Enhancing Interactive Voting-Based Map Matching: Improving Efficiency and Robustness for Heterogeneous GPS Trajectories", "abstract": "This paper presents an enhanced version of the Interactive Voting-Based Map\nMatching algorithm, designed to efficiently process trajectories with varying\nsampling rates. The main aim is to reconstruct GPS trajectories with high\naccuracy, independent of input data quality. Building upon the original\nalgorithm, developed exclusively for aligning GPS signals to road networks, we\nextend its capabilities by integrating trajectory imputation. Our improvements\nalso include the implementation of a distance-bounded interactive voting\nstrategy to reduce computational complexity, as well as modifications to\naddress missing data in the road network. Furthermore, we incorporate a\ncustom-built asset derived from OpenStreetMap, enabling this approach to be\nsmoothly applied in any geographic region covered by OpenStreetMap's road\nnetwork. These advancements preserve the core strengths of the original\nalgorithm while significantly extending its applicability to diverse real-world\nscenarios.", "published": "2025-08-15 05:51:59", "link": "http://arxiv.org/abs/2508.11235v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Air Quality PM2.5 Index Prediction Model Based on CNN-LSTM", "abstract": "With the intensification of global climate change, accurate prediction of air\nquality indicators, especially PM2.5 concentration, has become increasingly\nimportant in fields such as environmental protection, public health, and urban\nmanagement. To address this, we propose an air quality PM2.5 index prediction\nmodel based on a hybrid CNN-LSTM architecture. The model effectively combines\nConvolutional Neural Networks (CNN) for local spatial feature extraction and\nLong Short-Term Memory (LSTM) networks for modeling temporal dependencies in\ntime series data. Using a multivariate dataset collected from an industrial\narea in Beijing between 2010 and 2015 -- which includes hourly records of PM2.5\nconcentration, temperature, dew point, pressure, wind direction, wind speed,\nand precipitation -- the model predicts the average PM2.5 concentration over\n6-hour intervals. Experimental results show that the model achieves a root mean\nsquare error (RMSE) of 5.236, outperforming traditional time series models in\nboth accuracy and generalization. This demonstrates its strong potential in\nreal-world applications such as air pollution early warning systems. However,\ndue to the complexity of multivariate inputs, the model demands high\ncomputational resources, and its ability to handle diverse atmospheric factors\nstill requires optimization. Future work will focus on enhancing scalability\nand expanding support for more complex multivariate weather prediction tasks.", "published": "2025-08-15 04:46:25", "link": "http://arxiv.org/abs/2508.11215v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Borrowing From the Future: Enhancing Early Risk Assessment through Contrastive Learning", "abstract": "Risk assessments for a pediatric population are often conducted across\nmultiple stages. For example, clinicians may evaluate risks prenatally, at\nbirth, and during Well-Child visits. Although predictions made at later stages\ntypically achieve higher precision, it is clinically desirable to make reliable\nrisk assessments as early as possible. Therefore, this study focuses on\nimproving prediction performance in early-stage risk assessments. Our solution,\n\\textbf{Borrowing From the Future (BFF)}, is a contrastive multi-modal\nframework that treats each time window as a distinct modality. In BFF, a model\nis trained on all available data throughout the time while performing a risk\nassessment using up-to-date information. This contrastive framework allows the\nmodel to ``borrow'' informative signals from later stages (e.g., Well-Child\nvisits) to implicitly supervise the learning at earlier stages (e.g.,\nprenatal/birth stages). We validate BFF on two real-world pediatric outcome\nprediction tasks, demonstrating consistent improvements in early risk\nassessments. The code is available at https://github.com/scotsun/bff.", "published": "2025-08-15 04:40:21", "link": "http://arxiv.org/abs/2508.11210v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Meta-learning Structure-Preserving Dynamics", "abstract": "Structure-preserving approaches to dynamics modeling have demonstrated great\npotential for modeling physical systems due to their strong inductive biases\nthat enforce conservation laws and dissipative behavior. However, the resulting\nmodels are typically trained for fixed system configurations, requiring\nexplicit knowledge of system parameters as well as costly retraining for each\nnew set of parameters -- a major limitation in many-query or parameter-varying\nscenarios. Meta-learning offers a potential solution, but existing approaches\nlike optimization-based meta-learning often suffer from training instability or\nlimited generalization capability. Inspired by ideas from computer vision, we\nintroduce a modulation-based meta-learning framework that directly conditions\nstructure-preserving models on compact latent representations of potentially\nunknown system parameters, avoiding the need for gray-box system knowledge and\nexplicit optimization during adaptation. Through the application of novel\nmodulation strategies to parametric energy-conserving and dissipative systems,\nwe enable scalable and generalizable learning across parametric families of\ndynamical systems. Experiments on standard benchmark problems demonstrate that\nour approach achieves accurate predictions in few-shot learning settings,\nwithout compromising on the essential physical constraints necessary for\ndynamical stability and effective generalization performance across parameter\nspace.", "published": "2025-08-15 04:30:27", "link": "http://arxiv.org/abs/2508.11205v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "The Role of Entanglement in Quantum Reservoir Computing with Coupled Kerr Nonlinear Oscillators", "abstract": "Quantum Reservoir Computing (QRC) uses quantum dynamics to efficiently\nprocess temporal data. In this work, we investigate a QRC framework based on\ntwo coupled Kerr nonlinear oscillators, a system well-suited for time-series\nprediction tasks due to its complex nonlinear interactions and potentially\nhigh-dimensional state space. We explore how its performance in time-series\nprediction depends on key physical parameters: input drive strength, Kerr\nnonlinearity, and oscillator coupling, and analyze the role of entanglement in\nimproving the reservoir's computational performance, focusing on its effect on\npredicting non-trivial time series. Using logarithmic negativity to quantify\nentanglement and normalized root mean square error (NRMSE) to evaluate\npredictive accuracy, our results suggest that entanglement provides a\ncomputational advantage on average-up to a threshold in the input\nfrequency-that persists under some levels of dissipation and dephasing. In\nparticular, we find that higher dissipation rates can enhance performance.\nWhile the entanglement advantage manifests as improvements in both average and\nworst-case performance, it does not lead to improvements in the best-case\nerror. These findings contribute to the broader understanding of quantum\nreservoirs for high performance, efficient quantum machine learning and\ntime-series forecasting.", "published": "2025-08-15 02:59:02", "link": "http://arxiv.org/abs/2508.11175v1", "categories": ["quant-ph", "cs.LG", "eess.SP"], "primary_category": "quant-ph"}
{"title": "Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated Learning", "abstract": "The Internet of Things (IoT) ecosystem produces massive volumes of multimodal\ndata from diverse sources, including sensors, cameras, and microphones. With\nadvances in edge intelligence, IoT devices have evolved from simple data\nacquisition units into computationally capable nodes, enabling localized\nprocessing of heterogeneous multimodal data. This evolution necessitates\ndistributed learning paradigms that can efficiently handle such data.\nFurthermore, the continuous nature of data generation and the limited storage\ncapacity of edge devices demand an online learning framework. Multimodal Online\nFederated Learning (MMO-FL) has emerged as a promising approach to meet these\nrequirements. However, MMO-FL faces new challenges due to the inherent\ninstability of IoT devices, which often results in modality quantity and\nquality imbalance (QQI) during data collection. In this work, we systematically\ninvestigate the impact of QQI within the MMO-FL framework and present a\ncomprehensive theoretical analysis quantifying how both types of imbalance\ndegrade learning performance. To address these challenges, we propose the\nModality Quantity and Quality Rebalanced (QQR) algorithm, a prototype learning\nbased method designed to operate in parallel with the training process.\nExtensive experiments on two real-world multimodal datasets show that the\nproposed QQR algorithm consistently outperforms benchmarks under modality\nimbalance conditions with promising learning performance.", "published": "2025-08-15 02:13:39", "link": "http://arxiv.org/abs/2508.11159v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Towards the Next-generation Bayesian Network Classifiers", "abstract": "Bayesian network classifiers provide a feasible solution to tabular data\nclassification, with a number of merits like high time and memory efficiency,\nand great explainability. However, due to the parameter explosion and data\nsparsity issues, Bayesian network classifiers are restricted to low-order\nfeature dependency modeling, making them struggle in extrapolating the\noccurrence probabilities of complex real-world data. In this paper, we propose\na novel paradigm to design high-order Bayesian network classifiers, by learning\ndistributional representations for feature values, as what has been done in\nword embedding and graph representation learning. The learned distributional\nrepresentations are encoded with the semantic relatedness between different\nfeatures through their observed co-occurrence patterns in training data, which\nthen serve as a hallmark to extrapolate the occurrence probabilities of new\ntest samples. As a classifier design realization, we remake the K-dependence\nBayesian classifier (KDB) by extending it into a neural version, i.e.,\nNeuralKDB, where a novel neural network architecture is designed to learn\ndistributional representations of feature values and parameterize the\nconditional probabilities between interdependent features. A stochastic\ngradient descent based algorithm is designed to train the NeuralKDB model\nefficiently. Extensive classification experiments on 60 UCI datasets\ndemonstrate that the proposed NeuralKDB classifier excels in capturing\nhigh-order feature dependencies and significantly outperforms the conventional\nBayesian network classifiers, as well as other competitive classifiers,\nincluding two neural network based classifiers without distributional\nrepresentation learning.", "published": "2025-08-15 01:31:06", "link": "http://arxiv.org/abs/2508.11145v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "CTRL Your Shift: Clustered Transfer Residual Learning for Many Small Datasets", "abstract": "Machine learning (ML) tasks often utilize large-scale data that is drawn from\nseveral distinct sources, such as different locations, treatment arms, or\ngroups. In such settings, practitioners often desire predictions that not only\nexhibit good overall accuracy, but also remain reliable within each source and\npreserve the differences that matter across sources. For instance, several\nasylum and refugee resettlement programs now use ML-based employment\npredictions to guide where newly arriving families are placed within a host\ncountry, which requires generating informative and differentiated predictions\nfor many and often small source locations. However, this task is made\nchallenging by several common characteristics of the data in these settings:\nthe presence of numerous distinct data sources, distributional shifts between\nthem, and substantial variation in sample sizes across sources. This paper\nintroduces Clustered Transfer Residual Learning (CTRL), a meta-learning method\nthat combines the strengths of cross-domain residual learning and adaptive\npooling/clustering in order to simultaneously improve overall accuracy and\npreserve source-level heterogeneity. We provide theoretical results that\nclarify how our objective navigates the trade-off between data quantity and\ndata quality. We evaluate CTRL alongside other state-of-the-art benchmarks on 5\nlarge-scale datasets. This includes a dataset from the national asylum program\nin Switzerland, where the algorithmic geographic assignment of asylum seekers\nis currently being piloted. CTRL consistently outperforms the benchmarks across\nseveral key metrics and when using a range of different base learners.", "published": "2025-08-15 01:27:17", "link": "http://arxiv.org/abs/2508.11144v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "A Dynamically Weighted ADMM Framework for Byzantine Resilience", "abstract": "The alternating direction of multipliers method (ADMM) is a popular method to\nsolve distributed consensus optimization utilizing efficient communication\namong various nodes in the network. However, in the presence of faulty or\nattacked nodes, even a small perturbation (or sharing false data) during the\ncommunication can lead to divergence of the solution. To address this issue, in\nthis work we consider ADMM under the effect of Byzantine threat, where an\nunknown subset of nodes is subject to Byzantine attacks or faults. We propose\nDynamically Weighted ADMM (DW-ADMM), a novel variant of ADMM that uses dynamic\nweights on the edges of the network, thus promoting resilient distributed\noptimization. We establish that the proposed method (i) produces a nearly\nidentical solution to conventional ADMM in the error-free case, and (ii)\nguarantees a bounded solution with respect to the global minimizer, even under\nByzantine threat. Finally, we demonstrate the effectiveness of our proposed\nalgorithm using an illustrative numerical simulation.", "published": "2025-08-15 16:27:31", "link": "http://arxiv.org/abs/2508.11572v1", "categories": ["math.OC", "cs.MA", "cs.SY", "eess.SY"], "primary_category": "math.OC"}
{"title": "Tapas are free! Training-Free Adaptation of Programmatic Agents via LLM-Guided Program Synthesis in Dynamic Environments", "abstract": "Autonomous agents in safety-critical applications must continuously adapt to\ndynamic conditions without compromising performance and reliability. This work\nintroduces TAPA (Training-free Adaptation of Programmatic Agents), a novel\nframework that positions large language models (LLMs) as intelligent moderators\nof the symbolic action space. Unlike prior programmatic agents that typically\ngenerate a monolithic policy program or rely on fixed symbolic action sets,\nTAPA synthesizes and adapts modular programs for individual high-level actions,\nreferred to as logical primitives. By decoupling strategic intent from\nexecution, TAPA enables meta-agents to operate over an abstract, interpretable\naction space while the LLM dynamically generates, composes, and refines\nsymbolic programs tailored to each primitive. Extensive experiments across\ncybersecurity and swarm intelligence domains validate TAPA's effectiveness. In\nautonomous DDoS defense scenarios, TAPA achieves 77.7% network uptime while\nmaintaining near-perfect detection accuracy in unknown dynamic environments. In\nswarm intelligence formation control under environmental and adversarial\ndisturbances, TAPA consistently preserves consensus at runtime where baseline\nmethods fail completely. This work promotes a paradigm shift for autonomous\nsystem design in evolving environments, from policy adaptation to dynamic\naction adaptation.", "published": "2025-08-15 12:02:46", "link": "http://arxiv.org/abs/2508.11425v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "FACET:Teacher-Centred LLM-Based Multi-Agent Systems-Towards Personalized Educational Worksheets", "abstract": "The increasing heterogeneity of student populations poses significant\nchallenges for teachers, particularly in mathematics education, where\ncognitive, motivational, and emotional differences strongly influence learning\noutcomes. While AI-driven personalization tools have emerged, most remain\nperformance-focused, offering limited support for teachers and neglecting\nbroader pedagogical needs. This paper presents the FACET framework, a\nteacher-facing, large language model (LLM)-based multi-agent system designed to\ngenerate individualized classroom materials that integrate both cognitive and\nmotivational dimensions of learner profiles. The framework comprises three\nspecialized agents: (1) learner agents that simulate diverse profiles\nincorporating topic proficiency and intrinsic motivation, (2) a teacher agent\nthat adapts instructional content according to didactical principles, and (3)\nan evaluator agent that provides automated quality assurance. We tested the\nsystem using authentic grade 8 mathematics curriculum content and evaluated its\nfeasibility through a) automated agent-based assessment of output quality and\nb) exploratory feedback from K-12 in-service teachers. Results from ten\ninternal evaluations highlighted high stability and alignment between generated\nmaterials and learner profiles, and teacher feedback particularly highlighted\nstructure and suitability of tasks. The findings demonstrate the potential of\nmulti-agent LLM architectures to provide scalable, context-aware\npersonalization in heterogeneous classroom settings, and outline directions for\nextending the framework to richer learner profiles and real-world classroom\ntrials.", "published": "2025-08-15 11:10:40", "link": "http://arxiv.org/abs/2508.11401v1", "categories": ["cs.HC", "cs.MA"], "primary_category": "cs.HC"}
{"title": "Defending a City from Multi-Drone Attacks: A Sequential Stackelberg Security Games Approach", "abstract": "To counter an imminent multi-drone attack on a city, defenders have deployed\ndrones across the city. These drones must intercept/eliminate the threat, thus\nreducing potential damage from the attack. We model this as a Sequential\nStackelberg Security Game, where the defender first commits to a mixed\nsequential defense strategy, and the attacker then best responds. We develop an\nefficient algorithm called S2D2, which outputs a defense strategy. We\ndemonstrate the efficacy of S2D2 in extensive experiments on data from 80 real\ncities, improving the performance of the defender in comparison to greedy\nheuristics based on prior works. We prove that under some reasonable\nassumptions about the city structure, S2D2 outputs an approximate Strong\nStackelberg Equilibrium (SSE) with a convenient structure.", "published": "2025-08-15 10:28:15", "link": "http://arxiv.org/abs/2508.11380v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "A multigrid method for CutFEM and its implementation on GPU", "abstract": "We present a multigrid method for an unfitted finite element discretization\nof the Dirichlet boundary value problem. The discretization employs Nitsche's\nmethod to implement the boundary condition and additional face based ghost\npenalties for stabilization. We apply standard intergrid operators, relying on\nthe fact that the relevant domain of computation does not grow under mesh\nrefinement. The smoother is a parallel implementation of the multiplicative\nvertex-patch smoother with inconsistent treatment of ghost penalties. Our\ncomputational results show that we obtain a fast converging method.\nFurthermore, runtime comparison to fitted methods show that the losses are\nmoderate although many optimizations for Cartesian vertex patches cannot be\napplied on cut patches.", "published": "2025-08-15 17:17:24", "link": "http://arxiv.org/abs/2508.11608v1", "categories": ["math.NA", "cs.NA", "65N55, 65Y05, 65Y10"], "primary_category": "math.NA"}
{"title": "Augmented Lagrangian Solvers for Poroelasticity with Fracture Contact Mechanics", "abstract": "In the subsurface, fractures and the surrounding porous rock can deform in\ninteraction with fluid flow. Advanced mathematical models governing these\ncoupled processes typically combine fluid flow, poroelasticity, and fracture\ncontact mechanics, representing fractures as co-dimension one objects within\nthe porous medium. The resulting system of equations is complex and highly\nnonlinear. As a result, convergence issues with nonlinear solvers are common,\nwhich hinders the practical implementation of such models.\n  One particular source of difficulty for the nonlinear solvers comes from the\nfracture contact mechanics, due to its inherently nonsmooth character. In this\npaper, we investigate solvers based on the augmented Lagrangian formulation of\nthe frictional contact problem. This includes two classical solvers, namely the\ngeneralized Newton method (using complementarity functions) and the return map\nmethod. In addition, we propose a new augmented Lagrangian solver that combines\nthe two approaches.\n  Numerical experiments in two and three dimensions are conducted to assess the\nperformance of the solvers on problems of mixed-dimensional poromechanics with\nfracture contact mechanics. The experiments indicate that for simpler setups,\nthe solvers behave quite predictably and in accordance with established\nheuristics from the contact mechanics literature. However, as the complexity of\nthe problem increases, the solvers become more unpredictable, and break with\nthese heuristics. In particular, the two classical solvers become highly\nsensitive to the augmentation parameter. The new solver converges across a\nlarger range of this parameter in most cases, but nevertheless also displays\nunpredictable behavior.", "published": "2025-08-15 14:39:23", "link": "http://arxiv.org/abs/2508.11508v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "A Monotonicity-Based Regularization Approach to Shape Reconstruction for the Helmholtz Equation", "abstract": "We consider an inverse boundary value problem for determining unknown\nscatterers, which is governed by the Helmholtz equation in a bounded domain. To\naddress this, we develop a novel convex data-fitting formulation that is\ncapable of reconstructing the shape of the unknown scatterers.Our formulation\nis based on a monotonicity relation between the scattering index and boundary\nmeasurements. We use this relation to obtain a pixel-wise constraint on the\nunknown scattering index, and then minimize a data-fitting functional defined\nas the sum of all positive eigenvalues of a linearized residual operator. The\nmain advantages of our new approach are that this is a convex data-fitting\nproblem that does not require additional PDE solutions. The global convergence\nand stability of the method are rigorously established to demonstrate the\ntheoretical soundness. In addition, several numerical experiments are conducted\nto verify the effectiveness of the proposed approach in shape reconstruction.", "published": "2025-08-15 12:40:46", "link": "http://arxiv.org/abs/2508.11439v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Virtual element method for thermomechanical analysis of electronic packaging structures with multi-scale features", "abstract": "This paper presents two approaches: the virtual element method (VEM) and the\nstabilization-free virtual element method (SFVEM) for analyzing\nthermomechanical behavior in electronic packaging structures with geometric\nmulti-scale features. Since the virtual element method allows the use of\narbitrary polygonal elements, the inherent mesh flexibility of VEM allows\nlocalized mesh modifications without affecting global mesh structure, making it\nparticularly effective for the analysis of electronic packaging reliability\ninvolving complex geometries and multiple geometric scales. The approach\nimplements a novel non-matching mesh generation strategy that strategically\ncombines polygonal meshes for complex small-scale regions with regular\nquadrilateral meshes for larger domains. The VEM formulation addresses both\nheat conduction and thermomechanical coupling problems, with comprehensive\nverification through analytical benchmarks and practical electronic packaging\ncase studies, including Through-Silicon Via (TSV), Ball Grid Array (BGA), and\nPlastic Ball Grid Array (PBGA) structures. Results demonstrate that the method\naccurately captures stress concentrations at material interfaces and provides\nreliable thermal and mechanical response predictions. Some MATLAB codes for the\nnumerical examples are provided at\nhttps://github.com/yanpeng-gong/VEM-electronic-packaging and on the VEMhub\nwebsite (www.vemhub.com).", "published": "2025-08-15 11:26:29", "link": "http://arxiv.org/abs/2508.11410v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Combining Nonlinear FETI-DP Methods and Quasi-Newton Methods using an SQP Approach", "abstract": "The combination of nonlinear FETI-DP (Dual Primal Finite Element Tearing and\nInterconnecting) and Quasi-Newton methods using a sequential quadratic\nprogramming (SQP) approach is considered. Nonlinear FETI-DP methods are\nparallel iterative solution methods for nonlinear finite element problems,\nbased on divide and conquer, using Lagrange multipliers. In the method, we use\nQuasi-Newton approximations of Hessian for the quadratic programs, where the\ninitial approximation uses the exact Hessian. To accelerate the convergence, we\nrecompute the initial Hessian and restart the Quasi-Newton approximation. We\nprovide numerical experiments using homogeneous model problems from nonlinear\nstructural mechanics.", "published": "2025-08-15 08:26:20", "link": "http://arxiv.org/abs/2508.11309v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "AirBreath Sensing: Protecting Over-the-Air Distributed Sensing Against Interference", "abstract": "A distinctive function of sixth-generation (6G) networks is the integration\nof distributed sensing and edge artificial intelligence (AI) to enable\nintelligent perception of the physical world. This resultant platform, termed\nintegrated sensing and edge AI (ISEA), is envisioned to enable a broad spectrum\nof Internet-of-Things (IoT) applications, including remote surgery, autonomous\ndriving, and holographic telepresence. Recently, the communication bottleneck\nconfronting the implementation of an ISEA system is overcome by the development\nof over-the-air computing (AirComp) techniques, which facilitate simultaneous\naccess through over-the-air data feature fusion. Despite its advantages,\nAirComp with uncoded transmission remains vulnerable to interference. To tackle\nthis challenge, we propose AirBreath sensing, a spectrum-efficient framework\nthat cascades feature compression and spread spectrum to mitigate interference\nwithout bandwidth expansion. This work reveals a fundamental tradeoff between\nthese two operations under a fixed bandwidth constraint: increasing the\ncompression ratio may reduce sensing accuracy but allows for more aggressive\ninterference suppression via spread spectrum, and vice versa. This tradeoff is\nregulated by a key variable called breathing depth, defined as the feature\nsubspace dimension that matches the processing gain in spread spectrum. To\noptimally control the breathing depth, we mathematically characterize and\noptimize this aforementioned tradeoff by designing a tractable surrogate for\nsensing accuracy, measured by classification discriminant gain (DG).\nExperimental results on real datasets demonstrate that AirBreath sensing\neffectively mitigates interference in ISEA systems, and the proposed control\nalgorithm achieves near-optimal performance as benchmarked with a brute-force\nsearch.", "published": "2025-08-15 07:03:11", "link": "http://arxiv.org/abs/2508.11267v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Polynomial preserving recovery for PHT-splines", "abstract": "We propose a polynomial preserving recovery method for PHT-splines within\nisogeometric analysis to obtain more accurate gradient approximations. The\nmethod fully exploits the local interpolation properties of PHT-splines and\navoids the need for information on gradient superconvergent points. By\nleveraging the superconvergence argument of difference quotients and the\ninterior error estimate, we establish the superconvergence property of the\nrecovered gradient on translation invariant meshes. As a byproduct, a\nrecovery-based a posteriori error estimator is developed for adaptive\nrefinement. Numerical results confirm the theoretical findings and demonstrate\nthe effectiveness of the proposed method.", "published": "2025-08-15 05:46:59", "link": "http://arxiv.org/abs/2508.11233v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Two intriguing variants of the AAA algorithm for rational approximation", "abstract": "We consider the problem of finding a rational function in barycentric form to\napproximate a given function or data set in $\\mathbb{R}$ or $\\mathbb{C}$. The\nfamous AAA algorithm, introduced in 2018, constructs such a rational function:\nthe barycentric weights are the entries of the final right singular vector of a\nLoewner matrix with more rows than columns. We present two variants of the AAA\nalgorithm, inspired by two intriguing quotations from the original paper. In\nthe first, which we call AAAsmooth, we take the barycentric weights to be a\ncomplex linear combination of the last two right singular vectors, which\neliminates the problem of spurious poles in real-valued problems and yields\nsmoother convergence curves. In the second, AAAbudget, we incorporate first\nderivative information. This allows us to use a smaller, square alternative to\nthe Loewner matrix, so the SVDs are cheaper while the resulting approximant is\nsimilar to the result of standard AAA. We present numerical tests showing that\nwhile both variants behave fairly similarly to standard AAA, AAAsmooth can give\nsomewhat better results and AAAbudget can be much faster.", "published": "2025-08-15 02:38:35", "link": "http://arxiv.org/abs/2508.11169v1", "categories": ["math.NA", "cs.NA", "65D15, 30C10", "G.1.2"], "primary_category": "math.NA"}
{"title": "Goal-Oriented Low-Rank Tensor Decompositions for Numerical Simulation Data", "abstract": "We introduce a new low-dimensional model of high-dimensional numerical\nsimulation data based on low-rank tensor decompositions. Our new model aims to\nminimize differences between the model data and simulation data as well as\nfunctions of the model data and functions of the simulation data. This novel\napproach to dimensionality reduction of simulation data provides a means of\ndirectly incorporating quantities of interests and invariants associated with\nconservation principles associated with the simulation data into the\nlow-dimensional model, thus enabling more accurate analysis of the simulation\nwithout requiring access to the full set of high-dimensional data.\nComputational results of applying this approach to two standard low-rank tensor\ndecompositions of data arising from simulation of combustion and plasma physics\nare presented.", "published": "2025-08-15 01:13:15", "link": "http://arxiv.org/abs/2508.11139v1", "categories": ["math.NA", "cs.NA", "15A69, 65F55"], "primary_category": "math.NA"}
{"title": "Stealing Accuracy: Predicting Day-ahead Electricity Prices with Temporal Hierarchy Forecasting (THieF)", "abstract": "We introduce the concept of temporal hierarchy forecasting (THieF) in\npredicting day-ahead electricity prices and show that reconciling forecasts for\nhourly products, 2- to 12-hour blocks, and baseload contracts significantly (up\nto 13%) improves accuracy at all levels. These results remain consistent\nthroughout a challenging 4-year test period (2021-2024) in the German power\nmarket and across model architectures, including linear regression, a shallow\nneural network, gradient boosting, and a state-of-the-art transformer. Given\nthat (i) trading of block products is becoming more common and (ii) the\ncomputational cost of reconciliation is comparable to that of predicting hourly\nprices alone, we recommend using it in daily forecasting practice.", "published": "2025-08-15 10:13:51", "link": "http://arxiv.org/abs/2508.11372v1", "categories": ["q-fin.ST", "econ.EM"], "primary_category": "q-fin.ST"}
{"title": "Enhancing In-the-Wild Speech Emotion Conversion with Resynthesis-based Duration Modeling", "abstract": "Speech Emotion Conversion aims to modify the emotion expressed in input\nspeech while preserving lexical content and speaker identity. Recently,\ngenerative modeling approaches have shown promising results in changing local\nacoustic properties such as fundamental frequency, spectral envelope and\nenergy, but often lack the ability to control the duration of sounds. To\naddress this, we propose a duration modeling framework using resynthesis-based\ndiscrete content representations, enabling modification of speech duration to\nreflect target emotions and achieve controllable speech rates without using\nparallel data. Experimental results reveal that the inclusion of the proposed\nduration modeling framework significantly enhances emotional expressiveness, in\nthe in-the-wild MSP-Podcast dataset. Analyses show that low-arousal emotions\ncorrelate with longer durations and slower speech rates, while high-arousal\nemotions produce shorter, faster speech.", "published": "2025-08-15 15:26:58", "link": "http://arxiv.org/abs/2508.11535v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Speech Emotion Recognition Using Fine-Tuned DWFormer:A Study on Track 1 of the IERPChallenge 2024", "abstract": "The field of artificial intelligence has a strong interest in the topic of\nemotion recognition. The majority of extant emotion recognition models are\noriented towards enhancing the precision of discrete emotion label prediction.\nGiven the direct relationship between human personality and emotion, as well as\nthe significant inter-individual differences in subjective emotional\nexpression, the IERP Challenge 2024 incorporates personality traits into\nemotion recognition research. This paper presents the Fosafer submissions to\nthe Track 1 of the IERP Challenge 2024. This task primarily concerns the\nrecognition of emotions in audio, while also providing text and audio features.\nIn Track 1, we utilized exclusively audio-based features and fine-tuned a\npre-trained speech emotion recognition model, DWFormer, through the integration\nof data augmentation and score fusion strategies, thereby achieving the first\nplace among the participating teams.", "published": "2025-08-15 10:13:43", "link": "http://arxiv.org/abs/2508.11371v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Mitigating Category Imbalance: Fosafer System for the Multimodal Emotion and Intent Joint Understanding Challenge", "abstract": "This paper presents Fosafer approach to the Track 2 Mandarin in the\nMultimodal Emotion and Intent Joint Understandingchallenge, which focuses on\nachieving joint recognition of emotion and intent in Mandarin, despite the\nissue of category imbalance. To alleviate this issue, we use a variety of data\naugmentation techniques across text, video, and audio modalities. Additionally,\nwe introduce the SampleWeighted Focal Contrastive loss, designed to address the\nchallenges of recognizing minority class samples and those that are\nsemantically similar but difficult to distinguish. Moreover, we fine-tune the\nHubert model to adapt the emotion and intent joint recognition. To mitigate\nmodal competition, we introduce a modal dropout strategy. For the final\npredictions, a plurality voting approach is used to determine the results. The\nexperimental results demonstrate the effectiveness of our method, which\nachieves the second-best performance in the Track 2 Mandarin challenge.", "published": "2025-08-15 09:55:16", "link": "http://arxiv.org/abs/2508.11362v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MoE-TTS: Enhancing Out-of-Domain Text Understanding for Description-based TTS via Mixture-of-Experts", "abstract": "Description-based text-to-speech (TTS) models exhibit strong performance on\nin-domain text descriptions, i.e., those encountered during training. However,\nin real-world applications, the diverse range of user-generated descriptions\ninevitably introduces numerous out-of-domain inputs that challenge the text\nunderstanding capabilities of these systems. To address this issue, we propose\nMoE-TTS, a description-based TTS model designed to enhance the understanding of\nout-of-domain text descriptions. MoE-TTS employs a modality-based\nmixture-of-experts (MoE) approach to augment a pre-trained textual large\nlanguage model (LLM) with a set of specialized weights adapted to the speech\nmodality while maintaining the original LLM frozen during training. This\napproach allows MoE-TTS to effectively leverage the pre-trained knowledge and\ntext understanding abilities of textual LLMs. Our experimental results indicate\nthat: first, even the most advanced closed-source commercial products can be\nchallenged by carefully designed out-of-domain description test sets; second,\nMoE-TTS achieves superior performance in generating speech that more accurately\nreflects the descriptions. We encourage readers to listen to the demos at\nhttps://welkinyang.github.io/MoE-TTS/.", "published": "2025-08-15 08:53:56", "link": "http://arxiv.org/abs/2508.11326v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "EmoSSLSphere: Multilingual Emotional Speech Synthesis with Spherical Vectors and Discrete Speech Tokens", "abstract": "This paper introduces EmoSSLSphere, a novel framework for multilingual\nemotional text-to-speech (TTS) synthesis that combines spherical emotion\nvectors with discrete token features derived from self-supervised learning\n(SSL). By encoding emotions in a continuous spherical coordinate space and\nleveraging SSL-based representations for semantic and acoustic modeling,\nEmoSSLSphere enables fine-grained emotional control, effective cross-lingual\nemotion transfer, and robust preservation of speaker identity. We evaluate\nEmoSSLSphere on English and Japanese corpora, demonstrating significant\nimprovements in speech intelligibility, spectral fidelity, prosodic\nconsistency, and overall synthesis quality. Subjective evaluations further\nconfirm that our method outperforms baseline models in terms of naturalness and\nemotional expressiveness, underscoring its potential as a scalable solution for\nmultilingual emotional TTS.", "published": "2025-08-15 07:20:31", "link": "http://arxiv.org/abs/2508.11273v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Liquid Crystal-Based RIS Loss-Trade-Off Analysis", "abstract": "Liquid crystal (LC) technology has emerged as a promising solution for large\nreconfigurable intelligent surfaces (RISs) at millimeter wave (mmWave) bands,\noffering advantages such as low power consumption, scalability, and\ncontinuously tunable phase shifts. For LC-RIS based on the delay-line\narchitecture, i.e., with dedicated phase shifters, there exists a trade-off\nbetween the maximum achievable phase-shift range and the corresponding\ninsertion loss, which has not been studied for LC-RIS-assisted wireless systems\nyet. In this paper, we investigate this trade-off where a base station (BS) and\nan RIS are configured to minimize the transmit power while satisfying a given\nquality of service (QoS) for a number of users. Simulation results reveal a\nfundamental trade-off between the total transmit power and the achievable data\nrate as a function of the LC phase-shift range.", "published": "2025-08-15 14:09:28", "link": "http://arxiv.org/abs/2508.11489v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Reducing AoI and Improving Throughput for NOMA-assisted SGF Systems: A Hierarchical Learning Approach", "abstract": "A non-orthogonal multiple access (NOMA) assisted semi-grant-free (SGF)\nframework is proposed to enable channel access for grant-free users (GFUs) by\nusing residual resources from grant-based users. Under this framework, the\nproblem of joint beamforming design and transmission scheduling is formulated\nto improve the system throughput and reduce the age-of-information of GFUs. The\naforementioned problem is transferred into a Markov Decision Process to model\nthe changing environment with the transmission/ waiting/ retransmission of\nGFUs. In an effort to solve the pertinent problem, firstly, a deep\nreinforcement learning (DRL) based transmission scheduling approach is proposed\nfor determining the optimal transmission probability based on the available\ntransmission slots and transmission status of GFUs. Secondly, a hierarchical\nlearning algorithm is proposed to analyze the channel state information of GBUs\nand the transmission status of GFUs, and to train an upper-level policy based\non this analysis for beamforming to achieve efficient grant-based transmission,\nwhile a lower-level policy adapts to maximize the utilization of transmission\nslots allocated by the upper-level agent. The two policies interact to improve\nchannel access and avoid collisions. Numerical results reveal that 1) The DRL\nbased transmission scheduling outperforms existing adaptive and state-dependent\nbaselines in AoI reduction, where an average\nthree-time-slots-earlier-transmission can be obtained compared to the\nstate-dependent choice, and five time slots earlier can be achieved when\ncomparing to the adaptive choice; 2) The hierarchical learning algorithm is\nable to achieve approximately a 31.82% gain while maintaining the average AoI\nof GFUs within 1.5 time slots. 3) The effectiveness of the hierarchical\nlearning scheme in NOMA-assisted SGF system is validated across scenarios with\nGFUs counts from 1-5 times of GBUs.", "published": "2025-08-15 13:39:39", "link": "http://arxiv.org/abs/2508.11473v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Efficient Artifacts Removal for Adaptive Deep Brain Stimulation and a Temporal Event Localization Analysis", "abstract": "Adaptive deep brain stimulation (aDBS) leverages symptom-related biomarkers\nto deliver personalized neuromodulation therapy, with the potential to improve\ntreatment efficacy and reduce power consumption compared to conventional DBS.\nHowever, stimulation-induced signal contamination remains a major technical\nbarrier to advancing its clinical application. Existing artifact removal\nstrategies, both front-end and back-end, face trade-offs between artifact\nsuppression and algorithmic flexibility. Among back-end algorithms, Shrinkage\nand Manifold-based Artifact Removal using Template Adaptation (SMARTA) has\nshown promising performance in mitigating stimulus artifacts with minimal\ndistortion to local field potentials (LFPs), but its high computational demand\nand inability to handle transient direct current (DC) artifacts limit its use\nin real-time applications. To address this, we developed SMARTA+, a\ncomputationally efficient extension of SMARTA capable of suppressing both\nstimulus and transient DC artifacts while supporting flexible algorithmic\ndesign. We evaluated SMARTA+ using semi-real aDBS data and real data from\nParkinson's disease patients. Compared to SMARTA and other established methods,\nSMARTA+ achieved comparable or superior artifact removal while significantly\nreducing computation time. It preserved spectral and temporal structures,\nranging from beta band to high-frequency oscillations, and demonstrated\nrobustness across diverse stimulation protocols. Temporal event localization\nanalysis further showed improved accuracy in detecting beta bursts. These\nfindings support SMARTA+ as a promising tool for advancing real-time,\nclosed-loop aDBS systems.", "published": "2025-08-15 13:16:28", "link": "http://arxiv.org/abs/2508.11459v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Importance-Aware Robust Semantic Transmission for LEO Satellite-Ground Communication", "abstract": "Satellite-ground semantic communication is anticipated to serve a critical\nrole in the forthcoming 6G era. Nonetheless, task-oriented data transmission in\nsuch systems remains a formidable challenge, primarily due to the dynamic\nnature of signal-to-noise ratio (SNR) fluctuations and the stringent bandwidth\nlimitations inherent to low Earth orbit (LEO) satellite channels. In response\nto these constraints, we propose an importance-aware robust semantic\ntransmission (IRST) framework, specifically designed for scenarios\ncharacterized by bandwidth scarcity and channel variability. The IRST scheme\nbegins by applying a segmentation model enhancement algorithm to improve the\ngranularity and accuracy of semantic segmentation. Subsequently, a task-driven\nsemantic selection method is employed to prioritize the transmission of\nsemantically vital content based on real-time channel state information.\nFurthermore, the framework incorporates a stack-based, SNR-aware channel codec\ncapable of executing adaptive channel coding in alignment with SNR variations.\nComparative evaluations across diverse operating conditions demonstrate the\nsuperior performance and resilience of the IRST model relative to existing\nbenchmarks.", "published": "2025-08-15 13:14:58", "link": "http://arxiv.org/abs/2508.11457v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Important Bit Prefix M-ary Quadrature Amplitude Modulation for Semantic Communications", "abstract": "M-ary Quadrature Amplitude Modulation (MQAM) is a commonly used channel\nmodulation technology in wireless communication systems. To achieve dedicated\nchannel modulation for semantic communication (SemCom), we propose an\nImportant-Bit-Prefixed MQAM (IBP-MQAM) scheme and derive its approximate\nexpression of important symbol error rate (ISER) and unimportant symbol error\nrate (USER). By extracting and quantifying text semantics using Latent\nDirichlet Allocation (LDA), we verify that IBP-MQAM achieves improved\nperformance over MQAM in SemCom scenarios and further analyze the effects of\nkey system parameters.", "published": "2025-08-15 09:33:45", "link": "http://arxiv.org/abs/2508.11351v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Direct data-driven interpolation and approximation of linear parameter-varying system trajectories", "abstract": "We consider the problem of estimating missing values in trajectories of\nlinear parameter-varying (LPV) systems. We solve this interpolation problem for\nthe class of shifted-affine LPV systems. Conditions for the existence and\nuniqueness of solutions are given and a direct data-driven algorithm for its\ncomputation is presented, i.e., the data-generating system is not given by a\nparametric model but is implicitly specified by data. We illustrate the\napplicability of the proposed solution on illustrative examples of a\nmass-spring-damper system with exogenous and endogenous parameter variation.", "published": "2025-08-15 09:03:59", "link": "http://arxiv.org/abs/2508.11332v1", "categories": ["eess.SY", "cs.SY", "eess.SP", "math.OC"], "primary_category": "eess.SY"}
{"title": "Enabling low-power massive MIMO with ternary ADCs for AIoT sensing", "abstract": "The proliferation of networked devices and the surging demand for ubiquitous\nintelligence have given rise to the artificial intelligence of things (AIoT).\nHowever, the utilization of high-resolution analog-to-digital converters (ADCs)\nand numerous radio frequency chains significantly raises power consumption.\nThis paper explores a cost-effective solution using ternary ADCs (T-ADCs) in\nmassive multiple-input-multiple-output (MIMO) systems for low-power AIoT and\nspecifically addresses channel sensing challenges. The channel is first\nestimated through a pilot-aided scheme and refined using a joint-pilot-and-data\n(JPD) approach. To assess the performance limits of this two-threshold ADC\nsystem, the analysis includes its hardware-ideal counterpart, the parallel\none-bit ADCs (PO-ADCs) and a realistic scenario where noise variance is unknown\nat the receiver is considered. Analytical findings indicate that the JPD scheme\neffectively mitigates performance degradation in channel estimation due to\ncoarse quantization effects under mild conditions, without necessitating\nadditional pilot overhead. For deterministic and random channels, we propose\nmodified expectation maximization (EM) and variational inference EM estimators,\nrespectively. Extensive simulations validate the theoretical results and\ndemonstrate the effectiveness of the proposed estimators in terms of mean\nsquare error and symbol error rate, which showcases the feasibility of\nimplementing T-ADCs and the associated JPD scheme for greener AIoT smart\nsensing.", "published": "2025-08-15 05:49:40", "link": "http://arxiv.org/abs/2508.11234v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "KAN-HAR: A Human activity recognition based on Kolmogorov-Arnold Network", "abstract": "Human Activity Recognition (HAR) plays a critical role in numerous\napplications, including healthcare monitoring, fitness tracking, and smart\nenvironments. Traditional deep learning (DL) approaches, while effective, often\nrequire extensive parameter tuning and may lack interpretability. In this work,\nwe investigate the use of a single three-axis accelerometer and the\nKolmogorov--Arnold Network (KAN) for HAR tasks, leveraging its ability to model\ncomplex nonlinear relationships with improved interpretability and parameter\nefficiency. The MotionSense dataset, containing smartphone-based motion sensor\nsignals across various physical activities, is employed to evaluate the\nproposed approach. Our methodology involves preprocessing and normalization of\naccelerometer and gyroscope data, followed by KAN-based feature learning and\nclassification. Experimental results demonstrate that the KAN achieves\ncompetitive or superior classification performance compared to conventional\ndeep neural networks, while maintaining a significantly reduced parameter\ncount. This highlights the potential of KAN architectures as an efficient and\ninterpretable alternative for real-world HAR systems. The open-source\nimplementation of the proposed framework is available at the Project's GitHub\nRepository.", "published": "2025-08-15 03:32:53", "link": "http://arxiv.org/abs/2508.11186v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Near-Field Variable-Width Beam Coverage and Codebook Design for XL-RIS", "abstract": "To mitigate the issue of limited base station coverage caused by severe\nhigh-frequency electromagnetic wave attenuation, Extremely Large Reconfigurable\nIntelligent Surface (XL-RIS) has garnered significant attention due to its high\nbeam gain. However, XL-RIS exhibits a narrower beam width compared to\ntraditional RIS, which increases the complexity of beam alignment and\nbroadcast. To address this problem, we propose a variable-width beam generation\nalgorithm under the near-field assumption and apply it to the near-field\ncodebook design for XL-RIS. Our algorithm can achieve beam coverage for\narbitrarily shaped codeword regions and generate a joint codebook for the\nmulti-XL-RIS system. The simulation results demonstrate that our proposed\nscheme enables user equipment (UE) to achieve higher spectral efficiency and\nlower communication outage probability within the codeword region compared to\nexisting works. Furthermore, our scheme exhibits better robustness to codeword\nregion location and area variations.", "published": "2025-08-15 03:08:27", "link": "http://arxiv.org/abs/2508.11178v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis", "abstract": "The proliferation of high-quality text from Large Language Models (LLMs)\ndemands reliable and efficient detection methods. While existing training-free\napproaches show promise, they often rely on surface-level statistics and\noverlook fundamental signal properties of the text generation process. In this\nwork, we reframe detection as a signal processing problem, introducing a novel\nparadigm that analyzes the sequence of token log-probabilities in the frequency\ndomain. By systematically analyzing the signal's spectral properties using the\nglobal Discrete Fourier Transform (DFT) and the local Short-Time Fourier\nTransform (STFT), we find that human-written text consistently exhibits\nsignificantly higher spectral energy. This higher energy reflects the\nlarger-amplitude fluctuations inherent in human writing compared to the\nsuppressed dynamics of LLM-generated text. Based on this key insight, we\nconstruct SpecDetect, a detector built on a single, robust feature from the\nglobal DFT: DFT total energy. We also propose an enhanced version,\nSpecDetect++, which incorporates a sampling discrepancy mechanism to further\nboost robustness. Extensive experiments demonstrate that our approach\noutperforms the state-of-the-art model while running in nearly half the time.\nOur work introduces a new, efficient, and interpretable pathway for\nLLM-generated text detection, showing that classical signal processing\ntechniques offer a surprisingly powerful solution to this modern challenge.", "published": "2025-08-15 09:13:42", "link": "http://arxiv.org/abs/2508.11343v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning", "abstract": "Graph ``pre-training and prompt-tuning'' aligns downstream tasks with\npre-trained objectives to enable efficient knowledge transfer under limited\nsupervision. However, existing methods rely on homophily-based low-frequency\nknowledge, failing to handle diverse spectral distributions in real-world\ngraphs with varying homophily. Our theoretical analysis reveals a spectral\nspecificity principle: optimal knowledge transfer requires alignment between\npre-trained spectral filters and the intrinsic spectrum of downstream graphs.\nUnder limited supervision, large spectral gaps between pre-training and\ndownstream tasks impede effective adaptation. To bridge this gap, we propose\nthe HS-GPPT model, a novel framework that ensures spectral alignment throughout\nboth pre-training and prompt-tuning. We utilize a hybrid spectral filter\nbackbone and local-global contrastive learning to acquire abundant spectral\nknowledge. Then we design prompt graphs to align the spectral distribution with\npretexts, facilitating spectral knowledge transfer across homophily and\nheterophily. Extensive experiments validate the effectiveness under both\ntransductive and inductive learning settings. Our code is available at\nhttps://anonymous.4open.science/r/HS-GPPT-62D2/.", "published": "2025-08-15 08:55:57", "link": "http://arxiv.org/abs/2508.11328v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks", "abstract": "Deploying multiple machine learning models on resource-constrained robotic\nplatforms for different perception tasks often results in redundant\ncomputations, large memory footprints, and complex integration challenges. In\nresponse, this work presents Visual Perception Engine (VPEngine), a modular\nframework designed to enable efficient GPU usage for visual multitasking while\nmaintaining extensibility and developer accessibility. Our framework\narchitecture leverages a shared foundation model backbone that extracts image\nrepresentations, which are efficiently shared, without any unnecessary GPU-CPU\nmemory transfers, across multiple specialized task-specific model heads running\nin parallel. This design eliminates the computational redundancy inherent in\nfeature extraction component when deploying traditional sequential models while\nenabling dynamic task prioritization based on application demands. We\ndemonstrate our framework's capabilities through an example implementation\nusing DINOv2 as the foundation model with multiple task (depth, object\ndetection and semantic segmentation) heads, achieving up to 3x speedup compared\nto sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine\noffers efficient GPU utilization and maintains a constant memory footprint\nwhile allowing per-task inference frequencies to be adjusted dynamically during\nruntime. The framework is written in Python and is open source with ROS2 C++\n(Humble) bindings for ease of use by the robotics community across diverse\nrobotic platforms. Our example implementation demonstrates end-to-end real-time\nperformance at $\\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized\nmodels.", "published": "2025-08-15 16:42:23", "link": "http://arxiv.org/abs/2508.11584v2", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "ADMIRE-BayesOpt: Accelerated Data MIxture RE-weighting for Language Models with Bayesian Optimization", "abstract": "Determining the optimal data mixture for large language model training\nremains a challenging problem with an outsized impact on performance. In\npractice, language model developers continue to rely on heuristic exploration\nsince no learning-based approach has emerged as a reliable solution. In this\nwork, we propose to view the selection of training data mixtures as a black-box\nhyperparameter optimization problem, for which Bayesian Optimization is a\nwell-established class of appropriate algorithms. Firstly, we cast data mixture\nlearning as a sequential decision-making problem, in which we aim to find a\nsuitable trade-off between the computational cost of training exploratory\n(proxy-) models and final mixture performance. Secondly, we systematically\nexplore the properties of transferring mixtures learned at a small scale to\nlarger-scale experiments, providing insights and highlighting opportunities for\nresearch at a modest scale. By proposing Multi-fidelity Bayesian Optimization\nas a suitable method in this common scenario, we introduce a natural framework\nto balance experiment cost with model fit, avoiding the risks of overfitting to\nsmaller scales while minimizing the number of experiments at high cost. We\npresent results for pre-training and instruction finetuning across models\nranging from 1 million to 7 billion parameters, varying from simple\narchitectures to state-of-the-art models and benchmarks spanning dozens of\ndatasets. We demonstrate consistently strong results relative to a wide range\nof baselines, resulting inspeed-ups of over 500% in determining the best data\nmixture on our largest experiments. In addition, we broaden access to research\nby sharing ADMIRE IFT Runs, a dataset of 460 full training & evaluation runs\nworth over 13,000 GPU hours, greatly reducing the cost of conducting research\nin this area.", "published": "2025-08-15 15:53:09", "link": "http://arxiv.org/abs/2508.11551v2", "categories": ["stat.ML", "cs.AI", "cs.LG"], "primary_category": "stat.ML"}
{"title": "TimeMachine: Fine-Grained Facial Age Editing with Identity Preservation", "abstract": "With the advancement of generative models, facial image editing has made\nsignificant progress. However, achieving fine-grained age editing while\npreserving personal identity remains a challenging task. In this paper, we\npropose TimeMachine, a novel diffusion-based framework that achieves accurate\nage editing while keeping identity features unchanged. To enable fine-grained\nage editing, we inject high-precision age information into the multi-cross\nattention module, which explicitly separates age-related and identity-related\nfeatures. This design facilitates more accurate disentanglement of age\nattributes, thereby allowing precise and controllable manipulation of facial\naging. Furthermore, we propose an Age Classifier Guidance (ACG) module that\npredicts age directly in the latent space, instead of performing denoising\nimage reconstruction during training. By employing a lightweight module to\nincorporate age constraints, this design enhances age editing accuracy by\nmodest increasing training cost. Additionally, to address the lack of\nlarge-scale, high-quality facial age datasets, we construct a HFFA dataset\n(High-quality Fine-grained Facial-Age dataset) which contains one million\nhigh-resolution images labeled with identity and facial attributes.\nExperimental results demonstrate that TimeMachine achieves state-of-the-art\nperformance in fine-grained age editing while preserving identity consistency.", "published": "2025-08-15 07:46:37", "link": "http://arxiv.org/abs/2508.11284v2", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "When Does Language Transfer Help? Sequential Fine-Tuning for Cross-Lingual Euphemism Detection", "abstract": "Euphemisms are culturally variable and often ambiguous, posing challenges for\nlanguage models, especially in low-resource settings. This paper investigates\nhow cross-lingual transfer via sequential fine-tuning affects euphemism\ndetection across five languages: English, Spanish, Chinese, Turkish, and\nYoruba. We compare sequential fine-tuning with monolingual and simultaneous\nfine-tuning using XLM-R and mBERT, analyzing how performance is shaped by\nlanguage pairings, typological features, and pretraining coverage. Results show\nthat sequential fine-tuning with a high-resource L1 improves L2 performance,\nespecially for low-resource languages like Yoruba and Turkish. XLM-R achieves\nlarger gains but is more sensitive to pretraining gaps and catastrophic\nforgetting, while mBERT yields more stable, though lower, results. These\nfindings highlight sequential fine-tuning as a simple yet effective strategy\nfor improving euphemism detection in multilingual models, particularly when\nlow-resource languages are involved.", "published": "2025-08-15 22:40:35", "link": "http://arxiv.org/abs/2508.11831v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Every 28 Days the AI Dreams of Soft Skin and Burning Stars: Scaffolding AI Agents with Hormones and Emotions", "abstract": "Despite significant advances, AI systems struggle with the frame problem:\ndetermining what information is contextually relevant from an exponentially\nlarge possibility space. We hypothesize that biological rhythms, particularly\nhormonal cycles, serve as natural relevance filters that could address this\nfundamental challenge. We develop a framework that embeds simulated menstrual\nand circadian cycles into Large Language Models through system prompts\ngenerated from periodic functions modeling key hormones including estrogen,\ntestosterone, and cortisol. Across multiple state-of-the-art models, linguistic\nanalysis reveals emotional and stylistic variations that track biological\nphases; sadness peaks during menstruation while happiness dominates ovulation\nand circadian patterns show morning optimism transitioning to nocturnal\nintrospection. Benchmarking on SQuAD, MMLU, Hellaswag, and AI2-ARC demonstrates\nsubtle but consistent performance variations aligning with biological\nexpectations, including optimal function in moderate rather than extreme\nhormonal ranges. This methodology provides a novel approach to contextual AI\nwhile revealing how societal biases regarding gender and biology are embedded\nwithin language models.", "published": "2025-08-15 22:26:42", "link": "http://arxiv.org/abs/2508.11829v1", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL"}
{"title": "A Survey of Idiom Datasets for Psycholinguistic and Computational Research", "abstract": "Idioms are figurative expressions whose meanings often cannot be inferred\nfrom their individual words, making them difficult to process computationally\nand posing challenges for human experimental studies. This survey reviews\ndatasets developed in psycholinguistics and computational linguistics for\nstudying idioms, focusing on their content, form, and intended use.\nPsycholinguistic resources typically contain normed ratings along dimensions\nsuch as familiarity, transparency, and compositionality, while computational\ndatasets support tasks like idiomaticity detection/classification,\nparaphrasing, and cross-lingual modeling. We present trends in annotation\npractices, coverage, and task framing across 53 datasets. Although recent\nefforts expanded language coverage and task diversity, there seems to be no\nrelation yet between psycholinguistic and computational research on idioms.", "published": "2025-08-15 22:24:09", "link": "http://arxiv.org/abs/2508.11828v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hallucination Detection and Mitigation in Scientific Text Simplification using Ensemble Approaches: DS@GT at CLEF 2025 SimpleText", "abstract": "In this paper, we describe our methodology for the CLEF 2025 SimpleText Task\n2, which focuses on detecting and evaluating creative generation and\ninformation distortion in scientific text simplification. Our solution\nintegrates multiple strategies: we construct an ensemble framework that\nleverages BERT-based classifier, semantic similarity measure, natural language\ninference model, and large language model (LLM) reasoning. These diverse\nsignals are combined using meta-classifiers to enhance the robustness of\nspurious and distortion detection. Additionally, for grounded generation, we\nemploy an LLM-based post-editing system that revises simplifications based on\nthe original input texts.", "published": "2025-08-15 21:57:27", "link": "http://arxiv.org/abs/2508.11823v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-Guided Planning and Summary-Based Scientific Text Simplification: DS@GT at CLEF 2025 SimpleText", "abstract": "In this paper, we present our approach for the CLEF 2025 SimpleText Task 1,\nwhich addresses both sentence-level and document-level scientific text\nsimplification. For sentence-level simplification, our methodology employs\nlarge language models (LLMs) to first generate a structured plan, followed by\nplan-driven simplification of individual sentences. At the document level, we\nleverage LLMs to produce concise summaries and subsequently guide the\nsimplification process using these summaries. This two-stage, LLM-based\nframework enables more coherent and contextually faithful simplifications of\nscientific text.", "published": "2025-08-15 21:44:52", "link": "http://arxiv.org/abs/2508.11816v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Labels or Input? Rethinking Augmentation in Multimodal Hate Detection", "abstract": "The modern web is saturated with multimodal content, intensifying the\nchallenge of detecting hateful memes, where harmful intent is often conveyed\nthrough subtle interactions between text and image under the guise of humor or\nsatire. While recent advances in Vision-Language Models (VLMs) show promise,\nthese models lack support for fine-grained supervision and remain susceptible\nto implicit hate speech. In this paper, we present a dual-pronged approach to\nimprove multimodal hate detection. First, we propose a prompt optimization\nframework that systematically varies prompt structure, supervision granularity,\nand training modality. We show that prompt design and label scaling both\ninfluence performance, with structured prompts improving robustness even in\nsmall models, and InternVL2 achieving the best F1-scores across binary and\nscaled settings. Second, we introduce a multimodal data augmentation pipeline\nthat generates 2,479 counterfactually neutral memes by isolating and rewriting\nthe hateful modality. This pipeline, powered by a multi-agent LLM-VLM setup,\nsuccessfully reduces spurious correlations and improves classifier\ngeneralization. Our approaches inspire new directions for building synthetic\ndata to train robust and fair vision-language models. Our findings demonstrate\nthat prompt structure and data composition are as critical as model size, and\nthat targeted augmentation can support more trustworthy and context-sensitive\nhate detection.", "published": "2025-08-15 21:31:00", "link": "http://arxiv.org/abs/2508.11808v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CY", "cs.MM", "I.2.7; I.2.10"], "primary_category": "cs.CV"}
{"title": "What Matters for Bioacoustic Encoding", "abstract": "Bioacoustics, the study of sounds produced by living organisms, plays a vital\nrole in conservation, biodiversity monitoring, and behavioral studies. Many\ntasks in this field, such as species, individual, and behavior classification\nand detection, are well-suited to machine learning. However, they often suffer\nfrom limited annotated data, highlighting the need for a general-purpose\nbioacoustic encoder capable of extracting useful representations for diverse\ndownstream tasks. Such encoders have been proposed before, but are often\nlimited in scope due to a focus on a narrow range of species (typically birds),\nand a reliance on a single model architecture or training paradigm. Moreover,\nthey are usually evaluated on a small set of tasks and datasets. In this work,\nwe present a large-scale empirical study that covers aspects of bioacoustics\nthat are relevant to research but have previously been scarcely considered:\ntraining data diversity and scale, model architectures and training recipes,\nand the breadth of evaluation tasks and datasets. We obtain encoders that are\nstate-of-the-art on the existing and proposed benchmarks. We also identify what\nmatters for training these encoders, such that this work can be extended when\nmore data are available or better architectures are proposed. Specifically,\nacross 26 datasets with tasks including species classification, detection,\nindividual ID, and vocal repertoire discovery, we find self-supervised\npre-training followed by supervised post-training on a mixed bioacoustics +\ngeneral-audio corpus yields the strongest in- and out-of-distribution\nperformance. We show the importance of data diversity in both stages. To\nsupport ongoing research and application, we will release the model\ncheckpoints.", "published": "2025-08-15 23:52:34", "link": "http://arxiv.org/abs/2508.11845v1", "categories": ["cs.SD", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.SD"}
{"title": "Ontology-Guided Query Expansion for Biomedical Document Retrieval using Large Language Models", "abstract": "Effective Question Answering (QA) on large biomedical document collections\nrequires effective document retrieval techniques. The latter remains a\nchallenging task due to the domain-specific vocabulary and semantic ambiguity\nin user queries. We propose BMQExpander, a novel ontology-aware query expansion\npipeline that combines medical knowledge - definitions and relationships - from\nthe UMLS Metathesaurus with the generative capabilities of large language\nmodels (LLMs) to enhance retrieval effectiveness. We implemented several\nstate-of-the-art baselines, including sparse and dense retrievers, query\nexpansion methods, and biomedical-specific solutions. We show that BMQExpander\nhas superior retrieval performance on three popular biomedical Information\nRetrieval (IR) benchmarks: NFCorpus, TREC-COVID, and SciFact - with\nimprovements of up to 22.1% in NDCG@10 over sparse baselines and up to 6.5%\nover the strongest baseline. Further, BMQExpander generalizes robustly under\nquery perturbation settings, in contrast to supervised baselines, achieving up\nto 15.7% improvement over the strongest baseline. As a side contribution, we\npublish our paraphrased benchmarks. Finally, our qualitative analysis shows\nthat BMQExpander has fewer hallucinations compared to other LLM-based query\nexpansion baselines.", "published": "2025-08-15 19:23:26", "link": "http://arxiv.org/abs/2508.11784v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Bayesian Learning for Pilot Decontamination in Cell-Free Massive MIMO", "abstract": "Pilot contamination (PC) arises when the pilot sequences assigned to user\nequipments (UEs) are not mutually orthogonal, eventually due to their reuse. In\nthis work, we propose a novel expectation propagation (EP)-based joint channel\nestimation and data detection (JCD) algorithm specifically designed to mitigate\nthe effects of PC in the uplink of cell-free massive multiple-input\nmultiple-output (CF-MaMIMO) systems. This modified bilinear-EP algorithm is\ndistributed, scalable, demonstrates strong robustness to PC, and outperforms\nstate-of-the-art Bayesian learning algorithms. Through a comprehensive\nperformance evaluation, we assess the performance of Bayesian learning\nalgorithms for different pilot sequences and observe that the use of\nnon-orthogonal pilots can lead to better performance compared to shared\northogonal sequences. Motivated by this analysis, we introduce a new metric to\nquantify PC at the UE level. We show that the performance of the considered\nalgorithms degrades monotonically with respect to this metric, providing a\nvaluable theoretical and practical tool for understanding and managing PC via\niterative JCD algorithms.", "published": "2025-08-15 19:52:12", "link": "http://arxiv.org/abs/2508.11791v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "SafeSieve: From Heuristics to Experience in Progressive Pruning for LLM-based Multi-Agent Communication", "abstract": "LLM-based multi-agent systems exhibit strong collaborative capabilities but\noften suffer from redundant communication and excessive token overhead.\nExisting methods typically enhance efficiency through pretrained GNNs or greedy\nalgorithms, but often isolate pre- and post-task optimization, lacking a\nunified strategy. To this end, we present SafeSieve, a progressive and adaptive\nmulti-agent pruning algorithm that dynamically refines the inter-agent\ncommunication through a novel dual-mechanism. SafeSieve integrates initial\nLLM-based semantic evaluation with accumulated performance feedback, enabling a\nsmooth transition from heuristic initialization to experience-driven\nrefinement. Unlike existing greedy Top-k pruning methods, SafeSieve employs\n0-extension clustering to preserve structurally coherent agent groups while\neliminating ineffective links. Experiments across benchmarks (SVAMP, HumanEval,\netc.) showcase that SafeSieve achieves 94.01% average accuracy while reducing\ntoken usage by 12.4%-27.8%. Results further demonstrate robustness under prompt\ninjection attacks (1.23% average accuracy drop). In heterogeneous settings,\nSafeSieve reduces deployment costs by 13.3% while maintaining performance.\nThese results establish SafeSieve as a robust, efficient, and scalable\nframework for practical multi-agent systems. Our code can be found in\nhttps://anonymous.4open.science/r/SafeSieve-D8F2FFUN.", "published": "2025-08-15 13:44:50", "link": "http://arxiv.org/abs/2508.11733v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "Stochastic Modeling of Filtration with Sieving in Graded Pore Networks", "abstract": "We model filtration of a feed solution, containing both small and large\nfoulant particles, by a membrane filter. The membrane interior is modeled as a\nnetwork of pores, allowing for the simultaneous adsorption of small particles\nand sieving of large particles, two fouling mechanisms typically observed\nduring the early stages of commercial filtration applications. In our model,\nfirst-principles continuum partial differential equations model transport of\nthe small particles and adsorptive fouling in each pore, while sieving\nparticles are assumed to follow a discrete Poisson arrival process with a\nbiased random walk through the pore network. Our goals are to understand the\nrelative influences of each fouling mode and highlight the effect of their\ncoupling on the performance of filters with a pore-size gradient (specifically,\nwe consider a banded filter with different pore sizes in each band). Our\nresults suggest that, due to the discrete nature of pore blockage, sieving\nalters qualitatively the rate of the flux decline. Moreover, the difference\nbetween sieving particle sizes and the initial pore size (radius) in each band\nplays a crucial role in indicating the onset and disappearance of\nsieving-adsorption competition. Lastly, we demonstrate a phase transition in\nthe filter lifetime as the arrival frequency of sieving particles increases.", "published": "2025-08-15 21:52:20", "link": "http://arxiv.org/abs/2508.11820v1", "categories": ["physics.flu-dyn", "cs.NA", "cs.NI", "math.NA", "math.PR", "76S05, 76-10, 34B45, 35R02"], "primary_category": "physics.flu-dyn"}
{"title": "Statistical analysis of multivariate planar curves and applications to X-ray classification", "abstract": "Recent developments in computer vision have enabled the availability of\nsegmented images across various domains, such as medicine, where segmented\nradiography images play an important role in diagnosis-making. As prediction\nproblems are common in medical image analysis, this work explores the use of\nsegmented images (through the associated contours they highlight) as predictors\nin a supervised classification context. Consequently, we develop a new approach\nfor image analysis that takes into account the shape of objects within images.\nFor this aim, we introduce a new formalism that extends the study of single\nrandom planar curves to the joint analysis of multiple planar curves-referred\nto here as multivariate planar curves. In this framework, we propose a solution\nto the alignment issue in statistical shape analysis. The obtained multivariate\nshape variables are then used in functional classification methods through\ntangent projections. Detection of cardiomegaly in segmented X-rays and\nnumerical experiments on synthetic data demonstrate the appeal and robustness\nof the proposed method.", "published": "2025-08-15 19:13:27", "link": "http://arxiv.org/abs/2508.11780v1", "categories": ["stat.ME", "cs.CV", "stat.ML"], "primary_category": "stat.ME"}
{"title": "BaMANI: Bayesian Multi-Algorithm causal Network Inference", "abstract": "Improved computational power has enabled different disciplines to predict\ncausal relationships among modeled variables using Bayesian network inference.\nWhile many alternative algorithms have been proposed to improve the efficiency\nand reliability of network prediction, the predicted causal networks reflect\nthe generative process but also bear an opaque imprint of the specific\ncomputational algorithm used. Following a ``wisdom of the crowds\" strategy, we\ndeveloped an ensemble learning approach to marginalize the impact of a single\nalgorithm on Bayesian causal network inference. To introduce the approach, we\nfirst present the theoretical foundation of this framework. Next, we present a\ncomprehensive implementation of the framework in terms of a new software tool\ncalled BaMANI (Bayesian Multi-Algorithm causal Network Inference). Finally, we\ndescribe a BaMANI use-case from biology, particularly within human breast\ncancer studies.", "published": "2025-08-15 17:38:51", "link": "http://arxiv.org/abs/2508.11741v1", "categories": ["stat.ML", "cs.LG", "q-bio.QM"], "primary_category": "stat.ML"}
{"title": "Causal Structure Learning in Hawkes Processes with Complex Latent Confounder Networks", "abstract": "Multivariate Hawkes process provides a powerful framework for modeling\ntemporal dependencies and event-driven interactions in complex systems. While\nexisting methods primarily focus on uncovering causal structures among observed\nsubprocesses, real-world systems are often only partially observed, with latent\nsubprocesses posing significant challenges. In this paper, we show that\ncontinuous-time event sequences can be represented by a discrete-time model as\nthe time interval shrinks, and we leverage this insight to establish necessary\nand sufficient conditions for identifying latent subprocesses and the causal\ninfluences. Accordingly, we propose a two-phase iterative algorithm that\nalternates between inferring causal relationships among discovered subprocesses\nand uncovering new latent subprocesses, guided by path-based conditions that\nguarantee identifiability. Experiments on both synthetic and real-world\ndatasets show that our method effectively recovers causal structures despite\nthe presence of latent subprocesses.", "published": "2025-08-15 09:03:39", "link": "http://arxiv.org/abs/2508.11727v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Digital Post-Distortion Architectures for Nonlinear Power Amplifiers: Volterra and Kernel Methods", "abstract": "In modern 5G user equipments (UEs), the power amplifier (PA) contributes\nsignificantly to power consumption during uplink transmissions, especially in\ncell-edge scenarios. While reducing power backoff can enhance PA efficiency, it\nintroduces nonlinear distortions that degrade signal quality. Existing\nsolutions, such as digital pre-distortion, require complex feedback mechanisms\nfor optimal performance, leading to increased UE complexity and power\nconsumption. Instead, in this study we explore digital post-distortion (DPoD)\ntechniques, which compensate for these distortions at the base station,\nleveraging its superior computational resources. In this study, we conduct an\ncomprehensive study concerning the challenges and advantages associated with\napplying DPoD in time-domain, frequency-domain, and DFT-s-domain. Our findings\nsuggest that implementing DPoD in the time-domain, complemented by\nfrequency-domain channel equalization, strikes a good balance between low\ncomputational complexity and efficient nonlinearity compensation. In addition,\nwe demonstrate that memory has to be taken into account regardless of the\nmemory of the PA. Subsequently, we show how to pose the complex-valued problem\nof nonlinearity compensation in a real Hilbert space, emphasizing the potential\nperformance enhancements as a result. We then discuss the traditional Volterra\nseries and show an equivalent kernel method that can reduce algorithmic\ncomplexity. Simulations validate the results of our analysis and show that our\nproposed algorithm can significantly improve performance compared to\nstate-of-the-art algorithms.", "published": "2025-08-15 20:00:42", "link": "http://arxiv.org/abs/2508.11792v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Scaling Wideband Massive MIMO Radar via Beamspace Dimension Reduction", "abstract": "We present an architecture for scaling digital beamforming for wideband\nmassive MIMO radar. Conventional spatial processing becomes computationally\nprohibitive as array size grows; for example, the computational complexity of\nMVDR beamforming scales as O(N^3) for an N-element array. In this paper, we\nshow that energy concentration in beamspace provides the basis for drastic\ncomplexity reduction, with array scaling governed by the O(NlogN) complexity of\nthe spatial FFT used for beamspace transformation. Specifically, we propose an\narchitecture for windowed beamspace MVDR beamforming, parallelized across\ntargets and subbands, and evaluate its efficacy for beamforming and\ninterference suppression for government-supplied wideband radar data from the\nDARPA SOAP (Scalable On-Array Processing) program. We demonstrate that our\napproach achieves detection performance comparable to full-dimensional\nbenchmarks while significantly reducing computational and training overhead,\nand provide insight into tradeoffs between beamspace window size and FFT\nresolution in balancing complexity, detection accuracy, and interference\nsuppression.", "published": "2025-08-15 19:49:53", "link": "http://arxiv.org/abs/2508.11790v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "What Matters for Bioacoustic Encoding", "abstract": "Bioacoustics, the study of sounds produced by living organisms, plays a vital\nrole in conservation, biodiversity monitoring, and behavioral studies. Many\ntasks in this field, such as species, individual, and behavior classification\nand detection, are well-suited to machine learning. However, they often suffer\nfrom limited annotated data, highlighting the need for a general-purpose\nbioacoustic encoder capable of extracting useful representations for diverse\ndownstream tasks. Such encoders have been proposed before, but are often\nlimited in scope due to a focus on a narrow range of species (typically birds),\nand a reliance on a single model architecture or training paradigm. Moreover,\nthey are usually evaluated on a small set of tasks and datasets. In this work,\nwe present a large-scale empirical study that covers aspects of bioacoustics\nthat are relevant to research but have previously been scarcely considered:\ntraining data diversity and scale, model architectures and training recipes,\nand the breadth of evaluation tasks and datasets. We obtain encoders that are\nstate-of-the-art on the existing and proposed benchmarks. We also identify what\nmatters for training these encoders, such that this work can be extended when\nmore data are available or better architectures are proposed. Specifically,\nacross 26 datasets with tasks including species classification, detection,\nindividual ID, and vocal repertoire discovery, we find self-supervised\npre-training followed by supervised post-training on a mixed bioacoustics +\ngeneral-audio corpus yields the strongest in- and out-of-distribution\nperformance. We show the importance of data diversity in both stages. To\nsupport ongoing research and application, we will release the model\ncheckpoints.", "published": "2025-08-15 23:52:34", "link": "http://arxiv.org/abs/2508.11845v2", "categories": ["cs.SD", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.SD"}
{"title": "Contextual Attention-Based Multimodal Fusion of LLM and CNN for Sentiment Analysis", "abstract": "This paper introduces a novel approach for multimodal sentiment analysis on\nsocial media, particularly in the context of natural disasters, where\nunderstanding public sentiment is crucial for effective crisis management.\nUnlike conventional methods that process text and image modalities separately,\nour approach seamlessly integrates Convolutional Neural Network (CNN) based\nimage analysis with Large Language Model (LLM) based text processing,\nleveraging Generative Pre-trained Transformer (GPT) and prompt engineering to\nextract sentiment relevant features from the CrisisMMD dataset. To effectively\nmodel intermodal relationships, we introduce a contextual attention mechanism\nwithin the fusion process. Leveraging contextual-attention layers, this\nmechanism effectively captures intermodality interactions, enhancing the\nmodel's comprehension of complex relationships between textual and visual data.\nThe deep neural network architecture of our model learns from these fused\nfeatures, leading to improved accuracy compared to existing baselines.\nExperimental results demonstrate significant advancements in classifying social\nmedia data into informative and noninformative categories across various\nnatural disasters. Our model achieves a notable 2.43% increase in accuracy and\n5.18% in F1-score, highlighting its efficacy in processing complex multimodal\ndata. Beyond quantitative metrics, our approach provides deeper insight into\nthe sentiments expressed during crises. The practical implications extend to\nreal time disaster management, where enhanced sentiment analysis can optimize\nthe accuracy of emergency interventions. By bridging the gap between multimodal\nanalysis, LLM powered text understanding, and disaster response, our work\npresents a promising direction for Artificial Intelligence (AI) driven crisis\nmanagement solutions. Keywords:", "published": "2025-08-15 21:34:13", "link": "http://arxiv.org/abs/2508.13196v1", "categories": ["cs.LG", "cs.AI", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Data-driven optimized high-order WENO schemes with low-dissipation and low-dispersion", "abstract": "Classical high-order weighted essentially non-oscillatory (WENO) schemes are\ndesigned to achieve optimal convergence order for smooth solutions and to\nmaintain non-oscillatory behaviors for discontinuities. However, their spectral\nproperties are not optimal, which limits the ability to capture high-frequency\nwaves and small-scale features. In this paper, we propose a data-driven\noptimized method to improve the spectral properties of the WENO schemes. By\nanalyzing the approximate dispersion relation (ADR), the spectral error of the\nschemes can be bounded by the reconstructed errors of a series of trigonometric\nfunctions with different wavenumbers. Therefore, we propose the new schemes\nWENO5-JS/Z-NN that introduce a compensation term parameterized by a neural\nnetwork to the weight function of the WENO5-JS/Z schemes. The neural network is\ntrained such that the generated weights can minimize the reconstructed errors\nover a large number of spatial stencils, and furthermore, improve the spectral\naccuracy. Meanwhile, the Total Variation Diminishing (TVD) constraint and\nanti-dissipation penalization are incorporated into the loss function to\nenhance the shock-capturing capability and preserve stability in simulating\nhigh-frequency waves. Compared to WENO5-JS/Z, our schemes maintain the ability\nto capture discontinuities while providing higher resolution for fine-scale\nflow features. The ADR indicates that the new schemes can match the exact\nspectrum more accurately over a broader range of wavenumbers.", "published": "2025-08-15 05:29:29", "link": "http://arxiv.org/abs/2508.13190v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Preference Models assume Proportional Hazards of Utilities", "abstract": "Approaches for estimating preferences from human annotated data typically\ninvolves inducing a distribution over a ranked list of choices such as the\nPlackett-Luce model. Indeed, modern AI alignment tools such as Reward Modelling\nand Direct Preference Optimization are based on the statistical assumptions\nposed by the Plackett-Luce model. In this paper, I will connect the\nPlackett-Luce model to another classical and well known statistical model, the\nCox Proportional Hazards model and attempt to shed some light on the\nimplications of the connection therein.", "published": "2025-08-15 00:08:56", "link": "http://arxiv.org/abs/2508.13189v1", "categories": ["stat.ML", "cs.AI", "cs.LG"], "primary_category": "stat.ML"}
