{"title": "Evaluating Transformer Models and Human Behaviors on Chinese Character\n  Naming", "abstract": "Neural network models have been proposed to explain the grapheme-phoneme\nmapping process in humans for many alphabet languages. These models not only\nsuccessfully learned the correspondence of the letter strings and their\npronunciation, but also captured human behavior in nonce word naming tasks. How\nwould the neural models perform for a non-alphabet language (e.g., Chinese)\nunknown character task? How well would the model capture human behavior? In\nthis study, we first collect human speakers' answers on unknown character\nnaming tasks and then evaluate a set of transformer models by comparing their\nperformances with human behaviors on an unknown Chinese character naming task.\nWe found that the models and humans behaved very similarly, that they had\nsimilar accuracy distribution for each character, and had a substantial overlap\nin answers. In addition, the models' answers are highly correlated with humans'\nanswers. These results suggested that the transformer models can well capture\nhuman's character naming behavior.", "published": "2023-03-22 03:58:38", "link": "http://arxiv.org/abs/2303.12294v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XWikiGen: Cross-lingual Summarization for Encyclopedic Text Generation\n  in Low Resource Languages", "abstract": "Lack of encyclopedic text contributors, especially on Wikipedia, makes\nautomated text generation for low resource (LR) languages a critical problem.\nExisting work on Wikipedia text generation has focused on English only where\nEnglish reference articles are summarized to generate English Wikipedia pages.\nBut, for low-resource languages, the scarcity of reference articles makes\nmonolingual summarization ineffective in solving this problem. Hence, in this\nwork, we propose XWikiGen, which is the task of cross-lingual multi-document\nsummarization of text from multiple reference articles, written in various\nlanguages, to generate Wikipedia-style text. Accordingly, we contribute a\nbenchmark dataset, XWikiRef, spanning ~69K Wikipedia articles covering five\ndomains and eight languages. We harness this dataset to train a two-stage\nsystem where the input is a set of citations and a section title and the output\nis a section-specific LR summary. The proposed system is based on a novel idea\nof neural unsupervised extractive summarization to coarsely identify salient\ninformation followed by a neural abstractive model to generate the\nsection-specific text. Extensive experiments show that multi-domain training is\nbetter than the multi-lingual setup on average.", "published": "2023-03-22 04:52:43", "link": "http://arxiv.org/abs/2303.12308v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering", "abstract": "Commonsense question-answering (QA) methods combine the power of pre-trained\nLanguage Models (LM) with the reasoning provided by Knowledge Graphs (KG). A\ntypical approach collects nodes relevant to the QA pair from a KG to form a\nWorking Graph (WG) followed by reasoning using Graph Neural Networks(GNNs).\nThis faces two major challenges: (i) it is difficult to capture all the\ninformation from the QA in the WG, and (ii) the WG contains some irrelevant\nnodes from the KG. To address these, we propose GrapeQA with two simple\nimprovements on the WG: (i) Prominent Entities for Graph Augmentation\nidentifies relevant text chunks from the QA pair and augments the WG with\ncorresponding latent representations from the LM, and (ii) Context-Aware Node\nPruning removes nodes that are less relevant to the QA pair. We evaluate our\nresults on OpenBookQA, CommonsenseQA and MedQA-USMLE and see that GrapeQA shows\nconsistent improvements over its LM + KG predecessor (QA-GNN in particular) and\nlarge improvements on OpenBookQA.", "published": "2023-03-22 05:35:29", "link": "http://arxiv.org/abs/2303.12320v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MEGA: Multilingual Evaluation of Generative AI", "abstract": "Generative AI models have shown impressive performance on many Natural\nLanguage Processing tasks such as language understanding, reasoning, and\nlanguage generation. An important question being asked by the AI community\ntoday is about the capabilities and limits of these models, and it is clear\nthat evaluating generative AI is very challenging. Most studies on generative\nLLMs have been restricted to English and it is unclear how capable these models\nare at understanding and generating text in other languages. We present the\nfirst comprehensive benchmarking of generative LLMs - MEGA, which evaluates\nmodels on standard NLP benchmarks, covering 16 NLP datasets across 70\ntypologically diverse languages. We compare the performance of generative LLMs\nincluding Chat-GPT and GPT-4 to State of the Art (SOTA) non-autoregressive\nmodels on these tasks to determine how well generative models perform compared\nto the previous generation of LLMs. We present a thorough analysis of the\nperformance of models across languages and tasks and discuss challenges in\nimproving the performance of generative LLMs on low-resource languages. We\ncreate a framework for evaluating generative LLMs in the multilingual setting\nand provide directions for future progress in the field.", "published": "2023-03-22 13:03:10", "link": "http://arxiv.org/abs/2303.12528v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AfroDigits: A Community-Driven Spoken Digit Dataset for African\n  Languages", "abstract": "The advancement of speech technologies has been remarkable, yet its\nintegration with African languages remains limited due to the scarcity of\nAfrican speech corpora. To address this issue, we present AfroDigits, a\nminimalist, community-driven dataset of spoken digits for African languages,\ncurrently covering 38 African languages. As a demonstration of the practical\napplications of AfroDigits, we conduct audio digit classification experiments\non six African languages [Igbo (ibo), Yoruba (yor), Rundi (run), Oshiwambo\n(kua), Shona (sna), and Oromo (gax)] using the Wav2Vec2.0-Large and XLS-R\nmodels. Our experiments reveal a useful insight on the effect of mixing African\nspeech corpora during finetuning. AfroDigits is the first published audio digit\ndataset for African languages and we believe it will, among other things, pave\nthe way for Afro-centric speech applications such as the recognition of\ntelephone numbers, and street numbers. We release the dataset and platform\npublicly at https://huggingface.co/datasets/chrisjay/crowd-speech-africa and\nhttps://huggingface.co/spaces/chrisjay/afro-speech respectively.", "published": "2023-03-22 14:09:20", "link": "http://arxiv.org/abs/2303.12582v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can We Identify Stance Without Target Arguments? A Study for Rumour\n  Stance Classification", "abstract": "Considering a conversation thread, rumour stance classification aims to\nidentify the opinion (e.g. agree or disagree) of replies towards a target\n(rumour story). Although the target is expected to be an essential component in\ntraditional stance classification, we show that rumour stance classification\ndatasets contain a considerable amount of real-world data whose stance could be\nnaturally inferred directly from the replies, contributing to the strong\nperformance of the supervised models without awareness of the target. We find\nthat current target-aware models underperform in cases where the context of the\ntarget is crucial. Finally, we propose a simple yet effective framework to\nenhance reasoning with the targets, achieving state-of-the-art performance on\ntwo benchmark datasets.", "published": "2023-03-22 15:44:15", "link": "http://arxiv.org/abs/2303.12665v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JaCoText: A Pretrained Model for Java Code-Text Generation", "abstract": "Pretrained transformer-based models have shown high performance in natural\nlanguage generation task. However, a new wave of interest has surged: automatic\nprogramming language generation. This task consists of translating natural\nlanguage instructions to a programming code. Despite the fact that well-known\npretrained models on language generation have achieved good performance in\nlearning programming languages, effort is still needed in automatic code\ngeneration. In this paper, we introduce JaCoText, a model based on Transformers\nneural network. It aims to generate java source code from natural language\ntext. JaCoText leverages advantages of both natural language and code\ngeneration models. More specifically, we study some findings from the state of\nthe art and use them to (1) initialize our model from powerful pretrained\nmodels, (2) explore additional pretraining on our java dataset, (3) carry out\nexperiments combining the unimodal and bimodal data in the training, and (4)\nscale the input and output length during the fine-tuning of the model.\nConducted experiments on CONCODE dataset show that JaCoText achieves new\nstate-of-the-art results.", "published": "2023-03-22 19:01:25", "link": "http://arxiv.org/abs/2303.12869v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Understanding the Generalization of Medical Text-to-SQL Models\n  and Datasets", "abstract": "Electronic medical records (EMRs) are stored in relational databases. It can\nbe challenging to access the required information if the user is unfamiliar\nwith the database schema or general database fundamentals. Hence, researchers\nhave explored text-to-SQL generation methods that provide healthcare\nprofessionals direct access to EMR data without needing a database expert.\nHowever, currently available datasets have been essentially \"solved\" with\nstate-of-the-art models achieving accuracy greater than or near 90%. In this\npaper, we show that there is still a long way to go before solving text-to-SQL\ngeneration in the medical domain. To show this, we create new splits of the\nexisting medical text-to-SQL dataset MIMICSQL that better measure the\ngeneralizability of the resulting models. We evaluate state-of-the-art language\nmodels on our new split showing substantial drops in performance with accuracy\ndropping from up to 92% to 28%, thus showing substantial room for improvement.\nMoreover, we introduce a novel data augmentation approach to improve the\ngeneralizability of the language models. Overall, this paper is the first step\ntowards developing more robust text-to-SQL models in the medical\ndomain.\\footnote{The dataset and code will be released upon acceptance.", "published": "2023-03-22 20:26:30", "link": "http://arxiv.org/abs/2303.12898v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing the Generalizability of Deep Contextualized Language\n  Representations For Text Classification", "abstract": "This study evaluates the robustness of two state-of-the-art deep contextual\nlanguage representations, ELMo and DistilBERT, on supervised learning of binary\nprotest news classification and sentiment analysis of product reviews. A\n\"cross-context\" setting is enabled using test sets that are distinct from the\ntraining data. Specifically, in the news classification task, the models are\ndeveloped on local news from India and tested on the local news from China. In\nthe sentiment analysis task, the models are trained on movie reviews and tested\non customer reviews. This comparison is aimed at exploring the limits of the\nrepresentative power of today's Natural Language Processing systems on the path\nto the systems that are generalizable to real-life scenarios. The models are\nfine-tuned and fed into a Feed-Forward Neural Network and a Bidirectional Long\nShort Term Memory network. Multinomial Naive Bayes and Linear Support Vector\nMachine are used as traditional baselines. The results show that, in binary\ntext classification, DistilBERT is significantly better than ELMo on\ngeneralizing to the cross-context setting. ELMo is observed to be significantly\nmore robust to the cross-context test data than both baselines. On the other\nhand, the baselines performed comparably well to ELMo when the training and\ntest data are subsets of the same corpus (no cross-context). DistilBERT is also\nfound to be 30% smaller and 83% faster than ELMo. The results suggest that\nDistilBERT can transfer generic semantic knowledge to other domains better than\nELMo. DistilBERT is also favorable in incorporating into real-life systems for\nit requires a smaller computational training budget. When generalization is not\nthe utmost preference and test domain is similar to the training domain, the\ntraditional ML algorithms can still be considered as more economic alternatives\nto deep language representations.", "published": "2023-03-22 22:31:09", "link": "http://arxiv.org/abs/2303.12936v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization\n  for Few-shot Generalization", "abstract": "Prompt tuning is a parameter-efficient method, which learns soft prompts and\nconditions frozen language models to perform specific downstream tasks. Though\neffective, prompt tuning under few-shot settings on the one hand heavily relies\non a good initialization of soft prompts. On the other hand, it can easily\noverfit to few-shot training samples, thereby undermining generalizability.\nExisting works leverage pre-training or supervised meta-learning to initialize\nsoft prompts but they fail to data-efficiently generalize to unseen downstream\ntasks. To address the above problems, this paper proposes a novel\nSelf-sUpervised meta-Prompt learning framework with MEta-gradient\nRegularization for few-shot generalization (SUPMER). SUPMER leverages\nself-supervised meta-learning with a diverse set of well-designed meta-training\ntasks to learn a universal prompt initialization for efficient adaptation using\nonly unlabeled data. Additionally, it jointly meta-learns a gradient\nregularization function to transform raw gradients into a domain-generalizable\ndirection, thus alleviating the problem of overfitting. Extensive experiments\nshow that SUPMER achieves better performance for different few-shot downstream\ntasks, and also exhibits a stronger domain generalization ability. The code for\nSUPMER will be available at https://github.com/beepkh/SUPMER.", "published": "2023-03-22 05:04:21", "link": "http://arxiv.org/abs/2303.12314v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Integrating Image Features with Convolutional Sequence-to-sequence\n  Network for Multilingual Visual Question Answering", "abstract": "Visual Question Answering (VQA) is a task that requires computers to give\ncorrect answers for the input questions based on the images. This task can be\nsolved by humans with ease but is a challenge for computers. The\nVLSP2022-EVJVQA shared task carries the Visual Question Answering task in the\nmultilingual domain on a newly released dataset: UIT-EVJVQA, in which the\nquestions and answers are written in three different languages: English,\nVietnamese and Japanese. We approached the challenge as a sequence-to-sequence\nlearning task, in which we integrated hints from pre-trained state-of-the-art\nVQA models and image features with Convolutional Sequence-to-Sequence network\nto generate the desired answers. Our results obtained up to 0.3442 by F1 score\non the public test set, 0.4210 on the private test set, and placed 3rd in the\ncompetition.", "published": "2023-03-22 15:49:33", "link": "http://arxiv.org/abs/2303.12671v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4", "abstract": "Artificial intelligence (AI) researchers have been developing and refining\nlarge language models (LLMs) that exhibit remarkable capabilities across a\nvariety of domains and tasks, challenging our understanding of learning and\ncognition. The latest model developed by OpenAI, GPT-4, was trained using an\nunprecedented scale of compute and data. In this paper, we report on our\ninvestigation of an early version of GPT-4, when it was still in active\ndevelopment by OpenAI. We contend that (this early version of) GPT-4 is part of\na new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that\nexhibit more general intelligence than previous AI models. We discuss the\nrising capabilities and implications of these models. We demonstrate that,\nbeyond its mastery of language, GPT-4 can solve novel and difficult tasks that\nspan mathematics, coding, vision, medicine, law, psychology and more, without\nneeding any special prompting. Moreover, in all of these tasks, GPT-4's\nperformance is strikingly close to human-level performance, and often vastly\nsurpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's\ncapabilities, we believe that it could reasonably be viewed as an early (yet\nstill incomplete) version of an artificial general intelligence (AGI) system.\nIn our exploration of GPT-4, we put special emphasis on discovering its\nlimitations, and we discuss the challenges ahead for advancing towards deeper\nand more comprehensive versions of AGI, including the possible need for\npursuing a new paradigm that moves beyond next-word prediction. We conclude\nwith reflections on societal influences of the recent technological leap and\nfuture research directions.", "published": "2023-03-22 16:51:28", "link": "http://arxiv.org/abs/2303.12712v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Interpretable Bangla Sarcasm Detection using BERT and Explainable AI", "abstract": "A positive phrase or a sentence with an underlying negative motive is usually\ndefined as sarcasm that is widely used in today's social media platforms such\nas Facebook, Twitter, Reddit, etc. In recent times active users in social media\nplatforms are increasing dramatically which raises the need for an automated\nNLP-based system that can be utilized in various tasks such as determining\nmarket demand, sentiment analysis, threat detection, etc. However, since\nsarcasm usually implies the opposite meaning and its detection is frequently a\nchallenging issue, data meaning extraction through an NLP-based model becomes\nmore complicated. As a result, there has been a lot of study on sarcasm\ndetection in English over the past several years, and there's been a noticeable\nimprovement and yet sarcasm detection in the Bangla language's state remains\nthe same. In this article, we present a BERT-based system that can achieve\n99.60\\% while the utilized traditional machine learning algorithms are only\ncapable of achieving 89.93\\%. Additionally, we have employed Local\nInterpretable Model-Agnostic Explanations that introduce explainability to our\nsystem. Moreover, we have utilized a newly collected bangla sarcasm dataset,\nBanglaSarc that was constructed specifically for the evaluation of this study.\nThis dataset consists of fresh records of sarcastic and non-sarcastic comments,\nthe majority of which are acquired from Facebook and YouTube comment sections.", "published": "2023-03-22 17:35:35", "link": "http://arxiv.org/abs/2303.12772v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Are LLMs the Master of All Trades? : Exploring Domain-Agnostic Reasoning\n  Skills of LLMs", "abstract": "The potential of large language models (LLMs) to reason like humans has been\na highly contested topic in Machine Learning communities. However, the\nreasoning abilities of humans are multifaceted and can be seen in various\nforms, including analogical, spatial and moral reasoning, among others. This\nfact raises the question whether LLMs can perform equally well across all these\ndifferent domains. This research work aims to investigate the performance of\nLLMs on different reasoning tasks by conducting experiments that directly use\nor draw inspirations from existing datasets on analogical and spatial\nreasoning. Additionally, to evaluate the ability of LLMs to reason like human,\ntheir performance is evaluted on more open-ended, natural language questions.\nMy findings indicate that LLMs excel at analogical and moral reasoning, yet\nstruggle to perform as proficiently on spatial reasoning tasks. I believe these\nexperiments are crucial for informing the future development of LLMs,\nparticularly in contexts that require diverse reasoning proficiencies. By\nshedding light on the reasoning abilities of LLMs, this study aims to push\nforward our understanding of how they can better emulate the cognitive\nabilities of humans.", "published": "2023-03-22 22:53:44", "link": "http://arxiv.org/abs/2303.12810v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Salient Span Masking for Temporal Understanding", "abstract": "Salient Span Masking (SSM) has shown itself to be an effective strategy to\nimprove closed-book question answering performance. SSM extends general masked\nlanguage model pretraining by creating additional unsupervised training\nsentences that mask a single entity or date span, thus oversampling factual\ninformation. Despite the success of this paradigm, the span types and sampling\nstrategies are relatively arbitrary and not widely studied for other tasks.\nThus, we investigate SSM from the perspective of temporal tasks, where learning\na good representation of various temporal expressions is important. To that\nend, we introduce Temporal Span Masking (TSM) intermediate training. First, we\nfind that SSM alone improves the downstream performance on three temporal tasks\nby an avg. +5.8 points. Further, we are able to achieve additional improvements\n(avg. +0.29 points) by adding the TSM task. These comprise the new best\nreported results on the targeted tasks. Our analysis suggests that the\neffectiveness of SSM stems from the sentences chosen in the training data\nrather than the mask choice: sentences with entities frequently also contain\ntemporal expressions. Nonetheless, the additional targeted spans of TSM can\nstill improve performance, especially in a zero-shot context.", "published": "2023-03-22 18:49:43", "link": "http://arxiv.org/abs/2303.12860v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Transformer Performance for French Clinical Notes\n  Classification Using Mixture of Experts on a Limited Dataset", "abstract": "Transformer-based models have shown outstanding results in natural language\nprocessing but face challenges in applications like classifying small-scale\nclinical texts, especially with constrained computational resources. This study\npresents a customized Mixture of Expert (MoE) Transformer models for\nclassifying small-scale French clinical texts at CHU Sainte-Justine Hospital.\nThe MoE-Transformer addresses the dual challenges of effective training with\nlimited data and low-resource computation suitable for in-house hospital use.\nDespite the success of biomedical pre-trained models such as CamemBERT-bio,\nDrBERT, and AliBERT, their high computational demands make them impractical for\nmany clinical settings. Our MoE-Transformer model not only outperforms\nDistillBERT, CamemBERT, FlauBERT, and Transformer models on the same dataset\nbut also achieves impressive results: an accuracy of 87\\%, precision of 87\\%,\nrecall of 85\\%, and F1-score of 86\\%. While the MoE-Transformer does not\nsurpass the performance of biomedical pre-trained BERT models, it can be\ntrained at least 190 times faster, offering a viable alternative for settings\nwith limited data and computational resources. Although the MoE-Transformer\naddresses challenges of generalization gaps and sharp minima, demonstrating\nsome limitations for efficient and accurate clinical text classification, this\nmodel still represents a significant advancement in the field. It is\nparticularly valuable for classifying small French clinical narratives within\nthe privacy and constraints of hospital-based computational resources.", "published": "2023-03-22 20:10:29", "link": "http://arxiv.org/abs/2303.12892v2", "categories": ["cs.CL", "eess.SP"], "primary_category": "cs.CL"}
{"title": "W2KPE: Keyphrase Extraction with Word-Word Relation", "abstract": "This paper describes our submission to ICASSP 2023 MUG Challenge Track 4,\nKeyphrase Extraction, which aims to extract keyphrases most relevant to the\nconference theme from conference materials. We model the challenge as a\nsingle-class Named Entity Recognition task and developed techniques for better\nperformance on the challenge: For the data preprocessing, we encode the split\nkeyphrases after word segmentation. In addition, we increase the amount of\ninput information that the model can accept at one time by fusing multiple\npreprocessed sentences into one segment. We replace the loss function with the\nmulti-class focal loss to address the sparseness of keyphrases. Besides, we\nscore each appearance of keyphrases and add an extra output layer to fit the\nscore to rank keyphrases. Exhaustive evaluations are performed to find the best\ncombination of the word segmentation tool, the pre-trained embedding model, and\nthe corresponding hyperparameters. With these proposals, we scored 45.04 on the\nfinal test set.", "published": "2023-03-22 15:32:40", "link": "http://arxiv.org/abs/2303.13463v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Deep RL with Hierarchical Action Exploration for Dialogue Generation", "abstract": "Traditionally, approximate dynamic programming is employed in dialogue\ngeneration with greedy policy improvement through action sampling, as the\nnatural language action space is vast. However, this practice is inefficient\nfor reinforcement learning (RL) due to the sparsity of eligible responses with\nhigh action values, which leads to weak improvement sustained by random\nsampling. This paper presents theoretical analysis and experiments that reveal\nthe performance of the dialogue policy is positively correlated with the\nsampling size. To overcome this limitation, we introduce a novel\ndual-granularity Q-function that explores the most promising response category\nto intervene in the sampling process. Our approach extracts actions based on a\ngrained hierarchy, thereby achieving the optimum with fewer policy iterations.\nAdditionally, we use offline RL and learn from multiple reward functions\ndesigned to capture emotional nuances in human interactions. Empirical studies\ndemonstrate that our algorithm outperforms baselines across automatic metrics\nand human evaluations. Further testing reveals that our algorithm exhibits both\nexplainability and controllability and generates responses with higher expected\nrewards.", "published": "2023-03-22 09:29:22", "link": "http://arxiv.org/abs/2303.13465v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mining Clinical Notes for Physical Rehabilitation Exercise Information:\n  Natural Language Processing Algorithm Development and Validation Study", "abstract": "Post-stroke patient rehabilitation requires precise, personalized treatment\nplans. Natural Language Processing (NLP) offers potential to extract valuable\nexercise information from clinical notes, aiding in the development of more\neffective rehabilitation strategies. Objective: This study aims to develop and\nevaluate a variety of NLP algorithms to extract and categorize physical\nrehabilitation exercise information from the clinical notes of post-stroke\npatients treated at the University of Pittsburgh Medical Center. A cohort of\n13,605 patients diagnosed with stroke was identified, and their clinical notes\ncontaining rehabilitation therapy notes were retrieved. A comprehensive\nclinical ontology was created to represent various aspects of physical\nrehabilitation exercises. State-of-the-art NLP algorithms were then developed\nand compared, including rule-based, machine learning-based algorithms, and\nlarge language model (LLM)-based algorithms (ChatGPT). Analysis was conducted\non a dataset comprising 23,724 notes with detailed demographic and clinical\ncharacteristics. The rule-based NLP algorithm demonstrated superior performance\nin most areas, particularly in detecting the 'Right Side' location with an F1\nscore of 0.975, outperforming Gradient Boosting by 0.063. Gradient Boosting\nexcelled in 'Lower Extremity' location detection (F1 score: 0.978), surpassing\nrule-based NLP by 0.023. It also showed notable performance in 'Passive Range\nof Motion' with an F1 score of 0.970, a 0.032 improvement over rule-based NLP.\nThe rule-based algorithm efficiently handled 'Duration', 'Sets', and 'Reps'\nwith F1 scores up to 0.65. LLM-based NLP, particularly ChatGPT with few-shot\nprompts, achieved high recall but generally lower precision and F1 scores.\nHowever, it notably excelled in 'Backward Plane' motion detection, achieving an\nF1 score of 0.846, surpassing the rule-based algorithm's 0.720.", "published": "2023-03-22 13:46:16", "link": "http://arxiv.org/abs/2303.13466v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Turkish Speech Recognition via Hybrid CTC/Attention\n  Architecture and Multi-feature Fusion Network", "abstract": "In recent years, End-to-End speech recognition technology based on deep\nlearning has developed rapidly. Due to the lack of Turkish speech data, the\nperformance of Turkish speech recognition system is poor. Firstly, this paper\nstudies a series of speech recognition tuning technologies. The results show\nthat the performance of the model is the best when the data enhancement\ntechnology combining speed perturbation with noise addition is adopted and the\nbeam search width is set to 16. Secondly, to maximize the use of effective\nfeature information and improve the accuracy of feature extraction, this paper\nproposes a new feature extractor LSPC. LSPC and LiGRU network are combined to\nform a shared encoder structure, and model compression is realized. The results\nshow that the performance of LSPC is better than MSPC and VGGnet when only\nusing Fbank features, and the WER is improved by 1.01% and 2.53% respectively.\nFinally, based on the above two points, a new multi-feature fusion network is\nproposed as the main structure of the encoder. The results show that the WER of\nthe proposed feature fusion network based on LSPC is improved by 0.82% and\n1.94% again compared with the single feature (Fbank feature and Spectrogram\nfeature) extraction using LSPC. Our model achieves performance comparable to\nthat of advanced End-to-End models.", "published": "2023-03-22 04:11:35", "link": "http://arxiv.org/abs/2303.12300v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RepoCoder: Repository-Level Code Completion Through Iterative Retrieval\n  and Generation", "abstract": "The task of repository-level code completion is to continue writing the\nunfinished code based on a broader context of the repository. While for\nautomated code completion tools, it is difficult to utilize the useful\ninformation scattered in different files. We propose RepoCoder, a simple,\ngeneric, and effective framework to address the challenge. It streamlines the\nrepository-level code completion process by incorporating a similarity-based\nretriever and a pre-trained code language model in an iterative\nretrieval-generation pipeline. RepoCoder makes effective utilization of\nrepository-level information for code completion and has the ability to\ngenerate code at various levels of granularity. Moreover, we propose a new\nbenchmark RepoEval, which consists of the latest and high-quality real-world\nrepositories covering line, API invocation, and function body completion\nscenarios. Experimental results indicate that RepoCoder significantly improves\nthe In-File completion baseline by over 10% in all settings and consistently\noutperforms the vanilla retrieval-augmented code completion approach.\nFurthermore, we validate the effectiveness of RepoCoder through comprehensive\nanalysis, providing valuable insights for future research. Our source code and\nbenchmark are publicly available:\nhttps://github.com/microsoft/CodeT/tree/main/RepoCoder", "published": "2023-03-22 13:54:46", "link": "http://arxiv.org/abs/2303.12570v3", "categories": ["cs.CL", "cs.AI", "cs.PL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Can we trust the evaluation on ChatGPT?", "abstract": "ChatGPT, the first large language model (LLM) with mass adoption, has\ndemonstrated remarkable performance in numerous natural language tasks. Despite\nits evident usefulness, evaluating ChatGPT's performance in diverse problem\ndomains remains challenging due to the closed nature of the model and its\ncontinuous updates via Reinforcement Learning from Human Feedback (RLHF). We\nhighlight the issue of data contamination in ChatGPT evaluations, with a case\nstudy of the task of stance detection. We discuss the challenge of preventing\ndata contamination and ensuring fair model evaluation in the age of closed and\ncontinuously trained models.", "published": "2023-03-22 17:32:56", "link": "http://arxiv.org/abs/2303.12767v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Open-source Frame Semantic Parsing", "abstract": "While the state-of-the-art for frame semantic parsing has progressed\ndramatically in recent years, it is still difficult for end-users to apply\nstate-of-the-art models in practice. To address this, we present Frame Semantic\nTransformer, an open-source Python library which achieves near state-of-the-art\nperformance on FrameNet 1.7, while focusing on ease-of-use. We use a T5 model\nfine-tuned on Propbank and FrameNet exemplars as a base, and improve\nperformance by using FrameNet lexical units to provide hints to T5 at inference\ntime. We enhance robustness to real-world data by using textual data\naugmentations during training.", "published": "2023-03-22 17:57:47", "link": "http://arxiv.org/abs/2303.12788v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "From Wide to Deep: Dimension Lifting Network for Parameter-efficient\n  Knowledge Graph Embedding", "abstract": "Knowledge graph embedding (KGE) that maps entities and relations into vector\nrepresentations is essential for downstream applications. Conventional KGE\nmethods require high-dimensional representations to learn the complex structure\nof knowledge graph, but lead to oversized model parameters. Recent advances\nreduce parameters by low-dimensional entity representations, while developing\ntechniques (e.g., knowledge distillation or reinvented representation forms) to\ncompensate for reduced dimension. However, such operations introduce\ncomplicated computations and model designs that may not benefit large knowledge\ngraphs. To seek a simple strategy to improve the parameter efficiency of\nconventional KGE models, we take inspiration from that deeper neural networks\nrequire exponentially fewer parameters to achieve expressiveness comparable to\nwider networks for compositional structures. We view all entity representations\nas a single-layer embedding network, and conventional KGE methods that adopt\nhigh-dimensional entity representations equal widening the embedding network to\ngain expressiveness. To achieve parameter efficiency, we instead propose a\ndeeper embedding network for entity representations, i.e., a narrow entity\nembedding layer plus a multi-layer dimension lifting network (LiftNet).\nExperiments on three public datasets show that by integrating LiftNet, four\nconventional KGE methods with 16-dimensional representations achieve comparable\nlink prediction accuracy as original models that adopt 512-dimensional\nrepresentations, saving 68.4% to 96.9% parameters.", "published": "2023-03-22 07:34:33", "link": "http://arxiv.org/abs/2303.12816v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Selective Data Augmentation for Robust Speech Translation", "abstract": "Speech translation (ST) systems translate speech in one language to text in\nanother language. End-to-end ST systems (e2e-ST) have gained popularity over\ncascade systems because of their enhanced performance due to reduced latency\nand computational cost. Though resource intensive, e2e-ST systems have the\ninherent ability to retain para and non-linguistic characteristics of the\nspeech unlike cascade systems. In this paper, we propose to use an e2e\narchitecture for English-Hindi (en-hi) ST. We use two imperfect machine\ntranslation (MT) services to translate Libri-trans en text into hi text. While\neach service gives MT data individually to generate parallel ST data, we\npropose a data augmentation strategy of noisy MT data to aid robust ST. The\nmain contribution of this paper is the proposal of a data augmentation\nstrategy. We show that this results in better ST (BLEU score) compared to brute\nforce augmentation of MT data. We observed an absolute improvement of 1.59 BLEU\nscore with our approach.", "published": "2023-03-22 19:36:07", "link": "http://arxiv.org/abs/2304.03169v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Dual-Quaternions: Theory and Applications in Sound", "abstract": "Sound is a fundamental and rich source of information; playing a key role in\nmany areas from humanities and social sciences through to engineering and\nmathematics. Sound is more than just data 'signals'. It encapsulates physical,\nsensorial and emotional, as well as social, cultural and environmental factors.\nSound contributes to the transformation of our experiences, environments and\nbeliefs. Sound is all around us and everywhere. Hence, it should come as no\nsurprise that sound is a complex multicomponent entity with a vast assortment\nof characteristics and applications. Of course, an important question is, what\nis the best way to store and represent sound digitally to capture these\ncharacteristics? What model or method is best for manipulating, extracting and\nfiltering sounds? There are a large number of representations and models,\nhowever, one approach that has yet to be used with sound is dual-quaternions.\nWhile dual-quaternions have established themselves in many fields of science\nand computing as an efficient mathematical model for providing an unambiguous,\nun-cumbersome, computationally effective means of representing multi-component\ndata. Sound is one area that has yet to explore and reap the benefits of\ndual-quaternions (using sound and audio-related dual-quaternion models). This\narticle aims to explore the exciting potential and possibilities\ndual-quaternions offer when applied and combined with sound-based models\n(including but not limited to the applications, tools, machine-learning,\nstatistical and computational sound-related algorithms).", "published": "2023-03-22 16:40:24", "link": "http://arxiv.org/abs/2303.12692v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-supervised Learning with Speech Modulation Dropout", "abstract": "We show that training a multi-headed self-attention-based deep network to\npredict deleted, information-dense 2-8 Hz speech modulations over a 1.5-second\nsection of a speech utterance is an effective way to make machines learn to\nextract speech modulations using time-domain contextual information. Our work\nexhibits that, once trained on large volumes of unlabelled data, the outputs of\nthe self-attention layers vary in time with a modulation peak at 4 Hz. These\npre-trained layers can be used to initialize parts of an Automatic Speech\nRecognition system to reduce its reliance on labeled speech data greatly.", "published": "2023-03-22 20:55:54", "link": "http://arxiv.org/abs/2303.12908v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Music-Driven Group Choreography", "abstract": "Music-driven choreography is a challenging problem with a wide variety of\nindustrial applications. Recently, many methods have been proposed to\nsynthesize dance motions from music for a single dancer. However, generating\ndance motion for a group remains an open problem. In this paper, we present\n$\\rm AIOZ-GDANCE$, a new large-scale dataset for music-driven group dance\ngeneration. Unlike existing datasets that only support single dance, our new\ndataset contains group dance videos, hence supporting the study of group\nchoreography. We propose a semi-autonomous labeling method with humans in the\nloop to obtain the 3D ground truth for our dataset. The proposed dataset\nconsists of 16.7 hours of paired music and 3D motion from in-the-wild videos,\ncovering 7 dance styles and 16 music genres. We show that naively applying\nsingle dance generation technique to creating group dance motion may lead to\nunsatisfactory results, such as inconsistent movements and collisions between\ndancers. Based on our new dataset, we propose a new method that takes an input\nmusic sequence and a set of 3D positions of dancers to efficiently produce\nmultiple group-coherent choreographies. We propose new evaluation metrics for\nmeasuring group dance quality and perform intensive experiments to demonstrate\nthe effectiveness of our method. Our project facilitates future research on\ngroup dance generation and is available at:\nhttps://aioz-ai.github.io/AIOZ-GDANCE/", "published": "2023-03-22 06:26:56", "link": "http://arxiv.org/abs/2303.12337v2", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Posthoc Interpretation via Quantization", "abstract": "In this paper, we introduce a new approach, called Posthoc Interpretation via\nQuantization (PIQ), for interpreting decisions made by trained classifiers. Our\nmethod utilizes vector quantization to transform the representations of a\nclassifier into a discrete, class-specific latent space. The class-specific\ncodebooks act as a bottleneck that forces the interpreter to focus on the parts\nof the input data deemed relevant by the classifier for making a prediction.\nOur model formulation also enables learning concepts by incorporating the\nsupervision of pretrained annotation models such as state-of-the-art image\nsegmentation models. We evaluated our method through quantitative and\nqualitative studies involving black-and-white images, color images, and audio.\nAs a result of these studies we found that PIQ generates interpretations that\nare more easily understood by participants to our user studies when compared to\nseveral other interpretation methods in the literature.", "published": "2023-03-22 15:37:43", "link": "http://arxiv.org/abs/2303.12659v2", "categories": ["cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Dense-Localizing Audio-Visual Events in Untrimmed Videos: A Large-Scale\n  Benchmark and Baseline", "abstract": "Existing audio-visual event localization (AVE) handles manually trimmed\nvideos with only a single instance in each of them. However, this setting is\nunrealistic as natural videos often contain numerous audio-visual events with\ndifferent categories. To better adapt to real-life applications, in this paper\nwe focus on the task of dense-localizing audio-visual events, which aims to\njointly localize and recognize all audio-visual events occurring in an\nuntrimmed video. The problem is challenging as it requires fine-grained\naudio-visual scene and context understanding. To tackle this problem, we\nintroduce the first Untrimmed Audio-Visual (UnAV-100) dataset, which contains\n10K untrimmed videos with over 30K audio-visual events. Each video has 2.8\naudio-visual events on average, and the events are usually related to each\nother and might co-occur as in real-life scenes. Next, we formulate the task\nusing a new learning-based framework, which is capable of fully integrating\naudio and visual modalities to localize audio-visual events with various\nlengths and capture dependencies between them in a single pass. Extensive\nexperiments demonstrate the effectiveness of our method as well as the\nsignificance of multi-scale cross-modal perception and dependency modeling for\nthis task.", "published": "2023-03-22 22:00:17", "link": "http://arxiv.org/abs/2303.12930v2", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
