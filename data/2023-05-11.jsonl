{"title": "KGA: A General Machine Unlearning Framework Based on Knowledge Gap\n  Alignment", "abstract": "Recent legislation of the \"right to be forgotten\" has led to the interest in\nmachine unlearning, where the learned models are endowed with the function to\nforget information about specific training instances as if they have never\nexisted in the training set. Previous work mainly focuses on computer vision\nscenarios and largely ignores the essentials of unlearning in NLP field, where\ntext data contains more explicit and sensitive personal information than\nimages. In this paper, we propose a general unlearning framework called KGA to\ninduce forgetfulness. Different from previous work that tries to recover\ngradients or forces models to perform close to one specific distribution, KGA\nmaintains distribution differences (i.e., knowledge gap). This relaxes the\ndistribution assumption. Furthermore, we first apply the unlearning method to\nvarious NLP tasks (i.e., classification, translation, response generation) and\npropose several unlearning evaluation metrics with pertinence. Experiments on\nlarge-scale datasets show that KGA yields comprehensive improvements over\nbaselines, where extensive analyses further validate the effectiveness of KGA\nand provide insight into unlearning for NLP tasks.", "published": "2023-05-11 02:44:29", "link": "http://arxiv.org/abs/2305.06535v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic uncertainty guides the extension of conventions to new\n  referents", "abstract": "A long tradition of studies in psycholinguistics has examined the formation\nand generalization of ad hoc conventions in reference games, showing how newly\nacquired conventions for a given target transfer to new referential contexts.\nHowever, another axis of generalization remains understudied: how do\nconventions formed for one target transfer to completely distinct targets, when\nspecific lexical choices are unlikely to repeat? This paper presents two dyadic\nstudies (N = 240) that address this axis of generalization, focusing on the\nrole of nameability -- the a priori likelihood that two individuals will share\nthe same label. We leverage the recently-released KiloGram dataset, a\ncollection of abstract tangram images that is orders of magnitude larger than\npreviously available, exhibiting high diversity of properties like nameability.\nOur first study asks how nameability shapes convention formation, while the\nsecond asks how new conventions generalize to entirely new targets of\nreference. Our results raise new questions about how ad hoc conventions extend\nbeyond target-specific re-use of specific lexical choices.", "published": "2023-05-11 03:01:40", "link": "http://arxiv.org/abs/2305.06539v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chain-of-Dictionary Prompting Elicits Translation in Large Language\n  Models", "abstract": "Large language models (LLMs) have shown surprisingly good performance in\nmultilingual neural machine translation (MNMT) even when trained without\nparallel data. Yet, despite the fact that the amount of training data is\ngigantic, they still struggle with translating rare words, particularly for\nlow-resource languages. Even worse, it is usually unrealistic to retrieve\nrelevant demonstrations for in-context learning with low-resource languages on\nLLMs, which restricts the practical use of LLMs for translation -- how should\nwe mitigate this problem? To this end, we present a novel method, CoD, which\naugments LLMs with prior knowledge with the chains of multilingual dictionaries\nfor a subset of input words to elicit translation abilities for LLMs. Extensive\nexperiments indicate that augmenting ChatGPT with CoD elicits large gains by up\nto 13x chrF++ points for MNMT (3.08 to 42.63 for English to Serbian written in\nCyrillic script) on FLORES-200 full devtest set. We further demonstrate the\nimportance of chaining the multilingual dictionaries, as well as the\nsuperiority of CoD to few-shot demonstration for low-resource languages.", "published": "2023-05-11 05:19:47", "link": "http://arxiv.org/abs/2305.06575v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from\n  Book Reviews", "abstract": "The analysis of consumer sentiment, as expressed through reviews, can provide\na wealth of insight regarding the quality of a product. While the study of\nsentiment analysis has been widely explored in many popular languages,\nrelatively less attention has been given to the Bangla language, mostly due to\na lack of relevant data and cross-domain adaptability. To address this\nlimitation, we present BanglaBook, a large-scale dataset of Bangla book reviews\nconsisting of 158,065 samples classified into three broad categories: positive,\nnegative, and neutral. We provide a detailed statistical analysis of the\ndataset and employ a range of machine learning models to establish baselines\nincluding SVM, LSTM, and Bangla-BERT. Our findings demonstrate a substantial\nperformance advantage of pre-trained models over models that rely on manually\ncrafted features, emphasizing the necessity for additional training resources\nin this domain. Additionally, we conduct an in-depth error analysis by\nexamining sentiment unigrams, which may provide insight into common\nclassification errors in under-resourced languages like Bangla. Our codes and\ndata are publicly available at https://github.com/mohsinulkabir14/BanglaBook.", "published": "2023-05-11 06:27:38", "link": "http://arxiv.org/abs/2305.06595v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Autocorrelations Decay in Texts and Applicability Limits of Language\n  Models", "abstract": "We show that the laws of autocorrelations decay in texts are closely related\nto applicability limits of language models. Using distributional semantics we\nempirically demonstrate that autocorrelations of words in texts decay according\nto a power law. We show that distributional semantics provides coherent\nautocorrelations decay exponents for texts translated to multiple languages.\nThe autocorrelations decay in generated texts is quantitatively and often\nqualitatively different from the literary texts. We conclude that language\nmodels exhibiting Markov behavior, including large autoregressive language\nmodels, may have limitations when applied to long texts, whether analysis or\ngeneration.", "published": "2023-05-11 07:23:01", "link": "http://arxiv.org/abs/2305.06615v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Serial Contrastive Knowledge Distillation for Continual Few-shot\n  Relation Extraction", "abstract": "Continual few-shot relation extraction (RE) aims to continuously train a\nmodel for new relations with few labeled training data, of which the major\nchallenges are the catastrophic forgetting of old relations and the overfitting\ncaused by data sparsity. In this paper, we propose a new model, namely SCKD, to\naccomplish the continual few-shot RE task. Specifically, we design serial\nknowledge distillation to preserve the prior knowledge from previous models and\nconduct contrastive learning with pseudo samples to keep the representations of\nsamples in different relations sufficiently distinguishable. Our experiments on\ntwo benchmark datasets validate the effectiveness of SCKD for continual\nfew-shot RE and its superiority in knowledge transfer and memory utilization\nover state-of-the-art models.", "published": "2023-05-11 07:25:47", "link": "http://arxiv.org/abs/2305.06616v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Continual Relation Extraction by Distinguishing Analogous\n  Semantics", "abstract": "Continual relation extraction (RE) aims to learn constantly emerging\nrelations while avoiding forgetting the learned relations. Existing works store\na small number of typical samples to re-train the model for alleviating\nforgetting. However, repeatedly replaying these samples may cause the\noverfitting problem. We conduct an empirical study on existing works and\nobserve that their performance is severely affected by analogous relations. To\naddress this issue, we propose a novel continual extraction model for analogous\nrelations. Specifically, we design memory-insensitive relation prototypes and\nmemory augmentation to overcome the overfitting problem. We also introduce\nintegrated training and focal knowledge distillation to enhance the performance\non analogous relations. Experimental results show the superiority of our model\nand demonstrate its effectiveness in distinguishing analogous relations and\novercoming overfitting.", "published": "2023-05-11 07:32:20", "link": "http://arxiv.org/abs/2305.06620v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PROM: A Phrase-level Copying Mechanism with Pre-training for Abstractive\n  Summarization", "abstract": "Based on the remarkable achievements of pre-trained language models in\nabstractive summarization, the copying mechanism has proved helpful by\nimproving the factuality, stability, and overall performance. This work\nproposes PROM, a new PhRase-level cOpying Mechanism that enhances attention on\nn-grams, which can be applied to zero-shot summarization with pre-training.\nPROM adds an indicator layer to explicitly pick up tokens in n-gram that can be\ncopied from the source, and calculates an auxiliary loss for the copying\nprediction. Empirical studies show that PROM makes significant improvements in\nfine-tuning on benchmarks. In zero-shot setting, PROM is utilized in the\nself-supervised pre-training on raw corpora and provides new general baselines\non a wide range of summarization datasets. Further analysis shows that PROM\nperforms more reasonable copying and contributes to faithfulness.", "published": "2023-05-11 08:29:05", "link": "http://arxiv.org/abs/2305.06647v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QURG: Question Rewriting Guided Context-Dependent Text-to-SQL Semantic\n  Parsing", "abstract": "Context-dependent Text-to-SQL aims to translate multi-turn natural language\nquestions into SQL queries. Despite various methods have exploited\ncontext-dependence information implicitly for contextual SQL parsing, there are\nfew attempts to explicitly address the dependencies between current question\nand question context. This paper presents QURG, a novel Question Rewriting\nGuided approach to help the models achieve adequate contextual understanding.\nSpecifically, we first train a question rewriting model to complete the current\nquestion based on question context, and convert them into a rewriting edit\nmatrix. We further design a two-stream matrix encoder to jointly model the\nrewriting relations between question and context, and the schema linking\nrelations between natural language and structured schema. Experimental results\nshow that QURG significantly improves the performances on two large-scale\ncontext-dependent datasets SParC and CoSQL, especially for hard and long-turn\nquestions.", "published": "2023-05-11 08:45:55", "link": "http://arxiv.org/abs/2305.06655v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advancing Neural Encoding of Portuguese with Transformer Albertina PT-*", "abstract": "To advance the neural encoding of Portuguese (PT), and a fortiori the\ntechnological preparation of this language for the digital age, we developed a\nTransformer-based foundation model that sets a new state of the art in this\nrespect for two of its variants, namely European Portuguese from Portugal\n(PT-PT) and American Portuguese from Brazil (PT-BR).\n  To develop this encoder, which we named Albertina PT-*, a strong model was\nused as a starting point, DeBERTa, and its pre-training was done over data sets\nof Portuguese, namely over data sets we gathered for PT-PT and PT-BR, and over\nthe brWaC corpus for PT-BR. The performance of Albertina and competing models\nwas assessed by evaluating them on prominent downstream language processing\ntasks adapted for Portuguese.\n  Both Albertina PT-PT and PT-BR versions are distributed free of charge and\nunder the most permissive license possible and can be run on consumer-grade\nhardware, thus seeking to contribute to the advancement of research and\ninnovation in language technology for Portuguese.", "published": "2023-05-11 10:56:20", "link": "http://arxiv.org/abs/2305.06721v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The First Parallel Corpora for Kurdish Sign Language", "abstract": "Kurdish Sign Language (KuSL) is the natural language of the Kurdish Deaf\npeople. We work on automatic translation between spoken Kurdish and KuSL. Sign\nlanguages evolve rapidly and follow grammatical rules that differ from spoken\nlanguages. Consequently,those differences should be considered during any\ntranslation. We proposed an avatar-based automatic translation of Kurdish texts\nin the Sorani (Central Kurdish) dialect into the Kurdish Sign language. We\ndeveloped the first parallel corpora for that pair that we use to train a\nStatistical Machine Translation (SMT) engine. We tested the outcome\nunderstandability and evaluated it using the Bilingual Evaluation Understudy\n(BLEU). Results showed 53.8% accuracy. Compared to the previous experiments in\nthe field, the result is considerably high. We suspect the reason to be the\nsimilarity between the structure of the two pairs. We plan to make the\nresources publicly available under CC BY-NC-SA 4.0 license on the Kurdish-BLARK\n(https://kurdishblark.github.io/).", "published": "2023-05-11 12:10:20", "link": "http://arxiv.org/abs/2305.06747v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Idiomatic Multiword Expressions in Clinical Terminology using\n  Definition-Based Representation Learning", "abstract": "This paper shines a light on the potential of definition-based semantic\nmodels for detecting idiomatic and semi-idiomatic multiword expressions (MWEs)\nin clinical terminology. Our study focuses on biomedical entities defined in\nthe UMLS ontology and aims to help prioritize the translation efforts of these\nentities. In particular, we develop an effective tool for scoring the\nidiomaticity of biomedical MWEs based on the degree of similarity between the\nsemantic representations of those MWEs and a weighted average of the\nrepresentation of their constituents. We achieve this using a biomedical\nlanguage model trained to produce similar representations for entity names and\ntheir definitions, called BioLORD. The importance of this definition-based\napproach is highlighted by comparing the BioLORD model to two other\nstate-of-the-art biomedical language models based on Transformer: SapBERT and\nCODER. Our results show that the BioLORD model has a strong ability to identify\nidiomatic MWEs, not replicated in other models. Our corpus-free idiomaticity\nestimation helps ontology translators to focus on more challenging MWEs.", "published": "2023-05-11 13:42:58", "link": "http://arxiv.org/abs/2305.06801v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards a Computational Analysis of Suspense: Detecting Dangerous\n  Situations", "abstract": "Suspense is an important tool in storytelling to keep readers engaged and\nwanting to read more. However, it has so far not been studied extensively in\nComputational Literary Studies. In this paper, we focus on one of the elements\nauthors can use to build up suspense: dangerous situations. We introduce a\ncorpus of texts annotated with dangerous situations, distinguishing between 7\ntypes of danger. Additionally, we annotate parts of the text that describe fear\nexperienced by a character, regardless of the actual presence of danger. We\npresent experiments towards the automatic detection of these situations,\nfinding that unsupervised baseline methods can provide valuable signals for the\ndetection, but more complex methods are necessary for further analysis. Not\nunexpectedly, the description of danger and fear often relies heavily on the\ncontext, both local (e.g., situations where danger is only mentioned, but not\nactually present) and global (e.g., \"storm\" being used in a literal sense in an\nadventure novel, but metaphorically in a romance novel).", "published": "2023-05-11 14:12:55", "link": "http://arxiv.org/abs/2305.06818v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "IUST_NLP at SemEval-2023 Task 10: Explainable Detecting Sexism with\n  Transformers and Task-adaptive Pretraining", "abstract": "This paper describes our system on SemEval-2023 Task 10: Explainable\nDetection of Online Sexism (EDOS). This work aims to design an automatic system\nfor detecting and classifying sexist content in online spaces. We propose a set\nof transformer-based pre-trained models with task-adaptive pretraining and\nensemble learning. The main contributions of our system include analyzing the\nperformance of different transformer-based pre-trained models and combining\nthese models, as well as providing an efficient method using large amounts of\nunlabeled data for model adaptive pretraining. We have also explored several\nother strategies. On the test dataset, our system achieves F1-scores of 83%,\n64%, and 47% on subtasks A, B, and C, respectively.", "published": "2023-05-11 15:29:04", "link": "http://arxiv.org/abs/2305.06892v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Open-Domain Question Answering in the Era of Large Language\n  Models", "abstract": "Lexical matching remains the de facto evaluation method for open-domain\nquestion answering (QA). Unfortunately, lexical matching fails completely when\na plausible candidate answer does not appear in the list of gold answers, which\nis increasingly the case as we shift from extractive to generative models. The\nrecent success of large language models (LLMs) for QA aggravates lexical\nmatching failures since candidate answers become longer, thereby making\nmatching with the gold answers even more challenging. Without accurate\nevaluation, the true progress in open-domain QA remains unknown. In this paper,\nwe conduct a thorough analysis of various open-domain QA models, including\nLLMs, by manually evaluating their answers on a subset of NQ-open, a popular\nbenchmark. Our assessments reveal that while the true performance of all models\nis significantly underestimated, the performance of the InstructGPT (zero-shot)\nLLM increases by nearly +60%, making it on par with existing top models, and\nthe InstructGPT (few-shot) model actually achieves a new state-of-the-art on\nNQ-open. We also find that more than 50% of lexical matching failures are\nattributed to semantically equivalent answers. We further demonstrate that\nregex matching ranks QA models consistent with human judgments, although still\nsuffering from unnecessary strictness. Finally, we demonstrate that automated\nevaluation models are a reasonable surrogate for lexical matching in some\ncircumstances, but not for long-form answers generated by LLMs. The automated\nmodels struggle in detecting hallucinations in LLM answers and are thus unable\nto evaluate LLMs. At this time, there appears to be no substitute for human\nevaluation.", "published": "2023-05-11 17:14:33", "link": "http://arxiv.org/abs/2305.06984v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Not All Languages Are Created Equal in LLMs: Improving Multilingual\n  Capability by Cross-Lingual-Thought Prompting", "abstract": "Large language models (LLMs) demonstrate impressive multilingual capability,\nbut their performance varies substantially across different languages. In this\nwork, we introduce a simple yet effective method, called cross-lingual-thought\nprompting (XLT), to systematically improve the multilingual capability of LLMs.\nSpecifically, XLT is a generic template prompt that stimulates cross-lingual\nand logical reasoning skills to enhance task performance across languages. We\nconduct comprehensive evaluations on 7 typical benchmarks related to reasoning,\nunderstanding, and generation tasks, covering both high-resource and\nlow-resource languages. Experimental results show that XLT not only remarkably\nenhances the performance of various multilingual tasks but also significantly\nreduces the gap between the average performance and the best performance of\neach task in different languages. Notably, XLT brings over 10 points of average\nimprovement in arithmetic reasoning and open-domain question-answering tasks.", "published": "2023-05-11 17:44:17", "link": "http://arxiv.org/abs/2305.07004v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Subword Segmental Machine Translation: Unifying Segmentation and Target\n  Sentence Generation", "abstract": "Subword segmenters like BPE operate as a preprocessing step in neural machine\ntranslation and other (conditional) language models. They are applied to\ndatasets before training, so translation or text generation quality relies on\nthe quality of segmentations. We propose a departure from this paradigm, called\nsubword segmental machine translation (SSMT). SSMT unifies subword segmentation\nand MT in a single trainable model. It learns to segment target sentence words\nwhile jointly learning to generate target sentences. To use SSMT during\ninference we propose dynamic decoding, a text generation algorithm that adapts\nsegmentations as it generates translations. Experiments across 6 translation\ndirections show that SSMT improves chrF scores for morphologically rich\nagglutinative languages. Gains are strongest in the very low-resource scenario.\nSSMT also learns subwords that are closer to morphemes compared to baselines\nand proves more robust on a test set constructed for evaluating morphological\ncompositional generalisation.", "published": "2023-05-11 17:44:29", "link": "http://arxiv.org/abs/2305.07005v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A General-Purpose Multilingual Document Encoder", "abstract": "Massively multilingual pretrained transformers (MMTs) have tremendously\npushed the state of the art on multilingual NLP and cross-lingual transfer of\nNLP models in particular. While a large body of work leveraged MMTs to mine\nparallel data and induce bilingual document embeddings, much less effort has\nbeen devoted to training general-purpose (massively) multilingual document\nencoder that can be used for both supervised and unsupervised document-level\ntasks. In this work, we pretrain a massively multilingual document encoder as a\nhierarchical transformer model (HMDE) in which a shallow document transformer\ncontextualizes sentence representations produced by a state-of-the-art\npretrained multilingual sentence encoder. We leverage Wikipedia as a readily\navailable source of comparable documents for creating training data, and train\nHMDE by means of a cross-lingual contrastive objective, further exploiting the\ncategory hierarchy of Wikipedia for creation of difficult negatives. We\nevaluate the effectiveness of HMDE in two arguably most common and prominent\ncross-lingual document-level tasks: (1) cross-lingual transfer for topical\ndocument classification and (2) cross-lingual document retrieval. HMDE is\nsignificantly more effective than (i) aggregations of segment-based\nrepresentations and (ii) multilingual Longformer. Crucially, owing to its\nmassively multilingual lower transformer, HMDE successfully generalizes to\nlanguages unseen in document-level pretraining. We publicly release our code\nand models at\nhttps://github.com/ogaloglu/pre-training-multilingual-document-encoders .", "published": "2023-05-11 17:55:45", "link": "http://arxiv.org/abs/2305.07016v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Contrastive Learning with Noise-Guided Attack: Towards\n  Continual Relation Extraction in the Wild", "abstract": "The principle of continual relation extraction~(CRE) involves adapting to\nemerging novel relations while preserving od knowledge. While current endeavors\nin CRE succeed in preserving old knowledge, they tend to fail when exposed to\ncontaminated data streams. We assume this is attributed to their reliance on an\nartificial hypothesis that the data stream has no annotation errors, which\nhinders real-world applications for CRE. Considering the ubiquity of noisy\nlabels in real-world datasets, in this paper, we formalize a more practical\nlearning scenario, termed as \\textit{noisy-CRE}. Building upon this challenging\nsetting, we develop a noise-resistant contrastive framework named as\n\\textbf{N}oise-guided \\textbf{a}ttack in \\textbf{C}ontrative\n\\textbf{L}earning~(NaCL) to learn incremental corrupted relations. Compared to\ndirect noise discarding or inaccessible noise relabeling, we present modifying\nthe feature space to match the given noisy labels via attacking can better\nenrich contrastive representations. Extensive empirical validations highlight\nthat NaCL can achieve consistent performance improvements with increasing noise\nrates, outperforming state-of-the-art baselines.", "published": "2023-05-11 18:48:18", "link": "http://arxiv.org/abs/2305.07085v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overinformative Question Answering by Humans and Machines", "abstract": "When faced with a polar question, speakers often provide overinformative\nanswers going beyond a simple \"yes\" or \"no\". But what principles guide the\nselection of additional information? In this paper, we provide experimental\nevidence from two studies suggesting that overinformativeness in human\nanswering is driven by considerations of relevance to the questioner's goals\nwhich they flexibly adjust given the functional context in which the question\nis uttered. We take these human results as a strong benchmark for investigating\nquestion-answering performance in state-of-the-art neural language models,\nconducting an extensive evaluation on items from human experiments. We find\nthat most models fail to adjust their answering behavior in a human-like way\nand tend to include irrelevant information. We show that GPT-3 is highly\nsensitive to the form of the prompt and only achieves human-like answer\npatterns when guided by an example and cognitively-motivated explanation.", "published": "2023-05-11 21:41:41", "link": "http://arxiv.org/abs/2305.07151v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SmartPhone: Exploring Keyword Mnemonic with Auto-generated Verbal and\n  Visual Cues", "abstract": "In second language vocabulary learning, existing works have primarily focused\non either the learning interface or scheduling personalized retrieval practices\nto maximize memory retention. However, the learning content, i.e., the\ninformation presented on flashcards, has mostly remained constant. Keyword\nmnemonic is a notable learning strategy that relates new vocabulary to existing\nknowledge by building an acoustic and imagery link using a keyword that sounds\nalike. Beyond that, producing verbal and visual cues associated with the\nkeyword to facilitate building these links requires a manual process and is not\nscalable. In this paper, we explore an opportunity to use large language models\nto automatically generate verbal and visual cues for keyword mnemonics. Our\napproach, an end-to-end pipeline for auto-generating verbal and visual cues,\ncan automatically generate highly memorable cues. We investigate the\neffectiveness of our approach via a human participant experiment by comparing\nit with manually generated cues.", "published": "2023-05-11 20:58:10", "link": "http://arxiv.org/abs/2305.10436v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel Dataset Towards Extracting Virus-Host Interactions", "abstract": "We describe a novel dataset for the automated recognition of named taxonomic\nand other entities relevant to the association of viruses with their hosts. We\nfurther describe some initial results using pre-trained models on the\nnamed-entity recognition (NER) task on this novel dataset. We propose that our\ndataset of manually annotated abstracts now offers a Gold Standard Corpus for\ntraining future NER models in the automated extraction of host-pathogen\ndetection methods from scientific publications, and further explain how our\nwork makes first steps towards predicting the important human health-related\nconcept of viral spillover risk automatically from the scientific literature.", "published": "2023-05-11 17:20:49", "link": "http://arxiv.org/abs/2305.13317v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unicode Normalization and Grapheme Parsing of Indic Languages", "abstract": "Writing systems of Indic languages have orthographic syllables, also known as\ncomplex graphemes, as unique horizontal units. A prominent feature of these\nlanguages is these complex grapheme units that comprise consonants/consonant\nconjuncts, vowel diacritics, and consonant diacritics, which, together make a\nunique Language. Unicode-based writing schemes of these languages often\ndisregard this feature of these languages and encode words as linear sequences\nof Unicode characters using an intricate scheme of connector characters and\nfont interpreters. Due to this way of using a few dozen Unicode glyphs to write\nthousands of different unique glyphs (complex graphemes), there are serious\nambiguities that lead to malformed words. In this paper, we are proposing two\nlibraries: i) a normalizer for normalizing inconsistencies caused by a\nUnicode-based encoding scheme for Indic languages and ii) a grapheme parser for\nAbugida text. It deconstructs words into visually distinct orthographic\nsyllables or complex graphemes and their constituents. Our proposed normalizer\nis a more efficient and effective tool than the previously used IndicNLP\nnormalizer. Moreover, our parser and normalizer are also suitable tools for\ngeneral Abugida text processing as they performed well in our robust word-based\nand NLP experiments. We report the pipeline for the scripts of 7 languages in\nthis work and develop the framework for the integration of more scripts.", "published": "2023-05-11 14:34:08", "link": "http://arxiv.org/abs/2306.01743v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Randomized Smoothing with Masked Inference for Adversarially Robust Text\n  Classifications", "abstract": "Large-scale pre-trained language models have shown outstanding performance in\na variety of NLP tasks. However, they are also known to be significantly\nbrittle against specifically crafted adversarial examples, leading to\nincreasing interest in probing the adversarial robustness of NLP systems. We\nintroduce RSMI, a novel two-stage framework that combines randomized smoothing\n(RS) with masked inference (MI) to improve the adversarial robustness of NLP\nsystems. RS transforms a classifier into a smoothed classifier to obtain robust\nrepresentations, whereas MI forces a model to exploit the surrounding context\nof a masked token in an input sequence. RSMI improves adversarial robustness by\n2 to 3 times over existing state-of-the-art methods on benchmark datasets. We\nalso perform in-depth qualitative analysis to validate the effectiveness of the\ndifferent stages of RSMI and probe the impact of its components through\nextensive ablations. By empirically proving the stability of RSMI, we put it\nforward as a practical method to robustly train large-scale NLP models. Our\ncode and datasets are available at https://github.com/Han8931/rsmi_nlp", "published": "2023-05-11 01:50:16", "link": "http://arxiv.org/abs/2305.06522v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GeoGLUE: A GeoGraphic Language Understanding Evaluation Benchmark", "abstract": "With a fast developing pace of geographic applications, automatable and\nintelligent models are essential to be designed to handle the large volume of\ninformation. However, few researchers focus on geographic natural language\nprocessing, and there has never been a benchmark to build a unified standard.\nIn this work, we propose a GeoGraphic Language Understanding Evaluation\nbenchmark, named GeoGLUE. We collect data from open-released geographic\nresources and introduce six natural language understanding tasks, including\ngeographic textual similarity on recall, geographic textual similarity on\nrerank, geographic elements tagging, geographic composition analysis,\ngeographic where what cut, and geographic entity alignment. We also pro vide\nevaluation experiments and analysis of general baselines, indicating the\neffectiveness and significance of the GeoGLUE benchmark.", "published": "2023-05-11 03:21:56", "link": "http://arxiv.org/abs/2305.06545v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ONCE: Boosting Content-based Recommendation with Both Open- and\n  Closed-source Large Language Models", "abstract": "Personalized content-based recommender systems have become indispensable\ntools for users to navigate through the vast amount of content available on\nplatforms like daily news websites and book recommendation services. However,\nexisting recommenders face significant challenges in understanding the content\nof items. Large language models (LLMs), which possess deep semantic\ncomprehension and extensive knowledge from pretraining, have proven to be\neffective in various natural language processing tasks. In this study, we\nexplore the potential of leveraging both open- and closed-source LLMs to\nenhance content-based recommendation. With open-source LLMs, we utilize their\ndeep layers as content encoders, enriching the representation of content at the\nembedding level. For closed-source LLMs, we employ prompting techniques to\nenrich the training data at the token level. Through comprehensive experiments,\nwe demonstrate the high effectiveness of both types of LLMs and show the\nsynergistic relationship between them. Notably, we observed a significant\nrelative improvement of up to 19.32% compared to existing state-of-the-art\nrecommendation models. These findings highlight the immense potential of both\nopen- and closed-source of LLMs in enhancing content-based recommendation\nsystems. We will make our code and LLM-generated data available for other\nresearchers to reproduce our results.", "published": "2023-05-11 04:51:21", "link": "http://arxiv.org/abs/2305.06566v4", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Fused Gromov-Wasserstein Framework for Unsupervised Knowledge Graph\n  Entity Alignment", "abstract": "Entity alignment is the task of identifying corresponding entities across\ndifferent knowledge graphs (KGs). Although recent embedding-based entity\nalignment methods have shown significant advancements, they still struggle to\nfully utilize KG structural information. In this paper, we introduce FGWEA, an\nunsupervised entity alignment framework that leverages the Fused\nGromov-Wasserstein (FGW) distance, allowing for a comprehensive comparison of\nentity semantics and KG structures within a joint optimization framework. To\naddress the computational challenges associated with optimizing FGW, we devise\na three-stage progressive optimization algorithm. It starts with a basic\nsemantic embedding matching, proceeds to approximate cross-KG structural and\nrelational similarity matching based on iterative updates of high-confidence\nentity links, and ultimately culminates in a global structural comparison\nbetween KGs. We perform extensive experiments on four entity alignment datasets\ncovering 14 distinct KGs across five languages. Without any supervision or\nhyper-parameter tuning, FGWEA surpasses 21 competitive baselines, including\ncutting-edge supervised entity alignment methods. Our code is available at\nhttps://github.com/squareRoot3/FusedGW-Entity-Alignment.", "published": "2023-05-11 05:17:54", "link": "http://arxiv.org/abs/2305.06574v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SemEval-2023 Task 2: Fine-grained Multilingual Named Entity Recognition\n  (MultiCoNER 2)", "abstract": "We present the findings of SemEval-2023 Task 2 on Fine-grained Multilingual\nNamed Entity Recognition (MultiCoNER 2). Divided into 13 tracks, the task\nfocused on methods to identify complex fine-grained named entities (like\nWRITTENWORK, VEHICLE, MUSICALGRP) across 12 languages, in both monolingual and\nmultilingual scenarios, as well as noisy settings. The task used the MultiCoNER\nV2 dataset, composed of 2.2 million instances in Bangla, Chinese, English,\nFarsi, French, German, Hindi, Italian., Portuguese, Spanish, Swedish, and\nUkrainian. MultiCoNER 2 was one of the most popular tasks of SemEval-2023. It\nattracted 842 submissions from 47 teams, and 34 teams submitted system papers.\nResults showed that complex entity types such as media titles and product names\nwere the most challenging. Methods fusing external knowledge into transformer\nmodels achieved the best performance, and the largest gains were on the\nCreative Work and Group classes, which are still challenging even with external\nknowledge. Some fine-grained classes proved to be more challenging than others,\nsuch as SCIENTIST, ARTWORK, and PRIVATECORP. We also observed that noisy data\nhas a significant impact on model performance, with an average drop of 10% on\nthe noisy subset. The task highlights the need for future research on improving\nNER robustness on noisy data containing complex entities.", "published": "2023-05-11 05:56:08", "link": "http://arxiv.org/abs/2305.06586v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FactKG: Fact Verification via Reasoning on Knowledge Graphs", "abstract": "In real world applications, knowledge graphs (KG) are widely used in various\ndomains (e.g. medical applications and dialogue agents). However, for fact\nverification, KGs have not been adequately utilized as a knowledge source. KGs\ncan be a valuable knowledge source in fact verification due to their\nreliability and broad applicability. A KG consists of nodes and edges which\nmakes it clear how concepts are linked together, allowing machines to reason\nover chains of topics. However, there are many challenges in understanding how\nthese machine-readable concepts map to information in text. To enable the\ncommunity to better use KGs, we introduce a new dataset, FactKG: Fact\nVerification via Reasoning on Knowledge Graphs. It consists of 108k natural\nlanguage claims with five types of reasoning: One-hop, Conjunction, Existence,\nMulti-hop, and Negation. Furthermore, FactKG contains various linguistic\npatterns, including colloquial style claims as well as written style claims to\nincrease practicality. Lastly, we develop a baseline approach and analyze\nFactKG over these reasoning types. We believe FactKG can advance both\nreliability and practicality in KG-based fact verification.", "published": "2023-05-11 06:08:49", "link": "http://arxiv.org/abs/2305.06590v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Structured Chain-of-Thought Prompting for Code Generation", "abstract": "Large Language Models (LLMs) (e.g., ChatGPT) have shown impressive\nperformance in code generation. LLMs take prompts as inputs, and\nChain-of-Thought (CoT) prompting is the state-of-the-art prompting technique.\nCoT prompting asks LLMs first to generate CoTs (i.e., intermediate natural\nlanguage reasoning steps) and then output the code. However, CoT prompting is\ndesigned for natural language generation and has low accuracy in code\ngeneration.\n  In this paper, we propose Structured CoTs (SCoTs) and present a novel\nprompting technique for code generation, named SCoT prompting. Our motivation\nis source code contains rich structural information and any code can be\ncomposed of three program structures (i.e., sequence, branch, and loop\nstructures). Intuitively, structured intermediate reasoning steps make for\nstructured source code. Thus, we ask LLMs to use program structures to build\nCoTs, obtaining SCoTs. Then, LLMs generate the final code based on SCoTs.\nCompared to CoT prompting, SCoT prompting explicitly constrains LLMs to think\nabout how to solve requirements from the view of source code and further the\nperformance of LLMs in code generation. We apply SCoT prompting to two LLMs\n(i.e., ChatGPT and Codex) and evaluate it on three benchmarks (i.e., HumanEval,\nMBPP, and MBCPP). (1) SCoT prompting outperforms the state-of-the-art baseline\n- CoT prompting by up to 13.79% in Pass@1. (2) Human evaluation shows human\ndevelopers prefer programs from SCoT prompting. (3) SCoT prompting is robust to\nexamples and achieves substantial improvements.", "published": "2023-05-11 06:43:37", "link": "http://arxiv.org/abs/2305.06599v3", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "When the Majority is Wrong: Modeling Annotator Disagreement for\n  Subjective Tasks", "abstract": "Though majority vote among annotators is typically used for ground truth\nlabels in natural language processing, annotator disagreement in tasks such as\nhate speech detection may reflect differences in opinion across groups, not\nnoise. Thus, a crucial problem in hate speech detection is determining whether\na statement is offensive to the demographic group that it targets, when that\ngroup may constitute a small fraction of the annotator pool. We construct a\nmodel that predicts individual annotator ratings on potentially offensive text\nand combines this information with the predicted target group of the text to\nmodel the opinions of target group members. We show gains across a range of\nmetrics, including raising performance over the baseline by 22% at predicting\nindividual annotators' ratings and by 33% at predicting variance among\nannotators, which provides a metric for model uncertainty downstream. We find\nthat annotator ratings can be predicted using their demographic information and\nopinions on online content, without the need to track identifying annotator IDs\nthat link each annotator to their ratings. We also find that use of\nnon-invasive survey questions on annotators' online experiences helps to\nmaximize privacy and minimize unnecessary collection of demographic information\nwhen predicting annotators' opinions.", "published": "2023-05-11 07:55:20", "link": "http://arxiv.org/abs/2305.06626v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cost-efficient Crowdsourcing for Span-based Sequence Labeling: Worker\n  Selection and Data Augmentation", "abstract": "This paper introduces a novel crowdsourcing worker selection algorithm,\nenhancing annotation quality and reducing costs. Unlike previous studies\ntargeting simpler tasks, this study contends with the complexities of label\ninterdependencies in sequence labeling. The proposed algorithm utilizes a\nCombinatorial Multi-Armed Bandit (CMAB) approach for worker selection, and a\ncost-effective human feedback mechanism. The challenge of dealing with\nimbalanced and small-scale datasets, which hinders offline simulation of worker\nselection, is tackled using an innovative data augmentation method termed\nshifting, expanding, and shrinking (SES). Rigorous testing on CoNLL 2003 NER\nand Chinese OEI datasets showcased the algorithm's efficiency, with an increase\nin F1 score up to 100.04% of the expert-only baseline, alongside cost savings\nup to 65.97%. The paper also encompasses a dataset-independent test emulating\nannotation evaluation through a Bernoulli distribution, which still led to an\nimpressive 97.56% F1 score of the expert baseline and 59.88% cost savings.\nFurthermore, our approach can be seamlessly integrated into Reinforcement\nLearning from Human Feedback (RLHF) systems, offering a cost-effective solution\nfor obtaining human feedback.", "published": "2023-05-11 09:40:24", "link": "http://arxiv.org/abs/2305.06683v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable\n  ELements for explaining neural net classifiers on NLP tasks", "abstract": "Transformer architectures are complex and their use in NLP, while it has\nengendered many successes, makes their interpretability or explainability\nchallenging. Recent debates have shown that attention maps and attribution\nmethods are unreliable (Pruthi et al., 2019; Brunner et al., 2019). In this\npaper, we present some of their limitations and introduce COCKATIEL, which\nsuccessfully addresses some of them. COCKATIEL is a novel, post-hoc,\nconcept-based, model-agnostic XAI technique that generates meaningful\nexplanations from the last layer of a neural net model trained on an NLP\nclassification task by using Non-Negative Matrix Factorization (NMF) to\ndiscover the concepts the model leverages to make predictions and by exploiting\na Sensitivity Analysis to estimate accurately the importance of each of these\nconcepts for the model. It does so without compromising the accuracy of the\nunderlying model or requiring a new one to be trained. We conduct experiments\nin single and multi-aspect sentiment analysis tasks and we show COCKATIEL's\nsuperior ability to discover concepts that align with humans' on Transformer\nmodels without any supervision, we objectively verify the faithfulness of its\nexplanations through fidelity metrics, and we showcase its ability to provide\nmeaningful explanations in two different datasets.", "published": "2023-05-11 12:22:20", "link": "http://arxiv.org/abs/2305.06754v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "THUIR@COLIEE 2023: Incorporating Structural Knowledge into Pre-trained\n  Language Models for Legal Case Retrieval", "abstract": "Legal case retrieval techniques play an essential role in modern intelligent\nlegal systems. As an annually well-known international competition, COLIEE is\naiming to achieve the state-of-the-art retrieval model for legal texts. This\npaper summarizes the approach of the championship team THUIR in COLIEE 2023. To\nbe specific, we design structure-aware pre-trained language models to enhance\nthe understanding of legal cases. Furthermore, we propose heuristic\npre-processing and post-processing approaches to reduce the influence of\nirrelevant messages. In the end, learning-to-rank methods are employed to merge\nfeatures with different dimensions. Experimental results demonstrate the\nsuperiority of our proposal. Official results show that our run has the best\nperformance among all submissions. The implementation of our method can be\nfound at https://github.com/CSHaitao/THUIR-COLIEE2023.", "published": "2023-05-11 14:08:53", "link": "http://arxiv.org/abs/2305.06812v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "THUIR@COLIEE 2023: More Parameters and Legal Knowledge for Legal Case\n  Entailment", "abstract": "This paper describes the approach of the THUIR team at the COLIEE 2023 Legal\nCase Entailment task. This task requires the participant to identify a specific\nparagraph from a given supporting case that entails the decision for the query\ncase. We try traditional lexical matching methods and pre-trained language\nmodels with different sizes. Furthermore, learning-to-rank methods are employed\nto further improve performance. However, learning-to-rank is not very robust on\nthis task. which suggests that answer passages cannot simply be determined with\ninformation retrieval techniques. Experimental results show that more\nparameters and legal knowledge contribute to the legal case entailment task.\nFinally, we get the third place in COLIEE 2023. The implementation of our\nmethod can be found at https://github.com/CSHaitao/THUIR-COLIEE2023.", "published": "2023-05-11 14:11:48", "link": "http://arxiv.org/abs/2305.06817v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Think Twice: Measuring the Efficiency of Eliminating Prediction\n  Shortcuts of Question Answering Models", "abstract": "While the Large Language Models (LLMs) dominate a majority of language\nunderstanding tasks, previous work shows that some of these results are\nsupported by modelling spurious correlations of training datasets. Authors\ncommonly assess model robustness by evaluating their models on\nout-of-distribution (OOD) datasets of the same task, but these datasets might\nshare the bias of the training dataset.\n  We propose a simple method for measuring a scale of models' reliance on any\nidentified spurious feature and assess the robustness towards a large set of\nknown and newly found prediction biases for various pre-trained models and\ndebiasing methods in Question Answering (QA). We find that while existing\ndebiasing methods can mitigate reliance on a chosen spurious feature, the OOD\nperformance gains of these methods can not be explained by mitigated reliance\non biased features, suggesting that biases are shared among different QA\ndatasets. Finally, we evidence this to be the case by measuring that the\nperformance of models trained on different QA datasets relies comparably on the\nsame bias features. We hope these results will motivate future work to refine\nthe reports of LMs' robustness to a level of adversarial samples addressing\nspecific spurious features.", "published": "2023-05-11 14:35:00", "link": "http://arxiv.org/abs/2305.06841v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Active Retrieval Augmented Generation", "abstract": "Despite the remarkable ability of large language models (LMs) to comprehend\nand generate language, they have a tendency to hallucinate and create factually\ninaccurate output. Augmenting LMs by retrieving information from external\nknowledge resources is one promising solution. Most existing retrieval\naugmented LMs employ a retrieve-and-generate setup that only retrieves\ninformation once based on the input. This is limiting, however, in more general\nscenarios involving generation of long texts, where continually gathering\ninformation throughout generation is essential. In this work, we provide a\ngeneralized view of active retrieval augmented generation, methods that\nactively decide when and what to retrieve across the course of the generation.\nWe propose Forward-Looking Active REtrieval augmented generation (FLARE), a\ngeneric method which iteratively uses a prediction of the upcoming sentence to\nanticipate future content, which is then utilized as a query to retrieve\nrelevant documents to regenerate the sentence if it contains low-confidence\ntokens. We test FLARE along with baselines comprehensively over 4 long-form\nknowledge-intensive generation tasks/datasets. FLARE achieves superior or\ncompetitive performance on all tasks, demonstrating the effectiveness of our\nmethod. Code and datasets are available at https://github.com/jzbjyb/FLARE.", "published": "2023-05-11 17:13:40", "link": "http://arxiv.org/abs/2305.06983v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SMATCH++: Standardized and Extended Evaluation of Semantic Graphs", "abstract": "The Smatch metric is a popular method for evaluating graph distances, as is\nnecessary, for instance, to assess the performance of semantic graph parsing\nsystems. However, we observe some issues in the metric that jeopardize\nmeaningful evaluation. E.g., opaque pre-processing choices can affect results,\nand current graph-alignment solvers do not provide us with upper-bounds.\nWithout upper-bounds, however, fair evaluation is not guaranteed. Furthermore,\nadaptions of Smatch for extended tasks (e.g., fine-grained semantic similarity)\nare spread out, and lack a unifying framework.\n  For better inspection, we divide the metric into three modules:\npre-processing, alignment, and scoring. Examining each module, we specify its\ngoals and diagnose potential issues, for which we discuss and test mitigation\nstrategies. For pre-processing, we show how to fully conform to annotation\nguidelines that allow structurally deviating but valid graphs. For safer and\nenhanced alignment, we show the feasibility of optimal alignment in a standard\nevaluation setup, and develop a lossless graph compression method that shrinks\nthe search space and significantly increases efficiency. For improved scoring,\nwe propose standardized and extended metric calculation of fine-grained\nsub-graph meaning aspects. Our code is available at\nhttps://github.com/flipz357/smatchpp", "published": "2023-05-11 17:29:47", "link": "http://arxiv.org/abs/2305.06993v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Recommendation as Instruction Following: A Large Language Model\n  Empowered Recommendation Approach", "abstract": "In the past decades, recommender systems have attracted much attention in\nboth research and industry communities, and a large number of studies have been\ndevoted to developing effective recommendation models. Basically speaking,\nthese models mainly learn the underlying user preference from historical\nbehavior data, and then estimate the user-item matching relationships for\nrecommendations. Inspired by the recent progress on large language models\n(LLMs), we take a different approach to developing the recommendation models,\nconsidering recommendation as instruction following by LLMs. The key idea is\nthat the preferences or needs of a user can be expressed in natural language\ndescriptions (called instructions), so that LLMs can understand and further\nexecute the instruction for fulfilling the recommendation task. Instead of\nusing public APIs of LLMs, we instruction tune an open-source LLM (3B\nFlan-T5-XL), in order to better adapt LLMs to recommender systems. For this\npurpose, we first design a general instruction format for describing the\npreference, intention, task form and context of a user in natural language.\nThen we manually design 39 instruction templates and automatically generate a\nlarge amount of user-personalized instruction data (252K instructions) with\nvarying types of preferences and intentions. To demonstrate the effectiveness\nof our approach, we instantiate the instruction templates into several\nwidely-studied recommendation (or search) tasks, and conduct extensive\nexperiments on these tasks with real-world datasets. Experiment results show\nthat the proposed approach can outperform several competitive baselines,\nincluding the powerful GPT-3.5, on these evaluation tasks. Our approach sheds\nlight on developing more user-friendly recommender systems, in which users can\nfreely communicate with the system and obtain more accurate recommendations via\nnatural language instructions.", "published": "2023-05-11 17:39:07", "link": "http://arxiv.org/abs/2305.07001v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Exploring Zero and Few-shot Techniques for Intent Classification", "abstract": "Conversational NLU providers often need to scale to thousands of\nintent-classification models where new customers often face the cold-start\nproblem. Scaling to so many customers puts a constraint on storage space as\nwell. In this paper, we explore four different zero and few-shot intent\nclassification approaches with this low-resource constraint: 1) domain\nadaptation, 2) data augmentation, 3) zero-shot intent classification using\ndescriptions large language models (LLMs), and 4) parameter-efficient\nfine-tuning of instruction-finetuned language models. Our results show that all\nthese approaches are effective to different degrees in low-resource settings.\nParameter-efficient fine-tuning using T-few recipe (Liu et al., 2022) on\nFlan-T5 (Chang et al., 2022) yields the best performance even with just one\nsample per intent. We also show that the zero-shot method of prompting LLMs\nusing intent descriptions", "published": "2023-05-11 22:07:27", "link": "http://arxiv.org/abs/2305.07157v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Toxicity Inspector: A Framework to Evaluate Ground Truth in Toxicity\n  Detection Through Feedback", "abstract": "Toxic language is difficult to define, as it is not monolithic and has many\nvariations in perceptions of toxicity. This challenge of detecting toxic\nlanguage is increased by the highly contextual and subjectivity of its\ninterpretation, which can degrade the reliability of datasets and negatively\naffect detection model performance. To fill this void, this paper introduces a\ntoxicity inspector framework that incorporates a human-in-the-loop pipeline\nwith the aim of enhancing the reliability of toxicity benchmark datasets by\ncentering the evaluator's values through an iterative feedback cycle. The\ncenterpiece of this framework is the iterative feedback process, which is\nguided by two metric types (hard and soft) that provide evaluators and dataset\ncreators with insightful examination to balance the tradeoff between\nperformance gains and toxicity avoidance.", "published": "2023-05-11 11:56:42", "link": "http://arxiv.org/abs/2305.10433v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Generative Pre-trained Transformer: A Comprehensive Review on Enabling\n  Technologies, Potential Applications, Emerging Challenges, and Future\n  Directions", "abstract": "The Generative Pre-trained Transformer (GPT) represents a notable\nbreakthrough in the domain of natural language processing, which is propelling\nus toward the development of machines that can understand and communicate using\nlanguage in a manner that closely resembles that of humans. GPT is based on the\ntransformer architecture, a deep neural network designed for natural language\nprocessing tasks. Due to their impressive performance on natural language\nprocessing tasks and ability to effectively converse, GPT have gained\nsignificant popularity among researchers and industrial communities, making\nthem one of the most widely used and effective models in natural language\nprocessing and related fields, which motivated to conduct this review. This\nreview provides a detailed overview of the GPT, including its architecture,\nworking process, training procedures, enabling technologies, and its impact on\nvarious applications. In this review, we also explored the potential challenges\nand limitations of a GPT. Furthermore, we discuss potential solutions and\nfuture directions. Overall, this paper aims to provide a comprehensive\nunderstanding of GPT, enabling technologies, their impact on various\napplications, emerging challenges, and potential solutions.", "published": "2023-05-11 19:20:38", "link": "http://arxiv.org/abs/2305.10435v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Good are Commercial Large Language Models on African Languages?", "abstract": "Recent advancements in Natural Language Processing (NLP) has led to the\nproliferation of large pretrained language models. These models have been shown\nto yield good performance, using in-context learning, even on unseen tasks and\nlanguages. They have also been exposed as commercial APIs as a form of\nlanguage-model-as-a-service, with great adoption. However, their performance on\nAfrican languages is largely unknown. We present a preliminary analysis of\ncommercial large language models on two tasks (machine translation and text\nclassification) across eight African languages, spanning different language\nfamilies and geographical areas. Our results suggest that commercial language\nmodels produce below-par performance on African languages. We also find that\nthey perform better on text classification than machine translation. In\ngeneral, our findings present a call-to-action to ensure African languages are\nwell represented in commercial large language models, given their growing\npopularity.", "published": "2023-05-11 02:29:53", "link": "http://arxiv.org/abs/2305.06530v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Domain Incremental Lifelong Learning in an Open World", "abstract": "Lifelong learning (LL) is an important ability for NLP models to learn new\ntasks continuously. Architecture-based approaches are reported to be effective\nimplementations for LL models. However, it is non-trivial to extend previous\napproaches to domain incremental LL scenarios since they either require access\nto task identities in the testing phase or cannot handle samples from unseen\ntasks. In this paper, we propose \\textbf{Diana}: a\n\\underline{d}ynam\\underline{i}c \\underline{a}rchitecture-based\nlifelo\\underline{n}g le\\underline{a}rning model that tries to learn a sequence\nof tasks with a prompt-enhanced language model. Four types of hierarchically\norganized prompts are used in Diana to capture knowledge from different\ngranularities. Specifically, we dedicate task-level prompts to capture\ntask-specific knowledge to retain high LL performances and maintain\ninstance-level prompts to learn knowledge shared across input samples to\nimprove the model's generalization performance. Moreover, we dedicate separate\nprompts to explicitly model unseen tasks and introduce a set of prompt key\nvectors to facilitate knowledge sharing between tasks. Extensive experiments\ndemonstrate that Diana outperforms state-of-the-art LL models, especially in\nhandling unseen tasks. We release the code and data at\n\\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/diana}.", "published": "2023-05-11 04:19:08", "link": "http://arxiv.org/abs/2305.06555v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Long-Tailed Question Answering in an Open World", "abstract": "Real-world data often have an open long-tailed distribution, and building a\nunified QA model supporting various tasks is vital for practical QA\napplications. However, it is non-trivial to extend previous QA approaches since\nthey either require access to seen tasks of adequate samples or do not\nexplicitly model samples from unseen tasks. In this paper, we define Open\nLong-Tailed QA (OLTQA) as learning from long-tailed distributed data and\noptimizing performance over seen and unseen QA tasks. We propose an OLTQA model\nthat encourages knowledge sharing between head, tail and unseen tasks, and\nexplicitly mines knowledge from a large pre-trained language model (LM).\nSpecifically, we organize our model through a pool of fine-grained components\nand dynamically combine these components for an input to facilitate knowledge\nsharing. A retrieve-then-rerank frame is further introduced to select\nin-context examples, which guild the LM to generate text that express knowledge\nfor QA tasks. Moreover, a two-stage training approach is introduced to\npre-train the framework by knowledge distillation (KD) from the LM and then\njointly train the frame and a QA model through an adaptive mutual KD method. On\na large-scale OLTQA dataset we curate from 43 existing QA datasets, our model\nconsistently outperforms the state-of-the-art. We release the code and data at\n\\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/oltqa}.", "published": "2023-05-11 04:28:58", "link": "http://arxiv.org/abs/2305.06557v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How to Index Item IDs for Recommendation Foundation Models", "abstract": "Recommendation foundation model utilizes large language models (LLM) for\nrecommendation by converting recommendation tasks into natural language tasks.\nIt enables generative recommendation which directly generates the item(s) to\nrecommend rather than calculating a ranking score for each and every candidate\nitem as in traditional recommendation models, simplifying the recommendation\npipeline from multi-stage filtering to single-stage filtering. To avoid\ngenerating excessively long text and hallucinated recommendations when deciding\nwhich item(s) to recommend, creating LLM-compatible item IDs to uniquely\nidentify each item is essential for recommendation foundation models. In this\nstudy, we systematically examine the item ID creation and indexing problem for\nrecommendation foundation models, using P5 as an example of the backbone LLM.\nTo emphasize the importance of item indexing, we first discuss the issues of\nseveral trivial item indexing methods, such as random indexing, title indexing,\nand independent indexing. We then propose four simple yet effective solutions,\nincluding sequential indexing, collaborative indexing, semantic (content-based)\nindexing, and hybrid indexing. Our study highlights the significant influence\nof item indexing methods on the performance of LLM-based recommendation, and\nour results on real-world datasets validate the effectiveness of our proposed\nsolutions. The research also demonstrates how recent advances on language\nmodeling and traditional IR principles such as indexing can help each other for\nbetter learning and inference. Source code and data are available at\nhttps://github.com/Wenyueh/LLM-RecSys-ID.", "published": "2023-05-11 05:02:37", "link": "http://arxiv.org/abs/2305.06569v6", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of\n  Language Models", "abstract": "A salient characteristic of pre-trained language models (PTLMs) is a\nremarkable improvement in their generalization capability and emergence of new\ncapabilities with increasing model capacity and pre-training dataset size.\nConsequently, we are witnessing the development of enormous models pushing the\nstate-of-the-art. It is, however, imperative to realize that this inevitably\nleads to prohibitively long training times, extortionate computing costs, and a\ndetrimental environmental impact. Significant efforts are underway to make PTLM\ntraining more efficient through innovations in model architectures, training\npipelines, and loss function design, with scant attention being paid to\noptimizing the utility of training data. The key question that we ask is\nwhether it is possible to train PTLMs by employing only highly informative\nsubsets of the training data while maintaining downstream performance? Building\nupon the recent progress in informative data subset selection, we show how we\ncan employ submodular optimization to select highly representative subsets of\nthe training corpora and demonstrate that the proposed framework can be applied\nto efficiently train multiple PTLMs (BERT, BioBERT, GPT-2) using only a\nfraction of data. Further, we perform a rigorous empirical evaluation to show\nthat the resulting models achieve up to $\\sim99\\%$ of the performance of the\nfully-trained models. We made our framework publicly available at\nhttps://github.com/Efficient-AI/ingenious.", "published": "2023-05-11 09:24:41", "link": "http://arxiv.org/abs/2305.06677v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WebCPM: Interactive Web Search for Chinese Long-form Question Answering", "abstract": "Long-form question answering (LFQA) aims at answering complex, open-ended\nquestions with detailed, paragraph-length responses. The de facto paradigm of\nLFQA necessitates two procedures: information retrieval, which searches for\nrelevant supporting facts, and information synthesis, which integrates these\nfacts into a coherent answer. In this paper, we introduce WebCPM, the first\nChinese LFQA dataset. One unique feature of WebCPM is that its information\nretrieval is based on interactive web search, which engages with a search\nengine in real time. Following WebGPT, we develop a web search interface. We\nrecruit annotators to search for relevant information using our interface and\nthen answer questions. Meanwhile, the web search behaviors of our annotators\nwould be recorded. In total, we collect 5,500 high-quality question-answer\npairs, together with 14,315 supporting facts and 121,330 web search actions. We\nfine-tune pre-trained language models to imitate human behaviors for web search\nand to generate answers based on the collected facts. Our LFQA pipeline, built\non these fine-tuned models, generates answers that are no worse than\nhuman-written ones in 32.5% and 47.5% of the cases on our dataset and DuReader,\nrespectively.", "published": "2023-05-11 14:47:29", "link": "http://arxiv.org/abs/2305.06849v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "AfriQA: Cross-lingual Open-Retrieval Question Answering for African\n  Languages", "abstract": "African languages have far less in-language content available digitally,\nmaking it challenging for question answering systems to satisfy the information\nneeds of users. Cross-lingual open-retrieval question answering (XOR QA)\nsystems -- those that retrieve answer content from other languages while\nserving people in their native language -- offer a means of filling this gap.\nTo this end, we create AfriQA, the first cross-lingual QA dataset with a focus\non African languages. AfriQA includes 12,000+ XOR QA examples across 10 African\nlanguages. While previous datasets have focused primarily on languages where\ncross-lingual QA augments coverage from the target language, AfriQA focuses on\nlanguages where cross-lingual answer content is the only high-coverage source\nof answer content. Because of this, we argue that African languages are one of\nthe most important and realistic use cases for XOR QA. Our experiments\ndemonstrate the poor performance of automatic translation and multilingual\nretrieval methods. Overall, AfriQA proves challenging for state-of-the-art QA\nmodels. We hope that the dataset enables the development of more equitable QA\ntechnology.", "published": "2023-05-11 15:34:53", "link": "http://arxiv.org/abs/2305.06897v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Self-Chained Image-Language Model for Video Localization and Question\n  Answering", "abstract": "Recent studies have shown promising results on utilizing large pre-trained\nimage-language models for video question answering. While these image-language\nmodels can efficiently bootstrap the representation learning of video-language\nmodels, they typically concatenate uniformly sampled video frames as visual\ninputs without explicit language-aware, temporal modeling. When only a portion\nof a video input is relevant to the language query, such uniform frame sampling\ncan often lead to missing important visual cues. Although humans often find a\nvideo moment to focus on and rewind the moment to answer questions, training a\nquery-aware video moment localizer often requires expensive annotations and\nhigh computational costs. To address this issue, we propose Self-Chained Video\nLocalization-Answering (SeViLA), a novel framework that leverages a single\nimage-language model (BLIP-2) to tackle both temporal keyframe localization and\nQA on videos. SeViLA framework consists of two modules: Localizer and Answerer,\nwhere both are parameter-efficiently fine-tuned from BLIP-2. We propose two\nways of chaining these modules for cascaded inference and self-refinement.\nFirst, in the forward chain, the Localizer finds multiple language-aware\nkeyframes in a video, which the Answerer uses to predict the answer. Second, in\nthe reverse chain, the Answerer generates keyframe pseudo-labels to refine the\nLocalizer, alleviating the need for expensive video moment localization\nannotations. Our SeViLA framework outperforms several strong baselines on 5\nchallenging video QA and event prediction benchmarks, and achieves the\nstate-of-the-art in both fine-tuning (NExT-QA, STAR) and zero-shot (NExT-QA,\nSTAR, How2QA, VLEP) settings. We also analyze the impact of Localizer,\ncomparisons of Localizer with other temporal localization models,\npre-training/self-refinement of Localizer, and varying the number of keyframes.", "published": "2023-05-11 17:23:00", "link": "http://arxiv.org/abs/2305.06988v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Region-Aware Pretraining for Open-Vocabulary Object Detection with\n  Vision Transformers", "abstract": "We present Region-aware Open-vocabulary Vision Transformers (RO-ViT) - a\ncontrastive image-text pretraining recipe to bridge the gap between image-level\npretraining and open-vocabulary object detection. At the pretraining phase, we\npropose to randomly crop and resize regions of positional embeddings instead of\nusing the whole image positional embeddings. This better matches the use of\npositional embeddings at region-level in the detection finetuning phase. In\naddition, we replace the common softmax cross entropy loss in contrastive\nlearning with focal loss to better learn the informative yet difficult\nexamples. Finally, we leverage recent advances in novel object proposals to\nimprove open-vocabulary detection finetuning. We evaluate our full model on the\nLVIS and COCO open-vocabulary detection benchmarks and zero-shot transfer.\nRO-ViT achieves a state-of-the-art 34.1 $AP_r$ on LVIS, surpassing the best\nexisting approach by +7.8 points in addition to competitive zero-shot transfer\ndetection. Surprisingly, RO-ViT improves the image-level representation as well\nand achieves the state of the art on 9 out of 12 metrics on COCO and Flickr\nimage-text retrieval benchmarks, outperforming competitive approaches with\nlarger models.", "published": "2023-05-11 17:53:29", "link": "http://arxiv.org/abs/2305.07011v4", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Musketeer: Joint Training for Multi-task Vision Language Model with Task\n  Explanation Prompts", "abstract": "We present a vision-language model whose parameters are jointly trained on\nall tasks and fully shared among multiple heterogeneous tasks which may\ninterfere with each other, resulting in a single model which we named\nMusketeer. The integration of knowledge across heterogeneous tasks is enabled\nby a novel feature called Task Explanation Prompt (TEP). With rich and\nstructured information such as task input/output format, TEP reduces\ninterference among tasks, allowing the model to focus on their shared\nstructure. With a single model, Musketeer achieves results comparable to or\nbetter than strong baselines trained on single tasks, almost uniformly across\nmultiple tasks.", "published": "2023-05-11 17:57:49", "link": "http://arxiv.org/abs/2305.07019v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Are Machine Rationales (Not) Useful to Humans? Measuring and Improving\n  Human Utility of Free-Text Rationales", "abstract": "Among the remarkable emergent capabilities of large language models (LMs) is\nfree-text rationalization; beyond a certain scale, large LMs are capable of\ngenerating seemingly useful rationalizations, which in turn, can dramatically\nenhance their performances on leaderboards. This phenomenon raises a question:\ncan machine generated rationales also be useful for humans, especially when lay\nhumans try to answer questions based on those machine rationales? We observe\nthat human utility of existing rationales is far from satisfactory, and\nexpensive to estimate with human studies. Existing metrics like task\nperformance of the LM generating the rationales, or similarity between\ngenerated and gold rationales are not good indicators of their human utility.\nWhile we observe that certain properties of rationales like conciseness and\nnovelty are correlated with their human utility, estimating them without human\ninvolvement is challenging. We show that, by estimating a rationale's\nhelpfulness in answering similar unseen instances, we can measure its human\nutility to a better extent. We also translate this finding into an automated\nscore, GEN-U, that we propose, which can help improve LMs' ability to generate\nrationales with better human utility, while maintaining most of its task\nperformance. Lastly, we release all code and collected data with this project.", "published": "2023-05-11 19:01:13", "link": "http://arxiv.org/abs/2305.07095v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OneCAD: One Classifier for All image Datasets using multimodal learning", "abstract": "Vision-Transformers (ViTs) and Convolutional neural networks (CNNs) are\nwidely used Deep Neural Networks (DNNs) for classification task. These model\narchitectures are dependent on the number of classes in the dataset it was\ntrained on. Any change in number of classes leads to change (partial or full)\nin the model's architecture. This work addresses the question: Is it possible\nto create a number-of-class-agnostic model architecture?. This allows model's\narchitecture to be independent of the dataset it is trained on. This work\nhighlights the issues with the current architectures (ViTs and CNNs). Also,\nproposes a training and inference framework OneCAD (One Classifier for All\nimage Datasets) to achieve close-to number-of-class-agnostic transformer model.\nTo best of our knowledge this is the first work to use Mask-Image-Modeling\n(MIM) with multimodal learning for classification task to create a DNN model\narchitecture agnostic to the number of classes. Preliminary results are shown\non natural and medical image datasets. Datasets: MNIST, CIFAR10, CIFAR100 and\nCOVIDx. Code will soon be publicly available on github.", "published": "2023-05-11 22:40:47", "link": "http://arxiv.org/abs/2305.07167v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Masked Audio Text Encoders are Effective Multi-Modal Rescorers", "abstract": "Masked Language Models (MLMs) have proven to be effective for second-pass\nrescoring in Automatic Speech Recognition (ASR) systems. In this work, we\npropose Masked Audio Text Encoder (MATE), a multi-modal masked language model\nrescorer which incorporates acoustic representations into the input space of\nMLM. We adopt contrastive learning for effectively aligning the modalities by\nlearning shared representations. We show that using a multi-modal rescorer is\nbeneficial for domain generalization of the ASR system when target domain data\nis unavailable. MATE reduces word error rate (WER) by 4%-16% on in-domain, and\n3%-7% on out-of-domain datasets, over the text-only baseline. Additionally,\nwith very limited amount of training data (0.8 hours), MATE achieves a WER\nreduction of 8%-23% over the first-pass baseline.", "published": "2023-05-11 22:44:43", "link": "http://arxiv.org/abs/2305.07677v2", "categories": ["cs.SD", "cs.CL", "cs.LG"], "primary_category": "cs.SD"}
{"title": "Learning the Visualness of Text Using Large Vision-Language Models", "abstract": "Visual text evokes an image in a person's mind, while non-visual text fails\nto do so. A method to automatically detect visualness in text will enable\ntext-to-image retrieval and generation models to augment text with relevant\nimages. This is particularly challenging with long-form text as text-to-image\ngeneration and retrieval models are often triggered for text that is designed\nto be explicitly visual in nature, whereas long-form text could contain many\nnon-visual sentences. To this end, we curate a dataset of 3,620 English\nsentences and their visualness scores provided by multiple human annotators. We\nalso propose a fine-tuning strategy that adapts large vision-language models\nlike CLIP by modifying the model's contrastive learning objective to map text\nidentified as non-visual to a common NULL image while matching visual text to\ntheir corresponding images in the document. We evaluate the proposed approach\non its ability to (i) classify visual and non-visual text accurately, and (ii)\nattend over words that are identified as visual in psycholinguistic studies.\nEmpirical evaluation indicates that our approach performs better than several\nheuristics and baseline models for the proposed task. Furthermore, to highlight\nthe importance of modeling the visualness of text, we conduct qualitative\nanalyses of text-to-image generation systems like DALL-E. Project webpage:\nhttps://gaurav22verma.github.io/text-visualness/", "published": "2023-05-11 17:45:16", "link": "http://arxiv.org/abs/2305.10434v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CoMoSpeech: One-Step Speech and Singing Voice Synthesis via Consistency\n  Model", "abstract": "Denoising diffusion probabilistic models (DDPMs) have shown promising\nperformance for speech synthesis. However, a large number of iterative steps\nare required to achieve high sample quality, which restricts the inference\nspeed. Maintaining sample quality while increasing sampling speed has become a\nchallenging task. In this paper, we propose a \"Co\"nsistency \"Mo\"del-based\n\"Speech\" synthesis method, CoMoSpeech, which achieve speech synthesis through a\nsingle diffusion sampling step while achieving high audio quality. The\nconsistency constraint is applied to distill a consistency model from a\nwell-designed diffusion-based teacher model, which ultimately yields superior\nperformances in the distilled CoMoSpeech. Our experiments show that by\ngenerating audio recordings by a single sampling step, the CoMoSpeech achieves\nan inference speed more than 150 times faster than real-time on a single NVIDIA\nA100 GPU, which is comparable to FastSpeech2, making diffusion-sampling based\nspeech synthesis truly practical. Meanwhile, objective and subjective\nevaluations on text-to-speech and singing voice synthesis show that the\nproposed teacher models yield the best audio quality, and the one-step sampling\nbased CoMoSpeech achieves the best inference speed with better or comparable\naudio quality to other conventional multi-step diffusion model baselines. Audio\nsamples are available at https://comospeech.github.io/.", "published": "2023-05-11 15:51:46", "link": "http://arxiv.org/abs/2305.06908v4", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Extending Audio Masked Autoencoders Toward Audio Restoration", "abstract": "Audio classification and restoration are among major downstream tasks in\naudio signal processing. However, restoration derives less of a benefit from\npretrained models compared to the overwhelming success of pretrained models in\nclassification tasks. Due to such unbalanced benefits, there has been rising\ninterest in how to improve the performance of pretrained models for restoration\ntasks, e.g., speech enhancement (SE). Previous works have shown that the\nfeatures extracted by pretrained audio encoders are effective for SE tasks, but\nthese speech-specialized encoder-only models usually require extra decoders to\nbecome compatible with SE, and involve complicated pretraining procedures or\ncomplex data augmentation. Therefore, in pursuit of a universal audio model,\nthe audio masked autoencoder (MAE) whose backbone is the autoencoder of Vision\nTransformers (ViT-AE), is extended from audio classification to SE, a\nrepresentative restoration task with well-established evaluation standards.\nViT-AE learns to restore masked audio signal via a mel-to-mel mapping during\npretraining, which is similar to restoration tasks like SE. We propose\nvariations of ViT-AE for a better SE performance, where the mel-to-mel\nvariations yield high scores in non-intrusive metrics and the STFT-oriented\nvariation is effective at intrusive metrics such as PESQ. Different variations\ncan be used in accordance with the scenarios. Comprehensive evaluations reveal\nthat MAE pretraining is beneficial to SE tasks and help the ViT-AE to better\ngeneralize to out-of-domain distortions. We further found that large-scale\nnoisy data of general audio sources, rather than clean speech, is sufficiently\neffective for pretraining.", "published": "2023-05-11 10:23:35", "link": "http://arxiv.org/abs/2305.06701v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Universal Source Separation with Weakly Labelled Data", "abstract": "Universal source separation (USS) is a fundamental research task for\ncomputational auditory scene analysis, which aims to separate mono recordings\ninto individual source tracks. There are three potential challenges awaiting\nthe solution to the audio source separation task. First, previous audio source\nseparation systems mainly focus on separating one or a limited number of\nspecific sources. There is a lack of research on building a unified system that\ncan separate arbitrary sources via a single model. Second, most previous\nsystems require clean source data to train a separator, while clean source data\nare scarce. Third, there is a lack of USS system that can automatically detect\nand separate active sound classes in a hierarchical level. To use large-scale\nweakly labeled/unlabeled audio data for audio source separation, we propose a\nuniversal audio source separation framework containing: 1) an audio tagging\nmodel trained on weakly labeled data as a query net; and 2) a conditional\nsource separation model that takes query net outputs as conditions to separate\narbitrary sound sources. We investigate various query nets, source separation\nmodels, and training strategies and propose a hierarchical USS strategy to\nautomatically detect and separate sound classes from the AudioSet ontology. By\nsolely leveraging the weakly labelled AudioSet, our USS system is successful in\nseparating a wide variety of sound classes, including sound event separation,\nmusic source separation, and speech enhancement. The USS system achieves an\naverage signal-to-distortion ratio improvement (SDRi) of 5.57 dB over 527 sound\nclasses of AudioSet; 10.57 dB on the DCASE 2018 Task 2 dataset; 8.12 dB on the\nMUSDB18 dataset; an SDRi of 7.28 dB on the Slakh2100 dataset; and an SSNR of\n9.00 dB on the voicebank-demand dataset. We release the source code at\nhttps://github.com/bytedance/uss", "published": "2023-05-11 16:41:55", "link": "http://arxiv.org/abs/2305.07447v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Tackling Interpretability in Audio Classification Networks with\n  Non-negative Matrix Factorization", "abstract": "This paper tackles two major problem settings for interpretability of audio\nprocessing networks, post-hoc and by-design interpretation. For post-hoc\ninterpretation, we aim to interpret decisions of a network in terms of\nhigh-level audio objects that are also listenable for the end-user. This is\nextended to present an inherently interpretable model with high performance. To\nthis end, we propose a novel interpreter design that incorporates non-negative\nmatrix factorization (NMF). In particular, an interpreter is trained to\ngenerate a regularized intermediate embedding from hidden layers of a target\nnetwork, learnt as time-activations of a pre-learnt NMF dictionary. Our\nmethodology allows us to generate intuitive audio-based interpretations that\nexplicitly enhance parts of the input signal most relevant for a network's\ndecision. We demonstrate our method's applicability on a variety of\nclassification tasks, including multi-label data for real-world audio and\nmusic.", "published": "2023-05-11 20:50:51", "link": "http://arxiv.org/abs/2305.07132v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "V2Meow: Meowing to the Visual Beat via Video-to-Music Generation", "abstract": "Video-to-music generation demands both a temporally localized high-quality\nlistening experience and globally aligned video-acoustic signatures. While\nrecent music generation models excel at the former through advanced audio\ncodecs, the exploration of video-acoustic signatures has been confined to\nspecific visual scenarios. In contrast, our research confronts the challenge of\nlearning globally aligned signatures between video and music directly from\npaired music and videos, without explicitly modeling domain-specific rhythmic\nor semantic relationships. We propose V2Meow, a video-to-music generation\nsystem capable of producing high-quality music audio for a diverse range of\nvideo input types using a multi-stage autoregressive model. Trained on 5k hours\nof music audio clips paired with video frames mined from in-the-wild music\nvideos, V2Meow is competitive with previous domain-specific models when\nevaluated in a zero-shot manner. It synthesizes high-fidelity music audio\nwaveforms solely by conditioning on pre-trained general-purpose visual features\nextracted from video frames, with optional style control via text prompts.\nThrough both qualitative and quantitative evaluations, we demonstrate that our\nmodel outperforms various existing music generation systems in terms of\nvisual-audio correspondence and audio quality. Music samples are available at\ntinyurl.com/v2meow.", "published": "2023-05-11 06:26:41", "link": "http://arxiv.org/abs/2305.06594v2", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speaker Diaphragm Excursion Prediction: deep attention and online\n  adaptation", "abstract": "Speaker protection algorithm is to leverage the playback signal properties to\nprevent over excursion while maintaining maximum loudness, especially for the\nmobile phone with tiny loudspeakers. This paper proposes efficient DL solutions\nto accurately model and predict the nonlinear excursion, which is challenging\nfor conventional solutions. Firstly, we build the experiment and pre-processing\npipeline, where the feedback current and voltage are sampled as input, and\nlaser is employed to measure the excursion as ground truth. Secondly, one\nFFTNet model is proposed to explore the dominant low-frequency and other\nunknown harmonics, and compares to a baseline ConvNet model. In addition, BN\nre-estimation is designed to explore the online adaptation; and INT8\nquantization based on AI Model efficiency toolkit (AIMET\\footnote{AIMET is a\nproduct of Qualcomm Innovation Center, Inc.}) is applied to further reduce the\ncomplexity. The proposed algorithm is verified in two speakers and 3 typical\ndeployment scenarios, and $>$99\\% residual DC is less than 0.1 mm, much better\nthan traditional solutions.", "published": "2023-05-11 08:17:55", "link": "http://arxiv.org/abs/2305.06640v1", "categories": ["eess.AS", "cs.AI", "cs.IT", "cs.SD", "math.IT"], "primary_category": "eess.AS"}
