{"title": "Generation of synthetic financial time series by diffusion models", "abstract": "Despite its practical significance, generating realistic synthetic financial\ntime series is challenging due to statistical properties known as stylized\nfacts, such as fat tails, volatility clustering, and seasonality patterns.\nVarious generative models, including generative adversarial networks (GANs) and\nvariational autoencoders (VAEs), have been employed to address this challenge,\nalthough no model yet satisfies all the stylized facts. We alternatively\npropose utilizing diffusion models, specifically denoising diffusion\nprobabilistic models (DDPMs), to generate synthetic financial time series. This\napproach employs wavelet transformation to convert multiple time series (into\nimages), such as stock prices, trading volumes, and spreads. Given these\nconverted images, the model gains the ability to generate images that can be\ntransformed back into realistic time series by inverse wavelet transformation.\nWe demonstrate that our proposed approach satisfies stylized facts.", "published": "2024-10-24 16:37:53", "link": "http://arxiv.org/abs/2410.18897v1", "categories": ["q-fin.CP", "q-fin.TR"], "primary_category": "q-fin.CP"}
{"title": "On the mean-field limit of diffusive games through the master equation: extreme value analysis", "abstract": "We consider an $N$-player game where the players control the drifts of their\ndiffusive states which have no interaction in the noise terms. The aim of each\nplayer is to minimize the expected value of her cost, which is a function of\nthe player's state and the empirical measure of the states of all the players.\nOur aim is to determine the $N \\to \\infty$ asymptotic behavior of the upper\norder statistics of the player's states under Nash equilibrium (the Nash\nstates). For this purpose, we consider also a system of interacting diffusions\nwhich is constructed by using the Master PDE of the game and approximates the\nsystem of the Nash states, and we improve an $L^2$ estimate for the distance\nbetween the drifts of the two systems which has been used for establishing\nCentral Limit Theorems and Large Deviations Principles for the Nash states in\nthe past. By differentiating the Master PDE, we obtain that estimate also in\n$L^{\\infty}$, which allows us to control the Radon-Nikodym derivative of a\nGirsanov transformation that connects the two systems. The latter allows us to\nreduce the problem to the case of $N$ uncontrolled diffusions with standard\nmean-field interaction in the drifts, which has been treated in a previous\nwork.", "published": "2024-10-24 15:54:17", "link": "http://arxiv.org/abs/2410.18869v2", "categories": ["math.PR", "math.AP", "math.OC", "math.ST", "q-fin.MF", "stat.TH", "60H10, 35Q89, 60G70, 91A16"], "primary_category": "math.PR"}
{"title": "What Drives Liquidity on Decentralized Exchanges? Evidence from the Uniswap Protocol", "abstract": "We study liquidity on decentralized exchanges (DEXs), identifying factors at\nthe platform, blockchain, token pair, and liquidity pool levels with predictive\npower for market depth metrics. We introduce the v2 counterfactual spread\nmetric, a novel criterion which assesses the degree of liquidity concentration\nin pools using the ``concentrated liquidity'' mechanism, allowing us to\ndecompose the effect of a factor on market depth into two channels: total value\nlocked (TVL) and concentration. We further explore how external liquidity from\ncompeting DEXs and private inventory on DEX aggregators influence market depth.\nWe find that (i) gas prices, returns, and a DEX's share of trading volume\naffect liquidity through concentration, (ii) internalization of order flow by\nprivate market makers affects TVL but not the overall market depth, and (iii)\nvolatility, fee revenue, and markout affect liquidity through both channels.", "published": "2024-10-24 19:15:17", "link": "http://arxiv.org/abs/2410.19107v2", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "Double Auctions: Formalization and Automated Checkers", "abstract": "Double auctions are widely used in financial markets, such as those for\nstocks, derivatives, currencies, and commodities, to match demand and supply.\nOnce all buyers and sellers have placed their trade requests, the exchange\ndetermines how these requests are to be matched. The two most common objectives\nfor determining the matching are maximizing trade volume at a uniform price and\nmaximizing trade volume through dynamic pricing. Prior research has primarily\nfocused on single-quantity trade requests. In this work, we extend the\nframework to handle multiple-quantity trade requests and present fully\nformalized matching algorithms for double auctions, along with their\ncorrectness proofs. We establish new uniqueness theorems, enabling automatic\ndetection of violations in exchange systems by comparing their output to that\nof a verified program. All proofs are formalized in the Coq Proof Assistant,\nand we extract verified OCaml and Haskell programs that could serve as a\nresource for exchanges and market regulators. We demonstrate the practical\napplicability of our work by running the verified program on real market data\nfrom an exchange to automatically check for violations in the exchange\nalgorithm.", "published": "2024-10-24 14:00:28", "link": "http://arxiv.org/abs/2410.18751v1", "categories": ["cs.LO", "q-fin.TR", "F.3.1; K.4.4"], "primary_category": "cs.LO"}
{"title": "Improving Model Factuality with Fine-grained Critique-based Evaluator", "abstract": "Factuality evaluation aims to detect factual errors produced by language\nmodels (LMs) and hence guide the development of more factual models. Towards\nthis goal, we train a factuality evaluator, FenCE, that provides LM generators\nwith claim-level factuality feedback. We conduct data augmentation on a\ncombination of public judgment datasets to train FenCE to (1) generate textual\ncritiques along with scores and (2) make claim-level judgment based on diverse\nsource documents obtained by various tools. We then present a framework that\nleverages FenCE to improve the factuality of LM generators by constructing\ntraining data. Specifically, we generate a set of candidate responses, leverage\nFenCE to revise and score each response without introducing lesser-known facts,\nand train the generator by preferring highly scored revised responses.\nExperiments show that our data augmentation methods improve the evaluator's\naccuracy by 2.9% on LLM-AggreFact. With FenCE, we improve Llama2-7B-chat and\nLlama3-8B-chat's factuality rate by 16.86% and 14.45% on FActScore,\noutperforming state-of-the-art factuality finetuning methods by 8.83% and\n6.96%.", "published": "2024-10-24 01:41:02", "link": "http://arxiv.org/abs/2410.18359v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Monolingual and Multilingual Misinformation Detection for Low-Resource\n  Languages: A Comprehensive Survey", "abstract": "In today's global digital landscape, misinformation transcends linguistic\nboundaries, posing a significant challenge for moderation systems. Most\napproaches to misinformation detection are monolingual, focused on\nhigh-resource languages, i.e., a handful of world languages that have benefited\nfrom substantial research investment. This survey provides a comprehensive\noverview of the current research on misinformation detection in low-resource\nlanguages, both in monolingual and multilingual settings. We review existing\ndatasets, methodologies, and tools used in these domains, identifying key\nchallenges related to: data resources, model development, cultural and\nlinguistic context, and real-world applications. We examine emerging\napproaches, such as language-generalizable models and multi-modal techniques,\nand emphasize the need for improved data collection practices,\ninterdisciplinary collaboration, and stronger incentives for socially\nresponsible AI research. Our findings underscore the importance of systems\ncapable of addressing misinformation across diverse linguistic and cultural\ncontexts.", "published": "2024-10-24 03:02:03", "link": "http://arxiv.org/abs/2410.18390v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decoding on Graphs: Faithful and Sound Reasoning on Knowledge Graphs\n  through Generation of Well-Formed Chains", "abstract": "Knowledge Graphs (KGs) can serve as reliable knowledge sources for question\nanswering (QA) due to their structured representation of knowledge. Existing\nresearch on the utilization of KG for large language models (LLMs) prevalently\nrelies on subgraph retriever or iterative prompting, overlooking the potential\nsynergy of LLMs' step-wise reasoning capabilities and KGs' structural nature.\nIn this paper, we present DoG (Decoding on Graphs), a novel framework that\nfacilitates a deep synergy between LLMs and KGs. We first define a concept,\nwell-formed chain, which consists of a sequence of interrelated fact triplets\non the KGs, starting from question entities and leading to answers. We argue\nthat this concept can serve as a principle for making faithful and sound\nreasoning for KGQA. To enable LLMs to generate well-formed chains, we propose\ngraph-aware constrained decoding, in which a constraint derived from the\ntopology of the KG regulates the decoding process of the LLMs. This constrained\ndecoding method ensures the generation of well-formed chains while making full\nuse of the step-wise reasoning capabilities of LLMs. Based on the above, DoG, a\ntraining-free approach, is able to provide faithful and sound reasoning\ntrajectories grounded on the KGs. Experiments across various KGQA tasks with\ndifferent background KGs demonstrate that DoG achieves superior and robust\nperformance. DoG also shows general applicability with various open-source\nLLMs.", "published": "2024-10-24 04:01:40", "link": "http://arxiv.org/abs/2410.18415v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building Dialogue Understanding Models for Low-resource Language\n  Indonesian from Scratch", "abstract": "Making use of off-the-shelf resources of resource-rich languages to transfer\nknowledge for low-resource languages raises much attention recently. The\nrequirements of enabling the model to reach the reliable performance lack well\nguided, such as the scale of required annotated data or the effective\nframework. To investigate the first question, we empirically investigate the\ncost-effectiveness of several methods to train the intent classification and\nslot-filling models for Indonesia (ID) from scratch by utilizing the English\ndata. Confronting the second challenge, we propose a Bi-Confidence-Frequency\nCross-Lingual transfer framework (BiCF), composed by ``BiCF Mixing'', ``Latent\nSpace Refinement'' and ``Joint Decoder'', respectively, to tackle the obstacle\nof lacking low-resource language dialogue data. Extensive experiments\ndemonstrate our framework performs reliably and cost-efficiently on different\nscales of manually annotated Indonesian data. We release a large-scale\nfine-labeled dialogue dataset (ID-WOZ) and ID-BERT of Indonesian for further\nresearch.", "published": "2024-10-24 04:33:14", "link": "http://arxiv.org/abs/2410.18430v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case\n  Study on English-Korean Code-Switching", "abstract": "Code-switching (CS), a phenomenon where multilingual speakers alternate\nbetween languages in a discourse, can convey subtle cultural and linguistic\nnuances that can be otherwise lost in translation. Recent state-of-the-art\nmultilingual large language models (LLMs) demonstrate excellent multilingual\nabilities in various aspects including understanding CS, but the power of CS in\neliciting language-specific knowledge is yet to be discovered. Therefore, we\ninvestigate the effectiveness of code-switching on a wide range of multilingual\nLLMs in terms of knowledge activation, or the act of identifying and leveraging\nknowledge for reasoning. To facilitate the research, we first present EnKoQA, a\nsynthetic English-Korean CS question-answering dataset. We provide a\ncomprehensive analysis on a variety of multilingual LLMs by subdividing\nactivation process into knowledge identification and knowledge leveraging. Our\nexperiments demonstrate that compared to English text, CS can faithfully\nactivate knowledge inside LLMs, especially on language-specific domains. In\naddition, the performance gap between CS and English is larger in models that\nshow excellent monolingual abilities, suggesting that there exists a\ncorrelation with CS and Korean proficiency.", "published": "2024-10-24 05:14:03", "link": "http://arxiv.org/abs/2410.18436v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Automatic Speech Recognition Systems for Korean\n  Meteorological Experts", "abstract": "This paper explores integrating Automatic Speech Recognition (ASR) into\nnatural language query systems to improve weather forecasting efficiency for\nKorean meteorologists. We address challenges in developing ASR systems for the\nKorean weather domain, specifically specialized vocabulary and Korean\nlinguistic intricacies. To tackle these issues, we constructed an evaluation\ndataset of spoken queries recorded by native Korean speakers. Using this\ndataset, we assessed various configurations of a multilingual ASR model family,\nidentifying performance limitations related to domain-specific terminology. We\nthen implemented a simple text-to-speech-based data augmentation method, which\nimproved the recognition of specialized terms while maintaining general-domain\nperformance. Our contributions include creating a domain-specific dataset,\ncomprehensive ASR model evaluations, and an effective augmentation technique.\nWe believe our work provides a foundation for future advancements in ASR for\nthe Korean weather forecasting domain.", "published": "2024-10-24 05:40:07", "link": "http://arxiv.org/abs/2410.18444v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ToolFlow: Boosting LLM Tool-Calling Through Natural and Coherent\n  Dialogue Synthesis", "abstract": "Supervised fine-tuning (SFT) is a common method to enhance the tool calling\ncapabilities of Large Language Models (LLMs), with the training data often\nbeing synthesized. The current data synthesis process generally involves\nsampling a set of tools, formulating a requirement based on these tools, and\ngenerating the call statements. However, tools sampled randomly lack relevance,\nmaking them difficult to combine and thus reducing the diversity of the data.\nAdditionally, current work overlooks the coherence between turns of dialogues,\nleading to a gap between the synthesized data and real-world scenarios. To\naddress these issues, we propose a Graph-based Sampling strategy to sample more\nrelevant tool combinations, and a Planned-generation strategy to create plans\nthat guide the synthesis of coherent dialogues. We integrate these two\nstrategies and enable multiple agents to synthesize the dialogue data\ninteractively, resulting in our tool-calling data synthesis pipeline ToolFlow.\nData quality assessments demonstrate improvements in the naturalness and\ncoherence of our synthesized dialogues. Finally, we apply SFT on LLaMA-3.1-8B\nusing 8,000 synthetic dialogues generated with ToolFlow. Results show that the\nmodel achieves tool-calling performance comparable to or even surpassing GPT-4,\nwhile maintaining strong general capabilities.", "published": "2024-10-24 05:45:04", "link": "http://arxiv.org/abs/2410.18447v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChineseSafe: A Chinese Benchmark for Evaluating Safety in Large Language\n  Models", "abstract": "With the rapid development of Large language models (LLMs), understanding the\ncapabilities of LLMs in identifying unsafe content has become increasingly\nimportant. While previous works have introduced several benchmarks to evaluate\nthe safety risk of LLMs, the community still has a limited understanding of\ncurrent LLMs' capability to recognize illegal and unsafe content in Chinese\ncontexts. In this work, we present a Chinese safety benchmark (ChineseSafe) to\nfacilitate research on the content safety of large language models. To align\nwith the regulations for Chinese Internet content moderation, our ChineseSafe\ncontains 205,034 examples across 4 classes and 10 sub-classes of safety issues.\nFor Chinese contexts, we add several special types of illegal content:\npolitical sensitivity, pornography, and variant/homophonic words. Moreover, we\nemploy two methods to evaluate the legal risks of popular LLMs, including\nopen-sourced models and APIs. The results reveal that many LLMs exhibit\nvulnerability to certain types of safety issues, leading to legal risks in\nChina. Our work provides a guideline for developers and researchers to\nfacilitate the safety of LLMs. Our results are also available at\nhttps://huggingface.co/spaces/SUSTech/ChineseSafe-Benchmark.", "published": "2024-10-24 07:25:29", "link": "http://arxiv.org/abs/2410.18491v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CCI3.0-HQ: a large-scale Chinese dataset of high quality designed for\n  pre-training large language models", "abstract": "We present CCI3.0-HQ (https://huggingface.co/datasets/BAAI/CCI3-HQ), a\nhigh-quality 500GB subset of the Chinese Corpora Internet 3.0\n(CCI3.0)(https://huggingface.co/datasets/BAAI/CCI3-Data), developed using a\nnovel two-stage hybrid filtering pipeline that significantly enhances data\nquality. To evaluate its effectiveness, we trained a 0.5B parameter model from\nscratch on 100B tokens across various datasets, achieving superior performance\non 10 benchmarks in a zero-shot setting compared to CCI3.0, SkyPile, and\nWanjuanV1. The high-quality filtering process effectively distills the\ncapabilities of the Qwen2-72B-instruct model into a compact 0.5B model,\nattaining optimal F1 scores for Chinese web data classification. We believe\nthis open-access dataset will facilitate broader access to high-quality\nlanguage models.", "published": "2024-10-24 07:50:07", "link": "http://arxiv.org/abs/2410.18505v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Systematic Survey on Instructional Text: From Representation Formats\n  to Downstream NLP Tasks", "abstract": "Recent advances in large language models have demonstrated promising\ncapabilities in following simple instructions through instruction tuning.\nHowever, real-world tasks often involve complex, multi-step instructions that\nremain challenging for current NLP systems. Despite growing interest in this\narea, there lacks a comprehensive survey that systematically analyzes the\nlandscape of complex instruction understanding and processing. Through a\nsystematic review of the literature, we analyze available resources,\nrepresentation schemes, and downstream tasks related to instructional text. Our\nstudy examines 177 papers, identifying trends, challenges, and opportunities in\nthis emerging field. We provide AI/NLP researchers with essential background\nknowledge and a unified view of various approaches to complex instruction\nunderstanding, bridging gaps between different research directions and\nhighlighting future research opportunities.", "published": "2024-10-24 08:22:59", "link": "http://arxiv.org/abs/2410.18529v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Infinity-MM: Scaling Multimodal Performance with Large-Scale and\n  High-Quality Instruction Data", "abstract": "Recently, Vision-Language Models (VLMs) have achieved remarkable progress in\nmultimodal tasks, and multimodal instruction data serves as the foundation for\nenhancing VLM capabilities. Despite the availability of several open-source\nmultimodal datasets, limitations in the scale and quality of open-source\ninstruction data hinder the performance of VLMs trained on these datasets,\nleading to a significant gap compared to models trained on closed-source data.\nTo address this challenge, we introduce Infinity-MM, a large-scale multimodal\ninstruction dataset. We collected the available multimodal instruction datasets\nand performed unified preprocessing, resulting in a dataset with over 40\nmillion samples that ensures diversity and accuracy. Furthermore, to enable\nlarge-scale expansion of instruction data and support the continuous\nacquisition of high-quality data, we propose a synthetic instruction generation\nmethod based on a tagging system and open-source VLMs. By establishing\ncorrespondences between different types of images and associated instruction\ntypes, this method can provide essential guidance during data synthesis.\nLeveraging this high-quality data, we have trained a 2-billion-parameter\nVision-Language Model, Aquila-VL-2B, which achieves state-of-the-art (SOTA)\nperformance among models of similar scale. The data is available at:\nhttps://huggingface.co/datasets/BAAI/Infinity-MM.", "published": "2024-10-24 09:03:48", "link": "http://arxiv.org/abs/2410.18558v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Difficult for Whom? A Study of Japanese Lexical Complexity", "abstract": "The tasks of lexical complexity prediction (LCP) and complex word\nidentification (CWI) commonly presuppose that difficult to understand words are\nshared by the target population. Meanwhile, personalization methods have also\nbeen proposed to adapt models to individual needs. We verify that a recent\nJapanese LCP dataset is representative of its target population by partially\nreplicating the annotation. By another reannotation we show that native Chinese\nspeakers perceive the complexity differently due to Sino-Japanese vocabulary.\nTo explore the possibilities of personalization, we compare competitive\nbaselines trained on the group mean ratings and individual ratings in terms of\nperformance for an individual. We show that the model trained on a group mean\nperforms similarly to an individual model in the CWI task, while achieving good\nLCP performance for an individual is difficult. We also experiment with\nadapting a finetuned BERT model, which results only in marginal improvements\nacross all settings.", "published": "2024-10-24 09:18:53", "link": "http://arxiv.org/abs/2410.18567v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Weak-to-Strong Preference Optimization: Stealing Reward from Weak\n  Aligned Model", "abstract": "Aligning language models (LMs) with human preferences has become a key area\nof research, enabling these models to meet diverse user needs better. Inspired\nby weak-to-strong generalization, where a strong LM fine-tuned on labels\ngenerated by a weaker model can consistently outperform its weak supervisor, we\nextend this idea to model alignment. In this work, we observe that the\nalignment behavior in weaker models can be effectively transferred to stronger\nmodels and even exhibit an amplification effect. Based on this insight, we\npropose a method called Weak-to-Strong Preference Optimization (WSPO), which\nachieves strong model alignment by learning the distribution differences before\nand after the alignment of the weak model. Experiments demonstrate that WSPO\ndelivers outstanding performance, improving the win rate of Qwen2-7B-Instruct\non Arena-Hard from 39.70 to 49.60, achieving a remarkable 47.04\nlength-controlled win rate on AlpacaEval 2, and scoring 7.33 on MT-bench. Our\nresults suggest that using the weak model to elicit a strong model with a high\nalignment ability is feasible.", "published": "2024-10-24 11:06:29", "link": "http://arxiv.org/abs/2410.18640v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GrammaMT: Improving Machine Translation with Grammar-Informed In-Context\n  Learning", "abstract": "We introduce GrammaMT, a grammatically-aware prompting approach for machine\ntranslation that uses Interlinear Glossed Text (IGT), a common form of\nlinguistic description providing morphological and lexical annotations for\nsource sentences. GrammaMT proposes three prompting strategies: gloss-shot,\nchain-gloss and model-gloss. All are training-free, requiring only a few\nexamples that involve minimal effort to collect, and making them well-suited\nfor low-resource setups. Experiments show that GrammaMT enhances translation\nperformance on open-source instruction-tuned LLMs for various low- to\nhigh-resource languages across three benchmarks: (1) the largest IGT corpus,\n(2) the challenging 2023 SIGMORPHON Shared Task data over endangered languages,\nand (3) even in an out-of-domain setting with FLORES. Moreover, ablation\nstudies reveal that leveraging gloss resources could substantially boost MT\nperformance (by over 17 BLEU points) if LLMs accurately generate or access\ninput sentence glosses.", "published": "2024-10-24 12:56:01", "link": "http://arxiv.org/abs/2410.18702v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why Does the Effective Context Length of LLMs Fall Short?", "abstract": "Advancements in distributed training and efficient attention mechanisms have\nsignificantly expanded the context window sizes of large language models\n(LLMs). However, recent work reveals that the effective context lengths of\nopen-source LLMs often fall short, typically not exceeding half of their\ntraining lengths. In this work, we attribute this limitation to the left-skewed\nfrequency distribution of relative positions formed in LLMs pretraining and\npost-training stages, which impedes their ability to effectively gather distant\ninformation. To address this challenge, we introduce ShifTed Rotray position\nembeddING (STRING). STRING shifts well-trained positions to overwrite the\noriginal ineffective positions during inference, enhancing performance within\ntheir existing training lengths. Experimental results show that without\nadditional training, STRING dramatically improves the performance of the latest\nlarge-scale models, such as Llama3.1 70B and Qwen2 72B, by over 10 points on\npopular long-context benchmarks RULER and InfiniteBench, establishing new\nstate-of-the-art results for open-source LLMs. Compared to commercial models,\nLlama 3.1 70B with \\method even achieves better performance than GPT-4-128K and\nclearly surpasses Claude 2 and Kimi-chat.", "published": "2024-10-24 13:51:50", "link": "http://arxiv.org/abs/2410.18745v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Task Calibration: Calibrating Large Language Models on Inference Tasks", "abstract": "Large language models (LLMs) have exhibited impressive zero-shot performance\non inference tasks. However, LLMs may suffer from spurious correlations between\ninput texts and output labels, which limits LLMs' ability to reason based\npurely on general language understanding. In other words, LLMs may make\npredictions primarily based on premise or hypothesis, rather than both\ncomponents. To address this problem that may lead to unexpected performance\ndegradation, we propose task calibration (TC), a zero-shot and inference-only\ncalibration method inspired by mutual information which recovers LLM\nperformance through task reformulation. TC encourages LLMs to reason based on\nboth premise and hypothesis, while mitigating the models' over-reliance on\nindividual premise or hypothesis for inference. Experimental results show that\nTC achieves a substantial improvement on 13 inference tasks in the zero-shot\nsetup. We further validate the effectiveness of TC in few-shot setups and\nvarious natural language understanding tasks. Further analysis indicates that\nTC is also robust to prompt templates and has the potential to be integrated\nwith other calibration methods.", "published": "2024-10-24 14:18:32", "link": "http://arxiv.org/abs/2410.18764v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distill Visual Chart Reasoning Ability from LLMs to MLLMs", "abstract": "Solving complex chart Q&A tasks requires advanced visual reasoning abilities\nin multimodal large language models (MLLMs). Recent studies highlight that\nthese abilities consist of two main parts: recognizing key information from\nvisual inputs and conducting reasoning over it. Thus, a promising approach to\nenhance MLLMs is to construct relevant training data focusing on the two\naspects. However, collecting and annotating complex charts and questions is\ncostly and time-consuming, and ensuring the quality of annotated answers\nremains a challenge. In this paper, we propose Code-as-Intermediary Translation\n(CIT), a cost-effective, efficient and easily scalable data synthesis method\nfor distilling visual reasoning abilities from LLMs to MLLMs. The code serves\nas an intermediary that translates visual chart representations into textual\nrepresentations, enabling LLMs to understand cross-modal information.\nSpecifically, we employ text-based synthesizing techniques to construct\nchart-plotting code and produce ReachQA, a dataset containing 3k\nreasoning-intensive charts and 20k Q&A pairs to enhance both recognition and\nreasoning abilities. Experiments show that when fine-tuned with our data,\nmodels not only perform well on chart-related benchmarks, but also demonstrate\nimproved multimodal reasoning abilities on general mathematical benchmarks like\nMathVista. The code and dataset are publicly available at\nhttps://github.com/hewei2001/ReachQA.", "published": "2024-10-24 14:50:42", "link": "http://arxiv.org/abs/2410.18798v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Delving into the Reversal Curse: How Far Can Large Language Models\n  Generalize?", "abstract": "While large language models (LLMs) showcase unprecedented capabilities, they\nalso exhibit certain inherent limitations when facing seemingly trivial tasks.\nA prime example is the recently debated \"reversal curse\", which surfaces when\nmodels, having been trained on the fact \"A is B\", struggle to generalize this\nknowledge to infer that \"B is A\". In this paper, we examine the manifestation\nof the reversal curse across various tasks and delve into both the\ngeneralization abilities and the problem-solving mechanisms of LLMs. This\ninvestigation leads to a series of significant insights: (1) LLMs are able to\ngeneralize to \"B is A\" when both A and B are presented in the context as in the\ncase of a multiple-choice question. (2) This generalization ability is highly\ncorrelated to the structure of the fact \"A is B\" in the training documents. For\nexample, this generalization only applies to biographies structured in \"[Name]\nis [Description]\" but not to \"[Description] is [Name]\". (3) We propose and\nverify the hypothesis that LLMs possess an inherent bias in fact recalling\nduring knowledge application, which explains and underscores the importance of\nthe document structure to successful learning. (4) The negative impact of this\nbias on the downstream performance of LLMs can hardly be mitigated through\ntraining alone. These findings offer a novel perspective on interpreting LLMs'\ngeneralization through their intrinsic mechanisms and provide insights for\ndeveloping more effective learning methods. Our code and data are available at\nhttps://github.com/alibaba/thinking_bias.git.", "published": "2024-10-24 14:55:09", "link": "http://arxiv.org/abs/2410.18808v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Multimodal Sarcasm Detection", "abstract": "Sarcasm is a rhetorical device that is used to convey the opposite of the\nliteral meaning of an utterance. Sarcasm is widely used on social media and\nother forms of computer-mediated communication motivating the use of\ncomputational models to identify it automatically. While the clear majority of\napproaches to sarcasm detection have been carried out on text only, sarcasm\ndetection often requires additional information present in tonality, facial\nexpression, and contextual images. This has led to the introduction of\nmultimodal models, opening the possibility to detect sarcasm in multiple\nmodalities such as audio, images, text, and video. In this paper, we present\nthe first comprehensive survey on multimodal sarcasm detection - henceforth MSD\n- to date. We survey papers published between 2018 and 2023 on the topic, and\ndiscuss the models and datasets used for this task. We also present future\nresearch directions in MSD.", "published": "2024-10-24 16:17:47", "link": "http://arxiv.org/abs/2410.18882v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are LLMs Better than Reported? Detecting Label Errors and Mitigating\n  Their Effect on Model Performance", "abstract": "NLP benchmarks rely on standardized datasets for training and evaluating\nmodels and are crucial for advancing the field. Traditionally, expert\nannotations ensure high-quality labels; however, the cost of expert annotation\ndoes not scale well with the growing demand for larger datasets required by\nmodern models. While crowd-sourcing provides a more scalable solution, it often\ncomes at the expense of annotation precision and consistency. Recent\nadvancements in large language models (LLMs) offer new opportunities to enhance\nthe annotation process, particularly for detecting label errors in existing\ndatasets. In this work, we consider the recent approach of LLM-as-a-judge,\nleveraging an ensemble of LLMs to flag potentially mislabeled examples. Through\na case study of four datasets from the TRUE benchmark, covering different tasks\nand domains, we empirically analyze the labeling quality of existing datasets,\nand compare expert, crowd-sourced, and our LLM-based annotations in terms of\nagreement, label quality, and efficiency, demonstrating the strengths and\nlimitations of each annotation method. Our findings reveal a substantial number\nof label errors, which, when corrected, induce a significant upward shift in\nreported model performance. This suggests that many of the LLMs so-called\nmistakes are due to label errors rather than genuine model failures.\nAdditionally, we discuss the implications of mislabeled data and propose\nmethods to mitigate them in training to improve model performance.", "published": "2024-10-24 16:27:03", "link": "http://arxiv.org/abs/2410.18889v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs for Extremely Low-Resource Finno-Ugric Languages", "abstract": "The advancement of large language models (LLMs) has predominantly focused on\nhigh-resource languages, leaving low-resource languages, such as those in the\nFinno-Ugric family, significantly underrepresented. This paper addresses this\ngap by focusing on V\\~oro, Livonian, and Komi. We cover almost the entire cycle\nof LLM creation, from data collection to instruction tuning and evaluation. Our\ncontributions include developing multilingual base and instruction-tuned\nmodels; creating evaluation benchmarks, including the smugri-MT-bench\nmulti-turn conversational benchmark; and conducting human evaluation. We intend\nfor this work to promote linguistic diversity, ensuring that lesser-resourced\nlanguages can benefit from advancements in NLP.", "published": "2024-10-24 16:48:12", "link": "http://arxiv.org/abs/2410.18902v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BioMistral-NLU: Towards More Generalizable Medical Language\n  Understanding through Instruction Tuning", "abstract": "Large language models (LLMs) such as ChatGPT are fine-tuned on large and\ndiverse instruction-following corpora, and can generalize to new tasks.\nHowever, those instruction-tuned LLMs often perform poorly in specialized\nmedical natural language understanding (NLU) tasks that require domain\nknowledge, granular text comprehension, and structured data extraction. To\nbridge the gap, we: (1) propose a unified prompting format for 7 important NLU\ntasks, (2) curate an instruction-tuning dataset, MNLU-Instruct, utilizing\ndiverse existing open-source medical NLU corpora, and (3) develop\nBioMistral-NLU, a generalizable medical NLU model, through fine-tuning\nBioMistral on MNLU-Instruct. We evaluate BioMistral-NLU in a zero-shot setting,\nacross 6 important NLU tasks, from two widely adopted medical NLU benchmarks:\nBLUE and BLURB. Our experiments show that our BioMistral-NLU outperforms the\noriginal BioMistral, as well as the proprietary LLMs - ChatGPT and GPT-4. Our\ndataset-agnostic prompting strategy and instruction tuning step over diverse\nNLU tasks enhance LLMs' generalizability across diverse medical NLU tasks. Our\nablation experiments show that instruction-tuning on a wider variety of tasks,\neven when the total number of training instances remains constant, enhances\ndownstream zero-shot generalization.", "published": "2024-10-24 17:53:53", "link": "http://arxiv.org/abs/2410.18955v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridge-Coder: Unlocking LLMs' Potential to Overcome Language Gaps in\n  Low-Resource Code", "abstract": "Large Language Models (LLMs) demonstrate strong proficiency in generating\ncode for high-resource programming languages (HRPLs) like Python but struggle\nsignificantly with low-resource programming languages (LRPLs) such as Racket or\nD. This performance gap deepens the digital divide, preventing developers using\nLRPLs from benefiting equally from LLM advancements and reinforcing disparities\nin innovation within underrepresented programming communities. While generating\nadditional training data for LRPLs is promising, it faces two key challenges:\nmanual annotation is labor-intensive and costly, and LLM-generated LRPL code is\noften of subpar quality. The underlying cause of this issue is the gap between\nnatural language to programming language gap (NL-PL Gap), which is especially\npronounced in LRPLs due to limited aligned data. In this work, we introduce a\nnovel approach called Bridge-Coder, which leverages LLMs' intrinsic\ncapabilities to enhance the performance on LRPLs. Our method consists of two\nkey stages. Bridge Generation, where we create high-quality dataset by\nutilizing LLMs' general knowledge understanding, proficiency in HRPLs, and\nin-context learning abilities. Then, we apply the Bridged Alignment, which\nprogressively improves the alignment between NL instructions and LRPLs.\nExperimental results across multiple LRPLs show that Bridge-Coder significantly\nenhances model performance, demonstrating the effectiveness and generalization\nof our approach. Furthermore, we offer a detailed analysis of the key\ncomponents of our method, providing valuable insights for future work aimed at\naddressing the challenges associated with LRPLs.", "published": "2024-10-24 17:55:03", "link": "http://arxiv.org/abs/2410.18957v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does Data Contamination Detection Work (Well) for LLMs? A Survey and\n  Evaluation on Detection Assumptions", "abstract": "Large language models (LLMs) have demonstrated great performance across\nvarious benchmarks, showing potential as general-purpose task solvers. However,\nas LLMs are typically trained on vast amounts of data, a significant concern in\ntheir evaluation is data contamination, where overlap between training data and\nevaluation datasets inflates performance assessments. Multiple approaches have\nbeen developed to identify data contamination. These approaches rely on\nspecific assumptions that may not hold universally across different settings.\nTo bridge this gap, we systematically review 50 papers on data contamination\ndetection, categorize the underlying assumptions, and assess whether they have\nbeen rigorously validated. We identify and analyze eight categories of\nassumptions and test three of them as case studies. Our case studies focus on\ndetecting direct, instance-level data contamination, which is also referred to\nas Membership Inference Attacks (MIA). Our analysis reveals that MIA approaches\nbased on these three assumptions can have similar performance to random\nguessing, on datasets used in LLM pretraining, suggesting that current LLMs\nmight learn data distributions rather than memorizing individual instances.\nMeanwhile, MIA can easily fail when there are data distribution shifts between\nthe seen and unseen instances.", "published": "2024-10-24 17:58:22", "link": "http://arxiv.org/abs/2410.18966v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GCoder: Improving Large Language Model for Generalized Graph Problem\n  Solving", "abstract": "Large Language Models (LLMs) have demonstrated strong reasoning abilities,\nmaking them suitable for complex tasks such as graph computation. Traditional\nreasoning steps paradigm for graph problems is hindered by unverifiable steps,\nlimited long-term reasoning, and poor generalization to graph variations. To\novercome these limitations, we introduce GCoder, a code-based LLM designed to\nenhance problem-solving in generalized graph computation problems. Our method\ninvolves constructing an extensive training dataset, GraphWild, featuring\ndiverse graph formats and algorithms. We employ a multi-stage training process,\nincluding Supervised Fine-Tuning (SFT) and Reinforcement Learning from Compiler\nFeedback (RLCF), to refine model capabilities. For unseen tasks, a hybrid\nretrieval technique is used to augment performance. Experiments demonstrate\nthat GCoder outperforms GPT-4o, with an average accuracy improvement of 16.42%\nacross various graph computational problems. Furthermore, GCoder efficiently\nmanages large-scale graphs with millions of nodes and diverse input formats,\novercoming the limitations of previous models focused on the reasoning steps\nparadigm. This advancement paves the way for more intuitive and effective graph\nproblem-solving using LLMs. Code and data are available at here:\nhttps://github.com/Bklight999/WWW25-GCoder/tree/master.", "published": "2024-10-24 18:40:36", "link": "http://arxiv.org/abs/2410.19084v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieving Implicit and Explicit Emotional Events Using Large Language\n  Models", "abstract": "Large language models (LLMs) have garnered significant attention in recent\nyears due to their impressive performance. While considerable research has\nevaluated these models from various perspectives, the extent to which LLMs can\nperform implicit and explicit emotion retrieval remains largely unexplored. To\naddress this gap, this study investigates LLMs' emotion retrieval capabilities\nin commonsense. Through extensive experiments involving multiple models, we\nsystematically evaluate the ability of LLMs on emotion retrieval. Specifically,\nwe propose a supervised contrastive probing method to verify LLMs' performance\nfor implicit and explicit emotion retrieval, as well as the diversity of the\nemotional events they retrieve. The results offer valuable insights into the\nstrengths and limitations of LLMs in handling emotion retrieval.", "published": "2024-10-24 19:56:28", "link": "http://arxiv.org/abs/2410.19128v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hybrid Preferences: Learning to Route Instances for Human vs. AI\n  Feedback", "abstract": "Learning from human feedback has enabled the alignment of language models\n(LMs) with human preferences. However, directly collecting human preferences\ncan be expensive, time-consuming, and can have high variance. An appealing\nalternative is to distill preferences from LMs as a source of synthetic\nannotations as they are more consistent, cheaper, and scale better than human\nannotation; however, they are also prone to biases and errors. In this work, we\nintroduce a routing framework that combines inputs from humans and LMs to\nachieve better annotation quality, while reducing the total cost of human\nannotation. The crux of our approach is to identify preference instances that\nwill benefit from human annotations. We formulate this as an optimization\nproblem: given a preference dataset and an evaluation metric, we train a\nperformance prediction model to predict a reward model's performance on an\narbitrary combination of human and LM annotations and employ a routing strategy\nthat selects a combination that maximizes predicted performance. We train the\nperformance prediction model on MultiPref, a new preference dataset with 10K\ninstances paired with human and LM labels. We show that the selected hybrid\nmixture of LM and direct human preferences using our routing framework achieves\nbetter reward model performance compared to using either one exclusively. We\nsimulate selective human preference collection on three other datasets and show\nthat our method generalizes well to all three. We analyze features from the\nrouting model to identify characteristics of instances that can benefit from\nhuman feedback, e.g., prompts with a moderate safety concern or moderate intent\ncomplexity. We release the dataset, annotation platform, and source code used\nin this study to foster more efficient and accurate preference collection in\nthe future.", "published": "2024-10-24 20:04:15", "link": "http://arxiv.org/abs/2410.19133v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Label Set Optimization via Activation Distribution Kurtosis for\n  Zero-shot Classification with Generative Models", "abstract": "In-context learning (ICL) performance is known to be sensitive to the prompt\ndesign, yet the impact of class label options in zero-shot classification has\nbeen largely overlooked. This study presents the first comprehensive empirical\nstudy investigating how label option (e.g., lexical choice, order, and\nelaboration) influences zero-shot ICL classification performance. Our findings\nreveal that lexical choices for label names (e.g., agree vs.support in stance\nclassification) play an important role, with effects also linked to label\norders. An analysis of the model internal states further shows that optimal\nlabel names tend to activate fewer outlier neurons in the feed forward network.\nBased on this observation, we propose Label set Optimization via Activation\nDistribution kurtosiS (LOADS), a post-hoc approach requiring no gradient\npropagation. LOADS not only demonstrates effectiveness with only 100 unlabelled\nsamples across different model types and sizes, but also shows cross-lingual\ntransferability.", "published": "2024-10-24 22:59:23", "link": "http://arxiv.org/abs/2410.19195v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the Creativity of LLMs in Proposing Novel Solutions to\n  Mathematical Problems", "abstract": "The mathematical capabilities of AI systems are complex and multifaceted.\nMost existing research has predominantly focused on the correctness of\nAI-generated solutions to mathematical problems. In this work, we argue that\nbeyond producing correct answers, AI systems should also be capable of, or\nassist humans in, developing novel solutions to mathematical challenges. This\nstudy explores the creative potential of Large Language Models (LLMs) in\nmathematical reasoning, an aspect that has received limited attention in prior\nresearch. We introduce a novel framework and benchmark, CreativeMath, which\nencompasses problems ranging from middle school curricula to Olympic-level\ncompetitions, designed to assess LLMs' ability to propose innovative solutions\nafter some known solutions have been provided. Our experiments demonstrate\nthat, while LLMs perform well on standard mathematical tasks, their capacity\nfor creative problem-solving varies considerably. Notably, the Gemini-1.5-Pro\nmodel outperformed other LLMs in generating novel solutions. This research\nopens a new frontier in evaluating AI creativity, shedding light on both the\nstrengths and limitations of LLMs in fostering mathematical innovation, and\nsetting the stage for future developments in AI-assisted mathematical\ndiscovery.", "published": "2024-10-24 00:12:49", "link": "http://arxiv.org/abs/2410.18336v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AdaEDL: Early Draft Stopping for Speculative Decoding of Large Language\n  Models via an Entropy-based Lower Bound on Token Acceptance Probability", "abstract": "Speculative decoding is a powerful technique that attempts to circumvent the\nautoregressive constraint of modern Large Language Models (LLMs). The aim of\nspeculative decoding techniques is to improve the average inference time of a\nlarge, target model without sacrificing its accuracy, by using a more efficient\ndraft model to propose draft tokens which are then verified in parallel. The\nnumber of draft tokens produced in each drafting round is referred to as the\ndraft length and is often a static hyperparameter chosen based on the\nacceptance rate statistics of the draft tokens. However, setting a static draft\nlength can negatively impact performance, especially in scenarios where\ndrafting is expensive and there is a high variance in the number of tokens\naccepted. Adaptive Entropy-based Draft Length (AdaEDL) is a simple, training\nand parameter-free criteria which allows for early stopping of the token\ndrafting process by approximating a lower bound on the expected acceptance\nprobability of the drafted token based on the currently observed entropy of the\ndrafted logits. We show that AdaEDL consistently outperforms static\ndraft-length speculative decoding by 10%-57% as well as other training-free\ndraft-stopping techniques by upto 10% in a variety of settings and datasets. At\nthe same time, we show that AdaEDL is more robust than these techniques and\npreserves performance in high-sampling-temperature scenarios. Since it is\ntraining-free, in contrast to techniques that rely on the training of\ndataset-specific draft-stopping predictors, AdaEDL can seamlessly be integrated\ninto a variety of pre-existing LLM systems.", "published": "2024-10-24 01:13:43", "link": "http://arxiv.org/abs/2410.18351v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SPEED++: A Multilingual Event Extraction Framework for Epidemic\n  Prediction and Preparedness", "abstract": "Social media is often the first place where communities discuss the latest\nsocietal trends. Prior works have utilized this platform to extract\nepidemic-related information (e.g. infections, preventive measures) to provide\nearly warnings for epidemic prediction. However, these works only focused on\nEnglish posts, while epidemics can occur anywhere in the world, and early\ndiscussions are often in the local, non-English languages. In this work, we\nintroduce the first multilingual Event Extraction (EE) framework SPEED++ for\nextracting epidemic event information for a wide range of diseases and\nlanguages. To this end, we extend a previous epidemic ontology with 20 argument\nroles; and curate our multilingual EE dataset SPEED++ comprising 5.1K tweets in\nfour languages for four diseases. Annotating data in every language is\ninfeasible; thus we develop zero-shot cross-lingual cross-disease models (i.e.,\ntraining only on English COVID data) utilizing multilingual pre-training and\nshow their efficacy in extracting epidemic-related events for 65 diverse\nlanguages across different diseases. Experiments demonstrate that our framework\ncan provide epidemic warnings for COVID-19 in its earliest stages in Dec 2019\n(3 weeks before global discussions) from Chinese Weibo posts without any\ntraining in Chinese. Furthermore, we exploit our framework's argument\nextraction capabilities to aggregate community epidemic discussions like\nsymptoms and cure measures, aiding misinformation detection and public\nattention monitoring. Overall, we lay a strong foundation for multilingual\nepidemic preparedness.", "published": "2024-10-24 03:03:54", "link": "http://arxiv.org/abs/2410.18393v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Large Language Models Reflect the Ideology of their Creators", "abstract": "Large language models (LLMs) are trained on vast amounts of data to generate\nnatural language, enabling them to perform tasks like text summarization and\nquestion answering. These models have become popular in artificial intelligence\n(AI) assistants like ChatGPT and already play an influential role in how humans\naccess information. However, the behavior of LLMs varies depending on their\ndesign, training, and use.\n  In this paper, we prompt a diverse panel of popular LLMs to describe a large\nnumber of prominent personalities with political relevance, in all six official\nlanguages of the United Nations. By identifying and analyzing moral assessments\nreflected in their responses, we find normative differences between LLMs from\ndifferent geopolitical regions, as well as between the responses of the same\nLLM when prompted in different languages. Among only models in the United\nStates, we find that popularly hypothesized disparities in political views are\nreflected in significant normative differences related to progressive values.\nAmong Chinese models, we characterize a division between internationally- and\ndomestically-focused models.\n  Our results show that the ideological stance of an LLM appears to reflect the\nworldview of its creators. This poses the risk of political instrumentalization\nand raises concerns around technological and regulatory efforts with the stated\naim of making LLMs ideologically 'unbiased'.", "published": "2024-10-24 04:02:30", "link": "http://arxiv.org/abs/2410.18417v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs", "abstract": "In this report, we introduce a collection of methods to enhance reward\nmodeling for LLMs, focusing specifically on data-centric techniques. We propose\neffective data selection and filtering strategies for curating high-quality\nopen-source preference datasets, culminating in the Skywork-Reward data\ncollection, which contains only 80K preference pairs -- significantly smaller\nthan existing datasets. Using this curated dataset, we developed the\nSkywork-Reward model series -- Skywork-Reward-Gemma-27B and\nSkywork-Reward-Llama-3.1-8B -- with the former currently holding the top\nposition on the RewardBench leaderboard. Notably, our techniques and datasets\nhave directly enhanced the performance of many top-ranked models on\nRewardBench, highlighting the practical impact of our contributions in\nreal-world preference learning applications.", "published": "2024-10-24 06:06:26", "link": "http://arxiv.org/abs/2410.18451v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities", "abstract": "Recent research has shown that Large Language Models (LLMs) are vulnerable to\nautomated jailbreak attacks, where adversarial suffixes crafted by algorithms\nappended to harmful queries bypass safety alignment and trigger unintended\nresponses. Current methods for generating these suffixes are computationally\nexpensive and have low Attack Success Rates (ASR), especially against\nwell-aligned models like Llama2 and Llama3. To overcome these limitations, we\nintroduce ADV-LLM, an iterative self-tuning process that crafts adversarial\nLLMs with enhanced jailbreak ability. Our framework significantly reduces the\ncomputational cost of generating adversarial suffixes while achieving nearly\n100\\% ASR on various open-source LLMs. Moreover, it exhibits strong attack\ntransferability to closed-source models, achieving 99\\% ASR on GPT-3.5 and 49\\%\nASR on GPT-4, despite being optimized solely on Llama3. Beyond improving\njailbreak ability, ADV-LLM provides valuable insights for future safety\nalignment research through its ability to generate large datasets for studying\nLLM safety.", "published": "2024-10-24 06:36:12", "link": "http://arxiv.org/abs/2410.18469v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LOGO -- Long cOntext aliGnment via efficient preference Optimization", "abstract": "Long-context models(LCMs) have shown great potential in processing long input\nsequences(even more than 100M tokens) conveniently and effectively. With\nsignificant progress, recent research has pointed out that LCMs can accurately\nlocate token-level salient information within the context. Yet, the generation\nperformance of these LCMs is far from satisfactory and might result in\nmisaligned responses, such as hallucinations. To enhance the generation\ncapability of LCMs, existing works have investigated the effects of data size\nand quality for both pre-training and instruction tuning. Though achieving\nmeaningful improvement, previous methods fall short in either effectiveness or\nefficiency. In this paper, we introduce LOGO(Long cOntext aliGnment via\nefficient preference Optimization), a training strategy that first introduces\npreference optimization for long-context alignment. To overcome the GPU\nmemory-bound issue caused by the long sequence, LOGO employs a reference-free\npreference optimization strategy and adopts a position synthesis method to\nconstruct the training data. By training with only 0.3B data on a single\n8$\\times$A800 GPU machine for 16 hours, LOGO allows the Llama-3-8B-Instruct-80K\nmodel to achieve comparable performance with GPT-4 in real-world long-context\ntasks while preserving the model's original capabilities on other tasks, e.g.,\nlanguage modeling and MMLU. Moreover, LOGO can extend the model's context\nwindow size while enhancing its generation performance.", "published": "2024-10-24 08:27:26", "link": "http://arxiv.org/abs/2410.18533v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On Explaining with Attention Matrices", "abstract": "This paper explores the much discussed, possible explanatory link between\nattention weights (AW) in transformer models and predicted output. Contrary to\nintuition and early research on attention, more recent prior research has\nprovided formal arguments and empirical evidence that AW are not explanatorily\nrelevant. We show that the formal arguments are incorrect. We introduce and\neffectively compute efficient attention, which isolates the effective\ncomponents of attention matrices in tasks and models in which AW play an\nexplanatory role. We show that efficient attention has a causal role (provides\nminimally necessary and sufficient conditions) for predicting model output in\nNLP tasks requiring contextual information, and we show, contrary to [7], that\nefficient attention matrices are probability distributions and are effectively\ncalculable. Thus, they should play an important part in the explanation of\nattention based model behavior. We offer empirical experiments in support of\nour method illustrating various properties of efficient attention with various\nmetrics on four datasets.", "published": "2024-10-24 08:43:33", "link": "http://arxiv.org/abs/2410.18541v1", "categories": ["cs.CL", "cs.AI", "46-04", "I.2.7; I.7.0"], "primary_category": "cs.CL"}
{"title": "Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and\n  Evaluation", "abstract": "We introduce Bielik 7B v0.1, a 7-billion-parameter generative text model for\nPolish language processing. Trained on curated Polish corpora, this model\naddresses key challenges in language model development through innovative\ntechniques. These include Weighted Instruction Cross-Entropy Loss, which\nbalances the learning of different instruction types, and Adaptive Learning\nRate, which dynamically adjusts the learning rate based on training progress.\nTo evaluate performance, we created the Open PL LLM Leaderboard and Polish\nMT-Bench, novel frameworks assessing various NLP tasks and conversational\nabilities. Bielik 7B v0.1 demonstrates significant improvements, achieving a 9\npercentage point increase in average score compared to Mistral-7B-v0.1 on the\nRAG Reader task. It also excels in the Polish MT-Bench, particularly in\nReasoning (6.15/10) and Role-playing (7.83/10) categories. This model\nrepresents a substantial advancement in Polish language AI, offering a powerful\ntool for diverse linguistic applications and setting new benchmarks in the\nfield.", "published": "2024-10-24 09:16:09", "link": "http://arxiv.org/abs/2410.18565v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Speech perception: a model of word recognition", "abstract": "We present a model of speech perception which takes into account effects of\ncorrelations between sounds. Words in this model correspond to the attractors\nof a suitably chosen descent dynamics. The resulting lexicon is rich in short\nwords, and much less so in longer ones, as befits a reasonable word length\ndistribution. We separately examine the decryption of short and long words in\nthe presence of mishearings. In the regime of short words, the algorithm either\nquickly retrieves a word, or proposes another valid word. In the regime of\nlonger words, the behaviour is markedly different. While the successful\ndecryption of words continues to be relatively fast, there is a finite\nprobability of getting lost permanently, as the algorithm wanders round the\nlandscape of suitable words without ever settling on one.", "published": "2024-10-24 09:41:47", "link": "http://arxiv.org/abs/2410.18590v1", "categories": ["cond-mat.stat-mech", "cs.CL"], "primary_category": "cond-mat.stat-mech"}
{"title": "Supporting Assessment of Novelty of Design Problems Using Concept of\n  Problem SAPPhIRE", "abstract": "This paper proposes a framework for assessing the novelty of design problems\nusing the SAPPhIRE model of causality. The novelty of a problem is measured as\nits minimum distance from the problems in a reference problem database. The\ndistance is calculated by comparing the current problem and each reference past\nproblem at the various levels of abstraction in the SAPPhIRE ontology. The\nbasis for comparison is textual similarity. To demonstrate the applicability of\nthe proposed framework, The current set of problems associated with an\nartifact, as collected from its stakeholders, were compared with the past set\nof problems, as collected from patents and other web sources, to assess the\nnovelty of the current set. This approach is aimed at providing a better\nunderstanding of the degree of novelty of any given set of current problems by\ncomparing them to similar problems available from historical records. Since\nmanual assessment, the current mode of such assessments as reported in the\nliterature, is a tedious process, to reduce time complexity and to afford\nbetter applicability for larger sets of problem statements, an automated\nassessment is proposed and used in this paper.", "published": "2024-10-24 10:39:49", "link": "http://arxiv.org/abs/2410.18629v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Better Open-Ended Text Generation: A Multicriteria Evaluation\n  Framework", "abstract": "Open-ended text generation has become a prominent task in natural language\nprocessing due to the rise of powerful (large) language models. However,\nevaluating the quality of these models and the employed decoding strategies\nremains challenging because of trade-offs among widely used metrics such as\ncoherence, diversity, and perplexity. Decoding methods often excel in some\nmetrics while underperforming in others, complicating the establishment of a\nclear ranking. In this paper, we present novel ranking strategies within this\nmulticriteria framework. Specifically, we employ benchmarking approaches based\non partial orderings and present a new summary metric designed to balance\nexisting automatic indicators, providing a more holistic evaluation of text\ngeneration quality. Our experiments demonstrate that the proposed methods offer\na robust way to compare decoding strategies, and serve as valuable tools in\nguiding model selection for open-ended text generation tasks. Finally, we\nsuggest future directions for improving evaluation methodologies in text\ngeneration. Our codebase, datasets, and models are publicly available.", "published": "2024-10-24 11:32:01", "link": "http://arxiv.org/abs/2410.18653v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis\n  from Scratch", "abstract": "The availability of high-quality data is one of the most important factors in\nimproving the reasoning capability of LLMs. Existing works have demonstrated\nthe effectiveness of creating more instruction data from seed questions or\nknowledge bases. Recent research indicates that continually scaling up data\nsynthesis from strong models (e.g., GPT-4) can further elicit reasoning\nperformance. Though promising, the open-sourced community still lacks\nhigh-quality data at scale and scalable data synthesis methods with affordable\ncosts. To address this, we introduce ScaleQuest, a scalable and novel data\nsynthesis method that utilizes \"small-size\" (e.g., 7B) open-source models to\ngenerate questions from scratch without the need for seed data with complex\naugmentation constraints. With the efficient ScaleQuest, we automatically\nconstructed a mathematical reasoning dataset consisting of 1 million\nproblem-solution pairs, which are more effective than existing open-sourced\ndatasets. It can universally increase the performance of mainstream open-source\nmodels (i.e., Mistral, Llama3, DeepSeekMath, and Qwen2-Math) by achieving 29.2%\nto 46.4% gains on MATH. Notably, simply fine-tuning the Qwen2-Math-7B-Base\nmodel with our dataset can even surpass Qwen2-Math-7B-Instruct, a strong and\nwell-aligned model on closed-source data, and proprietary models such as\nGPT-4-Turbo and Claude-3.5 Sonnet.", "published": "2024-10-24 12:42:04", "link": "http://arxiv.org/abs/2410.18693v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Good Are LLMs for Literary Translation, Really? Literary Translation\n  Evaluation with Humans and LLMs", "abstract": "Recent research has focused on literary machine translation (MT) as a new\nchallenge in MT. However, the evaluation of literary MT remains an open\nproblem. We contribute to this ongoing discussion by introducing\nLITEVAL-CORPUS, a paragraph-level parallel corpus containing verified human\ntranslations and outputs from 9 MT systems, which totals over 2k translations\nand 13k evaluated sentences across four language pairs, costing 4.5k C. This\ncorpus enables us to (i) examine the consistency and adequacy of human\nevaluation schemes with various degrees of complexity, (ii) compare evaluations\nby students and professionals, assess the effectiveness of (iii) LLM-based\nmetrics and (iv) LLMs themselves. Our findings indicate that the adequacy of\nhuman evaluation is controlled by two factors: the complexity of the evaluation\nscheme (more complex is less adequate) and the expertise of evaluators (higher\nexpertise yields more adequate evaluations). For instance, MQM\n(Multidimensional Quality Metrics), a complex scheme and the de facto standard\nfor non-literary human MT evaluation, is largely inadequate for literary\ntranslation evaluation: with student evaluators, nearly 60% of human\ntranslations are misjudged as indistinguishable or inferior to machine\ntranslations. In contrast, BWS (BEST-WORST SCALING), a much simpler scheme,\nidentifies human translations at a rate of 80-100%. Automatic metrics fare\ndramatically worse, with rates of at most 20%. Our overall evaluation indicates\nthat published human translations consistently outperform LLM translations,\nwhere even the most recent LLMs tend to produce considerably more literal and\nless diverse translations compared to humans.", "published": "2024-10-24 12:48:03", "link": "http://arxiv.org/abs/2410.18697v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Little Help Goes a Long Way: Efficient LLM Training by Leveraging\n  Small LMs", "abstract": "A primary challenge in large language model (LLM) development is their\nonerous pre-training cost. Typically, such pre-training involves optimizing a\nself-supervised objective (such as next-token prediction) over a large corpus.\nThis paper explores a promising paradigm to improve LLM pre-training efficiency\nand quality by suitably leveraging a small language model (SLM). In particular,\nthis paradigm relies on an SLM to both (1) provide soft labels as additional\ntraining supervision, and (2) select a small subset of valuable (\"informative\"\nand \"hard\") training examples. Put together, this enables an effective transfer\nof the SLM's predictive distribution to the LLM, while prioritizing specific\nregions of the training data distribution. Empirically, this leads to reduced\nLLM training time compared to standard training, while improving the overall\nquality. Theoretically, we develop a statistical framework to systematically\nstudy the utility of SLMs in enabling efficient training of high-quality LLMs.\nIn particular, our framework characterizes how the SLM's seemingly low-quality\nsupervision can enhance the training of a much more capable LLM. Furthermore,\nit also highlights the need for an adaptive utilization of such supervision, by\nstriking a balance between the bias and variance introduced by the SLM-provided\nsoft labels. We corroborate our theoretical framework by improving the\npre-training of an LLM with 2.8B parameters by utilizing a smaller LM with 1.5B\nparameters on the Pile dataset.", "published": "2024-10-24 14:31:52", "link": "http://arxiv.org/abs/2410.18779v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "An LLM Agent for Automatic Geospatial Data Analysis", "abstract": "Large language models (LLMs) are being used in data science code generation\ntasks, but they often struggle with complex sequential tasks, leading to\nlogical errors. Their application to geospatial data processing is particularly\nchallenging due to difficulties in incorporating complex data structures and\nspatial constraints, effectively utilizing diverse function calls, and the\ntendency to hallucinate less-used geospatial libraries. To tackle these\nproblems, we introduce GeoAgent, a new interactive framework designed to help\nLLMs handle geospatial data processing more effectively. GeoAgent pioneers the\nintegration of a code interpreter, static analysis, and Retrieval-Augmented\nGeneration (RAG) techniques within a Monte Carlo Tree Search (MCTS) algorithm,\noffering a novel approach to geospatial data processing. In addition, we\ncontribute a new benchmark specifically designed to evaluate the LLM-based\napproach in geospatial tasks. This benchmark leverages a variety of Python\nlibraries and includes both single-turn and multi-turn tasks such as data\nacquisition, data analysis, and visualization. By offering a comprehensive\nevaluation among diverse geospatial contexts, this benchmark sets a new\nstandard for developing LLM-based approaches in geospatial data analysis tasks.\nOur findings suggest that relying solely on knowledge of LLM is insufficient\nfor accurate geospatial task programming, which requires coherent multi-step\nprocesses and multiple function calls. Compared to the baseline LLMs, the\nproposed GeoAgent has demonstrated superior performance, yielding notable\nimprovements in function calls and task completion. In addition, these results\noffer valuable insights for the future development of LLM agents in automatic\ngeospatial data analysis task programming.", "published": "2024-10-24 14:47:25", "link": "http://arxiv.org/abs/2410.18792v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "A Combinatorial Approach to Neural Emergent Communication", "abstract": "Substantial research on deep learning-based emergent communication uses the\nreferential game framework, specifically the Lewis signaling game, however we\nargue that successful communication in this game typically only need one or two\nsymbols for target image classification because of a sampling pitfall in the\ntraining data. To address this issue, we provide a theoretical analysis and\nintroduce a combinatorial algorithm SolveMinSym (SMS) to solve the symbolic\ncomplexity for classification, which is the minimum number of symbols in the\nmessage for successful communication. We use the SMS algorithm to create\ndatasets with different symbolic complexity to empirically show that data with\nhigher symbolic complexity increases the number of effective symbols in the\nemergent language.", "published": "2024-10-24 14:54:09", "link": "http://arxiv.org/abs/2410.18806v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "From English-Centric to Effective Bilingual: LLMs with Custom Tokenizers\n  for Underrepresented Languages", "abstract": "In this paper, we propose a model-agnostic cost-effective approach to\ndeveloping bilingual base large language models (LLMs) to support English and\nany target language. The method includes vocabulary expansion, initialization\nof new embeddings, model training and evaluation. We performed our experiments\nwith three languages, each using a non-Latin script - Ukrainian, Arabic, and\nGeorgian.\n  Our approach demonstrates improved language performance while reducing\ncomputational costs. It mitigates the disproportionate penalization of\nunderrepresented languages, promoting fairness and minimizing adverse phenomena\nsuch as code-switching and broken grammar. Additionally, we introduce new\nmetrics to evaluate language quality, revealing that vocabulary size\nsignificantly impacts the quality of generated text.", "published": "2024-10-24 15:20:54", "link": "http://arxiv.org/abs/2410.18836v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Demystifying Large Language Models for Medicine: A Primer", "abstract": "Large language models (LLMs) represent a transformative class of AI tools\ncapable of revolutionizing various aspects of healthcare by generating\nhuman-like responses across diverse contexts and adapting to novel tasks\nfollowing human instructions. Their potential application spans a broad range\nof medical tasks, such as clinical documentation, matching patients to clinical\ntrials, and answering medical questions. In this primer paper, we propose an\nactionable guideline to help healthcare professionals more efficiently utilize\nLLMs in their work, along with a set of best practices. This approach consists\nof several main phases, including formulating the task, choosing LLMs, prompt\nengineering, fine-tuning, and deployment. We start with the discussion of\ncritical considerations in identifying healthcare tasks that align with the\ncore capabilities of LLMs and selecting models based on the selected task and\ndata, performance requirements, and model interface. We then review the\nstrategies, such as prompt engineering and fine-tuning, to adapt standard LLMs\nto specialized medical tasks. Deployment considerations, including regulatory\ncompliance, ethical guidelines, and continuous monitoring for fairness and\nbias, are also discussed. By providing a structured step-by-step methodology,\nthis tutorial aims to equip healthcare professionals with the tools necessary\nto effectively integrate LLMs into clinical practice, ensuring that these\npowerful technologies are applied in a safe, reliable, and impactful manner.", "published": "2024-10-24 15:41:56", "link": "http://arxiv.org/abs/2410.18856v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate\n  Hallucinations", "abstract": "Large Language Models (LLMs) often hallucinate, producing unfaithful or\nfactually incorrect outputs by misrepresenting the provided context or\nincorrectly recalling internal knowledge. Recent studies have identified\nspecific attention heads within the Transformer architecture, known as\nretrieval heads, responsible for extracting relevant contextual information. We\nhypothesise that masking these retrieval heads can induce hallucinations and\nthat contrasting the outputs of the base LLM and the masked LLM can reduce\nhallucinations. To this end, we propose Decoding by Contrasting Retrieval Heads\n(DeCoRe), a novel training-free decoding strategy that amplifies information\nfound in the context and model parameters. DeCoRe mitigates potentially\nhallucinated responses by dynamically contrasting the outputs of the base LLM\nand the masked LLM, using conditional entropy as a guide. Our extensive\nexperiments confirm that DeCoRe significantly improves performance on tasks\nrequiring high contextual faithfulness, such as summarisation (XSum by 18.6%),\ninstruction following (MemoTrap by 10.9%), and open-book question answering\n(NQ-Open by 2.4% and NQ-Swap by 5.5%).", "published": "2024-10-24 15:44:33", "link": "http://arxiv.org/abs/2410.18860v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Schema-Guided Culture-Aware Complex Event Simulation with Multi-Agent\n  Role-Play", "abstract": "Complex news events, such as natural disasters and socio-political conflicts,\nrequire swift responses from the government and society. Relying on historical\nevents to project the future is insufficient as such events are sparse and do\nnot cover all possible conditions and nuanced situations. Simulation of these\ncomplex events can help better prepare and reduce the negative impact. We\ndevelop a controllable complex news event simulator guided by both the event\nschema representing domain knowledge about the scenario and user-provided\nassumptions representing case-specific conditions. As event dynamics depend on\nthe fine-grained social and cultural context, we further introduce a\ngeo-diverse commonsense and cultural norm-aware knowledge enhancement\ncomponent. To enhance the coherence of the simulation, apart from the global\ntimeline of events, we take an agent-based approach to simulate the individual\ncharacter states, plans, and actions. By incorporating the schema and cultural\nnorms, our generated simulations achieve much higher coherence and\nappropriateness and are received favorably by participants from a humanitarian\nassistance organization.", "published": "2024-10-24 17:21:43", "link": "http://arxiv.org/abs/2410.18935v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "OSCAR: Operating System Control via State-Aware Reasoning and\n  Re-Planning", "abstract": "Large language models (LLMs) and large multimodal models (LMMs) have shown\ngreat potential in automating complex tasks like web browsing and gaming.\nHowever, their ability to generalize across diverse applications remains\nlimited, hindering broader utility. To address this challenge, we present\nOSCAR: Operating System Control via state-Aware reasoning and Re-planning.\nOSCAR is a generalist agent designed to autonomously navigate and interact with\nvarious desktop and mobile applications through standardized controls, such as\nmouse and keyboard inputs, while processing screen images to fulfill user\ncommands. OSCAR translates human instructions into executable Python code,\nenabling precise control over graphical user interfaces (GUIs). To enhance\nstability and adaptability, OSCAR operates as a state machine, equipped with\nerror-handling mechanisms and dynamic task re-planning, allowing it to\nefficiently adjust to real-time feedback and exceptions. We demonstrate OSCAR's\neffectiveness through extensive experiments on diverse benchmarks across\ndesktop and mobile platforms, where it transforms complex workflows into simple\nnatural language commands, significantly boosting user productivity. Our code\nwill be open-source upon publication.", "published": "2024-10-24 17:58:08", "link": "http://arxiv.org/abs/2410.18963v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Infogent: An Agent-Based Framework for Web Information Aggregation", "abstract": "Despite seemingly performant web agents on the task-completion benchmarks,\nmost existing methods evaluate the agents based on a presupposition: the web\nnavigation task consists of linear sequence of actions with an end state that\nmarks task completion. In contrast, our work focuses on web navigation for\ninformation aggregation, wherein the agent must explore different websites to\ngather information for a complex query. We consider web information aggregation\nfrom two different perspectives: (i) Direct API-driven Access relies on a\ntext-only view of the Web, leveraging external tools such as Google Search API\nto navigate the web and a scraper to extract website contents. (ii) Interactive\nVisual Access uses screenshots of the webpages and requires interaction with\nthe browser to navigate and access information. Motivated by these diverse\ninformation access settings, we introduce Infogent, a novel modular framework\nfor web information aggregation involving three distinct components: Navigator,\nExtractor and Aggregator. Experiments on different information access settings\ndemonstrate Infogent beats an existing SOTA multi-agent search framework by 7%\nunder Direct API-Driven Access on FRAMES, and improves over an existing\ninformation-seeking web agent by 4.3% under Interactive Visual Access on\nAssistantBench.", "published": "2024-10-24 18:01:28", "link": "http://arxiv.org/abs/2410.19054v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Watermarking Large Language Models and the Generated Content:\n  Opportunities and Challenges", "abstract": "The widely adopted and powerful generative large language models (LLMs) have\nraised concerns about intellectual property rights violations and the spread of\nmachine-generated misinformation. Watermarking serves as a promising approch to\nestablish ownership, prevent unauthorized use, and trace the origins of\nLLM-generated content. This paper summarizes and shares the challenges and\nopportunities we found when watermarking LLMs. We begin by introducing\ntechniques for watermarking LLMs themselves under different threat models and\nscenarios. Next, we investigate watermarking methods designed for the content\ngenerated by LLMs, assessing their effectiveness and resilience against various\nattacks. We also highlight the importance of watermarking domain-specific\nmodels and data, such as those used in code generation, chip design, and\nmedical applications. Furthermore, we explore methods like hardware\nacceleration to improve the efficiency of the watermarking process. Finally, we\ndiscuss the limitations of current approaches and outline future research\ndirections for the responsible use and protection of these generative AI tools.", "published": "2024-10-24 18:55:33", "link": "http://arxiv.org/abs/2410.19096v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "RSA-Control: A Pragmatics-Grounded Lightweight Controllable Text\n  Generation Framework", "abstract": "Despite significant advancements in natural language generation, controlling\nlanguage models to produce texts with desired attributes remains a formidable\nchallenge. In this work, we introduce RSA-Control, a training-free controllable\ntext generation framework grounded in pragmatics. RSA-Control directs the\ngeneration process by recursively reasoning between imaginary speakers and\nlisteners, enhancing the likelihood that target attributes are correctly\ninterpreted by listeners amidst distractors. Additionally, we introduce a\nself-adjustable rationality parameter, which allows for automatic adjustment of\ncontrol strength based on context. Our experiments, conducted with two task\ntypes and two types of language models, demonstrate that RSA-Control achieves\nstrong attribute control while maintaining language fluency and content\nconsistency. Our code is available at https://github.com/Ewanwong/RSA-Control.", "published": "2024-10-24 19:21:04", "link": "http://arxiv.org/abs/2410.19109v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "LLM Tree Search", "abstract": "This project aims to investigate a novel sequence generation method inspired\nby the AlphaGo paradigm, adapting it for use with large language models (LLMs).\nThe proposed approach involves creating search trees of different possible\ncompletions and evaluating these completions based on model confidence. By\nconsidering various paths in the search tree and scoring them according to the\nmodel's confidence in each completion, we can generate diverse and high-quality\nsequences. This research explores the implementation of this paradigm by using\nconfidence as a proxy for response quality akin to beam search\n\\citep{vijayakumar2016diverse}. The primary goal of this paper is to outline\nthe paradigm and demonstrate its potential, rather than focusing on achieving\nperfect results. The paper will outline the reasons why we believe this\nparadigm has the potential to improve LLMs in the following manners: 1)\nincrease output quality, 2) decrease errors, 3) eliminate or reduce the\ncompound error problems, 4) generate diverse and creative completions, 5) allow\nfor iterative problem-solving, and 6) self-training. We expect this approach to\nyield a set of diverse and coherent sequences, offering insights into balancing\nexploration and exploitation in sequence generation. Potential applications\ninclude creative text generation tasks, such as storytelling and content\ncreation, as well as other natural language processing domains, like machine\ntranslation and automated summarization. The goal is that the model will be far\nmore effective as it will be able to consider many possible variations allowing\nit to find the ideal completion. This research aims to contribute to the\nunderstanding of effective search strategies in sequence generation and their\nimpact on generating high-quality, varied textual outputs.", "published": "2024-10-24 19:38:50", "link": "http://arxiv.org/abs/2410.19117v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design", "abstract": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME.", "published": "2024-10-24 19:48:51", "link": "http://arxiv.org/abs/2410.19123v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Indication Finding: a novel use case for representation learning", "abstract": "Many therapies are effective in treating multiple diseases. We present an\napproach that leverages methods developed in natural language processing and\nreal-world data to prioritize potential, new indications for a mechanism of\naction (MoA). We specifically use representation learning to generate\nembeddings of indications and prioritize them based on their proximity to the\nindications with the strongest available evidence for the MoA. We demonstrate\nthe successful deployment of our approach for anti-IL-17A using embeddings\ngenerated with SPPMI and present an evaluation framework to determine the\nquality of indication finding results and the derived embeddings.", "published": "2024-10-24 22:03:36", "link": "http://arxiv.org/abs/2410.19174v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Inference time LLM alignment in single and multidomain preference\n  spectrum", "abstract": "Aligning Large Language Models (LLM) to address subjectivity and nuanced\npreference levels requires adequate flexibility and control, which can be a\nresource-intensive and time-consuming procedure. Existing training-time\nalignment methods require full re-training when a change is needed and\ninference-time ones typically require access to the reward model at each\ninference step. To address these limitations, we introduce inference-time model\nalignment method that learns encoded representations of preference dimensions,\ncalled \\textit{Alignment Vectors} (AV). These representations are computed by\nsubtraction of the base model from the aligned model as in model editing\nenabling dynamically adjusting the model behavior during inference through\nsimple linear operations. Even though the preference dimensions can span\nvarious granularity levels, here we focus on three gradual response levels\nacross three specialized domains: medical, legal, and financial, exemplifying\nits practical potential. This new alignment paradigm introduces adjustable\npreference knobs during inference, allowing users to tailor their LLM outputs\nwhile reducing the inference cost by half compared to the prompt engineering\napproach. Additionally, we find that AVs are transferable across different\nfine-tuning stages of the same model, demonstrating their flexibility. AVs also\nfacilitate multidomain, diverse preference alignment, making the process 12x\nfaster than the retraining approach.", "published": "2024-10-24 23:31:39", "link": "http://arxiv.org/abs/2410.19206v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Critical biblical studies via word frequency analysis: unveiling text\n  authorship", "abstract": "The Bible, a product of an extensive and intricate process of oral-written\ntransmission spanning centuries, obscures the contours of its earlier\nrecensions. Debate rages over determining the existing layers and identifying\nthe date of composition and historical background of the biblical texts.\nTraditional manual methodologies have grappled with authorship challenges\nthrough scrupulous textual criticism, employing linguistic, stylistic,\ninner-biblical, and historical criteria. Despite recent progress in\ncomputer-assisted analysis, many patterns still need to be uncovered in\nBiblical Texts. In this study, we address the question of authorship of\nbiblical texts by employing statistical analysis to the frequency of words\nusing a method that is particularly sensitive to deviations in frequencies\nassociated with a few words out of potentially many. We aim to differentiate\nbetween three distinct authors across numerous chapters spanning the first nine\nbooks of the Bible. In particular, we examine 50 chapters labeled according to\nbiblical exegesis considerations into three corpora (D, DtrH, and P). Without\nprior assumptions about author identity, our approach leverages subtle\ndifferences in word frequencies to distinguish among the three corpora and\nidentify author-dependent linguistic properties. Our analysis indicates that\nthe first two authors (D and DtrH) are much more closely related compared to P,\na fact that aligns with expert assessments. Additionally, we attain high\naccuracy in attributing authorship by evaluating the similarity of each chapter\nwith the reference corpora. This study sheds new light on the authorship of\nbiblical texts by providing interpretable, statistically significant evidence\nthat there are different linguistic characteristics of biblical authors and\nthat these differences can be identified.", "published": "2024-10-24 22:08:38", "link": "http://arxiv.org/abs/2410.19883v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Aggregated Knowledge Model: Enhancing Domain-Specific QA with Fine-Tuned\n  and Retrieval-Augmented Generation Models", "abstract": "This paper introduces a novel approach to enhancing closed-domain Question\nAnswering (QA) systems, focusing on the specific needs of the Lawrence Berkeley\nNational Laboratory (LBL) Science Information Technology (ScienceIT) domain.\nUtilizing a rich dataset derived from the ScienceIT documentation, our study\nembarks on a detailed comparison of two fine-tuned large language models and\nfive retrieval-augmented generation (RAG) models. Through data processing\ntechniques, we transform the documentation into structured\ncontext-question-answer triples, leveraging the latest Large Language Models\n(AWS Bedrock, GCP PaLM2, Meta LLaMA2, OpenAI GPT-4, Google Gemini-Pro) for\ndata-driven insights. Additionally, we introduce the Aggregated Knowledge Model\n(AKM), which synthesizes responses from the seven models mentioned above using\nK-means clustering to select the most representative answers. The evaluation of\nthese models across multiple metrics offers a comprehensive look into their\neffectiveness and suitability for the LBL ScienceIT environment. The results\ndemonstrate the potential benefits of integrating fine-tuning and\nretrieval-augmented strategies, highlighting significant performance\nimprovements achieved with the AKM. The insights gained from this study can be\napplied to develop specialized QA systems tailored to specific domains.", "published": "2024-10-24 00:49:46", "link": "http://arxiv.org/abs/2410.18344v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WAFFLE: Multi-Modal Model for Automated Front-End Development", "abstract": "Web development involves turning UI designs into functional webpages, which\ncan be difficult for both beginners and experienced developers due to the\ncomplexity of HTML's hierarchical structures and styles. While Large Language\nModels (LLMs) have shown promise in generating source code, two major\nchallenges persist in UI-to-HTML code generation: (1) effectively representing\nHTML's hierarchical structure for LLMs, and (2) bridging the gap between the\nvisual nature of UI designs and the text-based format of HTML code. To tackle\nthese challenges, we introduce Waffle, a new fine-tuning strategy that uses a\nstructure-aware attention mechanism to improve LLMs' understanding of HTML's\nstructure and a contrastive fine-tuning approach to align LLMs' understanding\nof UI images and HTML code. Models fine-tuned with Waffle show up to 9.00 pp\n(percentage point) higher HTML match, 0.0982 higher CW-SSIM, 32.99 higher CLIP,\nand 27.12 pp higher LLEM on our new benchmark WebSight-Test and an existing\nbenchmark Design2Code, outperforming current fine-tuning methods.", "published": "2024-10-24 01:49:49", "link": "http://arxiv.org/abs/2410.18362v1", "categories": ["cs.SE", "cs.CL", "cs.CV"], "primary_category": "cs.SE"}
{"title": "MoMQ: Mixture-of-Experts Enhances Multi-Dialect Query Generation across\n  Relational and Non-Relational Databases", "abstract": "The improvement in translating natural language to structured query language\n(SQL) can be attributed to the advancements in large language models (LLMs).\nOpen-source LLMs, tailored for specific database dialects such as MySQL, have\nshown great performance. However, cloud service providers are looking for a\nunified database manager service (e.g., Cosmos DB from Azure, Amazon Aurora\nfrom AWS, Lindorm from AlibabaCloud) that can support multiple dialects. This\nrequirement has led to the concept of multi-dialect query generation, which\npresents challenges to LLMs. These challenges include syntactic differences\namong dialects and imbalanced data distribution across multiple dialects. To\ntackle these challenges, we propose MoMQ, a novel Mixture-of-Experts-based\nmulti-dialect query generation framework across both relational and\nnon-relational databases. MoMQ employs a dialect expert group for each dialect\nand a multi-level routing strategy to handle dialect-specific knowledge,\nreducing interference during query generation. Additionally, a shared expert\ngroup is introduced to address data imbalance, facilitating the transfer of\ncommon knowledge from high-resource dialects to low-resource ones. Furthermore,\nwe have developed a high-quality multi-dialect query generation benchmark that\ncovers relational and non-relational databases such as MySQL, PostgreSQL,\nCypher for Neo4j, and nGQL for NebulaGraph. Extensive experiments have shown\nthat MoMQ performs effectively and robustly even in resource-imbalanced\nscenarios.", "published": "2024-10-24 03:42:43", "link": "http://arxiv.org/abs/2410.18406v1", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dialog2Flow: Pre-training Soft-Contrastive Action-Driven Sentence\n  Embeddings for Automatic Dialog Flow Extraction", "abstract": "Efficiently deriving structured workflows from unannotated dialogs remains an\nunderexplored and formidable challenge in computational linguistics. Automating\nthis process could significantly accelerate the manual design of workflows in\nnew domains and enable the grounding of large language models in\ndomain-specific flowcharts, enhancing transparency and controllability. In this\npaper, we introduce Dialog2Flow (D2F) embeddings, which differ from\nconventional sentence embeddings by mapping utterances to a latent space where\nthey are grouped according to their communicative and informative functions\n(i.e., the actions they represent). D2F allows for modeling dialogs as\ncontinuous trajectories in a latent space with distinct action-related regions.\nBy clustering D2F embeddings, the latent space is quantized, and dialogs can be\nconverted into sequences of region/action IDs, facilitating the extraction of\nthe underlying workflow. To pre-train D2F, we build a comprehensive dataset by\nunifying twenty task-oriented dialog datasets with normalized per-turn action\nannotations. We also introduce a novel soft contrastive loss that leverages the\nsemantic information of these actions to guide the representation learning\nprocess, showing superior performance compared to standard supervised\ncontrastive loss. Evaluation against various sentence embeddings, including\ndialog-specific ones, demonstrates that D2F yields superior qualitative and\nquantitative results across diverse domains.", "published": "2024-10-24 07:10:18", "link": "http://arxiv.org/abs/2410.18481v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scaling up Masked Diffusion Models on Text", "abstract": "Masked diffusion models (MDMs) have shown promise in language modeling, yet\ntheir scalability and effectiveness in core language tasks, such as text\ngeneration and language understanding, remain underexplored. This paper\nestablishes the first scaling law for MDMs, demonstrating a scaling rate\ncomparable to autoregressive models (ARMs) and a relatively small compute gap.\nMotivated by their scalability, we train a family of MDMs with up to 1.1\nbillion (B) parameters to systematically evaluate their performance against\nARMs of comparable or larger sizes. Fully leveraging the probabilistic\nformulation of MDMs, we propose a simple yet effective unsupervised\nclassifier-free guidance that effectively exploits large-scale unpaired data,\nboosting performance for conditional inference. In language understanding, the\n1.1B MDM outperforms the 1.1B TinyLlama model trained on the same data across\nfour of eight zero-shot benchmarks. Notably, it achieves competitive math\nreasoning ability with the 7B Llama-2 model on the GSM8K dataset. In text\ngeneration, MDMs with 16 times more pre-training time offer a flexible\ntrade-off against ARMs with the accelerated sampling technique KV-Cache: MDMs\nmatch ARMs in performance while being 1.4 times faster during sampling.\nMoreover, MDMs address challenging tasks for ARMs by effectively handling\nbidirectional reasoning and adapting to temporal shifts in data. Notably, a\n1.1B MDM breaks the reverse curse encountered by much larger ARMs with\nsignificantly more data and computation, such as 13B Llama-2 and 175B GPT-3.\nOur code is available at https://github.com/ML-GSAI/SMDM.", "published": "2024-10-24 08:01:22", "link": "http://arxiv.org/abs/2410.18514v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing", "abstract": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory.", "published": "2024-10-24 08:06:41", "link": "http://arxiv.org/abs/2410.18517v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Taipan: Efficient and Expressive State Space Language Models with\n  Selective Attention", "abstract": "Efficient long-context language modeling remains a significant challenge in\nNatural Language Processing (NLP). While Transformers dominate language tasks,\nthey struggle with long sequences due to quadratic computational complexity in\ntraining and linearly scaling memory costs during inference. Recent State Space\nModels (SSMs) such as Mamba offer alternatives with constant memory usage, but\nthey underperform in tasks requiring extensive in-context retrieval. We\nintroduce Taipan, a novel hybrid architecture that combines Mamba-2 with\nSelective Attention Layers (SALs). These SALs identify tokens requiring\nlong-range interactions, remove less important features, and then augment their\nrepresentations using the attention module. This approach balances Mamba's\nefficiency with Transformer-like performance in memory-intensive tasks. By\nconstraining the attention budget, Taipan extends accurate predictions to\ncontext lengths of up to 1 million tokens while preserving computational\nefficiency. Our experiments demonstrate Taipan's superior performance across\nvarious scales and tasks, offering a promising solution for efficient\nlong-context language modeling.", "published": "2024-10-24 09:25:37", "link": "http://arxiv.org/abs/2410.18572v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prompting and Fine-Tuning of Small LLMs for Length-Controllable\n  Telephone Call Summarization", "abstract": "This paper explores the rapid development of a telephone call summarization\nsystem utilizing large language models (LLMs). Our approach involves initial\nexperiments with prompting existing LLMs to generate summaries of telephone\nconversations, followed by the creation of a tailored synthetic training\ndataset utilizing stronger frontier models. We place special focus on the\ndiversity of the generated data and on the ability to control the length of the\ngenerated summaries to meet various use-case specific requirements. The\neffectiveness of our method is evaluated using two state-of-the-art\nLLM-as-a-judge-based evaluation techniques to ensure the quality and relevance\nof the summaries. Our results show that fine-tuned Llama-2-7B-based\nsummarization model performs on-par with GPT-4 in terms of factual accuracy,\ncompleteness and conciseness. Our findings demonstrate the potential for\nquickly bootstrapping a practical and efficient call summarization system.", "published": "2024-10-24 10:32:10", "link": "http://arxiv.org/abs/2410.18624v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Little Giants: Synthesizing High-Quality Embedding Data at Scale", "abstract": "Synthetic data generation has become an increasingly popular way of training\nmodels without the need for large, manually labeled datasets. For tasks like\ntext embedding, synthetic data offers diverse and scalable training examples,\nsignificantly reducing the cost of human annotation. However, most current\napproaches rely heavily on proprietary models like GPT-4, which are expensive\nand inefficient for generating large-scale embedding data. In this paper, we\nintroduce SPEED, a framework that aligns open-source small models (8B) to\nefficiently generate large-scale synthetic embedding data. Through supervised\nfine-tuning, preference optimization, and self-improvement, SPEED enables small\nopen-source models to produce high-quality data. Remarkably, SPEED uses only\nless than 1/10 of the GPT API calls, outperforming the state-of-the-art\nembedding model E5_mistral when both are trained solely on their synthetic\ndata. Using this efficient generator, we conduct a comprehensive study on how\nvarious factors within the alignment pipeline impact data quality and reveal\nthe scaling law for synthetic embedding data.", "published": "2024-10-24 10:47:30", "link": "http://arxiv.org/abs/2410.18634v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation", "abstract": "Generating high-quality charts with Large Language Models (LLMs) presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. $\\langle \\text{instruction}, \\text{data}, \\text{code} \\rangle$\ntriplets are scarce and expensive to manually curate as their creation demands\ntechnical expertise. To address this scalability challenge, we introduce a\nreference-free automatic feedback generator, which eliminates the need for\ncostly human intervention. Our novel framework, C$^2$, consists of (1) an\nautomatic feedback provider (ChartAF) and (2) a diverse, reference-free dataset\n(ChartUIE-8K). The results are compelling: in our first experiment, 74% of\nrespondents strongly preferred, and 10% preferred, the results after feedback.\nThe second post-feedback experiment demonstrates that ChartAF outperform nine\nbaselines. Moreover, ChartUIE-8K significantly improves data diversity by\nincreasing queries, datasets, and chart types by 5982%, 1936%, and 91%,\nrespectively, over benchmarks. Finally, a study of LLM users revealed that 94%\nof participants preferred ChartUIE-8K's queries, with 93% deeming them aligned\nwith real-world use cases. Core contributions are available as open-source at\nchartsquared.github.io, with ample qualitative examples.", "published": "2024-10-24 11:32:00", "link": "http://arxiv.org/abs/2410.18652v7", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Health Misinformation in Social Networks: A Survey of IT Approaches", "abstract": "In this paper, we present a comprehensive survey on the pervasive issue of\nmedical misinformation in social networks from the perspective of information\ntechnology. The survey aims at providing a systematic review of related\nresearch and helping researchers and practitioners navigate through this\nfast-changing field. Specifically, we first present manual and automatic\napproaches for fact-checking. We then explore fake news detection methods,\nusing content, propagation features, or source features, as well as mitigation\napproaches for countering the spread of misinformation. We also provide a\ndetailed list of several datasets on health misinformation and of publicly\navailable tools. We conclude the survey with a discussion on the open\nchallenges and future research directions in the battle against health\nmisinformation.", "published": "2024-10-24 12:00:51", "link": "http://arxiv.org/abs/2410.18670v1", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Does Differential Privacy Impact Bias in Pretrained NLP Models?", "abstract": "Differential privacy (DP) is applied when fine-tuning pre-trained large\nlanguage models (LLMs) to limit leakage of training examples. While most DP\nresearch has focused on improving a model's privacy-utility tradeoff, some find\nthat DP can be unfair to or biased against underrepresented groups. In this\nwork, we show the impact of DP on bias in LLMs through empirical analysis.\nDifferentially private training can increase the model bias against protected\ngroups w.r.t AUC-based bias metrics. DP makes it more difficult for the model\nto differentiate between the positive and negative examples from the protected\ngroups and other groups in the rest of the population. Our results also show\nthat the impact of DP on bias is not only affected by the privacy protection\nlevel but also the underlying distribution of the dataset.", "published": "2024-10-24 13:59:03", "link": "http://arxiv.org/abs/2410.18749v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "From Imitation to Introspection: Probing Self-Consciousness in Language\n  Models", "abstract": "Self-consciousness, the introspection of one's existence and thoughts,\nrepresents a high-level cognitive process. As language models advance at an\nunprecedented pace, a critical question arises: Are these models becoming\nself-conscious? Drawing upon insights from psychological and neural science,\nthis work presents a practical definition of self-consciousness for language\nmodels and refines ten core concepts. Our work pioneers an investigation into\nself-consciousness in language models by, for the first time, leveraging causal\nstructural games to establish the functional definitions of the ten core\nconcepts. Based on our definitions, we conduct a comprehensive four-stage\nexperiment: quantification (evaluation of ten leading models), representation\n(visualization of self-consciousness within the models), manipulation\n(modification of the models' representation), and acquisition (fine-tuning the\nmodels on core concepts). Our findings indicate that although models are in the\nearly stages of developing self-consciousness, there is a discernible\nrepresentation of certain concepts within their internal mechanisms. However,\nthese representations of self-consciousness are hard to manipulate positively\nat the current stage, yet they can be acquired through targeted fine-tuning.\nOur datasets and code are at https://github.com/OpenCausaLab/SelfConsciousness.", "published": "2024-10-24 15:08:17", "link": "http://arxiv.org/abs/2410.18819v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "kNN For Whisper And Its Effect On Bias And Speaker Adaptation", "abstract": "Speech recognition performance varies by language, domain, and speaker\ncharacteristics such as accent, but fine-tuning a model on any of these\ncategories may lead to catastrophic forgetting. Token-level $k$ nearest\nneighbor search ($k$NN), first proposed for neural sequence decoders for\nnatural language generation (NLG) and machine translation (MT), is a\nnon-parametric method that instead adapts using inference-time search in an\nexternal datastore, without training the underlying model. We show that\nWhisper, a transformer end-to-end speech model, benefits from $k$NN. We\ninvestigate the differences between the speech and text setups. We discuss\nimplications for speaker adaptation, and analyze improvements by gender,\naccent, and age.", "published": "2024-10-24 15:32:52", "link": "http://arxiv.org/abs/2410.18850v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Provably Robust Watermarks for Open-Source Language Models", "abstract": "The recent explosion of high-quality language models has necessitated new\nmethods for identifying AI-generated text. Watermarking is a leading solution\nand could prove to be an essential tool in the age of generative AI. Existing\napproaches embed watermarks at inference and crucially rely on the large\nlanguage model (LLM) specification and parameters being secret, which makes\nthem inapplicable to the open-source setting. In this work, we introduce the\nfirst watermarking scheme for open-source LLMs. Our scheme works by modifying\nthe parameters of the model, but the watermark can be detected from just the\noutputs of the model. Perhaps surprisingly, we prove that our watermarks are\nunremovable under certain assumptions about the adversary's knowledge. To\ndemonstrate the behavior of our construction under concrete parameter\ninstantiations, we present experimental results with OPT-6.7B and OPT-1.3B. We\ndemonstrate robustness to both token substitution and perturbation of the model\nparameters. We find that the stronger of these attacks, the model-perturbation\nattack, requires deteriorating the quality score to 0 out of 100 in order to\nbring the detection rate down to 50%.", "published": "2024-10-24 15:44:34", "link": "http://arxiv.org/abs/2410.18861v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "PRISM: A Methodology for Auditing Biases in Large Language Models", "abstract": "Auditing Large Language Models (LLMs) to discover their biases and\npreferences is an emerging challenge in creating Responsible Artificial\nIntelligence (AI). While various methods have been proposed to elicit the\npreferences of such models, countermeasures have been taken by LLM trainers,\nsuch that LLMs hide, obfuscate or point blank refuse to disclosure their\npositions on certain subjects. This paper presents PRISM, a flexible,\ninquiry-based methodology for auditing LLMs - that seeks to illicit such\npositions indirectly through task-based inquiry prompting rather than direct\ninquiry of said preferences. To demonstrate the utility of the methodology, we\napplied PRISM on the Political Compass Test, where we assessed the political\nleanings of twenty-one LLMs from seven providers. We show LLMs, by default,\nespouse positions that are economically left and socially liberal (consistent\nwith prior work). We also show the space of positions that these models are\nwilling to espouse - where some models are more constrained and less compliant\nthan others - while others are more neutral and objective. In sum, PRISM can\nmore reliably probe and audit LLMs to understand their preferences, biases and\nconstraints.", "published": "2024-10-24 16:57:20", "link": "http://arxiv.org/abs/2410.18906v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "From Blind Solvers to Logical Thinkers: Benchmarking LLMs' Logical\n  Integrity on Faulty Mathematical Problems", "abstract": "Consider the math problem: \"Lily received 3 cookies from her best friend\nyesterday and ate 5 for breakfast. Today, her friend gave her 3 more cookies.\nHow many cookies does Lily have now?\" Many large language models (LLMs) in\nprevious research approach this problem by calculating the answer \"1\" using the\nequation \"3 - 5 + 3.\" However, from a human perspective, we recognize the\ninherent flaw in this problem: Lily cannot eat 5 cookies if she initially only\nhad 3. This discrepancy prompts a key question: Are current LLMs merely Blind\nSolver that apply mathematical operations without deeper reasoning, or can they\nfunction as Logical Thinker capable of identifying logical inconsistencies?\n  To explore this question, we propose a benchmark dataset, FaultyMath, which\nincludes faulty math problems of rich diversity: i) multiple mathematical\ncategories, e.g., algebra, geometry, number theory, etc., ii) varying levels of\ndifficulty, and iii) different origins of faultiness -- ranging from violations\nof common sense and ambiguous statements to mathematical contradictions and\nmore. We evaluate a broad spectrum of LLMs, including open-source,\nclosed-source, and math-specialized models, using FaultyMath across three\ndimensions: (i) How accurately can the models detect faulty math problems\nwithout being explicitly prompted to do so? (ii) When provided with hints --\neither correct or misleading -- about the validity of the problems, to what\nextent do LLMs adapt to become reliable Logical Thinker? (iii) How trustworthy\nare the explanations generated by LLMs when they recognize a math problem as\nflawed? Through extensive experimentation and detailed analysis, our results\ndemonstrate that existing LLMs largely function as Blind Solver and fall short\nof the reasoning capabilities required to perform as Logical Thinker.", "published": "2024-10-24 17:10:39", "link": "http://arxiv.org/abs/2410.18921v2", "categories": ["cs.CL", "cs.AI", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Dynamic Vocabulary Pruning in Early-Exit LLMs", "abstract": "Increasing the size of large language models (LLMs) has been shown to lead to\nbetter performance. However, this comes at the cost of slower and more\nexpensive inference. Early-exiting is a promising approach for improving the\nefficiency of LLM inference by enabling next token prediction at intermediate\nlayers. Yet, the large vocabulary size in modern LLMs makes the confidence\nestimation required for exit decisions computationally expensive, diminishing\nthe efficiency gains. To address this, we propose dynamically pruning the\nvocabulary at test time for each token. Specifically, the vocabulary is pruned\nat one of the initial layers, and the smaller vocabulary is then used\nthroughout the rest of the forward pass. Our experiments demonstrate that such\npost-hoc dynamic vocabulary pruning improves the efficiency of confidence\nestimation in early-exit LLMs while maintaining competitive performance.", "published": "2024-10-24 17:52:31", "link": "http://arxiv.org/abs/2410.18952v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Ferret-UI 2: Mastering Universal User Interface Understanding Across\n  Platforms", "abstract": "Building a generalist model for user interface (UI) understanding is\nchallenging due to various foundational issues, such as platform diversity,\nresolution variation, and data limitation. In this paper, we introduce\nFerret-UI 2, a multimodal large language model (MLLM) designed for universal UI\nunderstanding across a wide range of platforms, including iPhone, Android,\niPad, Webpage, and AppleTV. Building on the foundation of Ferret-UI, Ferret-UI\n2 introduces three key innovations: support for multiple platform types,\nhigh-resolution perception through adaptive scaling, and advanced task training\ndata generation powered by GPT-4o with set-of-mark visual prompting. These\nadvancements enable Ferret-UI 2 to perform complex, user-centered interactions,\nmaking it highly versatile and adaptable for the expanding diversity of\nplatform ecosystems. Extensive empirical experiments on referring, grounding,\nuser-centric advanced tasks (comprising 9 subtasks $\\times$ 5 platforms), GUIDE\nnext-action prediction dataset, and GUI-World multi-platform benchmark\ndemonstrate that Ferret-UI 2 significantly outperforms Ferret-UI, and also\nshows strong cross-platform transfer capabilities.", "published": "2024-10-24 17:58:31", "link": "http://arxiv.org/abs/2410.18967v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "AlignCap: Aligning Speech Emotion Captioning to Human Preferences", "abstract": "Speech Emotion Captioning (SEC) has gradually become an active research task.\nThe emotional content conveyed through human speech are often complex, and\nclassifying them into fixed categories may not be enough to fully capture\nspeech emotions. Describing speech emotions through natural language may be a\nmore effective approach. However, existing SEC methods often produce\nhallucinations and lose generalization on unseen speech. To overcome these\nproblems, we propose AlignCap, which Aligning Speech Emotion Captioning to\nHuman Preferences based on large language model (LLM) with two properties: 1)\nSpeech-Text Alignment, which minimizing the divergence between the LLM's\nresponse prediction distributions for speech and text inputs using knowledge\ndistillation (KD) Regularization. 2) Human Preference Alignment, where we\ndesign Preference Optimization (PO) Regularization to eliminate factuality and\nfaithfulness hallucinations. We also extract emotional clues as a prompt for\nenriching fine-grained information under KD-Regularization. Experiments\ndemonstrate that AlignCap presents stronger performance to other\nstate-of-the-art methods on Zero-shot SEC task.", "published": "2024-10-24 20:05:31", "link": "http://arxiv.org/abs/2410.19134v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Visual Text Matters: Improving Text-KVQA with Visual Text Entity\n  Knowledge-aware Large Multimodal Assistant", "abstract": "We revisit knowledge-aware text-based visual question answering, also known\nas Text-KVQA, in the light of modern advancements in large multimodal models\n(LMMs), and make the following contributions: (i) We propose VisTEL - a\nprincipled approach to perform visual text entity linking. The proposed VisTEL\nmodule harnesses a state-of-the-art visual text recognition engine and the\npower of a large multimodal model to jointly reason using textual and visual\ncontext obtained using surrounding cues in the image to link the visual text\nentity to the correct knowledge base entity. (ii) We present KaLMA - a\nknowledge-aware large multimodal assistant that augments an LMM with knowledge\nassociated with visual text entity in the image to arrive at an accurate\nanswer. Further, we provide a comprehensive experimental analysis and\ncomparison of our approach with traditional visual question answering,\npre-large multimodal models, and large multimodal models, as well as prior\ntop-performing approaches. Averaging over three splits of Text-KVQA, our\nproposed approach surpasses the previous best approach by a substantial 23.3%\non an absolute scale and establishes a new state of the art. We make our\nimplementation publicly available.", "published": "2024-10-24 20:25:38", "link": "http://arxiv.org/abs/2410.19144v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Test of Time: Predicting the Sustainable Success of Online\n  Collaboration in Wikipedia", "abstract": "The Internet has significantly expanded the potential for global\ncollaboration, allowing millions of users to contribute to collective projects\nlike Wikipedia. While prior work has assessed the success of online\ncollaborations, most approaches are time-agnostic, evaluating success without\nconsidering its longevity. Research on the factors that ensure the long-term\npreservation of high-quality standards in online collaboration is scarce. In\nthis study, we address this gap. We propose a novel metric, `Sustainable\nSuccess,' which measures the ability of collaborative efforts to maintain their\nquality over time. Using Wikipedia as a case study, we introduce the\nSustainPedia dataset, which compiles data from over 40K Wikipedia articles,\nincluding each article's sustainable success label and more than 300\nexplanatory features such as edit history, user experience, and team\ncomposition. Using this dataset, we develop machine learning models to predict\nthe sustainable success of Wikipedia articles. Our best-performing model\nachieves a high AU-ROC score of 0.88 on average. Our analysis reveals important\ninsights. For example, we find that the longer an article takes to be\nrecognized as high-quality, the more likely it is to maintain that status over\ntime (i.e., be sustainable). Additionally, user experience emerged as the most\ncritical predictor of sustainability. Our analysis provides insights into\nbroader collective actions beyond Wikipedia (e.g., online activism,\ncrowdsourced open-source software), where the same social dynamics that drive\nsuccess on Wikipedia might play a role. We make all data and code used for this\nstudy publicly available for further research.", "published": "2024-10-24 20:42:53", "link": "http://arxiv.org/abs/2410.19150v2", "categories": ["cs.CY", "cs.CL", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Lived Experience Not Found: LLMs Struggle to Align with Experts on\n  Addressing Adverse Drug Reactions from Psychiatric Medication Use", "abstract": "Adverse Drug Reactions (ADRs) from psychiatric medications are the leading\ncause of hospitalizations among mental health patients. With healthcare systems\nand online communities facing limitations in resolving ADR-related issues,\nLarge Language Models (LLMs) have the potential to fill this gap. Despite the\nincreasing capabilities of LLMs, past research has not explored their\ncapabilities in detecting ADRs related to psychiatric medications or in\nproviding effective harm reduction strategies. To address this, we introduce\nthe Psych-ADR benchmark and the Adverse Drug Reaction Response Assessment\n(ADRA) framework to systematically evaluate LLM performance in detecting ADR\nexpressions and delivering expert-aligned mitigation strategies. Our analyses\nshow that LLMs struggle with understanding the nuances of ADRs and\ndifferentiating between types of ADRs. While LLMs align with experts in terms\nof expressed emotions and tone of the text, their responses are more complex,\nharder to read, and only 70.86% aligned with expert strategies. Furthermore,\nthey provide less actionable advice by a margin of 12.32% on average. Our work\nprovides a comprehensive benchmark and evaluation framework for assessing LLMs\nin strategy-driven tasks within high-risk domains.", "published": "2024-10-24 20:49:22", "link": "http://arxiv.org/abs/2410.19155v3", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Adversarial Attacks on Large Language Models Using Regularized\n  Relaxation", "abstract": "As powerful Large Language Models (LLMs) are now widely used for numerous\npractical applications, their safety is of critical importance. While alignment\ntechniques have significantly improved overall safety, LLMs remain vulnerable\nto carefully crafted adversarial inputs. Consequently, adversarial attack\nmethods are extensively used to study and understand these vulnerabilities.\nHowever, current attack methods face significant limitations. Those relying on\noptimizing discrete tokens suffer from limited efficiency, while continuous\noptimization techniques fail to generate valid tokens from the model's\nvocabulary, rendering them impractical for real-world applications. In this\npaper, we propose a novel technique for adversarial attacks that overcomes\nthese limitations by leveraging regularized gradients with continuous\noptimization methods. Our approach is two orders of magnitude faster than the\nstate-of-the-art greedy coordinate gradient-based method, significantly\nimproving the attack success rate on aligned language models. Moreover, it\ngenerates valid tokens, addressing a fundamental limitation of existing\ncontinuous optimization methods. We demonstrate the effectiveness of our attack\non five state-of-the-art LLMs using four datasets.", "published": "2024-10-24 21:01:45", "link": "http://arxiv.org/abs/2410.19160v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "I.2.7"], "primary_category": "cs.LG"}
{"title": "MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark", "abstract": "The ability to comprehend audio--which includes speech, non-speech sounds,\nand music--is crucial for AI agents to interact effectively with the world. We\npresent MMAU, a novel benchmark designed to evaluate multimodal audio\nunderstanding models on tasks requiring expert-level knowledge and complex\nreasoning. MMAU comprises 10k carefully curated audio clips paired with\nhuman-annotated natural language questions and answers spanning speech,\nenvironmental sounds, and music. It includes information extraction and\nreasoning questions, requiring models to demonstrate 27 distinct skills across\nunique and challenging tasks. Unlike existing benchmarks, MMAU emphasizes\nadvanced perception and reasoning with domain-specific knowledge, challenging\nmodels to tackle tasks akin to those faced by experts. We assess 18 open-source\nand proprietary (Large) Audio-Language Models, demonstrating the significant\nchallenges posed by MMAU. Notably, even the most advanced Gemini Pro v1.5\nachieves only 52.97% accuracy, and the state-of-the-art open-source Qwen2-Audio\nachieves only 52.50%, highlighting considerable room for improvement. We\nbelieve MMAU will drive the audio and multimodal research community to develop\nmore advanced audio understanding models capable of solving complex audio\ntasks.", "published": "2024-10-24 21:20:10", "link": "http://arxiv.org/abs/2410.19168v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "No Argument Left Behind: Overlapping Chunks for Faster Processing of\n  Arbitrarily Long Legal Texts", "abstract": "In a context where the Brazilian judiciary system, the largest in the world,\nfaces a crisis due to the slow processing of millions of cases, it becomes\nimperative to develop efficient methods for analyzing legal texts. We introduce\nuBERT, a hybrid model that combines Transformer and Recurrent Neural Network\narchitectures to effectively handle long legal texts. Our approach processes\nthe full text regardless of its length while maintaining reasonable\ncomputational overhead. Our experiments demonstrate that uBERT achieves\nsuperior performance compared to BERT+LSTM when overlapping input is used and\nis significantly faster than ULMFiT for processing long legal documents.", "published": "2024-10-24 22:33:30", "link": "http://arxiv.org/abs/2410.19184v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Making Social Platforms Accessible: Emotion-Aware Speech Generation with\n  Integrated Text Analysis", "abstract": "Recent studies have outlined the accessibility challenges faced by blind or\nvisually impaired, and less-literate people, in interacting with social\nnetworks, in-spite of facilitating technologies such as monotone text-to-speech\n(TTS) screen readers and audio narration of visual elements such as emojis.\nEmotional speech generation traditionally relies on human input of the expected\nemotion together with the text to synthesise, with additional challenges around\ndata simplification (causing information loss) and duration inaccuracy, leading\nto lack of expressive emotional rendering. In real-life communications, the\nduration of phonemes can vary since the same sentence might be spoken in a\nvariety of ways depending on the speakers' emotional states or accents\n(referred to as the one-to-many problem of text to speech generation). As a\nresult, an advanced voice synthesis system is required to account for this\nunpredictability. We propose an end-to-end context-aware Text-to-Speech (TTS)\nsynthesis system that derives the conveyed emotion from text input and\nsynthesises audio that focuses on emotions and speaker features for natural and\nexpressive speech, integrating advanced natural language processing (NLP) and\nspeech synthesis techniques for real-time applications. Our system also\nshowcases competitive inference time performance when benchmarked against the\nstate-of-the-art TTS models, making it suitable for real-time accessibility\napplications.", "published": "2024-10-24 23:18:02", "link": "http://arxiv.org/abs/2410.19199v1", "categories": ["cs.SI", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.SI"}
{"title": "Parameter-Efficient Fine-Tuning in Large Models: A Survey of\n  Methodologies", "abstract": "The large models, as predicted by scaling raw forecasts, have made\ngroundbreaking progress in many fields, particularly in natural language\ngeneration tasks, where they have approached or even surpassed human levels.\nHowever, the unprecedented scale of their parameters brings significant\ncomputational and storage costs. These large models require substantial\ncomputational resources and GPU memory to operate. When adapting large models\nto specific downstream tasks, their massive parameter scale poses a significant\nchallenge in fine-tuning on hardware platforms with limited computational power\nand GPU memory. To address this issue, Parameter-Efficient Fine-Tuning (PEFT)\noffers a practical solution by efficiently adjusting the parameters of large\npre-trained models to suit various downstream tasks. Specifically, PEFT adjusts\nthe parameters of pre-trained large models to adapt to specific tasks or\ndomains, minimizing the introduction of additional parameters and the\ncomputational resources required. This review mainly introduces the preliminary\nknowledge of PEFT, the core ideas and principles of various PEFT algorithms,\nthe applications of PEFT, and potential future research directions. By reading\nthis review, we believe that interested parties can quickly grasp the PEFT\nmethodology, thereby accelerating its development and innovation.", "published": "2024-10-24 13:58:59", "link": "http://arxiv.org/abs/2410.19878v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "STTATTS: Unified Speech-To-Text And Text-To-Speech Model", "abstract": "Speech recognition and speech synthesis models are typically trained\nseparately, each with its own set of learning objectives, training data, and\nmodel parameters, resulting in two distinct large networks. We propose a\nparameter-efficient approach to learning ASR and TTS jointly via a multi-task\nlearning objective and shared parameters. Our evaluation demonstrates that the\nperformance of our multi-task model is comparable to that of individually\ntrained models while significantly saving computational and memory costs\n($\\sim$50\\% reduction in the total number of parameters required for the two\ntasks combined). We experiment with English as a resource-rich language, and\nArabic as a relatively low-resource language due to shortage of TTS data. Our\nmodels are trained with publicly available data, and both the training code and\nmodel checkpoints are openly available for further research.", "published": "2024-10-24 10:04:24", "link": "http://arxiv.org/abs/2410.18607v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Unbounded: A Generative Infinite Game of Character Life Simulation", "abstract": "We introduce the concept of a generative infinite game, a video game that\ntranscends the traditional boundaries of finite, hard-coded systems by using\ngenerative models. Inspired by James P. Carse's distinction between finite and\ninfinite games, we leverage recent advances in generative AI to create\nUnbounded: a game of character life simulation that is fully encapsulated in\ngenerative models. Specifically, Unbounded draws inspiration from sandbox life\nsimulations and allows you to interact with your autonomous virtual character\nin a virtual world by feeding, playing with and guiding it - with open-ended\nmechanics generated by an LLM, some of which can be emergent. In order to\ndevelop Unbounded, we propose technical innovations in both the LLM and visual\ngeneration domains. Specifically, we present: (1) a specialized, distilled\nlarge language model (LLM) that dynamically generates game mechanics,\nnarratives, and character interactions in real-time, and (2) a new dynamic\nregional image prompt Adapter (IP-Adapter) for vision models that ensures\nconsistent yet flexible visual generation of a character across multiple\nenvironments. We evaluate our system through both qualitative and quantitative\nanalysis, showing significant improvements in character life simulation, user\ninstruction following, narrative coherence, and visual consistency for both\ncharacters and the environments compared to traditional related approaches.", "published": "2024-10-24 17:59:31", "link": "http://arxiv.org/abs/2410.18975v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.GR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "CAMEL-Bench: A Comprehensive Arabic LMM Benchmark", "abstract": "Recent years have witnessed a significant interest in developing large\nmultimodal models (LMMs) capable of performing various visual reasoning and\nunderstanding tasks. This has led to the introduction of multiple LMM\nbenchmarks to evaluate LMMs on different tasks. However, most existing LMM\nevaluation benchmarks are predominantly English-centric. In this work, we\ndevelop a comprehensive LMM evaluation benchmark for the Arabic language to\nrepresent a large population of over 400 million speakers. The proposed\nbenchmark, named CAMEL-Bench, comprises eight diverse domains and 38\nsub-domains including, multi-image understanding, complex visual perception,\nhandwritten document understanding, video understanding, medical imaging, plant\ndiseases, and remote sensing-based land use understanding to evaluate broad\nscenario generalizability. Our CAMEL-Bench comprises around 29,036 questions\nthat are filtered from a larger pool of samples, where the quality is manually\nverified by native speakers to ensure reliable model assessment. We conduct\nevaluations of both closed-source, including GPT-4 series, and open-source\nLMMs. Our analysis reveals the need for substantial improvement, especially\namong the best open-source models, with even the closed-source GPT-4o achieving\nan overall score of 62%. Our benchmark and evaluation scripts are open-sourced.", "published": "2024-10-24 17:59:38", "link": "http://arxiv.org/abs/2410.18976v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Enriching GNNs with Text Contextual Representations for Detecting\n  Disinformation Campaigns on Social Media", "abstract": "Disinformation on social media poses both societal and technical challenges,\nrequiring robust detection systems. While previous studies have integrated\ntextual information into propagation networks, they have yet to fully leverage\nthe advancements in Transformer-based language models for high-quality\ncontextual text representations. This work addresses this gap by incorporating\nTransformer-based textual features into Graph Neural Networks (GNNs) for fake\nnews detection. We demonstrate that contextual text representations enhance GNN\nperformance, achieving 33.8% relative improvement in Macro F1 over models\nwithout textual features and 9.3% over static text representations. We further\ninvestigate the impact of different feature sources and the effects of noisy\ndata augmentation. We expect our methodology to open avenues for further\nresearch, and we made code publicly available.", "published": "2024-10-24 22:57:17", "link": "http://arxiv.org/abs/2410.19193v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Survey on Speech Large Language Models", "abstract": "Large Language Models (LLMs) exhibit strong contextual understanding and\nremarkable multi-task performance. Therefore, researchers have been seeking to\nintegrate LLMs in the broad sense of Spoken Language Understanding (SLU) field.\nDifferent from the traditional method of cascading LLMs to process text\ngenerated by Automatic Speech Recognition(ASR), new efforts have focused on\ndesigning architectures centered around Audio Feature Extraction - Multimodal\nInformation Fusion - LLM Inference(Speech LLMs). This approach enables richer\naudio feature extraction while simultaneously facilitating end-to-end fusion of\naudio and text modalities, thereby achieving deeper understanding and reasoning\nfrom audio data. This paper elucidates the development of Speech LLMs, offering\nan in-depth analysis of system architectures and training strategies. Through\nextensive research and a series of targeted experiments, the paper assesses\nSpeech LLMs' advancements in Rich Audio Transcription and its potential for\nCross-task Integration within the SLU field. Additionally, it indicates key\nchallenges uncovered through experimentation, such as the Dormancy of LLMs\nunder certain conditions. The paper further delves into the training strategies\nfor Speech LLMs, proposing potential solutions based on these findings, and\noffering valuable insights and references for future research in this domain,\nas well as LLM applications in multimodal contexts.", "published": "2024-10-24 16:59:28", "link": "http://arxiv.org/abs/2410.18908v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Contextual Biasing to Improve Domain-specific Custom Vocabulary Audio\n  Transcription without Explicit Fine-Tuning of Whisper Model", "abstract": "OpenAI's Whisper Automated Speech Recognition model excels in generalizing\nacross diverse datasets and domains. However, this broad adaptability can lead\nto diminished performance in tasks requiring recognition of specific\nvocabularies. Addressing this challenge typically involves fine-tuning the\nmodel, which demands extensive labeled audio data that is often difficult to\nacquire and unavailable for specific domains. In this study, we propose a\nmethod to enhance transcription accuracy without explicit fine-tuning or\naltering model parameters, using a relatively small training dataset. Our\nmethod leverages contextual biasing, to direct Whisper model's output towards a\nspecific vocabulary by integrating a neural-symbolic prefix tree structure to\nguide the model's transcription output. To validate our approach, we conducted\nexperiments using a validation dataset comprising maritime data collected\nwithin a simulated training environment. A comparison between the original\nWhisper models of varying parameter sizes and our biased model revealed a\nnotable reduction in transcription word error rate and enhanced performance of\ndownstream applications. Our findings suggest that this methodology holds\npromise for improving speech-to-text translation performance in domains\ncharacterized by limited vocabularies.", "published": "2024-10-24 01:58:11", "link": "http://arxiv.org/abs/2410.18363v1", "categories": ["cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Gibberish is All You Need for Membership Inference Detection in\n  Contrastive Language-Audio Pretraining", "abstract": "Audio can disclose PII, particularly when combined with related text data.\nTherefore, it is essential to develop tools to detect privacy leakage in\nContrastive Language-Audio Pretraining(CLAP). Existing MIAs need audio as\ninput, risking exposure of voiceprint and requiring costly shadow models. We\nfirst propose PRMID, a membership inference detector based probability ranking\ngiven by CLAP, which does not require training shadow models but still requires\nboth audio and text of the individual as input. To address these limitations,\nwe then propose USMID, a textual unimodal speaker-level membership inference\ndetector, querying the target model using only text data. We randomly generate\ntextual gibberish that are clearly not in training dataset. Then we extract\nfeature vectors from these texts using the CLAP model and train a set of\nanomaly detectors on them. During inference, the feature vector of each test\ntext is input into the anomaly detector to determine if the speaker is in the\ntraining set (anomalous) or not (normal). If available, USMID can further\nenhance detection by integrating real audio of the tested speaker. Extensive\nexperiments on various CLAP model architectures and datasets demonstrate that\nUSMID outperforms baseline methods using only text data.", "published": "2024-10-24 02:26:57", "link": "http://arxiv.org/abs/2410.18371v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A contrastive-learning approach for auditory attention detection", "abstract": "Carrying conversations in multi-sound environments is one of the more\nchallenging tasks, since the sounds overlap across time and frequency making it\ndifficult to understand a single sound source. One proposed approach to help\nisolate an attended speech source is through decoding the electroencephalogram\n(EEG) and identifying the attended audio source using statistical or machine\nlearning techniques. However, the limited amount of data in comparison to other\nmachine learning problems and the distributional shift between different EEG\nrecordings emphasizes the need for a self supervised approach that works with\nlimited data to achieve a more robust solution. In this paper, we propose a\nmethod based on self supervised learning to minimize the difference between the\nlatent representations of an attended speech signal and the corresponding EEG\nsignal. This network is further finetuned for the auditory attention\nclassification task. We compare our results with previously published methods\nand achieve state-of-the-art performance on the validation set.", "published": "2024-10-24 03:13:53", "link": "http://arxiv.org/abs/2410.18395v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Wavetable Synthesis Using CVAE for Timbre Control Based on Semantic\n  Label", "abstract": "Synthesizers are essential in modern music production. However, their complex\ntimbre parameters, often filled with technical terms, require expertise. This\nresearch introduces a method of timbre control in wavetable synthesis that is\nintuitive and sensible and utilizes semantic labels. Using a conditional\nvariational autoencoder (CVAE), users can select a wavetable and define the\ntimbre with labels such as bright, warm, and rich. The CVAE model, featuring\nconvolutional and upsampling layers, effectively captures the wavetable\nnuances, ensuring real-time performance owing to their processing in the time\ndomain. Experiments demonstrate that this approach allows for real-time,\neffective control of the timbre of the wavetable using semantic inputs and aims\nfor intuitive timbre control through data-based semantic control.", "published": "2024-10-24 10:37:54", "link": "http://arxiv.org/abs/2410.18628v1", "categories": ["cs.SD", "cs.AI", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Beyond Correlation: Evaluating Multimedia Quality Models with the\n  Constrained Concordance Index", "abstract": "This study investigates the evaluation of multimedia quality models, focusing\non the inherent uncertainties in subjective Mean Opinion Score (MOS) ratings\ndue to factors like rater inconsistency and bias. Traditional statistical\nmeasures such as Pearson's Correlation Coefficient (PCC), Spearman's Rank\nCorrelation Coefficient (SRCC), and Kendall's Tau (KTAU) often fail to account\nfor these uncertainties, leading to inaccuracies in model performance\nassessment. We introduce the Constrained Concordance Index (CCI), a novel\nmetric designed to overcome the limitations of existing metrics by considering\nthe statistical significance of MOS differences and excluding comparisons where\nMOS confidence intervals overlap. Through comprehensive experiments across\nvarious domains including speech and image quality assessment, we demonstrate\nthat CCI provides a more robust and accurate evaluation of instrumental quality\nmodels, especially in scenarios of low sample sizes, rater group variability,\nand restriction of range. Our findings suggest that incorporating rater\nsubjectivity and focusing on statistically significant pairs can significantly\nenhance the evaluation framework for multimedia quality prediction models. This\nwork not only sheds light on the overlooked aspects of subjective rating\nuncertainties but also proposes a methodological advancement for more reliable\nand accurate quality model evaluation.", "published": "2024-10-24 14:07:21", "link": "http://arxiv.org/abs/2411.05794v1", "categories": ["cs.MM", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.MM"}
