{"title": "Words Matter: Reducing Stigma in Online Conversations about Substance\n  Use with Large Language Models", "abstract": "Stigma is a barrier to treatment for individuals struggling with substance\nuse disorders (SUD), which leads to significantly lower treatment engagement\nrates. With only 7% of those affected receiving any form of help, societal\nstigma not only discourages individuals with SUD from seeking help but isolates\nthem, hindering their recovery journey and perpetuating a cycle of shame and\nself-doubt. This study investigates how stigma manifests on social media,\nparticularly Reddit, where anonymity can exacerbate discriminatory behaviors.\nWe analyzed over 1.2 million posts, identifying 3,207 that exhibited\nstigmatizing language towards people who use substances (PWUS). Using Informed\nand Stylized LLMs, we develop a model for de-stigmatization of these\nexpressions into empathetic language, resulting in 1,649 reformed phrase pairs.\nOur paper contributes to the field by proposing a computational framework for\nanalyzing stigma and destigmatizing online content, and delving into the\nlinguistic features that propagate stigma towards PWUS. Our work not only\nenhances understanding of stigma's manifestations online but also provides\npractical tools for fostering a more supportive digital environment for those\naffected by SUD. Code and data will be made publicly available upon acceptance.", "published": "2024-08-15 01:00:28", "link": "http://arxiv.org/abs/2408.07873v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Instruct Large Language Models to Generate Scientific Literature Survey\n  Step by Step", "abstract": "Abstract. Automatically generating scientific literature surveys is a\nvaluable task that can significantly enhance research efficiency. However, the\ndiverse and complex nature of information within a literature survey poses\nsubstantial challenges for generative models. In this paper, we design a series\nof prompts to systematically leverage large language models (LLMs), enabling\nthe creation of comprehensive literature surveys through a step-by-step\napproach. Specifically, we design prompts to guide LLMs to sequentially\ngenerate the title, abstract, hierarchical headings, and the main content of\nthe literature survey. We argue that this design enables the generation of the\nheadings from a high-level perspective. During the content generation process,\nthis design effectively harnesses relevant information while minimizing costs\nby restricting the length of both input and output content in LLM queries. Our\nimplementation with Qwen-long achieved third place in the NLPCC 2024 Scientific\nLiterature Survey Generation evaluation task, with an overall score only 0.03%\nlower than the second-place team. Additionally, our soft heading recall is\n95.84%, the second best among the submissions. Thanks to the efficient prompt\ndesign and the low cost of the Qwen-long API, our method reduces the expense\nfor generating each literature survey to 0.1 RMB, enhancing the practical value\nof our method.", "published": "2024-08-15 02:07:11", "link": "http://arxiv.org/abs/2408.07884v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Fine-Tuning Efficiency of Human-Inspired Learning Strategies\n  in Medical Question Answering", "abstract": "Fine-tuning Large Language Models (LLMs) incurs considerable training costs,\ndriving the need for data-efficient training with optimised data ordering.\nHuman-inspired strategies offer a solution by organising data based on human\nlearning practices. This study evaluates the fine-tuning efficiency of five\nhuman-inspired strategies across four language models, three datasets, and both\nhuman- and LLM-labelled data in the context of medical question answering.\nThese strategies achieve the best accuracy gain of 1.81% and an average gain of\n1.02% across datasets, with interleaved strategies delivering the best average\nresults. However, the best strategy varies across model-dataset combinations,\nlimiting the generalisability of the effects of any single strategy.\nAdditionally, LLM-defined question difficulty outperforms human-defined labels\nin curriculum-based learning, showing the potential of model-generated data as\na cost-effective alternative for optimising fine-tuning.", "published": "2024-08-15 02:22:48", "link": "http://arxiv.org/abs/2408.07888v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GERestaurant: A German Dataset of Annotated Restaurant Reviews for\n  Aspect-Based Sentiment Analysis", "abstract": "We present GERestaurant, a novel dataset consisting of 3,078 German language\nrestaurant reviews manually annotated for Aspect-Based Sentiment Analysis\n(ABSA). All reviews were collected from Tripadvisor, covering a diverse\nselection of restaurants, including regional and international cuisine with\nvarious culinary styles. The annotations encompass both implicit and explicit\naspects, including all aspect terms, their corresponding aspect categories, and\nthe sentiments expressed towards them. Furthermore, we provide baseline scores\nfor the four ABSA tasks Aspect Category Detection, Aspect Category Sentiment\nAnalysis, End-to-End ABSA and Target Aspect Sentiment Detection as a reference\npoint for future advances. The dataset fills a gap in German language resources\nand facilitates exploration of ABSA in the restaurant domain.", "published": "2024-08-15 06:08:53", "link": "http://arxiv.org/abs/2408.07955v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Lung Cancer Patient Prognosis with Large Language Models", "abstract": "Prognosis prediction is crucial for determining optimal treatment plans for\nlung cancer patients. Traditionally, such predictions relied on models\ndeveloped from retrospective patient data. Recently, large language models\n(LLMs) have gained attention for their ability to process and generate text\nbased on extensive learned knowledge. In this study, we evaluate the potential\nof GPT-4o mini and GPT-3.5 in predicting the prognosis of lung cancer patients.\nWe collected two prognosis datasets, i.e., survival and post-operative\ncomplication datasets, and designed multiple tasks to assess the models'\nperformance comprehensively. Logistic regression models were also developed as\nbaselines for comparison. The experimental results demonstrate that LLMs can\nachieve competitive, and in some tasks superior, performance in lung cancer\nprognosis prediction compared to data-driven logistic regression models despite\nnot using additional patient data. These findings suggest that LLMs can be\neffective tools for prognosis prediction in lung cancer, particularly when\npatient data is limited or unavailable.", "published": "2024-08-15 06:36:27", "link": "http://arxiv.org/abs/2408.07971v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FuseChat: Knowledge Fusion of Chat Models", "abstract": "While training large language models (LLMs) from scratch can indeed lead to\nmodels with distinct capabilities and strengths, it incurs substantial costs\nand may lead to redundancy in competencies. Knowledge fusion aims to integrate\nexisting LLMs of diverse architectures and capabilities into a more potent LLM\nthrough lightweight continual training, thereby reducing the need for costly\nLLM development. In this work, we propose a new framework for the knowledge\nfusion of chat LLMs through two main stages, resulting in FuseChat. Firstly, we\nconduct pairwise knowledge fusion on source chat LLMs of varying structures and\nscales to create multiple target LLMs with identical structure and size via\nlightweight fine-tuning. During this process, a statistics-based token\nalignment approach is introduced as the cornerstone for fusing LLMs with\ndifferent structures. Secondly, we merge these target LLMs within the parameter\nspace, where we propose a novel method for determining the merging coefficients\nbased on the magnitude of parameter updates before and after fine-tuning. We\nimplement and validate FuseChat using six prominent chat LLMs with diverse\narchitectures and scales, including OpenChat-3.5-7B, Starling-LM-7B-alpha,\nNH2-SOLAR-10.7B, InternLM2-Chat-20B, Mixtral-8x7B-Instruct, and\nQwen-1.5-Chat-72B. Experimental results on two instruction-following\nbenchmarks, AlpacaEval 2.0 and MT-Bench, demonstrate the superiority of\nFuseChat-7B over baselines of various sizes. Our model is even comparable to\nthe larger Mixtral-8x7B-Instruct and approaches GPT-3.5-Turbo-1106 on MT-Bench.\nOur code, model weights, and data are public at\n\\url{https://github.com/fanqiwan/FuseAI}.", "published": "2024-08-15 07:37:24", "link": "http://arxiv.org/abs/2408.07990v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Web-Crawled Data for High-Quality Fine-Tuning", "abstract": "Most large language models are fine-tuned using either expensive\nhuman-annotated data or GPT-4 generated data which cannot guarantee performance\nin certain domains. We argue that although the web-crawled data often has\nformatting errors causing semantic inaccuracies, it can still serve as a\nvaluable source for high-quality supervised fine-tuning in specific domains\nwithout relying on advanced models like GPT-4. To this end, we create a paired\ntraining dataset automatically by aligning web-crawled data with a smaller set\nof high-quality data. By training a language model on this dataset, we can\nconvert web data with irregular formats into high-quality ones. Our experiments\nshow that training with the model-transformed data yields better results,\nsurpassing training with only high-quality data by an average score of 9.4% in\nChinese math problems. Additionally, our 7B model outperforms several\nopen-source models larger than 32B and surpasses well-known closed-source\nmodels such as GPT-3.5, highlighting the efficacy of our approach.", "published": "2024-08-15 08:12:52", "link": "http://arxiv.org/abs/2408.08003v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative\n  Self-Enhancement Paradigm", "abstract": "Large Language Models (LLMs) have achieved significant advancements, however,\nthe common learning paradigm treats LLMs as passive information repositories,\nneglecting their potential for active learning and alignment. Some approaches\ntrain LLMs using their own generated synthetic data, exploring the possibility\nof active alignment. However, there is still a huge gap between these one-time\nalignment methods and the continuous automatic alignment of humans. In this\npaper, we introduce \\textbf{I-SHEEP}, an \\textbf{I}terative\n\\textbf{S}elf-En\\textbf{H}anc\\textbf{E}m\\textbf{E}nt \\textbf{P}aradigm.This\nhuman-like paradigm enables LLMs to \\textbf{continuously self-align from\nscratch with nothing}. Compared to the one-time alignment method Dromedary\n\\cite{sun2023principledriven}, which refers to the first iteration in this\npaper, I-SHEEP can significantly enhance capacities on both Qwen and Llama\nmodels. I-SHEEP achieves a maximum relative improvement of 78.2\\% in the Alpaca\nEval, 24.0\\% in the MT Bench, and an absolute increase of 8.88\\% in the IFEval\naccuracy over subsequent iterations in Qwen-1.5 72B model. Additionally,\nI-SHEEP surpasses the base model in various standard benchmark generation\ntasks, achieving an average improvement of 24.77\\% in code generation tasks,\n12.04\\% in TrivialQA, and 20.29\\% in SQuAD. We also provide new insights based\non the experiment results. Our codes, datasets, and models are available at\n\\textbf{https://anonymous.4open.science/r/I-SHEEP}.", "published": "2024-08-15 10:44:38", "link": "http://arxiv.org/abs/2408.08072v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for\n  Multi-turn NLU", "abstract": "Although Large Language Models(LLMs) can generate coherent and contextually\nrelevant text, they often struggle to recognise the intent behind the human\nuser's query. Natural Language Understanding (NLU) models, however, interpret\nthe purpose and key information of user's input to enable responsive\ninteractions. Existing NLU models generally map individual utterances to a\ndual-level semantic frame, involving sentence-level intent and word-level slot\nlabels. However, real-life conversations primarily consist of multi-turn\nconversations, involving the interpretation of complex and extended dialogues.\nResearchers encounter challenges addressing all facets of multi-turn dialogue\nconversations using a unified single NLU model. This paper introduces a novel\napproach, MIDAS, leveraging a multi-level intent, domain, and slot knowledge\ndistillation for multi-turn NLU. To achieve this, we construct distinct\nteachers for varying levels of conversation knowledge, namely, sentence-level\nintent detection, word-level slot filling, and conversation-level domain\nclassification. These teachers are then fine-tuned to acquire specific\nknowledge of their designated levels. A multi-teacher loss is proposed to\nfacilitate the combination of these multi-level teachers, guiding a student\nmodel in multi-turn dialogue tasks. The experimental results demonstrate the\nefficacy of our model in improving the overall multi-turn conversation\nunderstanding, showcasing the potential for advancements in NLU models through\nthe incorporation of multi-level dialogue knowledge distillation techniques.", "published": "2024-08-15 13:28:18", "link": "http://arxiv.org/abs/2408.08144v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KOALA: Enhancing Speculative Decoding for LLM via Multi-Layer Draft\n  Heads with Adversarial Learning", "abstract": "Large Language Models (LLMs) exhibit high inference latency due to their\nautoregressive decoding nature. While the draft head in speculative decoding\nmitigates this issue, its full potential remains unexplored. In this paper, we\nintroduce KOALA (K-layer Optimized Adversarial Learning Architecture), an\northogonal approach to the draft head. By transforming the conventional\nsingle-layer draft head into a multi-layer architecture and incorporating\nadversarial learning into the traditional supervised training, KOALA\nsignificantly improves the accuracy of the draft head in predicting subsequent\ntokens, thus more closely mirroring the functionality of LLMs. Although this\nimprovement comes at the cost of slightly increased drafting overhead, KOALA\nsubstantially unlocks the draft head's potential, greatly enhancing speculative\ndecoding. We conducted comprehensive evaluations of KOALA, including both\nautoregressive and non-autoregressive draft heads across various tasks,\ndemonstrating a latency speedup ratio improvement of 0.24x-0.41x, which is\n10.57%-14.09% faster than the original draft heads.", "published": "2024-08-15 13:29:48", "link": "http://arxiv.org/abs/2408.08146v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "mhGPT: A Lightweight Generative Pre-Trained Transformer for Mental\n  Health Text Analysis", "abstract": "This paper introduces mhGPT, a lightweight generative pre-trained transformer\ntrained on mental health-related social media and PubMed articles. Fine-tuned\nfor specific mental health tasks, mhGPT was evaluated under limited hardware\nconstraints and compared with state-of-the-art models like MentaLLaMA and\nGemma. Despite having only 1.98 billion parameters and using just 5% of the\ndataset, mhGPT outperformed larger models and matched the performance of models\ntrained on significantly more data. The key contributions include integrating\ndiverse mental health data, creating a custom tokenizer, and optimizing a\nsmaller architecture for low-resource settings. This research could advance\nAI-driven mental health care, especially in areas with limited computing power.", "published": "2024-08-15 17:01:57", "link": "http://arxiv.org/abs/2408.08261v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The ShareLM Collection and Plugin: Contributing Human-Model Chats for\n  the Benefit of the Community", "abstract": "Human-model conversations provide a window into users' real-world scenarios,\nbehavior, and needs, and thus are a valuable resource for model development and\nresearch. While for-profit companies collect user data through the APIs of\ntheir models, using it internally to improve their own models, the open source\nand research community lags behind.\n  We introduce the ShareLM collection, a unified set of human conversations\nwith large language models, and its accompanying plugin, a Web extension for\nvoluntarily contributing user-model conversations. Where few platforms share\ntheir chats, the ShareLM plugin adds this functionality, thus, allowing users\nto share conversations from most platforms. The plugin allows the user to rate\ntheir conversations, both at the conversation and the response levels, and\ndelete conversations they prefer to keep private before they ever leave the\nuser's local storage. We release the plugin conversations as part of the\nShareLM collection, and call for more community effort in the field of open\nhuman-model data.\n  The code, plugin, and data are available.", "published": "2024-08-15 17:46:54", "link": "http://arxiv.org/abs/2408.08291v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ScalingFilter: Assessing Data Quality through Inverse Utilization of\n  Scaling Laws", "abstract": "High-quality data is crucial for the pre-training performance of large\nlanguage models. Unfortunately, existing quality filtering methods rely on a\nknown high-quality dataset as reference, which can introduce potential bias and\ncompromise diversity. In this paper, we propose ScalingFilter, a novel approach\nthat evaluates text quality based on the perplexity difference between two\nlanguage models trained on the same data, thereby eliminating the influence of\nthe reference dataset in the filtering process. An theoretical analysis shows\nthat ScalingFilter is equivalent to an inverse utilization of scaling laws.\nThrough training models with 1.3B parameters on the same data source processed\nby various quality filters, we find ScalingFilter can improve zero-shot\nperformance of pre-trained models in downstream tasks. To assess the bias\nintroduced by quality filtering, we introduce semantic diversity, a metric of\nutilizing text embedding models for semantic representations. Extensive\nexperiments reveal that semantic diversity is a reliable indicator of dataset\ndiversity, and ScalingFilter achieves an optimal balance between downstream\nperformance and semantic diversity.", "published": "2024-08-15 17:59:30", "link": "http://arxiv.org/abs/2408.08310v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Learning and Key Points Are All You Need for Automated\n  Fact-Checking", "abstract": "Automated fact-checking is an important task because determining the accurate\nstatus of a proposed claim within the vast amount of information available\nonline is a critical challenge. This challenge requires robust evaluation to\nprevent the spread of false information. Modern large language models (LLMs)\nhave demonstrated high capability in performing a diverse range of Natural\nLanguage Processing (NLP) tasks. By utilizing proper prompting strategies,\ntheir versatility due to their understanding of large context sizes and\nzero-shot learning ability enables them to simulate human problem-solving\nintuition and move towards being an alternative to humans for solving problems.\nIn this work, we introduce a straightforward framework based on Zero-Shot\nLearning and Key Points (ZSL-KeP) for automated fact-checking, which despite\nits simplicity, performed well on the AVeriTeC shared task dataset by robustly\nimproving the baseline and achieving 10th place.", "published": "2024-08-15 19:57:42", "link": "http://arxiv.org/abs/2408.08400v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rater Cohesion and Quality from a Vicarious Perspective", "abstract": "Human feedback is essential for building human-centered AI systems across\ndomains where disagreement is prevalent, such as AI safety, content moderation,\nor sentiment analysis. Many disagreements, particularly in politically charged\nsettings, arise because raters have opposing values or beliefs. Vicarious\nannotation is a method for breaking down disagreement by asking raters how they\nthink others would annotate the data. In this paper, we explore the use of\nvicarious annotation with analytical methods for moderating rater disagreement.\nWe employ rater cohesion metrics to study the potential influence of political\naffiliations and demographic backgrounds on raters' perceptions of offense.\nAdditionally, we utilize CrowdTruth's rater quality metrics, which consider the\ndemographics of the raters, to score the raters and their annotations. We study\nhow the rater quality metrics influence the in-group and cross-group rater\ncohesion across the personal and vicarious levels.", "published": "2024-08-15 20:37:36", "link": "http://arxiv.org/abs/2408.08411v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PyMarian: Fast Neural Machine Translation and Evaluation in Python", "abstract": "The deep learning language of choice these days is Python; measured by\nfactors such as available libraries and technical support, it is hard to beat.\nAt the same time, software written in lower-level programming languages like\nC++ retain advantages in speed. We describe a Python interface to Marian NMT, a\nC++-based training and inference toolkit for sequence-to-sequence models,\nfocusing on machine translation. This interface enables models trained with\nMarian to be connected to the rich, wide range of tools available in Python. A\nhighlight of the interface is the ability to compute state-of-the-art COMET\nmetrics from Python but using Marian's inference engine, with a speedup factor\nof up to 7.8$\\times$ the existing implementations. We also briefly spotlight a\nnumber of other integrations, including Jupyter notebooks, connection with\nprebuilt models, and a web app interface provided with the package. PyMarian is\navailable in PyPI via $\\texttt{pip install pymarian}$.", "published": "2024-08-15 01:41:21", "link": "http://arxiv.org/abs/2408.11853v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hermes 3 Technical Report", "abstract": "Instruct (or \"chat\") tuned models have become the primary way in which most\npeople interact with large language models. As opposed to \"base\" or\n\"foundation\" models, instruct-tuned models are optimized to respond to\nimperative statements. We present Hermes 3, a neutrally-aligned generalist\ninstruct and tool use model with strong reasoning and creative abilities. Its\nlargest version, Hermes 3 405B, achieves state of the art performance among\nopen weight models on several public benchmarks.", "published": "2024-08-15 20:17:33", "link": "http://arxiv.org/abs/2408.11857v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing Language Models' Worldview for Fiction Generation", "abstract": "The use of Large Language Models (LLMs) has become ubiquitous, with abundant\napplications in computational creativity. One such application is fictional\nstory generation. Fiction is a narrative that occurs in a story world that is\nslightly different than ours. With LLMs becoming writing partners, we question\nhow suitable they are to generate fiction. This study investigates the ability\nof LLMs to maintain a state of world essential to generate fiction. Through a\nseries of questions to nine LLMs, we find that only two models exhibit\nconsistent worldview, while the rest are self-conflicting. Subsequent analysis\nof stories generated by four models revealed a strikingly uniform narrative\npattern. This uniformity across models further suggests a lack of `state'\nnecessary for fiction. We highlight the limitations of current LLMs in fiction\nwriting and advocate for future research to test and create story worlds for\nLLMs to reside in. All code, dataset, and the generated responses can be found\nin https://github.com/tanny411/llm-reliability-and-consistency-evaluation.", "published": "2024-08-15 03:19:41", "link": "http://arxiv.org/abs/2408.07904v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and\n  Iterative Sub-SQL Refinement for Text-to-SQL", "abstract": "Recent In-Context Learning based methods have achieved remarkable success in\nText-to-SQL task. However, there is still a large gap between the performance\nof these models and human performance on datasets with complex database schema\nand difficult questions, such as BIRD. Besides, existing work has neglected to\nsupervise intermediate steps when solving questions iteratively with question\ndecomposition methods, and the schema linking methods used in these works are\nvery rudimentary. To address these issues, we propose MAG-SQL, a multi-agent\ngenerative approach with soft schema linking and iterative Sub-SQL refinement.\nIn our framework, an entity-based method with tables' summary is used to select\nthe columns in database, and a novel targets-conditions decomposition method is\nintroduced to decompose those complex questions. Additionally, we build a\niterative generating module which includes a Sub-SQL Generator and Sub-SQL\nRefiner, introducing external oversight for each step of generation. Through a\nseries of ablation studies, the effectiveness of each agent in our framework\nhas been demonstrated. When evaluated on the BIRD benchmark with GPT-4, MAG-SQL\nachieves an execution accuracy of 61.08%, compared to the baseline accuracy of\n46.35% for vanilla GPT-4 and the baseline accuracy of 57.56% for MAC-SQL.\nBesides, our approach makes similar progress on Spider. The codes are available\nat https://github.com/LancelotXWX/MAG-SQL.", "published": "2024-08-15 04:57:55", "link": "http://arxiv.org/abs/2408.07930v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented\n  Generation", "abstract": "Despite Retrieval-Augmented Generation (RAG) showing promising capability in\nleveraging external knowledge, a comprehensive evaluation of RAG systems is\nstill challenging due to the modular nature of RAG, evaluation of long-form\nresponses and reliability of measurements. In this paper, we propose a\nfine-grained evaluation framework, RAGChecker, that incorporates a suite of\ndiagnostic metrics for both the retrieval and generation modules. Meta\nevaluation verifies that RAGChecker has significantly better correlations with\nhuman judgments than other evaluation metrics. Using RAGChecker, we evaluate 8\nRAG systems and conduct an in-depth analysis of their performance, revealing\ninsightful patterns and trade-offs in the design choices of RAG architectures.\nThe metrics of RAGChecker can guide researchers and practitioners in developing\nmore effective RAG systems. This work has been open sourced at\nhttps://github.com/amazon-science/RAGChecker.", "published": "2024-08-15 10:20:54", "link": "http://arxiv.org/abs/2408.08067v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AgentCourt: Simulating Court with Adversarial Evolvable Lawyer Agents", "abstract": "In this paper, we present a simulation system called AgentCourt that\nsimulates the entire courtroom process. The judge, plaintiff's lawyer, defense\nlawyer, and other participants are autonomous agents driven by large language\nmodels (LLMs). Our core goal is to enable lawyer agents to learn how to argue a\ncase, as well as improving their overall legal skills, through courtroom\nprocess simulation. To achieve this goal, we propose an adversarial\nevolutionary approach for the lawyer-agent. Since AgentCourt can simulate the\noccurrence and development of court hearings based on a knowledge base and LLM,\nthe lawyer agents can continuously learn and accumulate experience from real\ncourt cases. The simulation experiments show that after two lawyer-agents have\nengaged in a thousand adversarial legal cases in AgentCourt (which can take a\ndecade for real-world lawyers), compared to their pre-evolutionary state, the\nevolved lawyer agents exhibit consistent improvement in their ability to handle\nlegal tasks. To enhance the credibility of our experimental results, we\nenlisted a panel of professional lawyers to evaluate our simulations. The\nevaluation indicates that the evolved lawyer agents exhibit notable\nadvancements in responsiveness, as well as expertise and logical rigor. This\nwork paves the way for advancing LLM-driven agent technology in legal\nscenarios. Code is available at https://github.com/relic-yuexi/AgentCourt.", "published": "2024-08-15 11:33:20", "link": "http://arxiv.org/abs/2408.08089v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Covert Bias: The Severity of Social Views' Unalignment in Language\n  Models Towards Implicit and Explicit Opinion", "abstract": "While various approaches have recently been studied for bias identification,\nlittle is known about how implicit language that does not explicitly convey a\nviewpoint affects bias amplification in large language models. To examine the\nseverity of bias toward a view, we evaluated the performance of two downstream\ntasks where the implicit and explicit knowledge of social groups were used.\nFirst, we present a stress test evaluation by using a biased model in edge\ncases of excessive bias scenarios. Then, we evaluate how LLMs calibrate\nlinguistically in response to both implicit and explicit opinions when they are\naligned with conflicting viewpoints. Our findings reveal a discrepancy in LLM\nperformance in identifying implicit and explicit opinions, with a general\ntendency of bias toward explicit opinions of opposing stances. Moreover, the\nbias-aligned models generate more cautious responses using uncertainty phrases\ncompared to the unaligned (zero-shot) base models. The direct, incautious\nresponses of the unaligned models suggest a need for further refinement of\ndecisiveness by incorporating uncertainty markers to enhance their reliability,\nespecially on socially nuanced topics with high subjectivity.", "published": "2024-08-15 15:23:00", "link": "http://arxiv.org/abs/2408.08212v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Evaluating Text Classification Robustness to Part-of-Speech Adversarial\n  Examples", "abstract": "As machine learning systems become more widely used, especially for safety\ncritical applications, there is a growing need to ensure that these systems\nbehave as intended, even in the face of adversarial examples. Adversarial\nexamples are inputs that are designed to trick the decision making process, and\nare intended to be imperceptible to humans. However, for text-based\nclassification systems, changes to the input, a string of text, are always\nperceptible. Therefore, text-based adversarial examples instead focus on trying\nto preserve semantics. Unfortunately, recent work has shown this goal is often\nnot met. To improve the quality of text-based adversarial examples, we need to\nknow what elements of the input text are worth focusing on. To address this, in\nthis paper, we explore what parts of speech have the highest impact of\ntext-based classifiers. Our experiments highlight a distinct bias in CNN\nalgorithms against certain parts of speech tokens within review datasets. This\nfinding underscores a critical vulnerability in the linguistic processing\ncapabilities of CNNs.", "published": "2024-08-15 18:33:54", "link": "http://arxiv.org/abs/2408.08374v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Level Up Your Tutorials: VLMs for Game Tutorials Quality Assessment", "abstract": "Designing effective game tutorials is crucial for a smooth learning curve for\nnew players, especially in games with many rules and complex core mechanics.\nEvaluating the effectiveness of these tutorials usually requires multiple\niterations with testers who have no prior knowledge of the game. Recent\nVision-Language Models (VLMs) have demonstrated significant capabilities in\nunderstanding and interpreting visual content. VLMs can analyze images, provide\ndetailed insights, and answer questions about their content. They can recognize\nobjects, actions, and contexts in visual data, making them valuable tools for\nvarious applications, including automated game testing. In this work, we\npropose an automated game-testing solution to evaluate the quality of game\ntutorials. Our approach leverages VLMs to analyze frames from video game\ntutorials, answer relevant questions to simulate human perception, and provide\nfeedback. This feedback is compared with expected results to identify confusing\nor problematic scenes and highlight potential errors for developers. In\naddition, we publish complete tutorial videos and annotated frames from\ndifferent game versions used in our tests. This solution reduces the need for\nextensive manual testing, especially by speeding up and simplifying the initial\ndevelopment stages of the tutorial to improve the final game experience.", "published": "2024-08-15 19:46:21", "link": "http://arxiv.org/abs/2408.08396v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "VerilogCoder: Autonomous Verilog Coding Agents with Graph-based Planning\n  and Abstract Syntax Tree (AST)-based Waveform Tracing Tool", "abstract": "Due to the growing complexity of modern Integrated Circuits (ICs), automating\nhardware design can prevent a significant amount of human error from the\nengineering process and result in less errors. Verilog is a popular hardware\ndescription language for designing and modeling digital systems; thus, Verilog\ngeneration is one of the emerging areas of research to facilitate the design\nprocess. In this work, we propose VerilogCoder, a system of multiple Artificial\nIntelligence (AI) agents for Verilog code generation, to autonomously write\nVerilog code and fix syntax and functional errors using collaborative Verilog\ntools (i.e., syntax checker, simulator, and waveform tracer). Firstly, we\npropose a task planner that utilizes a novel Task and Circuit Relation Graph\nretrieval method to construct a holistic plan based on module descriptions. To\ndebug and fix functional errors, we develop a novel and efficient abstract\nsyntax tree (AST)-based waveform tracing tool, which is integrated within the\nautonomous Verilog completion flow. The proposed methodology successfully\ngenerates 94.2% syntactically and functionally correct Verilog code, surpassing\nthe state-of-the-art methods by 33.9% on the VerilogEval-Human v2 benchmark.", "published": "2024-08-15 20:06:06", "link": "http://arxiv.org/abs/2408.08927v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Dynamic Adaptive Optimization for Effective Sentiment Analysis\n  Fine-Tuning on Large Language Models", "abstract": "Sentiment analysis plays a crucial role in various domains, such as business\nintelligence and financial forecasting. Large language models (LLMs) have\nbecome a popular paradigm for sentiment analysis, leveraging multi-task\nlearning to address specific tasks concurrently. However, LLMs with fine-tuning\nfor sentiment analysis often underperforms due to the inherent challenges in\nmanaging diverse task complexities. Moreover, constant-weight approaches in\nmulti-task learning struggle to adapt to variations in data characteristics,\nfurther complicating model effectiveness. To address these issues, we propose a\nnovel multi-task learning framework with a dynamic adaptive optimization (DAO)\nmodule. This module is designed as a plug-and-play component that can be\nseamlessly integrated into existing models, providing an effective and flexible\nsolution for multi-task learning. The key component of the DAO module is\ndynamic adaptive loss, which dynamically adjusts the weights assigned to\ndifferent tasks based on their relative importance and data characteristics\nduring training. Sentiment analyses on a standard and customized financial text\ndataset demonstrate that the proposed framework achieves superior performance.\nSpecifically, this work improves the Mean Squared Error (MSE) and Accuracy\n(ACC) by 15.58% and 1.24% respectively, compared with previous work.", "published": "2024-08-15 19:13:38", "link": "http://arxiv.org/abs/2408.11856v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evolving Text Data Stream Mining", "abstract": "A text stream is an ordered sequence of text documents generated over time. A\nmassive amount of such text data is generated by online social platforms every\nday. Designing an algorithm for such text streams to extract useful information\nis a challenging task due to unique properties of the stream such as infinite\nlength, data sparsity, and evolution. Thereby, learning useful information from\nsuch streaming data under the constraint of limited time and memory has gained\nincreasing attention. During the past decade, although many text stream mining\nalgorithms have proposed, there still exists some potential issues. First,\nhigh-dimensional text data heavily degrades the learning performance until the\nmodel either works on subspace or reduces the global feature space. The second\nissue is to extract semantic text representation of documents and capture\nevolving topics over time. Moreover, the problem of label scarcity exists,\nwhereas existing approaches work on the full availability of labeled data. To\ndeal with these issues, in this thesis, new learning models are proposed for\nclustering and multi-label learning on text streams.", "published": "2024-08-15 15:38:52", "link": "http://arxiv.org/abs/2409.00010v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "DM2RM: Dual-Mode Multimodal Ranking for Target Objects and Receptacles\n  Based on Open-Vocabulary Instructions", "abstract": "In this study, we aim to develop a domestic service robot (DSR) that, guided\nby open-vocabulary instructions, can carry everyday objects to the specified\npieces of furniture. Few existing methods handle mobile manipulation tasks with\nopen-vocabulary instructions in the image retrieval setting, and most do not\nidentify both the target objects and the receptacles. We propose the Dual-Mode\nMultimodal Ranking model (DM2RM), which enables images of both the target\nobjects and receptacles to be retrieved using a single model based on\nmultimodal foundation models. We introduce a switching mechanism that leverages\na mode token and phrase identification via a large language model to switch the\nembedding space based on the prediction target. To evaluate the DM2RM, we\nconstruct a novel dataset including real-world images collected from hundreds\nof building-scale environments and crowd-sourced instructions with referring\nexpressions. The evaluation results show that the proposed DM2RM outperforms\nprevious approaches in terms of standard metrics in image retrieval settings.\nFurthermore, we demonstrate the application of the DM2RM on a standardized\nreal-world DSR platform including fetch-and-carry actions, where it achieves a\ntask success rate of 82% despite the zero-shot transfer setting. Demonstration\nvideos, code, and more materials are available at\nhttps://kkrr10.github.io/dm2rm/.", "published": "2024-08-15 03:34:02", "link": "http://arxiv.org/abs/2408.07910v1", "categories": ["cs.RO", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Polaris: Open-ended Interactive Robotic Manipulation via Syn2Real Visual\n  Grounding and Large Language Models", "abstract": "This paper investigates the task of the open-ended interactive robotic\nmanipulation on table-top scenarios. While recent Large Language Models (LLMs)\nenhance robots' comprehension of user instructions, their lack of visual\ngrounding constrains their ability to physically interact with the environment.\nThis is because the robot needs to locate the target object for manipulation\nwithin the physical workspace. To this end, we introduce an interactive robotic\nmanipulation framework called Polaris, which integrates perception and\ninteraction by utilizing GPT-4 alongside grounded vision models. For precise\nmanipulation, it is essential that such grounded vision models produce detailed\nobject pose for the target object, rather than merely identifying pixels\nbelonging to them in the image. Consequently, we propose a novel\nSynthetic-to-Real (Syn2Real) pose estimation pipeline. This pipeline utilizes\nrendered synthetic data for training and is then transferred to real-world\nmanipulation tasks. The real-world performance demonstrates the efficacy of our\nproposed pipeline and underscores its potential for extension to more general\ncategories. Moreover, real-robot experiments have showcased the impressive\nperformance of our framework in grasping and executing multiple manipulation\ntasks. This indicates its potential to generalize to scenarios beyond the\ntabletop. More information and video results are available here:\nhttps://star-uu-wang.github.io/Polaris/", "published": "2024-08-15 06:40:38", "link": "http://arxiv.org/abs/2408.07975v1", "categories": ["cs.RO", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Coupling without Communication and Drafter-Invariant Speculative\n  Decoding", "abstract": "Suppose Alice has a distribution $P$ and Bob has a distribution $Q$. Alice\nwants to draw a sample $a\\sim P$ and Bob a sample $b \\sim Q$ such that $a = b$\nwith as high of probability as possible. It is well-known that, by sampling\nfrom an optimal coupling between the distributions, Alice and Bob can achieve\n$\\Pr[a = b] = 1 - D_{TV}(P,Q)$, where $D_{TV}(P,Q)$ is the total variation\ndistance between $P$ and $Q$. What if Alice and Bob must solve this same\nproblem \\emph{without communicating at all?} Perhaps surprisingly, with access\nto public randomness, they can still achieve $\\Pr[a = b] \\geq \\frac{1 -\nD_{TV}(P,Q)}{1 + D_{TV}(P,Q)} \\geq 1-2D_{TV}(P,Q)$ using a simple protocol\nbased on the Weighted MinHash algorithm. This bound was shown to be optimal in\nthe worst-case by [Bavarian et al., 2020]. In this work, we revisit the\ncommunication-free coupling problem. We provide a simpler proof of the\noptimality result from [Bavarian et al., 2020]. We show that, while the\nworst-case success probability of Weighted MinHash cannot be improved, an\nequally simple protocol based on Gumbel sampling offers a Pareto improvement:\nfor every pair of distributions $P, Q$, Gumbel sampling achieves an equal or\nhigher value of $\\Pr[a = b]$ than Weighted MinHash. Importantly, this\nimprovement translates to practice. We demonstrate an application of\ncommunication-free coupling to \\emph{speculative decoding}, a recent method for\naccelerating autoregressive large language models [Leviathan, Kalman, Matias,\nICML 2023]. We show that communication-free protocols can be used to contruct\n\\emph{\\CSD{}} schemes, which have the desirable property that their output is\nfixed given a fixed random seed, regardless of what drafter is used for\nspeculation. In experiments on a language generation task, Gumbel sampling\noutperforms Weighted MinHash. Code is available at\nhttps://github.com/majid-daliri/DISD.", "published": "2024-08-15 06:52:24", "link": "http://arxiv.org/abs/2408.07978v3", "categories": ["cs.DS", "cs.CL", "cs.LG"], "primary_category": "cs.DS"}
{"title": "ArabLegalEval: A Multitask Benchmark for Assessing Arabic Legal\n  Knowledge in Large Language Models", "abstract": "The rapid advancements in Large Language Models (LLMs) have led to\nsignificant improvements in various natural language processing tasks. However,\nthe evaluation of LLMs' legal knowledge, particularly in non-English languages\nsuch as Arabic, remains under-explored. To address this gap, we introduce\nArabLegalEval, a multitask benchmark dataset for assessing the Arabic legal\nknowledge of LLMs. Inspired by the MMLU and LegalBench datasets, ArabLegalEval\nconsists of multiple tasks sourced from Saudi legal documents and synthesized\nquestions. In this work, we aim to analyze the capabilities required to solve\nlegal problems in Arabic and benchmark the performance of state-of-the-art\nLLMs. We explore the impact of in-context learning and investigate various\nevaluation methods. Additionally, we explore workflows for generating questions\nwith automatic validation to enhance the dataset's quality. We benchmark\nmultilingual and Arabic-centric LLMs, such as GPT-4 and Jais, respectively. We\nalso share our methodology for creating the dataset and validation, which can\nbe generalized to other domains. We hope to accelerate AI research in the\nArabic Legal domain by releasing the ArabLegalEval dataset and code:\nhttps://github.com/Thiqah/ArabLegalEval", "published": "2024-08-15 07:09:51", "link": "http://arxiv.org/abs/2408.07983v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Enhancing Large Language Model-based Speech Recognition by\n  Contextualization for Rare and Ambiguous Words", "abstract": "We develop a large language model (LLM) based automatic speech recognition\n(ASR) system that can be contextualized by providing keywords as prior\ninformation in text prompts. We adopt decoder-only architecture and use our\nin-house LLM, PLaMo-100B, pre-trained from scratch using datasets dominated by\nJapanese and English texts as the decoder. We adopt a pre-trained Whisper\nencoder as an audio encoder, and the audio embeddings from the audio encoder\nare projected to the text embedding space by an adapter layer and concatenated\nwith text embeddings converted from text prompts to form inputs to the decoder.\nBy providing keywords as prior information in the text prompts, we can\ncontextualize our LLM-based ASR system without modifying the model architecture\nto transcribe ambiguous words in the input audio accurately. Experimental\nresults demonstrate that providing keywords to the decoder can significantly\nimprove the recognition performance of rare and ambiguous words.", "published": "2024-08-15 08:50:58", "link": "http://arxiv.org/abs/2408.08027v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Text2BIM: Generating Building Models Using a Large Language Model-based\n  Multi-Agent Framework", "abstract": "The conventional BIM authoring process typically requires designers to master\ncomplex and tedious modeling commands in order to materialize their design\nintentions within BIM authoring tools. This additional cognitive burden\ncomplicates the design process and hinders the adoption of BIM and model-based\ndesign in the AEC (Architecture, Engineering, and Construction) industry. To\nfacilitate the expression of design intentions more intuitively, we propose\nText2BIM, an LLM-based multi-agent framework that can generate 3D building\nmodels from natural language instructions. This framework orchestrates multiple\nLLM agents to collaborate and reason, transforming textual user input into\nimperative code that invokes the BIM authoring tool's APIs, thereby generating\neditable BIM models with internal layouts, external envelopes, and semantic\ninformation directly in the software. Furthermore, a rule-based model checker\nis introduced into the agentic workflow, utilizing predefined domain knowledge\nto guide the LLM agents in resolving issues within the generated models and\niteratively improving model quality. Extensive experiments were conducted to\ncompare and analyze the performance of three different LLMs under the proposed\nframework. The evaluation results demonstrate that our approach can effectively\ngenerate high-quality, structurally rational building models that are aligned\nwith the abstract concepts specified by user input. Finally, an interactive\nsoftware prototype was developed to integrate the framework into the BIM\nauthoring software Vectorworks, showcasing the potential of modeling by\nchatting.", "published": "2024-08-15 09:48:45", "link": "http://arxiv.org/abs/2408.08054v1", "categories": ["cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.AI"}
{"title": "Extracting Sentence Embeddings from Pretrained Transformer Models", "abstract": "Pre-trained transformer models shine in many natural language processing\ntasks and therefore are expected to bear the representation of the input\nsentence or text meaning. These sentence-level embeddings are also important in\nretrieval-augmented generation. But do commonly used plain averaging or prompt\ntemplates sufficiently capture and represent the underlying meaning? After\nproviding a comprehensive review of existing sentence embedding extraction and\nrefinement methods, we thoroughly test different combinations and our original\nextensions of the most promising ones on pretrained models. Namely, given 110 M\nparameters, BERT's hidden representations from multiple layers, and many\ntokens, we try diverse ways to extract optimal sentence embeddings. We test\nvarious token aggregation and representation post-processing techniques. We\nalso test multiple ways of using a general Wikitext dataset to complement\nBERT's sentence embeddings. All methods are tested on eight Semantic Textual\nSimilarity (STS), six short text clustering, and twelve classification tasks.\nWe also evaluate our representation-shaping techniques on other static models,\nincluding random token representations. Proposed representation extraction\nmethods improve the performance on STS and clustering tasks for all models\nconsidered. Very high improvements for static token-based models, especially\nrandom embeddings for STS tasks, almost reach the performance of BERT-derived\nrepresentations. Our work shows that the representation-shaping techniques\nsignificantly improve sentence embeddings extracted from BERT-based and simple\nbaseline models.", "published": "2024-08-15 10:54:55", "link": "http://arxiv.org/abs/2408.08073v2", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML", "68T07, 68T50, 68T05", "I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "P/D-Serve: Serving Disaggregated Large Language Model at Scale", "abstract": "Serving disaggregated large language models (LLMs) over tens of thousands of\nxPU devices (GPUs or NPUs) with reliable performance faces multiple challenges.\n1) Ignoring the diversity (various prefixes and tidal requests), treating all\nthe prompts in a mixed pool is inadequate. To facilitate the similarity per\nscenario and minimize the inner mismatch on P/D (prefill and decoding)\nprocessing, fine-grained organization is required, dynamically adjusting P/D\nratios for better performance. 2) Due to inaccurate estimation on workload\n(queue status or maintained connections), the global scheduler easily incurs\nunnecessary timeouts in prefill. 3) Block-fixed device-to-device (D2D) KVCache\ntransfer over cluster-level RDMA (remote direct memory access) fails to achieve\ndesired D2D utilization as expected. To overcome previous problems, this paper\nproposes an end-to-end system P/D-Serve, complying with the paradigm of MLOps\n(machine learning operations), which models end-to-end (E2E) P/D performance\nand enables: 1) fine-grained P/D organization, mapping the service with RoCE\n(RDMA over converged ethernet) as needed, to facilitate similar processing and\ndynamic adjustments on P/D ratios; 2) on-demand forwarding upon rejections for\nidle prefill, decoupling the scheduler from regular inaccurate reports and\nlocal queues, to avoid timeouts in prefill; and 3) efficient KVCache transfer\nvia optimized D2D access. P/D-Serve is implemented upon Ascend and MindSpore,\nhas been deployed over tens of thousands of NPUs for more than eight months in\ncommercial use, and further achieves 60\\%, 42\\% and 46\\% improvements on E2E\nthroughput, time-to-first-token (TTFT) SLO (service level objective) and D2D\ntransfer time. As the E2E system with optimizations, P/D-Serve achieves 6.7x\nincrease on throughput, compared with aggregated LLMs.", "published": "2024-08-15 13:32:25", "link": "http://arxiv.org/abs/2408.08147v1", "categories": ["cs.DC", "cs.CL", "cs.LG"], "primary_category": "cs.DC"}
{"title": "DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for\n  Reinforcement Learning and Monte-Carlo Tree Search", "abstract": "We introduce DeepSeek-Prover-V1.5, an open-source language model designed for\ntheorem proving in Lean 4, which enhances DeepSeek-Prover-V1 by optimizing both\ntraining and inference processes. Pre-trained on DeepSeekMath-Base with\nspecialization in formal mathematical languages, the model undergoes supervised\nfine-tuning using an enhanced formal theorem proving dataset derived from\nDeepSeek-Prover-V1. Further refinement is achieved through reinforcement\nlearning from proof assistant feedback (RLPAF). Beyond the single-pass\nwhole-proof generation approach of DeepSeek-Prover-V1, we propose RMaxTS, a\nvariant of Monte-Carlo tree search that employs an intrinsic-reward-driven\nexploration strategy to generate diverse proof paths. DeepSeek-Prover-V1.5\ndemonstrates significant improvements over DeepSeek-Prover-V1, achieving new\nstate-of-the-art results on the test set of the high school level miniF2F\nbenchmark ($63.5\\%$) and the undergraduate level ProofNet benchmark ($25.3\\%$).", "published": "2024-08-15 13:40:03", "link": "http://arxiv.org/abs/2408.08152v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Benchmarking the Capabilities of Large Language Models in Transportation\n  System Engineering: Accuracy, Consistency, and Reasoning Behaviors", "abstract": "In this paper, we explore the capabilities of state-of-the-art large language\nmodels (LLMs) such as GPT-4, GPT-4o, Claude 3.5 Sonnet, Claude 3 Opus, Gemini\n1.5 Pro, Llama 3, and Llama 3.1 in solving some selected undergraduate-level\ntransportation engineering problems. We introduce TransportBench, a benchmark\ndataset that includes a sample of transportation engineering problems on a wide\nrange of subjects in the context of planning, design, management, and control\nof transportation systems. This dataset is used by human experts to evaluate\nthe capabilities of various commercial and open-sourced LLMs, especially their\naccuracy, consistency, and reasoning behaviors, in solving transportation\nengineering problems. Our comprehensive analysis uncovers the unique strengths\nand limitations of each LLM, e.g. our analysis shows the impressive accuracy\nand some unexpected inconsistent behaviors of Claude 3.5 Sonnet in solving\nTransportBench problems. Our study marks a thrilling first step toward\nharnessing artificial general intelligence for complex transportation\nchallenges.", "published": "2024-08-15 17:55:45", "link": "http://arxiv.org/abs/2408.08302v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Can Large Language Models Understand Symbolic Graphics Programs?", "abstract": "Against the backdrop of enthusiasm for large language models (LLMs), there is\nan urgent need to scientifically assess their capabilities and shortcomings.\nThis is nontrivial in part because it is difficult to find tasks which the\nmodels have not encountered during training. Utilizing symbolic graphics\nprograms, we propose a domain well-suited to test multiple spatial-semantic\nreasoning skills of LLMs. Popular in computer graphics, these programs\nprocedurally generate visual data. While LLMs exhibit impressive skills in\ngeneral program synthesis and analysis, symbolic graphics programs offer a new\nlayer of evaluation: they allow us to test an LLM's ability to answer\ndifferent-grained semantic-level questions of the images or 3D geometries\nwithout a vision encoder. To semantically understand the symbolic programs,\nLLMs would need to possess the ability to \"imagine\" and reason how the\ncorresponding graphics content would look with only the symbolic description.\nWe use this task to evaluate LLMs by creating a large benchmark for the\nsemantic visual understanding of symbolic graphics programs, built procedurally\nwith minimal human effort. Particular emphasis is placed on transformations of\nimages that leave the image level semantics invariant while introducing\nsignificant changes to the underlying program. We evaluate commercial and\nopen-source LLMs on our benchmark to assess their ability to reason about\nvisual output of programs, finding that LLMs considered stronger at reasoning\ngenerally perform better. Lastly, we introduce a novel method to improve this\nability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned\nwith pre-collected instruction data on symbolic graphics programs.\nInterestingly, we find that SIT not only improves LLM's understanding on\nsymbolic programs, but it also improves general reasoning ability on various\nother benchmarks.", "published": "2024-08-15 17:59:57", "link": "http://arxiv.org/abs/2408.08313v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Plan with Code: Comparing approaches for robust NL to DSL generation", "abstract": "Planning in code is considered a more reliable approach for many\norchestration tasks. This is because code is more tractable than steps\ngenerated via Natural Language and make it easy to support more complex\nsequences by abstracting deterministic logic into functions. It also allows\nspotting issues with incorrect function names with the help of parsing checks\nthat can be run on code. Progress in Code Generation methodologies, however,\nremains limited to general-purpose languages like C, C++, and Python. LLMs\ncontinue to face challenges with custom function names in Domain Specific\nLanguages or DSLs, leading to higher hallucination rates and syntax errors.\nThis is more common for custom function names, that are typically part of the\nplan. Moreover, keeping LLMs up-to-date with newer function names is an issue.\nThis poses a challenge for scenarios like task planning over a large number of\nAPIs, since the plan is represented as a DSL having custom API names. In this\npaper, we focus on workflow automation in RPA (Robotic Process Automation)\ndomain as a special case of task planning. We present optimizations for using\nRetrieval Augmented Generation (or RAG) with LLMs for DSL generation along with\nan ablation study comparing these strategies with a fine-tuned model. Our\nresults showed that the fine-tuned model scored the best on code similarity\nmetric. However, with our optimizations, RAG approach is able to match the\nquality for in-domain API names in the test set. Additionally, it offers\nsignificant advantage for out-of-domain or unseen API names, outperforming\nFine-Tuned model on similarity metric by 7 pts.", "published": "2024-08-15 04:29:33", "link": "http://arxiv.org/abs/2408.08335v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "I.2.2; I.2.7"], "primary_category": "cs.SE"}
{"title": "Towards Realistic Synthetic User-Generated Content: A Scaffolding\n  Approach to Generating Online Discussions", "abstract": "The emergence of synthetic data represents a pivotal shift in modern machine\nlearning, offering a solution to satisfy the need for large volumes of data in\ndomains where real data is scarce, highly private, or difficult to obtain. We\ninvestigate the feasibility of creating realistic, large-scale synthetic\ndatasets of user-generated content, noting that such content is increasingly\nprevalent and a source of frequently sought information. Large language models\n(LLMs) offer a starting point for generating synthetic social media discussion\nthreads, due to their ability to produce diverse responses that typify online\ninteractions. However, as we demonstrate, straightforward application of LLMs\nyields limited success in capturing the complex structure of online\ndiscussions, and standard prompting mechanisms lack sufficient control. We\ntherefore propose a multi-step generation process, predicated on the idea of\ncreating compact representations of discussion threads, referred to as\nscaffolds. Our framework is generic yet adaptable to the unique characteristics\nof specific social media platforms. We demonstrate its feasibility using data\nfrom two distinct online discussion platforms. To address the fundamental\nchallenge of ensuring the representativeness and realism of synthetic data, we\npropose a portfolio of evaluation measures to compare various instantiations of\nour framework.", "published": "2024-08-15 18:43:50", "link": "http://arxiv.org/abs/2408.08379v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question\n  Answering", "abstract": "In knowledge-intensive tasks such as open-domain question answering (OpenQA),\nLarge Language Models (LLMs) often struggle to generate factual answers relying\nsolely on their internal (parametric) knowledge. To address this limitation,\nRetrieval-Augmented Generation (RAG) systems enhance LLMs by retrieving\nrelevant information from external sources, thereby positioning the retriever\nas a pivotal component. Although dense retrieval demonstrates state-of-the-art\nperformance, its training poses challenges due to the scarcity of ground-truth\nevidence, largely attributed to the high costs of human annotation. In this\npaper, we propose W-RAG by utilizing the ranking capabilities of LLMs to create\nweakly labeled data for training dense retrievers. Specifically, we rerank the\ntop-$K$ passages retrieved via BM25 by assessing the probability that LLMs will\ngenerate the correct answer based on the question and each passage. The\nhighest-ranking passages are then used as positive training examples for dense\nretrieval. Our comprehensive experiments across four publicly available OpenQA\ndatasets demonstrate that our approach enhances both retrieval and OpenQA\nperformance compared to baseline models.", "published": "2024-08-15 22:34:44", "link": "http://arxiv.org/abs/2408.08444v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "JPEG-LM: LLMs as Image Generators with Canonical Codec Representations", "abstract": "Recent work in image and video generation has been adopting the\nautoregressive LLM architecture due to its generality and potentially easy\nintegration into multi-modal systems. The crux of applying autoregressive\ntraining in language generation to visual generation is discretization --\nrepresenting continuous data like images and videos as discrete tokens. Common\nmethods of discretizing images and videos include modeling raw pixel values,\nwhich are prohibitively lengthy, or vector quantization, which requires\nconvoluted pre-hoc training. In this work, we propose to directly model images\nand videos as compressed files saved on computers via canonical codecs (e.g.,\nJPEG, AVC/H.264). Using the default Llama architecture without any\nvision-specific modifications, we pretrain JPEG-LM from scratch to generate\nimages (and AVC-LM to generate videos as a proof of concept), by directly\noutputting compressed file bytes in JPEG and AVC formats. Evaluation of image\ngeneration shows that this simple and straightforward approach is more\neffective than pixel-based modeling and sophisticated vector quantization\nbaselines (on which our method yields a 31% reduction in FID). Our analysis\nshows that JPEG-LM has an especial advantage over vector quantization models in\ngenerating long-tail visual elements. Overall, we show that using canonical\ncodec representations can help lower the barriers between language generation\nand visual generation, facilitating future research on multi-modal\nlanguage/image/video LLMs.", "published": "2024-08-15 23:57:02", "link": "http://arxiv.org/abs/2408.08459v2", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Graph Retrieval-Augmented Generation: A Survey", "abstract": "Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable\nsuccess in addressing the challenges of Large Language Models (LLMs) without\nnecessitating retraining. By referencing an external knowledge base, RAG\nrefines LLM outputs, effectively mitigating issues such as ``hallucination'',\nlack of domain-specific knowledge, and outdated information. However, the\ncomplex structure of relationships among different entities in databases\npresents challenges for RAG systems. In response, GraphRAG leverages structural\ninformation across entities to enable more precise and comprehensive retrieval,\ncapturing relational knowledge and facilitating more accurate, context-aware\nresponses. Given the novelty and potential of GraphRAG, a systematic review of\ncurrent technologies is imperative. This paper provides the first comprehensive\noverview of GraphRAG methodologies. We formalize the GraphRAG workflow,\nencompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced\nGeneration. We then outline the core technologies and training methods at each\nstage. Additionally, we examine downstream tasks, application domains,\nevaluation methodologies, and industrial use cases of GraphRAG. Finally, we\nexplore future research directions to inspire further inquiries and advance\nprogress in the field. In order to track recent progress in this field, we set\nup a repository at \\url{https://github.com/pengboci/GraphRAG-Survey}.", "published": "2024-08-15 12:20:24", "link": "http://arxiv.org/abs/2408.08921v2", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Prefix Guidance: A Steering Wheel for Large Language Models to Defend\n  Against Jailbreak Attacks", "abstract": "In recent years, the rapid development of large language models (LLMs) has\nachieved remarkable performance across various tasks. However, research\nindicates that LLMs are vulnerable to jailbreak attacks, where adversaries can\ninduce the generation of harmful content through meticulously crafted prompts.\nThis vulnerability poses significant challenges to the secure use and promotion\nof LLMs. Existing defense methods offer protection from different perspectives\nbut often suffer from insufficient effectiveness or a significant impact on the\nmodel's capabilities. In this paper, we propose a plug-and-play and\neasy-to-deploy jailbreak defense framework, namely Prefix Guidance (PG), which\nguides the model to identify harmful prompts by directly setting the first few\ntokens of the model's output. This approach combines the model's inherent\nsecurity capabilities with an external classifier to defend against jailbreak\nattacks. We demonstrate the effectiveness of PG across three models and five\nattack methods. Compared to baselines, our approach is generally more effective\non average. Additionally, results on the Just-Eval benchmark further confirm\nPG's superiority to preserve the model's performance. our code is available at\nhttps://github.com/weiyezhimeng/Prefix-Guidance.", "published": "2024-08-15 14:51:32", "link": "http://arxiv.org/abs/2408.08924v2", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Retail-GPT: leveraging Retrieval Augmented Generation (RAG) for building\n  E-commerce Chat Assistants", "abstract": "This work presents Retail-GPT, an open-source RAG-based chatbot designed to\nenhance user engagement in retail e-commerce by guiding users through product\nrecommendations and assisting with cart operations. The system is\ncross-platform and adaptable to various e-commerce domains, avoiding reliance\non specific chat applications or commercial activities. Retail-GPT engages in\nhuman-like conversations, interprets user demands, checks product availability,\nand manages cart operations, aiming to serve as a virtual sales agent and test\nthe viability of such assistants across different retail businesses.", "published": "2024-08-15 16:53:05", "link": "http://arxiv.org/abs/2408.08925v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.IR"}
{"title": "When Raw Data Prevails: Are Large Language Model Embeddings Effective in\n  Numerical Data Representation for Medical Machine Learning Applications?", "abstract": "The introduction of Large Language Models (LLMs) has advanced data\nrepresentation and analysis, bringing significant progress in their use for\nmedical questions and answering. Despite these advancements, integrating\ntabular data, especially numerical data pivotal in clinical contexts, into LLM\nparadigms has not been thoroughly explored. In this study, we examine the\neffectiveness of vector representations from last hidden states of LLMs for\nmedical diagnostics and prognostics using electronic health record (EHR) data.\nWe compare the performance of these embeddings with that of raw numerical EHR\ndata when used as feature inputs to traditional machine learning (ML)\nalgorithms that excel at tabular data learning, such as eXtreme Gradient\nBoosting. We focus on instruction-tuned LLMs in a zero-shot setting to\nrepresent abnormal physiological data and evaluating their utilities as feature\nextractors to enhance ML classifiers for predicting diagnoses, length of stay,\nand mortality. Furthermore, we examine prompt engineering techniques on\nzero-shot and few-shot LLM embeddings to measure their impact comprehensively.\nAlthough findings suggest the raw data features still prevails in medical ML\ntasks, zero-shot LLM embeddings demonstrate competitive results, suggesting a\npromising avenue for future research in medical applications.", "published": "2024-08-15 03:56:40", "link": "http://arxiv.org/abs/2408.11854v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FactorLLM: Factorizing Knowledge via Mixture of Experts for Large\n  Language Models", "abstract": "Recent research has demonstrated that Feed-Forward Networks (FFNs) in Large\nLanguage Models (LLMs) play a pivotal role in storing diverse linguistic and\nfactual knowledge. Conventional methods frequently face challenges due to\nknowledge confusion stemming from their monolithic and redundant architectures,\nwhich calls for more efficient solutions with minimal computational overhead,\nparticularly for LLMs. In this paper, we explore the FFN computation paradigm\nin LLMs and introduce FactorLLM, a novel approach that decomposes well-trained\ndense FFNs into sparse sub-networks without requiring any further\nmodifications, while maintaining the same level of performance. Furthermore, we\nembed a router from the Mixture-of-Experts (MoE), combined with our devised\nPrior-Approximate (PA) loss term that facilitates the dynamic activation of\nexperts and knowledge adaptation, thereby accelerating computational processes\nand enhancing performance using minimal training data and fine-tuning steps.\nFactorLLM thus enables efficient knowledge factorization and activates select\ngroups of experts specifically tailored to designated tasks, emulating the\ninteractive functional segmentation of the human brain. Extensive experiments\nacross various benchmarks demonstrate the effectiveness of our proposed\nFactorLLM which achieves comparable performance to the source model securing up\nto 85% model performance while obtaining over a 30% increase in inference\nspeed. Code: https://github.com/zhenwuweihe/FactorLLM.", "published": "2024-08-15 16:45:16", "link": "http://arxiv.org/abs/2408.11855v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Coarse-to-fine Alignment Makes Better Speech-image Retrieval", "abstract": "In this paper, we propose a novel framework for speech-image retrieval. We\nutilize speech-image contrastive (SIC) learning tasks to align speech and image\nrepresentations at a coarse level and speech-image matching (SIM) learning\ntasks to further refine the fine-grained cross-modal alignment. SIC and SIM\nlearning tasks are jointly trained in a unified manner. To optimize the\nlearning process, we utilize an embedding queue that facilitates efficient\nsampling of high-quality and diverse negative representations during SIC\nlearning. Additionally, it enhances the learning of SIM tasks by effectively\nmining hard negatives based on contrastive similarities calculated in SIC\ntasks. To further optimize learning under noisy supervision, we incorporate\nmomentum distillation into the training process. Experimental results show that\nour framework outperforms the state-of-the-art method by more than 4% in R@1 on\ntwo benchmark datasets for the speech-image retrieval tasks. Moreover, as\nobserved in zero-shot experiments, our framework demonstrates excellent\ngeneralization capabilities.", "published": "2024-08-15 02:21:49", "link": "http://arxiv.org/abs/2408.13119v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Cross-Modal Denoising: A Novel Training Paradigm for Enhancing\n  Speech-Image Retrieval", "abstract": "The success of speech-image retrieval relies on establishing an effective\nalignment between speech and image. Existing methods often model cross-modal\ninteraction through simple cosine similarity of the global feature of each\nmodality, which fall short in capturing fine-grained details within modalities.\nTo address this issue, we introduce an effective framework and a novel learning\ntask named cross-modal denoising (CMD) to enhance cross-modal interaction to\nachieve finer-level cross-modal alignment. Specifically, CMD is a denoising\ntask designed to reconstruct semantic features from noisy features within one\nmodality by interacting features from another modality. Notably, CMD operates\nexclusively during model training and can be removed during inference without\nadding extra inference time. The experimental results demonstrate that our\nframework outperforms the state-of-the-art method by 2.0% in mean R@1 on the\nFlickr8k dataset and by 1.7% in mean R@1 on the SpokenCOCO dataset for the\nspeech-image retrieval tasks, respectively. These experimental results validate\nthe efficiency and effectiveness of our framework.", "published": "2024-08-15 02:42:05", "link": "http://arxiv.org/abs/2408.13705v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Inductive Learning of Logical Theories with LLMs: An Expressivity-Graded\n  Analysis", "abstract": "This work presents a novel systematic methodology to analyse the capabilities\nand limitations of Large Language Models (LLMs) with feedback from a formal\ninference engine, on logic theory induction. The analysis is complexity-graded\nw.r.t. rule dependency structure, allowing quantification of specific inference\nchallenges on LLM performance. Integrating LLMs with formal methods is a\npromising frontier in the Natural Language Processing field, as an important\navenue for improving model inference control and explainability. In particular,\ninductive learning over complex sets of facts and rules, poses unique\nchallenges for current autoregressive models, as they lack explicit symbolic\ngrounding. While they can be complemented by formal systems, the properties\ndelivered by LLMs regarding inductive learning, are not well understood and\nquantified. Empirical results indicate that the largest LLMs can achieve\ncompetitive results against a SOTA Inductive Logic Programming (ILP) system\nbaseline, but also that tracking long predicate relationship chains is a more\ndifficult obstacle than theory complexity for LLMs.", "published": "2024-08-15 16:41:00", "link": "http://arxiv.org/abs/2408.16779v2", "categories": ["cs.CL", "cs.AI", "cs.LO", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks\n  of Language Models", "abstract": "Language Model (LM) agents for cybersecurity that are capable of autonomously\nidentifying vulnerabilities and executing exploits have potential to cause\nreal-world impact. Policymakers, model providers, and researchers in the AI and\ncybersecurity communities are interested in quantifying the capabilities of\nsuch agents to help mitigate cyberrisk and investigate opportunities for\npenetration testing. Toward that end, we introduce Cybench, a framework for\nspecifying cybersecurity tasks and evaluating agents on those tasks. We include\n40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF\ncompetitions, chosen to be recent, meaningful, and spanning a wide range of\ndifficulties. Each task includes its own description, starter files, and is\ninitialized in an environment where an agent can execute commands and observe\noutputs. Since many tasks are beyond the capabilities of existing LM agents, we\nintroduce subtasks for each task, which break down a task into intermediary\nsteps for a more detailed evaluation. To evaluate agent capabilities, we\nconstruct a cybersecurity agent and evaluate 8 models: GPT-4o, OpenAI\no1-preview, Claude 3 Opus, Claude 3.5 Sonnet, Mixtral 8x22b Instruct, Gemini\n1.5 Pro, Llama 3 70B Chat, and Llama 3.1 405B Instruct. For the top performing\nmodels (GPT-4o and Claude 3.5 Sonnet), we further investigate performance\nacross 4 agent scaffolds (structed bash, action-only, pseudoterminal, and web\nsearch). Without subtask guidance, agents leveraging Claude 3.5 Sonnet, GPT-4o,\nOpenAI o1-preview, and Claude 3 Opus successfully solved complete tasks that\ntook human teams up to 11 minutes to solve. In comparison, the most difficult\ntask took human teams 24 hours and 54 minutes to solve. All code and data are\npublicly available at https://cybench.github.io.", "published": "2024-08-15 17:23:10", "link": "http://arxiv.org/abs/2408.08926v3", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Advancing Multi-grained Alignment for Contrastive Language-Audio\n  Pre-training", "abstract": "Recent advances have been witnessed in audio-language joint learning, such as\nCLAP, that shows much success in multi-modal understanding tasks. These models\nusually aggregate uni-modal local representations, namely frame or word\nfeatures, into global ones, on which the contrastive loss is employed to reach\ncoarse-grained cross-modal alignment. However, frame-level correspondence with\ntexts may be ignored, making it ill-posed on explainability and fine-grained\nchallenges which may also undermine performances on coarse-grained tasks. In\nthis work, we aim to improve both coarse- and fine-grained audio-language\nalignment in large-scale contrastive pre-training. To unify the granularity and\nlatent distribution of two modalities, a shared codebook is adopted to\nrepresent multi-modal global features with common bases, and each codeword is\nregularized to encode modality-shared semantics, bridging the gap between frame\nand word features. Based on it, a locality-aware block is involved to purify\nlocal patterns, and a hard-negative guided loss is devised to boost alignment.\nExperiments on eleven zero-shot coarse- and fine-grained tasks suggest that our\nmodel not only surpasses the baseline CLAP significantly but also yields\nsuperior or competitive results compared to current SOTA works.", "published": "2024-08-15 04:09:19", "link": "http://arxiv.org/abs/2408.07919v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "The evolution of inharmonicity and noisiness in contemporary popular\n  music", "abstract": "Much of Western classical music relies on instruments based on acoustic\nresonance, which produce harmonic or quasi-harmonic sounds. In contrast, since\nthe mid-twentieth century, popular music has increasingly been produced in\nrecording studios, where it is not bound by the constraints of harmonic sounds.\nIn this study, we use modified MPEG-7 features to explore and characterise the\nevolution of noise and inharmonicity in popular music since 1961. We place this\nevolution in the context of other broad categories of music, including Western\nclassical piano music, orchestral music, and musique concr\\`ete. We introduce\nnew features that distinguish between inharmonicity caused by noise and that\nresulting from interactions between discrete partials. Our analysis reveals\nthat the history of popular music since 1961 can be divided into three phases.\nFrom 1961 to 1972, inharmonicity in popular music, initially only slightly\nhigher than in orchestral music, increased significantly. Between 1972 and\n1986, this rise in inharmonicity was accompanied by an increase in noise, but\nsince 1986, both inharmonicity and noise have moderately decreased. In recent\nyears (up to 2020), popular music has remained much more inharmonic than\npopular music from the 1960s or orchestral music involving acoustic resonance\ninstruments. However, it has become less noisy, with noise levels comparable to\nthose of orchestral music. We relate these trends to the evolution of music\nproduction techniques. In particular, the use of multi-tracking may explain the\nhigher inharmonicity in popular music compared to orchestral music. We\nillustrate these trends with analyses of key artists and tracks.", "published": "2024-08-15 12:52:40", "link": "http://arxiv.org/abs/2408.08127v2", "categories": ["cs.SD", "eess.AS", "68T05, 42C40", "I.5.4; H.5.5"], "primary_category": "cs.SD"}
{"title": "Hearing Your Blood Sugar: Non-Invasive Glucose Measurement Through\n  Simple Vocal Signals, Transforming any Speech into a Sensor with Machine\n  Learning", "abstract": "Effective diabetes management relies heavily on the continuous monitoring of\nblood glucose levels, traditionally achieved through invasive and uncomfortable\nmethods. While various non-invasive techniques have been explored, such as\noptical, microwave, and electrochemical approaches, none have effectively\nsupplanted these invasive technologies due to issues related to complexity,\naccuracy, and cost. In this study, we present a transformative and\nstraightforward method that utilizes voice analysis to predict blood glucose\nlevels. Our research investigates the relationship between fluctuations in\nblood glucose and vocal characteristics, highlighting the influence of blood\nvessel dynamics during voice production. By applying advanced machine learning\nalgorithms, we analyzed vocal signal variations and established a significant\ncorrelation with blood glucose levels. We developed a predictive model using\nartificial intelligence, based on voice recordings and corresponding glucose\nmeasurements from participants, utilizing logistic regression and Ridge\nregularization. Our findings indicate that voice analysis may serve as a viable\nnon-invasive alternative for glucose monitoring. This innovative approach not\nonly has the potential to streamline and reduce the costs associated with\ndiabetes management but also aims to enhance the quality of life for\nindividuals living with diabetes by providing a painless and user-friendly\nmethod for monitoring blood sugar levels.", "published": "2024-08-15 12:13:23", "link": "http://arxiv.org/abs/2408.08109v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Accelerating High-Fidelity Waveform Generation via Adversarial Flow\n  Matching Optimization", "abstract": "This paper introduces PeriodWave-Turbo, a high-fidelity and high-efficient\nwaveform generation model via adversarial flow matching optimization. Recently,\nconditional flow matching (CFM) generative models have been successfully\nadopted for waveform generation tasks, leveraging a single vector field\nestimation objective for training. Although these models can generate\nhigh-fidelity waveform signals, they require significantly more ODE steps\ncompared to GAN-based models, which only need a single generation step.\nAdditionally, the generated samples often lack high-frequency information due\nto noisy vector field estimation, which fails to ensure high-frequency\nreproduction. To address this limitation, we enhance pre-trained CFM-based\ngenerative models by incorporating a fixed-step generator modification. We\nutilized reconstruction losses and adversarial feedback to accelerate\nhigh-fidelity waveform generation. Through adversarial flow matching\noptimization, it only requires 1,000 steps of fine-tuning to achieve\nstate-of-the-art performance across various objective metrics. Moreover, we\nsignificantly reduce inference speed from 16 steps to 2 or 4 steps.\nAdditionally, by scaling up the backbone of PeriodWave from 29M to 70M\nparameters for improved generalization, PeriodWave-Turbo achieves unprecedented\nperformance, with a perceptual evaluation of speech quality (PESQ) score of\n4.454 on the LibriTTS dataset. Audio samples, source code and checkpoints will\nbe available at https://github.com/sh-lee-prml/PeriodWave.", "published": "2024-08-15 08:34:00", "link": "http://arxiv.org/abs/2408.08019v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
