{"title": "STIL -- Simultaneous Slot Filling, Translation, Intent Classification,\n  and Language Identification: Initial Results using mBART on MultiATIS++", "abstract": "Slot-filling, Translation, Intent classification, and Language\nidentification, or STIL, is a newly-proposed task for multilingual Natural\nLanguage Understanding (NLU). By performing simultaneous slot filling and\ntranslation into a single output language (English in this case), some portion\nof downstream system components can be monolingual, reducing development and\nmaintenance cost. Results are given using the multilingual BART model (Liu et\nal., 2020) fine-tuned on 7 languages using the MultiATIS++ dataset. When no\ntranslation is performed, mBART's performance is comparable to the current\nstate of the art system (Cross-Lingual BERT by Xu et al. (2020)) for the\nlanguages tested, with better average intent classification accuracy (96.07%\nversus 95.50%) but worse average slot F1 (89.87% versus 90.81%). When\nsimultaneous translation is performed, average intent classification accuracy\ndegrades by only 1.7% relative and average slot F1 degrades by only 1.2%\nrelative.", "published": "2020-10-02 03:09:26", "link": "http://arxiv.org/abs/2010.00760v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enriching Word Embeddings with Temporal and Spatial Information", "abstract": "The meaning of a word is closely linked to sociocultural factors that can\nchange over time and location, resulting in corresponding meaning changes.\nTaking a global view of words and their meanings in a widely used language,\nsuch as English, may require us to capture more refined semantics for use in\ntime-specific or location-aware situations, such as the study of cultural\ntrends or language use. However, popular vector representations for words do\nnot adequately include temporal or spatial information. In this work, we\npresent a model for learning word representation conditioned on time and\nlocation. In addition to capturing meaning changes over time and location, we\nrequire that the resulting word embeddings retain salient semantic and\ngeometric properties. We train our model on time- and location-stamped corpora,\nand show using both quantitative and qualitative evaluations that it can\ncapture semantics across time and locations. We note that our model compares\nfavorably with the state-of-the-art for time-specific embedding, and serves as\na new benchmark for location-specific embeddings.", "published": "2020-10-02 03:15:03", "link": "http://arxiv.org/abs/2010.00761v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Fine-grained Sentiment Classification Exploiting Local Context\n  Embedding", "abstract": "Target-oriented sentiment classification is a fine-grained task of natural\nlanguage processing to analyze the sentiment polarity of the targets. To\nimprove the performance of sentiment classification, many approaches proposed\nvarious attention mechanisms to capture the important context words of a\ntarget. However, previous approaches ignored the significant relatedness of a\ntarget's sentiment and its local context. This paper proposes a local\ncontext-aware network (LCA-Net), equipped with the local context embedding and\nlocal context prediction loss, to strengthen the model by emphasizing the\nsentiment information of the local context. The experimental results on three\ncommon datasets show that local context-aware network performs superior to\nexisting approaches in extracting local context features. Besides, the local\ncontext-aware framework is easy to adapt to many models, with the potential to\nimprove other target-level tasks.", "published": "2020-10-02 03:54:37", "link": "http://arxiv.org/abs/2010.00767v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JAKET: Joint Pre-training of Knowledge Graph and Language Understanding", "abstract": "Knowledge graphs (KGs) contain rich information about world knowledge,\nentities and relations. Thus, they can be great supplements to existing\npre-trained language models. However, it remains a challenge to efficiently\nintegrate information from KG into language modeling. And the understanding of\na knowledge graph requires related context. We propose a novel joint\npre-training framework, JAKET, to model both the knowledge graph and language.\nThe knowledge module and language module provide essential information to\nmutually assist each other: the knowledge module produces embeddings for\nentities in text while the language module generates context-aware initial\nembeddings for entities and relations in the graph. Our design enables the\npre-trained model to easily adapt to unseen knowledge graphs in new domains.\nExperimental results on several knowledge-aware NLP tasks show that our\nproposed framework achieves superior performance by effectively leveraging\nknowledge in language understanding.", "published": "2020-10-02 05:53:36", "link": "http://arxiv.org/abs/2010.00796v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MEGATRON-CNTRL: Controllable Story Generation with External Knowledge\n  Using Large-Scale Language Models", "abstract": "Existing pre-trained large language models have shown unparalleled generative\ncapabilities. However, they are not controllable. In this paper, we propose\nMEGATRON-CNTRL, a novel framework that uses large-scale language models and\nadds control to text generation by incorporating an external knowledge base.\nOur framework consists of a keyword predictor, a knowledge retriever, a\ncontextual knowledge ranker, and a conditional text generator. As we do not\nhave access to ground-truth supervision for the knowledge ranker, we make use\nof weak supervision from sentence embedding. The empirical results show that\nour model generates more fluent, consistent, and coherent stories with less\nrepetition and higher diversity compared to prior work on the ROC story\ndataset. We showcase the controllability of our model by replacing the keywords\nused to generate stories and re-running the generation process. Human\nevaluation results show that 77.5% of these stories are successfully controlled\nby the new keywords. Furthermore, by scaling our model from 124 million to 8.3\nbillion parameters we demonstrate that larger models improve both the quality\nof generation (from 74.5% to 93.0% for consistency) and controllability (from\n77.5% to 91.5%).", "published": "2020-10-02 08:07:12", "link": "http://arxiv.org/abs/2010.00840v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Text Style Transfer with Padded Masked Language Models", "abstract": "We propose Masker, an unsupervised text-editing method for style transfer. To\ntackle cases when no parallel source-target pairs are available, we train\nmasked language models (MLMs) for both the source and the target domain. Then\nwe find the text spans where the two models disagree the most in terms of\nlikelihood. This allows us to identify the source tokens to delete to transform\nthe source text to match the style of the target domain. The deleted tokens are\nreplaced with the target MLM, and by using a padded MLM variant, we avoid\nhaving to predetermine the number of inserted tokens. Our experiments on\nsentence fusion and sentiment transfer demonstrate that Masker performs\ncompetitively in a fully unsupervised setting. Moreover, in low-resource\nsettings, it improves supervised methods' accuracy by over 10 percentage points\nwhen pre-training them on silver training data generated by Masker.", "published": "2020-10-02 15:33:42", "link": "http://arxiv.org/abs/2010.01054v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntax Representation in Word Embeddings and Neural Networks -- A Survey", "abstract": "Neural networks trained on natural language processing tasks capture syntax\neven though it is not provided as a supervision signal. This indicates that\nsyntactic analysis is essential to the understating of language in artificial\nintelligence systems. This overview paper covers approaches of evaluating the\namount of syntactic information included in the representations of words for\ndifferent neural network architectures. We mainly summarize re-search on\nEnglish monolingual data on language modeling tasks and multilingual data for\nneural machine translation systems and multilingual language models. We\ndescribe which pre-trained models and representations of language are best\nsuited for transfer to syntactic tasks.", "published": "2020-10-02 15:44:58", "link": "http://arxiv.org/abs/2010.01063v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HUMAN: Hierarchical Universal Modular ANnotator", "abstract": "A lot of real-world phenomena are complex and cannot be captured by single\ntask annotations. This causes a need for subsequent annotations, with\ninterdependent questions and answers describing the nature of the subject at\nhand. Even in the case a phenomenon is easily captured by a single task, the\nhigh specialisation of most annotation tools can result in having to switch to\nanother tool if the task only slightly changes.\n  We introduce HUMAN, a novel web-based annotation tool that addresses the\nabove problems by a) covering a variety of annotation tasks on both textual and\nimage data, and b) the usage of an internal deterministic state machine,\nallowing the researcher to chain different annotation tasks in an\ninterdependent manner. Further, the modular nature of the tool makes it easy to\ndefine new annotation tasks and integrate machine learning algorithms e.g., for\nactive learning. HUMAN comes with an easy-to-use graphical user interface that\nsimplifies the annotation task and management.", "published": "2020-10-02 16:20:30", "link": "http://arxiv.org/abs/2010.01080v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Transfer Learning for Complex Word Identification", "abstract": "Complex Word Identification (CWI) is a task centered on detecting\nhard-to-understand words, or groups of words, in texts from different areas of\nexpertise. The purpose of CWI is to highlight problematic structures that\nnon-native speakers would usually find difficult to understand. Our approach\nuses zero-shot, one-shot, and few-shot learning techniques, alongside\nstate-of-the-art solutions for Natural Language Processing (NLP) tasks (i.e.,\nTransformers). Our aim is to provide evidence that the proposed models can\nlearn the characteristics of complex words in a multilingual environment by\nrelying on the CWI shared task 2018 dataset available for four different\nlanguages (i.e., English, German, Spanish, and also French). Our approach\nsurpasses state-of-the-art cross-lingual results in terms of macro F1-score on\nEnglish (0.774), German (0.782), and Spanish (0.734) languages, for the\nzero-shot learning scenario. At the same time, our model also outperforms the\nstate-of-the-art monolingual result for German (0.795 macro F1-score).", "published": "2020-10-02 17:09:47", "link": "http://arxiv.org/abs/2010.01108v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cost-effective Selection of Pretraining Data: A Case Study of\n  Pretraining BERT on Social Media", "abstract": "Recent studies on domain-specific BERT models show that effectiveness on\ndownstream tasks can be improved when models are pretrained on in-domain data.\nOften, the pretraining data used in these models are selected based on their\nsubject matter, e.g., biology or computer science. Given the range of\napplications using social media text, and its unique language variety, we\npretrain two models on tweets and forum text respectively, and empirically\ndemonstrate the effectiveness of these two resources. In addition, we\ninvestigate how similarity measures can be used to nominate in-domain\npretraining data. We publicly release our pretrained models at\nhttps://bit.ly/35RpTf0.", "published": "2020-10-02 18:06:31", "link": "http://arxiv.org/abs/2010.01150v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Extraction of Rules Governing Morphological Agreement", "abstract": "Creating a descriptive grammar of a language is an indispensable step for\nlanguage documentation and preservation. However, at the same time it is a\ntedious, time-consuming task. In this paper, we take steps towards automating\nthis process by devising an automated framework for extracting a first-pass\ngrammatical specification from raw text in a concise, human- and\nmachine-readable format. We focus on extracting rules describing agreement, a\nmorphosyntactic phenomenon at the core of the grammars of many of the world's\nlanguages. We apply our framework to all languages included in the Universal\nDependencies project, with promising results. Using cross-lingual transfer,\neven with no expert annotations in the language of interest, our framework\nextracts a grammatical specification which is nearly equivalent to those\ncreated with large amounts of gold-standard annotated data. We confirm this\nfinding with human expert evaluations of the rules that our framework produces,\nwhich have an average accuracy of 78%. We release an interface demonstrating\nthe extracted rules at https://neulab.github.io/lase/.", "published": "2020-10-02 18:31:45", "link": "http://arxiv.org/abs/2010.01160v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DocuBot : Generating financial reports using natural language\n  interactions", "abstract": "The financial services industry perpetually processes an overwhelming amount\nof complex data. Digital reports are often created based on tedious manual\nanalysis as well as visualization of the underlying trends and characteristics\nof data. Often, the accruing costs of human computation errors in creating\nthese reports are very high. We present DocuBot, a novel AI-powered virtual\nassistant for creating and modifying content in digital documents by modeling\nnatural language interactions as \"skills\" and using them to transform\nunderlying data. DocuBot has the ability to agglomerate saved skills for reuse,\nenabling humans to automatically generate recurrent reports. DocuBot also has\nthe capability to continuously learn domain-specific and user-specific\nvocabulary by interacting with the user. We present evidence that DocuBot adds\nvalue to the financial industry and demonstrate its impact with experiments\ninvolving real and simulated users tasked with creating PowerPoint\npresentations.", "published": "2020-10-02 19:06:29", "link": "http://arxiv.org/abs/2010.01169v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Unsupervised Data for Emotion Recognition in Conversations", "abstract": "Emotion Recognition in Conversations (ERC) aims to predict the emotional\nstate of speakers in conversations, which is essentially a text classification\ntask. Unlike the sentence-level text classification problem, the available\nsupervised data for the ERC task is limited, which potentially prevents the\nmodels from playing their maximum effect. In this paper, we propose a novel\napproach to leverage unsupervised conversation data, which is more accessible.\nSpecifically, we propose the Conversation Completion (ConvCom) task, which\nattempts to select the correct answer from candidate answers to fill a masked\nutterance in a conversation. Then, we Pre-train a basic COntext- Dependent\nEncoder (Pre-CODE) on the ConvCom task. Finally, we fine-tune the Pre-CODE on\nthe datasets of ERC. Experimental results demonstrate that pre-training on\nunsupervised data achieves significant improvement of performance on the ERC\ndatasets, particularly on the minority emotion classes.", "published": "2020-10-02 13:28:47", "link": "http://arxiv.org/abs/2010.01908v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Which *BERT? A Survey Organizing Contextualized Encoders", "abstract": "Pretrained contextualized text encoders are now a staple of the NLP\ncommunity. We present a survey on language representation learning with the aim\nof consolidating a series of shared lessons learned across a variety of recent\nefforts. While significant advancements continue at a rapid pace, we find that\nenough has now been discovered, in different directions, that we can begin to\norganize advances according to common themes. Through this organization, we\nhighlight important considerations when interpreting recent contributions and\nchoosing which model to use.", "published": "2020-10-02 08:34:34", "link": "http://arxiv.org/abs/2010.00854v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SST-BERT at SemEval-2020 Task 1: Semantic Shift Tracing by Clustering in\n  BERT-based Embedding Spaces", "abstract": "Lexical semantic change detection (also known as semantic shift tracing) is a\ntask of identifying words that have changed their meaning over time.\nUnsupervised semantic shift tracing, focal point of SemEval2020, is\nparticularly challenging. Given the unsupervised setup, in this work, we\npropose to identify clusters among different occurrences of each target word,\nconsidering these as representatives of different word meanings. As such,\ndisagreements in obtained clusters naturally allow to quantify the level of\nsemantic shift per each target word in four target languages. To leverage this\nidea, clustering is performed on contextualized (BERT-based) embeddings of word\noccurrences. The obtained results show that our approach performs well both\nmeasured separately (per language) and overall, where we surpass all provided\nSemEval baselines.", "published": "2020-10-02 08:38:40", "link": "http://arxiv.org/abs/2010.00857v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Continual Learning for Natural Language Generation in Task-oriented\n  Dialog Systems", "abstract": "Natural language generation (NLG) is an essential component of task-oriented\ndialog systems. Despite the recent success of neural approaches for NLG, they\nare typically developed in an offline manner for particular domains. To better\nfit real-life applications where new data come in a stream, we study NLG in a\n\"continual learning\" setting to expand its knowledge to new domains or\nfunctionalities incrementally. The major challenge towards this goal is\ncatastrophic forgetting, meaning that a continually trained model tends to\nforget the knowledge it has learned before. To this end, we propose a method\ncalled ARPER (Adaptively Regularized Prioritized Exemplar Replay) by replaying\nprioritized historical exemplars, together with an adaptive regularization\ntechnique based on ElasticWeight Consolidation. Extensive experiments to\ncontinually learn new domains and intents are conducted on MultiWoZ-2.0 to\nbenchmark ARPER with a wide range of techniques. Empirical results demonstrate\nthat ARPER significantly outperforms other methods by effectively mitigating\nthe detrimental catastrophic forgetting issue.", "published": "2020-10-02 10:32:29", "link": "http://arxiv.org/abs/2010.00910v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MultiCQA: Zero-Shot Transfer of Self-Supervised Text Matching Models on\n  a Massive Scale", "abstract": "We study the zero-shot transfer capabilities of text matching models on a\nmassive scale, by self-supervised training on 140 source domains from community\nquestion answering forums in English. We investigate the model performances on\nnine benchmarks of answer selection and question similarity tasks, and show\nthat all 140 models transfer surprisingly well, where the large majority of\nmodels substantially outperforms common IR baselines. We also demonstrate that\nconsidering a broad selection of source domains is crucial for obtaining the\nbest zero-shot transfer performances, which contrasts the standard procedure\nthat merely relies on the largest and most similar domains. In addition, we\nextensively study how to best combine multiple source domains. We propose to\nincorporate self-supervised with supervised multi-task learning on all\navailable source domains. Our best zero-shot transfer model considerably\noutperforms in-domain BERT and the previous state of the art on six benchmarks.\nFine-tuning of our model with in-domain data results in additional large gains\nand achieves the new state of the art on all nine benchmarks.", "published": "2020-10-02 13:22:12", "link": "http://arxiv.org/abs/2010.00980v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "LUKE: Deep Contextualized Entity Representations with Entity-aware\n  Self-attention", "abstract": "Entity representations are useful in natural language tasks involving\nentities. In this paper, we propose new pretrained contextualized\nrepresentations of words and entities based on the bidirectional transformer.\nThe proposed model treats words and entities in a given text as independent\ntokens, and outputs contextualized representations of them. Our model is\ntrained using a new pretraining task based on the masked language model of\nBERT. The task involves predicting randomly masked words and entities in a\nlarge entity-annotated corpus retrieved from Wikipedia. We also propose an\nentity-aware self-attention mechanism that is an extension of the\nself-attention mechanism of the transformer, and considers the types of tokens\n(words or entities) when computing attention scores. The proposed model\nachieves impressive empirical performance on a wide range of entity-related\ntasks. In particular, it obtains state-of-the-art results on five well-known\ndatasets: Open Entity (entity typing), TACRED (relation classification),\nCoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering),\nand SQuAD 1.1 (extractive question answering). Our source code and pretrained\nrepresentations are available at https://github.com/studio-ousia/luke.", "published": "2020-10-02 15:38:03", "link": "http://arxiv.org/abs/2010.01057v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Data-Efficient Pretraining via Contrastive Self-Supervision", "abstract": "For natural language processing `text-to-text' tasks, the prevailing\napproaches heavily rely on pretraining large self-supervised models on\nincreasingly larger `task-external' data. Transfer learning from high-resource\npretraining works well, but research has focused on settings with very large\ndata and compute requirements, while the potential of efficient low-resource\nlearning, without large `task-external' pretraining, remains under-explored. In\nthis work, we evaluate against three core challenges for resource efficient\nlearning. Namely, we analyze: (1) pretraining data ($X$) efficiency; (2) zero\nto few-shot label ($Y$) efficiency; and (3) long-tail generalization, since\nlong-tail preservation has been linked to algorithmic fairness and because data\nin the tail is limited by definition. To address these challenges, we propose a\ndata and compute efficient self-supervised, contrastive text encoder,\npretrained on 60MB of `task-internal' text data, and compare it to RoBERTa,\nwhich was pretrained on 160GB of `task-external' text. We find our method\noutperforms RoBERTa, while pretraining and fine-tuning in a 1/5th of RoBERTa's\nfine-tuning time.", "published": "2020-10-02 15:41:57", "link": "http://arxiv.org/abs/2010.01061v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-Modal Open-Domain Dialogue", "abstract": "Recent work in open-domain conversational agents has demonstrated that\nsignificant improvements in model engagingness and humanness metrics can be\nachieved via massive scaling in both pre-training data and model size\n(Adiwardana et al., 2020; Roller et al., 2020). However, if we want to build\nagents with human-like abilities, we must expand beyond handling just text. A\nparticularly important topic is the ability to see images and communicate about\nwhat is perceived. With the goal of engaging humans in multi-modal dialogue, we\ninvestigate combining components from state-of-the-art open-domain dialogue\nagents with those from state-of-the-art vision models. We study incorporating\ndifferent image fusion schemes and domain-adaptive pre-training and fine-tuning\nstrategies, and show that our best resulting model outperforms strong existing\nmodels in multi-modal dialogue while simultaneously performing as well as its\npredecessor (text-only) BlenderBot (Roller et al., 2020) in text-based\nconversation. We additionally investigate and incorporate safety components in\nour final model, and show that such efforts do not diminish model performance\nwith respect to engagingness metrics.", "published": "2020-10-02 16:20:39", "link": "http://arxiv.org/abs/2010.01082v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cycle-Consistent Adversarial Autoencoders for Unsupervised Text Style\n  Transfer", "abstract": "Unsupervised text style transfer is full of challenges due to the lack of\nparallel data and difficulties in content preservation. In this paper, we\npropose a novel neural approach to unsupervised text style transfer, which we\nrefer to as Cycle-consistent Adversarial autoEncoders (CAE) trained from\nnon-parallel data. CAE consists of three essential components: (1) LSTM\nautoencoders that encode a text in one style into its latent representation and\ndecode an encoded representation into its original text or a transferred\nrepresentation into a style-transferred text, (2) adversarial style transfer\nnetworks that use an adversarially trained generator to transform a latent\nrepresentation in one style into a representation in another style, and (3) a\ncycle-consistent constraint that enhances the capacity of the adversarial style\ntransfer networks in content preservation. The entire CAE with these three\ncomponents can be trained end-to-end. Extensive experiments and in-depth\nanalyses on two widely-used public datasets consistently validate the\neffectiveness of proposed CAE in both style transfer and content preservation\nagainst several strong baselines in terms of four automatic evaluation metrics\nand human evaluation.", "published": "2020-10-02 00:43:39", "link": "http://arxiv.org/abs/2010.00735v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Contrastive Learning of Medical Visual Representations from Paired\n  Images and Text", "abstract": "Learning visual representations of medical images (e.g., X-rays) is core to\nmedical image understanding but its progress has been held back by the scarcity\nof human annotations. Existing work commonly relies on fine-tuning weights\ntransferred from ImageNet pretraining, which is suboptimal due to drastically\ndifferent image characteristics, or rule-based label extraction from the\ntextual report data paired with medical images, which is inaccurate and hard to\ngeneralize. Meanwhile, several recent studies show exciting results from\nunsupervised contrastive learning from natural images, but we find these\nmethods help little on medical images because of their high inter-class\nsimilarity. We propose ConVIRT, an alternative unsupervised strategy to learn\nmedical visual representations by exploiting naturally occurring paired\ndescriptive text. Our new method of pretraining medical image encoders with the\npaired text data via a bidirectional contrastive objective between the two\nmodalities is domain-agnostic, and requires no additional expert input. We test\nConVIRT by transferring our pretrained weights to 4 medical image\nclassification tasks and 2 zero-shot retrieval tasks, and show that it leads to\nimage representations that considerably outperform strong baselines in most\nsettings. Notably, in all 4 classification tasks, our method requires only 10\\%\nas much labeled training data as an ImageNet initialized counterpart to achieve\nbetter or comparable performance, demonstrating superior data efficiency.", "published": "2020-10-02 02:10:18", "link": "http://arxiv.org/abs/2010.00747v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Building Large Lexicalized Ontologies from Text: a Use Case in Automatic\n  Indexing of Biotechnology Patents", "abstract": "This paper presents a tool, TyDI, and methods experimented in the building of\na termino-ontology, i.e. a lexicalized ontology aimed at fine-grained\nindexation for semantic search applications. TyDI provides facilities for\nknowledge engineers and domain experts to efficiently collaborate to validate,\norganize and conceptualize corpus extracted terms. A use case on biotechnology\npatent search demonstrates TyDI's potential.", "published": "2020-10-02 08:42:56", "link": "http://arxiv.org/abs/2010.00860v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Autoregressive Entity Retrieval", "abstract": "Entities are at the center of how we represent and aggregate knowledge. For\ninstance, Encyclopedias such as Wikipedia are structured by entities (e.g., one\nper Wikipedia article). The ability to retrieve such entities given a query is\nfundamental for knowledge-intensive tasks such as entity linking and\nopen-domain question answering. Current approaches can be understood as\nclassifiers among atomic labels, one for each entity. Their weight vectors are\ndense entity representations produced by encoding entity meta information such\nas their descriptions. This approach has several shortcomings: (i) context and\nentity affinity is mainly captured through a vector dot product, potentially\nmissing fine-grained interactions; (ii) a large memory footprint is needed to\nstore dense representations when considering large entity sets; (iii) an\nappropriately hard set of negative data has to be subsampled at training time.\nIn this work, we propose GENRE, the first system that retrieves entities by\ngenerating their unique names, left to right, token-by-token in an\nautoregressive fashion. This mitigates the aforementioned technical issues\nsince: (i) the autoregressive formulation directly captures relations between\ncontext and entity name, effectively cross encoding both; (ii) the memory\nfootprint is greatly reduced because the parameters of our encoder-decoder\narchitecture scale with vocabulary size, not entity count; (iii) the softmax\nloss is computed without subsampling negative data. We experiment with more\nthan 20 datasets on entity disambiguation, end-to-end entity linking and\ndocument retrieval tasks, achieving new state-of-the-art or very competitive\nresults while using a tiny fraction of the memory footprint of competing\nsystems. Finally, we demonstrate that new entities can be added by simply\nspecifying their names. Code and pre-trained models at\nhttps://github.com/facebookresearch/GENRE.", "published": "2020-10-02 10:13:31", "link": "http://arxiv.org/abs/2010.00904v3", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "TeRo: A Time-aware Knowledge Graph Embedding via Temporal Rotation", "abstract": "In the last few years, there has been a surge of interest in learning\nrepresentations of entitiesand relations in knowledge graph (KG). However, the\nrecent availability of temporal knowledgegraphs (TKGs) that contain time\ninformation for each fact created the need for reasoning overtime in such TKGs.\nIn this regard, we present a new approach of TKG embedding, TeRo, which defines\nthe temporal evolution of entity embedding as a rotation from the initial time\nto the currenttime in the complex vector space. Specially, for facts involving\ntime intervals, each relation isrepresented as a pair of dual complex\nembeddings to handle the beginning and the end of therelation, respectively. We\nshow our proposed model overcomes the limitations of the existing KG embedding\nmodels and TKG embedding models and has the ability of learning and\ninferringvarious relation patterns over time. Experimental results on four\ndifferent TKGs show that TeRo significantly outperforms existing\nstate-of-the-art models for link prediction. In addition, we analyze the effect\nof time granularity on link prediction over TKGs, which as far as we know\nhasnot been investigated in previous literature.", "published": "2020-10-02 14:35:27", "link": "http://arxiv.org/abs/2010.01029v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-domain Clinical Natural Language Processing with MedCAT: the\n  Medical Concept Annotation Toolkit", "abstract": "Electronic health records (EHR) contain large volumes of unstructured text,\nrequiring the application of Information Extraction (IE) technologies to enable\nclinical analysis. We present the open-source Medical Concept Annotation\nToolkit (MedCAT) that provides: a) a novel self-supervised machine learning\nalgorithm for extracting concepts using any concept vocabulary including\nUMLS/SNOMED-CT; b) a feature-rich annotation interface for customising and\ntraining IE models; and c) integrations to the broader CogStack ecosystem for\nvendor-agnostic health system deployment. We show improved performance in\nextracting UMLS concepts from open datasets (F1:0.448-0.738 vs 0.429-0.650).\nFurther real-world validation demonstrates SNOMED-CT extraction at 3 large\nLondon hospitals with self-supervised training over ~8.8B words from ~17M\nclinical records and further fine-tuning with ~6K clinician annotated examples.\nWe show strong transferability (F1 > 0.94) between hospitals, datasets, and\nconcept types indicating cross-domain EHR-agnostic utility for accelerated\nclinical and research use cases.", "published": "2020-10-02 19:01:02", "link": "http://arxiv.org/abs/2010.01165v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Training Strategies to Handle Missing Modalities for Audio-Visual\n  Expression Recognition", "abstract": "Automatic audio-visual expression recognition can play an important role in\ncommunication services such as tele-health, VOIP calls and human-machine\ninteraction. Accuracy of audio-visual expression recognition could benefit from\nthe interplay between the two modalities. However, most audio-visual expression\nrecognition systems, trained in ideal conditions, fail to generalize in real\nworld scenarios where either the audio or visual modality could be missing due\nto a number of reasons such as limited bandwidth, interactors' orientation,\ncaller initiated muting. This paper studies the performance of a state-of-the\nart transformer when one of the modalities is missing. We conduct ablation\nstudies to evaluate the model in the absence of either modality. Further, we\npropose a strategy to randomly ablate visual inputs during training at the clip\nor frame level to mimic real world scenarios. Results conducted on in-the-wild\ndata, indicate significant generalization in proposed models trained on missing\ncues, with gains up to 17% for frame level ablations, showing that these\ntraining strategies cope better with the loss of input modalities.", "published": "2020-10-02 00:42:59", "link": "http://arxiv.org/abs/2010.00734v2", "categories": ["eess.AS", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Deep Composer Classification Using Symbolic Representation", "abstract": "In this study, we train deep neural networks to classify composer on a\nsymbolic domain. The model takes a two-channel two-dimensional input, i.e.,\nonset and note activations of time-pitch representation, which is converted\nfrom MIDI recordings and performs a single-label classification. On the\nexperiments conducted on MAESTRO dataset, we report an F1 value of 0.8333 for\nthe classification of 13~classical composers.", "published": "2020-10-02 07:40:44", "link": "http://arxiv.org/abs/2010.00823v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Polyphonic Piano Transcription Using Autoregressive Multi-State Note\n  Model", "abstract": "Recent advances in polyphonic piano transcription have been made primarily by\na deliberate design of neural network architectures that detect different note\nstates such as onset or sustain and model the temporal evolution of the states.\nThe majority of them, however, use separate neural networks for each note\nstate, thereby optimizing multiple loss functions, and also they handle the\ntemporal evolution of note states by abstract connections between the\nstate-wise neural networks or using a post-processing module. In this paper, we\npropose a unified neural network architecture where multiple note states are\npredicted as a softmax output with a single loss function and the temporal\norder is learned by an auto-regressive connection within the single neural\nnetwork. This compact model allows to increase note states without\narchitectural complexity. Using the MAESTRO dataset, we examine various\ncombinations of multiple note states including on, onset, sustain, re-onset,\noffset, and off. We also show that the autoregressive module effectively learns\ninter-state dependency of notes. Finally, we show that our proposed model\nachieves performance comparable to state-of-the-arts with fewer parameters.", "published": "2020-10-02 17:03:19", "link": "http://arxiv.org/abs/2010.01104v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
