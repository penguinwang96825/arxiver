{"title": "AVECL-UMONS database for audio-visual event classification and localization", "abstract": "We introduce the AVECL-UMons dataset for audio-visual event classification and localization in the context of office environments. The audio-visual dataset is composed of 11 event classes recorded at several realistic positions in two different rooms. Two types of sequences are recorded according to the number of events in the sequence. The dataset comprises 2662 unilabel sequences and 2724 multilabel sequences corresponding to a total of 5.24 hours. The dataset is publicly accessible online : https://zenodo.org/record/3965492#.X09wsobgrCI.", "published": "2020-10-02 14:26:02", "link": "http://arxiv.org/abs/2011.01018v1", "categories": ["cs.IR", "cs.CV", "cs.MM", "cs.SD"], "primary_category": "cs.IR"}
{"title": "Training Strategies to Handle Missing Modalities for Audio-Visual Expression Recognition", "abstract": "Automatic audio-visual expression recognition can play an important role in communication services such as tele-health, VOIP calls and human-machine interaction. Accuracy of audio-visual expression recognition could benefit from the interplay between the two modalities. However, most audio-visual expression recognition systems, trained in ideal conditions, fail to generalize in real world scenarios where either the audio or visual modality could be missing due to a number of reasons such as limited bandwidth, interactors' orientation, caller initiated muting. This paper studies the performance of a state-of-the art transformer when one of the modalities is missing. We conduct ablation studies to evaluate the model in the absence of either modality. Further, we propose a strategy to randomly ablate visual inputs during training at the clip or frame level to mimic real world scenarios. Results conducted on in-the-wild data, indicate significant generalization in proposed models trained on missing cues, with gains up to 17% for frame level ablations, showing that these training strategies cope better with the loss of input modalities.", "published": "2020-10-02 00:42:59", "link": "http://arxiv.org/abs/2010.00734v2", "categories": ["eess.AS", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
{"title": "6G Cellular Networks and Connected Autonomous Vehicles", "abstract": "With 5G mobile communication systems been commercially rolled out, research discussions on next generation mobile systems, i.e., 6G, have started. On the other hand, vehicular technologies are also evolving rapidly, from connected vehicles as coined by V2X (vehicle to everything) to autonomous vehicles to the combination of the two, i.e., the networks of connected autonomous vehicles (CAV). How fast the evolution of these two areas will go head-in-head is of great importance, which is the focus of this paper. Based on a brief overview on the technological evolution of V2X to CAV and 6G key technologies, this paper explores two complementary research directions, namely, 6G for CAVs versus CAVs for 6G. The former investigates how various 6G key enablers, such as THz, cell free communication and artificial intelligence (AI), can be utilized to provide CAV mission-critical services. The latter discusses how CAVs can facilitate effective deployment and operation of 6G systems. This paper attempts to investigate the interactions between the two technologies to spark more research efforts in these areas.", "published": "2020-10-02 13:04:58", "link": "http://arxiv.org/abs/2010.00972v1", "categories": ["eess.SP", "cs.NI"], "primary_category": "eess.SP"}
