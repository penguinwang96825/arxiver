{"title": "InfoBehavior: Self-supervised Representation Learning for Ultra-long\n  Behavior Sequence via Hierarchical Grouping", "abstract": "E-commerce companies have to face abnormal sellers who sell potentially-risky\nproducts. Typically, the risk can be identified by jointly considering product\ncontent (e.g., title and image) and seller behavior. This work focuses on\nbehavior feature extraction as behavior sequences can provide valuable clues\nfor the risk discovery by reflecting the sellers' operation habits. Traditional\nfeature extraction techniques heavily depend on domain experts and adapt poorly\nto new tasks. In this paper, we propose a self-supervised method InfoBehavior\nto automatically extract meaningful representations from ultra-long raw\nbehavior sequences instead of the costly feature selection procedure.\nInfoBehavior utilizes Bidirectional Transformer as feature encoder due to its\nexcellent capability in modeling long-term dependency. However, it is\nintractable for commodity GPUs because the time and memory required by\nTransformer grow quadratically with the increase of sequence length. Thus, we\npropose a hierarchical grouping strategy to aggregate ultra-long raw behavior\nsequences to length-processable high-level embedding sequences. Moreover, we\nintroduce two types of pretext tasks. Sequence-related pretext task defines a\ncontrastive-based training objective to correctly select the masked-out\ncoarse-grained/fine-grained behavior sequences against other \"distractor\"\nbehavior sequences; Domain-related pretext task designs a classification\ntraining objective to correctly predict the domain-specific statistical results\nof anomalous behavior. We show that behavior representations from the\npre-trained InfoBehavior can be directly used or integrated with features from\nother side information to support a wide range of downstream tasks.\nExperimental results demonstrate that InfoBehavior significantly improves the\nperformance of Product Risk Management and Intellectual Property Protection.", "published": "2021-06-13 03:45:45", "link": "http://arxiv.org/abs/2106.06905v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Memory-efficient Transformers via Top-$k$ Attention", "abstract": "Following the success of dot-product attention in Transformers, numerous\napproximations have been recently proposed to address its quadratic complexity\nwith respect to the input length. While these variants are memory and compute\nefficient, it is not possible to directly use them with popular pre-trained\nlanguage models trained using vanilla attention, without an expensive\ncorrective pre-training stage. In this work, we propose a simple yet highly\naccurate approximation for vanilla attention. We process the queries in chunks,\nand for each query, compute the top-$k$ scores with respect to the keys. Our\napproach offers several advantages: (a) its memory usage is linear in the input\nsize, similar to linear attention variants, such as Performer and RFA (b) it is\na drop-in replacement for vanilla attention that does not require any\ncorrective pre-training, and (c) it can also lead to significant memory savings\nin the feed-forward layers after casting them into the familiar query-key-value\nframework. We evaluate the quality of top-$k$ approximation for multi-head\nattention layers on the Long Range Arena Benchmark, and for feed-forward layers\nof T5 and UnifiedQA on multiple QA datasets. We show our approach leads to\naccuracy that is nearly-identical to vanilla attention in multiple setups\nincluding training from scratch, fine-tuning, and zero-shot inference.", "published": "2021-06-13 02:30:23", "link": "http://arxiv.org/abs/2106.06899v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis of Covid-19 Tweets using Evolutionary\n  Classification-Based LSTM Model", "abstract": "As the Covid-19 outbreaks rapidly all over the world day by day and also\naffects the lives of million, a number of countries declared complete lock-down\nto check its intensity. During this lockdown period, social media plat-forms\nhave played an important role to spread information about this pandemic across\nthe world, as people used to express their feelings through the social\nnetworks. Considering this catastrophic situation, we developed an experimental\napproach to analyze the reactions of people on Twitter taking into ac-count the\npopular words either directly or indirectly based on this pandemic. This paper\nrepresents the sentiment analysis on collected large number of tweets on\nCoronavirus or Covid-19. At first, we analyze the trend of public sentiment on\nthe topics related to Covid-19 epidemic using an evolutionary classification\nfollowed by the n-gram analysis. Then we calculated the sentiment ratings on\ncollected tweet based on their class. Finally, we trained the long-short term\nnetwork using two types of rated tweets to predict sentiment on Covid-19 data\nand obtained an overall accuracy of 84.46%.", "published": "2021-06-13 04:27:21", "link": "http://arxiv.org/abs/2106.06910v1", "categories": ["cs.CL", "cs.IR", "15-04", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Cross-utterance Reranking Models with BERT and Graph Convolutional\n  Networks for Conversational Speech Recognition", "abstract": "How to effectively incorporate cross-utterance information cues into a neural\nlanguage model (LM) has emerged as one of the intriguing issues for automatic\nspeech recognition (ASR). Existing research efforts on improving\ncontextualization of an LM typically regard previous utterances as a sequence\nof additional input and may fail to capture complex global structural\ndependencies among these utterances. In view of this, we in this paper seek to\nrepresent the historical context information of an utterance as\ngraph-structured data so as to distill cross-utterances, global word\ninteraction relationships. To this end, we apply a graph convolutional network\n(GCN) on the resulting graph to obtain the corresponding GCN embeddings of\nhistorical words. GCN has recently found its versatile applications on\nsocial-network analysis, text summarization, and among others due mainly to its\nability of effectively capturing rich relational information among elements.\nHowever, GCN remains largely underexplored in the context of ASR, especially\nfor dealing with conversational speech. In addition, we frame ASR N-best\nreranking as a prediction problem, leveraging bidirectional encoder\nrepresentations from transformers (BERT) as the vehicle to not only seize the\nlocal intrinsic word regularity patterns inherent in a candidate hypothesis but\nalso incorporate the cross-utterance, historical word interaction cues\ndistilled by GCN for promoting performance. Extensive experiments conducted on\nthe AMI benchmark dataset seem to confirm the pragmatic utility of our methods,\nin relation to some current top-of-the-line methods.", "published": "2021-06-13 05:30:16", "link": "http://arxiv.org/abs/2106.06922v6", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Common Sense Beyond English: Evaluating and Improving Multilingual\n  Language Models for Commonsense Reasoning", "abstract": "Commonsense reasoning research has so far been limited to English. We aim to\nevaluate and improve popular multilingual language models (ML-LMs) to help\nadvance commonsense reasoning (CSR) beyond English. We collect the Mickey\nCorpus, consisting of 561k sentences in 11 different languages, which can be\nused for analyzing and improving ML-LMs. We propose Mickey Probe, a\nlanguage-agnostic probing task for fairly evaluating the common sense of\npopular ML-LMs across different languages. In addition, we also create two new\ndatasets, X-CSQA and X-CODAH, by translating their English versions to 15 other\nlanguages, so that we can evaluate popular ML-LMs for cross-lingual commonsense\nreasoning. To improve the performance beyond English, we propose a simple yet\neffective method -- multilingual contrastive pre-training (MCP). It\nsignificantly enhances sentence representations, yielding a large performance\ngain on both benchmarks.", "published": "2021-06-13 07:14:03", "link": "http://arxiv.org/abs/2106.06937v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SASICM A Multi-Task Benchmark For Subtext Recognition", "abstract": "Subtext is a kind of deep semantics which can be acquired after one or more\nrounds of expression transformation. As a popular way of expressing one's\nintentions, it is well worth studying. In this paper, we try to make computers\nunderstand whether there is a subtext by means of machine learning. We build a\nChinese dataset whose source data comes from the popular social media (e.g.\nWeibo, Netease Music, Zhihu, and Bilibili). In addition, we also build a\nbaseline model called SASICM to deal with subtext recognition. The F1 score of\nSASICMg, whose pretrained model is GloVe, is as high as 64.37%, which is 3.97%\nhigher than that of BERT based model, 12.7% higher than that of traditional\nmethods on average, including support vector machine, logistic regression\nclassifier, maximum entropy classifier, naive bayes classifier and decision\ntree and 2.39% higher than that of the state-of-the-art, including MARIN and\nBTM. The F1 score of SASICMBERT, whose pretrained model is BERT, is 65.12%,\nwhich is 0.75% higher than that of SASICMg. The accuracy rates of SASICMg and\nSASICMBERT are 71.16% and 70.76%, respectively, which can compete with those of\nother methods which are mentioned before.", "published": "2021-06-13 08:29:15", "link": "http://arxiv.org/abs/2106.06944v2", "categories": ["cs.CL", "cs.AI", "03B65, 91F20 (Primary), 68T50 (Secondary)", "I.2.2"], "primary_category": "cs.CL"}
{"title": "Exploring and Distilling Posterior and Prior Knowledge for Radiology\n  Report Generation", "abstract": "Automatically generating radiology reports can improve current clinical\npractice in diagnostic radiology. On one hand, it can relieve radiologists from\nthe heavy burden of report writing; On the other hand, it can remind\nradiologists of abnormalities and avoid the misdiagnosis and missed diagnosis.\nYet, this task remains a challenging job for data-driven neural networks, due\nto the serious visual and textual data biases. To this end, we propose a\nPosterior-and-Prior Knowledge Exploring-and-Distilling approach (PPKED) to\nimitate the working patterns of radiologists, who will first examine the\nabnormal regions and assign the disease topic tags to the abnormal regions, and\nthen rely on the years of prior medical knowledge and prior working experience\naccumulations to write reports. Thus, the PPKED includes three modules:\nPosterior Knowledge Explorer (PoKE), Prior Knowledge Explorer (PrKE) and\nMulti-domain Knowledge Distiller (MKD). In detail, PoKE explores the posterior\nknowledge, which provides explicit abnormal visual regions to alleviate visual\ndata bias; PrKE explores the prior knowledge from the prior medical knowledge\ngraph (medical knowledge) and prior radiology reports (working experience) to\nalleviate textual data bias. The explored knowledge is distilled by the MKD to\ngenerate the final reports. Evaluated on MIMIC-CXR and IU-Xray datasets, our\nmethod is able to outperform previous state-of-the-art models on these two\ndatasets.", "published": "2021-06-13 11:10:02", "link": "http://arxiv.org/abs/2106.06963v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Shape of Elephant: Study of Macro Properties of Word Embeddings Spaces", "abstract": "Pre-trained word representations became a key component in many NLP tasks.\nHowever, the global geometry of the word embeddings remains poorly understood.\nIn this paper, we demonstrate that a typical word embeddings cloud is shaped as\na high-dimensional simplex with interpretable vertices and propose a simple yet\neffective method for enumeration of these vertices. We show that the proposed\nmethod can detect and describe vertices of the simplex for GloVe and fasttext\nspaces.", "published": "2021-06-13 11:19:49", "link": "http://arxiv.org/abs/2106.06964v1", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Contrastive Attention for Automatic Chest X-ray Report Generation", "abstract": "Recently, chest X-ray report generation, which aims to automatically generate\ndescriptions of given chest X-ray images, has received growing research\ninterests. The key challenge of chest X-ray report generation is to accurately\ncapture and describe the abnormal regions. In most cases, the normal regions\ndominate the entire chest X-ray image, and the corresponding descriptions of\nthese normal regions dominate the final report. Due to such data bias,\nlearning-based models may fail to attend to abnormal regions. In this work, to\neffectively capture and describe abnormal regions, we propose the Contrastive\nAttention (CA) model. Instead of solely focusing on the current input image,\nthe CA model compares the current input image with normal images to distill the\ncontrastive information. The acquired contrastive information can better\nrepresent the visual features of abnormal regions. According to the experiments\non the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into\nseveral existing models can boost their performance across most metrics. In\naddition, according to the analysis, the CA model can help existing models\nbetter attend to the abnormal regions and provide more accurate descriptions\nwhich are crucial for an interpretable diagnosis. Specifically, we achieve the\nstate-of-the-art results on the two public datasets.", "published": "2021-06-13 11:20:31", "link": "http://arxiv.org/abs/2106.06965v5", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Thinking Like Transformers", "abstract": "What is the computational model behind a Transformer? Where recurrent neural\nnetworks have direct parallels in finite state machines, allowing clear\ndiscussion and thought around architecture variants or trained models,\nTransformers have no such familiar parallel. In this paper we aim to change\nthat, proposing a computational model for the transformer-encoder in the form\nof a programming language. We map the basic components of a transformer-encoder\n-- attention and feed-forward computation -- into simple primitives, around\nwhich we form a programming language: the Restricted Access Sequence Processing\nLanguage (RASP). We show how RASP can be used to program solutions to tasks\nthat could conceivably be learned by a Transformer, and how a Transformer can\nbe trained to mimic a RASP solution. In particular, we provide RASP programs\nfor histograms, sorting, and Dyck-languages. We further use our model to relate\ntheir difficulty in terms of the number of required layers and attention heads:\nanalyzing a RASP program implies a maximum number of heads and layers necessary\nto encode a task in a transformer. Finally, we see how insights gained from our\nabstraction might be used to explain phenomena seen in recent works.", "published": "2021-06-13 13:04:46", "link": "http://arxiv.org/abs/2106.06981v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of\n  Transcribed Audio", "abstract": "This paper introduces GigaSpeech, an evolving, multi-domain English speech\nrecognition corpus with 10,000 hours of high quality labeled audio suitable for\nsupervised training, and 40,000 hours of total audio suitable for\nsemi-supervised and unsupervised training. Around 40,000 hours of transcribed\naudio is first collected from audiobooks, podcasts and YouTube, covering both\nread and spontaneous speaking styles, and a variety of topics, such as arts,\nscience, sports, etc. A new forced alignment and segmentation pipeline is\nproposed to create sentence segments suitable for speech recognition training,\nand to filter out segments with low-quality transcription. For system training,\nGigaSpeech provides five subsets of different sizes, 10h, 250h, 1000h, 2500h,\nand 10000h. For our 10,000-hour XL training subset, we cap the word error rate\nat 4% during the filtering/validation stage, and for all our other smaller\ntraining subsets, we cap it at 0%. The DEV and TEST evaluation sets, on the\nother hand, are re-processed by professional human transcribers to ensure high\ntranscription quality. Baseline systems are provided for popular speech\nrecognition toolkits, namely Athena, ESPnet, Kaldi and Pika.", "published": "2021-06-13 04:09:16", "link": "http://arxiv.org/abs/2106.06909v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GenSF: Simultaneous Adaptation of Generative Pre-trained Models and Slot\n  Filling", "abstract": "In transfer learning, it is imperative to achieve strong alignment between a\npre-trained model and a downstream task. Prior work has done this by proposing\ntask-specific pre-training objectives, which sacrifices the inherent\nscalability of the transfer learning paradigm. We instead achieve strong\nalignment by simultaneously modifying both the pre-trained model and the\nformulation of the downstream task, which is more efficient and preserves the\nscalability of transfer learning. We present GenSF (Generative Slot Filling),\nwhich leverages a generative pre-trained open-domain dialog model for slot\nfilling. GenSF (1) adapts the pre-trained model by incorporating inductive\nbiases about the task and (2) adapts the downstream task by reformulating slot\nfilling to better leverage the pre-trained model's capabilities. GenSF achieves\nstate-of-the-art results on two slot filling datasets with strong gains in\nfew-shot and zero-shot settings. We achieve a 9 F1 score improvement in\nzero-shot slot filling. This highlights the value of strong alignment between\nthe pre-trained model and the downstream task.", "published": "2021-06-13 17:44:17", "link": "http://arxiv.org/abs/2106.07055v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Schema-Guided Paradigm for Zero-Shot Dialog", "abstract": "Developing mechanisms that flexibly adapt dialog systems to unseen tasks and\ndomains is a major challenge in dialog research. Neural models implicitly\nmemorize task-specific dialog policies from the training data. We posit that\nthis implicit memorization has precluded zero-shot transfer learning. To this\nend, we leverage the schema-guided paradigm, wherein the task-specific dialog\npolicy is explicitly provided to the model. We introduce the Schema Attention\nModel (SAM) and improved schema representations for the STAR corpus. SAM\nobtains significant improvement in zero-shot settings, with a +22 F1 score\nimprovement over prior work. These results validate the feasibility of\nzero-shot generalizability in dialog. Ablation experiments are also presented\nto demonstrate the efficacy of SAM.", "published": "2021-06-13 17:44:45", "link": "http://arxiv.org/abs/2106.07056v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Dataset of Dynamic Reverberant Sound Scenes with Directional\n  Interferers for Sound Event Localization and Detection", "abstract": "This report presents the dataset and baseline of Task 3 of the DCASE2021\nChallenge on Sound Event Localization and Detection (SELD). The dataset is\nbased on emulation of real recordings of static or moving sound events under\nreal conditions of reverberation and ambient noise, using spatial room impulse\nresponses captured in a variety of rooms and delivered in two spatial formats.\nThe acoustical synthesis remains the same as in the previous iteration of the\nchallenge, however the new dataset brings more challenging conditions of\npolyphony and overlapping instances of the same class. The most important\ndifference of the new dataset is the introduction of directional interferers,\nmeaning sound events that are localized in space but do not belong to the\ntarget classes to be detected and are not annotated. Since such interfering\nevents are expected in every real-world scenario of SELD, the new dataset aims\nto promote systems that deal with this condition effectively. A modified\nSELDnet baseline employing the recent ACCDOA representation of SELD problems\naccompanies the dataset and it is shown to outperform the previous one. The new\ndataset is shown to be significantly more challenging for both baselines\naccording to all considered metrics. To investigate the individual and combined\neffects of ambient noise, interferers, and reverberation, we study the\nperformance of the baseline on different versions of the dataset excluding or\nincluding combinations of these factors. The results indicate that by far the\nmost detrimental effects are caused by directional interferers.", "published": "2021-06-13 13:54:21", "link": "http://arxiv.org/abs/2106.06999v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SoundDet: Polyphonic Moving Sound Event Detection and Localization from\n  Raw Waveform", "abstract": "We present a new framework SoundDet, which is an end-to-end trainable and\nlight-weight framework, for polyphonic moving sound event detection and\nlocalization. Prior methods typically approach this problem by preprocessing\nraw waveform into time-frequency representations, which is more amenable to\nprocess with well-established image processing pipelines. Prior methods also\ndetect in segment-wise manner, leading to incomplete and partial detections.\nSoundDet takes a novel approach and directly consumes the raw, multichannel\nwaveform and treats the spatio-temporal sound event as a complete\n\"sound-object\" to be detected. Specifically, SoundDet consists of a backbone\nneural network and two parallel heads for temporal detection and spatial\nlocalization, respectively. Given the large sampling rate of raw waveform, the\nbackbone network first learns a set of phase-sensitive and frequency-selective\nbank of filters to explicitly retain direction-of-arrival information, whilst\nbeing highly computationally and parametrically efficient than standard 1D/2D\nconvolution. A dense sound event proposal map is then constructed to handle the\nchallenges of predicting events with large varying temporal duration.\nAccompanying the dense proposal map are a temporal overlapness map and a motion\nsmoothness map that measure a proposal's confidence to be an event from\ntemporal detection accuracy and movement consistency perspective. Involving the\ntwo maps guarantees SoundDet to be trained in a spatio-temporally unified\nmanner. Experimental results on the public DCASE dataset show the advantage of\nSoundDet on both segment-based and our newly proposed event-based evaluation\nsystem.", "published": "2021-06-13 11:43:41", "link": "http://arxiv.org/abs/2106.06969v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "WASE: Learning When to Attend for Speaker Extraction in Cocktail Party\n  Environments", "abstract": "In the speaker extraction problem, it is found that additional information\nfrom the target speaker contributes to the tracking and extraction of the\ntarget speaker, which includes voiceprint, lip movement, facial expression, and\nspatial information. However, no one cares for the cue of sound onset, which\nhas been emphasized in the auditory scene analysis and psychology. Inspired by\nit, we explicitly modeled the onset cue and verified the effectiveness in the\nspeaker extraction task. We further extended to the onset/offset cues and got\nperformance improvement. From the perspective of tasks, our onset/offset-based\nmodel completes the composite task, a complementary combination of speaker\nextraction and speaker-dependent voice activity detection. We also combined\nvoiceprint with onset/offset cues. Voiceprint models voice characteristics of\nthe target while onset/offset models the start/end information of the speech.\nFrom the perspective of auditory scene analysis, the combination of two\nperception cues can promote the integrity of the auditory object. The\nexperiment results are also close to state-of-the-art performance, using nearly\nhalf of the parameters. We hope that this work will inspire communities of\nspeech processing and psychology, and contribute to communication between them.\nOur code will be available in https://github.com/aispeech-lab/wase/.", "published": "2021-06-13 14:56:05", "link": "http://arxiv.org/abs/2106.07016v1", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
