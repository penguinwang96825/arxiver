{"title": "Generalisation in Named Entity Recognition: A Quantitative Analysis", "abstract": "Named Entity Recognition (NER) is a key NLP task, which is all the more\nchallenging on Web and user-generated content with their diverse and\ncontinuously changing language. This paper aims to quantify how this diversity\nimpacts state-of-the-art NER methods, by measuring named entity (NE) and\ncontext variability, feature sparsity, and their effects on precision and\nrecall. In particular, our findings indicate that NER approaches struggle to\ngeneralise in diverse genres with limited training data. Unseen NEs, in\nparticular, play an important role, which have a higher incidence in diverse\ngenres such as social media than in more regular genres such as newswire.\nCoupled with a higher incidence of unseen features more generally and the lack\nof large training corpora, this leads to significantly lower F1 scores for\ndiverse genres as compared to more regular ones. We also find that leading\nsystems rely heavily on surface forms found in training data, having problems\ngeneralising beyond these, and offer explanations for this observation.", "published": "2017-01-11 08:02:40", "link": "http://arxiv.org/abs/1701.02877v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multifaceted Evaluation of Neural versus Phrase-Based Machine\n  Translation for 9 Language Directions", "abstract": "We aim to shed light on the strengths and weaknesses of the newly introduced\nneural machine translation paradigm. To that end, we conduct a multifaceted\nevaluation in which we compare outputs produced by state-of-the-art neural\nmachine translation and phrase-based machine translation systems for 9 language\ndirections across a number of dimensions. Specifically, we measure the\nsimilarity of the outputs, their fluency and amount of reordering, the effect\nof sentence length and performance across different error categories. We find\nout that translations produced by neural machine translation systems are\nconsiderably different, more fluent and more accurate in terms of word order\ncompared to those produced by phrase-based systems. Neural machine translation\nsystems are also more accurate at producing inflected forms, but they perform\npoorly when translating very long sentences.", "published": "2017-01-11 09:32:47", "link": "http://arxiv.org/abs/1701.02901v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question Analysis for Arabic Question Answering Systems", "abstract": "The first step of processing a question in Question Answering(QA) Systems is\nto carry out a detailed analysis of the question for the purpose of determining\nwhat it is asking for and how to perfectly approach answering it. Our Question\nanalysis uses several techniques to analyze any question given in natural\nlanguage: a Stanford POS Tagger & parser for Arabic language, a named entity\nrecognizer, tokenizer,Stop-word removal, Question expansion, Question\nclassification and Question focus extraction components. We employ numerous\ndetection rules and trained classifier using features from this analysis to\ndetect important elements of the question, including: 1) the portion of the\nquestion that is a referring to the answer (the focus); 2) different terms in\nthe question that identify what type of entity is being asked for (the lexical\nanswer types); 3) Question expansion ; 4) a process of classifying the question\ninto one or more of several and different types; and We describe how these\nelements are identified and evaluate the effect of accurate detection on our\nquestion-answering system using the Mean Reciprocal Rank(MRR) accuracy measure.", "published": "2017-01-11 11:12:24", "link": "http://arxiv.org/abs/1701.02925v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual RST Discourse Parsing", "abstract": "Discourse parsing is an integral part of understanding information flow and\nargumentative structure in documents. Most previous research has focused on\ninducing and evaluating models from the English RST Discourse Treebank.\nHowever, discourse treebanks for other languages exist, including Spanish,\nGerman, Basque, Dutch and Brazilian Portuguese. The treebanks share the same\nunderlying linguistic theory, but differ slightly in the way documents are\nannotated. In this paper, we present (a) a new discourse parser which is\nsimpler, yet competitive (significantly better on 2/3 metrics) to state of the\nart for English, (b) a harmonization of discourse treebanks across languages,\nenabling us to present (c) what to the best of our knowledge are the first\nexperiments on cross-lingual discourse parsing.", "published": "2017-01-11 12:16:25", "link": "http://arxiv.org/abs/1701.02946v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distinguishing Antonyms and Synonyms in a Pattern-based Neural Network", "abstract": "Distinguishing between antonyms and synonyms is a key task to achieve high\nperformance in NLP systems. While they are notoriously difficult to distinguish\nby distributional co-occurrence models, pattern-based methods have proven\neffective to differentiate between the relations. In this paper, we present a\nnovel neural network model AntSynNET that exploits lexico-syntactic patterns\nfrom syntactic parse trees. In addition to the lexical and syntactic\ninformation, we successfully integrate the distance between the related words\nalong the syntactic path as a new pattern feature. The results from\nclassification experiments show that AntSynNET improves the performance over\nprior pattern-based methods.", "published": "2017-01-11 13:11:48", "link": "http://arxiv.org/abs/1701.02962v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Job Detection in Twitter", "abstract": "In this report, we propose a new application for twitter data called\n\\textit{job detection}. We identify people's job category based on their\ntweets. As a preliminary work, we limited our task to identify only IT workers\nfrom other job holders. We have used and compared both simple bag of words\nmodel and a document representation based on Skip-gram model. Our results show\nthat the model based on Skip-gram, achieves a 76\\% precision and 82\\% recall.", "published": "2017-01-11 18:42:09", "link": "http://arxiv.org/abs/1701.03092v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "De-identification In practice", "abstract": "We report our effort to identify the sensitive information, subset of data\nitems listed by HIPAA (Health Insurance Portability and Accountability), from\nmedical text using the recent advances in natural language processing and\nmachine learning techniques. We represent the words with high dimensional\ncontinuous vectors learned by a variant of Word2Vec called Continous Bag Of\nWords (CBOW). We feed the word vectors into a simple neural network with a Long\nShort-Term Memory (LSTM) architecture. Without any attempts to extract manually\ncrafted features and considering that our medical dataset is too small to be\nfed into neural network, we obtained promising results. The results thrilled us\nto think about the larger scale of the project with precise parameter tuning\nand other possible improvements.", "published": "2017-01-11 19:22:56", "link": "http://arxiv.org/abs/1701.03129v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parsing Universal Dependencies without training", "abstract": "We propose UDP, the first training-free parser for Universal Dependencies\n(UD). Our algorithm is based on PageRank and a small set of head attachment\nrules. It features two-step decoding to guarantee that function words are\nattached as leaf nodes. The parser requires no training, and it is competitive\nwith a delexicalized transfer system. UDP offers a linguistically sound\nunsupervised alternative to cross-lingual parsing for UD, which can be used as\na baseline for such systems. The parser has very few parameters and is\ndistinctly robust to domain change across languages.", "published": "2017-01-11 20:56:29", "link": "http://arxiv.org/abs/1701.03163v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating High-Quality and Informative Conversation Responses with\n  Sequence-to-Sequence Models", "abstract": "Sequence-to-sequence models have been applied to the conversation response\ngeneration problem where the source sequence is the conversation history and\nthe target sequence is the response. Unlike translation, conversation\nresponding is inherently creative. The generation of long, informative,\ncoherent, and diverse responses remains a hard task. In this work, we focus on\nthe single turn setting. We add self-attention to the decoder to maintain\ncoherence in longer responses, and we propose a practical approach, called the\nglimpse-model, for scaling to large datasets. We introduce a stochastic\nbeam-search algorithm with segment-by-segment reranking which lets us inject\ndiversity earlier in the generation process. We trained on a combined data set\nof over 2.3B conversation messages mined from the web. In human evaluation\nstudies, our method produces longer responses overall, with a higher proportion\nrated as acceptable and excellent as length increases, compared to baseline\nsequence-to-sequence models with explicit length-promotion. A back-off strategy\nproduces better responses overall, in the full spectrum of lengths.", "published": "2017-01-11 22:55:04", "link": "http://arxiv.org/abs/1701.03185v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Decoding as Continuous Optimization in Neural Machine\n  Translation", "abstract": "We propose a novel decoding approach for neural machine translation (NMT)\nbased on continuous optimisation. We convert decoding - basically a discrete\noptimization problem - into a continuous optimization problem. The resulting\nconstrained continuous optimisation problem is then tackled using\ngradient-based methods. Our powerful decoding framework enables decoding\nintractable models such as the intersection of left-to-right and right-to-left\n(bidirectional) as well as source-to-target and target-to-source (bilingual)\nNMT models. Our empirical results show that our decoding framework is\neffective, and leads to substantial improvements in translations generated from\nthe intersected models where the typical greedy or beam search is not feasible.\nWe also compare our framework against reranking, and analyse its advantages and\ndisadvantages.", "published": "2017-01-11 06:02:44", "link": "http://arxiv.org/abs/1701.02854v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Decoding with Finite-State Transducers on GPUs", "abstract": "Weighted finite automata and transducers (including hidden Markov models and\nconditional random fields) are widely used in natural language processing (NLP)\nto perform tasks such as morphological analysis, part-of-speech tagging,\nchunking, named entity recognition, speech recognition, and others.\nParallelizing finite state algorithms on graphics processing units (GPUs) would\nbenefit many areas of NLP. Although researchers have implemented GPU versions\nof basic graph algorithms, limited previous work, to our knowledge, has been\ndone on GPU algorithms for weighted finite automata. We introduce a GPU\nimplementation of the Viterbi and forward-backward algorithm, achieving\ndecoding speedups of up to 5.2x over our serial implementation running on\ndifferent computer architectures and 6093x over OpenFST.", "published": "2017-01-11 16:07:27", "link": "http://arxiv.org/abs/1701.03038v2", "categories": ["cs.CL", "cs.DC"], "primary_category": "cs.CL"}
{"title": "Efficient Twitter Sentiment Classification using Subjective Distant\n  Supervision", "abstract": "As microblogging services like Twitter are becoming more and more influential\nin today's globalised world, its facets like sentiment analysis are being\nextensively studied. We are no longer constrained by our own opinion. Others\nopinions and sentiments play a huge role in shaping our perspective. In this\npaper, we build on previous works on Twitter sentiment analysis using Distant\nSupervision. The existing approach requires huge computation resource for\nanalysing large number of tweets. In this paper, we propose techniques to speed\nup the computation process for sentiment analysis. We use tweet subjectivity to\nselect the right training samples. We also introduce the concept of EFWS\n(Effective Word Score) of a tweet that is derived from polarity scores of\nfrequently used words, which is an additional heuristic that can be used to\nspeed up the sentiment classification with standard machine learning\nalgorithms. We performed our experiments using 1.6 million tweets. Experimental\nevaluations show that our proposed technique is more efficient and has higher\naccuracy compared to previously proposed methods. We achieve overall accuracies\nof around 80% (EFWS heuristic gives an accuracy around 85%) on a training\ndataset of 100K tweets, which is half the size of the dataset used for the\nbaseline model. The accuracy of our proposed model is 2-3% higher than the\nbaseline model, and the model effectively trains at twice the speed of the\nbaseline model.", "published": "2017-01-11 16:39:04", "link": "http://arxiv.org/abs/1701.03051v1", "categories": ["cs.SI", "cs.CL", "cs.IR"], "primary_category": "cs.SI"}
{"title": "RUBER: An Unsupervised Method for Automatic Evaluation of Open-Domain\n  Dialog Systems", "abstract": "Open-domain human-computer conversation has been attracting increasing\nattention over the past few years. However, there does not exist a standard\nautomatic evaluation metric for open-domain dialog systems; researchers usually\nresort to human annotation for model evaluation, which is time- and\nlabor-intensive. In this paper, we propose RUBER, a Referenced metric and\nUnreferenced metric Blended Evaluation Routine, which evaluates a reply by\ntaking into consideration both a groundtruth reply and a query (previous\nuser-issued utterance). Our metric is learnable, but its training does not\nrequire labels of human satisfaction. Hence, RUBER is flexible and extensible\nto different datasets and languages. Experiments on both retrieval and\ngenerative dialog systems show that RUBER has a high correlation with human\nannotation.", "published": "2017-01-11 17:43:57", "link": "http://arxiv.org/abs/1701.03079v2", "categories": ["cs.CL", "cs.HC", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Attention-Based Multimodal Fusion for Video Description", "abstract": "Currently successful methods for video description are based on\nencoder-decoder sentence generation using recur-rent neural networks (RNNs).\nRecent work has shown the advantage of integrating temporal and/or spatial\nattention mechanisms into these models, in which the decoder net-work predicts\neach word in the description by selectively giving more weight to encoded\nfeatures from specific time frames (temporal attention) or to features from\nspecific spatial regions (spatial attention). In this paper, we propose to\nexpand the attention model to selectively attend not just to specific times or\nspatial regions, but to specific modalities of input such as image features,\nmotion features, and audio features. Our new modality-dependent attention\nmechanism, which we call multimodal attention, provides a natural way to fuse\nmultimodal information for video description. We evaluate our method on the\nYoutube2Text dataset, achieving results that are competitive with current state\nof the art. More importantly, we demonstrate that our model incorporating\nmultimodal attention as well as temporal attention significantly outperforms\nthe model that uses temporal attention alone.", "published": "2017-01-11 19:16:42", "link": "http://arxiv.org/abs/1701.03126v2", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
