{"title": "Contextual and Position-Aware Factorization Machines for Sentiment\n  Classification", "abstract": "While existing machine learning models have achieved great success for\nsentiment classification, they typically do not explicitly capture\nsentiment-oriented word interaction, which can lead to poor results for\nfine-grained analysis at the snippet level (a phrase or sentence).\nFactorization Machine provides a possible approach to learning element-wise\ninteraction for recommender systems, but they are not directly applicable to\nour task due to the inability to model contexts and word sequences. In this\nwork, we develop two Position-aware Factorization Machines which consider word\ninteraction, context and position information. Such information is jointly\nencoded in a set of sentiment-oriented word interaction vectors. Compared to\ntraditional word embeddings, SWI vectors explicitly capture sentiment-oriented\nword interaction and simplify the parameter learning. Experimental results show\nthat while they have comparable performance with state-of-the-art methods for\ndocument-level classification, they benefit the snippet/sentence-level\nsentiment analysis.", "published": "2018-01-18 18:51:58", "link": "http://arxiv.org/abs/1801.06172v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Non-Adversarial Unsupervised Word Translation", "abstract": "Unsupervised word translation from non-parallel inter-lingual corpora has\nattracted much research interest. Very recently, neural network methods trained\nwith adversarial loss functions achieved high accuracy on this task. Despite\nthe impressive success of the recent techniques, they suffer from the typical\ndrawbacks of generative adversarial models: sensitivity to hyper-parameters,\nlong training time and lack of interpretability. In this paper, we make the\nobservation that two sufficiently similar distributions can be aligned\ncorrectly with iterative matching methods. We present a novel method that first\naligns the second moment of the word distributions of the two languages and\nthen iteratively refines the alignment. Extensive experiments on word\ntranslation of European and Non-European languages show that our method\nachieves better performance than recent state-of-the-art deep adversarial\napproaches and is competitive with the supervised baseline. It is also\nefficient, easy to parallelize on CPU and interpretable.", "published": "2018-01-18 16:59:19", "link": "http://arxiv.org/abs/1801.06126v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Natural Language Multitasking: Analyzing and Improving Syntactic\n  Saliency of Hidden Representations", "abstract": "We train multi-task autoencoders on linguistic tasks and analyze the learned\nhidden sentence representations. The representations change significantly when\ntranslation and part-of-speech decoders are added. The more decoders a model\nemploys, the better it clusters sentences according to their syntactic\nsimilarity, as the representation space becomes less entangled. We explore the\nstructure of the representation space by interpolating between sentences, which\nyields interesting pseudo-English sentences, many of which have recognizable\nsyntactic structure. Lastly, we point out an interesting property of our\nmodels: The difference-vector between two sentences can be added to change a\nthird sentence with similar features in a meaningful way.", "published": "2018-01-18 14:10:37", "link": "http://arxiv.org/abs/1801.06024v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Universal Language Model Fine-tuning for Text Classification", "abstract": "Inductive transfer learning has greatly impacted computer vision, but\nexisting approaches in NLP still require task-specific modifications and\ntraining from scratch. We propose Universal Language Model Fine-tuning\n(ULMFiT), an effective transfer learning method that can be applied to any task\nin NLP, and introduce techniques that are key for fine-tuning a language model.\nOur method significantly outperforms the state-of-the-art on six text\nclassification tasks, reducing the error by 18-24% on the majority of datasets.\nFurthermore, with only 100 labeled examples, it matches the performance of\ntraining from scratch on 100x more data. We open-source our pretrained models\nand code.", "published": "2018-01-18 17:54:52", "link": "http://arxiv.org/abs/1801.06146v5", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy\n  Learning", "abstract": "Training a task-completion dialogue agent via reinforcement learning (RL) is\ncostly because it requires many interactions with real users. One common\nalternative is to use a user simulator. However, a user simulator usually lacks\nthe language complexity of human interlocutors and the biases in its design may\ntend to degrade the agent. To address these issues, we present Deep Dyna-Q,\nwhich to our knowledge is the first deep RL framework that integrates planning\nfor task-completion dialogue policy learning. We incorporate into the dialogue\nagent a model of the environment, referred to as the world model, to mimic real\nuser response and generate simulated experience. During dialogue policy\nlearning, the world model is constantly updated with real user experience to\napproach real user behavior, and in turn, the dialogue agent is optimized using\nboth real experience and simulated experience. The effectiveness of our\napproach is demonstrated on a movie-ticket booking task in both simulated and\nhuman-in-the-loop settings.", "published": "2018-01-18 18:57:33", "link": "http://arxiv.org/abs/1801.06176v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
