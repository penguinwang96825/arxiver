{"title": "Syntactic representation learning for neural network based TTS with\n  syntactic parse tree traversal", "abstract": "Syntactic structure of a sentence text is correlated with the prosodic\nstructure of the speech that is crucial for improving the prosody and\nnaturalness of a text-to-speech (TTS) system. Nowadays TTS systems usually try\nto incorporate syntactic structure information with manually designed features\nbased on expert knowledge. In this paper, we propose a syntactic representation\nlearning method based on syntactic parse tree traversal to automatically\nutilize the syntactic structure information. Two constituent label sequences\nare linearized through left-first and right-first traversals from constituent\nparse tree. Syntactic representations are then extracted at word level from\neach constituent label sequence by a corresponding uni-directional gated\nrecurrent unit (GRU) network. Meanwhile, nuclear-norm maximization loss is\nintroduced to enhance the discriminability and diversity of the embeddings of\nconstituent labels. Upsampled syntactic representations and phoneme embeddings\nare concatenated to serve as the encoder input of Tacotron2. Experimental\nresults demonstrate the effectiveness of our proposed approach, with mean\nopinion score (MOS) increasing from 3.70 to 3.82 and ABX preference exceeding\nby 17% compared with the baseline. In addition, for sentences with multiple\nsyntactic parse trees, prosodic differences can be clearly perceived from the\nsynthesized speeches.", "published": "2020-12-13 05:52:07", "link": "http://arxiv.org/abs/2012.06971v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Iterative Utterance Segmentation for Neural Semantic Parsing", "abstract": "Neural semantic parsers usually fail to parse long and complex utterances\ninto correct meaning representations, due to the lack of exploiting the\nprinciple of compositionality. To address this issue, we present a novel\nframework for boosting neural semantic parsers via iterative utterance\nsegmentation. Given an input utterance, our framework iterates between two\nneural modules: a segmenter for segmenting a span from the utterance, and a\nparser for mapping the span into a partial meaning representation. Then, these\nintermediate parsing results are composed into the final meaning\nrepresentation. One key advantage is that this framework does not require any\nhandcraft templates or additional labeled data for utterance segmentation: we\nachieve this through proposing a novel training method, in which the parser\nprovides pseudo supervision for the segmenter. Experiments on Geo,\nComplexWebQuestions, and Formulas show that our framework can consistently\nimprove performances of neural semantic parsers in different domains. On data\nsplits that require compositional generalization, our framework brings\nsignificant accuracy gains: Geo 63.1 to 81.2, Formulas 59.7 to 72.7,\nComplexWebQuestions 27.1 to 56.3.", "published": "2020-12-13 09:46:24", "link": "http://arxiv.org/abs/2012.07019v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SPARTA: Speaker Profiling for ARabic TAlk", "abstract": "This paper proposes a novel approach to an automatic estimation of three\nspeaker traits from Arabic speech: gender, emotion, and dialect. After showing\npromising results on different text classification tasks, the multi-task\nlearning (MTL) approach is used in this paper for Arabic speech classification\ntasks. The dataset was assembled from six publicly available datasets. First,\nThe datasets were edited and thoroughly divided into train, development, and\ntest sets (open to the public), and a benchmark was set for each task and\ndataset throughout the paper. Then, three different networks were explored:\nLong Short Term Memory (LSTM), Convolutional Neural Network (CNN), and\nFully-Connected Neural Network (FCNN) on five different types of features: two\nraw features (MFCC and MEL) and three pre-trained vectors (i-vectors,\nd-vectors, and x-vectors). LSTM and CNN networks were implemented using raw\nfeatures: MFCC and MEL, where FCNN was explored on the pre-trained vectors\nwhile varying the hyper-parameters of these networks to obtain the best results\nfor each dataset and task. MTL was evaluated against the single task learning\n(STL) approach for the three tasks and six datasets, in which the MTL and\npre-trained vectors almost constantly outperformed STL. All the data and\npre-trained models used in this paper are available and can be acquired by the\npublic.", "published": "2020-12-13 14:45:01", "link": "http://arxiv.org/abs/2012.07073v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mask-Align: Self-Supervised Neural Word Alignment", "abstract": "Word alignment, which aims to align translationally equivalent words between\nsource and target sentences, plays an important role in many natural language\nprocessing tasks. Current unsupervised neural alignment methods focus on\ninducing alignments from neural machine translation models, which does not\nleverage the full context in the target sequence. In this paper, we propose\nMask-Align, a self-supervised word alignment model that takes advantage of the\nfull context on the target side. Our model masks out each target token and\npredicts it conditioned on both source and the remaining target tokens. This\ntwo-step process is based on the assumption that the source token contributing\nmost to recovering the masked target token should be aligned. We also introduce\nan attention variant called leaky attention, which alleviates the problem of\nunexpected high cross-attention weights on special tokens such as periods.\nExperiments on four language pairs show that our model outperforms previous\nunsupervised neural aligners and obtains new state-of-the-art results.", "published": "2020-12-13 21:44:29", "link": "http://arxiv.org/abs/2012.07162v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discriminative Pre-training for Low Resource Title Compression in\n  Conversational Grocery", "abstract": "The ubiquity of smart voice assistants has made conversational shopping\ncommonplace. This is especially true for low consideration segments like\ngrocery. A central problem in conversational grocery is the automatic\ngeneration of short product titles that can be read out fast during a\nconversation. Several supervised models have been proposed in the literature\nthat leverage manually labeled datasets and additional product features to\ngenerate short titles automatically. However, obtaining large amounts of\nlabeled data is expensive and most grocery item pages are not as feature-rich\nas other categories. To address this problem we propose a pre-training based\nsolution that makes use of unlabeled data to learn contextual product\nrepresentations which can then be fine-tuned to obtain better title compression\neven in a low resource setting. We use a self-attentive BiLSTM encoder network\nwith a time distributed softmax layer for the title compression task. We\novercome the vocabulary mismatch problem by using a hybrid embedding layer that\ncombines pre-trained word embeddings with trainable character level\nconvolutions. We pre-train this network as a discriminator on a replaced-token\ndetection task over a large number of unlabeled grocery product titles.\nFinally, we fine tune this network, without any modifications, with a small\nlabeled dataset for the title compression task. Experiments on Walmart's online\ngrocery catalog show our model achieves performance comparable to\nstate-of-the-art models like BERT and XLNet. When fine tuned on all of the\navailable training data our model attains an F1 score of 0.8558 which lags the\nbest performing model, BERT-Base, by 2.78% and XLNet by 0.28% only, while using\n55 times lesser parameters than both. Further, when allowed to fine tune on 5%\nof the training data only, our model outperforms BERT-Base by 24.3% in F1\nscore.", "published": "2020-12-13 02:34:56", "link": "http://arxiv.org/abs/2012.06943v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KVL-BERT: Knowledge Enhanced Visual-and-Linguistic BERT for Visual\n  Commonsense Reasoning", "abstract": "Reasoning is a critical ability towards complete visual understanding. To\ndevelop machine with cognition-level visual understanding and reasoning\nabilities, the visual commonsense reasoning (VCR) task has been introduced. In\nVCR, given a challenging question about an image, a machine must answer\ncorrectly and then provide a rationale justifying its answer. The methods\nadopting the powerful BERT model as the backbone for learning joint\nrepresentation of image content and natural language have shown promising\nimprovements on VCR. However, none of the existing methods have utilized\ncommonsense knowledge in visual commonsense reasoning, which we believe will be\ngreatly helpful in this task. With the support of commonsense knowledge,\ncomplex questions even if the required information is not depicted in the image\ncan be answered with cognitive reasoning. Therefore, we incorporate commonsense\nknowledge into the cross-modal BERT, and propose a novel Knowledge Enhanced\nVisual-and-Linguistic BERT (KVL-BERT for short) model. Besides taking visual\nand linguistic contents as input, external commonsense knowledge extracted from\nConceptNet is integrated into the multi-layer Transformer. In order to reserve\nthe structural information and semantic representation of the original\nsentence, we propose using relative position embedding and mask-self-attention\nto weaken the effect between the injected commonsense knowledge and other\nunrelated components in the input sequence. Compared to other task-specific\nmodels and general task-agnostic pre-training models, our KVL-BERT outperforms\nthem by a large margin.", "published": "2020-12-13 08:22:33", "link": "http://arxiv.org/abs/2012.07000v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "C2C-GenDA: Cluster-to-Cluster Generation for Data Augmentation of Slot\n  Filling", "abstract": "Slot filling, a fundamental module of spoken language understanding, often\nsuffers from insufficient quantity and diversity of training data. To remedy\nthis, we propose a novel Cluster-to-Cluster generation framework for Data\nAugmentation (DA), named C2C-GenDA. It enlarges the training set by\nreconstructing existing utterances into alternative expressions while keeping\nsemantic. Different from previous DA works that reconstruct utterances one by\none independently, C2C-GenDA jointly encodes multiple existing utterances of\nthe same semantics and simultaneously decodes multiple unseen expressions.\nJointly generating multiple new utterances allows to consider the relations\nbetween generated instances and encourages diversity. Besides, encoding\nmultiple existing utterances endows C2C with a wider view of existing\nexpressions, helping to reduce generation that duplicates existing data.\nExperiments on ATIS and Snips datasets show that instances augmented by\nC2C-GenDA improve slot filling by 7.99 (11.9%) and 5.76 (13.6%) F-scores\nrespectively, when there are only hundreds of training utterances.", "published": "2020-12-13 08:35:37", "link": "http://arxiv.org/abs/2012.07004v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Context-Enhanced Entity and Relation Embedding for Knowledge Graph\n  Completion", "abstract": "Most researches for knowledge graph completion learn representations of\nentities and relations to predict missing links in incomplete knowledge graphs.\nHowever, these methods fail to take full advantage of both the contextual\ninformation of entity and relation. Here, we extract contexts of entities and\nrelations from the triplets which they compose. We propose a model named AggrE,\nwhich conducts efficient aggregations respectively on entity context and\nrelation context in multi-hops, and learns context-enhanced entity and relation\nembeddings for knowledge graph completion. The experiment results show that\nAggrE is competitive to existing models.", "published": "2020-12-13 09:20:42", "link": "http://arxiv.org/abs/2012.07011v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Contextual Causality from Time-consecutive Images", "abstract": "Causality knowledge is crucial for many artificial intelligence systems.\nConventional textual-based causality knowledge acquisition methods typically\nrequire laborious and expensive human annotations. As a result, their scale is\noften limited. Moreover, as no context is provided during the annotation, the\nresulting causality knowledge records (e.g., ConceptNet) typically do not take\nthe context into consideration. To explore a more scalable way of acquiring\ncausality knowledge, in this paper, we jump out of the textual domain and\ninvestigate the possibility of learning contextual causality from the visual\nsignal. Compared with pure text-based approaches, learning causality from the\nvisual signal has the following advantages: (1) Causality knowledge belongs to\nthe commonsense knowledge, which is rarely expressed in the text but rich in\nvideos; (2) Most events in the video are naturally time-ordered, which provides\na rich resource for us to mine causality knowledge from; (3) All the objects in\nthe video can be used as context to study the contextual property of causal\nrelations. In detail, we first propose a high-quality dataset Vis-Causal and\nthen conduct experiments to demonstrate that with good language and visual\nrepresentation models as well as enough training signals, it is possible to\nautomatically discover meaningful causal knowledge from the videos. Further\nanalysis also shows that the contextual property of causal relations indeed\nexists, taking which into consideration might be crucial if we want to use the\ncausality knowledge in real applications, and the visual signal could serve as\na good resource for learning such contextual causality.", "published": "2020-12-13 20:24:48", "link": "http://arxiv.org/abs/2012.07138v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Comparative Analysis of Methods for Cloud Segmentation in Ground-Based\n  Infrared Images", "abstract": "The increasing penetration of photovoltaic systems in the power grid makes it\nvulnerable to cloud shadow projection. Real-time cloud segmentation in\nground-based infrared images is important to reduce the noise in intra-hour\nglobal solar irradiance forecasting. We present a comparison between\ndiscriminative and generative models for cloud segmentation. The performances\nof supervised and unsupervised learning methods in cloud segmentation are\nevaluated. The discriminative models are solved in the primal formulation to\nmake them feasible in real-time applications. The performances are compared\nusing the j-statistic. Infrared image preprocessing to remove stationary\nartifacts increases the overall performance in the analyzed methods. The\ninclusion of features from neighboring pixels in the feature vectors leads to a\nperformance improvement in some of the cases. Markov Random Fields achieve the\nbest performance in both unsupervised and supervised generative models.\nDiscriminative models solved in the primal yield a dramatically lower computing\ntime along with high performance in the segmentation. Generative and\ndiscriminative models are comparable when preprocessing is applied to the\ninfrared images.", "published": "2020-12-13 00:41:47", "link": "http://arxiv.org/abs/2012.06930v3", "categories": ["eess.IV", "eess.AS"], "primary_category": "eess.IV"}
{"title": "Self-supervised Text-independent Speaker Verification using Prototypical\n  Momentum Contrastive Learning", "abstract": "In this study, we investigate self-supervised representation learning for\nspeaker verification (SV). First, we examine a simple contrastive learning\napproach (SimCLR) with a momentum contrastive (MoCo) learning framework, where\nthe MoCo speaker embedding system utilizes a queue to maintain a large set of\nnegative examples. We show that better speaker embeddings can be learned by\nmomentum contrastive learning. Next, alternative augmentation strategies are\nexplored to normalize extrinsic speaker variabilities of two random segments\nfrom the same speech utterance. Specifically, augmentation in the waveform\nlargely improves the speaker representations for SV tasks. The proposed MoCo\nspeaker embedding is further improved when a prototypical memory bank is\nintroduced, which encourages the speaker embeddings to be closer to their\nassigned prototypes with an intermediate clustering step. In addition, we\ngeneralize the self-supervised framework to a semi-supervised scenario where\nonly a small portion of the data is labeled. Comprehensive experiments on the\nVoxceleb dataset demonstrate that our proposed self-supervised approach\nachieves competitive performance compared with existing techniques, and can\napproach fully supervised results with partially labeled data.", "published": "2020-12-13 23:23:39", "link": "http://arxiv.org/abs/2012.07178v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
