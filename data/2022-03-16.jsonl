{"title": "Transforming Sequence Tagging Into A Seq2Seq Task", "abstract": "Pretrained, large, generative language models (LMs) have had great success in\na wide range of sequence tagging and structured prediction tasks. Casting a\nsequence tagging task as a Seq2Seq one requires deciding the formats of the\ninput and output sequences. However, we lack a principled understanding of the\ntrade-offs associated with these formats (such as the effect on model accuracy,\nsequence length, multilingual generalization, hallucination). In this paper, we\nrigorously study different formats one could use for casting input text\nsentences and their output labels into the input and target (i.e., output) of a\nSeq2Seq model. Along the way, we introduce a new format, which we show to to be\nboth simpler and more effective. Additionally the new format demonstrates\nsignificant gains in the multilingual settings -- both zero-shot transfer\nlearning and joint training. Lastly, we find that the new format is more robust\nand almost completely devoid of hallucination -- an issue we find common in\nexisting formats. With well over a 1000 experiments studying 14 different\nformats, over 7 diverse public benchmarks -- including 3 multilingual datasets\nspanning 7 languages -- we believe our findings provide a strong empirical\nbasis in understanding how we should tackle sequence tagging tasks.", "published": "2022-03-16 03:48:14", "link": "http://arxiv.org/abs/2203.08378v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Iteratively Prompt Pre-trained Language Models for Chain of Thought", "abstract": "While Pre-trained Language Models (PLMs) internalize a great amount of world\nknowledge, they have been shown incapable of recalling these knowledge to solve\ntasks requiring complex & multi-step reasoning. Similar to how humans develop a\n\"chain of thought\" for these tasks, how can we equip PLMs with such abilities?\nIn this work, we explore an iterative prompting framework, a new prompting\nparadigm which progressively elicits relevant knowledge from PLMs for\nmulti-step inference. We identify key limitations of existing prompting\nmethods, namely they are either restricted to queries with a single\nidentifiable relation/predicate, or being agnostic to input contexts, which\nmakes it difficult to capture variabilities across different inference steps.\nWe propose an iterative context-aware prompter, which addresses these\nlimitations by learning to dynamically synthesize prompts conditioned on the\ncurrent step's contexts. Experiments on three datasets involving multi-step\nreasoning show the effectiveness of the iterative scheme and the context-aware\nprompter design.", "published": "2022-03-16 04:12:20", "link": "http://arxiv.org/abs/2203.08383v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages", "abstract": "While there has been a recent burgeoning of applications at the intersection\nof natural and programming languages, such as code generation and code\nsummarization, these applications are usually English-centric. This creates a\nbarrier for program developers who are not proficient in English. To mitigate\nthis gap in technology development across languages, we propose a multilingual\ndataset, MCoNaLa, to benchmark code generation from natural language commands\nextending beyond English. Modeled off of the methodology from the English\nCode/Natural Language Challenge (CoNaLa) dataset, we annotated a total of 896\nNL-code pairs in three languages: Spanish, Japanese, and Russian. We present a\nquantitative evaluation of performance on the MCoNaLa dataset by testing with\nstate-of-the-art code generation systems. While the difficulties vary across\nthese three languages, all systems lag significantly behind their English\ncounterparts, revealing the challenges in adapting code generation to new\nlanguages.", "published": "2022-03-16 04:21:50", "link": "http://arxiv.org/abs/2203.08388v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging the Data Gap between Training and Inference for Unsupervised\n  Neural Machine Translation", "abstract": "Back-translation is a critical component of Unsupervised Neural Machine\nTranslation (UNMT), which generates pseudo parallel data from target\nmonolingual data. A UNMT model is trained on the pseudo parallel data with\ntranslated source, and translates natural source sentences in inference. The\nsource discrepancy between training and inference hinders the translation\nperformance of UNMT models. By carefully designing experiments, we identify two\nrepresentative characteristics of the data gap in source: (1) style gap (i.e.,\ntranslated vs. natural text style) that leads to poor generalization\ncapability; (2) content gap that induces the model to produce hallucination\ncontent biased towards the target language. To narrow the data gap, we propose\nan online self-training approach, which simultaneously uses the pseudo parallel\ndata {natural source, translated target} to mimic the inference scenario.\nExperimental results on several widely-used language pairs show that our\napproach outperforms two strong baselines (XLM and MASS) by remedying the style\nand content gaps.", "published": "2022-03-16 04:50:27", "link": "http://arxiv.org/abs/2203.08394v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Ability of Multilingual Masked Language Models: A Study of\n  Language Structure", "abstract": "Multilingual pre-trained language models, such as mBERT and XLM-R, have shown\nimpressive cross-lingual ability. Surprisingly, both of them use multilingual\nmasked language model (MLM) without any cross-lingual supervision or aligned\ndata. Despite the encouraging results, we still lack a clear understanding of\nwhy cross-lingual ability could emerge from multilingual MLM. In our work, we\nargue that cross-language ability comes from the commonality between languages.\nSpecifically, we study three language properties: constituent order,\ncomposition and word co-occurrence. First, we create an artificial language by\nmodifying property in source language. Then we study the contribution of\nmodified property through the change of cross-language transfer results on\ntarget language. We conduct experiments on six languages and two cross-lingual\nNLP tasks (textual entailment, sentence retrieval). Our main conclusion is that\nthe contribution of constituent order and word co-occurrence is limited, while\nthe composition is more crucial to the success of cross-linguistic transfer.", "published": "2022-03-16 07:09:35", "link": "http://arxiv.org/abs/2203.08430v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Say What You Don't Know: Improving the Consistency of Abstractive\n  Summarization by Constraining Beam Search", "abstract": "Abstractive summarization systems today produce fluent and relevant output,\nbut often \"hallucinate\" statements not supported by the source text. We analyze\nthe connection between hallucinations and training data, and find evidence that\nmodels hallucinate because they train on target summaries that are unsupported\nby the source. Based on our findings, we present PINOCCHIO, a new decoding\nmethod that improves the consistency of a transformer-based abstractive\nsummarizer by constraining beam search to avoid hallucinations. Given the model\nstates and outputs at a given step, PINOCCHIO detects likely model\nhallucinations based on various measures of attribution to the source text.\nPINOCCHIO backtracks to find more consistent output, and can opt to produce no\nsummary at all when no consistent generation can be found. In experiments, we\nfind that PINOCCHIO improves the consistency of generation (in terms of F1) by\nan average of~67% on two abstractive summarization datasets.", "published": "2022-03-16 07:13:52", "link": "http://arxiv.org/abs/2203.08436v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structurally Diverse Sampling for Sample-Efficient Training and\n  Comprehensive Evaluation", "abstract": "A growing body of research has demonstrated the inability of NLP models to\ngeneralize compositionally and has tried to alleviate it through specialized\narchitectures, training schemes, and data augmentation, among other approaches.\nIn this work, we study a different approach: training on instances with diverse\nstructures. We propose a model-agnostic algorithm for subsampling such sets of\ninstances from a labeled instance pool with structured outputs. Evaluating on\nboth compositional template splits and traditional IID splits of 5 semantic\nparsing datasets of varying complexity, we show that structurally diverse\ntraining using our algorithm leads to comparable or better generalization than\nprior algorithms in 9 out of 10 dataset-split type pairs. In general, we find\nstructural diversity to consistently improve sample efficiency compared to\nrandom train sets. Moreover, we show that structurally diverse sampling yields\ncomprehensive test sets that are a lot more challenging than IID test sets.\nFinally, we provide two explanations for improved generalization from diverse\ntrain sets: 1) improved coverage of output substructures, and 2) a reduction in\nspurious correlations between these substructures.", "published": "2022-03-16 07:41:27", "link": "http://arxiv.org/abs/2203.08445v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KinyaBERT: a Morphology-aware Kinyarwanda Language Model", "abstract": "Pre-trained language models such as BERT have been successful at tackling\nmany natural language processing tasks. However, the unsupervised sub-word\ntokenization methods commonly used in these models (e.g., byte-pair encoding -\nBPE) are sub-optimal at handling morphologically rich languages. Even given a\nmorphological analyzer, naive sequencing of morphemes into a standard BERT\narchitecture is inefficient at capturing morphological compositionality and\nexpressing word-relative syntactic regularities. We address these challenges by\nproposing a simple yet effective two-tier BERT architecture that leverages a\nmorphological analyzer and explicitly represents morphological\ncompositionality. Despite the success of BERT, most of its evaluations have\nbeen conducted on high-resource languages, obscuring its applicability on\nlow-resource languages. We evaluate our proposed method on the low-resource\nmorphologically rich Kinyarwanda language, naming the proposed model\narchitecture KinyaBERT. A robust set of experimental results reveal that\nKinyaBERT outperforms solid baselines by 2% in F1 score on a named entity\nrecognition task and by 4.3% in average score of a machine-translated GLUE\nbenchmark. KinyaBERT fine-tuning has better convergence and achieves more\nrobust results on multiple tasks even in the presence of translation noise.", "published": "2022-03-16 08:36:14", "link": "http://arxiv.org/abs/2203.08459v2", "categories": ["cs.CL", "I.2.7; I.2"], "primary_category": "cs.CL"}
{"title": "HeterMPC: A Heterogeneous Graph Neural Network for Response Generation\n  in Multi-Party Conversations", "abstract": "Recently, various response generation models for two-party conversations have\nachieved impressive improvements, but less effort has been paid to multi-party\nconversations (MPCs) which are more practical and complicated. Compared with a\ntwo-party conversation where a dialogue context is a sequence of utterances,\nbuilding a response generation model for MPCs is more challenging, since there\nexist complicated context structures and the generated responses heavily rely\non both interlocutors (i.e., speaker and addressee) and history utterances. To\naddress these challenges, we present HeterMPC, a heterogeneous graph-based\nneural network for response generation in MPCs which models the semantics of\nutterances and interlocutors simultaneously with two types of nodes in a graph.\nBesides, we also design six types of meta relations with\nnode-edge-type-dependent parameters to characterize the heterogeneous\ninteractions within the graph. Through multi-hop updating, HeterMPC can\nadequately utilize the structural knowledge of conversations for response\ngeneration. Experimental results on the Ubuntu Internet Relay Chat (IRC)\nchannel benchmark show that HeterMPC outperforms various baseline models for\nresponse generation in MPCs.", "published": "2022-03-16 09:50:32", "link": "http://arxiv.org/abs/2203.08500v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ConTinTin: Continual Learning from Task Instructions", "abstract": "The mainstream machine learning paradigms for NLP often work with two\nunderlying presumptions. First, the target task is predefined and static; a\nsystem merely needs to learn to solve it exclusively. Second, the supervision\nof a task mainly comes from a set of labeled examples. A question arises: how\nto build a system that can keep learning new tasks from their instructions?\nThis work defines a new learning paradigm ConTinTin (Continual Learning from\nTask Instructions), in which a system should learn a sequence of new tasks one\nby one, each task is explained by a piece of textual instruction. The system is\nrequired to (i) generate the expected outputs of a new task by learning from\nits instruction, (ii) transfer the knowledge acquired from upstream tasks to\nhelp solve downstream tasks (i.e., forward-transfer), and (iii) retain or even\nimprove the performance on earlier tasks after learning new tasks (i.e.,\nbackward-transfer). This new problem is studied on a stream of more than 60\ntasks, each equipped with an instruction. Technically, our method\nInstructionSpeak contains two strategies that make full use of task\ninstructions to improve forward-transfer and backward-transfer: one is to learn\nfrom negative outputs, the other is to re-visit instructions of previous tasks.\nTo our knowledge, this is the first time to study ConTinTin in NLP. In addition\nto the problem formulation and our promising approach, this work also\ncontributes to providing rich analyses for the community to better understand\nthis novel learning problem.", "published": "2022-03-16 10:27:18", "link": "http://arxiv.org/abs/2203.08512v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Morphological Reinflection with Multiple Arguments: An Extended\n  Annotation schema and a Georgian Case Study", "abstract": "In recent years, a flurry of morphological datasets had emerged, most notably\nUniMorph, a multi-lingual repository of inflection tables. However, the flat\nstructure of the current morphological annotation schema makes the treatment of\nsome languages quirky, if not impossible, specifically in cases of polypersonal\nagreement, where verbs agree with multiple arguments using true affixes. In\nthis paper, we propose to address this phenomenon by expanding the UniMorph\nannotation schema to a hierarchical feature structure that naturally\naccommodates complex argument marking. We apply this extended schema to one\nsuch language, Georgian, and provide a human-verified, accurate and balanced\nmorphological dataset for Georgian verbs. The dataset has 4 times more tables\nand 6 times more verb forms compared to the existing UniMorph dataset, covering\nall possible variants of argument marking, demonstrating the adequacy of our\nproposed scheme. Experiments with a standard reinflection model show that\ngeneralization is easy when the data is split at the form level, but extremely\nhard when splitting along lemma lines. Expanding the other languages in\nUniMorph to this schema is expected to improve both the coverage, consistency\nand interpretability of this benchmark.", "published": "2022-03-16 10:47:29", "link": "http://arxiv.org/abs/2203.08527v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Pre-training with Language and Task Adaptation for\n  Multilingual Text Style Transfer", "abstract": "We exploit the pre-trained seq2seq model mBART for multilingual text style\ntransfer. Using machine translated data as well as gold aligned English\nsentences yields state-of-the-art results in the three target languages we\nconsider. Besides, in view of the general scarcity of parallel data, we propose\na modular approach for multilingual formality transfer, which consists of two\ntraining strategies that target adaptation to both language and task. Our\napproach achieves competitive performance without monolingual task-specific\nparallel data and can be applied to other style transfer tasks as well as to\nother languages.", "published": "2022-03-16 11:27:48", "link": "http://arxiv.org/abs/2203.08552v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Dependency Parsing with Worst-Case Aware Automated Curriculum\n  Learning", "abstract": "Large multilingual pretrained language models such as mBERT and XLM-RoBERTa\nhave been found to be surprisingly effective for cross-lingual transfer of\nsyntactic parsing models (Wu and Dredze 2019), but only between related\nlanguages. However, source and training languages are rarely related, when\nparsing truly low-resource languages. To close this gap, we adopt a method from\nmulti-task learning, which relies on automated curriculum learning, to\ndynamically optimize for parsing performance on outlier languages. We show that\nthis approach is significantly better than uniform and size-proportional\nsampling in the zero-shot setting.", "published": "2022-03-16 11:33:20", "link": "http://arxiv.org/abs/2203.08555v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Geographic Adaptation of Pretrained Language Models", "abstract": "While pretrained language models (PLMs) have been shown to possess a plethora\nof linguistic knowledge, the existing body of research has largely neglected\nextralinguistic knowledge, which is generally difficult to obtain by\npretraining on text alone. Here, we contribute to closing this gap by examining\ngeolinguistic knowledge, i.e., knowledge about geographic variation in\nlanguage. We introduce geoadaptation, an intermediate training step that\ncouples language modeling with geolocation prediction in a multi-task learning\nsetup. We geoadapt four PLMs, covering language groups from three geographic\nareas, and evaluate them on five different tasks: fine-tuned (i.e., supervised)\ngeolocation prediction, zero-shot (i.e., unsupervised) geolocation prediction,\nfine-tuned language identification, zero-shot language identification, and\nzero-shot prediction of dialect features. Geoadaptation is very successful at\ninjecting geolinguistic knowledge into the PLMs: the geoadapted PLMs\nconsistently outperform PLMs adapted using only language modeling (by\nespecially wide margins on zero-shot prediction tasks), and we obtain new\nstate-of-the-art results on two benchmarks for geolocation prediction and\nlanguage identification. Furthermore, we show that the effectiveness of\ngeoadaptation stems from its ability to geographically retrofit the\nrepresentation space of the PLMs.", "published": "2022-03-16 11:55:00", "link": "http://arxiv.org/abs/2203.08565v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-Context Learning for Few-Shot Dialogue State Tracking", "abstract": "Collecting and annotating task-oriented dialogues is time-consuming and\ncostly; thus, zero and few shot learning could greatly benefit dialogue state\ntracking (DST). In this work, we propose an in-context learning (ICL) framework\nfor zero-shot and few-shot learning DST, where a large pre-trained language\nmodel (LM) takes a test instance and a few exemplars as input, and directly\ndecodes the dialogue state without any parameter updates. To better leverage a\ntabular domain description in the LM prompt, we reformulate DST into a\ntext-to-SQL problem. We also propose a novel approach to retrieve annotated\ndialogues as exemplars. Empirical results on MultiWOZ show that our method\nIC-DST substantially outperforms previous fine-tuned state-of-the-art models in\nfew-shot settings. In addition, we test IC-DST in zero-shot settings, in which\nthe model only takes a fixed task instruction as input, finding that it\noutperforms previous zero-shot methods by a large margin.", "published": "2022-03-16 11:58:24", "link": "http://arxiv.org/abs/2203.08568v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph Neural Networks for Multiparallel Word Alignment", "abstract": "After a period of decrease, interest in word alignments is increasing again\nfor their usefulness in domains such as typological research, cross-lingual\nannotation projection, and machine translation. Generally, alignment algorithms\nonly use bitext and do not make use of the fact that many parallel corpora are\nmultiparallel. Here, we compute high-quality word alignments between multiple\nlanguage pairs by considering all language pairs together. First, we create a\nmultiparallel word alignment graph, joining all bilingual word alignment pairs\nin one graph. Next, we use graph neural networks (GNNs) to exploit the graph\nstructure. Our GNN approach (i) utilizes information about the meaning,\nposition, and language of the input words, (ii) incorporates information from\nmultiple parallel sentences, (iii) adds and removes edges from the initial\nalignments, and (iv) yields a prediction model that can generalize beyond the\ntraining sentences. We show that community detection provides valuable\ninformation for multiparallel word alignment. Our method outperforms previous\nwork on three word-alignment datasets and on a downstream task.", "published": "2022-03-16 14:41:35", "link": "http://arxiv.org/abs/2203.08654v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CUE Vectors: Modular Training of Language Models Conditioned on Diverse\n  Contextual Signals", "abstract": "We propose a framework to modularize the training of neural language models\nthat use diverse forms of sentence-external context (including metadata) by\neliminating the need to jointly train sentence-external and within-sentence\nencoders. Our approach, contextual universal embeddings (CUE), trains LMs on\none set of context, such as date and author, and adapts to novel metadata\ntypes, such as article title, or previous sentence. The model consists of a\npretrained neural sentence LM, a BERT-based context encoder, and a masked\ntransformer decoder that estimates LM probabilities using sentence-internal and\nsentence-external information. When context or metadata are unavailable, our\nmodel learns to combine contextual and sentence-internal information using\nnoisy oracle unigram embeddings as a proxy. Real contextual information can be\nintroduced later and used to adapt a small number of parameters that map\ncontextual data into the decoder's embedding space. We validate the CUE\nframework on a NYTimes text corpus with multiple metadata types, for which the\nLM perplexity can be lowered from 36.6 to 27.4 by conditioning on context.\nBootstrapping a contextual LM with only a subset of the context/metadata during\ntraining retains 85\\% of the achievable gain. Training the model initially with\nproxy context retains 67% of the perplexity gain after adapting to real\ncontext. Furthermore, we can swap one type of pretrained sentence LM for\nanother without retraining the context encoders, by only adapting the decoder\nmodel. Overall, we obtain a modular framework that allows incremental, scalable\ntraining of context-enhanced LMs.", "published": "2022-03-16 17:37:28", "link": "http://arxiv.org/abs/2203.08774v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-Trained Multilingual Sequence-to-Sequence Models: A Hope for\n  Low-Resource Language Translation?", "abstract": "What can pre-trained multilingual sequence-to-sequence models like mBART\ncontribute to translating low-resource languages? We conduct a thorough\nempirical experiment in 10 languages to ascertain this, considering five\nfactors: (1) the amount of fine-tuning data, (2) the noise in the fine-tuning\ndata, (3) the amount of pre-training data in the model, (4) the impact of\ndomain mismatch, and (5) language typology. In addition to yielding several\nheuristics, the experiments form a framework for evaluating the data\nsensitivities of machine translation systems. While mBART is robust to domain\ndifferences, its translations for unseen and typologically distant languages\nremain below 3.0 BLEU. In answer to our title's question, mBART is not a\nlow-resource panacea; we therefore encourage shifting the emphasis from new\nmodels to new data.", "published": "2022-03-16 18:15:17", "link": "http://arxiv.org/abs/2203.08850v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Morphological Processing of Low-Resource Languages: Where We Are and\n  What's Next", "abstract": "Automatic morphological processing can aid downstream natural language\nprocessing applications, especially for low-resource languages, and assist\nlanguage documentation efforts for endangered languages. Having long been\nmultilingual, the field of computational morphology is increasingly moving\ntowards approaches suitable for languages with minimal or no annotated\nresources. First, we survey recent developments in computational morphology\nwith a focus on low-resource languages. Second, we argue that the field is\nready to tackle the logical next challenge: understanding a language's\nmorphology from raw text alone. We perform an empirical study on a truly\nunsupervised version of the paradigm completion task and show that, while\nexisting state-of-the-art models bridged by two newly proposed models we devise\nperform reasonably, there is still much room for improvement. The stakes are\nhigh: solving this task will increase the language coverage of morphological\nresources by a number of magnitudes.", "published": "2022-03-16 19:47:04", "link": "http://arxiv.org/abs/2203.08909v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Synthetic Question Value Estimation for Domain Adaptation of Question\n  Answering", "abstract": "Synthesizing QA pairs with a question generator (QG) on the target domain has\nbecome a popular approach for domain adaptation of question answering (QA)\nmodels. Since synthetic questions are often noisy in practice, existing work\nadapts scores from a pretrained QA (or QG) model as criteria to select\nhigh-quality questions. However, these scores do not directly serve the\nultimate goal of improving QA performance on the target domain. In this paper,\nwe introduce a novel idea of training a question value estimator (QVE) that\ndirectly estimates the usefulness of synthetic questions for improving the\ntarget-domain QA performance. By conducting comprehensive experiments, we show\nthat the synthetic questions selected by QVE can help achieve better\ntarget-domain QA performance, in comparison with existing techniques. We\nadditionally show that by using such questions and only around 15% of the human\nannotations on the target domain, we can achieve comparable performance to the\nfully-supervised baselines.", "published": "2022-03-16 20:22:31", "link": "http://arxiv.org/abs/2203.08926v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "C-MORE: Pretraining to Answer Open-Domain Questions by Consulting\n  Millions of References", "abstract": "We consider the problem of pretraining a two-stage open-domain question\nanswering (QA) system (retriever + reader) with strong transfer capabilities.\nThe key challenge is how to construct a large amount of high-quality\nquestion-answer-context triplets without task-specific annotations.\nSpecifically, the triplets should align well with downstream tasks by: (i)\ncovering a wide range of domains (for open-domain applications), (ii) linking a\nquestion to its semantically relevant context with supporting evidence (for\ntraining the retriever), and (iii) identifying the correct answer in the\ncontext (for training the reader). Previous pretraining approaches generally\nfall short of one or more of these requirements. In this work, we automatically\nconstruct a large-scale corpus that meets all three criteria by consulting\nmillions of references cited within Wikipedia. The well-aligned pretraining\nsignals benefit both the retriever and the reader significantly. Our pretrained\nretriever leads to 2%-10% absolute gains in top-20 accuracy. And with our\npretrained reader, the entire system improves by up to 4% in exact match.", "published": "2022-03-16 20:30:05", "link": "http://arxiv.org/abs/2203.08928v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Analysis of Negation in Natural Language Understanding Corpora", "abstract": "This paper analyzes negation in eight popular corpora spanning six natural\nlanguage understanding tasks. We show that these corpora have few negations\ncompared to general-purpose English, and that the few negations in them are\noften unimportant. Indeed, one can often ignore negations and still make the\nright predictions. Additionally, experimental results show that\nstate-of-the-art transformers trained with these corpora obtain substantially\nworse results with instances that contain negation, especially if the negations\nare important. We conclude that new corpora accounting for negation are needed\nto solve natural language understanding tasks when negation is present.", "published": "2022-03-16 20:31:53", "link": "http://arxiv.org/abs/2203.08929v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speaker Information Can Guide Models to Better Inductive Biases: A Case\n  Study On Predicting Code-Switching", "abstract": "Natural language processing (NLP) models trained on people-generated data can\nbe unreliable because, without any constraints, they can learn from spurious\ncorrelations that are not relevant to the task. We hypothesize that enriching\nmodels with speaker information in a controlled, educated way can guide them to\npick up on relevant inductive biases. For the speaker-driven task of predicting\ncode-switching points in English--Spanish bilingual dialogues, we show that\nadding sociolinguistically-grounded speaker features as prepended prompts\nsignificantly improves accuracy. We find that by adding influential phrases to\nthe input, speaker-informed models learn useful and explainable linguistic\ninformation. To our knowledge, we are the first to incorporate speaker\ncharacteristics in a neural model for code-switching, and more generally, take\na step towards developing transparent, personalized models that use speaker\ninformation in a controlled way.", "published": "2022-03-16 22:56:58", "link": "http://arxiv.org/abs/2203.08979v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Label Semantics for Few Shot Named Entity Recognition", "abstract": "We study the problem of few shot learning for named entity recognition.\nSpecifically, we leverage the semantic information in the names of the labels\nas a way of giving the model additional signal and enriched priors. We propose\na neural architecture that consists of two BERT encoders, one to encode the\ndocument and its tokens and another one to encode each of the labels in natural\nlanguage format. Our model learns to match the representations of named\nentities computed by the first encoder with label representations computed by\nthe second encoder. The label semantics signal is shown to support improved\nstate-of-the-art results in multiple few shot NER benchmarks and on-par\nperformance in standard benchmarks. Our model is especially effective in low\nresource settings.", "published": "2022-03-16 23:21:05", "link": "http://arxiv.org/abs/2203.08985v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AdapLeR: Speeding up Inference by Adaptive Length Reduction", "abstract": "Pre-trained language models have shown stellar performance in various\ndownstream tasks. But, this usually comes at the cost of high latency and\ncomputation, hindering their usage in resource-limited settings. In this work,\nwe propose a novel approach for reducing the computational cost of BERT with\nminimal loss in downstream performance. Our method dynamically eliminates less\ncontributing tokens through layers, resulting in shorter lengths and\nconsequently lower computational cost. To determine the importance of each\ntoken representation, we train a Contribution Predictor for each layer using a\ngradient-based saliency method. Our experiments on several diverse\nclassification tasks show speedups up to 22x during inference time without much\nsacrifice in performance. We also validate the quality of the selected tokens\nin our method using human annotations in the ERASER benchmark. In comparison to\nother widely used strategies for selecting important tokens, such as saliency\nand attention, our proposed method has a significantly lower false positive\nrate in generating rationales. Our code is freely available at\nhttps://github.com/amodaresi/AdapLeR .", "published": "2022-03-16 23:41:38", "link": "http://arxiv.org/abs/2203.08991v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Continuous Detection, Rapidly React: Unseen Rumors Detection based on\n  Continual Prompt-Tuning", "abstract": "Since open social platforms allow for a large and continuous flow of\nunverified information, rumors can emerge unexpectedly and spread quickly.\nHowever, existing rumor detection (RD) models often assume the same training\nand testing distributions and can not cope with the continuously changing\nsocial network environment. This paper proposed a Continual Prompt-Tuning RD\n(CPT-RD) framework, which avoids catastrophic forgetting (CF) of upstream tasks\nduring sequential task learning and enables bidirectional knowledge transfer\nbetween domain tasks. Specifically, we propose the following strategies: (a)\nOur design explicitly decouples shared and domain-specific knowledge, thus\nreducing the interference among different domains during optimization; (b)\nSeveral technologies aim to transfer knowledge of upstream tasks to deal with\nemergencies; (c) A task-conditioned prompt-wise hypernetwork (TPHNet) is used\nto consolidate past domains. In addition, CPT-RD avoids CF without the\nnecessity of a rehearsal buffer.", "published": "2022-03-16 09:38:50", "link": "http://arxiv.org/abs/2203.11720v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Afrocentric NLP for African Languages: Where We Are and Where We\n  Can Go", "abstract": "Aligning with ACL 2022 special Theme on \"Language Diversity: from Low\nResource to Endangered Languages\", we discuss the major linguistic and\nsociopolitical challenges facing development of NLP technologies for African\nlanguages. Situating African languages in a typological framework, we discuss\nhow the particulars of these languages can be harnessed. To facilitate future\nresearch, we also highlight current efforts, communities, venues, datasets, and\ntools. Our main objective is to motivate and advocate for an Afrocentric\napproach to technology development. With this in mind, we recommend\n\\textit{what} technologies to build and \\textit{how} to build, evaluate, and\ndeploy them based on the needs of local African communities.", "published": "2022-03-16 02:14:57", "link": "http://arxiv.org/abs/2203.08351v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Spot the Difference: A Cooperative Object-Referring Game in\n  Non-Perfectly Co-Observable Scene", "abstract": "Visual dialog has witnessed great progress after introducing various\nvision-oriented goals into the conversation, especially such as GuessWhich and\nGuessWhat, where the only image is visible by either and both of the questioner\nand the answerer, respectively. Researchers explore more on visual dialog tasks\nin such kind of single- or perfectly co-observable visual scene, while somewhat\nneglect the exploration on tasks of non perfectly co-observable visual scene,\nwhere the images accessed by two agents may not be exactly the same, often\noccurred in practice. Although building common ground in non-perfectly\nco-observable visual scene through conversation is significant for advanced\ndialog agents, the lack of such dialog task and corresponding large-scale\ndataset makes it impossible to carry out in-depth research. To break this\nlimitation, we propose an object-referring game in non-perfectly co-observable\nvisual scene, where the goal is to spot the difference between the similar\nvisual scenes through conversing in natural language. The task addresses\nchallenges of the dialog strategy in non-perfectly co-observable visual scene\nand the ability of categorizing objects. Correspondingly, we construct a\nlarge-scale multimodal dataset, named SpotDiff, which contains 87k Virtual\nReality images and 97k dialogs generated by self-play. Finally, we give\nbenchmark models for this task, and conduct extensive experiments to evaluate\nits performance as well as analyze its main challenges.", "published": "2022-03-16 02:55:33", "link": "http://arxiv.org/abs/2203.08362v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Multi-View Document Representation Learning for Open-Domain Dense\n  Retrieval", "abstract": "Dense retrieval has achieved impressive advances in first-stage retrieval\nfrom a large-scale document collection, which is built on bi-encoder\narchitecture to produce single vector representation of query and document.\nHowever, a document can usually answer multiple potential queries from\ndifferent views. So the single vector representation of a document is hard to\nmatch with multi-view queries, and faces a semantic mismatch problem. This\npaper proposes a multi-view document representation learning framework, aiming\nto produce multi-view embeddings to represent documents and enforce them to\nalign with different queries. First, we propose a simple yet effective method\nof generating multiple embeddings through viewers. Second, to prevent\nmulti-view embeddings from collapsing to the same one, we further propose a\nglobal-local loss with annealed temperature to encourage the multiple viewers\nto better align with different potential queries. Experiments show our method\noutperforms recent works and achieves state-of-the-art results.", "published": "2022-03-16 03:36:38", "link": "http://arxiv.org/abs/2203.08372v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again", "abstract": "The strong few-shot in-context learning capability of large pre-trained\nlanguage models (PLMs) such as GPT-3 is highly appealing for application\ndomains such as biomedicine, which feature high and diverse demands of language\ntechnologies but also high data annotation costs. In this paper, we present the\nfirst systematic and comprehensive study to compare the few-shot performance of\nGPT-3 in-context learning with fine-tuning smaller (i.e., BERT-sized) PLMs on\ntwo highly representative biomedical information extraction tasks, named entity\nrecognition and relation extraction. We follow the true few-shot setting to\navoid overestimating models' few-shot performance by model selection over a\nlarge validation set. We also optimize GPT-3's performance with known\ntechniques such as contextual calibration and dynamic in-context example\nretrieval. However, our results show that GPT-3 still significantly\nunderperforms compared to simply fine-tuning a smaller PLM. In addition, GPT-3\nin-context learning also yields smaller gains in accuracy when more training\ndata becomes available. Our in-depth analyses further reveal issues of the\nin-context learning setting that may be detrimental to information extraction\ntasks in general. Given the high cost of experimenting with GPT-3, we hope our\nstudy provides guidance for biomedical researchers and practitioners towards\nmore promising directions such as fine-tuning small PLMs.", "published": "2022-03-16 05:56:08", "link": "http://arxiv.org/abs/2203.08410v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Understanding and Improving Sequence-to-Sequence Pretraining for Neural\n  Machine Translation", "abstract": "In this paper, we present a substantial step in better understanding the SOTA\nsequence-to-sequence (Seq2Seq) pretraining for neural machine\ntranslation~(NMT). We focus on studying the impact of the jointly pretrained\ndecoder, which is the main difference between Seq2Seq pretraining and previous\nencoder-based pretraining approaches for NMT. By carefully designing\nexperiments on three language pairs, we find that Seq2Seq pretraining is a\ndouble-edged sword: On one hand, it helps NMT models to produce more diverse\ntranslations and reduce adequacy-related translation errors. On the other hand,\nthe discrepancies between Seq2Seq pretraining and NMT finetuning limit the\ntranslation quality (i.e., domain discrepancy) and induce the over-estimation\nissue (i.e., objective discrepancy). Based on these observations, we further\npropose simple and effective strategies, named in-domain pretraining and input\nadaptation to remedy the domain and objective discrepancies, respectively.\nExperimental results on several language pairs show that our approach can\nconsistently improve both translation performance and model robustness upon\nSeq2Seq pretraining.", "published": "2022-03-16 07:36:28", "link": "http://arxiv.org/abs/2203.08442v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can Pre-trained Language Models Interpret Similes as Smart as Human?", "abstract": "Simile interpretation is a crucial task in natural language processing.\nNowadays, pre-trained language models (PLMs) have achieved state-of-the-art\nperformance on many tasks. However, it remains under-explored whether PLMs can\ninterpret similes or not. In this paper, we investigate the ability of PLMs in\nsimile interpretation by designing a novel task named Simile Property Probing,\ni.e., to let the PLMs infer the shared properties of similes. We construct our\nsimile property probing datasets from both general textual corpora and\nhuman-designed questions, containing 1,633 examples covering seven main\ncategories. Our empirical study based on the constructed datasets shows that\nPLMs can infer similes' shared properties while still underperforming humans.\nTo bridge the gap with human performance, we additionally design a\nknowledge-enhanced training objective by incorporating the simile knowledge\ninto PLMs via knowledge embedding methods. Our method results in a gain of\n8.58% in the probing task and 1.37% in the downstream task of sentiment\nclassification. The datasets and code are publicly available at\nhttps://github.com/Abbey4799/PLMs-Interpret-Simile.", "published": "2022-03-16 07:57:34", "link": "http://arxiv.org/abs/2203.08452v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "E-KAR: A Benchmark for Rationalizing Natural Language Analogical\n  Reasoning", "abstract": "The ability to recognize analogies is fundamental to human cognition.\nExisting benchmarks to test word analogy do not reveal the underneath process\nof analogical reasoning of neural models. Holding the belief that models\ncapable of reasoning should be right for the right reasons, we propose a\nfirst-of-its-kind Explainable Knowledge-intensive Analogical Reasoning\nbenchmark (E-KAR). Our benchmark consists of 1,655 (in Chinese) and 1,251 (in\nEnglish) problems sourced from the Civil Service Exams, which require intensive\nbackground knowledge to solve. More importantly, we design a free-text\nexplanation scheme to explain whether an analogy should be drawn, and manually\nannotate them for each and every question and candidate answer. Empirical\nresults suggest that this benchmark is very challenging for some\nstate-of-the-art models for both explanation generation and analogical question\nanswering tasks, which invites further research in this area.", "published": "2022-03-16 09:16:38", "link": "http://arxiv.org/abs/2203.08480v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TegTok: Augmenting Text Generation via Task-specific and Open-world\n  Knowledge", "abstract": "Generating natural and informative texts has been a long-standing problem in\nNLP. Much effort has been dedicated into incorporating pre-trained language\nmodels (PLMs) with various open-world knowledge, such as knowledge graphs or\nwiki pages. However, their ability to access and manipulate the task-specific\nknowledge is still limited on downstream tasks, as this type of knowledge is\nusually not well covered in PLMs and is hard to acquire. To address the\nproblem, we propose augmenting TExt Generation via Task-specific and Open-world\nKnowledge (TegTok) in a unified framework. Our model selects knowledge entries\nfrom two types of knowledge sources through dense retrieval and then injects\nthem into the input encoding and output decoding stages respectively on the\nbasis of PLMs. With the help of these two types of knowledge, our model can\nlearn what and how to generate. Experiments on two text generation tasks of\ndialogue generation and question generation, and on two datasets show that our\nmethod achieves better performance than various baseline models.", "published": "2022-03-16 10:37:59", "link": "http://arxiv.org/abs/2203.08517v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LEVEN: A Large-Scale Chinese Legal Event Detection Dataset", "abstract": "Recognizing facts is the most fundamental step in making judgments, hence\ndetecting events in the legal documents is important to legal case analysis\ntasks. However, existing Legal Event Detection (LED) datasets only concern\nincomprehensive event types and have limited annotated data, which restricts\nthe development of LED methods and their downstream applications. To alleviate\nthese issues, we present LEVEN a large-scale Chinese LEgal eVENt detection\ndataset, with 8,116 legal documents and 150,977 human-annotated event mentions\nin 108 event types. Not only charge-related events, LEVEN also covers general\nevents, which are critical for legal case understanding but neglected in\nexisting LED datasets. To our knowledge, LEVEN is the largest LED dataset and\nhas dozens of times the data scale of others, which shall significantly promote\nthe training and evaluation of LED methods. The results of extensive\nexperiments indicate that LED is challenging and needs further effort.\nMoreover, we simply utilize legal events as side information to promote\ndownstream applications. The method achieves improvements of average 2.2 points\nprecision in low-resource judgment prediction, and 1.5 points mean average\nprecision in unsupervised case retrieval, which suggests the fundamentality of\nLED. The source code and dataset can be obtained from\nhttps://github.com/thunlp/LEVEN.", "published": "2022-03-16 11:40:02", "link": "http://arxiv.org/abs/2203.08556v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Turning Stocks into Memes: A Dataset for Understanding How Social\n  Communities Can Drive Wall Street", "abstract": "Who actually expresses an intent to buy GameStop shares on Reddit? What\nconvinces people to buy stocks? Are people convinced to support a coordinated\nplan to adversely impact Wall Street investors? Existing literature on\nunderstanding intent has mainly relied on surveys and self reporting; however\nthere are limitations to these methodologies. Hence, in this paper, we develop\nan annotated dataset of communications centered on the GameStop phenomenon to\nanalyze the subscriber intentions behaviors within the r/WallStreetBets\ncommunity to buy (or not buy) stocks. Likewise, we curate a dataset to better\nunderstand how intent interacts with a user's general support towards the\ncoordinated actions of the community for GameStop. Overall, our dataset can\nprovide insight to social scientists on the persuasive power to buy into social\nmovements online by adopting common language and narrative. WARNING: This paper\ncontains offensive language that commonly appears on Reddit's r/WallStreetBets\nsubreddit.", "published": "2022-03-16 15:34:10", "link": "http://arxiv.org/abs/2203.08694v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Multi-Stage Prompting for Knowledgeable Dialogue Generation", "abstract": "Existing knowledge-grounded dialogue systems typically use finetuned versions\nof a pretrained language model (LM) and large-scale knowledge bases. These\nmodels typically fail to generalize on topics outside of the knowledge base,\nand require maintaining separate potentially large checkpoints each time\nfinetuning is needed. In this paper, we aim to address these limitations by\nleveraging the inherent knowledge stored in the pretrained LM as well as its\npowerful generation ability. We propose a multi-stage prompting approach to\ngenerate knowledgeable responses from a single pretrained LM. We first prompt\nthe LM to generate knowledge based on the dialogue context. Then, we further\nprompt it to generate responses based on the dialogue context and the\npreviously generated knowledge. Results show that our knowledge generator\noutperforms the state-of-the-art retrieval-based model by 5.8% when combining\nknowledge relevance and correctness. In addition, our multi-stage prompting\noutperforms the finetuning-based dialogue model in terms of response\nknowledgeability and engagement by up to 10% and 5%, respectively. Furthermore,\nwe scale our model up to 530 billion parameters and show that larger LMs\nimprove the generation correctness score by up to 10%, and response relevance,\nknowledgeability and engagement by up to 10%. Our code is available at:\nhttps://github.com/NVIDIA/Megatron-LM.", "published": "2022-03-16 16:53:43", "link": "http://arxiv.org/abs/2203.08745v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sample, Translate, Recombine: Leveraging Audio Alignments for Data\n  Augmentation in End-to-end Speech Translation", "abstract": "End-to-end speech translation relies on data that pair source-language speech\ninputs with corresponding translations into a target language. Such data are\nnotoriously scarce, making synthetic data augmentation by back-translation or\nknowledge distillation a necessary ingredient of end-to-end training. In this\npaper, we present a novel approach to data augmentation that leverages audio\nalignments, linguistic properties, and translation. First, we augment a\ntranscription by sampling from a suffix memory that stores text and audio data.\nSecond, we translate the augmented transcript. Finally, we recombine\nconcatenated audio segments and the generated translation. Besides training an\nMT-system, we only use basic off-the-shelf components without fine-tuning.\nWhile having similar resource demands as knowledge distillation, adding our\nmethod delivers consistent improvements of up to 0.9 and 1.1 BLEU points on\nfive language pairs on CoVoST 2 and on two language pairs on Europarl-ST,\nrespectively.", "published": "2022-03-16 17:15:46", "link": "http://arxiv.org/abs/2203.08757v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Creating Multimedia Summaries Using Tweets and Videos", "abstract": "While popular televised events such as presidential debates or TV shows are\nairing, people provide commentary on them in real-time. In this paper, we\npropose a simple yet effective approach to combine social media commentary and\nvideos to create a multimedia summary of televised events. Our approach\nidentifies scenes from these events based on spikes of mentions of people\ninvolved in the event and automatically selects tweets and frames from the\nvideos that occur during the time period of the spike that talk about and show\nthe people being discussed.", "published": "2022-03-16 20:37:49", "link": "http://arxiv.org/abs/2203.08931v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "BPE vs. Morphological Segmentation: A Case Study on Machine Translation\n  of Four Polysynthetic Languages", "abstract": "Morphologically-rich polysynthetic languages present a challenge for NLP\nsystems due to data sparsity, and a common strategy to handle this issue is to\napply subword segmentation. We investigate a wide variety of supervised and\nunsupervised morphological segmentation methods for four polysynthetic\nlanguages: Nahuatl, Raramuri, Shipibo-Konibo, and Wixarika. Then, we compare\nthe morphologically inspired segmentation methods against Byte-Pair Encodings\n(BPEs) as inputs for machine translation (MT) when translating to and from\nSpanish. We show that for all language pairs except for Nahuatl, an\nunsupervised morphological segmentation algorithm outperforms BPEs consistently\nand that, although supervised methods achieve better segmentation scores, they\nunder-perform in MT challenges. Finally, we contribute two new morphological\nsegmentation datasets for Raramuri and Shipibo-Konibo, and a parallel corpus\nfor Raramuri--Spanish.", "published": "2022-03-16 21:27:20", "link": "http://arxiv.org/abs/2203.08954v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FormNet: Structural Encoding beyond Sequential Modeling in Form Document\n  Information Extraction", "abstract": "Sequence modeling has demonstrated state-of-the-art performance on natural\nlanguage and document understanding tasks. However, it is challenging to\ncorrectly serialize tokens in form-like documents in practice due to their\nvariety of layout patterns. We propose FormNet, a structure-aware sequence\nmodel to mitigate the suboptimal serialization of forms. First, we design Rich\nAttention that leverages the spatial relationship between tokens in a form for\nmore precise attention score calculation. Second, we construct Super-Tokens for\neach word by embedding representations from their neighboring tokens through\ngraph convolutions. FormNet therefore explicitly recovers local syntactic\ninformation that may have been lost during serialization. In experiments,\nFormNet outperforms existing methods with a more compact model size and less\npre-training data, establishing new state-of-the-art performance on CORD, FUNSD\nand Payment benchmarks.", "published": "2022-03-16 06:02:02", "link": "http://arxiv.org/abs/2203.08411v2", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding", "abstract": "Visual grounding, i.e., localizing objects in images according to natural\nlanguage queries, is an important topic in visual language understanding. The\nmost effective approaches for this task are based on deep learning, which\ngenerally require expensive manually labeled image-query or patch-query pairs.\nTo eliminate the heavy dependence on human annotations, we present a novel\nmethod, named Pseudo-Q, to automatically generate pseudo language queries for\nsupervised training. Our method leverages an off-the-shelf object detector to\nidentify visual objects from unlabeled images, and then language queries for\nthese objects are obtained in an unsupervised fashion with a pseudo-query\ngeneration module. Then, we design a task-related query prompt module to\nspecifically tailor generated pseudo language queries for visual grounding\ntasks. Further, in order to fully capture the contextual relationships between\nimages and language queries, we develop a visual-language model equipped with\nmulti-level cross-modality attention mechanism. Extensive experimental results\ndemonstrate that our method has two notable benefits: (1) it can reduce human\nannotation costs significantly, e.g., 31% on RefCOCO without degrading original\nmodel's performance under the fully supervised setting, and (2) without bells\nand whistles, it achieves superior or comparable performance compared to\nstate-of-the-art weakly-supervised visual grounding methods on all the five\ndatasets we have experimented. Code is available at\nhttps://github.com/LeapLabTHU/Pseudo-Q.", "published": "2022-03-16 09:17:41", "link": "http://arxiv.org/abs/2203.08481v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Whither the Priors for (Vocal) Interactivity?", "abstract": "Voice-based communication is often cited as one of the most `natural' ways in\nwhich humans and robots might interact, and the recent availability of accurate\nautomatic speech recognition and intelligible speech synthesis has enabled\nresearchers to integrate advanced off-the-shelf spoken language technology\ncomponents into their robot platforms. Despite this, the resulting interactions\nare anything but `natural'. It transpires that simply giving a robot a voice\ndoesn't mean that a user will know how (or when) to talk to it, and the\nresulting `conversations' tend to be stilted, one-sided and short. On the\nsurface, these difficulties might appear to be fairly trivial consequences of\nusers' unfamiliarity with robots (and \\emph{vice versa}), and that any problems\nwould be mitigated by long-term use by the human, coupled with `deep learning'\nby the robot. However, it is argued here that such communication failures are\nindicative of a deeper malaise: a fundamental lack of basic principles --\n\\emph{priors} -- underpinning not only speech-based interaction in particular,\nbut (vocal) interactivity in general. This is evidenced not only by the fact\nthat contemporary spoken language systems already require training data sets\nthat are orders-of-magnitude greater than that experienced by a young child,\nbut also by the lack of design principles for creating effective communicative\nhuman-robot interaction. This short position paper identifies some of the key\nareas where theoretical insights might help overcome these shortfalls.", "published": "2022-03-16 12:06:46", "link": "http://arxiv.org/abs/2203.08578v1", "categories": ["cs.RO", "cs.CL", "cs.HC"], "primary_category": "cs.RO"}
{"title": "Less is More: Summary of Long Instructions is Better for Program\n  Synthesis", "abstract": "Despite the success of large pre-trained language models (LMs) such as Codex,\nthey show below-par performance on the larger and more complicated programming\nrelated questions. We show that LMs benefit from the summarized version of\ncomplicated questions. Our findings show that superfluous information often\npresent in problem description such as human characters, background stories,\nand names (which are included to help humans in understanding a task) does not\nhelp models in understanding a task. To this extent, we create a meta-dataset\nfrom the frequently used APPS dataset and the newly created CodeContests\ndataset for the program synthesis task. Our meta-dataset consists of human and\nsynthesized summaries of the long and complicated programming questions.\nExperimental results on Codex show that our proposed approach outperforms\nbaseline by 8.13% on the APPS dataset and 11.88% on the CodeContests dataset on\naverage in terms of strict accuracy. Our analysis shows that summaries\nsignificantly improve performance for introductory (9.86%) and interview\n(11.48%) programming questions. However, it shows improvement by a small margin\n(~ 2%) for competitive programming questions, implying scope for future\nresearch in this direction.", "published": "2022-03-16 13:04:12", "link": "http://arxiv.org/abs/2203.08597v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Feasibility Study of Answer-Agnostic Question Generation for Education", "abstract": "We conduct a feasibility study into the applicability of answer-agnostic\nquestion generation models to textbook passages. We show that a significant\nportion of errors in such systems arise from asking irrelevant or\nuninterpretable questions and that such errors can be ameliorated by providing\nsummarized input. We find that giving these models human-written summaries\ninstead of the original text results in a significant increase in acceptability\nof generated questions (33% $\\rightarrow$ 83%) as determined by expert\nannotators. We also find that, in the absence of human-written summaries,\nautomatic summarization can serve as a good middle ground.", "published": "2022-03-16 15:16:18", "link": "http://arxiv.org/abs/2203.08685v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Training Data is More Valuable than You Think: A Simple and Effective\n  Method by Retrieving from Training Data", "abstract": "Retrieval-based methods have been shown to be effective in NLP tasks via\nintroducing external knowledge. However, the indexing and retrieving of\nlarge-scale corpora bring considerable computational cost. Surprisingly, we\nfound that REtrieving from the traINing datA (REINA) only can lead to\nsignificant gains on multiple NLG and NLU tasks. We retrieve the labeled\ntraining instances most similar to the input text and then concatenate them\nwith the input to feed into the model to generate the output. Experimental\nresults show that this simple method can achieve significantly better\nperformance on a variety of NLU and NLG tasks, including summarization, machine\ntranslation, language modeling, and question answering tasks. For instance, our\nproposed method achieved state-of-the-art results on XSum, BigPatent, and\nCommonsenseQA. Our code is released, https://github.com/microsoft/REINA .", "published": "2022-03-16 17:37:27", "link": "http://arxiv.org/abs/2203.08773v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Are Shortest Rationales the Best Explanations for Human Understanding?", "abstract": "Existing self-explaining models typically favor extracting the shortest\npossible rationales - snippets of an input text \"responsible for\" corresponding\noutput - to explain the model prediction, with the assumption that shorter\nrationales are more intuitive to humans. However, this assumption has yet to be\nvalidated. Is the shortest rationale indeed the most human-understandable? To\nanswer this question, we design a self-explaining model, LimitedInk, which\nallows users to extract rationales at any target length. Compared to existing\nbaselines, LimitedInk achieves compatible end-task performance and\nhuman-annotated rationale agreement, making it a suitable representation of the\nrecent class of self-explaining models. We use LimitedInk to conduct a user\nstudy on the impact of rationale length, where we ask human judges to predict\nthe sentiment label of documents based only on LimitedInk-generated rationales\nwith different lengths. We show rationales that are too short do not help\nhumans predict labels better than randomly masked text, suggesting the need for\nmore careful design of the best human rationales.", "published": "2022-03-16 17:52:07", "link": "http://arxiv.org/abs/2203.08788v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multimodal Learning on Graphs for Disease Relation Extraction", "abstract": "Objective: Disease knowledge graphs are a way to connect, organize, and\naccess disparate information about diseases with numerous benefits for\nartificial intelligence (AI). To create knowledge graphs, it is necessary to\nextract knowledge from multimodal datasets in the form of relationships between\ndisease concepts and normalize both concepts and relationship types.\n  Methods: We introduce REMAP, a multimodal approach for disease relation\nextraction and classification. The REMAP machine learning approach jointly\nembeds a partial, incomplete knowledge graph and a medical language dataset\ninto a compact latent vector space, followed by aligning the multimodal\nembeddings for optimal disease relation extraction.\n  Results: We apply REMAP approach to a disease knowledge graph with 96,913\nrelations and a text dataset of 1.24 million sentences. On a dataset annotated\nby human experts, REMAP improves text-based disease relation extraction by\n10.0% (accuracy) and 17.2% (F1-score) by fusing disease knowledge graphs with\ntext information. Further, REMAP leverages text information to recommend new\nrelationships in the knowledge graph, outperforming graph-based methods by 8.4%\n(accuracy) and 10.4% (F1-score).\n  Conclusion: REMAP is a multimodal approach for extracting and classifying\ndisease relationships by fusing structured knowledge and text information.\nREMAP provides a flexible neural architecture to easily find, access, and\nvalidate AI-driven relationships between disease concepts.", "published": "2022-03-16 19:07:51", "link": "http://arxiv.org/abs/2203.08893v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Memorizing Transformers", "abstract": "Language models typically need to be trained or finetuned in order to acquire\nnew knowledge, which involves updating their weights. We instead envision\nlanguage models that can simply read and memorize new data at inference time,\nthus acquiring new knowledge immediately. In this work, we extend language\nmodels with the ability to memorize the internal representations of past\ninputs. We demonstrate that an approximate kNN lookup into a non-differentiable\nmemory of recent (key, value) pairs improves language modeling across various\nbenchmarks and tasks, including generic webtext (C4), math papers (arXiv),\nbooks (PG-19), code (Github), as well as formal theorems (Isabelle). We show\nthat the performance steadily improves when we increase the size of memory up\nto 262K tokens. On benchmarks including code and mathematics, we find that the\nmodel is capable of making use of newly defined functions and theorems during\ntest time.", "published": "2022-03-16 19:54:35", "link": "http://arxiv.org/abs/2203.08913v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "AdaLoGN: Adaptive Logic Graph Network for Reasoning-Based Machine\n  Reading Comprehension", "abstract": "Recent machine reading comprehension datasets such as ReClor and LogiQA\nrequire performing logical reasoning over text. Conventional neural models are\ninsufficient for logical reasoning, while symbolic reasoners cannot directly\napply to text. To meet the challenge, we present a neural-symbolic approach\nwhich, to predict an answer, passes messages over a graph representing logical\nrelations between text units. It incorporates an adaptive logic graph network\n(AdaLoGN) which adaptively infers logical relations to extend the graph and,\nessentially, realizes mutual and iterative reinforcement between neural and\nsymbolic reasoning. We also implement a novel subgraph-to-node message passing\nmechanism to enhance context-option interaction for answering multiple-choice\nquestions. Our approach shows promising results on ReClor and LogiQA.", "published": "2022-03-16 23:51:01", "link": "http://arxiv.org/abs/2203.08992v1", "categories": ["cs.CL", "cs.AI", "cs.NE", "cs.SC"], "primary_category": "cs.CL"}
{"title": "Linking Theories and Methods in Cognitive Sciences via Joint Embedding\n  of the Scientific Literature: The Example of Cognitive Control", "abstract": "Traditionally, theory and practice of Cognitive Control are linked via\nliterature reviews by human domain experts. This approach, however, is\ninadequate to track the ever-growing literature. It may also be biased, and\nyield redundancies and confusion.\n  Here we present an alternative approach. We performed automated text analyses\non a large body of scientific texts to create a joint representation of tasks\nand constructs. More specifically, 385,705 scientific abstracts were first\nmapped into an embedding space using a transformers-based language model.\nDocument embeddings were then used to identify a task-construct graph embedding\nthat grounds constructs on tasks and supports nuanced meaning of the constructs\nby taking advantage of constrained random walks in the graph.\n  This joint task-construct graph embedding, can be queried to generate task\nbatteries targeting specific constructs, may reveal knowledge gaps in the\nliterature, and inspire new tasks and novel hypotheses.", "published": "2022-03-16 11:03:09", "link": "http://arxiv.org/abs/2203.11016v2", "categories": ["cs.AI", "cs.CL", "cs.IR", "q-bio.NC"], "primary_category": "cs.AI"}
{"title": "A New Quantum CNN Model for Image Classification", "abstract": "Quantum density matrix represents all the information of the entire quantum\nsystem, and novel models of meaning employing density matrices naturally model\nlinguistic phenomena such as hyponymy and linguistic ambiguity, among others in\nquantum question answering tasks. Naturally, we argue that the quantum density\nmatrix can enhance the image feature information and the relationship between\nthe features for the classical image classification. Specifically, we (i)\ncombine density matrices and CNN to design a new mechanism; (ii) apply the new\nmechanism to some representative classical image classification tasks. A series\nof experiments show that the application of quantum density matrix in image\nclassification has the generalization and high efficiency on different\ndatasets. The application of quantum density matrix both in classical question\nanswering tasks and classical image classification tasks show more effective\nperformance.", "published": "2022-03-16 12:23:25", "link": "http://arxiv.org/abs/2203.11155v5", "categories": ["cs.CL", "cs.LG", "quant-ph"], "primary_category": "cs.CL"}
{"title": "A Squeeze-and-Excitation and Transformer based Cross-task System for\n  Environmental Sound Recognition", "abstract": "Environmental sound recognition (ESR) is an emerging research topic in audio\npattern recognition. Many tasks are presented to resort to computational models\nfor ESR in real-life applications. However, current models are usually designed\nfor individual tasks, and are not robust and applicable to other tasks.\nCross-task models, which promote unified knowledge modeling across various\ntasks, have not been thoroughly investigated. In this article, we propose a\ncross-task model for three different tasks of ESR: 1) acoustic scene\nclassification; 2) urban sound tagging; and 3) anomalous sound detection. An\narchitecture named SE-Trans is presented that uses attention mechanism-based\nSqueeze-and-Excitation and Transformer encoder modules to learn the channelwise\nrelationship and temporal dependencies of the acoustic features. FMix is\nemployed as the data augmentation method that improves the performance of ESR.\nEvaluations for the three tasks are conducted on the recent databases of\ndetection and classification of acoustic scenes and event challenges. The\nexperimental results show that the proposed cross-task model achieves\nstate-of-the-art performance on all tasks. Further analysis demonstrates that\nthe proposed cross-task model can effectively utilize acoustic knowledge across\ndifferent ESR tasks.", "published": "2022-03-16 02:07:02", "link": "http://arxiv.org/abs/2203.08350v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Instance-level loss based multiple-instance learning framework for\n  acoustic scene classification", "abstract": "In the acoustic scene classification (ASC) task, an acoustic scene consists\nof diverse sounds and is inferred by identifying combinations of distinct\nattributes among them. This study aims to extract and cluster these attributes\neffectively using an improved multiple-instance learning (MIL) framework for\nASC. MIL, known as a weakly supervised learning method, is a strategy for\nextracting an instance from a bundle of frames composing an input audio clip\nand inferring a scene corresponding to the input data using these unlabeled\ninstances. However, many studies pointed out an underestimation problem of MIL.\nIn this study, we develop a MIL framework more suitable for ASC systems by\ndefining instance-level labels and loss to extract and cluster instances\neffectively. Furthermore, we design a fully separated convolutional module,\nwhich is a lightweight neural network comprising pointwise, frequency-sided\ndepthwise, and temporal-sided depthwise convolutional filters. As a result,\ncompared to vanilla MIL, the confidence and proportion of positive instances\nincrease significantly, overcoming the underestimation problem and improving\nthe classification accuracy up to 11%. The proposed system achieved a\nperformance of 81.1% and 72.3% on the TAU urban acoustic scenes 2019 and 2020\nmobile datasets with 139 K parameters, respectively. Especially, it achieves\nthe highest performance among the systems having under the 1 M parameters on\nthe TAU urban acoustic scenes 2019 dataset.", "published": "2022-03-16 07:26:12", "link": "http://arxiv.org/abs/2203.08439v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Pushing the limits of raw waveform speaker recognition", "abstract": "In recent years, speaker recognition systems based on raw waveform inputs\nhave received increasing attention. However, the performance of such systems\nare typically inferior to the state-of-the-art handcrafted feature-based\ncounterparts, which demonstrate equal error rates under 1% on the popular\nVoxCeleb1 test set. This paper proposes a novel speaker recognition model based\non raw waveform inputs. The model incorporates recent advances in machine\nlearning and speaker verification, including the Res2Net backbone module and\nmulti-layer feature aggregation. Our best model achieves an equal error rate of\n0.89%, which is competitive with the state-of-the-art models based on\nhandcrafted features, and outperforms the best model based on raw waveform\ninputs by a large margin. We also explore the application of the proposed model\nin the context of self-supervised learning framework. Our self-supervised model\noutperforms single phase-based existing works in this line of research.\nFinally, we show that self-supervised pre-training is effective for the\nsemi-supervised scenario where we only have a small set of labelled training\ndata, along with a larger set of unlabelled examples.", "published": "2022-03-16 09:28:03", "link": "http://arxiv.org/abs/2203.08488v2", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Learning Audio Representations with MLPs", "abstract": "In this paper, we propose an efficient MLP-based approach for learning audio\nrepresentations, namely timestamp and scene-level audio embeddings. We use an\nencoder consisting of sequentially stacked gated MLP blocks, which accept 2D\nMFCCs as inputs. In addition, we also provide a simple temporal\ninterpolation-based algorithm for computing scene-level embeddings from\ntimestamp embeddings. The audio representations generated by our method are\nevaluated across a diverse set of benchmarks at the Holistic Evaluation of\nAudio Representations (HEAR) challenge, hosted at the NeurIPS 2021 competition\ntrack. We achieved first place on the Speech Commands (full), Speech Commands\n(5 hours), and the Mridingham Tonic benchmarks. Furthermore, our approach is\nalso the most resource-efficient among all the submitted methods, in terms of\nboth the number of model parameters and the time required to compute\nembeddings.", "published": "2022-03-16 09:33:36", "link": "http://arxiv.org/abs/2203.08490v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
