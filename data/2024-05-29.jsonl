{"title": "HLOB -- Information Persistence and Structure in Limit Order Books", "abstract": "We introduce a novel large-scale deep learning model for Limit Order Book\nmid-price changes forecasting, and we name it `HLOB'. This architecture (i)\nexploits the information encoded by an Information Filtering Network, namely\nthe Triangulated Maximally Filtered Graph, to unveil deeper and non-trivial\ndependency structures among volume levels; and (ii) guarantees deterministic\ndesign choices to handle the complexity of the underlying system by drawing\ninspiration from the groundbreaking class of Homological Convolutional Neural\nNetworks. We test our model against 9 state-of-the-art deep learning\nalternatives on 3 real-world Limit Order Book datasets, each including 15\nstocks traded on the NASDAQ exchange, and we systematically characterize the\nscenarios where HLOB outperforms state-of-the-art architectures. Our approach\nsheds new light on the spatial distribution of information in Limit Order Books\nand on its degradation over increasing prediction horizons, narrowing the gap\nbetween microstructural modeling and deep learning-based forecasting in\nhigh-frequency financial markets.", "published": "2024-05-29 09:46:44", "link": "http://arxiv.org/abs/2405.18938v3", "categories": ["q-fin.TR", "cs.LG"], "primary_category": "q-fin.TR"}
{"title": "Optimizing Broker Performance Evaluation through Intraday Modeling of Execution Cost", "abstract": "Minimizing execution costs for large orders is a fundamental challenge in\nfinance. Firms often depend on brokers to manage their trades due to limited\ninternal resources for optimizing trading strategies. This paper presents a\nmethodology for evaluating the effectiveness of broker execution algorithms\nusing trading data. We focus on two primary cost components: a linear cost that\nquantifies short-term execution quality and a quadratic cost associated with\nthe price impact of trades. Using a model with transient price impact, we\nderive analytical formulas for estimating these costs. Furthermore, we enhance\nestimation accuracy by introducing novel methods such as weighting price\nchanges based on their expected impact content. Our results demonstrate\nsubstantial improvements in estimating both linear and impact costs, providing\na robust and efficient framework for selecting the most cost-effective brokers.", "published": "2024-05-29 09:41:31", "link": "http://arxiv.org/abs/2405.18936v2", "categories": ["q-fin.TR", "q-fin.MF"], "primary_category": "q-fin.TR"}
{"title": "Efficient Model-agnostic Alignment via Bayesian Persuasion", "abstract": "With recent advancements in large language models (LLMs), alignment has\nemerged as an effective technique for keeping LLMs consensus with human intent.\nCurrent methods primarily involve direct training through Supervised\nFine-tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF), both of\nwhich require substantial computational resources and extensive ground truth\ndata. This paper explores an efficient method for aligning black-box large\nmodels using smaller models, introducing a model-agnostic and lightweight\nBayesian Persuasion Alignment framework. We formalize this problem as an\noptimization of the signaling strategy from the small model's perspective. In\nthe persuasion process, the small model (Advisor) observes the information item\n(i.e., state) and persuades large models (Receiver) to elicit improved\nresponses. The Receiver then generates a response based on the input, the\nsignal from the Advisor, and its updated belief about the information item.\nThrough training using our framework, we demonstrate that the Advisor can\nsignificantly enhance the performance of various Receivers across a range of\ntasks. We theoretically analyze our persuasion framework and provide an upper\nbound on the Advisor's regret, confirming its effectiveness in learning the\noptimal signaling strategy. Our Empirical results demonstrates that GPT-2 can\nsignificantly improve the performance of various models, achieving an average\nenhancement of 16.1% in mathematical reasoning ability and 13.7% in code\ngeneration. We hope our work can provide an initial step toward rethinking the\nalignment framework from the Bayesian Persuasion perspective.", "published": "2024-05-29 02:57:07", "link": "http://arxiv.org/abs/2405.18718v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reverse Image Retrieval Cues Parametric Memory in Multimodal LLMs", "abstract": "Despite impressive advances in recent multimodal large language models\n(MLLMs), state-of-the-art models such as from the GPT-4 suite still struggle\nwith knowledge-intensive tasks. To address this, we consider Reverse Image\nRetrieval (RIR) augmented generation, a simple yet effective strategy to\naugment MLLMs with web-scale reverse image search results. RIR robustly\nimproves knowledge-intensive visual question answering (VQA) of GPT-4V by\n37-43%, GPT-4 Turbo by 25-27%, and GPT-4o by 18-20% in terms of open-ended VQA\nevaluation metrics. To our surprise, we discover that RIR helps the model to\nbetter access its own world knowledge. Concretely, our experiments suggest that\nRIR augmentation helps by providing further visual and textual cues without\nnecessarily containing the direct answer to a query. In addition, we elucidate\ncases in which RIR can hurt performance and conduct a human evaluation.\nFinally, we find that the overall advantage of using RIR makes it difficult for\nan agent that can choose to use RIR to perform better than an approach where\nRIR is the default setting.", "published": "2024-05-29 04:00:41", "link": "http://arxiv.org/abs/2405.18740v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toxicity Detection for Free", "abstract": "Current LLMs are generally aligned to follow safety requirements and tend to\nrefuse toxic prompts. However, LLMs can fail to refuse toxic prompts or be\novercautious and refuse benign examples. In addition, state-of-the-art toxicity\ndetectors have low TPRs at low FPR, incurring high costs in real-world\napplications where toxic examples are rare. In this paper, we introduce\nModeration Using LLM Introspection (MULI), which detects toxic prompts using\nthe information extracted directly from LLMs themselves. We found we can\ndistinguish between benign and toxic prompts from the distribution of the first\nresponse token's logits. Using this idea, we build a robust detector of toxic\nprompts using a sparse logistic regression model on the first response token\nlogits. Our scheme outperforms SOTA detectors under multiple metrics.", "published": "2024-05-29 07:03:31", "link": "http://arxiv.org/abs/2405.18822v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding and Addressing the Under-Translation Problem from the\n  Perspective of Decoding Objective", "abstract": "Neural Machine Translation (NMT) has made remarkable progress over the past\nyears. However, under-translation and over-translation remain two challenging\nproblems in state-of-the-art NMT systems. In this work, we conduct an in-depth\nanalysis on the underlying cause of under-translation in NMT, providing an\nexplanation from the perspective of decoding objective. To optimize the beam\nsearch objective, the model tends to overlook words it is less confident about,\nleading to the under-translation phenomenon. Correspondingly, the model's\nconfidence in predicting the End Of Sentence (EOS) diminishes when\nunder-translation occurs, serving as a mild penalty for under-translated\ncandidates. Building upon this analysis, we propose employing the confidence of\npredicting EOS as a detector for under-translation, and strengthening the\nconfidence-based penalty to penalize candidates with a high risk of\nunder-translation. Experiments on both synthetic and real-world data show that\nour method can accurately detect and rectify under-translated outputs, with\nminor impact on other correct translations.", "published": "2024-05-29 09:25:49", "link": "http://arxiv.org/abs/2405.18922v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Encoding Hierarchical Schema via Concept Flow for Multifaceted Ideology\n  Detection", "abstract": "Multifaceted ideology detection (MID) aims to detect the ideological leanings\nof texts towards multiple facets. Previous studies on ideology detection mainly\nfocus on one generic facet and ignore label semantics and explanatory\ndescriptions of ideologies, which are a kind of instructive information and\nreveal the specific concepts of ideologies. In this paper, we develop a novel\nconcept semantics-enhanced framework for the MID task. Specifically, we propose\na bidirectional iterative concept flow (BICo) method to encode multifaceted\nideologies. BICo enables the concepts to flow across levels of the schema tree\nand enriches concept representations with multi-granularity semantics.\nFurthermore, we explore concept attentive matching and concept-guided\ncontrastive learning strategies to guide the model to capture ideology features\nwith the learned concept semantics. Extensive experiments on the benchmark\ndataset show that our approach achieves state-of-the-art performance in MID,\nincluding in the cross-topic scenario.", "published": "2024-05-29 10:37:28", "link": "http://arxiv.org/abs/2405.18974v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Auxiliary Knowledge-Induced Learning for Automatic Multi-Label Medical\n  Document Classification", "abstract": "The International Classification of Diseases (ICD) is an authoritative\nmedical classification system of different diseases and conditions for clinical\nand management purposes. ICD indexing assigns a subset of ICD codes to a\nmedical record. Since human coding is labour-intensive and error-prone, many\nstudies employ machine learning to automate the coding process. ICD coding is a\nchallenging task, as it needs to assign multiple codes to each medical document\nfrom an extremely large hierarchically organized collection. In this paper, we\npropose a novel approach for ICD indexing that adopts three ideas: (1) we use a\nmulti-level deep dilated residual convolution encoder to aggregate the\ninformation from the clinical notes and learn document representations across\ndifferent lengths of the texts; (2) we formalize the task of ICD classification\nwith auxiliary knowledge of the medical records, which incorporates not only\nthe clinical texts but also different clinical code terminologies and drug\nprescriptions for better inferring the ICD codes; and (3) we introduce a graph\nconvolutional network to leverage the co-occurrence patterns among ICD codes,\naiming to enhance the quality of label representations. Experimental results\nshow the proposed method achieves state-of-the-art performance on a number of\nmeasures.", "published": "2024-05-29 13:44:07", "link": "http://arxiv.org/abs/2405.19084v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MEMoE: Enhancing Model Editing with Mixture of Experts Adaptors", "abstract": "Model editing aims to efficiently alter the behavior of Large Language Models\n(LLMs) within a desired scope, while ensuring no adverse impact on other\ninputs. Recent years have witnessed various model editing methods been\nproposed. However, these methods either exhibit poor overall performance or\nstruggle to strike a balance between generalization and locality. We propose\nMEMoE, a model editing adapter utilizing a Mixture of Experts (MoE)\narchitecture with a knowledge anchor routing strategy. MEMoE updates knowledge\nusing a bypass MoE structure, keeping the original parameters unchanged to\npreserve the general ability of LLMs. And, the knowledge anchor routing ensures\nthat inputs requiring similar knowledge are routed to the same expert, thereby\nenhancing the generalization of the updated knowledge. Experimental results\nshow the superiority of our approach over both batch editing and sequential\nbatch editing tasks, exhibiting exceptional overall performance alongside\noutstanding balance between generalization and locality. Our code will be\navailable.", "published": "2024-05-29 13:49:44", "link": "http://arxiv.org/abs/2405.19086v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PathReasoner: Modeling Reasoning Path with Equivalent Extension for\n  Logical Question Answering", "abstract": "Logical reasoning task has attracted great interest since it was proposed.\nFaced with such a task, current competitive models, even large language models\n(e.g., ChatGPT and PaLM 2), still perform badly. Previous promising LMs\nstruggle in logical consistency modeling and logical structure perception. To\nthis end, we model the logical reasoning task by transforming each logical\nsample into reasoning paths and propose an architecture \\textbf{PathReasoner}.\nIt addresses the task from the views of both data and model. To expand the\ndiversity of the logical samples, we propose an atom extension strategy\nsupported by equivalent logical formulas, to form new reasoning paths. From the\nmodel perspective, we design a stack of transformer-style blocks. In\nparticular, we propose a path-attention module to joint model in-atom and\ncross-atom relations with the high-order diffusion strategy. Experiments show\nthat PathReasoner achieves competitive performances on two logical reasoning\nbenchmarks and great generalization abilities.", "published": "2024-05-29 14:14:05", "link": "http://arxiv.org/abs/2405.19109v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lower Bounds on the Expressivity of Recurrent Neural Language Models", "abstract": "The recent successes and spread of large neural language models (LMs) call\nfor a thorough understanding of their computational ability. Describing their\ncomputational abilities through LMs' \\emph{representational capacity} is a\nlively area of research. However, investigation into the representational\ncapacity of neural LMs has predominantly focused on their ability to\n\\emph{recognize} formal languages. For example, recurrent neural networks\n(RNNs) with Heaviside activations are tightly linked to regular languages,\ni.e., languages defined by finite-state automata (FSAs). Such results, however,\nfall short of describing the capabilities of RNN \\emph{language models} (LMs),\nwhich are definitionally \\emph{distributions} over strings. We take a fresh\nlook at the representational capacity of RNN LMs by connecting them to\n\\emph{probabilistic} FSAs and demonstrate that RNN LMs with linearly bounded\nprecision can express arbitrary regular LMs.", "published": "2024-05-29 16:02:09", "link": "http://arxiv.org/abs/2405.19222v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AlchemistCoder: Harmonizing and Eliciting Code Capability by Hindsight\n  Tuning on Multi-source Data", "abstract": "Open-source Large Language Models (LLMs) and their specialized variants,\nparticularly Code LLMs, have recently delivered impressive performance.\nHowever, previous Code LLMs are typically fine-tuned on single-source data with\nlimited quality and diversity, which may insufficiently elicit the potential of\npre-trained Code LLMs. In this paper, we present AlchemistCoder, a series of\nCode LLMs with enhanced code generation and generalization capabilities\nfine-tuned on multi-source data. To achieve this, we pioneer to unveil inherent\nconflicts among the various styles and qualities in multi-source code corpora\nand introduce data-specific prompts with hindsight relabeling, termed\nAlchemistPrompts, to harmonize different data sources and instruction-response\npairs. Additionally, we propose incorporating the data construction process\ninto the fine-tuning data as code comprehension tasks, including instruction\nevolution, data filtering, and code review. Extensive experiments demonstrate\nthat AlchemistCoder holds a clear lead among all models of the same size\n(6.7B/7B) and rivals or even surpasses larger models (15B/33B/70B), showcasing\nthe efficacy of our method in refining instruction-following capabilities and\nadvancing the boundaries of code intelligence.", "published": "2024-05-29 16:57:33", "link": "http://arxiv.org/abs/2405.19265v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PediatricsGPT: Large Language Models as Chinese Medical Assistants for\n  Pediatric Applications", "abstract": "Developing intelligent pediatric consultation systems offers promising\nprospects for improving diagnostic efficiency, especially in China, where\nhealthcare resources are scarce. Despite recent advances in Large Language\nModels (LLMs) for Chinese medicine, their performance is sub-optimal in\npediatric applications due to inadequate instruction data and vulnerable\ntraining procedures. To address the above issues, this paper builds PedCorpus,\na high-quality dataset of over 300,000 multi-task instructions from pediatric\ntextbooks, guidelines, and knowledge graph resources to fulfil diverse\ndiagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the\nfirst Chinese pediatric LLM assistant built on a systematic and robust training\npipeline. In the continuous pre-training phase, we introduce a hybrid\ninstruction pre-training mechanism to mitigate the internal-injected knowledge\ninconsistency of LLMs for medical domain adaptation. Immediately, the\nfull-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the\ngeneral medical knowledge schema into the models. After that, we devise a\ndirect following preference optimization to enhance the generation of\npediatrician-like humanistic responses. In the parameter-efficient secondary\nSFT phase, a mixture of universal-specific experts strategy is presented to\nresolve the competency conflict between medical generalist and pediatric\nexpertise mastery. Extensive results based on the metrics, GPT-4, and doctor\nevaluations on distinct doctor downstream tasks show that PediatricsGPT\nconsistently outperforms previous Chinese medical LLMs. Our model and dataset\nwill be open-source for community development.", "published": "2024-05-29 16:59:38", "link": "http://arxiv.org/abs/2405.19266v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MASSIVE Multilingual Abstract Meaning Representation: A Dataset and\n  Baselines for Hallucination Detection", "abstract": "Abstract Meaning Representation (AMR) is a semantic formalism that captures\nthe core meaning of an utterance. There has been substantial work developing\nAMR corpora in English and more recently across languages, though the limited\nsize of existing datasets and the cost of collecting more annotations are\nprohibitive. With both engineering and scientific questions in mind, we\nintroduce MASSIVE-AMR, a dataset with more than 84,000 text-to-graph\nannotations, currently the largest and most diverse of its kind: AMR graphs for\n1,685 information-seeking utterances mapped to 50+ typologically diverse\nlanguages. We describe how we built our resource and its unique features before\nreporting on experiments using large language models for multilingual AMR and\nSPARQL parsing as well as applying AMRs for hallucination detection in the\ncontext of knowledge base question answering, with results shedding light on\npersistent issues using LLMs for structured parsing.", "published": "2024-05-29 17:17:22", "link": "http://arxiv.org/abs/2405.19285v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Integrating Multi-scale Contextualized Information for Byte-based Neural\n  Machine Translation", "abstract": "Subword tokenization is a common method for vocabulary building in Neural\nMachine Translation (NMT) models. However, increasingly complex tasks have\nrevealed its disadvantages. First, a vocabulary cannot be modified once it is\nlearned, making it hard to adapt to new words. Second, in multilingual\ntranslation, the imbalance in data volumes across different languages spreads\nto the vocabulary, exacerbating translations involving low-resource languages.\nWhile byte-based tokenization addresses these issues, byte-based models\nstruggle with the low information density inherent in UTF-8 byte sequences.\nPrevious works enhance token semantics through local contextualization but fail\nto select an appropriate contextualizing scope based on the input.\nConsequently, we propose the Multi-Scale Contextualization (MSC) method, which\nlearns contextualized information of varying scales across different hidden\nstate dimensions. It then leverages the attention module to dynamically\nintegrate the multi-scale contextualized information. Experiments show that MSC\nsignificantly outperforms subword-based and other byte-based methods in both\nmultilingual and out-of-domain scenarios. Code can be found in\nhttps://github.com/ictnlp/Multiscale-Contextualization.", "published": "2024-05-29 17:19:04", "link": "http://arxiv.org/abs/2405.19290v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Expert-Guided Extinction of Toxic Tokens for Debiased Generation", "abstract": "Large language models (LLMs) can elicit social bias during generations,\nespecially when inference with toxic prompts. Controlling the sensitive\nattributes in generation encounters challenges in data distribution,\ngeneralizability, and efficiency. Specifically, fine-tuning and retrieval\ndemand extensive unbiased corpus, while direct prompting requires meticulously\ncurated instructions for correcting the output in multiple rounds of thoughts\nbut poses challenges on memory and inference latency. In this work, we propose\nthe Expert-Guided Extinction of Toxic Tokens for Debiased Generation (EXPOSED)\nto eliminate the undesired harmful outputs for LLMs without the aforementioned\nrequirements. EXPOSED constructs a debiasing expert based on the abundant toxic\ncorpus to expose and elicit the potentially dangerous tokens. It then processes\nthe output to the LLMs and constructs a fair distribution by suppressing and\nattenuating the toxic tokens. EXPOSED is evaluated on fairness benchmarks over\nthree LLM families. Extensive experiments demonstrate that compared with other\nbaselines, the proposed EXPOSED significantly reduces the potential social bias\nwhile balancing fairness and generation performance.", "published": "2024-05-29 17:26:52", "link": "http://arxiv.org/abs/2405.19299v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Nearest Neighbor Speculative Decoding for LLM Generation and Attribution", "abstract": "Large language models (LLMs) often hallucinate and lack the ability to\nprovide attribution for their generations. Semi-parametric LMs, such as kNN-LM,\napproach these limitations by refining the output of an LM for a given prompt\nusing its nearest neighbor matches in a non-parametric data store. However,\nthese models often exhibit slow inference speeds and produce non-fluent texts.\nIn this paper, we introduce Nearest Neighbor Speculative Decoding (NEST), a\nnovel semi-parametric language modeling approach that is capable of\nincorporating real-world text spans of arbitrary length into the LM generations\nand providing attribution to their sources. NEST performs token-level retrieval\nat each inference step to compute a semi-parametric mixture distribution and\nidentify promising span continuations in a corpus. It then uses an approximate\nspeculative decoding procedure that accepts a prefix of the retrieved span or\ngenerates a new token. NEST significantly enhances the generation quality and\nattribution rate of the base LM across a variety of knowledge-intensive tasks,\nsurpassing the conventional kNN-LM method and performing competitively with\nin-context retrieval augmentation. In addition, NEST substantially improves the\ngeneration speed, achieving a 1.8x speedup in inference time when applied to\nLlama-2-Chat 70B.", "published": "2024-05-29 17:55:03", "link": "http://arxiv.org/abs/2405.19325v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive In-conversation Team Building for Language Model Agents", "abstract": "Leveraging multiple large language model (LLM) agents has shown to be a\npromising approach for tackling complex tasks, while the effective design of\nmultiple agents for a particular application remains an art. It is thus\nintriguing to answer a critical question: Given a task, how can we build a team\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\noffers a flexible solution, realized through a novel agent design named Captain\nAgent. It dynamically forms and manages teams for each step of a task-solving\nprocess, utilizing nested group conversations and reflection to ensure diverse\nexpertise and prevent stereotypical outputs, allowing for a flexible yet\nstructured approach to problem-solving. A comprehensive evaluation across six\nreal-world scenarios demonstrates that Captain Agent significantly outperforms\nexisting multi-agent methods with 21.94% improvement in average accuracy,\nproviding outstanding performance without requiring task-specific prompt\nengineering. Our exploration of different backbone LLM and cost analysis\nfurther shows that Captain Agent can improve the conversation quality of weak\nLLM and achieve competitive performance with extremely low cost, which\nilluminates the application of multi-agent systems.", "published": "2024-05-29 18:08:37", "link": "http://arxiv.org/abs/2405.19425v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Agreement: Diagnosing the Rationale Alignment of Automated Essay\n  Scoring Methods based on Linguistically-informed Counterfactuals", "abstract": "While current Automated Essay Scoring (AES) methods demonstrate high scoring\nagreement with human raters, their decision-making mechanisms are not fully\nunderstood. Our proposed method, using counterfactual intervention assisted by\nLarge Language Models (LLMs), reveals that BERT-like models primarily focus on\nsentence-level features, whereas LLMs such as GPT-3.5, GPT-4 and Llama-3 are\nsensitive to conventions & accuracy, language complexity, and organization,\nindicating a more comprehensive rationale alignment with scoring rubrics.\nMoreover, LLMs can discern counterfactual interventions when giving feedback on\nessays. Our approach improves understanding of neural AES methods and can also\napply to other domains seeking transparency in model-driven decisions.", "published": "2024-05-29 18:16:32", "link": "http://arxiv.org/abs/2405.19433v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Critical Learning Periods: Leveraging Early Training Dynamics for\n  Efficient Data Pruning", "abstract": "Neural Machine Translation models are extremely data and compute-hungry.\nHowever, not all data points contribute equally to model training and\ngeneralization. Data pruning to remove the low-value data points has the\nbenefit of drastically reducing the compute budget without significant drop in\nmodel performance. In this paper, we propose a new data pruning technique:\nCheckpoints Across Time (CAT), that leverages early model training dynamics to\nidentify the most relevant data points for model performance. We benchmark CAT\nagainst several data pruning techniques including COMET-QE, LASER and LaBSE. We\nfind that CAT outperforms the benchmarks on Indo-European languages on multiple\ntest sets. When applied to English-German, English-French and English-Swahili\ntranslation tasks, CAT achieves comparable performance to using the full\ndataset, while pruning up to 50% of training data. We inspect the data points\nthat CAT selects and find that it tends to favour longer sentences and\nsentences with unique or rare words.", "published": "2024-05-29 19:21:49", "link": "http://arxiv.org/abs/2405.19462v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Full-duplex Speech Dialogue Scheme Based On Large Language Models", "abstract": "We present a generative dialogue system capable of operating in a full-duplex\nmanner, allowing for seamless interaction. It is based on a large language\nmodel (LLM) carefully aligned to be aware of a perception module, a motor\nfunction module, and the concept of a simple finite state machine (called\nneural FSM) with two states. The perception and motor function modules operate\nin tandem, allowing the system to speak and listen to the user simultaneously.\nThe LLM generates textual tokens for inquiry responses and makes autonomous\ndecisions to start responding to, wait for, or interrupt the user by emitting\ncontrol tokens to the neural FSM. All these tasks of the LLM are carried out as\nnext token prediction on a serialized view of the dialogue in real-time. In\nautomatic quality evaluations simulating real-life interaction, the proposed\nsystem reduces the average conversation response latency by more than threefold\ncompared with LLM-based half-duplex dialogue systems while responding within\nless than 500 milliseconds in more than 50% of evaluated interactions. Running\nan LLM with only 8 billion parameters, our system exhibits an 8% higher\ninterruption precision rate than the best available commercial LLM for\nvoice-based dialogue.", "published": "2024-05-29 20:05:46", "link": "http://arxiv.org/abs/2405.19487v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unlearning Climate Misinformation in Large Language Models", "abstract": "Misinformation regarding climate change is a key roadblock in addressing one\nof the most serious threats to humanity. This paper investigates factual\naccuracy in large language models (LLMs) regarding climate information. Using\ntrue/false labeled Q&A data for fine-tuning and evaluating LLMs on\nclimate-related claims, we compare open-source models, assessing their ability\nto generate truthful responses to climate change questions. We investigate the\ndetectability of models intentionally poisoned with false climate information,\nfinding that such poisoning may not affect the accuracy of a model's responses\nin other domains. Furthermore, we compare the effectiveness of unlearning\nalgorithms, fine-tuning, and Retrieval-Augmented Generation (RAG) for factually\ngrounding LLMs on climate change topics. Our evaluation reveals that unlearning\nalgorithms can be effective for nuanced conceptual claims, despite previous\nfindings suggesting their inefficacy in privacy contexts. These insights aim to\nguide the development of more factually reliable LLMs and highlight the need\nfor additional work to secure LLMs against misinformation attacks.", "published": "2024-05-29 23:11:53", "link": "http://arxiv.org/abs/2405.19563v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-based Hierarchical Concept Decomposition for Interpretable\n  Fine-Grained Image Classification", "abstract": "(Renyi Qu's Master's Thesis) Recent advancements in interpretable models for\nvision-language tasks have achieved competitive performance; however, their\ninterpretability often suffers due to the reliance on unstructured text outputs\nfrom large language models (LLMs). This introduces randomness and compromises\nboth transparency and reliability, which are essential for addressing safety\nissues in AI systems. We introduce \\texttt{Hi-CoDe} (Hierarchical Concept\nDecomposition), a novel framework designed to enhance model interpretability\nthrough structured concept analysis. Our approach consists of two main\ncomponents: (1) We use GPT-4 to decompose an input image into a structured\nhierarchy of visual concepts, thereby forming a visual concept tree. (2) We\nthen employ an ensemble of simple linear classifiers that operate on\nconcept-specific features derived from CLIP to perform classification. Our\napproach not only aligns with the performance of state-of-the-art models but\nalso advances transparency by providing clear insights into the decision-making\nprocess and highlighting the importance of various concepts. This allows for a\ndetailed analysis of potential failure modes and improves model compactness,\ntherefore setting a new benchmark in interpretability without compromising the\naccuracy.", "published": "2024-05-29 00:36:56", "link": "http://arxiv.org/abs/2405.18672v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Calibrating Reasoning in Language Models with Internal Consistency", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in\nvarious reasoning tasks, aided by techniques like chain-of-thought prompting\nthat elicits verbalized reasoning. However, LLMs often generate text with\nobvious mistakes and contradictions, raising doubts about their ability to\nrobustly process and utilize generated rationales. In this work, we investigate\nreasoning in LLMs through the lens of internal representations, focusing on how\nthese representations are influenced by generated rationales. Our preliminary\nanalysis reveals that while generated rationales improve answer accuracy,\ninconsistencies emerge between the model's internal representations in middle\nlayers and those in final layers, potentially undermining the reliability of\ntheir reasoning processes. To address this, we propose internal consistency as\na measure of the model's confidence by examining the agreement of latent\npredictions decoded from intermediate layers. Extensive empirical studies\nacross different models and datasets demonstrate that internal consistency\neffectively distinguishes between correct and incorrect reasoning paths.\nMotivated by this, we propose a new approach to calibrate reasoning by\nup-weighting reasoning paths with high internal consistency, resulting in a\nsignificant boost in reasoning performance. Further analysis uncovers distinct\npatterns in attention and feed-forward modules across layers, providing\ninsights into the emergence of internal inconsistency. In summary, our results\ndemonstrate the potential of using internal representations for self-evaluation\nof LLMs. Our code is available at github.com/zhxieml/internal-consistency.", "published": "2024-05-29 02:44:12", "link": "http://arxiv.org/abs/2405.18711v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Contextual Position Encoding: Learning to Count What's Important", "abstract": "The attention mechanism is a critical component of Large Language Models\n(LLMs) that allows tokens in a sequence to interact with each other, but is\norder-invariant. Incorporating position encoding (PE) makes it possible to\naddress by position, such as attending to the i-th token. However, current PE\nmethods use token counts to derive position, and thus cannot generalize to\nhigher levels of abstraction, such as attending to the i-th sentence. In this\npaper, we propose a new position encoding method, Contextual Position Encoding\n(CoPE), that allows positions to be conditioned on context by incrementing\nposition only on certain tokens determined by the model. This allows more\ngeneral position addressing such as attending to the $i$-th particular word,\nnoun, or sentence. We show that CoPE can solve the selective copy, counting and\nFlip-Flop tasks where popular position embeddings fail, and improves perplexity\non language modeling and coding tasks.", "published": "2024-05-29 02:57:15", "link": "http://arxiv.org/abs/2405.18719v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Genshin: General Shield for Natural Language Processing with Large\n  Language Models", "abstract": "Large language models (LLMs) like ChatGPT, Gemini, or LLaMA have been\ntrending recently, demonstrating considerable advancement and generalizability\npower in countless domains. However, LLMs create an even bigger black box\nexacerbating opacity, with interpretability limited to few approaches. The\nuncertainty and opacity embedded in LLMs' nature restrict their application in\nhigh-stakes domains like financial fraud, phishing, etc. Current approaches\nmainly rely on traditional textual classification with posterior interpretable\nalgorithms, suffering from attackers who may create versatile adversarial\nsamples to break the system's defense, forcing users to make trade-offs between\nefficiency and robustness. To address this issue, we propose a novel cascading\nframework called Genshin (General Shield for Natural Language Processing with\nLarge Language Models), utilizing LLMs as defensive one-time plug-ins. Unlike\nmost applications of LLMs that try to transform text into something new or\nstructural, Genshin uses LLMs to recover text to its original state. Genshin\naims to combine the generalizability of the LLM, the discrimination of the\nmedian model, and the interpretability of the simple model. Our experiments on\nthe task of sentimental analysis and spam detection have shown fatal flaws of\nthe current median models and exhilarating results on LLMs' recovery ability,\ndemonstrating that Genshin is both effective and efficient. In our ablation\nstudy, we unearth several intriguing observations. Utilizing the LLM defender,\na tool derived from the 4th paradigm, we have reproduced BERT's 15% optimal\nmask rate results in the 3rd paradigm of NLP. Additionally, when employing the\nLLM as a potential adversarial tool, attackers are capable of executing\neffective attacks that are nearly semantically lossless.", "published": "2024-05-29 04:04:05", "link": "http://arxiv.org/abs/2405.18741v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Musical Phrase Segmentation via Grammatical Induction", "abstract": "We outline a solution to the challenge of musical phrase segmentation that\nuses grammatical induction algorithms, a class of algorithms which infer a\ncontext-free grammar from an input sequence. We analyze the performance of five\ngrammatical induction algorithms on three datasets using various musical\nviewpoint combinations. Our experiments show that the LONGESTFIRST algorithm\nachieves the best F1 scores across all three datasets and that input encodings\nthat include the duration viewpoint result in the best performance.", "published": "2024-05-29 04:04:36", "link": "http://arxiv.org/abs/2405.18742v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Language Generation with Strictly Proper Scoring Rules", "abstract": "Language generation based on maximum likelihood estimation (MLE) has become\nthe fundamental approach for text generation. Maximum likelihood estimation is\ntypically performed by minimizing the log-likelihood loss, also known as the\nlogarithmic score in statistical decision theory. The logarithmic score is\nstrictly proper in the sense that it encourages honest forecasts, where the\nexpected score is maximized only when the model reports true probabilities.\nAlthough many strictly proper scoring rules exist, the logarithmic score is the\nonly local scoring rule among them that depends exclusively on the probability\nof the observed sample, making it capable of handling the exponentially large\nsample space of natural text. In this work, we propose a straightforward\nstrategy for adapting scoring rules to language generation, allowing for\nlanguage modeling with any non-local scoring rules. Leveraging this strategy,\nwe train language generation models using two classic strictly proper scoring\nrules, the Brier score and the Spherical score, as alternatives to the\nlogarithmic score. Experimental results indicate that simply substituting the\nloss function, without adjusting other hyperparameters, can yield substantial\nimprovements in model's generation capabilities. Moreover, these improvements\ncan scale up to large language models (LLMs) such as LLaMA-7B and LLaMA-13B.\nSource code: \\url{https://github.com/shaochenze/ScoringRulesLM}.", "published": "2024-05-29 09:09:00", "link": "http://arxiv.org/abs/2405.18906v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Better Chain-of-Thought: A Reflection on Effectiveness and\n  Faithfulness", "abstract": "Chain-of-thought (CoT) prompting demonstrates varying performance under\ndifferent reasoning tasks. Previous work attempts to evaluate it but falls\nshort in providing an in-depth analysis of patterns that influence the CoT. In\nthis paper, we study the CoT performance from the perspective of effectiveness\nand faithfulness. For the former, we identify key factors that influence CoT\neffectiveness on performance improvement, including problem difficulty,\ninformation gain, and information flow. For the latter, we interpret the\nunfaithful CoT issue by conducting a joint analysis of the information\ninteraction among the question, CoT, and answer. The result demonstrates that,\nwhen the LLM predicts answers, it can recall correct information missing in the\nCoT from the question, leading to the problem. Finally, we propose a novel\nalgorithm to mitigate this issue, in which we recall extra information from the\nquestion to enhance the CoT generation and evaluate CoTs based on their\ninformation gain. Extensive experiments demonstrate that our approach enhances\nboth the faithfulness and effectiveness of CoT.", "published": "2024-05-29 09:17:46", "link": "http://arxiv.org/abs/2405.18915v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Kestrel: Point Grounding Multimodal LLM for Part-Aware 3D\n  Vision-Language Understanding", "abstract": "While 3D MLLMs have achieved significant progress, they are restricted to\nobject and scene understanding and struggle to understand 3D spatial structures\nat the part level. In this paper, we introduce Kestrel, representing a novel\napproach that empowers 3D MLLMs with part-aware understanding, enabling better\ninterpretation and segmentation grounding of 3D objects at the part level.\nDespite its significance, the current landscape lacks tasks and datasets that\nendow and assess this capability. Therefore, we propose two novel tasks: (1)\nPart-Aware Point Grounding, the model is tasked with directly predicting a\npart-level segmentation mask based on user instructions, and (2) Part-Aware\nPoint Grounded Captioning, the model provides a detailed caption that includes\npart-level descriptions and their corresponding masks. To support learning and\nevaluating for these tasks, we introduce 3DCoMPaT Grounded Instructions Dataset\n(3DCoMPaT-GRIN). 3DCoMPaT-GRIN Vanilla, comprising 789k part-aware point\ncloud-instruction-segmentation mask triplets, is used to evaluate MLLMs'\nability of part-aware segmentation grounding. 3DCoMPaT-GRIN Grounded Caption,\ncontaining 107k part-aware point cloud-instruction-grounded caption triplets,\nassesses both MLLMs' part-aware language comprehension and segmentation\ngrounding capabilities. Our introduced tasks, dataset, and Kestrel represent a\npreliminary effort to bridge the gap between human cognition and 3D MLLMs,\ni.e., the ability to perceive and engage with the environment at both global\nand part levels. Extensive experiments on the 3DCoMPaT-GRIN show that Kestrel\ncan generate user-specified segmentation masks, a capability not present in any\nexisting 3D MLLM. Kestrel thus established a benchmark for evaluating the\npart-aware language comprehension and segmentation grounding of 3D objects.\nProject page at https://feielysia.github.io/Kestrel.github.io/", "published": "2024-05-29 09:43:48", "link": "http://arxiv.org/abs/2405.18937v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Cracking the Code of Juxtaposition: Can AI Models Understand the\n  Humorous Contradictions", "abstract": "Recent advancements in large multimodal language models have demonstrated\nremarkable proficiency across a wide range of tasks. Yet, these models still\nstruggle with understanding the nuances of human humor through juxtaposition,\nparticularly when it involves nonlinear narratives that underpin many jokes and\nhumor cues. This paper investigates this challenge by focusing on comics with\ncontradictory narratives, where each comic consists of two panels that create a\nhumorous contradiction. We introduce the YesBut benchmark, which comprises\ntasks of varying difficulty aimed at assessing AI's capabilities in recognizing\nand interpreting these comics, ranging from literal content comprehension to\ndeep narrative reasoning. Through extensive experimentation and analysis of\nrecent commercial or open-sourced large (vision) language models, we assess\ntheir capability to comprehend the complex interplay of the narrative humor\ninherent in these comics. Our results show that even state-of-the-art models\nstill lag behind human performance on this task. Our findings offer insights\ninto the current limitations and potential improvements for AI in understanding\nhuman creative expressions.", "published": "2024-05-29 13:51:43", "link": "http://arxiv.org/abs/2405.19088v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Multi-stage Retrieve and Re-rank Model for Automatic Medical Coding\n  Recommendation", "abstract": "The International Classification of Diseases (ICD) serves as a definitive\nmedical classification system encompassing a wide range of diseases and\nconditions. The primary objective of ICD indexing is to allocate a subset of\nICD codes to a medical record, which facilitates standardized documentation and\nmanagement of various health conditions. Most existing approaches have suffered\nfrom selecting the proper label subsets from an extremely large ICD collection\nwith a heavy long-tailed label distribution. In this paper, we leverage a\nmulti-stage ``retrieve and re-rank'' framework as a novel solution to ICD\nindexing, via a hybrid discrete retrieval method, and re-rank retrieved\ncandidates with contrastive learning that allows the model to make more\naccurate predictions from a simplified label space. The retrieval model is a\nhybrid of auxiliary knowledge of the electronic health records (EHR) and a\ndiscrete retrieval method (BM25), which efficiently collects high-quality\ncandidates. In the last stage, we propose a label co-occurrence guided\ncontrastive re-ranking model, which re-ranks the candidate labels by pulling\ntogether the clinical notes with positive ICD codes. Experimental results show\nthe proposed method achieves state-of-the-art performance on a number of\nmeasures on the MIMIC-III benchmark.", "published": "2024-05-29 13:54:30", "link": "http://arxiv.org/abs/2405.19093v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Faithful Chart Summarization with ChaTS-Pi", "abstract": "Chart-to-summary generation can help explore data, communicate insights, and\nhelp the visually impaired people. Multi-modal generative models have been used\nto produce fluent summaries, but they can suffer from factual and perceptual\nerrors. In this work we present CHATS-CRITIC, a reference-free chart\nsummarization metric for scoring faithfulness. CHATS-CRITIC is composed of an\nimage-to-text model to recover the table from a chart, and a tabular entailment\nmodel applied to score the summary sentence by sentence. We find that\nCHATS-CRITIC evaluates the summary quality according to human ratings better\nthan reference-based metrics, either learned or n-gram based, and can be\nfurther used to fix candidate summaries by removing not supported sentences. We\nthen introduce CHATS-PI, a chart-to-summary pipeline that leverages\nCHATS-CRITIC during inference to fix and rank sampled candidates from any\nchart-summarization model. We evaluate CHATS-PI and CHATS-CRITIC using human\nraters, establishing state-of-the-art results on two popular chart-to-summary\ndatasets.", "published": "2024-05-29 13:55:06", "link": "http://arxiv.org/abs/2405.19094v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DGRC: An Effective Fine-tuning Framework for Distractor Generation in\n  Chinese Multi-choice Reading Comprehension", "abstract": "When evaluating a learner's knowledge proficiency, the multiple-choice\nquestion is an efficient and widely used format in standardized tests.\nNevertheless, generating these questions, particularly plausible distractors\n(incorrect options), poses a considerable challenge. Generally, the distractor\ngeneration can be classified into cloze-style distractor generation (CDG) and\nnatural questions distractor generation (NQDG). In contrast to the CDG,\nutilizing pre-trained language models (PLMs) for NQDG presents three primary\nchallenges: (1) PLMs are typically trained to generate ``correct'' content,\nlike answers, while rarely trained to generate ``plausible\" content, like\ndistractors; (2) PLMs often struggle to produce content that aligns well with\nspecific knowledge and the style of exams; (3) NQDG necessitates the model to\nproduce longer, context-sensitive, and question-relevant distractors. In this\nstudy, we introduce a fine-tuning framework named DGRC for NQDG in Chinese\nmulti-choice reading comprehension from authentic examinations. DGRC comprises\nthree major components: hard chain-of-thought, multi-task learning, and\ngeneration mask patterns. The experiment results demonstrate that DGRC\nsignificantly enhances generation performance, achieving a more than 2.5-fold\nimprovement in BLEU scores.", "published": "2024-05-29 14:47:01", "link": "http://arxiv.org/abs/2405.19139v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WRDScore: New Metric for Evaluation of Natural Language Generation\n  Models", "abstract": "Evaluating natural language generation models, particularly for method name\nprediction, poses significant challenges. A robust metric must account for the\nversatility of method naming, considering both semantic and syntactic\nvariations. Traditional overlap-based metrics, such as ROUGE, fail to capture\nthese nuances. Existing embedding-based metrics often suffer from imbalanced\nprecision and recall, lack normalized scores, or make unrealistic assumptions\nabout sequences. To address these limitations, we leverage the theory of\noptimal transport and construct WRDScore, a novel metric that strikes a balance\nbetween simplicity and effectiveness. In the WRDScore framework, we define\nprecision as the maximum degree to which the predicted sequence's tokens are\nincluded in the reference sequence, token by token. Recall is calculated as the\ntotal cost of the optimal transport plan that maps the reference sequence to\nthe predicted one. Finally, WRDScore is computed as the harmonic mean of\nprecision and recall, balancing these two complementary metrics. Our metric is\nlightweight, normalized, and precision-recall-oriented, avoiding unrealistic\nassumptions while aligning well with human judgments. Experiments on a\nhuman-curated dataset confirm the superiority of WRDScore over other available\ntext metrics.", "published": "2024-05-29 16:00:46", "link": "http://arxiv.org/abs/2405.19220v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Robust Preference Optimization through Reward Model Distillation", "abstract": "Language model (LM) post-training (or alignment) involves maximizing a reward\nfunction that is derived from preference annotations. Direct Preference\nOptimization (DPO) is a popular offline alignment method that trains a policy\ndirectly on preference data without the need to train a reward model or apply\nreinforcement learning. However, the empirical evidence suggests that DPO\ntypically assigns implicit rewards that overfit, and trend towards infinite\nmagnitude. This frequently leads to degenerate policies, sometimes causing even\nthe probabilities of the preferred generations to go to zero. In this work, we\nanalyze this phenomenon and use distillation to get a better proxy for the true\npreference distribution over generation pairs: we train the LM such that its\ninduced implicit reward, i.e., the scaled log-likelihood ratio of the model to\nthe reference model, matches an explicit reward model trained on the preference\ndata. Moreover, to account for uncertainty in the reward model we are\ndistilling from, we optimize against a family of reward models that, as a\nwhole, is likely to include at least one reasonable proxy for the preference\ndistribution. Our results show that distilling from such a family of reward\nmodels leads to improved robustness to distribution shift in preference\nannotations, while preserving the simple supervised nature of DPO.", "published": "2024-05-29 17:39:48", "link": "http://arxiv.org/abs/2405.19316v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Two-Layer Retrieval-Augmented Generation Framework for Low-Resource\n  Medical Question Answering Using Reddit Data: Proof-of-Concept Study", "abstract": "The increasing use of social media to share lived and living experiences of\nsubstance use presents a unique opportunity to obtain information on side\neffects, use patterns, and opinions on novel psychoactive substances. However,\ndue to the large volume of data, obtaining useful insights through natural\nlanguage processing technologies such as large language models is challenging.\nThis paper aims to develop a retrieval-augmented generation (RAG) architecture\nfor medical question answering pertaining to clinicians' queries on emerging\nissues associated with health-related topics, using user-generated medical\ninformation on social media. We proposed a two-layer RAG framework for\nquery-focused answer generation and evaluated a proof of concept for the\nframework in the context of query-focused summary generation from social media\nforums, focusing on emerging drug-related information. Our modular framework\ngenerates individual summaries followed by an aggregated summary to answer\nmedical queries from large amounts of user-generated social media data in an\nefficient manner. We compared the performance of a quantized large language\nmodel (Nous-Hermes-2-7B-DPO), deployable in low-resource settings, with GPT-4.\nFor this proof-of-concept study, we used user-generated data from Reddit to\nanswer clinicians' questions on the use of xylazine and ketamine. Our framework\nachieves comparable median scores in terms of relevance, length, hallucination,\ncoverage, and coherence when evaluated using GPT-4 and Nous-Hermes-2-7B-DPO,\nevaluated for 20 queries with 76 samples. There was no statistically\nsignificant difference between the two for coverage, coherence, relevance,\nlength, and hallucination. A statistically significant difference was noted for\nthe Coleman-Liau Index. Our RAG framework can effectively answer medical\nquestions about targeted topics and can be deployed in resource-constrained\nsettings.", "published": "2024-05-29 20:56:52", "link": "http://arxiv.org/abs/2405.19519v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Stress-Testing Capability Elicitation With Password-Locked Models", "abstract": "To determine the safety of large language models (LLMs), AI developers must\nbe able to assess their dangerous capabilities. But simple prompting strategies\noften fail to elicit an LLM's full capabilities. One way to elicit capabilities\nmore robustly is to fine-tune the LLM to complete the task. In this paper, we\ninvestigate the conditions under which fine-tuning-based elicitation suffices\nto elicit capabilities. To do this, we introduce password-locked models, LLMs\nfine-tuned such that some of their capabilities are deliberately hidden.\nSpecifically, these LLMs are trained to exhibit these capabilities only when a\npassword is present in the prompt, and to imitate a much weaker LLM otherwise.\nPassword-locked models enable a novel method of evaluating capabilities\nelicitation methods, by testing whether these password-locked capabilities can\nbe elicited without using the password. We find that a few high-quality\ndemonstrations are often sufficient to fully elicit password-locked\ncapabilities. More surprisingly, fine-tuning can elicit other capabilities that\nhave been locked using the same password, or even different passwords.\nFurthermore, when only evaluations, and not demonstrations, are available,\napproaches like reinforcement learning are still often able to elicit\ncapabilities. Overall, our findings suggest that fine-tuning is an effective\nmethod of eliciting hidden capabilities of current models, but may be\nunreliable when high-quality demonstrations are not available, e.g. as may be\nthe case when models' (hidden) capabilities exceed those of human\ndemonstrators.", "published": "2024-05-29 22:26:26", "link": "http://arxiv.org/abs/2405.19550v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Quo Vadis ChatGPT? From Large Language Models to Large Knowledge Models", "abstract": "The startling success of ChatGPT and other large language models (LLMs) using\ntransformer-based generative neural network architecture in applications such\nas natural language processing and image synthesis has many researchers excited\nabout potential opportunities in process systems engineering (PSE). The almost\nhuman-like performance of LLMs in these areas is indeed very impressive,\nsurprising, and a major breakthrough. Their capabilities are very useful in\ncertain tasks, such as writing first drafts of documents, code writing\nassistance, text summarization, etc. However, their success is limited in\nhighly scientific domains as they cannot yet reason, plan, or explain due to\ntheir lack of in-depth domain knowledge. This is a problem in domains such as\nchemical engineering as they are governed by fundamental laws of physics and\nchemistry (and biology), constitutive relations, and highly technical knowledge\nabout materials, processes, and systems. Although purely data-driven machine\nlearning has its immediate uses, the long-term success of AI in scientific and\nengineering domains would depend on developing hybrid AI systems that use first\nprinciples and technical knowledge effectively. We call these hybrid AI systems\nLarge Knowledge Models (LKMs), as they will not be limited to only NLP-based\ntechniques or NLP-like applications. In this paper, we discuss the challenges\nand opportunities in developing such systems in chemical engineering.", "published": "2024-05-29 23:06:54", "link": "http://arxiv.org/abs/2405.19561v1", "categories": ["cs.AI", "cs.CL", "I.2.0; I.2.7"], "primary_category": "cs.AI"}
{"title": "A Deep Convolutional Neural Network-based Model for Aspect and Polarity\n  Classification in Hausa Movie Reviews", "abstract": "Aspect-based Sentiment Analysis (ABSA) is crucial for understanding sentiment\nnuances in text, especially across diverse languages and cultures. This paper\nintroduces a novel Deep Convolutional Neural Network (CNN)-based model tailored\nfor aspect and polarity classification in Hausa movie reviews, an\nunderrepresented language in sentiment analysis research. A comprehensive Hausa\nABSA dataset is created, filling a significant gap in resource availability.\nThe dataset, preprocessed using sci-kit-learn for TF-IDF transformation,\nincludes manually annotated aspect-level feature ontology words and sentiment\npolarity assignments. The proposed model combines CNNs with attention\nmechanisms for aspect-word prediction, leveraging contextual information and\nsentiment polarities. With 91% accuracy on aspect term extraction and 92% on\nsentiment polarity classification, the model outperforms traditional machine\nmodels, offering insights into specific aspects and sentiments. This study\nadvances ABSA research, particularly in underrepresented languages, with\nimplications for cross-cultural linguistic research.", "published": "2024-05-29 23:45:42", "link": "http://arxiv.org/abs/2405.19575v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Toward Conversational Agents with Context and Time Sensitive Long-term\n  Memory", "abstract": "There has recently been growing interest in conversational agents with\nlong-term memory which has led to the rapid development of language models that\nuse retrieval-augmented generation (RAG). Until recently, most work on RAG has\nfocused on information retrieval from large databases of texts, like Wikipedia,\nrather than information from long-form conversations. In this paper, we argue\nthat effective retrieval from long-form conversational data faces two unique\nproblems compared to static database retrieval: 1) time/event-based queries,\nwhich requires the model to retrieve information about previous conversations\nbased on time or the order of a conversational event (e.g., the third\nconversation on Tuesday), and 2) ambiguous queries that require surrounding\nconversational context to understand. To better develop RAG-based agents that\ncan deal with these challenges, we generate a new dataset of ambiguous and\ntime-based questions that build upon a recent dataset of long-form, simulated\nconversations, and demonstrate that standard RAG based approaches handle such\nquestions poorly. We then develop a novel retrieval model which combines\nchained-of-table search methods, standard vector-database retrieval, and a\nprompting method to disambiguate queries, and demonstrate that this approach\nsubstantially improves over current methods at solving these tasks. We believe\nthat this new dataset and more advanced RAG agent can act as a key benchmark\nand stepping stone towards effective memory augmented conversational agents\nthat can be used in a wide variety of AI applications.", "published": "2024-05-29 18:19:46", "link": "http://arxiv.org/abs/2406.00057v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cascade-Aware Training of Language Models", "abstract": "Reducing serving cost and latency is a fundamental concern for the deployment\nof language models (LMs) in business applications. To address this, cascades of\nLMs offer an effective solution that conditionally employ smaller models for\nsimpler queries. Cascaded systems are typically built with independently\ntrained models, neglecting the advantages of considering inference-time\ninteractions of the cascaded LMs during training. In this paper, we present\ncascade-aware training(CAT), an approach to optimizing the overall quality-cost\nperformance tradeoff of a cascade of LMs. We achieve inference-time benefits by\ntraining the small LM with awareness of its place in a cascade and downstream\ncapabilities. We demonstrate the value of the proposed method with over 60 LM\ntasks of the SuperGLUE, WMT22, and FLAN2021 datasets.", "published": "2024-05-29 22:28:46", "link": "http://arxiv.org/abs/2406.00060v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prompting or Fine-tuning? Exploring Large Language Models for Causal\n  Graph Validation", "abstract": "This study explores the capability of Large Language Models (LLMs) to\nevaluate causality in causal graphs generated by conventional statistical\ncausal discovery methods-a task traditionally reliant on manual assessment by\nhuman subject matter experts. To bridge this gap in causality assessment, LLMs\nare employed to evaluate the causal relationships by determining whether a\ncausal connection between variable pairs can be inferred from textual context.\nOur study compares two approaches: (1) prompting-based method for zero-shot and\nfew-shot causal inference and, (2) fine-tuning language models for the causal\nrelation prediction task. While prompt-based LLMs have demonstrated versatility\nacross various NLP tasks, our experiments on biomedical and general-domain\ndatasets show that fine-tuned models consistently outperform them, achieving up\nto a 20.5-point improvement in F1 score-even when using smaller-parameter\nlanguage models. These findings provide valuable insights into the strengths\nand limitations of both approaches for causal graph evaluation.", "published": "2024-05-29 09:06:18", "link": "http://arxiv.org/abs/2406.16899v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Zipper: A Multi-Tower Decoder Architecture for Fusing Modalities", "abstract": "Integrating multiple generative foundation models, especially those trained\non different modalities, into something greater than the sum of its parts poses\nsignificant challenges. Two key hurdles are the availability of aligned data\n(concepts that contain similar meaning but is expressed differently in\ndifferent modalities), and effectively leveraging unimodal representations in\ncross-domain generative tasks, without compromising their original unimodal\ncapabilities.\n  We propose Zipper, a multi-tower decoder architecture that addresses these\nconcerns by using cross-attention to flexibly compose multimodal generative\nmodels from independently pre-trained unimodal decoders. In our experiments\nfusing speech and text modalities, we show the proposed architecture performs\nvery competitively in scenarios with limited aligned text-speech data. We also\nshowcase the flexibility of our model to selectively maintain unimodal (e.g.,\ntext-to-text generation) generation performance by freezing the corresponding\nmodal tower (e.g. text). In cross-modal tasks such as automatic speech\nrecognition (ASR) where the output modality is text, we show that freezing the\ntext backbone results in negligible performance degradation. In cross-modal\ntasks such as text-to-speech generation (TTS) where the output modality is\nspeech, we show that using a pre-trained speech backbone results in superior\nperformance to the baseline.", "published": "2024-05-29 00:23:55", "link": "http://arxiv.org/abs/2405.18669v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Can GPT Redefine Medical Understanding? Evaluating GPT on Biomedical\n  Machine Reading Comprehension", "abstract": "Large language models (LLMs) have shown remarkable performance on many tasks\nin different domains. However, their performance in closed-book biomedical\nmachine reading comprehension (MRC) has not been evaluated in depth. In this\nwork, we evaluate GPT on four closed-book biomedical MRC benchmarks. We\nexperiment with different conventional prompting techniques as well as\nintroduce our own novel prompting method. To solve some of the retrieval\nproblems inherent to LLMs, we propose a prompting strategy named Implicit\nRetrieval Augmented Generation (RAG) that alleviates the need for using vector\ndatabases to retrieve important chunks in traditional RAG setups. Moreover, we\nreport qualitative assessments on the natural language generation outputs from\nour approach. The results show that our new prompting technique is able to get\nthe best performance in two out of four datasets and ranks second in rest of\nthem. Experiments show that modern-day LLMs like GPT even in a zero-shot\nsetting can outperform supervised models, leading to new state-of-the-art\n(SoTA) results on two of the benchmarks.", "published": "2024-05-29 01:12:53", "link": "http://arxiv.org/abs/2405.18682v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Preference-based Reinforcement Learning via Aligned Experience\n  Estimation", "abstract": "Preference-based reinforcement learning (PbRL) has shown impressive\ncapabilities in training agents without reward engineering. However, a notable\nlimitation of PbRL is its dependency on substantial human feedback. This\ndependency stems from the learning loop, which entails accurate reward learning\ncompounded with value/policy learning, necessitating a considerable number of\nsamples. To boost the learning loop, we propose SEER, an efficient PbRL method\nthat integrates label smoothing and policy regularization techniques. Label\nsmoothing reduces overfitting of the reward model by smoothing human preference\nlabels. Additionally, we bootstrap a conservative estimate $\\widehat{Q}$ using\nwell-supported state-action pairs from the current replay memory to mitigate\noverestimation bias and utilize it for policy learning regularization. Our\nexperimental results across a variety of complex tasks, both in online and\noffline settings, demonstrate that our approach improves feedback efficiency,\noutperforming state-of-the-art methods by a large margin. Ablation studies\nfurther reveal that SEER achieves a more accurate Q-function compared to prior\nwork.", "published": "2024-05-29 01:49:20", "link": "http://arxiv.org/abs/2405.18688v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Correctable Landmark Discovery via Large Models for Vision-Language\n  Navigation", "abstract": "Vision-Language Navigation (VLN) requires the agent to follow language\ninstructions to reach a target position. A key factor for successful navigation\nis to align the landmarks implied in the instruction with diverse visual\nobservations. However, previous VLN agents fail to perform accurate modality\nalignment especially in unexplored scenes, since they learn from limited\nnavigation data and lack sufficient open-world alignment knowledge. In this\nwork, we propose a new VLN paradigm, called COrrectable LaNdmark DiScOvery via\nLarge ModEls (CONSOLE). In CONSOLE, we cast VLN as an open-world sequential\nlandmark discovery problem, by introducing a novel correctable landmark\ndiscovery scheme based on two large models ChatGPT and CLIP. Specifically, we\nuse ChatGPT to provide rich open-world landmark cooccurrence commonsense, and\nconduct CLIP-driven landmark discovery based on these commonsense priors. To\nmitigate the noise in the priors due to the lack of visual constraints, we\nintroduce a learnable cooccurrence scoring module, which corrects the\nimportance of each cooccurrence according to actual observations for accurate\nlandmark discovery. We further design an observation enhancement strategy for\nan elegant combination of our framework with different VLN agents, where we\nutilize the corrected landmark features to obtain enhanced observation features\nfor action decision. Extensive experimental results on multiple popular VLN\nbenchmarks (R2R, REVERIE, R4R, RxR) show the significant superiority of CONSOLE\nover strong baselines. Especially, our CONSOLE establishes the new\nstate-of-the-art results on R2R and R4R in unseen scenarios. Code is available\nat https://github.com/expectorlin/CONSOLE.", "published": "2024-05-29 03:05:59", "link": "http://arxiv.org/abs/2405.18721v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CtrlA: Adaptive Retrieval-Augmented Generation via Inherent Control", "abstract": "Retrieval-augmented generation (RAG) has emerged as a promising solution for\nmitigating hallucinations of large language models (LLMs) with retrieved\nexternal knowledge. Adaptive RAG enhances this approach by enabling dynamic\nretrieval during generation, activating retrieval only when the query exceeds\nLLM's internal knowledge. Existing methods primarily focus on detecting LLM's\nconfidence via statistical uncertainty. Instead, we present the first attempts\nto solve adaptive RAG from a representation perspective and develop an inherent\ncontrol-based framework, termed \\name. Specifically, we extract the features\nthat represent the honesty and confidence directions of LLM and adopt them to\ncontrol LLM behavior and guide retrieval timing decisions. We also design a\nsimple yet effective query formulation strategy to support adaptive retrieval.\nExperiments show that \\name is superior to existing adaptive RAG methods on a\ndiverse set of tasks, the honesty steering can effectively make LLMs more\nhonest and confidence monitoring is a promising indicator of retrieval\ntrigger.Our code is available at \\url{https://github.com/HSLiu-Initial/CtrlA}.", "published": "2024-05-29 03:17:16", "link": "http://arxiv.org/abs/2405.18727v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "LMO-DP: Optimizing the Randomization Mechanism for Differentially\n  Private Fine-Tuning (Large) Language Models", "abstract": "Differentially Private Stochastic Gradient Descent (DP-SGD) and its variants\nhave been proposed to ensure rigorous privacy for fine-tuning large-scale\npre-trained language models. However, they rely heavily on the Gaussian\nmechanism, which may overly perturb the gradients and degrade the accuracy,\nespecially in stronger privacy regimes (e.g., the privacy budget $\\epsilon <\n3$). To address such limitations, we propose a novel Language Model-based\nOptimal Differential Privacy (LMO-DP) mechanism, which takes the first step to\nenable the tight composition of accurately fine-tuning (large) language models\nwith a sub-optimal DP mechanism, even in strong privacy regimes (e.g., $0.1\\leq\n\\epsilon<3$). Furthermore, we propose a novel offline optimal noise search\nmethod to efficiently derive the sub-optimal DP that significantly reduces the\nnoise magnitude. For instance, fine-tuning RoBERTa-large (with 300M parameters)\non the SST-2 dataset can achieve an accuracy of 92.20% (given $\\epsilon=0.3$,\n$\\delta=10^{-10}$) by drastically outperforming the Gaussian mechanism (e.g.,\n$\\sim 50\\%$ for small $\\epsilon$ and $\\delta$). We also draw similar findings\non the text generation tasks on GPT-2. Finally, to our best knowledge, LMO-DP\nis also the first solution to accurately fine-tune Llama-2 with strong\ndifferential privacy guarantees. The code will be released soon and available\nupon request.", "published": "2024-05-29 05:32:50", "link": "http://arxiv.org/abs/2405.18776v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Simulation, Modelling and Classification of Wiki Contributors: Spotting\n  The Good, The Bad, and The Ugly", "abstract": "Data crowdsourcing is a data acquisition process where groups of voluntary\ncontributors feed platforms with highly relevant data ranging from news,\ncomments, and media to knowledge and classifications. It typically processes\nuser-generated data streams to provide and refine popular services such as\nwikis, collaborative maps, e-commerce sites, and social networks. Nevertheless,\nthis modus operandi raises severe concerns regarding ill-intentioned data\nmanipulation in adversarial environments. This paper presents a simulation,\nmodelling, and classification approach to automatically identify human and\nnon-human (bots) as well as benign and malign contributors by using data\nfabrication to balance classes within experimental data sets, data stream\nmodelling to build and update contributor profiles and, finally, autonomic data\nstream classification. By employing WikiVoyage - a free worldwide wiki travel\nguide open to contribution from the general public - as a testbed, our approach\nproves to significantly boost the confidence and quality of the classifier by\nusing a class-balanced data stream, comprising both real and synthetic data.\nOur empirical results show that the proposed method distinguishes between\nbenign and malign bots as well as human contributors with a classification\naccuracy of up to 92 %.", "published": "2024-05-29 07:56:08", "link": "http://arxiv.org/abs/2405.18845v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLMs achieve adult human performance on higher-order theory of mind\n  tasks", "abstract": "This paper examines the extent to which large language models (LLMs) have\ndeveloped higher-order theory of mind (ToM); the human ability to reason about\nmultiple mental and emotional states in a recursive manner (e.g. I think that\nyou believe that she knows). This paper builds on prior work by introducing a\nhandwritten test suite -- Multi-Order Theory of Mind Q&A -- and using it to\ncompare the performance of five LLMs to a newly gathered adult human benchmark.\nWe find that GPT-4 and Flan-PaLM reach adult-level and near adult-level\nperformance on ToM tasks overall, and that GPT-4 exceeds adult performance on\n6th order inferences. Our results suggest that there is an interplay between\nmodel size and finetuning for the realisation of ToM abilities, and that the\nbest-performing LLMs have developed a generalised capacity for ToM. Given the\nrole that higher-order ToM plays in a wide range of cooperative and competitive\nhuman behaviours, these findings have significant implications for user-facing\nLLM applications.", "published": "2024-05-29 08:31:16", "link": "http://arxiv.org/abs/2405.18870v2", "categories": ["cs.AI", "cs.CL", "cs.HC", "I.2.7; H.1.2"], "primary_category": "cs.AI"}
{"title": "Are queries and keys always relevant? A case study on Transformer wave\n  functions", "abstract": "The dot product attention mechanism, originally designed for natural language\nprocessing tasks, is a cornerstone of modern Transformers. It adeptly captures\nsemantic relationships between word pairs in sentences by computing a\nsimilarity overlap between queries and keys. In this work, we explore the\nsuitability of Transformers, focusing on their attention mechanisms, in the\nspecific domain of the parametrization of variational wave functions to\napproximate ground states of quantum many-body spin Hamiltonians. Specifically,\nwe perform numerical simulations on the two-dimensional $J_1$-$J_2$ Heisenberg\nmodel, a common benchmark in the field of quantum many-body systems on lattice.\nBy comparing the performance of standard attention mechanisms with a simplified\nversion that excludes queries and keys, relying solely on positions, we achieve\ncompetitive results while reducing computational cost and parameter usage.\nFurthermore, through the analysis of the attention maps generated by standard\nattention mechanisms, we show that the attention weights become effectively\ninput-independent at the end of the optimization. We support the numerical\nresults with analytical calculations, providing physical insights of why\nqueries and keys should be, in principle, omitted from the attention mechanism\nwhen studying large systems.", "published": "2024-05-29 08:32:37", "link": "http://arxiv.org/abs/2405.18874v2", "categories": ["cond-mat.dis-nn", "cs.CL", "physics.comp-ph"], "primary_category": "cond-mat.dis-nn"}
{"title": "Are You Sure? Rank Them Again: Repeated Ranking For Better Preference\n  Datasets", "abstract": "Training Large Language Models (LLMs) with Reinforcement Learning from AI\nFeedback (RLAIF) aligns model outputs more closely with human preferences. This\ninvolves an evaluator model ranking multiple candidate responses to user\nprompts. However, the rankings from popular evaluator models such as GPT-4 can\nbe inconsistent. We propose the Repeat Ranking method - where we evaluate the\nsame responses multiple times and train only on those responses which are\nconsistently ranked. Using 2,714 prompts in 62 languages, we generated\nresponses from 7 top multilingual LLMs and had GPT-4 rank them five times each.\nEvaluating on MT-Bench chat benchmarks in six languages, our method\noutperformed the standard practice of training on all available prompts. Our\nwork highlights the quality versus quantity trade-off in RLAIF dataset\ngeneration and offers a stackable strategy for enhancing dataset and thus model\nquality.", "published": "2024-05-29 10:08:31", "link": "http://arxiv.org/abs/2405.18952v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EasyAnimate: A High-Performance Long Video Generation Method based on\n  Transformer Architecture", "abstract": "This paper presents EasyAnimate, an advanced method for video generation that\nleverages the power of transformer architecture for high-performance outcomes.\nWe have expanded the DiT framework originally designed for 2D image synthesis\nto accommodate the complexities of 3D video generation by incorporating a\nmotion module block. It is used to capture temporal dynamics, thereby ensuring\nthe production of consistent frames and seamless motion transitions. The motion\nmodule can be adapted to various DiT baseline methods to generate video with\ndifferent styles. It can also generate videos with different frame rates and\nresolutions during both training and inference phases, suitable for both images\nand videos. Moreover, we introduce slice VAE, a novel approach to condense the\ntemporal axis, facilitating the generation of long duration videos. Currently,\nEasyAnimate exhibits the proficiency to generate videos with 144 frames. We\nprovide a holistic ecosystem for video production based on DiT, encompassing\naspects such as data pre-processing, VAE training, DiT models training (both\nthe baseline model and LoRA model), and end-to-end video inference. Code is\navailable at: https://github.com/aigc-apps/EasyAnimate. We are continuously\nworking to enhance the performance of our method.", "published": "2024-05-29 11:11:07", "link": "http://arxiv.org/abs/2405.18991v2", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Evaluating the External and Parametric Knowledge Fusion of Large\n  Language Models", "abstract": "Integrating external knowledge into large language models (LLMs) presents a\npromising solution to overcome the limitations imposed by their antiquated and\nstatic parametric memory. Prior studies, however, have tended to over-reliance\non external knowledge, underestimating the valuable contributions of an LLMs'\nintrinsic parametric knowledge. The efficacy of LLMs in blending external and\nparametric knowledge remains largely unexplored, especially in cases where\nexternal knowledge is incomplete and necessitates supplementation by their\nparametric knowledge. We propose to deconstruct knowledge fusion into four\ndistinct scenarios, offering the first thorough investigation of LLM behavior\nacross each. We develop a systematic pipeline for data construction and\nknowledge infusion to simulate these fusion scenarios, facilitating a series of\ncontrolled experiments. Our investigation reveals that enhancing parametric\nknowledge within LLMs can significantly bolster their capability for knowledge\nintegration. Nonetheless, we identify persistent challenges in memorizing and\neliciting parametric knowledge, and determining parametric knowledge\nboundaries. Our findings aim to steer future explorations on harmonizing\nexternal and parametric knowledge within LLMs.", "published": "2024-05-29 11:48:27", "link": "http://arxiv.org/abs/2405.19010v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "DiveR-CT: Diversity-enhanced Red Teaming Large Language Model Assistants\n  with Relaxing Constraints", "abstract": "Recent advances in large language model assistants have made them\nindispensable, raising significant concerns over managing their safety.\nAutomated red teaming offers a promising alternative to the labor-intensive and\nerror-prone manual probing for vulnerabilities, providing more consistent and\nscalable safety evaluations. However, existing approaches often compromise\ndiversity by focusing on maximizing attack success rate. Additionally, methods\nthat decrease the cosine similarity from historical embeddings with semantic\ndiversity rewards lead to novelty stagnation as history grows. To address these\nissues, we introduce DiveR-CT, which relaxes conventional constraints on the\nobjective and semantic reward, granting greater freedom for the policy to\nenhance diversity. Our experiments demonstrate DiveR-CT's marked superiority\nover baselines by 1) generating data that perform better in various diversity\nmetrics across different attack success rate levels, 2) better-enhancing\nresiliency in blue team models through safety tuning based on collected data,\n3) allowing dynamic control of objective weights for reliable and controllable\nattack success rates, and 4) reducing susceptibility to reward\noveroptimization. Overall, our method provides an effective and efficient\napproach to LLM red teaming, accelerating real-world deployment.", "published": "2024-05-29 12:12:09", "link": "http://arxiv.org/abs/2405.19026v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "BLSP-KD: Bootstrapping Language-Speech Pre-training via Knowledge\n  Distillation", "abstract": "Recent end-to-end approaches have shown promise in extending large language\nmodels (LLMs) to speech inputs, but face limitations in directly assessing and\noptimizing alignment quality and fail to achieve fine-grained alignment due to\nspeech-text length mismatch. We introduce BLSP-KD, a novel approach for\nBootstrapping Language-Speech Pretraining via Knowledge Distillation, which\naddresses these limitations through two key techniques. First, it optimizes\nspeech-text alignment by minimizing the divergence between the LLM's next-token\nprediction distributions for speech and text inputs using knowledge\ndistillation. Second, it employs a continuous-integrate-andfire strategy to\nsegment speech into tokens that correspond one-to-one with text tokens,\nenabling fine-grained alignment. We also introduce Partial LoRA (PLoRA), a new\nadaptation method supporting LLM finetuning for speech inputs under knowledge\ndistillation. Quantitative evaluation shows that BLSP-KD outperforms previous\nend-to-end baselines and cascaded systems with comparable scale of parameters,\nfacilitating general instruction-following capabilities for LLMs with speech\ninputs. This approach provides new possibilities for extending LLMs to spoken\nlanguage interactions.", "published": "2024-05-29 12:32:08", "link": "http://arxiv.org/abs/2405.19041v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MetaToken: Detecting Hallucination in Image Descriptions by Meta\n  Classification", "abstract": "Large Vision Language Models (LVLMs) have shown remarkable capabilities in\nmultimodal tasks like visual question answering or image captioning. However,\ninconsistencies between the visual information and the generated text, a\nphenomenon referred to as hallucinations, remain an unsolved problem with\nregard to the trustworthiness of LVLMs. To address this problem, recent works\nproposed to incorporate computationally costly Large (Vision) Language Models\nin order to detect hallucinations on a sentence- or subsentence-level. In this\nwork, we introduce MetaToken, a lightweight binary classifier to detect\nhallucinations on the token-level at negligible cost. Based on a statistical\nanalysis, we reveal key factors of hallucinations in LVLMs. MetaToken can be\napplied to any open-source LVLM without any knowledge about ground truth data\nproviding a calibrated detection of hallucinations. We evaluate our method on\nfour state-of-the-art LVLMs demonstrating the effectiveness of our approach.", "published": "2024-05-29 15:28:42", "link": "http://arxiv.org/abs/2405.19186v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on\n  Long Videos", "abstract": "Long-form video understanding is complicated by the high redundancy of video\ndata and the abundance of query-irrelevant information. To tackle these\nchallenges, we propose VideoTree, a training-free framework which builds a\nquery-adaptive and hierarchical video representation for LLM reasoning over\nlong-form videos. First, VideoTree extracts query-relevant information from the\ninput video through an iterative process, progressively refining the selection\nof keyframes based on their relevance to the query. Furthermore, VideoTree\nleverages the inherent hierarchical structure of long video data, which is\noften overlooked by existing LLM-based methods. Specifically, we incorporate\nmulti-granularity information into a tree-based representation, allowing\nVideoTree to extract query-relevant details from long videos in a\ncoarse-to-fine manner. This enables the model to effectively handle a wide\nrange of video queries with varying levels of detail. Finally, VideoTree\naggregates the hierarchical query-relevant information within the tree\nstructure and feeds it into an LLM reasoning model to answer the query. Our\nexperiments show that our method improves both reasoning accuracy and\nefficiency. Specifically, VideoTree outperforms existing training-free\napproaches on EgoSchema and NExT-QA with less inference time, achieving 61.1%\nand 75.6% accuracy on the test set without additional video-specific training.\nMoreover, on the long split of Video-MME (average 44 minutes), VideoTree\nachieves better performance than GPT-4V and many other MLLMs that were\nextensively trained on video data.", "published": "2024-05-29 15:49:09", "link": "http://arxiv.org/abs/2405.19209v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Faster Cascades via Speculative Decoding", "abstract": "Cascades and speculative decoding are two common approaches to improving\nlanguage models' inference efficiency. Both approaches involve interleaving\nmodels of different sizes, but via fundamentally distinct mechanisms: cascades\nemploy a deferral rule that invokes the larger model only for \"hard\" inputs,\nwhile speculative decoding uses speculative execution to primarily invoke the\nlarger model in parallel verification mode. These mechanisms offer different\nbenefits: empirically, cascades offer better cost-quality trade-offs, often\neven outperforming the large model, while theoretically, speculative decoding\noffers a guarantee of quality-neutrality. In this paper, we leverage the best\nof both these approaches by designing new speculative cascading techniques that\nimplement their deferral rule through speculative execution. We characterize\nthe optimal deferral rule for our speculative cascades, and employ a plug-in\napproximation to the optimal rule. Experiments with Gemma and T5 models on a\nrange of language benchmarks show that our approach yields better cost quality\ntrade-offs than cascading and speculative decoding baselines.", "published": "2024-05-29 16:55:08", "link": "http://arxiv.org/abs/2405.19261v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Weak-to-Strong Search: Align Large Language Models via Searching over\n  Small Language Models", "abstract": "Large language models are usually fine-tuned to align with human preferences.\nHowever, fine-tuning a large language model can be challenging. In this work,\nwe introduce $\\textit{weak-to-strong search}$, framing the alignment of a large\nlanguage model as a test-time greedy search to maximize the log-probability\ndifference between small tuned and untuned models while sampling from the\nfrozen large model. This method serves both as (1) a compute-efficient model\nup-scaling strategy that avoids directly tuning the large model and as (2) an\ninstance of weak-to-strong generalization that enhances a strong model with\nweak test-time guidance. Empirically, we demonstrate the flexibility of\nweak-to-strong search across different tasks. In controlled-sentiment\ngeneration and summarization, we use tuned and untuned $\\texttt{gpt2}$s to\nimprove the alignment of large models without additional training. Crucially,\nin a more difficult instruction-following benchmark, AlpacaEval 2.0, we show\nthat reusing off-the-shelf small models (e.g., $\\texttt{zephyr-7b-beta}$ and\nits untuned version) can improve the length-controlled win rates of both\nwhite-box and black-box large models against $\\texttt{gpt-4-turbo}$ (e.g.,\n$34.4\\% \\rightarrow 37.9\\%$ for $\\texttt{Llama-3-70B-Instruct}$ and $16.0\\%\n\\rightarrow 20.1\\%$ for $\\texttt{gpt-3.5-turbo-instruct}$), despite the small\nmodels' low win rates $\\approx 10.0\\%$.", "published": "2024-05-29 16:55:32", "link": "http://arxiv.org/abs/2405.19262v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Models Trained to do Arithmetic Predict Human Risky and\n  Intertemporal Choice", "abstract": "The observed similarities in the behavior of humans and Large Language Models\n(LLMs) have prompted researchers to consider the potential of using LLMs as\nmodels of human cognition. However, several significant challenges must be\naddressed before LLMs can be legitimately regarded as cognitive models. For\ninstance, LLMs are trained on far more data than humans typically encounter,\nand may have been directly trained on human data in specific cognitive tasks or\naligned with human preferences. Consequently, the origins of these behavioral\nsimilarities are not well understood. In this paper, we propose a novel way to\nenhance the utility of LLMs as cognitive models. This approach involves (i)\nleveraging computationally equivalent tasks that both an LLM and a rational\nagent need to master for solving a cognitive problem and (ii) examining the\nspecific task distributions required for an LLM to exhibit human-like\nbehaviors. We apply this approach to decision-making -- specifically risky and\nintertemporal choice -- where the key computationally equivalent task is the\narithmetic of expected value calculations. We show that an LLM pretrained on an\necologically valid arithmetic dataset, which we call Arithmetic-GPT, predicts\nhuman behavior better than many traditional cognitive models. Pretraining LLMs\non ecologically valid arithmetic datasets is sufficient to produce a strong\ncorrespondence between these models and human decision-making. Our results also\nsuggest that LLMs used as cognitive models should be carefully investigated via\nablation studies of the pretraining data.", "published": "2024-05-29 17:37:14", "link": "http://arxiv.org/abs/2405.19313v1", "categories": ["cs.AI", "cs.CL", "econ.GN", "q-fin.EC"], "primary_category": "cs.AI"}
{"title": "Matryoshka Query Transformer for Large Vision-Language Models", "abstract": "Large Vision-Language Models (LVLMs) typically encode an image into a fixed\nnumber of visual tokens (e.g., 576) and process these tokens with a language\nmodel. Despite their strong performance, LVLMs face challenges in adapting to\nvarying computational constraints. This raises the question: can we achieve\nflexibility in the number of visual tokens to suit different tasks and\ncomputational resources? We answer this with an emphatic yes. Inspired by\nMatryoshka Representation Learning, we introduce the Matryoshka Query\nTransformer (MQT), capable of encoding an image into m visual tokens during\ninference, where m can be any number up to a predefined maximum. This is\nachieved by employing a query transformer with M latent query tokens to\ncompress the visual embeddings. During each training step, we randomly select m\n<= M latent query tokens and train the model using only these first m tokens,\ndiscarding the rest. Combining MQT with LLaVA, we train a single model once,\nand flexibly and drastically reduce the number of inference-time visual tokens\nwhile maintaining similar or better performance compared to training\nindependent models for each number of tokens. Our model, MQT-LLAVA, matches\nLLaVA-1.5 performance across 11 benchmarks using a maximum of 256 tokens\ninstead of LLaVA's fixed 576. Reducing to 16 tokens (8x less TFLOPs) only\nsacrifices the performance by 2.4 points on MMBench. On certain tasks such as\nScienceQA and MMMU, we can even go down to only 2 visual tokens with\nperformance drops of just 3% and 6% each. Our exploration of the trade-off\nbetween the accuracy and computational cost brought about by the number of\nvisual tokens facilitates future research to achieve the best of both worlds.", "published": "2024-05-29 17:39:42", "link": "http://arxiv.org/abs/2405.19315v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Are Large Language Models Chameleons? An Attempt to Simulate Social\n  Surveys", "abstract": "Can large language models (LLMs) simulate social surveys? To answer this\nquestion, we conducted millions of simulations in which LLMs were asked to\nanswer subjective questions. A comparison of different LLM responses with the\nEuropean Social Survey (ESS) data suggests that the effect of prompts on bias\nand variability is fundamental, highlighting major cultural, age, and gender\nbiases. We further discussed statistical methods for measuring the difference\nbetween LLM answers and survey data and proposed a novel measure inspired by\nJaccard similarity, as LLM-generated responses are likely to have a smaller\nvariance. Our experiments also reveal that it is important to analyze the\nrobustness and variability of prompts before using LLMs to simulate social\nsurveys, as their imitation abilities are approximate at best.", "published": "2024-05-29 17:54:22", "link": "http://arxiv.org/abs/2405.19323v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model\n  Series", "abstract": "Large Language Models (LLMs) have made great strides in recent years to\nachieve unprecedented performance across different tasks. However, due to\ncommercial interest, the most competitive models like GPT, Gemini, and Claude\nhave been gated behind proprietary interfaces without disclosing the training\ndetails. Recently, many institutions have open-sourced several strong LLMs like\nLLaMA-3, comparable to existing closed-source LLMs. However, only the model's\nweights are provided with most details (e.g., intermediate checkpoints,\npre-training corpus, and training code, etc.) being undisclosed. To improve the\ntransparency of LLMs, the research community has formed to open-source truly\nopen LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training\ncorpus and training code) are being provided. These models have greatly\nadvanced the scientific study of these large models including their strengths,\nweaknesses, biases and risks. However, we observe that the existing truly open\nLLMs on reasoning, knowledge, and coding tasks are still inferior to existing\nstate-of-the-art LLMs with similar model sizes. To this end, we open-source\nMAP-Neo, a highly capable and transparent bilingual language model with 7B\nparameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is the\nfirst fully open-sourced bilingual LLM with comparable performance compared to\nexisting state-of-the-art LLMs. Moreover, we open-source all details to\nreproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning\npipeline, checkpoints, and well-optimized training/evaluation framework are\nprovided. Finally, we hope our MAP-Neo will enhance and strengthen the open\nresearch community and inspire more innovations and creativities to facilitate\nthe further improvements of LLMs.", "published": "2024-05-29 17:57:16", "link": "http://arxiv.org/abs/2405.19327v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "X-VILA: Cross-Modality Alignment for Large Language Model", "abstract": "We introduce X-VILA, an omni-modality model designed to extend the\ncapabilities of large language models (LLMs) by incorporating image, video, and\naudio modalities. By aligning modality-specific encoders with LLM inputs and\ndiffusion decoders with LLM outputs, X-VILA achieves cross-modality\nunderstanding, reasoning, and generation. To facilitate this cross-modality\nalignment, we curate an effective interleaved any-to-any modality\ninstruction-following dataset. Furthermore, we identify a significant problem\nwith the current cross-modality alignment method, which results in visual\ninformation loss. To address the issue, we propose a visual alignment mechanism\nwith a visual embedding highway module. We then introduce a resource-efficient\nrecipe for training X-VILA, that exhibits proficiency in any-to-any modality\nconversation, surpassing previous approaches by large margins. X-VILA also\nshowcases emergent properties across modalities even in the absence of similar\ntraining data. The project will be made open-source.", "published": "2024-05-29 17:59:58", "link": "http://arxiv.org/abs/2405.19335v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Deep Learning for Assessment of Oral Reading Fluency", "abstract": "Reading fluency assessment is a critical component of literacy programmes,\nserving to guide and monitor early education interventions. Given the resource\nintensive nature of the exercise when conducted by teachers, the development of\nautomatic tools that can operate on audio recordings of oral reading is\nattractive as an objective and highly scalable solution. Multiple complex\naspects such as accuracy, rate and expressiveness underlie human judgements of\nreading fluency. In this work, we investigate end-to-end modeling on a training\ndataset of children's audio recordings of story texts labeled by human experts.\nThe pre-trained wav2vec2.0 model is adopted due its potential to alleviate the\nchallenges from the limited amount of labeled data. We report the performance\nof a number of system variations on the relevant measures, and also probe the\nlearned embeddings for lexical and acoustic-prosodic features known to be\nimportant to the perception of reading fluency.", "published": "2024-05-29 18:09:35", "link": "http://arxiv.org/abs/2405.19426v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Preference Learning Algorithms Do Not Learn Preference Rankings", "abstract": "Preference learning algorithms (e.g., RLHF and DPO) are frequently used to\nsteer LLMs to produce generations that are more preferred by humans, but our\nunderstanding of their inner workings is still limited. In this work, we study\nthe conventional wisdom that preference learning trains models to assign higher\nlikelihoods to more preferred outputs than less preferred outputs, measured via\nranking accuracy. Surprisingly, we find that most state-of-the-art\npreference-tuned models achieve a ranking accuracy of less than 60% on common\npreference datasets. We furthermore derive the idealized ranking accuracy that\na preference-tuned LLM would achieve if it optimized the DPO or RLHF objective\nperfectly. We demonstrate that existing models exhibit a significant alignment\ngap -- i.e., a gap between the observed and idealized ranking accuracies. We\nattribute this discrepancy to the DPO objective, which is empirically and\ntheoretically ill-suited to fix even mild ranking errors in the reference\nmodel, and derive a simple and efficient formula for quantifying the difficulty\nof learning a given preference datapoint. Finally, we demonstrate that ranking\naccuracy strongly correlates with the empirically popular win rate metric when\nthe model is close to the reference model used in the objective, shedding\nfurther light on the differences between on-policy (e.g., RLHF) and off-policy\n(e.g., DPO) preference learning algorithms.", "published": "2024-05-29 21:29:44", "link": "http://arxiv.org/abs/2405.19534v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CheXpert Plus: Augmenting a Large Chest X-ray Dataset with Text\n  Radiology Reports, Patient Demographics and Additional Image Formats", "abstract": "Since the release of the original CheXpert paper five years ago, CheXpert has\nbecome one of the most widely used and cited clinical AI datasets. The\nemergence of vision language models has sparked an increase in demands for\nsharing reports linked to CheXpert images, along with a growing interest among\nAI fairness researchers in obtaining demographic data. To address this,\nCheXpert Plus serves as a new collection of radiology data sources, made\npublicly available to enhance the scaling, performance, robustness, and\nfairness of models for all subsequent machine learning tasks in the field of\nradiology. CheXpert Plus is the largest text dataset publicly released in\nradiology, with a total of 36 million text tokens, including 13 million\nimpression tokens. To the best of our knowledge, it represents the largest text\nde-identification effort in radiology, with almost 1 million PHI spans\nanonymized. It is only the second time that a large-scale English paired\ndataset has been released in radiology, thereby enabling, for the first time,\ncross-institution training at scale. All reports are paired with high-quality\nimages in DICOM format, along with numerous image and patient metadata covering\nvarious clinical and socio-economic groups, as well as many pathology labels\nand RadGraph annotations. We hope this dataset will boost research for AI\nmodels that can further assist radiologists and help improve medical care. Data\nis available at the following URL:\nhttps://stanfordaimi.azurewebsites.net/datasets/5158c524-d3ab-4e02-96e9-6ee9efc110a1\nModels are available at the following URL:\nhttps://github.com/Stanford-AIMI/chexpert-plus", "published": "2024-05-29 21:48:56", "link": "http://arxiv.org/abs/2405.19538v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Selective Explanations", "abstract": "Feature attribution methods explain black-box machine learning (ML) models by\nassigning importance scores to input features. These methods can be\ncomputationally expensive for large ML models. To address this challenge, there\nhas been increasing efforts to develop amortized explainers, where a machine\nlearning model is trained to predict feature attribution scores with only one\ninference. Despite their efficiency, amortized explainers can produce\ninaccurate predictions and misleading explanations. In this paper, we propose\nselective explanations, a novel feature attribution method that (i) detects\nwhen amortized explainers generate low-quality explanations and (ii) improves\nthese explanations using a technique called explanations with initial guess.\nOur selective explanation method allows practitioners to specify the fraction\nof samples that receive explanations with initial guess, offering a principled\nway to bridge the gap between amortized explainers and their high-quality\ncounterparts.", "published": "2024-05-29 23:08:31", "link": "http://arxiv.org/abs/2405.19562v1", "categories": ["cs.CY", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Dr-LLaVA: Visual Instruction Tuning with Symbolic Clinical Grounding", "abstract": "Vision-Language Models (VLM) can support clinicians by analyzing medical\nimages and engaging in natural language interactions to assist in diagnostic\nand treatment tasks. However, VLMs often exhibit \"hallucinogenic\" behavior,\ngenerating textual outputs not grounded in contextual multimodal information.\nThis challenge is particularly pronounced in the medical domain, where we do\nnot only require VLM outputs to be accurate in single interactions but also to\nbe consistent with clinical reasoning and diagnostic pathways throughout\nmulti-turn conversations. For this purpose, we propose a new alignment\nalgorithm that uses symbolic representations of clinical reasoning to ground\nVLMs in medical knowledge. These representations are utilized to (i) generate\nGPT-4-guided visual instruction tuning data at scale, simulating clinician-VLM\nconversations with demonstrations of clinical reasoning, and (ii) create an\nautomatic reward function that evaluates the clinical validity of VLM\ngenerations throughout clinician-VLM interactions. Our algorithm eliminates the\nneed for human involvement in training data generation or reward model\nconstruction, reducing costs compared to standard reinforcement learning with\nhuman feedback (RLHF). We apply our alignment algorithm to develop Dr-LLaVA, a\nconversational VLM finetuned for analyzing bone marrow pathology slides,\ndemonstrating strong performance in multi-turn medical conversations.", "published": "2024-05-29 23:19:28", "link": "http://arxiv.org/abs/2405.19567v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Conveyor: Efficient Tool-aware LLM Serving with Tool Partial Execution", "abstract": "The complexity of large language model (LLM) serving workloads has\nsubstantially increased due to the integration with external tool invocations,\nsuch as ChatGPT plugins. In this paper, we identify a new opportunity for\nefficient LLM serving for requests that trigger tools: tool partial execution\nalongside LLM decoding. To this end, we design Conveyor, an efficient LLM\nserving system optimized for handling requests involving external tools. We\nintroduce a novel interface for tool developers to expose partial execution\nopportunities to the LLM serving system and a request scheduler that\nfacilitates partial tool execution. Our results demonstrate that tool partial\nexecution can improve request completion latency by up to 38.8%.", "published": "2024-05-29 21:24:15", "link": "http://arxiv.org/abs/2406.00059v2", "categories": ["cs.CL", "cs.DC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "STAT: Shrinking Transformers After Training", "abstract": "We present STAT: a simple algorithm to prune transformer models without any\nfine-tuning. STAT eliminates both attention heads and neurons from the network,\nwhile preserving accuracy by calculating a correction to the weights of the\nnext layer. Each layer block in the network is compressed using a series of\nprincipled matrix factorizations that preserve the network structure. Our\nentire algorithm takes minutes to compress BERT, and less than three hours to\ncompress models with 7B parameters using a single GPU. Using only several\nhundred data examples, STAT preserves the output of the network and improves\nupon existing gradient-free pruning methods. It is even competitive with\nmethods that include significant fine-tuning. We demonstrate our method on both\nencoder and decoder architectures, including BERT, DistilBERT, and Llama-2\nusing benchmarks such as GLUE, Squad, WikiText2.", "published": "2024-05-29 22:59:11", "link": "http://arxiv.org/abs/2406.00061v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Unlocking the Potential of Large Language Models for Clinical Text\n  Anonymization: A Comparative Study", "abstract": "Automated clinical text anonymization has the potential to unlock the\nwidespread sharing of textual health data for secondary usage while assuring\npatient privacy and safety. Despite the proposal of many complex and\ntheoretically successful anonymization solutions in literature, these\ntechniques remain flawed. As such, clinical institutions are still reluctant to\napply them for open access to their data. Recent advances in developing Large\nLanguage Models (LLMs) pose a promising opportunity to further the field, given\ntheir capability to perform various tasks. This paper proposes six new\nevaluation metrics tailored to the challenges of generative anonymization with\nLLMs. Moreover, we present a comparative study of LLM-based methods, testing\nthem against two baseline techniques. Our results establish LLM-based models as\na reliable alternative to common approaches, paving the way toward trustworthy\nanonymization of clinical text.", "published": "2024-05-29 23:07:58", "link": "http://arxiv.org/abs/2406.00062v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Cephalo: Multi-Modal Vision-Language Models for Bio-Inspired Materials\n  Analysis and Design", "abstract": "We present Cephalo, a series of multimodal vision large language models\n(V-LLMs) designed for materials science applications, integrating visual and\nlinguistic data for enhanced understanding. A key innovation of Cephalo is its\nadvanced dataset generation method. Cephalo is trained on integrated image and\ntext data from thousands of scientific papers and science-focused Wikipedia\ndata demonstrates can interpret complex visual scenes, generate precise\nlanguage descriptions, and answer queries about images effectively. The\ncombination of a vision encoder with an autoregressive transformer supports\nmultimodal natural language understanding, which can be coupled with other\ngenerative methods to create an image-to-text-to-3D pipeline. To develop more\ncapable models from smaller ones, we report both mixture-of-expert methods and\nmodel merging. We examine the models in diverse use cases that incorporate\nbiological materials, fracture and engineering analysis, protein biophysics,\nand bio-inspired design based on insect behavior. Generative applications\ninclude bio-inspired designs, including pollen-inspired architected materials,\nas well as the synthesis of bio-inspired material microstructures from a\nphotograph of a solar eclipse. Additional model fine-tuning with a series of\nmolecular dynamics results demonstrate Cephalo's enhanced capabilities to\naccurately predict statistical features of stress and atomic energy\ndistributions, as well as crack dynamics and damage in materials.", "published": "2024-05-29 13:34:32", "link": "http://arxiv.org/abs/2405.19076v3", "categories": ["cs.CV", "cond-mat.mes-hall", "cond-mat.mtrl-sci", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "LLMs Meet Multimodal Generation and Editing: A Survey", "abstract": "With the recent advancement in large language models (LLMs), there is a\ngrowing interest in combining LLMs with multimodal learning. Previous surveys\nof multimodal large language models (MLLMs) mainly focus on multimodal\nunderstanding. This survey elaborates on multimodal generation and editing\nacross various domains, comprising image, video, 3D, and audio. Specifically,\nwe summarize the notable advancements with milestone works in these fields and\ncategorize these studies into LLM-based and CLIP/T5-based methods. Then, we\nsummarize the various roles of LLMs in multimodal generation and exhaustively\ninvestigate the critical technical components behind these methods and the\nmultimodal datasets utilized in these studies. Additionally, we dig into\ntool-augmented multimodal agents that can leverage existing generative models\nfor human-computer interaction. Lastly, we discuss the advancements in the\ngenerative AI safety field, investigate emerging applications, and discuss\nfuture prospects. Our work provides a systematic and insightful overview of\nmultimodal generation and processing, which is expected to advance the\ndevelopment of Artificial Intelligence for Generative Content (AIGC) and world\nmodels. A curated list of all related papers can be found at\nhttps://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation", "published": "2024-05-29 17:59:20", "link": "http://arxiv.org/abs/2405.19334v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MM", "cs.SD"], "primary_category": "cs.AI"}
{"title": "One-Shot Safety Alignment for Large Language Models via Optimal\n  Dualization", "abstract": "The growing safety concerns surrounding large language models raise an urgent\nneed to align them with diverse human preferences to simultaneously enhance\ntheir helpfulness and safety. A promising approach is to enforce safety\nconstraints through Reinforcement Learning from Human Feedback (RLHF). For such\nconstrained RLHF, typical Lagrangian-based primal-dual policy optimization\nmethods are computationally expensive and often unstable. This paper presents a\nperspective of dualization that reduces constrained alignment to an equivalent\nunconstrained alignment problem. We do so by pre-optimizing a smooth and convex\ndual function that has a closed form. This shortcut eliminates the need for\ncumbersome primal-dual policy iterations, greatly reducing the computational\nburden and improving training stability. Our strategy leads to two practical\nalgorithms in model-based and preference-based settings (MoCAN and PeCAN,\nrespectively). A broad range of experiments demonstrate the effectiveness and\nmerits of our algorithms.", "published": "2024-05-29 22:12:52", "link": "http://arxiv.org/abs/2405.19544v3", "categories": ["cs.AI", "cs.CL", "cs.LG", "math.OC", "stat.ML"], "primary_category": "cs.AI"}
{"title": "Reverse the auditory processing pathway: Coarse-to-fine audio\n  reconstruction from fMRI", "abstract": "Drawing inspiration from the hierarchical processing of the human auditory\nsystem, which transforms sound from low-level acoustic features to high-level\nsemantic understanding, we introduce a novel coarse-to-fine audio\nreconstruction method. Leveraging non-invasive functional Magnetic Resonance\nImaging (fMRI) data, our approach mimics the inverse pathway of auditory\nprocessing. Initially, we utilize CLAP to decode fMRI data coarsely into a\nlow-dimensional semantic space, followed by a fine-grained decoding into the\nhigh-dimensional AudioMAE latent space guided by semantic features. These\nfine-grained neural features serve as conditions for audio reconstruction\nthrough a Latent Diffusion Model (LDM). Validation on three public fMRI\ndatasets-Brain2Sound, Brain2Music, and Brain2Speech-underscores the superiority\nof our coarse-to-fine decoding method over stand-alone fine-grained approaches,\nshowcasing state-of-the-art performance in metrics like FD, FAD, and KL.\nMoreover, by employing semantic prompts during decoding, we enhance the quality\nof reconstructed audio when semantic features are suboptimal. The demonstrated\nversatility of our model across diverse stimuli highlights its potential as a\nuniversal brain-to-audio framework. This research contributes to the\ncomprehension of the human auditory system, pushing boundaries in neural\ndecoding and audio reconstruction methodologies.", "published": "2024-05-29 03:16:14", "link": "http://arxiv.org/abs/2405.18726v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Gaussian Flow Bridges for Audio Domain Transfer with Unpaired Data", "abstract": "Audio domain transfer is the process of modifying audio signals to match\ncharacteristics of a different domain, while retaining the original content.\nThis paper investigates the potential of Gaussian Flow Bridges, an emerging\napproach in generative modeling, for this problem. The presented framework\naddresses the transport problem across different distributions of audio signals\nthrough the implementation of a series of two deterministic probability flows.\nThe proposed framework facilitates manipulation of the target distribution\nproperties through a continuous control variable, which defines a certain\naspect of the target domain. Notably, this approach does not rely on paired\nexamples for training. To address identified challenges on maintaining the\nspeech content consistent, we recommend a training strategy that incorporates\nchunk-based minibatch Optimal Transport couplings of data samples and noise.\nComparing our unsupervised method with established baselines, we find\ncompetitive performance in tasks of reverberation and distortion manipulation.\nDespite encoutering limitations, the intriguing results obtained in this study\nunderscore potential for further exploration.", "published": "2024-05-29 20:23:01", "link": "http://arxiv.org/abs/2405.19497v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On the Condition Monitoring of Bolted Joints through Acoustic Emission\n  and Deep Transfer Learning: Generalization, Ordinal Loss and\n  Super-Convergence", "abstract": "This paper investigates the use of deep transfer learning based on\nconvolutional neural networks (CNNs) to monitor the condition of bolted joints\nusing acoustic emissions. Bolted structures are critical components in many\nmechanical systems, and the ability to monitor their condition status is\ncrucial for effective structural health monitoring. We evaluated the\nperformance of our methodology using the ORION-AE benchmark, a structure\ncomposed of two thin beams connected by three bolts, where highly noisy\nacoustic emission measurements were taken to detect changes in the applied\ntightening torque of the bolts. The data used from this structure is derived\nfrom the transformation of acoustic emission data streams into images using\ncontinuous wavelet transform, and leveraging pretrained CNNs for feature\nextraction and denoising. Our experiments compared single-sensor versus\nmultiple-sensor fusion for estimating the tightening level (loosening) of bolts\nand evaluated the use of raw versus prefiltered data on the performance. We\nparticularly focused on the generalization capabilities of CNN-based transfer\nlearning across different measurement campaigns and we studied ordinal loss\nfunctions to penalize incorrect predictions less severely when close to the\nground truth, thereby encouraging misclassification errors to be in adjacent\nclasses. Network configurations as well as learning rate schedulers are also\ninvestigated, and super-convergence is obtained, i.e., high classification\naccuracy is achieved in a few number of iterations with different networks.\nFurthermore, results demonstrate the generalization capabilities of CNN-based\ntransfer learning for monitoring bolted structures by acoustic emission with\nvarying amounts of prior information required during training.", "published": "2024-05-29 13:07:21", "link": "http://arxiv.org/abs/2405.20887v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
