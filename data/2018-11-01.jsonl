{"title": "Dial2Desc: End-to-end Dialogue Description Generation", "abstract": "We first propose a new task named Dialogue Description (Dial2Desc). Unlike\nother existing dialogue summarization tasks such as meeting summarization, we\ndo not maintain the natural flow of a conversation but describe an object or an\naction of what people are talking about. The Dial2Desc system takes a dialogue\ntext as input, then outputs a concise description of the object or the action\ninvolved in this conversation. After reading this short description, one can\nquickly extract the main topic of a conversation and build a clear picture in\nhis mind, without reading or listening to the whole conversation. Based on the\nexisting dialogue dataset, we build a new dataset, which has more than one\nhundred thousand dialogue-description pairs. As a step forward, we demonstrate\nthat one can get more accurate and descriptive results using a new neural\nattentive model that exploits the interaction between utterances from different\nspeakers, compared with other baselines.", "published": "2018-11-01 02:10:50", "link": "http://arxiv.org/abs/1811.00185v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Empathetic Open-domain Conversation Models: a New Benchmark and\n  Dataset", "abstract": "One challenge for dialogue agents is recognizing feelings in the conversation\npartner and replying accordingly, a key communicative skill. While it is\nstraightforward for humans to recognize and acknowledge others' feelings in a\nconversation, this is a significant challenge for AI systems due to the paucity\nof suitable publicly-available datasets for training and evaluation. This work\nproposes a new benchmark for empathetic dialogue generation and\nEmpatheticDialogues, a novel dataset of 25k conversations grounded in emotional\nsituations. Our experiments indicate that dialogue models that use our dataset\nare perceived to be more empathetic by human evaluators, compared to models\nmerely trained on large-scale Internet conversation data. We also present\nempirical comparisons of dialogue model adaptations for empathetic responding,\nleveraging existing models or datasets without requiring lengthy re-training of\nthe full model.", "published": "2018-11-01 03:43:10", "link": "http://arxiv.org/abs/1811.00207v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Textbook Question Answering with Multi-modal Context Graph Understanding\n  and Self-supervised Open-set Comprehension", "abstract": "In this work, we introduce a novel algorithm for solving the textbook\nquestion answering (TQA) task which describes more realistic QA problems\ncompared to other recent tasks. We mainly focus on two related issues with\nanalysis of the TQA dataset. First, solving the TQA problems requires to\ncomprehend multi-modal contexts in complicated input data. To tackle this issue\nof extracting knowledge features from long text lessons and merging them with\nvisual features, we establish a context graph from texts and images, and\npropose a new module f-GCN based on graph convolutional networks (GCN). Second,\nscientific terms are not spread over the chapters and subjects are split in the\nTQA dataset. To overcome this so called \"out-of-domain\" issue, before learning\nQA problems, we introduce a novel self-supervised open-set learning process\nwithout any annotations. The experimental results show that our model\nsignificantly outperforms prior state-of-the-art methods. Moreover, ablation\nstudies validate that both methods of incorporating f-GCN for extracting\nknowledge from multi-modal contexts and our newly proposed self-supervised\nlearning process are effective for TQA problems.", "published": "2018-11-01 05:10:44", "link": "http://arxiv.org/abs/1811.00232v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Spelling Error Correction Using a Nested RNN Model and Pseudo Training\n  Data", "abstract": "We propose a nested recurrent neural network (nested RNN) model for English\nspelling error correction and generate pseudo data based on phonetic similarity\nto train it. The model fuses orthographic information and context as a whole\nand is trained in an end-to-end fashion. This avoids feature engineering and\ndoes not rely on a noisy channel model as in traditional methods. Experiments\nshow that the proposed method is superior to existing systems in correcting\nspelling errors.", "published": "2018-11-01 05:21:47", "link": "http://arxiv.org/abs/1811.00238v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GlobalTrait: Personality Alignment of Multilingual Word Embeddings", "abstract": "We propose a multilingual model to recognize Big Five Personality traits from\ntext data in four different languages: English, Spanish, Dutch and Italian. Our\nanalysis shows that words having a similar semantic meaning in different\nlanguages do not necessarily correspond to the same personality traits.\nTherefore, we propose a personality alignment method, GlobalTrait, which has a\nmapping for each trait from the source language to the target language\n(English), such that words that correlate positively to each trait are close\ntogether in the multilingual vector space. Using these aligned embeddings for\ntraining, we can transfer personality related training features from\nhigh-resource languages such as English to other low-resource languages, and\nget better multilingual results, when compared to using simple monolingual and\nunaligned multilingual embeddings. We achieve an average F-score increase\n(across all three languages except English) from 65 to 73.4 (+8.4), when\ncomparing our monolingual model to multilingual using CNN with personality\naligned embeddings. We also show relatively good performance in the regression\ntasks, and better classification results when evaluating our model on a\nseparate Chinese dataset.", "published": "2018-11-01 05:26:27", "link": "http://arxiv.org/abs/1811.00240v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the End-to-End Solution to Mandarin-English Code-switching Speech\n  Recognition", "abstract": "Code-switching (CS) refers to a linguistic phenomenon where a speaker uses\ndifferent languages in an utterance or between alternating utterances. In this\nwork, we study end-to-end (E2E) approaches to the Mandarin-English\ncode-switching speech recognition (CSSR) task. We first examine the\neffectiveness of using data augmentation and byte-pair encoding (BPE) subword\nunits. More importantly, we propose a multitask learning recipe, where a\nlanguage identification task is explicitly learned in addition to the E2E\nspeech recognition task. Furthermore, we introduce an efficient word vocabulary\nexpansion method for language modeling to alleviate data sparsity issues under\nthe code-switching scenario. Experimental results on the SEAME data, a\nMandarin-English CS corpus, demonstrate the effectiveness of the proposed\nmethods.", "published": "2018-11-01 05:29:02", "link": "http://arxiv.org/abs/1811.00241v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hybrid Self-Attention Network for Machine Translation", "abstract": "The encoder-decoder is the typical framework for Neural Machine Translation\n(NMT), and different structures have been developed for improving the\ntranslation performance. Transformer is one of the most promising structures,\nwhich can leverage the self-attention mechanism to capture the semantic\ndependency from global view. However, it cannot distinguish the relative\nposition of different tokens very well, such as the tokens located at the left\nor right of the current token, and cannot focus on the local information around\nthe current token either. To alleviate these problems, we propose a novel\nattention mechanism named Hybrid Self-Attention Network (HySAN) which\naccommodates some specific-designed masks for self-attention network to extract\nvarious semantic, such as the global/local information, the left/right part\ncontext. Finally, a squeeze gate is introduced to combine different kinds of\nSANs for fusion. Experimental results on three machine translation tasks show\nthat our proposed framework outperforms the Transformer baseline significantly\nand achieves superior results over state-of-the-art NMT systems.", "published": "2018-11-01 06:35:21", "link": "http://arxiv.org/abs/1811.00253v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language-Independent Representor for Neural Machine Translation", "abstract": "Current Neural Machine Translation (NMT) employs a language-specific encoder\nto represent the source sentence and adopts a language-specific decoder to\ngenerate target translation. This language-dependent design leads to\nlarge-scale network parameters and makes the duality of the parallel data\nunderutilized. To address the problem, we propose in this paper a\nlanguage-independent representor to replace the encoder and decoder by using\nweight sharing. This shared representor can not only reduce large portion of\nnetwork parameters, but also facilitate us to fully explore the language\nduality by jointly training source-to-target, target-to-source, left-to-right\nand right-to-left translations within a multi-task learning framework.\nExperiments show that our proposed framework can obtain significant\nimprovements over conventional NMT models on resource-rich and low-resource\ntranslation tasks with only a quarter of parameters.", "published": "2018-11-01 06:47:23", "link": "http://arxiv.org/abs/1811.00258v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Describe Phrases with Local and Global Contexts", "abstract": "When reading a text, it is common to become stuck on unfamiliar words and\nphrases, such as polysemous words with novel senses, rarely used idioms,\ninternet slang, or emerging entities. If we humans cannot figure out the\nmeaning of those expressions from the immediate local context, we consult\ndictionaries for definitions or search documents or the web to find other\nglobal context to help in interpretation. Can machines help us do this work?\nWhich type of context is more important for machines to solve the problem? To\nanswer these questions, we undertake a task of describing a given phrase in\nnatural language based on its local and global contexts. To solve this task, we\npropose a neural description model that consists of two context encoders and a\ndescription decoder. In contrast to the existing methods for non-standard\nEnglish explanation [Ni+ 2017] and definition generation [Noraset+ 2017;\nGadetsky+ 2018], our model appropriately takes important clues from both local\nand global contexts. Experimental results on three existing datasets (including\nWordNet, Oxford and Urban Dictionaries) and a dataset newly created from\nWikipedia demonstrate the effectiveness of our method over previous work.", "published": "2018-11-01 07:18:33", "link": "http://arxiv.org/abs/1811.00266v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Unsupervised Word Mapping by Maximizing Mean Discrepancy", "abstract": "Cross-lingual word embeddings aim to capture common linguistic regularities\nof different languages, which benefit various downstream tasks ranging from\nmachine translation to transfer learning. Recently, it has been shown that\nthese embeddings can be effectively learned by aligning two disjoint\nmonolingual vector spaces through a linear transformation (word mapping). In\nthis work, we focus on learning such a word mapping without any supervision\nsignal. Most previous work of this task adopts parametric metrics to measure\ndistribution differences, which typically requires a sophisticated alternate\noptimization process, either in the form of \\emph{minmax game} or intermediate\n\\emph{density estimation}. This alternate optimization process is relatively\nhard and unstable. In order to avoid such sophisticated alternate optimization,\nwe propose to learn unsupervised word mapping by directly maximizing the mean\ndiscrepancy between the distribution of transferred embedding and target\nembedding. Extensive experimental results show that our proposed model\noutperforms competitive baselines by a large margin.", "published": "2018-11-01 07:54:31", "link": "http://arxiv.org/abs/1811.00275v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Linear Time Neural Machine Translation with Capsule Networks", "abstract": "In this study, we first investigate a novel capsule network with dynamic\nrouting for linear time Neural Machine Translation (NMT), referred as\n\\textsc{CapsNMT}. \\textsc{CapsNMT} uses an aggregation mechanism to map the\nsource sentence into a matrix with pre-determined size, and then applys a deep\nLSTM network to decode the target sequence from the source representation.\nUnlike the previous work \\cite{sutskever2014sequence} to store the source\nsentence with a passive and bottom-up way, the dynamic routing policy encodes\nthe source sentence with an iterative process to decide the credit attribution\nbetween nodes from lower and higher layers. \\textsc{CapsNMT} has two core\nproperties: it runs in time that is linear in the length of the sequences and\nprovides a more flexible way to select, represent and aggregates the part-whole\ninformation of the source sentence. On WMT14 English-German task and a larger\nWMT14 English-French task, \\textsc{CapsNMT} achieves comparable results with\nthe state-of-the-art NMT systems. To the best of our knowledge, this is the\nfirst work that capsule networks have been empirically investigated for\nsequence to sequence problems.", "published": "2018-11-01 09:37:48", "link": "http://arxiv.org/abs/1811.00287v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How2: A Large-scale Dataset for Multimodal Language Understanding", "abstract": "In this paper, we introduce How2, a multimodal collection of instructional\nvideos with English subtitles and crowdsourced Portuguese translations. We also\npresent integrated sequence-to-sequence baselines for machine translation,\nautomatic speech recognition, spoken language translation, and multimodal\nsummarization. By making available data and code for several multimodal natural\nlanguage tasks, we hope to stimulate more research on these and similar\nchallenges, to obtain a deeper understanding of multimodality in language\nprocessing.", "published": "2018-11-01 12:47:11", "link": "http://arxiv.org/abs/1811.00347v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Latent Variable Model for Multi-modal Translation", "abstract": "In this work, we propose to model the interaction between visual and textual\nfeatures for multi-modal neural machine translation (MMT) through a latent\nvariable model. This latent variable can be seen as a multi-modal stochastic\nembedding of an image and its description in a foreign language. It is used in\na target-language decoder and also to predict image features. Importantly, our\nmodel formulation utilises visual and textual inputs during training but does\nnot require that images be available at test time. We show that our latent\nvariable MMT formulation improves considerably over strong baselines, including\na multi-task learning approach (Elliott and K\\'ad\\'ar, 2017) and a conditional\nvariational auto-encoder approach (Toyama et al., 2016). Finally, we show\nimprovements due to (i) predicting image features in addition to only\nconditioning on them, (ii) imposing a constraint on the minimum amount of\ninformation encoded in the latent variable, and (iii) by training on additional\ntarget-language image descriptions (i.e. synthetic data).", "published": "2018-11-01 13:19:27", "link": "http://arxiv.org/abs/1811.00357v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Helping each Other: A Framework for Customer-to-Customer Suggestion\n  Mining using a Semi-supervised Deep Neural Network", "abstract": "Suggestion mining is increasingly becoming an important task along with\nsentiment analysis. In today's cyberspace world, people not only express their\nsentiments and dispositions towards some entities or services, but they also\nspend considerable time sharing their experiences and advice to fellow\ncustomers and the product/service providers with two-fold agenda: helping\nfellow customers who are likely to share a similar experience, and motivating\nthe producer to bring specific changes in their offerings which would be more\nappreciated by the customers. In our current work, we propose a hybrid deep\nlearning model to identify whether a review text contains any suggestion. The\nmodel employs semi-supervised learning to leverage the useful information from\nthe large amount of unlabeled data. We evaluate the performance of our proposed\nmodel on a benchmark customer review dataset, comprising of the reviews of\nHotel and Electronics domains. Our proposed approach shows the F-scores of\n65.6% and 65.5% for the Hotel and Electronics review datasets, respectively.\nThese performances are significantly better compared to the existing\nstate-of-the-art system.", "published": "2018-11-01 13:49:58", "link": "http://arxiv.org/abs/1811.00379v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Addressing word-order Divergence in Multilingual Neural Machine\n  Translation for extremely Low Resource Languages", "abstract": "Transfer learning approaches for Neural Machine Translation (NMT) train a NMT\nmodel on the assisting-target language pair (parent model) which is later\nfine-tuned for the source-target language pair of interest (child model), with\nthe target language being the same. In many cases, the assisting language has a\ndifferent word order from the source language. We show that divergent word\norder adversely limits the benefits from transfer learning when little to no\nparallel corpus between the source and target language is available. To bridge\nthis divergence, We propose to pre-order the assisting language sentence to\nmatch the word order of the source language and train the parent model. Our\nexperiments on many language pairs show that bridging the word order gap leads\nto significant improvement in the translation quality.", "published": "2018-11-01 13:53:27", "link": "http://arxiv.org/abs/1811.00383v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DialogueRNN: An Attentive RNN for Emotion Detection in Conversations", "abstract": "Emotion detection in conversations is a necessary step for a number of\napplications, including opinion mining over chat history, social media threads,\ndebates, argumentation mining, understanding consumer feedback in live\nconversations, etc. Currently, systems do not treat the parties in the\nconversation individually by adapting to the speaker of each utterance. In this\npaper, we describe a new method based on recurrent neural networks that keeps\ntrack of the individual party states throughout the conversation and uses this\ninformation for emotion classification. Our model outperforms the state of the\nart by a significant margin on two different datasets.", "published": "2018-11-01 14:27:19", "link": "http://arxiv.org/abs/1811.00405v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Dual-Cascade Learning with Pseudo-Feedback Distillation for\n  Query-based Extractive Summarization", "abstract": "We propose Dual-CES -- a novel unsupervised, query-focused, multi-document\nextractive summarizer. Dual-CES is designed to better handle the tradeoff\nbetween saliency and focus in summarization. To this end, Dual-CES employs a\ntwo-step dual-cascade optimization approach with saliency-based pseudo-feedback\ndistillation. Overall, Dual-CES significantly outperforms all other\nstate-of-the-art unsupervised alternatives. Dual-CES is even shown to be able\nto outperform strong supervised summarizers.", "published": "2018-11-01 15:32:38", "link": "http://arxiv.org/abs/1811.00436v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual NMT with a language-independent attention bridge", "abstract": "In this paper, we propose a multilingual encoder-decoder architecture capable\nof obtaining multilingual sentence representations by means of incorporating an\nintermediate {\\em attention bridge} that is shared across all languages. That\nis, we train the model with language-specific encoders and decoders that are\nconnected via self-attention with a shared layer that we call attention bridge.\nThis layer exploits the semantics from each language for performing translation\nand develops into a language-independent meaning representation that can\nefficiently be used for transfer learning. We present a new framework for the\nefficient development of multilingual NMT using this model and scheduled\ntraining. We have tested the approach in a systematic way with a multi-parallel\ndata set. We show that the model achieves substantial improvements over strong\nbilingual models and that it also works well for zero-shot translation, which\ndemonstrates its ability of abstraction and transfer learning.", "published": "2018-11-01 17:06:09", "link": "http://arxiv.org/abs/1811.00498v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Coherent and Cohesive Long-form Text Generation", "abstract": "Generating coherent and cohesive long-form texts is a challenging task.\nPrevious works relied on large amounts of human-generated texts to train neural\nlanguage models. However, few attempted to explicitly improve neural language\nmodels from the perspectives of coherence and cohesion. In this work, we\npropose a new neural language model that is equipped with two neural\ndiscriminators which provide feedback signals at the levels of sentence\n(cohesion) and paragraph (coherence). Our model is trained using a simple yet\nefficient variant of policy gradient, called negative-critical sequence\ntraining, which is proposed to eliminate the need of training a separate critic\nfor estimating baseline. Results demonstrate the effectiveness of our approach,\nshowing improvements over the strong baseline -- recurrent attention-based\nbidirectional MLE-trained neural language model.", "published": "2018-11-01 17:30:50", "link": "http://arxiv.org/abs/1811.00511v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Embeddings Jointly Induced from Contexts and Concepts:\n  Simple, Strong and Scalable", "abstract": "Word embeddings induced from local context are prevalent in NLP. A simple and\neffective context-based multilingual embedding learner is Levy et al. (2017)'s\nS-ID (sentence ID) method. Another line of work induces high-performing\nmultilingual embeddings from concepts (Dufter et al., 2018). In this paper, we\npropose Co+Co, a simple and scalable method that combines context-based and\nconcept-based learning. From a sentence aligned corpus, concepts are extracted\nvia sampling; words are then associated with their concept ID and sentence ID\nin embedding learning. This is the first work that successfully combines\ncontext-based and concept-based embedding learning. We show that Co+Co performs\nwell for two different application scenarios: the Parallel Bible Corpus (1000+\nlanguages, low-resource) and EuroParl (12 languages, high-resource). Among\nmethods applicable to both corpora, Co+Co performs best in our evaluation setup\nof six tasks.", "published": "2018-11-01 18:48:57", "link": "http://arxiv.org/abs/1811.00586v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shifting the Baseline: Single Modality Performance on Visual Navigation\n  & QA", "abstract": "We demonstrate the surprising strength of unimodal baselines in multimodal\ndomains, and make concrete recommendations for best practices in future\nresearch. Where existing work often compares against random or majority class\nbaselines, we argue that unimodal approaches better capture and reflect dataset\nbiases and therefore provide an important comparison when assessing the\nperformance of multimodal techniques. We present unimodal ablations on three\nrecent datasets in visual navigation and QA, seeing an up to 29% absolute gain\nin performance over published baselines.", "published": "2018-11-01 19:54:58", "link": "http://arxiv.org/abs/1811.00613v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Structured Commonsense Knowledge in Story Completion", "abstract": "The ability to select an appropriate story ending is the first step towards\nperfect narrative comprehension. Story ending prediction requires not only the\nexplicit clues within the context, but also the implicit knowledge (such as\ncommonsense) to construct a reasonable and consistent story. However, most\nprevious approaches do not explicitly use background commonsense knowledge. We\npresent a neural story ending selection model that integrates three types of\ninformation: narrative sequence, sentiment evolution and commonsense knowledge.\nExperiments show that our model outperforms state-of-the-art approaches on a\npublic dataset, ROCStory Cloze Task , and the performance gain from adding the\nadditional commonsense knowledge is significant.", "published": "2018-11-01 20:30:25", "link": "http://arxiv.org/abs/1811.00625v1", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Embedding Individual Table Columns for Resilient SQL Chatbots", "abstract": "Most of the world's data is stored in relational databases. Accessing these\nrequires specialized knowledge of the Structured Query Language (SQL), putting\nthem out of the reach of many people. A recent research thread in Natural\nLanguage Processing (NLP) aims to alleviate this problem by automatically\ntranslating natural language questions into SQL queries. While the proposed\nsolutions are a great start, they lack robustness and do not easily generalize:\nthe methods require high quality descriptions of the database table columns,\nand the most widely used training dataset, WikiSQL, is heavily biased towards\nusing those descriptions as part of the questions.\n  In this work, we propose solutions to both problems: we entirely eliminate\nthe need for column descriptions, by relying solely on their contents, and we\naugment the WikiSQL dataset by paraphrasing column names to reduce bias. We\nshow that the accuracy of existing methods drops when trained on our augmented,\ncolumn-agnostic dataset, and that our own method reaches state of the art\naccuracy, while relying on column contents only.", "published": "2018-11-01 21:03:41", "link": "http://arxiv.org/abs/1811.00633v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing and learning the language for different types of harassment", "abstract": "Disclaimer: This paper is concerned with violent online harassment. To\ndescribe the subject at an adequate level of realism, examples of our collected\ntweets involve violent, threatening, vulgar and hateful speech language in the\ncontext of racial, sexual, political, appearance and intellectual harassment.\n  The presence of a significant amount of harassment in user-generated content\nand its negative impact calls for robust automatic detection approaches. This\nrequires that we can identify different forms or types of harassment. Earlier\nwork has classified harassing language in terms of hurtfulness, abusiveness,\nsentiment, and profanity. However, to identify and understand harassment more\naccurately, it is essential to determine the context that represents the\ninterrelated conditions in which they occur. In this paper, we introduce the\nnotion of contextual type to harassment involving five categories: (i) sexual,\n(ii) racial, (iii) appearance-related, (iv) intellectual and (v) political. We\nutilize an annotated corpus from Twitter distinguishing these types of\nharassment. To study the context for each type that sheds light on the\nlinguistic meaning, interpretation, and distribution, we conduct two lines of\ninvestigation: an extensive linguistic analysis, and a statistical distribution\nof unigrams. We then build type-ware classifiers to automate the identification\nof type-specific harassment. Our experiments demonstrate that these classifiers\nprovide competitive accuracy for identifying and analyzing harassment on social\nmedia. We present extensive discussion and major observations about the\neffectiveness of type-aware classifiers using a detailed comparison setup\nproviding insight into the role of type-dependent features.", "published": "2018-11-01 21:40:26", "link": "http://arxiv.org/abs/1811.00644v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Learning Dynamics Of Language Models with SVCCA", "abstract": "Research has shown that neural models implicitly encode linguistic features,\nbut there has been no research showing \\emph{how} these encodings arise as the\nmodels are trained. We present the first study on the learning dynamics of\nneural language models, using a simple and flexible analysis method called\nSingular Vector Canonical Correlation Analysis (SVCCA), which enables us to\ncompare learned representations across time and across models, without the need\nto evaluate directly on annotated data. We probe the evolution of syntactic,\nsemantic, and topic representations and find that part-of-speech is learned\nearlier than topic; that recurrent layers become more similar to those of a\ntagger during training; and embedding layers less similar. Our results and\nmethods could inform better learning algorithms for NLP models, possibly to\nincorporate linguistic information more effectively.", "published": "2018-11-01 04:51:20", "link": "http://arxiv.org/abs/1811.00225v3", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "A sequential guiding network with attention for image captioning", "abstract": "The recent advances of deep learning in both computer vision (CV) and natural\nlanguage processing (NLP) provide us a new way of understanding semantics, by\nwhich we can deal with more challenging tasks such as automatic description\ngeneration from natural images. In this challenge, the encoder-decoder\nframework has achieved promising performance when a convolutional neural\nnetwork (CNN) is used as image encoder and a recurrent neural network (RNN) as\ndecoder. In this paper, we introduce a sequential guiding network that guides\nthe decoder during word generation. The new model is an extension of the\nencoder-decoder framework with attention that has an additional guiding long\nshort-term memory (LSTM) and can be trained in an end-to-end manner by using\nimage/descriptions pairs. We validate our approach by conducting extensive\nexperiments on a benchmark dataset, i.e., MS COCO Captions. The proposed model\nachieves significant improvement comparing to the other state-of-the-art deep\nlearning models.", "published": "2018-11-01 05:03:26", "link": "http://arxiv.org/abs/1811.00228v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Corpus for Reasoning About Natural Language Grounded in Photographs", "abstract": "We introduce a new dataset for joint reasoning about natural language and\nimages, with a focus on semantic diversity, compositionality, and visual\nreasoning challenges. The data contains 107,292 examples of English sentences\npaired with web photographs. The task is to determine whether a natural\nlanguage caption is true about a pair of photographs. We crowdsource the data\nusing sets of visually rich images and a compare-and-contrast task to elicit\nlinguistically diverse language. Qualitative analysis shows the data requires\ncompositional joint reasoning, including about quantities, comparisons, and\nrelations. Evaluation using state-of-the-art visual reasoning methods shows the\ndata presents a strong challenge.", "published": "2018-11-01 16:47:44", "link": "http://arxiv.org/abs/1811.00491v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Multiple-Attribute Text Style Transfer", "abstract": "The dominant approach to unsupervised \"style transfer\" in text is based on\nthe idea of learning a latent representation, which is independent of the\nattributes specifying its \"style\". In this paper, we show that this condition\nis not necessary and is not always met in practice, even with domain\nadversarial training that explicitly aims at learning such disentangled\nrepresentations. We thus propose a new model that controls several factors of\nvariation in textual data where this condition on disentanglement is replaced\nwith a simpler mechanism based on back-translation. Our method allows control\nover multiple attributes, like gender, sentiment, product type, etc., and a\nmore fine-grained control on the trade-off between content preservation and\nchange of style with a pooling operator in the latent space. Our experiments\ndemonstrate that the fully entangled model produces better generations, even\nwhen tested on new and more challenging benchmarks comprising reviews with\nmultiple sentences and multiple attributes.", "published": "2018-11-01 18:00:00", "link": "http://arxiv.org/abs/1811.00552v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Difficulties of Cross-Lingual Transfer with Order Differences: A Case\n  Study on Dependency Parsing", "abstract": "Different languages might have different word orders. In this paper, we\ninvestigate cross-lingual transfer and posit that an order-agnostic model will\nperform better when transferring to distant foreign languages. To test our\nhypothesis, we train dependency parsers on an English corpus and evaluate their\ntransfer performance on 30 other languages. Specifically, we compare encoders\nand decoders based on Recurrent Neural Networks (RNNs) and modified\nself-attentive architectures. The former relies on sequential information while\nthe latter is more flexible at modeling word order. Rigorous experiments and\ndetailed analysis shows that RNN-based architectures transfer well to languages\nthat are close to English, while self-attentive models have better overall\ncross-lingual transferability and perform especially well on distant languages.", "published": "2018-11-01 18:11:01", "link": "http://arxiv.org/abs/1811.00570v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DeepTileBars: Visualizing Term Distribution for Neural Information\n  Retrieval", "abstract": "Most neural Information Retrieval (Neu-IR) models derive query-to-document\nranking scores based on term-level matching. Inspired by TileBars, a classical\nterm distribution visualization method, in this paper, we propose a novel\nNeu-IR model that handles query-to-document matching at the subtopic and higher\nlevels. Our system first splits the documents into topical segments,\n\"visualizes\" the matchings between the query and the segments, and then feeds\nan interaction matrix into a Neu-IR model, DeepTileBars, to obtain the final\nranking scores. DeepTileBars models the relevance signals occurring at\ndifferent granularities in a document's topic hierarchy. It better captures the\ndiscourse structure of a document and thus the matching patterns. Although its\ndesign and implementation are light-weight, DeepTileBars outperforms other\nstate-of-the-art Neu-IR models on benchmark datasets including the Text\nREtrieval Conference (TREC) 2010-2012 Web Tracks and LETOR 4.0.", "published": "2018-11-01 19:39:14", "link": "http://arxiv.org/abs/1811.00606v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Exploring Semantic Incrementality with Dynamic Syntax and Vector Space\n  Semantics", "abstract": "One of the fundamental requirements for models of semantic processing in\ndialogue is incrementality: a model must reflect how people interpret and\ngenerate language at least on a word-by-word basis, and handle phenomena such\nas fragments, incomplete and jointly-produced utterances. We show that the\nincremental word-by-word parsing process of Dynamic Syntax (DS) can be assigned\na compositional distributional semantics, with the composition operator of DS\ncorresponding to the general operation of tensor contraction from multilinear\nalgebra. We provide abstract semantic decorations for the nodes of DS trees, in\nterms of vectors, tensors, and sums thereof; using the latter to model the\nunderspecified elements crucial to assigning partial representations during\nincremental processing. As a working example, we give an instantiation of this\ntheory using plausibility tensors of compositional distributional semantics,\nand show how our framework can incrementally assign a semantic plausibility\nmeasure as it parses phrases and sentences.", "published": "2018-11-01 19:59:06", "link": "http://arxiv.org/abs/1811.00614v1", "categories": ["cs.CL", "cs.AI", "03B65", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Dialogue Natural Language Inference", "abstract": "Consistency is a long standing issue faced by dialogue models. In this paper,\nwe frame the consistency of dialogue agents as natural language inference (NLI)\nand create a new natural language inference dataset called Dialogue NLI. We\npropose a method which demonstrates that a model trained on Dialogue NLI can be\nused to improve the consistency of a dialogue model, and evaluate the method\nwith human evaluation and with automatic metrics on a suite of evaluation sets\ndesigned to measure a dialogue model's consistency.", "published": "2018-11-01 23:10:03", "link": "http://arxiv.org/abs/1811.00671v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Generation of Medical Question-Answer Pairs", "abstract": "Question answering (QA) has achieved promising progress recently. However,\nanswering a question in real-world scenarios like the medical domain is still\nchallenging, due to the requirement of external knowledge and the insufficient\nquantity of high-quality training data. In the light of these challenges, we\nstudy the task of generating medical QA pairs in this paper. With the insight\nthat each medical question can be considered as a sample from the latent\ndistribution of questions given answers, we propose an automated medical QA\npair generation framework, consisting of an unsupervised key phrase detector\nthat explores unstructured material for validity, and a generator that involves\na multi-pass decoder to integrate structural knowledge for diversity. A series\nof experiments have been conducted on a real-world dataset collected from the\nNational Medical Licensing Examination of China. Both automatic evaluation and\nhuman annotation demonstrate the effectiveness of the proposed method. Further\ninvestigation shows that, by incorporating the generated QA pairs for training,\nsignificant improvement in terms of accuracy can be achieved for the\nexamination QA system.", "published": "2018-11-01 23:50:43", "link": "http://arxiv.org/abs/1811.00681v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Improving Information Retrieval Results for Persian Documents using\n  FarsNet", "abstract": "In this paper, we propose a new method for query expansion, which uses\nFarsNet (Persian WordNet) to find similar tokens related to the query and\nexpand the semantic meaning of the query. For this purpose, we use synonymy\nrelations in FarsNet and extract the related synonyms to query words. This\nalgorithm is used to enhance information retrieval systems and improve search\nresults. The overall evaluation of this system in comparison to the baseline\nmethod (without using query expansion) shows an improvement of about 9 percent\nin Mean Average Precision (MAP).", "published": "2018-11-01 17:48:18", "link": "http://arxiv.org/abs/1811.00854v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "AttentionXML: Label Tree-based Attention-Aware Deep Model for\n  High-Performance Extreme Multi-Label Text Classification", "abstract": "Extreme multi-label text classification (XMTC) is an important problem in the\nera of big data, for tagging a given text with the most relevant multiple\nlabels from an extremely large-scale label set. XMTC can be found in many\napplications, such as item categorization, web page tagging, and news\nannotation. Traditionally most methods used bag-of-words (BOW) as inputs,\nignoring word context as well as deep semantic information. Recent attempts to\novercome the problems of BOW by deep learning still suffer from 1) failing to\ncapture the important subtext for each label and 2) lack of scalability against\nthe huge number of labels. We propose a new label tree-based deep learning\nmodel for XMTC, called AttentionXML, with two unique features: 1) a multi-label\nattention mechanism with raw text as input, which allows to capture the most\nrelevant part of text to each label; and 2) a shallow and wide probabilistic\nlabel tree (PLT), which allows to handle millions of labels, especially for\n\"tail labels\". We empirically compared the performance of AttentionXML with\nthose of eight state-of-the-art methods over six benchmark datasets, including\nAmazon-3M with around 3 million labels. AttentionXML outperformed all competing\nmethods under all experimental settings. Experimental results also show that\nAttentionXML achieved the best performance against tail labels among label\ntree-based methods. The code and datasets are available at\nhttp://github.com/yourh/AttentionXML .", "published": "2018-11-01 16:30:33", "link": "http://arxiv.org/abs/1811.01727v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Explainable NLP: A Generative Explanation Framework for Text\n  Classification", "abstract": "Building explainable systems is a critical problem in the field of Natural\nLanguage Processing (NLP), since most machine learning models provide no\nexplanations for the predictions. Existing approaches for explainable machine\nlearning systems tend to focus on interpreting the outputs or the connections\nbetween inputs and outputs. However, the fine-grained information is often\nignored, and the systems do not explicitly generate the human-readable\nexplanations. To better alleviate this problem, we propose a novel generative\nexplanation framework that learns to make classification decisions and generate\nfine-grained explanations at the same time. More specifically, we introduce the\nexplainable factor and the minimum risk training approach that learn to\ngenerate more reasonable explanations. We construct two new datasets that\ncontain summaries, rating scores, and fine-grained reasons. We conduct\nexperiments on both datasets, comparing with several strong neural network\nbaseline systems. Experimental results show that our method surpasses all\nbaselines on both datasets, and is able to generate concise explanations at the\nsame time.", "published": "2018-11-01 02:45:57", "link": "http://arxiv.org/abs/1811.00196v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "MOHONE: Modeling Higher Order Network Effects in KnowledgeGraphs via\n  Network Infused Embeddings", "abstract": "Many knowledge graph embedding methods operate on triples and are therefore\nimplicitly limited by a very local view of the entire knowledge graph. We\npresent a new framework MOHONE to effectively model higher order network\neffects in knowledge-graphs, thus enabling one to capture varying degrees of\nnetwork connectivity (from the local to the global). Our framework is generic,\nexplicitly models the network scale, and captures two different aspects of\nsimilarity in networks: (a) shared local neighborhood and (b) structural\nrole-based similarity. First, we introduce methods that learn network\nrepresentations of entities in the knowledge graph capturing these varied\naspects of similarity. We then propose a fast, efficient method to incorporate\nthe information captured by these network representations into existing\nknowledge graph embeddings. We show that our method consistently and\nsignificantly improves the performance on link prediction of several different\nknowledge-graph embedding methods including TRANSE, TRANSD, DISTMULT, and\nCOMPLEX(by at least 4 points or 17% in some cases).", "published": "2018-11-01 03:04:09", "link": "http://arxiv.org/abs/1811.00198v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Progressive Memory Banks for Incremental Domain Adaptation", "abstract": "This paper addresses the problem of incremental domain adaptation (IDA) in\nnatural language processing (NLP). We assume each domain comes one after\nanother, and that we could only access data in the current domain. The goal of\nIDA is to build a unified model performing well on all the domains that we have\nencountered. We adopt the recurrent neural network (RNN) widely used in NLP,\nbut augment it with a directly parameterized memory bank, which is retrieved by\nan attention mechanism at each step of RNN transition. The memory bank provides\na natural way of IDA: when adapting our model to a new domain, we progressively\nadd new slots to the memory bank, which increases the number of parameters, and\nthus the model capacity. We learn the new memory slots and fine-tune existing\nparameters by back-propagation. Experimental results show that our approach\nachieves significantly better performance than fine-tuning alone. Compared with\nexpanding hidden states, our approach is more robust for old domains, shown by\nboth empirical and theoretical results. Our model also outperforms previous\nwork of IDA including elastic weight consolidation and progressive neural\nnetworks in the experiments.", "published": "2018-11-01 05:22:01", "link": "http://arxiv.org/abs/1811.00239v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Truly unsupervised acoustic word embeddings using weak top-down\n  constraints in encoder-decoder models", "abstract": "We investigate unsupervised models that can map a variable-duration speech\nsegment to a fixed-dimensional representation. In settings where unlabelled\nspeech is the only available resource, such acoustic word embeddings can form\nthe basis for \"zero-resource\" speech search, discovery and indexing systems.\nMost existing unsupervised embedding methods still use some supervision, such\nas word or phoneme boundaries. Here we propose the encoder-decoder\ncorrespondence autoencoder (EncDec-CAE), which, instead of true word segments,\nuses automatically discovered segments: an unsupervised term discovery system\nfinds pairs of words of the same unknown type, and the EncDec-CAE is trained to\nreconstruct one word given the other as input. We compare it to a standard\nencoder-decoder autoencoder (AE), a variational AE with a prior over its latent\nembedding, and downsampling. EncDec-CAE outperforms its closest competitor by\n24% relative in average precision on two languages in a word discrimination\ntask.", "published": "2018-11-01 14:17:01", "link": "http://arxiv.org/abs/1811.00403v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Auditing Data Provenance in Text-Generation Models", "abstract": "To help enforce data-protection regulations such as GDPR and detect\nunauthorized uses of personal data, we develop a new \\emph{model auditing}\ntechnique that helps users check if their data was used to train a machine\nlearning model. We focus on auditing deep-learning models that generate\nnatural-language text, including word prediction and dialog generation. These\nmodels are at the core of popular online services and are often trained on\npersonal data such as users' messages, searches, chats, and comments.\n  We design and evaluate a black-box auditing method that can detect, with very\nfew queries to a model, if a particular user's texts were used to train it\n(among thousands of other users). We empirically show that our method can\nsuccessfully audit well-generalized models that are not overfitted to the\ntraining data. We also analyze how text-generation models memorize word\nsequences and explain why this memorization makes them amenable to auditing.", "published": "2018-11-01 17:32:44", "link": "http://arxiv.org/abs/1811.00513v2", "categories": ["cs.CR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CR"}
{"title": "Online Embedding Compression for Text Classification using Low Rank\n  Matrix Factorization", "abstract": "Deep learning models have become state of the art for natural language\nprocessing (NLP) tasks, however deploying these models in production system\nposes significant memory constraints. Existing compression methods are either\nlossy or introduce significant latency. We propose a compression method that\nleverages low rank matrix factorization during training,to compress the word\nembedding layer which represents the size bottleneck for most NLP models. Our\nmodels are trained, compressed and then further re-trained on the downstream\ntask to recover accuracy while maintaining the reduced size. Empirically, we\nshow that the proposed method can achieve 90% compression with minimal impact\nin accuracy for sentence classification tasks, and outperforms alternative\nmethods like fixed-point quantization or offline word embedding compression. We\nalso analyze the inference time and storage space for our method through FLOP\ncalculations, showing that we can compress DNN models by a configurable ratio\nand regain accuracy loss without introducing additional latency compared to\nfixed point quantization. Finally, we introduce a novel learning rate schedule,\nthe Cyclically Annealed Learning Rate (CALR), which we empirically demonstrate\nto outperform other popular adaptive learning rate algorithms on a sentence\nclassification benchmark.", "published": "2018-11-01 21:38:18", "link": "http://arxiv.org/abs/1811.00641v1", "categories": ["cs.LG", "cs.CL", "cs.NA", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Implicit Regularization of Stochastic Gradient Descent in Natural\n  Language Processing: Observations and Implications", "abstract": "Deep neural networks with remarkably strong generalization performances are\nusually over-parameterized. Despite explicit regularization strategies are used\nfor practitioners to avoid over-fitting, the impacts are often small. Some\ntheoretical studies have analyzed the implicit regularization effect of\nstochastic gradient descent (SGD) on simple machine learning models with\ncertain assumptions. However, how it behaves practically in state-of-the-art\nmodels and real-world datasets is still unknown. To bridge this gap, we study\nthe role of SGD implicit regularization in deep learning systems. We show pure\nSGD tends to converge to minimas that have better generalization performances\nin multiple natural language processing (NLP) tasks. This phenomenon coexists\nwith dropout, an explicit regularizer. In addition, neural network's finite\nlearning capability does not impact the intrinsic nature of SGD's implicit\nregularization effect. Specifically, under limited training samples or with\ncertain corrupted labels, the implicit regularization effect remains strong. We\nfurther analyze the stability by varying the weight initialization range. We\ncorroborate these experimental findings with a decision boundary visualization\nusing a 3-layer neural network for interpretation. Altogether, our work enables\na deepened understanding on how implicit regularization affects the deep\nlearning model and sheds light on the future study of the over-parameterized\nmodel's generalization ability.", "published": "2018-11-01 22:24:25", "link": "http://arxiv.org/abs/1811.00659v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Weakly supervised CRNN system for sound event detection with large-scale\n  unlabeled in-domain data", "abstract": "Sound event detection (SED) is typically posed as a supervised learning\nproblem requiring training data with strong temporal labels of sound events.\nHowever, the production of datasets with strong labels normally requires\nunaffordable labor cost. It limits the practical application of supervised SED\nmethods. The recent advances in SED approaches focuses on detecting sound\nevents by taking advantages of weakly labeled or unlabeled training data. In\nthis paper, we propose a joint framework to solve the SED task using\nlarge-scale unlabeled in-domain data. In particular, a state-of-the-art general\naudio tagging model is first employed to predict weak labels for unlabeled\ndata. On the other hand, a weakly supervised architecture based on the\nconvolutional recurrent neural network (CRNN) is developed to solve the strong\nannotations of sound events with the aid of the unlabeled data with predicted\nlabels. It is found that the SED performance generally increases as more\nunlabeled data is added into the training. To address the noisy label problem\nof unlabeled data, an ensemble strategy is applied to increase the system\nrobustness. The proposed system is evaluated on the SED dataset of DCASE 2018\nchallenge. It reaches a F1-score of 21.0%, resulting in an improvement of 10%\nover the baseline system.", "published": "2018-11-01 10:16:41", "link": "http://arxiv.org/abs/1811.00301v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Learning for Tube Amplifier Emulation", "abstract": "Analog audio effects and synthesizers often owe their distinct sound to\ncircuit nonlinearities. Faithfully modeling such significant aspect of the\noriginal sound in virtual analog software can prove challenging. The current\nwork proposes a generic data-driven approach to virtual analog modeling and\napplies it to the Fender Bassman 56F-A vacuum-tube amplifier. Specifically, a\nfeedforward variant of the WaveNet deep neural network is trained to carry out\na regression on audio waveform samples from input to output of a SPICE model of\nthe tube amplifier. The output signals are pre-emphasized to assist the model\nat learning the high-frequency content. The results of a listening test suggest\nthat the proposed model accurately emulates the reference device. In\nparticular, the model responds to user control changes, and faithfully\nrestitutes the range of sonic characteristics found across the configurations\nof the original device.", "published": "2018-11-01 12:03:03", "link": "http://arxiv.org/abs/1811.00334v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Sequence-to-sequence Models for Small-Footprint Keyword Spotting", "abstract": "In this paper, we propose a sequence-to-sequence model for keyword spotting\n(KWS). Compared with other end-to-end architectures for KWS, our model\nsimplifies the pipelines of production-quality KWS system and satisfies the\nrequirement of high accuracy, low-latency, and small-footprint. We also\nevaluate the performances of different encoder architectures, which include\nLSTM and GRU. Experiments on the real-world wake-up data show that our approach\noutperforms the recently proposed attention-based end-to-end model.\nSpecifically speaking, with 73K parameters, our sequence-to-sequence model\nachieves $\\sim$3.05\\% false rejection rate (FRR) at 0.1 false alarm (FA) per\nhour.", "published": "2018-11-01 12:53:53", "link": "http://arxiv.org/abs/1811.00348v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "End-to-end Models with auditory attention in Multi-channel Keyword\n  Spotting", "abstract": "In this paper, we propose an attention-based end-to-end model for\nmulti-channel keyword spotting (KWS), which is trained to optimize the KWS\nresult directly. As a result, our model outperforms the baseline model with\nsignal pre-processing techniques in both the clean and noisy testing data. We\nalso found that multi-task learning results in a better performance when the\ntraining and testing data are similar. Transfer learning and multi-target\nspectral mapping can dramatically enhance the robustness to the noisy\nenvironment. At 0.1 false alarm (FA) per hour, the model with transfer learning\nand multi-target mapping gain an absolute 30% improvement in the wake-up rate\nin the noisy data with SNR about -20.", "published": "2018-11-01 12:56:22", "link": "http://arxiv.org/abs/1811.00350v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Designing an Effective Metric Learning Pipeline for Speaker Diarization", "abstract": "State-of-the-art speaker diarization systems utilize knowledge from external\ndata, in the form of a pre-trained distance metric, to effectively determine\nrelative speaker identities to unseen data. However, much of recent focus has\nbeen on choosing the appropriate feature extractor, ranging from pre-trained\n$i-$vectors to representations learned via different sequence modeling\narchitectures (e.g. 1D-CNNs, LSTMs, attention models), while adopting\noff-the-shelf metric learning solutions. In this paper, we argue that,\nregardless of the feature extractor, it is crucial to carefully design a metric\nlearning pipeline, namely the loss function, the sampling strategy and the\ndiscrimnative margin parameter, for building robust diarization systems.\nFurthermore, we propose to adopt a fine-grained validation process to obtain a\ncomprehensive evaluation of the generalization power of metric learning\npipelines. To this end, we measure diarization performance across different\nlanguage speakers, and variations in the number of speakers in a recording.\nUsing empirical studies, we provide interesting insights into the effectiveness\nof different design choices and make recommendations.", "published": "2018-11-01 01:51:17", "link": "http://arxiv.org/abs/1811.00183v1", "categories": ["stat.ML", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "stat.ML"}
{"title": "Neural Music Synthesis for Flexible Timbre Control", "abstract": "The recent success of raw audio waveform synthesis models like WaveNet\nmotivates a new approach for music synthesis, in which the entire process ---\ncreating audio samples from a score and instrument information --- is modeled\nusing generative neural networks. This paper describes a neural music synthesis\nmodel with flexible timbre controls, which consists of a recurrent neural\nnetwork conditioned on a learned instrument embedding followed by a WaveNet\nvocoder. The learned embedding space successfully captures the diverse\nvariations in timbres within a large dataset and enables timbre control and\nmorphing by interpolating between instruments in the embedding space. The\nsynthesis quality is evaluated both numerically and perceptually, and an\ninteractive web demo is presented.", "published": "2018-11-01 04:41:40", "link": "http://arxiv.org/abs/1811.00223v1", "categories": ["cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Referenceless Performance Evaluation of Audio Source Separation using\n  Deep Neural Networks", "abstract": "Current performance evaluation for audio source separation depends on\ncomparing the processed or separated signals with reference signals. Therefore,\ncommon performance evaluation toolkits are not applicable to real-world\nsituations where the ground truth audio is unavailable. In this paper, we\npropose a performance evaluation technique that does not require reference\nsignals in order to assess separation quality. The proposed technique uses a\ndeep neural network (DNN) to map the processed audio into its quality score.\nOur experiment results show that the DNN is capable of predicting the\nsources-to-artifacts ratio from the blind source separation evaluation toolkit\nwithout the need for reference signals.", "published": "2018-11-01 15:50:42", "link": "http://arxiv.org/abs/1811.00454v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS", "68T01, 68T10, 68T45, 62H25", "H.5.5; I.5; I.2.6; I.4.3; I.4; I.2"], "primary_category": "cs.SD"}
{"title": "Deep Segment Attentive Embedding for Duration Robust Speaker\n  Verification", "abstract": "LSTM-based speaker verification usually uses a fixed-length local segment\nrandomly truncated from an utterance to learn the utterance-level speaker\nembedding, while using the average embedding of all segments of a test\nutterance to verify the speaker, which results in a critical mismatch between\ntesting and training. This mismatch degrades the performance of speaker\nverification, especially when the durations of training and testing utterances\nare very different. To alleviate this issue, we propose the deep segment\nattentive embedding method to learn the unified speaker embeddings for\nutterances of variable duration. Each utterance is segmented by a sliding\nwindow and LSTM is used to extract the embedding of each segment. Instead of\nonly using one local segment, we use the whole utterance to learn the\nutterance-level embedding by applying an attentive pooling to the embeddings of\nall segments. Moreover, the similarity loss of segment-level embeddings is\nintroduced to guide the segment attention to focus on the segments with more\nspeaker discriminations, and jointly optimized with the similarity loss of\nutterance-level embeddings. Systematic experiments on Tongdun and VoxCeleb show\nthat the proposed method significantly improves robustness of duration variant\nand achieves the relative Equal Error Rate reduction of 50% and 11.54% ,\nrespectively.", "published": "2018-11-01 01:21:41", "link": "http://arxiv.org/abs/1811.00883v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
