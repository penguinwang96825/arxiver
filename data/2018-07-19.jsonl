{"title": "Few-Shot Adaptation for Multimedia Semantic Indexing", "abstract": "We propose a few-shot adaptation framework, which bridges zero-shot learning and supervised many-shot learning, for semantic indexing of image and video data. Few-shot adaptation provides robust parameter estimation with few training examples, by optimizing the parameters of zero-shot learning and supervised many-shot learning simultaneously. In this method, first we build a zero-shot detector, and then update it by using the few examples. Our experiments show the effectiveness of the proposed framework on three datasets: TRECVID Semantic Indexing 2010, 2014, and ImageNET. On the ImageNET dataset, we show that our method outperforms recent few-shot learning methods. On the TRECVID 2014 dataset, we achieve 15.19% and 35.98% in Mean Average Precision under the zero-shot condition and the supervised condition, respectively. To the best of our knowledge, these are the best results on this dataset.", "published": "2018-07-19 00:58:33", "link": "http://arxiv.org/abs/1807.07203v1", "categories": ["cs.MM", "cs.CV"], "primary_category": "cs.MM"}
{"title": "Automatically Designing CNN Architectures for Medical Image Segmentation", "abstract": "Deep neural network architectures have traditionally been designed and explored with human expertise in a long-lasting trial-and-error process. This process requires huge amount of time, expertise, and resources. To address this tedious problem, we propose a novel algorithm to optimally find hyperparameters of a deep network architecture automatically. We specifically focus on designing neural architectures for medical image segmentation task. Our proposed method is based on a policy gradient reinforcement learning for which the reward function is assigned a segmentation evaluation utility (i.e., dice index). We show the efficacy of the proposed method with its low computational cost in comparison with the state-of-the-art medical image segmentation networks. We also present a new architecture design, a densely connected encoder-decoder CNN, as a strong baseline architecture to apply the proposed hyperparameter search algorithm. We apply the proposed algorithm to each layer of the baseline architectures. As an application, we train the proposed system on cine cardiac MR images from Automated Cardiac Diagnosis Challenge (ACDC) MICCAI 2017. Starting from a baseline segmentation architecture, the resulting network architecture obtains the state-of-the-art results in accuracy without performing any trial-and-error based architecture design approaches or close supervision of the hyperparameters changes.", "published": "2018-07-19 23:47:12", "link": "http://arxiv.org/abs/1807.07663v1", "categories": ["stat.ML", "cs.CV", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Anomaly Detection for Water Treatment System based on Neural Network with Automatic Architecture Optimization", "abstract": "We continue to develop our neural network (NN) based forecasting approach to anomaly detection (AD) using the Secure Water Treatment (SWaT) industrial control system (ICS) testbed dataset. We propose genetic algorithms (GA) to find the best NN architecture for a given dataset, using the NAB metric to assess the quality of different architectures. The drawbacks of the F1-metric are analyzed. Several techniques are proposed to improve the quality of AD: exponentially weighted smoothing, mean p-powered error measure, individual error weight for each variable, disjoint prediction windows. Based on the techniques used, an approach to anomaly interpretation is introduced.", "published": "2018-07-19 08:22:21", "link": "http://arxiv.org/abs/1807.07282v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Bounded Information Rate Variational Autoencoders", "abstract": "This paper introduces a new member of the family of Variational Autoencoders (VAE) that constrains the rate of information transferred by the latent layer. The latent layer is interpreted as a communication channel, the information rate of which is bound by imposing a pre-set signal-to-noise ratio. The new constraint subsumes the mutual information between the input and latent variables, combining naturally with the likelihood objective of the observed data as used in a conventional VAE. The resulting Bounded-Information-Rate Variational Autoencoder (BIR-VAE) provides a meaningful latent representation with an information resolution that can be specified directly in bits by the system designer. The rate constraint can be used to prevent overtraining, and the method naturally facilitates quantisation of the latent variables at the set rate. Our experiments confirm that the BIR-VAE has a meaningful latent representation and that its performance is at least as good as state-of-the-art competing algorithms, but with lower computational complexity.", "published": "2018-07-19 09:13:57", "link": "http://arxiv.org/abs/1807.07306v2", "categories": ["cs.LG", "cs.IT", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Sequence to Logic with Copy and Cache", "abstract": "Generating logical form equivalents of human language is a fresh way to employ neural architectures where long short-term memory effectively captures dependencies in both encoder and decoder units.\n  The logical form of the sequence usually preserves information from the natural language side in the form of similar tokens, and recently a copying mechanism has been proposed which increases the probability of outputting tokens from the source input through decoding.\n  In this paper we propose a caching mechanism as a more general form of the copying mechanism which also weighs all the words from the source vocabulary according to their relation to the current decoding context.\n  Our results confirm that the proposed method achieves improvements in sequence/token-level accuracy on sequence to logical form tasks. Further experiments on cross-domain adversarial attacks show substantial improvements when using the most influential examples of other domains for training.", "published": "2018-07-19 10:32:52", "link": "http://arxiv.org/abs/1807.07333v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.", "published": "2018-07-19 17:17:23", "link": "http://arxiv.org/abs/1807.07543v2", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Randomized Greedy Sensor Selection: Leveraging Weak Submodularity", "abstract": "We study the problem of estimating a random process from the observations collected by a network of sensors that operate under resource constraints. When the dynamics of the process and sensor observations are described by a state-space model and the resource are unlimited, the conventional Kalman filter provides the minimum mean-square error (MMSE) estimates. However, at any given time, restrictions on the available communications bandwidth and computational capabilities and/or power impose a limitation on the number of network nodes whose observations can be used to compute the estimates. We formulate the problem of selecting the most informative subset of the sensors as a combinatorial problem of maximizing a monotone set function under a uniform matroid constraint. For the MMSE estimation criterion we show that the maximum element-wise curvature of the objective function satisfies a certain upper-bound constraint and is, therefore, weak submodular. We develop an efficient randomized greedy algorithm for sensor selection and establish guarantees on the estimator's performance in this setting. Extensive simulation results demonstrate the efficacy of the randomized greedy algorithm compared to state-of-the-art greedy and semidefinite programming relaxation methods.", "published": "2018-07-19 21:38:35", "link": "http://arxiv.org/abs/1807.08627v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Distributed Power Control Schemes for In-Band Full-Duplex Energy Harvesting Wireless Networks", "abstract": "This paper studies two power control problems in energy harvesting wireless networks where one hybrid base station (HBS) and all user equipments (UEs) are operating in in-band full-duplex mode. We consider minimizing the aggregate power subject to the quality of service requirement constraint, and maximizing the aggregate throughput. We address these two problems by proposing two distributed power control schemes for controlling the uplink transmit power by the UEs and the downlink energy harvesting signal power by the HBS. In our proposed schemes, the HBS updates the downlink transmit power level of the energy-harvesting signal so that each UE is enabled to harvest its required energy for powering the operating circuit and transmitting its uplink information signal with the power level determined by the proposed schemes. We show that our proposed power control schemes converge to their corresponding unique fixed points starting from any arbitrary initial transmit power. We will show that our proposed schemes well address the stated problems, which is also demonstrated by our extensive simulation results.", "published": "2018-07-19 19:40:43", "link": "http://arxiv.org/abs/1807.07622v1", "categories": ["eess.SP", "cs.NI"], "primary_category": "eess.SP"}
{"title": "Robust Oil-spill Forensics and Petroleum Source Differentiation using Quantized Peak Topography Maps", "abstract": "Identification and classification of environmental forensics, with the petroleum forensics as the main application, requires an effective technology or method to distinguish between the closely located forensics as they share many main biomarkers. Two-dimensional gas chromatography is one of these technologies with which a petroleum forensic is separated into its chemical compounds, resulting in a three-dimensional image, GCXGC image. Therefore, distinguishing between two petroleum forensics is equivalent to the comparison between their corresponding GCXGC images. In this paper, we present a technique, called Quantized Peak Topography Map (QPTM), which results in a better separation between the GCXGC images. We validate our proposed method on a model dataset, consisting of thirtyfour GCXGC images, extracted from the different parts of the world.", "published": "2018-07-19 15:18:13", "link": "http://arxiv.org/abs/1807.07484v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
