{"title": "Imparting Interpretability to Word Embeddings while Preserving Semantic\n  Structure", "abstract": "As an ubiquitous method in natural language processing, word embeddings are\nextensively employed to map semantic properties of words into a dense vector\nrepresentation. They capture semantic and syntactic relations among words but\nthe vectors corresponding to the words are only meaningful relative to each\nother. Neither the vector nor its dimensions have any absolute, interpretable\nmeaning. We introduce an additive modification to the objective function of the\nembedding learning algorithm that encourages the embedding vectors of words\nthat are semantically related to a predefined concept to take larger values\nalong a specified dimension, while leaving the original semantic learning\nmechanism mostly unaffected. In other words, we align words that are already\ndetermined to be related, along predefined concepts. Therefore, we impart\ninterpretability to the word embedding by assigning meaning to its vector\ndimensions. The predefined concepts are derived from an external lexical\nresource, which in this paper is chosen as Roget's Thesaurus. We observe that\nalignment along the chosen concepts is not limited to words in the Thesaurus\nand extends to other related words as well. We quantify the extent of\ninterpretability and assignment of meaning from our experimental results.\nManual human evaluation results have also been presented to further verify that\nthe proposed method increases interpretability. We also demonstrate the\npreservation of semantic coherence of the resulting vector space by using\nword-analogy and word-similarity tests. These tests show that the\ninterpretability-imparted word embeddings that are obtained by the proposed\nframework do not sacrifice performances in common benchmark tests.", "published": "2018-07-19 08:14:59", "link": "http://arxiv.org/abs/1807.07279v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Statistical Model Compression for Small-Footprint Natural Language\n  Understanding", "abstract": "In this paper we investigate statistical model compression applied to natural\nlanguage understanding (NLU) models. Small-footprint NLU models are important\nfor enabling offline systems on hardware restricted devices, and for decreasing\non-demand model loading latency in cloud-based systems. To compress NLU models,\nwe present two main techniques, parameter quantization and perfect feature\nhashing. These techniques are complementary to existing model pruning\nstrategies such as L1 regularization. We performed experiments on a large scale\nNLU system. The results show that our approach achieves 14-fold reduction in\nmemory usage compared to the original models with minimal predictive\nperformance impact.", "published": "2018-07-19 16:23:35", "link": "http://arxiv.org/abs/1807.07520v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can We Assess Mental Health through Social Media and Smart Devices?\n  Addressing Bias in Methodology and Evaluation", "abstract": "Predicting mental health from smartphone and social media data on a\nlongitudinal basis has recently attracted great interest, with very promising\nresults being reported across many studies. Such approaches have the potential\nto revolutionise mental health assessment, if their development and evaluation\nfollows a real world deployment setting. In this work we take a closer look at\nstate-of-the-art approaches, using different mental health datasets and\nindicators, different feature sources and multiple simulations, in order to\nassess their ability to generalise. We demonstrate that under a pragmatic\nevaluation framework, none of the approaches deliver or even approach the\nreported performances. In fact, we show that current state-of-the-art\napproaches can barely outperform the most na\\\"ive baselines in the real-world\nsetting, posing serious questions not only about their deployment ability, but\nalso about the contribution of the derived features for the mental health\nassessment task and how to make better use of such data in the future.", "published": "2018-07-19 11:44:10", "link": "http://arxiv.org/abs/1807.07351v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Using Deep Neural Networks to Translate Multi-lingual Threat\n  Intelligence", "abstract": "The multilingual nature of the Internet increases complications in the\ncybersecurity community's ongoing efforts to strategically mine threat\nintelligence from OSINT data on the web. OSINT sources such as social media,\nblogs, and dark web vulnerability markets exist in diverse languages and hinder\nsecurity analysts, who are unable to draw conclusions from intelligence in\nlanguages they don't understand. Although third party translation engines are\ngrowing stronger, they are unsuited for private security environments. First,\nsensitive intelligence is not a permitted input to third party engines due to\nprivacy and confidentiality policies. In addition, third party engines produce\ngeneralized translations that tend to lack exclusive cybersecurity terminology.\nIn this paper, we address these issues and describe our system that enables\nthreat intelligence understanding across unfamiliar languages. We create a\nneural network based system that takes in cybersecurity data in a different\nlanguage and outputs the respective English translation. The English\ntranslation can then be understood by an analyst, and can also serve as input\nto an AI based cyber-defense system that can take mitigative action. As a proof\nof concept, we have created a pipeline which takes Russian threats and\ngenerates its corresponding English, RDF, and vectorized representations. Our\nnetwork optimizes translations on specifically, cybersecurity data.", "published": "2018-07-19 16:14:08", "link": "http://arxiv.org/abs/1807.07517v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Towards Explainable and Controllable Open Domain Dialogue Generation\n  with Dialogue Acts", "abstract": "We study open domain dialogue generation with dialogue acts designed to\nexplain how people engage in social chat. To imitate human behavior, we propose\nmanaging the flow of human-machine interactions with the dialogue acts as\npolicies. The policies and response generation are jointly learned from\nhuman-human conversations, and the former is further optimized with a\nreinforcement learning approach. With the dialogue acts, we achieve significant\nimprovement over state-of-the-art methods on response quality for given\ncontexts and dialogue length in both machine-machine simulation and\nhuman-machine conversation.", "published": "2018-07-19 06:41:05", "link": "http://arxiv.org/abs/1807.07255v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Rearranging the Familiar: Testing Compositional Generalization in\n  Recurrent Networks", "abstract": "Systematic compositionality is the ability to recombine meaningful units with\nregular and predictable outcomes, and it's seen as key to humans' capacity for\ngeneralization in language. Recent work has studied systematic compositionality\nin modern seq2seq models using generalization to novel navigation instructions\nin a grounded environment as a probing tool, requiring models to quickly\nbootstrap the meaning of new words. We extend this framework here to settings\nwhere the model needs only to recombine well-trained functional words (such as\n\"around\" and \"right\") in novel contexts. Our findings confirm and strengthen\nthe earlier ones: seq2seq models can be impressively good at generalizing to\nnovel combinations of previously-seen input, but only when they receive\nextensive training on the specific pattern to be generalized (e.g.,\ngeneralizing from many examples of \"X around right\" to \"jump around right\"),\nwhile failing when generalization requires novel application of compositional\nrules (e.g., inferring the meaning of \"around right\" from those of \"right\" and\n\"around\").", "published": "2018-07-19 17:23:13", "link": "http://arxiv.org/abs/1807.07545v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech", "abstract": "In this work, we propose a new solution for parallel wave generation by\nWaveNet. In contrast to parallel WaveNet (van den Oord et al., 2018), we\ndistill a Gaussian inverse autoregressive flow from the autoregressive WaveNet\nby minimizing a regularized KL divergence between their highly-peaked output\ndistributions. Our method computes the KL divergence in closed-form, which\nsimplifies the training algorithm and provides very efficient distillation. In\naddition, we introduce the first text-to-wave neural architecture for speech\nsynthesis, which is fully convolutional and enables fast end-to-end training\nfrom scratch. It significantly outperforms the previous pipeline that connects\na text-to-spectrogram model to a separately trained WaveNet (Ping et al.,\n2018). We also successfully distill a parallel waveform synthesizer conditioned\non the hidden representation in this end-to-end model.", "published": "2018-07-19 08:15:41", "link": "http://arxiv.org/abs/1807.07281v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Capsule based Approach for Polyphonic Sound Event Detection", "abstract": "Polyphonic sound event detection (polyphonic SED) is an interesting but\nchallenging task due to the concurrence of multiple sound events. Recently, SED\nmethods based on convolutional neural networks (CNN) and recurrent neural\nnetworks (RNN) have shown promising performance. Generally, CNN are designed\nfor local feature extraction while RNN are used to model the temporal\ndependency among these local features. Despite their success, it is still\ninsufficient for existing deep learning techniques to separate individual sound\nevent from their mixture, largely due to the overlapping characteristic of\nfeatures. Motivated by the success of Capsule Networks (CapsNet), we propose a\nmore suitable capsule based approach for polyphonic SED. Specifically, several\ncapsule layers are designed to effectively select representative frequency\nbands for each individual sound event. The temporal dependency of capsule's\noutputs is then modeled by a RNN. And a dynamic threshold method is proposed\nfor making the final decision based on RNN outputs. Experiments on the TUT-SED\nSynthetic 2016 dataset show that the proposed approach obtains an F1-score of\n68.8% and an error rate of 0.45, outperforming the previous state-of-the-art\nmethod of 66.4% and 0.48, respectively.", "published": "2018-07-19 13:58:32", "link": "http://arxiv.org/abs/1807.07436v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Noise Adaptive Speech Enhancement using Domain Adversarial Training", "abstract": "In this study, we propose a novel noise adaptive speech enhancement (SE)\nsystem, which employs a domain adversarial training (DAT) approach to tackle\nthe issue of a noise type mismatch between the training and testing conditions.\nSuch a mismatch is a critical problem in deep-learning-based SE systems. A\nlarge mismatch may cause a serious performance degradation to the SE\nperformance. Because we generally use a well-trained SE system to handle\nvarious unseen noise types, a noise type mismatch commonly occurs in real-world\nscenarios. The proposed noise adaptive SE system contains an\nencoder-decoder-based enhancement model and a domain discriminator model.\nDuring adaptation, the DAT approach encourages the encoder to produce\nnoise-invariant features based on the information from the discriminator model\nand consequentially increases the robustness of the enhancement model to unseen\nnoise types. Herein, we regard stationary noises as the source domain (with the\nground truth of clean speech) and non-stationary noises as the target domain\n(without the ground truth). We evaluated the proposed system on TIMIT\nsentences. The experiment results show that the proposed noise adaptive SE\nsystem successfully provides significant improvements in PESQ (19.0%), SSNR\n(39.3%), and STOI (27.0%) over the SE system without an adaptation.", "published": "2018-07-19 15:42:26", "link": "http://arxiv.org/abs/1807.07501v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-to-Score Alignment using Transposition-invariant Features", "abstract": "Audio-to-score alignment is an important pre-processing step for in-depth\nanalysis of classical music. In this paper, we apply novel\ntransposition-invariant audio features to this task. These low-dimensional\nfeatures represent local pitch intervals and are learned in an unsupervised\nfashion by a gated autoencoder. Our results show that the proposed features are\nindeed fully transposition-invariant and enable accurate alignments between\ntransposed scores and performances. Furthermore, they can even outperform\nwidely used features for audio-to-score alignment on `untransposed data', and\nthus are a viable and more flexible alternative to well-established features\nfor music alignment and matching.", "published": "2018-07-19 08:13:34", "link": "http://arxiv.org/abs/1807.07278v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
