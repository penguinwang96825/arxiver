{"title": "Context-Aware Attention for Understanding Twitter Abuse", "abstract": "The original goal of any social media platform is to facilitate users to\nindulge in healthy and meaningful conversations. But more often than not, it\nhas been found that it becomes an avenue for wanton attacks. We want to\nalleviate this issue and hence we try to provide a detailed analysis of how\nabusive behavior can be monitored in Twitter. The complexity of the natural\nlanguage constructs makes this task challenging. We show how applying\ncontextual attention to Long Short Term Memory networks help us give near state\nof art results on multiple benchmarks abuse detection data sets from Twitter.", "published": "2018-09-24 02:18:01", "link": "http://arxiv.org/abs/1809.08726v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deformable Stacked Structure for Named Entity Recognition", "abstract": "Neural architecture for named entity recognition has achieved great success\nin the field of natural language processing. Currently, the dominating\narchitecture consists of a bi-directional recurrent neural network (RNN) as the\nencoder and a conditional random field (CRF) as the decoder. In this paper, we\npropose a deformable stacked structure for named entity recognition, in which\nthe connections between two adjacent layers are dynamically established. We\nevaluate the deformable stacked structure by adapting it to different layers.\nOur model achieves the state-of-the-art performances on the OntoNotes dataset.", "published": "2018-09-24 02:42:40", "link": "http://arxiv.org/abs/1809.08730v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!", "abstract": "Motivated by recent findings on the probabilistic modeling of acceptability\njudgments, we propose syntactic log-odds ratio (SLOR), a normalized language\nmodel score, as a metric for referenceless fluency evaluation of natural\nlanguage generation output at the sentence level. We further introduce WPSLOR,\na novel WordPiece-based version, which harnesses a more compact language model.\nEven though word-overlap metrics like ROUGE are computed with the help of\nhand-written references, our referenceless methods obtain a significantly\nhigher correlation with human fluency scores on a benchmark dataset of\ncompressed sentences. Finally, we present ROUGE-LM, a reference-based metric\nwhich is a natural extension of WPSLOR to the case of available references. We\nshow that ROUGE-LM yields a significantly higher correlation with human\njudgments than all baseline metrics, including WPSLOR on its own.", "published": "2018-09-24 02:42:42", "link": "http://arxiv.org/abs/1809.08731v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Transductive Learning and Beyond: Morphological Generation in the\n  Minimal-Resource Setting", "abstract": "Neural state-of-the-art sequence-to-sequence (seq2seq) models often do not\nperform well for small training sets. We address paradigm completion, the\nmorphological task of, given a partial paradigm, generating all missing forms.\nWe propose two new methods for the minimal-resource setting: (i) Paradigm\ntransduction: Since we assume only few paradigms available for training, neural\nseq2seq models are able to capture relationships between paradigm cells, but\nare tied to the idiosyncracies of the training set. Paradigm transduction\nmitigates this problem by exploiting the input subset of inflected forms at\ntest time. (ii) Source selection with high precision (SHIP): Multi-source\nmodels which learn to automatically select one or multiple sources to predict a\ntarget inflection do not perform well in the minimal-resource setting. SHIP is\nan alternative to identify a reliable source if training data is limited. On a\n52-language benchmark dataset, we outperform the previous state of the art by\nup to 9.71% absolute accuracy.", "published": "2018-09-24 02:56:31", "link": "http://arxiv.org/abs/1809.08733v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Multitask Learning for Community Question Answering Using\n  Task-Specific Embeddings", "abstract": "We address jointly two important tasks for Question Answering in community\nforums: given a new question, (i) find related existing questions, and (ii)\nfind relevant answers to this new question. We further use an auxiliary task to\ncomplement the previous two, i.e., (iii) find good answers with respect to the\nthread question in a question-comment thread. We use deep neural networks\n(DNNs) to learn meaningful task-specific embeddings, which we then incorporate\ninto a conditional random field (CRF) model for the multitask setting,\nperforming joint learning over a complex graph structure. While DNNs alone\nachieve competitive results when trained to produce the embeddings, the CRF,\nwhich makes use of the embeddings and the dependencies between the tasks,\nimproves the results significantly and consistently across a variety of\nevaluation metrics, thus showing the complementarity of DNNs and structured\nlearning.", "published": "2018-09-24 13:49:14", "link": "http://arxiv.org/abs/1809.08928v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Jointly Multiple Events Extraction via Attention-based Graph Information\n  Aggregation", "abstract": "Event extraction is of practical utility in natural language processing. In\nthe real world, it is a common phenomenon that multiple events existing in the\nsame sentence, where extracting them are more difficult than extracting a\nsingle event. Previous works on modeling the associations between events by\nsequential modeling methods suffer a lot from the low efficiency in capturing\nvery long-range dependencies. In this paper, we propose a novel Jointly\nMultiple Events Extraction (JMEE) framework to jointly extract multiple event\ntriggers and arguments by introducing syntactic shortcut arcs to enhance\ninformation flow and attention-based graph convolution networks to model graph\ninformation. The experiment results demonstrate that our proposed framework\nachieves competitive results compared with state-of-the-art methods.", "published": "2018-09-24 17:46:27", "link": "http://arxiv.org/abs/1809.09078v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stochastic Answer Networks for SQuAD 2.0", "abstract": "This paper presents an extension of the Stochastic Answer Network (SAN), one\nof the state-of-the-art machine reading comprehension models, to be able to\njudge whether a question is unanswerable or not. The extended SAN contains two\ncomponents: a span detector and a binary classifier for judging whether the\nquestion is unanswerable, and both components are jointly optimized.\nExperiments show that SAN achieves the results competitive to the\nstate-of-the-art on Stanford Question Answering Dataset (SQuAD) 2.0. To\nfacilitate the research on this field, we release our code:\nhttps://github.com/kevinduh/san_mrc.", "published": "2018-09-24 19:58:07", "link": "http://arxiv.org/abs/1809.09194v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hindi-English Code-Switching Speech Corpus", "abstract": "Code-switching refers to the usage of two languages within a sentence or\ndiscourse. It is a global phenomenon among multilingual communities and has\nemerged as an independent area of research. With the increasing demand for the\ncode-switching automatic speech recognition (ASR) systems, the development of a\ncode-switching speech corpus has become highly desirable. However, for training\nsuch systems, very limited code-switched resources are available as yet. In\nthis work, we present our first efforts in building a code-switching ASR system\nin the Indian context. For that purpose, we have created a Hindi-English\ncode-switching speech database. The database not only contains the speech\nutterances with code-switching properties but also covers the session and the\nspeaker variations like pronunciation, accent, age, gender, etc. This database\ncan be applied in several speech signal processing applications, such as\ncode-switching ASR, language identification, language modeling, speech\nsynthesis etc. This paper mainly presents an analysis of the statistics of the\ncollected code-switching speech corpus. Later, the performance results for the\nASR task have been reported for the created database.", "published": "2018-09-24 03:39:47", "link": "http://arxiv.org/abs/1810.00662v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recognizing Film Entities in Podcasts", "abstract": "In this paper, we propose a Named Entity Recognition (NER) system to identify\nfilm titles in podcast audio. Taking inspiration from NER systems for noisy\ntext in social media, we implement a two-stage approach that is robust to\ncomputer transcription errors and does not require significant computational\nexpense to accommodate new film titles/releases. Evaluating on a diverse set of\npodcasts, we demonstrate more than a 20% increase in F1 score across three\nbaseline approaches when combining fuzzy-matching with a linear model aware of\nfilm-specific metadata.", "published": "2018-09-24 01:06:46", "link": "http://arxiv.org/abs/1809.08711v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Speaker Naming in Movies", "abstract": "We propose a new model for speaker naming in movies that leverages visual,\ntextual, and acoustic modalities in an unified optimization framework. To\nevaluate the performance of our model, we introduce a new dataset consisting of\nsix episodes of the Big Bang Theory TV show and eighteen full movies covering\ndifferent genres. Our experiments show that our multimodal model significantly\noutperforms several competitive baselines on the average weighted F-score\nmetric. To demonstrate the effectiveness of our framework, we design an\nend-to-end memory network model that leverages our speaker naming model and\nachieves state-of-the-art results on the subtitles task of the MovieQA 2017\nChallenge.", "published": "2018-09-24 05:00:05", "link": "http://arxiv.org/abs/1809.08761v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Information-Weighted Neural Cache Language Models for ASR", "abstract": "Neural cache language models (LMs) extend the idea of regular cache language\nmodels by making the cache probability dependent on the similarity between the\ncurrent context and the context of the words in the cache. We make an extensive\ncomparison of 'regular' cache models with neural cache models, both in terms of\nperplexity and WER after rescoring first-pass ASR results. Furthermore, we\npropose two extensions to this neural cache model that make use of the content\nvalue/information weight of the word: firstly, combining the cache probability\nand LM probability with an information-weighted interpolation and secondly,\nselectively adding only content words to the cache. We obtain a 29.9%/32.1%\n(validation/test set) relative improvement in perplexity with respect to a\nbaseline LSTM LM on the WikiText-2 dataset, outperforming previous work on\nneural cache LMs. Additionally, we observe significant WER reductions with\nrespect to the baseline model on the WSJ ASR task.", "published": "2018-09-24 10:07:27", "link": "http://arxiv.org/abs/1809.08826v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain\n  Semantic Parsing and Text-to-SQL Task", "abstract": "We present Spider, a large-scale, complex and cross-domain semantic parsing\nand text-to-SQL dataset annotated by 11 college students. It consists of 10,181\nquestions and 5,693 unique complex SQL queries on 200 databases with multiple\ntables, covering 138 different domains. We define a new complex and\ncross-domain semantic parsing and text-to-SQL task where different complex SQL\nqueries and databases appear in train and test sets. In this way, the task\nrequires the model to generalize well to both new SQL queries and new database\nschemas. Spider is distinct from most of the previous semantic parsing tasks\nbecause they all use a single database and the exact same programs in the train\nset and the test set. We experiment with various state-of-the-art models and\nthe best model achieves only 12.4% exact matching accuracy on a database split\nsetting. This shows that Spider presents a strong challenge for future\nresearch. Our dataset and task are publicly available at\nhttps://yale-lily.github.io/spider", "published": "2018-09-24 13:03:13", "link": "http://arxiv.org/abs/1809.08887v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WiRe57 : A Fine-Grained Benchmark for Open Information Extraction", "abstract": "We build a reference for the task of Open Information Extraction, on five\ndocuments. We tentatively resolve a number of issues that arise, including\ninference and granularity. We seek to better pinpoint the requirements for the\ntask. We produce our annotation guidelines specifying what is correct to\nextract and what is not. In turn, we use this reference to score existing Open\nIE systems. We address the non-trivial problem of evaluating the extractions\nproduced by systems against the reference tuples, and share our evaluation\nscript. Among seven compared extractors, we find the MinIE system to perform\nbest.", "published": "2018-09-24 14:19:59", "link": "http://arxiv.org/abs/1809.08962v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Translating Navigation Instructions in Natural Language to a High-Level\n  Plan for Behavioral Robot Navigation", "abstract": "We propose an end-to-end deep learning model for translating free-form\nnatural language instructions to a high-level plan for behavioral robot\nnavigation. We use attention models to connect information from both the user\ninstructions and a topological representation of the environment. We evaluate\nour model's performance on a new dataset containing 10,050 pairs of navigation\ninstructions. Our model significantly outperforms baseline approaches.\nFurthermore, our results suggest that it is possible to leverage the\nenvironment map as a relevant knowledge base to facilitate the translation of\nfree-form navigational instruction.", "published": "2018-09-24 06:09:20", "link": "http://arxiv.org/abs/1810.00663v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Central Bank Communication and the Yield Curve: A Semi-Automatic\n  Approach using Non-Negative Matrix Factorization", "abstract": "Communication is now a standard tool in the central bank's monetary policy\ntoolkit. Theoretically, communication provides the central bank an opportunity\nto guide public expectations, and it has been shown empirically that central\nbank communication can lead to financial market fluctuations. However, there\nhas been little research into which dimensions or topics of information are\nmost important in causing these fluctuations. We develop a semi-automatic\nmethodology that summarizes the FOMC statements into its main themes,\nautomatically selects the best model based on coherency, and assesses whether\nthere is a significant impact of these themes on the shape of the U.S Treasury\nyield curve using topic modeling methods from the machine learning literature.\nOur findings suggest that the FOMC statements can be decomposed into three\ntopics: (i) information related to the economic conditions and the mandates,\n(ii) information related to monetary policy tools and intermediate targets, and\n(iii) information related to financial markets and the financial crisis. We\nfind that statements are most influential during the financial crisis and the\neffects are mostly present in the curvature of the yield curve through\ninformation related to the financial theme.", "published": "2018-09-24 01:46:05", "link": "http://arxiv.org/abs/1809.08718v1", "categories": ["econ.GN", "cs.CL", "q-fin.EC"], "primary_category": "econ.GN"}
{"title": "Scalable inference of topic evolution via models for latent geometric\n  structures", "abstract": "We develop new models and algorithms for learning the temporal dynamics of\nthe topic polytopes and related geometric objects that arise in topic model\nbased inference. Our model is nonparametric Bayesian and the corresponding\ninference algorithm is able to discover new topics as the time progresses. By\nexploiting the connection between the modeling of topic polytope evolution,\nBeta-Bernoulli process and the Hungarian matching algorithm, our method is\nshown to be several orders of magnitude faster than existing topic modeling\napproaches, as demonstrated by experiments working with several million\ndocuments in under two dozens of minutes.", "published": "2018-09-24 03:23:07", "link": "http://arxiv.org/abs/1809.08738v3", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Chargrid: Towards Understanding 2D Documents", "abstract": "We introduce a novel type of text representation that preserves the 2D layout\nof a document. This is achieved by encoding each document page as a\ntwo-dimensional grid of characters. Based on this representation, we present a\ngeneric document understanding pipeline for structured documents. This pipeline\nmakes use of a fully convolutional encoder-decoder network that predicts a\nsegmentation mask and bounding boxes. We demonstrate its capabilities on an\ninformation extraction task from invoices and show that it significantly\noutperforms approaches based on sequential text or document images.", "published": "2018-09-24 08:37:02", "link": "http://arxiv.org/abs/1809.08799v1", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "From Audio to Semantics: Approaches to end-to-end spoken language\n  understanding", "abstract": "Conventional spoken language understanding systems consist of two main\ncomponents: an automatic speech recognition module that converts audio to a\ntranscript, and a natural language understanding module that transforms the\nresulting text (or top N hypotheses) into a set of domains, intents, and\narguments. These modules are typically optimized independently. In this paper,\nwe formulate audio to semantic understanding as a sequence-to-sequence problem\n[1]. We propose and compare various encoder-decoder based approaches that\noptimize both modules jointly, in an end-to-end manner. Evaluations on a\nreal-world task show that 1) having an intermediate text representation is\ncrucial for the quality of the predicted semantics, especially the intent\narguments and 2) jointly optimizing the full system improves overall accuracy\nof prediction. Compared to independently trained models, our best jointly\ntrained model achieves similar domain and intent prediction F1 scores, but\nimproves argument word error rate by 18% relative.", "published": "2018-09-24 19:46:24", "link": "http://arxiv.org/abs/1809.09190v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Text Similarity in Vector Space Models: A Comparative Study", "abstract": "Automatic measurement of semantic text similarity is an important task in\nnatural language processing. In this paper, we evaluate the performance of\ndifferent vector space models to perform this task. We address the real-world\nproblem of modeling patent-to-patent similarity and compare TFIDF (and related\nextensions), topic models (e.g., latent semantic indexing), and neural models\n(e.g., paragraph vectors). Contrary to expectations, the added computational\ncost of text embedding methods is justified only when: 1) the target text is\ncondensed; and 2) the similarity comparison is trivial. Otherwise, TFIDF\nperforms surprisingly well in other cases: in particular for longer and more\ntechnical texts or for making finer-grained distinctions between nearest\nneighbors. Unexpectedly, extensions to the TFIDF method, such as adding noun\nphrases or calculating term weights incrementally, were not helpful in our\ncontext.", "published": "2018-09-24 10:54:52", "link": "http://arxiv.org/abs/1810.00664v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
