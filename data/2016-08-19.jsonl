{"title": "Who did What: A Large-Scale Person-Centered Cloze Dataset", "abstract": "We have constructed a new \"Who-did-What\" dataset of over 200,000\nfill-in-the-gap (cloze) multiple choice reading comprehension problems\nconstructed from the LDC English Gigaword newswire corpus. The WDW dataset has\na variety of novel features. First, in contrast with the CNN and Daily Mail\ndatasets (Hermann et al., 2015) we avoid using article summaries for question\nformation. Instead, each problem is formed from two independent articles --- an\narticle given as the passage to be read and a separate article on the same\nevents used to form the question. Second, we avoid anonymization --- each\nchoice is a person named entity. Third, the problems have been filtered to\nremove a fraction that are easily solved by simple baselines, while remaining\n84% solvable by humans. We report performance benchmarks of standard systems\nand propose the WDW dataset as a challenge task for the community.", "published": "2016-08-19 00:13:10", "link": "http://arxiv.org/abs/1608.05457v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Selection of Context Configurations for Improved\n  Class-Specific Word Representations", "abstract": "This paper is concerned with identifying contexts useful for training word\nrepresentation models for different word classes such as adjectives (A), verbs\n(V), and nouns (N). We introduce a simple yet effective framework for an\nautomatic selection of class-specific context configurations. We construct a\ncontext configuration space based on universal dependency relations between\nwords, and efficiently search this space with an adapted beam search algorithm.\nIn word similarity tasks for each word class, we show that our framework is\nboth effective and efficient. Particularly, it improves the Spearman's rho\ncorrelation with human scores on SimLex-999 over the best previously proposed\nclass-specific contexts by 6 (A), 6 (V) and 5 (N) rho points. With our selected\ncontext configurations, we train on only 14% (A), 26.2% (V), and 33.6% (N) of\nall dependency-based contexts, resulting in a reduced training time. Our\nresults generalise: we show that the configurations our algorithm learns for\none English training setup outperform previously proposed context types in\nanother training setup for English. Moreover, basing the configuration space on\nuniversal dependencies, it is possible to transfer the learned configurations\nto German and Italian. We also demonstrate improved per-class results over\nother context types in these two languages.", "published": "2016-08-19 08:30:35", "link": "http://arxiv.org/abs/1608.05528v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Start for Sequence to Sequence Architecture", "abstract": "The sequence to sequence architecture is widely used in the response\ngeneration and neural machine translation to model the potential relationship\nbetween two sentences. It typically consists of two parts: an encoder that\nreads from the source sentence and a decoder that generates the target sentence\nword by word according to the encoder's output and the last generated word.\nHowever, it faces to the cold start problem when generating the first word as\nthere is no previous word to refer. Existing work mainly use a special start\nsymbol </s>to generate the first word. An obvious drawback of these work is\nthat there is not a learnable relationship between words and the start symbol.\nFurthermore, it may lead to the error accumulation for decoding when the first\nword is incorrectly generated. In this paper, we proposed a novel approach to\nlearning to generate the first word in the sequence to sequence architecture\nrather than using the start symbol. Experimental results on the task of\nresponse generation of short text conversation show that the proposed approach\noutperforms the state-of-the-art approach in both of the automatic and manual\nevaluations.", "published": "2016-08-19 09:48:13", "link": "http://arxiv.org/abs/1608.05554v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Human Reading with Neural Attention", "abstract": "When humans read text, they fixate some words and skip others. However, there\nhave been few attempts to explain skipping behavior with computational models,\nas most existing work has focused on predicting reading times (e.g.,~using\nsurprisal). In this paper, we propose a novel approach that models both\nskipping and reading, using an unsupervised architecture that combines a neural\nattention with autoencoding, trained on raw text using reinforcement learning.\nOur model explains human reading behavior as a tradeoff between precision of\nlanguage understanding (encoding the input accurately) and economy of attention\n(fixating as few words as possible). We evaluate the model on the Dundee\neye-tracking corpus, showing that it accurately predicts skipping behavior and\nreading times, is competitive with surprisal, and captures known qualitative\nfeatures of human reading.", "published": "2016-08-19 14:03:46", "link": "http://arxiv.org/abs/1608.05604v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Distributed Representations to Disambiguate Biomedical and\n  Clinical Concepts", "abstract": "In this paper, we report a knowledge-based method for Word Sense\nDisambiguation in the domains of biomedical and clinical text. We combine word\nrepresentations created on large corpora with a small number of definitions\nfrom the UMLS to create concept representations, which we then compare to\nrepresentations of the context of ambiguous terms. Using no relational\ninformation, we obtain comparable performance to previous approaches on the\nMSH-WSD dataset, which is a well-known dataset in the biomedical domain.\nAdditionally, our method is fast and easy to set up and extend to other\ndomains. Supplementary materials, including source code, can be found at https:\n//github.com/clips/yarn", "published": "2016-08-19 14:05:03", "link": "http://arxiv.org/abs/1608.05605v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
