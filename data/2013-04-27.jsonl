{"title": "Constant conditional entropy and related hypotheses", "abstract": "Constant entropy rate (conditional entropies must remain constant as the\nsequence length increases) and uniform information density (conditional\nprobabilities must remain constant as the sequence length increases) are two\ninformation theoretic principles that are argued to underlie a wide range of\nlinguistic phenomena. Here we revise the predictions of these principles to the\nlight of Hilberg's law on the scaling of conditional entropy in language and\nrelated laws. We show that constant entropy rate (CER) and two interpretations\nfor uniform information density (UID), full UID and strong UID, are\ninconsistent with these laws. Strong UID implies CER but the reverse is not\ntrue. Full UID, a particular case of UID, leads to costly uncorrelated\nsequences that are totally unrealistic. We conclude that CER and its particular\ncases are incomplete hypotheses about the scaling of conditional entropies.", "published": "2013-04-27 11:47:40", "link": "http://arxiv.org/abs/1304.7359v2", "categories": ["cond-mat.stat-mech", "cs.CL", "cs.IT", "math.IT", "physics.data-an"], "primary_category": "cond-mat.stat-mech"}
