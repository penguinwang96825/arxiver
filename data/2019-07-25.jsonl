{"title": "Summary Refinement through Denoising", "abstract": "We propose a simple method for post-processing the outputs of a text\nsummarization system in order to refine its overall quality. Our approach is to\ntrain text-to-text rewriting models to correct information redundancy errors\nthat may arise during summarization. We train on synthetically generated noisy\nsummaries, testing three different types of noise that introduce out-of-context\ninformation within each summary. When applied on top of extractive and\nabstractive summarization baselines, our summary denoising models yield metric\nimprovements while reducing redundancy.", "published": "2019-07-25 07:37:41", "link": "http://arxiv.org/abs/1907.10873v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grammatical Sequence Prediction for Real-Time Neural Semantic Parsing", "abstract": "While sequence-to-sequence (seq2seq) models achieve state-of-the-art\nperformance in many natural language processing tasks, they can be too slow for\nreal-time applications. One performance bottleneck is predicting the most\nlikely next token over a large vocabulary; methods to circumvent this\nbottleneck are a current research topic. We focus specifically on using seq2seq\nmodels for semantic parsing, where we observe that grammars often exist which\nspecify valid formal representations of utterance semantics. By developing a\ngeneric approach for restricting the predictions of a seq2seq model to\ngrammatically permissible continuations, we arrive at a widely applicable\ntechnique for speeding up semantic parsing. The technique leads to a 74%\nspeed-up on an in-house dataset with a large vocabulary, compared to the same\nneural model without grammatical restrictions.", "published": "2019-07-25 13:45:48", "link": "http://arxiv.org/abs/1907.11049v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HireNet: a Hierarchical Attention Model for the Automatic Analysis of\n  Asynchronous Video Job Interviews", "abstract": "New technologies drastically change recruitment techniques. Some research\nprojects aim at designing interactive systems that help candidates practice job\ninterviews. Other studies aim at the automatic detection of social signals\n(e.g. smile, turn of speech, etc...) in videos of job interviews. These studies\nare limited with respect to the number of interviews they process, but also by\nthe fact that they only analyze simulated job interviews (e.g. students\npretending to apply for a fake position). Asynchronous video interviewing tools\nhave become mature products on the human resources market, and thus, a popular\nstep in the recruitment process. As part of a project to help recruiters, we\ncollected a corpus of more than 7000 candidates having asynchronous video job\ninterviews for real positions and recording videos of themselves answering a\nset of questions. We propose a new hierarchical attention model called HireNet\nthat aims at predicting the hirability of the candidates as evaluated by\nrecruiters. In HireNet, an interview is considered as a sequence of questions\nand answers containing salient socials signals. Two contextual sources of\ninformation are modeled in HireNet: the words contained in the question and in\nthe job position. Our model achieves better F1-scores than previous approaches\nfor each modality (verbal content, audio and video). Results from early and\nlate multimodal fusion suggest that more sophisticated fusion schemes are\nneeded to improve on the monomodal results. Finally, some examples of moments\ncaptured by the attention mechanisms suggest our model could potentially be\nused to help finding key moments in an asynchronous job interview.", "published": "2019-07-25 14:00:18", "link": "http://arxiv.org/abs/1907.11062v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DropAttention: A Regularization Method for Fully-Connected\n  Self-Attention Networks", "abstract": "Variants dropout methods have been designed for the fully-connected layer,\nconvolutional layer and recurrent layer in neural networks, and shown to be\neffective to avoid overfitting. As an appealing alternative to recurrent and\nconvolutional layers, the fully-connected self-attention layer surprisingly\nlacks a specific dropout method. This paper explores the possibility of\nregularizing the attention weights in Transformers to prevent different\ncontextualized feature vectors from co-adaption. Experiments on a wide range of\ntasks show that DropAttention can improve performance and reduce overfitting.", "published": "2019-07-25 14:03:06", "link": "http://arxiv.org/abs/1907.11065v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Transfer for Distantly Supervised and Low-resources\n  Indonesian NER", "abstract": "Manually annotated corpora for low-resource languages are usually small in\nquantity (gold), or large but distantly supervised (silver). Inspired by recent\nprogress of injecting pre-trained language model (LM) on many Natural Language\nProcessing (NLP) task, we proposed to fine-tune pre-trained language model from\nhigh-resources languages to low-resources languages to improve the performance\nof both scenarios. Our empirical experiment demonstrates significant\nimprovement when fine-tuning pre-trained language model in cross-lingual\ntransfer scenarios for small gold corpus and competitive results in large\nsilver compare to supervised cross-lingual transfer, which will be useful when\nthere is no parallel annotation in the same task to begin. We compare our\nproposed method of cross-lingual transfer using pre-trained LM to different\nsources of transfer such as mono-lingual LM and Part-of-Speech tagging (POS) in\nthe downstream task of both large silver and small gold NER dataset by\nexploiting character-level input of bi-directional language model task.", "published": "2019-07-25 16:04:09", "link": "http://arxiv.org/abs/1907.11158v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Time Masking: Leveraging Temporal Information in Spoken Dialogue Systems", "abstract": "In a spoken dialogue system, dialogue state tracker (DST) components track\nthe state of the conversation by updating a distribution of values associated\nwith each of the slots being tracked for the current user turn, using the\ninteractions until then. Much of the previous work has relied on modeling the\nnatural order of the conversation, using distance based offsets as an\napproximation of time. In this work, we hypothesize that leveraging the\nwall-clock temporal difference between turns is crucial for finer-grained\ncontrol of dialogue scenarios. We develop a novel approach that applies a {\\it\ntime mask}, based on the wall-clock time difference, to the associated slot\nembeddings and empirically demonstrate that our proposed approach outperforms\nexisting approaches that leverage distance offsets, on both an internal\nbenchmark dataset as well as DSTC2.", "published": "2019-07-25 21:33:29", "link": "http://arxiv.org/abs/1907.11315v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Ranking Based Cost-sensitive Multi-label Learning for Distant\n  Supervision Relation Extraction", "abstract": "Knowledge base provides a potential way to improve the intelligence of\ninformation retrieval (IR) systems, for that knowledge base has numerous\nrelations between entities which can help the IR systems to conduct inference\nfrom one entity to another entity. Relation extraction is one of the\nfundamental techniques to construct a knowledge base. Distant supervision is a\nsemi-supervised learning method for relation extraction which learns with\nlabeled and unlabeled data. However, this approach suffers the problem of\nrelation overlapping in which one entity tuple may have multiple relation\nfacts. We believe that relation types can have latent connections, which we\ncall class ties, and can be exploited to enhance relation extraction. However,\nthis property between relation classes has not been fully explored before. In\nthis paper, to exploit class ties between relations to improve relation\nextraction, we propose a general ranking based multi-label learning framework\ncombined with convolutional neural networks, in which ranking based loss\nfunctions with regularization technique are introduced to learn the latent\nconnections between relations. Furthermore, to deal with the problem of class\nimbalance in distant supervision relation extraction, we further adopt\ncost-sensitive learning to rescale the costs from the positive and negative\nlabels. Extensive experiments on a widely used dataset show the effectiveness\nof our model to exploit class ties and to relieve class imbalance problem.", "published": "2019-07-25 07:41:45", "link": "http://arxiv.org/abs/1907.11521v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "INS: An Interactive Chinese News Synthesis System", "abstract": "Nowadays, we are surrounded by more and more online news articles. Tens or\nhundreds of news articles need to be read if we wish to explore a hot news\nevent or topic. So it is of vital importance to automatically synthesize a\nbatch of news articles related to the event or topic into a new synthesis\narticle (or overview article) for reader's convenience. It is so challenging to\nmake news synthesis fully automatic that there is no successful solution by\nnow. In this paper, we put forward a novel Interactive News Synthesis system\n(i.e. INS), which can help generate news overview articles automatically or by\ninteracting with users. More importantly, INS can serve as a tool for editors\nto help them finish their jobs. In our experiments, INS performs well on both\ntopic representation and synthesis article generation. A user study also\ndemonstrates the usefulness and users' satisfaction with the INS tool. A demo\nvideo is available at \\url{https://youtu.be/7ItteKW3GEk}.", "published": "2019-07-25 01:01:27", "link": "http://arxiv.org/abs/1907.10781v1", "categories": ["cs.CL", "cs.HC", "68U15"], "primary_category": "cs.CL"}
{"title": "Adaptive Noise Injection: A Structure-Expanding Regularization for RNN", "abstract": "The vanilla LSTM has become one of the most potential architectures in\nword-level language modeling, like other recurrent neural networks, overfitting\nis always a key barrier for its effectiveness. The existing noise-injected\nregularizations introduce the random noises of fixation intensity, which\ninhibits the learning of the RNN throughout the training process. In this\npaper, we propose a new structure-expanding regularization method called\nAdjective Noise Injection (ANI), which considers the output of an extra RNN\nbranch as a kind of adaptive noises and injects it into the main-branch RNN\noutput. Due to the adaptive noises can be improved as the training processes,\nits negative effects can be weakened and even transformed into a positive\neffect to further improve the expressiveness of the main-branch RNN. As a\nresult, ANI can regularize the RNN in the early stage of training and further\npromoting its training performance in the later stage. We conduct experiments\non three widely-used corpora: PTB, WT2, and WT103, whose results verify both\nthe regularization and promoting the training performance functions of ANI.\nFurthermore, we design a series simulation experiments to explore the reasons\nthat may lead to the regularization effect of ANI, and we find that in training\nprocess, the robustness against the parameter update errors can be strengthened\nwhen the LSTM equipped with ANI.", "published": "2019-07-25 07:58:08", "link": "http://arxiv.org/abs/1907.10885v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Using Answer Set Programming for Commonsense Reasoning in the Winograd\n  Schema Challenge", "abstract": "The Winograd Schema Challenge (WSC) is a natural language understanding task\nproposed as an alternative to the Turing test in 2011. In this work we attempt\nto solve WSC problems by reasoning with additional knowledge. By using an\napproach built on top of graph-subgraph isomorphism encoded using Answer Set\nProgramming (ASP) we were able to handle 240 out of 291 WSC problems. The ASP\nencoding allows us to add additional constraints in an elaboration tolerant\nmanner. In the process we present a graph based representation of WSC problems\nas well as relevant commonsense knowledge. This paper is under consideration\nfor acceptance in TPLP.", "published": "2019-07-25 14:45:04", "link": "http://arxiv.org/abs/1907.11112v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "HEIDL: Learning Linguistic Expressions with Deep Learning and\n  Human-in-the-Loop", "abstract": "While the role of humans is increasingly recognized in machine learning\ncommunity, representation of and interaction with models in current\nhuman-in-the-loop machine learning (HITL-ML) approaches are too low-level and\nfar-removed from human's conceptual models. We demonstrate HEIDL, a prototype\nHITL-ML system that exposes the machine-learned model through high-level,\nexplainable linguistic expressions formed of predicates representing semantic\nstructure of text. In HEIDL, human's role is elevated from simply evaluating\nmodel predictions to interpreting and even updating the model logic directly by\nenabling interaction with rule predicates themselves. Raising the currency of\ninteraction to such semantic levels calls for new interaction paradigms between\nhumans and machines that result in improved productivity for text analytics\nmodel development process. Moreover, by involving humans in the process, the\nhuman-machine co-created models generalize better to unseen data as domain\nexperts are able to instill their expertise by extrapolating from what has been\nlearned by automated algorithms from few labelled data.", "published": "2019-07-25 16:45:06", "link": "http://arxiv.org/abs/1907.11184v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Interactive Lungs Auscultation with Reinforcement Learning Agent", "abstract": "To perform a precise auscultation for the purposes of examination of\nrespiratory system normally requires the presence of an experienced doctor.\nWith most recent advances in machine learning and artificial intelligence,\nautomatic detection of pathological breath phenomena in sounds recorded with\nstethoscope becomes a reality. But to perform a full auscultation in home\nenvironment by layman is another matter, especially if the patient is a child.\nIn this paper we propose a unique application of Reinforcement Learning for\ntraining an agent that interactively guides the end user throughout the\nauscultation procedure. We show that \\textit{intelligent} selection of\nauscultation points by the agent reduces time of the examination fourfold\nwithout significant decrease in diagnosis accuracy compared to exhaustive\nauscultation.", "published": "2019-07-25 11:04:08", "link": "http://arxiv.org/abs/1907.11238v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
