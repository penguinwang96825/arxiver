{"title": "Cross-document Event Identity via Dense Annotation", "abstract": "In this paper, we study the identity of textual events from different\ndocuments. While the complex nature of event identity is previously studied\n(Hovy et al., 2013), the case of events across documents is unclear. Prior work\non cross-document event coreference has two main drawbacks. First, they\nrestrict the annotations to a limited set of event types. Second, they\ninsufficiently tackle the concept of event identity. Such annotation setup\nreduces the pool of event mentions and prevents one from considering the\npossibility of quasi-identity relations. We propose a dense annotation approach\nfor cross-document event coreference, comprising a rich source of event\nmentions and a dense annotation effort between related document pairs. To this\nend, we design a new annotation workflow with careful quality control and an\neasy-to-use annotation interface. In addition to the links, we further collect\noverlapping event contexts, including time, location, and participants, to shed\nsome light on the relation between identity decisions and context. We present\nan open-access dataset for cross-document event coreference, CDEC-WN, collected\nfrom English Wikinews and open-source our annotation toolkit to encourage\nfurther research on cross-document tasks.", "published": "2021-09-14 03:57:58", "link": "http://arxiv.org/abs/2109.06417v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Commonsense-Focused Dialogues for Response Generation: An Empirical\n  Study", "abstract": "Smooth and effective communication requires the ability to perform latent or\nexplicit commonsense inference. Prior commonsense reasoning benchmarks (such as\nSocialIQA and CommonsenseQA) mainly focus on the discriminative task of\nchoosing the right answer from a set of candidates, and do not involve\ninteractive language generation as in dialogue. Moreover, existing dialogue\ndatasets do not explicitly focus on exhibiting commonsense as a facet. In this\npaper, we present an empirical study of commonsense in dialogue response\ngeneration. We first auto-extract commonsensical dialogues from existing\ndialogue datasets by leveraging ConceptNet, a commonsense knowledge graph.\nFurthermore, building on social contexts/situations in SocialIQA, we collect a\nnew dialogue dataset with 25K dialogues aimed at exhibiting social commonsense\nin an interactive setting. We evaluate response generation models trained using\nthese datasets and find that models trained on both extracted and our collected\ndata produce responses that consistently exhibit more commonsense than\nbaselines. Finally we propose an approach for automatic evaluation of\ncommonsense that relies on features derived from ConceptNet and pre-trained\nlanguage and dialog models, and show reasonable correlation with human\nevaluation of responses' commonsense quality. We are releasing a subset of our\ncollected data, Commonsense-Dialogues, containing about 11K dialogs.", "published": "2021-09-14 04:32:09", "link": "http://arxiv.org/abs/2109.06427v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncovering Implicit Gender Bias in Narratives through Commonsense\n  Inference", "abstract": "Pre-trained language models learn socially harmful biases from their training\ncorpora, and may repeat these biases when used for generation. We study gender\nbiases associated with the protagonist in model-generated stories. Such biases\nmay be expressed either explicitly (\"women can't park\") or implicitly (e.g. an\nunsolicited male character guides her into a parking space). We focus on\nimplicit biases, and use a commonsense reasoning engine to uncover them.\nSpecifically, we infer and analyze the protagonist's motivations, attributes,\nmental states, and implications on others. Our findings regarding implicit\nbiases are in line with prior work that studied explicit biases, for example\nshowing that female characters' portrayal is centered around appearance, while\nmale figures' focus on intellect.", "published": "2021-09-14 04:57:45", "link": "http://arxiv.org/abs/2109.06437v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Task-adaptive Pre-training and Self-training are Complementary for\n  Natural Language Understanding", "abstract": "Task-adaptive pre-training (TAPT) and Self-training (ST) have emerged as the\nmajor semi-supervised approaches to improve natural language understanding\n(NLU) tasks with massive amount of unlabeled data. However, it's unclear\nwhether they learn similar representations or they can be effectively combined.\nIn this paper, we show that TAPT and ST can be complementary with simple TFS\nprotocol by following TAPT -> Finetuning -> Self-training (TFS) process.\nExperimental results show that TFS protocol can effectively utilize unlabeled\ndata to achieve strong combined gains consistently across six datasets covering\nsentiment classification, paraphrase identification, natural language\ninference, named entity recognition and dialogue slot classification. We\ninvestigate various semi-supervised settings and consistently show that gains\nfrom TAPT and ST can be strongly additive by following TFS procedure. We hope\nthat TFS could serve as an important semi-supervised baseline for future NLP\nstudies.", "published": "2021-09-14 06:24:28", "link": "http://arxiv.org/abs/2109.06466v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Untrustworthy Samples: Data Filtering for Open-domain\n  Dialogues with Bayesian Optimization", "abstract": "Being able to reply with a related, fluent, and informative response is an\nindispensable requirement for building high-quality conversational agents. In\norder to generate better responses, some approaches have been proposed, such as\nfeeding extra information by collecting large-scale datasets with human\nannotations, designing neural conversational models (NCMs) with complex\narchitecture and loss functions, or filtering out untrustworthy samples based\non a dialogue attribute, e.g., Relatedness or Genericness. In this paper, we\nfollow the third research branch and present a data filtering method for\nopen-domain dialogues, which identifies untrustworthy samples from training\ndata with a quality measure that linearly combines seven dialogue attributes.\nThe attribute weights are obtained via Bayesian Optimization (BayesOpt) that\naims to optimize an objective function for dialogue generation iteratively on\nthe validation set. Then we score training samples with the quality measure,\nsort them in descending order, and filter out those at the bottom. Furthermore,\nto accelerate the \"filter-train-evaluate\" iterations involved in BayesOpt on\nlarge-scale datasets, we propose a training framework that integrates maximum\nlikelihood estimation (MLE) and negative training method (NEG). The training\nmethod updates parameters of a trained NCMs on two small sets with newly\nmaintained and removed samples, respectively. Specifically, MLE is applied to\nmaximize the log-likelihood of newly maintained samples, while NEG is used to\nminimize the log-likelihood of newly removed ones. Experimental results on two\ndatasets show that our method can effectively identify untrustworthy samples,\nand NCMs trained on the filtered datasets achieve better performance.", "published": "2021-09-14 06:42:54", "link": "http://arxiv.org/abs/2109.06471v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilevel profiling of situation and dialogue-based deep networks for\n  movie genre classification using movie trailers", "abstract": "Automated movie genre classification has emerged as an active and essential\narea of research and exploration. Short duration movie trailers provide useful\ninsights about the movie as video content consists of the cognitive and the\naffective level features. Previous approaches were focused upon either\ncognitive or affective content analysis. In this paper, we propose a novel\nmulti-modality: situation, dialogue, and metadata-based movie genre\nclassification framework that takes both cognition and affect-based features\ninto consideration. A pre-features fusion-based framework that takes into\naccount: situation-based features from a regular snapshot of a trailer that\nincludes nouns and verbs providing the useful affect-based mapping with the\ncorresponding genres, dialogue (speech) based feature from audio, metadata\nwhich together provides the relevant information for cognitive and affect based\nvideo analysis. We also develop the English movie trailer dataset (EMTD), which\ncontains 2000 Hollywood movie trailers belonging to five popular genres:\nAction, Romance, Comedy, Horror, and Science Fiction, and perform\ncross-validation on the standard LMTD-9 dataset for validating the proposed\nframework. The results demonstrate that the proposed methodology for movie\ngenre classification has performed excellently as depicted by the F1 scores,\nprecision, recall, and area under the precision-recall curves.", "published": "2021-09-14 07:33:56", "link": "http://arxiv.org/abs/2109.06488v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tribrid: Stance Classification with Neural Inconsistency Detection", "abstract": "We study the problem of performing automatic stance classification on social\nmedia with neural architectures such as BERT. Although these architectures\ndeliver impressive results, their level is not yet comparable to the one of\nhumans and they might produce errors that have a significant impact on the\ndownstream task (e.g., fact-checking). To improve the performance, we present a\nnew neural architecture where the input also includes automatically generated\nnegated perspectives over a given claim. The model is jointly learned to make\nsimultaneously multiple predictions, which can be used either to improve the\nclassification of the original perspective or to filter out doubtful\npredictions. In the first case, we propose a weakly supervised method for\ncombining the predictions into a final one. In the second case, we show that\nusing the confidence scores to remove doubtful predictions allows our method to\nachieve human-like performance over the retained information, which is still a\nsizable part of the original input.", "published": "2021-09-14 08:13:03", "link": "http://arxiv.org/abs/2109.06508v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Prompt-based Few-shot Learning for Grounded Dialog Generation", "abstract": "Dialog models can be greatly strengthened through grounding on various\nexternal information, but grounded dialog corpora are usually not naturally\naccessible. In this work, we focus on the few-shot learning for grounded dialog\ngeneration (GDG). We first propose a simple prompting method for GDG tasks,\nwhere different constructs of model input, such as the grounding source and the\nconversation context, are distinguished through continuous or discrete prompts.\nOn three typical GDG tasks, we empirically demonstrate and analyze in-depth the\neffectiveness of our method. We then conduct extensive experiments to\nthoroughly investigate how our prompting method works with different\npre-trained models. We show that prompted language models perform superiorly to\nconversational models, and further analyze various factors that influence the\neffects of prompting. Overall, our work introduces a prompt-based perspective\nto the few-shot learning for GDG tasks, and provides valuable findings and\ninsights for future research.", "published": "2021-09-14 08:17:57", "link": "http://arxiv.org/abs/2109.06513v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Sampling of Dependency Structures", "abstract": "Probabilistic distributions over spanning trees in directed graphs are a\nfundamental model of dependency structure in natural language processing,\nsyntactic dependency trees. In NLP, dependency trees often have an additional\nroot constraint: only one edge may emanate from the root. However, no sampling\nalgorithm has been presented in the literature to account for this additional\nconstraint. In this paper, we adapt two spanning tree sampling algorithms to\nfaithfully sample dependency trees from a graph subject to the root constraint.\nWilson (1996)'s sampling algorithm has a running time of $\\mathcal{O}(H)$ where\n$H$ is the mean hitting time of the graph. Colbourn (1996)'s sampling algorithm\nhas a running time of $\\mathcal{O}(N^3)$, which is often greater than the mean\nhitting time of a directed graph. Additionally, we build upon Colbourn's\nalgorithm and present a novel extension that can sample $K$ trees without\nreplacement in $\\mathcal{O}(K N^3 + K^2 N)$ time. To the best of our knowledge,\nno algorithm has been given for sampling spanning trees without replacement\nfrom a directed graph.", "published": "2021-09-14 08:33:12", "link": "http://arxiv.org/abs/2109.06521v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Different Strokes for Different Folks: Investigating Appropriate Further\n  Pre-training Approaches for Diverse Dialogue Tasks", "abstract": "Loading models pre-trained on the large-scale corpus in the general domain\nand fine-tuning them on specific downstream tasks is gradually becoming a\nparadigm in Natural Language Processing. Previous investigations prove that\nintroducing a further pre-training phase between pre-training and fine-tuning\nphases to adapt the model on the domain-specific unlabeled data can bring\npositive effects. However, most of these further pre-training works just keep\nrunning the conventional pre-training task, e.g., masked language model, which\ncan be regarded as the domain adaptation to bridge the data distribution gap.\nAfter observing diverse downstream tasks, we suggest that different tasks may\nalso need a further pre-training phase with appropriate training tasks to\nbridge the task formulation gap. To investigate this, we carry out a study for\nimproving multiple task-oriented dialogue downstream tasks through designing\nvarious tasks at the further pre-training phase. The experiment shows that\ndifferent downstream tasks prefer different further pre-training tasks, which\nhave intrinsic correlation and most further pre-training tasks significantly\nimprove certain target tasks rather than all. Our investigation indicates that\nit is of great importance and effectiveness to design appropriate further\npre-training tasks modeling specific information that benefit downstream tasks.\nBesides, we present multiple constructive empirical conclusions for enhancing\ntask-oriented dialogues.", "published": "2021-09-14 08:42:50", "link": "http://arxiv.org/abs/2109.06524v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Bill Similarity with Annotated and Augmented Corpora of Bills", "abstract": "Bill writing is a critical element of representative democracy. However, it\nis often overlooked that most legislative bills are derived, or even directly\ncopied, from other bills. Despite the significance of bill-to-bill linkages for\nunderstanding the legislative process, existing approaches fail to address\nsemantic similarities across bills, let alone reordering or paraphrasing which\nare prevalent in legal document writing. In this paper, we overcome these\nlimitations by proposing a 5-class classification task that closely reflects\nthe nature of the bill generation process. In doing so, we construct a\nhuman-labeled dataset of 4,721 bill-to-bill relationships at the\nsubsection-level and release this annotated dataset to the research community.\nTo augment the dataset, we generate synthetic data with varying degrees of\nsimilarity, mimicking the complex bill writing process. We use BERT variants\nand apply multi-stage training, sequentially fine-tuning our models with\nsynthetic and human-labeled datasets. We find that the predictive performance\nsignificantly improves when training with both human-labeled and synthetic\ndata. Finally, we apply our trained model to infer section- and bill-level\nsimilarities. Our analysis shows that the proposed methodology successfully\ncaptures the similarities across legal documents at various levels of\naggregation.", "published": "2021-09-14 08:50:06", "link": "http://arxiv.org/abs/2109.06527v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Gradient-based Adversarial Training for Text Classification by\n  Contrastive Learning and Auto-Encoder", "abstract": "Recent work has proposed several efficient approaches for generating\ngradient-based adversarial perturbations on embeddings and proved that the\nmodel's performance and robustness can be improved when they are trained with\nthese contaminated embeddings. While they paid little attention to how to help\nthe model to learn these adversarial samples more efficiently. In this work, we\nfocus on enhancing the model's ability to defend gradient-based adversarial\nattack during the model's training process and propose two novel adversarial\ntraining approaches: (1) CARL narrows the original sample and its adversarial\nsample in the representation space while enlarging their distance from\ndifferent labeled samples. (2) RAR forces the model to reconstruct the original\nsample from its adversarial representation. Experiments show that the proposed\ntwo approaches outperform strong baselines on various text classification\ndatasets. Analysis experiments find that when using our approaches, the\nsemantic representation of the input sentence won't be significantly affected\nby adversarial perturbations, and the model's performance drops less under\nadversarial attack. That is to say, our approaches can effectively improve the\nrobustness of the model. Besides, RAR can also be used to generate text-form\nadversarial samples.", "published": "2021-09-14 09:08:58", "link": "http://arxiv.org/abs/2109.06536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Challenging Instances are Worth Learning: Generating Valuable Negative\n  Samples for Response Selection Training", "abstract": "Retrieval-based chatbot selects the appropriate response from candidates\naccording to the context, which heavily depends on a response selection module.\nA response selection module is generally a scoring model to evaluate candidates\nand is usually trained on the annotated positive response and sampled negative\nresponses. Sampling negative responses lead to two risks: a). The sampled\nnegative instances, especially that from random sampling methods, are mostly\nirrelevant to the dialogue context and too easy to be fitted at the training\nstage while causing a weak model in the real scenario. b). The so-called\nnegative instances may be positive, which is known as the fake negative\nproblem. To address the above issue, we employ pre-trained language models,\nsuch as the DialoGPT to construct more challenging negative instances to\nenhance the model robustness. Specifically, we provide garbled context to the\npre-trained model to generate responses and filter the fake negative ones. In\nthis way, our negative instances are fluent, context-related, and more\nchallenging for the model to learn, while can not be positive. Extensive\nexperiments show that our method brings significant and stable improvements on\nthe dialogue response selection capacity.", "published": "2021-09-14 09:16:24", "link": "http://arxiv.org/abs/2109.06538v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MDAPT: Multilingual Domain Adaptive Pretraining in a Single Model", "abstract": "Domain adaptive pretraining, i.e. the continued unsupervised pretraining of a\nlanguage model on domain-specific text, improves the modelling of text for\ndownstream tasks within the domain. Numerous real-world applications are based\non domain-specific text, e.g. working with financial or biomedical documents,\nand these applications often need to support multiple languages. However,\nlarge-scale domain-specific multilingual pretraining data for such scenarios\ncan be difficult to obtain, due to regulations, legislation, or simply a lack\nof language- and domain-specific text. One solution is to train a single\nmultilingual model, taking advantage of the data available in as many languages\nas possible. In this work, we explore the benefits of domain adaptive\npretraining with a focus on adapting to multiple languages within a specific\ndomain. We propose different techniques to compose pretraining corpora that\nenable a language model to both become domain-specific and multilingual.\nEvaluation on nine domain-specific datasets-for biomedical named entity\nrecognition and financial sentence classification-covering seven different\nlanguages show that a single multilingual domain-specific model can outperform\nthe general multilingual model, and performs close to its monolingual\ncounterpart. This finding holds across two different pretraining methods,\nadapter-based pretraining and full model pretraining.", "published": "2021-09-14 11:50:26", "link": "http://arxiv.org/abs/2109.06605v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An MRC Framework for Semantic Role Labeling", "abstract": "Semantic Role Labeling (SRL) aims at recognizing the predicate-argument\nstructure of a sentence and can be decomposed into two subtasks: predicate\ndisambiguation and argument labeling. Prior work deals with these two tasks\nindependently, which ignores the semantic connection between the two tasks. In\nthis paper, we propose to use the machine reading comprehension (MRC) framework\nto bridge this gap. We formalize predicate disambiguation as multiple-choice\nmachine reading comprehension, where the descriptions of candidate senses of a\ngiven predicate are used as options to select the correct sense. The chosen\npredicate sense is then used to determine the semantic roles for that\npredicate, and these semantic roles are used to construct the query for another\nMRC model for argument labeling. In this way, we are able to leverage both the\npredicate semantics and the semantic role semantics for argument labeling. We\nalso propose to select a subset of all the possible semantic roles for\ncomputational efficiency. Experiments show that the proposed framework achieves\nstate-of-the-art or comparable results to previous work. Code is available at\n\\url{https://github.com/ShannonAI/MRC-SRL}.", "published": "2021-09-14 13:04:08", "link": "http://arxiv.org/abs/2109.06660v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Inference for Multilingual Neural Machine Translation", "abstract": "Multilingual NMT has become an attractive solution for MT deployment in\nproduction. But to match bilingual quality, it comes at the cost of larger and\nslower models. In this work, we consider several ways to make multilingual NMT\nfaster at inference without degrading its quality. We experiment with several\n\"light decoder\" architectures in two 20-language multi-parallel settings:\nsmall-scale on TED Talks and large-scale on ParaCrawl. Our experiments\ndemonstrate that combining a shallow decoder with vocabulary filtering leads to\nmore than twice faster inference with no loss in translation quality. We\nvalidate our findings with BLEU and chrF (on 380 language pairs), robustness\nevaluation and human evaluation.", "published": "2021-09-14 13:28:13", "link": "http://arxiv.org/abs/2109.06679v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A system for information extraction from scientific texts in Russian", "abstract": "In this paper, we present a system for information extraction from scientific\ntexts in the Russian language. The system performs several tasks in an\nend-to-end manner: term recognition, extraction of relations between terms, and\nterm linking with entities from the knowledge base. These tasks are extremely\nimportant for information retrieval, recommendation systems, and\nclassification. The advantage of the implemented methods is that the system\ndoes not require a large amount of labeled data, which saves time and effort\nfor data labeling and therefore can be applied in low- and mid-resource\nsettings. The source code is publicly available and can be used for different\nresearch purposes.", "published": "2021-09-14 14:08:37", "link": "http://arxiv.org/abs/2109.06703v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KFCNet: Knowledge Filtering and Contrastive Learning Network for\n  Generative Commonsense Reasoning", "abstract": "Pre-trained language models have led to substantial gains over a broad range\nof natural language processing (NLP) tasks, but have been shown to have\nlimitations for natural language generation tasks with high-quality\nrequirements on the output, such as commonsense generation and ad keyword\ngeneration. In this work, we present a novel Knowledge Filtering and\nContrastive learning Network (KFCNet) which references external knowledge and\nachieves better generation performance. Specifically, we propose a BERT-based\nfilter model to remove low-quality candidates, and apply contrastive learning\nseparately to each of the encoder and decoder, within a general\nencoder--decoder architecture. The encoder contrastive module helps to capture\nglobal target semantics during encoding, and the decoder contrastive module\nenhances the utility of retrieved prototypes while learning general features.\nExtensive experiments on the CommonGen benchmark show that our model\noutperforms the previous state of the art by a large margin: +6.6 points (42.5\nvs. 35.9) for BLEU-4, +3.7 points (33.3 vs. 29.6) for SPICE, and +1.3 points\n(18.3 vs. 17.0) for CIDEr. We further verify the effectiveness of the proposed\ncontrastive module on ad keyword generation, and show that our model has\npotential commercial value.", "published": "2021-09-14 14:10:37", "link": "http://arxiv.org/abs/2109.06704v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Information Seeking for Open-Domain Question Answering", "abstract": "Information seeking is an essential step for open-domain question answering\nto efficiently gather evidence from a large corpus. Recently, iterative\napproaches have been proven to be effective for complex questions, by\nrecursively retrieving new evidence at each step. However, almost all existing\niterative approaches use predefined strategies, either applying the same\nretrieval function multiple times or fixing the order of different retrieval\nfunctions, which cannot fulfill the diverse requirements of various questions.\nIn this paper, we propose a novel adaptive information-seeking strategy for\nopen-domain question answering, namely AISO. Specifically, the whole retrieval\nand answer process is modeled as a partially observed Markov decision process,\nwhere three types of retrieval operations (e.g., BM25, DPR, and hyperlink) and\none answer operation are defined as actions. According to the learned policy,\nAISO could adaptively select a proper retrieval action to seek the missing\nevidence at each step, based on the collected evidence and the reformulated\nquery, or directly output the answer when the evidence set is sufficient for\nthe question. Experiments on SQuAD Open and HotpotQA fullwiki, which serve as\nsingle-hop and multi-hop open-domain QA benchmarks, show that AISO outperforms\nall baseline methods with predefined strategies in terms of both retrieval and\nanswer evaluations.", "published": "2021-09-14 15:08:13", "link": "http://arxiv.org/abs/2109.06747v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Zero-shot Cross-lingual Transfer between Closely Related\n  Languages by injecting Character-level Noise", "abstract": "Cross-lingual transfer between a high-resource language and its dialects or\nclosely related language varieties should be facilitated by their similarity.\nHowever, current approaches that operate in the embedding space do not take\nsurface similarity into account. This work presents a simple yet effective\nstrategy to imrove cross-lingual transfer between closely related varieties. We\npropose to augment the data of the high-resource source language with\ncharacter-level noise to make the model more robust towards spelling\nvariations. Our strategy shows consistent improvements over several languages\nand tasks: Zero-shot transfer of POS tagging and topic identification between\nlanguage varieties from the Finnic, West and North Germanic, and Western\nRomance language branches. Our work provides evidence for the usefulness of\nsimple surface-level noise in improving transfer between language varieties.", "published": "2021-09-14 15:38:08", "link": "http://arxiv.org/abs/2109.06772v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Everything Is All It Takes: A Multipronged Strategy for Zero-Shot\n  Cross-Lingual Information Extraction", "abstract": "Zero-shot cross-lingual information extraction (IE) describes the\nconstruction of an IE model for some target language, given existing\nannotations exclusively in some other language, typically English. While the\nadvance of pretrained multilingual encoders suggests an easy optimism of \"train\non English, run on any language\", we find through a thorough exploration and\nextension of techniques that a combination of approaches, both new and old,\nleads to better performance than any one cross-lingual strategy in particular.\nWe explore techniques including data projection and self-training, and how\ndifferent pretrained encoders impact them. We use English-to-Arabic IE as our\ninitial example, demonstrating strong performance in this setting for event\nextraction, named entity recognition, part-of-speech tagging, and dependency\nparsing. We then apply data projection and self-training to three tasks across\neight target languages. Because no single set of techniques performs the best\nacross all tasks, we encourage practitioners to explore various configurations\nof the techniques described in this work when seeking to improve on zero-shot\ntraining.", "published": "2021-09-14 16:21:14", "link": "http://arxiv.org/abs/2109.06798v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Perils of Using Mechanical Turk to Evaluate Open-Ended Text\n  Generation", "abstract": "Recent text generation research has increasingly focused on open-ended\ndomains such as story and poetry generation. Because models built for such\ntasks are difficult to evaluate automatically, most researchers in the space\njustify their modeling choices by collecting crowdsourced human judgments of\ntext quality (e.g., Likert scores of coherence or grammaticality) from Amazon\nMechanical Turk (AMT). In this paper, we first conduct a survey of 45\nopen-ended text generation papers and find that the vast majority of them fail\nto report crucial details about their AMT tasks, hindering reproducibility. We\nthen run a series of story evaluation experiments with both AMT workers and\nEnglish teachers and discover that even with strict qualification filters, AMT\nworkers (unlike teachers) fail to distinguish between model-generated text and\nhuman-generated references. We show that AMT worker judgments improve when they\nare shown model-generated output alongside human-generated references, which\nenables the workers to better calibrate their ratings. Finally, interviews with\nthe English teachers provide deeper insights into the challenges of the\nevaluation process, particularly when rating model-generated text.", "published": "2021-09-14 17:20:30", "link": "http://arxiv.org/abs/2109.06835v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ePiC: Employing Proverbs in Context as a Benchmark for Abstract Language\n  Understanding", "abstract": "While large language models have shown exciting progress on several NLP\nbenchmarks, evaluating their ability for complex analogical reasoning remains\nunder-explored. Here, we introduce a high-quality crowdsourced dataset of\nnarratives for employing proverbs in context as a benchmark for abstract\nlanguage understanding. The dataset provides fine-grained annotation of aligned\nspans between proverbs and narratives, and contains minimal lexical overlaps\nbetween narratives and proverbs, ensuring that models need to go beyond\nsurface-level reasoning to succeed. We explore three tasks: (1) proverb\nrecommendation and alignment prediction, (2) narrative generation for a given\nproverb and topic, and (3) identifying narratives with similar motifs. Our\nexperiments show that neural language models struggle on these tasks compared\nto humans, and these tasks pose multiple learning challenges.", "published": "2021-09-14 17:21:12", "link": "http://arxiv.org/abs/2109.06838v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Summarize-then-Answer: Generating Concise Explanations for Multi-hop\n  Reading Comprehension", "abstract": "How can we generate concise explanations for multi-hop Reading Comprehension\n(RC)? The current strategies of identifying supporting sentences can be seen as\nan extractive question-focused summarization of the input text. However, these\nextractive explanations are not necessarily concise i.e. not minimally\nsufficient for answering a question. Instead, we advocate for an abstractive\napproach, where we propose to generate a question-focused, abstractive summary\nof input paragraphs and then feed it to an RC system. Given a limited amount of\nhuman-annotated abstractive explanations, we train the abstractive explainer in\na semi-supervised manner, where we start from the supervised model and then\ntrain it further through trial and error maximizing a conciseness-promoted\nreward function. Our experiments demonstrate that the proposed abstractive\nexplainer can generate more compact explanations than an extractive explainer\nwith limited supervision (only 2k instances) while maintaining sufficiency.", "published": "2021-09-14 17:44:34", "link": "http://arxiv.org/abs/2109.06853v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Legal Transformer Models May Not Always Help", "abstract": "Deep learning-based Natural Language Processing methods, especially\ntransformers, have achieved impressive performance in the last few years.\nApplying those state-of-the-art NLP methods to legal activities to automate or\nsimplify some simple work is of great value. This work investigates the value\nof domain adaptive pre-training and language adapters in legal NLP tasks. By\ncomparing the performance of language models with domain adaptive pre-training\non different tasks and different dataset splits, we show that domain adaptive\npre-training is only helpful with low-resource downstream tasks, thus far from\nbeing a panacea. We also benchmark the performance of adapters in a typical\nlegal NLP task and show that they can yield similar performance to full model\ntuning with much smaller training costs. As an additional result, we release\nLegalRoBERTa, a RoBERTa model further pre-trained on legal corpora.", "published": "2021-09-14 17:53:55", "link": "http://arxiv.org/abs/2109.06862v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatically Exposing Problems with Neural Dialog Models", "abstract": "Neural dialog models are known to suffer from problems such as generating\nunsafe and inconsistent responses. Even though these problems are crucial and\nprevalent, they are mostly manually identified by model designers through\ninteractions. Recently, some research instructs crowdworkers to goad the bots\ninto triggering such problems. However, humans leverage superficial clues such\nas hate speech, while leaving systematic problems undercover. In this paper, we\npropose two methods including reinforcement learning to automatically trigger a\ndialog model into generating problematic responses. We show the effect of our\nmethods in exposing safety and contradiction issues with state-of-the-art\ndialog models.", "published": "2021-09-14 20:00:51", "link": "http://arxiv.org/abs/2109.06950v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Searching for More Efficient Dynamic Programs", "abstract": "Computational models of human language often involve combinatorial problems.\nFor instance, a probabilistic parser may marginalize over exponentially many\ntrees to make predictions. Algorithms for such problems often employ dynamic\nprogramming and are not always unique. Finding one with optimal asymptotic\nruntime can be unintuitive, time-consuming, and error-prone. Our work aims to\nautomate this laborious process. Given an initial correct declarative program,\nwe search for a sequence of semantics-preserving transformations to improve its\nrunning time as much as possible. To this end, we describe a set of program\ntransformations, a simple metric for assessing the efficiency of a transformed\nprogram, and a heuristic search procedure to improve this metric. We show that\nin practice, automated search -- like the mental search performed by human\nprogrammers -- can find substantial improvements to the initial program.\nEmpirically, we show that many common speed-ups described in the NLP literature\ncould have been discovered automatically by our system.", "published": "2021-09-14 20:52:55", "link": "http://arxiv.org/abs/2109.06966v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NOPE: A Corpus of Naturally-Occurring Presuppositions in English", "abstract": "Understanding language requires grasping not only the overtly stated content,\nbut also making inferences about things that were left unsaid. These inferences\ninclude presuppositions, a phenomenon by which a listener learns about new\ninformation through reasoning about what a speaker takes as given.\nPresuppositions require complex understanding of the lexical and syntactic\nproperties that trigger them as well as the broader conversational context. In\nthis work, we introduce the Naturally-Occurring Presuppositions in English\n(NOPE) Corpus to investigate the context-sensitivity of 10 different types of\npresupposition triggers and to evaluate machine learning models' ability to\npredict human inferences. We find that most of the triggers we investigate\nexhibit moderate variability. We further find that transformer-based models\ndraw correct inferences in simple cases involving presuppositions, but they\nfail to capture the minority of exceptional cases in which human judgments\nreveal complex interactions between context and triggers.", "published": "2021-09-14 22:03:23", "link": "http://arxiv.org/abs/2109.06987v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Will this Question be Answered? Question Filtering via Answer Model\n  Distillation for Efficient Question Answering", "abstract": "In this paper we propose a novel approach towards improving the efficiency of\nQuestion Answering (QA) systems by filtering out questions that will not be\nanswered by them. This is based on an interesting new finding: the answer\nconfidence scores of state-of-the-art QA systems can be approximated well by\nmodels solely using the input question text. This enables preemptive filtering\nof questions that are not answered by the system due to their answer confidence\nscores being lower than the system threshold. Specifically, we learn\nTransformer-based question models by distilling Transformer-based answering\nmodels. Our experiments on three popular QA datasets and one industrial QA\nbenchmark demonstrate the ability of our question models to approximate the\nPrecision/Recall curves of the target QA system well. These question models,\nwhen used as filters, can effectively trade off lower computation cost of QA\nsystems for lower Recall, e.g., reducing computation by ~60%, while only losing\n~3-4% of Recall.", "published": "2021-09-14 23:07:49", "link": "http://arxiv.org/abs/2109.07009v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Written Justifications are Key to Aggregate Crowdsourced Forecasts", "abstract": "This paper demonstrates that aggregating crowdsourced forecasts benefits from\nmodeling the written justifications provided by forecasters. Our experiments\nshow that the majority and weighted vote baselines are competitive, and that\nthe written justifications are beneficial to call a question throughout its\nlife except in the last quarter. We also conduct an error analysis shedding\nlight into the characteristics that make a justification unreliable.", "published": "2021-09-14 23:28:26", "link": "http://arxiv.org/abs/2109.07017v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Frequency Effects on Syntactic Rule Learning in Transformers", "abstract": "Pre-trained language models perform well on a variety of linguistic tasks\nthat require symbolic reasoning, raising the question of whether such models\nimplicitly represent abstract symbols and rules. We investigate this question\nusing the case study of BERT's performance on English subject-verb agreement.\nUnlike prior work, we train multiple instances of BERT from scratch, allowing\nus to perform a series of controlled interventions at pre-training time. We\nshow that BERT often generalizes well to subject-verb pairs that never occurred\nin training, suggesting a degree of rule-governed behavior. We also find,\nhowever, that performance is heavily influenced by word frequency, with\nexperiments showing that both the absolute frequency of a verb form, as well as\nthe frequency relative to the alternate inflection, are causally implicated in\nthe predictions BERT makes at inference time. Closer analysis of these\nfrequency effects reveals that BERT's behavior is consistent with a system that\ncorrectly applies the SVA rule in general but struggles to overcome strong\ntraining priors and to estimate agreement features (singular vs. plural) on\ninfrequent lexical items.", "published": "2021-09-14 23:45:20", "link": "http://arxiv.org/abs/2109.07020v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Biomedical BERT Models for Vocabulary Alignment at Scale in\n  the UMLS Metathesaurus", "abstract": "The current UMLS (Unified Medical Language System) Metathesaurus construction\nprocess for integrating over 200 biomedical source vocabularies is expensive\nand error-prone as it relies on the lexical algorithms and human editors for\ndeciding if the two biomedical terms are synonymous. Recent advances in Natural\nLanguage Processing such as Transformer models like BERT and its biomedical\nvariants with contextualized word embeddings have achieved state-of-the-art\n(SOTA) performance on downstream tasks. We aim to validate if these approaches\nusing the BERT models can actually outperform the existing approaches for\npredicting synonymy in the UMLS Metathesaurus. In the existing Siamese Networks\nwith LSTM and BioWordVec embeddings, we replace the BioWordVec embeddings with\nthe biomedical BERT embeddings extracted from each BERT model using different\nways of extraction. In the Transformer architecture, we evaluate the use of the\ndifferent biomedical BERT models that have been pre-trained using different\ndatasets and tasks. Given the SOTA performance of these BERT models for other\ndownstream tasks, our experiments yield surprisingly interesting results: (1)\nin both model architectures, the approaches employing these biomedical\nBERT-based models do not outperform the existing approaches using Siamese\nNetwork with BioWordVec embeddings for the UMLS synonymy prediction task, (2)\nthe original BioBERT large model that has not been pre-trained with the UMLS\noutperforms the SapBERT models that have been pre-trained with the UMLS, and\n(3) using the Siamese Networks yields better performance for synonymy\nprediction when compared to using the biomedical BERT models.", "published": "2021-09-14 16:52:16", "link": "http://arxiv.org/abs/2109.13348v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hunspell for Sorani Kurdish Spell Checking and Morphological Analysis", "abstract": "Spell checking and morphological analysis are two fundamental tasks in text\nand natural language processing and are addressed in the early stages of the\ndevelopment of language technology. Despite the previous efforts, there is no\nprogress in open-source to create such tools for Sorani Kurdish, also known as\nCentral Kurdish, as a less-resourced language. In this paper, we present our\nefforts in annotating a lexicon with morphosyntactic tags and also, extracting\nmorphological rules of Sorani Kurdish to build a morphological analyzer, a\nstemmer and a spell-checking system using Hunspell. This implementation can be\nused for further developments in the field by researchers and also, be\nintegrated into text editors under a publicly available license.", "published": "2021-09-14 00:24:20", "link": "http://arxiv.org/abs/2109.06374v1", "categories": ["cs.CL", "cs.FL"], "primary_category": "cs.CL"}
{"title": "Compression, Transduction, and Creation: A Unified Framework for\n  Evaluating Natural Language Generation", "abstract": "Natural language generation (NLG) spans a broad range of tasks, each of which\nserves for specific objectives and desires different properties of generated\ntext. The complexity makes automatic evaluation of NLG particularly\nchallenging. Previous work has typically focused on a single task and developed\nindividual evaluation metrics based on specific intuitions. In this paper, we\npropose a unifying perspective that facilitates the design of metrics for a\nwide range of language generation tasks and quality aspects. Based on the\nnature of information change from input to output, we classify NLG tasks into\ncompression (e.g., summarization), transduction (e.g., text rewriting), and\ncreation (e.g., dialog). The information alignment, or overlap, between input,\ncontext, and output text plays a common central role in characterizing the\ngeneration. Using the uniform concept of information alignment, we develop a\nfamily of interpretable metrics for various NLG tasks and aspects, often\nwithout need of gold reference data. To operationalize the metrics, we train\nself-supervised models to approximate information alignment as a prediction\ntask. Experiments show the uniformly designed metrics achieve stronger or\ncomparable correlations with human judgement compared to state-of-the-art\nmetrics in each of diverse tasks, including text summarization, style transfer,\nand knowledge-grounded dialog. With information alignment as the intermediate\nrepresentation, we deliver a composable library for easy NLG evaluation and\nfuture metric design.", "published": "2021-09-14 01:00:42", "link": "http://arxiv.org/abs/2109.06379v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rationales for Sequential Predictions", "abstract": "Sequence models are a critical component of modern NLP systems, but their\npredictions are difficult to explain. We consider model explanations though\nrationales, subsets of context that can explain individual model predictions.\nWe find sequential rationales by solving a combinatorial optimization: the best\nrationale is the smallest subset of input tokens that would predict the same\noutput as the full sequence. Enumerating all subsets is intractable, so we\npropose an efficient greedy algorithm to approximate this objective. The\nalgorithm, which is called greedy rationalization, applies to any model. For\nthis approach to be effective, the model should form compatible conditional\ndistributions when making predictions on incomplete subsets of the context.\nThis condition can be enforced with a short fine-tuning step. We study greedy\nrationalization on language modeling and machine translation. Compared to\nexisting baselines, greedy rationalization is best at optimizing the\ncombinatorial objective and provides the most faithful rationales. On a new\ndataset of annotated sequential rationales, greedy rationales are most similar\nto human rationales.", "published": "2021-09-14 01:25:15", "link": "http://arxiv.org/abs/2109.06387v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adaptive Proposal Generation Network for Temporal Sentence Localization\n  in Videos", "abstract": "We address the problem of temporal sentence localization in videos (TSLV).\nTraditional methods follow a top-down framework which localizes the target\nsegment with pre-defined segment proposals. Although they have achieved decent\nperformance, the proposals are handcrafted and redundant. Recently, bottom-up\nframework attracts increasing attention due to its superior efficiency. It\ndirectly predicts the probabilities for each frame as a boundary. However, the\nperformance of bottom-up model is inferior to the top-down counterpart as it\nfails to exploit the segment-level interaction. In this paper, we propose an\nAdaptive Proposal Generation Network (APGN) to maintain the segment-level\ninteraction while speeding up the efficiency. Specifically, we first perform a\nforeground-background classification upon the video and regress on the\nforeground frames to adaptively generate proposals. In this way, the\nhandcrafted proposal design is discarded and the redundant proposals are\ndecreased. Then, a proposal consolidation module is further developed to\nenhance the semantic of the generated proposals. Finally, we locate the target\nmoments with these generated proposals following the top-down framework.\nExtensive experiments on three challenging benchmarks show that our proposed\nAPGN significantly outperforms previous state-of-the-art methods.", "published": "2021-09-14 02:02:36", "link": "http://arxiv.org/abs/2109.06398v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Progressively Guide to Attend: An Iterative Alignment Framework for\n  Temporal Sentence Grounding", "abstract": "A key solution to temporal sentence grounding (TSG) exists in how to learn\neffective alignment between vision and language features extracted from an\nuntrimmed video and a sentence description. Existing methods mainly leverage\nvanilla soft attention to perform the alignment in a single-step process.\nHowever, such single-step attention is insufficient in practice, since\ncomplicated relations between inter- and intra-modality are usually obtained\nthrough multi-step reasoning. In this paper, we propose an Iterative Alignment\nNetwork (IA-Net) for TSG task, which iteratively interacts inter- and\nintra-modal features within multiple steps for more accurate grounding.\nSpecifically, during the iterative reasoning process, we pad multi-modal\nfeatures with learnable parameters to alleviate the nowhere-to-attend problem\nof non-matched frame-word pairs, and enhance the basic co-attention mechanism\nin a parallel manner. To further calibrate the misaligned attention caused by\neach reasoning step, we also devise a calibration module following each\nattention module to refine the alignment knowledge. With such iterative\nalignment scheme, our IA-Net can robustly capture the fine-grained relations\nbetween vision and language domains step-by-step for progressively reasoning\nthe temporal boundaries. Extensive experiments conducted on three challenging\nbenchmarks demonstrate that our proposed model performs better than the\nstate-of-the-arts.", "published": "2021-09-14 02:08:23", "link": "http://arxiv.org/abs/2109.06400v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Exploring Personality and Online Social Engagement: An Investigation of\n  MBTI Users on Twitter", "abstract": "Text-based personality prediction by computational models is an emerging\nfield with the potential to significantly improve on key weaknesses of\nsurvey-based personality assessment. We investigate 3848 profiles from Twitter\nwith self-labeled Myers-Briggs personality traits (MBTI) - a framework closely\nrelated to the Five Factor Model of personality - to better understand how\ntext-based digital traces from social engagement online can be used to predict\nuser personality traits. We leverage BERT, a state-of-the-art NLP architecture\nbased on deep learning, to analyze various sources of text that hold most\npredictive power for our task. We find that biographies, statuses, and liked\ntweets contain significant predictive power for all dimensions of the MBTI\nsystem. We discuss our findings and their implications for the validity of the\nMBTI and the lexical hypothesis, a foundational theory underlying the Five\nFactor Model that links language use and behavior. Our results hold optimistic\nimplications for personality psychologists, computational linguists, and other\nsocial scientists aiming to predict personality from observational text data\nand explore the links between language and core behavioral traits.", "published": "2021-09-14 02:26:30", "link": "http://arxiv.org/abs/2109.06402v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Gradient Imitation Reinforcement Learning for Low Resource Relation\n  Extraction", "abstract": "Low-resource Relation Extraction (LRE) aims to extract relation facts from\nlimited labeled corpora when human annotation is scarce. Existing works either\nutilize self-training scheme to generate pseudo labels that will cause the\ngradual drift problem, or leverage meta-learning scheme which does not solicit\nfeedback explicitly. To alleviate selection bias due to the lack of feedback\nloops in existing LRE learning paradigms, we developed a Gradient Imitation\nReinforcement Learning method to encourage pseudo label data to imitate the\ngradient descent direction on labeled data and bootstrap its optimization\ncapability through trial and error. We also propose a framework called GradLRE,\nwhich handles two major scenarios in low-resource relation extraction. Besides\nthe scenario where unlabeled data is sufficient, GradLRE handles the situation\nwhere no unlabeled data is available, by exploiting a contextualized\naugmentation method to generate data. Experimental results on two public\ndatasets demonstrate the effectiveness of GradLRE on low resource relation\nextraction when comparing with baselines.", "published": "2021-09-14 03:51:15", "link": "http://arxiv.org/abs/2109.06415v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "YES SIR!Optimizing Semantic Space of Negatives with Self-Involvement\n  Ranker", "abstract": "Pre-trained model such as BERT has been proved to be an effective tool for\ndealing with Information Retrieval (IR) problems. Due to its inspiring\nperformance, it has been widely used to tackle with real-world IR problems such\nas document ranking. Recently, researchers have found that selecting \"hard\"\nrather than \"random\" negative samples would be beneficial for fine-tuning\npre-trained models on ranking tasks. However, it remains elusive how to\nleverage hard negative samples in a principled way. To address the\naforementioned issues, we propose a fine-tuning strategy for document ranking,\nnamely Self-Involvement Ranker (SIR), to dynamically select hard negative\nsamples to construct high-quality semantic space for training a high-quality\nranking model. Specifically, SIR consists of sequential compressors implemented\nwith pre-trained models. Front compressor selects hard negative samples for\nrear compressor. Moreover, SIR leverages supervisory signal to adaptively\nadjust semantic space of negative samples. Finally, supervisory signal in rear\ncompressor is computed based on condition probability and thus can control\nsample dynamic and further enhance the model performance. SIR is a lightweight\nand general framework for pre-trained models, which simplifies the ranking\nprocess in industry practice. We test our proposed solution on MS MARCO with\ndocument ranking setting, and the results show that SIR can significantly\nimprove the ranking performance of various pre-trained models. Moreover, our\nmethod became the new SOTA model anonymously on MS MARCO Document ranking\nleaderboard in May 2021.", "published": "2021-09-14 04:56:34", "link": "http://arxiv.org/abs/2109.06436v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Logic-level Evidence Retrieval and Graph-based Verification Network for\n  Table-based Fact Verification", "abstract": "Table-based fact verification task aims to verify whether the given statement\nis supported by the given semi-structured table. Symbolic reasoning with\nlogical operations plays a crucial role in this task. Existing methods leverage\nprograms that contain rich logical information to enhance the verification\nprocess. However, due to the lack of fully supervised signals in the program\ngeneration process, spurious programs can be derived and employed, which leads\nto the inability of the model to catch helpful logical operations. To address\nthe aforementioned problems, in this work, we formulate the table-based fact\nverification task as an evidence retrieval and reasoning framework, proposing\nthe Logic-level Evidence Retrieval and Graph-based Verification network\n(LERGV). Specifically, we first retrieve logic-level program-like evidence from\nthe given table and statement as supplementary evidence for the table. After\nthat, we construct a logic-level graph to capture the logical relations between\nentities and functions in the retrieved evidence, and design a graph-based\nverification network to perform logic-level graph-based reasoning based on the\nconstructed graph to classify the final entailment relation. Experimental\nresults on the large-scale benchmark TABFACT show the effectiveness of the\nproposed approach.", "published": "2021-09-14 07:25:36", "link": "http://arxiv.org/abs/2109.06480v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "AligNART: Non-autoregressive Neural Machine Translation by Jointly\n  Learning to Estimate Alignment and Translate", "abstract": "Non-autoregressive neural machine translation (NART) models suffer from the\nmulti-modality problem which causes translation inconsistency such as token\nrepetition. Most recent approaches have attempted to solve this problem by\nimplicitly modeling dependencies between outputs. In this paper, we introduce\nAligNART, which leverages full alignment information to explicitly reduce the\nmodality of the target distribution. AligNART divides the machine translation\ntask into $(i)$ alignment estimation and $(ii)$ translation with aligned\ndecoder inputs, guiding the decoder to focus on simplified one-to-one\ntranslation. To alleviate the alignment estimation problem, we further propose\na novel alignment decomposition method. Our experiments show that AligNART\noutperforms previous non-iterative NART models that focus on explicit modality\nreduction on WMT14 En$\\leftrightarrow$De and WMT16 Ro$\\rightarrow$En.\nFurthermore, AligNART achieves BLEU scores comparable to those of the\nstate-of-the-art connectionist temporal classification based models on WMT14\nEn$\\leftrightarrow$De. We also observe that AligNART effectively addresses the\ntoken repetition problem even without sequence-level knowledge distillation.", "published": "2021-09-14 07:26:33", "link": "http://arxiv.org/abs/2109.06481v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "conSultantBERT: Fine-tuned Siamese Sentence-BERT for Matching Jobs and\n  Job Seekers", "abstract": "In this paper we focus on constructing useful embeddings of textual\ninformation in vacancies and resumes, which we aim to incorporate as features\ninto job to job seeker matching models alongside other features. We explain our\ntask where noisy data from parsed resumes, heterogeneous nature of the\ndifferent sources of data, and crosslinguality and multilinguality present\ndomain-specific challenges.\n  We address these challenges by fine-tuning a Siamese Sentence-BERT (SBERT)\nmodel, which we call conSultantBERT, using a large-scale, real-world, and high\nquality dataset of over 270,000 resume-vacancy pairs labeled by our staffing\nconsultants. We show how our fine-tuned model significantly outperforms\nunsupervised and supervised baselines that rely on TF-IDF-weighted feature\nvectors and BERT embeddings. In addition, we find our model successfully\nmatches cross-lingual and multilingual textual content.", "published": "2021-09-14 07:57:05", "link": "http://arxiv.org/abs/2109.06501v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Netmarble AI Center's WMT21 Automatic Post-Editing Shared Task\n  Submission", "abstract": "This paper describes Netmarble's submission to WMT21 Automatic Post-Editing\n(APE) Shared Task for the English-German language pair. First, we propose a\nCurriculum Training Strategy in training stages. Facebook Fair's WMT19 news\ntranslation model was chosen to engage the large and powerful pre-trained\nneural networks. Then, we post-train the translation model with different\nlevels of data at each training stages. As the training stages go on, we make\nthe system learn to solve multiple tasks by adding extra information at\ndifferent training stages gradually. We also show a way to utilize the\nadditional data in large volume for APE tasks. For further improvement, we\napply Multi-Task Learning Strategy with the Dynamic Weight Average during the\nfine-tuning stage. To fine-tune the APE corpus with limited data, we add some\nrelated subtasks to learn a unified representation. Finally, for better\nperformance, we leverage external translations as augmented machine translation\n(MT) during the post-training and fine-tuning. As experimental results show,\nour APE system significantly improves the translations of provided MT results\nby -2.848 and +3.74 on the development dataset in terms of TER and BLEU,\nrespectively. It also demonstrates its effectiveness on the test dataset with\nhigher quality than the development dataset.", "published": "2021-09-14 08:21:18", "link": "http://arxiv.org/abs/2109.06515v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Just What do You Think You're Doing, Dave?' A Checklist for Responsible\n  Data Use in NLP", "abstract": "A key part of the NLP ethics movement is responsible use of data, but exactly\nwhat that means or how it can be best achieved remain unclear. This position\npaper discusses the core legal and ethical principles for collection and\nsharing of textual data, and the tensions between them. We propose a potential\nchecklist for responsible data (re-)use that could both standardise the peer\nreview of conference submissions, as well as enable a more in-depth view of\npublished research across the community. Our proposal aims to contribute to the\ndevelopment of a consistent standard for data (re-)use, embraced across NLP\nconferences.", "published": "2021-09-14 11:36:42", "link": "http://arxiv.org/abs/2109.06598v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Expert Knowledge-Guided Length-Variant Hierarchical Label Generation for\n  Proposal Classification", "abstract": "To advance the development of science and technology, research proposals are\nsubmitted to open-court competitive programs developed by government agencies\n(e.g., NSF). Proposal classification is one of the most important tasks to\nachieve effective and fair review assignments. Proposal classification aims to\nclassify a proposal into a length-variant sequence of labels. In this paper, we\nformulate the proposal classification problem into a hierarchical multi-label\nclassification task. Although there are certain prior studies, proposal\nclassification exhibit unique features: 1) the classification result of a\nproposal is in a hierarchical discipline structure with different levels of\ngranularity; 2) proposals contain multiple types of documents; 3) domain\nexperts can empirically provide partial labels that can be leveraged to improve\ntask performances. In this paper, we focus on developing a new deep proposal\nclassification framework to jointly model the three features. In particular, to\nsequentially generate labels, we leverage previously-generated labels to\npredict the label of next level; to integrate partial labels from experts, we\nuse the embedding of these empirical partial labels to initialize the state of\nneural networks. Our model can automatically identify the best length of label\nsequence to stop next label prediction. Finally, we present extensive results\nto demonstrate that our method can jointly model partial labels, textual\ninformation, and semantic dependencies in label sequences, and, thus, achieve\nadvanced performances.", "published": "2021-09-14 13:09:28", "link": "http://arxiv.org/abs/2109.06661v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Novel Global Feature-Oriented Relational Triple Extraction Model based\n  on Table Filling", "abstract": "Table filling based relational triple extraction methods are attracting\ngrowing research interests due to their promising performance and their\nabilities on extracting triples from complex sentences. However, this kind of\nmethods are far from their full potential because most of them only focus on\nusing local features but ignore the global associations of relations and of\ntoken pairs, which increases the possibility of overlooking some important\ninformation during triple extraction. To overcome this deficiency, we propose a\nglobal feature-oriented triple extraction model that makes full use of the\nmentioned two kinds of global associations. Specifically, we first generate a\ntable feature for each relation. Then two kinds of global associations are\nmined from the generated table features. Next, the mined global associations\nare integrated into the table feature of each relation. This\n\"generate-mine-integrate\" process is performed multiple times so that the table\nfeature of each relation is refined step by step. Finally, each relation's\ntable is filled based on its refined table feature, and all triples linked to\nthis relation are extracted based on its filled table. We evaluate the proposed\nmodel on three benchmark datasets. Experimental results show our model is\neffective and it achieves state-of-the-art results on all of these datasets.\nThe source code of our work is available at: https://github.com/neukg/GRTE.", "published": "2021-09-14 14:13:42", "link": "http://arxiv.org/abs/2109.06705v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Controllable Dialogue Generation with Disentangled Multi-grained Style\n  Specification and Attribute Consistency Reward", "abstract": "Controllable text generation is an appealing but challenging task, which\nallows users to specify particular attributes of the generated outputs. In this\npaper, we propose a controllable dialogue generation model to steer response\ngeneration under multi-attribute constraints. Specifically, we define and\ncategorize the commonly used control attributes into global and local ones,\nwhich possess different granularities of effects on response generation. Then,\nwe significantly extend the conventional seq2seq framework by introducing a\nnovel two-stage decoder, which first uses a multi-grained style specification\nlayer to impose the stylistic constraints and determine word-level control\nstates of responses based on the attributes, and then employs a response\ngeneration layer to generate final responses maintaining both semantic\nrelevancy to the contexts and fidelity to the attributes. Furthermore, we train\nour model with an attribute consistency reward to promote response control with\nexplicit supervision signals. Extensive experiments and in-depth analyses on\ntwo datasets indicate that our model can significantly outperform competitive\nbaselines in terms of response quality, content diversity and controllability.", "published": "2021-09-14 14:29:38", "link": "http://arxiv.org/abs/2109.06717v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sparse Fuzzy Attention for Structured Sentiment Analysis", "abstract": "Attention scorers have achieved success in parsing tasks like semantic and\nsyntactic dependency parsing. However, in tasks modeled into parsing, like\nstructured sentiment analysis, \"dependency edges\" are very sparse which hinders\nparser performance. Thus we propose a sparse and fuzzy attention scorer with\npooling layers which improves parser performance and sets the new\nstate-of-the-art on structured sentiment analysis. We further explore the\nparsing modeling on structured sentiment analysis with second-order parsing and\nintroduce a novel sparse second-order edge building procedure that leads to\nsignificant improvement in parsing performance.", "published": "2021-09-14 14:37:56", "link": "http://arxiv.org/abs/2109.06719v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Temporal Variational Model for Story Generation", "abstract": "Recent language models can generate interesting and grammatically correct\ntext in story generation but often lack plot development and long-term\ncoherence. This paper experiments with a latent vector planning approach based\non a TD-VAE (Temporal Difference Variational Autoencoder), using the model for\nconditioning and reranking for text generation. The results demonstrate strong\nperformance in automatic cloze and swapping evaluations. The human judgments\nshow stories generated with TD-VAE reranking improve on a GPT-2 medium baseline\nand show comparable performance to a hierarchical LSTM reranking model.\nConditioning on the latent vectors proves disappointing and deteriorates\nperformance in human evaluation because it reduces the diversity of generation,\nand the models don't learn to progress the narrative. This highlights an\nimportant difference between technical task performance (e.g. cloze) and\ngenerating interesting stories.", "published": "2021-09-14 16:36:12", "link": "http://arxiv.org/abs/2109.06807v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LM-Critic: Language Models for Unsupervised Grammatical Error Correction", "abstract": "Training a model for grammatical error correction (GEC) requires a set of\nlabeled ungrammatical / grammatical sentence pairs, but manually annotating\nsuch pairs can be expensive. Recently, the Break-It-Fix-It (BIFI) framework has\ndemonstrated strong results on learning to repair a broken program without any\nlabeled examples, but this relies on a perfect critic (e.g., a compiler) that\nreturns whether an example is valid or not, which does not exist for the GEC\ntask. In this work, we show how to leverage a pretrained language model (LM) in\ndefining an LM-Critic, which judges a sentence to be grammatical if the LM\nassigns it a higher probability than its local perturbations. We apply this\nLM-Critic and BIFI along with a large set of unlabeled sentences to bootstrap\nrealistic ungrammatical / grammatical pairs for training a corrector. We\nevaluate our approach on GEC datasets across multiple domains (CoNLL-2014,\nBEA-2019, GMEG-wiki and GMEG-yahoo) and show that it outperforms existing\nmethods in both the unsupervised setting (+7.7 F0.5) and the supervised setting\n(+0.5 F0.5).", "published": "2021-09-14 17:06:43", "link": "http://arxiv.org/abs/2109.06822v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Types of Out-of-Distribution Texts and How to Detect Them", "abstract": "Despite agreement on the importance of detecting out-of-distribution (OOD)\nexamples, there is little consensus on the formal definition of OOD examples\nand how to best detect them. We categorize these examples by whether they\nexhibit a background shift or a semantic shift, and find that the two major\napproaches to OOD detection, model calibration and density estimation (language\nmodeling for text), have distinct behavior on these types of OOD data. Across\n14 pairs of in-distribution and OOD English natural language understanding\ndatasets, we find that density estimation methods consistently beat calibration\nmethods in background shift settings, while performing worse in semantic shift\nsettings. In addition, we find that both methods generally fail to detect\nexamples from challenge data, highlighting a weak spot for current methods.\nSince no single method works well across all settings, our results call for an\nexplicit definition of OOD examples when evaluating different detection\nmethods.", "published": "2021-09-14 17:12:38", "link": "http://arxiv.org/abs/2109.06827v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BenchIE: A Framework for Multi-Faceted Fact-Based Open Information\n  Extraction Evaluation", "abstract": "Intrinsic evaluations of OIE systems are carried out either manually -- with\nhuman evaluators judging the correctness of extractions -- or automatically, on\nstandardized benchmarks. The latter, while much more cost-effective, is less\nreliable, primarily because of the incompleteness of the existing OIE\nbenchmarks: the ground truth extractions do not include all acceptable variants\nof the same fact, leading to unreliable assessment of the models' performance.\nMoreover, the existing OIE benchmarks are available for English only. In this\nwork, we introduce BenchIE: a benchmark and evaluation framework for\ncomprehensive evaluation of OIE systems for English, Chinese, and German. In\ncontrast to existing OIE benchmarks, BenchIE is fact-based, i.e., it takes into\naccount informational equivalence of extractions: our gold standard consists of\nfact synsets, clusters in which we exhaustively list all acceptable surface\nforms of the same fact. Moreover, having in mind common downstream applications\nfor OIE, we make BenchIE multi-faceted; i.e., we create benchmark variants that\nfocus on different facets of OIE evaluation, e.g., compactness or minimality of\nextractions. We benchmark several state-of-the-art OIE systems using BenchIE\nand demonstrate that these systems are significantly less effective than\nindicated by existing OIE benchmarks. We make BenchIE (data and evaluation\ncode) publicly available on https://github.com/gkiril/benchie.", "published": "2021-09-14 17:43:16", "link": "http://arxiv.org/abs/2109.06850v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Decision-Focused Summarization", "abstract": "Relevance in summarization is typically defined based on textual information\nalone, without incorporating insights about a particular decision. As a result,\nto support risk analysis of pancreatic cancer, summaries of medical notes may\ninclude irrelevant information such as a knee injury. We propose a novel\nproblem, decision-focused summarization, where the goal is to summarize\nrelevant information for a decision. We leverage a predictive model that makes\nthe decision based on the full text to provide valuable insights on how a\ndecision can be inferred from text. To build a summary, we then select\nrepresentative sentences that lead to similar model decisions as using the full\ntext while accounting for textual non-redundancy. To evaluate our method\n(DecSum), we build a testbed where the task is to summarize the first ten\nreviews of a restaurant in support of predicting its future rating on Yelp.\nDecSum substantially outperforms text-only summarization methods and\nmodel-based explanation methods in decision faithfulness and\nrepresentativeness. We further demonstrate that DecSum is the only method that\nenables humans to outperform random chance in predicting which restaurant will\nbe better rated in the future.", "published": "2021-09-14 18:00:14", "link": "http://arxiv.org/abs/2109.06896v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Language-specificity of Multilingual BERT and the Impact of\n  Fine-tuning", "abstract": "Recent work has shown evidence that the knowledge acquired by multilingual\nBERT (mBERT) has two components: a language-specific and a language-neutral\none. This paper analyses the relationship between them, in the context of\nfine-tuning on two tasks -- POS tagging and natural language inference -- which\nrequire the model to bring to bear different degrees of language-specific\nknowledge. Visualisations reveal that mBERT loses the ability to cluster\nrepresentations by language after fine-tuning, a result that is supported by\nevidence from language identification experiments. However, further experiments\non 'unlearning' language-specific representations using gradient reversal and\niterative adversarial learning are shown not to add further improvement to the\nlanguage-independent component over and above the effect of fine-tuning. The\nresults presented here suggest that the process of fine-tuning causes a\nreorganisation of the model's limited representational capacity, enhancing\nlanguage-independent representations at the expense of language-specific ones.", "published": "2021-09-14 19:28:31", "link": "http://arxiv.org/abs/2109.06935v2", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "The Stem Cell Hypothesis: Dilemma behind Multi-Task Learning with\n  Transformer Encoders", "abstract": "Multi-task learning with transformer encoders (MTL) has emerged as a powerful\ntechnique to improve performance on closely-related tasks for both accuracy and\nefficiency while a question still remains whether or not it would perform as\nwell on tasks that are distinct in nature. We first present MTL results on five\nNLP tasks, POS, NER, DEP, CON, and SRL, and depict its deficiency over\nsingle-task learning. We then conduct an extensive pruning analysis to show\nthat a certain set of attention heads get claimed by most tasks during MTL, who\ninterfere with one another to fine-tune those heads for their own objectives.\nBased on this finding, we propose the Stem Cell Hypothesis to reveal the\nexistence of attention heads naturally talented for many tasks that cannot be\njointly trained to create adequate embeddings for all of those tasks. Finally,\nwe design novel parameter-free probes to justify our hypothesis and demonstrate\nhow attention heads are transformed across the five tasks during MTL through\nlabel analysis.", "published": "2021-09-14 19:32:11", "link": "http://arxiv.org/abs/2109.06939v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Three Step Training Approach with Data Augmentation for Morphological\n  Inflection", "abstract": "We present the BME submission for the SIGMORPHON 2021 Task 0 Part 1,\nGeneralization Across Typologically Diverse Languages shared task. We use an\nLSTM encoder-decoder model with three step training that is first trained on\nall languages, then fine-tuned on each language families and finally finetuned\non individual languages. We use a different type of data augmentation technique\nin the first two steps. Our system outperformed the only other submission.\nAlthough it remains worse than the Transformer baseline released by the\norganizers, our model is simpler and our data augmentation techniques are\neasily applicable to new languages. We perform ablation studies and show that\nthe augmentation techniques and the three training steps often help but\nsometimes have a negative effect.", "published": "2021-09-14 23:03:00", "link": "http://arxiv.org/abs/2109.07006v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2; D.0"], "primary_category": "cs.CL"}
{"title": "Deep learning-based NLP Data Pipeline for EHR Scanned Document\n  Information Extraction", "abstract": "Scanned documents in electronic health records (EHR) have been a challenge\nfor decades, and are expected to stay in the foreseeable future. Current\napproaches for processing often include image preprocessing, optical character\nrecognition (OCR), and text mining. However, there is limited work that\nevaluates the choice of image preprocessing methods, the selection of NLP\nmodels, and the role of document layout. The impact of each element remains\nunknown. We evaluated this method on a use case of two key indicators for sleep\napnea, Apnea hypopnea index (AHI) and oxygen saturation (SaO2) values, from\nscanned sleep study reports. Our data that included 955 manually annotated\nreports was secondarily utilized from a previous study in the University of\nTexas Medical Branch. We performed image preprocessing: gray-scaling followed\nby 1 iteration of dilating and erode, and 20% contrast increasing. The OCR was\nimplemented with the Tesseract OCR engine. A total of seven Bag-of-Words models\n(Logistic Regression, Ridge Regression, Lasso Regression, Support Vector\nMachine, k-Nearest Neighbor, Na\\\"ive Bayes, and Random Forest) and three deep\nlearning-based models (BiLSTM, BERT, and Clinical BERT) were evaluated. We also\nevaluated the combinations of image preprocessing methods (gray-scaling, dilate\n& erode, increased contrast by 20%, increased contrast by 60%), and two deep\nlearning architectures (with and without structured input that provides\ndocument layout information). Our proposed method using Clinical BERT reached\nan AUROC of 0.9743 and document accuracy of 94.76% for AHI, and an AUROC of\n0.9523, and document accuracy of 91.61% for SaO2. We demonstrated the proper\nuse of image preprocessing and document layout could be beneficial to scanned\ndocument processing.", "published": "2021-09-14 03:56:56", "link": "http://arxiv.org/abs/2110.11864v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Talking Space: inference from spatial linguistic meanings", "abstract": "This paper concerns the intersection of natural language and the physical\nspace around us in which we live, that we observe and/or imagine things within.\nMany important features of language have spatial connotations, for example,\nmany prepositions (like in, next to, after, on, etc.) are fundamentally\nspatial. Space is also a key factor of the meanings of many\nwords/phrases/sentences/text, and space is a, if not the key, context for\nreferencing (e.g. pointing) and embodiment.\n  We propose a mechanism for how space and linguistic structure can be made to\ninteract in a matching compositional fashion. Examples include Cartesian space,\nsubway stations, chesspieces on a chess-board, and Penrose's staircase. The\nstarting point for our construction is the DisCoCat model of compositional\nnatural language meaning, which we relax to accommodate physical space. We\naddress the issue of having multiple agents/objects in a space, including the\ncase that each agent has different capabilities with respect to that space,\ne.g., the specific moves each chesspiece can make, or the different velocities\none may be able to reach.\n  Once our model is in place, we show how inferences drawing from the structure\nof physical space can be made. We also how how linguistic model of space can\ninteract with other such models related to our senses and/or embodiment, such\nas the conceptual spaces of colour, taste and smell, resulting in a rich\ncompositional model of meaning that is close to human experience and embodiment\nin the world.", "published": "2021-09-14 09:53:26", "link": "http://arxiv.org/abs/2109.06554v2", "categories": ["cs.CL", "cs.AI", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Non-Parametric Unsupervised Domain Adaptation for Neural Machine\n  Translation", "abstract": "Recently, $k$NN-MT has shown the promising capability of directly\nincorporating the pre-trained neural machine translation (NMT) model with\ndomain-specific token-level $k$-nearest-neighbor ($k$NN) retrieval to achieve\ndomain adaptation without retraining. Despite being conceptually attractive, it\nheavily relies on high-quality in-domain parallel corpora, limiting its\ncapability on unsupervised domain adaptation, where in-domain parallel corpora\nare scarce or nonexistent. In this paper, we propose a novel framework that\ndirectly uses in-domain monolingual sentences in the target language to\nconstruct an effective datastore for $k$-nearest-neighbor retrieval. To this\nend, we first introduce an autoencoder task based on the target language, and\nthen insert lightweight adapters into the original NMT model to map the\ntoken-level representation of this task to the ideal representation of\ntranslation task. Experiments on multi-domain datasets demonstrate that our\nproposed approach significantly improves the translation accuracy with\ntarget-side monolingual data, while achieving comparable performance with\nback-translation.", "published": "2021-09-14 11:50:01", "link": "http://arxiv.org/abs/2109.06604v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Non-autoregressive Transformer with Unified Bidirectional Decoder for\n  Automatic Speech Recognition", "abstract": "Non-autoregressive (NAR) transformer models have been studied intensively in\nautomatic speech recognition (ASR), and a substantial part of NAR transformer\nmodels is to use the casual mask to limit token dependencies. However, the\ncasual mask is designed for the left-to-right decoding process of the\nnon-parallel autoregressive (AR) transformer, which is inappropriate for the\nparallel NAR transformer since it ignores the right-to-left contexts. Some\nmodels are proposed to utilize right-to-left contexts with an extra decoder,\nbut these methods increase the model complexity. To tackle the above problems,\nwe propose a new non-autoregressive transformer with a unified bidirectional\ndecoder (NAT-UBD), which can simultaneously utilize left-to-right and\nright-to-left contexts. However, direct use of bidirectional contexts will\ncause information leakage, which means the decoder output can be affected by\nthe character information from the input of the same position. To avoid\ninformation leakage, we propose a novel attention mask and modify vanilla\nqueries, keys, and values matrices for NAT-UBD. Experimental results verify\nthat NAT-UBD can achieve character error rates (CERs) of 5.0%/5.5% on the\nAishell1 dev/test sets, outperforming all previous NAR transformer models.\nMoreover, NAT-UBD can run 49.8x faster than the AR transformer baseline when\ndecoding in a single step.", "published": "2021-09-14 13:39:39", "link": "http://arxiv.org/abs/2109.06684v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Semantic Answer Type Prediction using BERT: IAI at the ISWC SMART Task\n  2020", "abstract": "This paper summarizes our participation in the SMART Task of the ISWC 2020\nChallenge. A particular question we are interested in answering is how well\nneural methods, and specifically transformer models, such as BERT, perform on\nthe answer type prediction task compared to traditional approaches. Our main\nfinding is that coarse-grained answer types can be identified effectively with\nstandard text classification methods, with over 95% accuracy, and BERT can\nbring only marginal improvements. For fine-grained type detection, on the other\nhand, BERT clearly outperforms previous retrieval-based approaches.", "published": "2021-09-14 14:27:49", "link": "http://arxiv.org/abs/2109.06714v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "What are the attackers doing now? Automating cyber threat intelligence\n  extraction from text on pace with the changing threat landscape: A survey", "abstract": "Cybersecurity researchers have contributed to the automated extraction of CTI\nfrom textual sources, such as threat reports and online articles, where\ncyberattack strategies, procedures, and tools are described. The goal of this\narticle is to aid cybersecurity researchers understand the current techniques\nused for cyberthreat intelligence extraction from text through a survey of\nrelevant studies in the literature. We systematically collect \"CTI extraction\nfrom text\"-related studies from the literature and categorize the CTI\nextraction purposes. We propose a CTI extraction pipeline abstracted from these\nstudies. We identify the data sources, techniques, and CTI sharing formats\nutilized in the context of the proposed pipeline. Our work finds ten types of\nextraction purposes, such as extraction indicators of compromise extraction,\nTTPs (tactics, techniques, procedures of attack), and cybersecurity keywords.\nWe also identify seven types of textual sources for CTI extraction, and textual\ndata obtained from hacker forums, threat reports, social media posts, and\nonline news articles have been used by almost 90% of the studies. Natural\nlanguage processing along with both supervised and unsupervised machine\nlearning techniques such as named entity recognition, topic modelling,\ndependency parsing, supervised classification, and clustering are used for CTI\nextraction. We observe the technical challenges associated with these studies\nrelated to obtaining available clean, labelled data which could assure\nreplication, validation, and further extension of the studies. As we find the\nstudies focusing on CTI information extraction from text, we advocate for\nbuilding upon the current CTI extraction work to help cybersecurity\npractitioners with proactive decision making such as threat prioritization,\nautomated threat modelling to utilize knowledge from past cybersecurity\nincidents.", "published": "2021-09-14 16:38:41", "link": "http://arxiv.org/abs/2109.06808v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning", "abstract": "Commonsense is defined as the knowledge that is shared by everyone. However,\ncertain types of commonsense knowledge are correlated with culture and\ngeographic locations and they are only shared locally. For example, the\nscenarios of wedding ceremonies vary across regions due to different customs\ninfluenced by historical and religious factors. Such regional characteristics,\nhowever, are generally omitted in prior work. In this paper, we construct a\nGeo-Diverse Visual Commonsense Reasoning dataset (GD-VCR) to test\nvision-and-language models' ability to understand cultural and\ngeo-location-specific commonsense. In particular, we study two state-of-the-art\nVision-and-Language models, VisualBERT and ViLBERT trained on VCR, a standard\nmultimodal commonsense benchmark with images primarily from Western regions. We\nthen evaluate how well the trained models can generalize to answering the\nquestions in GD-VCR. We find that the performance of both models for\nnon-Western regions including East Asia, South Asia, and Africa is\nsignificantly lower than that for Western region. We analyze the reasons behind\nthe performance disparity and find that the performance gap is larger on QA\npairs that: 1) are concerned with culture-related scenarios, e.g., weddings,\nreligious activities, and festivals; 2) require high-level geo-diverse\ncommonsense reasoning rather than low-order perception and recognition. Dataset\nand code are released at https://github.com/WadeYin9712/GD-VCR.", "published": "2021-09-14 17:52:55", "link": "http://arxiv.org/abs/2109.06860v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Performance-Efficiency Trade-offs in Unsupervised Pre-training for\n  Speech Recognition", "abstract": "This paper is a study of performance-efficiency trade-offs in pre-trained\nmodels for automatic speech recognition (ASR). We focus on wav2vec 2.0, and\nformalize several architecture designs that influence both the model\nperformance and its efficiency. Putting together all our observations, we\nintroduce SEW (Squeezed and Efficient Wav2vec), a pre-trained model\narchitecture with significant improvements along both performance and\nefficiency dimensions across a variety of training setups. For example, under\nthe 100h-960h semi-supervised setup on LibriSpeech, SEW achieves a 1.9x\ninference speedup compared to wav2vec 2.0, with a 13.5% relative reduction in\nword error rate. With a similar inference time, SEW reduces word error rate by\n25-50% across different model sizes.", "published": "2021-09-14 17:58:09", "link": "http://arxiv.org/abs/2109.06870v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "fairseq S^2: A Scalable and Integrable Speech Synthesis Toolkit", "abstract": "This paper presents fairseq S^2, a fairseq extension for speech synthesis. We\nimplement a number of autoregressive (AR) and non-AR text-to-speech models, and\ntheir multi-speaker variants. To enable training speech synthesis models with\nless curated data, a number of preprocessing tools are built and their\nimportance is shown empirically. To facilitate faster iteration of development\nand analysis, a suite of automatic metrics is included. Apart from the features\nadded specifically for this extension, fairseq S^2 also benefits from the\nscalability offered by fairseq and can be easily integrated with other\nstate-of-the-art systems provided in this framework. The code, documentation,\nand pre-trained models are available at\nhttps://github.com/pytorch/fairseq/tree/master/examples/speech_synthesis.", "published": "2021-09-14 18:20:28", "link": "http://arxiv.org/abs/2109.06912v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Residual Adapters for Parameter-Efficient ASR Adaptation to Atypical and\n  Accented Speech", "abstract": "Automatic Speech Recognition (ASR) systems are often optimized to work best\nfor speakers with canonical speech patterns. Unfortunately, these systems\nperform poorly when tested on atypical speech and heavily accented speech. It\nhas previously been shown that personalization through model fine-tuning\nsubstantially improves performance. However, maintaining such large models per\nspeaker is costly and difficult to scale. We show that by adding a relatively\nsmall number of extra parameters to the encoder layers via so-called residual\nadapter, we can achieve similar adaptation gains compared to model fine-tuning,\nwhile only updating a tiny fraction (less than 0.5%) of the model parameters.\nWe demonstrate this on two speech adaptation tasks (atypical and accented\nspeech) and for two state-of-the-art ASR architectures.", "published": "2021-09-14 20:04:47", "link": "http://arxiv.org/abs/2109.06952v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Explainable Identification of Dementia from Transcripts using\n  Transformer Networks", "abstract": "Alzheimer's disease (AD) is the main cause of dementia which is accompanied\nby loss of memory and may lead to severe consequences in peoples' everyday life\nif not diagnosed on time. Very few works have exploited transformer-based\nnetworks and despite the high accuracy achieved, little work has been done in\nterms of model interpretability. In addition, although Mini-Mental State Exam\n(MMSE) scores are inextricably linked with the identification of dementia,\nresearch works face the task of dementia identification and the task of the\nprediction of MMSE scores as two separate tasks. In order to address these\nlimitations, we employ several transformer-based models, with BERT achieving\nthe highest accuracy accounting for 87.50%. Concurrently, we propose an\ninterpretable method to detect AD patients based on siamese networks reaching\naccuracy up to 83.75%. Next, we introduce two multi-task learning models, where\nthe main task refers to the identification of dementia (binary classification),\nwhile the auxiliary one corresponds to the identification of the severity of\ndementia (multiclass classification). Our model obtains accuracy equal to\n86.25% on the detection of AD patients in the multi-task learning setting.\nFinally, we present some new methods to identify the linguistic patterns used\nby AD patients and non-AD ones, including text statistics, vocabulary\nuniqueness, word usage, correlations via a detailed linguistic analysis, and\nexplainability techniques (LIME). Findings indicate significant differences in\nlanguage between AD and non-AD patients.", "published": "2021-09-14 21:49:05", "link": "http://arxiv.org/abs/2109.06980v3", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Assisting the Human Fact-Checkers: Detecting All Previously Fact-Checked\n  Claims in a Document", "abstract": "Given the recent proliferation of false claims online, there has been a lot\nof manual fact-checking effort. As this is very time-consuming, human\nfact-checkers can benefit from tools that can support them and make them more\nefficient. Here, we focus on building a system that could provide such support.\nGiven an input document, it aims to detect all sentences that contain a claim\nthat can be verified by some previously fact-checked claims (from a given\ndatabase). The output is a re-ranked list of the document sentences, so that\nthose that can be verified are ranked as high as possible, together with\ncorresponding evidence. Unlike previous work, which has looked into claim\nretrieval, here we take a document-level perspective. We create a new manually\nannotated dataset for this task, and we propose suitable evaluation measures.\nWe further experiment with a learning-to-rank approach, achieving sizable\nperformance gains over several strong baselines. Our analysis demonstrates the\nimportance of modeling text similarity and stance, while also taking into\naccount the veracity of the retrieved previously fact-checked claims. We\nbelieve that this research would be of interest to fact-checkers, journalists,\nmedia, and regulatory authorities.", "published": "2021-09-14 13:46:52", "link": "http://arxiv.org/abs/2109.07410v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR", "cs.LG", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Overlap-aware low-latency online speaker diarization based on end-to-end\n  local segmentation", "abstract": "We propose to address online speaker diarization as a combination of\nincremental clustering and local diarization applied to a rolling buffer\nupdated every 500ms. Every single step of the proposed pipeline is designed to\ntake full advantage of the strong ability of a recently proposed end-to-end\noverlap-aware segmentation to detect and separate overlapping speakers. In\nparticular, we propose a modified version of the statistics pooling layer\n(initially introduced in the x-vector architecture) to give less weight to\nframes where the segmentation model predicts simultaneous speakers.\nFurthermore, we derive cannot-link constraints from the initial segmentation\nstep to prevent two local speakers from being wrongfully merged during the\nincremental clustering step. Finally, we show how the latency of the proposed\napproach can be adjusted between 500ms and 5s to match the requirements of a\nparticular use case, and we provide a systematic analysis of the influence of\nlatency on the overall performance (on AMI, DIHARD and VoxConverse).", "published": "2021-09-14 07:27:57", "link": "http://arxiv.org/abs/2109.06483v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cross-speaker emotion disentangling and transfer for end-to-end speech\n  synthesis", "abstract": "The cross-speaker emotion transfer task in text-to-speech (TTS) synthesis\nparticularly aims to synthesize speech for a target speaker with the emotion\ntransferred from reference speech recorded by another (source) speaker. During\nthe emotion transfer process, the identity information of the source speaker\ncould also affect the synthesized results, resulting in the issue of speaker\nleakage. This paper proposes a new method with the aim to synthesize\ncontrollable emotional expressive speech and meanwhile maintain the target\nspeaker's identity in the cross-speaker emotion TTS task. The proposed method\nis a Tacotron2-based framework with emotion embedding as the conditioning\nvariable to provide emotion information. Two emotion disentangling modules are\ncontained in our method to 1) get speaker-irrelevant and emotion-discriminative\nembedding, and 2) explicitly constrain the emotion and speaker identity of\nsynthetic speech to be that as expected. Moreover, we present an intuitive\nmethod to control the emotion strength in the synthetic speech for the target\nspeaker. Specifically, the learned emotion embedding is adjusted with a\nflexible scalar value, which allows controlling the emotion strength conveyed\nby the embedding. Extensive experiments have been conducted on a Mandarin\ndisjoint corpus, and the results demonstrate that the proposed method is able\nto synthesize reasonable emotional speech for the target speaker. Compared to\nthe state-of-the-art reference embedding learned methods, our method gets the\nbest performance on the cross-speaker emotion transfer task, indicating that\nour method achieves the new state-of-the-art performance on learning the\nspeaker-irrelevant emotion embedding. Furthermore, the strength ranking test\nand pitch trajectories plots demonstrate that the proposed method can\neffectively control the emotion strength, leading to prosody-diverse synthetic\nspeech.", "published": "2021-09-14 14:46:47", "link": "http://arxiv.org/abs/2109.06733v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-Supervised Metric Learning With Graph Clustering For Speaker\n  Diarization", "abstract": "In this paper, we propose a novel algorithm for speaker diarization using\nmetric learning for graph based clustering. The graph clustering algorithms use\nan adjacency matrix consisting of similarity scores. These scores are computed\nbetween speaker embeddings extracted from pairs of audio segments within the\ngiven recording. In this paper, we propose an approach that jointly learns the\nspeaker embeddings and the similarity metric using principles of\nself-supervised learning. The metric learning network implements a neural model\nof the probabilistic linear discriminant analysis (PLDA). The self-supervision\nis derived from the pseudo labels obtained from a previous iteration of\nclustering. The entire model of representation learning and metric learning is\ntrained with a binary cross entropy loss. By combining the self-supervision\nbased metric learning along with the graph-based clustering algorithm, we\nachieve significant relative improvements of 60% and 7% over the x-vector PLDA\nagglomerative hierarchical clustering (AHC) approach on AMI and the DIHARD\ndatasets respectively in terms of diarization error rates (DER).", "published": "2021-09-14 17:07:33", "link": "http://arxiv.org/abs/2109.06824v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Structure-Enhanced Pop Music Generation via Harmony-Aware Learning", "abstract": "Pop music generation has always been an attractive topic for both musicians\nand scientists for a long time. However, automatically composing pop music with\na satisfactory structure is still a challenging issue. In this paper, we\npropose to leverage harmony-aware learning for structure-enhanced pop music\ngeneration. On the one hand, one of the participants of harmony, chord,\nrepresents the harmonic set of multiple notes, which is integrated closely with\nthe spatial structure of music, the texture. On the other hand, the other\nparticipant of harmony, chord progression, usually accompanies the development\nof the music, which promotes the temporal structure of music, the form.\nMoreover, when chords evolve into chord progression, the texture and form can\nbe bridged by the harmony naturally, which contributes to the joint learning of\nthe two structures. Furthermore, we propose the Harmony-Aware Hierarchical\nMusic Transformer (HAT), which can exploit the structure adaptively from the\nmusic, and make the musical tokens interact hierarchically to enhance the\nstructure in multi-level musical elements. Experimental results reveal that\ncompared to the existing methods, HAT owns a much better understanding of the\nstructure and it can also improve the quality of generated music, especially in\nthe form and texture.", "published": "2021-09-14 05:04:13", "link": "http://arxiv.org/abs/2109.06441v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Machine-learning Framework for Acoustic Design Assessment in Early\n  Design Stages", "abstract": "In time-cost scale model studies, predicting acoustic performance by using\nsimulation methods is a commonly used method that is preferred. In this field,\nbuilding acoustic simulation tools are complicated by several challenges,\nincluding the high cost of acoustic tools, the need for acoustic expertise, and\nthe time-consuming process of acoustic simulation. The goal of this project is\nto introduce a simple model with a short calculation time to estimate the room\nacoustic condition in the early design stages of the building. This paper\npresents a working prototype for a new method of machine learning (ML) to\napproximate a series of typical room acoustic parameters using only geometric\ndata as input characteristics. A novel dataset consisting of acoustical\nsimulations of a single room with 2916 different configurations are used to\ntrain and test the proposed model. In the stimulation process, features that\ninclude room dimensions, window size, material absorption coefficient,\nfurniture, and shading type have been analysed by using Pachyderm acoustic\nsoftware. The mentioned dataset is used as the input of seven machine-learning\nmodels based on fully connected Deep Neural Networks (DNN). The average error\nof ML models is between 1% to 3%, and the average error of the new predicted\nsamples after the validation process is between 2% to 12%.", "published": "2021-09-14 06:04:27", "link": "http://arxiv.org/abs/2109.06459v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Development of In Situ Acoustic Instruments for The Aquatic Environment\n  Study", "abstract": "Based on the analysis of existing acoustic methods and instruments, a\nprototype of an automated instrument has been developed to perform joint\nmeasurements in situ of two parameters: sound speed and ultrasound attenuation.\nThe device is based on existing sound velocity profilers. It was proposed to\nreplace the TDC-GP22 converters used in the sound speed meter ISZ-1 with more\nadvanced modern modified converters TDC-GP30, which can significantly improve\nthe accuracy of measuring the amplitude of the reflected acoustic signal. The\nprograms for processing signals from the primary acoustic transducer have been\ndeveloped. The model of the device passed preliminary tests.", "published": "2021-09-14 10:00:48", "link": "http://arxiv.org/abs/2109.09684v1", "categories": ["eess.SP", "cs.SD", "eess.AS", "physics.ao-ph"], "primary_category": "eess.SP"}
