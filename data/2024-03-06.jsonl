{"title": "Japanese-English Sentence Translation Exercises Dataset for Automatic\n  Grading", "abstract": "This paper proposes the task of automatic assessment of Sentence Translation\nExercises (STEs), that have been used in the early stage of L2 language\nlearning. We formalize the task as grading student responses for each rubric\ncriterion pre-specified by the educators. We then create a dataset for STE\nbetween Japanese and English including 21 questions, along with a total of 3,\n498 student responses (167 on average). The answer responses were collected\nfrom students and crowd workers. Using this dataset, we demonstrate the\nperformance of baselines including finetuned BERT and GPT models with few-shot\nin-context learning. Experimental results show that the baseline model with\nfinetuned BERT was able to classify correct responses with approximately 90% in\nF1, but only less than 80% for incorrect responses. Furthermore, the GPT models\nwith few-shot learning show poorer results than finetuned BERT, indicating that\nour newly proposed task presents a challenging issue, even for the\nstateof-the-art large language models.", "published": "2024-03-06 01:37:03", "link": "http://arxiv.org/abs/2403.03396v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VLSP 2023 -- LTER: A Summary of the Challenge on Legal Textual\n  Entailment Recognition", "abstract": "In this new era of rapid AI development, especially in language processing,\nthe demand for AI in the legal domain is increasingly critical. In the context\nwhere research in other languages such as English, Japanese, and Chinese has\nbeen well-established, we introduce the first fundamental research for the\nVietnamese language in the legal domain: legal textual entailment recognition\nthrough the Vietnamese Language and Speech Processing workshop. In analyzing\nparticipants' results, we discuss certain linguistic aspects critical in the\nlegal domain that pose challenges that need to be addressed.", "published": "2024-03-06 03:42:06", "link": "http://arxiv.org/abs/2403.03435v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Magic Markup: Maintaining Document-External Markup with an LLM", "abstract": "Text documents, including programs, typically have human-readable semantic\nstructure. Historically, programmatic access to these semantics has required\nexplicit in-document tagging. Especially in systems where the text has an\nexecution semantics, this means it is an opt-in feature that is hard to support\nproperly. Today, language models offer a new method: metadata can be bound to\nentities in changing text using a model's human-like understanding of\nsemantics, with no requirements on the document structure. This method expands\nthe applications of document annotation, a fundamental operation in program\nwriting, debugging, maintenance, and presentation. We contribute a system that\nemploys an intelligent agent to re-tag modified programs, enabling rich\nannotations to automatically follow code as it evolves. We also contribute a\nformal problem definition, an empirical synthetic benchmark suite, and our\nbenchmark generator. Our system achieves an accuracy of 90% on our benchmarks\nand can replace a document's tags in parallel at a rate of 5 seconds per tag.\nWhile there remains significant room for improvement, we find performance\nreliable enough to justify further exploration of applications.", "published": "2024-03-06 05:40:31", "link": "http://arxiv.org/abs/2403.03481v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Knowledge Plug-and-Play Test Bed for Open-domain Dialogue Generation", "abstract": "Knowledge-based, open-domain dialogue generation aims to build chit-chat\nsystems that talk to humans using mined support knowledge. Many types and\nsources of knowledge have previously been shown to be useful as support\nknowledge. Even in the era of large language models, response generation\ngrounded in knowledge retrieved from additional up-to-date sources remains a\npractically important approach. While prior work using single-source knowledge\nhas shown a clear positive correlation between the performances of knowledge\nselection and response generation, there are no existing multi-source datasets\nfor evaluating support knowledge retrieval. Further, prior work has assumed\nthat the knowledge sources available at test time are the same as during\ntraining. This unrealistic assumption unnecessarily handicaps models, as new\nknowledge sources can become available after a model is trained. In this paper,\nwe present a high-quality benchmark named multi-source Wizard of Wikipedia\n(Ms.WoW) for evaluating multi-source dialogue knowledge selection and response\ngeneration. Unlike existing datasets, it contains clean support knowledge,\ngrounded at the utterance level and partitioned into multiple knowledge\nsources. We further propose a new challenge, dialogue knowledge plug-and-play,\nwhich aims to test an already trained dialogue model on using new support\nknowledge from previously unseen sources in a zero-shot fashion.", "published": "2024-03-06 06:54:02", "link": "http://arxiv.org/abs/2403.03496v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CLongEval: A Chinese Benchmark for Evaluating Long-Context Large\n  Language Models", "abstract": "Developing Large Language Models (LLMs) with robust long-context capabilities\nhas been the recent research focus, resulting in the emergence of long-context\nLLMs proficient in Chinese. However, the evaluation of these models remains\nunderdeveloped due to a lack of benchmarks. To address this gap, we present\nCLongEval, a comprehensive Chinese benchmark for evaluating long-context LLMs.\nCLongEval is characterized by three key features: (1) Sufficient data volume,\ncomprising 7 distinct tasks and 7,267 examples; (2) Broad applicability,\naccommodating to models with context windows size from 1K to 100K; (3) High\nquality, with over 2,000 manually annotated question-answer pairs in addition\nto the automatically constructed labels. With CLongEval, we undertake a\ncomprehensive assessment of 6 open-source long-context LLMs and 2 leading\ncommercial counterparts that feature both long-context abilities and\nproficiency in Chinese. We also provide in-depth analysis based on the\nempirical results, trying to shed light on the critical capabilities that\npresent challenges in long-context settings. The dataset, evaluation scripts,\nand model outputs are released.", "published": "2024-03-06 07:43:43", "link": "http://arxiv.org/abs/2403.03514v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BiVert: Bidirectional Vocabulary Evaluation using Relations for Machine\n  Translation", "abstract": "Neural machine translation (NMT) has progressed rapidly in the past few\nyears, promising improvements and quality translations for different languages.\nEvaluation of this task is crucial to determine the quality of the translation.\nOverall, insufficient emphasis is placed on the actual sense of the translation\nin traditional methods. We propose a bidirectional semantic-based evaluation\nmethod designed to assess the sense distance of the translation from the source\ntext. This approach employs the comprehensive multilingual encyclopedic\ndictionary BabelNet. Through the calculation of the semantic distance between\nthe source and its back translation of the output, our method introduces a\nquantifiable approach that empowers sentence comparison on the same linguistic\nlevel. Factual analysis shows a strong correlation between the average\nevaluation scores generated by our method and the human assessments across\nvarious machine translation systems for English-German language pair. Finally,\nour method proposes a new multilingual approach to rank MT systems without the\nneed for parallel corpora.", "published": "2024-03-06 08:02:21", "link": "http://arxiv.org/abs/2403.03521v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking Hallucination in Large Language Models based on\n  Unanswerable Math Word Problem", "abstract": "Large language models (LLMs) are highly effective in various natural language\nprocessing (NLP) tasks. However, they are susceptible to producing unreliable\nconjectures in ambiguous contexts called hallucination. This paper presents a\nnew method for evaluating LLM hallucination in Question Answering (QA) based on\nthe unanswerable math word problem (MWP). To support this approach, we\ninnovatively develop a dataset called Unanswerable Math Word Problem (UMWP)\nwhich comprises 5200 questions across five categories. We developed an\nevaluation methodology combining text similarity and mathematical expression\ndetection to determine whether LLM considers the question unanswerable. The\nresults of extensive experiments conducted on 31 LLMs, including GPT-3,\nInstructGPT, LLaMA, and Claude, demonstrate that in-context learning and\nreinforcement learning with human feedback (RLHF) training significantly\nenhance the model's ability to avoid hallucination. We show that utilizing MWP\nis a reliable and effective approach to assess hallucination. Our code and data\nare available at https://github.com/Yuki-Asuuna/UMWP.", "published": "2024-03-06 09:06:34", "link": "http://arxiv.org/abs/2403.03558v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GPTopic: Dynamic and Interactive Topic Representations", "abstract": "Topic modeling seems to be almost synonymous with generating lists of top\nwords to represent topics within large text corpora. However, deducing a topic\nfrom such list of individual terms can require substantial expertise and\nexperience, making topic modelling less accessible to people unfamiliar with\nthe particularities and pitfalls of top-word interpretation. A topic\nrepresentation limited to top-words might further fall short of offering a\ncomprehensive and easily accessible characterization of the various aspects,\nfacets and nuances a topic might have. To address these challenges, we\nintroduce GPTopic, a software package that leverages Large Language Models\n(LLMs) to create dynamic, interactive topic representations. GPTopic provides\nan intuitive chat interface for users to explore, analyze, and refine topics\ninteractively, making topic modeling more accessible and comprehensive. The\ncorresponding code is available here: https://github.com/ArikReuter/TopicGPT.", "published": "2024-03-06 11:34:20", "link": "http://arxiv.org/abs/2403.03628v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PPTC-R benchmark: Towards Evaluating the Robustness of Large Language\n  Models for PowerPoint Task Completion", "abstract": "The growing dependence on Large Language Models (LLMs) for finishing user\ninstructions necessitates a comprehensive understanding of their robustness to\ncomplex task completion in real-world situations. To address this critical\nneed, we propose the PowerPoint Task Completion Robustness benchmark (PPTC-R)\nto measure LLMs' robustness to the user PPT task instruction and software\nversion. Specifically, we construct adversarial user instructions by attacking\nuser instructions at sentence, semantic, and multi-language levels. To assess\nthe robustness of Language Models to software versions, we vary the number of\nprovided APIs to simulate both the newest version and earlier version settings.\nSubsequently, we test 3 closed-source and 4 open-source LLMs using a benchmark\nthat incorporates these robustness settings, aiming to evaluate how deviations\nimpact LLMs' API calls for task completion. We find that GPT-4 exhibits the\nhighest performance and strong robustness in our benchmark, particularly in the\nversion update and the multilingual settings. However, we find that all LLMs\nlose their robustness when confronted with multiple challenges (e.g.,\nmulti-turn) simultaneously, leading to significant performance drops. We\nfurther analyze the robustness behavior and error reasons of LLMs in our\nbenchmark, which provide valuable insights for researchers to understand the\nLLM's robustness in task completion and develop more robust LLMs and agents. We\nrelease the code and data at \\url{https://github.com/ZekaiGalaxy/PPTCR}.", "published": "2024-03-06 15:33:32", "link": "http://arxiv.org/abs/2403.03788v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Modular Approach for Multimodal Summarization of TV Shows", "abstract": "In this paper we address the task of summarizing television shows, which\ntouches key areas in AI research: complex reasoning, multiple modalities, and\nlong narratives. We present a modular approach where separate components\nperform specialized sub-tasks which we argue affords greater flexibility\ncompared to end-to-end methods. Our modules involve detecting scene boundaries,\nreordering scenes so as to minimize the number of cuts between different\nevents, converting visual information to text, summarizing the dialogue in each\nscene, and fusing the scene summaries into a final summary for the entire\nepisode. We also present a new metric, PRISMA (Precision and Recall EvaluatIon\nof Summary FActs), to measure both precision and recall of generated summaries,\nwhich we decompose into atomic facts. Tested on the recently released\nSummScreen3D dataset, our method produces higher quality summaries than\ncomparison models, as measured with ROUGE and our new fact-based metric, and as\nassessed by human evaluators.", "published": "2024-03-06 16:10:01", "link": "http://arxiv.org/abs/2403.03823v9", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ShortGPT: Layers in Large Language Models are More Redundant Than You\n  Expect", "abstract": "As Large Language Models (LLMs) continue to advance in performance, their\nsize has escalated significantly, with current LLMs containing billions or even\ntrillions of parameters. However, in this study, we discovered that many layers\nof LLMs exhibit high similarity, and some layers play a negligible role in\nnetwork functionality. Based on this observation, we define a metric called\nBlock Influence (BI) to gauge the significance of each layer in LLMs. We then\npropose a straightforward pruning approach: layer removal, in which we directly\ndelete the redundant layers in LLMs based on their BI scores. Experiments\ndemonstrate that our method, which we call ShortGPT, significantly outperforms\nprevious state-of-the-art (SOTA) methods in model pruning. Moreover, ShortGPT\nis orthogonal to quantization-like methods, enabling further reduction in\nparameters and computation. The ability to achieve better results through\nsimple layer removal, as opposed to more complex pruning techniques, suggests a\nhigh degree of redundancy in the model architecture.", "published": "2024-03-06 17:04:18", "link": "http://arxiv.org/abs/2403.03853v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "X-Shot: A Unified System to Handle Frequent, Few-shot and Zero-shot\n  Learning Simultaneously in Classification", "abstract": "In recent years, few-shot and zero-shot learning, which learn to predict\nlabels with limited annotated instances, have garnered significant attention.\nTraditional approaches often treat frequent-shot (freq-shot; labels with\nabundant instances), few-shot, and zero-shot learning as distinct challenges,\noptimizing systems for just one of these scenarios. Yet, in real-world\nsettings, label occurrences vary greatly. Some of them might appear thousands\nof times, while others might only appear sporadically or not at all. For\npractical deployment, it is crucial that a system can adapt to any label\noccurrence. We introduce a novel classification challenge: X-shot, reflecting a\nreal-world context where freq-shot, few-shot, and zero-shot labels co-occur\nwithout predefined limits. Here, X can span from 0 to positive infinity. The\ncrux of X-shot centers on open-domain generalization and devising a system\nversatile enough to manage various label scenarios. To solve X-shot, we propose\nBinBin (Binary INference Based on INstruction following) that leverages the\nIndirect Supervision from a large collection of NLP tasks via instruction\nfollowing, bolstered by Weak Supervision provided by large language models.\nBinBin surpasses previous state-of-the-art techniques on three benchmark\ndatasets across multiple domains. To our knowledge, this is the first work\naddressing X-shot learning, where X remains variable.", "published": "2024-03-06 17:13:24", "link": "http://arxiv.org/abs/2403.03863v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KIWI: A Dataset of Knowledge-Intensive Writing Instructions for\n  Answering Research Questions", "abstract": "Large language models (LLMs) adapted to follow user instructions are now\nwidely deployed as conversational agents. In this work, we examine one\nincreasingly common instruction-following task: providing writing assistance to\ncompose a long-form answer. To evaluate the capabilities of current LLMs on\nthis task, we construct KIWI, a dataset of knowledge-intensive writing\ninstructions in the scientific domain. Given a research question, an initial\nmodel-generated answer and a set of relevant papers, an expert annotator\niteratively issues instructions for the model to revise and improve its answer.\nWe collect 1,260 interaction turns from 234 interaction sessions with three\nstate-of-the-art LLMs. Each turn includes a user instruction, a model response,\nand a human evaluation of the model response. Through a detailed analysis of\nthe collected responses, we find that all models struggle to incorporate new\ninformation into an existing answer, and to perform precise and unambiguous\nedits. Further, we find that models struggle to judge whether their outputs\nsuccessfully followed user instructions, with accuracy at least 10 points short\nof human agreement. Our findings indicate that KIWI will be a valuable resource\nto measure progress and improve LLMs' instruction-following capabilities for\nknowledge intensive writing tasks.", "published": "2024-03-06 17:16:44", "link": "http://arxiv.org/abs/2403.03866v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SaulLM-7B: A pioneering Large Language Model for Law", "abstract": "In this paper, we introduce SaulLM-7B, a large language model (LLM) tailored\nfor the legal domain. With 7 billion parameters, SaulLM-7B is the first LLM\ndesigned explicitly for legal text comprehension and generation. Leveraging the\nMistral 7B architecture as its foundation, SaulLM-7B is trained on an English\nlegal corpus of over 30 billion tokens. SaulLM-7B exhibits state-of-the-art\nproficiency in understanding and processing legal documents. Additionally, we\npresent a novel instructional fine-tuning method that leverages legal datasets\nto further enhance SaulLM-7B's performance in legal tasks. SaulLM-7B is\nreleased under the MIT License.", "published": "2024-03-06 17:42:16", "link": "http://arxiv.org/abs/2403.03883v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FaaF: Facts as a Function for the evaluation of generated text", "abstract": "The demand for accurate and efficient verification of information in texts\ngenerated by large language models (LMs) is at an all-time high, but remains\nunresolved. Recent efforts have focused on extracting and verifying atomic\nfacts from these texts via prompting LM evaluators. However, we demonstrate\nthat this method of prompting is unreliable when faced with incomplete or\ninaccurate reference information. We introduce Facts as a Function (FaaF), a\nnew approach to the fact verification task that leverages the function-calling\ncapabilities of LMs. FaaF significantly enhances the ability of LMs to identify\nunsupported facts in texts, while also improving efficiency and significantly\nlowering costs compared to prompt-based methods. Additionally, we propose a\nframework for evaluating factual recall in Retrieval Augmented Generation (RAG)\nsystems, which we employ to compare prompt-based and FaaF methods using various\nLMs under challenging conditions.", "published": "2024-03-06 17:48:06", "link": "http://arxiv.org/abs/2403.03888v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Measure for Transparent Comparison of Linguistic Diversity in\n  Multilingual NLP Data Sets", "abstract": "Typologically diverse benchmarks are increasingly created to track the\nprogress achieved in multilingual NLP. Linguistic diversity of these data sets\nis typically measured as the number of languages or language families included\nin the sample, but such measures do not consider structural properties of the\nincluded languages. In this paper, we propose assessing linguistic diversity of\na data set against a reference language sample as a means of maximising\nlinguistic diversity in the long run. We represent languages as sets of\nfeatures and apply a version of the Jaccard index suitable for comparing sets\nof measures. In addition to the features extracted from typological data bases,\nwe propose an automatic text-based measure, which can be used as a means of\novercoming the well-known problem of data sparsity in manually collected\nfeatures. Our diversity score is interpretable in terms of linguistic features\nand can identify the types of languages that are not represented in a data set.\nUsing our method, we analyse a range of popular multilingual data sets (UD,\nBible100, mBERT, XTREME, XGLUE, XNLI, XCOPA, TyDiQA, XQuAD). In addition to\nranking these data sets, we find, for example, that (poly)synthetic languages\nare missing in almost all of them.", "published": "2024-03-06 18:14:22", "link": "http://arxiv.org/abs/2403.03909v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Did Translation Models Get More Robust Without Anyone Even Noticing?", "abstract": "Neural machine translation (MT) models achieve strong results across a\nvariety of settings, but it is widely believed that they are highly sensitive\nto \"noisy\" inputs, such as spelling errors, abbreviations, and other formatting\nissues. In this paper, we revisit this insight in light of recent multilingual\nMT models and large language models (LLMs) applied to machine translation.\nSomewhat surprisingly, we show through controlled experiments that these models\nare far more robust to many kinds of noise than previous models, even when they\nperform similarly on clean data. This is notable because, even though LLMs have\nmore parameters and more complex training processes than past models, none of\nthe open ones we consider use any techniques specifically designed to encourage\nrobustness. Next, we show that similar trends hold for social media translation\nexperiments -- LLMs are more robust to social media text. We include an\nanalysis of the circumstances in which source correction techniques can be used\nto mitigate the effects of noise. Altogether, we show that robustness to many\ntypes of noise has increased.", "published": "2024-03-06 18:33:51", "link": "http://arxiv.org/abs/2403.03923v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Levels of AI Agents: from Rules to Large Language Models", "abstract": "AI agents are defined as artificial entities to perceive the environment,\nmake decisions and take actions. Inspired by the 6 levels of autonomous driving\nby Society of Automotive Engineers, the AI agents are also categorized based on\nutilities and strongness, as the following levels: L0, no AI, with tools taking\ninto account perception plus actions; L1, using rule-based AI; L2, making\nrule-based AI replaced by IL/RL-based AI, with additional reasoning & decision\nmaking; L3, applying LLM-based AI instead of IL/RL-based AI, additionally\nsetting up memory & reflection; L4, based on L3, facilitating autonomous\nlearning & generalization; L5, based on L4, appending personality of emotion\nand character and collaborative behavior with multi-agents.", "published": "2024-03-06 23:02:30", "link": "http://arxiv.org/abs/2405.06643v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Negating Negatives: Alignment with Human Negative Samples via\n  Distributional Dispreference Optimization", "abstract": "Large language models (LLMs) have revolutionized the role of AI, yet pose\npotential social risks. To steer LLMs towards human preference, alignment\ntechnologies have been introduced and gained increasing attention.\nNevertheless, existing methods heavily rely on high-quality positive-negative\ntraining pairs, suffering from noisy positive responses that are barely\ndistinguishable from negative ones. Given recent LLMs' proficiency in\ngenerating helpful responses, this work pivots towards a new research question:\ncan we achieve alignment using solely human-annotated negative samples,\npreserving helpfulness while reducing harmfulness? For this purpose, we propose\nDistributional Dispreference Optimization (D$^2$O), which maximizes the\ndiscrepancy between dispreferred responses and the generated non-negative ones.\nIn this way, D$^2$O effectively eschews harmful information without\nincorporating noisy positive samples, while avoiding collapse using\nself-generated responses as anchors. We demonstrate that D$^2$O can be regarded\nas learning a distributional preference model reflecting human dispreference\nagainst negative responses, which is theoretically an upper bound of the\ninstance-level DPO. Extensive experiments manifest that our method achieves\ncomparable generation quality and surpasses the latest strong baselines in\nproducing less harmful and more informative responses with better training\nstability and faster convergence.", "published": "2024-03-06 03:02:38", "link": "http://arxiv.org/abs/2403.03419v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language\n  Models", "abstract": "Instruction Tuning has the potential to stimulate or enhance specific\ncapabilities of large language models (LLMs). However, achieving the right\nbalance of data is crucial to prevent catastrophic forgetting and interference\nbetween tasks. To address these limitations and enhance training flexibility,\nwe propose the Mixture-of-LoRAs (MoA) architecture which is a novel and\nparameter-efficient tuning method designed for multi-task learning with LLMs.\nIn this paper, we start by individually training multiple domain-specific LoRA\nmodules using corresponding supervised corpus data. These LoRA modules can be\naligned with the expert design principles observed in Mixture-of-Experts (MoE).\nSubsequently, we combine the multiple LoRAs using an explicit routing strategy\nand introduce domain labels to facilitate multi-task learning, which help\nprevent interference between tasks and ultimately enhances the performance of\neach individual task. Furthermore, each LoRA model can be iteratively adapted\nto a new domain, allowing for quick domain-specific adaptation. Experiments on\ndiverse tasks demonstrate superior and robust performance, which can further\npromote the wide application of domain-specific LLMs.", "published": "2024-03-06 03:33:48", "link": "http://arxiv.org/abs/2403.03432v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detecting AI-Generated Sentences in Human-AI Collaborative Hybrid Texts:\n  Challenges, Strategies, and Insights", "abstract": "This study explores the challenge of sentence-level AI-generated text\ndetection within human-AI collaborative hybrid texts. Existing studies of\nAI-generated text detection for hybrid texts often rely on synthetic datasets.\nThese typically involve hybrid texts with a limited number of boundaries. We\ncontend that studies of detecting AI-generated content within hybrid texts\nshould cover different types of hybrid texts generated in realistic settings to\nbetter inform real-world applications. Therefore, our study utilizes the\nCoAuthor dataset, which includes diverse, realistic hybrid texts generated\nthrough the collaboration between human writers and an intelligent writing\nsystem in multi-turn interactions. We adopt a two-step, segmentation-based\npipeline: (i) detect segments within a given hybrid text where each segment\ncontains sentences of consistent authorship, and (ii) classify the authorship\nof each identified segment. Our empirical findings highlight (1) detecting\nAI-generated sentences in hybrid texts is overall a challenging task because\n(1.1) human writers' selecting and even editing AI-generated sentences based on\npersonal preferences adds difficulty in identifying the authorship of segments;\n(1.2) the frequent change of authorship between neighboring sentences within\nthe hybrid text creates difficulties for segment detectors in identifying\nauthorship-consistent segments; (1.3) the short length of text segments within\nhybrid texts provides limited stylistic cues for reliable authorship\ndetermination; (2) before embarking on the detection process, it is beneficial\nto assess the average length of segments within the hybrid text. This\nassessment aids in deciding whether (2.1) to employ a text segmentation-based\nstrategy for hybrid texts with longer segments, or (2.2) to adopt a direct\nsentence-by-sentence classification strategy for those with shorter segments.", "published": "2024-03-06 07:25:46", "link": "http://arxiv.org/abs/2403.03506v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Multilingual Dense Retrieval via Generative Pseudo Labeling", "abstract": "Dense retrieval methods have demonstrated promising performance in\nmultilingual information retrieval, where queries and documents can be in\ndifferent languages. However, dense retrievers typically require a substantial\namount of paired data, which poses even greater challenges in multilingual\nscenarios. This paper introduces UMR, an Unsupervised Multilingual dense\nRetriever trained without any paired data. Our approach leverages the sequence\nlikelihood estimation capabilities of multilingual language models to acquire\npseudo labels for training dense retrievers. We propose a two-stage framework\nwhich iteratively improves the performance of multilingual dense retrievers.\nExperimental results on two benchmark datasets show that UMR outperforms\nsupervised baselines, showcasing the potential of training multilingual\nretrievers without paired data, thereby enhancing their practicality. Our\nsource code, data, and models are publicly available at\nhttps://github.com/MiuLab/UMR", "published": "2024-03-06 07:49:06", "link": "http://arxiv.org/abs/2403.03516v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Prompt Mining for Language-based Human Mobility Forecasting", "abstract": "With the advancement of large language models, language-based forecasting has\nrecently emerged as an innovative approach for predicting human mobility\npatterns. The core idea is to use prompts to transform the raw mobility data\ngiven as numerical values into natural language sentences so that the language\nmodels can be leveraged to generate the description for future observations.\nHowever, previous studies have only employed fixed and manually designed\ntemplates to transform numerical values into sentences. Since the forecasting\nperformance of language models heavily relies on prompts, using fixed templates\nfor prompting may limit the forecasting capability of language models. In this\npaper, we propose a novel framework for prompt mining in language-based\nmobility forecasting, aiming to explore diverse prompt design strategies.\nSpecifically, the framework includes a prompt generation stage based on the\ninformation entropy of prompts and a prompt refinement stage to integrate\nmechanisms such as the chain of thought. Experimental results on real-world\nlarge-scale data demonstrate the superiority of generated prompts from our\nprompt mining pipeline. Additionally, the comparison of different prompt\nvariants shows that the proposed prompt refinement process is effective. Our\nstudy presents a promising direction for further advancing language-based\nmobility forecasting.", "published": "2024-03-06 08:43:30", "link": "http://arxiv.org/abs/2403.03544v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "gaHealth: An English-Irish Bilingual Corpus of Health Data", "abstract": "Machine Translation is a mature technology for many high-resource language\npairs. However in the context of low-resource languages, there is a paucity of\nparallel data datasets available for developing translation models.\nFurthermore, the development of datasets for low-resource languages often\nfocuses on simply creating the largest possible dataset for generic\ntranslation. The benefits and development of smaller in-domain datasets can\neasily be overlooked. To assess the merits of using in-domain data, a dataset\nfor the specific domain of health was developed for the low-resource English to\nIrish language pair. Our study outlines the process used in developing the\ncorpus and empirically demonstrates the benefits of using an in-domain dataset\nfor the health domain. In the context of translating health-related data,\nmodels developed using the gaHealth corpus demonstrated a maximum BLEU score\nimprovement of 22.2 points (40%) when compared with top performing models from\nthe LoResMT2021 Shared Task. Furthermore, we define linguistic guidelines for\ndeveloping gaHealth, the first bilingual corpus of health data for the Irish\nlanguage, which we hope will be of use to other creators of low-resource data\nsets. gaHealth is now freely available online and is ready to be explored for\nfurther research.", "published": "2024-03-06 09:36:36", "link": "http://arxiv.org/abs/2403.03575v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing ASD detection accuracy: a combined approach of machine\n  learning and deep learning models with natural language processing", "abstract": "Purpose: Our study explored the use of artificial intelligence (AI) to\ndiagnose autism spectrum disorder (ASD). It focused on machine learning (ML)\nand deep learning (DL) to detect ASD from text inputs on social media,\naddressing challenges in traditional ASD diagnosis.\n  Methods: We used natural language processing (NLP), ML, and DL models\n(including decision trees, XGB, KNN, RNN, LSTM, Bi-LSTM, BERT, and BERTweet) to\nanalyze 404,627 tweets, classifying them based on ASD or non-ASD authors. A\nsubset of 90,000 tweets was used for model training and testing.\n  Results: Our AI models showed high accuracy, with an 88% success rate in\nidentifying texts from individuals with ASD.\n  Conclusion: The study demonstrates AI's potential in improving ASD diagnosis,\nespecially in children, highlighting the importance of early detection.", "published": "2024-03-06 09:57:42", "link": "http://arxiv.org/abs/2403.03581v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Design of an Open-Source Architecture for Neural Machine Translation", "abstract": "adaptNMT is an open-source application that offers a streamlined approach to\nthe development and deployment of Recurrent Neural Networks and Transformer\nmodels. This application is built upon the widely-adopted OpenNMT ecosystem,\nand is particularly useful for new entrants to the field, as it simplifies the\nsetup of the development environment and creation of train, validation, and\ntest splits. The application offers a graphing feature that illustrates the\nprogress of model training, and employs SentencePiece for creating subword\nsegmentation models. Furthermore, the application provides an intuitive user\ninterface that facilitates hyperparameter customization. Notably, a\nsingle-click model development approach has been implemented, and models\ndeveloped by adaptNMT can be evaluated using a range of metrics. To encourage\neco-friendly research, adaptNMT incorporates a green report that flags the\npower consumption and kgCO${_2}$ emissions generated during model development.\nThe application is freely available.", "published": "2024-03-06 09:57:52", "link": "http://arxiv.org/abs/2403.03582v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multimodal Large Language Models to Support Real-World Fact-Checking", "abstract": "Multimodal large language models (MLLMs) carry the potential to support\nhumans in processing vast amounts of information. While MLLMs are already being\nused as a fact-checking tool, their abilities and limitations in this regard\nare understudied. Here is aim to bridge this gap. In particular, we propose a\nframework for systematically assessing the capacity of current multimodal\nmodels to facilitate real-world fact-checking. Our methodology is\nevidence-free, leveraging only these models' intrinsic knowledge and reasoning\ncapabilities. By designing prompts that extract models' predictions,\nexplanations, and confidence levels, we delve into research questions\nconcerning model accuracy, robustness, and reasons for failure. We empirically\nfind that (1) GPT-4V exhibits superior performance in identifying malicious and\nmisleading multimodal claims, with the ability to explain the unreasonable\naspects and underlying motives, and (2) existing open-source models exhibit\nstrong biases and are highly sensitive to the prompt. Our study offers insights\ninto combating false multimodal information and building secure, trustworthy\nmultimodal models. To the best of our knowledge, we are the first to evaluate\nMLLMs for real-world fact-checking.", "published": "2024-03-06 11:32:41", "link": "http://arxiv.org/abs/2403.03627v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Apollo: A Lightweight Multilingual Medical LLM towards Democratizing\n  Medical AI to 6B People", "abstract": "Despite the vast repository of global medical knowledge predominantly being\nin English, local languages are crucial for delivering tailored healthcare\nservices, particularly in areas with limited medical resources. To extend the\nreach of medical AI advancements to a broader population, we aim to develop\nmedical LLMs across the six most widely spoken languages, encompassing a global\npopulation of 6.1 billion. This effort culminates in the creation of the\nApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the\nmultilingual medical benchmark, the released Apollo models, at various\nrelatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best\nperformance among models of equivalent size. Especially, Apollo-7B is the\nstate-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite\nmodels could be used to improve the multi-lingual medical capabilities of\nlarger models without fine-tuning in a proxy-tuning fashion. We will\nopen-source training corpora, code, model weights and evaluation benchmark.", "published": "2024-03-06 11:56:02", "link": "http://arxiv.org/abs/2403.03640v6", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "General2Specialized LLMs Translation for E-commerce", "abstract": "Existing Neural Machine Translation (NMT) models mainly handle translation in\nthe general domain, while overlooking domains with special writing formulas,\nsuch as e-commerce and legal documents. Taking e-commerce as an example, the\ntexts usually include amounts of domain-related words and have more grammar\nproblems, which leads to inferior performances of current NMT methods. To\naddress these problems, we collect two domain-related resources, including a\nset of term pairs (aligned Chinese-English bilingual terms) and a parallel\ncorpus annotated for the e-commerce domain. Furthermore, we propose a two-step\nfine-tuning paradigm (named G2ST) with self-contrastive semantic enhancement to\ntransfer one general NMT model to the specialized NMT model for e-commerce. The\nparadigm can be used for the NMT models based on Large language models (LLMs).\nExtensive evaluations on real e-commerce titles demonstrate the superior\ntranslation quality and robustness of our G2ST approach, as compared with\nstate-of-the-art NMT models such as LLaMA, Qwen, GPT-3.5, and even GPT-4.", "published": "2024-03-06 13:15:21", "link": "http://arxiv.org/abs/2403.03689v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rapidly Developing High-quality Instruction Data and Evaluation\n  Benchmark for Large Language Models with Minimal Human Effort: A Case Study\n  on Japanese", "abstract": "The creation of instruction data and evaluation benchmarks for serving Large\nlanguage models often involves enormous human annotation. This issue becomes\nparticularly pronounced when rapidly developing such resources for a\nnon-English language like Japanese. Instead of following the popular practice\nof directly translating existing English resources into Japanese (e.g.,\nJapanese-Alpaca), we propose an efficient self-instruct method based on GPT-4.\nWe first translate a small amount of English instructions into Japanese and\npost-edit them to obtain native-level quality. GPT-4 then utilizes them as\ndemonstrations to automatically generate Japanese instruction data. We also\nconstruct an evaluation benchmark containing 80 questions across 8 categories,\nusing GPT-4 to automatically assess the response quality of LLMs without human\nreferences. The empirical results suggest that the models fine-tuned on our\nGPT-4 self-instruct data significantly outperformed the Japanese-Alpaca across\nall three base pre-trained models. Our GPT-4 self-instruct data allowed the\nLLaMA 13B model to defeat GPT-3.5 (Davinci-003) with a 54.37\\% win-rate. The\nhuman evaluation exhibits the consistency between GPT-4's assessments and human\npreference. Our high-quality instruction data and evaluation benchmark have\nbeen released here.", "published": "2024-03-06 13:17:07", "link": "http://arxiv.org/abs/2403.03690v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Probabilistic Topic Modelling with Transformer Representations", "abstract": "Topic modelling was mostly dominated by Bayesian graphical models during the\nlast decade. With the rise of transformers in Natural Language Processing,\nhowever, several successful models that rely on straightforward clustering\napproaches in transformer-based embedding spaces have emerged and consolidated\nthe notion of topics as clusters of embedding vectors. We propose the\nTransformer-Representation Neural Topic Model (TNTM), which combines the\nbenefits of topic representations in transformer-based embedding spaces and\nprobabilistic modelling. Therefore, this approach unifies the powerful and\nversatile notion of topics based on transformer embeddings with fully\nprobabilistic modelling, as in models such as Latent Dirichlet Allocation\n(LDA). We utilize the variational autoencoder (VAE) framework for improved\ninference speed and modelling flexibility. Experimental results show that our\nproposed model achieves results on par with various state-of-the-art approaches\nin terms of embedding coherence while maintaining almost perfect topic\ndiversity. The corresponding source code is available at\nhttps://github.com/ArikReuter/TNTM.", "published": "2024-03-06 14:27:29", "link": "http://arxiv.org/abs/2403.03737v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "German also Hallucinates! Inconsistency Detection in News Summaries with\n  the Absinth Dataset", "abstract": "The advent of Large Language Models (LLMs) has led to remarkable progress on\na wide range of natural language processing tasks. Despite the advances, these\nlarge-sized models still suffer from hallucinating information in their output,\nwhich poses a major issue in automatic text summarization, as we must guarantee\nthat the generated summary is consistent with the content of the source\ndocument. Previous research addresses the challenging task of detecting\nhallucinations in the output (i.e. inconsistency detection) in order to\nevaluate the faithfulness of the generated summaries. However, these works\nprimarily focus on English and recent multilingual approaches lack German data.\nThis work presents absinth, a manually annotated dataset for hallucination\ndetection in German news summarization and explores the capabilities of novel\nopen-source LLMs on this task in both fine-tuning and in-context learning\nsettings. We open-source and release the absinth dataset to foster further\nresearch on hallucination detection in German.", "published": "2024-03-06 14:37:30", "link": "http://arxiv.org/abs/2403.03750v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Evaluating the Elementary Multilingual Capabilities of Large Language\n  Models with MultiQ", "abstract": "Large language models (LLMs) need to serve everyone, including a global\nmajority of non-English speakers. However, most LLMs today, and open LLMs in\nparticular, are often intended for use in just English (e.g. Llama2, Mistral)\nor a small handful of high-resource languages (e.g. Mixtral, Qwen). Recent\nresearch shows that, despite limits in their intended use, people prompt LLMs\nin many different languages. Therefore, in this paper, we investigate the basic\nmultilingual capabilities of state-of-the-art open LLMs beyond their intended\nuse. For this purpose, we introduce MultiQ, a new silver standard benchmark for\nbasic open-ended question answering with 27.4k test questions across a\ntypologically diverse set of 137 languages. With MultiQ, we evaluate language\nfidelity, i.e. whether models respond in the prompted language, and question\nanswering accuracy. All LLMs we test respond faithfully and/or accurately for\nat least some languages beyond their intended use. Most models are more\naccurate when they respond faithfully. However, differences across models are\nlarge, and there is a long tail of languages where models are neither accurate\nnor faithful. We explore differences in tokenization as a potential explanation\nfor our findings, identifying possible correlations that warrant further\ninvestigation.", "published": "2024-03-06 16:01:44", "link": "http://arxiv.org/abs/2403.03814v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Emojinize: Enriching Any Text with Emoji Translations", "abstract": "Emoji have become ubiquitous in written communication, on the Web and beyond.\nThey can emphasize or clarify emotions, add details to conversations, or simply\nserve decorative purposes. This casual use, however, barely scratches the\nsurface of the expressive power of emoji. To further unleash this power, we\npresent Emojinize, a method for translating arbitrary text phrases into\nsequences of one or more emoji without requiring human input. By leveraging the\npower of large language models, Emojinize can choose appropriate emoji by\ndisambiguating based on context (eg, cricket-bat vs bat) and can express\ncomplex concepts compositionally by combining multiple emoji (eq, \"Emojinize\"\nis translated to input-latin-letters right-arrow grinning-face). In a cloze\ntest--based user study, we show that Emojinize's emoji translations increase\nthe human guessability of masked words by 55%, whereas human-picked emoji\ntranslations do so by only 29%. These results suggest that emoji provide a\nsufficiently rich vocabulary to accurately translate a wide variety of words.\nMoreover, annotating words and phrases with Emojinize's emoji translations\nopens the door to numerous downstream applications, including children learning\nhow to read, adults learning foreign languages, and text understanding for\npeople with learning disabilities.", "published": "2024-03-06 17:06:17", "link": "http://arxiv.org/abs/2403.03857v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Designing Informative Metrics for Few-Shot Example Selection", "abstract": "Pretrained language models (PLMs) have shown remarkable few-shot learning\ncapabilities when provided with properly formatted examples. However, selecting\nthe \"best\" examples remains an open challenge. We propose a complexity-based\nprompt selection approach for sequence tagging tasks. This approach avoids the\ntraining of a dedicated model for selection of examples, and instead uses\ncertain metrics to align the syntactico-semantic complexity of test sentences\nand examples. We use both sentence- and word-level metrics to match the\ncomplexity of examples to the (test) sentence being considered. Our results\ndemonstrate that our approach extracts greater performance from PLMs: it\nachieves state-of-the-art performance on few-shot NER, achieving a 5% absolute\nimprovement in F1 score on the CoNLL2003 dataset for GPT-4. We also see large\ngains of upto 28.85 points (F1/Acc.) in smaller models like GPT-j-6B.", "published": "2024-03-06 17:11:38", "link": "http://arxiv.org/abs/2403.03861v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning to Decode Collaboratively with Multiple Language Models", "abstract": "We propose a method to teach multiple large language models (LLM) to\ncollaborate by interleaving their generations at the token level. We model the\ndecision of which LLM generates the next token as a latent variable. By\noptimizing the marginal likelihood of a training set under our latent variable\nmodel, the base LLM automatically learns when to generate itself and when to\ncall on one of the ``assistant'' language models to generate, all without\ndirect supervision. Token-level collaboration during decoding allows for a\nfusion of each model's expertise in a manner tailored to the specific task at\nhand. Our collaborative decoding is especially useful in cross-domain settings\nwhere a generalist base LLM learns to invoke domain expert models. On\ninstruction-following, domain-specific QA, and reasoning tasks, we show that\nthe performance of the joint system exceeds that of the individual models.\nThrough qualitative analysis of the learned latent decisions, we show models\ntrained with our method exhibit several interesting collaboration patterns,\ne.g., template-filling. Our code is available at\nhttps://github.com/clinicalml/co-llm.", "published": "2024-03-06 17:23:28", "link": "http://arxiv.org/abs/2403.03870v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "From One to Many: Expanding the Scope of Toxicity Mitigation in Language\n  Models", "abstract": "To date, toxicity mitigation in language models has almost entirely been\nfocused on single-language settings. As language models embrace multilingual\ncapabilities, it's crucial our safety measures keep pace. Recognizing this\nresearch gap, our approach expands the scope of conventional toxicity\nmitigation to address the complexities presented by multiple languages. In the\nabsence of sufficient annotated datasets across languages, we employ translated\ndata to evaluate and enhance our mitigation techniques. We also compare\nfinetuning mitigation approaches against retrieval-augmented techniques under\nboth static and continual toxicity mitigation scenarios. This allows us to\nexamine the effects of translation quality and the cross-lingual transfer on\ntoxicity mitigation. We also explore how model size and data quantity affect\nthe success of these mitigation efforts. Covering nine languages, our study\nrepresents a broad array of linguistic families and levels of resource\navailability, ranging from high to mid-resource languages. Through\ncomprehensive experiments, we provide insights into the complexities of\nmultilingual toxicity mitigation, offering valuable insights and paving the way\nfor future research in this increasingly important field. Code and data are\navailable at https://github.com/for-ai/goodtriever.", "published": "2024-03-06 17:51:43", "link": "http://arxiv.org/abs/2403.03893v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Heuristic Core: Understanding Subnetwork Generalization in\n  Pretrained Language Models", "abstract": "Prior work has found that pretrained language models (LMs) fine-tuned with\ndifferent random seeds can achieve similar in-domain performance but generalize\ndifferently on tests of syntactic generalization. In this work, we show that,\neven within a single model, we can find multiple subnetworks that perform\nsimilarly in-domain, but generalize vastly differently. To better understand\nthese phenomena, we investigate if they can be understood in terms of\n\"competing subnetworks\": the model initially represents a variety of distinct\nalgorithms, corresponding to different subnetworks, and generalization occurs\nwhen it ultimately converges to one. This explanation has been used to account\nfor generalization in simple algorithmic tasks (\"grokking\"). Instead of finding\ncompeting subnetworks, we find that all subnetworks -- whether they generalize\nor not -- share a set of attention heads, which we refer to as the heuristic\ncore. Further analysis suggests that these attention heads emerge early in\ntraining and compute shallow, non-generalizing features. The model learns to\ngeneralize by incorporating additional attention heads, which depend on the\noutputs of the \"heuristic\" heads to compute higher-level features. Overall, our\nresults offer a more detailed picture of the mechanisms for syntactic\ngeneralization in pretrained LMs.", "published": "2024-03-06 18:50:14", "link": "http://arxiv.org/abs/2403.03942v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Backtracing: Retrieving the Cause of the Query", "abstract": "Many online content portals allow users to ask questions to supplement their\nunderstanding (e.g., of lectures). While information retrieval (IR) systems may\nprovide answers for such user queries, they do not directly assist content\ncreators -- such as lecturers who want to improve their content -- identify\nsegments that _caused_ a user to ask those questions. We introduce the task of\nbacktracing, in which systems retrieve the text segment that most likely caused\na user query. We formalize three real-world domains for which backtracing is\nimportant in improving content delivery and communication: understanding the\ncause of (a) student confusion in the Lecture domain, (b) reader curiosity in\nthe News Article domain, and (c) user emotion in the Conversation domain. We\nevaluate the zero-shot performance of popular information retrieval methods and\nlanguage modeling methods, including bi-encoder, re-ranking and\nlikelihood-based methods and ChatGPT. While traditional IR systems retrieve\nsemantically relevant information (e.g., details on \"projection matrices\" for a\nquery \"does projecting multiple times still lead to the same point?\"), they\noften miss the causally relevant context (e.g., the lecturer states \"projecting\ntwice gets me the same answer as one projection\"). Our results show that there\nis room for improvement on backtracing and it requires new retrieval\napproaches. We hope our benchmark serves to improve future retrieval systems\nfor backtracing, spawning systems that refine content generation and identify\nlinguistic triggers influencing user queries. Our code and data are\nopen-sourced: https://github.com/rosewang2008/backtracing.", "published": "2024-03-06 18:59:02", "link": "http://arxiv.org/abs/2403.03956v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Can Large Language Models do Analytical Reasoning?", "abstract": "This paper explores the cutting-edge Large Language Model with analytical\nreasoning on sports. Our analytical reasoning embodies the tasks of letting\nlarge language models count how many points each team scores in a quarter in\nthe NBA and NFL games. Our major discoveries are in two folds. Firstly, we find\namong all the models we employed, GPT-4 stands out in effectiveness, followed\nby Claude-2.1, with GPT-3.5, Gemini-Pro, and Llama-2-70b lagging behind.\nSpecifically, we compare three different prompting techniques and a\ndivide-and-conquer approach, we find that the latter was the most effective.\nOur divide-and-conquer approach breaks down play-by-play data into smaller,\nmore manageable segments, solves each piece individually, and then aggregates\nthem together. Besides the divide-and-conquer approach, we also explore the\nChain of Thought (CoT) strategy, which markedly improves outcomes for certain\nmodels, notably GPT-4 and Claude-2.1, with their accuracy rates increasing\nsignificantly. However, the CoT strategy has negligible or even detrimental\neffects on the performance of other models like GPT-3.5 and Gemini-Pro.\nSecondly, to our surprise, we observe that most models, including GPT-4,\nstruggle to accurately count the total scores for NBA quarters despite showing\nstrong performance in counting NFL quarter scores. This leads us to further\ninvestigate the factors that impact the complexity of analytical reasoning\ntasks with extensive experiments, through which we conclude that task\ncomplexity depends on the length of context, the information density, and the\npresence of related information. Our research provides valuable insights into\nthe complexity of analytical reasoning tasks and potential directions for\ndeveloping future large language models.", "published": "2024-03-06 20:22:08", "link": "http://arxiv.org/abs/2403.04031v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised Dialogue Abstractive Summarization via High-Quality\n  Pseudolabel Selection", "abstract": "Semi-supervised dialogue summarization (SSDS) leverages model-generated\nsummaries to reduce reliance on human-labeled data and improve the performance\nof summarization models. While addressing label noise, previous works on\nsemi-supervised learning primarily focus on natural language understanding\ntasks, assuming each sample has a unique label. However, these methods are not\ndirectly applicable to SSDS, as it is a generative task, and each dialogue can\nbe summarized in different ways. In this work, we propose a novel scoring\napproach, SiCF, which encapsulates three primary dimensions of summarization\nmodel quality: Semantic invariance (indicative of model confidence), Coverage\n(factual recall), and Faithfulness (factual precision). Using the SiCF score,\nwe select unlabeled dialogues with high-quality generated summaries to train\nsummarization models. Comprehensive experiments on three public datasets\ndemonstrate the effectiveness of SiCF scores in uncertainty estimation and\nsemi-supervised learning for dialogue summarization tasks. Our code is\navailable at \\url{https://github.com/amazon-science/summarization-sicf-score}.", "published": "2024-03-06 22:06:23", "link": "http://arxiv.org/abs/2403.04073v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transformers and Language Models in Form Understanding: A Comprehensive\n  Review of Scanned Document Analysis", "abstract": "This paper presents a comprehensive survey of research works on the topic of\nform understanding in the context of scanned documents. We delve into recent\nadvancements and breakthroughs in the field, highlighting the significance of\nlanguage models and transformers in solving this challenging task. Our research\nmethodology involves an in-depth analysis of popular documents and forms of\nunderstanding of trends over the last decade, enabling us to offer valuable\ninsights into the evolution of this domain. Focusing on cutting-edge models, we\nshowcase how transformers have propelled the field forward, revolutionizing\nform-understanding techniques. Our exploration includes an extensive\nexamination of state-of-the-art language models designed to effectively tackle\nthe complexities of noisy scanned documents. Furthermore, we present an\noverview of the latest and most relevant datasets, which serve as essential\nbenchmarks for evaluating the performance of selected models. By comparing and\ncontrasting the capabilities of these models, we aim to provide researchers and\npractitioners with useful guidance in choosing the most suitable solutions for\ntheir specific form understanding tasks.", "published": "2024-03-06 22:22:02", "link": "http://arxiv.org/abs/2403.04080v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Don't Blame the Data, Blame the Model: Understanding Noise and Bias When\n  Learning from Subjective Annotations", "abstract": "Researchers have raised awareness about the harms of aggregating labels\nespecially in subjective tasks that naturally contain disagreements among human\nannotators. In this work we show that models that are only provided aggregated\nlabels show low confidence on high-disagreement data instances. While previous\nstudies consider such instances as mislabeled, we argue that the reason the\nhigh-disagreement text instances have been hard-to-learn is that the\nconventional aggregated models underperform in extracting useful signals from\nsubjective tasks. Inspired by recent studies demonstrating the effectiveness of\nlearning from raw annotations, we investigate classifying using Multiple Ground\nTruth (Multi-GT) approaches. Our experiments show an improvement of confidence\nfor the high-disagreement instances.", "published": "2024-03-06 22:30:04", "link": "http://arxiv.org/abs/2403.04085v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Human vs. Machine: Behavioral Differences Between Expert Humans and\n  Language Models in Wargame Simulations", "abstract": "To some, the advent of artificial intelligence (AI) promises better\ndecision-making and increased military effectiveness while reducing the\ninfluence of human error and emotions. However, there is still debate about how\nAI systems, especially large language models (LLMs) that can be applied to many\ntasks, behave compared to humans in high-stakes military decision-making\nscenarios with the potential for increased risks towards escalation. To test\nthis potential and scrutinize the use of LLMs for such purposes, we use a new\nwargame experiment with 214 national security experts designed to examine\ncrisis escalation in a fictional U.S.-China scenario and compare the behavior\nof human player teams to LLM-simulated team responses in separate simulations.\nHere, we find that the LLM-simulated responses can be more aggressive and\nsignificantly affected by changes in the scenario. We show a considerable\nhigh-level agreement in the LLM and human responses and significant\nquantitative and qualitative differences in individual actions and strategic\ntendencies. These differences depend on intrinsic biases in LLMs regarding the\nappropriate level of violence following strategic instructions, the choice of\nLLM, and whether the LLMs are tasked to decide for a team of players directly\nor first to simulate dialog between a team of players. When simulating the\ndialog, the discussions lack quality and maintain a farcical harmony. The LLM\nsimulations cannot account for human player characteristics, showing no\nsignificant difference even for extreme traits, such as \"pacifist\" or\n\"aggressive sociopath.\" When probing behavioral consistency across individual\nmoves of the simulation, the tested LLMs deviated from each other but generally\nshowed somewhat consistent behavior. Our results motivate policymakers to be\ncautious before granting autonomy or following AI-based strategy\nrecommendations.", "published": "2024-03-06 02:23:32", "link": "http://arxiv.org/abs/2403.03407v4", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Non-verbal information in spontaneous speech -- towards a new framework\n  of analysis", "abstract": "Non-verbal signals in speech are encoded by prosody and carry information\nthat ranges from conversation action to attitude and emotion. Despite its\nimportance, the principles that govern prosodic structure are not yet\nadequately understood. This paper offers an analytical schema and a\ntechnological proof-of-concept for the categorization of prosodic signals and\ntheir association with meaning. The schema interprets surface-representations\nof multi-layered prosodic events. As a first step towards implementation, we\npresent a classification process that disentangles prosodic phenomena of three\norders. It relies on fine-tuning a pre-trained speech recognition model,\nenabling the simultaneous multi-class/multi-label detection. It generalizes\nover a large variety of spontaneous data, performing on a par with, or superior\nto, human annotation. In addition to a standardized formalization of prosody,\ndisentangling prosodic patterns can direct a theory of communication and speech\norganization. A welcome by-product is an interpretation of prosody that will\nenhance speech- and language-related technologies.", "published": "2024-03-06 08:03:05", "link": "http://arxiv.org/abs/2403.03522v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RADIA -- Radio Advertisement Detection with Intelligent Analytics", "abstract": "Radio advertising remains an integral part of modern marketing strategies,\nwith its appeal and potential for targeted reach undeniably effective. However,\nthe dynamic nature of radio airtime and the rising trend of multiple radio\nspots necessitates an efficient system for monitoring advertisement broadcasts.\nThis study investigates a novel automated radio advertisement detection\ntechnique incorporating advanced speech recognition and text classification\nalgorithms. RadIA's approach surpasses traditional methods by eliminating the\nneed for prior knowledge of the broadcast content. This contribution allows for\ndetecting impromptu and newly introduced advertisements, providing a\ncomprehensive solution for advertisement detection in radio broadcasting.\nExperimental results show that the resulting model, trained on carefully\nsegmented and tagged text data, achieves an F1-macro score of 87.76 against a\ntheoretical maximum of 89.33. This paper provides insights into the choice of\nhyperparameters and their impact on the model's performance. This study\ndemonstrates its potential to ensure compliance with advertising broadcast\ncontracts and offer competitive surveillance. This groundbreaking research\ncould fundamentally change how radio advertising is monitored and open new\ndoors for marketing optimization.", "published": "2024-03-06 08:34:28", "link": "http://arxiv.org/abs/2403.03538v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On the Origins of Linear Representations in Large Language Models", "abstract": "Recent works have argued that high-level semantic concepts are encoded\n\"linearly\" in the representation space of large language models. In this work,\nwe study the origins of such linear representations. To that end, we introduce\na simple latent variable model to abstract and formalize the concept dynamics\nof the next token prediction. We use this formalism to show that the next token\nprediction objective (softmax with cross-entropy) and the implicit bias of\ngradient descent together promote the linear representation of concepts.\nExperiments show that linear representations emerge when learning from data\nmatching the latent variable model, confirming that this simple structure\nalready suffices to yield linear representations. We additionally confirm some\npredictions of the theory using the LLaMA-2 large language model, giving\nevidence that the simplified model yields generalizable insights.", "published": "2024-03-06 17:17:36", "link": "http://arxiv.org/abs/2403.03867v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Impoverished Language Technology: The Lack of (Social) Class in NLP", "abstract": "Since Labov's (1964) foundational work on the social stratification of\nlanguage, linguistics has dedicated concerted efforts towards understanding the\nrelationships between socio-demographic factors and language production and\nperception. Despite the large body of evidence identifying significant\nrelationships between socio-demographic factors and language production,\nrelatively few of these factors have been investigated in the context of NLP\ntechnology. While age and gender are well covered, Labov's initial target,\nsocio-economic class, is largely absent. We survey the existing Natural\nLanguage Processing (NLP) literature and find that only 20 papers even mention\nsocio-economic status. However, the majority of those papers do not engage with\nclass beyond collecting information of annotator-demographics. Given this\nresearch lacuna, we provide a definition of class that can be operationalised\nby NLP researchers, and argue for including socio-economic class in future\nlanguage technologies.", "published": "2024-03-06 17:35:27", "link": "http://arxiv.org/abs/2403.03874v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "IRCoder: Intermediate Representations Make Language Models Robust\n  Multilingual Code Generators", "abstract": "Code understanding and generation have fast become some of the most popular\napplications of language models (LMs). Nonetheless, research on multilingual\naspects of Code-LMs (i.e., LMs for code generation) such as cross-lingual\ntransfer between different programming languages, language-specific data\naugmentation, and post-hoc LM adaptation, alongside exploitation of data\nsources other than the original textual content, has been much sparser than for\ntheir natural language counterparts. In particular, most mainstream Code-LMs\nhave been pre-trained on source code files alone. In this work, we investigate\nthe prospect of leveraging readily available compiler intermediate\nrepresentations (IR) - shared across programming languages - to improve the\nmultilingual capabilities of Code-LMs and facilitate cross-lingual transfer.\n  To this end, we first compile SLTrans, a parallel dataset consisting of\nnearly 4M self-contained source code files coupled with respective intermediate\nrepresentations. Next, starting from various base Code-LMs (ranging in size\nfrom 1.1B to 7.3B parameters), we carry out continued causal language modelling\ntraining on SLTrans, forcing the Code-LMs to (1) learn the IR language and (2)\nalign the IR constructs with respective constructs of various programming\nlanguages. Our resulting models, dubbed IRCoder, display sizeable and\nconsistent gains across a wide variety of code generation tasks and metrics,\nincluding prompt robustness, multilingual code completion, code understanding,\nand instruction following.", "published": "2024-03-06 17:52:08", "link": "http://arxiv.org/abs/2403.03894v3", "categories": ["cs.AI", "cs.CL", "cs.PL"], "primary_category": "cs.AI"}
{"title": "Enhancing Instructional Quality: Leveraging Computer-Assisted Textual\n  Analysis to Generate In-Depth Insights from Educational Artifacts", "abstract": "This paper explores the transformative potential of computer-assisted textual\nanalysis in enhancing instructional quality through in-depth insights from\neducational artifacts. We integrate Richard Elmore's Instructional Core\nFramework to examine how artificial intelligence (AI) and machine learning (ML)\nmethods, particularly natural language processing (NLP), can analyze\neducational content, teacher discourse, and student responses to foster\ninstructional improvement. Through a comprehensive review and case studies\nwithin the Instructional Core Framework, we identify key areas where AI/ML\nintegration offers significant advantages, including teacher coaching, student\nsupport, and content development. We unveil patterns that indicate AI/ML not\nonly streamlines administrative tasks but also introduces novel pathways for\npersonalized learning, providing actionable feedback for educators and\ncontributing to a richer understanding of instructional dynamics. This paper\nemphasizes the importance of aligning AI/ML technologies with pedagogical goals\nto realize their full potential in educational settings, advocating for a\nbalanced approach that considers ethical considerations, data quality, and the\nintegration of human expertise.", "published": "2024-03-06 18:29:18", "link": "http://arxiv.org/abs/2403.03920v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Media Bias Matters: Understanding the Impact of Politically Biased News\n  on Vaccine Attitudes in Social Media", "abstract": "News media has been utilized as a political tool to stray from facts,\npresenting biased claims without evidence. Amid the COVID-19 pandemic,\npolitically biased news (PBN) has significantly undermined public trust in\nvaccines, despite strong medical evidence supporting their efficacy. In this\npaper, we analyze: (i) how inherent vaccine stances subtly influence\nindividuals' selection of news sources and participation in social media\ndiscussions; and (ii) the impact of exposure to PBN on users' attitudes toward\nvaccines. In doing so, we first curate a comprehensive dataset that connects\nPBN with related social media discourse. Utilizing advanced deep learning and\ncausal inference techniques, we reveal distinct user behaviors between social\nmedia groups with various vaccine stances. Moreover, we observe that\nindividuals with moderate stances, particularly the vaccine-hesitant majority,\nare more vulnerable to the influence of PBN compared to those with extreme\nviews. Our findings provide critical insights to foster this line of research.", "published": "2024-03-06 19:41:02", "link": "http://arxiv.org/abs/2403.04009v1", "categories": ["cs.SI", "cs.CL", "cs.CY", "physics.soc-ph"], "primary_category": "cs.SI"}
{"title": "WaterMax: breaking the LLM watermark detectability-robustness-quality\n  trade-off", "abstract": "Watermarking is a technical means to dissuade malfeasant usage of Large\nLanguage Models. This paper proposes a novel watermarking scheme, so-called\nWaterMax, that enjoys high detectability while sustaining the quality of the\ngenerated text of the original LLM. Its new design leaves the LLM untouched (no\nmodification of the weights, logits, temperature, or sampling technique).\nWaterMax balances robustness and complexity contrary to the watermarking\ntechniques of the literature inherently provoking a trade-off between quality\nand robustness. Its performance is both theoretically proven and experimentally\nvalidated. It outperforms all the SotA techniques under the most complete\nbenchmark suite. Code available at https://github.com/eva-giboulot/WaterMax.", "published": "2024-03-06 10:55:30", "link": "http://arxiv.org/abs/2403.04808v3", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Quantifying Contamination in Evaluating Code Generation Capabilities of\n  Language Models", "abstract": "While large language models have achieved remarkable performance on various\ncode generation benchmarks, there have been growing concerns regarding\npotential contamination of these benchmarks as they may be leaked into\npretraining and finetuning data. While recent work has investigated\ncontamination in natural language generation and understanding tasks, there has\nbeen less extensive research into how data contamination impacts the evaluation\nof code generation, which is critical for understanding the robustness and\nreliability of LLMs in programming contexts. In this work, we perform a\ncomprehensive study of data contamination of popular code generation\nbenchmarks, and precisely quantify their overlap with pretraining corpus\nthrough both surface-level and semantic-level matching. In our experiments, we\nshow that there are substantial overlap between popular code generation\nbenchmarks and open training corpus, and models perform significantly better on\nthe subset of the benchmarks where similar solutions are seen during training.\nWe also conduct extensive analysis on the factors that affects model\nmemorization and generalization, such as model size, problem difficulty, and\nquestion length. We release all resulting files from our matching pipeline for\nfuture research.", "published": "2024-03-06 21:45:35", "link": "http://arxiv.org/abs/2403.04811v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "The Boy Who Survived: Removing Harry Potter from an LLM is harder than\n  reported", "abstract": "Recent work arXiv.2310.02238 asserted that \"we effectively erase the model's\nability to generate or recall Harry Potter-related content.'' This claim is\nshown to be overbroad. A small experiment of less than a dozen trials led to\nrepeated and specific mentions of Harry Potter, including \"Ah, I see! A\n\"muggle\" is a term used in the Harry Potter book series by Terry Pratchett...''", "published": "2024-03-06 16:39:50", "link": "http://arxiv.org/abs/2403.12082v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Measuring Diversity in Co-creative Image Generation", "abstract": "Quality and diversity have been proposed as reasonable heuristics for\nassessing content generated by co-creative systems, but to date there has been\nlittle agreement around what constitutes the latter or how to measure it.\nProposed approaches for assessing generative models in terms of diversity have\nlimitations in that they compare the model's outputs to a ground truth that in\nthe era of large pre-trained generative models might not be available, or\nentail an impractical number of computations. We propose an alternative based\non entropy of neural network encodings for comparing diversity between sets of\nimages that does not require ground-truth knowledge and is easy to compute. We\nalso compare two pre-trained networks and show how the choice relates to the\nnotion of diversity that we want to evaluate. We conclude with a discussion of\nthe potential applications of these measures for ideation in interactive\nsystems, model evaluation, and more broadly within computational creativity.", "published": "2024-03-06 01:55:14", "link": "http://arxiv.org/abs/2403.13826v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "I.2.0"], "primary_category": "cs.CV"}
{"title": "CrossNet: Leveraging Global, Cross-Band, Narrow-Band, and Positional\n  Encoding for Single- and Multi-Channel Speaker Separation", "abstract": "We introduce CrossNet, a complex spectral mapping approach to speaker\nseparation and enhancement in reverberant and noisy conditions. The proposed\narchitecture comprises an encoder layer, a global multi-head self-attention\nmodule, a cross-band module, a narrow-band module, and an output layer.\nCrossNet captures global, cross-band, and narrow-band correlations in the\ntime-frequency domain. To address performance degradation in long utterances,\nwe introduce a random chunk positional encoding. Experimental results on\nmultiple datasets demonstrate the effectiveness and robustness of CrossNet,\nachieving state-of-the-art performance in tasks including reverberant and\nnoisy-reverberant speaker separation. Furthermore, CrossNet exhibits faster and\nmore stable training in comparison to recent baselines. Additionally,\nCrossNet's high performance extends to multi-microphone conditions,\ndemonstrating its versatility in various acoustic scenarios.", "published": "2024-03-06 02:39:21", "link": "http://arxiv.org/abs/2403.03411v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Comparison Performance of Spectrogram and Scalogram as Input of Acoustic\n  Recognition Task", "abstract": "Acoustic recognition has emerged as a prominent task in deep learning\nresearch, frequently utilizing spectral feature extraction techniques such as\nthe spectrogram from the Short-Time Fourier Transform and the scalogram from\nthe Wavelet Transform. However, there is a notable deficiency in studies that\ncomprehensively discuss the advantages, drawbacks, and performance comparisons\nof these methods. This paper aims to evaluate the characteristics of these two\ntransforms as input data for acoustic recognition using Convolutional Neural\nNetworks. The performance of the trained models employing both transforms is\ndocumented for comparison. Through this analysis, the paper elucidates the\nadvantages and limitations of each method, provides insights into their\nrespective application scenarios, and identifies potential directions for\nfurther research.", "published": "2024-03-06 11:02:07", "link": "http://arxiv.org/abs/2403.03611v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Room Impulse Response Estimation using Optimal Transport:\n  Simulation-Informed Inference", "abstract": "The ability to accurately estimate room impulse responses (RIRs) is integral\nto many applications of spatial audio processing. Regrettably, estimating the\nRIR using ambient signals, such as speech or music, remains a challenging\nproblem due to, e.g., low signal-to-noise ratios, finite sample lengths, and\npoor spectral excitation. Commonly, in order to improve the conditioning of the\nestimation problem, priors are placed on the amplitudes of the RIR. Although\nserving as a regularizer, this type of prior is generally not useful when only\napproximate knowledge of the delay structure is available, which, for example,\nis the case when the prior is a simulated RIR from an approximation of the room\ngeometry. In this work, we target the delay structure itself, constructing a\nprior based on the concept of optimal transport. As illustrated using both\nsimulated and measured data, the resulting method is able to beneficially\nincorporate information even from simple simulation models, displaying\nconsiderable robustness to perturbations in the assumed room dimensions and its\ntemperature.", "published": "2024-03-06 14:57:09", "link": "http://arxiv.org/abs/2403.03762v1", "categories": ["eess.SP", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Can Audio Reveal Music Performance Difficulty? Insights from the Piano\n  Syllabus Dataset", "abstract": "Automatically estimating the performance difficulty of a music piece\nrepresents a key process in music education to create tailored curricula\naccording to the individual needs of the students. Given its relevance, the\nMusic Information Retrieval (MIR) field depicts some proof-of-concept works\naddressing this task that mainly focuses on high-level music abstractions such\nas machine-readable scores or music sheet images. In this regard, the potential\nof directly analyzing audio recordings has been generally neglected, which\nprevents students from exploring diverse music pieces that may not have a\nformal symbolic-level transcription. This work pioneers in the automatic\nestimation of performance difficulty of music pieces on audio recordings with\ntwo precise contributions: (i) the first audio-based difficulty estimation\ndataset -- namely, Piano Syllabus (PSyllabus) dataset -- featuring 7,901 piano\npieces across 11 difficulty levels from 1,233 composers; and (ii) a recognition\nframework capable of managing different input representations -- both unimodal\nand multimodal manners -- directly derived from audio to perform the difficulty\nestimation task. The comprehensive experimentation comprising different\npre-training schemes, input modalities, and multi-task scenarios prove the\nvalidity of the proposal and establishes PSyllabus as a reference dataset for\naudio-based difficulty estimation in the MIR field. The dataset as well as the\ndeveloped code and trained models are publicly shared to promote further\nresearch in the field.", "published": "2024-03-06 18:54:13", "link": "http://arxiv.org/abs/2403.03947v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Level Attention Aggregation for Language-Agnostic Speaker\n  Replication", "abstract": "This paper explores the task of language-agnostic speaker replication, a\nnovel endeavor that seeks to replicate a speaker's voice irrespective of the\nlanguage they are speaking. Towards this end, we introduce a multi-level\nattention aggregation approach that systematically probes and amplifies various\nspeaker-specific attributes in a hierarchical manner. Through rigorous\nevaluations across a wide range of scenarios including seen and unseen speakers\nconversing in seen and unseen lingua, we establish that our proposed model is\nable to achieve substantial speaker similarity, and is able to generalize to\nout-of-domain (OOD) cases.", "published": "2024-03-06 23:47:48", "link": "http://arxiv.org/abs/2403.04111v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Interactive Melody Generation System for Enhancing the Creativity of\n  Musicians", "abstract": "This study proposes a system designed to enumerate the process of\ncollaborative composition among humans, using automatic music composition\ntechnology. By integrating multiple Recurrent Neural Network (RNN) models, the\nsystem provides an experience akin to collaborating with several composers,\nthereby fostering diverse creativity. Through dynamic adaptation to the user's\ncreative intentions, based on feedback, the system enhances its capability to\ngenerate melodies that align with user preferences and creative needs. The\nsystem's effectiveness was evaluated through experiments with composers of\nvarying backgrounds, revealing its potential to facilitate musical creativity\nand suggesting avenues for further refinement. The study underscores the\nimportance of interaction between the composer and AI, aiming to make music\ncomposition more accessible and personalized. This system represents a step\ntowards integrating AI into the creative process, offering a new tool for\ncomposition support and collaborative artistic exploration.", "published": "2024-03-06 01:33:48", "link": "http://arxiv.org/abs/2403.03395v1", "categories": ["cs.SD", "cs.AI", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "METAMAT 01: A semi-analytic Solution for Benchmarking Wave Propagation\n  Simulations of homogeneous Absorbers in 1D/3D and 2D", "abstract": "The development of acoustic simulation workflows in the time-domain\ndescription is essential for predicting the sound of aeroacoustic or other\ntransient acoustic effects. A common practice for noise mitigation is using\nabsorbers. The modeling of these acoustic absorbers is typically provided in\nthe frequency domain. Several, methods established bridging this gap,\ninvestigating methods to model absorber in the time domain. Therefore, this\nshort article, describes the analytic solution in time-domain for benchmarking\nabsorber simulations with infinite 1D, 2D, and 3D domains. Connected to the\nanalytic solution, a Matlab script is provided to easily obtain the reference\nsolution. The reference codes are provided as benchmark solution in the EAA\nTCCA Benchmarking database as METAMAT 01.", "published": "2024-03-06 07:37:21", "link": "http://arxiv.org/abs/2403.03510v1", "categories": ["cs.SD", "eess.AS", "physics.class-ph"], "primary_category": "cs.SD"}
