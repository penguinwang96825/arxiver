{"title": "Semantic-aware Contrastive Learning for More Accurate Semantic Parsing", "abstract": "Since the meaning representations are detailed and accurate annotations which\nexpress fine-grained sequence-level semtantics, it is usually hard to train\ndiscriminative semantic parsers via Maximum Likelihood Estimation (MLE) in an\nautoregressive fashion. In this paper, we propose a semantic-aware contrastive\nlearning algorithm, which can learn to distinguish fine-grained meaning\nrepresentations and take the overall sequence-level semantic into\nconsideration. Specifically, a multi-level online sampling algorithm is\nproposed to sample confusing and diverse instances. Three semantic-aware\nsimilarity functions are designed to accurately measure the distance between\nmeaning representations as a whole. And a ranked contrastive loss is proposed\nto pull the representations of the semantic-identical instances together and\npush negative instances away. Experiments on two standard datasets show that\nour approach achieves significant improvements over MLE baselines and gets\nstate-of-the-art performances by simply applying semantic-aware contrastive\nlearning on a vanilla Seq2Seq model.", "published": "2023-01-19 07:04:32", "link": "http://arxiv.org/abs/2301.07919v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Author as Character and Narrator: Deconstructing Personal Narratives\n  from the r/AmITheAsshole Reddit Community", "abstract": "In the r/AmITheAsshole subreddit, people anonymously share first person\nnarratives that contain some moral dilemma or conflict and ask the community to\njudge who is at fault (i.e., who is \"the asshole\"). In general, first person\nnarratives are a unique storytelling domain where the author is the narrator\n(the person telling the story) but can also be a character (the person living\nthe story) and, thus, the author has two distinct voices presented in the\nstory. In this study, we identify linguistic and narrative features associated\nwith the author as the character or as a narrator. We use these features to\nanswer the following questions: (1) what makes an asshole character and (2)\nwhat makes an asshole narrator? We extract both Author-as-Character features\n(e.g., demographics, narrative event chain, and emotional arc) and\nAuthor-as-Narrator features (i.e., the style and emotion of the story as a\nwhole) in order to identify which aspects of the narrative are correlated with\nthe final moral judgment. Our work shows that \"assholes\" as Characters frame\nthemselves as lacking agency with a more positive personal arc, while\n\"assholes\" as Narrators will tell emotional and opinionated stories.", "published": "2023-01-19 14:50:36", "link": "http://arxiv.org/abs/2301.08104v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Embeddings Sometimes Contain Typological Generalizations", "abstract": "To what extent can neural network models learn generalizations about language\nstructure, and how do we find out what they have learned? We explore these\nquestions by training neural models for a range of natural language processing\ntasks on a massively multilingual dataset of Bible translations in 1295\nlanguages. The learned language representations are then compared to existing\ntypological databases as well as to a novel set of quantitative syntactic and\nmorphological features obtained through annotation projection. We conclude that\nsome generalizations are surprisingly close to traditional features from\nlinguistic typology, but that most of our models, as well as those of previous\nwork, do not appear to have made linguistically meaningful generalizations.\nCareful attention to details in the evaluation turns out to be essential to\navoid false positives. Furthermore, to encourage continued work in this field,\nwe release several resources covering most or all of the languages in our data:\n(i) multiple sets of language representations, (ii) multilingual word\nembeddings, (iii) projected and predicted syntactic and morphological features,\n(iv) software to provide linguistically sound evaluations of language\nrepresentations.", "published": "2023-01-19 15:09:59", "link": "http://arxiv.org/abs/2301.08115v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JCSE: Contrastive Learning of Japanese Sentence Embeddings and Its\n  Applications", "abstract": "Contrastive learning is widely used for sentence representation learning.\nDespite this prevalence, most studies have focused exclusively on English and\nfew concern domain adaptation for domain-specific downstream tasks, especially\nfor low-resource languages like Japanese, which are characterized by\ninsufficient target domain data and the lack of a proper training strategy. To\novercome this, we propose a novel Japanese sentence representation framework,\nJCSE (derived from ``Contrastive learning of Sentence Embeddings for\nJapanese''), that creates training data by generating sentences and\nsynthesizing them with sentences available in a target domain. Specifically, a\npre-trained data generator is finetuned to a target domain using our collected\ncorpus. It is then used to generate contradictory sentence pairs that are used\nin contrastive learning for adapting a Japanese language model to a specific\ntask in the target domain.\n  Another problem of Japanese sentence representation learning is the\ndifficulty of evaluating existing embedding methods due to the lack of\nbenchmark datasets. Thus, we establish a comprehensive Japanese Semantic\nTextual Similarity (STS) benchmark on which various embedding models are\nevaluated. Based on this benchmark result, multiple embedding methods are\nchosen and compared with JCSE on two domain-specific tasks, STS in a clinical\ndomain and information retrieval in an educational domain. The results show\nthat JCSE achieves significant performance improvement surpassing direct\ntransfer and other training strategies. This empirically demonstrates JCSE's\neffectiveness and practicability for downstream tasks of a low-resource\nlanguage.", "published": "2023-01-19 17:41:46", "link": "http://arxiv.org/abs/2301.08193v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reversing The Twenty Questions Game", "abstract": "Twenty questions is a widely popular verbal game. In recent years, many\ncomputerized versions of this game have been developed in which a user thinks\nof an entity and a computer attempts to guess this entity by asking a series of\nboolean-type (yes/no) questions. In this research, we aim to reverse this game\nby making the computer choose an entity at random. The human aims to guess this\nentity by quizzing the computer with natural language queries which the\ncomputer will then attempt to parse using a boolean question answering model.\nThe game ends when the human is successfully able to guess the entity of the\ncomputer's choice.", "published": "2023-01-19 12:51:59", "link": "http://arxiv.org/abs/2301.08718v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Keyword Embeddings for Query Suggestion", "abstract": "Nowadays, search engine users commonly rely on query suggestions to improve\ntheir initial inputs. Current systems are very good at recommending lexical\nadaptations or spelling corrections to users' queries. However, they often\nstruggle to suggest semantically related keywords given a user's query. The\nconstruction of a detailed query is crucial in some tasks, such as legal\nretrieval or academic search. In these scenarios, keyword suggestion methods\nare critical to guide the user during the query formulation. This paper\nproposes two novel models for the keyword suggestion task trained on scientific\nliterature. Our techniques adapt the architecture of Word2Vec and FastText to\ngenerate keyword embeddings by leveraging documents' keyword co-occurrence.\nAlong with these models, we also present a specially tailored negative sampling\napproach that exploits how keywords appear in academic publications. We devise\na ranking-based evaluation methodology following both known-item and ad-hoc\nsearch scenarios. Finally, we evaluate our proposals against the\nstate-of-the-art word and sentence embedding models showing considerable\nimprovements over the baselines for the tasks.", "published": "2023-01-19 11:13:04", "link": "http://arxiv.org/abs/2301.08006v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Improving Machine Translation with Phrase Pair Injection and Corpus\n  Filtering", "abstract": "In this paper, we show that the combination of Phrase Pair Injection and\nCorpus Filtering boosts the performance of Neural Machine Translation (NMT)\nsystems. We extract parallel phrases and sentences from the pseudo-parallel\ncorpus and augment it with the parallel corpus to train the NMT models. With\nthe proposed approach, we observe an improvement in the Machine Translation\n(MT) system for 3 low-resource language pairs, Hindi-Marathi, English-Marathi,\nand English-Pashto, and 6 translation directions by up to 2.7 BLEU points, on\nthe FLORES test data. These BLEU score improvements are over the models trained\nusing the whole pseudo-parallel corpus augmented with the parallel corpus.", "published": "2023-01-19 11:27:56", "link": "http://arxiv.org/abs/2301.08008v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis for Measuring Hope and Fear from Reddit Posts During\n  the 2022 Russo-Ukrainian Conflict", "abstract": "This paper proposes a novel lexicon-based unsupervised sentimental analysis\nmethod to measure the $``\\textit{hope}\"$ and $``\\textit{fear}\"$ for the 2022\nUkrainian-Russian Conflict. $\\textit{Reddit.com}$ is utilised as the main\nsource of human reactions to daily events during nearly the first three months\nof the conflict. The top 50 $``hot\"$ posts of six different subreddits about\nUkraine and news (Ukraine, worldnews, Ukraina, UkrainianConflict,\nUkraineWarVideoReport, UkraineWarReports) and their relative comments are\nscraped and a data set is created. On this corpus, multiple analyses such as\n(1) public interest, (2) hope/fear score, (3) stock price interaction are\nemployed. We promote using a dictionary approach, which scores the hopefulness\nof every submitted user post. The Latent Dirichlet Allocation (LDA) algorithm\nof topic modelling is also utilised to understand the main issues raised by\nusers and what are the key talking points. Experimental analysis shows that the\nhope strongly decreases after the symbolic and strategic losses of Azovstal\n(Mariupol) and Severodonetsk. Spikes in hope/fear, both positives and\nnegatives, are present after important battles, but also some non-military\nevents, such as Eurovision and football games.", "published": "2023-01-19 22:43:59", "link": "http://arxiv.org/abs/2301.08347v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Batch Prompting: Efficient Inference with Large Language Model APIs", "abstract": "Performing inference on large volumes of samples with large language models\n(LLMs) can be computationally and financially costly in industry and real-world\nuse. We propose batch prompting, a simple yet effective prompting approach that\nenables the LLM to run inference in batches, instead of one sample at a time.\nOur method reduces both token and time costs while retaining downstream\nperformance. We theoretically demonstrate that under a few-shot in-context\nlearning setting, the inference costs decrease almost inverse linearly with the\nnumber of samples in each batch. We extensively validate the effectiveness of\nbatch prompting on ten datasets across commonsense QA, arithmetic reasoning,\nand NLI/NLU: batch prompting significantly~(up to 5x with six samples in batch)\nreduces the LLM (Codex) inference token and time costs while achieving better\nor comparable performance. For state-of-the-art Chat-based LLMs, e.g., GPT-3.5\nand GPT-4, we show the benefits of batch prompting also hold. Further analysis\nshows that the number of samples in each batch and the complexity of tasks\naffect its performance. Moreover, batch prompting can be applied across\ndifferent reasoning methods using LLMs. Our code can be found at the site\nhttps://github.com/xlang-ai/batch-prompting.", "published": "2023-01-19 02:29:23", "link": "http://arxiv.org/abs/2301.08721v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Continuously Reliable Detection of New-Normal Misinformation: Semantic\n  Masking and Contrastive Smoothing in High-Density Latent Regions", "abstract": "Toxic misinformation campaigns have caused significant societal harm, e.g.,\naffecting elections and COVID-19 information awareness. Unfortunately, despite\nsuccesses of (gold standard) retrospective studies of misinformation that\nconfirmed their harmful effects after the fact, they arrive too late for timely\nintervention and reduction of such harm. By design, misinformation evades\nretrospective classifiers by exploiting two properties we call new-normal: (1)\nnever-seen-before novelty that cause inescapable generalization challenges for\nprevious classifiers, and (2) massive but short campaigns that end before they\ncan be manually annotated for new classifier training. To tackle these\nchallenges, we propose UFIT, which combines two techniques: semantic masking of\nstrong signal keywords to reduce overfitting, and intra-proxy smoothness\nregularization of high-density regions in the latent space to improve\nreliability and maintain accuracy. Evaluation of UFIT on public new-normal\nmisinformation data shows over 30% improvement over existing approaches on\nfuture (and unseen) campaigns. To the best of our knowledge, UFIT is the first\nsuccessful effort to achieve such high level of generalization on new-normal\nmisinformation data with minimal concession (1 to 5%) of accuracy compared to\noracles trained with full knowledge of all campaigns.", "published": "2023-01-19 10:16:56", "link": "http://arxiv.org/abs/2301.07981v1", "categories": ["cs.LG", "cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.LG"}
{"title": "THLNet: two-stage heterogeneous lightweight network for monaural speech\n  enhancement", "abstract": "In this paper, we propose a two-stage heterogeneous lightweight network for\nmonaural speech enhancement. Specifically, we design a novel two-stage\nframework consisting of a coarse-grained full-band mask estimation stage and a\nfine-grained low-frequency refinement stage. Instead of using a hand-designed\nreal-valued filter, we use a novel learnable complex-valued rectangular\nbandwidth (LCRB) filter bank as an extractor of compact features. Furthermore,\nconsidering the respective characteristics of the proposed two-stage task, we\nused a heterogeneous structure, i.e., a U-shaped subnetwork as the backbone of\nCoarseNet and a single-scale subnetwork as the backbone of FineNet. We\nconducted experiments on the VoiceBank + DEMAND and DNS datasets to evaluate\nthe proposed approach. The experimental results show that the proposed method\noutperforms the current state-of-the-art methods, while maintaining relatively\nsmall model size and low computational complexity.", "published": "2023-01-19 08:17:22", "link": "http://arxiv.org/abs/2301.07939v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Warning: Humans Cannot Reliably Detect Speech Deepfakes", "abstract": "Speech deepfakes are artificial voices generated by machine learning models.\nPrevious literature has highlighted deepfakes as one of the biggest security\nthreats arising from progress in artificial intelligence due to their potential\nfor misuse. However, studies investigating human detection capabilities are\nlimited. We presented genuine and deepfake audio to n = 529 individuals and\nasked them to identify the deepfakes. We ran our experiments in English and\nMandarin to understand if language affects detection performance and\ndecision-making rationale. We found that detection capability is unreliable.\nListeners only correctly spotted the deepfakes 73% of the time, and there was\nno difference in detectability between the two languages. Increasing listener\nawareness by providing examples of speech deepfakes only improves results\nslightly. As speech synthesis algorithms improve and become more realistic, we\ncan expect the detection task to become harder. The difficulty of detecting\nspeech deepfakes confirms their potential for misuse and signals that defenses\nagainst this threat are needed.", "published": "2023-01-19 00:17:48", "link": "http://arxiv.org/abs/2301.07829v2", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "SpotHitPy: A Study For ML-Based Song Hit Prediction Using Spotify", "abstract": "In this study, we approached the Hit Song Prediction problem, which aims to\npredict which songs will become Billboard hits. We gathered a dataset of nearly\n18500 hit and non-hit songs and extracted their audio features using the\nSpotify Web API. We test four machine-learning models on our dataset. We were\nable to predict the Billboard success of a song with approximately 86\\%\naccuracy. The most succesful algorithms were Random Forest and Support Vector\nMachine.", "published": "2023-01-19 10:13:52", "link": "http://arxiv.org/abs/2301.07978v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "From English to More Languages: Parameter-Efficient Model Reprogramming\n  for Cross-Lingual Speech Recognition", "abstract": "In this work, we propose a new parameter-efficient learning framework based\non neural model reprogramming for cross-lingual speech recognition, which can\n\\textbf{re-purpose} well-trained English automatic speech recognition (ASR)\nmodels to recognize the other languages. We design different auxiliary neural\narchitectures focusing on learnable pre-trained feature enhancement that, for\nthe first time, empowers model reprogramming on ASR. Specifically, we\ninvestigate how to select trainable components (i.e., encoder) of a\nconformer-based RNN-Transducer, as a frozen pre-trained backbone. Experiments\non a seven-language multilingual LibriSpeech speech (MLS) task show that model\nreprogramming only requires 4.2% (11M out of 270M) to 6.8% (45M out of 660M) of\nits original trainable parameters from a full ASR model to perform competitive\nresults in a range of 11.9% to 8.1% WER averaged across different languages. In\naddition, we discover different setups to make large-scale pre-trained ASR\nsucceed in both monolingual and multilingual speech recognition. Our methods\noutperform existing ASR tuning architectures and their extension with\nself-supervised losses (e.g., w2v-bert) in terms of lower WER and better\ntraining efficiency.", "published": "2023-01-19 02:37:56", "link": "http://arxiv.org/abs/2301.07851v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
