{"title": "FreshLLMs: Refreshing Large Language Models with Search Engine\n  Augmentation", "abstract": "Most large language models (LLMs) are trained once and never updated; thus,\nthey lack the ability to dynamically adapt to our ever-changing world. In this\nwork, we perform a detailed study of the factuality of LLM-generated text in\nthe context of answering questions that test current world knowledge.\nSpecifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a\ndiverse range of question and answer types, including questions that require\nfast-changing world knowledge as well as questions with false premises that\nneed to be debunked. We benchmark a diverse array of both closed and\nopen-source LLMs under a two-mode evaluation procedure that allows us to\nmeasure both correctness and hallucination. Through human evaluations involving\nmore than 50K judgments, we shed light on limitations of these models and\ndemonstrate significant room for improvement: for instance, all models\n(regardless of model size) struggle on questions that involve fast-changing\nknowledge and false premises. Motivated by these results, we present\nFreshPrompt, a simple few-shot prompting method that substantially boosts the\nperformance of an LLM on FreshQA by incorporating relevant and up-to-date\ninformation retrieved from a search engine into the prompt. Our experiments\nshow that FreshPrompt outperforms both competing search engine-augmented\nprompting methods such as Self-Ask (Press et al., 2022) as well as commercial\nsystems such as Perplexity.AI. Further analysis of FreshPrompt reveals that\nboth the number of retrieved evidences and their order play a key role in\ninfluencing the correctness of LLM-generated answers. Additionally, instructing\nthe LLM to generate concise and direct answers helps reduce hallucination\ncompared to encouraging more verbose answers. To facilitate future work, we\nrelease FreshQA at github.com/freshllms/freshqa and commit to updating it at\nregular intervals.", "published": "2023-10-05 00:04:12", "link": "http://arxiv.org/abs/2310.03214v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models be Good Path Planners? A Benchmark and\n  Investigation on Spatial-temporal Reasoning", "abstract": "Large language models (LLMs) have achieved remarkable success across a wide\nspectrum of tasks; however, they still face limitations in scenarios that\ndemand long-term planning and spatial reasoning. To facilitate this line of\nresearch, in this work, we propose a new benchmark, termed $\\textbf{P}$ath\n$\\textbf{P}$lanning from $\\textbf{N}$atural $\\textbf{L}$anguage\n($\\textbf{PPNL}$). Our benchmark evaluates LLMs' spatial-temporal reasoning by\nformulating ''path planning'' tasks that require an LLM to navigate to target\nlocations while avoiding obstacles and adhering to constraints. Leveraging this\nbenchmark, we systematically investigate LLMs including GPT-4 via different\nfew-shot prompting methodologies as well as BART and T5 of various sizes via\nfine-tuning. Our experimental results show the promise of few-shot GPT-4 in\nspatial reasoning, when it is prompted to reason and act interleavedly,\nalthough it still fails to perform long-term temporal reasoning. In contrast,\nwhile fine-tuned LLMs achieved impressive results on in-distribution reasoning\ntasks, they struggled to generalize to larger environments or environments with\nmore obstacles.", "published": "2023-10-05 01:42:16", "link": "http://arxiv.org/abs/2310.03249v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Emergent Abilities with Infinite Resolution Evaluation", "abstract": "The scientific scale-up of large language models (LLMs) necessitates a\ncomprehensive understanding of their scaling properties. However, the existing\nliterature on the scaling properties only yields an incomplete answer:\noptimization loss decreases predictably as the model size increases, in line\nwith established scaling law; yet no scaling law for task has been established\nand the task performances are far from predictable during scaling. Task\nperformances typically show minor gains on small models until they improve\ndramatically once models exceed a size threshold, exemplifying the ``emergent\nabilities''. In this study, we discover that small models, although they\nexhibit minor performance, demonstrate critical and consistent task performance\nimprovements that are not captured by conventional evaluation strategies due to\ninsufficient measurement resolution. To measure such improvements, we introduce\nPassUntil, an evaluation strategy with theoretically infinite resolution,\nthrough massive sampling in the decoding phase. With PassUntil, we conduct a\nquantitative investigation into the scaling law of task performance. The\ninvestigation contains two parts. Firstly, a strict task scaling law that is\nnot conventionally known to exist, is identified, enhancing the predictability\nof task performances. Remarkably, we are able to predict the performance of the\n2.4B model on code generation with merely 0.05\\% deviation before training\nstarts, which is the first systematic attempt to verify predictable scaling\nproposed by GPT-4's report. Secondly, we are able to study emergent abilities\nquantitatively. We identify a kind of accelerated emergence whose scaling curve\ncannot be fitted by standard scaling law function and has a increasing speed.\nWe then examine two hypothesis and imply that the ``multiple circuits\nhypothesis'' might be responsible for the accelerated emergence.", "published": "2023-10-05 02:35:00", "link": "http://arxiv.org/abs/2310.03262v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Formalism and Approach for Improving Robustness of Large Language\n  Models Using Risk-Adjusted Confidence Scores", "abstract": "Large Language Models (LLMs), such as ChatGPT, have achieved impressive\nmilestones in natural language processing (NLP). Despite their impressive\nperformance, the models are known to pose important risks. As these models are\ndeployed in real-world applications, a systematic understanding of different\nrisks posed by these models on tasks such as natural language inference (NLI),\nis much needed. In this paper, we define and formalize two distinct types of\nrisk: decision risk and composite risk. We also propose a risk-centric\nevaluation framework, and four novel metrics, for assessing LLMs on these risks\nin both in-domain and out-of-domain settings. Finally, we propose a\nrisk-adjusted calibration method called DwD for helping LLMs minimize these\nrisks in an overall NLI architecture. Detailed experiments, using four NLI\nbenchmarks, three baselines and two LLMs, including ChatGPT, show both the\npractical utility of the evaluation framework, and the efficacy of DwD in\nreducing decision and composite risk. For instance, when using DwD, an\nunderlying LLM is able to address an extra 20.1% of low-risk inference tasks\n(but which the LLM erroneously deems high-risk without risk adjustment) and\nskip a further 19.8% of high-risk tasks, which would have been answered\nincorrectly.", "published": "2023-10-05 03:20:41", "link": "http://arxiv.org/abs/2310.03283v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A New Dialogue Response Generation Agent for Large Language Models by\n  Asking Questions to Detect User's Intentions", "abstract": "Large Language Models (LLMs), such as ChatGPT, have recently been applied to\nvarious NLP tasks due to its open-domain generation capabilities. However,\nthere are two issues with applying LLMs to dialogue tasks. 1. During the\ndialogue process, users may have implicit intentions that might be overlooked\nby LLMs. Consequently, generated responses couldn't align with the user's\nintentions. 2. It is unlikely for LLMs to encompass all fields comprehensively.\nIn certain specific domains, their knowledge may be incomplete, and LLMs cannot\nupdate the latest knowledge in real-time. To tackle these issues, we propose a\nframework~\\emph{using LLM to \\textbf{E}nhance dialogue response generation by\nasking questions to \\textbf{D}etect user's \\textbf{I}mplicit\nin\\textbf{T}entions} (\\textbf{EDIT}). Firstly, EDIT generates open questions\nrelated to the dialogue context as the potential user's intention; Then, EDIT\nanswers those questions by interacting with LLMs and searching in\ndomain-specific knowledge bases respectively, and use LLMs to choose the proper\nanswers to questions as extra knowledge; Finally, EDIT enhances response\ngeneration by explicitly integrating those extra knowledge. Besides, previous\nquestion generation works only focus on asking questions with answers in\ncontext. In order to ask open questions, we construct a Context-Open-Question\n(COQ) dataset. On two task-oriented dialogue tasks (Wizard of Wikipedia and\nHoll-E), EDIT outperformed other LLMs.", "published": "2023-10-05 03:45:54", "link": "http://arxiv.org/abs/2310.03293v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Personalized Alignment for Evaluating Open-ended Text\n  Generation", "abstract": "Recent research has increasingly focused on evaluating large language models'\n(LLMs) alignment with diverse human values and preferences, particularly for\nopen-ended tasks like story generation. Traditional evaluation metrics rely\nheavily on lexical similarity with human-written references, often showing poor\ncorrelation with human judgments and failing to account for alignment with the\ndiversity of human preferences. To address these challenges, we introduce\nPerSE, an interpretable evaluation framework designed to assess alignment with\nspecific human preferences. It is tuned to infer specific preferences from an\nin-context personal profile and evaluate the alignment between the generated\ncontent and personal preferences. PerSE enhances interpretability by providing\ndetailed comments and fine-grained scoring, facilitating more personalized\ncontent generation. Our 13B LLaMA-2-based PerSE shows a 15.8% increase in\nKendall correlation and a 13.7% rise in accuracy with zero-shot reviewers\ncompared to GPT-4. It also outperforms GPT-4 by 46.01% in Kendall correlation\non new domains, indicating its transferability.", "published": "2023-10-05 04:15:48", "link": "http://arxiv.org/abs/2310.03304v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reformulating Domain Adaptation of Large Language Models as\n  Adapt-Retrieve-Revise: A Case Study on Chinese Legal Domain", "abstract": "While large language models (LLMs) like GPT-4 have recently demonstrated\nastonishing zero-shot capabilities in general domain tasks, they often generate\ncontent with hallucinations in specific domains such as Chinese law, hindering\ntheir application in these areas. This is typically due to the absence of\ntraining data that encompasses such a specific domain, preventing GPT-4 from\nacquiring in-domain knowledge. A pressing challenge is that it's not plausible\nto continue training LLMs of such scale on in-domain data.\n  This paper introduces a simple and effective domain adaptation framework for\nGPT-4 by reformulating generation as an \\textbf{adapt-retrieve-revise} process.\nThe initial step is to \\textbf{adapt} an affordable 7B LLM to the target domain\nby continuing learning on in-domain data. When solving a task, we leverage the\nadapted LLM to generate a draft answer given a task query. Then, the draft\nanswer will be used to \\textbf{retrieve} supporting evidence candidates from an\nexternal in-domain knowledge base. Finally, the draft answer and retrieved\nevidence are concatenated into a whole prompt to let GPT-4 assess the evidence\nand \\textbf{revise} the draft answer to generate the final answer.\n  Our proposal combines the advantages of the efficiency of adapting a smaller\n7B model with the evidence-assessing capability of GPT-4 and effectively\nprevents GPT-4 from generating hallucinatory content. In the zero-shot setting\nof four Chinese legal tasks, our method improves accuracy by 33.3\\% compared to\nthe direct generation by GPT-4. When compared to two stronger retrieval-based\nbaselines, our method outperforms them by 15.4\\% and 23.9\\%. Our code will be\nreleased", "published": "2023-10-05 05:55:06", "link": "http://arxiv.org/abs/2310.03328v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Hallucinations in Chinese Large Language Models", "abstract": "In this paper, we establish a benchmark named HalluQA (Chinese Hallucination\nQuestion-Answering) to measure the hallucination phenomenon in Chinese large\nlanguage models. HalluQA contains 450 meticulously designed adversarial\nquestions, spanning multiple domains, and takes into account Chinese historical\nculture, customs, and social phenomena. During the construction of HalluQA, we\nconsider two types of hallucinations: imitative falsehoods and factual errors,\nand we construct adversarial samples based on GLM-130B and ChatGPT. For\nevaluation, we design an automated evaluation method using GPT-4 to judge\nwhether a model output is hallucinated. We conduct extensive experiments on 24\nlarge language models, including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk\nand etc. Out of the 24 models, 18 achieved non-hallucination rates lower than\n50%. This indicates that HalluQA is highly challenging. We analyze the primary\ntypes of hallucinations in different types of models and their causes.\nAdditionally, we discuss which types of hallucinations should be prioritized\nfor different types of models.", "published": "2023-10-05 07:57:09", "link": "http://arxiv.org/abs/2310.03368v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM Based Multi-Document Summarization Exploiting Main-Event Biased\n  Monotone Submodular Content Extraction", "abstract": "Multi-document summarization is a challenging task due to its inherent\nsubjective bias, highlighted by the low inter-annotator ROUGE-1 score of 0.4\namong DUC-2004 reference summaries. In this work, we aim to enhance the\nobjectivity of news summarization by focusing on the main event of a group of\nrelated news documents and presenting it coherently with sufficient context.\nOur primary objective is to succinctly report the main event, ensuring that the\nsummary remains objective and informative. To achieve this, we employ an\nextract-rewrite approach that incorporates a main-event biased\nmonotone-submodular function for content selection. This enables us to extract\nthe most crucial information related to the main event from the document\ncluster. To ensure coherence, we utilize a fine-tuned Language Model (LLM) for\nrewriting the extracted content into a coherent text. The evaluation using\nobjective metrics and human evaluators confirms the effectiveness of our\napproach, as it surpasses potential baselines, demonstrating excellence in both\ncontent coverage, coherence, and informativeness.", "published": "2023-10-05 09:38:09", "link": "http://arxiv.org/abs/2310.03414v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controllable Multi-document Summarization: Coverage & Coherence\n  Intuitive Policy with Large Language Model Based Rewards", "abstract": "Memory-efficient large language models are good at refining text input for\nbetter readability. However, controllability is a matter of concern when it\ncomes to text generation tasks with long inputs, such as multi-document\nsummarization. In this work, we investigate for a generic controllable approach\nfor multi-document summarization that leverages the capabilities of LLMs to\nrefine the text. In particular, we train a controllable content extraction\nscheme to extract the text that will be refined by an LLM. The scheme is\ndesigned with a novel coverage and coherence intuitive policy, which is duly\nrewarded by a passively trained LLM. Our approach yields competitive results in\nthe evaluation using ROUGE metrics and outperforms potential baselines in\ncoherence, as per human evaluation.", "published": "2023-10-05 11:29:09", "link": "http://arxiv.org/abs/2310.03473v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Redefining Digital Health Interfaces with Large Language Models", "abstract": "Digital health tools have the potential to significantly improve the delivery\nof healthcare services. However, their adoption remains comparatively limited\ndue, in part, to challenges surrounding usability and trust. Large Language\nModels (LLMs) have emerged as general-purpose models with the ability to\nprocess complex information and produce human-quality text, presenting a wealth\nof potential applications in healthcare. Directly applying LLMs in clinical\nsettings is not straightforward, however, with LLMs susceptible to providing\ninconsistent or nonsensical answers. We demonstrate how LLM-based systems can\nutilize external tools and provide a novel interface between clinicians and\ndigital technologies. This enhances the utility and practical impact of digital\nhealthcare tools and AI models while addressing current issues with using LLMs\nin clinical settings such as hallucinations. We illustrate LLM-based interfaces\nwith the example of cardiovascular disease risk prediction. We develop a new\nprognostic tool using automated machine learning and demonstrate how LLMs can\nprovide a unique interface to both our model and existing risk scores,\nhighlighting the benefit compared to traditional interfaces for digital tools.", "published": "2023-10-05 14:18:40", "link": "http://arxiv.org/abs/2310.03560v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction", "abstract": "Large Language Models (LLMs) combined with instruction tuning have made\nsignificant progress when generalizing to unseen tasks. However, they have been\nless successful in Information Extraction (IE), lagging behind task-specific\nmodels. Typically, IE tasks are characterized by complex annotation guidelines\nthat describe the task and give examples to humans. Previous attempts to\nleverage such information have failed, even with the largest models, as they\nare not able to follow the guidelines out of the box. In this paper, we propose\nGoLLIE (Guideline-following Large Language Model for IE), a model able to\nimprove zero-shot results on unseen IE tasks by virtue of being fine-tuned to\ncomply with annotation guidelines. Comprehensive evaluation empirically\ndemonstrates that GoLLIE is able to generalize to and follow unseen guidelines,\noutperforming previous attempts at zero-shot information extraction. The\nablation study shows that detailed guidelines are key for good results.", "published": "2023-10-05 16:43:13", "link": "http://arxiv.org/abs/2310.03668v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers", "abstract": "In recent years, many interpretability methods have been proposed to help\ninterpret the internal states of Transformer-models, at different levels of\nprecision and complexity. Here, to analyze encoder-decoder Transformers, we\npropose a simple, new method: DecoderLens. Inspired by the LogitLens (for\ndecoder-only Transformers), this method involves allowing the decoder to\ncross-attend representations of intermediate encoder layers instead of using\nthe final encoder output, as is normally done in encoder-decoder models. The\nmethod thus maps previously uninterpretable vector representations to\nhuman-interpretable sequences of words or symbols. We report results from the\nDecoderLens applied to models trained on question answering, logical reasoning,\nspeech recognition and machine translation. The DecoderLens reveals several\nspecific subtasks that are solved at low or intermediate layers, shedding new\nlight on the information flow inside the encoder component of this important\nclass of models.", "published": "2023-10-05 17:04:59", "link": "http://arxiv.org/abs/2310.03686v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modular Speech-to-Text Translation for Zero-Shot Cross-Modal Transfer", "abstract": "Recent research has shown that independently trained encoders and decoders,\ncombined through a shared fixed-size representation, can achieve competitive\nperformance in speech-to-text translation. In this work, we show that this type\nof approach can be further improved with multilingual training. We observe\nsignificant improvements in zero-shot cross-modal speech translation, even\noutperforming a supervised approach based on XLSR for several languages.", "published": "2023-10-05 17:44:37", "link": "http://arxiv.org/abs/2310.03724v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PrIeD-KIE: Towards Privacy Preserved Document Key Information Extraction", "abstract": "In this paper, we introduce strategies for developing private Key Information\nExtraction (KIE) systems by leveraging large pretrained document foundation\nmodels in conjunction with differential privacy (DP), federated learning (FL),\nand Differentially Private Federated Learning (DP-FL). Through extensive\nexperimentation on six benchmark datasets (FUNSD, CORD, SROIE, WildReceipts,\nXFUND, and DOCILE), we demonstrate that large document foundation models can be\neffectively fine-tuned for the KIE task under private settings to achieve\nadequate performance while maintaining strong privacy guarantees. Moreover, by\nthoroughly analyzing the impact of various training and model parameters on\nmodel performance, we propose simple yet effective guidelines for achieving an\noptimal privacy-utility trade-off for the KIE task under global DP. Finally, we\nintroduce FeAm-DP, a novel DP-FL algorithm that enables efficiently upscaling\nglobal DP from a standalone context to a multi-client federated environment. We\nconduct a comprehensive evaluation of the algorithm across various client and\nprivacy settings, and demonstrate its capability to achieve comparable\nperformance and privacy guarantees to standalone DP, even when accommodating an\nincreasing number of participating clients. Overall, our study offers valuable\ninsights into the development of private KIE systems, and highlights the\npotential of document foundation models for privacy-preserved Document AI\napplications. To the best of authors' knowledge, this is the first work that\nexplores privacy preserved document KIE using document foundation models.", "published": "2023-10-05 12:13:00", "link": "http://arxiv.org/abs/2310.03777v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic and Human-AI Interactive Text Generation", "abstract": "In this tutorial, we focus on text-to-text generation, a class of natural\nlanguage generation (NLG) tasks, that takes a piece of text as input and then\ngenerates a revision that is improved according to some specific criteria\n(e.g., readability or linguistic styles), while largely retaining the original\nmeaning and the length of the text. This includes many useful applications,\nsuch as text simplification, paraphrase generation, style transfer, etc. In\ncontrast to text summarization and open-ended text completion (e.g., story),\nthe text-to-text generation tasks we discuss in this tutorial are more\nconstrained in terms of semantic consistency and targeted language styles. This\nlevel of control makes these tasks ideal testbeds for studying the ability of\nmodels to generate text that is both semantically adequate and stylistically\nappropriate. Moreover, these tasks are interesting from a technical standpoint,\nas they require complex combinations of lexical and syntactical\ntransformations, stylistic control, and adherence to factual knowledge, -- all\nat once. With a special focus on text simplification and revision, this\ntutorial aims to provide an overview of the state-of-the-art natural language\ngeneration research from four major aspects -- Data, Models, Human-AI\nCollaboration, and Evaluation -- and to discuss and showcase a few significant\nand recent advances: (1) the use of non-retrogressive approaches; (2) the shift\nfrom fine-tuning to prompting with large language models; (3) the development\nof new learnable metric and fine-grained human evaluation framework; (4) a\ngrowing body of studies and datasets on non-English languages; (5) the rise of\nHCI+NLP+Accessibility interdisciplinary research to create real-world writing\nassistant systems.", "published": "2023-10-05 20:26:15", "link": "http://arxiv.org/abs/2310.03878v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Representations of First-person Pronouns for Prediction of\n  Depression Symptom Severity", "abstract": "Prior work has shown that analyzing the use of first-person singular pronouns\ncan provide insight into individuals' mental status, especially depression\nsymptom severity. These findings were generated by counting frequencies of\nfirst-person singular pronouns in text data. However, counting doesn't capture\nhow these pronouns are used. Recent advances in neural language modeling have\nleveraged methods generating contextual embeddings. In this study, we sought to\nutilize the embeddings of first-person pronouns obtained from contextualized\nlanguage representation models to capture ways these pronouns are used, to\nanalyze mental status. De-identified text messages sent during online\npsychotherapy with weekly assessment of depression severity were used for\nevaluation. Results indicate the advantage of contextualized first-person\npronoun embeddings over standard classification token embeddings and\nfrequency-based pronoun analysis results in predicting depression symptom\nseverity. This suggests contextual representations of first-person pronouns can\nenhance the predictive utility of language used by people with depression\nsymptoms.", "published": "2023-10-05 00:55:46", "link": "http://arxiv.org/abs/2310.03232v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "InstructProtein: Aligning Human and Protein Language via Knowledge\n  Instruction", "abstract": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, but they fall short in comprehending biological sequences\nsuch as proteins. To address this challenge, we propose InstructProtein, an\ninnovative LLM that possesses bidirectional generation capabilities in both\nhuman and protein languages: (i) taking a protein sequence as input to predict\nits textual function description and (ii) using natural language to prompt\nprotein sequence generation. To achieve this, we first pre-train an LLM on both\nprotein and natural language corpora, enabling it to comprehend individual\nlanguages. Then supervised instruction tuning is employed to facilitate the\nalignment of these two distinct languages. Herein, we introduce a knowledge\ngraph-based instruction generation framework to construct a high-quality\ninstruction dataset, addressing annotation imbalance and instruction deficits\nin existing protein-text corpus. In particular, the instructions inherit the\nstructural relations between proteins and function annotations in knowledge\ngraphs, which empowers our model to engage in the causal modeling of protein\nfunctions, akin to the chain-of-thought processes in natural languages.\nExtensive experiments on bidirectional protein-text generation tasks show that\nInstructProtein outperforms state-of-the-art LLMs by large margins. Moreover,\nInstructProtein serves as a pioneering step towards text-based protein function\nprediction and sequence design, effectively bridging the gap between protein\nand human language understanding.", "published": "2023-10-05 02:45:39", "link": "http://arxiv.org/abs/2310.03269v1", "categories": ["q-bio.BM", "cs.CL"], "primary_category": "q-bio.BM"}
{"title": "Concise and Organized Perception Facilitates Reasoning in Large Language\n  Models", "abstract": "Exploiting large language models (LLMs) to tackle reasoning has garnered\ngrowing attention. It still remains highly challenging to achieve satisfactory\nresults in complex logical problems, characterized by plenty of premises within\nthe context and requiring multi-hop reasoning. In particular, the reasoning\ncapabilities of LLMs are brittle to disorder and distractibility. In this work,\nwe first examine the mechanism from the perspective of information flow and\nreveal that LLMs confront difficulties akin to human-like cognitive biases when\ndealing with disordered and irrelevant content in reasoning tasks. However, in\ncontrast to LLMs, disordered and irrelevant content does not significantly\ndecrease human performance, as humans have a propensity to distill the most\nrelevant information and systematically organize their thoughts, aiding them in\nresponding to questions.Stem from that, we further propose a novel reasoning\napproach named Concise and Organized Perception (COP). COP carefully analyzes\nthe given statements to identify the most pertinent information while\neliminating redundancy efficiently. It then prompts the LLMs in a more\norganized form that adapts to the model's inference process. By perceiving\nconcise and organized context, the reasoning abilities of LLMs can be better\nelicited. Extensive experimental results on several popular logical benchmarks\n(ProofWriter, PrOntoQA, PrOntoQA-OOD, and FOLIO) and mathematical benchmark\n(DI-GSM) show that COP significantly outperforms previous state-of-the-art\nmethods.", "published": "2023-10-05 04:47:49", "link": "http://arxiv.org/abs/2310.03309v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neural Language Model Pruning for Automatic Speech Recognition", "abstract": "We study model pruning methods applied to Transformer-based neural network\nlanguage models for automatic speech recognition. We explore three aspects of\nthe pruning frame work, namely criterion, method and scheduler, analyzing their\ncontribution in terms of accuracy and inference speed. To the best of our\nknowledge, such in-depth analyses on large-scale recognition systems has not\nbeen reported in the literature. In addition, we propose a variant of low-rank\napproximation suitable for incrementally compressing models, and delivering\nmultiple models with varied target sizes. Among other results, we show that a)\ndata-driven pruning outperforms magnitude-driven in several scenarios; b)\nincremental pruning achieves higher accuracy compared to one-shot pruning,\nespecially when targeting smaller sizes; and c) low-rank approximation presents\nthe best trade-off between size reduction and inference speed-up for moderate\ncompression.", "published": "2023-10-05 10:01:32", "link": "http://arxiv.org/abs/2310.03424v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Tik-to-Tok: Translating Language Models One Token at a Time: An\n  Embedding Initialization Strategy for Efficient Language Adaptation", "abstract": "Training monolingual language models for low and mid-resource languages is\nmade challenging by limited and often inadequate pretraining data. In this\nstudy, we propose a novel model conversion strategy to address this issue,\nadapting high-resources monolingual language models to a new target language.\nBy generalizing over a word translation dictionary encompassing both the source\nand target languages, we map tokens from the target tokenizer to semantically\nsimilar tokens from the source language tokenizer. This one-to-many token\nmapping improves tremendously the initialization of the embedding table for the\ntarget language. We conduct experiments to convert high-resource models to mid-\nand low-resource languages, namely Dutch and Frisian. These converted models\nachieve a new state-of-the-art performance on these languages across all sorts\nof downstream tasks. By reducing significantly the amount of data and time\nrequired for training state-of-the-art models, our novel model conversion\nstrategy has the potential to benefit many languages worldwide.", "published": "2023-10-05 11:45:29", "link": "http://arxiv.org/abs/2310.03477v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating Self-Supervised Speech Representations for Indigenous\n  American Languages", "abstract": "The application of self-supervision to speech representation learning has\ngarnered significant interest in recent years, due to its scalability to large\namounts of unlabeled data. However, much progress, both in terms of\npre-training and downstream evaluation, has remained concentrated in\nmonolingual models that only consider English. Few models consider other\nlanguages, and even fewer consider indigenous ones. In our submission to the\nNew Language Track of the ASRU 2023 ML-SUPERB Challenge, we present an ASR\ncorpus for Quechua, an indigenous South American Language. We benchmark the\nefficacy of large SSL models on Quechua, along with 6 other indigenous\nlanguages such as Guarani and Bribri, on low-resource ASR. Our results show\nsurprisingly strong performance by state-of-the-art SSL models, showing the\npotential generalizability of large-scale models to real-world data.", "published": "2023-10-05 16:11:14", "link": "http://arxiv.org/abs/2310.03639v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "TRAM: Bridging Trust Regions and Sharpness Aware Minimization", "abstract": "Sharpness-aware minimization (SAM) reports improving domain generalization by\nreducing the loss surface curvature in the parameter space. However,\ngeneralization during fine-tuning is often more dependent on the\ntransferability of representations in the function space. Trust-region methods\n(TR) target this goal by regularizing representation curvature to reduce\ncatastrophic forgetting of pre-trained task-agnostic information while adopting\ntask-specific skills. We consider unifying these strategies for low curvature\nin both parameter space and function space to improve out-of-domain (OOD)\ngeneralization. We propose Trust Region Aware Minimization (TRAM), a SAM\nalgorithm fine-tuning for low parameter sharpness and smooth, informative\nrepresentations preserving pre-trained structure. TRAM uses a trust region\nbound to inform the SAM adversarial neighborhood, introducing an awareness of\nfunction curvature within optimization for flatter minima. We empirically\nvalidate TRAM in vision (cross-dataset adaptation) and text (OOD language\nmodeling, zero-shot cross-lingual transfer) tasks where robust domain transfer\nand representation generality are critical. TRAM outperforms SAM- and TR-based\noptimization across all tasks, notably surpassing competing methods for hard\ntransfer between anticorrelated domains. TRAM establishes a novel standard in\nfine-tuning for domain-generalizable models with minimal additional computation\nover previous sharpness-aware methods.", "published": "2023-10-05 16:21:36", "link": "http://arxiv.org/abs/2310.03646v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MapperGPT: Large Language Models for Linking and Mapping Entities", "abstract": "Aligning terminological resources, including ontologies, controlled\nvocabularies, taxonomies, and value sets is a critical part of data integration\nin many domains such as healthcare, chemistry, and biomedical research. Entity\nmapping is the process of determining correspondences between entities across\nthese resources, such as gene identifiers, disease concepts, or chemical entity\nidentifiers. Many tools have been developed to compute such mappings based on\ncommon structural features and lexical information such as labels and synonyms.\nLexical approaches in particular often provide very high recall, but low\nprecision, due to lexical ambiguity. As a consequence of this, mapping efforts\noften resort to a labor intensive manual mapping refinement through a human\ncurator.\n  Large Language Models (LLMs), such as the ones employed by ChatGPT, have\ngeneralizable abilities to perform a wide range of tasks, including\nquestion-answering and information extraction. Here we present MapperGPT, an\napproach that uses LLMs to review and refine mapping relationships as a\npost-processing step, in concert with existing high-recall methods that are\nbased on lexical and structural heuristics.\n  We evaluated MapperGPT on a series of alignment tasks from different domains,\nincluding anatomy, developmental biology, and renal diseases. We devised a\ncollection of tasks that are designed to be particularly challenging for\nlexical methods. We show that when used in combination with high-recall\nmethods, MapperGPT can provide a substantial improvement in accuracy, beating\nstate-of-the-art (SOTA) methods such as LogMap.", "published": "2023-10-05 16:43:04", "link": "http://arxiv.org/abs/2310.03666v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Long Way to Go: Investigating Length Correlations in RLHF", "abstract": "Great success has been reported using Reinforcement Learning from Human\nFeedback (RLHF) to align large language models, with open preference datasets\nenabling wider experimentation, particularly for \"helpfulness\" in tasks like\ndialogue and web question answering. Alongside these improvements, however,\nRLHF also often drives models to produce longer outputs. This paper\ndemonstrates, on three diverse settings, that optimizing for response length\nis, much more than previously thought, a significant factor behind RLHF.\nStudying the strategies RL optimization uses to maximize reward, we find\nimprovements in reward to largely be driven by increasing response length,\ninstead of other features. Indeed, we find that even a purely length-based\nreward reproduces most downstream RLHF improvements over supervised fine-tuned\nmodels. Testing a comprehensive set of length-countering interventions, we\nidentify the dominant source of these biases to be reward models, which, by\nstudying training dynamics, we find are non-robust and easily influenced by\nlength biases in preference data.", "published": "2023-10-05 17:38:28", "link": "http://arxiv.org/abs/2310.03716v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Investigating Alternative Feature Extraction Pipelines For Clinical Note\n  Phenotyping", "abstract": "A common practice in the medical industry is the use of clinical notes, which\nconsist of detailed patient observations. However, electronic health record\nsystems frequently do not contain these observations in a structured format,\nrendering patient information challenging to assess and evaluate automatically.\nUsing computational systems for the extraction of medical attributes offers\nmany applications, including longitudinal analysis of patients, risk\nassessment, and hospital evaluation. Recent work has constructed successful\nmethods for phenotyping: extracting medical attributes from clinical notes.\nBERT-based models can be used to transform clinical notes into a series of\nrepresentations, which are then condensed into a single document representation\nbased on their CLS embeddings and passed into an LSTM (Mulyar et al., 2020).\nThough this pipeline yields a considerable performance improvement over\nprevious results, it requires extensive convergence time. This method also does\nnot allow for predicting attributes not yet identified in clinical notes.\n  Considering the wide variety of medical attributes that may be present in a\nclinical note, we propose an alternative pipeline utilizing ScispaCy (Neumann\net al., 2019) for the extraction of common diseases. We then train various\nsupervised learning models to associate the presence of these conditions with\npatient attributes. Finally, we replicate a ClinicalBERT (Alsentzer et al.,\n2019) and LSTM-based approach for purposes of comparison. We find that\nalternative methods moderately underperform the replicated LSTM approach. Yet,\nconsidering a complex tradeoff between accuracy and runtime, in addition to the\nfact that the alternative approach also allows for the detection of medical\nconditions that are not already present in a clinical note, its usage may be\nconsidered as a supplement to established methods.", "published": "2023-10-05 02:51:51", "link": "http://arxiv.org/abs/2310.03772v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Benchmarking a foundation LLM on its ability to re-label structure names\n  in accordance with the AAPM TG-263 report", "abstract": "Purpose: To introduce the concept of using large language models (LLMs) to\nre-label structure names in accordance with the American Association of\nPhysicists in Medicine (AAPM) Task Group (TG)-263 standard, and to establish a\nbenchmark for future studies to reference.\n  Methods and Materials: The Generative Pre-trained Transformer (GPT)-4\napplication programming interface (API) was implemented as a Digital Imaging\nand Communications in Medicine (DICOM) storage server, which upon receiving a\nstructure set DICOM file, prompts GPT-4 to re-label the structure names of both\ntarget volumes and normal tissues according to the AAPM TG-263. Three disease\nsites, prostate, head and neck, and thorax were selected for evaluation. For\neach disease site category, 150 patients were randomly selected for manually\ntuning the instructions prompt (in batches of 50) and 50 patients were randomly\nselected for evaluation. Structure names that were considered were those that\nwere most likely to be relevant for studies utilizing structure contours for\nmany patients.\n  Results: The overall re-labeling accuracy of both target volumes and normal\ntissues for prostate, head and neck, and thorax cases was 96.0%, 98.5%, and\n96.9% respectively. Re-labeling of target volumes was less accurate on average\nexcept for prostate - 100%, 93.1%, and 91.1% respectively.\n  Conclusions: Given the accuracy of GPT-4 in re-labeling structure names of\nboth target volumes and normal tissues as presented in this work, LLMs are\npoised to be the preferred method for standardizing structure names in\nradiation oncology, especially considering the rapid advancements in LLM\ncapabilities that are likely to continue.", "published": "2023-10-05 20:10:35", "link": "http://arxiv.org/abs/2310.03874v1", "categories": ["physics.med-ph", "cs.CL"], "primary_category": "physics.med-ph"}
{"title": "Trustworthy Formal Natural Language Specifications", "abstract": "Interactive proof assistants are computer programs carefully constructed to\ncheck a human-designed proof of a mathematical claim with high confidence in\nthe implementation. However, this only validates truth of a formal claim, which\nmay have been mistranslated from a claim made in natural language. This is\nespecially problematic when using proof assistants to formally verify the\ncorrectness of software with respect to a natural language specification. The\ntranslation from informal to formal remains a challenging, time-consuming\nprocess that is difficult to audit for correctness.\n  This paper shows that it is possible to build support for specifications\nwritten in expressive subsets of natural language, within existing proof\nassistants, consistent with the principles used to establish trust and\nauditability in proof assistants themselves. We implement a means to provide\nspecifications in a modularly extensible formal subset of English, and have\nthem automatically translated into formal claims, entirely within the Lean\nproof assistant. Our approach is extensible (placing no permanent restrictions\non grammatical structure), modular (allowing information about new words to be\ndistributed alongside libraries), and produces proof certificates explaining\nhow each word was interpreted and how the sentence's structure was used to\ncompute the meaning.\n  We apply our prototype to the translation of various English descriptions of\nformal specifications from a popular textbook into Lean formalizations; all can\nbe translated correctly with a modest lexicon with only minor modifications\nrelated to lexicon size.", "published": "2023-10-05 20:41:47", "link": "http://arxiv.org/abs/2310.03885v1", "categories": ["cs.PL", "cs.CL"], "primary_category": "cs.PL"}
{"title": "LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination\n  Abilities in Large Language Models", "abstract": "The emergent reasoning and Theory of Mind (ToM) abilities demonstrated by\nLarge Language Models (LLMs) make them promising candidates for developing\ncoordination agents. In this study, we introduce a new LLM-Coordination\nBenchmark aimed at a detailed analysis of LLMs within the context of Pure\nCoordination Games, where participating agents need to cooperate for the most\ngain. This benchmark evaluates LLMs through two distinct tasks: (1)\n\\emph{Agentic Coordination}, where LLMs act as proactive participants for\ncooperation in 4 pure coordination games; (2) \\emph{Coordination Question\nAnswering (QA)}, where LLMs are prompted to answer 198 multiple-choice\nquestions from the 4 games for evaluation of three key reasoning abilities:\nEnvironment Comprehension, ToM Reasoning, and Joint Planning. Furthermore, to\nenable LLMs for multi-agent coordination, we introduce a Cognitive Architecture\nfor Coordination (CAC) framework that can easily integrate different LLMs as\nplug-and-play modules for pure coordination games. Our findings indicate that\nLLM agents equipped with GPT-4-turbo achieve comparable performance to\nstate-of-the-art reinforcement learning methods in games that require\ncommonsense actions based on the environment. Besides, zero-shot coordination\nexperiments reveal that, unlike RL methods, LLM agents are robust to new unseen\npartners. However, results on Coordination QA show a large room for improvement\nin the Theory of Mind reasoning and joint planning abilities of LLMs. The\nanalysis also sheds light on how the ability of LLMs to understand their\nenvironment and their partner's beliefs and intentions plays a part in their\nability to plan for coordination. Our code is available at\n\\url{https://github.com/eric-ai-lab/llm_coordination}.", "published": "2023-10-05 21:18:15", "link": "http://arxiv.org/abs/2310.03903v2", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Exploring the evolution of research topics during the COVID-19 pandemic", "abstract": "The COVID-19 pandemic has changed the research agendas of most scientific\ncommunities, resulting in an overwhelming production of research articles in a\nvariety of domains, including medicine, virology, epidemiology, economy,\npsychology, and so on. Several open-access corpora and literature hubs were\nestablished; among them, the COVID-19 Open Research Dataset (CORD-19) has\nsystematically gathered scientific contributions for 2.5 years, by collecting\nand indexing over one million articles. Here, we present the CORD-19 Topic\nVisualizer (CORToViz), a method and associated visualization tool for\ninspecting the CORD-19 textual corpus of scientific abstracts. Our method is\nbased upon a careful selection of up-to-date technologies (including large\nlanguage models), resulting in an architecture for clustering articles along\northogonal dimensions and extraction techniques for temporal topic mining.\nTopic inspection is supported by an interactive dashboard, providing fast,\none-click visualization of topic contents as word clouds and topic trends as\ntime series, equipped with easy-to-drive statistical testing for analyzing the\nsignificance of topic emergence along arbitrarily selected time windows. The\nprocesses of data preparation and results visualization are completely general\nand virtually applicable to any corpus of textual documents - thus suited for\neffective adaptation to other contexts.", "published": "2023-10-05 22:16:41", "link": "http://arxiv.org/abs/2310.03928v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Tuning In to Neural Encoding: Linking Human Brain and Artificial\n  Supervised Representations of Language", "abstract": "To understand the algorithm that supports the human brain's language\nrepresentation, previous research has attempted to predict neural responses to\nlinguistic stimuli using embeddings generated by artificial neural networks\n(ANNs), a process known as neural encoding. However, most of these studies have\nfocused on probing neural representations of Germanic languages, such as\nEnglish, with unsupervised ANNs. In this paper, we propose to bridge the gap\nbetween human brain and supervised ANN representations of the Chinese language.\nSpecifically, we investigate how task tuning influences a pretained Transformer\nfor neural encoding and which tasks lead to the best encoding performances. We\ngenerate supervised representations on eight Natural Language Understanding\n(NLU) tasks using prompt-tuning, a technique that is seldom explored in neural\nencoding for language. We demonstrate that prompt-tuning yields representations\nthat better predict neural responses to Chinese stimuli than traditional\nfine-tuning on four tasks. Furthermore, we discover that tasks that require a\nfine-grained processing of concepts and entities lead to representations that\nare most predictive of brain activation patterns. Additionally, we reveal that\nthe proportion of tuned parameters highly influences the neural encoding\nperformance of fine-tuned models. Overall, our experimental findings could help\nus better understand the relationship between supervised artificial and brain\nlanguage representations.", "published": "2023-10-05 06:31:01", "link": "http://arxiv.org/abs/2310.04460v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Validating transformers for redaction of text from electronic health\n  records in real-world healthcare", "abstract": "Protecting patient privacy in healthcare records is a top priority, and\nredaction is a commonly used method for obscuring directly identifiable\ninformation in text. Rule-based methods have been widely used, but their\nprecision is often low causing over-redaction of text and frequently not being\nadaptable enough for non-standardised or unconventional structures of personal\nhealth information. Deep learning techniques have emerged as a promising\nsolution, but implementing them in real-world environments poses challenges due\nto the differences in patient record structure and language across different\ndepartments, hospitals, and countries.\n  In this study, we present AnonCAT, a transformer-based model and a blueprint\non how deidentification models can be deployed in real-world healthcare.\nAnonCAT was trained through a process involving manually annotated redactions\nof real-world documents from three UK hospitals with different electronic\nhealth record systems and 3116 documents. The model achieved high performance\nin all three hospitals with a Recall of 0.99, 0.99 and 0.96.\n  Our findings demonstrate the potential of deep learning techniques for\nimproving the efficiency and accuracy of redaction in global healthcare data\nand highlight the importance of building workflows which not just use these\nmodels but are also able to continually fine-tune and audit the performance of\nthese algorithms to ensure continuing effectiveness in real-world settings.\nThis approach provides a blueprint for the real-world use of de-identifying\nalgorithms through fine-tuning and localisation, the code together with\ntutorials is available on GitHub (https://github.com/CogStack/MedCAT).", "published": "2023-10-05 19:10:18", "link": "http://arxiv.org/abs/2310.04468v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Procedural Text Mining with Large Language Models", "abstract": "Recent advancements in the field of Natural Language Processing, particularly\nthe development of large-scale language models that are pretrained on vast\namounts of knowledge, are creating novel opportunities within the realm of\nKnowledge Engineering. In this paper, we investigate the usage of large\nlanguage models (LLMs) in both zero-shot and in-context learning settings to\ntackle the problem of extracting procedures from unstructured PDF text in an\nincremental question-answering fashion. In particular, we leverage the current\nstate-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model,\naccompanied by two variations of in-context learning that involve an ontology\nwith definitions of procedures and steps and a limited number of samples of\nfew-shot learning. The findings highlight both the promise of this approach and\nthe value of the in-context learning customisations. These modifications have\nthe potential to significantly address the challenge of obtaining sufficient\ntraining data, a hurdle often encountered in deep learning-based Natural\nLanguage Processing techniques for procedure extraction.", "published": "2023-10-05 08:27:33", "link": "http://arxiv.org/abs/2310.03376v1", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "The North System for Formosa Speech Recognition Challenge 2023", "abstract": "This report provides a concise overview of the proposed North system, which\naims to achieve automatic word/syllable recognition for Taiwanese Hakka\n(Sixian). The report outlines three key components of the system: the\nacquisition, composition, and utilization of the training data; the\narchitecture of the model; and the hardware specifications and operational\nstatistics. The demonstration of the system has been made public at\nhttps://asrvm.iis.sinica.edu.tw/hakka_sixian.", "published": "2023-10-05 10:29:18", "link": "http://arxiv.org/abs/2310.03443v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards Robust and Generalizable Training: An Empirical Study of Noisy\n  Slot Filling for Input Perturbations", "abstract": "In real dialogue scenarios, as there are unknown input noises in the\nutterances, existing supervised slot filling models often perform poorly in\npractical applications. Even though there are some studies on noise-robust\nmodels, these works are only evaluated on rule-based synthetic datasets, which\nis limiting, making it difficult to promote the research of noise-robust\nmethods. In this paper, we introduce a noise robustness evaluation dataset\nnamed Noise-SF for slot filling task. The proposed dataset contains five types\nof human-annotated noise, and all those noises are exactly existed in real\nextensive robust-training methods of slot filling into the proposed framework.\nBy conducting exhaustive empirical evaluation experiments on Noise-SF, we find\nthat baseline models have poor performance in robustness evaluation, and the\nproposed framework can effectively improve the robustness of models. Based on\nthe empirical experimental results, we make some forward-looking suggestions to\nfuel the research in this direction. Our dataset Noise-SF will be released at\nhttps://github.com/dongguanting/Noise-SF.", "published": "2023-10-05 12:59:57", "link": "http://arxiv.org/abs/2310.03518v1", "categories": ["cs.CL", "cs.AI", "cs.DS"], "primary_category": "cs.CL"}
{"title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users\n  Do Not Intend To!", "abstract": "Optimizing large language models (LLMs) for downstream use cases often\ninvolves the customization of pre-trained LLMs through further fine-tuning.\nMeta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5\nTurbo on custom datasets also encourage this practice. But, what are the safety\ncosts associated with such custom fine-tuning? We note that while existing\nsafety alignment infrastructures can restrict harmful behaviors of LLMs at\ninference time, they do not cover safety risks when fine-tuning privileges are\nextended to end-users. Our red teaming studies find that the safety alignment\nof LLMs can be compromised by fine-tuning with only a few adversarially\ndesigned training examples. For instance, we jailbreak GPT-3.5 Turbo's safety\nguardrails by fine-tuning it on only 10 such examples at a cost of less than\n$0.20 via OpenAI's APIs, making the model responsive to nearly any harmful\ninstructions. Disconcertingly, our research also reveals that, even without\nmalicious intent, simply fine-tuning with benign and commonly used datasets can\nalso inadvertently degrade the safety alignment of LLMs, though to a lesser\nextent. These findings suggest that fine-tuning aligned LLMs introduces new\nsafety risks that current safety infrastructures fall short of addressing --\neven if a model's initial safety alignment is impeccable, it is not necessarily\nto be maintained after custom fine-tuning. We outline and critically analyze\npotential mitigations and advocate for further research efforts toward\nreinforcing safety protocols for the custom fine-tuning of aligned LLMs.", "published": "2023-10-05 17:12:17", "link": "http://arxiv.org/abs/2310.03693v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Agent Instructs Large Language Models to be General Zero-Shot Reasoners", "abstract": "We introduce a method to improve the zero-shot reasoning abilities of large\nlanguage models on general language understanding tasks. Specifically, we build\nan autonomous agent to instruct the reasoning process of large language models.\nWe show this approach further unleashes the zero-shot reasoning abilities of\nlarge language models to more tasks. We study the performance of our method on\na wide set of datasets spanning generation, classification, and reasoning. We\nshow that our method generalizes to most tasks and obtains state-of-the-art\nzero-shot performance on 20 of the 29 datasets that we evaluate. For instance,\nour method boosts the performance of state-of-the-art large language models by\na large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and\nGPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement\nin reasoning is striking, with an average increase of 10.5%. With our method,\nLlama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.", "published": "2023-10-05 17:36:16", "link": "http://arxiv.org/abs/2310.03710v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DSPy: Compiling Declarative Language Model Calls into Self-Improving\n  Pipelines", "abstract": "The ML community is rapidly exploring techniques for prompting language\nmodels (LMs) and for stacking them into pipelines that solve complex tasks.\nUnfortunately, existing LM pipelines are typically implemented using hard-coded\n\"prompt templates\", i.e. lengthy strings discovered via trial and error. Toward\na more systematic approach for developing and optimizing LM pipelines, we\nintroduce DSPy, a programming model that abstracts LM pipelines as text\ntransformation graphs, i.e. imperative computational graphs where LMs are\ninvoked through declarative modules. DSPy modules are parameterized, meaning\nthey can learn (by creating and collecting demonstrations) how to apply\ncompositions of prompting, finetuning, augmentation, and reasoning techniques.\nWe design a compiler that will optimize any DSPy pipeline to maximize a given\nmetric. We conduct two case studies, showing that succinct DSPy programs can\nexpress and optimize sophisticated LM pipelines that reason about math word\nproblems, tackle multi-hop retrieval, answer complex questions, and control\nagent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and\nllama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot\nprompting (generally by over 25% and 65%, respectively) and pipelines with\nexpert-created demonstrations (by up to 5-46% and 16-40%, respectively). On top\nof that, DSPy programs compiled to open and relatively small LMs like\n770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely\non expert-written prompt chains for proprietary GPT-3.5. DSPy is available at\nhttps://github.com/stanfordnlp/dspy", "published": "2023-10-05 17:37:25", "link": "http://arxiv.org/abs/2310.03714v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical\n  Reasoning", "abstract": "The recently released GPT-4 Code Interpreter has demonstrated remarkable\nproficiency in solving challenging math problems, primarily attributed to its\nability to seamlessly reason with natural language, generate code, execute\ncode, and continue reasoning based on the execution output. In this paper, we\npresent a method to fine-tune open-source language models, enabling them to use\ncode for modeling and deriving math equations and, consequently, enhancing\ntheir mathematical reasoning abilities. We propose a method of generating novel\nand high-quality datasets with math problems and their code-based solutions,\nreferred to as MathCodeInstruct. Each solution interleaves natural language,\ncode, and execution results. We also introduce a customized supervised\nfine-tuning and inference approach. This approach yields the MathCoder models,\na family of models capable of generating code-based solutions for solving\nchallenging math problems. Impressively, the MathCoder models achieve\nstate-of-the-art scores among open-source LLMs on the MATH (45.2%) and GSM8K\n(83.9%) datasets, substantially outperforming other open-source alternatives.\nNotably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K\nand MATH but also outperforms GPT-4 on the competition-level MATH dataset. The\ndataset and models will be released at https://github.com/mathllm/MathCoder.", "published": "2023-10-05 17:52:09", "link": "http://arxiv.org/abs/2310.03731v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improved Baselines with Visual Instruction Tuning", "abstract": "Large multimodal models (LMM) have recently shown encouraging progress with\nvisual instruction tuning. In this note, we show that the fully-connected\nvision-language cross-modal connector in LLaVA is surprisingly powerful and\ndata-efficient. With simple modifications to LLaVA, namely, using\nCLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA\ndata with simple response formatting prompts, we establish stronger baselines\nthat achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint\nuses merely 1.2M publicly available data, and finishes full training in ~1 day\non a single 8-A100 node. We hope this can make state-of-the-art LMM research\nmore accessible. Code and model will be publicly available.", "published": "2023-10-05 17:59:56", "link": "http://arxiv.org/abs/2310.03744v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "HandMeThat: Human-Robot Communication in Physical and Social\n  Environments", "abstract": "We introduce HandMeThat, a benchmark for a holistic evaluation of instruction\nunderstanding and following in physical and social environments. While previous\ndatasets primarily focused on language grounding and planning, HandMeThat\nconsiders the resolution of human instructions with ambiguities based on the\nphysical (object states and relations) and social (human actions and goals)\ninformation. HandMeThat contains 10,000 episodes of human-robot interactions.\nIn each episode, the robot first observes a trajectory of human actions towards\nher internal goal. Next, the robot receives a human instruction and should take\nactions to accomplish the subgoal set through the instruction. In this paper,\nwe present a textual interface for our benchmark, where the robot interacts\nwith a virtual environment through textual commands. We evaluate several\nbaseline models on HandMeThat, and show that both offline and online\nreinforcement learning algorithms perform poorly on HandMeThat, suggesting\nsignificant room for future work on physical and social human-robot\ncommunications and interactions.", "published": "2023-10-05 16:14:46", "link": "http://arxiv.org/abs/2310.03779v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Contextualized Structural Self-supervised Learning for Ontology Matching", "abstract": "Ontology matching (OM) entails the identification of semantic relationships\nbetween concepts within two or more knowledge graphs (KGs) and serves as a\ncritical step in integrating KGs from various sources. Recent advancements in\ndeep OM models have harnessed the power of transformer-based language models\nand the advantages of knowledge graph embedding. Nevertheless, these OM models\nstill face persistent challenges, such as a lack of reference alignments,\nruntime latency, and unexplored different graph structures within an end-to-end\nframework. In this study, we introduce a novel self-supervised learning OM\nframework with input ontologies, called LaKERMap. This framework capitalizes on\nthe contextual and structural information of concepts by integrating implicit\nknowledge into transformers. Specifically, we aim to capture multiple\nstructural contexts, encompassing both local and global interactions, by\nemploying distinct training objectives. To assess our methods, we utilize the\nBio-ML datasets and tasks. The findings from our innovative approach reveal\nthat LaKERMap surpasses state-of-the-art systems in terms of alignment quality\nand inference time. Our models and codes are available here:\nhttps://github.com/ellenzhuwang/lakermap.", "published": "2023-10-05 18:51:33", "link": "http://arxiv.org/abs/2310.03840v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "How toxic is antisemitism? Potentials and limitations of automated\n  toxicity scoring for antisemitic online content", "abstract": "The Perspective API, a popular text toxicity assessment service by Google and\nJigsaw, has found wide adoption in several application areas, notably content\nmoderation, monitoring, and social media research. We examine its potentials\nand limitations for the detection of antisemitic online content that, by\ndefinition, falls under the toxicity umbrella term. Using a manually annotated\nGerman-language dataset comprising around 3,600 posts from Telegram and\nTwitter, we explore as how toxic antisemitic texts are rated and how the\ntoxicity scores differ regarding different subforms of antisemitism and the\nstance expressed in the texts. We show that, on a basic level, Perspective API\nrecognizes antisemitic content as toxic, but shows critical weaknesses with\nrespect to non-explicit forms of antisemitism and texts taking a critical\nstance towards it. Furthermore, using simple text manipulations, we demonstrate\nthat the use of widespread antisemitic codes can substantially reduce API\nscores, making it rather easy to bypass content moderation based on the\nservice's results.", "published": "2023-10-05 15:23:04", "link": "http://arxiv.org/abs/2310.04465v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "CLEVRER-Humans: Describing Physical and Causal Events the Human Way", "abstract": "Building machines that can reason about physical events and their causal\nrelationships is crucial for flexible interaction with the physical world.\nHowever, most existing physical and causal reasoning benchmarks are exclusively\nbased on synthetically generated events and synthetic natural language\ndescriptions of causal relationships. This design brings up two issues. First,\nthere is a lack of diversity in both event types and natural language\ndescriptions; second, causal relationships based on manually-defined heuristics\nare different from human judgments. To address both shortcomings, we present\nthe CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of\nphysical events with human labels. We employ two techniques to improve data\ncollection efficiency: first, a novel iterative event cloze task to elicit a\nnew representation of events in videos, which we term Causal Event Graphs\n(CEGs); second, a data augmentation technique based on neural language\ngenerative models. We convert the collected CEGs into questions and answers to\nbe consistent with prior work. Finally, we study a collection of baseline\napproaches for CLEVRER-Humans question-answering, highlighting the great\nchallenges set forth by our benchmark.", "published": "2023-10-05 16:09:48", "link": "http://arxiv.org/abs/2310.03635v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "stat.ML"], "primary_category": "cs.AI"}
{"title": "Latent Filling: Latent Space Data Augmentation for Zero-shot Speech\n  Synthesis", "abstract": "Previous works in zero-shot text-to-speech (ZS-TTS) have attempted to enhance\nits systems by enlarging the training data through crowd-sourcing or augmenting\nexisting speech data. However, the use of low-quality data has led to a decline\nin the overall system performance. To avoid such degradation, instead of\ndirectly augmenting the input data, we propose a latent filling (LF) method\nthat adopts simple but effective latent space data augmentation in the speaker\nembedding space of the ZS-TTS system. By incorporating a consistency loss, LF\ncan be seamlessly integrated into existing ZS-TTS systems without the need for\nadditional training stages. Experimental results show that LF significantly\nimproves speaker similarity while preserving speech quality.", "published": "2023-10-05 13:44:09", "link": "http://arxiv.org/abs/2310.03538v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "VaSAB: The variable size adaptive information bottleneck for\n  disentanglement on speech and singing voice", "abstract": "The information bottleneck auto-encoder is a tool for disentanglement\ncommonly used for voice transformation. The successful disentanglement relies\non the right choice of bottleneck size. Previous bottleneck auto-encoders\ncreated the bottleneck by the dimension of the latent space or through vector\nquantization and had no means to change the bottleneck size of a specific\nmodel. As the bottleneck removes information from the disentangled\nrepresentation, the choice of bottleneck size is a trade-off between\ndisentanglement and synthesis quality. We propose to build the information\nbottleneck using dropout which allows us to change the bottleneck through the\ndropout rate and investigate adapting the bottleneck size depending on the\ncontext. We experimentally explore into using the adaptive bottleneck for pitch\ntransformation and demonstrate that the adaptive bottleneck leads to improved\ndisentanglement of the F0 parameter for both, speech and singing voice leading\nto improved synthesis quality. Using the variable bottleneck size, we were able\nto achieve disentanglement for singing voice including extremely high pitches\nand create a universal voice model, that works on both speech and singing voice\nwith improved synthesis quality.", "published": "2023-10-05 10:30:33", "link": "http://arxiv.org/abs/2310.03444v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Performance and energy balance: a comprehensive study of\n  state-of-the-art sound event detection systems", "abstract": "In recent years, deep learning systems have shown a concerning trend toward\nincreased complexity and higher energy consumption. As researchers in this\ndomain and organizers of one of the Detection and Classification of Acoustic\nScenes and Events challenges tasks, we recognize the importance of addressing\nthe environmental impact of data-driven SED systems. In this paper, we propose\nan analysis focused on SED systems based on the challenge submissions. This\nincludes a comparison across the past two years and a detailed analysis of this\nyear's SED systems. Through this research, we aim to explore how the SED\nsystems are evolving every year in relation to their energy efficiency\nimplications.", "published": "2023-10-05 10:49:33", "link": "http://arxiv.org/abs/2310.03455v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaker localization using direct path dominance test based on sound\n  field directivity", "abstract": "Estimation of the direction-of-arrival (DoA) of a speaker in a room is\nimportant in many audio signal processing applications. Environments with\nreverberation that masks the DoA information are particularly challenging.\nRecently, a DoA estimation method that is robust to reverberation has been\ndeveloped. This method identifies time-frequency bins dominated by the\ncontribution from the direct path, which carries the correct DoA information.\nHowever, its implementation is computationally demanding as it requires\nfrequency smoothing to overcome the effect of coherent early reflections and\nmatrix decomposition to apply the direct-path dominance (DPD) test. In this\nwork, a novel computationally-efficient alternative to the DPD test is\nproposed, based on the directivity measure for sensor arrays, which requires\nneither frequency smoothing nor matrix decomposition, and which has been\nreformulated for sound field directivity with spherical microphone arrays. The\npaper presents the proposed method and a comparison to previous methods under a\nrange of reverberation and noise conditions. Result demonstrate that the\nproposed method shows comparable performance to the original method in terms of\nrobustness to reverberation and noise, and is about four times more\ncomputationally efficient for the given experiment.", "published": "2023-10-05 17:09:50", "link": "http://arxiv.org/abs/2310.03688v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Securing Voice Biometrics: One-Shot Learning Approach for Audio Deepfake\n  Detection", "abstract": "The Automatic Speaker Verification (ASV) system is vulnerable to fraudulent\nactivities using audio deepfakes, also known as logical-access voice spoofing\nattacks. These deepfakes pose a concerning threat to voice biometrics due to\nrecent advancements in generative AI and speech synthesis technologies. While\nseveral deep learning models for speech synthesis detection have been\ndeveloped, most of them show poor generalizability, especially when the attacks\nhave different statistical distributions from the ones seen. Therefore, this\npaper presents Quick-SpoofNet, an approach for detecting both seen and unseen\nsynthetic attacks in the ASV system using one-shot learning and metric learning\ntechniques. By using the effective spectral feature set, the proposed method\nextracts compact and representative temporal embeddings from the voice samples\nand utilizes metric learning and triplet loss to assess the similarity index\nand distinguish different embeddings. The system effectively clusters similar\nspeech embeddings, classifying bona fide speeches as the target class and\nidentifying other clusters as spoofing attacks. The proposed system is\nevaluated using the ASVspoof 2019 logical access (LA) dataset and tested\nagainst unseen deepfake attacks from the ASVspoof 2021 dataset. Additionally,\nits generalization ability towards unseen bona fide speech is assessed using\nspeech data from the VSDC dataset.", "published": "2023-10-05 19:30:22", "link": "http://arxiv.org/abs/2310.03856v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Event-Relational Graph Representation Learning for Acoustic Scene\n  Classification", "abstract": "Most deep learning-based acoustic scene classification (ASC) approaches\nidentify scenes based on acoustic features converted from audio clips\ncontaining mixed information entangled by polyphonic audio events (AEs).\nHowever, these approaches have difficulties in explaining what cues they use to\nidentify scenes. This paper conducts the first study on disclosing the\nrelationship between real-life acoustic scenes and semantic embeddings from the\nmost relevant AEs. Specifically, we propose an event-relational graph\nrepresentation learning (ERGL) framework for ASC to classify scenes, and\nsimultaneously answer clearly and straightly which cues are used in\nclassifying. In the event-relational graph, embeddings of each event are\ntreated as nodes, while relationship cues derived from each pair of nodes are\ndescribed by multi-dimensional edge features. Experiments on a real-life ASC\ndataset show that the proposed ERGL achieves competitive performance on ASC by\nlearning embeddings of only a limited number of AEs. The results show the\nfeasibility of recognizing diverse acoustic scenes based on the audio\nevent-relational graph. Visualizations of graph representations learned by ERGL\nare available here (https://github.com/Yuanbo2020/ERGL).", "published": "2023-10-05 20:48:59", "link": "http://arxiv.org/abs/2310.03889v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Challenges and Insights: Exploring 3D Spatial Features and Complex\n  Networks on the MISP Dataset", "abstract": "Multi-channel multi-talker speech recognition presents formidable challenges\nin the realm of speech processing, marked by issues such as background noise,\nreverberation, and overlapping speech. Overcoming these complexities requires\nleveraging contextual cues to separate target speech from a cacophonous mix,\nenabling accurate recognition. Among these cues, the 3D spatial feature has\nemerged as a cutting-edge solution, particularly when equipped with spatial\ninformation about the target speaker. Its exceptional ability to discern the\ntarget speaker within mixed audio, often rendering intermediate processing\nredundant, paves the way for the direct training of \"All-in-one\" ASR models.\nThese models have demonstrated commendable performance on both simulated and\nreal-world data. In this paper, we extend this approach to the MISP dataset to\nfurther validate its efficacy. We delve into the challenges encountered and\ninsights gained when applying 3D spatial features to MISP, while also exploring\npreliminary experiments involving the replacement of these features with more\ncomplex input and models.", "published": "2023-10-05 21:16:54", "link": "http://arxiv.org/abs/2310.03901v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "EFFUSE: Efficient Self-Supervised Feature Fusion for E2E ASR in Low\n  Resource and Multilingual Scenarios", "abstract": "Self-Supervised Learning (SSL) models have demonstrated exceptional\nperformance in various speech tasks, particularly in low-resource and\nmultilingual domains. Recent works show that fusing diverse SSL models could\nachieve superior performance compared to using one SSL model. However, fusing\nmodels increases the overall parameter size, leading to higher computational\ncosts. We propose EFFUSE, a novel approach that uses a single SSL model to\nmimic the features of multiple SSL models via prediction, resulting in a\nlightweight framework with competitive performance. Our experiments show that\nEFFUSE outperforms individual SSL models in multilingual speech recognition\ntasks. Our best performing model achieves an average SUPERB score increase of\n63.5 (6.3%) from the SSL baselines in Multilingual Speech Universal PERformance\nBenchmark (ML-SUPERB), while decreasing parameter size on average by 317M\nparameters (49%) from the fusion models.", "published": "2023-10-05 23:05:05", "link": "http://arxiv.org/abs/2310.03938v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Integrated Algorithm for Robust and Imperceptible Audio Adversarial\n  Examples", "abstract": "Audio adversarial examples are audio files that have been manipulated to fool\nan automatic speech recognition (ASR) system, while still sounding benign to a\nhuman listener. Most methods to generate such samples are based on a two-step\nalgorithm: first, a viable adversarial audio file is produced, then, this is\nfine-tuned with respect to perceptibility and robustness. In this work, we\npresent an integrated algorithm that uses psychoacoustic models and room\nimpulse responses (RIR) in the generation step. The RIRs are dynamically\ncreated by a neural network during the generation process to simulate a\nphysical environment to harden our examples against transformations experienced\nin over-the-air attacks. We compare the different approaches in three\nexperiments: in a simulated environment and in a realistic over-the-air\nscenario to evaluate the robustness, and in a human study to evaluate the\nperceptibility. Our algorithms considering psychoacoustics only or in addition\nto the robustness show an improvement in the signal-to-noise ratio (SNR) as\nwell as in the human perception study, at the cost of an increased word error\nrate (WER).", "published": "2023-10-05 06:59:09", "link": "http://arxiv.org/abs/2310.03349v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The ICASSP SP Cadenza Challenge: Music Demixing/Remixing for Hearing\n  Aids", "abstract": "This paper reports on the design and results of the 2024 ICASSP SP Cadenza\nChallenge: Music Demixing/Remixing for Hearing Aids. The Cadenza project is\nworking to enhance the audio quality of music for those with a hearing loss.\nThe scenario for the challenge was listening to stereo reproduction over\nloudspeakers via hearing aids. The task was to: decompose pop/rock music into\nvocal, drums, bass and other (VDBO); rebalance the different tracks with\nspecified gains and then remixing back to stereo. End-to-end approaches were\nalso accepted. 17 systems were submitted by 11 teams. Causal systems performed\npoorer than non-causal approaches. 9 systems beat the baseline. A common\napproach was to fine-tuning pretrained demixing models. The best approach used\nan ensemble of models.", "published": "2023-10-05 11:46:32", "link": "http://arxiv.org/abs/2310.03480v2", "categories": ["eess.AS", "cs.LG", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Deep Generative Models of Music Expectation", "abstract": "A prominent theory of affective response to music revolves around the\nconcepts of surprisal and expectation. In prior work, this idea has been\noperationalized in the form of probabilistic models of music which allow for\nprecise computation of song (or note-by-note) probabilities, conditioned on a\n'training set' of prior musical or cultural experiences. To date, however,\nthese models have been limited to compute exact probabilities through\nhand-crafted features or restricted to linear models which are likely not\nsufficient to represent the complex conditional distributions present in music.\nIn this work, we propose to use modern deep probabilistic generative models in\nthe form of a Diffusion Model to compute an approximate likelihood of a musical\ninput sequence. Unlike prior work, such a generative model parameterized by\ndeep neural networks is able to learn complex non-linear features directly from\na training set itself. In doing so, we expect to find that such models are able\nto more accurately represent the 'surprisal' of music for human listeners. From\nthe literature, it is known that there is an inverted U-shaped relationship\nbetween surprisal and the amount human subjects 'like' a given song. In this\nwork we show that pre-trained diffusion models indeed yield musical surprisal\nvalues which exhibit a negative quadratic relationship with measured subject\n'liking' ratings, and that the quality of this relationship is competitive with\nstate of the art methods such as IDyOM. We therefore present this model a\npreliminary step in developing modern deep generative models of music\nexpectation and subjective likability.", "published": "2023-10-05 12:25:39", "link": "http://arxiv.org/abs/2310.03500v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Diffusion Models as Masked Audio-Video Learners", "abstract": "Over the past several years, the synchronization between audio and visual\nsignals has been leveraged to learn richer audio-visual representations. Aided\nby the large availability of unlabeled videos, many unsupervised training\nframeworks have demonstrated impressive results in various downstream audio and\nvideo tasks. Recently, Masked Audio-Video Learners (MAViL) has emerged as a\nstate-of-the-art audio-video pre-training framework. MAViL couples contrastive\nlearning with masked autoencoding to jointly reconstruct audio spectrograms and\nvideo frames by fusing information from both modalities. In this paper, we\nstudy the potential synergy between diffusion models and MAViL, seeking to\nderive mutual benefits from these two frameworks. The incorporation of\ndiffusion into MAViL, combined with various training efficiency methodologies\nthat include the utilization of a masking ratio curriculum and adaptive batch\nsizing, results in a notable 32% reduction in pre-training Floating-Point\nOperations (FLOPS) and an 18% decrease in pre-training wall clock time.\nCrucially, this enhanced efficiency does not compromise the model's performance\nin downstream audio-classification tasks when compared to MAViL's performance.", "published": "2023-10-05 23:00:27", "link": "http://arxiv.org/abs/2310.03937v2", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
